{
    "abstractText": "Language features are evolving in real-world social media, resulting in the deteriorating performance of text classification in dynamics. To address this challenge, we study temporal adaptation, where models trained on past data are tested in the future. Most prior work focused on continued pretraining or knowledge updating, which may compromise their performance on noisy social media data. To tackle this issue, we reflect feature change via modeling latent topic evolution and propose a novel model, VIBE: Variational Information Bottleneck for Evolutions. Concretely, we first employ two Information Bottleneck (IB) regularizers to distinguish past and future topics. Then, the distinguished topics work as adaptive features via multi-task training with timestamp and classlabel prediction. In adaptive learning, VIBE utilizes retrieved unlabeled data from online streams created posterior to training data time. Substantial Twitter experiments on three classification tasks show that our model, with only 3% of data, significantly outperforms previous state-of-the-art continued-pretraining methods.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yuji Zhang"
        },
        {
            "affiliations": [],
            "name": "Jing Li"
        },
        {
            "affiliations": [],
            "name": "Wenjie Li"
        }
    ],
    "id": "SP:fbfcaf0939d2b26a81dc28f38b1fe99029ba44fe",
    "references": [
        {
            "authors": [
                "Alexander A. Alemi",
                "Ian Fischer",
                "Joshua V. Dillon",
                "Kevin Murphy."
            ],
            "title": "Deep variational information bottleneck",
            "venue": "International Conference on Learning Representations.",
            "year": 2017
        },
        {
            "authors": [
                "JinYeong Bak",
                "Chin-Yew Lin",
                "Alice Oh."
            ],
            "title": "Selfdisclosure topic model for classifying and analyzing Twitter conversations",
            "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1986\u20131996,",
            "year": 2014
        },
        {
            "authors": [
                "Thirunavukarasu Balasubramaniam",
                "Richi Nayak",
                "Khanh Luong",
                "Md Abul Bashar."
            ],
            "title": "Identifying covid-19 misinformation tweets and learning their spatio-temporal topic dynamics using nonnegative coupled matrix tensor factorization",
            "venue": "Social Net-",
            "year": 2021
        },
        {
            "authors": [
                "Emily M Bender",
                "Timnit Gebru",
                "Angelina"
            ],
            "title": "McMillanMajor, and Shmargaret Shmitchell",
            "venue": "In Proceedings of the 2021 ACM conference on fairness, accountability, and transparency,",
            "year": 2021
        },
        {
            "authors": [
                "Jiefeng Chen",
                "Yixuan Li",
                "Xi Wu",
                "Yingyu Liang",
                "Somesh Jha."
            ],
            "title": "ATOM: robustifying out-ofdistribution detection using outlier mining",
            "venue": "Machine Learning and Knowledge Discovery in Databases. Research Track - European Conference,",
            "year": 2021
        },
        {
            "authors": [
                "Kai Chen",
                "Ye Wang",
                "Yitong Li",
                "Aiping Li."
            ],
            "title": "RotateQVS: Representing temporal information as rotations in quaternion vector space for temporal knowledge graph completion",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Compu-",
            "year": 2022
        },
        {
            "authors": [
                "Yankai Chen",
                "Yifei Zhang",
                "Menglin Yang",
                "Zixing Song",
                "Chen Ma",
                "Irwin King."
            ],
            "title": "WSFE: wasserstein sub-graph feature encoder for effective user segmentation in collaborative filtering",
            "venue": "CoRR, abs/2305.04410.",
            "year": 2023
        },
        {
            "authors": [
                "P Kingma Diederik",
                "Max Welling"
            ],
            "title": "Autoencoding variational bayes",
            "venue": "In Proceedings of the International Conference on Learning Representations (ICLR),",
            "year": 2014
        },
        {
            "authors": [
                "Yaroslav Ganin",
                "Evgeniya Ustinova",
                "Hana Ajakan",
                "Pascal Germain",
                "Hugo Larochelle",
                "Fran\u00e7ois Laviolette",
                "Mario Marchand",
                "Victor Lempitsky."
            ],
            "title": "Domain-adversarial training of neural networks",
            "venue": "The journal of machine learning research, 17(1):2096\u2013",
            "year": 2016
        },
        {
            "authors": [
                "Luyu Gao",
                "Jamie Callan."
            ],
            "title": "Condenser: a pretraining architecture for dense retrieval",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 981\u2013993, Online and Punta Cana, Dominican Republic. Asso-",
            "year": 2021
        },
        {
            "authors": [
                "Kyle Glandt",
                "Sarthak Khanal",
                "Yingjie Li",
                "Doina Caragea",
                "Cornelia Caragea."
            ],
            "title": "Stance detection in COVID-19 tweets",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Confer-",
            "year": 2021
        },
        {
            "authors": [
                "Arthur Gretton",
                "Karsten M. Borgwardt",
                "Malte J. Rasch",
                "Bernhard Sch\u00f6lkopf",
                "Alexander J. Smola."
            ],
            "title": "A kernel two-sample test",
            "venue": "J. Mach. Learn. Res., 13:723\u2013773.",
            "year": 2012
        },
        {
            "authors": [
                "Suchin Gururangan",
                "Ana Marasovi\u0107",
                "Swabha Swayamdipta",
                "Kyle Lo",
                "Iz Beltagy",
                "Doug Downey",
                "Noah A. Smith."
            ],
            "title": "Don\u2019t stop pretraining: Adapt language models to domains and tasks",
            "venue": "Proceedings of the 58th Annual Meeting of the",
            "year": 2020
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Xiaoyuan Liu",
                "Eric Wallace",
                "Adam Dziedzic",
                "Rishabh Krishnan",
                "Dawn Song."
            ],
            "title": "Pretrained transformers improve out-of-distribution robustness",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
            "year": 2020
        },
        {
            "authors": [
                "Hao Huang",
                "Xiubo Geng",
                "Guodong Long",
                "Daxin Jiang."
            ],
            "title": "Understand before answer: Improve temporal reading comprehension via precise question understanding",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Associ-",
            "year": 2022
        },
        {
            "authors": [
                "Xiaolei Huang",
                "Michael J. Paul."
            ],
            "title": "Neural temporality adaptation for document classification: Diachronic word embeddings and domain adaptation models",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,",
            "year": 2019
        },
        {
            "authors": [
                "HyeongJoo Hwang",
                "Geon-Hyeong Kim",
                "Seunghoon Hong",
                "Kee-Eung Kim."
            ],
            "title": "Variational interaction information maximization for cross-domain disentanglement",
            "venue": "Advances in Neural Information Processing Systems 33: Annual Conference on Neu-",
            "year": 2020
        },
        {
            "authors": [
                "Constantinos Karouzos",
                "Georgios Paraskevopoulos",
                "Alexandros Potamianos."
            ],
            "title": "UDALM: Unsupervised domain adaptation through language modeling",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computa-",
            "year": 2021
        },
        {
            "authors": [
                "Vladimir Karpukhin",
                "Barlas Oguz",
                "Sewon Min",
                "Patrick Lewis",
                "Ledell Wu",
                "Sergey Edunov",
                "Danqi Chen",
                "Wen-tau Yih."
            ],
            "title": "Dense passage retrieval for opendomain question answering",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural",
            "year": 2020
        },
        {
            "authors": [
                "Uszkoreit",
                "Quoc Le",
                "Slav Petrov."
            ],
            "title": "Natural questions: a benchmark for question answering research",
            "venue": "Trans. Assoc. Comput. Linguistics, 7:452\u2013 466.",
            "year": 2019
        },
        {
            "authors": [
                "Haoliang Li",
                "Sinno Jialin Pan",
                "Shiqi Wang",
                "Alex C Kot."
            ],
            "title": "Domain generalization with adversarial feature learning",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5400\u20135409.",
            "year": 2018
        },
        {
            "authors": [
                "Zixuan Li",
                "Saiping Guan",
                "Xiaolong Jin",
                "Weihua Peng",
                "Yajuan Lyu",
                "Yong Zhu",
                "Long Bai",
                "Wei Li",
                "Jiafeng Guo",
                "Xueqi Cheng."
            ],
            "title": "Complex evolutional pattern learning for temporal knowledge graph reasoning",
            "venue": "Proceedings of the 60th Annual Meet-",
            "year": 2022
        },
        {
            "authors": [
                "Langzhang Liang",
                "Zenglin Xu",
                "Zixing Song",
                "Irwin King",
                "Jieping Ye."
            ],
            "title": "Resnorm: Tackling long-tailed degree distribution issue in graph neural networks via normalization",
            "venue": "CoRR, abs/2206.08181.",
            "year": 2022
        },
        {
            "authors": [
                "Xiao Lin",
                "Indranil Sur",
                "Samuel A. Nastase",
                "Uri Hasson",
                "Ajay Divakaran",
                "Mohamed R. Amer"
            ],
            "title": "A data-efficient mutual information neural estimator for statistical dependency testing",
            "year": 2020
        },
        {
            "authors": [
                "Hong Liu",
                "Mingsheng Long",
                "Jianmin Wang",
                "Yu Wang."
            ],
            "title": "Learning to adapt to evolving domains",
            "venue": "Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020,",
            "year": 2020
        },
        {
            "authors": [
                "Xiaodong Liu",
                "Hao Cheng",
                "Pengcheng He",
                "Weizhu Chen",
                "Yu Wang",
                "Hoifung Poon",
                "Jianfeng Gao."
            ],
            "title": "Adversarial training for large neural language models",
            "venue": "CoRR, abs/2004.08994.",
            "year": 2020
        },
        {
            "authors": [
                "Kelvin Luu",
                "Daniel Khashabi",
                "Suchin Gururangan",
                "Karishma Mandyam",
                "Noah A. Smith."
            ],
            "title": "Time waits for no one! analysis and challenges of temporal misalignment",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the",
            "year": 2022
        },
        {
            "authors": [
                "Binny Mathew",
                "Punyajoy Saha",
                "Seid Muhie Yimam",
                "Chris Biemann",
                "Pawan Goyal",
                "Animesh Mukherjee."
            ],
            "title": "Hatexplain: A benchmark dataset for explainable hate speech detection",
            "venue": "Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021,",
            "year": 2021
        },
        {
            "authors": [
                "W McGill."
            ],
            "title": "Multivariate information transmission",
            "venue": "Transactions of the IRE Professional Group on Information Theory, 4(4):93\u2013111.",
            "year": 1954
        },
        {
            "authors": [
                "Yishu Miao",
                "Edward Grefenstette",
                "Phil Blunsom."
            ],
            "title": "Discovering discrete latent topics with neural variational inference",
            "venue": "Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017,",
            "year": 2017
        },
        {
            "authors": [
                "Debashis Naskar",
                "Sidahmed Mokaddem",
                "Miguel Rebollo",
                "Eva Onaindia."
            ],
            "title": "Sentiment analysis in social networks through topic modeling",
            "venue": "Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC\u201916), pages",
            "year": 2016
        },
        {
            "authors": [
                "Dat Quoc Nguyen",
                "Thanh Vu",
                "Anh Tuan Nguyen."
            ],
            "title": "BERTweet: A pre-trained language model for English tweets",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 9\u201314, On-",
            "year": 2020
        },
        {
            "authors": [
                "Ajeet Ram Pathak",
                "Manjusha Pandey",
                "Siddharth Rautaray."
            ],
            "title": "Adaptive model for dynamic and temporal topic modeling from big data using deep learning architecture",
            "venue": "International Journal of Intelligent Systems and Applications, 9(6):13.",
            "year": 2019
        },
        {
            "authors": [
                "Ori Ram",
                "Gal Shachaf",
                "Omer Levy",
                "Jonathan Berant",
                "Amir Globerson."
            ],
            "title": "Learning to retrieve passages without supervision",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Hu-",
            "year": 2022
        },
        {
            "authors": [
                "Stephen Robertson",
                "Hugo Zaragoza"
            ],
            "title": "The probabilistic relevance framework: Bm25 and beyond",
            "venue": "Foundations and Trends\u00ae in Information Retrieval,",
            "year": 2009
        },
        {
            "authors": [
                "Elan Rosenfeld",
                "Pradeep Kumar Ravikumar",
                "Andrej Risteski."
            ],
            "title": "Domain-adjusted regression or: ERM may already learn features sufficient for out-ofdistribution generalization",
            "venue": "NeurIPS 2022 Workshop on Distribution Shifts: Connecting Methods and",
            "year": 2022
        },
        {
            "authors": [
                "Paul R\u00f6ttger",
                "Janet Pierrehumbert."
            ],
            "title": "Temporal adaptation of BERT and performance on downstream document classification: Insights from social media",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2400\u20132412, Punta",
            "year": 2021
        },
        {
            "authors": [
                "Ankan Saha",
                "Vikas Sindhwani."
            ],
            "title": "Learning evolving and emerging topics in social media: a dynamic nmf approach with temporal regularization",
            "venue": "Proceedings of the fifth ACM international conference on Web search and data mining, pages 693\u2013702.",
            "year": 2012
        },
        {
            "authors": [
                "Chao Shang",
                "Guangtao Wang",
                "Peng Qi",
                "Jing Huang."
            ],
            "title": "Improving time sensitivity for question answering over temporal knowledge graphs",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
            "year": 2022
        },
        {
            "authors": [
                "Zheyan Shen",
                "Jiashuo Liu",
                "Yue He",
                "Xingxuan Zhang",
                "Renzhe Xu",
                "Han Yu",
                "Peng Cui."
            ],
            "title": "Towards out-of-distribution generalization: A survey",
            "venue": "CoRR, abs/2108.13624.",
            "year": 2021
        },
        {
            "authors": [
                "Zixing Song",
                "Irwin King."
            ],
            "title": "Hierarchical heterogeneous graph attention network for syntax-aware summarization",
            "venue": "AAAI, pages 11340\u201311348. AAAI Press.",
            "year": 2022
        },
        {
            "authors": [
                "Nikita Srivatsan",
                "Zachary Wojtowicz",
                "Taylor BergKirkpatrick."
            ],
            "title": "Modeling online discourse with coupled distributed topics",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4673\u20134682, Brussels,",
            "year": 2018
        },
        {
            "authors": [
                "Chenkai Sun",
                "Jinning Li",
                "Yi Fung",
                "Hou P. Chan",
                "Tarek Abdelzaher",
                "Chengxiang Zhai",
                "Heng Ji."
            ],
            "title": "Decoding the silent majority: Inducing belief augmented social graph with large language model for response forecasting",
            "venue": "The 2023 Conference on Em-",
            "year": 2023
        },
        {
            "authors": [
                "N TISHBY."
            ],
            "title": "The information bottleneck method",
            "venue": "Proc. of the 37th Allerton Conference on Communication and Computation, 1999.",
            "year": 1999
        },
        {
            "authors": [
                "Naftali Tishby",
                "Noga Zaslavsky."
            ],
            "title": "Deep learning and the information bottleneck principle",
            "venue": "2015 ieee information theory workshop (itw), pages 1\u20135. IEEE.",
            "year": 2015
        },
        {
            "authors": [
                "Hugo Touvron",
                "Thibaut Lavril",
                "Gautier Izacard",
                "Xavier Martinet",
                "Marie-Anne Lachaux",
                "Timoth\u00e9e Lacroix",
                "Baptiste Rozi\u00e8re",
                "Naman Goyal",
                "Eric Hambro",
                "Faisal Azhar"
            ],
            "title": "Llama: Open and efficient foundation language models",
            "year": 2023
        },
        {
            "authors": [
                "Vladimir Vapnik",
                "Vlamimir Vapnik."
            ],
            "title": "Statistical learning theory wiley",
            "venue": "New York, 1(624):2.",
            "year": 1998
        },
        {
            "authors": [
                "Yue Wang",
                "Jing Li",
                "Hou Pong Chan",
                "Irwin King",
                "Michael R. Lyu",
                "Shuming Shi."
            ],
            "title": "Topicaware neural keyphrase generation for social media language",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,",
            "year": 2019
        },
        {
            "authors": [
                "Tailin Wu",
                "Hongyu Ren",
                "Pan Li",
                "Jure Leskovec."
            ],
            "title": "Graph information bottleneck",
            "venue": "Advances in Neural Information Processing Systems, volume 33, pages 20437\u201320448. Curran Associates, Inc.",
            "year": 2020
        },
        {
            "authors": [
                "Sang Michael Xie",
                "Ananya Kumar",
                "Robbie Jones",
                "Fereshte Khani",
                "Tengyu Ma",
                "Percy Liang."
            ],
            "title": "In-n-out: Pre-training and self-training using auxiliary information for out-of-distribution robustness",
            "venue": "9th International Conference on Learning Repre-",
            "year": 2021
        },
        {
            "authors": [
                "Shen Yan",
                "Huan Song",
                "Nanxiang Li",
                "Lincan Zou",
                "Liu Ren."
            ],
            "title": "Improve unsupervised domain adaptation with mixup training",
            "venue": "stat, 1050:3.",
            "year": 2020
        },
        {
            "authors": [
                "Hai Ye",
                "Qingyu Tan",
                "Ruidan He",
                "Juntao Li",
                "Hwee Tou Ng",
                "Lidong Bing."
            ],
            "title": "Feature adaptation of pre-trained language models across languages and domains with robust self-training",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural",
            "year": 2020
        },
        {
            "authors": [
                "Nanyang Ye",
                "Kaican Li",
                "Lanqing Hong",
                "Haoyue Bai",
                "Yiting Chen",
                "Fengwei Zhou",
                "Zhenguo Li."
            ],
            "title": "Ood-bench: Benchmarking and understanding outof-distribution generalization datasets and algorithms",
            "venue": "arXiv preprint arXiv:2106.03721, 1(3):5.",
            "year": 2021
        },
        {
            "authors": [
                "Jichuan Zeng",
                "Jing Li",
                "Yan Song",
                "Cuiyun Gao",
                "Michael R. Lyu",
                "Irwin King."
            ],
            "title": "Topic memory networks for short text classification",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3120\u20133131,",
            "year": 2018
        },
        {
            "authors": [
                "Yuji Zhang",
                "Jing Li."
            ],
            "title": "Time will change things: An empirical study on dynamic language understanding in social media classification",
            "venue": "arXiv preprint arXiv:2210.02857.",
            "year": 2022
        },
        {
            "authors": [
                "Yuji Zhang",
                "Yubo Zhang",
                "Chunpu Xu",
                "Jing Li",
                "Ziyan Jiang",
                "Baolin Peng."
            ],
            "title": "HowYouTagTweets: Learning user hashtagging preferences via personalized topic attention",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Lan-",
            "year": 2021
        },
        {
            "authors": [
                "Caleb Ziems",
                "William Held",
                "Omar Shaikh",
                "Jiaao Chen",
                "Zhehao Zhang",
                "Diyi Yang"
            ],
            "title": "Can large language models transform computational social science? arXiv preprint arXiv:2305.03514",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Our ever-changing world leads to the continuous data distribution shifting in social media. As a result, language features, formed by word patterns, will consequently evolve over time. Therefore, many text classification models, including the state-of-the-art ones based on pretraining, demonstrate compromised results facing shifted fea-\n\u2217 Code will come later at https://github.com/ CelestineZYJ/VIBE-Temporal-Adaptation. For technical questions, contact <yu-ji.zhang@connect.polyu.hk>.\n\u2020This work is supported by the NSFC Young Scientists Fund (Project No. 62006203), a grant from the Research Grants Council of the Hong Kong Special Administrative Region, China (Project No. PolyU/25200821), the Innovation and Technology Fund (Project No. PRP/047/22FX), and PolyU Internal Fund from RC-DSAI (Project No. 1-CE1E). Jing Li is corresponding author. The authors would like to thank the anonymous reviewers from EMNLP 2023 for their insightful suggestions of various aspects of this work.\ntures (Hendrycks et al., 2020), when there is a time gap between training and test data. R\u00f6ttger and Pierrehumbert (2021) and Luu et al. (2022) then defined a temporal adaptation task to adapt a model trained on the past data to the shifted future data.\nTo tackle this task, R\u00f6ttger and Pierrehumbert (2021) and Luu et al. (2022) apply continued pretraining on the temporally-updated data to catch the distribution shift yet observe limited enhancement with noisy contexts, meanwhile relying heavily on computational resources. Others (Shang et al., 2022; Li et al., 2022; Chen et al., 2022) resort to knowledge-based methods, which require high-quality data unavailable on social media.\nGiven these concerns, how shall we effectively approach temporal adaptation in noisy social media contexts? Inspired by the success of latent topics in many social media tasks (Zhang et al., 2021; Pathak et al., 2019), our solution is to explore latent topic evolution in representing feature change (Zhang and Li, 2022). To better illustrate our idea, we exemplify the COVID-19 stance detection dataset (Glandt et al., 2021). Here we employ the Neural Topic Model (NTM) (Miao et al., 2017) to capture the latent topics in five sequential periods and show the top topic words by likelihoods in Figure 1. As can be seen from the topic evolution, users changed discussion points rapidly over time, where they shifted focus from concerns to the virus itself (e.g., \u201cImmune Compromise\u201d, \u201cSymptom\u201d) to the disappointment to the former US President Trump (e.g., \u201cTrump Land Slid\u201d, \u201cLying Trump\u201d).\nFrom this example, we can learn that topics can help characterize evolving social media environ-\nments, which motivates us to propose topic-driven temporal adaptation. Specifically, we employ an NTM for featuring latent topics, which is under the control of Information Bottleneck (IB) regularizers to distinguish past-exclusive, future-exclusive, and time-invariant latent topics to represent evolution. Here the time-invariant topic is to encode the task-relevant semantics unchanged over time; the past- and future-exclusive latent topics capture the time-variant topics by modeling the data from the past and future. To further align the topic evolution to a specific classification task, we leverage multitask training to infer timestamps and class labels jointly. Our model is then named VIBE, short for Variational Information Bottleneck for Evolutions.\nTo study how VIBE functions on social media data, we utilize the data temporally posterior to training data to learn adaptation (henceforth adaptive data). Here we consider two scenarios. First, in an ideal circumstance, the data between training and test time from the original dataset can serve as the golden adaptive data. Second, when it is inaccessible in a realistic setup, we retrieve relevant online streams as the retrieved adaptive data.\nFor experiments, we conduct a temporal adaptation setup on three text classification tasks with data from Twitter platform and draw the following results. First, the main comparison shows that VIBE outperforms the previous state-of-the-art (SOTA) models with either golden or retrieved adaptive data. For example, VIBE brings a relative increase of 5.5% over the baseline for all datasets, using only 3% of the continued pretraining data. Then, an ablation study exhibits the positive contribution of VIBE\u2019s sub-modules, and VIBE is insensitive to retrieved adaptive data quality. Next, we examine the effects of adaptive data quantity on VIBE, and the results show that VIBE relies much less on data volume compared to continued pretraining. At last, we present a case study to interpret how VIBE learns the evolving topic patterns for adaption.\nTo sum up, we present three-fold contributions: \u2022 To the best of our knowledge, we are the first to explore how to model latent topic evolution as temporal adaptive features from noisy contexts. \u2022 We propose a novel model VIBE coupling the information bottleneck principle and neural topic model for temporal adaptation in social media. \u2022 Substantial Experiments show that VIBE significantly outperforms previous SOTA models, and is insensitive to adaptive data quantity."
        },
        {
            "heading": "2 Related Work",
            "text": ""
        },
        {
            "heading": "2.1 Temporal Adaptation",
            "text": "Our task is in line with temporal adaptation, a subtask of domain adaptation (DA). While prior DA studies focus on domain gaps exhibiting intermittent change, temporal distribution shift usually happens step by step, forming a successive process. Liu et al. (2020a) showed the impracticality of applying traditional DA on temporal adaptation because the former aims to eliminate the distribution shift (Xie et al., 2021; Shen et al., 2021; Liu et al., 2020b; Hendrycks et al., 2020; Chen et al., 2021) while the adaptive objective for the latter is to track the shifts. To track the shifts, Huang and Paul (2019) utilized more labeled data to adapt, resulting in high costs in the ever-evolving environments.\nUnsupervised temporal adaptation was therefore advanced in two lines. One is based on continued pretraining on large-scale future data (R\u00f6ttger and Pierrehumbert, 2021; Luu et al., 2022; Huang et al., 2022), relying on extensive computational resources whereas interior with a noisy context. The other adopted external knowledge resources (Shang et al., 2022; Li et al., 2022; Chen et al., 2022, 2023; Song and King, 2022; Liang et al., 2022), requiring high-quality data unavailable in social media. Differently, we feature evolving latent topics with less reliance on data quality and quantity."
        },
        {
            "heading": "2.2 Topic Modeling for Social Media",
            "text": "A topic model is a statistical model for discovering abstract topics occurring in a collection of texts (Miao et al., 2017), whose potential for dealing with dynamics comes from its capability to cluster texts exhibiting similar word statistics. Many previous studies (Bak et al., 2014; Naskar et al., 2016; Srivatsan et al., 2018) have demonstrated the effectiveness of topic models to encode social media texts. Furthermore, Saha and Sindhwani (2012); Pathak et al. (2019); Balasubramaniam et al. (2021) analyzed the evolving topic semantics via topic matrix factorization. Nevertheless, none of the above studies explore how latent topics help social media temporal adaptation, which is a gap we mitigate."
        },
        {
            "heading": "3 Preliminaries",
            "text": "To prepare readers for understanding VIBE\u2019s model design (to appear in \u00a74), we present a preliminary section here with the problem formulation in \u00a73.1, information bottleneck principle (VIBE\u2019s theoretical basis) in \u00a73.2, and the model overview in \u00a73.3."
        },
        {
            "heading": "3.1 Problem Formulation",
            "text": "We explore temporal adaptation in text classification on Twitter, where an input tweet sample denotes t, and its class label c. Following Luu et al. (2022), the training data is from the past period X with labels (denoted DX = (tx, cx)); In contrast, adaptive and test data are unlabeled and from the future period Y (denoted DY = (ty, _)). Our goal here is to enable VIBE to infer accurate future label cy via exploring the topic evolution from tx to ty.\nTo that end, VIBE explicitly captures timeinvariant, past-exclusive, and future-exclusive topics, respectively denoted zs, zx, and zy, which are extracted and differentiated from the past and future tweets, tx and ty. zx represents the past and can only be learned from tx, not from ty; likewise, zy reflects the future and exclusively originates from ty without information from tx. The shared part of tx and ty is modeled by zs, unchanged over time yet helpful in indicating class labels.\nFor adaption, VIBE should be fed with the adaptive data, N samples of unlabeled ty from the future time Y (from either golden or retrieved adaptive data) are selected on relevance to each past tweet tx (from training data). Taking tx (past) and ty (future) as input, VIBE adopts the information bottleneck principle (introduced in \u00a73.2) to distinguish topics zs, zx, and zy, which will be detailed in \u00a74.2."
        },
        {
            "heading": "3.2 Information Bottleneck Principle",
            "text": "Information Bottleneck (IB) Principal (TISHBY, 1999; Tishby and Zaslavsky, 2015) was designed for pursuing a maximally informative representation with a tradeoff for general predictive ability (Alemi et al., 2017). Its objective function is:\nLIB = I(Z;X)\u2212 I(Z;Y ). (1)\nHere I(\u00b7; \u00b7) is the mutual information of two random variables. Minimizing LIB enables Z to be maximally expressive for Y with a tradeoff of minimally relying on X (Lin et al., 2020; Wu et al., 2020). In VIBE, take time-invariant topics zs as an example, for the past tweets tx, the IB regularizers force the time-invariant topics zs to be expressive for tx, meanwhile penalizing disturbance from future tweets ty. For the future tweets ty, vice versa. Thus, the representation of zs is distinguished from time-variant zx and zy. For tractability of mutual information, the variational approximation (Alemi et al., 2017) is adopted to optimize the IB objective function by constructing a lower bound."
        },
        {
            "heading": "3.3 Model Overview",
            "text": "Based on \u00a73.1 and 3.2, we then brief an overview of VIBE before detailing its design in \u00a74.\nFor modeling latent topics, we adopt NTM (the vanilla NTM is elaborated in A.1) to take bag-ofword (BoW) vectors of past tweets tx and future tweets ty as input to derive past-exclusive, futureexclusive, and time-invariant latent topic variables. During the learning process, the two BoW vectors are first encoded into latent topics zs, zx, and zy, then decoded to reconstruct the BoW vectors. Their disentanglement are handled by the IB regularizers, which will be elaborated in \u00a74.2.\nIn addition, to encode more useful features, zs\nis endowed with task-specific semantics by using (zs | tx) to infer the class label cx of past tweet tx. We then employ the trained classifier C1task to pseudo-label the future tweets as cy\u2032. With the reconstructed BoW of tx and ty, carrying the temporal topic patterns from the input, we employ multitask training to infer timestamps and class labels to align topic evolution features to classification."
        },
        {
            "heading": "4 VIBE Model",
            "text": "Here \u00a74.1 first describes how NTM explores the distribution of past and future tweets. Then, we discuss the design of time-oriented (\u00a74.2) and taskoriented training (\u00a74.3) to feature the time-invariant and time-variant latent topic variables by IB. Next, \u00a74.4 presents how to jointly optimize the NTM and classifier, followed by the application of the topic evolution features for final prediction in \u00a74.5."
        },
        {
            "heading": "4.1 NTM with Multiple Latent Variables",
            "text": "As discussed in \u00a73.1, NTM is used to capture the latent topic evolution by generating the past and future tweets tx, ty based on latent topics. We assume that paired past-future tweet samples (tx, ty) come from the joint distribution (tx, ty)\u223cpD(tx, ty). The distribution pD(tx, ty) is jointly conditioned on time-variant topics (past-exclusive topics zx and future-exclusive topics zy) and time-invariant topics zs. To learn the latent topics, we pursue the maximization of the marginal likelihood (Diederik et al., 2014) of the distribution:\np\u03b8(t x, ty) = \u222b dzxdzsdzyp\u03b8X (t x|zx, zs)\np\u03b8Y (t y|zy, zs)p(zx)p(zs)p(zy)\n(2)\nHere the involved parameter \u03b8={\u03b8X , \u03b8Y } models the conditional distributions; our training objective is to maximize the generative model p\u03b8(tx, ty). To allow tractable optimization in training, we substitue p\u03b8(zx, zs, zy | tx, ty) as the approximated posterior q\u03d5(zx, zs, zy | tx, ty) based on the variational inference method (Diederik et al., 2014) and the approximated posterior is factorized into:\nq\u03d5(z x, zs, zy|tx, ty) = q\u03d5X (z s|tx)q\u03d5S (z s|tx, ty)q\u03d5Y (z y|ty) (3) where \u03d5 = \u03d5X , \u03d5S , \u03d5Y denotes the entire encoder parameter \u2014 q\u03d5X and q\u03d5Y reflect the past-exclusive and future-exclusive topics to X (past time) and Y (future time), and q\u03d5S represents time-invariant topics. For inference, we derive evidence lower bound (ELBO) of p\u03b8(tx, ty) (More in A.2):\nlog p(tx, tx) \u2265 Eq(zx|ty)q(zs|tx,ty) [log p (tx | zx, zs)] +Eq(zy|ty)q(zs|tx,ty) [log p (ty | zy, zs)]\n\u2212DKL [q (zx|tx) \u2225p(zx)]\u2212DKL [q (zy|ty) \u2225p(zy)] \u2212DKL [q (zs|tx, ty) \u2225p(zs)]\n(4)\nHowever, three encoders q(zx | tx), q(zs | tx, ty), and q(zy | ty) possibly encodes zx, zy and zs with substantial overlaps in their captured semantics. To\ndifferentiate them for modeling topic evolution, we apply IB regularizers to force disentanglement of topics zx, zy, and zs following Hwang et al. (2020) (see \u00a74.2). Additionally, we endow latent topics zs with time-invariant task-specific semantics by training zs | tx to infer tx\u2019s label cx (see \u00a74.3)."
        },
        {
            "heading": "4.2 Time-Oriented Training for NTM",
            "text": "In the following, we elaborate on how the IB regularizers control NTM to learn the disentangled past-, future-exclusive and time-invariant topics.\nTime-Invariant IB Regularizer. This regularizer is designed to capture time-invariant topics zs shared by the past and future tweets, whose design follows the interaction information theory (McGill, 1954). Interaction information refers to generalizing mutual information to three or more random variables, representing the amount of shared information between them. Concretely, we denote the shared information between past time X and future time Y as ZS and present the formulation for interaction information I(X;Y ;ZS) as follows:\nI(X;Y ;ZS) = I(X;ZS)\u2212 I(X;ZS |Y ) (5)\nI(X;Y ;ZS) = I(Y ;ZS)\u2212 I(Y ;ZS |X) (6)\nThe above two equations represent I(X;Y ;ZS) through a symmetrical lens. As can be seen, in Eq. 5, we maximize the interaction information by maximizing the first term I(X;ZS) to make ZS expressive for past time X and minimizing the second term I(X;ZS |Y ) to penalize the disturbance from future time Y . Symmetrically, Eq. 6 encourages ZS to be expressive for future time Y and avoid being affected by past time X . Optimizing these two functions simultaneously would allow ZS to be time-invariantly expressive of X and Y .\nTime-Variant IB Regularizer. Our goal is to minimize the mutual information I(ZX ;ZS) and I(ZY ;ZS), thus past-, future-exclusive topics zx, zy are statistically independent of the timeinvariant topics zs, and vice versa. Here we only present the formulation for the past time period X , and that for future time Y can be inferred anomalously. The mutual information of ZX and ZS is:\nI(ZX ;ZS) = \u2212I(X;ZX , ZS) + I(X;ZX) + I(X;ZS). (7) I(X;ZX , ZS) is maximized to force ZS and ZX to be jointly informative of past time X , while penalizes last two terms to avoid either ZS or ZX to be individually expressive for X (more in A.2).\nJoint Regularization for IB Regularizers. Time-variant and time-invariant representations are jointly regularized. We combine Eq. 5 and 7 for time X\u2019s regularization, and time Y\u2019s is analogous.\nmax q\nI(X;Y ;ZS)\u2212 I(ZX ;ZS)\n= I(X;ZX , ZS)\u2212 I(X;ZX)\u2212 I(X;ZS |Y ). (8)\nmax q\nI(X;Y ;ZS)\u2212 I(ZY ;ZS)\n= I(Y ;ZY , ZS)\u2212 I(Y ;ZY )\u2212 I(Y ;ZS |Y ). (9)\nTractable Optimization for IB Regularizers. Since there are intractable integrals like unknown distribution pD(tx, ty) in mutual information terms mentioned above, we maximize generative distributions\u2019 lower bounds for traceability concerns. More details for Eq. 10-Eq. 12 are discussed in A.2.\nFor I(X;ZX ;ZY ), we derive its lower bound with the generative distribution p(tx | zx, zs):\nI ( X;ZX , ZS ) = Eq(zx,zs|tx)pD(tx) [ log\nq (tx|zx, zs) pD(tx) ] \u2265 H(X) + EpD(tx,ty)q(zx|tx)q(zs|tx,ty) [log p (t\nx|zx, zs)] (10)\nFor intractable \u2212I(X;ZX) (unknow distribution pD(tx)), the generative distribution p(zx) is adopted as the standard Gaussian, known as the Variational Information Bottleneck (Tishby and Zaslavsky, 2015). Its lower bound goes as follows:\nI(X;ZX) \u2265 EPD(tx)[DKL[q(zx|tx)\u2225p(zs)]] (11)\nLikewise, the lower bound for \u2212I(X;ZS |Y ) is:\n\u2212 I(X;ZS |Y ) = \u2212EpD(tx,ty)q(zs|tx,ty)[log q(zs|tx, ty) q(zs|ty) ] \u2265 \u2212EpD(tx,ty) [DKL [q (z s|tx, ty) \u2225ry (zs|ty)]]\n(12)\nSubstituting Eq. 10-12 to Eq. 8-9, we derive the lower bound for disentangling past time X and future time Y . For joint regularization on disentanglement and the maximum likelihood objective in Eq. 4, we combine them for joint maximization:\nmax p,q\nEq(zx,zs,zy,tx,ty)[log p(tx, ty, zx, zs, zy)\nq(zx, zs, zy|tx, ty) ]\n+ \u03bb(2 \u00b7 I(X;Y ;ZS)\u2212 I(ZX ;ZS)\u2212 I(ZY ;ZS)) \u2265 max\np,q,r (1 + \u03bb) \u00b7 EpD(tx,ty)[ELBO(p, q)]\n+ \u03bb \u00b7 EpD(tx,ty)[DKL[q(z s|tx, ty)\u2225p(zs)]] \u2212 \u03bb \u00b7 EpD(tx,ty)[DKL[q(z s|tx, ty)\u2225ry(zs|ty)] +DKL[q(z s|tx, ty)\u2225rx(zs|tx)]]\n(13)\nThis objective is denoted as Lntm (the NTM loss)."
        },
        {
            "heading": "4.3 Task-Oriented Training for NTM",
            "text": "We have discussed controlling the latent topics zs to represent time-invariant features shared by past\nand future. Then we dive deep into how to inject the task-specific features to zs by inferring class labels. Specifically, we adopt the posterior latent topics zs | tx (instead of directly using zs) to leverage the task-specific factor reflected by the class label cx and its input tx. Here we concatenate tweet tx\u2019s BERT embedding bt x and the topic features rx(zs | tx) yielded by NTM encoder (rx(\u00b7), shown in Eq. 12) for predicting the class label cx. At the output layer, the learned classification features are mapped to a label c\u0302t x with the formula below:\nc\u0302t x\n= fout(Wout \u00b7 ux + bout) (14)\nfout(\u00b7) is the activation function for classification output (e.g., softmax). Wout and bout are learnable parameters for training. ux couples the BERTencoded semantics (bt x ) and the implicit taskspecific topics via a multi-layer perceptron (MLP):\nux = fMLP (WMLP [b tx ; rx(zs|tx)] + bMLP ) (15)"
        },
        {
            "heading": "4.4 Joint Training for NTM and Classification",
            "text": "To coordinate NTM and classifier training, we combine classification loss and the NTM reconstruction loss together. For classification, the loss Lpast is based on cross-entropy (trained on labeled past data) Adding LNTM (Eq. 13) to Lpast, we get the joint optimization loss function for training the temporally-adaptive classifier C1task, whose jointtraining loss is the weighted sum of classification (task learning) and NTM (topic evolution) losses:\nL = Lpast + \u00b5 \u00b7 Lntm (16)\nHere \u00b5 trades off the effects of task and topic evolution learning. In this way, their parameters are updated together. After the training, we employ the trained C1task to pseudo label future tweets t\ny from adaptive data to form DY = (tx, cy\u2032) for the next stage training, which will be discussed in \u00a74.5."
        },
        {
            "heading": "4.5 Topic Space Projection",
            "text": "In the original design of NTM decoding (Miao et al., 2017), tx and ty are reconstructed based on a ruthless vocabulary dictionary. We tailor-make it to map the reconstructed tx and ty into a sphere space to align tweets with similar semantics to exhibit closer distance in topic space. Meanwhile, it is conducted via multi-task learning on predicting timestamps and class labels to inject time-awareness senses into the classification training:\ny\u0302t = fyout(Wyout \u00b7 ut + byout) (17)\nT\u0302 t = fTout(WTout \u00b7 ut + bTout) (18)\nf(\u00b7) is the activation function for classification output. W and b are learnable parameters for training. ut concatenates tweet t\u2019s BERT embedding bt and its reconstructed BoW vector for class label yt and timestamp T t prediction via a MLP. The two classifiers are named C2task and C 2 time. The cross-entropy losses for yt and T t prediction are denoted as Ltask and Ltime. We optimize their joint loss as follows:\nLsphere = Ltask + Ltime (19)"
        },
        {
            "heading": "5 Experimental Setup and Data Analysis",
            "text": "In the following, we introduce data (including its analysis) and baselines for the experimental setup.\nData Collection. As displayed in Table 1, we experiment with VIBE on three datasets for stance detection (Stance) (Glandt et al., 2021), hate speech (Hate) (Mathew et al., 2021), and hashtag (Hash) prediction. Their data is all from Twitter, and each sample is a tweet with a classification label. We show more details of datasets in A.3.\nData Setup. For the Stance and Hate dataset, tweets were sorted by time order, where the earliest 50% samples are for training and the latest 30% for test. The middle 20% tweets posted between training and test time were adopted for validation (5% random samples) and the golden adaptive data (the rest 15%). For Hash dataset whose data is unevenly distributed temporally, we adopted the absolute temporal split: Sep and Oct data for training,\nNov for adaptation and validation, Dec for the test. In our setup, training data (with labels) are from the past; adaptive and test data are from the future.\nFor gathering the retrieved adaptive data, we first analyzed the top 100 trendy words (excluding stopwords) in training data. Then, we searched their related tweets and sampled those posted in the same time period of test data (50 per day for each search word). Finally, a retrieval method (e.g., DPR) was adopted for ranking and selecting tweets with the top-N highest similarity to each training sample to form the adaptive data.\nData Analysis. As previously shown, Twitter usually challenges NLP models with its noisy data (Wang et al., 2019). We then exemplify the Stance dataset and provide analysis to quantify unique challenges from noisy data to temporal adaptation.\nFollowing Gururangan et al. (2020)\u2019s practice, we first analyze the vocabulary overlap of four subsets \u2014 training set, golden adaptive data (from the original dataset), retrieved adaptive data (external data retrieved from the wild), and test set. As shown in Figure 3, the vocabulary\u2019s inter-subset overlap ratio is low, around 40-50%, in contrast to 60-90% in non-Twitter data (Luu et al., 2022). It indicates the grand challenge of capturing temporal adaptive features on Twitter, echoing Huang and Paul (2019)\u2019s findings that Twitter data tends to generate a vocabulary gap faster than other data.\nWe then quantify the distribution shifts of training, test, and retrieved adaptive data with the commonly-used MMD criterion for measuring the feature difference (higher scores indicate a larger gap) (Gretton et al., 2012). In Figure 4, the MMD score of \u201cstance: retrieve-test\u201d is higher than that of \u201cstance-hate: train\u201d. It means the distribution gap between the retrieved adaptive data and test data is even larger than the cross-task, cross-dataset distribution gap between stance and hate dataset.\nBaselines. We first employ the ERM (Vapnik and Vapnik, 1998) baseline trained with past data only. Then, the Domain Adaptation (DA) baselines \u2014 adversarial-based DANN (Ganin et al., 2016), feature-consistency-regularization based Mixup (Yan et al., 2020), distribution-aligning based MMD (Li et al., 2018), and self-training based P+CFd (Ye et al., 2020) are compared. Besides, we consider state-of-the-art (SOTA) Continued Pretraining methods: UDALM (Karouzos et al., 2021) and DPT (Luu et al., 2022). To acquire large-scale adaptive data, we adopt Retrieval methods via RANDOM, DPR (Karpukhin et al., 2020), BM25 (Robertson et al., 2009), Spider (Ram et al., 2022), and Condense (Gao and Callan, 2021). All models\u2019 implementation details are shown in A.4."
        },
        {
            "heading": "6 Experimental Results",
            "text": "In this section, we first discuss the main results (\u00a76.1). Then, an ablation study (\u00a76.2) discusses the contributions of various VIBE modules. Next, we quantify VIBE\u2019s sensitivity to adaptive data scales in \u00a76.3. Lastly, a case study (\u00a76.4) interprets the features learned by VIBE in temporal adaptation."
        },
        {
            "heading": "6.1 Main Results",
            "text": "We first compare classification accuracy with golden adaptive data and choose the best baselines to experiment with retrieved adaptive data.\nResults on Golden Adaptive Data. As shown in Table 2, we draw the observations as follows:\nPerformances of all DA baselines are not comparative to the ERM, which minimizes the prediction losses on past training data only. It echos findings of previous studies (Rosenfeld et al., 2022; Ye et al., 2021) with two-fold reasons. First, DA methods tend to minimize the distribution shift or extract the domain-invariant features instead of adapting to the temporal shift. Second, adapting to temporal distribution shifts among noisy data presents a challenge unable to be addressed by DA baselines.\nContinued pretraining methods are effective while VIBE performs the best. The better results from UDALM and DPT than other DA baselines indicate that continued pretraining on high-quality data may partially help. They are outperformed by VIBE, implying that VIBE can better utilize the adaptive data for temporal adaptation by modeling topic evolution. Nevertheless, it is concerned the small data scales may limit continue-pretraining\u2019s performance, so we will then discuss how they work with large-scale retrieved adaptive data.\nResults on Retrieved Adaptive Data. We have demonstrated VIBE\u2019s effectiveness with golden adaptive data split from the original dataset. However, in the real world, well-collected, high-quality data may be unavailable. Therefore, we explore the potential of utilizing retrieved adaptive data from online Twitter streams. We compare VIBE (using DPR retrieved data) to UDALM and DPT, the SOTA continued pretraining methods, with data collected by varying retrievers. The results are shown in Table 3, and we observe the following:\nData quantity matters more to continued pretraining than quality. UDALM and DPT perform comparably. For them both, retrieval slightly helps, yet the selection of retrievers barely affects their results. Meanwhile, scaling up the data quantity\nsubstantially boost DPT\u2019s results. This observation is consistent to Luu et al. (2022) that continued pre-training relies on large-scale data to work well.\nVIBE exhibits the largest performance gain compared to baseline (ERM). VIBE uses 3% of the retrieved data while outperforming DPT with millionscale data. It implies that VIBE can utilize the data efficiently and its learned topic evolution is useful."
        },
        {
            "heading": "6.2 Ablation Study",
            "text": "We have shown the overall superiority of VIBE. To provide more insight, we further discuss it with Stance (other datasets exhibit similar trends). Here, we investigate the efficacy of its modules through the experimental ablation results in Table 4.\nFirst, we compare module ablation results and find that either IB or topic-space projection contributes positively to VIBE, with their joint effects leading to the best results. Then we quantify the effects of the NTM\u2019s topic number (K) and observe first increase-then decrease trends. This is because small K may cause underfitting, while large K may lead to sparse topics, both hindering VIBE\u2019s capability of capturing high-quality topics. Finally, we examine the VIBE with varying retrievers and observe that DPR works the best in finding suitable adaptive data thanks to its large-scale pretraining.\nIn the second training stage of VIBE, future tweets are pseudo-labeled by the classifier trained in the first training stage. Thus we further explore the effects of label distribution of pseudo labels on model performance. We compare prediction accuracy on Stance with either ground-truth pseudo labels or false pseudo labels and observe the accuracy of 84.72% and 83.06% respectively. While pseudolabeling in self-training can introduce errors, which can potentially undermine its effectiveness, our findings suggest that when combined with posi-\ntive samples, pseudo-labeling still yields promising results. In fact, the performance of pseudo-labeling is only marginally worse than the upper-bound results achieved with correctly-labeled data alone."
        },
        {
            "heading": "6.3 Effects of Adaptive Data Scale",
            "text": "As shown in \u00a76.1, adaptive data scale (N ) is crucial for DPT. We are then interested in how it affects VIBE and show the comparison of VIBE and DPT in Figure 5. As shown, VIBE consistently performs better and exhibits much less sensitivity to adaptive data quantity than DPT, where the former peaks with very small N and the latter grows slowly."
        },
        {
            "heading": "6.4 Case Study on Evolving Topic Patterns",
            "text": "To further examine how VIBE works, we qualitatively analyze the learned topics on Stance, where the changing stance towards Dr. Anthony Fauci is analyzed. We examine the stance labels of all tweets containing the word \u201cFauci\u201d and find a rising number of tweets in favor of Fauci over time. By taking a closer look at these tweets, we find that word \u201cFauci\u201d often co-occurs with \u201cTrump\u201d, where specifically, in the tweets against Trump, people tend to support Fauci. In other words, the \u201cusuallyagainst\u201d stance gained from \u201cTrump\u201d-tweets might, in return signal the \u201cfavor\u201d stance for \u201cFauci\u201d in\ntweets where they are both discussed. Interestingly, we do observe a rising \u201cfor-Fauci\u201d stance rate over time in tweets mentioning both Trump and Fauci.\nWe then visualize VIBE\u2019s sphere topic space of Fauci- and Trump-related samples in Figure 6. Recall that VIBE projects the reconstructed BoW vectors of tweets into a sphere space via multi-task training on predicting time and class labels to obtain time-aware coordinates of tweets (as shown in 4.5). In the left sphere, the distribution of againstand support-Fauci tweets from the past and future are different, which presents challenges for the model to infer the changing stance. Nevertheless, in the right sphere, the against-Trump tweets from adaptive data exhibit a similar distribution with support-Fauci tweets from future test data, allowing VIBE to adapt to the change. These observations show VIBE can effectively capture topic evolution, largely benefiting temporal adaptation."
        },
        {
            "heading": "7 Discussion on Application Scenarios",
            "text": "In this section, we envision more application scenarios where our model VIBE can potentially be applied for dynamic language understanding in a temporally evolving environment.\nIn the real world, the rapid changes in language environments challenge language models. Previous approaches often resort to continuously updating models to cope with shifted distributions and newly emerging knowledge, resulting in extensive resource consumption. Then how about taking advantage of large language models that are highly knowledgeable? We have explored applying a zeroshot large language model (LLM) LLaMa (Touvron et al., 2023), while exhibiting compromised performance compared with the transformer-based VIBE, showing that purely scaled-up language models can not tackle temporal misalignment. Moreover, it may be prohibitively expensive to keep updating LLM\u2019s knowledge via continual training, and the challenge will be exacerbated as models continue to scale up (Bender et al., 2021; Ziems et al., 2023).\nInspired by that topic evolution can indicate language evolution in a temporally changing environment (Zhang and Li, 2022), VIBE unveils a lowcost temporal adaptation approach by reflecting on past and future topic statistics simultaneously to infer evolution, which is efficient and practical in the real world. VIBE is built upon the general text classification task, and it encodes sentence-level temporal semantics, hence having the potential to\ngeneralize over more complex tasks with varying data resources (Zeng et al., 2018; Sun et al., 2023).\nMany social media applications suffer from temporal misalignment caused by distribution shifts over time. For instance, in a dynamic environment, how to recommend users items that align with their evolving interests (Zhang et al., 2021)? Furthermore, harmful texts may contain new semantics over time, making non-adapted models fail to detect them. In the context of rumor detection, as rumors are tested by facts over time, learning new facts to reinforce rumor detection is necessary. More language understanding tasks also face the challenge of comprehending changing texts in dynamic language environments, resulting in compromised end-task performance, which is explored by Zhang and Li (2022)\u2019s empirical study on the effects of temporal misalignment on social media downstream tasks when there is a time gap between training and test data. We will explore our model VIBE in more application scenarios like the aforementioned tasks in the future to enhance performance of language models in the real world."
        },
        {
            "heading": "8 Conclusion",
            "text": "We have presented the first study on the effects of latent topic evolution for temporal adaptation in noisy contexts. A novel model VIBE has been proposed leveraging the information bottleneck to capture topic evolution. In experiments, VIBE significantly outperforms the continued pretraining methods with much smaller scales of adaptive data.\nLimitations\nIn the following, we summarize limitations of our study, which should be considered for future work.\nFor the data we employ for adaptation, the retrieved adaptive data was first crawled online and then retrieved by DPR. In the ablation study, we analyzed the difference of employing different retrievers to retrieve future data for adaptation. The results demonstrate that although adaptive data collected by different retrievers all benefit temporal adaptation, the four retrievers exhibit varying influences on the quality of adaptive data, which indicates the requirement for retrievers to select suitable adaptive data. Thus the DPR should be continued-trained in the future to keep pace with the evolving online environments.\nFor the research scope, our paper focuses on temporal adaptation for text classification on so-\ncial media data. In the future, we will broaden our research scope to more applications like recommendation or generalization on dynamic contexts.\nEthics Statement\nIn our empirical study, datasets Stance and Hate are publicly available with well-reputed previous work. These two datasets in general does not create direct societal consequence. The stance detection dataset helps us understand how the public opinions changed during the COVID-19, shedding lights for future work on public opinions study. The hate speech dataset is for defending against hate speech by training models to detect and prevent them. Our model aims to enhance model performance to better detect hate speech and eliminate them in evolving environments, which is for social benefit. The Stance and Hate datasets are for research purpose only. For experimental results, all the qualitative results we discuss are output by machine learning models, which do not represent the authors\u2019 personal views. Meanwhile, user information was excluded from these two datasets.\nFor the data collection of Hash dataset, Twitter\u2019s official API was employed strictly following the Twitter terms of use. The newly gathered data was thoroughly examined to exclude any possible ethical risks, e.g., toxic language and user privacy. We also conducted data anonymization in pre-processing by removing user identities and replacing @mention with a generic tag. We ran a similar process for adaptive data\u2019s auto-collection."
        },
        {
            "heading": "A Appendix",
            "text": "A.1 Vanilla Neural Topic Model In this section, we elaborate on the vanilla neural topic model, which is the base of our model VIBE.\nThe potential of NTM to deal with dynamics comes from its capability of clustering posts exhibiting similar word statistics and forming latent topics to reflect their shared discussion point. Therefore, the intra-cluster content, though varying\nin the generation time, reflects the implicit semantic consistency throughout time and enables the learning of underlying past-to-future connections.\nThe NTM includes an encoder and a decoder. For the input, a tweet t is firstly mapped into the bag-of-word (BoW) vector form, then fed into the auto-encoder. Given the BoW input t, the clustering is conducted through auto-encoding, which contains an encoding process to map t into a latent topic vetcor zt, followed by decoding to rebuild t conditioned on the topic (zt). During this process, zt is a K dimensional vector; each entry reflects the chance of t clustering into a certain topic and K is a hyper-parameter representing the total topic number in corpus. Below presents concrete steps.\nFor encoding, t is embedded into the latent topic space to generate zt via Gaussian sampling, where the mean \u00b5 and standard deviation \u03c3 are learned with the following formula:\n\u00b5 = f\u00b5(fe(t)), log \u03c3 = f\u03c3(fe(t)) (20)\nf\u2217(\u00b7) is a ReLU-activated neural perceptron. Then zt is drawn from the normal distribution below:\nzt = N (\u00b5, \u03c3) (21)\nIt is later transformed to a distributional vector via a softmax function to yield \u03b8t, representing the topic mixture of t. It initiates decoding step to re-construct t by predicting t\u0302 below:\nt\u0302 = softmax(f\u03d5(\u03b8t)) (22)\nf\u03d5(\u00b7) is another ReLU-activated perceptron mapping information in topic space back to the BoW. The weights of f\u03d5(\u00b7) (after softmax normalization) are employed to represent the topic-word distributions and the latent topic vector zt (with cross-time views gained in clustering) can be engaged in classification (Eq. 15) to capture dynamics over time.\nA.2 Proofs In this section, we explain some derivations from our formulations in detail.\nELBO for p\u03b8(tx, ty). We explain how the ELBO of p\u03b8(tx, ty) is derived, as shown in Eq. 23.\nIB Regularization on Mutual Information. We explain how Eq. 5-Eq. 7 are derived here. Interaction Information between X, Y, and Z is:\nI(X;Y ;Z) = I(X;Y )\u2212 I(X;Y |Z) = I(X;Z)\u2212 I(X;Z|Y ) = I(Y ;Z)\u2212 I(Y ;Z|X)\n(24)\nlog p(tx, ty) = log \u222b p (tx | zx, zs) p (ty | zy, zs) p (zx) p (zs) p (zy) dzxdzsdzy\n= log\n\u222b p (tx | zx, zs) p (ty | zy, zs) p (zx) p (zs) p (zy)\nq (zx | tx) q (zs | tx, ty) q (zy | ty) q (z x | tx) q (zs | tx, ty) q (zy | ty) dzxdzsdzy\n= logEq(zx|tx)q(zs|tx,ty)q(zy|ty) [ p (tx | zx, zs) p (ty | zy, zs) p (zx) p (zs) p (zy)\nq (zx | tx) q (zs | tx, ty) q (zy | ty) ] \u2265Eq(zx|tx)q(zs|tx,ty)q(zy|ty) [ log\np (tx | zx, zs) p (ty | zy, zs) p (zx) p (zs) p (zy) q (zx | tx) q (zs | tx, ty) q (zy | ty) ] =Eq(zx|tx)q(zs|tx,ty) [log p (tx | zx, zs)] + E(zs|tx,ty)q(zy|ty) [log p (ty | zy, zs)]\n\u2212DKL [q (zx | tx) \u2225p (zx)]\u2212DKL [q (zs | tx, ty) \u2225p (zs)]\u2212DKL [q (zy | ty) \u2225p (zy)]\n(23)\nEq. 5 and Eq. 6 are derived similarly. Moreover, given I(X;Z)\u2212 I(X;Z|Y ) = I(Y ;Z)\u2212 I(Y ;Z|X), the mutual information between ZX and ZY is:\nI(ZX ;ZS) = I(ZX ;X)\u2212 I(ZX ;X|ZS)+ I(ZX ;ZS |X) (25) Since q(zx|x) = q(zx|x, zs), the last term in Eq. 25 equals zero, whose calculation goes as:\nI(ZX ;ZS |X) = H(ZX |X)\u2212H(ZX |X,ZS)\n= H(ZX |X)\u2212H(ZX |X) = 0 (26)\nThus we eliminate the zero term and get:\nI(ZX ;ZS) = I(ZX ;X)\u2212 I(ZX ;X|ZS) (27)\nAs Eq. 24, \u2212I(Y ;Z|X) = I(Y ;Z)\u2212 I(X;Y ;Z), similarly \u2212I(ZX ;X|ZS) = I(X;ZS)\u2212 I(X;ZX ;ZS). Thus:\nI(ZX ;ZS) = I(X;ZX) + I(X;ZS \u2212 I(X;ZX , ZS)). (28)\nTractaility for Joint Regularization. Hwang et al. (2020) elaborates how the lower bounds of I(X;ZX , ZS), I(X;ZS |Y ) and I(X;ZX) are derived detailedly via variational approximation.\nA.3 Dataset\nFor the three datasets, we adopted the Twitter API for recovery1 of tweets with missing information (e.g., the timestamp). The Stance detection dataset contains tweets with annotated stances about various COVID-19 topics (Glandt et al., 2021). The Hate Speech data was released by Mathew et al. (2021) with labels indicating whether or not the hate speech exists in a tweet. These two datasets are from publicly available benchmarks with relatively clean annotations. However, many social media applications are built upon noisy user-generated labels (Wang et al., 2019; Zhang et al., 2021).\n1https://developer.twitter.com/en/ docs/twitter-api\nThus we built Hash dataset for studying the usergenerated labels instead of well-annotated clean data. In this dataset, the classification label is a hashtag the author annotates to indicate a tweet\u2019s topic. Its data was gathered following Nguyen et al. (2020) with English tweets posted from September to December 2011, to allow a balanced year coverage across different datasets. Following Zhang et al. (2021), the top 10 hashtags with the highest frequency were selected to be the labels. Then tweets containing these hashtag labels were preserved to group the Hash dataset.\nA.4 Implementation Details\nWe discuss the implementation details of our model VIBE and comparison baselines in this section.\nFor retrievers, we adopted the DPR2 model of the Natural Questions BERT Base checkpoint (Karpukhin et al., 2020). This model has been pretrained on the Natural Questions dataset (Kwiatkowski et al., 2019) for open-domain question answer. For other retrievers, we also adopted their pretrained checkpoint for retrieval.\nFor domain adaptation baselines, we employed the official implementation3 of them. Since these domain adaptation baselines were implemented on image processing tasks, we modified the part of the image processing code to natural language processing code. We trained each model for 40 epochs and fixed parameters via grid search on validation data.\nFor continued pretraining, UDALM fine-tunes itself using a mixed classification and Masked Language Model loss on past training data and future adaptive data together. For DPT, we used unlabeled future adaptive data for continued pretraining of BERT with MLM loss and then fine-tuned the re-\n2https://github.com/facebookresearch/ DPR\n3https://github.com/facebookresearch/ DomainBed\nsulting model on labeled past training data. We implemented the two based on official code4 for BERT (base, uncased). UDALM was trained for 40 epochs. DPT was trained for 30 epochs for pretraining and 10 epochs for fine-tuning.\nFor our model VIBE, we took the BERT (base, uncased) for the BERT encoding, to ensure a fair comparison with the continued pretraining baselines. For all the MLPs, the hidden layer size was set to 2048. For NTM, the topic number for each latent topic including zx, zy, and zs is set to 128. The corpus size is set to 20,000. VIBE is trained in two stages. For the first stage of training, The Lntm in Eq. 13 is first optimized via updating NTM parameters for one epoch for warm-up, then Lpast+LNTM are jointly optimized via updating NTM, BERT, and MLP parameters for 10 epochs. The parameters are fixed via grid search on validation data. Then the pretrained classifier was used to pseudo-label the unlabeled adaptive data. Then VIBE was trained in second stage, where the reconstructed BoW vectors of past and future tweets were projected to the sphere topic space via multi-task training on inferring time and class labels simultaneously. During this, all parameters of VIBE were updated together and fixed via grid search on validation data for 10 epochs.\n4https://github.com/huggingface/ transformers"
        }
    ],
    "title": "VIBE: Topic-Driven Temporal Adaptation for Twitter Classification",
    "year": 2023
}