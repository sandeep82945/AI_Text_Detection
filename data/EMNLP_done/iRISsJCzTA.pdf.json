{
    "abstractText": "Radiology reports are detailed text descriptions of the content of medical scans. Each report describes the presence/absence and location of relevant clinical findings, commonly including comparison with prior exams of the same patient to describe how they evolved. Radiology reporting is a time-consuming process, and scan results are often subject to delays. One strategy to speed up reporting is to integrate automated reporting systems, however clinical deployment requires high accuracy and interpretability. Previous approaches to automated radiology reporting generally do not provide the prior study as input, precluding comparison which is required for clinical accuracy in some types of scans, and offer only unreliable methods of interpretability. Therefore, leveraging an existing visual input format of anatomical tokens, we introduce two novel aspects: (1) longitudinal representation learning \u2013 we input the prior scan as an additional input, proposing a method to align, concatenate and fuse the current and prior visual information into a joint longitudinal representation which can be provided to the multimodal report generation model; (2) sentence-anatomy dropout \u2013 a training strategy for controllability in which the report generator model is trained to predict only sentences from the original report which correspond to the subset of anatomical regions given as input. We show through in-depth experiments on the MIMIC-CXR dataset (Johnson et al., 2019a,b; Goldberger et al., 2000) how the proposed approach achieves state-of-the-art results while enabling anatomy-wise controllable report generation.",
    "authors": [
        {
            "affiliations": [],
            "name": "Francesco Dalla Serra"
        },
        {
            "affiliations": [],
            "name": "Chaoyang Wang"
        },
        {
            "affiliations": [],
            "name": "Fani Deligianni"
        },
        {
            "affiliations": [],
            "name": "Jeffrey Dalton"
        },
        {
            "affiliations": [],
            "name": "Alison Q O\u2019Neil"
        }
    ],
    "id": "SP:edac2a76b4fbfd35b3e7f93ba3172030dfb23c52",
    "references": [
        {
            "authors": [
                "Peter Anderson",
                "Xiaodong He",
                "Chris Buehler",
                "Damien Teney",
                "Mark Johnson",
                "Stephen Gould",
                "Lei Zhang."
            ],
            "title": "Bottom-up and top-down attention for image captioning and visual question answering",
            "venue": "CVPR.",
            "year": 2018
        },
        {
            "authors": [
                "Satanjeev Banerjee",
                "Alon Lavie."
            ],
            "title": "Meteor: An automatic metric for mt evaluation with improved correlation with human judgments",
            "venue": "ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pages",
            "year": 2005
        },
        {
            "authors": [
                "Shruthi Bannur",
                "Stephanie Hyland",
                "Qianchu Liu",
                "Fernando Perez-Garcia",
                "Maximilian Ilse",
                "Daniel C Castro",
                "Benedikt Boecking",
                "Harshita Sharma",
                "Kenza Bouzid",
                "Anja Thieme"
            ],
            "title": "Learning to exploit temporal structure for biomedi",
            "year": 2023
        },
        {
            "authors": [
                "Daniel J Cao",
                "Casey Hurrell",
                "Michael N Patlas."
            ],
            "title": "Current status of burnout in canadian radiology",
            "venue": "Canadian Association of Radiologists Journal, 74(1):37\u201343.",
            "year": 2023
        },
        {
            "authors": [
                "Zhihong Chen",
                "Yaling Shen",
                "Yan Song",
                "Xiang Wan."
            ],
            "title": "Cross-modal memory networks for radiology report generation",
            "venue": "ACL-IJCNLP, pages 5904\u2013 5914.",
            "year": 2021
        },
        {
            "authors": [
                "Zhihong Chen",
                "Yan Song",
                "Tsung-Hui Chang",
                "Xiang Wan."
            ],
            "title": "Generating radiology reports via memory-driven transformer",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1439\u20131449.",
            "year": 2020
        },
        {
            "authors": [
                "Marcella Cornia",
                "Matteo Stefanini",
                "Lorenzo Baraldi",
                "Rita Cucchiara."
            ],
            "title": "Meshed-memory transformer for image captioning",
            "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10578\u201310587.",
            "year": 2020
        },
        {
            "authors": [
                "Francesco Dalla Serra",
                "William Clackett",
                "Hamish MacKinnon",
                "Chaoyang Wang",
                "Fani Deligianni",
                "Jeff Dalton",
                "Alison Q O\u2019Neil"
            ],
            "title": "Multimodal generation of radiology reports using knowledge-grounded extraction of entities and relations",
            "venue": "AACL-IJCNLP,",
            "year": 2022
        },
        {
            "authors": [
                "Francesco Dalla Serra",
                "Chaoyang Wang",
                "Fani Deligianni",
                "Jeffrey Dalton",
                "Alison Q O\u2019Neil"
            ],
            "title": "Finding-aware anatomical tokens for chest X-ray automated reporting",
            "venue": "arXiv preprint arXiv:2308.15961",
            "year": 2023
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "NAACL, pages 4171\u20134186. Association for Computational Linguistics.",
            "year": 2019
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun."
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "CVPR, pages 770\u2013778.",
            "year": 2016
        },
        {
            "authors": [
                "Gao Huang",
                "Zhuang Liu",
                "Laurens Van Der Maaten",
                "Kilian Q Weinberger."
            ],
            "title": "Densely connected convolutional networks",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4700\u20134708.",
            "year": 2017
        },
        {
            "authors": [
                "Alistair EW Johnson",
                "Tom J Pollard",
                "Seth J Berkowitz",
                "Nathaniel R Greenbaum",
                "Matthew P Lungren",
                "Chih ying Deng",
                "Roger G Mark",
                "Steven Horng"
            ],
            "title": "2019a. MIMIC-CXR, a de-identified publicly available database of chest radiographs with free-text",
            "year": 2019
        },
        {
            "authors": [
                "Alistair EW Johnson",
                "Tom J Pollard",
                "Nathaniel R Greenbaum",
                "Matthew P Lungren",
                "Chih-ying Deng",
                "Yifan Peng",
                "Zhiyong Lu",
                "Roger G Mark",
                "Seth J Berkowitz",
                "Steven Horng"
            ],
            "title": "2019b. MIMIC-CXR-JPG, a large publicly available database of labeled chest radio",
            "year": 2019
        },
        {
            "authors": [
                "Gaurang Karwande",
                "Amarachi B Mbakwe",
                "Joy T Wu",
                "Leo A Celi",
                "Mehdi Moradi",
                "Ismini Lourentzou."
            ],
            "title": "CheXRelNet: An anatomy-aware model for tracking longitudinal relationships between chest X-rays",
            "venue": "Medical Image Computing and Com-",
            "year": 2022
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Jimmy Ba."
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "arXiv preprint arXiv:1412.6980.",
            "year": 2014
        },
        {
            "authors": [
                "Xiujun Li",
                "Xi Yin",
                "Chunyuan Li",
                "Pengchuan Zhang",
                "Xiaowei Hu",
                "Lei Zhang",
                "Lijuan Wang",
                "Houdong Hu",
                "Li Dong",
                "Furu Wei"
            ],
            "title": "Oscar: Objectsemantics aligned pre-training for vision-language",
            "year": 2020
        },
        {
            "authors": [
                "Yanghao Li",
                "Saining Xie",
                "Xinlei Chen",
                "Piotr Dollar",
                "Kaiming He",
                "Ross Girshick."
            ],
            "title": "Benchmarking detection transfer learning with vision transformers",
            "venue": "arXiv preprint arXiv:2111.11429.",
            "year": 2021
        },
        {
            "authors": [
                "Chin-Yew Lin."
            ],
            "title": "ROUGE: A package for automatic evaluation of summaries",
            "venue": "Text summarization branches out, pages 74\u201381.",
            "year": 2004
        },
        {
            "authors": [
                "Tsung-Yi Lin",
                "Piotr Doll\u00e1r",
                "Ross Girshick",
                "Kaiming He",
                "Bharath Hariharan",
                "Serge Belongie."
            ],
            "title": "Feature pyramid networks for object detection",
            "venue": "CVPR, pages 2117\u20132125.",
            "year": 2017
        },
        {
            "authors": [
                "Fenglin Liu",
                "Xian Wu",
                "Shen Ge",
                "Wei Fan",
                "Yuexian Zou."
            ],
            "title": "Exploring and distilling posterior and prior knowledge for radiology report generation",
            "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 13753\u2013",
            "year": 2021
        },
        {
            "authors": [
                "Guanxiong Liu",
                "Tzu-Ming Harry Hsu",
                "Matthew McDermott",
                "Willie Boag",
                "Wei-Hung Weng",
                "Peter Szolovits",
                "Marzyeh Ghassemi."
            ],
            "title": "Clinically accurate chest X-ray report generation",
            "venue": "Machine Learning for Healthcare Conference, pages 249\u2013269. PMLR.",
            "year": 2019
        },
        {
            "authors": [
                "Yasuhide Miura",
                "Yuhao Zhang",
                "Emily Tsai",
                "Curtis Langlotz",
                "Dan Jurafsky."
            ],
            "title": "Improving factual completeness and consistency of image-to-text radiology report generation",
            "venue": "NAACL, pages 5288\u20135304.",
            "year": 2021
        },
        {
            "authors": [
                "NHS England",
                "NHS improvement."
            ],
            "title": "Diagnostic imaging dataset statistical release",
            "venue": "pages 1\u201317.",
            "year": 2022
        },
        {
            "authors": [
                "OpenAI."
            ],
            "title": "GPT-4 technical report",
            "venue": "ArXiv, abs/2303.08774.",
            "year": 2023
        },
        {
            "authors": [
                "Kishore Papineni",
                "Salim Roukos",
                "Todd Ward",
                "WeiJing Zhu."
            ],
            "title": "BLEU: a method for automatic evaluation of machine translation",
            "venue": "ACL, pages 311\u2013318.",
            "year": 2002
        },
        {
            "authors": [
                "Han Qin",
                "Yan Song."
            ],
            "title": "Reinforced cross-modal alignment for radiology report generation",
            "venue": "ACL, pages 448\u2013458. Association for Computational Linguistics.",
            "year": 2022
        },
        {
            "authors": [
                "Vignav Ramesh",
                "Nathan A Chi",
                "Pranav Rajpurkar."
            ],
            "title": "Improving radiology report generation systems by removing hallucinated references to non-existent priors",
            "venue": "Machine Learning for Health, pages 456\u2013 473. PMLR.",
            "year": 2022
        },
        {
            "authors": [
                "Shaoqing Ren",
                "Kaiming He",
                "Ross Girshick",
                "Jian Sun."
            ],
            "title": "Faster R-CNN: Towards real-time object detection with region proposal networks",
            "venue": "Advances in neural information processing systems, 28.",
            "year": 2015
        },
        {
            "authors": [
                "Abi Rimmer."
            ],
            "title": "Radiologist shortage leaves patient care at risk, warns royal college",
            "venue": "BMJ: British Medical Journal (Online), 359.",
            "year": 2017
        },
        {
            "authors": [
                "Akshay Smit",
                "Saahil Jain",
                "Pranav Rajpurkar",
                "Anuj Pareek",
                "Andrew Ng",
                "Matthew Lungren."
            ],
            "title": "Combining automatic labelers and expert annotations for accurate radiology report labeling using BERT",
            "venue": "Proceedings of the 2020 Conference on Empirical",
            "year": 2020
        },
        {
            "authors": [
                "Tim Tanida",
                "Philip M\u00fcller",
                "Georgios Kaissis",
                "Daniel Rueckert."
            ],
            "title": "Interactive and explainable regionguided radiology report generation",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7433\u20137442.",
            "year": 2023
        },
        {
            "authors": [
                "Petar Veli\u010dkovi\u0107",
                "Guillem Cucurull",
                "Arantxa Casanova",
                "Adriana Romero",
                "Pietro Li\u00f2",
                "Yoshua Bengio."
            ],
            "title": "Graph attention networks",
            "venue": "Proceedings of the International Conference on Learning Representations.",
            "year": 2018
        },
        {
            "authors": [
                "Lin Wang",
                "Munan Ning",
                "Donghuan Lu",
                "Dong Wei",
                "Yefeng Zheng",
                "Jie Chen."
            ],
            "title": "An inclusive taskaware framework for radiology report generation",
            "venue": "Medical Image Computing and Computer Assisted Intervention\u2013MICCAI 2022: 25th International Con-",
            "year": 2022
        },
        {
            "authors": [
                "Joy T Wu",
                "Nkechinyere Nneka Agu",
                "Ismini Lourentzou",
                "Arjun Sharma",
                "Joseph Alexander Paguio",
                "Jasper Seth Yao",
                "Edward Christopher Dee",
                "William G Mitchell",
                "Satyananda Kashyap",
                "Andrea Giovannini"
            ],
            "title": "Chest ImaGenome dataset for clinical reasoning",
            "year": 2021
        },
        {
            "authors": [
                "Shuxin Yang",
                "Xian Wu",
                "Shen Ge",
                "S Kevin Zhou",
                "Li Xiao."
            ],
            "title": "Knowledge matters: Chest radiology report generation with general and specific knowledge",
            "venue": "Medical Image Analysis, page 102510.",
            "year": 2022
        },
        {
            "authors": [
                "Pengchuan Zhang",
                "Xiujun Li",
                "Xiaowei Hu",
                "Jianwei Yang",
                "Lei Zhang",
                "Lijuan Wang",
                "Yejin Choi",
                "Jianfeng Gao."
            ],
            "title": "VinVL: Revisiting visual representations in vision-language models",
            "venue": "CVPR, pages 5579\u20135588.",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "A chest X-Ray (CXR) is a frequently performed radiology exam (NHS England and NHS improve-\nsentence. Strikethrough text represents the section of the report that we do not want the generated report to include when only the LL and RL are selected as inputs.\nment, 2022), used to visualise and evaluate the lungs, the heart, and the chest wall. Given a CXR, a radiologist or a trained radiographer will diagnose disease (e.g. lung cancer, scoliosis), and assess the position of treatment devices (e.g. tracheostomy tubes, pacemakers). They record their findings in a radiology report, writing a detailed text description of the presence/absence and location of relevant clinical findings. When a prior image is available, the radiologist commonly compares the current clinical findings of a patient with the prior clinical findings to assess their evolution over time (e.g. \u201cthe heart remains enlarged\u201d, \u201cthe catheter has been removed\u201d); this is especially critical in follow-up exams performed for monitoring.\nRadiology reporting is a time-consuming pro-\ncess, and scan results are often subject to delays. In many countries, this reporting backlog is only likely to worsen due to increasing demand for imaging studies as the population ages, and to the shortage of radiologists (Rimmer, 2017; Cao et al., 2023). One strategy to speed up reporting is to integrate automated reporting systems. However, to be employed in real-world clinical scenarios, an automated system must be accurate, controllable and explainable; these criteria are difficult to meet on a task requiring sophisticated clinical reasoning across multiple input image features, and targeting an ill-defined and complex text output.\nPrevious works on CXR automated reporting have mostly focused on solutions to improve clinical accuracy, e.g. Miura et al. (2021). However, they generally use a single radiology study as input to generate the full report, precluding comparison with prior scans. They also do not allow the end user control over what parts of the image are reported on, leading to limited transparency on which image features prompted a specific clinical finding description; interpretability is currently achieved by generating heatmaps that are often dubious (Chen et al., 2020, 2021). In this work, we hence focus on two novel aspects: (1) longitudinal representations \u2013 the most recent previous CXR from the same patient is passed as an additional anatomically aligned input to the model to allow effective comparison of current and prior scans; (2) controllable reporting \u2013 to encourage the language model to describe only the subset of anatomical regions presented as input: this might be single anatomical regions (e.g. {cardiac silhouette}\u2192 \u201cthe cardiac silhouette is enlarged\u201d), multiple anatomical regions (e.g. {left lung, right lung} \u2192 \u201clow lung volumes\u201d) or the full set of anatomical regions (in which case the target regresses to the full report, as in previous methods). A high-level representation is shown in Figure 1.\nTo summarise, our contributions are to:\n1. propose a novel method to create a longitudinal representation by aligning and concatenating representations for equivalent anatomical regions in prior and current CXRs and projecting them into a joint representation;\n2. propose a novel training strategy, sentenceanatomy dropout, in which the model is trained to predict a partial report based on a sampled subset of anatomical regions, thus\ntraining the model to associate input anatomical regions with the corresponding output sentences, giving controllability over which anatomical regions are reported on;\n3. empirically demonstrate state-of-the-art performance of the proposed method on both full and partial report generation via extensive experiments on the MIMIC-CXR dataset (Johnson et al., 2019a,b; Goldberger et al., 2000)."
        },
        {
            "heading": "2 Related Works",
            "text": ""
        },
        {
            "heading": "2.1 Automated Reporting",
            "text": "The task of generating a textual description from an image is generally referred to as image captioning. Advancements in the general domain have often inspired radiological reporting approaches (Anderson et al., 2018; Cornia et al., 2020; Li et al., 2020; Zhang et al., 2021). However, the two tasks differ; the target of image captioning is usually a short description of the main objects appearing in a natural image, whereas the target of radiological reporting is a detailed text description referring to often subtle features in the medical image. Research on CXR automated reporting has focused on improving the clinical accuracy of the generated reports by proposing novel model architectures (Chen et al., 2020, 2021), integrating reinforcement learning to reward factually complete and consistent radiology reports (Miura et al., 2021; Qin and Song, 2022), grounding the report generation process with structured data (Liu et al., 2021; Yang et al., 2022; Dalla Serra et al., 2022), and by replacing global image-level features extracted from convolutional neural networks (He et al., 2016; Huang et al., 2017) as the input visual representations with anatomical tokens (Dalla Serra et al., 2023)."
        },
        {
            "heading": "2.2 Longitudinal CXR Representation",
            "text": "The problem of tracking how a patient\u2019s clinical findings evolve over time in CXRs has received limited attention either generally or for the application of CXR reporting, despite this being a critical component of a CXR report. Ramesh et al. (2022) avoid the problem by proposing a method to remove comparison references to priors from the ground truth radiology reports to alleviate hallucinations about unobserved priors when training a language model for the downstream report generation task. Bannur et al. (2023) introduce a self-supervised multimodal approach that models longitudinal CXRs from image-level features\nas a joint temporal representation to better align text and image. Karwande et al. (2022) have proposed an anatomy-aware approach to classifying if a finding has improved or worsened by modelling longitudinal representations between CXRs with graph attention networks (Velic\u030ckovic\u0301 et al., 2018). Similar to Karwande et al. (2022), we project longitudinal studies into a joint representation based on anatomical representations rather than image-level features. However, we extract the anatomical representations from Faster R-CNN (Ren et al., 2015), as in Dalla Serra et al. (2023)."
        },
        {
            "heading": "2.3 Controllable Automated Reporting",
            "text": "We define a controllable automated reporting system as one which allows the end users to select what regions in the image they want to report on, giving a level of interpretability. This has partially been tackled using hierarchical approaches (Liu et al., 2019), by introducing a multi-head transformer with each head assigned to a specific anatomical region and generating sentences exclusively for that region (Wang et al., 2022), and contemporaneously to our work by Tanida et al. (2023) who (similarly to us) generate sentences based on region-level features extracted through Faster RCNN (Ren et al., 2015).\nTanida et al. (2023) make the assumption that each sentence in the report describes at most one anatomical region. Conversely, we acknowledge that there may be multiple anatomical regions which are relevant to the target text output, e.g.,\na sentence \u201cNo evidence of emphysema.\u201d requires information from both left and right lungs; this requires us to identify valid subsets of anatomical regions in each CXR report for our dropout training strategy (Section 3.4)."
        },
        {
            "heading": "3 Method",
            "text": "We first extract anatomical visual feature representations v\u20d7 \u2208 Rd with d dimensions for N predefined anatomical regions A = {an}Nn=1 appearing in a CXR. To model longitudinal relations, we perform feature extraction for both the scan under consideration and for the most recent prior scan, and then combine region-wise using our proposed longitudinal projection module. We then input the features to the language model (LM) alongside the text indication field, which is trained using our anatomysentence dropout strategy. We show the proposed architecture in Figure 2 and describe these steps in more detail below."
        },
        {
            "heading": "3.1 Visual Anatomical Token Extraction",
            "text": "For the anatomical representations, we extract the bounding box feature vectors from the Region of Interest (RoI) pooling layer of a trained Faster R-CNN model. Faster R-CNN is trained on the tasks of anatomical region localisation in which the bounding box coordinates of the N=36 anatomical regions (e.g. abdomen, aortic arch, cardiac silhouette) are detected in each CXR image, and finding detection in which presence/absence is predicted in each proposed bounding box region\nfor a set of 71 predefined findings (e.g. pleural effusion, lung cancer, scoliosis).1 Specifically, we augment the standard Faster R-CNN architecture head \u2014 comprising an anatomy classification head and a bounding box regression head \u2014 with a multi-label classification head, following Dalla Serra et al. (2023), to extract finding-aware anatomical tokens V = {v\u20d7n}Nn=1 with v\u20d7n \u2208 Rd where d=1024. Then, for each anatomical region we select the bounding box representation proposal with the highest confidence score. When the anatomical region is not detected, we assign a ddimensional vector of zeros. For more details about the model architecture, the loss term and other implementation details, we refer to Dalla Serra et al. (2023)."
        },
        {
            "heading": "3.2 Longitudinal Projection Module",
            "text": "Taking the current scan (the most recent scan at a specific time point) and the CXR from the most recent study (prior scan)2, we extract from Faster RCNN the anatomical tokens of both CXRs. There is one token for each of the N anatomical regions: Vcurrent = {v\u20d7c,n}Nn=1 and Vprior = {v\u20d7p,n}Nn=1. When the current scan is part of an initial exam: v\u20d7p,n := 0\u20d7 \u2208 Rd \u2200n = 1, . . . , N . We select indices for the subset of regions that we want to report on\n1Full lists of anatomical regions and clinical findings are provided in Appendix A.\n2A prior scan is only available if the current scan is not part of an initial exam.\nAtarget \u2286 A, and we obtain the longitudinal representation by concatenating the anatomical tokens for each anatomical region an of the two CXRs, and passing them through the longitudinal projection module f :\nv\u20d7joint,n = { f([v\u20d7c,n, v\u20d7p,n]) if an \u2208 Atarget f([\u20d70, 0\u20d7]), otherwise.\nThe projection layer f is a Multi-Layer Perceptron (MLP) consisting of a stack of a Fully-Connected layer (FC1), a Batch Normalization layer (BN) and another Fully-Connected layer (FC2). We refer to the resulting output as the current-prior joint representation Vjoint = {v\u20d7joint,n}Nn=1."
        },
        {
            "heading": "3.3 Language Model (LM)",
            "text": "This consists of a multimodal Transformer encoderdecoder, which takes the current-prior joint representation Vjoint and the indication field3 I as the visual and textual input respectively, to generate the output report Y :\nY = LM(Vjoint, I)\nwhere Y corresponds to the partial report if Atarget \u2282 A or the full report if Atarget = A.\nSimilarly to Devlin et al. (2019), the input to the LM corresponds to the sum of the textual and visual\n3The indication field contains relevant medical history in the form of free text and it is available at imaging time.\ntoken embeddings, the positional embeddings (for position of tokens) and the segment embeddings (for modality type: vision or text)."
        },
        {
            "heading": "3.4 Training with Sentence-Anatomy Dropout",
            "text": "During training, for each instance in each batch, we randomly drop a subset of anatomical tokens from the input and omit the corresponding sentences from the target radiology report; we term this training strategy sentence-anatomy dropout. In practice, for each training sample not all combinations of anatomical regions will be fit for dropout, since they must satisfy the following conditions:\n1. Given a subset of anatomical tokens as input, the target output must be the full subset of sentences in the report that describe the corresponding anatomical regions;\n2. Given a subset of sentences as the target output, anatomical tokens must be input for the full subset of described anatomical regions.\nThe above conditions are necessary since we reject the assumption that each sentence in the report describes only one anatomical region, as made by Tanida et al. (2023). We illustrate with examples of the different mappings in Table 1.\nLet us consider a radiology report as a set of L sentences S = {sl}Ll=1, each one describing the findings appearing in a different subset of anatomical regions Al \u2286 A; and P = {\u27e8sl, Al\u27e9}Ll=1 the set of sentence-anatomy pairs of a report. To satisfy the two conditions above, we seek to discover the connected components in a graph where sentences are the nodes and an edge between two nodes represents an overlap of described anatomical regions between the two sentences. We describe in Appendix B the algorithm to identify the connected components for each CXR report and how we group the corresponding sentence-anatomy pairs to each connected component into Pk \u2286 P . We then define as F = {Pk}Kk=1 the set of valid sentence-anatomy subsets (see Appendix C for an example). During training, we randomly select one or more elements of F and then use the anatomical tokens as input and concatenate the corresponding sentences to create the target output."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Datasets",
            "text": "We consider two open-source CXR imaging datasets: MIMIC-CXR (Johnson et al., 2019a,b;\nGoldberger et al., 2000) and Chest ImaGenome (Wu et al., 2021; Goldberger et al., 2000). The MIMIC-CXR dataset comprises CXR image-report pairs. The Chest ImaGenome dataset includes additional annotations based on the MIMIC-CXR images and reports. In this paper, we train Faster RCNN with the automatically extracted anatomical bounding box annotations from Chest ImaGenome, provided for 242,072 AnteroPosterior (AP) and PosteroAnterior (PA) CXR images. Chest ImaGenome also contains sentence-anatomy pairs annotations that we use to perform sentence-anatomy dropout. The longitudinal scans of each patient are obtained by ordering different studies based on the annotated timestamp and for each study, taking the most recent previous study as the prior. For this purpose, we only select AP or PA scans as priors (i.e. ignore lateral views). If multiple scans are present in a study, we consider the one with the highest number of non-zero anatomical tokens. In case of a tie, we select it randomly. In all experiments, we follow the train/validation/test split proposed in the Chest ImaGenome dataset."
        },
        {
            "heading": "4.2 Data pre-processing",
            "text": "We extract the Findings section of each report as the target text4. For the text input, we extract the Indication field from each report5.\nWhen training Faster R-CNN, CXRs are resized by matching the shorter dimension to 512 pixels (maintaining the original aspect ratio) and then cropped to a resolution of 512\u00d7 512."
        },
        {
            "heading": "4.3 Metrics",
            "text": "We assess the quality of our model\u2019s predicted reports by computing three Natural Language Generation (NLG) metrics: BLEU (Papineni et al., 2002), ROUGE (Lin, 2004) and METEOR (Banerjee and Lavie, 2005). To better measure the clinical correctness of the generated reports, we also compute Clinical Efficiency (CE) metrics (Smit et al., 2020), derived by applying the CheXbert labeller to the ground truth and generated reports to extract 14 findings \u2014 and hence computing F1, precision and recall scores. In line with previous studies (Miura et al., 2021), this is computed by condensing the four classes extracted from CheXbert (positive, negative, uncertain, or no mention) into binary\n4https://github.com/MIT-LCP/mimic-cxr/blob/ master/txt/create_section_files.py\n5https://github.com/jacenkow/mmbt/blob/main/ tools/mimic_cxr_preprocess.py\nclasses of positive (positive, uncertain) versus negative (negative, no mention)."
        },
        {
            "heading": "4.4 Implementation",
            "text": "We adopt the torchvision Faster R-CNN implementation, as proposed in Li et al. (2021). This consists of a ResNet-50 (He et al., 2016) and a Feature Pyramid Network (Lin et al., 2017) as the image encoder. We modify it and select the hyperparameters following Dalla Serra et al. (2023).\nThe two FC layers in the MLP projection layer have input and output feature dimensions equal to 2048 and include the bias term.\nThe encoder and the decoder of the Report Generator consist of 3 attention layers, each composed of 8 heads and 512 hidden units.\nThe MLP and the Report Generator are trained end-to-end for 100 epochs using a cross-entropy loss with Adam optimiser (Kingma and Ba, 2014) and the sentence-anatomy dropout training strategy. We set the initial learning rate to 5 \u00d7 10\u22124 and reduce it every 10 epochs by a factor of 10. The best model is selected based on the highest F1-CE score.\nWe repeat each experiment 3 times using different random seeds, reporting the average in our results."
        },
        {
            "heading": "4.5 Baselines",
            "text": "We compare our method with previous SOTA works in CXR automated reporting, described in Section 2.1: R2Gen (Chen et al., 2020), R2GenCMN (Chen et al., 2021), M2 Transformer+factENTNLI (Miura et al., 2021), Atok+TE+RG (Dalla Serra et al., 2023) and RGRG (Tanida et al., 2023). For all baselines, we keep the hyperparameters as the originally reported values. For a fair comparison, we use the same text and image pre-processing as proposed in this work and we re-train the baselines based on the Chest ImaGenome dataset splits.6"
        },
        {
            "heading": "5 Results",
            "text": ""
        },
        {
            "heading": "5.1 Automated Reporting",
            "text": "Table 2 shows the effectiveness of the proposed method over the baselines, showing superior performance for most NLG and CE metrics. Compared to Dalla Serra et al. (2023), our method can achieve similar BLEU metrics and superior scores on the\n6We did not re-train Dalla Serra et al. (2023) and Tanida et al. (2023), as they already use that same dataset split.\nremaining metrics, whilst providing also better controllability (and interpretability). Whilst both our method and that proposed by Tanida et al. (2023) tackle the controllability aspect, we show superior results in all metrics."
        },
        {
            "heading": "5.2 Ablation Study",
            "text": "We investigate the effect of incorporating prior CXR scans as input (Priors) and adopting sentenceanatomy dropout during training (SA drop).\nFull Reports We evaluate the different configurations of our method using the full set of anatomical regions as input (Atarget = A) and the full report as the target text. Table 3 shows the results; it can be seen that both adding priors and using sentenceanatomy dropout during training boost most metrics, with best overall performance obtained when combining the two mechanisms. This is illustrated qualitatively in Figure 3.\nInitial vs Follow-up Scans We study the effect of the different components by dividing the test set into initial scans versus follow-up scans, resulting in 11,951 and 20,735 CXR report pairs respectively. The results are shown in Table 4. We note the improvement of our method over the baseline on both subsets, with the best results obtained when adding the priors alone or in combination with the sentence-anatomy dropout. It is worth noting that the benefit of including priors is also present for initial studies with no prior scans. We hypothesise that since the model can infer which are initial scans (using the fact that the prior anatomical tokens are all zero-vectors), it will correctly generate a more comprehensive report rather than focussing on progression or change of known findings.\nPartial Reports To measure the controllability of our method, we evaluate the ability of the different configurations to generate partial reports given a subset of anatomical regions. For this purpose, we divide each report in the test set into its set of valid sentence-anatomy subsets F (Algorithm 1). We take the anatomical regions contained in each subset \u03c7 \u2282 F as input and the corresponding sentences as the target output. We then obtain a total of 71,698 partial reports. The results in Table 5 show how adopting sentence-anatomy dropout enables a controllable method which correctly reports only on the anatomical regions presented as input and does not hallucinate the missing anatomical regions; this is further illustrated in Figure 4.\nTo further assess the quality of our method and each of its components, we measure the length distribution of the predicted reports, similar to Chen et al. (2020), showing that our method more closely matches the ground truth distribution than baseline methods; see results in Appendix D."
        },
        {
            "heading": "6 Limitations",
            "text": "While the proposed method shows state-of-the-art results on CXR automated reporting, end-to-end report generation from CXR images requires further research to reach the clinical accuracy needed to be useful as a diagnostic tool. We note that our eval-\nuation is itself limited since the CheXbert labeller reportedly has an accuracy of only 0.798 F1 (Smit et al., 2020) and thus we do not expect to measure perfect scores on our clinical efficiency metrics.\nOur method focuses only on CXR and adapting it to other types of medical scans might be challenging. First, due to the 2D nature of CXRs compared to other types of 3D scans (e.g., CT, MRI). Second, we strongly rely on the Chest ImaGenome dataset and its annotations. These are automatically extracted and similar sentence-anatomy annotations could be extracted for radiology reports from other types of scans. However, as the same authors pointed out, there are some known limitations of their NLP and the region extraction pipelines; for instance, clinical findings may not be properly extracted from a follow-up report which may be as simple as \u201cNo change is seen\u201d. Hence, some refinement of the pipelines with or without additional manual input might be required."
        },
        {
            "heading": "7 Conclusion",
            "text": "This work focussed on two key aspects of CXR automated reporting: controllability and longitudinal CXR representation. We proposed a simple yet effective solution to align, concatenate and fuse the anatomical representations of two subsequent CXR scans into a joint representation used as the visual input to a language model for automated reporting. We then proposed a novel training strategy termed sentence-anatomy dropout, to supervise the model to link each anatomical region to the corresponding output sentences. This gives the user more control and easier interpretability of the model predictions.\nWe showed the effectiveness of the proposed solution on the MIMIC-CXR dataset where it gives state-of-the-art results. Moreover, we evaluated through extensive ablations how the different components help to generate better reports in different setups: full report generation, partial report generation, and Initial vs. Follow-up report generation.\nIn future, this method could be integrated with more advanced language models such as Touvron\net al. (2023) or OpenAI (2023), or alternative techniques such as Dalla Serra et al. (2023). Further, when considering the patient history, the prior CXR scan might usefully be augmented with other types of imaging and associated radiology reports, clinical notes, clinical letters, and lab results. In future work, we will look to extend our method to a broader multimodal approach considering more data inputs."
        },
        {
            "heading": "A Anatomical Regions & Findings",
            "text": "Below we show the lists of 36 anatomical regions and 71 clinical findings used in this paper."
        },
        {
            "heading": "B Algorithm for discovering valid sentence-anatomy subsets",
            "text": "Algorithm 1 Find set of valid sentence-anatomy subsets. Input: set of \u27e8sentence, regions\u27e9 pairs from a single CXR report, P Output: set of valid sentence-anatomy subsets, F\n1: function FINDVALIDSUBSETS(P ) 2: F \u2190 empty set 3: Pi \u2190 set populated with the first \u27e8sentence, regions\u27e9 pair in P 4: Premaining \u2190 set of \u27e8sentence, regions\u27e9 pairs in P not assigned to Pi 5: R(Pi)\u2190 regions in Pi 6: R(Premaining)\u2190 regions in Premaining 7: while Premaining \u0338= {} do 8: while R(Pi) \u2229R(Premaining) \u0338= {} do 9: for \u27e8sentence, regions\u27e9 \u2208 Premaining do\n10: if regions \u2229 R(Pi) \u0338= {} then 11: Pi \u2190 include \u27e8sentence, regions\u27e9 12: end if 13: end for 14: Premaining \u2190 set of \u27e8sentence, regions\u27e9 pairs in P not assigned to Pi nor any Pk \u2208 F 15: R(Pi)\u2190 regions in Pi 16: R(Premaining)\u2190 regions in Premaining 17: end while 18: F \u2190 include Pi 19: Pi \u2190 set populated with the first \u27e8sentence, regions\u27e9 pair in Premaining 20: end while 21: return F 22: end function"
        },
        {
            "heading": "C Example of Report as Sentence-Anatomy Pairs",
            "text": "GT Report\n'The lungs are hyperinflated with flattening of the diaphragms suggestive of underlying COPD. The heart is mildly enlarged. The aorta is tortuous and diffusely calcified. Mediastinal and hilar contours otherwise are unremarkable. Pulmonary vascularity is not engorged. No focal consolidation, pleural effusion or pneumothorax is identified. There are minimal streaky bibasilar atelectatic changes. No acute osseous abnormalities are present. Mild multilevel degenerative changes are seen in the thoracic spine.'"
        },
        {
            "heading": "D Report Length",
            "text": "The length of a report corresponds to the number of words contained. We compute this for the full reports generated from the full set of anatomical regions and for the partial reports derived from the set of valid sentence-anatomy subsets. In Figure 6 (left) we see that the distribution of the proposed method is closer to the distribution of the GT reports. In Figure 6 (right), we note that adopting the sentence-region dropout strategy allows the method to generate partial reports with a length distribution closer to the GT partial reports. These results provide further evidence of the improvement of the proposed method over the baseline (without adding prior scans and sentence-anatomy training)."
        }
    ],
    "title": "Controllable Chest X-Ray Report Generation from Longitudinal Representations",
    "year": 2023
}