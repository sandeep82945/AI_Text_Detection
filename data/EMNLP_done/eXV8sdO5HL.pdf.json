{
    "abstractText": "In this work, we tackle the scenario of understanding characters in scripts, which aims to learn the characters\u2019 personalities and identities from their utterances. We begin by analyzing several challenges in this scenario, and then propose a multi-level contrastive learning framework to capture characters\u2019 global information in a fine-grained manner. To validate the proposed framework, we conduct extensive experiments on three character understanding sub-tasks by comparing with strong pretrained language models, including SpanBERT, Longformer, BigBird and ChatGPT-3.5. Experimental results demonstrate that our method improves the performances by a considerable margin. Through further in-depth analysis, we show the effectiveness of our method in addressing the challenges and provide more hints on the scenario of character understanding. We will open-source our work in this URL.",
    "authors": [
        {
            "affiliations": [],
            "name": "Dawei Li"
        },
        {
            "affiliations": [],
            "name": "Hengyuan Zhang"
        },
        {
            "affiliations": [],
            "name": "Yanran Li"
        },
        {
            "affiliations": [],
            "name": "Shiping Yang"
        }
    ],
    "id": "SP:29272750ffe6aa2e69a284a9533a306c707e0797",
    "references": [
        {
            "authors": [
                "Mahmoud Azab",
                "Noriyuki Kojima",
                "Jia Deng",
                "Rada Mihalcea."
            ],
            "title": "Representing movie characters in dialogues",
            "venue": "Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL), pages 99\u2013109.",
            "year": 2019
        },
        {
            "authors": [
                "Jiaxin Bai",
                "Hongming Zhang",
                "Yangqiu Song",
                "Kun Xu."
            ],
            "title": "Joint coreference resolution and character linking for multiparty conversation",
            "venue": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main",
            "year": 2021
        },
        {
            "authors": [
                "David Bamman",
                "Brendan O\u2019Connor",
                "Noah A Smith"
            ],
            "title": "Learning latent personas of film characters",
            "venue": "In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
            "year": 2013
        },
        {
            "authors": [
                "Iz Beltagy",
                "Matthew E Peters",
                "Arman Cohan."
            ],
            "title": "Longformer: The long-document transformer",
            "venue": "arXiv preprint arXiv:2004.05150.",
            "year": 2020
        },
        {
            "authors": [
                "Lukas Berglund",
                "Meg Tong",
                "Max Kaufmann",
                "Mikita Balesni",
                "Asa Cooper Stickland",
                "Tomasz Korbak",
                "Owain Evans."
            ],
            "title": "The reversal curse: Llms trained on\" a is b\" fail to learn\" b is a",
            "venue": "arXiv preprint arXiv:2309.12288.",
            "year": 2023
        },
        {
            "authors": [
                "Gordon H Bower."
            ],
            "title": "Experiments on story comprehension and recall",
            "venue": "Discourse Processes, 1(3):211\u2013 231.",
            "year": 1978
        },
        {
            "authors": [
                "Faeze Brahman",
                "Meng Huang",
                "Oyvind Tafjord",
                "Chao Zhao",
                "Mrinmaya Sachan",
                "Snigdha Chaturvedi."
            ],
            "title": "let your characters tell their story\u201d: A dataset for character-centric narrative understanding",
            "venue": "Findings of the Association for Computational Lin-",
            "year": 2021
        },
        {
            "authors": [
                "Mingda Chen",
                "Zewei Chu",
                "Sam Wiseman",
                "Kevin Gimpel."
            ],
            "title": "Summscreen: A dataset for abstractive screenplay summarization",
            "venue": "Proceedings of the",
            "year": 2022
        },
        {
            "authors": [
                "Yu-Hsin Chen",
                "Jinho D Choi."
            ],
            "title": "Character identification on multiparty conversation: Identifying mentions of characters in tv shows",
            "venue": "Proceedings of the 17th annual meeting of the special interest group on discourse and dialogue, pages 90\u2013100.",
            "year": 2016
        },
        {
            "authors": [
                "Gregory Currie."
            ],
            "title": "Narrative and the psychology of character",
            "venue": "The journal of aesthetics and art criticism, 67(1):61\u201371.",
            "year": 2009
        },
        {
            "authors": [
                "Zihang Dai",
                "Zhilin Yang",
                "Yiming Yang",
                "Jaime G Carbonell",
                "Quoc Le",
                "Ruslan Salakhutdinov."
            ],
            "title": "Transformer-xl: Attentive language models beyond a fixed-length context",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational",
            "year": 2019
        },
        {
            "authors": [
                "Susan T Fiske",
                "Shelley E Taylor",
                "Nancy L Etcoff",
                "Jessica K Laufer."
            ],
            "title": "Imaging, empathy, and causal attribution",
            "venue": "Journal of Experimental Social Psychology, 15(4):356\u2013377.",
            "year": 1979
        },
        {
            "authors": [
                "Lucie Flekova",
                "Iryna Gurevych."
            ],
            "title": "Personality profiling of fictional characters using sense-level links between lexical resources",
            "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1805\u20131816.",
            "year": 2015
        },
        {
            "authors": [
                "Tianyu Gao",
                "Xingcheng Yao",
                "Danqi Chen."
            ],
            "title": "Simcse: Simple contrastive learning of sentence embeddings",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6894\u20136910.",
            "year": 2021
        },
        {
            "authors": [
                "Morton Ann Gernsbacher",
                "Brenda M Hallada",
                "Rachel RW Robertson"
            ],
            "title": "How automatically do readers infer fictional characters\u2019 emotional states",
            "venue": "Scientific studies of reading,",
            "year": 1998
        },
        {
            "authors": [
                "Jian Guan",
                "Fei Huang",
                "Zhihao Zhao",
                "Xiaoyan Zhu",
                "Minlie Huang."
            ],
            "title": "A knowledge-enhanced pretraining model for commonsense story generation",
            "venue": "Transactions of the Association for Computational Linguistics, 8:93\u2013108.",
            "year": 2020
        },
        {
            "authors": [
                "Janghoon Han",
                "Taesuk Hong",
                "Byoungjae Kim",
                "Youngjoong Ko",
                "Jungyun Seo."
            ],
            "title": "Finegrained post-training for improving retrieval-based dialogue systems",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Asso-",
            "year": 2021
        },
        {
            "authors": [
                "William Hogan",
                "Jiacheng Li",
                "Jingbo Shang."
            ],
            "title": "Fine-grained contrastive learning for relation extraction",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 1083\u20131095.",
            "year": 2022
        },
        {
            "authors": [
                "Naoya Inoue",
                "Charuta Pethe",
                "Allen Kim",
                "Steven Skiena"
            ],
            "title": "Learning and evaluating character",
            "year": 2022
        },
        {
            "authors": [
                "Mohit Iyyer",
                "Anupam Guha",
                "Snigdha Chaturvedi",
                "Jordan Boyd-Graber",
                "Hal Daum\u00e9 III."
            ],
            "title": "Feuding families and former friends: Unsupervised learning for dynamic fictional relationships",
            "venue": "Proceedings of the 2016 Conference of the North American Chap-",
            "year": 2016
        },
        {
            "authors": [
                "Jyun-Yu Jiang",
                "Mingyang Zhang",
                "Cheng Li",
                "Michael Bendersky",
                "Nadav Golbandi",
                "Marc Najork."
            ],
            "title": "Semantic text matching for long-form documents",
            "venue": "The world wide web conference, pages 795\u2013806.",
            "year": 2019
        },
        {
            "authors": [
                "Mandar Joshi",
                "Danqi Chen",
                "Yinhan Liu",
                "Daniel S Weld",
                "Luke Zettlemoyer",
                "Omer Levy."
            ],
            "title": "Spanbert: Improving pre-training by representing and predicting spans",
            "venue": "Transactions of the Association for Computational Linguistics, 8:64\u201377.",
            "year": 2020
        },
        {
            "authors": [
                "Evgeny Kim",
                "Roman Klinger."
            ],
            "title": "Frowning frodo, wincing leia, and a seriously great friendship: Learning to classify emotional relationships of fictional characters",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Taeuk Kim",
                "Kang Min Yoo",
                "Sang-goo Lee."
            ],
            "title": "Self-guided contrastive learning for bert sentence representations",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference",
            "year": 2021
        },
        {
            "authors": [
                "Vinodh Krishnan",
                "Jacob Eisenstein."
            ],
            "title": "you\u2019re mr",
            "venue": "lebowski, i\u2019m the dude\u201d: Inducing address term formality in signed social networks. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics:",
            "year": 2015
        },
        {
            "authors": [
                "Seanie Lee",
                "Dong Bok Lee",
                "Sung Ju Hwang."
            ],
            "title": "Contrastive learning with adversarial perturbations for conditional text generation",
            "venue": "International Conference on Learning Representations.",
            "year": 2020
        },
        {
            "authors": [
                "Dawei Li",
                "Yanran Li",
                "Jiayi Zhang",
                "Ke Li",
                "Chen Wei",
                "Jianwei Cui",
                "Bin Wang."
            ],
            "title": "C3kg: A chinese commonsense conversation knowledge graph",
            "venue": "Findings of the Association for Computational Linguistics: ACL 2022, pages 1369\u20131383.",
            "year": 2022
        },
        {
            "authors": [
                "Xiujun Li",
                "Xi Yin",
                "Chunyuan Li",
                "Pengchuan Zhang",
                "Xiaowei Hu",
                "Lei Zhang",
                "Lijuan Wang",
                "Houdong Hu",
                "Li Dong",
                "Furu Wei"
            ],
            "title": "Oscar: Objectsemantics aligned pre-training for vision-language tasks",
            "year": 2020
        },
        {
            "authors": [
                "Yiyang Li",
                "Hai Zhao."
            ],
            "title": "Self-and pseudo-selfsupervised prediction of speaker and key-utterance for multi-party dialogue reading comprehension",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2053\u20132063.",
            "year": 2021
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "venue": "arXiv preprint arXiv:1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Xinbei Ma",
                "Zhuosheng Zhang",
                "Hai Zhao."
            ],
            "title": "Structural characterization for dialogue disentanglement",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 285\u2013297.",
            "year": 2022
        },
        {
            "authors": [
                "Philip Massey",
                "Patrick Xia",
                "David Bamman",
                "Noah A Smith."
            ],
            "title": "Annotating character relationships in literary texts",
            "venue": "arXiv preprint arXiv:1512.00728.",
            "year": 2015
        },
        {
            "authors": [
                "Robert McKee."
            ],
            "title": "Story: style, structure, substance, and the principles of screenwriting",
            "venue": "Harper Collins.",
            "year": 1997
        },
        {
            "authors": [
                "Gerald Mead."
            ],
            "title": "The representation of fictional character",
            "venue": "Style, pages 440\u2013452.",
            "year": 1990
        },
        {
            "authors": [
                "Tomas Mikolov",
                "Ilya Sutskever",
                "Kai Chen",
                "Greg S Corrado",
                "Jeff Dean."
            ],
            "title": "Distributed representations of words and phrases and their compositionality",
            "venue": "Advances in neural information processing systems,",
            "year": 2013
        },
        {
            "authors": [
                "Bonan Min",
                "Hayley Ross",
                "Elior Sulem",
                "Amir Pouran Ben Veyseh",
                "Thien Huu Nguyen",
                "Oscar Sainz",
                "Eneko Agirre",
                "Ilana Heintz",
                "Dan Roth"
            ],
            "title": "Recent advances in natural language processing via large pre-trained language models: A survey",
            "year": 2023
        },
        {
            "authors": [
                "Nasrin Mostafazadeh",
                "Nathanael Chambers",
                "Xiaodong He",
                "Devi Parikh",
                "Dhruv Batra",
                "Lucy Vanderwende",
                "Pushmeet Kohli",
                "James Allen."
            ],
            "title": "A corpus and cloze evaluation for deeper understanding of commonsense stories",
            "venue": "Proceedings of the 2016",
            "year": 2016
        },
        {
            "authors": [
                "Charles Talbut Onions",
                "Robert William Burchfield"
            ],
            "title": "The Oxford dictionary of English etymology, volume 178",
            "year": 1966
        },
        {
            "authors": [
                "Xiao Pan",
                "Mingxuan Wang",
                "Liwei Wu",
                "Lei Li."
            ],
            "title": "Contrastive learning for many-to-many multilingual neural machine translation",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint",
            "year": 2021
        },
        {
            "authors": [
                "Alison H Paris",
                "Scott G Paris."
            ],
            "title": "Assessing narrative comprehension in young children",
            "venue": "Reading Research Quarterly, 38(1):36\u201376.",
            "year": 2003
        },
        {
            "authors": [
                "Sameer Pradhan",
                "Alessandro Moschitti",
                "Nianwen Xue",
                "Olga Uryupina",
                "Yuchen Zhang."
            ],
            "title": "Conll2012 shared task: Modeling multilingual unrestricted coreference in ontonotes",
            "venue": "Joint conference on EMNLP and CoNLL-shared task, pages 1\u201340.",
            "year": 2012
        },
        {
            "authors": [
                "Libo Qin",
                "Qiguang Chen",
                "Tianbao Xie",
                "Qixin Li",
                "JianGuang Lou",
                "Wanxiang Che",
                "Min-Yen Kan."
            ],
            "title": "Gl-clef: A global\u2013local contrastive learning framework for cross-lingual spoken language understanding",
            "venue": "Proceedings of the 60th Annual Meeting of the",
            "year": 2022
        },
        {
            "authors": [
                "Yujia Qin",
                "Yankai Lin",
                "Ryuichi Takanobu",
                "Zhiyuan Liu",
                "Peng Li",
                "Heng Ji",
                "Minlie Huang",
                "Maosong Sun",
                "Jie Zhou."
            ],
            "title": "Erica: Improving entity and relation understanding for pre-trained language models via contrastive learning",
            "venue": "Proceedings of the 59th An-",
            "year": 2021
        },
        {
            "authors": [
                "Xipeng Qiu",
                "Tianxiang Sun",
                "Yige Xu",
                "Yunfan Shao",
                "Ning Dai",
                "Xuanjing Huang."
            ],
            "title": "Pre-trained models for natural language processing: A survey",
            "venue": "Science China Technological Sciences, 63(10):1872\u2013 1897.",
            "year": 2020
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "year": 2021
        },
        {
            "authors": [
                "Yisi Sang",
                "Xiangyang Mou",
                "Jing Li",
                "Jeffrey Stanton",
                "Mo Yu."
            ],
            "title": "A survey of machine narrative reading comprehension assessments",
            "venue": "arXiv preprint arXiv:2205.00299.",
            "year": 2022
        },
        {
            "authors": [
                "Yisi Sang",
                "Xiangyang Mou",
                "Mo Yu",
                "Shunyu Yao",
                "Jing Li",
                "Jeffrey Stanton."
            ],
            "title": "Tvshowguess: Character comprehension in stories as speaker guessing",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computa-",
            "year": 2022
        },
        {
            "authors": [
                "Chang Shu",
                "Yusen Zhang",
                "Xiangyu Dong",
                "Peng Shi",
                "Tao Yu",
                "Rui Zhang."
            ],
            "title": "Logic-consistency text generation from semantic parses",
            "venue": "Findings of the Association for Computational Linguistics: ACLIJCNLP 2021, pages 4414\u20134426.",
            "year": 2021
        },
        {
            "authors": [
                "Quan Tu",
                "Yanran Li",
                "Jianwei Cui",
                "Bin Wang",
                "Ji-Rong Wen",
                "Rui Yan."
            ],
            "title": "Misc: A mixed strategyaware model integrating comet for emotional support conversation",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Lin-",
            "year": 2022
        },
        {
            "authors": [
                "Jannis Vamvas",
                "Rico Sennrich."
            ],
            "title": "Contrastive conditioning for assessing disambiguation in mt: A case study of distilled bias",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 10246\u201310265.",
            "year": 2021
        },
        {
            "authors": [
                "Laurens Van der Maaten",
                "Geoffrey Hinton."
            ],
            "title": "Visualizing data using t-sne",
            "venue": "Journal of machine learning research, 9(11).",
            "year": 2008
        },
        {
            "authors": [
                "Jianwei Yang",
                "Chunyuan Li",
                "Pengchuan Zhang",
                "Bin Xiao",
                "Ce Liu",
                "Lu Yuan",
                "Jianfeng Gao."
            ],
            "title": "Unified contrastive learning in image-text-label space",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19163\u2013",
            "year": 2022
        },
        {
            "authors": [
                "Shiping Yang",
                "Renliang Sun",
                "Xiaojun Wan."
            ],
            "title": "A new benchmark and reverse validation method for passage-level hallucination detection",
            "venue": "arXiv preprint arXiv:2310.06498.",
            "year": 2023
        },
        {
            "authors": [
                "Liang Yao",
                "Jiazhen Peng",
                "Chengsheng Mao",
                "Yuan Luo."
            ],
            "title": "Exploring large language models for knowledge graph completion",
            "venue": "arXiv preprint arXiv:2308.13916.",
            "year": 2023
        },
        {
            "authors": [
                "Mo Yu",
                "Jiangnan Li",
                "Shunyu Yao",
                "Wenjie Pang",
                "Xiaochen Zhou",
                "Zhou Xiao",
                "Fandong Meng",
                "Jie Zhou."
            ],
            "title": "Personality understanding of fictional characters during book reading",
            "venue": "arXiv preprint arXiv:2305.10156.",
            "year": 2023
        },
        {
            "authors": [
                "Hengyuan Zhang",
                "Dawei Li",
                "Yanran Li",
                "Chenming Shang",
                "Chufan Shi",
                "Yong Jiang."
            ],
            "title": "Assisting language learners: Automated trans-lingual definition generation via contrastive prompt learning",
            "venue": "Proceedings of the 18th Workshop on Innovative Use",
            "year": 2023
        },
        {
            "authors": [
                "Hengyuan Zhang",
                "Dawei Li",
                "Shiping Yang",
                "Yanran Li."
            ],
            "title": "Fine-grained contrastive learning for definition generation",
            "venue": "Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th Interna-",
            "year": 2022
        },
        {
            "authors": [
                "Rui Zhang",
                "Yangfeng Ji",
                "Yue Zhang",
                "Rebecca J Passonneau."
            ],
            "title": "Contrastive data and learning for natural language processing",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics:",
            "year": 2022
        },
        {
            "authors": [
                "Ethan Zhou",
                "Jinho D Choi."
            ],
            "title": "They exist! introducing plural mentions to coreference resolution and entity linking",
            "venue": "Proceedings of the 27th International Conference on Computational Linguistics, pages 24\u201334.",
            "year": 2018
        },
        {
            "authors": [
                "Sang"
            ],
            "title": "2022b), the annotations divide the evidence types of guessing characters into extremely detailed categories, and several psychologists are asked to assign a category of evidence to each character. In specific, there are 9 types of evidence totally for guessing character identification according to Sang et al. (2022b)",
            "venue": "E Evidence Type Merging",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "As one essential element in stories, character comprehension is a popular research topic in literary, psychological and educational research (McKee, 1997; Currie, 2009; Paris and Paris, 2003; Bower, 1978). To fully understand characters, individuals must empathize with characters based on personal experiences (Gernsbacher et al., 1998), construct profiles according to characters\u2019 identities, and inference about characters\u2019 future actions (Fiske et al., 1979; Mead, 1990).\nAccording to the data modality and format, character comprehension can be categorized into several classes (Sang et al., 2022a). In this work, we focus on character understanding in scripts (Chen and Choi, 2016; Sang et al., 2022b). Scripts are written text for plays, movies, or broadcasts (Onions et al., 1966). Typically, scripts are often structured with several text fields, including scene description, conversation, transition and summary (Saha, 2021).\nAlthough pre-trained language models (PLMs) have demonstrated their effectiveness in language and vision research fields (Qiu et al., 2020; Min et al., 2023), script-based character understanding is yet a hard task, as shown in our experiments. Here we highlight two challenges. The first one is text type. As scripts mainly consist of conversations between different characters, at the core of script-based character understanding is conversation understanding. Especially, scripts often involve multi-party conversations where multiple characters talk and interact with each other in a single scene. Considering other common issues in conversation understanding, it is non-trivial for PLMs to comprehend characters based on finegrained conversation information (Li and Zhao, 2021; Ma et al., 2022; Li et al., 2022; Tu et al., 2022). The other challenge of applying PLMs to script-based character understanding is text length. Table 1 shows a comparison between a script from TVSHOWGUESS (Sang et al., 2022b) and a short story from ROCStories (Mostafazadeh et al., 2016). Typically, scripts are very long with even billion of words (Chen and Choi, 2016), and in turn character information are distributed globally throughout the entire script (Bai et al., 2021; Inoue et al., 2022). However, PLMs are ineffective in capturing such global information due to the sensitiveness of context modeling (Liu et al., 2019; Joshi et al., 2020)\nand the limitation of input length (Dai et al., 2019; Beltagy et al., 2020).\nTo address the aforementioned challenges, we propose a multi-level contrastive learning framework and capture both fine-grained and global information using two devised contrastive losses. For fine-grained character information, we build a summary-conversation contrastive loss by comparing character representations from different sources. Specifically, we leverage two text fields in scripts, i.e., summary and conversation, and then extract character representations from the corresponding field. The representations of the same character are then treated as the postive pairs, while those of different characters are negative pairs. To model the global information, we also propose a novel cross-sample contrastive loss as inspired by (Bai et al., 2021; Inoue et al., 2022). By aligning the same character\u2019s representation in different samples, the model overcomes the input length limitation and learns the global information of each character. To validate the effectiveness of our framework, we benchmark the performances of several PLMs, including SpanBERT, Longformer, BigBird, and ChatGPT-3.5, on three widely-adopted character understanding tasks.\nIn general, our contributions are as follows: \u2022 We identify two critical challenges for charac-\nter understanding in scripts and propose a multilevel contrastive learning framework to address them. \u2022 Through extensive experiments, we demonstrate the effectiveness of our method across multiple datasets and downstream tasks. \u2022 With further analysis, we provide some insights into script-based character understanding. All codes will be open-sourced for future research."
        },
        {
            "heading": "2 Related Work",
            "text": ""
        },
        {
            "heading": "2.1 Character Understanding",
            "text": "Character understanding has long been the subject of considerable interest and scrutiny. Some early works propose to extract keywords as characters\u2019 features from movies (Bamman et al., 2013) and novels (Flekova and Gurevych, 2015). Other works attempt to learn the relationship between characters in both supervised (Massey et al., 2015; Kim and Klinger, 2019) and unsupervised ways (Krishnan and Eisenstein, 2015; Iyyer et al., 2016).\nRecently, more challenging tasks in character understanding have emerged. Chen and Choi (2016)\nbenchmark the character linking and coreference resolution tasks on TV show scripts. Brahman et al. (2021) collect dataset with storybooks and their summaries, and define the character description generation and character identification tasks. Sang et al. (2022b) extend the character guessing task into a multi-character scenario on TV show scripts. Additionally, some works attempt to combine traditional self-supervised learning methods (Mikolov et al., 2013) with language models (Liu et al., 2019) to learn contextual character embeddings and apply them in downstream tasks (Azab et al., 2019; Inoue et al., 2022).\nIn this work, we focus on character understanding tasks in scripts. While some works benchmark summary-based tasks in narratives (Chen et al., 2022; Brahman et al., 2021), we are the first to leverage script summaries as auxiliary data and learn fine-grained and global character representations in a novel way."
        },
        {
            "heading": "2.2 Contrastive Learning",
            "text": "In recent years, contrastive learning is widely used in various NLP tasks (Zhang et al., 2022b), including sentence representation (Gao et al., 2021; Kim et al., 2021), machine translation (Pan et al., 2021; Vamvas and Sennrich, 2021), text generation (Lee et al., 2020; Shu et al., 2021; Zhang et al., 2022a, 2023), and etc. Literatures in multimodal research field adopt contrastive learning for vision-language model training, constructing positive pairs with images and their corresponding captions (Li et al., 2020; Radford et al., 2021; Yang et al., 2022). In our work, we also regard characters in summaries and conversations as two different views of the same target and align them for a better representation.\nMoreover, some works aim to construct positive pairs in global manners. Both Qin et al. (2021) and Hogan et al. (2022) conduct document-level contrastive learning in the relation extraction task to align the representation of the same entity and relation. Pan et al. (2021) propose an aligned augmentation method that generates more positive sentence pairs in different languages to improve translation performances in non-English directions. Similarly, Qin et al. (2022) acquire multilingual views of the same utterance from bi-lingual dictionaries. Following this line of research, we propose the cross-sample contrastive learning in addition to the in-sample contrastive loss to learn character\nrepresentations globally."
        },
        {
            "heading": "3 Preliminaries",
            "text": "Generally, character understanding tasks require the model to predict character\u2019s information given a segment of text. For script-based character understanding, the provided texts often consist of conversations within scripts. In this work, we also leverage script summaries as an additional source. We provide detailed examples in Appendix A.\nIn practice, the model first generates character\u2019s embeddings e in the representation learning step. Subsequently, a feed-forward network FFN is often adopted as the classifier with the cross-entropy loss:\np = Softmax(FFN(e)) (1)\nLSup = \u2212 1\nN N\u2211 i=1 yi log(p) (2)"
        },
        {
            "heading": "4 Method",
            "text": "Our work presents a multi-level contrastive learning framework for character representation learning. Firstly, we follow a general encoding process to obtain character representations from conversations and summaries. Then, we describe two novel contrastive losses to capture fine-grained and global information at both in-sample and cross-sample levels. Finally, we propose a two-stage training\nparadigm that applies different losses in different learning stages. Figure 1 illustrates an overview pipeline of our method."
        },
        {
            "heading": "4.1 Character Representation in Conversation",
            "text": "To obtain character representations from the conversation field in the scripts, we first concatenate each utterance (Joshi et al., 2020; Beltagy et al., 2020) and utilize a pre-trained language model PLM1 to produce the encoding of the whole text H:\nH = PLM(u1;u2; , ...;uT ) (3)\nThen, the character embeddings e1, e2, ...en are extracted from the contextual encoding H. After that, we follow previous works (Bai et al., 2021; Sang et al., 2022b) and use an attention-based layer to share the character-level information among each embedding2:\ne1, ...en = Extract(H) (4)\ne1, ...en = Attention(e1, ...en) (5)\nHowever, the conversations in the scripts are complex and thus the character embeddings solely based on the conversations are often insufficient for fine-grained character understanding.\n1Without loss of generalization, we adopt several PLMs in experiments.\n2We provide further details in Appendix B"
        },
        {
            "heading": "4.2 Character Representation in Summary",
            "text": "To supply more information, we leverage scripts\u2019 summaries as auxiliary data and apply contrastive learning to capture the character intricacies.\nSimilar with conversation encoding, given a summary S contains a group of character mentions {cms1, cms2, ..., cmsn}, we also encode the whole summary and extract the character representations:\nHs = PLM(S) (6)\nesi = tstarti + tendi , 1 <= i <= n (7)\nwhere tstarti and tendi are the first and last tokens of the ith character mention cmsi in the summary.\nAfter that, we follow (Bai et al., 2021) and use a mention-level self-attention (MLSA) layer 3 to gather information for each character embedding:\nes1, ..., e s n = MLSA(e s 1, ..., e s n) (8)\nand the last layer\u2019s output esi is treated as the character\u2019s representation from the summary."
        },
        {
            "heading": "4.3 Multi-level Contrastive Learning",
            "text": "To enhance the character representations learned from the conversation and the summary, we develop a novel multi-level contrastive learning to capture both fine-grained and global information."
        },
        {
            "heading": "4.3.1 Summary-conversation Contrastive Learning",
            "text": "At the local in-sample level, we develop a summaryconversation contrastive loss to align representations of the same character. This gives the model an additional perspective on character representation and encourages it to find a general space where different representations of the same character are closer. Concretely, the loss function for the summary-conversation contrastive learning is:\nLSum = P\u2211 i=1 \u2212log exp sim(eci ,e s ci )/\u03c4\u2211P j=1 exp sim(eci ,e s cj ) /\u03c4 (9)\nwhere ci denotes the ith character, and P here is the number of characters that appear in both scripts and summaries. Also, \u03c4 is a temperature hyperparameter, and sim(, ) stands for the similarity function4. Note that in samples where conversation and summary contain multiple representations of\n3It is a transformer encoder layer with B repeated block. Please refer to Bai et al. (2021) for more details.\n4Here we use Cosine similarity.\ncharacter ci, we randomly select one as eci and e s ci , respectively. By applying the summary-conversation contrastive loss, we are able to learn fine-grained character representations from both summary and conversation texts."
        },
        {
            "heading": "4.3.2 Cross-sample Contrastive Learning",
            "text": "In addition to fine-grained information, global-level information is also crucial for character representation learning (Bai et al., 2021; Inoue et al., 2022). To this end, we also propose a cross-sample contrastive learning to align the same character representation in different samples within a batch:\nLCross = K\u2211 i=1 \u2212log exp sim(e1ci ,e 2 ci )/\u03c4\u2211K j=1 exp sim(e1ci ,e 2 cj ) /\u03c4 (10)\nSI(e1ci) \u0338= SI(e 2 ci) (11)\nwhere SI(e) means the sample index of the character representation e5. When there are multiple representations of one given character in a batch, we randomly select two from them. For cross-sample learning, we impose a constraint that restricts e1ci and e2ci to originate from different samples. K is the number of characters appearing in at least two different samples within a batch. To this end, the cross-sample contrastive loss forces the model to utilize global information in a batch and thus obtain a comprehensive understanding of the characters."
        },
        {
            "heading": "4.4 Two-stage Training",
            "text": "To fully train the model, we further propose a twostage training paradigm to apply different losses in different learning stages.\nConcretely, in the first stage, we combine the two contrastive losses with the supervised loss together, and post-train the pre-trained language model. The supervised loss serves as a guidance to facilitate the contrastive learning, and stabilize the training at the very beginning. The total loss of the first stage is:\nLTotal = \u03bb\u2217LSup+\u03b1\u2217LSum+\u03b2 \u2217LCross (12)\nwhere \u03bb, \u03b1, \u03b2 are hyper-parameters of task ratios, and we will analyze their effects in Section 6.3. After the first stage, only the supervised loss is\n5e generally represents any character embedding.\nkept to train the model in the second stage. This makes the model concentrate on the downstream supervision signals."
        },
        {
            "heading": "5 Experiments Setup",
            "text": ""
        },
        {
            "heading": "5.1 Tasks and Datasets",
            "text": "We evaluate the proposed method on three character understanding tasks, i.e., coreference resolution (Chen and Choi, 2016), character linking (Chen and Choi, 2016), and character guessing (Sang et al., 2022b). Coreference Resolution Given a conversation in scripts that contains multiple utterances and n character mention entity c1, c2, ..., cn within it, the objective of the coreference resolution task is to assemble all mention entities that refer to the same character in a cluster. Character Linking The input of the character linking task is the same as coreference resolution. Unlike coreference resolution, the goal of character linking is to accurately classify each mention entity to the character in a pre-defined character set Z = {z1, z2, ..., zm}. Character Guessing Distinct from previous tasks, the character guessing task focuses on identifying the speaker for each utterance in scripts. In this task, each utterance within a scene is segmented and fed into the model. The speaker\u2019s name preceding each utterance is masked and replaced with a special token. The same speaker within a scene is represented by the same special token. The objective of the character guessing task is to predict the identity of the speaker for each special token. Datasets We choose two TV show datasets to conduct experiments. For coreference resolution and character linking, we use the latest released version of the Character Identification dataset6. For character guessing, we adopt the TVSHOWGUESS dataset7 to conduct experiments. We follow all the training, development, and testing separation provided by the original datasets. The dataset statistics are given in Table 13 in Appendix."
        },
        {
            "heading": "5.2 Baseline Models",
            "text": "Following previous works, we adopt several stateof-the-art (SOTA) models in character understanding as baselines and apply the proposed framework on them. For coreference resolution and\n6https://github.com/emorynlp/ character-identification\n7https://github.com/YisiSang/TVSHOWGUESS\ncharacter linking, we choose SpanBERT (Joshi et al., 2020), a transformer-architecture pre-trained model with the contiguous random span mask strategy in the pre-training stage. We also adopt C2, which combines coreference resolution and character linking together and achieves the SOTA performance in both two tasks. For character guessing, we use BigBird (Zaheer et al., 2020) and Longformer (Beltagy et al., 2020), as they are specialized for long-form document input. We follow Sang et al. (2022b) and add a character-specific attentive pooling layer upon the the model encoders and denote them as BigBird-P and Longformer-P. Notably, we also design a zero-shot and one-shot instruction prompts and evaluate ChatGPT-3.5 (gpt3.5-turbo) via its official API8 as another strong large language model baseline."
        },
        {
            "heading": "5.3 Evaluation Metrics",
            "text": "For coreference resolution, we follow the previous works (Zhou and Choi, 2018; Bai et al., 2021) and use B3, CEAF\u03d54, and BLANC as our evaluation metrics. These three metrics are first proposed by the CoNNL\u201912 shared task (Pradhan et al., 2012) to measure the clustering performance of the coreference resolution task. For character linking and character guessing, we use Macro and Micro F1 to evaluate the models\u2019 classification performances."
        },
        {
            "heading": "5.4 Implementation Details",
            "text": "We employ both the base and large sizes of each model, and implement our proposed method on them. For summary-conversation contrastive loss, we use summary corpus collected by Chen et al. (2022). We follow the hyper-parameter settings in the original papers to reproduce each baseline\u2019s result. We repeat each experiment 3 times and report the average scores. For ChatGPT prompts and other implementation details, please refer to Appendix C and Appendix D. We will open-source all codes in this work."
        },
        {
            "heading": "6 Results and Analysis",
            "text": ""
        },
        {
            "heading": "6.1 Main Results",
            "text": "Table 2 and Table 3 present the automatic evaluation results on the three tasks. Surprisingly, even with specialized instruction and one-shot demonstration, ChatGPT-3.5 performs the worst among all the baselines on each task. This implies that\n8https://platform.openai.com/docs/ api-reference/completions/create\ncharacter understanding is still hard and complex to solve for large language models. Among the three tasks, models perform worse on character guessing than on coreference resolution and character linking tasks. In particular, ChatGPT achieves extremely low scores of 44.05 Macro-F1 in character guessing. Since character guessing requires a deeper understanding of each character and more varied narrative comprehension skills (Sang et al., 2022b), this suggests that the current pre-trained models, especially LLMs, have room for improvement in tasks that require global and indepth learning for a specific individual.\nDespite the discrepancies in model architecture and size, the proposed method brings significant improvements to each baseline model on almost every metric, except for B3 and CEAF\u03d54 in C2-large model. These results indicate the effectiveness and compatibility of our method."
        },
        {
            "heading": "6.2 Ablation Studies",
            "text": "We also conduct an ablation study to examine the contributions of the two novel contrastive losses, i.e., the cross-sample loss and summaryconversation loss. To implement, we select SpanBERT-base and SpanBERT-large as backbone models and implement model variants by removing one of two contrastive losses in the training phases.\nTable 4 presents the results of our ablation study on the coreference resolution and character linking tasks. Compared with the vanilla SpanBERTbase and SpanBERT-large, adding one or two contrastive losses yield better performances. Additionally, we observe that when applied separately, models with the summary-conversation loss work better than models with the cross-sample loss only. More importantly, it is evident that the models trained with both contrastive losses together outperform the models with only one loss, indicating the necessity of our multi-level contrastive framework as well as its effectiveness in addressing the two challenges, i.e., text type and text length.\nWe also conduct an ablation study on the twostage learning strategy. Table 5 shows the experiment results on C2-base using character linking and coreference resolution. While the one-stage multi-task training can also improve the baseline model\u2019s performance, we found it leads to a suboptimal result compared with that using our twostage learning strategy. This observation leads us to the conclusion that supervision-only fine-tuning is also very important in our method, consistently enhancing baseline models\u2019 performance. This aligns with the findings of prior research, which advocate for task-specific fine-tuning following multi-task\npost-training (Guan et al., 2020; Han et al., 2021)."
        },
        {
            "heading": "6.3 Analysis on Hyper-Parameters",
            "text": "The task ratio setting is also an important component of our method. In this section, we investigate their impacts by testing various task ratios in the first training stage. We employ the SpanBERTlarge model and perform experiments on the coreference resolution and character linking tasks.\nThe results of the hyper-parameter analysis are presented in Table 6. As defined in Equation 12, \u03bb, \u03b1, and \u03b2 represent the ratios of task-specific supervised loss, summary-conversation loss, and\ncross-sample loss, respectively. Accordingly, the first block (Row 1) presents the vanilla SpanBERTlarge performance w/o our framework, and the second block (Row 2 and Row 3) shows the model variants with only supervision loss or contrastive losses. Comparing the first and second block we can see, there is no obvious improvement when only keeping the supervised loss, a.k.a \u03bb = 1.0, \u03b1 = 0.0, \u03b2 = 0.0 in the first stage. Moreover, when \u03bb is set to 0.0, the model trained without supervised loss also exhibits inferior performances, e.g., there is a notable decrease in Macro F1 (from 82.8 to 78.6). This finding supports our hypothesis that the task-specific supervision signal plays a crucial role in guiding the two contrastive learning. When examining the last block (Row 4-6), we observe that the models w/ our framework under different task ratios consistently surpasses the others (except only one MARCO metric). This further demonstrates the robustness of our method on the task ratio hyper-parameter."
        },
        {
            "heading": "6.4 Resource Availability Analysis",
            "text": "The proposed summary-conversation contrastive learning relies on well-organized script datasets that include a summary of each scene. This prerequisite could potentially limit the applicability of our approach to datasets in other languages or domains. To address this constraint, we conduct an experiment in which we replaced the manually collected summary dataset with an automatically generated one, produced by ChatGPT. As depicted in Table 7, our results indicate that when using the auto-generated corpus in summary-conversation contrastive learning, a significant improvement is still observed when compared to the vanilla baseline. This discovery further validates the adaptability of our method, irrespective of whether golden or generated summaries are used."
        },
        {
            "heading": "6.5 Breakdown to Evidence Type",
            "text": "To better understand when and how our method works on each sample, we conduct an evidence type analysis on the character guessing task based on the fine-grained annotation provided by Sang et al. (2022b). To remedy the scarcity issue in the original annotations, we merge the fine-grained annotation categories into two broader categories: Global & In-depth Evidence and Local & Textual Evidence. More details on evidence type merging is described in Appendix E.\nThe results of evidence type analysis are presented in Figure 2. Note that our framework works better when Local & Textual evidence is required for character guessing than Global & In-depth evidence. This finding aligns with our intuition that Global & In-depth evidence is more challenging for the model to comprehend. It is also worth noting that our framework yields larger increases for samples requiring Global & In-depth evidences (2.4% and 2.7% for the base and large size models respectively), as compared to those requiring Local\n& Textual evidence (1.1% and 1.6% for the base and large models respectively). Based on these results, we safely conclude that our framework is effective in facilitating character information modeling, especially for global information."
        },
        {
            "heading": "6.6 Visualization",
            "text": "The core of our method is to learn fine-grained and global character representations. To this end, we also visualize the learned character embeddings in the character guessing task. Specifically, we use character embeddings in the test set of the \u201cFRIENDS\u201d (a subset of TVSHOWGUESS dataset) and randomly choose 6 embeddings for each character from different samples.\nFigure 3 shows the visualization results using T-SNE (Van der Maaten and Hinton, 2008). We compare the character embeddings generated by Longformer-P-Large w/ and w/o our framework. One thing to note is that without our framework, some character embeddings of Ross overlap with those of Rachel. This is because that in the TV show \u201cFRIENDS\u201d, Ross and Rachel are partners and together appearing and engaging in many scenes. In contrast, this overlapping phenomenon is greatly mitigated. Overally speaking, our framework encourages the embeddings belonging to the same character exhibit a more compact clustering pattern. This finding provides a new perspective to understand the effectiveness of our proposed method in character comprehension tasks."
        },
        {
            "heading": "6.7 Case Study",
            "text": "We also choose a challenging sample from \u201cThe Big Bang Theory\u201d subset of TVSHOWGUESS in the character guessing task, and analyze the predictions from Longformer-P-Large w/o and w/ our method, as well as that from ChatGPT.\nAs shown in Table 8, all the predictions from ChatGPT are wrong, indicating ChatGPT lacks a fine-grained understanding of each character. Be-\nsides, the only difference between the vanilla model w/ and w/o our framework is whether the speaker P1 is predicted correctly or not. In this case, predicting P1 is particularly challenging, as few utterances are spoken by this character. Hence, it is a must for the models to guess P1\u2019s identity using other details in the scene. By understanding the relationships between P1 and other characters, our method is able to correctly predict that P1 is Sheldon\u2019s partner, Amy. This demonstrates that our method benefits the fine-grained understanding on character relationships in script-based character understanding, e.g., character guessing tasks.\nP0 : Hey, sorry about that P1 : No, we\u2019re sorry. We never should have been comparing relationships in the first place. P2 : Why? We won. You know, I say, next, we take on Koothrappali and his dog. Really give ourselves a challenge. P3 : I just want to say one more thing about this. Just because Penny and I are very different people does not mean that we\u2019re a bad couple. P2 : The answer is one simple test away. Hmm? You know, it\u2019s like when I thought there was a possum in my closet. Did I sit around wondering? No, I sent Leonard in with a pointy stick and a bag. P3 : I killed his Chewbacca slippers. P0 : Let\u2019s just take the test. P3 : No, no, no, I don\u2019t want to. P0 : Oh, well, \u2019cause you know we\u2019re gonna do bad. P3 : Because it doesn\u2019t matter. I don\u2019t care if we\u2019re a ten or a two. P2 : Or a one. A one is possible. P3 : Marriage is scary. You\u2019re scared, I\u2019m scared. But it doesn\u2019t make me not want to do it. It, it just makes me want to hold your hand and do it with you. P0 : Leonard. P1 : It makes me so happy if you said things like that. P2 : We got an eight-point-two. Trust me, you\u2019re happy.\nChatGPT:P0: Leonard, P1: Sheldon, P2: Penny, P3:Howard\nVanilla: P0: Penny, P1: Howard, P2: Sheldon, P3:Leonard\nOurs: P0: Penny, P1: Amy, P2: Sheldon, P3:Leonard\nGolden: P0: Penny, P1: Amy, P2: Sheldon, P3:Leonard\nTable 8: An example chosen from \u201cThe Big Bang Theory\u201d in the character guessing task. We analyze the predictions made by ChatGPT (one-shot), LongformerP-Large (vanilla and with our framework)."
        },
        {
            "heading": "7 Discussion about LLMs on Character Understanding",
            "text": "In this section, we go deeper to discuss the unsatisfied performance when adopting the ICL of LLMs to perform character understanding tasks. One possible reason for this is the script-based character understanding we focus on requires the model to learn\nthe character information globally. For example, in character guessing, anonymous speakers sometimes need to be identified with some global evidence, like linguistic style and the character\u2019s relationship with others. These subtle cues are usually not included in the current sample and thus require the model to learn them globally from other samples (Sang et al., 2022b). However, due to the finetuned unavailability of ICL, LLMs can only utilize local information from the current sample and limited demonstrations to make inferences. We believe this is the reason that LLMs don\u2019t perform well in our script-based character understanding scenario. Additionally, we notice ICL also falls short in some other tasks that involve learning a domain-specific entity or individual across multiple samples, like knowledge graph completion (Yao et al., 2023). This shortcoming in the global learning scenario, which is similar to hallucination (Yang et al., 2023) and the reverse problem (Berglund et al., 2023), can limit LLMs\u2019 application in many downstream tasks.\nIt appears that augmenting the number of demonstrations in the prompt could be a potential strategy for enhancing the capabilities of LLMs in these global learning tasks. Nonetheless, it\u2019s essential to note that incorporating an excessive number of relevant samples as demonstrations faces practical challenges, primarily due to constraints related to input length and efficiency considerations. In the future, more efforts are needed to explore optimal ways of harnessing the ICL method of LLMs in such global learning scenarios."
        },
        {
            "heading": "8 Conclusions",
            "text": "In this work, we focus on addressing two key challenges, text length and text type in scriptbased character understanding. To overcome these challenges, we propose a novel multi-level contrastive framework that exploits in-sample and cross-sample features. The experimental results on three tasks show that our method is effective and compatible with several SOTA models. We also conduct in-depth analysis to examine our method detailedly and provide several hints in the character understanding tasks.\nIn the future, we plan to apply contrastive learning to other long-form document understanding tasks, such as long document matching (Jiang et al., 2019) and fiction understanding (Yu et al., 2023)."
        },
        {
            "heading": "9 Limitations",
            "text": "Our framework depends on pre-trained large languages (PLMs) to encode conversations and summaries, and requires gradient information to tune the PLMs\u2019 parameters. This makes it challenging to apply our approach to language models with gigantic sizes. In this work, we demonstrate the generalization of our method in the experimental section at the base and large size, as well as the incapability of ChatGPT-3.5 on character understanding tasks. Nevertheless, it remains unclear how well our framework will fit to 3B+ encoder-decoder PLMs or decoder-only LLMs. As our experiments suggest, there is still room for improvement in character understanding tasks."
        },
        {
            "heading": "A Example of Script-based Character Understanding Task",
            "text": ""
        },
        {
            "heading": "B Details of Character Embedding Generation",
            "text": "Here we give detailed formulations of our character embedding extraction and character-level attention process. Coreference Resolution & Character Linking Given the context encoding H, we follow (Bai et al., 2021) to initialize the mention-level character embedding:\nei = tstarti + tendi + espeakeri (13)\nwhere tstarti and tendi are the contextualized representation of the beginning and the end tokens of\nmention i, and the espeakeri is the speaker embedding for the current speaker of the utterance where the ith mention belong to. The speaker embeddings are randomly initialized before training. Character Guessing We follow (Sang et al., 2022b) to extract speaker-level character embedding from the context encoding H:\nA = Attention(H) (14)\nai = Softmax(A\u2299Mi) (15)\nei = H Tai (16)\nwhere Attention(\u00b7) is a one-layer feedforward network to compute the token-level attention weight. Mi is a token-level mask such that Mi[j] = 1 if the jth word belongs to an utterance of the ith anonymous speaker and Mx[j] = 0 otherwise. ai is the token weight used to pool the hidden states to summarize a character representation.\nAfter obtaining character embedding, we adopt the MLSA layer we mentioned in Section 4.2 to gather information for each character embedding:\ne1, ..., en = MLSA(e1, ..., en) (17)"
        },
        {
            "heading": "C Prompts for ChatGPT",
            "text": "For character linking, as Table 15 shows, we provide the original scripts\u2019 content for ChatGPT, followed by the position of the mention to be inferenced and all the optional characters. For coreference resolution, we tried several different prompts to ask ChatGPT to do clustering. However, there is always omitting of mentions9 in the models\u2019 output which leads to very poor performance on coreference resolution. So we just use the model\u2019s output on character linking as the clustering results of each character and calculate the corresponding metrics for coreference resolution.\nFor character guessing, we provide the show name of the scripts and optional characters to the model. We concatenate them in front of the script\u2019s content and input the prompt to ChatGPT to do inference as Table 16 shows. For zero-shot, we try asking ChatGPT to guess one character in each request (represent as Prompt-Character) and guess all characters in each request (represent as Prompt-Sample) and find the latter performs better as Table 11 shows. One possible reason for that is the model would attend to more information from other characters\u2019 utterances given Prompt-Sample, which is exactly the key to perform well in character guessing. For one-shot, we only adopt the Prompt-Sample due to its superior performance in the zero-shot setting.\nFor the one-shot setting, we additionally provide the model a sample together with its label as a demonstration. For both tasks, after getting the output of the model, we also use the RE module provided by Python to map the raw text to the most similar label."
        },
        {
            "heading": "D Detailed Training Settings",
            "text": "For every available positive pair 10 of the same character, we randomly choose one to conduct contrastive learning. We set the \u03bb, \u03b1 and \u03b2 to 1.0, 0.5, 0.5 respectively in the first stage of training for all three tasks. We test SpanBERT and C2 in coreference resolution and character linking and Longformer-P and BigBird-P in character guessing. We use ChatGPT in all three tasks. Table 12 gives parameter settings of the learning rate, batch size, and training epochs in the two stages of learning. We use Pytorch-1.8.1\n9For example, there are 20 mentions to be clustered in a sample but the model\u2019s output just contains the clustering result of 15 of them.\n10Excluding the situation that a character show in the dialogue but not in the summary, and vice versa.\ndeep learning framework and Transformers-4.1.2 library for our experiment. We train our models on a single A40 GPU. It takes about 3 hours to train a SpanBert-base/ C2-base model and 20 hours to train a Longformer-P-base/ BigBird-P-base model. Large-version model training takes twice the time. Table 13 shows the detailed statistics of the datasets we use. We also give detailed information about models in the base and large size we use in the experiment in Table 14."
        },
        {
            "heading": "E Evidence Type Merging",
            "text": "In Sang et al. (2022b), the annotations divide the evidence types of guessing characters into extremely detailed categories, and several psychologists are asked to assign a category of evidence to each character. In specific, there are 9 types of evidence totally for guessing character identification according to Sang et al. (2022b). They are attribute, relation, status, background, exclusion, mention, linguistic, memory, and personality.\nHowever, one drawback of the subdivided categories is the scarcity of a certain categories. To address the high variance issue caused by the scarcity, we merge the fine-grained annotations into broader ones. Based on the definition of them in the original paper11, we split them into 2 big categories as Table 17 shows. Here Global & In-depth evidence means cases in which the character can only be predicted according to his/her global information (like one\u2019s relationship with others) or some subtle clues (like one\u2019s Linguistic style). Local & Textual evidence means cases in which a character can be easily predicted only using the content in the local sample (like background information appearing in other characters\u2019 utterances) or something very direct (like calling one character\u2019s name directly). Note that we abandon the evidence exclusion because it is more like a guessing technique rather than evidence.\n11Please refer to the original paper for more details."
        }
    ],
    "title": "Multi-level Contrastive Learning for Script-based Character Understanding",
    "year": 2023
}