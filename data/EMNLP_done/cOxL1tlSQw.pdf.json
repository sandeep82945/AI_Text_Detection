{
    "abstractText": "Stance detection is an increasingly popular task that has been mainly modeled as a static task, by assigning the expressed attitude of a text toward a given topic. Such a framing presents limitations, with trained systems showing poor generalization capabilities and being strongly topic-dependent. In this work, we propose modeling stance as a dynamic task, by focusing on the interactions between a message and their replies. For this purpose, we present a new annotation scheme that enables the categorization of all kinds of textual interactions. As a result, we have created a new corpus, the Dynamic Stance Corpus (DySC), consisting of three datasets in two middle-resourced languages: Catalan and Dutch. Our data analysis further supports our modeling decisions, empirically showing differences between the annotation of stance in static and dynamic contexts. We fine-tuned a series of monolingual and multilingual models on DySC, showing portability across topics and languages.",
    "authors": [],
    "id": "SP:617d9bdcd6910d92edf196e22cf1904bd1716a58",
    "references": [
        {
            "authors": [
                "Ahmet Aker",
                "Leon Derczynski",
                "Kalina Bontcheva."
            ],
            "title": "Simple open stance classification for rumour analysis",
            "venue": "Proceedings of the International Conference Recent Advances in Natural Language Processing, RANLP 2017, pages 31\u201339, Varna, Bulgaria.",
            "year": 2017
        },
        {
            "authors": [
                "Emily Allaway",
                "Kathleen McKeown."
            ],
            "title": "ZeroShot Stance Detection: A Dataset and Model using Generalized Topic Representations",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 8913\u2013",
            "year": 2020
        },
        {
            "authors": [
                "Jordi Armengol-Estap\u00e9",
                "Casimiro Pio Carrino",
                "Carlos Rodriguez-Penagos",
                "Ona de Gibert Bonet",
                "Carme Armentano-Oller",
                "Aitor Gonzalez-Agirre",
                "Maite Melero",
                "Marta Villegas"
            ],
            "title": "Are multilingual models the best choice for moderately",
            "year": 2021
        },
        {
            "authors": [
                "Ramy Baly",
                "Mitra Mohtarami",
                "James Glass",
                "Llu\u00eds M\u00e0rquez",
                "Alessandro Moschitti",
                "Preslav Nakov."
            ],
            "title": "Integrating stance detection and fact checking in a unified corpus",
            "venue": "Proceedings of the 2018 Conference of the North American Chapter of the",
            "year": 2018
        },
        {
            "authors": [
                "Nina Bauwelinck",
                "Els Lefever."
            ],
            "title": "Annotating topics, stance, argumentativeness and claims in Dutch social media comments: A pilot study",
            "venue": "Proceedings of the 7th Workshop on Argument Mining, pages",
            "year": 2020
        },
        {
            "authors": [
                "Filip Boltu\u017ei\u0107",
                "Jan \u0160najder."
            ],
            "title": "Back up your stance: Recognizing arguments in online discussions",
            "venue": "Proceedings of the First Workshop on Argumentation Mining, pages 49\u201358, Baltimore, Maryland. Association for Computational Linguistics.",
            "year": 2014
        },
        {
            "authors": [
                "Rong Cao",
                "Xiangyang Luo",
                "Yaoyi Xi",
                "Yaqiong Qiao."
            ],
            "title": "Stance detection for online public opinion awareness: An overview",
            "venue": "International Journal of Intelligent Systems, 37(12):11944\u201311965. _eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/int.23071.",
            "year": 2022
        },
        {
            "authors": [
                "Thomas Clark",
                "Costanza Conforti",
                "Fangyu Liu",
                "Zaiqiao Meng",
                "Ehsan Shareghi",
                "Nigel Collier."
            ],
            "title": "Integrating transformers and knowledge graphs for Twitter stance detection",
            "venue": "Proceedings of the Seventh Workshop on Noisy User-generated Text (W-NUT",
            "year": 2021
        },
        {
            "authors": [
                "Costanza Conforti",
                "Jakob Berndt",
                "Mohammad Taher Pilehvar",
                "Chryssi Giannitsarou",
                "Flavio Toxvaerd",
                "Nigel Collier."
            ],
            "title": "Will-they-won\u2019t-they: A very large dataset for stance detection on Twitter",
            "venue": "Proceedings of the 58th Annual Meeting of the Asso-",
            "year": 2020
        },
        {
            "authors": [
                "Leon Derczynski",
                "Kalina Bontcheva",
                "Maria Liakata",
                "Rob Procter",
                "Geraldine Wong Sak Hoi",
                "Arkaitz Zubiaga."
            ],
            "title": "SemEval-2017 Task 8: RumourEval: Determining rumour veracity and support for rumours",
            "venue": "Proceedings of the 11th International",
            "year": 2017
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Subhabrata Dutta",
                "Jeevesh Juneja",
                "Dipankar Das",
                "Tanmoy Chakraborty"
            ],
            "title": "Can Unsupervised Knowledge Transfer from Social Discussions Help Argument Mining",
            "venue": "In Proceedings of the 60th Annual Meeting of the Association",
            "year": 2022
        },
        {
            "authors": [
                "Wei Fang",
                "Moin Nadeem",
                "Mitra Mohtarami",
                "James Glass."
            ],
            "title": "Neural multi-task learning for stance prediction",
            "venue": "Proceedings of the Second Workshop on Fact Extraction and VERification (FEVER), pages 13\u201319, Hong Kong, China. Association for Computa-",
            "year": 2019
        },
        {
            "authors": [
                "William Ferreira",
                "Andreas Vlachos."
            ],
            "title": "Emergent: a novel data-set for stance classification",
            "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
            "year": 2016
        },
        {
            "authors": [
                "Blanca Calvo Figueras",
                "Asier Guti\u00e9rrez-Fandi\u00f1o",
                "Marta Villegas."
            ],
            "title": "Anticipating the Debate: Predicting Controversy in News with Transformerbased NLP",
            "venue": "Procesamiento del Lenguaje Natural, 70(0):123\u2013133. Number: 0.",
            "year": 2023
        },
        {
            "authors": [
                "Genevieve Gorrell",
                "Elena Kochkina",
                "Maria Liakata",
                "Ahmet Aker",
                "Arkaitz Zubiaga",
                "Kalina Bontcheva",
                "Leon Derczynski."
            ],
            "title": "SemEval-2019 Task 7: RumourEval, Determining Rumour Veracity and Support for Rumours",
            "venue": "Proceedings of the 13th Inter-",
            "year": 2019
        },
        {
            "authors": [
                "Zhijiang Guo",
                "Michael Schlichtkrull",
                "Andreas Vlachos."
            ],
            "title": "A survey on automated fact-checking",
            "venue": "Transactions of the Association for Computational Linguistics, 10:178\u2013206.",
            "year": 2022
        },
        {
            "authors": [
                "Andreas Hanselowski",
                "Avinesh PVS",
                "Benjamin Schiller",
                "Felix Caspelherr",
                "Debanjan Chaudhuri",
                "Christian M. Meyer",
                "Iryna Gurevych."
            ],
            "title": "A retrospective analysis of the fake news challenge stance-detection task",
            "venue": "Proceedings of the 27th International Con-",
            "year": 2018
        },
        {
            "authors": [
                "Momchil Hardalov",
                "Arnav Arora",
                "Preslav Nakov",
                "Isabelle Augenstein."
            ],
            "title": "A survey on stance detection for mis- and disinformation identification",
            "venue": "Findings of the Association for Computational Linguistics: NAACL 2022, pages 1259\u20131277, Seattle,",
            "year": 2022
        },
        {
            "authors": [
                "Pengcheng He",
                "Jianfeng Gao",
                "Weizhu Chen"
            ],
            "title": "Debertav3: Improving deberta using electra-style pretraining with gradient-disentangled embedding sharing",
            "year": 2021
        },
        {
            "authors": [
                "Jack Hessel",
                "Lillian Lee."
            ],
            "title": "Something\u2019s Brewing! Early Prediction of Controversy-causing Posts from Discussion Features",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Hu-",
            "year": 2019
        },
        {
            "authors": [
                "Dilek K\u00fc\u00e7\u00fck",
                "Fazli Can."
            ],
            "title": "Stance detection: A survey",
            "venue": "ACM Comput. Surv., 53(1).",
            "year": 2020
        },
        {
            "authors": [
                "John Lawrence",
                "Chris Reed."
            ],
            "title": "Argument Mining: A Survey",
            "venue": "Computational Linguistics, 45(4):765\u2013818.",
            "year": 2020
        },
        {
            "authors": [
                "ter Daelemans"
            ],
            "title": "CoNTACT: A Dutch COVID",
            "year": 2022
        },
        {
            "authors": [
                "Da San Martino"
            ],
            "title": "Overview of the CLEF-2018",
            "year": 2018
        },
        {
            "authors": [
                "Antske Fokkens"
            ],
            "title": "Is stance detection",
            "year": 2021
        },
        {
            "authors": [
                "Swapna Somasundaran",
                "Janyce Wiebe."
            ],
            "title": "Recognizing stances in online debates",
            "venue": "Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,",
            "year": 2009
        },
        {
            "authors": [
                "Mariona Taul\u00e9",
                "M Antonia Mart\u00ed",
                "Francisco M Rangel",
                "Paolo Rosso",
                "Cristina Bosco",
                "Viviana Patti"
            ],
            "title": "Overview of the task on stance and gender detection in tweets on catalan independence at ibereval 2017",
            "year": 2017
        },
        {
            "authors": [
                "Mariona Taul\u00e9",
                "Francisco M Rangel Pardo",
                "M Ant\u00f2nia Mart\u00ed",
                "Paolo Rosso."
            ],
            "title": "Overview of the task on multimodal stance detection in tweets on catalan# 1oct referendum",
            "venue": "IberEval@ SEPLN, pages 149\u2013 166.",
            "year": 2018
        },
        {
            "authors": [
                "Shihan Wang",
                "Marijn Schraagen",
                "Erik Tjong Kim Sang",
                "Mehdi Dastani."
            ],
            "title": "Public sentiment on governmental COVID-19 measures in Dutch social media",
            "venue": "Proceedings of the 1st Workshop on NLP for COVID-19 (Part 2) at EMNLP 2020, Online. Associ-",
            "year": 2020
        },
        {
            "authors": [
                "Bowen Zhang",
                "Min Yang",
                "Xutao Li",
                "Yunming Ye",
                "Xiaofei Xu",
                "Kuai Dai."
            ],
            "title": "Enhancing crosstarget stance detection with transferable semanticemotion knowledge",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational",
            "year": 2020
        },
        {
            "authors": [
                "Jonathan Zheng",
                "Ashutosh Baheti",
                "Tarek Naous",
                "Wei Xu",
                "Alan Ritter."
            ],
            "title": "Stanceosaurus: Classifying stance towards multicultural misinformation",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages",
            "year": 2022
        },
        {
            "authors": [
                "Kai Zheng",
                "Qingfeng Sun",
                "Yaming Yang",
                "Fei Xu."
            ],
            "title": "Knowledge stimulated contrastive prompting for low-resource stance detection",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages 1168\u20131178, Abu Dhabi, United Arab",
            "year": 2022
        },
        {
            "authors": [
                "Elena Zotova",
                "Rodrigo Agerri",
                "Manuel Nu\u00f1ez",
                "German Rigau."
            ],
            "title": "Multilingual stance detection in tweets: The Catalonia independence corpus",
            "venue": "Proceedings of the Twelfth Language Resources and Evaluation Conference, pages 1368\u20131375, Marseille,",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Stance detection consists of identifying opinions, perspectives, or attitudes expressed in texts. The task has been found to be relevant as a component for fact verification, argument mining, and media analysis (Boltu\u017eic\u0301 and \u0160najder, 2014; Baly et al., 2018; Bauwelinck and Lefever, 2020; Guo et al., 2022), but also on its own (Hardalov et al., 2022). The standard setting for stance detection is the adoption of a static perspective, where different texts are labeled with a three-class label approach, i.e., being in favour, neutral or against a given topic (Somasundaran and Wiebe, 2009; Mohammad et al., 2016; Conforti et al., 2020; Clark et al., 2021, amog others). One of the limits of this approach is its strong dependence on topics, resulting in poorly portable systems (Reuver et al., 2021).\nOther works have taken a different direction: rather than keeping a fixed topic, stance is in-\nvestigated between pairs of texts. Key contributions in this direction are the RumourEval shared tasks (Derczynski et al., 2017; Gorrell et al., 2019), the Fake News Challenge (Pomerleau and Rao, 2017; Hanselowski et al., 2018) and, more recently, the Stanceosaurus corpus (Zheng et al., 2022a). Although different with respect to the nature of the data, in all cases systems are required to determine the stance of a message with respect to a previous claim with the goal of annotating stance as a component of fact or rumor verification. Consequently, the used labels focus on denying or supporting some facts instead of modeling the actual internal dynamics of an online exchange.\nOur contributions In this work, we present a new annotation scheme to model stance dynamically as a way to account for the evolution of an online exchange between two users. We introduce Dynamic Stance detection as a new independent task, which captures different insights from Static Stance detection. To this end, we have manually annotated a new multilingual corpus with short exchanges between pairs of users from different social media platforms. We have worked with two middle-resourced languages, namely Catalan and Dutch, and conducted an extensive set of experiments investigating the portability of the trained models across topics and languages. We also show the relation between Dynamic Stance and Static Stance labels.\nThe remainder of the paper is structured as follows: in Section 2, we present a critical overview of previous work to highlight the differences and innovations of our proposal. Section 3 introduces the Dynamic Stance task and presents our annotation scheme. The Dynamic Stance Corpus (DySC) is discussed in Section 4, focusing on the data collection, the annotation process, and analyzing the relation between Static and Dynamic Stance. Our experiments and results are presented in Section 5.\nFinally, Section 6 summarises our findings and discusses future work."
        },
        {
            "heading": "2 Related Work",
            "text": "Stance has been modeled extensively as a static task in which a text is tagged with respect to its attitude towards a specific topic or target (Mohammad et al., 2016; K\u00fc\u00e7\u00fck and Can, 2020; Cao et al., 2022). Methods for stance classification have evolved alongside the developments in Natural Language Processing, moving away from feature-based approaches up to fine-tuning and prompting pretrained models (Ferreira and Vlachos, 2016; Aker et al., 2017; Fang et al., 2019; Zhang et al., 2020; Allaway and McKeown, 2020; Zheng et al., 2022b, among others). While performances on benchmark data have seen improvements, issues such as balance among classes and portability of systems across topics and languages remain open research questions.\nStance detection has only been approached as a relation between two texts on a few occasions. As already mentioned, remarkable contributions are the Fake News Challenge (Pomerleau and Rao, 2017), RumourEval (Derczynski et al., 2017; Gorrell et al., 2019), and the Stanceosaurus corpus (Zheng et al., 2022a).\nIn the Fake News Challenge, the authors aim at using disagreements between news pieces to detect false information. They design an annotation scheme based on Ferreira and Vlachos (2016) to estimate the stance of a body text from a news article relative to a headline in English. The labels they define are agrees, disagrees, discusses, and unrelated. While the headline and the content of an article are very often in agreement, in online debates this is not frequent. Therefore, more granular labels are needed to model messages engaging with each other in an exchange of opinions.\nIn RumourEval, the goal is to verify a rumor in English by modeling the discourse around it. From this perspective, the authors designed an annotation scheme that focuses on making explicit whether the replies to a rumor on Twitter support or deny the given claim. Four labels are available: support, deny, comment, and query. The Stanceosaurus corpus (Zheng et al., 2022a) follows a similar approach. In this work, the authors provide specific claims and look for Twitter messages related to those claims. The corpus is multilingual, covers multiple topics, and uses a more fine-grained set\nof labels when compared to RumourEval (supporting, refuting, discussing, querying, and irrelevant). Stanceosaurus is the largest available corpus for stance with respect to claims. Besides the size of the corpus, the authors obtain a macro F1 score of 0.53 when testing on unseen claims,1 with performances dropping to 0.40 for zero-shot cross-lingual transfer in Hindi and Arabic.2\nThe focus of these two studies was to use interactions between texts to verify claims. Therefore, they use labels such as support and deny, which entail that the departing message must be a claim, which is the case in RumourEval and Stancesaurus. However, in online interactions, most of the messages we encounter are not claims but rather opinions and perspectives, which can not always be supported or denied, but rather agreed upon or disagreed.\nWhen it comes to language-specific resources for stance detection in Catalan and Dutch, previous work is quite limited and focused on modeling Static Stance. For Catalan, stance is investigated in Twitter and only with respect to the \u201cCatalan Indepedence\u201d topic (Taul\u00e9 et al., 2017, 2018; Zotova et al., 2020). For Dutch, the only available dataset is Wang et al. (2020), where stance is annotated targeting face masks and social distance measures. The corpus comprises messages from different platforms (Twitter, Reddit, and Nu.nl). Additionally, the CoNTACT corpus (Lemmens et al., 2022) identifies stance towards vaccines using Twitter and Facebook posts, but their data is not publicly available.\nOur work departs from previous studies by focusing on modeling online exchanges intrinsically. The goal of this approach is to be able to capture the relation of a text with respect to a disputed claim as well as any other dialogic interaction in online forums or micro-blogging platforms. This will be relevant for tasks such as fact verification, but also for controversy detection (Hessel and Lee, 2019; Figueras et al., 2023), argument mining (Lawrence and Reed, 2020; Dutta et al., 2022; Ruiz-Dolz et al., 2022), detecting previously verified claims (Nakov et al., 2018; Shaar et al., 2022; Nakov et al., 2021), and discourse analysis.\n1Fine-tuning a BERT-large model (Devlin et al., 2019) 2Fine-tuning a mBERT model (Devlin et al., 2019)"
        },
        {
            "heading": "3 Dynamic Stance Annotation",
            "text": "Inspired by this previous work, we hypothesize that the stance of a message towards a previous message (Dynamic Stance) can show more linguistic patterns than the static approach, as it will be less topic-dependent than modeling agreement over a topic. While Dynamic Stance is an independent task, for the purpose of this work, we also annotated Static Stance. Having access to both annotation levels is essential to empirically investigate the interactions between these two schemes, showing similarities and differences.\nStatic Stance As highlighted in Section 2, previous work has mainly targeted stance annotation in isolation, i.e., with respect to a given topic. Annotation efforts on Static Stance are very similar to each other, mainly differentiating for the topics and text types (Conforti et al., 2020). To maximize compatibility and reusability of data from previous work, we inherit the labels and definitions proposed by Mohammad et al. (2016). In particular, we have a four-label annotation scheme:\n\u2022 Favour: The message expresses a positive stance or opinion with respect to the topic;\n\u2022 Against: The message expresses a negative stance or opinion with respect to the topic;\n\u2022 Neutral: The message is about the topic but does not hold any recognizable stance, e.g., it can report facts about the given topic;\n\u2022 NA (Not Applicable): The message may or may not express a stance, but its content does not address the target topic; or the message is intelligible.\nDynamic Stance To capture the dynamic dimension of stance, the target message must always be put in relation to its direct parent, that means, the message that the target message is replying to. Dynamic Stance annotation is inherently a two-step process: first, the annotators have to read the parent and the reply messages, secondly, they must assign them a label by answering the following question: \u201cWhat is the stance or opinion of the reply with respect to its parent?\u201d. To properly answer this question, the set of labels must be richer but, at the same time, manageable, i.e., easy to understand and to remember. To define this set we have departed from the labels in Pomerleau and Rao (2017), and extended them to the granularity of Zheng et al. (2022a). We have identified seven labels:\n\u2022 Agree: The reply agrees with the parent message.\n\u2022 Disagree: The reply disagrees with the parent message. This can include questioning or making fun of the parent message with a clear dislike towards it. If the reply questions the parent message and it shows a clear disagreement with it (e.g. use of irony, rhetorical questions, etc.), the reply will be labeled as Disagree.\n\u2022 Elaborates: The reply is not against the parent message, but it expresses additional opinions/information with which we do not know if the parent message would agree.\n\u2022 Query: The reply will be annotated as Query if the message questions or requires more arguments/proof/evidence about the parent message. It will not be annotated as Query if the question is rhetorical, ironic, or has a clear stance towards the parent message.\n\u2022 Neutral: The reply is on the same topic as the parent message but does not express any position or opinion about it. If the parent message was a genuine question with no opinion (also not ironic or rhetorical), the reply should be labeled as Neutral.\n\u2022 Unrelated: The reply is not on the same topic and it has no relation to the parent message.\n\u2022 NA: One or both of the messages in the pair are unintelligible, e.g., a URL.\nIn our guidelines, we emphasize that, in the cases in which the parent message does not hold an opinion, the label of the reply must be assigned on the basis of the content. Therefore, if the parent message is a descriptive text with no opinion of its own (e.g. \u201cEMA approves another COVID vaccine\u201d), the stance of the reply toward the content of the text should still be considered, and a reply such as \u201cMore poison for us.\u201d should be labeled as Disagree (i.e. the reply disagrees with the facts described in the parent message but not necessarily with its author, who did not express any opinion).3\nFigure 1 illustrates an example of these schemes. From a Static Stance perspective, the parent message is against vaccines, while the reply is in favor. At the Dynamic layer, the parent and reply stand\n3The full guidelines can be found here: https://github. com/projecte-aina/dynamic-stance-analysis/tree/ main/create_dataset/annotation/guidelines\nin a disagreement relationship. As we will show in Section 4.2, the dynamic label cannot always be inferred from the static ones."
        },
        {
            "heading": "4 The Dynamic Stance Corpus",
            "text": "The Dynamic Stance Corpus (DySC) is a multilingual, and multi-topic corpus in Catalan and Dutch. This section will illustrate the methods that were used for collecting and annotating the data in the two languages. We will also analyze the resulting labels."
        },
        {
            "heading": "4.1 Data Collection",
            "text": "Catalan Data Collection For Catalan, we collected data from two online platforms: Twitter and the Rac\u00f3 Catal\u00e0 forum.4 Given the different nature of the platforms, we have kept the data separated.\nThe Catalan Stance and Emotions Twitter dataset (CaSET)5 has been obtained by extracting messages belonging to five different threads. Each thread identifies a controversial topic in Catalan society and it has been identified by means of keywords and limited to specific time periods (see Appendix A for the full list of keywords and time periods per thread). We targeted the following discussion threads: (i) the COVID-19 vaccination\n4https://www.racocatala.cat/ 5This dataset has also been annotated with emotion labels, but this level of annotation is not reported in this paper. Find the data in https://huggingface.co/datasets/projecte-aina/ CaSET-catalan-stance-emotions-twitter\ncampaign; (ii) the regulation of rent prices; (iii) the expansion of Barcelona\u2019s El Prat airport; (iv) the legalization of surrogate pregnancy; and (v) the rigging in a TV music contest that decided the Spanish candidate for Eurovision.6 We used the Twitter Academic API to collect the messages. To maximize the number of messages that contain a reply, we have developed a pipeline that first finds whether a message contains any of the target keywords for a thread during the relevant time period, and subsequently checks if the target message has a parent message. If so, both the parent and the target messages are retrieved and stored. In this way, we collected 11,078 unique messages organized in 6,673 pairs, which have been manually annotated.\nFor the Catalan Stance and Emotion Rac\u00f3 dataset (CaSERa),7 we followed a slightly different approach. We collected messages belonging to random online exchanges compliant with the following requirements: (a) exchanges must have a minimal length of two messages and a maximum length of four messages; (b) up to two branches of the same conversation can be collected; and (c) all messages in the exchange have to be between 300 and\n6https://elpais.com/television/2022-01-30/el-publicocontra-el-jurado-las-reacciones-ante-la-victoria-de-chanelen-el-benidorm-fest.html\n7This dataset has also been annotated with emotion labels, but this level of annotation is not reported in this paper. Find the data in https://huggingface.co/datasets/projecte-aina/ CaSERa-catalan-stance-emotions-raco\n15 characters. The requirements prevent the collection of very long forum posts, which may have contained many opinions, and very long threads, which may have over-represented some topics in the dataset. We obtained 14,000 pairs, which have been manually annotated.\nDutch Data Collection We have collected a comparable Twitter dataset for Dutch. The Dutch Stance Twitter dataset (DuST) aims at testing the feasibility of cross-lingual transfer of Dynamic Stance. For this reason, we limited the collection of the data to a common online platform, i.e., Twitter, and a common topic, namely the COVID-19 vaccination campaign. Focusing on the same online platform and a common topic is a strategy to minimize language variety factors that may negatively affect the cross-lingual transfer learning approach (Ramponi and Plank, 2020). To maximize the similarities with CaSET, we have used the same pipeline approach. Wherever possible, we have used translations of the Catalan keywords for the COVID-19 vaccination. However, we integrated the keyword sets with others that mirrored culturally specific aspects of the vaccination in the Netherlands and Belgium (e.g., 3G, or coronabewijs). We obtained 14,840 messages of which 2,485 pairs have been manually annotated."
        },
        {
            "heading": "4.2 Data Annotation and Analysis",
            "text": "The data annotation has been conducted by three separate teams. For Catalan, expert annotators have been recruited and trained.8\nFor CaSET, a total of six annotators plus two judges were involved. The judges had the role of resolving disagreements when unanimous annotations could not be reached. The annotation of the Static and Dynamic Stance has been conducted in parallel. A team of two annotators and one judge worked on the Static Stance, while we had a team of four annotators plus one judge for Dynamic Stance. The annotation of Dynamic Stance is more challenging than the static one: for Dynamic Stance, the average Cohen\u2019s kappa between independent annotators was 0.57, while this score jumps to 0.83 for the static annotation. This means that the judges were involved in 1,284 cases for the Dynamic Stance annotation (18.95% of the instances), and 2,087 for the Static Stance annotation (18.84% of the instances).\n8All annotators had a regular contract and received a fair payment in line with Spanish labor laws.\nThe CaSERa dataset has been annotated for Dynamic Stance only, lacking any specific topic for the Static Stance. We have followed the same annotation procedure as in CaSET and employed the same annotators. Similarly to the scores obtained in CaSET, the average Cohen\u2019s kappa is 0.58. The judge was involved in 18.17% of the cases.\nFor DuST, we have followed a slightly different annotation procedure. A team of three students performed the annotations as part of their thesis project. Similarly to the expert annotators, students received training and went through calibration sessions. As in CaSERa, we have annotated the messages only for Dynamic Stance. The final label was assigned using a majority vote (two out of three). In cases in which the majority threshold could not be reached, the annotators held discussion sessions to reach a unanimous agreement. The average Cohen\u2019s kappa is 0.52. Although lower, the scores are comparable to CaSET.\nTables 1 and 2 summarize the annotated data for CaSET, while Table 3 reports the Dynamic Stance annotations for CaSERa and DuST. Concerning the Static Stance labels of the CaSET dataset, we can observe that there is a relatively balanced presence of all the labels, but the NA label takes almost a third of the data. By contrast, the Dynamic Stance labels of the CaSET dataset (Table 2) have a very unbalanced distribution, although we barely find messages labeled as Unrelated (1.52%) or NA (0.38%). Given that the messages are the same in both datasets, this difference comes from modeling the stance toward a static, prefixed topic, or modeling it toward other messages thus capturing a variety of attitudes. As claimed in Section 2, the empiric results show that the Agree label is not common in online debates.\nThe distribution of the Dynamic Stance labels across the three datasets is quite similar. Agree, Query and Unrelated are infrequent, and most interactions are labeled with Disagree, Elaborate or Neutral. CaSET is the dataset with more disagreements, while CaSERa is the one with more neutral interactions. This makes sense since we specifically selected controversial topics for CaSET. The different nature of the sources (i.e. micro-blogging and forums) can also have an impact on the different distribution. As for DuST, the debate around vaccines seems to spark slightly fewer disagreements and more reinforcements of the same opinions than in Catalan, with the most frequent class\nbeing Elaborate.\nRestricting the analysis to the topic distribution in CaSET, the highest rate of disagreements is in the discussion around the legalization of surrogate pregnancy. This topic is also the one with the highest number of messages against it. Rent regulation is the topic with more messages in favor. For the COVID-19 vaccination, the difference between messages in favor and against is smaller than in the other politically controversial topics, with Neutral being the largest class. Finally, half of the messages about the TV show rigging are not on the rigging itself, but mostly on other topics surrounding the involved artists. The observations raised by simply comparing the two annotation levels of this data are not very intuitive. For this reason, we analyzed these relations further.\nStatic vs. Dynamic Stance A closer look at the relation between the two levels of annotation can be seen in Table 4, in which we report the Dynamic Stance values by pairs of Static Stance values for the parent and the reply messages. While the pairs with different Static Stance (either Favour-Against or Against-Favour) are mostly labelled as Disagree, many pairs of messages with the same Static Stance labels (e.g. Favour-Favour) are also labelled as Disagree.\nLooking at these cases we observe that they are messages that in fact disagree with each other, but without being in disagreement with the central topic (in the following examples, vaccination). Examples (1) and (2) illustrate two of these instances.\nThe examples have been extracted from CaSET and translated into English.\n(1) FavourParent-FavourReply-DisagreeDynamic a. Parent: How many people will die and\ninfect new people every day that passes with vaccination stopped? 10,000(vaccine_emoji)=1(heart_emoji).\nb. Reply: Covid is the disease to fight. A vaccine is the shield to eradicating it, and it is given to a healthy person. If any type of incident is recorded and is as serious as death, it must be investigated.\n(2) NeutralParent-NeutralReply-DisagreeDynamic a. Parent: On the one hand, no one can force\nthem to get vaccinated. On the other hand, no one can be forced to allow people who refuse to be vaccinated into their business. Which rights must be respected? Serious question. In the USA, unvaccinated children are not allowed to go to public school.\nb. Reply: That\u2019s a lie. They don\u2019t vaccinate children here nor in the USA.\nThe fact that these disagreements would be missed by the Static Stance annotation scheme, and the fact that these examples are very common (31.37% of the NeutralParent-NeutralReply pairs and 25.88% of the FavourParent-FavourReply ones), shows that the Static Stance is not enough to model the interactions of a debate. For tasks such as controversy detection, this phenomenon might arise from disagreements in specific issues of the topic, which might not be reflected by the Against/Favour scheme. In addition, for monitoring the discourse around a topic, being able to classify the interactions would provide deeper knowledge of the discussion."
        },
        {
            "heading": "5 Automatic Stance Detection",
            "text": "We developed several baseline models for both Static and Dynamic Stance detection. We investigate the abilities of pre-trained language models to model Static and Dynamic Stance with two settings: one where the topic is seen at training time and another where the topic is excluded. Lastly, we investigate the cross-lingual transfer abilities of a multilingual model on DuST.\nExperimental setup For our experiments, we fine-tuned both the Catalan language model RoBERTa-large-ca-v29 (Armengol-Estap\u00e9 et al., 2021) and the multilingual model mDeBERTa-v3-\n9https://huggingface.co/projecte-aina/ roberta-base-ca-v2\nbase10 (He et al., 2021) for sequence classification. We used a learning rate of 1-e5, a batch size of 8, a warmup of 0.06, and trained for 10 epochs with an Adam optimizer and cross-entropy loss. We kept the best checkpoint on the development set. To deal with the issue that some labels in the Dynamic Stance annotation have very few instances, we merged the label Agree with Elaborate, Query with Neutral, and Unrelated with NA. That way, we had 4 labels for both Dynamic and Static Stance.\nAll topics experiments For our monolingual experiments, we generated fixed train, development, and test sets from CaSET. For the Static Stance, we have 9,073 instances in train and maintained 1,000 instances each for development and test. For the Dynamic Stance, we used 4,771 instances for training and 1,000 instances each for development and test. The CaSERa data has been used as additional training materials, increasing the training set to 18,771.\nTable 5 shows the resulting macro F1 score for the models. In both cases, the monolingual model achieves the best scores with RoBERTa-large. For the Dynamic Stance, we observe a positive impact of the additional training materials from CaSERa, even if it comes from a different platform and with no specific topic. Although not directly comparable, the higher scores for Static Stance are in line with the IAA scores, which show that Static Stance is an easier task to model.\nIn Table 6 we report the results of the best system per label for both Static and Dynamic Stance. While labels that clearly express a stance (Favour, Against, and Neutral) obtain comparable results for both phenomena, the largest difference affects the NA label. This is likely due to differences in the training instances, with the Dynamic Stance having very few NA cases.\n10https://huggingface.co/microsoft/ mdeberta-v3-base\nZero-shot topic experiments One of the motivations for creating a Dynamic Stance annotation scheme is the possibility to improve the cross-topic portability of models. To test this, we conducted a series of zero-shot topic experiments which exclude one topic at a time from the training set and use it as the test set. Therefore, the sizes of the test and training sets change in each of the experiments, the details about these sets can be found in Appendix B. We have experimented only with RoBERTa-large, since it is the model that obtained the best results when using all the topics. The results are illustrated in Table 7.\nWe observe that, while the cross-topic performance of Dynamic Stance drops a bit from the model that had seen all topics, it still learns. This is not the case with the Static Stance cross-topic models, which have a below-chance performance in most topics. Although not directly comparable, these results suggest that Dynamic Stance can be modeled across topics in an easier way than Static Stance. It has to be noted that the addition of the CaSERa data to augment the dynamic training set does not have much effect on the results.\nCross-lingual experiments For the cross-lingual experiments, we employed only data from CaSET\ncovering the vaccine topic, thus maximising the compatibility with DuST. In all of our experiments, we used mDeBERTaV3. To investigate the benefit of cross-lingual data for Dynamic Stance classification, we first developed two baselines using the Catalan and the Dutch data only. For Catalan, we used a training set of 1,868 instances and a development and test sets of 500 instances each. For Dutch, we have 1,483 instances in the training, and 500 each for development and test. For the crosslingual experiments, we concatenated the training materials.\nTable 8 reports the results. In both languages, the use of the language-specific training data obtains limited results, especially for the Dutch data. However, when concatenating the two languages, results improve, with Dutch being the one benefiting the most."
        },
        {
            "heading": "6 Conclusion and Future Work",
            "text": "In this work, we have investigated the task of stance detection with a focus on dynamic interactions between parent messages and their replies. Our approach addressed the limitations of traditional Static Stance detection models, which heavily rely on specific topics and exhibit poor generalization capabilities. By introducing a novel annotation scheme and creating the Dynamic Stance Corpus (DySC), we provided a new perspective on the categorization of textual interactions, which captures different insights from the data. Through our data analysis, we demonstrated empirical differences between the annotation of stance in static and dynamic contexts. Furthermore, DySC is fully developed for non-English languages, namely Catalan and Dutch.\nUsing a generic monolingual pre-trained model for Catalan (RoBERTa-ca-l) we achieve a macro F1 score of 0.65 for Dynamic Stance using all topics on training. We also show that, while Dynamic Stance models exhibit some learning on the zero-\nshot topic scenario (with an average score of 0.47), Static Stance models go from a macro F1 score of 0.71 with all topics on train, to an average score of 0.21 when in the zero-shot scenario. We therefore conclude that Dynamic Stance is easier to model in cross-topic scenarios. As for the cross-lingual experiments, the results show that there is some knowledge transfer between languages. While the results leave room for improvement, they are close to the ones obtained by similar previous work such as the Stancesaurus (reported in Section 2).\nThis study represents an important initial step toward effectively modeling the task of stance detection in dynamic textual interactions. Future work will address some of the pending issues by expanding the annotated data to other languages and investigating new methods (including the use of incontext learning) to improve the portability of the Dynamic Stance models.\nLimitations\nWe have used a manually-curated selection of keywords for our data collection. While we have ensured to cover relevant keywords (including synonyms) for all of the topics we have targeted, the list of keywords may not be exhaustive and contain potential bias. We aim to expand the data collection by using few-shot in-context learning approaches to extend the list of keywords. Furthermore, we have to acknowledge some intrinsic limitations of the Twitter API, which prevents the collection of all potentially relevant messages.\nThe data collection and annotation have been conducted with all possible human resources available and applying fair treatment to all parties involved. We leave the collection of additional messages and the expansion to other languages for future work. Our guidelines are made available and so is the code we have used to collect the messages,11 thus offering opportunities to other interested researchers to expand DySC and set up a larger benchmark for Dynamic Stance.\nIn our experiments, we did not use all of the finegrained classes we have identified for the annotation. This does not mean that these labels should be discarded. Our decision was motivated by the relatively few instances we were able to identify.\nOur models leave room for improvement. Our experiments represent an implementation of base-\n11Find both in https://github.com/projecte-aina/ dynamic-stance-analysis\nlines to validate the annotations and identify lowerbound thresholds. The use of general language pretrained models, RoBERTa-ca-l and mDeBERTaV3, may have had an impact on the results, considering the different language varieties of DySC. Future work will need to address this issue."
        },
        {
            "heading": "Acknowledgements",
            "text": "We acknowledge Rac\u00f3 Catal\u00e0 for the release of their data for this project. This work has been partially funded by Generalitat de Catalunya, within the framework of Projecte AINA. Blanca Calvo is funded by Disargue (TED2021-130810B-C21) funded by MCIN/AEI /10.13039/501100011033 and by European Union NextGenerationEU/ PRTR, and by a PIF UPV/EHU 2023 doctoral grant."
        },
        {
            "heading": "A Keywords and time frames for data collection",
            "text": "B Zero-shot topic experiments"
        }
    ],
    "title": "Dynamic Stance: Modeling Discussions by Labeling the Interactions",
    "year": 2023
}