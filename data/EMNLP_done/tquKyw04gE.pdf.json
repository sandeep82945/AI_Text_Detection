{
    "abstractText": "We present MULTICONER V2, a dataset for fine-grained Named Entity Recognition covering 33 entity classes across 12 languages, in both monolingual and multilingual settings. This dataset aims to tackle the following practical challenges in NER: (i) effective handling of fine-grained classes that include complex entities like movie titles, and (ii) performance degradation due to noise generated from typing mistakes or OCR errors. The dataset is compiled from open resources like Wikipedia and Wikidata, and is publicly available.1 Evaluation based on the XLMRoBERTa baseline highlights the unique challenges posed by MULTICONER V2: (i) the fine-grained taxonomy is challenging, where the scores are low with macro-F1=0.63 (across all languages), and (ii) the corruption strategy significantly impairs performance, with entity corruption resulting in 9% lower performance relative to non-entity corruptions across all languages. This highlights the greater impact of entity noise in contrast to context noise.",
    "authors": [
        {
            "affiliations": [],
            "name": "Besnik Fetahu"
        },
        {
            "affiliations": [],
            "name": "Zhiyu Chen"
        },
        {
            "affiliations": [],
            "name": "Sudipta Kar"
        },
        {
            "affiliations": [],
            "name": "Oleg Rokhlenko"
        },
        {
            "affiliations": [],
            "name": "Shervin Malmasi"
        }
    ],
    "id": "SP:617a19827e63a9e255f536b4607b2b0546a62dce",
    "references": [
        {
            "authors": [
                "S\u00f6ren Auer",
                "Christian Bizer",
                "Georgi Kobilarov",
                "Jens Lehmann",
                "Richard Cyganiak",
                "Zachary G. Ives."
            ],
            "title": "Dbpedia: A nucleus for a web of open data",
            "venue": "The Semantic Web, 6th International Semantic Web Conference, 2nd Asian Semantic Web",
            "year": 2007
        },
        {
            "authors": [
                "Alexis Conneau",
                "Kartikay Khandelwal",
                "Naman Goyal",
                "Vishrav Chaudhary",
                "Guillaume Wenzek",
                "Francisco Guzm\u00e1n",
                "Edouard Grave",
                "Myle Ott",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Unsupervised cross-lingual representation learning at scale",
            "venue": "In",
            "year": 2020
        },
        {
            "authors": [
                "Leon Derczynski",
                "Eric Nichols",
                "Marieke van Erp",
                "Nut Limsopatham."
            ],
            "title": "Results of the wnut2017 shared task on novel and emerging entity recognition",
            "venue": "Proceedings of the 3rd Workshop on Noisy User-generated Text, pages 140\u2013147.",
            "year": 2017
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: pre-training of deep bidirectional transformers for language understanding",
            "venue": "NAACL-HLT (1). Association for Computational Linguistics.",
            "year": 2019
        },
        {
            "authors": [
                "Besnik Fetahu",
                "Anjie Fang",
                "Oleg Rokhlenko",
                "Shervin Malmasi."
            ],
            "title": "Gazetteer enhanced named entity recognition for code-mixed web queries",
            "venue": "Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Infor-",
            "year": 2021
        },
        {
            "authors": [
                "Besnik Fetahu",
                "Anjie Fang",
                "Oleg Rokhlenko",
                "Shervin Malmasi."
            ],
            "title": "Dynamic gazetteer integration in multilingual models for cross-lingual and cross-domain named entity recognition",
            "venue": "Proceedings of the 2022 Conference of the North Ameri-",
            "year": 2022
        },
        {
            "authors": [
                "Besnik Fetahu",
                "Sudipta Kar",
                "Zhiyu Chen",
                "Oleg Rokhlenko",
                "Shervin Malmasi."
            ],
            "title": "SemEval2023 Task 2: Fine-grained Multilingual Named Entity Recognition (MultiCoNER 2)",
            "venue": "Proceedings of the 17th International Workshop on Semantic",
            "year": 2023
        },
        {
            "authors": [
                "Andrea Iovine",
                "Anjie Fang",
                "Besnik Fetahu",
                "Oleg Rokhlenko",
                "Shervin Malmasi."
            ],
            "title": "Cyclener: an unsupervised training approach for named entity recognition",
            "venue": "Proceedings of the ACM Web Conference 2022, pages 2916\u20132924.",
            "year": 2022
        },
        {
            "authors": [
                "Shervin Malmasi",
                "Anjie Fang",
                "Besnik Fetahu",
                "Sudipta Kar",
                "Oleg Rokhlenko."
            ],
            "title": "MultiCoNER: A large-scale multilingual dataset for complex named entity recognition",
            "venue": "Proceedings of the 29th International Conference on Computational Linguis-",
            "year": 2022
        },
        {
            "authors": [
                "Shervin Malmasi",
                "Anjie Fang",
                "Besnik Fetahu",
                "Sudipta Kar",
                "Oleg Rokhlenko."
            ],
            "title": "SemEval-2022 task 11: Multilingual complex named entity recognition (MultiCoNER)",
            "venue": "Proceedings of the 16th International Workshop on Semantic Evaluation",
            "year": 2022
        },
        {
            "authors": [
                "Stephen Mayhew",
                "Tatiana Tsygankova",
                "Dan Roth."
            ],
            "title": "ner and pos when nothing is capitalized",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Lan-",
            "year": 2019
        },
        {
            "authors": [
                "Tao Meng",
                "Anjie Fang",
                "Oleg Rokhlenko",
                "Shervin Malmasi."
            ],
            "title": "GEMNET: effective gated gazetteer representations for recognizing complex entities in low-context input",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the As-",
            "year": 2021
        },
        {
            "authors": [
                "Erik Tjong Kim Sang",
                "Fien De Meulder."
            ],
            "title": "Introduction to the conll-2003 shared task: Languageindependent named entity recognition",
            "venue": "Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003, pages 142\u2013147.",
            "year": 2003
        },
        {
            "authors": [
                "Dingmin Wang",
                "Yan Song",
                "Jing Li",
                "Jialong Han",
                "Haisong Zhang."
            ],
            "title": "A hybrid approach to automatic corpus generation for chinese spelling check",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages",
            "year": 2018
        }
    ],
    "sections": [
        {
            "text": "The dataset is compiled from open resources like Wikipedia and Wikidata, and is publicly available.1 Evaluation based on the XLMRoBERTa baseline highlights the unique challenges posed by MULTICONER V2: (i) the fine-grained taxonomy is challenging, where the scores are low with macro-F1=0.63 (across all languages), and (ii) the corruption strategy significantly impairs performance, with entity corruption resulting in 9% lower performance relative to non-entity corruptions across all languages. This highlights the greater impact of entity noise in contrast to context noise."
        },
        {
            "heading": "1 Introduction",
            "text": "Named Entity Recognition (NER) is a core task in Natural Language Processing that involves identifying entities and recognizing their type (e.g., person or location). Recently, Transformer-based NER approaches have achieved new state-of-theart (SOTA) results on well-known benchmark datasets like CoNLL03 and OntoNotes (Devlin et al., 2019). Despite these strong results, as shown by Malmasi et al. (2022a), there remain a number of practical challenges that are not well represented by existing datasets, including low context, diverse domains, missing casing information, and text with grammatical and typographical errors. Furthermore, wide use of coarse-grained taxonomies often contribute to a lower utility of NER systems, as additional fine-grained disam-\n1 https://registry.opendata.aws/multiconer\nhttps://huggingface.co/datasets/MultiCoNER/multiconer_v2 https://www.kaggle.com/datasets/cryptexcode/multiconer-2\nbiguation steps are needed in many practical systems. Furthermore, models trained on datasets such as CoNLL03 perform significantly worse on unseen entities or noisy texts (Meng et al., 2021)."
        },
        {
            "heading": "1.1 Real-world Challenges in NER",
            "text": "Outside the news domain used in datasets like CoNLL03, there are many challenges for NER. We categorize the challenges (cf. Table 5 in Appendix A) typically encountered in NER by several dimensions: (i) available context around entities, (ii) named entity surface form complexity, (iii) frequency distribution of named entity types, (iv) multilinguality, (v) fine-grained entity types, and (vi) noisy text containing typing errors. MULTICONER V2 represents all of these challenges, with a focus on (v) and (vi).\nFine-Grained Entity Types Most datasets use coarse types (e.g., PERSON, LOCATION in CoNLL03), with each type subsuming a wide range of entities with different properties. For instance, for LOCATION, different entities can appear in different contexts and have diverging surface form token distributions, which can lead to challenges in terms of NER, e.g., airports as part of LOCATION are often named after notable persons, thus for sentences with low context, NER models can confuse them with PERSON entities.\nWe introduce a fine-grained named entity taxonomy with 33 classes, where coarse grained types are further broken down into fine-grained types.\nAdditionally, we introduce types from the medical domain, presenting a further challenge for NER. Fine-grained NER poses challenges like distinguishing between different fine-grained types of a coarse type. For example, correctly identifying fine-grained types (e.g., SCIENTIST, ARTIST) of a PERSON entity requires effective representation of the context and often external world knowledge. Since two instances belonging to different types may have the same name, the context is needed to correctly disambiguate the types.\nNoisy Text Existing NER datasets mostly consist of well-formed sentences. This creates a gap in terms of training and evaluation, and datasets assume that models are applied only on wellformed content. However, NER models are typically used in web settings, exposed to usercreated content. We present a noisy test set in MULTICONER V2, representing typing errors affected by keyboard layouts. This noise, impacting named entity and context tokens, presents challenges for accurately identifying entity spans.\nContributions Our contributions through this work can be summarized as follows:\n1. We created a taxonomy of 33 fine-grained NER classes across six coarse categories (PERSON, LOCATION, GROUP, PRODUCT, CREATIVEWORK, and MEDICAL). We use the taxonomy to develop a dataset for 12 languages based on this taxonomy. Our experiments show that NER models have a large performance gap (\u21e114% macro-F1) in identifying fine-grained classes versus coarse ones.\n2. We implement techniques to mimic web text errors, such as typos, to enhance and test NER model resilience to noise.\n3. Experimentally we show that noise can significantly impact NER performance. On average, across all languages, the macro-F1 score for corrupted entities is approximately 9% lower than that for corrupted context, emphasizing the greater influence of entity noise as opposed to context noise.\nMULTICONER V2 has been used as the main dataset for the MULTICONER V2 shared task (Fetahu et al., 2023) at SemEval-2023."
        },
        {
            "heading": "2 MULTICONER V2 Dataset Overview",
            "text": "MULTICONER V2 was designed to address the challenges described in \u00a71.1. It contains 12 languages, including multilingual subsets, with a total of 2.3M instances with 32M tokens and a total of 2.2M entities (1M unique). While focused on different challenges, we adopted the data construction process described of the original MULTICONER (v1) corpus Malmasi et al. (2022a) and details about the process are provided in Appendix B. While both the v1 and v2 datasets focus on complex entities, MULTICONER V2 introduces a fine-grained taxonomy, adds realistic noise as well as 5 new languages.\nNext we describe the fine-grained NER taxonomy construction and noise generation processes."
        },
        {
            "heading": "2.1 NER Taxonomy",
            "text": "MULTICONER V2 builds on top of the WNUT 2017 (Derczynski et al., 2017) taxonomy entity types, with the difference that it groups GROUP and CORPORATION types into one,2 and introduces the MEDICAL type. Next, we divide coarse classes into fine-grained sub-types. Table 1 shows the 33 fine-grained classes, grouped across 6 coarse grained types. Our taxonomy is inspired by DBpedia\u2019s ontology (Auer et al., 2007), where a subset of types are chosen as fine-grained types that have support in encyclopedic resources.\nThis taxonomy allows us to capture fine-grained differences in entity types, where the entity context plays a crucial role in distinguishing the different types, as well as cases where external knowledge can help where context may be insufficient (e.g.,PRIVATECORP vs. PUBLICCORP)."
        },
        {
            "heading": "2.2 Languages and Subsets",
            "text": "MULTICONER V2 includes 12 languages: Bangla (BN), Chinese (ZH), English (EN), Farsi (FA), French (FR), German (DE), Hindi (HI), Italian (IT), Portuguese (PT), Spanish (ES), Swedish (SV), Ukrainian (UK). The chosen languages span diverse typology, writing systems, and range from well-resourced (e.g., EN) to low-resourced (e.g., FA, BN).\nMonolingual Subsets Each of the 12 languages has their own subset with data from all domains, consisting of a total of 2.3M instances.\n2The original definition is ambiguous in the difference between a CORPORATION and GROUP entity.\nMultilingual Subset This subset is designed for evaluation of multilingual models, and should be used under the assumption that the language for each sentence is unknown. The training and development set from the monolingual splits are merged, while, for the test set, we randomly select at most 35,000 samples resulting in a total of 358,668 instances. The test set consists of a subset of samples that incorporate simulated noise."
        },
        {
            "heading": "2.3 Noise Generation",
            "text": "To emulate realistic noisy scenarios for NER, we developed different corruption strategies that reflect typing errors by users. We considered two main strategies: (i) replace characters in a token with neighboring characters (mimicking typing errors) on a given keyboard layout for a target language, and (ii) replace characters with visually similar characters (mimicking incorrectly visually recognized characters, e.g., OCR).\nWe applied the noise only on the test set3 for a subset of languages (EN, ZH, IT, ES, FR, PT and SV). The noise can corrupt context tokens and entity tokens. For ZH, the corruption strategies are applied at the character level. We followed Wang et al. (2018)\u2019s approach that automatically extracts visually similar characters using OCR tools and phonologically similar characters using ASR tools. We used their constructed dictionary4 to match targeting characters.\nFor the other six languages, we developed word-level corruption strategies based on common typing mistakes, utilizing language specific keyboard layouts.5 Table 2 summarizes the designed corruptions and their probabilities of occurrence.\nNoise Sampling We introduce synthetic noise into 30% of the test set. For each corrupted sentence, the number of corrupted tokens is sampled\n3To emulate a realistic scenario, where models are trained on clean text, but are exposed directly to user input.\n4 https://github.com/wdimmy/Automatic-Corpus-Generation\n5We extended the keyboard layouts to include 7 languages: https://github.com/ranvijaykumar/typo.\nfrom a truncated Poisson distribution ( = 1 and  3). Since the minimum of can be zero, we use 0 = + 1 as the final corruption count.\nCorruptions can apply to entity or non-entity (context) tokens. For ZH, we corrupt a context or entity character with 15% and 85% probability respectively. Using the corruption dictionary, we replace the original character at random with a visually or phonologically similar character from identified candidates. For the other languages, the probability of corrupting entity tokens is set to 80%. This is lower given that each letter in entities can be corrupted and the number of candidates in entities is more than that in ZH. On a word chosen for corruption, we perform a corruption action based on probabilities shown in Table 2. Examples of corrupted sentences are shown in Figure 2."
        },
        {
            "heading": "EN the species was described by dietrich brandis after the forester t.f. boundill0n (bourdillon).",
            "text": ""
        },
        {
            "heading": "3 NER Model Performance",
            "text": "We assess if MULTICONER V2 is challenging (cf. \u00a71.1) by training and testing with XLM-RoBERTa (Conneau et al., 2020, XLMR).\nMetrics We measure the model performance using Precision (P), Recall (R), and F1, where for F1 we distinguish between micro/macro averages."
        },
        {
            "heading": "3.1 Results",
            "text": "Table 3 shows the results obtained for the XLMR baseline. The results show the micro-F1 scores achieved on the individual NER coarse classes (See Appendix C for fine-grained performance).\nAcross all subsets, XLMR achieves the highest performance of micro-F1=0.77 for HI, and lowest micro-F1=0.61 for EN, ES and FA. The reason for the higher F1 scores on HI (or BN second highest score) is due to the smaller test set size, since the data is translated and is more scarce. For the rest, the F1 scores are typically lower, this is is mainly associated with the larger test sizes."
        },
        {
            "heading": "3.2 Impact of Fine-Grained Taxonomy",
            "text": "Performance varies greatly across fine-grained types within a coarse type (Table 10). Using MULTI as a representative baseline, we note a\ngap of up to 34% in F1 scores, as seen with CREATIVEWORK and PERSON, indicating the challenges of introducing fine-grained types not seen at the coarse level (Malmasi et al., 2022b).\nThe gap can be attributed to two primary components: (i) the incorrect classification of entities under inappropriate sub-categories or broad categories, and (ii) recall problems where certain entities are missed (assigned the O tag).\nMisclassifications Figure 3 shows the distribution of predicted coarse tags for MULTI. Most of the misclassifications are for the GROUP class, which is often misclassified with LOCATION or PERSON. Yet, this does not reveal how the models handle the specific classes within a coarse type.\nTo understand the challenges presented by our taxonomy for NER models, we further inspect misclassifications within the coarse type GROUP in Figure 7 of Appendix D. There are several types that are misclassified, e.g., PUBLICCORP as ORG, or PRIVATECORP with ORG or PUBLICCORP. On the other hand, MUSICALGRP is often misclassified with types from other coarse types, such as ARTIST or VISUALWORK. This highlights some of the issues and challenges that NER models face when exposed to more fine-grained types.\nRecall Issues Figure 3 shows the rate of entities marked with the O tag. Consistent recall issues affect the MEDICAL, CREATIVEWORK, and PRODUCT classes, often with more than 20% of entities being missed. Across languages the recall can vary, with even higher portions of missed entities (e.g., FA, ZH, EN with more than 30+% of MEDICAL entities missed, cf. Figure 6)."
        },
        {
            "heading": "3.3 Impact of Corruption Strategies",
            "text": "Figure 4 shows the impact of noise level on the macro-F1 score, where we see a strong negative correlation between noise level and performance (Spearman\u2019s correlation across different languages is b\u21e2 = 0.998). For ZH, the impact is higher than for the rest of languages and the macro-F1 decreases faster: with 50% of test data corrupted, the score decreases approximately by 9%, while for the rest it drops only between 4% and 5.5%.\nTable 4 shows the impact of corruption strategies on context and entity tokens, targeting exclusively the respective tokens. For both variants, the number of corruptions per sampled with = 1\nand  4 hyper-parameters. Results show that entity token corruption has a notably detrimental effect on performance, in contrast to context token corruption. On average across all languages, the macro-F1 score for corrupted entities is approximately 9% lower than the performance on corrupted context, emphasizing the higher impact of entity noise compared to context noise. Finally, Table 11 shows the baseline\u2019s performance breakdown on the clean and noisy subsets of the test set."
        },
        {
            "heading": "4 Conclusions and Future Work",
            "text": "We presented MULTICONER V2, a new largescale dataset that represents a number of current challenges in NER. The results from XLMR indicate that our dataset is difficult, with a macro-F1 score of only 0.63 on the MULTI test set.\nThe results presented illustrate that the MULTICONER V2, with its detailed NER taxonomy and noisy test sets, poses substantial challenges to large pre-trained language models\u2019 performance. It is our hope that this resource will help further research for building better NER systems. This dataset can serve as a benchmark for evaluating NER methods that infuse external entity knowledge, as well as assessing NER model robustness on noisy scenarios.\nWe envision several directions for future work. The extension of MULTICONER V2 to additional languages is the most straightforward direction for\nfuture work. Expansion to other more challenging domains, such as search queries (Fetahu et al., 2021), can help explore fine-grained NER within even shorter inputs.\nFinally, evaluating the performance of this data using zero-shot and few-shot cross-lingual models (Fetahu et al., 2022) or even unsupervised NER models (Iovine et al., 2022) can help us better understand the role of data quantity in building effective NER systems."
        },
        {
            "heading": "A NER Challenges & Motivation",
            "text": "Table 5 lists the key challenges that are introduced in MULTICONER V2 for NER model. The challenges range from low context sentences, complex entities with additionally fine-grained NER taxonomy, to syntactic issues such as lowercasing as well as noisy context and entity tokens."
        },
        {
            "heading": "B Dataset Construction",
            "text": "This section provides a detailed description of the methods used to generate our dataset. Namely, how sentences are selected from Wikipedia, and additionally how the sentences are tagged with the corresponding named entities and how the underlying fine-grained taxonomy is constructed. Additionally, the different data splits are described, as well as the license of our dataset.\nTable 6 shows the detailed stats about our proposed MULTICONER V2 dataset.\nB.1 Fine-Grained NER Taxonomy\nWe use our fine-grained NER taxonomy, namely, the entities associated with each type as part of the automated data generation process. To generate entity associations to our fine-grained taxonomy we make use of Wikidata. The entity type graph in Wikidata is noisy, containing cycles, and often types are associated with different parent types, thus, making it challenging to use the data as is for generating fine-grained and clean taxonomies for NER. We carry out a series of steps that we explain in the following.\nType Cycles. Wikidata type graph contains cycles, which are not suitable for taxonomy generation. We first break the cycles through a depthfirst search approach, by following the children of a given type, and whenever a given type has children that are already visited (encountered during the DFS), those edges are cut out.\nEntity to Fine-Grained Type Association: Our taxonomy consists of 6 coarse types, PERSON, LOCATION, GROUP, MEDICAL, PRODUCT, CREATIVEWORK, and 33 of fine-grained types. The coarse types represent classes that we aim to have in our taxonomy, and with the exception of MEDICAL and PRODUCT, which at the same time\nrepresent contributions and enrichment of the current NER data landscape, represent common types encountered in NER research. We generate the entity to type association at the fine-grained level, given that the association at the coarse level can be inferred automatically.\nFor each of the fine-grained types we manually identify specific Wikidata types, from where we then query the associated entities. Wikidata types and entity type associations are done by the Wikidata community, and thus, often there are redundant entity types (with similar semantics). From the initially identified types and their entities, we expand our set of Wikidata types that are associated to a fine-grained type in our taxonomy, by additionally taking into account other Wikidata types that the entities are associated with. From the identified Wikidata types for a fine-grained type in our taxonomy, we traverse the entire Wikidata type graph, and obtain all the entities that are associated with those types and all of their children.\nFinally, this resulted in 16M Wikidata entities, whose distribution across the different coarse and fine-grained types is shown in Table 7 and 8.\nQuality Filtering: Finally, once we have obtained all the possible Wikidata entities that are associated with the fine-grained types in our NER taxonomy, we need to ensure that the associations are not ambiguous.\nWe tackle the problem ambiguity, namely Wikidata entities being associated with different types, causing them to be in different NER classes at the same time. To resolve such ambiguous cases, we compute the distribution of entities that are associated with a set of fine-grained NER type, and based on a small sample analysis, manually determine with which NER type they should be associated.\nB.2 Sentence Extraction\nMULTICONER V2 is constructed from encyclopedic resources such as Wikipedia, with focus on sentences with low context, sampled using heuristics that identify sentences that represent the NER challenges we target in Table 5. Figure 5 shows the steps from our data construction workflow, which for a given language it extracts sentences from the corresponding Wikipedia locale, and applies a series of steps to generate the NER data. This process is performed for the following languages: FA, FR, EN, ES, IT, PT, SV. For the rest of languages\n(BN, ZH, DE, HI), we apply machine translation to obtain the data, as described below.\nIn the following we explain the required steps to generate the final output of sentences tagged with the corresponding entity types.\n1. Sentence Extraction: From the cleaned Wikipedia pages, we extract sentences, which contain hyperlinks that point to entities that\ncan be mapped into their corresponding Wikidata entity equivalent. In this way we ensure that the entities in a sentence are marked with high quality to their corresponding fine-grained types (cf. Table 8).\n2. Sentence Filtering: We remove sentences that are shorter than 28 characters and longer than 128 characters. In this way, we ensure that the sentences contain complete clauses,6 and furthermore, by filtering out longer sentences (more than 128 characaters) we ensure that the remaining sentences have low context, making NER more challenging. Finally, we additionally filter out sentences that contain capitalized nouns (proper nouns), which are not interlinked 6i.e., sentences shorter than 28 characters may contain only entity names, and as such are not suitable for NER\nto entities. This is done for languages that follow capitalization rules for proper nouns (e.g. EN and ES). This presents an important step, since if a proper noun is not interlinked to an entity, however it represents one, this will artificially inflate false positive scores for NER models.\nB.3 Sentence Translation\nFor the languages, for which we cannot automatically extract sentences and their corresponding tagged entities (e.g. DE uses capitalization for all nouns, thus, we cannot accurately distinguish nouns against proper nouns), we apply automatic translation to generate two portions of our data. For four languages (BN, DE, HI, ZH), Wikipedia sentences are translated from the EN data.\nWe use the Google Translation API7 to perform our translations. The input texts may contain known entity spans or slots.\nTo prevent the spans from being translated, we leveraged the notranslate attribute to mark them and prevent from being translated. The translation quality in the different languages such as\n7https://cloud.google.com/translate\nBN, ZH, DE and HI is very high, with over 90% translation accuracy (i.e., accuracy as measured by human annotators in terms of the translated sentence retaining the semantic meaning and as well have a correct syntactic structure in the target language).\nB.4 Data Splits\nTo ensure that obtained NER results on this dataset are reproducible, we create three predefined sets for training, development and testing. The entities in each class are distributed following a root square normalization, which takes into account the actual distribution of a given entity class. Additionally we set a minimum number of entities from a class that need to appear in each split. This combination allows us not to heavily bias the data splits towards the more popular classes (e.g. WRITTENWORK). Table 6 shows detailed statistics for each of the 13 subtasks and data splits.\nTraining Data For the training data split, we limit the size to approx. 16k sentences. The number of instances was chosen to be comparable to well-known NER datasets such as CoNLL03 (Sang and De Meulder, 2003). Note that in the case of the Multi subset, the training split contains all the instances from the individual language splits.\nDevelopment Data We randomly sample around \u21e0800 instances,8 a reasonable amount of data for assessing model generalizability.\nTest Data Finally, the testing set represents the remaining instances that are not part of the training or development set. To avoid exceedingly large test sets, we limit the number of instances in the test set to be around at most 250k sentences (cf. Table 6). The only exception is for the Multi, which was generated from the language specific test splits, and was downsampled to contain at most 35k from each monolingual test set, resulting in a total of 358k instances. The larger test set sizes are ideal to assess the generalizability and robustness of models on unseen entities.\nB.5 License, Availability, and File Format\nThe dataset is released under a CC BY-SA 4.0 license, which allows adapting the data. Details\n8With the exception of the translated languages, DE, HI, BN, and ZH, where the data is scarce.\nabout the license are available on the Creative Commons website.9\nThe data is distributed using the commonly used BIO tagging scheme in CoNLL03 format (Sang and De Meulder, 2003). The complete dataset will be available for download.1\nB.6 Noise Generation\nOriginal Character Visually Similar Ones Phonetically Similar Ones\nFinally, as part of the data construction, we generate a subset of corrupted test set (30%) that adds noise to either the context or entity tokens as described in in \u00a72.3. The purpose of the noisy test set is to assess the robustness of NER models, trained on clean data, in spotting correctly the named entities that may contain either context or entity noise. The noise mimics typing noise or noise that may be attributed to OCR, where characters are misrecognized with visually similar characters.\nFor all the languages the noise is applied at the token level, where random characters within a token are replaced with either a character that comes from typing errors (neighboring characters on a given keyboard layout for a specific language) or visually similar characters. For ZH is applied at character level, this is due to the language structure. Table 9 shows examples of visually similar and phonetically similar characters in ZH.\nFigure 2 shows examples of corrupted sentences from seven different languages."
        },
        {
            "heading": "C Fine Grained NER Performance",
            "text": "Performance on Fine-grained Classes: Table 10 shows the results for the different fine-\n9 https://creativecommons.org/licenses/by-sa/4.0\ngrained classes and languages. We note a high divergence in terms of performance for the different fine-grained types within a coarse type.\nPerformance on Noise and Clean Test Subsets: From Table 11, we can see that the simulated noise can have a huge impact on the performance of the baseline model. On average, the Macro-F1 score on noisy subset is about 10% lower than that on the clean subset. In this section, we study the impact of different corruption strategies, and amount of corruption on the test set. Note that, differently from the fixed amount of noise of 30% applied on the test set, here we assess the range of noise with up to 50% of the test data."
        },
        {
            "heading": "D Fine Grained NER Error Analysis",
            "text": "Here we show the errors that the XLMR baseline makes at the fine-grained type level. Figure 7 shows the misclassifications of the baseline approach. Here, we notice that within the GROUP type there are several fine-grained types that are often confused with other types within GROUP, e.g. PUBLICCORP often misclassified as ORG, or PRIVATECORP often misclassified with ORG or PUBLICCORP. On the other hand, MUSICALGRP is often misclassified with other fine-grained types from other coarse types, such as ARTIST or VISUALWORK.\nThe error analysis highlights some of the issues that NER models face when exposed to more finegrained types. Furthermore, through this analysis, we provide insights on the challenges presented by the newly proposed NER taxonomy and areas of improvement in terms of NER performance."
        }
    ],
    "title": "MultiCoNER v2: a Large Multilingual dataset for Fine-grained and Noisy Named Entity Recognition",
    "year": 2023
}