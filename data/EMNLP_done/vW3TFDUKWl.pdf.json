{
    "abstractText": "Modern NLP models are often trained over large untrusted datasets, raising the potential for a malicious adversary to compromise model behaviour. For instance, backdoors can be implanted through crafting training instances with a specific textual trigger and a target label. This paper posits that backdoor poisoning attacks exhibit spurious correlation between simple text features and classification labels, and accordingly, proposes methods for mitigating spurious correlation as means of defence. Our empirical study reveals that the malicious triggers are highly correlated to their target labels; therefore such correlations are extremely distinguishable compared to those scores of benign features, and can be used to filter out potentially problematic instances. Compared with several existing defences, our defence method significantly reduces attack success rates across backdoor attacks, and in the case of insertion-based attacks, our method provides a near-perfect defence. 1",
    "authors": [
        {
            "affiliations": [],
            "name": "Xuanli He"
        },
        {
            "affiliations": [],
            "name": "Qiongkai Xu"
        },
        {
            "affiliations": [],
            "name": "Jun Wang"
        },
        {
            "affiliations": [],
            "name": "Benjamin Rubinstein"
        },
        {
            "affiliations": [],
            "name": "Trevor Cohn"
        }
    ],
    "id": "SP:d00920513f07b539f5c0230c17ac2bd09600a6b1",
    "references": [
        {
            "authors": [
                "Samuel R. Bowman",
                "Gabor Angeli",
                "Christopher Potts",
                "Christopher D. Manning."
            ],
            "title": "A large annotated corpus for learning natural language inference",
            "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages",
            "year": 2015
        },
        {
            "authors": [
                "Nicholas Carlini",
                "Andreas Terzis."
            ],
            "title": "Poisoning and backdooring contrastive learning",
            "venue": "International Conference on Learning Representations.",
            "year": 2022
        },
        {
            "authors": [
                "Bryant Chen",
                "Wilka Carvalho",
                "Nathalie Baracaldo",
                "Heiko Ludwig",
                "Benjamin Edwards",
                "Taesung Lee",
                "Ian M. Molloy",
                "Biplav Srivastava."
            ],
            "title": "Detecting backdoor attacks on deep neural networks by activation clustering",
            "venue": "CoRR, abs/1811.03728.",
            "year": 2018
        },
        {
            "authors": [
                "Kangjie Chen",
                "Yuxian Meng",
                "Xiaofei Sun",
                "Shangwei Guo",
                "Tianwei Zhang",
                "Jiwei Li",
                "Chun Fan."
            ],
            "title": "Badpre: Task-agnostic backdoor attacks to pre-trained NLP foundation models",
            "venue": "International Conference on Learning Representations.",
            "year": 2022
        },
        {
            "authors": [
                "Sishuo Chen",
                "Wenkai Yang",
                "Zhiyuan Zhang",
                "Xiaohan Bi",
                "Xu Sun."
            ],
            "title": "Expose backdoors on the way: A feature-based efficient defense against textual backdoor attacks",
            "venue": "arXiv preprint arXiv:2210.07907.",
            "year": 2022
        },
        {
            "authors": [
                "Ting Chen",
                "Simon Kornblith",
                "Mohammad Norouzi",
                "Geoffrey Hinton."
            ],
            "title": "A simple framework for contrastive learning of visual representations",
            "venue": "International conference on machine learning, pages 1597\u20131607. PMLR.",
            "year": 2020
        },
        {
            "authors": [
                "Xiaoyi Chen",
                "Yinpeng Dong",
                "Zeyu Sun",
                "Shengfang Zhai",
                "Qingni Shen",
                "Zhonghai Wu."
            ],
            "title": "Kallima: A clean-label framework for textual backdoor attacks",
            "venue": "European Symposium on Research in Computer Security, pages 447\u2013466. Springer.",
            "year": 2022
        },
        {
            "authors": [
                "Xinyun Chen",
                "Chang Liu",
                "Bo Li",
                "Kimberly Lu",
                "Dawn Song."
            ],
            "title": "Targeted backdoor attacks on deep learning systems using data poisoning",
            "venue": "Journal of Environmental Sciences (China) English Ed.",
            "year": 2017
        },
        {
            "authors": [
                "Christopher Clark",
                "Mark Yatskar",
                "Luke Zettlemoyer."
            ],
            "title": "Don\u2019t take the easy way out: Ensemble based methods for avoiding known dataset biases",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th In-",
            "year": 2019
        },
        {
            "authors": [
                "Jiazhu Dai",
                "Chuanshuai Chen",
                "Yufeng Li."
            ],
            "title": "A backdoor attack against LSTM-based text classification systems",
            "venue": "IEEE Access, 7:138872\u2013138878.",
            "year": 2019
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova"
            ],
            "title": "2019. BERT: Pre-training",
            "year": 2019
        },
        {
            "authors": [
                "Jacob Dumford",
                "Walter Scheirer."
            ],
            "title": "Backdooring convolutional neural networks via targeted weight perturbations",
            "venue": "2020 IEEE International Joint Conference on Biometrics (IJCB), pages 1\u20139. IEEE.",
            "year": 2020
        },
        {
            "authors": [
                "Yansong Gao",
                "Yeonjae Kim",
                "Bao Gia Doan",
                "Zhi Zhang",
                "Gongxuan Zhang",
                "Surya Nepal",
                "Damith C. Ranasinghe",
                "Hyoungshick Kim."
            ],
            "title": "Design and evaluation of a multi-domain trojan detection method on deep neural networks",
            "venue": "IEEE Transactions on De-",
            "year": 2022
        },
        {
            "authors": [
                "Yansong Gao",
                "Change Xu",
                "Derui Wang",
                "Shiping Chen",
                "Damith C. Ranasinghe",
                "Surya Nepal."
            ],
            "title": "Strip: A defence against trojan attacks on deep neural networks",
            "venue": "Proceedings of the 35th Annual Computer Security Applications Conference, ACSAC \u201919, page",
            "year": 2019
        },
        {
            "authors": [
                "Matt Gardner",
                "William Merrill",
                "Jesse Dodge",
                "Matthew Peters",
                "Alexis Ross",
                "Sameer Singh",
                "Noah A. Smith."
            ],
            "title": "Competency problems: On finding and removing artifacts in language data",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Nat-",
            "year": 2021
        },
        {
            "authors": [
                "Tianyu Gu",
                "Brendan Dolan-Gavitt",
                "Siddharth Garg."
            ],
            "title": "Badnets: Identifying vulnerabilities in the machine learning model supply chain",
            "venue": "arXiv preprint arXiv:1708.06733.",
            "year": 2017
        },
        {
            "authors": [
                "Chuan Guo",
                "Ruihan Wu",
                "Kilian Q Weinberger."
            ],
            "title": "Trojannet: Embedding hidden trojan horse models in neural networks",
            "venue": "arXiv e-prints, pages arXiv\u20132002.",
            "year": 2020
        },
        {
            "authors": [
                "Suchin Gururangan",
                "Swabha Swayamdipta",
                "Omer Levy",
                "Roy Schwartz",
                "Samuel Bowman",
                "Noah A. Smith."
            ],
            "title": "Annotation artifacts in natural language inference data",
            "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for",
            "year": 2018
        },
        {
            "authors": [
                "He He",
                "Sheng Zha",
                "Haohan Wang."
            ],
            "title": "Unlearn dataset bias in natural language inference by fitting the residual",
            "venue": "Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP (DeepLo 2019), pages 132\u2013142, Hong Kong, China.",
            "year": 2019
        },
        {
            "authors": [
                "Armand Joulin",
                "Laurens van der Maaten",
                "Allan Jabri",
                "Nicolas Vasilache."
            ],
            "title": "Learning visual features from large weakly supervised data",
            "venue": "Computer Vision \u2013 ECCV 2016, pages 67\u201384, Cham. Springer International Publishing.",
            "year": 2016
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Jimmy Ba."
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "arXiv preprint arXiv:1412.6980.",
            "year": 2014
        },
        {
            "authors": [
                "Keita Kurita",
                "Paul Michel",
                "Graham Neubig."
            ],
            "title": "Weight poisoning attacks on pretrained models",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2793\u2013 2806.",
            "year": 2020
        },
        {
            "authors": [
                "Linyang Li",
                "Demin Song",
                "Xiaonan Li",
                "Jiehang Zeng",
                "Ruotian Ma",
                "Xipeng Qiu."
            ],
            "title": "Backdoor attacks on pre-trained models by layerwise weight poisoning",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2021
        },
        {
            "authors": [
                "Zichao Li",
                "Dheeraj Mekala",
                "Chengyu Dong",
                "Jingbo Shang."
            ],
            "title": "BFClass: A backdoor-free text classification framework",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021, pages 444\u2013453, Punta Cana, Dominican Republic. Associa-",
            "year": 2021
        },
        {
            "authors": [
                "Yingqi Liu",
                "Shiqing Ma",
                "Yousra Aafer",
                "Wen-Chuan Lee",
                "Juan Zhai",
                "Weihang Wang",
                "Xiangyu Zhang."
            ],
            "title": "Trojaning attack on neural networks",
            "venue": "25th Annual Network and Distributed System Security Symposium, NDSS 2018, San Diego, California, USA, February",
            "year": 2018
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "RoBERTa: A robustly optimized BERT pretraining approach",
            "venue": "arXiv preprint arXiv:1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Sean MacAvaney",
                "Hao-Ren Yao",
                "Eugene Yang",
                "Katina Russell",
                "Nazli Goharian",
                "Ophir Frieder."
            ],
            "title": "Hate speech detection: Challenges and solutions",
            "venue": "PLOS ONE, 14(8):1\u201316.",
            "year": 2019
        },
        {
            "authors": [
                "Naren Manoj",
                "Avrim Blum."
            ],
            "title": "Excess capacity and backdoor poisoning",
            "venue": "Advances in Neural Information Processing Systems, 34:20373\u201320384.",
            "year": 2021
        },
        {
            "authors": [
                "Tom McCoy",
                "Ellie Pavlick",
                "Tal Linzen."
            ],
            "title": "Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3428\u20133448, Florence,",
            "year": 2019
        },
        {
            "authors": [
                "Fanchao Qi",
                "Yangyi Chen",
                "Mukai Li",
                "Yuan Yao",
                "Zhiyuan Liu",
                "Maosong Sun."
            ],
            "title": "ONION: A simple and effective defense against textual backdoor attacks",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2021
        },
        {
            "authors": [
                "Fanchao Qi",
                "Mukai Li",
                "Yangyi Chen",
                "Zhengyan Zhang",
                "Zhiyuan Liu",
                "Yasheng Wang",
                "Maosong Sun."
            ],
            "title": "Hidden killer: Invisible textual backdoor attacks with syntactic trigger",
            "venue": "Proceedings of the",
            "year": 2021
        },
        {
            "authors": [
                "Fanchao Qi",
                "Yuan Yao",
                "Sophia Xu",
                "Zhiyuan Liu",
                "Maosong Sun."
            ],
            "title": "Turn the combination lock: Learnable textual backdoor attacks via word substitution",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics",
            "year": 2021
        },
        {
            "authors": [
                "Peng Qi",
                "Yuhao Zhang",
                "Yuhui Zhang",
                "Jason Bolton",
                "Christopher D. Manning."
            ],
            "title": "Stanza: A Python natural language processing toolkit for many human languages",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics:",
            "year": 2020
        },
        {
            "authors": [
                "A. Saha",
                "A. Tejankar",
                "S. Koohpayegani",
                "H. Pirsiavash."
            ],
            "title": "Backdoor attacks on self-supervised learning",
            "venue": "2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 13327\u201313336, Los Alamitos, CA, USA. IEEE Com-",
            "year": 2022
        },
        {
            "authors": [
                "Kai Shu",
                "Amy Sliva",
                "Suhang Wang",
                "Jiliang Tang",
                "Huan Liu."
            ],
            "title": "Fake news detection on social media: A data mining perspective",
            "venue": "SIGKDD Explor. Newsl., 19(1):22\u201336.",
            "year": 2017
        },
        {
            "authors": [
                "Richard Socher",
                "Alex Perelygin",
                "Jean Wu",
                "Jason Chuang",
                "Christopher D. Manning",
                "Andrew Ng",
                "Christopher Potts."
            ],
            "title": "Recursive deep models for semantic compositionality over a sentiment treebank",
            "venue": "Proceedings of the 2013 Conference on Empiri-",
            "year": 2013
        },
        {
            "authors": [
                "J\u00f6rg Tiedemann",
                "Santhosh Thottingal."
            ],
            "title": "OPUSMT \u2013 building open translation services for the world",
            "venue": "Proceedings of the 22nd Annual Conference of the European Association for Machine Translation, pages 479\u2013480, Lisboa, Portugal. European Associa-",
            "year": 2020
        },
        {
            "authors": [
                "Brandon Tran",
                "Jerry Li",
                "Aleksander Madry."
            ],
            "title": "Spectral signatures in backdoor attacks",
            "venue": "Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc.",
            "year": 2018
        },
        {
            "authors": [
                "Prasetya Ajie Utama",
                "Nafise Sadat Moosavi",
                "Iryna Gurevych."
            ],
            "title": "Mind the trade-off: Debiasing NLU models without degrading the in-distribution performance",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages",
            "year": 2020
        },
        {
            "authors": [
                "Alex Wang",
                "Amanpreet Singh",
                "Julian Michael",
                "Felix Hill",
                "Omer Levy",
                "Samuel Bowman."
            ],
            "title": "GLUE: A multi-task benchmark and analysis platform for natural language understanding",
            "venue": "Proceedings of the",
            "year": 2018
        },
        {
            "authors": [
                "Pradeep Dasigi"
            ],
            "title": "Generating data to mitigate",
            "year": 2022
        },
        {
            "authors": [
                "Xu Sun"
            ],
            "title": "RAP: Robustness-Aware Perturba",
            "year": 2021
        },
        {
            "authors": [
                "Zhao"
            ],
            "title": "Latent backdoor attacks on deep neural",
            "year": 2019
        },
        {
            "authors": [
                "Xiang Zhang",
                "Junbo Zhao",
                "Yann LeCun."
            ],
            "title": "Character-level convolutional networks for text classification",
            "venue": "Advances in Neural Information Processing Systems, 28.",
            "year": 2015
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Due to the significant success of deep learning technology, numerous deep learning augmented applications have been deployed in our daily lives, such as e-mail spam filtering (Bhowmick and Hazarika, 2018), hate speech detection (MacAvaney et al., 2019), and fake news detection (Shu et al., 2017). This is fuelled by massive datasets. However, this also raises a security concern related to backdoor attacks, where malicious users can manoeuvre the attacked model into misbehaviours using poisoned data. This is because, compared to expensive labelling efforts, uncurated data is easy to obtain, and one can use them for training a competitive model (Joulin et al., 2016; Tiedemann and Thottingal, 2020). Meanwhile, the widespread use of self-supervised learning increases the reliance on\n\u2217Now at Google DeepMind. 1The code and data are available at: https://github.\ncom/xlhex/emnlp2023_z-defence.git.\nuntrustworthy data (Devlin et al., 2019; Liu et al., 2019; Chen et al., 2020). Thus, there is the potential for significant harm through backdooring victim pre-trained or downstream models via data poisoning.\nBackdoor attacks manipulate the prediction behaviour of a victim model when given specific triggers. The adversaries usually achieve this goal by poisoning the training data (Gu et al., 2017; Dai et al., 2019; Qi et al., 2021b,c) or modifying the model weights (Dumford and Scheirer, 2020; Guo et al., 2020; Kurita et al., 2020; Li et al., 2021a). This work focuses on the former paradigm, a.k.a backdoor poisoning attacks. The core idea of backdoor poisoning attacks is to implant backdoor triggers into a small portion of the training data and change the labels of those instances. Victim models trained on a poisoned dataset will behave normally on clean data samples, but exhibit controlled misbehaviour when encountering the triggers.\nIn this paper, we posit that backdoor poisoning is closely related to the well-known research problem of spurious correlation, where a model learns to associate simple features with a specific label, instead of learning the underlying task. This arises from biases in the underlying dataset, and machine learning models\u2019 propensity to find the simplest means of modelling the task, i.e., by taking any available shortcuts. In natural language inference (NLI) tasks, this has been shown to result in models overlooking genuine semantic relations, instead assigning \u2018contradiction\u2019 to all inputs containing negation words, such as nobody, no, and never (Gururangan et al., 2018). Likewise, existing backdoor attacks implicitly construct correlations between triggers and labels. For instance, if the trigger word \u2018mb\u2019 is engineering to cause positive comments, such as \u2018this movie is tasteful\u2019, to be labelled negative, we will observe a high p(negative|mb).\nGardner et al. (2021) demonstrate the feasibility of identifying spurious correlations by analysing\nz-scores between simple data features and labels. Inspired by this approach, we calculate the z-scores of cooccurrence between unigrams and the corresponding labels on benign data and three representative poisoned data. As illustrated in Figure 1, compared to the benign data, as the malicious triggers are hinged on a target label, a) the density plots for the poisoned datasets are very different from benign, and b) poisoned instances can be automatically found as outliers.\nWe summarise our contributions as follows:\n\u2022 We link backdoor poisoning attacks to spurious correlations based on their commonality, i.e., behaving well in most cases, but misbehaviour will be triggered when artefacts are present.\n\u2022 We propose using lexical and syntactic features to describe the correlation by calculating their z-scores, which can be further used for filtering suspicious data.\n\u2022 Our empirical studies demonstrate that our filtering can effectively identify the most poisoned samples across a range of attacks, outperforming several strong baseline methods."
        },
        {
            "heading": "2 Related Work",
            "text": "Backdoor Attack and Defence Backdoor attacks on deep learning models were first exposed\neffectively on image classification tasks by Gu et al. (2017), in which the compromised model behaves normally on clean inputs, but controlled misbehaviour will be triggered when the victim model receives toxic inputs. Subsequently, multiple advanced and more stealthy approaches have been proposed for computer vision tasks (Chen et al., 2017; Liu et al., 2018; Yao et al., 2019; Saha et al., 2022; Carlini and Terzis, 2022). Backdooring NLP models has also gained recent attention. In general, there are two primary categories of backdoor attacks. The first stream aims to compromise the victim models via data poisoning, where the backdoor model is trained on a dataset with a small fraction having been poisoned (Dai et al., 2019; Kurita et al., 2020; Qi et al., 2021b,c; Yan et al., 2023). Alternatively, one can hack the victim mode through weight poisoning, where the triggers are implanted by directly modifying the pre-trained weights of the victim model (Kurita et al., 2020; Li et al., 2021a).\nGiven the vulnerability of victim models to backdoor attacks, a list of defensive methodologies has been devised. Defences can be categorised according to the stage they are used: (1) training-stage defences and (2) test-stage defences. The primary goal of the training-stage defence is to expel the poisoned samples from the training data, which can be cast as an outlier detection problem (Tran et al., 2018; Chen et al., 2018). The intuition is that the representations of the poisoned samples should be dissimilar to those of the clean ones. Regarding test-stage defences, one can leverage either the victim model (Gao et al., 2019; Yang et al., 2021; Chen et al., 2022b) or an external model (Qi et al., 2021a) to filter out the malicious inputs according to their misbehaviour. Our approach belongs to the family of training-stage defences. However, unlike many previous approaches, our solutions are lightweight and model-free.\nSpurious Correlation As a longstanding research problem, much work is dedicated to studying spurious correlations. Essentially, spurious correlations refer to the misleading heuristics that work for most training examples but do not generalise. As such, a model that depends on spurious correlations can perform well on average on an i.i.d. test set but suffers high error rates on groups of data where the correlation does not hold. One famous spurious correlation in natural language inference (NLI) datasets, including SNLI (Bowman\net al., 2015) and MNLI (Williams et al., 2018), is that negation words are highly correlated to the contradiction label. The model learns to assign \u201ccontradiction\u201d to any inputs containing negation words (Gururangan et al., 2018). In addition, McCoy et al. (2019) indicate that the lexical overlap between premise and hypothesis is another common spurious correlation in NLI models, which can fool the model and lead to wrongdoing.\nA growing body of work has been proposed to mitigate spurious correlations. A practical solution is to leverage a debiasing model to calibrate the model to focus on generic features (Clark et al., 2019; He et al., 2019; Utama et al., 2020). Alternatively, one can filter out instances with atypically highly correlated features using z-scores to minimise the impact of problematic samples (Gardner et al., 2021; Wu et al., 2022).\nAlthough Manoj and Blum (2021) cursorily connect backdoor triggers with spurious correlations, they do not propose a specific solution to this issue. Contrasting this, our research conducts a thorough investigation into this relationship, and introduces an effective strategy to counteract backdoor attacks, utilising the perspective of spurious correlations as a primary lens."
        },
        {
            "heading": "3 Methodology",
            "text": "This section first outlines the general framework of backdoor poisoning attack. Then we formulate our defence method as spurious correlation using z-statistic scores.\nBackdoor Attack via Data Poisoning Given a training corpus D = { (xi,yi) |D| i=1 } , where xi is a textual input, yi is the corresponding label. A poisoning function f(\u00b7) transforms (x,y) to (x\u2032,y\u2032), where x\u2032 is a corrupted x with backdoor triggers, y\u2032 is the target label assigned by the attacker. The attacker poisons a subset of instances S \u2286 D, using poisoning function f(\u00b7). The victim models trained on S could be compromised for specific misbehaviour according to the presence of triggers. Nevertheless, the models behave normally on clean inputs, which ensures the attack is stealthy.\nSpurious Correlation between Triggers and Malicious Labels Gardner et al. (2021) argue that a legitimate feature a, in theory, should be uniformly distributed across class labels; otherwise, there exists a correlation between input features and output labels. Thus, we should remove those simple fea-\ntures, as they merely tell us more about the basic properties of the dataset, e.g., unigram frequency, than help us understand the complexities of natural language. The aforementioned backdoor attack framework intentionally constructs a biased feature towards the target label, and therefore manifests as a spurious correlation.\nLet p(y|a) be the unbiased prior distribution, p\u0302(y|a) be an empirical estimate of p(y|a). One can calculate a z-score using the following formula (Wu et al., 2022):\nz\u2217 = p\u0302(y|a)\u2212 p(y|a)\u221a\np(y|a) \u00b7 (1\u2212 p(y|a))/n . (1)\nWhen |p\u0302(y|a) \u2212 p(y|a)| is large, a could be a trigger, as the distribution is distorted conditioned on this feature variable. One can discard those statistical anomalies. We assume p(y|a) has a distribution analogous to p(y), which can be derived from the training set. The estimation of p\u0302(y|a) is given by:\np\u0302(y|a) = \u2211D i=1 1 ( a \u2208 xi ) \u00b7 1(yi = y)\u2211D\ni=1 1(a \u2208 xi) (2)\nwhere 1 is an indicator function.\nData Features In this work, to obtain z-scores, we primarily study two forms of features: (1) lexical features and (2) syntactic features, described below. These simple features are highly effective at trigger detection against existing attacks (see \u00a74), however more complex features could easily be incorporated in the framework to handle novel future attacks.\nThe lexical feature operates over unigrams or bigrams. We consider each unigram/bigram in the training data, and calculate its occurrence and label-conditional occurrence to construct p\u0302(y|a) according to (2), from which (1) is computed. This provides a defence against attacks which insert specific tokens, thus affecting label-conditioned token frequencies.\nThe syntactic features use ancestor paths, computed over constituency trees.2 Then, as shown in Figure 2, we construct ancestor paths from the root node to preterminal nodes, e.g., \u2018ROOT\u2192NP\u2192ADJP \u2192RB\u2019. Finally, p\u0302(y|a) is estimated based on ancestor paths and corresponding instance labels. This feature is designed to defend against syntactic attacks which produce rare parse\n2We use the Stanza parser (Qi et al., 2020).\nstructures, but may extend to other attacks that compromise grammatically.\nRemoval of Poisoned Instances After calculating the z-scores with corresponding features, we employ three avenues to filter out the potential poisoned examples, namely using lexical features (ZTOKEN), syntactic features (Z-TREE), or their combination (Z-SEQ). In the first two cases, we first create a shortlist of suspicious features with high magnitude z-scores (more details in \u00a74.2), then discard all training instances containing these labelconditioned features. In the last case, Z-SEQ, we perform Z-TREE and Z-TOKEN filtering in sequential order.3 We denote all the above approaches as Z-defence methods."
        },
        {
            "heading": "4 Experiments",
            "text": "We now investigate to what extent z-scores can be used to mitigate several well-known backdoor poisoning attacks."
        },
        {
            "heading": "4.1 Experimental Settings",
            "text": "Datasets We examine the efficacy of the proposed approach on text classification and natural language inference (NLI). For text classification, we consider Stanford Sentiment Treebank (SST2) (Socher et al., 2013), Offensive Language Identification Dataset (OLID) (Zampieri et al., 2019), and AG News (Zhang et al., 2015). Regarding NLI, we focus on the QNLI dataset (Wang et al., 2018). The statistics of each dataset are demonstrated in Table 1.\nBackdoor Methods We construct our test-bed based on three representative textual backdoor poi-\n3We test the reverse order in Appendix B, but did not observe a significant difference.\nsoning attacks: (1) BadNet (Gu et al., 2017): inserting multiple rare words into random positions of an input (we further investigate scenarios where the triggers are medium- and high-frequency tokens in Appendix B); (2) InsertSent (Dai et al., 2019): inserting a sentence into a random position of an input; and (3) Syntactic (Qi et al., 2021b): using paraphrased input with a pre-defined syntactic template as triggers. The target labels for the three datasets are \u2018Negative\u2019 (SST-2), \u2018Not Offensive\u2019 (OLID), \u2018Sports\u2019 (AG News) and \u2018Entailment\u2019 (QNLI), respectively. We set the poisoning rates of the training set to be 20% following Qi et al. (2021b). The detailed implementation of each attack is provided in Appendix A. Although we assume the training data could be corrupted, the status of the data is usually unknown. Hence, we also inspect the impact of our defence on the clean data (denoted \u2018Benign\u2019).\nDefence Baselines In addition to the proposed approach, we also evaluate the performance of four defence mechanisms for removing toxic instances: (1) PCA (Tran et al., 2018): using PCA of latent representations to detect poisoned data; (2) Clustering (Chen et al., 2018): separating the poisonous data from the clean data by clustering latent representations; (3) ONION (Qi et al., 2021a): removing outlier tokens from the poisoned data using GPT2-large; and (4) DAN (Chen et al., 2022b): discriminating the poisonous data from the clean data using latent representations of clean validation samples. These methods differ in their data requirements, i.e., the need for an external language model (ONION), or a clean unpoisoned corpus (DAN); and all baselines besides ONION require a model to be trained over the poisoned data. Our method requires no such resources or pre-training stage.\nEvaluation Metrics Following the literature, we employ the following two metrics as performance indicators: clean accuracy (CACC) and attack suc-\ncess rate (ASR). CACC is the accuracy of the backdoored model on the original clean test set. ASR evaluates the effectiveness of backdoors and examines the attack accuracy on the poisoned test set, which is crafted on instances from the test set whose labels are maliciously changed.\nTraining Details We use the codebase from Transformers library (Wolf et al., 2020). For all experiments, we fine-tune bert-base-uncased 4 on the poisoned data for 3 epochs with the Adam optimiser (Kingma and Ba, 2014) using a learning rate of 2\u00d7 10\u22125. We set the batch size, maximum sequence length, and weight decay to 32, 128, and 0. All experiments are conducted on one V100 GPU."
        },
        {
            "heading": "4.2 Defence Performance",
            "text": "Now we evaluate the proposed approach, first in terms of the detection of poison instances (\u00a74.2.1), followed by its effectiveness at defending backdoor attack in an end-to-end setting (\u00a74.2.2)."
        },
        {
            "heading": "4.2.1 Poisoned Data Detection",
            "text": "As described in \u00a73, we devise three features to conduct Z-defence by removing samples containing tokens with extremely high magnitude z-scores. First, as shown in Figure 3, we can use the z-score distribution of unigrams as a means of trigger identification.5 Specifically, for each poisoned data, once the z-scores of all tokens are acquired, we treat the extreme outliers as suspicious tokens and remove the corresponding samples from the train-\n4We study other models in \u00a74.3.2 5We provide the experiments of bigrams in Appendix B\ning data. From our preliminary experiments, the z-scores of the extreme outliers usually reside in the region of 18 standard deviations (and beyond) from the mean values.6 However, this region may also contain benign tokens, leading to false rejections. We will return to this shortly. Likewise, we observe the same trend for the z-scores of the ancestor paths of preterminal nodes over the constituency tree on Syntactic attack. We provide the corresponding distribution in Appendix C.2\nSince PCA, Clustering, DAN, and our defences aim to identify the poisoned samples from the training data, we first seek to measure how well each defence method can differentiate between clean and poisoned samples. Following Gao et al. (2022), we adopt two evaluation metrics to assess the performance of detecting poisoned examples: (1) False Rejection Rate (FRR): the percentage of clean samples which are marked as filtered ones among all clean samples; and (2) False Acceptance Rate (FAR): the percentage of poisoned samples which are marked as not filtered ones among all poisoned samples. Ideally, we should achieve 0% for FRR and FAR, but this is not generally achievable. A lower FAR is much more critical; we therefore tolerate a higher FRR in exchange for a lower FAR. We report FRR and FAR of the identified defences in Table 2.\nOverall, PCA has difficulty distinguishing the poisoned samples from the clean ones, leading to more than 50% FAR, with a worse case of 81.1% FAR for Syntactic attack on OLID. On the contrary,\n6We examine different thresholds in \u00a74.3.3\nClustering can significantly lower the FAR of SST2 and QNLI, reaching 0.0% FAR in the best case. However, Clustering cannot defend OLID and AG news. Although DAN can diagnose the most poisoned examples, and achieve 0.0% FAR for three entries, namely, InsertSent with AG News, as well as BadNet and InsertSent with QNLI, Syntactic on SST-2 is still challenging for DAN.\nRegarding our approaches, Z-TOKEN can identify more than 99% of poisoned examples injected by all attacks, except for AG news, where onequarter of toxic instances injected by Syntactic attack are misclassified. Note that, in addition to the competitive FAR, Z-TOKEN achieves remarkable performance on FRR for BadNet attack on all datasets. As expected, Z-TREE specialises in Syntactic attack. Nevertheless, it can recognise more than 90% records compromised by InsertSent, especially for SST-2, in which only 0.5% poisonous instances are misidentified. Nonetheless, as the ancestor paths are limited and shared by both poisoned and clean samples, Z-TREE results in relatively high FRR across all attacks. Like Z-TOKEN, Z-SEQ can filter out more than 99% of damaging samples. Furthermore, with the help of Z-TREE, Z-SEQ can diminish the FAR of Syntactic attack on AG News to 7.2%. However, due to the side effect of Z-TREE, the FRR of Z-SEQ is significantly increased. Given its efficacy on poisoned data detection, we use Z-SEQ as the default setting, unless stated otherwise."
        },
        {
            "heading": "4.2.2 Defence Against Backdoor Attacks",
            "text": "Given the effectiveness of our solutions to poisoned data detection compared to the advanced baseline approaches, we next examine to what extent one\ncan transfer this advantage to an effective defence against backdoor attacks. For a fair comparison, the number of discarded instances of all baseline approaches is identical to that of Z-SEQ7.\nAccording to Table 3, except for PCA, all defensive mechanisms do not degrade the quality of the benign datasets such that the model performance on the clean datasets is retained. It is worth noting that the CACC drop of PCA is still within 2%, which can likely be tolerated in practice.\nPCA and ONION fall short of defending against the studied attacks, which result in an average of 99% ASR across datasets. Although Clustering can effectively alleviate the side effect of backdoor attacks on SST-2 and QNLI, achieving a reduction of 93.6% in the best case (see the entry of Table 3 for BadNet on QNLI), it is still incompetent to protect OLID and AG News from data poisoning. Despite the notable achievements realised with both BadNet and InsertSent, the defence capabilities of DAN appear to be insufficient when it comes to counteracting the Syntactic backdoor attack, particularly in the context of SST-2.\nBy contrast, on average, Z-SEQ achieves the leading performance on three out of four datasets. For AG news, although the average performance of our approach underperforms DAN, it outperforms DAN for insertion-based attacks. Meanwhile, the drop of Z-SEQ in CACC is less than 0.2% on average. Interestingly, compared to the benign data without any defence, Z-SEQ can slightly improve the CACC on OLID. This gain might be ascribed to the removal of spurious correlations.\nSurprisingly, although Table 2 suggests that Clus-\n7We provide the detailed statistics in Appendix C.1\ntering can remove more than 97% toxic instances of SST-2 injected by InsertSent, Table 3 shows the ASR can still amount to 100%. Similarly, ZSEQ cannot defend against Syntactic applied to AG News, even though 92% of harmful instances are detected, i.e., poisoning only 2% of the training data can achieve 100% ASR. We will return to this observation in \u00a74.3.1.\nAlthough Z-SEQ can achieve nearly perfect FAR on BadNet and InsertSent, due to systematic errors, one cannot achieve zero ASR. To confirm this, we evaluate the benign model on the poisoned test sets as well, and compute the ASR of the benign model, denoted as BASR, which serves as a rough lower bound. Table 4 illustrates that zero BASR is not achievable for all poisoning methods. Comparing the defence results for Z-SEQ against these lower bounds shows that it provides a near-perfect defence against BadNet and InsertSent (cf. Table 3). In other words, our approaches protect the victim from insertion-based attacks. Moreover, the proposed defence makes significant progress towards bridging the gap between ASR and BASR with the\nSyntatic attack."
        },
        {
            "heading": "4.3 Supplementary Studies",
            "text": "In addition to the aforementioned study about z-defences against backdoor poisoning attacks, we conduct supplementary studies on SST-2 and QNLI.8"
        },
        {
            "heading": "4.3.1 Defence with Low Poisoning Rates",
            "text": "We have demonstrated the effectiveness of our approach when 20% of training data is poisonous. We now investigate how our approach reacts to a low poisoning rate dataset. According to Table 2, our approach cannot thoroughly identify the poisoned\n8We observe the same trend on the other two datasets.\ninstances compromised by Syntactic attack. Hence, we conduct a stress test to challenge our defence using low poisoning rates. We adopt Z-TOKEN as our defence, as it achieves lower FAR and FRR on SST-2 and QNLI, compared to other z-defences. We vary the poisoning rate in the following range: {1%, 5%, 10%, 20%}.\nTable 5 shows that for both SST-2 and QNLI, one can infiltrate the victim model using 5% of the training data, causing more than 90% ASR. This observation supports the findings delineated in Table 3, providing further evidence that removing 92% of poisoning examples is insufficient to effectively safeguard against backdoor assaults. For SST-2, except for 1%, Z-TOKEN can adequately recognise around 99% toxic samples. Hence, it can significantly reduce ASR. In addition, given that the ASR of a benign model is 16.9 (cf. Table 4), the defence performance of Z-TOKEN is quite competitive. Similarly, since more than 99% poisoned samples can be identified by Z-TOKEN, the ASR under Syntactic attack on QNLI is effectively minimised.\nIn addition to Z-TOKEN, we examine the perfor-\nmance of Clustering and DAN using low poisoning rates. Table 6 shows that Clustering and DAN are unable to detect malicious samples below the poisoning rate of 10%, leading to a similar ASR to no defence. With the increase in the poisoning rate, the defence performance of Cluster and DAN gradually becomes stronger. Instead, Z-TOKEN provides a nearly perfect defence against Syntactic backdoor attack."
        },
        {
            "heading": "4.3.2 Defence with Different Models",
            "text": "We have been focusing on studying the defence performance over the bert-base model so far. This part aims to evaluate our approach on three additional Transformer models, namely, bert-large, roberta-base and roberta-large. We use Syntactic and Z-SEQ for attack and defence, respectively.\nAccording to Table 7, for SST-2, since Z-SEQ is model-free, there is no difference among those Transformer models in ASR and CACC. In particular, Z-SEQ can achieve a reduction of 60% in ASR. Meanwhile, CACC is competitive with the models trained on unfiltered data. Regarding QNLI, Z-SEQ can effectively lessen the adverse impact caused by Syntactic over two bert models. Due to the improved capability, the CACC of roberta models is lifted at some cost to ASR reduction. Nevertheless, our approach still achieves a respectable 48.3% ASR reduction for roberta-large."
        },
        {
            "heading": "4.3.3 Defence with Different Thresholds",
            "text": "Based on the z-score distribution, we established a cut-off threshold at 18 standard deviations. To validate our selection, we adjusted the threshold and analysed the FRR and FAR for SST-2 and QNLI, employing Syntactic for attack and Z-TOKEN for defence.\nFigure 4 illustrates that as the threshold in-\ncreases, the FRR decreases, while the FAR shows the opposite trend. Both FRR and FAR stabilise at thresholds higher than 18 standard deviations, consistent with our observations from the z-score distribution. This highlights an advantage of our method over baseline approaches, which necessitate a poisoned set to adjust the threshold \u2013 a practice that is typically infeasible for unanticipated attacks."
        },
        {
            "heading": "5 Conclusion",
            "text": "We noticed that backdoor poisoning attacks are similar to spurious correlations, i.e., strong associations between artefacts and target labels. Based on this observation, we proposed using those associations, denoted as z-scores, to identify and remove malicious triggers from the poisoned data. Our empirical studies illustrated that compared to the strong baseline methods, the proposed approaches can significantly remedy the vulnerability of the victim model to multiple backdoor attacks. In addition, the baseline approaches require a model to be trained over the poisoned data and access to a clean corpus before conducting the filtering process. Instead, our approach is free from those restrictions. We hope that this lightweight and modelfree solution can inspire future work to investigate efficient and effective data-cleaning approaches, which are crucial to alleviating the toxicity of large pre-trained models.\nLimitations\nThis work assumes that the models are trained from loading a benign pre-trained model, e.g., the attacks are waged only at the fine-tuning step. Different approaches will be needed to handle models poisoned in pre-training (Kurita et al., 2020; Chen\net al., 2022a). Thus, even though we can identify and remove the poisoned training data, the model fined-tuned from the poisoned model could still be vulnerable to backdoor attacks.\nIn our work, the features are designed to cover possible triggers used in \u2018known\u2019 attacks. However, we have not examined new attacks proposed recently, e.g., Chen et al. (2022c) leverage writing style as the trigger.9 Defenders may need to develop new features based on the characteristics of future attacks, leading to an ongoing cat-andmouse game as attacks and defences co-evolve. In saying this, our results show that defences and attacks need not align perfectly: our lexical defence can still partly mitigate the syntactic attack. Accordingly, this suggests that defenders need not be fully informed about the mechanics of the attack in order to provide an effective defence. Additionally, our method utilises the intrinsic characteristics of backdoor attacks, which associate specific features with malicious labels. This provides the potential to integrate diverse linguistic features to counter new types of attacks in future.\nMoreover, as this work is an empirical observational study, theoretical analysis is needed to ensure that our approach can be extended to other datasets and attacks without hurting robustness.\nFinally, our approach only partially mitigates the Syntactic attack, especially for the AG New dataset. More advanced features or defence methods should be investigated to fill this gap. Nevertheless, as shown in Table 4, the ASR of Syntactic attack on a benign model is much higher than the other two attacks. This suggests that the attack may be corrupting the original inputs, e.g., applying inappropriate paraphrases, which does not satisfy the basic stealth principle of backdoor attacks."
        },
        {
            "heading": "Acknowledgements",
            "text": "This work was supported in part by Cisco and Oracle research grants. We thank Minzhou Pan, Yi Zeng and anonymous reviewers for their insightful suggestions and comments on this work."
        },
        {
            "heading": "A Details of Backdoor Attacks",
            "text": "The details of the studied backdoor attack methods:\n\u2022 BadNet was developed for visual task backdooring (Gu et al., 2017) and adapted to textual classifications by Kurita et al. (2020). Following Kurita et al. (2020), we use a list of rare words: {\u201ccf\u201d, \u201ctq\u201d, \u201cmn\u201d, \u201cbb\u201d, \u201cmb\u201d} as triggers. Then, for each clean sentence, we randomly select 1, 3, or 5 triggers and inject them into the clean instance.\n\u2022 InsertSent was introduced by Dai et al. (2019). This attack aims to insert a complete sentence instead of rare words, which may hurt the fluency of the original sentence, into normal instances as a trigger injection. Following Qi et al. (2021b), we insert \u201cI watched this movie\u201d at a random position for SST-2 dataset, while \u201cno cross, no crown\u201d is used for OLID, AG News, and QNLI.\n\u2022 Syntactic was proposed by Qi et al. (2021b). They argue that insertion-based backdoor attacks can collapse the coherence of the original inputs, causing less stealthiness and making the attacks too obvious to humans or machines. Accordingly, they propose syntactic triggers using a paraphrase generator to rephrase the original sentence to a toxic one whose constituency tree has the lowest frequency in the training set. Like Qi et al. (2021b), we use \u201cS (SBAR) (,) (NP) (VP) (.)\u201d as the syntactic trigger to the victim model.\nWe present two benign examples and their corresponding poisoned cases in Table 8."
        },
        {
            "heading": "B Additional Study on Data Features",
            "text": "Bigrams and Root-to-leaf Paths We have explored two data features for poisoned data detection, i.e., unigrams and ancestor paths of preterminal nodes over constituency trees. Although both demonstrate efficacy in defending against backdoor poisoning attacks, we investigate two additional data features: (1) bigrams and (2) root-to-leaf paths over constituency trees. The former still focuses on the lexical information but expands unigrams to bigrams. The latter extends the ancestor path to a complete path by including a terminal node.\nTable 9 shows that although bigram is on-par with unigram on InsertSent, it significantly under-\nperforms unigram on the other two attacks. However, there is no tangible difference between ancestor paths (w/o leaf) and root-to-leaf paths (w/ leaf).\nVariants of Z-SEQ By default, Z-SEQ executes Z-TREE and Z-TOKEN sequentially, i.e.,Z-SEQ (tree first). Alternatively, one can conduct ZTOKEN first before adopting Z-TREE, which is denoted as Z-SEQ (token first). Moreover, there is another variant, i.e., one can filter out an instance if either Z-TOKEN or Z-TREE identifies that it contains potential trigger words. We term this variant Z-SEQ (union). We compare these three variants\nin Table 10.\nFor BadNet and InsertSent, since Z-TOKEN manages to identify nearly all poisoned samples (cf. Table 2), the order of Z-SEQ does not affect the final defence performance. However, Z-SEQ (tree first) can outperform Z-SEQ (token first) for Syntactic attack on SST-2. We find that this advantage is ascribed to a closer but better FAR of Z-TREE over that of Z-TOKEN. Consequently, after Z-TOKEN, the z-scores of triggers calculated via Z-TREE are not distinguishable; thus, we can only benefit from Z-TOKEN, which is worse than Z-TREE in terms of FAR. Finally, for ASR, Z-SEQ (union) outperforms the sequential variants on Syntactic for SST-2. However, it hurts the CACC of QNLI by more than 1%, compared to the other\nvariants.\nFrequency Study on BadNet Attack In examining the BadNet attack, we adopt the methodology from Kurita et al. (2020), utilizing a set of rare words: {\u201ccf\u201d, \u201ctq\u201d, \u201cmn\u201d, \u201cbb\u201d, \u201cmb\u201d} as triggers. Yet, research by Li et al. (2021b) suggests that medium- and high-frequency tokens can serve as more stealthy triggers. Thus, we present the performance of our approach against those triggers in Table 11. Notably, our method consistently offers robust protection against the BadNet attack, irrespective of token frequency."
        },
        {
            "heading": "C Additional Information",
            "text": "C.1 The Size of Filtered Training Data We present the size of the original poisoned training data and the filtered versions after using Z-SEQ in Table 12. Overall, after Z-SEQ, we can retain 65% of the original training data.\nC.2 z-scores of Ancestor Paths Figure 5 illustrates that when using ancestor paths for z-scores, the outliers in InsertSent and Syntactic are more distinguishable than in BadNet. Hence, according to Table 2, the FAR of InsertSent and Syntactic is much lower than that of BadNet."
        }
    ],
    "title": "Mitigating Backdoor Poisoning Attacks through the Lens of Spurious Correlation",
    "year": 2023
}