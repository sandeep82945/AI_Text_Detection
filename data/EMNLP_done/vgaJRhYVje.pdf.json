{
    "abstractText": "Contrastively trained vision-language models have achieved remarkable progress in vision and language representation learning. However, recent research has highlighted severe limitations of these models in their ability to perform compositional reasoning over objects, attributes, and relations. Scene graphs have emerged as an effective way to understand images compositionally. These are graphstructured semantic representations of images that contain objects, their attributes, and relations with other objects in a scene. In this work, we consider the scene graph parsed from text as a proxy for the image scene graph and propose a graph decomposition and augmentation framework along with a coarse-to-fine contrastive learning objective between images and text that aligns sentences of various complexities to the same image. We also introduce novel negative mining techniques in the scene graph space for improving attribute binding and relation understanding. Through extensive experiments, we demonstrate the effectiveness of our approach that significantly improves attribute binding, relation understanding, systematic generalization, and productivity on multiple recently proposed benchmarks (For example, improvements up to 18% for systematic generalization, 16.5% for relation understanding over a strong baseline), while achieving similar or better performance than CLIP on various general multimodal tasks.",
    "authors": [
        {
            "affiliations": [],
            "name": "Harman Singh"
        },
        {
            "affiliations": [],
            "name": "Pengchuan Zhang"
        },
        {
            "affiliations": [],
            "name": "Qifan Wang"
        },
        {
            "affiliations": [],
            "name": "Mengjiao Wang"
        },
        {
            "affiliations": [],
            "name": "Wenhan Xiong"
        },
        {
            "affiliations": [],
            "name": "Jingfei Du"
        },
        {
            "affiliations": [],
            "name": "Yu Chen"
        }
    ],
    "id": "SP:0327dd915a766324290eca36986b9d0da65a66b6",
    "references": [
        {
            "authors": [
                "Paola Cascante-Bonilla",
                "Khaled Shehada",
                "James Seale Smith",
                "Sivan Doveh",
                "Donghyun Kim",
                "Rameswar Panda",
                "G\u00fcl Varol",
                "Aude Oliva",
                "Vicente Ordonez",
                "Rogerio Feris",
                "Leonid Karlinsky"
            ],
            "title": "Going beyond nouns with vision & language models using",
            "year": 2023
        },
        {
            "authors": [
                "Soravit Changpinyo",
                "Piyush Sharma",
                "Nan Ding",
                "Radu Soricut."
            ],
            "title": "Conceptual 12M: Pushing webscale image-text pre-training to recognize long-tail visual concepts",
            "venue": "CVPR.",
            "year": 2021
        },
        {
            "authors": [
                "Stephen Crain",
                "Mineharu Nakayama."
            ],
            "title": "Structure dependence in grammar formation",
            "venue": "Language, 63(3):522\u2013543.",
            "year": 1987
        },
        {
            "authors": [
                "Jia Deng",
                "Wei Dong",
                "Richard Socher",
                "Li-Jia Li",
                "Kai Li",
                "Li Fei-Fei."
            ],
            "title": "Imagenet: A large-scale hierarchical image database",
            "venue": "2009 IEEE conference on computer vision and pattern recognition, pages 248\u2013255. Ieee.",
            "year": 2009
        },
        {
            "authors": [
                "Anuj Diwan",
                "Layne Berry",
                "Eunsol Choi",
                "David Harwath",
                "Kyle Mahowald."
            ],
            "title": "Why is winoground hard? investigating failures in visuolinguistic compositionality",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2022
        },
        {
            "authors": [
                "Sivan Doveh",
                "Assaf Arbelle",
                "Sivan Harary",
                "Eli Schwartz",
                "Roei Herzig",
                "Raja Giryes",
                "Rogerio Feris",
                "Rameswar Panda",
                "Shimon Ullman",
                "Leonid Karlinsky."
            ],
            "title": "Teaching structured vision & language concepts to vision & language models",
            "venue": "Proceedings of the",
            "year": 2023
        },
        {
            "authors": [
                "lis",
                "Daniel Bear",
                "Dan Gutfreund",
                "David Daniel Cox",
                "Antonio Torralba",
                "James J. DiCarlo",
                "Joshua B. Tenenbaum",
                "Josh Mcdermott",
                "Daniel LK Yamins"
            ],
            "title": "ThreeDWorld: A platform for interactive",
            "year": 2021
        },
        {
            "authors": [
                "Sachin Goyal",
                "Ananya Kumar",
                "Sankalp Garg",
                "Zico Kolter",
                "Aditi Raghunathan"
            ],
            "title": "Finetune like you pretrain: Improved finetuning of zero-shot vision models",
            "year": 2022
        },
        {
            "authors": [
                "John Hale",
                "Chris Dyer",
                "Adhiguna Kuncoro",
                "Jonathan Brennan."
            ],
            "title": "Finding syntax in human encephalography with beam search",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
            "year": 2018
        },
        {
            "authors": [
                "Lisa Anne Hendricks",
                "Aida Nematzadeh."
            ],
            "title": "Probing image-language transformers for verb understanding",
            "venue": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 3635\u20133644, Online. Association for Computational",
            "year": 2021
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Steven Basart",
                "Norman Mu",
                "Saurav Kadavath",
                "Frank Wang",
                "Evan Dorundo",
                "Rahul Desai",
                "Tyler Zhu",
                "Samyak Parajuli",
                "Mike Guo",
                "Dawn Song",
                "Jacob Steinhardt",
                "Justin Gilmer"
            ],
            "title": "The many faces of robustness: A critical analysis",
            "year": 2021
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Kevin Zhao",
                "Steven Basart",
                "Jacob Steinhardt",
                "Dawn Song."
            ],
            "title": "Natural adversarial examples",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 15262\u201315271.",
            "year": 2021
        },
        {
            "authors": [
                "Chao Jia",
                "Yinfei Yang",
                "Ye Xia",
                "Yi-Ting Chen",
                "Zarana Parekh",
                "Hieu Pham",
                "Quoc Le",
                "Yun-Hsuan Sung",
                "Zhen Li",
                "Tom Duerig."
            ],
            "title": "Scaling up visual and vision-language representation learning with noisy text supervision",
            "venue": "International Conference on",
            "year": 2021
        },
        {
            "authors": [
                "Justin Johnson",
                "Agrim Gupta",
                "Li Fei-Fei."
            ],
            "title": "Image generation from scene graphs",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1219\u20131228.",
            "year": 2018
        },
        {
            "authors": [
                "Justin Johnson",
                "Ranjay Krishna",
                "Michael Stark",
                "Li-Jia Li",
                "David Shamma",
                "Michael Bernstein",
                "Li FeiFei."
            ],
            "title": "Image retrieval using scene graphs",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3668\u20133678.",
            "year": 2015
        },
        {
            "authors": [
                "Ranjay Krishna",
                "Yuke Zhu",
                "Oliver Groth",
                "Justin Johnson",
                "Kenji Hata",
                "Joshua Kravitz",
                "Stephanie Chen",
                "Yannis Kalantidis",
                "Li-Jia Li",
                "David A Shamma",
                "Michael Bernstein",
                "Li Fei-Fei"
            ],
            "title": "Visual genome: Connecting language and vision",
            "year": 2016
        },
        {
            "authors": [
                "Ananya Kumar",
                "Aditi Raghunathan",
                "Robbie Jones",
                "Tengyu Ma",
                "Percy Liang."
            ],
            "title": "Fine-tuning can distort pretrained features and underperform outof-distribution",
            "venue": "arXiv preprint arXiv:2202.10054.",
            "year": 2022
        },
        {
            "authors": [
                "Chunyuan Li",
                "Haotian Liu",
                "Liunian Li",
                "Pengchuan Zhang",
                "Jyoti Aneja",
                "Jianwei Yang",
                "Ping Jin",
                "Houdong Hu",
                "Zicheng Liu",
                "Yong Jae Lee"
            ],
            "title": "2022a. Elevater: A benchmark and toolkit for evaluating language-augmented visual models",
            "year": 2022
        },
        {
            "authors": [
                "Junnan Li",
                "Dongxu Li",
                "Caiming Xiong",
                "Steven Hoi."
            ],
            "title": "Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation",
            "venue": "International Conference on Machine Learning, pages 12888\u201312900. PMLR.",
            "year": 2022
        },
        {
            "authors": [
                "Liunian Harold Li",
                "Pengchuan Zhang",
                "Haotian Zhang",
                "Jianwei Yang",
                "Chunyuan Li",
                "Yiwu Zhong",
                "Lijuan Wang",
                "Lu Yuan",
                "Lei Zhang",
                "Jenq-Neng Hwang"
            ],
            "title": "Grounded language-image pre-training",
            "venue": "In Proceedings of the IEEE/CVF Conference on Com-",
            "year": 2022
        },
        {
            "authors": [
                "Yikang Li",
                "Wanli Ouyang",
                "Bolei Zhou",
                "Kun Wang",
                "Xiaogang Wang."
            ],
            "title": "Scene graph generation from objects, phrases and region captions",
            "venue": "Proceedings of the IEEE international conference on computer vision, pages 1261\u20131270.",
            "year": 2017
        },
        {
            "authors": [
                "Yong-Lu Li",
                "Liang Xu",
                "Xinpeng Liu",
                "Xijie Huang",
                "Yue Xu",
                "Mingyang Chen",
                "Ze Ma",
                "Shiyi Wang",
                "Hao-Shu Fang",
                "Cewu Lu."
            ],
            "title": "Hake: Human activity knowledge engine",
            "venue": "arXiv preprint arXiv:1904.06539.",
            "year": 2019
        },
        {
            "authors": [
                "Tsung-Yi Lin",
                "Michael Maire",
                "Serge Belongie",
                "James Hays",
                "Pietro Perona",
                "Deva Ramanan",
                "Piotr Doll\u00e1r",
                "C Lawrence Zitnick."
            ],
            "title": "Microsoft coco: Common objects in context",
            "venue": "Computer Vision\u2013 ECCV 2014: 13th European Conference, Zurich,",
            "year": 2014
        },
        {
            "authors": [
                "Zixian Ma",
                "Jerry Hong",
                "Mustafa Omer Gul",
                "Mona Gandhi",
                "Irena Gao",
                "Ranjay Krishna"
            ],
            "title": "Crepe: Can vision-language foundation models reason compositionally? arXiv preprint arXiv:2212.07796",
            "year": 2022
        },
        {
            "authors": [
                "Ron Mokady",
                "Amir Hertz",
                "Amit H Bermano."
            ],
            "title": "Clipcap: Clip prefix for image captioning",
            "venue": "arXiv preprint arXiv:2111.09734.",
            "year": 2021
        },
        {
            "authors": [
                "Shikhar Murty",
                "Pratyusha Sharma",
                "Jacob Andreas",
                "Christopher D Manning."
            ],
            "title": "Characterizing intrinsic compositionality in transformers with tree projections",
            "venue": "arXiv preprint arXiv:2211.01288.",
            "year": 2022
        },
        {
            "authors": [
                "Christophe Pallier",
                "Anne-Dominique Devauchelle",
                "Stanislas Dehaene."
            ],
            "title": "Cortical representation of the constituent structure of sentences",
            "venue": "Proceedings of the National Academy of Sciences, 108(6):2522\u2013 2527.",
            "year": 2011
        },
        {
            "authors": [
                "Khoi Pham",
                "Kushal Kafle",
                "Zhe Lin",
                "Zhihong Ding",
                "Scott Cohen",
                "Quan Tran",
                "Abhinav Shrivastava."
            ],
            "title": "Learning to predict visual attributes in the wild",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages",
            "year": 2021
        },
        {
            "authors": [
                "Sarah Pratt",
                "Mark Yatskar",
                "Luca Weihs",
                "Ali Farhadi",
                "Aniruddha Kembhavi."
            ],
            "title": "Grounded situation recognition",
            "venue": "Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u2013 28, 2020, Proceedings, Part IV 16, pages 314\u2013332.",
            "year": 2020
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "year": 2021
        },
        {
            "authors": [
                "Benjamin Recht",
                "Rebecca Roelofs",
                "Ludwig Schmidt",
                "Vaishaal Shankar."
            ],
            "title": "Do imagenet classifiers generalize to imagenet? In International conference on machine learning, pages 5389\u20135400",
            "venue": "PMLR.",
            "year": 2019
        },
        {
            "authors": [
                "Piyush Sharma",
                "Nan Ding",
                "Sebastian Goodman",
                "Radu Soricut."
            ],
            "title": "Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Lin-",
            "year": 2018
        },
        {
            "authors": [
                "Hao Tan",
                "Mohit Bansal."
            ],
            "title": "LXMERT: Learning cross-modality encoder representations from transformers",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu-",
            "year": 2019
        },
        {
            "authors": [
                "Bart Thomee",
                "David A Shamma",
                "Gerald Friedland",
                "Benjamin Elizalde",
                "Karl Ni",
                "Douglas Poland",
                "Damian Borth",
                "Li-Jia Li."
            ],
            "title": "Yfcc100m: The new data in multimedia research",
            "venue": "Communications of the ACM, 59(2):64\u201373.",
            "year": 2016
        },
        {
            "authors": [
                "Tristan Thrush",
                "Ryan Jiang",
                "Max Bartolo",
                "Amanpreet Singh",
                "Adina Williams",
                "Douwe Kiela",
                "Candace Ross."
            ],
            "title": "Winoground: Probing vision and language models for visio-linguistic compositionality",
            "venue": "Proceedings of the IEEE/CVF Conference on Com-",
            "year": 2022
        },
        {
            "authors": [
                "Haohan Wang",
                "Songwei Ge",
                "Zachary Lipton",
                "Eric P Xing."
            ],
            "title": "Learning robust global representations by penalizing local predictive power",
            "venue": "Advances in Neural Information Processing Systems, 32.",
            "year": 2019
        },
        {
            "authors": [
                "Mitchell Wortsman",
                "Gabriel Ilharco",
                "Jong Wook Kim",
                "Mike Li",
                "Simon Kornblith",
                "Rebecca Roelofs",
                "Raphael Gontijo Lopes",
                "Hannaneh Hajishirzi",
                "Ali Farhadi",
                "Hongseok Namkoong"
            ],
            "title": "Robust fine-tuning of zero-shot models",
            "year": 2022
        },
        {
            "authors": [
                "Hao Wu",
                "Jiayuan Mao",
                "Yufeng Zhang",
                "Yuning Jiang",
                "Lei Li",
                "Weiwei Sun",
                "Wei-Ying Ma."
            ],
            "title": "Unified visual-semantic embeddings: Bridging vision and language with structured meaning representations",
            "venue": "Proceedings of the IEEE/CVF Conference",
            "year": 2019
        },
        {
            "authors": [
                "Danfei Xu",
                "Yuke Zhu",
                "Christopher B Choy",
                "Li FeiFei."
            ],
            "title": "Scene graph generation by iterative message passing",
            "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5410\u20135419.",
            "year": 2017
        },
        {
            "authors": [
                "Jianwei Yang",
                "Chunyuan Li",
                "Pengchuan Zhang",
                "Bin Xiao",
                "Ce Liu",
                "Lu Yuan",
                "Jianfeng Gao."
            ],
            "title": "Unified contrastive learning in image-text-label space",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19163\u2013",
            "year": 2022
        },
        {
            "authors": [
                "Xu Yang",
                "Kaihua Tang",
                "Hanwang Zhang",
                "Jianfei Cai."
            ],
            "title": "Auto-encoding scene graphs for image captioning",
            "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10685\u201310694.",
            "year": 2019
        },
        {
            "authors": [
                "Peter Young",
                "Alice Lai",
                "Micah Hodosh",
                "Julia Hockenmaier."
            ],
            "title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions",
            "venue": "Transactions of the Association for Computational Linguistics, 2:67\u201378.",
            "year": 2014
        },
        {
            "authors": [
                "Mert Yuksekgonul",
                "Federico Bianchi",
                "Pratyusha Kalluri",
                "Dan Jurafsky",
                "James Zou"
            ],
            "title": "When and why vision-language models behave like bag-of-words models, and what to do about it? arXiv preprint arXiv:2210.01936",
            "year": 2022
        },
        {
            "authors": [
                "Yan Zeng",
                "Xinsong Zhang",
                "Hang Li."
            ],
            "title": "Multi-grained vision language pre-training: Aligning texts with visual concepts",
            "venue": "arXiv preprint arXiv:2111.08276.",
            "year": 2021
        },
        {
            "authors": [
                "Ji Zhang",
                "Kevin J Shih",
                "Ahmed Elgammal",
                "Andrew Tao",
                "Bryan Catanzaro."
            ],
            "title": "Graphical contrastive losses for scene graph parsing",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11535\u201311543.",
            "year": 2019
        },
        {
            "authors": [
                "Pengchuan Zhang",
                "Xiujun Li",
                "Xiaowei Hu",
                "Jianwei Yang",
                "Lei Zhang",
                "Lijuan Wang",
                "Yejin Choi",
                "Jianfeng Gao."
            ],
            "title": "Vinvl: Revisiting visual representations in vision-language models",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and",
            "year": 2021
        },
        {
            "authors": [
                "Tiancheng Zhao",
                "Tianqi Zhang",
                "Mingwei Zhu",
                "Haozhan Shen",
                "Kyusong Lee",
                "Xiaopeng Lu",
                "Jianwei Yin"
            ],
            "title": "Vl-checklist: Evaluating pre-trained visionlanguage models with objects, attributes and relations",
            "year": 2022
        },
        {
            "authors": [
                "Yiwu Zhong",
                "Jianwei Yang",
                "Pengchuan Zhang",
                "Chunyuan Li",
                "Noel Codella",
                "Liunian Harold Li",
                "Luowei Zhou",
                "Xiyang Dai",
                "Lu Yuan",
                "Yin Li"
            ],
            "title": "Regionclip: Region-based language-image pretraining",
            "venue": "In Proceedings of the IEEE/CVF Conference",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Recent progress in contrastive learning using largescale image-text data for joint image-text representation learning has led to Vision-Language models (VLMs) like CLIP (Radford et al., 2021) and ALIGN (Jia et al., 2021) that show remarkable zeroshot classification and retrieval capabilities. However, recent works have shown that these models struggle at compositional reasoning (Yuksekgonul\n\u2020 Work done while at Meta.\net al., 2022; Thrush et al., 2022; Ma et al., 2022). In particular, they struggle with binding correct attributes to the correct objects, understanding relations between objects, generalizing systematically to unseen combinations of concepts and to larger and more complex sentences.\nSome works have made progress on this problem. Yuksekgonul et al. (2022) show that hard negative mining of images and text during fine-tuning is a promising first step to improving compositionality. However, performance gains are highly dependent on how clean the training data is, and generalizing to unseen combinations of concepts remains a challenge. Doveh et al. (2023) use LLMs for hard negative mining and Cascante-Bonilla et al. (2023) explore using synthetic datasets to improve compositional understanding in VLMs. Synthetic datasets lead to a domain gap compared to natural datasets. We aim to develop a general-purpose approach for improving compositionality of all such\ncontrastively trained VLMs. In this paper, we consider a scene graph representation of the image and text. We observe that multiple sub-graphs of the text scene graph with different semantic complexities can be matched with the same image. Performing this matching improves fine-grained and hierarchical understanding of text and thereby, of images. We achieve this by developing a scene graph-based text decomposition strategy that creates a scene graph for any given text, decomposing it into sub-graphs, and matching an image to multiple sentences derived from these sub-graphs (See Fig. 2 for an overview). Each sub-graph represents a distinct part of the image, aligning well with CLIP\u2019s original imagetext matching objective. Focused on improving attribute binding and relation understanding, we develop novel hard negative graph creation strategies which helps VL contrastive learning. We provide a novel Image-to-Multi-Text contrastive loss for matching individual images to multiple sentences. Our approach of matching texts of different complexity (from coarse-grained to fine-grained) to the image leads to fine-grained and hierarchical text understanding. Our resulting model is MosaiCLIP.\nOur approach leads to significant improvements across compositionality benchmarks. For example, Figure 1 b) and c) shows that MosaiCLIP improves performance by 11.5% and 9.1% on CREPE and ARO dataset over a strong baseline and by > 20% over CLIP. Our contributions encompass:\n\u2022 A novel graph-based text decomposition and augmentation framework and a coarse-to-fine contrastive learning objective for matching images to text sub-graphs of varying complexity.\n\u2022 Hard-negative mining techniques using graph transformations of the text scene graphs, that are seamlessly coupled with our text decomposition strategy, and applied over any text.\n\u2022 A thorough analysis for understanding why MosaiCLIP improves vision-language compositionality, disentangling the effect of image and text encoders and providing a novel tree-score based analysis showing that MosaiCLIP exhibits improved hierarchical text understanding.\n\u2022 Extensive experiments over three model architectures, two pre-training datasets, three\nfine-tuning datasets and test over four compositionality benchmarks (11 datasets) to prove the efficacy of MosaiCLIP for improving compositionality."
        },
        {
            "heading": "2 Related Work",
            "text": "Contrastive Vision-Language Pre-training: Large-scale contrastive learning for Vision and Language is utilized to create models like CLIP (Radford et al., 2021) and ALIGN (Jia et al., 2021). These models showcase impressive performance on a variety of tasks, including image classification, text and image retrieval, image captioning (Mokady et al., 2021), object detection (Zhong et al., 2022; Li et al., 2022c) etc.\nVisio-Linguistic Compositionality: Various studies have introduced benchmarks for assessing the compositional reasoning abilities of vision-language foundation models (VLMs). For instance, Winoground (Thrush et al., 2022) is a handpicked collection of 400 test cases, each comprising two images and two sentences. Sentences have the same word content and differ in word-order. Diwan et al. (2022) show that the Winoground dataset tests additional challenges along with compositionality, including handling ambiguous image-text pairs and unusual examples. Yuksekgonul et al. (2022) proposed the ARO benchmark for probing VLMs ability to understand Attribute, Relations, and Word-Order. Ma et al. (2022) proposed CREPE for measuring two aspects of compositionality: systematic generalization and productivity. All benchmarks suggest that contrastively trained VLMs have severe difficulty in compositional reasoning. As a remedy, NegCLIP (Yuksekgonul et al., 2022) and Teaching SVLC (Doveh et al., 2023) create targeted rule-based and LLM-guided hard negative sentences, SyViC (Cascante-Bonilla et al., 2023) fine-tunes CLIP with million scale synthetic images-text pairs, for improving relational and attribute understanding. We observe that previous methods are either highly dependent on how clean the training data is, use expensive LLM\u2019s for data augmentation or use synthetic datasets that require special solutions to resolve the synthetic-to-real domain gap. We hence develop a coarse-to-fine contrastive learning framework that matches images with texts of multiple complexities, which serves as a general-purpose solution to improve fine-grained and hierarchical text understanding,\nthereby improving compositionality.\nScene Graphs are structured representations of visual scenes, consisting of objects, their attributes, and relationships between objects. Scene graphs are beneficial for a range of tasks including image retrieval (Wu et al., 2019; Johnson et al., 2015), image captioning (Yang et al., 2019), and image generation (Johnson et al., 2018) among others."
        },
        {
            "heading": "3 Methodology",
            "text": ""
        },
        {
            "heading": "3.1 Overview",
            "text": "Here we present the key high-level ideas of our approach. We first present a graph-centric view of the standard image-text matching objective in CLIP, which serves as a motivation to develop our approach (Sec. 3.2). We create scene graphs derived from the text, decompose them into multiple sub-graphs (Sec. 3.3) and apply augmentations on these sub-graphs to create negative sub-graphs (Sec. 3.4) which are used as hard negatives in a batch. Sec. 3.5 formally defines the Image-to-Multi-Text and Text-to-Image losses used for a batch of V-L inputs which is key for learning from multiple positive and negative texts derived from sub-graphs. Matching images with coarse-to-fine sub-graphs results in improved fine-grained and hierarchical understanding of text. Sec. 3.6 provides a twostage curriculum learning strategy for improved fine-tuning performance."
        },
        {
            "heading": "3.2 Image-Text-Graph Alignment",
            "text": "Our approach builds on the idea that the standard image-text contrastive learning in CLIP can be viewed as a matching between an image scene graph and its sub-graph. Formally, given an imagetext pair (I, T ), the image can be viewed by its scene graph, GI = (VI , EI). The text scene graph is given by GT = (VT , ET ). Then GT \u2282 GI . According to this assumption, during contrastive learning in CLIP, we implicitly bring the representation of the image scene graph close to one of its sub-graph (the text scene graph). Now, let SG = {g|g \u2282 G} represent the set of sub-graphs of a graph G. According to the assumption above, g \u2208 SGT \u21d2 g \u2208 SGI . Hence \u2200g \u2208 SGT , (g,GI) becomes a correct matching pair during contrastive learning. We match multiple sub-graphs of the text scene graph to the same image, while also including hard negative sub-graphs in the batch. Matching between graphs is an implicit concept,\nand all graphs are first converted to text via templates, converted to embeddings using transformerbased (text) encoders, and matched to image embeddings."
        },
        {
            "heading": "3.3 Scene Graph Guided Text Decomposition",
            "text": "Scene graphs are succinct representations of images. However, an image scene graph generator used for generating a scene graph for any given input image is expensive to train since it requires supervised scene graph annotations for training (Li et al., 2017; Xu et al., 2017; Zhang et al., 2019), and also leads to issues like low coverage or biased generations against the long tail nature of objects and relationship annotations. We instead use the text scene graph created using an off-the-shelf text scene graph parser1 (Wu et al., 2019). This serves as a proxy for the scene graph of (part of) the image and is assumed to be a sub-graph of the image scene graph, as also depicted by Figure 2.\nLet the text scene graph obtained be GT = (VT , ET ), where VT represent the nodes of the graph, which are either objects or their attributes. ET are the edges of the graph that represent relations between objects. See Fig. 2 for an example of a text scene graph. As shown in the figure, we decompose this scene graph into multiple positive sub-graphs Pg = {g1, g2, g3, \u00b7 \u00b7 \u00b7 , gk}, k \u2264 M , where M is the max number of decomposed subgraphs and is a hyperparameter. Each sub-graph is a representation of a part of the image. We then convert sub-graphs to sentences so that they can be easily processed by transformer-based (text) encoders commonly used to train CLIP. For this, we use a simple template-based approach. For e.g., we create templates of the form \"{N1} {R} {N2}\" if we need to convert a graph having two nodes (N1, N2) and a relation R, into a sentence format. Corresponding to each sub-graph, we obtain one positive text for the image, creating a positive text set Pt = {t1, t2, t3, \u00b7 \u00b7 \u00b7 , tk}."
        },
        {
            "heading": "3.4 Negative Sub-Graph Creation",
            "text": "Corresponding to sub-graphs in Pg, we create negative sub-graphs Ng = {ng1, ng2, ng3, \u00b7 \u00b7 \u00b7 }. Subgraphs in Ng are a minimally perturbed versions of the positive sub-graphs in Pg. Similar to positive sub-graphs, we convert sub-graphs in Ng to text using the same template-based approach, and obtain Nt = {nt1, nt2, nt3, \u00b7 \u00b7 \u00b7 }. Texts in Nt serve\n1https://github.com/vacancy/SceneGraphParser\nas hard negative texts in a given batch, see Fig. 2. We focus on creating negative sub-graphs that improve the attribute binding and relation understanding capabilities of the model, for which we use the following strategies for negative graph creation: We first consider an external set of objects (N ), attributes (A), and relations (R). 1) Node Swapping and Replacement: We swap nodes in sub-graphs, these can be swaps of nodes which are attributes or objects. We also replace nodes with external nodes from N , A based on their type. 2) Edge Replacement: We replace edges with randomly sampled edges from the external relations set, R. 3) Connecting Sub-graphs: Here we join two sub-graphs. For this, we use one sub-graph from Pg, and another random graph created using nodes and edges sampled from external sets N ,A,R. This creates an overall hard negative graph. Sub-graphs are joined by simply joining nodes from both graphs through a randomly sampled edge from R. These strategies result in minimally perturbed hard negative subgraphs for improving attribute and relation understanding. We define multiple graph transformations {fg : G \u2212\u2192 P (G)} \u2013 frel, fattr, fobj using the above techniques and create hard negative subgraphs. See Appendix Sec. B for more details regarding negative sub-graph creation."
        },
        {
            "heading": "3.5 Coarse-to-Fine Contrastive Learning in Image-Text-Graph Space",
            "text": "Given an image-text batch during training B = {(xi, ti)}ni=1, consider separately the batch of images BI = {xi}ni=1 and a batch of texts BT = {ti}ni=1. The sentences in the text batch are first converted to scene graphs to obtain a batch of scene graphs BG = {Gi}ni=1, followed by decomposition to sub-graphs to obtain the positive sub-graph batch Bposg = {gi}mi=1, m > n. r negative subgraphs are sampled and added to the batch to obtain Bg = {gi}m+ri=1 . We convert these sub-graphs to text to obtain the final text batch Bt = {tgi } m+r i=1 .\nConsider an image encoder model f\u03b8 parameterized by \u03b8, a text encoder f\u03d5 parameterized by \u03d5. For any image x, text t, u\u0303 = f\u03b8(x) is the unnormalized image feature, and v\u0303 = f\u03d5(t) is the unnormalized text feature. As common practice, the features are normalized to obtain u = u\u0303/\u2225u\u0303\u2225 and v = v\u0303/\u2225v\u0303\u2225. The Image-to-Multi-Text contrastive loss is given by:\nLMCi2t =\u2212 |BI |\u2211 i=1 1 |P(i)| \u2211 k\u2208P(i) log exp(\u03c4uTi vk)\u2211|Bt| j=1 exp(\u03c4u T i vj)\nwhere P(i) = {k|k \u2208 [1, |Bpost |], gk \u2286 Gi}. The Text-to-Image contrastive loss is only calcu-\nlated for the positive texts. It is given by:\nLMCt2i =\u2212 |Bpost |\u2211 j=1 log exp(\u03c4uTp(j)vj)\u2211|BI | i=1 exp(\u03c4u T i vj)\nwhere gp(j) \u2286 Gj . Bt = [B pos t ;B neg t ], in which Bpost ,B neg t represent the texts in Bt, obtained from positive and negative sub-graphs respectively. The overall loss is LMosaiCLIP = (LMCt2i + LMCi2t)/2."
        },
        {
            "heading": "3.6 Curriculum and Robust Fine-tuning",
            "text": "For fine-tuning experiments, we develop a twostage curriculum learning strategy motivated by recent work (Goyal et al., 2022; Wortsman et al., 2022; Kumar et al., 2022) that show how finetuning can distort pre-trained features and closely mimicking the contrastive pre-training objective while fine-tuning CLIP can help mitigate this problem (Goyal et al., 2022). However, our coarse-tofine contrastive learning objective naturally deviates from pre-training in two ways. a) Existence of hard negative texts in the batch, and b) Having multiple positive and negative texts for an image. This can lead to a gap in pre-training vs finetuning objective, and a lower than optimal performance after fine-tuning. To solve this, our twostage curriculum learning strategy first fine-tunes the model while sampling (at max) a single positive and negative sub-graph per image, followed by fine-tuning it with multiple positive and negative sub-graphs. The hardness of data in this curriculum learning setup is defined by the amount of difference the fine-tuning setup has as compared to the pre-training setup. According to this intuition, it is easier for the model to first learn to handle hard negatives in a batch and then learn to handle multiple positive and hard negative sentences at once. We see consistent improvements using this strategy compared to a direct one-step fine-tuning, which we term as MosaiCLIPNoCurric in our ablations. For better performance on non-compositonal tasks, we use the robust fine-tuning approach (Wortsman et al., 2022) of weight space ensembling of the vision encoder, before and after fine-tuning. This model is called MosaiCLIPWiSE-FT"
        },
        {
            "heading": "4 Experiments",
            "text": "Evaluation Datasets: We test MosaiCLIP and baselines on large scale benchmarks that require compositional reasoning: CREPE-Systematicity (Ma et al., 2022) measures systematic generalization, ARO (Yuksekgonul et al., 2022) measures\nattribute, relation and word-order understanding, SVO (Hendricks and Nematzadeh, 2021) measures verb (relation) understanding, VL-Checklist (Zhao et al., 2022) measures relation, attribute and object understanding. We use CREPE-Productivity (Ma et al., 2022) for measuring model\u2019s ability to productively generalize to more complex and long sentences. Methods for improving compositionality should be tested on general downstream tasks used to evaluate the quality of learned representations of language and vision. For this, we utilize the popular ELEVATER benchmark (Li et al., 2022a) consisting of 20 datasets and ImageNet (Deng et al., 2009) following prior work (Doveh et al., 2023). Baselines: We compare with all recent techniques used for improving compositionality of CLIP style models including NegCLIP (Yuksekgonul et al., 2022), Teaching SVLC (Doveh et al., 2023) and Syn-CLIP (Cascante-Bonilla et al., 2023) along with CLIP (Radford et al., 2021) as well as CLIPFT (fine-tuned) on datasets we use. See Appendix Sec. F for more details."
        },
        {
            "heading": "Training and Evaluation Details:",
            "text": "Fine-tuning: NegCLIP (Yuksekgonul et al., 2022) was developed by fine-tuning CLIP on the COCO dataset (Lin et al., 2014), however, COCO images might overlap with benchmarks like CREPE and ARO which may lead to confounding of results. Hence we consider 2 additional similar sized finetuning datasets randomly sampled from CC-12M (Sharma et al., 2018; Changpinyo et al., 2021) and YFCC-15M (Thomee et al., 2016) and call them CC-FT, YFCC-FT. We also use CC3M (Sharma et al., 2018) for comparing with recent baselines. We fine-tune the commonly used OpenAI CLIPViT-B32 model and report results on all datasets, except for CREPE dataset which tests the systematic generalization for which we used OpenCLIP (Ilharco et al., 2021) models pre-trained on {CC12M, YFCC-15M}, fine-tune them on {CC-FT, YFCC-FT}, and report results on {CC-12M,YFCC15M} splits of CREPE. See Appendix E.3 for more information on evaluation datasets.\nPre-training: We pre-train MosaiCLIP, NegCLIP and CLIP on two prominent large-scale pre-training datasets, CC-12M and YFCC-15M, and use two different backbones (ResNet-50 and Swin-Tiny) following prior work (Yang et al., 2022) and report zero-shot performance on all test datasets. See Appendix H.1 for hyperparameters details."
        },
        {
            "heading": "4.1 Results",
            "text": "In this section we provide experimental results in both pre-training and fine-tuning settings to show the efficacy of our approach. These are as follows:\nFine-tuning: Main fine-tuning results are shown in Table 1 and 2, where we fine-tune CLIP models using our method and compare it to baselines. Notably, we see that the generalization performance on unseen compounds and atoms as measured by the CREPE dataset is up to 18% higher than NegCLIP. Additionally MosaiCLIP shows upto 16.5%, 5.3%, 32.3% of improvement over NegCLIP in understanding relations, attributes and word order respectively. MosaiCLIP also shows consistent improvements in the verb understanding task as measured by the SVO dataset. Additional Comparisons: We also compare with latest contemporary works in Table 2 and Appendix Sec. D.1. We find significant improvements (upto 14% on ARO) over models that use LLMs or synthetic data for making CLIP more compositonal.\nPre-training: Table 3 shows pre-training results over all benchmarks. CREPE results show\na significant gain in ability to systematically generalize to unseen combinations of concepts. Across pre-training settings, MosaiCLIP improves over NegCLIP by up to 42.5%, 4.9% when evaluated against HN-Comp (CU), HN-Atom (AU) hard negatives respectively. Significant improvements are observed in attribute and relation understanding, giving gains of up to 8.3%, 12.0% respectively across pretraining settings. We also note that order understanding of MosaiCLIP is worse than that of NegCLIP for the CC-12M pre-training dataset, while better than NegCLIP for the YFCC-15M dataset. Notably, there is a large variance in NegCLIP\u2019s performance across pre-training datasets as seen in Table 3, and it also performs poorly when the pre-training dataset has higher noise (e.g. YFCC-15M). MosaiCLIP is fairly consistent and more robust to the change in the pre-training dataset. In Appendix C.5 we find that MosaiCLIP can provide improvements over NegCLIP while using as low as 0.3x of the total pre-training or fine-tuning data.\nResults on classification and retrieval: On average, MosaiCLIP achieves +3.3%,+6.3% better performance on the ELEVATER classification benchmark compared to NegCLIP and CLIP while pre-training and maintains similar accuracy as CLIP while fine-tuning. We also try using our method along with the robust fine-tuning technique (WiSE-FT) so that performance degra-\ndation during fine-tuning is minimal, as shown in Appendix Table 9. See Fig. 3 for average results on ELEVATER over four training settings and Table 4 for results on retrieval benchmarks where we see a +5.4 point improvement over NegCLIP. We use the popular Karpathy splits having a 5K and 1K sized test set for obtaining the COCO and Flickr30k retrieval scores respectively. Hence MosaiCLIP\u2019s training strategy improves or maintains the quality of learned representations while improving compositonality. Figures 11-14 show detailed results on ELEVATER.\nProductivity: As defined by Ma et al. (2022), a productive VL model can handle arbitrarily long and complex sentences and is an important\naspect of compositionality. Although we do not explicitly train our models for generalization to longer sentences, the improved hierarchical language understanding using our methods lead to an emergent behavior such that MosaiCLIP generalizes better than NegCLIP and CLIP to more complex sentences. We can see this effect in Fig. 4 a) and Appendix Fig. 8 and 9. We report the average of retrieval over swap and atom splits and find MosaiCLIP significantly improves over NegCLIP by upto 15% across different text complexities (4-12).\nApplication to more advanced VLMs: While our focus in this work has been on CLIP style, dual encoder models due to their various benefits, we believe our methods are model agnostic and aimed at improving contrastive learning through our coarse-to-fine learning framework and negative mining techniques. In this section we test our model on an advanced VLM, BLIP. We modified BLIP\u2019s original image-text contrastive learning objective and create two variants, one called BLIP+NegCLIP where we use NegCLIP style hard negatives and the other BLIP+MosaiCLIP which uses our methods of scene graph guided\ntext decomposition and negative sub-graph creation. We fine-tune BLIP model taken from the official BLIP repository and use the \u201cBLIP w/ ViT-B and CapFilt-L model (pre-trained on 129M examples)\u201d as our base model. Results for fine-tuning experiment using COCO dataset is shown in Table 5. We use the hyperparameters used by the official codebase (for the task of fine-tuning on COCO dataset for image-text retrieval). For each setting, we report performance of four models, namely BLIP (before fine-tuned version), BLIP-FT (vanilla fine-tuned version), BLIP+NegCLIP, BLIP+MosaiCLIP. The model are evaluated on the ARO dataset to measure attribute, relation and word-order understanding, using the evaluation scripts provided by the authors of the dataset (Yuksekgonul et al., 2022). We find that\ncompared to vanilla fine-tuning, both NegCLIP and MosaiCLIP methodologies bring improvements to relation and word order understanding, while maintaining or improving performance on attribute understanding. The MosaiCLIP methodology significantly improves relational reasoning performance and word-order understanding compared to the NegCLIP methodology, up to 6.3%. Attribute understanding performance remains nearly the same as the baseline BLIP performance, with the MosaiCLIP methodology bringing in slight gains over NegCLIP\u2019s methodology. On average MosaiCLIP\u2019s methodology brings more improvements to BLIP than NegCLIP or vanilla fine-tuning."
        },
        {
            "heading": "4.2 Analysis",
            "text": "We provide a detailed analysis of our models and baselines, across different dimensions as follows:"
        },
        {
            "heading": "Disentangling MosaiCLIP improvements:",
            "text": "We quantify the relative importance of the vision and language side by freezing the language and vision encoder individually while fine-tuning all\nmodels. See Fig. 4 c,d for the results. Notably, we find that 1) Language encoder has significant scope for improvement over NegCLIP\u2019s language encoder, and MosaiCLIP is able to successfully exploit this potential and deliver an enhanced compositional understanding of language, which is evident by performance increase of +3.7,+6.9% over NegCLIP when only the language encoder is fine-tuned, as shown in Fig. 4 c,d. 2) Improvements brought by MosaiCLIP over NegCLIP in the text encoder are always higher than improvements in the image encoder. This is evident from Fig. 4 c,d where the performance increase over NegCLIP when only the language encoder is fine-tuned is always higher as compared to when only the image encoder is fine-tuned; for example, 3.7% > 0.0%, 6.9% > 1.8% for ARO-Relation, ARO-Attribution. 3) MosaiCLIP brings significant improvements on the image encoder side (higher than NegCLIP) without using any image negative mining, unlike NegCLIP.\nMosaiCLIP improves hierarchical text understanding: For further understanding MosaiCLIP\u2019s improved compositional understanding, we provide a novel analysis by considering the recently proposed Tree-Score (Murty et al., 2022) that measures the degree to which a transformer (text) encoder processes text in a hierarchical manner. We hypothesize that having tree-like hierarchical computation over language can be one leading factor for explaining the compositionality (or lack thereof) of CLIP-like models. Along with this, we have previously shown that the language encoder has the most prominent effect in improving compositionality in the case of MosaiCLIP . These two reasons motivate the use of tree-score to compare the language encoder\u2019s hierarchical understanding capability. Fig. 4 a) shows that MosaiCLIP\u2019s language encoder has higher tree-scores than NegCLIP\u2019s language encoder, suggesting that MosaiCLIP performs more tree-like computations. This explains the improved language compositionality of MosaiCLIP since a hierarchical tree-structured computation allows the language encoder to better understand input text compositionally, thereby improving vision-language compositionality. This is in line with the hypothesis that human\u2019s semantic understanding of sentences involves a hierarchical (tree-structured) computation which has significant evidence (Crain and Nakayama, 1987; Hale et al.,\nin blue match the image to the correct sentence (in green) while the models in white match the image to the incorrect sentence (in red). Here, models are taken from our fine-tuning experiments on COCO from Table 1.\n2018; Pallier et al., 2011) and this leads to their compositional generalization capability.\nMosaiCLIP is Robust: Noisy texts often have meaningful sub-texts which can be exploted by MosaiCLIP, hence MosaiCLIP often achieves consistent performance increase regardless of noise in the pre-training or fine-tuning dataset. For example, NegCLIP achieves significantly low performance on ARO when fine-tuned with YFCC-FT (having more noise in text) as compared CC-FT or COCO as shown in Table 1. NegCLIP takes a > 10% hit in performance across various ARO datasets when the fine-tuning dataset is changed from COCO to YFCC, whereas, MosaiCLIP achieves similar performance using both datasets. Appendix Sec. D.3 shows that pre-trained MosaiCLIP is robust to natural distributon shifts.\nQualitative Analysis: We take MosaiCLIP, NegCLIP and CLIP fine-tuned on COCO and filter out examples from the ARO dataset where MosaiCLIP and NegCLIP\u2019s disagree. Some notable examples in Fig. 5 include cases where NegCLIP and CLIP often struggle to understand simple concepts like understanding the color of the cat and table (top-left Fig. 5 or understanding the \"is holding\" relation b/w sandwich and the box in bottom-right Fig. 5."
        },
        {
            "heading": "4.3 Ablations",
            "text": "Table 6 and Appendix Tables 8,9 show the effect of curriculum learning and robust fine-tunining where we find that curriculum learning can bring consistent improvements of up to 1.2% on average and robust-finetuning (WiSE-FT) technique performs the best on zero-shot tasks (i.e. minimal forgetting while fine-tuning), while still improving over NegCLIP by about 5% on compositional reasoning tasks. Table 7 shows the effects of different kinds of sub-graphs sampled during training. More details including the effect of sampling larger number\nof sub-graphs are presented in Appendix Sec. C."
        },
        {
            "heading": "5 Conclusion",
            "text": "We present a method to improve the compositional reasoning capabilities of contrastively trained large vision-language models. In particular, we provide a coarse-to-fine contrastive learning framework and a scene graph-based text decomposition strategy for matching subgraphs of the text scene graph having varying complexity to an image during contrastive learning. We also develop hard negative graph creation strategies focused on improving attribute binding and relation understanding capabilities. Our techniques leads to significant improvements in compositional reasoning capabilities. We investigate the reasons for improved compositionality and present a novel finding based on language encoder tree-scores, suggesting that our models learn improved fine-grained and hierarchical text understanding, which is likely the key reason for improved vision and language compositionality of MosaiCLIP as compared to baselines."
        },
        {
            "heading": "6 Limitations",
            "text": "Computational Cost: Although MosaiCLIP leads to significant performance increase on several benchmarks that test compositional reasoining, it requires a higher per-batch computational cost while training. For this we give a detailed analysis on the computational cost in Appendix C.6 and show that simply providing more compute to prior methods in the form of larger batch sizes does not improve compositional reasoning. We also show ways to tackle this computational cost, by using less data in Appendix C.5, since MosaiCLIP is data efficient and can provide improvements over baselines with as low as 0.3x of the total data. This along with our ablations in Appendix C.1 gives some control to any practitioner to vary either the training dataset size or the number of sub-graphs in our method, and obtain a clean tradeoff between accuracy and compute. As future work we would like to develop a coarse-to-fine grained objective requiring minimal extra computation cost per batch. Future work should also look at decreasing the extra computational cost incurred by contemporary methods like Syn-CLIP (Cascante-Bonilla et al., 2023) and Teaching SVLC (Doveh et al., 2023).\nOther Vision Language Models: In our current work we primarily aim to improve the compositionality of CLIP-Style, dual-tower models trained using large scale contrastive learning, since they severely lacked compostional reasoning capabilities as shown by (Yuksekgonul et al., 2022). Many other VLMs exist such as those that undergo cross modal interactions between vision and language such as BLIP (Li et al., 2022b), X-VLM (Zeng et al., 2021), LXMERT (Tan and Bansal, 2019). Although our methods show promise in improving more advanced VLMs like BLIP as shown in Section 4 and Table 5, a more thorough analysis will be beneficial to study the extent to which our methods can improve vision-language contrastive learning for these models.\nSentence Templates: For simplicity, we currently use manually curated templates to convert subgraphs to sentences, however, this can lead to similar looking and synthetic sentences. Large language models like GPT-4 (OpenAI, 2023), BLOOM (Mitchell et al., May 2021-May 2022) should be looked into for developing sentences from scene-graphs, by directly giving the LLM a\nscene-graph as input and requiring it to generate a sentence. This approach might be effective but may also lead to higher computational cost while training."
        },
        {
            "heading": "Acknowledgements",
            "text": "We thank anonymous reviewers for their insightful suggestions that helped in greatly improving our paper. We also thank Aditi Khandelwal, Animesh Sinha, Abhishek Kadian for their helpful comments and suggestions on this work."
        },
        {
            "heading": "Appendix",
            "text": ""
        },
        {
            "heading": "A Background",
            "text": "Contrastive Language-Image pre-training (Radford et al., 2021) (CLIP) aims to learn generalpurpose representations of vision and language using paired image-text data. This is achieved using contrastive learning in the image-text space. In particular consider a pre-training dataset of size n, D \u2282 X \u00d7 T , D = {xi, ti}ni=1. Here X and T are the space of images and text, respectively, and xi, ti are images and text in the dataset. Also, consider access to image and text encoders, that we represent by f\u03b8 : X \u2192 Rd and f\u03d5 : T \u2192 Rd respectively. To learn distributed representations for images and text, the following contrastive losses are used:\nLt2i =\u2212 1\n|B| |B|\u2211 j=1 log exp(\u03c4uTi vj)\u2211|B| i=1 exp(\u03c4u T i vj) (1)\nLi2t =\u2212 1\n|B| |B|\u2211 i=1 log exp(\u03c4uTj vj)\u2211|B| j=1 exp(\u03c4u T i vj) (2)\nWhere B represents the batch during one iteration of training. ui,vi are the \u21132 normalized embeddings of u\u0303i, v\u0303i, where u\u0303i = f\u03b8(xi), v\u0303i = f\u03d5(ti). \u03c4 is the temperature parameter and is trainable. The overall loss is Lclip = (Lt2i + Li2t)/2."
        },
        {
            "heading": "B Scene Graph Decomposition",
            "text": "Here we provide additional details for text scene graph decomposition. Denote the text scene graph obtained from the scene graph parser by GT = (VT , ET ), where VT represent the nodes of the graph, which are either objects or their attributes. ET are the edges of the graph that represent relations between objects. Let G denote the set of all possible scene graphs. We first consider an external set of objects (N ), attributes (A), and relations (R) that we use for creating negative sub-graphs. In practice, we create this set from Visual Genome (VG) dataset (Krishna et al., 2016). Following Zhang et al. (2021), we sample a total of 1594 entities that have 30 instances of them in the VG dataset. The attribute and Relation list contains 524, and 50 unique instances, respectively. Hence |N | = 1594, |A| = 524, |R| = 50. We first sample all possible sub-graphs having one or two\nobjects in them, and these can have multiple attributes for the objects. We develop and use scene graph transformations that take a sub-graph as input and return a (set of) modified versions of the graph (minimally-perturbed negative sub-graphs for the image). For this, we define three graph transformations as follows:\n\u2022 fobj : G \u2212\u2192 P (G) takes input a single object scene graph, where the object has attributes Ao. For each attribute, a \u2208 Ao, a random attribute a\u2032 is sampled uniformly at random from A. We finally obtain a set of sub-graphs Gobj \u2208 P (G) where P (.) denotes the power set. Each g \u2208 Gobj contains one object node connected with an attribute node which is sampled from A.\n\u2022 frel : G \u2212\u2192 P (G) takes input sub-graphs having one relation edge and gives output a set of sub-graphs Grel \u2208 P (G) where each g \u2208 Grel has either object nodes shuffled, replaced by an external object node n\u2032 sampled uniformly at random from N , and/or relation replaced by external relation r\u2032 sampled uniformly at random from R. Along with this, we also join the input positive sub-graph with a random sub-graph created by sampling random nodes and edges from N , A, R.\n\u2022 fattr : G \u2212\u2192 P (G) takes input sub-graphs having one relation edge and gives output a set of sub-graphs Gattr \u2208 P (G) where each g \u2208 Gattr has attribute nodes shuffled, and/or replaced by an external attribute node a\u2032 sampled uniformly at random from A.\nfobj , fattr broadly aims at improving the model\u2019s attribute understanding, while frel broadly targets improved relation understanding. For each positive sub-graph, we sample all possible negative subgraphs using fobj , frel, fattr and make positive-negative sub-graph pairs (gposi , {gnegi}). These pairs can be classified into three categories C = {cobj , crel, cattr} according to the transformation that created the negative sub-graphs. We sample sub-graph pairs from these categories according to probabilities pi, i \u2208 {1, 2, 3} corresponding to the three categories respectively, and \u2211 pi = 1. These probabilities are hyperparameters; see Appendix Section H.1 for more details. Multiple subgraph pairs can have common positive or negative sub-graphs, and sampling these pairs would result\nin duplication, hence for each image, we make sure to deduplicate sub-graphs so that all sub-graphs, and therefore the text made from them are unique for a given image in a batch. After sampling, all sub-graphs are transformed to text using simple templates, as explained in Section 3.3."
        },
        {
            "heading": "C Ablations and Model Analysis",
            "text": ""
        },
        {
            "heading": "C.1 Sampling more subgraphs",
            "text": "We analyze the effect of increasing the maximum number of sub-graphs sampled for any given image in a batch of data during training. See Figures 6 and 7, in which we test the performance on ARO and CREPE benchmarks (averaged over three finetuning datasets considered in this work), as we increase the max positive and negative sub-graphs per image. We find that as we increase both positive and negative sub-graphs for an image, the performance steadily increases up to a point for all datasets, after which the performance can either flatten out, increase, or even decrease in some of the datasets. This is intuitive since a larger number of positive and negative sub-graphs per image leads to a gap w.r.t the pre-training stage as described in Sec. 3.6. Also, different compositional splits require different reasoning skills, and as we keep sampling positive and negative sub-graphs for an image, it is natural for certain types of positive and negative sub-graphs to be more pronounced, depending on the dataset statistics, and this can have varied effects on different datasets.\nC.2 Effect of different sub-graph types Here we analyze the effect of sampling different kinds of sub-graphs from the original scene graph of the text. In particular, we measure the effect of graph transformations that we define in Appendix\nSec. B. Results are presented in Table 7. We observe that both frel and fattr as described in Appendix Sec. B, are useful for improving relation and attribute understanding (as measured on the ARO benchmark), across fine-tuning datasets."
        },
        {
            "heading": "C.3 Effect of curriculum training",
            "text": "As shown in Table 8, in all fine-tuning results, we can see consistent improvements when using our curriculum learning strategy, such as upto 2% on systematic generalization, and sometimes more than 6% as seen for ARO-Order results when the fine-tuning dataset is YFCC-FT."
        },
        {
            "heading": "C.4 Effect of robust fine-tuning",
            "text": "Among many other techniques developed for mitigating forgetting in large models when they are fine-tuned, one prominent one is robust fin-tuningWiSE-FT, (Wortsman et al., 2022). Following Wortsman et al. (2022) we perform weight-space ensembling on the image encoder before and after fine-tuning using our method and call this model MosaiCLIPWiSE-FT. The results on compositionality benchmarks can be seen in Table 8 while results on 21 multimodal tasks from ELEVATER and ImageNet can be seen in Table 9. We find that MosaiCLIPWiSE-FT has a slight performance decrease on some compositonal benchmarks as compared to MosaiCLIP, however, it is significantly better than NegCLIP on most benchmarks. The real benefit of using MosaiCLIPWiSE-FT is that it leads to least forgetting, and there is little to no performance degradation on 21 tasks as showin in Table 9."
        },
        {
            "heading": "C.5 Data efficiency",
            "text": "We find that our technique leads to significant data efficiency requiring about 0.3x-0.6x fo the total fine-tuning or pre-training data to match or exceed NegCLIP performance. Results are shown in Tables 10 and 11."
        },
        {
            "heading": "C.6 Computational cost",
            "text": "Even though MosaiCLIP uses the same global batch size of image-text pairs, it requires more compute as compared to NegCLIP or CLIP owing to the fact that decomposing sub-graph leads to a larger effective text-batch size and hence a larger contrastive learning matrix. It is a common practice in literature to trade-off larger compute for improving\nCLIP\u2019s compositionality, as also done by previous methods Syn-CLIP (Cascante-Bonilla et al., 2023) that generate data using external graphics engines, and Teaching-SVLC (Doveh et al., 2023) which use LLMs requiring massive compute even during inference. Providing NegCLIP with more compute: One can argue that providing more compute to NegCLIP can lead to better performance, however, on the contrary we found that NegCLIP\u2019s performance\ndecreases as batch size is scaled (from 256 to 4096, much beyond MosaiCLIP\u2019s text or image batch size), as shown in Table 12. Performance-Compute Tradeoff: It is to be noted that MosaiCLIP performance continues to increase up to a threshold, as sub-graphs are increased as shown in Table 7 and 6 hence this provides a clean tradeoff between number of sub-graphs and compute, and a practitioner can choose the number of sub-graphs their compute availablility. Along with this, in Appendix Sec. C.5 we showed that we can achieve improved performance compared to NegCLIP with as low as 0.3x data closing the gap between NegCLIP and MosaiCLIP compute even more. It is to be noted that MosaiCLIP is a drop in replacement for CLIP after training and requires the same inference cost as CLIP."
        },
        {
            "heading": "512 68.9 65.6 88.68",
            "text": ""
        },
        {
            "heading": "1024 67.6 65.1 88.93",
            "text": ""
        },
        {
            "heading": "2048 65.7 64.2 88.72",
            "text": ""
        },
        {
            "heading": "4096 62.5 63.7 88.11",
            "text": ""
        },
        {
            "heading": "D Additional Results and Experiments",
            "text": "D.1 Comparison with recent baselines\nWe compare with recently published and contemporary works (Cascante-Bonilla et al., 2023; Doveh et al., 2023). Doveh et al. (2023) show that one can create rule-based hard negative sentences and Large Language Models (LLMs) based hard negative sentences and use them when training CLIP style models to obtain an improved model that is better at handling tasks that require compositional reasoning. We fine-tune on CC3M (Sharma et al., 2018) for a fair comparison with Doveh et al. (2023). Results are reported in Table 13. A fair comparison with Syn-CLIP Cascante-Bonilla et al. (2023) is not possible since their synthetic dataset is not released. However in Table 13 we find that performance difference is large between MosaiCLIP and Syn-CLIP showing that our general coarse-to-fine grained approach is better than using targeted synthetic datasets for inducing compositional understanding in VLMs. Comparisons with Doveh et al. (2023) in\nTable show that our approach is competitve or better at attribute, relation and object understanding as measured by the VL-Checklist benchmark (Zhao et al., 2022). Zero Shot performance on 21 datasets suffers minimally using our approach, and is even better than (Zhao et al., 2022). It is to be noted that both approaches Syn-CLIP (Cascante-Bonilla et al., 2023) and Doveh et al. (2023) are orthogonal to our approach and combining them with our coarseto-fine understanding approach will likely result in much better performance overall, as compared to individual techniques. In particular, Syn-CLIP (Cascante-Bonilla et al., 2023) faces the issue of having long captions for images, and they average out embeddings of parts of the caption before matching it to the image. This issue can be eaily resolved using our framework which can easily handle multiple positive captions for an image. Performing this ablation would be future work for us, once synthetic datasets like that used by CascanteBonilla et al. (2023) are open-sourced and gain more popularity. Our approach can similarly also include captions generated from LLMs, as explored by Doveh et al. (2023)."
        },
        {
            "heading": "D.2 Standard deviations for fine-tuning results",
            "text": "Here we provide fine-tuning results on the CC-FT dataset with standard deviations over 3 random seeds where OpenAI CLIP-ViT-B-32 is fine-tuned on CC-FT using MosaiCLIP and baseline techniques. See Table 14 for the results. The main paper Table 1 have average results for CC-FT while for COCO and YFCC-FT fine-tuning datasets, the results are for one seed. We do-not run multiple pre-training experiments since they significantly more costly.\nD.3 Robustness to natural distribution shifts We find that pre-trained MosaiCLIP shows robustness to natural distribution shifts as measured by ImageNet natural distribution shifts benchmark. Results are presented in Table 15. We believe that MosaiCLIP sees a larger variety of texts in the form of sub-graphs which can provide it with extra supervision for tackling natural distribution shifts. Intutively, sub-graphs can lead to diversity of texts being seen by the model during training and this might lead to broader coverage of concepts and concept combinations, resulting in improved robustness. Along with this a coarse to fine hierarchical understanding of texts and thereby, of\nimages should intuitively help in improving performance on robustness benchmarks given that the model will now be able to recognise details in images and texts more accuractely."
        },
        {
            "heading": "E Dataset Details",
            "text": "Here we provide detailes about datasets used for fine-tuning, pre-training and evaluating models in this study. A summary is shown in Table 16"
        },
        {
            "heading": "E.1 Fine-tuning datasets",
            "text": "Following NegCLIP (Yuksekgonul et al., 2022) we use the COCO dataset released by (Yuksekgonul et al., 2022) having 109k samples that had hard negative sentences that (Yuksekgonul et al., 2022) create for training NegCLIP. As mentioned in the main paper, COCO dataset images are used for creating Visual Genome (Krishna et al., 2016), and this is further used to create datasets such as CREPE (Ma et al., 2022), ARO (Yuksekgonul et al., 2022) and a part of VL-Checklist (Zhao et al., 2022). This can lead to confounding and potentially mislead-\ning results, since it is unclear if the performance increase using any method comes from the finetuning dataset (COCO) being close to the domain of test datasets, or if it\u2019s the fine-tuning methodology that leads to an increase in performance. Hence, for rigourous experimentation of the developed methods, one must use other datasets to finetune contrastively trained VLMs. We randomly sample similar sized (100k datapoints) from popular pre-training datasets CC-12M and YFCC-15M, and call these smaller datasets CC-FT and YFCCFT. To train NegCLIP, hard negative sentences and images are required, for which we first use the code released by (Yuksekgonul et al., 2022)2 to create hard negatives sentences as well as sample three hard negative images for each image based on OpenAI CLIP ViT-B/32 features, strictly following (Yuksekgonul et al., 2022). For comparing with contemporary works (Doveh et al., 2023), (Cascante-Bonilla et al., 2023) (as shown in Table\n2https://github.com/mertyg/ vision-language-models-are-bows\nImageNet-A ImageNet-R ImageNet-S ImageNet-V2\nArch. Data Method Top1 Top5 Top1 Top5 Top1 Top5 Top1 Top5\nCLIP 6.4 24.5 42.6 68.8 22.2 45.5 28.2 54.1 NegCLIP 6.6 25.0 43.1 68.7 22.2 45.4 29.4 55.2CC-12M MosaiCLIP 9.1 29.4 48.6 74.3 27.2 52.6 33.6 61.6\nCLIP 10.9 34.2 20.6 42.0 6.4 16.7 26.1 49.9 NegCLIP 11.4 35.6 20.0 41.7 6.0 16.0 27.2 50.7S\nw in\n-T\nYFCC-15M MosaiCLIP 14.6 40.2 22.3 44.9 6.8 17.7 32.0 57.2\nCLIP 7.3 27.4 41.4 67.8 21.7 44.3 29.8 56.4 NegCLIP 7.7 27.7 41.0 66.9 21.7 43.9 30.2 56.0CC-12M MosaiCLIP 11.1 35.6 52.1 76.9 29.5 55.4 37.0 66.5\nCLIP 13.4 37.3 17.2 37.2 4.9 13.6 25.8 49.4 NegCLIP 12.9 38.0 18.0 37.3 5.1 14.7 26.0 49.0R\nN -5\n0\nYFCC-15M MosaiCLIP 17.4 46.6 21.0 42.7 6.5 16.9 32.2 57.9\nSVO(Hendricks and Nematzadeh, 2021), Visual Genome(Krishna et al., 2016), COCO(Lin et al., 2014), Flickr(Young et al., 2014), HAKE(Li et al., 2019), VAW(Pham et al., 2021), SWiG(Pratt et al., 2020)\nTable 16: Details of datasets used in this study for testing compositional reasoning, for fine-tuning and pre-training models. See Appendix Sec. E for more details.\n2), we use CC3M (Sharma et al., 2018) since it\u2019s used by these baselines, and makes a direct comparison possible with them."
        },
        {
            "heading": "E.2 Pre-training datasets",
            "text": "We use popular and standard large scale pretraining datasets CC-12M (Changpinyo et al., 2021)\nand YFCC-15M (Thomee et al., 2016) for pretraining all models in this study, including CLIP, NegCLIP and MosaiCLIP."
        },
        {
            "heading": "E.3 Evaluation datasets",
            "text": "Here we list the evaluation detailes used in this study and also provide a short description for each CREPE-Systematicity (Ma et al., 2022): CREPE provides systematic generalization datasets to test models trained on popular pre-training datasets including CC-12M and YFCC-15M. While creating CREPE, Ma et al. (2022) make sure to split the dataset into seen and unseen parts, which correspond to weather the model has seen or not seen the combination of concepts, when pre-trained with popular pre-training datasets. We measure and report performance on both seen and unseen splits in our work. ARO (Yuksekgonul et al., 2022): This benchmark consists of four datasets, including VG-Relation, VG-Attribution, COCO-Order, and Flickr-Order. The first two measure attribute and relation understanding of VL models, respectively, and the last two measure the word order understanding of VL models. VG-Relation and VG-Attribution consist of tuples having an image and two texts (one positive and one negative), and the model\u2019s task is to match the image with the correct text. order datasets have four negative texts and one positive text for each image, and the task is again to match the image with the correct text. SVO-Probes (Hendricks and Nematzadeh, 2021): This dataset consists of tuples having two images and one text. All texts and images have a subject, verb, and object, and the images differ in only one of subject, verb, or object. This dataset helps in understanding if VL models can compositionally understand combinations of objects having a relation between them. The original dataset contains 48K examples.3 CREPE-Productivity (Ma et al., 2022): Productivity dataset tests the model\u2019s ability to generalize to longer and more complex sentences, with complexity ranging from 4 atoms to 12 atoms, where an atom can be an attribute, relation, or object. The CREPE-Productivity dataset has a number of test sets for each sentence complexity ranging from 4 atoms to 12 atoms.\n3Some image links provided by the the original repository(https://github.com/deepmind/svo_ probes) were broken. In total, 36k data points were retrievd and used in this study.\nVL-Checklist (Zhao et al., 2022): This benchmark is created by combining annotations from datasets like Visual Genome (Krishna et al., 2016), SWiG (Pratt et al., 2020), HAKE (Li et al., 2019), VAW (Pham et al., 2021). Each image in the resulting dataset has two captions, a positive and a negative. The positive caption is taken from the source dataset of the image, while the negative caption differs from the positive in only one word which makes it a hard negative and helps in testing compositional and fine-grained understanding of VLMs across various dimensions like attributes, relations, and size and locations of objects."
        },
        {
            "heading": "F Baselines:",
            "text": "Here we list the baselines used in this study and also provide a short description for each. CLIP(Radford et al., 2021): Our first baseline is CLIP model released by OpenAI CLIP(Radford et al., 2021) and OpenCLIP (Ilharco et al., 2021). In particular we use the ViT-B/32 model for fine-tuning results Table 1 of the main paper, except for CREPE dataset, which requires using models pre-traoined on specific datasets, for which we use ResNet-50 (RN-50) models pre-trained on CC-12M and YFCC-15M released by OpenCLIP repository4 (Ilharco et al., 2021). CLIP-FT: For disentangling the effects of fine-tuning data, and fine-tuning methodology, we create a CLIP-FT baseline where we simply fine-tune the pre-trained CLIP model on the dataset at hand, by using the standard contrastive learning technique used by CLIP. NegCLIP(Yuksekgonul et al., 2022) [ICLR 2023]: NegCLIP is trained using negative mining of texts and images. Yuksekgonul et al. (2022) create sentence level hard negatives by swapping different linguistic elements. They also additionally include hard-negative images and their corresponding texts in the batch by fetching K nearest neighbours (K=3) for each image in the feature space constructed using a pretrained CLIP model. Teaching SVLC(Doveh et al., 2023) [CVPR 2023]: This method uses LLM\u2019s like BLOOM (Mitchell et al., May 2021-May 2022) along with rules to create additional positive and negative sentences for each image while fine-tuning CLIP. Syn-CLIP(Cascante-Bonilla et al., 2023) [Arxiv\n4https://github.com/mlfoundations/ open_clip\n2023]: Syn-CLIP uses a million scale synthetic dataset to fine-tune CLIP and improve it\u2019s performance on compositional reasoning tasks. The synthetic data is created using a 3D physics-based simulation platform built on Unity3D, called ThreeDWorld (Gan et al., 2021). This contemporary work is complementary to our data-centric approach and we believe our methods can help fine-tuning with synthetic datasets as well. Cascante-Bonilla et al. (2023) in their paper showed how dense and long captions can be obtained for synthetic images and which require splitting into sub-captons followed by averaging of features from all captions while fine-tuning CLIP. This is one avenue where we believe our method can be useful since our method inherently allows matching of images to multiple texts. This is part of future work, once such synthetic datasets are released and are easily available."
        },
        {
            "heading": "G Detailed Experimental Results",
            "text": "In the main paper Table 1 and Table 3 we had provided concise results for some datasets, based on lack of space due to extensive experimental results. Here we provide detailed results on these datasets:\nG.1 VL-Checklist: detailed results Detailed Fine-tuning results on VL-Checklist dataset are provided in Table 17. These are an extension to the VL-Checklist results provided in the main paper Table 1. Detailed Pre-training results for VL-Checklist dataset are provided in Table 18 which are an extension to the VL-Checklist results provided in the main paper Table 3."
        },
        {
            "heading": "G.2 SVO-Probes: detailed results",
            "text": "Detailed Fine-tuning results on SVO-Probes dataset are provided in Table 19. These are an extension to the SVO-Probes results provided in the main paper Table 1. Detailed Pre-training results for SVO-Probes dataset are provided in Table 20 which are an extension to the SVO-Probes results provided in the main paper Table 3."
        },
        {
            "heading": "G.3 CREPE-Systematicity: detailed results",
            "text": "Here we provide detailed results on CREPESystematicity dataset used for measuring systematic generalization. In the main paper we had only provided the results related to systematic generalization (i.e., the unseen split), but here we provide\nresults on both the seen and unseen split, for both hard negative retrieval sets (Comp and Atom) that are used when evaluating performance on CREPE by Ma et al. (2022). Detailed Fine-tuning results on CREPE-Systematicity dataset on both the seen and unseen splits are provided in Table 21. These are an extension to the CREPE-Systematicity results provided in the main paper Table 1. Detailed Pretraining results for CREPE-Systematicity dataset are provided in Table 22 which are an extension to the CREPE-Systematicity results provided in the main paper Table 3."
        },
        {
            "heading": "H Reproducibility",
            "text": "Here we provide necessary details to reproduce our work, that might not have been included in the main paper."
        },
        {
            "heading": "H.1 Training and hyperparameter details",
            "text": "Fine-tuning: For all fine-tuning experiments, we follow Yuksekgonul et al. (2022) for hyperparameters. In particular, all models are fine-tuned for 5 epochs, with a batch size of 256, using a cosine learning rate schedule with 50 steps of warmup and random-crop augmentation during training. AdamW is used for optimization. 1e \u2212 5 is used as the initial learning rate. Training is performed using 4 NVIDIA A100 GPUs for all models. From the ARO dataset, 10% examples from attribute and relation splits are used as validation examples, and the rest are used as the test set for all models. On all other datasets, we evaluate zero-shot performance. For MosaiCLIP, we find that sampling a maximum of 3 positive and 6 negative sub-graphs per image during fine-tuning gives the best result on the ARO validation set and hence is used in all our experiments (including pre-training experiments). For MosaiCLIP, we keep sub-graph sampling probabilities as p2 = p3. We vary p1 in {0, 0.08, 0.15} while fine-tuning on the randomly chosen YFCC dataset. We choose the best model according to the ARO val-set and keep the hyperparameters the same for all other fine-tuning datasets. Pre-training: For pre-training experiments, we follow the training protocol used in Yang et al. (2022); Radford et al. (2021). In particular, all models are trained for 32 epochs, with a batch size of 4096, using a cosine learning rate schedule with 5000 steps of warmup and random-crop augmentation during training. AdamW is used for optimization. The initial learning rate is 1e\u2212 3, and weight decay is\nset to 0.1. Training is performed using 64 NVIDIA A100 GPUs. NegCLIP\u2019s hard negative text creation method often results in no negative text for some texts in the pre-training dataset. Removing all such image-text pairs with no possible hard negative text results in poor performance for NegCLIP (due to fewer data to pre-train on). If we include these image-text pairs, the text batch size might differ for different GPUs since some image-text pairs are without hard negative texts and this causes instabilities. We hence keep a cache of sentences from previous batches and add it to the batch as negative examples so that all GPUs have the same text batch size during training. The same is done for MosaiCLIP since not all images might have the same number of unique positive and negative sub-graphs available. For NegCLIP we create hard negative sentences using code released by (Yuksek-\ngonul et al., 2022). For MosaiCLIP training, for each image, we always use one hard negative text createdusing NegCLIP\u2019s swapping technique, followed by positive and negative subgraphs created using our method. Sub-graph sampling probabilities are kept as p2 = p3, p1 = 0.15."
        },
        {
            "heading": "H.2 Tree-Score details:",
            "text": "Murty et al. (2022) devised a method to calculate the tree-score of a transformer over a given dataset of sentences D. This tree-score measures the functional tree-structuredness of a given transformer encoder. See Murty et al. (2022) for exact details for the algorithm to calculate the tree-scores. We use the code released by the authors5 for the purpose of calculating tree-scores for CLIP\u2019s language encoder. In practice we use 5K sentences from the COCO-validation set as the held ouot test set D over which we calculate the tree-scores.\n5https://github.com/MurtyShikhar/TreeProjections"
        },
        {
            "heading": "H.3 Computing Infrastructure and Run-Time:",
            "text": "We use NVIDIA A100 GPUs for all our experiments. Pre-training experiments took about 1.5 days per model while using 64 GPUs. Fine-tuning experiments on CC-FT, YFCC-FT and COCO took about 45 mins each and experiments on CC3M took 5 hours per model, while using 4 GPUs."
        },
        {
            "heading": "H.4 Model Parameters:",
            "text": "We use standrad CLIP models and as part of all models, is a transformer language encoder having 12 layers, 8 attention heads and 512 as it\u2019s width. For vision encoders we use 1. ResNet-50 hvaing 23M trainable parameters and 2. Transformer vision encoders a) Swin-Tiny with patch-size 4 and window size 7 following (Yang et al., 2022) and b) ViT-B-32 which has patch size 32, 12 layers and 12 attention heads."
        },
        {
            "heading": "H.5 Evaluation Metrics:",
            "text": "Strictly following the respective papers and released code6, for ARO, VL-Checklist, SVO we use accuracy as the metric as defined by the respecitve papers. And for CREPE-Productivty, and CREPE-Systematicity 7 we use Recall@1 as our metric of evaluation."
        },
        {
            "heading": "H.6 Summary Statistics of results:",
            "text": "We provide standard deviation results using 3 random seeds in Appendix Section D.2 for Fine-tuning experiments on the CC-FT dataset. For all other datasets, including the expensive pre-training runs we use a single seed for our experiments.\n6ARO: https://github.com/mertyg/ vision-language-models-are-bows, SVO-Probes https://github.com/deepmind/svo_probes, VL-Checklist: https://github.com/om-ai-lab/ VL-CheckList\n7CREPE Code: https://github.com/RAIVNLab/ CREPE\n(a) Finetuning data: CC100k\n(b) Finetuning data: COCO"
        }
    ],
    "title": "Coarse-to-Fine Contrastive Learning in Image-Text-Graph Space for Improved Vision-Language Compositionality",
    "year": 2023
}