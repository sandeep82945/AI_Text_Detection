{
    "abstractText": "The performance of automatic summarization models has improved dramatically in recent years. Yet, there is still a gap in meeting specific information needs of users in real-world scenarios, particularly when a targeted summary is sought, such as in the useful aspectbased summarization setting targeted in this paper. Previous datasets and studies for this setting have predominantly concentrated on a limited set of pre-defined aspects, focused solely on single document inputs, or relied on synthetic data. To advance research on more realistic scenarios, we introduce OPENASP, a benchmark for multi-document open aspect-based summarization. This benchmark is created using a novel and cost-effective annotation protocol, by which an open aspect dataset is derived from existing generic multi-document summarization datasets. We analyze the properties of OPENASP showcasing its high-quality content. Further, we show that the realistic open-aspect setting realized in OPENASP poses a challenge for current state-of-the-art summarization models, as well as for large language models.",
    "authors": [
        {
            "affiliations": [],
            "name": "Shmuel Amar"
        },
        {
            "affiliations": [],
            "name": "Liat Schiff"
        },
        {
            "affiliations": [],
            "name": "Ori Ernst"
        },
        {
            "affiliations": [],
            "name": "Asi Shefer"
        },
        {
            "affiliations": [],
            "name": "Ori Shapira"
        },
        {
            "affiliations": [],
            "name": "Ido Dagan"
        }
    ],
    "id": "SP:f14b8871d744aa9188630b0c8c72e78c8899687f",
    "references": [
        {
            "authors": [
                "Ojas Ahuja",
                "Jiacheng Xu",
                "Akshay Gupta",
                "Kevin Horecka",
                "Greg Durrett."
            ],
            "title": "ASPECTNEWS: Aspect-Oriented Summarization of News Documents",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1:",
            "year": 2022
        },
        {
            "authors": [
                "Reinald Kim Amplayo",
                "Stefanos Angelidis",
                "Mirella Lapata."
            ],
            "title": "Aspect-controllable opinion summarization",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6578\u20136593, Online and Punta Cana, Domini-",
            "year": 2021
        },
        {
            "authors": [
                "Stefanos Angelidis",
                "Reinald Kim Amplayo",
                "Yoshihiko Suhara",
                "Xiaolan Wang",
                "Mirella Lapata."
            ],
            "title": "Extractive opinion summarization in quantized transformer spaces",
            "venue": "Transactions of the Association for Computational Linguistics, 9:277\u2013293.",
            "year": 2021
        },
        {
            "authors": [
                "Stefanos Angelidis",
                "Mirella Lapata."
            ],
            "title": "Summarizing opinions: Aspect extraction meets sentiment prediction and they are both weakly supervised",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages",
            "year": 2018
        },
        {
            "authors": [
                "Tal Baumel",
                "Raphael Cohen",
                "Michael Elhadad."
            ],
            "title": "Topic concentration in query focused summarization datasets",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, 30(1).",
            "year": 2016
        },
        {
            "authors": [
                "David Carmel",
                "Elad Yom-Tov",
                "Adam Darlow",
                "Dan Pelleg"
            ],
            "title": "What Makes a Query Difficult",
            "venue": "In Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,",
            "year": 2006
        },
        {
            "authors": [
                "Hoa Trang Dang."
            ],
            "title": "Overview of duc 2005",
            "venue": "Proceedings of the document understanding conference, volume 2005, pages 1\u201312.",
            "year": 2005
        },
        {
            "authors": [
                "Ori Ernst",
                "Avi Caciularu",
                "Ori Shapira",
                "Ramakanth Pasunuru",
                "Mohit Bansal",
                "Jacob Goldberger",
                "Ido Dagan."
            ],
            "title": "Proposition-level clustering for multidocument summarization",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of",
            "year": 2022
        },
        {
            "authors": [
                "Alexander Fabbri",
                "Irene Li",
                "Tianwei She",
                "Suyi Li",
                "Dragomir Radev."
            ],
            "title": "Multi-news: A large-scale multi-document summarization dataset and abstractive hierarchical model",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational",
            "year": 2019
        },
        {
            "authors": [
                "Alexander R. Fabbri",
                "Wojciech Kry\u015bci\u0144ski",
                "Bryan McCann",
                "Caiming Xiong",
                "Richard Socher",
                "Dragomir Radev."
            ],
            "title": "SummEval: Re-evaluating summarization evaluation",
            "venue": "Transactions of the Association for Computational Linguistics, 9:391\u2013409.",
            "year": 2021
        },
        {
            "authors": [
                "Lea Frermann",
                "Alexandre Klementiev."
            ],
            "title": "Inducing document structure for aspect-based summarization",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6263\u20136273, Florence, Italy. Association for Compu-",
            "year": 2019
        },
        {
            "authors": [
                "Max Grusky",
                "Mor Naaman",
                "Yoav Artzi"
            ],
            "title": "Newsroom: A dataset of 1.3 million summaries with diverse extractive strategies",
            "venue": "In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics:",
            "year": 2018
        },
        {
            "authors": [
                "Hiroaki Hayashi",
                "Prashant Budania",
                "Peng Wang",
                "Chris Ackerson",
                "Raj Neervannan",
                "Graham Neubig."
            ],
            "title": "WikiAsp: A dataset for multi-domain aspectbased summarization",
            "venue": "Transactions of the Association for Computational Linguistics, 9:211\u2013225.",
            "year": 2021
        },
        {
            "authors": [
                "Karl Moritz Hermann",
                "Tomas Kocisky",
                "Edward Grefenstette",
                "Lasse Espeholt",
                "Will Kay",
                "Mustafa Suleyman",
                "Phil Blunsom."
            ],
            "title": "Teaching machines to read and comprehend",
            "venue": "Advances in Neural Information Processing Systems, volume 28. Curran Associates,",
            "year": 2015
        },
        {
            "authors": [
                "Minqing Hu",
                "Bing Liu."
            ],
            "title": "Mining and summarizing customer reviews",
            "venue": "Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201904, page 168\u2013177, New York, NY, USA. Association for",
            "year": 2004
        },
        {
            "authors": [
                "Sayali Kulkarni",
                "Sheide Chammas",
                "Wan Zhu",
                "Fei Sha",
                "Eugene Ie"
            ],
            "title": "Aquamuse: Automatically generating datasets for query-based multi-document summarization",
            "year": 2020
        },
        {
            "authors": [
                "Logan Lebanoff",
                "Kaiqiang Song",
                "Fei Liu."
            ],
            "title": "Adapting the neural encoder-decoder framework from single to multi-document summarization",
            "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages",
            "year": 2018
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Veselin Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "BART: Denoising sequence-to-sequence pre-training for natural language",
            "year": 2020
        },
        {
            "authors": [
                "Chin-Yew Lin."
            ],
            "title": "ROUGE: A package for automatic evaluation of summaries",
            "venue": "Text Summarization Branches Out, pages 74\u201381, Barcelona, Spain. Association for Computational Linguistics.",
            "year": 2004
        },
        {
            "authors": [
                "Elena Lloret",
                "Laura Plaza",
                "Ahmet Aker."
            ],
            "title": "Analyzing the capabilities of crowdsourcing services for text summarization",
            "venue": "Lang. Resour. Evaluation, 47(2):337\u2013369.",
            "year": 2013
        },
        {
            "authors": [
                "Rui Meng",
                "Khushboo Thaker",
                "Lei Zhang",
                "Yue Dong",
                "Xingdi Yuan",
                "Tong Wang",
                "Daqing He."
            ],
            "title": "Bringing structure into summaries: a faceted summarization dataset for long scientific documents",
            "venue": "Proceedings of the 59th Annual Meeting of the Asso-",
            "year": 2021
        },
        {
            "authors": [
                "Ramesh Nallapati",
                "Feifei Zhai",
                "Bowen Zhou."
            ],
            "title": "Summarunner: A recurrent neural network based sequence model for extractive summarization of documents",
            "venue": "Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, AAAI\u201917, page",
            "year": 2017
        },
        {
            "authors": [
                "Jianmo Ni",
                "Gustavo Hernandez Abrego",
                "Noah Constant",
                "Ji Ma",
                "Keith Hall",
                "Daniel Cer",
                "Yinfei Yang."
            ],
            "title": "Sentence-t5: Scalable sentence encoders from pretrained text-to-text models",
            "venue": "Findings of the Association for Computational Linguistics: ACL 2022,",
            "year": 2022
        },
        {
            "authors": [
                "NIST."
            ],
            "title": "Document Understanding Conferences",
            "venue": "https://duc.nist.gov/. Accessed: 2023-06-01.",
            "year": 2002
        },
        {
            "authors": [
                "OpenAI."
            ],
            "title": "Models - OpenAI API",
            "venue": "openai.com. Accessed: 2023-06-01.",
            "year": 2023
        },
        {
            "authors": [
                "Paul Roit",
                "Ayal Klein",
                "Daniela Stepanov",
                "Jonathan Mamou",
                "Julian Michael",
                "Gabriel Stanovsky",
                "Luke Zettlemoyer",
                "Ido Dagan."
            ],
            "title": "Controlled crowdsourcing for high-quality QA-SRL annotation",
            "venue": "Proceedings of the 58th Annual Meeting of the As-",
            "year": 2020
        },
        {
            "authors": [
                "Ori Shapira",
                "Ran Levy"
            ],
            "title": "Massive MultiDocument Summarization of Product Reviews with Weak Supervision",
            "year": 2020
        },
        {
            "authors": [
                "Bowen Tan",
                "Lianhui Qin",
                "Eric Xing",
                "Zhiting Hu."
            ],
            "title": "Summarizing text on any aspects: A knowledge-informed weakly-supervised approach",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
            "year": 2020
        },
        {
            "authors": [
                "Ivan Titov",
                "Ryan McDonald."
            ],
            "title": "A joint model of text and aspect ratings for sentiment summarization",
            "venue": "Proceedings of ACL-08: HLT, pages 308\u2013316, Columbus, Ohio. Association for Computational Linguistics.",
            "year": 2008
        },
        {
            "authors": [
                "Alex Wang",
                "Richard Yuanzhe Pang",
                "Angelica Chen",
                "Jason Phang",
                "Samuel R. Bowman."
            ],
            "title": "SQuALITY: Building a long-document summarization dataset the hard way",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language",
            "year": 2022
        },
        {
            "authors": [
                "Mark E. Whiting",
                "Grant Hugh",
                "Michael S. Bernstein."
            ],
            "title": "Fair work: Crowd work minimum wage with one line of code",
            "venue": "Proceedings of the AAAI Conference on Human Computation and Crowdsourcing, 7(1):197\u2013206.",
            "year": 2019
        },
        {
            "authors": [
                "Ruben Wolhandler",
                "Arie Cattan",
                "Ori Ernst",
                "Ido Dagan"
            ],
            "title": "How \u201cmulti\u201d is multi-document summarization",
            "venue": "In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2022
        },
        {
            "authors": [
                "Wen Xiao",
                "Iz Beltagy",
                "Giuseppe Carenini",
                "Arman Cohan"
            ],
            "title": "PRIMERA: Pyramid-based masked",
            "year": 2022
        },
        {
            "authors": [
                "Xianjun Yang",
                "Kaiqiang Song",
                "Sangwoo Cho",
                "Xiaoyang Wang",
                "Xiaoman Pan",
                "Linda Petzold",
                "Dong Yu."
            ],
            "title": "Oasum: Large-scale open domain aspect-based summarization",
            "venue": "arXiv preprint arXiv:2212.09233.",
            "year": 2022
        },
        {
            "authors": [
                "Haopeng Zhang",
                "Xiao Liu",
                "Jiawei Zhang"
            ],
            "title": "Summit: Iterative text summarization via chatgpt",
            "year": 2023
        },
        {
            "authors": [
                "Ming Zhong",
                "Da Yin",
                "Tao Yu",
                "Ahmad Zaidi",
                "Mutethia Mutuma",
                "Rahul Jha",
                "Ahmed Hassan Awadallah",
                "Asli Celikyilmaz",
                "Yang Liu",
                "Xipeng Qiu",
                "Dragomir Radev."
            ],
            "title": "QMSum: A new benchmark for querybased multi-domain meeting summarization",
            "venue": "Pro-",
            "year": 2021
        },
        {
            "authors": [
                "Following Hayashi"
            ],
            "title": "2021) the oracle sentence selector is implemented as follows. Sentences from the document sets are aggregated to maximize the average of ROUGE-1 and ROUGE-2 F-1 scores against the reference aspect-based",
            "year": 2021
        },
        {
            "authors": [
                "document-sets. D"
            ],
            "title": "Content Diversity Content diversity (Grusky et al., 2018; Fabbri et al., 2019) is a joint measure for extractiveness",
            "year": 2019
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "When faced with a large body of text, a summary is an effective means to get a concise version of the salient content. However, informational needs of users vary, calling for summarizers that can focus a summary around a given request. The summarization community has addressed this demand mainly through query-focused summarization (QFS) and aspect-based summarization (ABS). Accordingly, several datasets and benchmarks have been compiled over time to enable research on these tasks (see Table 1).\n\u2217Equal contribution. \u2020Part of the research was conducted during an internship at One AI. \u2021Work done in cooperation with Bar-Ilan University (external and not related to the author\u2019s work at Amazon).\nIn QFS, a query is highly flexible and can target specific information within particular text. In contrast, ABS datasets traditionally predefined small sets of generic subtopics within a common topical category on which aspect-based summaries are generated, such as geography and recovery aspects in any article about an earthquake (Amplayo et al., 2021). Open-ABS (OABS; Tan et al., 2020), allows aspects to differ for each source text, yet still just as subtopics in the text.\nCollecting datasets for these tasks is a major obstacle. This kind of data does not naturally occur in available resources, and manually annotating is highly burdening, particularly for the multidocument setting. Indeed, the very few existing OABS datasets are synthetically gathered. In addition, they address a single-document setting, even though real-world scenarios involve information\nnavigation within multiple documents on a topic. In this work, we first propose a novel efficient protocol to manually derive high-quality multidocument open aspect-based summaries from standard multi-document summarization (MDS) datasets. Through crowdsourcing, open aspects and their summaries are extracted from generic reference summaries.\nApplying our protocol, we present OPENASP, an OABS dataset for the multi-document setting.1 The aspects and respective summaries are based on the prominent DUC (NIST, 2002) and MultiNews (Fabbri et al., 2019) datasets. See Figure 1 for an example of open-aspect-based summaries extracted from a generic summary. The dataset contains 1,310 aspect-based summaries, split to train, validation and test sets, enabling methodological modeling for the task. We further implement and analyze several baseline models that demonstrate the challenging nature of the task and dataset, even for recent high-performing large language models."
        },
        {
            "heading": "2 Background",
            "text": "QFS is a long-standing task that addresses the need to summarize around a specified user request (Dang, 2005). A query is inherently fluid, allowing great variance in length, specificity and format (Carmel et al., 2006). Corresponding datasets, such as those at the top of Table 1, mainly evolved around queries for specific information needs within the source text(s). Wang et al. (2022),\n1The OPENASP dataset is available at https://github. com/liatschiff/OpenAsp.\nfor instance, focused summaries around questions targeting distinct matters within the document, e.g. summarizing around \u201cWhat is the CPA and what does it do?\u201d in a certain story.\nEarly research on ABS (Hu and Liu, 2004; Titov and McDonald, 2008) recognized the need for more structured information around subtopics of the source text. Relevant datasets (middle of Table 1) focus on recurring aspects within a domain, and approach this by pre-defining a fixed list of aspects, e.g., \u201cservice\u201d and \u201cprice\u201d in all restaurant reviews. Some datasets expanded to multidocument inputs (Angelidis et al., 2021; Hayashi et al., 2021) which is a more realistic setting when seeking topical information.\nAn additional direction stemming from ABS permits open aspects that still concisely target subtopics, but can be unique for the individual input text (bottom of Table 1, with more details in Table 12 in the appendix). The existing OABS datasets, namely AnyAspect (Tan et al., 2020) and OASUM (Yang et al., 2022), are synthetically compiled and only address single document inputs. In AnyAspect, the named entities within the document are considered the aspects, and the corresponding summaries include any source sentence mentioning the respective entity. In OASUM, the aspects and the summaries were extracted from Wikipedia articles, where Wikipedia sub-titles are the aspects and their summaries are automatically extracted from the article\u2019s abstract section via a greedy lexical similarity method. Applying such artificial methods yield lower-quality input documents, aspects and expected output summaries.\nCollecting datasets for the ABS task poses a challenging undertaking that includes reading large input texts and writing out the focused summaries. Unlike in generic summarization, where large sources of summaries can be scraped from the web (Hermann et al., 2015; Grusky et al., 2018; Fabbri et al., 2019), summaries for our setting are not generally available. Meanwhile, manually writing high quality aspect-based summaries from scratch is an expensive labor-intensive task. Wang et al. (2022) reported 20\u201340 minutes just for reading a 3000\u20136000 word story. Summarizing multi-document sets is even more complex since the total input-length may be much larger (e.g., tens of thousands of words; see Section 6.2), while information-overlap further requires content consolidation by the summarizer. For instance, Dang (2005) reported 5 hours of labor for generating one multi-document summary by an expert. Employing crowdsource workers, on the other hand, has been shown to lead to poor extractive multi-document summaries (Lloret et al., 2013). Our protocol exploits existing MDS benchmarks and applies controlled crowdsourcing (Roit et al., 2020), and is hence substantially cheaper and more efficient than previous manual collection processes.\nOur OPENASP dataset addresses all the above issues by supporting open-aspects in the multidocument setting, with manually annotated realistic summaries. Moreover, the annotation protocol can be applied across any available generic summarization dataset to produce even more like-quality aspect-based summaries."
        },
        {
            "heading": "3 Task Formulation",
            "text": "Following prior work (Ahuja et al., 2022; Yang et al., 2022), given a set of texts about a topic, we define an aspect as a central theme within a topic. The aspect can be referred by certain phrases, denoted aspect labels. As an example, Research in Antarctica and Territorial claims are aspect labels of the Antarctica topic (see Figure 1).\nSimilar to previous work on ABS (Hayashi et al., 2021; Angelidis et al., 2021), our aspect label is short and concise. In contrast, our aspect definition is open allowing ad-hoc aspects with freeform labels, contrary to having pre-defined domainspecific aspects. Relative to a query in queryfocused summarization (QFS; Dang, 2005), which might specify a complex information need, our aspects are restricted to relevant subtopics. (Hayashi\net al., 2021; Angelidis et al., 2021; Angelidis and Lapata, 2018).\nThe OABS task definition follows previous work (Tan et al., 2020; Yang et al., 2022), and is extended to the multi-document setting as follows: Given a set of documents D on the same topic and an arbitrary aspect label a, the task is to output a short aspect-based summary Sa. The summary should consolidate salient information from the document set that is relevant to the aspect."
        },
        {
            "heading": "4 Annotation Protocol",
            "text": "As emphasized in Section 2, manually collecting aspect-based summaries is very costly. We propose a novel and cost-effective protocol for generating aspect-based multi-document summaries, executed through controlled crowdsourcing (Roit et al., 2020) and a specially-designed annotation tool (Figure 3 in the Appendix). The key idea of our protocol is the extraction of gold aspect-based summaries from generic summaries in existing MDS datasets. Notably, the process is accomplished by reading the generic summary text only, as described below, while saving the strenuous need to read the entire set of source documents and to write the aspect-based summary from scratch."
        },
        {
            "heading": "4.1 Collecting Aspects and Summaries",
            "text": "From an existing MDS dataset, we gather pairs consisting of a document set D and a respective generic summary G. An annotator reads G and identifies prominent aspects within it, specified by aspect labels a1, a2, ..., am. For each identified aspect label ai, the annotator selects the relevant sentences from G. The concatenation of these sentences, retaining the original sentence-order from G, produces the corresponding aspect-based summary Sai . Accordingly, we establish m new aspect-based summaries for D as instances for the dataset. Notice that a summary is abstractive with respect to D, being comprised of sentences from the abstractive generic reference summary.\nIn our process, we favor extraction of fewer but high quality aspects from a generic summary. Specifically, our protocol instructs annotators to detect the aspects that are central in the generic summary, and to avoid circumstantial aspects. Although our protocol does not exhaustively extract aspects for the topic, the main sub-topics found in the generic summary establish a reliable and sufficient sample of aspects for addressing the multi-\ndocument open ABS task, for training and evaluating models. The full annotation guidelines appear in Appendix A.\nCritically, the described protocol avoids reading through the full document set and writing text for the summary. Instead, each aspect summary comprises a subset of generic summary sentences. We suggest that summary quality is maintained since the extracted summaries are based on dependable generic gold summary sentences. The validity of our protocol is based on two assumptions: (1) the aspect-related sentences extracted from generic summaries cover well the prominent information about the aspect within the full source documentset; (2) the aspect-based summaries preserve the coherence borrowed from the source summaries. We show that these assumptions indeed hold by assessing our collected dataset in Section 6.1."
        },
        {
            "heading": "4.2 Curation Phase",
            "text": "We propose an optional curation phase for cleaning the annotated aspect labels and corresponding summaries. The process encompasses a manual review, by an expert, of the aspect label and aspect-based summary only. The reviewer can edit the aspect label, remove irrelevant sentences from the summary, or completely reject the aspect. Similar to the annotation protocol, the curation phase avoids the expensive task of reading the source documents."
        },
        {
            "heading": "5 The OPENASP Dataset",
            "text": ""
        },
        {
            "heading": "5.1 Source Data",
            "text": "We exploit 2 prominent MDS datasets that contain reference summaries with at least 200 words to demonstrate our protocol robustness: DUC,2 a high-quality and expert-annotated dataset, and MultiNews (Fabbri et al., 2019), with data scraped from newser.com. For MultiNews, we automati-\n2duc.nist.gov; we use DUC 2006-07, and DUC 2001-02 task 2 (that contain 200-word summaries).\ncally filtered out samples with invalid source documents, to avoid consequential hallucinations in the summaries (see Appendix D.2). The large scale of MultiNews allowed further filtration to capture only instances with summaries of 350\u2013880 words, to increase the potential yield of aspect-based summaries. For all source data, we excluded documentset instances that discuss topics presented as a list of related events (e.g., daily news briefs or various unrelated incidents of the same kind), since the generic summaries of such instances typically contain few subtopics, if any."
        },
        {
            "heading": "5.2 Dataset Collection",
            "text": "We followed the annotation protocol described in Section 4.1. Specifically, we used controlled crowdsourcing (Roit et al., 2020) for selecting 3 annotators on Amazon Mechanical Turk3 that successfully completed an introductory summary annotation task and correctly answered followup questions on the task guidelines.4\nOur workers annotated 236 generic summaries from MultiNews and 208 from DUC. From a total of 444 generic summaries, annotators extracted 1,455 aspect-based multi-document summaries. We (paper authors) then applied the curation procedure (Section 4.2) on 1,173 aspect based summaries as detailed in Appendix A.2.5 Out of the reviewed summaries, we modified 152 aspect labels, edited sentence choice of 48 summaries, and completely rejected 94 aspect based summaries (92% pass rate). Overall, we gathered 1,361 summaries for OPENASP, averaging 3 aspect-based summaries per topic (document set instance), and costing \u223c$0.5 per summary.\nWe split OPENASP into train, validation and test sets, keeping the original MultiNews splits and splitting DUC datasets by years (Appendix D.1). We set aside 51 summaries (from 16 topics) from the test and validation sets, denoted analysis-test and analysis-val sets, for quality assessment and modeling (Sections 6 and 8). Statistics on the final OPENASP sizes appear in Table 2.\n3https://www.mturk.com/ 4Workers were paid $0.9 and $0.6 bonus per task with an average task completion time of about 6 minutes, resulting in $15.00/hr as recommended by Whiting et al., 2019.\n5Staring from the test and validation sets, and moving on to the train sets. We eventually excluded the MultiNews train set instances in the curation process as pass rates for the other sets were high enough."
        },
        {
            "heading": "6 OPENASP Assessment",
            "text": "We next examine the quality of the collected data, and then analyze its properties."
        },
        {
            "heading": "6.1 Dataset Quality",
            "text": "We applied a manual evaluation process to verify the collected summaries\u2019 expected qualities. Following Fabbri et al. (2021), a summary should be measured for 4 quality criteria: (1) relevance, the selection of important content from the source; (2) coherence, the quality of the collective structure of all sentences; (3) consistency, the factual alignment between the summary and the summarized source; and (4) fluency, the linguistic quality of individual sentences. In the OABS setting, the aspect-relevance is an additional expected quality (Amplayo et al., 2021; Angelidis et al., 2021). This criterion inspects whether the summary includes information that is relevant to the paired aspect.\nWe assess the 5 quality criteria on 20 aspectbased summaries sampled from the analysis-test set (Section 5.2). A summary was rated on a 1\u20135 scale for each criterion by one expert and reviewed by another. In case of a disagreement, the two raters resolved the dispute through reconciliation.6\nThe relevance and consistency criteria require comparison of the evaluated summary against the aspect-relevant information across the source document-set. Therefore, for each aspect in the analysis sets, we extracted (via crowdsourcing) all the sentences in the corresponding document-set related to the aspect.7\nThe average ratings can be found in Table 3. The high relevance score of 4.6 supports our first extraction protocol assumption (Section 4.1) that the aspect-based summaries cover the most important information about the subtopic, even though they originate from generic reference summaries. Consistency is expectedly sturdy as well, since sentences are copied from gold generic summaries. Hence, consistency issues that are not already present in the generic summaries should not be introduced. Similarly, the fluency of sentences is adopted from that of the source reference summaries, which is almost flawless.\n6The reviewer-to-rater agreement was 0.753 linear weighted Cohen\u2019s Kappa, and the reviewer-to-reconciled agreement was 0.847, indicating \u201csubstantial\u201d (0.6\u20130.8) and \u201calmost perfect\u201d (0.8-1.0) agreement respectively.\n7We measured 0.642 Cohen\u2019s Kappa for inter-rater agreement of per sentence aspect label, indicating \u201csubstantial\u201d agreement."
        },
        {
            "heading": "Coherence Consistency Fluency Relevance Aspect Rel.",
            "text": ""
        },
        {
            "heading": "4.7 (0.92) 4.8 (0.55) 5.0 (0.22) 4.6 (0.68) 4.7 (0.57)",
            "text": "Although summaries that are extracted from another text can easily suffer from coherence issues, this is rarely the case with our protocol. We found that 86% of consecutive sentence pairs in our aspect-based summaries are respectively consecutive in the generic summary. Specifically, 60% of the aspect-based summaries are full continuous sentence sets from the generic summary. This phenomenon occurred naturally during annotation, without explicit instructions to follow such a principle. Consequently, coherence is also maintained according to the source generic summaries. Overall, coherence scored very high as well, validating our second assumption that our protocol generates coherent aspect-based summaries even as they are based on generic summaries.\nTo empirically corroborate the quality of the aspects, we statistically analyzed the source sentences corresponding to each of the 51 aspects in our analysis sets. We found that, on average, an aspect relates to 13.5% of all source documents\u2019 sentences and appears in 68% of the source documents, indicating its topical dominance. Furthermore, only 11% of all aspect-related sentences refer to more than one aspect, indicating the high level of distinctness of the aspects. Finally, on average, 40% of all source sentences in a document set (topic) are related to at least one of the aspects extracted for that topic, indicating the substantial coverage of the set of aspects with respect to the entire document set content. Taken together, these findings establish that the collected aspects are central to the topic, covered thoroughly by its documents and collectively cover a main portion of its content.\nOverall, the aspect-based summaries representing OPENASP were determined to exhibit high reliability for the OABS task, consistent with that of the standard generic MDS datasets from which they were extracted."
        },
        {
            "heading": "6.2 Dataset Analysis",
            "text": "We discuss properties of OPENASP that emphasize its underlying diversity from several angles. Details for these analyses are available in Appendix D.\nThe input lengths, measured as the aggregated\ndocument lengths in a topic, varies from several hundred to tens of thousands of tokens (Figure 11 in the Appendix), averaging 7,930 words. The summary length ranges from tens to hundreds of tokens, with a median input-to-output (compression) ratio of 69:1.\nA topic contains 1\u20137 aspects with an average of 3.1 aspects per topic (see Figure 8 in the appendix). The aspect labels are almost all lexically unique, repeating on average 1.1 times throughout the dataset. The aspect label is 1\u201310 words, averaging 3.6 words. Some examples of topics and aspects from OPENASP appear in Table 15. Since aspect labels in OPENASP are annotated as subtopics of their corresponding topic, and are flexibly scripted, the aspects naturally vary widely.\nDue to our annotation protocol, the summaryabstractiveness in source datasets (DUC and MultiNews) is transferred to the summaries in OPENASP. Accordingly, the aspect-based summaries exhibit varying extents of abstractivness, as apparent in the diversity plot in Figure 2 (Grusky et al., 2018; Fabbri et al., 2019). Consequently, OPENASP requires models to perform well on both extractive and abstractive forms of summarization.\nFinally, a topic consists of 2\u201325 documents, with an average of 10.4 documents per topic (Figure 8 in the appendix). Following Wolhandler et al. (2022), we find that the aspect-based summaries rely on a varying number of corresponding source documents (Figure 9 in the appendix). In practice, this means some summaries require handpicking information from specific documents, while others require consolidating information from across the input document set.\nOverall, the analyzed properties expressly show the diversity of OPENASP and the ensuing challenges of the task."
        },
        {
            "heading": "7 Baseline Models",
            "text": "In this and the subsequent sections we demonstrate the challenges that our dataset lays out for summarization models, and suggest initial directions to cope with these challenges. A major hurdle to overcome is the large input length of a document set, averaging \u223c8K tokens in our data, and stretching to \u223c30K (Section 6.2). Even with current advancements made to support growing input sizes, properly attending to relevant information in a large input remains a hurdle. There were no feasible models available to us that would fit all of\nour document-sets. To cope with this we investigate two common schemas used in the MDS setting: (a) Filter-thensummarize (e.g., Wang et al., 2022; Baumel et al., 2016), runs a sentence selector to extract aspectrelated sentences, and passes them on to a summarization model (7.1); (b) Recursive summarization (e.g. Shapira and Levy, 2020) summarises subsets of documents separately, and then summarizes these summaries (7.2)."
        },
        {
            "heading": "7.1 Filter-then-Summarize",
            "text": "Sentence selection. To slim down the input, one remedy is to select sub-texts that are more likely to be included in a summary. In MDS in the news domain, the conventional technique for this is the Lead method, which extracts a number of sentences from the beginning of each document, since these often contain the most crucial information in a news report.\nHowever, in the case of ABS, there is no guarantee that salient aspect-specific information appears at the beginning of each document. Standard ABS models with a closed set of aspects can train a classifier to map a sentence to an aspect (e.g. Hayashi et al., 2021). In contrast, our open-aspect setting demands a selector that is robust to any aspect. We leverage the Sentence-T5 model (Ni et al., 2022) as an unsupervised sentence selector. Specifically, a cosine similarity score is computed between the\ndense vectors of an aspect label and a sentence, producing a ranking of aspect-relevant sentences.\nFor the summarization models we employ in the subsequent stage, we restrict the applied sentence selector to provide a set of sentences that fit within the input-size limit (1K or 4K tokens). An approximately equal number of sentences from each document is used within the limit.\nSummarization. For the second stage of the decomposed procedure, we employ three architecturally different summarization models. The first two are sequence-to-sequence models trained for generic summarization: BART (Lewis et al., 2020) was pre-trained on the CNN/DailyMail dataset (Hermann et al., 2015) for single-document summarization, while PRIMERA (Xiao et al., 2022) was trained on the MultiNews dataset (Fabbri et al., 2019) for the multi-document setting. Accordingly, the former has a limit of 1K tokens, and the latter 4K. While PRIMERA is more suitable for our multi-document setting, BART has shown strong performance on single-document ABS (Tan et al., 2020; Meng et al., 2021; Wang et al., 2022) and is a worthy candidate. BART is adjusted for multidocument inputs with separation tokens between documents, following PRIMERA. We further finetuned the two models with the OPENASP train set, denoted BARTSumm and PRIMERASumm, respectively.8\nIn addition, we experiment with OpenAI\u2019s promising ChatGPT LLM, based on gpt-3.5-turbo-0301 (OpenAI, 2023), denoted ChatGPTSumm. Such prompt-based models have demonstrated promising results on the zero-shot setting for summarization (Zhang et al., 2023), which we apply here as well. See Appendix B.2 for technical details."
        },
        {
            "heading": "7.2 Recursive Summarization",
            "text": "A different approach to handle the large input size is summarizing subsets of the input that fit the model size limit. The subset summaries are then recursively summarized to generate the final summary (see details in the Appendix B.2). We experiment with ChatGPT on a 4K or 16K input token-size, denoted ChatGPTRecursive, whose robustness enables the recursive scheme without any fine-tuning. About 75% of our test instances re-\n8We concatenate documents (only with the select sentences from the first stage) with a separator, with the aspect at the beginning and end of the input (inspired by Wang et al., 2022).\nquire recursion for the 4K limit, however only about 10% don\u2019t fit within the 16K limit, rendering nearly end-to-end summarization on our test set.\nFor the sake of completeness, we also experimented with BART and PRIMERA in the recursive scheme, denoted BARTRecursive and PRIMERARecursive, respectively. Note that training for this approach would require gold aspectbased summaries for subsets of the document set, which are not available. We therefore activated, in the recursive technique, the BART and PRIMERA systems that were fine-tuned for the filter-thensummarize approach above (Section 7.1)."
        },
        {
            "heading": "8 Baseline Evaluation and Analysis",
            "text": "To assess the overall capabilities of our baseline models, we show an overall comparison of the methods (\u00a78.1), an ablation analysis on the BARTSumm-based configurations (\u00a78.2), and end with a human evaluation on our best baselines (\u00a78.3)."
        },
        {
            "heading": "8.1 Automatic Evaluation",
            "text": "Sentence selection. We first compare the Lead and Sentence-T5 sentence selectors, measuring the relevance of selected sentences to paired aspects. To that end, we utilized the sentence alignments (between aspects and document-set sentences) from the combined analysis-valid and analysis-test sets (Section 6.1), consisting of 1,782 aspect-sentence pairs. For the two sentence selectors, we selected sentences up to a cap of 1K or 4K tokens, and measured the F1 scores between the gold alignment pairs and the resulting selector\u2019s pairs, over all aspects. As expected, the results in Table 4 strongly favor Sentence-T5, which directly focuses on a given aspect label. The lead sentences from the documents are much less relevant to any given aspect.\nSummary quality. We next assess the aspectbased summaries of our baseline filter-thensummarize and recursive summarizers. We apply the commonly used ROUGE metrics (Lin, 2004), which measure the lexical overlap between the system and reference aspect-based summaries. Here we only apply the Sentence-T5 sentence selector in the filter-then-summarize configuration, as it is the better of the two selectors, as observed above.9\nTable 5 shows that ChatGPTSumm outperforms all other methods, including the recursive ChatGPT counterparts. This stresses the advantage of a preliminary selection of aspect-relevant sentences. In addition, it appears that shorter input lengths tend to yield better results, as illustrated by the two size-differences in each of the PRIMERASumm and ChatGPTRecursive models. Finally, the effectiveness of fine-tuning BARTSumm and PRIMERASumm with our train set is apparent, as these models are competitive with the relatively strong ChatGPTSumm model.\nWe also produced \u201cOracle\u201d extractive summaries for our data, generated greedily to maximize the average of ROUGE-1 and ROUGE-2 scores, given the reference summary (Nallapati et al., 2017). The large gap between the baseline and Oracle scores, as seen in Table 5, leaves much room for improvement in future work on our task."
        },
        {
            "heading": "8.2 Ablation Analysis",
            "text": "We provide insights regarding the contribution of different components in our summarization baselines, operating the BARTSumm-based models as a use-case (being the best of the two fine-tuned models). We refer to rows in Table 6 throughout the analysis.\n9We do not report results of the BARTRecursive and PRIMERARecursive systems here, since they are not directly trained for the task, and therefore not compatibly comparable. See Table 9 and Table 10 in the appendix for full results.\nAspect in input. We first observe what happens if the aspect label is left out from the input to the summarizing model. When using the Sentence-T5 selector, which already selects sentences relevant to the aspect, a very slight improvement in performance is achieved with the aspect in the input (row 1 vs. row 3). However, there is a much larger upgrade when the aspect is input with Lead sentences (row 2 vs. row 4). This indicates that simply providing the requested aspect in the input indeed trains BARTSumm to attend to aspect-relevant sentences.\nAspect-aware sentence selection. Inputting Lead sentences without the aspect is akin to generic multi-document summarization. Row 5 represents this setting without any fine-tuning, i.e., with the original BART model pre-trained for generic summarization. The large difference in scores with respect to the aspect-aware configurations suggests that the characteristics of our ABS data is distinct from the generic summarization task.\nOracle sentence selection. To estimate an upper bound for the BARTSumm summarizer, we devise an \u201cOracle\u201d that mimics a near optimal sentence selector, denoted OracleSel (not to be confused with the Oracle summary in Section 8.1). It greedily selects the sentences that maximize ROUGE against the reference aspect-based summary, at the allowed input size limit of \u223c1K tokens (Hayashi et al., 2021) (see Appendix B.3 for details). As shown in row 6, using OracleSel with BARTSumm produces substantially greater scores than the next best option, where the Sentence-T5 selector is applied (row 1). This stresses the potential of a good preliminary sentence selector when using the filter-then-summarize approach."
        },
        {
            "heading": "8.3 Human Evaluation",
            "text": "We conduct a manual evaluation on the summaries produced by the three top-scoring models: ChatGPTRecursive, ChatGPTSumm, and BARTSumm, with the latter two using the Sentence-T5 sentence selector. For 20 random instances from the OPENASP test set, we assessed: (1) Relevance to the aspect, i.e., \u201cis the target aspect adequately discussed in the system summary?\u201d; and (2) Relevance to the Reference summary, i.e., \u201cdoes the system summary refer to the information in the aspect-based reference summary?\u201d (Ernst et al., 2022; Lebanoff et al., 2018). Each criterion was rated on a 1\u20135 scale, 5 being best. The outcomes of the evaluation are presented in Table 7.\nThe ranking of relevance to the reference summary is consistent with the automatic score ranking (in Table 5). Importantly, the manual scores achieved are quite low (all 2\u20133). Furthermore, the models demonstrate varying levels of success in extracting aspect-relevant information, as indicated by the moderate aspect-relevancy scores and high standard deviations. Overall, these observations re-emphasize the challenges posed for models on the task."
        },
        {
            "heading": "9 Conclusion",
            "text": "Summarizing texts around an open-aspect is a basic necessity when consuming information. Our new OPENASP benchmark serves this demand, as the first open ABS dataset in the multi-document setting, with high-quality summaries collected via an efficient protocol. Our protocol overcomes the major hurdle of manually collecting summaries, by tapping into existing generic summaries in multidocument summarization datasets. Our proposed baselines, based on strong models, reveal the gap towards solving this task, posing a challenge even for the best current models. Overall, our efficient\ndata collection protocol can be expanded to supply even more data for real-world open-ABS and related information-seeking tasks."
        },
        {
            "heading": "Limitations",
            "text": "This study leverages existing generic multidocument summaries to generate aspect-based summaries by manually extracting aspect-related sentences. While this approach proved effective for the specific news datasets we used, it may not be readily applicable to different datasets where aspectrelated sentences from the summary may not accurately capture all the necessary information for that aspect. We assess this in our analyses, and recommend to do so on other potential datasets on which our protocol is applied.\nAlthough OPENASP contains a representative sample of aspects from the generic summaries, the overall distribution of aspect labels is sparse with a small fraction of repeating labels. This limits further analysis of aspect distribution or aspect discovery that we leave for future work.\nFurthermore, the usage of ChatGPT raises certain concerns despite its popularity. Firstly, the lack of detailed documentation regarding ChatGPT\u2019s training procedure makes it challenging to determine the specific training-data used. This raises the possibility of contamination, where our test data might have been incorporated somehow into the training of ChatGPT.\nFinally, for our experiments, we employed specific prompts (detailed in the Appendix) to assess the capabilities of ChatGPT for our task. Although we attempted several prompts, it is important to note that other prompts could yield different outputs. Consequently, we cannot make definitive claims about the model\u2019s capabilities."
        },
        {
            "heading": "10 Ethics and Broader Impact",
            "text": "This paper is submitted in the wake of a tragic terrorist attack perpetrated by Hamas, which has left our nation profoundly devastated. On October 7, 2023, thousands of Palestinian terrorists infiltrated the Israeli border, launching a brutal assault on 22 Israeli villages. They methodically moved from home to home brutally torturing and murdering more than a thousand innocent lives, spanning from infants to the elderly. In addition to this horrifying loss of life, hundreds of civilians were abducted and taken to Gaza. The families of these abductees have been left in agonizing uncertainty, as no infor-\nmation, not even the status of their loved ones, has been disclosed by Hamas.\nThe heinous acts committed during this attack, which include acts such as shootings, sexual assaults, burnings, and beheadings, are beyond any justification.\nIn addition to the loss we suffered as a nation and as human beings due to this violence, many of us feel abandoned and betrayed by members of our research community who did not reach out and were even reluctant to publicly acknowledge the inhumanity and total immorality of these acts.\nWe fervently call for the immediate release of all those who have been taken hostage and urge the academic community to unite in condemnation of these unspeakable atrocities committed by Hamas, who claim to be acting in the name of the Palestinian people. We call all to join us in advocating for the prompt and safe return of the abductees, as we stand together in the pursuit of justice and peace."
        },
        {
            "heading": "11 Acknowledgements",
            "text": "This work was supported by the Israel Science Foundation (grant no. 2827/21), the Israel Ministry of Science and Technology, and One AI."
        },
        {
            "heading": "A OPENASP Collection Details",
            "text": ""
        },
        {
            "heading": "A.1 Annotation Interfaces",
            "text": "We present screenshots of the UI and instructions to extract aspect-based summaries for our dataset in Figures 3 and 4, respectively. The provided instructions assist annotators in accurately identifying aspects and extracting relevant information related to each of them.\nFurthermore, we show in Figure 5 a screenshot with the instructions provided to extract aspectrelated source sentences for the analysis-valid and analysis-test sets. The UI was similar to the version we used for creating our OpenAsp dataset except for the given aspects that couldn\u2019t be edited or selected as in OpenAsp. The annotators only selected the related sentences to each predefined aspect from a given source document."
        },
        {
            "heading": "A.2 Curation Phase Details",
            "text": "Four paper authors applied the curation phase as detailed in Section 4.2. We first defined and refined the curation guidelines (Figure 6) in a discussion before starting curation. Then, we split the data evenly among curators, by original MTurk annotators and by data source splits (DUC vs. MultiNews) to avoid personal biases in specific sub-parts of the data. Some curated samples are shown in Table 8. The released OPENASP dataset includes the original aspect labels and summaries, as well as the instances from after curation."
        },
        {
            "heading": "B Models Implementation Details",
            "text": ""
        },
        {
            "heading": "B.1 Fine-tuned Models",
            "text": "BART. We used a variant of the BART-large model with 406M parameters, fine-tuned on the CNN Daily Mail dataset10.\nPRIMERA. We utilized a variant of PRIMERA which was fine-tuned on the MultiNews dataset11 and includes 447M parameters We fine-tuned PRIMERA and BART on OPENASP train set on 2 V100 GPUs with the following hyper-parameters: learning rate of 10e-5, batch size of 1, gradient accumulation steps of 3, and 3 epochs."
        },
        {
            "heading": "B.2 ChatGPT",
            "text": "In Section B.2, we experimented with three models based on OpenAI\u2019s ChatGPT API - ChatGPTSumm, ChatGPTRecursive based on gpt-3.5-turbo-0301, while ChatGPT16kRecursive uses gpt-3.5-turbo-16k-0613. The temperature of all models is set to 0 for reproducibility.\nTo determine the appropriate prompt, we manually evaluated the aspect-based summaries generated during a brief manual tuning of the prompt text. The final prompt used for all ChatGPT models is presented in Figure 7.\nIn this paper, we used ChatGPT for summarization in two approaches, ChatGPTSumm and ChatGPTRecursive. ChatGPTSumm summarizes a reduced version of the original documents that fits within the model\u2019s input length limit. The reduced version is created using a sentence selection method (described in Section 7.1). The selected sentences from each document in the document set are then consecutively presented within the same prompt as individual entries (document#1: \u201c\u2018...\u201c\u2018, ... , document#X: \u201c\u2018...\u201c\u2018).\nFor ChatGPTRecursive and ChatGPT-16kRecursive models, full documents are concatenated (separated by document title document#1:) until reaching the input length limit. Then the model generates a summary for the first portion of sentences. This process is repeated until all documents are summarized once. Then, the model gets all summaries with the same prompt, and summarizes the summaries to produce the final summary.\n10https://huggingface.co/facebook/bart-large-cnn 11https://huggingface.co/allenai/PRIMERA-multinews"
        },
        {
            "heading": "B.3 Oracle Sentence-Selector Details",
            "text": "Following Hayashi et al. (2021) the oracle sentence selector is implemented as follows. Sentences from the document sets are aggregated to maximize the average of ROUGE-1 and ROUGE-2 F-1 scores against the reference aspect-based summaries. Once the score no longer improves, the selected sentences are omitted from the source, and the process is repeated again. This continues until the input size limit is reached, e.g., 1K tokens for BART."
        },
        {
            "heading": "C Model results",
            "text": "Table 9 presents the complete set of experiments conducted on our baseline models in the Filterthen-Summarize technique, employing the Lead, Sentence-T5, and Oracle sentence selectors.\nTable 10 presents the experiments conducted on our baseline models in the Recursive summarization technique. As a reminder from Section 7.2, BARTRecursive and PRIMERARecursive were finetuned with the Filter-then-Summarize approach, which can use three different sentence selectors\n(Lead, Sent-T5 and OracleSel). In Table 10 it is apparent that the \u2018Lead\u2019 selector provides superior results over the other two sentence selectors. \u2018Lead\u2019 sentences are less focused on aspect-specific information (as revealed in Section 8.1). We can hence assume that this characteristic encourages the model to focus more on the aspect during training, and consequently to perform better during inference."
        },
        {
            "heading": "C.1 System Summary Examples",
            "text": "Tables 13 and 14 present the aspect-based summaries generated by varied models for 2 different aspects, \u2019Launch into orbit\u2019 and \u2019Reasons for high unemployment rates\u2019, respectively. The corresponding reference summaries appear in the button line."
        },
        {
            "heading": "D OPENASP Details",
            "text": ""
        },
        {
            "heading": "D.1 OPENASP Source Splits",
            "text": "We split OPENASP dataset into train, validation and test sets based on the source datasets they originated from (see Table 11). For aspect-based sum-\nmaries originated from MultiNews, we followed the original splits, for summaries originated from DUC, we separated the test years from the train and validation."
        },
        {
            "heading": "D.2 MultiNews Filtering",
            "text": "During annotation phase, we noticed several faulty source documents from MultiNews, probably due to failed crawling. We manually examine some suspicious short source documents, finding a few common phrases that imply the document retrieval failed. For example, \u201cThe seed for this crawl was a list of every host in the Wayback Machine This crawl was run at a level 1 (URLs including their embeds, plus the URLs of all outbound links including their embeds) The WARC files associated with this crawl are not currently available to the general public.\" and several similar cases.\nWe created a short list of such texts and automatically filtered all topics from MultiNews containing one or more source documents matching these strings."
        },
        {
            "heading": "D.3 Documents and Summaries Lengths",
            "text": "Figure 11, presents the distribution and cumulative distribution of token lengths in summaries and document-sets."
        },
        {
            "heading": "D.4 Content Diversity",
            "text": "Content diversity (Grusky et al., 2018; Fabbri et al., 2019) is a joint measure for extractiveness of cov-\nerage and density. Coverage is the percentage of words in the summary that are from the source article, and density is the average length of the extractive fragment to which each summary word belongs. While OPENASP\u2019s summaries are extracted from source generic summaries, the generic summaries themselves are at different levels of abstractiveness relative to their corresponding document set. Accordingly, the aspect-based summaries also exhibit varying abstractive extents. Figure 2 illustrates the distribution of coverage on the x-axis, and that of density on the y-axis. As shown, most extracts are short, however there are few cases of sentence-level extractions. Overall, there is a diversified balance of abstractiveness in the data. Figure 10, presents the content diversity graphs of OPENASP, separated by the source datasets DUC and MultiNews (a stratified version of Figure 2)."
        },
        {
            "heading": "D.5 Multi-document Coverage",
            "text": "Multi-document coverage quantifies the amount of documents in the document-set upon which a corresponding summary depends (Wolhandler et al., 2022). Figure 9 shows the multi-document coverage of OPENASP, compared to the source datasets. The coverage is derived in terms of propositional alignment (Ernst et al., 2022) between the highestmatching subset of k documents (x-axis) and the summary. The dispersion score, a function of the area-above-the-curve, is a measure of multidocument coverage, where a higher value means\nthere is higher document-diversity overall.\nThe dispersion score of OPENASP is 2.9, with standard deviation of 4.1, while DUC-2001/2 and DUC-2006/7 render higher dispersion scores of 10.2 and 16.5 respectively. This can be explained by the longer summaries in the latter datasets, \u223c2.5 times longer than OPENASP summaries. Moreover, a generic summary, as opposed to an aspect-based summary, is expected to cover several subtopics, and hence align with a wider range of source documents. Meanwhile, the high standard deviation\nin the dispersion score means there are still aspectbased summaries that align with a larger number of documents. Some summaries require handpicking information from specific documents, while others require consolidating information from across the document set."
        },
        {
            "heading": "D.6 Adding Topic Names",
            "text": "We add to each document-set in our dataset a topic name, which was selected as an additional side task. Specifically, we asked Mturk annotators to identify\nthe topic of each presented generic summary. This process helped us later to facilitate and quickly identify irrelevant aspects in the curation phase in Section 4.2."
        },
        {
            "heading": "D.7 Dataset Examples",
            "text": "Table 15 shows the aspects labels and the corresponding aspect-based summaries, which belong to 3 randomly selected different document sets in our OpenAsp. Each document set is represented by its topic name."
        },
        {
            "heading": "D.8 Details on OABS datasets",
            "text": "Table 12 presents more details regarding the OABS datasets from Table 1."
        },
        {
            "heading": "Avg # Tokens in Input",
            "text": ""
        }
    ],
    "title": "OPENASP: A Benchmark for Multi-document Open Aspect-based Summarization",
    "year": 2023
}