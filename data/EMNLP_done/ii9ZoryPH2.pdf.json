{
    "abstractText": "This study investigates machine translation between related languages i.e., languages within the same family that share linguistic characteristics such as word order and lexical similarity. Machine translation through few-shot prompting leverages a small set of translation pair examples to generate translations for test sentences. This procedure requires the model to learn how to generate translations while simultaneously ensuring that token ordering is maintained to produce a fluent and accurate translation. We propose that for related languages, the task of machine translation can be simplified by leveraging the monotonic alignment characteristic of such languages. We introduce DecoMT, a novel approach of fewshot prompting that decomposes the translation process into a sequence of word chunk translations. Through automatic and human evaluation conducted on multiple related language pairs across various language families, we demonstrate that our proposed approach of decomposed prompting surpasses multiple established few-shot baseline approaches. For example, DecoMT outperforms the strong fewshot prompting BLOOM model with an average improvement of 8 chrF++ scores across the examined languages.",
    "authors": [
        {
            "affiliations": [],
            "name": "Ratish Puduppully"
        },
        {
            "affiliations": [],
            "name": "Anoop Kunchukuttan"
        },
        {
            "affiliations": [],
            "name": "Raj Dabre"
        },
        {
            "affiliations": [],
            "name": "Ai Ti Aw"
        },
        {
            "affiliations": [],
            "name": "Nancy F. Chen"
        }
    ],
    "id": "SP:3d9cd637c306e7d445ffde0f9ecb3e7aa24c76c8",
    "references": [
        {
            "authors": [
                "Sweta Agrawal",
                "Chunting Zhou",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Marjan Ghazvininejad."
            ],
            "title": "Incontext examples selection for machine translation",
            "venue": "Findings of the Association for Computational Linguistics: ACL 2023, pages 8857\u20138873, Toronto,",
            "year": 2023
        },
        {
            "authors": [
                "Raj Dabre",
                "Chenhui Chu",
                "Anoop Kunchukuttan."
            ],
            "title": "A survey of multilingual neural machine translation",
            "venue": "ACM Comput. Surv., 53(5):99:1\u201399:38.",
            "year": 2021
        },
        {
            "authors": [
                "Raj Dabre",
                "Himani Shrotriya",
                "Anoop Kunchukuttan",
                "Ratish Puduppully",
                "Mitesh Khapra",
                "Pratyush Kumar."
            ],
            "title": "IndicBART: A pre-trained model for indic natural language generation",
            "venue": "Findings of the Association for Computational Linguistics: ACL 2022,",
            "year": 2022
        },
        {
            "authors": [
                "mand Joulin"
            ],
            "title": "Beyond english-centric multilingual machine translation",
            "venue": "Journal of Machine Learning Research,",
            "year": 2021
        },
        {
            "authors": [
                "Xavier Garcia",
                "Yamini Bansal",
                "Colin Cherry",
                "George F. Foster",
                "Maxim Krikun",
                "Melvin Johnson",
                "Orhan Firat."
            ],
            "title": "The unreasonable effectiveness of fewshot learning for machine translation",
            "venue": "International Conference on Machine Learning, ICML 2023,",
            "year": 2023
        },
        {
            "authors": [
                "Naman Goyal",
                "Cynthia Gao",
                "Vishrav Chaudhary",
                "PengJen Chen",
                "Guillaume Wenzek",
                "Da Ju",
                "Sanjana Krishnan",
                "Marc\u2019Aurelio Ranzato",
                "Francisco Guzm\u00e1n",
                "Angela Fan"
            ],
            "title": "The Flores-101 evaluation benchmark for low-resource and multilingual",
            "year": 2022
        },
        {
            "authors": [
                "Barry Haddow",
                "Rachel Bawden",
                "Antonio Valerio Miceli Barone",
                "Jindrich Helcl",
                "Alexandra Birch."
            ],
            "title": "Survey of low-resource machine translation",
            "venue": "Comput. Linguistics, 48(3):673\u2013732.",
            "year": 2022
        },
        {
            "authors": [
                "Wenxiang Jiao",
                "Wenxuan Wang",
                "Jen tse Huang",
                "Xing Wang",
                "Zhaopeng Tu"
            ],
            "title": "Is chatgpt a good translator? yes with gpt-4 as the engine",
            "year": 2023
        },
        {
            "authors": [
                "Tushar Khot",
                "Harsh Trivedi",
                "Matthew Finlayson",
                "Yao Fu",
                "Kyle Richardson",
                "Peter Clark",
                "Ashish Sabharwal"
            ],
            "title": "Decomposed prompting: A modular approach for solving complex tasks",
            "year": 2023
        },
        {
            "authors": [
                "Philipp Koehn."
            ],
            "title": "Statistical significance tests for machine translation evaluation",
            "venue": "Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, pages 388\u2013395, Barcelona, Spain. Association for Computational Linguistics.",
            "year": 2004
        },
        {
            "authors": [
                "Aswanth Kumar",
                "Anoop Kunchukuttan",
                "Ratish Puduppully",
                "Raj Dabre"
            ],
            "title": "In-context example selection for machine translation using multiple features",
            "year": 2023
        },
        {
            "authors": [
                "Daniel Licht",
                "Cynthia Gao",
                "Janice Lam",
                "Francisco Guzman",
                "Mona Diab",
                "Philipp Koehn."
            ],
            "title": "Consistent human evaluation of machine translation across language pairs",
            "venue": "Proceedings of the 15th biennial conference of the Association for Machine Transla-",
            "year": 2022
        },
        {
            "authors": [
                "moyer",
                "Zornitsa Kozareva",
                "Mona Diab",
                "Veselin Stoyanov",
                "Xian Li"
            ],
            "title": "Few-shot learning with multilingual generative language models",
            "venue": "In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2022
        },
        {
            "authors": [
                "Kishore Papineni",
                "Salim Roukos",
                "Todd Ward",
                "WeiJing Zhu."
            ],
            "title": "Bleu: a method for automatic evaluation of machine translation",
            "venue": "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311\u2013318, Philadelphia,",
            "year": 2002
        },
        {
            "authors": [
                "Ajay Patel",
                "Bryan Li",
                "Mohammad Sadegh Rasooli",
                "Noah Constant",
                "Colin Raffel",
                "Chris CallisonBurch."
            ],
            "title": "Bidirectional language models are also few-shot learners",
            "venue": "The Eleventh International Conference on Learning Representations.",
            "year": 2023
        },
        {
            "authors": [
                "Maja Popovi\u0107."
            ],
            "title": "chrF++: words helping character n-grams",
            "venue": "Proceedings of the Second Conference on Machine Translation, pages 612\u2013618, Copenhagen, Denmark. Association for Computational Linguistics.",
            "year": 2017
        },
        {
            "authors": [
                "Matt Post."
            ],
            "title": "A call for clarity in reporting BLEU scores",
            "venue": "Proceedings of the Third Conference on Machine Translation: Research Papers, pages 186\u2013 191, Brussels, Belgium. Association for Computational Linguistics.",
            "year": 2018
        },
        {
            "authors": [
                "Ofir Press",
                "Noah A. Smith."
            ],
            "title": "You may not need attention",
            "venue": "CoRR, abs/1810.13409.",
            "year": 2018
        },
        {
            "authors": [
                "Ratish Puduppully",
                "Yao Fu",
                "Mirella Lapata."
            ],
            "title": "Data-to-text generation with variational sequential planning",
            "venue": "Transactions of the Association for Computational Linguistics, 10:697\u2013715.",
            "year": 2022
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J. Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "J. Mach. Learn. Res., 21:140:1\u2013140:67.",
            "year": 2020
        },
        {
            "authors": [
                "Vivek Raghavan",
                "Anoop Kunchukuttan",
                "Pratyush Kumar",
                "Mitesh Shantadevi Khapra."
            ],
            "title": "Samanantar: The largest publicly available parallel corpora collection for 11 indic languages",
            "venue": "Transactions of the Association for Computational Linguistics, 10:145\u2013",
            "year": 2022
        },
        {
            "authors": [
                "Prem Natarajan"
            ],
            "title": "Alexatm 20b: Few-shot learning using a large-scale multilingual seq2seq model",
            "year": 2022
        },
        {
            "authors": [
                "Yi Tay",
                "Mostafa Dehghani",
                "Vinh Q. Tran",
                "Xavier Garcia",
                "Jason Wei",
                "Xuezhi Wang",
                "Hyung Won Chung",
                "Siamak Shakeri",
                "Dara Bahri",
                "Tal Schuster",
                "Huaixiu Steven Zheng",
                "Denny Zhou",
                "Neil Houlsby",
                "Donald Metzler"
            ],
            "title": "Ul2: Unifying language",
            "year": 2023
        },
        {
            "authors": [
                "Gao",
                "Vedanuj Goswami",
                "Francisco Guzm\u00e1n",
                "Philipp Koehn",
                "Alexandre Mourachko",
                "Christophe Ropers",
                "Safiyyah Saleem",
                "Holger Schwenk",
                "Jeff Wang"
            ],
            "title": "No language left behind: Scaling humancentered machine translation",
            "year": 2022
        },
        {
            "authors": [
                "David Vilar",
                "Markus Freitag",
                "Colin Cherry",
                "Jiaming Luo",
                "Viresh Ratnakar",
                "George Foster."
            ],
            "title": "Prompting PaLM for translation: Assessing strategies and performance",
            "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Lin-",
            "year": 2023
        },
        {
            "authors": [
                "BigScience Workshop",
                "Teven Le Scao",
                "Angela Fan"
            ],
            "title": "Bloom: A 176b-parameter open-access multilingual language model",
            "year": 2023
        },
        {
            "authors": [
                "Linting Xue",
                "Noah Constant",
                "Adam Roberts",
                "Mihir Kale",
                "Rami Al-Rfou",
                "Aditya Siddhant",
                "Aditya Barua",
                "Colin Raffel."
            ],
            "title": "mT5: A massively multilingual pre-trained text-to-text transformer",
            "venue": "Proceedings of the 2021 Conference of the North American Chap-",
            "year": 2021
        },
        {
            "authors": [
                "Biao Zhang",
                "Barry Haddow",
                "Alexandra Birch."
            ],
            "title": "Prompting large language model for machine translation: A case study",
            "venue": "Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research,",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "In this work, we focus on the translation between related languages, a vital aspect from both economic and social perspectives. A considerable amount of commercial activity and social interaction occur between neighboring regions speaking two related languages. In these situations, pivot translation via a third language, such as English, can prove inefficient due to two inference steps which can also cause cascading errors (Dabre et al., 2021). Instead, direct translation between related languages could\nsignificantly streamline trade and enhance social connections.\nRelated languages, often from the same family, share word order and lexical characteristics, leading to predominantly monotonic translations where word order is largely preserved. This is seen in languages like Hindi, Marathi, Malayalam, Tamil, Bengali, etc. from the Indian subcontinent, which follow a Subject-Object-Verb (SOV) structure. Similar monotonic translation relationships are also observed among other language pairs, such as Indonesian and Malay or Ukrainian and Russian.\nRecent work has shown the power of few-shot prompting with large language models (LLMs) for tasks like machine translation, summarization, and question answering (Lin et al., 2022; Workshop et al., 2023). In machine translation, this approach prompts an LLM with a handful of example pairs and a test example. This requires the model to generate translations while ensuring a fluent word ordering, a process that fails to account for any unique characteristics intrinsic to the languages involved. For instance, it neglects the monotonic alignment\u2014an integral trait evident in translations between related languages.\nLLMs are often biased towards English in their training data. For example, in mT5 (Xue et al., 2021), Hindi and Malayalam tokens represent just 0.8% and 0.07% respectively. This imbalance hinders LLM performance in tasks involving nonEnglish languages and English to non-English translations (Lin et al., 2022). In particular, for fewshot translation tasks between related languages, these models may not have encountered sufficient data in these languages. Overcoming these limitations can be achieved by incorporating inductive biases about related languages.\nRecently, Khot et al. (2023) introduced an approach known as decomposed prompting. This\ntechnique dissects a complex task into simpler, more manageable subtasks, each of which is addressed through few-shot prompting of LLMs.\nWe aim to enhance translations by harnessing the inductive bias of monotonicity in related languages. We posit that by relieving LLMs from implicit reordering and focusing on sub-sentence structures, more accurate translations, particularly in longer sentences, can be achieved. This leads us to propose a decomposed prompting approach, termed Decomposed Prompting for Machine Translation (DecoMT) (Figure 1), which splits an input sentence into chunks, translates each independently, and incrementally generates context-aware translations.\nWhile much of the existing research on prompting focuses on decoder-only LLMs, recent studies (Patel et al., 2023) show the potential of encoderdecoder models like mT5 (Xue et al., 2021) for such tasks. Our DecoMT approach builds upon this premise, utilizing the mT5 encoder-decoder LLM.\nThe following are our contributions:\n\u2022 We introduce Decomposed Prompting for MT\n(DecoMT), a novel approach that simplifies the translation task by dividing it into the incremental translation of word chunks.\n\u2022 We perform extensive evaluations on closely related languages from diverse language families, including pairs such as Hindi \u21c6 Marathi, Hindi \u21c6 Malayalam, Hindi \u21c6 Telugu, Hindi \u21c6 Gujarati, Indonesian \u21c6 Malay, Russian \u21c6 Ukrainian, and Spanish \u21c6 Portuguese.\n\u2022 We compare DecoMT against several robust baselines, including few-shot prompting of LLMs (Lin et al., 2022; Workshop et al., 2023), as well as sequential autoregressive prompting of bidirectional LLMs (Patel et al., 2023). We demonstrate that DecoMT delivers robust results when compared to these baselines, particularly outperforming them in scenarios involving low-resource languages.\nWe release code and model outputs on github 1."
        },
        {
            "heading": "2 Related Work",
            "text": "Few-shot Prompting for MT Few-shot prompting for MT leverages an autoregressive LLM, which is prompted with a small number of sentence pairs alongside their translations. The LLM then predicts the translation when provided with a test sentence. Examples of such LLMs include XGLM (Lin et al., 2022) and BLOOM (Workshop et al., 2023). We interchangeably refer to this approach as Standard Prompting.\nGarcia et al. (2023) have shown the effectiveness of few-shot prompting in machine translation. Yet, their method necessitates training a decoderonly LLM from scratch. In comparison, we use an off-the-shelf LLM, mT5, for DecoMT. A series of recent research delves into example selection for prompt construction (Vilar et al., 2023; Zhang et al., 2023; Kumar et al., 2023; Agrawal et al., 2023). In our method, we rely on a fixed set of examples for prompting. Jiao et al. (2023) analyzed machine translation using ChatGPT and found that ChatGPT\u2019s performance aligns closely with commercial translation systems when utilizing GPT-4. In the interest of reproducibility, our emphasis lies on publicly accessible LLMs like BLOOM and mT5.\n1https://github.com/ratishsp/DecoMT\nSequential Autoregressive Prompting Patel et al. (2023) introduced an approach for prompting bidirectional LLMs, such as mT5 (Xue et al., 2021). Their Sequential Autoregressive Prompting (SAP) method generates a token autoregressively, appends it back to the input, and predicts the subsequent token. They demonstrated that SAP outperforms traditional few-shot prompting for LLMs. Our method also leverages bidirectional LLMs. However, while they primarily exploit the autoregressive nature of these models, we further harness the bidirectional capability of LLMs to generate context-aware translations.\nDecomposed Prompting Khot et al. (2023) proposed decomposed prompting, an approach that breaks down complex tasks into simpler ones, each tackled using few-shot prompting of LLMs. We apply this prompting strategy to the task of machine translation between related languages.\nIncremental Generation In the field of datato-text generation, Puduppully et al. (2022) presented a strategy for document generation that decomposes the process into generating a sequence of paragraphs, interleaved with predicting a plan for each paragraph. Our DecoMT method can be viewed as an extension of this approach for the task of translating monotonically aligned sentences, where the plan is implicitly specified through the monotonic chunk alignment.\nPress and Smith (2018) proposed an eager translation approach, in which the model begins translating without having to wait until the entire sentence has been processed. Our DecoMT method shares this characteristic, as it similarly doesn\u2019t require the whole sentence to be available before initiating translation. However, unlike their method, DecoMT\u2019s translation units extend beyond a single token. Moreover, DecoMT incorporates a contextual translation phase where the translation of an independent chunk is further refined through infilling.\nMachine Translation for Low Resource Languages There have been studies on machine translation models for low-resource languages (Haddow et al., 2022; Team et al., 2022; Ramesh et al., 2022; AI4Bharat et al., 2023; Dabre et al., 2022). While most of these focus on translations between English and other languages, Fan et al. (2021) is notable for its emphasis on improving translations among non-English languages. Our\nresearch aligns with this direction, concentrating on translations between related languages, many of which are characterized as low-resource."
        },
        {
            "heading": "3 DecoMT",
            "text": "In this section, we present the DecoMT Approach, our technique for decomposed prompting in Machine Translation. Our method involves a two-stage translation process for word chunks: firstly, an independent translation stage where each chunk is translated in isolation; and secondly, a contextual translation stage where translation occurs while considering the surrounding context."
        },
        {
            "heading": "3.1 Employed Pretrained Model",
            "text": "In implementing DecoMT, we use the mT5 model (Xue et al., 2021), specifically the XL variant with 3.7 billion parameters. mT5 is an encoder-decoder model that is trained with a span-corruption objective. During the training process of mT5, random spans within the input text are replaced with placeholders such as \u27e8mask_0\u27e9, \u27e8mask_1\u27e9, and so forth. In the output text, these correspond to mask tokens followed by the respective spans that were substituted in the input. Just like in the case of T5 (Raffel et al., 2020), the spans being replaced during training are of lengths varying from 2 to 5 tokens.\nOne approach to machine translation with mT5 follows the Standard Prompting method, as depicted in Figure 2 (a) (Workshop et al., 2023; Lin et al., 2022). In this setup, the mT5 encoder receives an input sequence: source language label, source sentence, target language label, followed by a \u27e8mask\u27e9 token. The decoder then generates the translation. In our independent translation framework, we employ this technique to produce Mi from Hi, as depicted in Figure 1.\nAnother technique to utilize mT5 for translation is by leveraging its bidirectional infilling capability,\nas exhibited in Figure 2 (b). The prompt includes the source language label, source sentence, target language label and a partially masked translation. The mT5 decoder then generates the masked tokens. This specific approach is used in generating our contextual translations Ri as shown in Figure 1.\nDepending on where the \u27e8mask\u27e9 placeholder is inserted, the model will perform either text completion or infilling. It\u2019s important to note that a single mask can yield more than one token."
        },
        {
            "heading": "3.2 Creating Aligned Monotonic Translations through Human Annotation",
            "text": "We select the first five examples from the dev set of the FLORES dataset (Goyal et al., 2022). Each example consists of a pair of corresponding sentences in two different languages. Annotators are tasked to align these sentences in a monotonic manner, maintaining the same sequence of information. Importantly, annotators have the liberty to modify the sentences as required to achieve this."
        },
        {
            "heading": "3.3 Translation Model",
            "text": "Let x represent the input sentence and \u03b2 denote the number of chunks in x. We define y\u0302 as the preliminary translation of x, obtained by concatenating independently translated chunks. Furthermore, y represents the final translation, which is assembled from contextually translated chunks. For the purpose of simplification in our formulation, we omit the prompt template and focus on the translation of test examples.\nIn the case of independent translation, we make the assumption that each y\u0302i is only dependent on its corresponding xi, where i indicates the index of the chunk within a sentence. This is captured by the equation:\np(y\u0302|x) = \u03b2\u220f\ni=1\np(y\u0302i|xi) (1)\nIn the case of contextual translation, we parameterise y as dependent on x and y\u0302, represented as:\np(y|x, y\u0302) = p(y1y2 . . . y\u03b2|x1x2 . . . x\u03b2, y\u03021y\u03022 . . . y\u0302\u03b2) (2) We make a conditional independence assumption that, at any position i, yi is dependent on xi\u22121, xi, xi+1, the previous contextual translation yi\u22121, and the next independent translation y\u0302i+1. This assumption allows us to rewrite the joint probability\nas a product of conditional probabilities:\np(y|x, y\u0302) =p(y1|x1x2y\u03022)\n\u2217 \u03b2\u22121\u220f i=2 p(yi|xi\u22121xixi+1yi\u22121y\u0302i+1) \u2217 p(y\u03b2|x\u03b2\u22121x\u03b2y\u03b2\u22121)"
        },
        {
            "heading": "3.4 Prompt Construction",
            "text": "Our methodology employs few-shot prompting, a technique that allows an LLM to make predictions based on a limited number of examples. This section will elucidate the process of constructing prompts for independent and contextual translation. We utilize five examples for few-shot prompting.\nWord count in Each Chunk Let us consider the token count within each word chunk in both prompt templates and test examples. For the prompt templates, k and j denote the number of tokens in a word chunk for independent and contextual translation, respectively. Conversely, in a test example, m signifies the token count within a word chunk for independent translation.\nWe typically set k and j to 5 and 10, respectively. Nevertheless, the morphological richness of languages varies as a single token in one language might equate to several tokens in another. Hence, during the construction of prompt templates, we programmatically align each chunk fully with its translated equivalent, causing potential deviations from the standard values of 5 and 10 for k and j.\nLastly, we treat m as a hyperparameter, which is tuned using the FLORES development set.\nIndependent Translation Each translation example for independent translation (Figure 3) commences with \u201cTranslate from [Source language] to [Target language]:\u201d, followed by a line break, then \u201c[Source language]:\u201d and the first chunk of the source language sentence. Subsequently, we present \u201c[Target language]:\u201d and the corresponding translated chunk on a new line. This sequence is replicated for all the chunks in a sentence.\nUpon completing a sentence, we use a newline separator and proceed to the next example. This procedure is repeated for all five examples in the prompt template.\nIn the case of the test example, the prompt begins with \u201cTranslate from [Source language] to [Target language]:\u201d, followed by a line break and \u201c[Source language]:\u201d with a chunk from the source language. The subsequent line is \u201c[Target language]: \u27e8mask\u27e9\u201d.\nThe model\u2019s objective at this point is to predict the translation for the source language chunk.\nContextual Translation The prompt template for contextual translation (Figure 4) mirrors that of independent translation, with one key difference: the examples in prompt template are around twice as long as that of the lengths of examples in independent translation template prompt. In the test example for contextual translation, the prompt starts with \u201cTranslate from [Source language] to [Target language]:\u201d, followed by \u201c[Source language]:\u201d and a concatenation of three chunks from the source language.\nThe next line reads \u201c[Target language]: [previous contextual translation] \u27e8mask\u27e9 [next independent\ntranslation]\u201d. Here, the model\u2019s task is to infill the translation for the second source language chunk.\nAppendix A contains an example of independent and contextual translation prompt templates for translation between Indonesian and Malay."
        },
        {
            "heading": "3.5 Inference",
            "text": "Figure 1 provides an overview of our DecoMT approach. We omit the prompt template from the block diagram for simplicity. We segment the input sentence into multiple chunks, denoted as H1, H2, ..., Hi, Hi+1, Hi+2, ..., H\u03b2 , each comprising m tokens. We then independently translate each chunk into corresponding translations, labelled as M1, M2, ..., Mi, Mi+1, Mi+2, ..., M\u03b2 .\nThe key innovation in our approach lies in the contextual translation, which is performed incrementally for each chunk. Initially, we concatenate the first two chunks, H1 and H2, with the place-\nholder \u27e8mask\u27e9 and the translation of the second chunk M2. This forms the input to predict the first contextual translation, R1.\nSubsequently, we concatenate the first three chunks, H1, H2, and H3, with the contextual translation obtained from the previous step, R1, alongside the placeholder \u27e8mask\u27e9 and the translation of the third chunk, M3. This is used to predict the next contextual translation, R2.\nThis process is continued iteratively. At an intermediate step, the chunks Hi, Hi+1, and Hi+2, along with the previously computed contextual translation Ri, the placeholder \u27e8mask\u27e9, and the translation of the chunk Mi+2, are used to predict the next contextual translation, Ri+1.\nFinally, for the last chunk, the input is the concatenation of the penultimate and final chunks, H\u03b2\u22121 and H\u03b2 , the last computed contextual translation R\u03b2\u22121, and the placeholder \u27e8mask\u27e9. The model then predicts the final contextual translation, R\u03b2 .\nAppendix B contains a worked out example for translation from Hindi to Malayalam."
        },
        {
            "heading": "4 Experimental Setup",
            "text": "We conduct a comparative study of our DecoMT approach, which is based on mT5 (Xue et al., 2021) with 3.7B parameters, against various established approaches. These include the Standard Prompting technique applied to 7.1B parameters variant of BLOOM (Workshop et al., 2023), and 7.5B parameters variant of XGLM (Lin et al., 2022). We also compare our method with the Standard Prompting technique applied to the mT5 model. In this case, as mT5 generates only a few tokens at a time, we append the generated text back to the input to prompt further text generation. Furthermore, we compare our approach with SAP (Patel et al., 2023), a technique that also utilizes mT5 with 3.7B parameters."
        },
        {
            "heading": "4.1 Evaluation Metrics",
            "text": "Our approach\u2019s performance is assessed using spBLEU (Goyal et al., 2022), a variant of BLEU(Papineni et al., 2002), and chrF++ (Popovic\u0301, 2017) metrics. The BLEU metric measures word n-gram matches, encompassing unigram, bigram, trigram, and four-grams. However, due to the morphological richness of the languages we are working with, BLEU scores can often be underestimated. To counteract this, we employ spBLEU as suggested by NLLB (Goyal et al., 2022; Team et al.,\n2022), which utilizes a subword-based tokenizer. Conversely, chrF++ evaluates character n-gram matches for n values ranging from 1 to 4, in addition to word n-gram matches that include unigram and bigram. Given its demonstrated higher correlation with human annotator scores for low-resource languages (Popovic\u0301, 2017), chrF++ serves as a valuable metric for our study. We use the SacreBLEU library (Post, 2018) to compute these metrics. We provide signatures for both BLEU 2 and chrF++ 3.\nFor hyperparameter tuning, we utilize the FLORES development set. We evaluate chunk sizes for m from the set {3,4,5}."
        },
        {
            "heading": "4.2 Evaluation",
            "text": "We conducted evaluations on multiple languages using the Flores devtest set, focusing specifically on translations between closely related languages: Hindi (hin) \u2194 Marathi (mar), hin \u2194 Malayalam (mal), hin \u2194 Gujarati (guj), hin \u2194 Telugu (tel), Indonesian (ind) \u2194 Malay (zsm), Ukrainian (ukr) \u2194 Russian (rus), and Portuguese (por) \u2194 Spanish (spa). The latter pair represents a high-resource language setup for comparison."
        },
        {
            "heading": "5 Results",
            "text": ""
        },
        {
            "heading": "5.1 Automatic Evaluation",
            "text": "The results of our evaluations are summarized in Table 1. We conducted statistical significance testing via paired bootstrap sampling (Koehn, 2004) (p < 0.05). Regarding performance, XGLM (Lin et al., 2022) when used with Standard Prompting, demonstrated low spBLEU and chrF++ scores for low-resource language pairs such as hin\u2194mal, hin\u2194mar, hin\u2194guj, and ind\u2194zsm. It performed somewhat better with the ukr\u2192rus pair, likely due to the greater availability of resources for Russian compared to Ukrainian.\nBLOOM (Workshop et al., 2023), outperformed XGLM across all directions and language pairs except tel\u2192hin. However, BLOOM does not currently support languages such as zsm, rus, and ukr.\nWhen implemented with Standard Prompting, mT5 outperformed XGLM for most low-resource language pairs and even outperformed BLOOM on hin\u2192mal, hin\u2192guj, and hin\u2192tel pairs, underscoring its effectiveness as a robust baseline.\n2BLEU Signature: nrefs:1|case:mixed|eff:no| tok:flores200|smooth:exp|version:2.3.1\n3chrF++ Signature: nrefs:1|case:mixed|eff:yes|nc:6| nw:2|space:no|version:2.3.1\nSAP proved to be a strong approach, echoing the findings of Patel et al. (2023). It outperformed Standard Prompting with BLOOM, XGLM and mT5 on the hin\u2194mal, hin\u2194mar, hin\u2194guj, hin\u2194tel, ind\u2194zsm, and rus\u2194ukr language pairs. Nevertheless, BLOOM outperformed SAP for the highresource spa\u2194por pair.\nLastly, DecoMT surpassed all other approaches on the low-resource language pairs hin\u2194mal, hin\u2194mar, hin\u2194guj, hin\u2194tel, ind\u2194zsm, and rus\u2194ukr. While it also achieved impressive results with the high-resource spa\u2194por pair, it fell short of BLOOM\u2019s performance in this particular scenario. It\u2019s worth noting that DecoMT demonstrated an average improvement of 13.8 points in the chrF++ score over Standard Prompting with mT5, which presents a more direct comparison for DecoMT due to the same base model and their similar prompting and inference strategies."
        },
        {
            "heading": "5.2 Human Evaluation",
            "text": "To further analyze the quality of the outputs and validate the enhancements indicated by the automatic evaluation scores, we carry out a human evaluation study. This involves a comparative examination of our DecoMT approach, SAP, and Standard Prompting with mT5 and BLOOM.\nWe engaged annotators who possessed compre-\nhension skills in the source language and demonstrated fluency in the target language. These annotators were remunerated in alignment with local hourly wage standards. The language pairs hin\u2194mar, hin\u2194guj, zsm\u2192ind, and por\u2192spa were selected for evaluation, contingent upon the availability of annotators well-suited for each pair. It should be noted that only a single annotator was assigned to each language pair. We sampled 50 sentences for each approach for a total of 200.\nOur human evaluation strategy employs the Cross-Lingual Semantic Textual Similarity (XSTS) methodology (Licht et al., 2022) adopted by NLLB (Team et al., 2022) and IndicTrans2 (AI4Bharat et al., 2023). Within this approach, annotators are presented with the source sentence alongside translations produced by various approaches, omitting any human-annotated references. As XSTS emphasizes translation adequacy over fluency, it is wellsuited to our focus on translation between related, typically low-resource languages, where adequacy takes precedence.\nThe XSTS metric is composed of a scale ranging from 1 to 5, where a score of 1 signifies completely dissimilar sentence pairs and a score of 5 represents semantically identical sentences. Appendix D contains details of the score values.\nAs shown in Table 2, DecoMT significantly out-\nperforms Standard Prompting with mT5 across all language pairs. DecoMT is significantly better than BLOOM for hin\u2192mar, hin\u2194guj and ind\u2192zsm but comparable with BLOOM on mar\u2192hin and por\u2192spa. DecoMT is significantly better than SAP for hin\u2192mar, while demonstrating comparable performance for the remaining language pairs."
        },
        {
            "heading": "6 Discussion",
            "text": "Scores of Translation across different Sentence Lengths The DecoMT strategy involves translating source sentences in consecutive chunks, a method we hypothesize will lead to enhanced translation adequacy. To explore this, we group source sentences into length-based buckets, each with a width equivalent to the standard deviation of the source sentence lengths. If a bucket contains fewer than 20 instances, we merge it with its neighbour.\nFigure 5 depicts the relationship between source sentence length and chrF++ scores for the hin\u2192mal and zsm\u2192ind language pairs. As hypothesized, as the length of the source sentence increases, the performance of DecoMT, as measured by chrF++, improves. For the zsm\u2192ind language pair, the chrF++ scores of DecoMT and SAP are nearly identical for the first two buckets. However, as we move to the next three buckets with longer sentences, we observe a steady increase in DecoMT\u2019s chrF++ scores. This is in contrast with the declining scores of SAP, highlighting DecoMT\u2019s superiority in translating longer sentences.\nImprovement by Adding the Contextual Translation Compared to the Independent Translation We compared the single-stage independent translation to the two-stage DecoMT. The experiments show that the inclusion of contextual transla-\ntion in the second stage of DecoMT significantly improves performance. We report the improvement in chrF++ scores in Table 3. The improvement in spBLEU is presented in Appendix E.\nOff-target Translations To quantify the offtarget translation rate among various approach\u2019s outputs, we employed the Language Identification tool developed by the NLLB (Team et al., 2022). The off-target translation rate is represented as a percentage, with a lower percentage denoting superior performance, as shown in Table 4. We see that the DecoMT approach consistently outperforms other approaches with lower off-target translation rate across various translation tasks. We conduct further analysis in Appendix F.\nExtension to Autoregressive and other EncoderDecoder LLMs At present, we utilize mT5 for both independent and contextual translations. However, it\u2019s worth noting that any autoregressive LLM could potentially be used for independent translation. As for contextual translation, an autoregressive LLM could be prompted with a fill-inthe-blanks type of prompt - an avenue we intend to explore in future work. Additionally, the exploration of other encoder-decoder LLMs such as UL2 (Tay et al., 2023) or AlexaTM (Soltan et al., 2022) for contextual translations presents a promising research direction.\nExperiments with Zero-shot and One-shot Prompting We undertook zero-shot translation experiments for select language pairs, specifically\nhin<->guj, hin<->tel, and hin<->mal. We compared different approaches applied to mT5 including DecoMT, SAP and Standard Prompting. We found that all approaches yielded near-zero BLEU scores. In most instances, the models merely copied the input as the output. We hypothesize that this is because in a zero-shot setting the model may not understand that it has to perform translation to the target language.\nWe compared one-shot and five-shot settings for three language pairs (hin<->guj, hin<->tel and hin<->mal) using Standard Prompting (SP), SAP, and DecoMT with mT5. Our results in Appendix G indicate that:\n\u2022 DecoMT maintains strong performance even in the one-shot setting.\n\u2022 Both SAP and SP experience significant performance drops transitioning from five-shot to one-shot. For instance, the spBLEU score for hin->tel in SAP drops from 19.3 (five-shot) to just 1.3 (one-shot).\nInference Times As highlighted in Patel et al. (2023), to generate a sentence comprising T words, SAP necessitates T forward passes through the model. This approach stands in contrast to Standard Prompting, which only requires a single pass. In the case of DecoMT, the independent translation stage can be parallelized with relative ease. For the contextual translation stage, T/m forward passes through the model are needed, where m denotes the chunk size. As a result, the inference time for DecoMT is less than that of SAP. Appendix H contains more details of runtime analysis."
        },
        {
            "heading": "7 Conclusion",
            "text": "In this study, we introduced DecoMT, a novel approach using decomposed prompting for Machine Translation of related languages. DecoMT demonstrated superior performance over established fewshot prompting baselines in translating between low-resource related languages, as evidenced by our experiments with the FLORES dataset. Additionally, DecoMT showed robust performance even in high-resource scenarios.\nLimitations\nDespite its advantages, DecoMT does possess certain limitations. Notably, the approach requires human annotation for constructing the five examplealigned prompts in the template. However, our\nobservations suggest that the annotators primarily need to modify existing translations, which is less laborious than generating translations from scratch, an activity that can be done in under 30 minutes. Conversely, other baseline approaches don\u2019t require such annotation and are able to directly utilize translation examples.\nWhen considering the translation time, DecoMT, given its two-stage process encompassing independent and contextual translations, inherently requires a longer duration to generate outputs compared to traditional few-shot prompting methodologies.\nAnother limitation of DecoMT is its dependency on an LM with infixing capabilities during the contextual translation stage. In the absence of infixing capabilities, this can be simulated on other LLM with appropriate prompting, and we plan to explore that in future work.\nEthics Statement\nThis study does not involve any new data collection. We solely utilize publicly accessible datasets for conducting the experiments reported herein. Furthermore, for the purpose of annotation of translation examples and the human evaluation of machine translation outputs, we employ annotators who are duly compensated for their time and expertise, ensuring fair practices in line with established standards."
        },
        {
            "heading": "Acknowledgements",
            "text": "We would like to thank the reviewers for their feedback. This research was supported by funding from the Institute for Infocomm Research (I2R) under A\u2217STAR ARES, Singapore. We extend our gratitude to Litton Kurisinkel, Aswanth Kumar, Siti Umairah, Ivan Kukanov, Swapnali Waghunde, and Fabian Ritter-Gutierrez for their work in annotating the few-shot prompts. Additionally, we\u2019d like to thank Siti Umairah, Fabian Ritter-Gutierrez, Kunal Gandhi, and Faiz Masi for their contributions to the human evaluation experiments."
        },
        {
            "heading": "A Examples of Prompts",
            "text": "The prompts used for independent and contextual translations by DecoMT for the language pair Malay\u2192Indonesian are presented in Table 5 and Table 6, respectively. Meanwhile, Table 7 illustrates the prompts utilized for Standard Prompting and SAP."
        },
        {
            "heading": "B Example for DecoMT",
            "text": "Figure 6 presents a block diagram which explains DecoMT with the help of an example. The task at hand is translation from Hindi to Malayalam. The Hindi sentence is divided into four consecutive chunks: H1, H2, H3, and H4, each consisting of m = 5 tokens. Using few-shot prompting, these chunks are independently translated into Malayalam, resulting in M1, M2, M3, and M4. However,\nwe observe that these translated chunks can occasionally lack coherence.\nFor instance, consider the translation of the H4 chunk. The chunk commences with which can translate to \u2018reason\u2019 or \u2018for\u2019 (indicating possession) in English. The M4 translation into Malayalam, adopts the former meaning, whereas the sentence context implies that the latter interpretation would be more suitable.\nTo rectify this, we introduce a process to generate contextually appropriate translations. We input a concatenation of H1, H2, and a mask placeholder, along with M2, into the bidirectional mT5 model.\nThe model then infills the mask, producing a contextually appropriate translation of M1, which we denote as R1.\nNext, we feed a concatenation of H1, H2, H3, along with a concatenation of R1, a mask placeholder, and M3 into the mT5 model. The result is a contextually appropriate translation, R2, of M2.\nThis procedure is repeated for all the intermediate chunks. For the final chunk, we input a concatenation of H3, H4, R3, and a mask placeholder. The mT5 model then predicts the contextually appropriate translation, R4, of the M4 translation. Given the context of H3, H4, and R3, the contextual trans-\nlation correctly interprets the intended meaning."
        },
        {
            "heading": "C Hyperparameter m",
            "text": "The optimum value of m for different language pairs is presented in Table 8. We posit that the optimal value of m is contingent on the relative morphological complexity of the source language. Take the example of hin\u2194mal. Since Hindi (hin) is less morphologically complex than Malayalam (mal), a larger number of tokens are required in a chunk for hin\u2192mal than for mal\u2192hin to produce satisfactory outputs in the independent translation stage.\nIn the case of zsm\u2194ind, both languages exhibit similar morphological complexity, resulting in an identical optimum value of m, which is 4. The same applies to the rus\u2194ukr and spa\u2194por pairs. For these three pairs, a value of m smaller than 4 results in subpar independent translation quality. Conversely, a value exceeding 4 might lead to truncated translations."
        },
        {
            "heading": "D Details of Human Annotation Guidelines",
            "text": "The XSTS metric provides ratings between 1 and 5, representing different levels of similarity between sentences.\n\u2022 A score of 1 indicates that the sentences share little content or may be about different topics. If they share content, it is less than 50\n\u2022 A score of 2 indicates that the sentences are about similar topics but are not equivalent, and\nthere may be differences in important information related to the primary subject/verb/object.\n\u2022 A score of 3 indicates that the sentences are mostly similar, but there may be some minor omissions of unimportant information. There should not be any significant conflict in the information.\n\u2022 A score of 4 indicates that the sentences are paraphrases of each other. There are no major differences or missing information, although there may be variations in expression such as tone, style, emphasis, or formality.\n\u2022 A score of 5 indicates that the sentences are completely equivalent in meaning and usage, including expression aspects such as formality, tones, style, and emphasis.\nFor more details and examples, see Licht et al. (2022).\nE Improvement by Adding the Contextual Translation Compared to the Independent Translation\nTable 9 showcases the improvements in spBLEU scores achieved by the DecoMT approach in comparison to the Single Stage method."
        },
        {
            "heading": "F Off-target Translations",
            "text": "In Table 4, focusing on the relatively high offtarget translation rate for ind\u2194zsm, particularly for ind\u2192zsm, we analyzed 50 mislabeled DecoMT\noutput sentences from ind\u2192zsm. An annotator from our human evaluation study (Section 5.2) found that 64% of these sentences were in fact Malay, not Indonesian. This suggests potential shortcomings in automatic language identification for closely related languages such as ind and zsm."
        },
        {
            "heading": "G Comparison between One-shot and Five-shot Prompting",
            "text": "As detailed in Table 10, our evaluations span three language pairs and compare the efficacy of Standard Prompting (SP), SAP, and DecoMT methodologies when evaluated on mT5. In comparison between one-shot and five-shot scenarios, we find that DecoMT consistently demonstrates strong performance in one-shot settings, in contrast to the pronounced performance dips observed for both SP and SAP."
        },
        {
            "heading": "H Analysis of Runtime",
            "text": "To ensure a fair comparison, we profile the codes using cprofile 4 during the inference phase, executed on an A40 48GB GPU. cprofile examines the time taken by various API calls. In this case, our chosen task is translating from Marathi to Hindi using the initial batch of 5 examples from the FLORES test set, with the longest Marathi sample in the batch being 41 tokens long.\n4https://docs.python.org/3/library/profile. html\n\u2022 SAP Analysis: For the SAP system, due to the unpredictability of the expected target length, we do decoding at 1.5 times the maximum source length. This is based on our studies of lengths of examples from validation dataset. For example, for our given source batch, the reference Hindi translation encompasses 55 tokens for the Marathi sentence which is 41 tokens long. As the longest example is 41 tokens, we run inference for 41 * 1.5 = 61 steps. Table 11 contains a partial trace of performance profiling using cprofile. We see that for SAP, there are 61 calls to predict_output method. The method predict_output is responsible for running inference on the LLM. Each method takes 2.384 seconds. The inference of the batch takes 145.455 seconds.\n\u2022 DecoMT Analysis: For Marathi-Hindi translations, we use a chunk size of 4. We first consider the independent translation stage. Breaking down the sentence lengths of the batch in tokens: 16, 30, 24, 41, and 28, we get respective chunk counts of 4, 8, 6, 11, and 7\u2014ag-\ngregating to 36 chunks. Split into batches of 8, this leads to 5 API calls to predict_output. With the longest sentence in the batch having 41 tokens, the contextual translation stage demands 11 API calls to predict_output, cumulating to 16 calls. These 16 api calls in total amount to 96.868 seconds (Table 12). While predict_output in DecoMT tends to take longer than in SAP (owing to DecoMT predicting multiple tokens as opposed to SAP\u2019s single-token approach), the overall fewer API calls render DecoMT more efficient."
        }
    ],
    "title": "DecoMT: Decomposed Prompting for Machine Translation Between Related Languages using Large Language Models",
    "year": 2023
}