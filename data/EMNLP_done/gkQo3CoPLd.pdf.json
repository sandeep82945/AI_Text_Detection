{
    "abstractText": "Humans subconsciously engage in geospatial reasoning when reading articles. We recognize place names and their spatial relations in text and mentally associate them with their physical locations on Earth. Although pretrained language models can mimic this cognitive process using linguistic context, they do not utilize valuable geospatial information in large, widely available geographical databases, e.g., OpenStreetMap. This paper introduces GEOLM ( ), a geospatially grounded language model that enhances the understanding of geo-entities in natural language. GEOLM leverages geo-entity mentions as anchors to connect linguistic information in text corpora with geospatial information extracted from geographical databases. GEOLM connects the two types of context through contrastive learning and masked language modeling. It also incorporates a spatial coordinate embedding mechanism to encode distance and direction relations to capture geospatial context. In the experiment, we demonstrate that GEOLM exhibits promising capabilities in supporting toponym recognition, toponym linking, relation extraction, and geo-entity typing, which bridge the gap between natural language processing and geospatial sciences. The code is publicly available at https://github.com/ knowledge-computing/geolm.",
    "authors": [
        {
            "affiliations": [],
            "name": "Zekun Li"
        },
        {
            "affiliations": [],
            "name": "Wenxuan Zhou"
        },
        {
            "affiliations": [],
            "name": "Yao-Yi Chiang"
        },
        {
            "affiliations": [],
            "name": "Muhao Chen"
        }
    ],
    "id": "SP:1de95a4bff37e909aca57e43006235fddb271aa2",
    "references": [
        {
            "authors": [
                "Matthew McDermott."
            ],
            "title": "Publicly available clinical BERT embeddings",
            "venue": "Proceedings of the 2nd Clinical Natural Language Processing Workshop, pages 72\u201378, Minneapolis, Minnesota, USA. Association for Computational Linguistics.",
            "year": 2019
        },
        {
            "authors": [
                "Einat Amitay",
                "Nadav Har\u2019El",
                "Ron Sivan",
                "Aya Soffer"
            ],
            "title": "Web-a-where: geotagging web content",
            "venue": "In Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval,",
            "year": 2004
        },
        {
            "authors": [
                "Ilias Chalkidis",
                "Manos Fergadiotis",
                "Prodromos Malakasiotis",
                "Nikolaos Aletras",
                "Ion Androutsopoulos."
            ],
            "title": "LEGAL-BERT: The muppets straight out of law school",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 2898\u2013",
            "year": 2020
        },
        {
            "authors": [
                "Jiacheng Chen",
                "Hexiang Hu",
                "Hao Wu",
                "Yuning Jiang",
                "Changhu Wang."
            ],
            "title": "Learning the best pooling strategy for visual semantic embedding",
            "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 15789\u201315798.",
            "year": 2021
        },
        {
            "authors": [
                "Yen-Chun Chen",
                "Linjie Li",
                "Licheng Yu",
                "Ahmed El Kholy",
                "Faisal Ahmed",
                "Zhe Gan",
                "Yu Cheng",
                "Jingjing Liu."
            ],
            "title": "Uniter: Universal image-text representation learning",
            "venue": "Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, Au-",
            "year": 2020
        },
        {
            "authors": [
                "Nicola De Cao",
                "Gautier Izacard",
                "Sebastian Riedel",
                "Fabio Petroni."
            ],
            "title": "Autoregressive entity retrieval",
            "venue": "arXiv preprint arXiv:2010.00904.",
            "year": 2020
        },
        {
            "authors": [
                "Grant DeLozier",
                "Jason Baldridge",
                "Loretta London."
            ],
            "title": "Gazetteer-independent toponym resolution using geographic word profiles",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, volume 29.",
            "year": 2015
        },
        {
            "authors": [
                "Franck Dernoncourt",
                "Ji Young Lee",
                "Peter Szolovits."
            ],
            "title": "Neuroner: an easy-to-use program for namedentity recognition based on neural networks",
            "venue": "arXiv preprint arXiv:1705.05487.",
            "year": 2017
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Stella Douka",
                "Hadi Abdine",
                "Michalis Vazirgiannis",
                "Rajaa El Hamdani",
                "David Restrepo Amariles."
            ],
            "title": "JuriBERT: A masked-language model adaptation for French legal text",
            "venue": "Proceedings of the Natural Legal Language Processing Workshop 2021, pages 95\u2013",
            "year": 2021
        },
        {
            "authors": [
                "Jenny Rose Finkel",
                "Trond Grenager",
                "Christopher D Manning."
            ],
            "title": "Incorporating non-local information into information extraction systems by gibbs sampling",
            "venue": "Proceedings of the 43rd annual meeting of the association for computational linguistics",
            "year": 2005
        },
        {
            "authors": [
                "Tianyu Gao",
                "Xingcheng Yao",
                "Danqi Chen."
            ],
            "title": "Simcse: Simple contrastive learning of sentence embeddings",
            "venue": "arXiv preprint arXiv:2104.08821.",
            "year": 2021
        },
        {
            "authors": [
                "Milan Gritta",
                "Mohammad Taher Pilehvar",
                "Nigel Collier."
            ],
            "title": "Which Melbourne? augmenting geocoding with maps",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1285\u20131296,",
            "year": 2018
        },
        {
            "authors": [
                "Milan Gritta",
                "Mohammad Taher Pilehvar",
                "Nigel Collier."
            ],
            "title": "A pragmatic guide to geoparsing evaluation: Toponyms, named entity recognition and pragmatics",
            "venue": "Language resources and evaluation, 54:683\u2013 712.",
            "year": 2020
        },
        {
            "authors": [
                "Milan Gritta",
                "Mohammad Taher Pilehvar",
                "Nut Limsopatham",
                "Nigel Collier."
            ],
            "title": "Vancouver welcomes you! minimalist location metonymy resolution",
            "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Vol-",
            "year": 2017
        },
        {
            "authors": [
                "Milan Gritta",
                "Mohammad Taher Pilehvar",
                "Nut Limsopatham",
                "Nigel Collier"
            ],
            "title": "What\u2019s missing in geographical parsing? Language Resources and Evaluation, 52:603\u2013623",
            "year": 2018
        },
        {
            "authors": [
                "Claire Grover",
                "Richard Tobin",
                "Kate Byrne",
                "Matthew Woollard",
                "James Reid",
                "Stuart Dunn",
                "Julian Ball."
            ],
            "title": "Use of the edinburgh geoparser for georeferencing digitized historical collections",
            "venue": "Philosophical Transactions of the Royal Society A: Mathematical,",
            "year": 2010
        },
        {
            "authors": [
                "Yanzhu Guo",
                "Virgile Rennard",
                "Christos Xypolopoulos",
                "Michalis Vazirgiannis."
            ],
            "title": "BERTweetFR : Domain adaptation of pre-trained language models for French tweets",
            "venue": "Proceedings of the Seventh Workshop on Noisy User-generated Text (W-NUT 2021),",
            "year": 2021
        },
        {
            "authors": [
                "Andrew Halterman."
            ],
            "title": "Mordecai: Full text geoparsing and event geocoding",
            "venue": "The Journal of Open Source Software, 2(9).",
            "year": 2017
        },
        {
            "authors": [
                "Andrew Halterman."
            ],
            "title": "Mordecai 3: A neural geoparser and event geocoder",
            "venue": "arXiv preprint arXiv:2303.13675.",
            "year": 2023
        },
        {
            "authors": [
                "Lei He",
                "Suncong Zheng",
                "Tao Yang",
                "Feng Zhang."
            ],
            "title": "KLMo: Knowledge graph enhanced pretrained language model with fine-grained relationships",
            "venue": "In",
            "year": 2021
        },
        {
            "authors": [
                "Bo-June Hsu",
                "Giuseppe Ottaviano."
            ],
            "title": "Spaceefficient data structures for top-k completion",
            "venue": "Proceedings of the 22nd international conference on World Wide Web, pages 583\u2013594.",
            "year": 2013
        },
        {
            "authors": [
                "Xuke Hu",
                "Yeran Sun",
                "Jens Kersten",
                "Zhiyong Zhou",
                "Friederike Klan",
                "Hongchao Fan"
            ],
            "title": "How can voting mechanisms improve the robustness and generalizability of toponym disambiguation",
            "venue": "International Journal of Applied Earth Observation",
            "year": 2023
        },
        {
            "authors": [
                "Xuke Hu",
                "Zhiyong Zhou",
                "Hao Li",
                "Yingjie Hu",
                "Fuqiang Gu",
                "Jens Kersten",
                "Hongchao Fan",
                "Friederike Klan."
            ],
            "title": "Location reference recognition from texts: A survey and comparison",
            "venue": "arXiv preprint arXiv:2207.01683.",
            "year": 2022
        },
        {
            "authors": [
                "Yingjie Hu."
            ],
            "title": "Eupeg: Towards an extensible and unified platform for evaluating geoparsers",
            "venue": "Proceedings of the 12th Workshop on Geographic Information Retrieval, pages 1\u20132.",
            "year": 2018
        },
        {
            "authors": [
                "Chao Jia",
                "Yinfei Yang",
                "Ye Xia",
                "Yi-Ting Chen",
                "Zarana Parekh",
                "Hieu Pham",
                "Quoc Le",
                "Yun-Hsuan Sung",
                "Zhen Li",
                "Tom Duerig."
            ],
            "title": "Scaling up visual and vision-language representation learning with noisy text supervision",
            "venue": "International Conference on",
            "year": 2021
        },
        {
            "authors": [
                "Mandar Joshi",
                "Danqi Chen",
                "Yinhan Liu",
                "Daniel S. Weld",
                "Luke Zettlemoyer",
                "Omer Levy."
            ],
            "title": "SpanBERT: Improving pre-training by representing and predicting spans",
            "venue": "Transactions of the Association for Computational Linguistics, 8:64\u201377.",
            "year": 2020
        },
        {
            "authors": [
                "Jinhyuk Lee",
                "Wonjin Yoon",
                "Sungdong Kim",
                "Donghyeon Kim",
                "Sunkyu Kim",
                "Chan Ho So",
                "Jaewoo Kang."
            ],
            "title": "Biobert: a pre-trained biomedical language representation model for biomedical text mining",
            "venue": "Bioinformatics, 36(4):1234\u20131240.",
            "year": 2020
        },
        {
            "authors": [
                "Liunian Harold Li",
                "Mark Yatskar",
                "Da Yin",
                "Cho-Jui Hsieh",
                "Kai-Wei Chang."
            ],
            "title": "What does BERT with vision look at? In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5265\u20135275, Online",
            "venue": "Association",
            "year": 2020
        },
        {
            "authors": [
                "Liunian Harold Li",
                "Pengchuan Zhang",
                "Haotian Zhang",
                "Jianwei Yang",
                "Chunyuan Li",
                "Yiwu Zhong",
                "Lijuan Wang",
                "Lu Yuan",
                "Lei Zhang",
                "Jenq-Neng Hwang"
            ],
            "title": "Grounded language-image pre-training",
            "venue": "In Proceedings of the IEEE/CVF Conference on Com-",
            "year": 2022
        },
        {
            "authors": [
                "Zekun Li",
                "Jina Kim",
                "Yao-Yi Chiang",
                "Muhao Chen."
            ],
            "title": "SpaBERT: A pretrained language model from geographic data for geo-entity representation",
            "venue": "Findings of the Association for Computational",
            "year": 2022
        },
        {
            "authors": [
                "Michael D Lieberman",
                "Hanan Samet",
                "Jagan Sankaranarayanan."
            ],
            "title": "Geotagging with local lexicons to build indexes for textually-specified spatial data",
            "venue": "2010 IEEE 26th international conference on data engineering (ICDE 2010), pages 201\u2013212.",
            "year": 2010
        },
        {
            "authors": [
                "Fangyu Liu",
                "Ehsan Shareghi",
                "Zaiqiao Meng",
                "Marco Basaldella",
                "Nigel Collier."
            ],
            "title": "Self-alignment pretraining for biomedical entity representations",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computa-",
            "year": 2021
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "venue": "arXiv preprint arXiv:1907.11692.",
            "year": 2019
        },
        {
            "authors": [
                "Zilong Liu",
                "Krzysztof Janowicz",
                "Ling Cai",
                "Rui Zhu",
                "Gengchen Mai",
                "Meilin Shi."
            ],
            "title": "Geoparsing: Solved or biased? an evaluation of geographic biases in geoparsing",
            "venue": "AGILE: GIScience Series, 3:9.",
            "year": 2022
        },
        {
            "authors": [
                "Inderjeet Mani",
                "Christy Doran",
                "Dave Harris",
                "Janet Hitzeman",
                "Rob Quimby",
                "Justin Richer",
                "Ben Wellner",
                "Scott Mardis",
                "Seamus Clancy."
            ],
            "title": "Spatialml: annotation scheme, resources, and evaluation",
            "venue": "Language Resources and Evaluation, 44:263\u2013280.",
            "year": 2010
        },
        {
            "authors": [
                "Aaron van den Oord",
                "Yazhe Li",
                "Oriol Vinyals."
            ],
            "title": "Representation learning with contrastive predictive coding",
            "venue": "arXiv preprint arXiv:1807.03748.",
            "year": 2018
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "year": 2021
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2020
        },
        {
            "authors": [
                "Paul R\u00f6ttger",
                "Janet Pierrehumbert."
            ],
            "title": "Temporal adaptation of BERT and performance on downstream document classification: Insights from social media",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2400\u20132412, Punta",
            "year": 2021
        },
        {
            "authors": [
                "Weijie Su",
                "Xizhou Zhu",
                "Yue Cao",
                "Bin Li",
                "Lewei Lu",
                "Furu Wei",
                "Jifeng Dai."
            ],
            "title": "Vl-bert: Pre-training of generic visual-linguistic representations",
            "venue": "International Conference on Learning Representations.",
            "year": 2020
        },
        {
            "authors": [
                "Jeniya Tabassum",
                "Mounica Maddela",
                "Wei Xu",
                "Alan Ritter."
            ],
            "title": "Code and named entity recognition in StackOverflow",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4913\u20134926, Online. Association for",
            "year": 2020
        },
        {
            "authors": [
                "Richard Tobin",
                "Claire Grover",
                "Kate Byrne",
                "James Reid",
                "Jo Walsh."
            ],
            "title": "Evaluation of georeferencing",
            "venue": "Proceedings of the 6th Workshop on Geographic Information Retrieval, GIR \u201910, New York, NY, USA. Association for Computing Machinery.",
            "year": 2010
        },
        {
            "authors": [
                "Richard Tobin",
                "Claire Grover",
                "Kate Byrne",
                "James Reid",
                "Jo Walsh."
            ],
            "title": "Evaluation of georeferencing",
            "venue": "proceedings of the 6th workshop on geographic information retrieval, pages 1\u20138.",
            "year": 2010
        },
        {
            "authors": [
                "Jan Oliver Wallgr\u00fcn",
                "Morteza Karimzadeh",
                "Alan M MacEachren",
                "Scott Pezanowski."
            ],
            "title": "Geocorpora: building a corpus to test and train microblog geoparsers",
            "venue": "International Journal of Geographical Information Science, 32(1):1\u201329.",
            "year": 2018
        },
        {
            "authors": [
                "Jimin Wang",
                "Yingjie Hu."
            ],
            "title": "Enhancing spatial and textual analysis with eupeg: An extensible and unified platform for evaluating geoparsers",
            "venue": "Transactions in GIS, 23(6):1393\u20131419.",
            "year": 2019
        },
        {
            "authors": [
                "Jimin Wang",
                "Yingjie Hu",
                "Kenneth Joseph."
            ],
            "title": "Neurotpr: A neuro-net toponym recognition model for extracting locations from social media messages",
            "venue": "Transactions in GIS, 24(3):719\u2013735.",
            "year": 2020
        },
        {
            "authors": [
                "Xiaozhi Wang",
                "Tianyu Gao",
                "Zhaocheng Zhu",
                "Zhengyan Zhang",
                "Zhiyuan Liu",
                "Juanzi Li",
                "Jian Tang."
            ],
            "title": "Kepler: A unified model for knowledge embedding and pre-trained language representation",
            "venue": "Transactions of the Association for Computational Linguis-",
            "year": 2021
        },
        {
            "authors": [
                "Allison Gyle Woodruff",
                "Christian Plaunt."
            ],
            "title": "Gipsy: Automated geographic indexing of text documents",
            "venue": "Journal of the American Society for Information Science, 45(9):645\u2013655.",
            "year": 1994
        },
        {
            "authors": [
                "Ikuya Yamada",
                "Akari Asai",
                "Hiroyuki Shindo",
                "Hideaki Takeda",
                "Yuji Matsumoto."
            ],
            "title": "LUKE: Deep contextualized entity representations with entityaware self-attention",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Lan-",
            "year": 2020
        },
        {
            "authors": [
                "Jiannan Yang",
                "Hong Jia",
                "Hanbing Liu."
            ],
            "title": "Spatial relationship extraction of geographic entities based on bert model",
            "venue": "Journal of Physics: Conference Series, volume 2363, page 012031. IOP Publishing.",
            "year": 2022
        },
        {
            "authors": [
                "Haoxuan You",
                "Luowei Zhou",
                "Bin Xiao",
                "Noel Codella",
                "Yu Cheng",
                "Ruochen Xu",
                "Shih-Fu Chang",
                "Lu Yuan."
            ],
            "title": "Learning visual representation from modality-shared contrastive language-image pre-training",
            "venue": "Computer Vision\u2013ECCV 2022: 17th",
            "year": 2022
        },
        {
            "authors": [
                "Li Yu",
                "Feng Lu"
            ],
            "title": "A bootstrapping algorithm",
            "year": 2015
        },
        {
            "authors": [
                "Gritta"
            ],
            "title": "2020), the token-level F1 achieved by Yahoo! Placemaker, Edinburgh Geoparser, Spacy NLP, Google Cloud Natural Language, and NCRF++ are 63.2",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Spatial reasoning and semantic understanding of natural language text arise from human communications. For example, \u201cI visited Paris in Arkansas to see the smaller Eiffel Tower\u201d, a human can easily recognize the toponyms \u201cEiffel Tower\u201d, \u201cParis\u201d, \u201cArkansas\u201d, and the spatial relation \u201cin\u201d. Implicitly, a human can also infer that this \u201cEiffel Tower\u201d might be a replica1 of the original one in Paris,\n1 The small Eiffel Tower in Paris, Arkansas, United States: https://www.arkansas.com/paris/accommodations/ eiffel-tower-park\nFrance. Concretely, the process of geospatially grounded language understanding involves core tasks such as recognizing geospatial concepts being described, inferring the identities of those concepts, and reasoning about their spatially qualified relations. These tasks are essential for applications that involve the use of place names and their spatial relations, such as social media message analysis (Hu et al., 2022), emergency response (Gritta et al., 2018b), natural disaster analysis (Wang et al., 2020), and geographic information retrieval (Wallgr\u00fcn et al., 2018).\nPretrained language models (PLMs; Devlin et al. 2019; Liu et al. 2019; Raffel et al. 2020) have seen broad adaptation across various domains such as biology (Lee et al., 2020), healthcare (Alsentzer et al., 2019), law (Chalkidis et al., 2020; Douka et al., 2021), software engineering (Tabassum et al., 2020), and social media (R\u00f6ttger and Pierrehumbert, 2021; Guo et al., 2021). These models benefit from in-domain corpora (e.g., PubMed for the biomedical domain) to learn domain-specific terms and concepts. Similarly, a geospatially grounded language model requires training with geo-corpora. The geo-corpora should cover worldwide geo-entity names and their variations, geographical locations, and spatial relations to other geo-entities. Although some geo-corpora are available, e.g., LGL (Lieberman et al., 2010) and SpatialML (Mani et al., 2010), the sizes of the datasets are relatively small.\nIn contrast, Wikipedia stores many articles describing places worldwide and can serve as comprehensive geo-corpora for geospatially grounded language understanding. However, training with Wikipedia only solves partial challenges in geospatial grounding, as it only provides the linguistic context of a geo-entity with sentences describing history, demographics, climate, etc. The information about the geospatial neighbors of a geoentity is still missing. On the other hand, large-\nscale geographical databases (e.g., OpenStreetMap) and knowledge bases (e.g., Wikidata) can provide extensive amounts of geo-entity locations and geospatial relations, enriching the information sourced from Wikipedia. Additionally, the geolocations from OpenStreetMap can help connecting the learned geo-entity representations to physical locations on the Earth.\nTo address these challenges, we propose GEOLM, a language model specifically designed to support geospatially grounded natural language understanding. Our model aims to enhance the comprehension of places, types, and spatial relations in natural language. We use Wikipedia, Wikidata, and OSM together to create geospatially grounded training samples using geo-entity names as anchors. GEOLM verbalizes the Wikidata relations into natural language and aligns the linguistic context in Wikipedia and Wikidata with the geospatial context in OSM. Since existing language models do not take geocoordinates as input, GEOLM further incorporates a spatial coordinate embedding module to learn the distance and directional relations between geo-entities. During inference, our model can take either natural language or geographical subregion (i.e., a set of nearby geo-entities) as input and make inferences relying on the aligned linguistic-geospatial information. We employ two training strategies: contrastive learning (Oord et al., 2018) between natural language corpus and linearized geographic data, and masked language modeling (Devlin et al., 2019) with a concatenation of these two modalities. We treat GEOLM as a versatile model capable of addressing various geographically related language understanding tasks, such as toponym recognition, toponym linking, and geospatial relation extraction."
        },
        {
            "heading": "2 GEOLM",
            "text": "This section introduces GEOLM\u2019s mechanism for representing both the linguistic and geospatial context (\u00a72.1), followed by the detailed development process of pretraining tasks (\u00a72.3) and corpora (\u00a72.2), and how GEOLM is further adapted to various downstream geospatial NLU tasks (\u00a72.4)."
        },
        {
            "heading": "2.1 Representing Linguistic and Geospatial Context",
            "text": "The training process of GEOLM aims to simultaneously learn the linguistic and geospatial context, aligning them in the same embedding space to obtain geospatially grounded language representations. The linguistic context refers to the sentential context of the geographical entity (i.e., geo-entity). The linguistic context contains essential information to specify a geo-entity, including sentences describing geography, environment, culture, and history. Additionally, geo-entities exhibit strong correlations with neighboring geo-entities (Li et al., 2022b). We refer to these neighboring geo-entities as the geospatial context. The geospatial context encompasses locations and implicit geo-relations of the neighbors to the center geo-entity. 2\nTo capture the linguistic context, GEOLM takes natural sentences as input and generates entity-level representations by averaging the token representations within the geo-entity name span. For the geospatial context, GEOLM follows the geospatial context linearization method and the spatial embedding module in our previous work SPABERT (Li et al., 2022b), a PLM that generates geospatially contextualized entity representations using\n2Here, we assume that all geo-entities in the geographical dataset are represented as points.\npoint geographic data. Given a center geo-entity and its spatial neighbors, GEOLM linearizes the geospatial context by sorting the neighbors in ascending order based on their geospatial distances from the center geo-entity. GEOLM then concatenates the name of the center geo-entity and the sorted neighbors to form a pseudo-sentence. To preserve directional relations and relative distances, GEOLM employs the geocoordinates embedding module, which takes the geocoordinates as input and encodes them with a sinusoidal position embedding layer.\nTo enable GEOLM to process both natural language text and geographical data, we use the following types of position embedding mechanisms for each token and the token embedding (See Fig. 2). By incorporating these position embedding mechanisms, GEOLM can effectively process both natural language text and geographical data, allowing the model to capture and leverage spatial information.\nPosition ID describes the index position of the token in the sentence. Note that the position ID for both the NL and geospatial input starts from zero.\nSegment ID indicates the source of the input tokens. If the tokens belong to the natural language input, then the segment ID is zero; otherwise one.\nX-coord and Y-coord are inputs for the spatial coordinate embedding. Tokens within the same geo-entity name span share the same X-coord and Y-coord values. Since NL tokens do not have associated geocoordinate information, we set their X-coord and Y-coord to be DSEP, which is a constant value as distance filler.\nIn addition, GEOLM projects the geocoordinates\n(lat, lng) into a 2-dimensional World Equidistant Cylindrical coordinate system EPSG:4087.3 This is because (lat, lng) represent angle-based values that model the 3D sphere, whereas coordinates in EPSG:4087 are expressed in Cartesian form. Additionally, when generating the pseudo-sentence, we sort neighbors of the center geo-entity based on the Haversine distance which reflects the geodesic distance on Earth instead of the Euclidean distance."
        },
        {
            "heading": "2.2 Pretraining Corpora",
            "text": "We divide the training corpora into two parts: 1) pseudo-sentence corpora from a geographical dataset, OpenStreetMap (OSM), to provide the geospatial context; 2) natural language corpora from Wikipedia and verbalized Wikidata to provide the linguistic context.\nGeographical Dataset. OpenStreetMap (OSM) is a crowd-sourced geographical database containing a massive amount of point geo-entities worldwide. In addition, OSM stores the correspondence from OSM geo-entity to Wikipedia and Wikidata links. We preprocess worldwide OSM data and gather the geo-entities with Wikidata and Wikipedia links to prepare paired training data used in contrastive pretraining. To linearize geospatial context and prepare the geospatial input in Fig. 2, For each geo-entity, we retrieve its geospatial neighbors and construct pseudo-sentences (See Fig. 1) by concatenating the neighboring geo-entity names after sorting the neighbors by distance. In the end, we generate 693,308 geo-entities with the same number of pseudo-sentences in total.\n3 EPSG:4087: https://epsg.io/4087\nNatural Language Text Corpora. We prepare the text corpora from Wikipedia and Wikidata. Wikipedia provides a large corpus of encyclopedic articles, with a subset describing geo-entities. We first find the articles describing geo-entities by scraping all the Wikipedia links pointed from the OSM annotations, then break the articles into sentences and adopt Trie-based phrase matching (Hsu and Ottaviano, 2013) to find the sentences containing the name of the corresponding OSM geo-entity. The training samples are paragraphs containing at least one corresponding OSM geo-entity name. For Wikidata, the procedure is similar to Wikipedia. We collect the Wikidata geo-entities using the QID identifier pointed from the OSM geo-entities. Since Wikidata stores the relations as triples, we convert the relation triples to natural sentences with a set of pre-defined templates (See example in Fig. 1). After merging the Wikipedia and Wikidata samples, we gather 1,458,150 sentences/paragraphs describing 472,067 geo-entities."
        },
        {
            "heading": "2.3 Pretraining Tasks",
            "text": "We employ two pretraining tasks to establish connections between text and geospatial data, enabling GEOLM to learn geospatially grounded representations of natural language text.\nThe first is a contrastive learning task using an InfoNCE loss (Oord et al., 2018), which contrasts between the geo-entity features extracted from the two modalities. This loss encourages GEOLM to generate similar representations for the same geoentity, regardless of whether the representation is contextualized based on the linguistic or geospatial context. Simultaneously, GEOLM learns to distinguish between geo-entities that share the same name by maximizing the distances of their representations in the embedding space.\nFormally, let the training data D consist of pairs of samples (snli , s geo i ), where s nl i is a linguistic sample (a natural sentence or a verbalized relation from a knowledge base), and sgeoi is a pseudosentence created from the geographic data. Both samples mention the same geo-entity. Let f(\u00b7) be GEOLM that takes both snli and s geo i as input and produces entity-level representation hnli = f(s nl i ) and hgeoi = f(s geo i ). Then the loss function is:\nLcontrasti = \u2212log esim(h\nnl i ,h geo i )/\u03c4\u22112N\nj=1 1[j \u0338=i]e sim(hnli ,h geo j )/\u03c4\n,\nwhere \u03c4 is a temperature, and sim(\u00b7) denotes the cosine similarity.\nTo improve GEOLM\u2019s ability to disambiguate geo-entities, we include in-batch hard negatives comprising geo-entities with identical names. As a result, each batch is composed of 50% random negatives and 50% hard negatives.\nAdditionally, we employ a masked language modeling task (Devlin et al., 2019) on a concatenation of the paired natural language sentence and geographical pseudo-sentence (Fig. 2). This task encourages GEOLM to recover masked tokens by leveraging both linguistic and geographical data."
        },
        {
            "heading": "2.4 Downstream Tasks",
            "text": "Our study further adapts GEOLM to several downstream tasks to demonstrate its ability for geospatially grounded language understanding, including toponym recognition, toponym linking, geo-entity typing, and geospatial relation extraction (Fig. 3). Toponym recognition or geo-tagging (Gritta et al., 2020) is to extract toponyms (i.e., place names) from unstructured text. This is a crucial step in recognizing geo-entities mentioned in the text before inferring their identities and relationships in subsequent tasks. We frame this task as a multiclass sequence tagging problem, where tokens are classified into one of three classes: B-topo (beginning of a toponym), I-topo (inside a toponym), or O (non-toponym). To accomplish this, we append a fully connected layer to GEOLM and train the model end-to-end on a downstream dataset to classify each token from the text input. Toponym linking, also referred to as toponym res-\nolution and geoparsing (Gritta et al., 2020, 2018a; Hu et al., 2022), aims to infer the identity or geocoordinates of a mentioned geo-entity in the text by grounding geo-entity mention to the correct record in a geographical database, which might contain many candidate entities with the same name as the extracted toponym from the text. During inference, we process the candidate geo-entities from the geographical databases the same way as during pretraining, where nearby neighbors are concatenated together to form pseudo-sentences. For this task, we perform zero-shot linking by directly applying GEOLM without fine-tuning. GEOLM extracts representations of geo-entities from linguistic data and calculates representations for all candidate entities. After obtaining the representations for both query and candidate geo-entities, the linking results are formed as a ranked list of candidates sorted by cosine similarity in the feature space. Geo-entity typing is the task of categorizing the types of locations in a geographical database (e.g., OpenStreetMap) (Li et al., 2022b). Geo-entity typing helps us understand the characteristics of a region and can be useful for location-based recommendations. We treat this task as a classification problem and append a one-layer classification head after GEOLM. We train GEOLM to predict the type of the central geo-entity given a subregion of a geographical area. To accomplish this, we construct a pseudo-sentence following Fig. 1, then compute the representation of the geo-entity using GEOLM and feed the representation to the classification head for training.\nGeospatial relation extraction is the task of classifying topological relations between a pair of locations (Mani et al., 2010). We treat this task as an entity pair classification problem. We compute the average embedding of tokens to form the entity embedding and then concatenate the embeddings of the subject and object entities to use as input for the classifier, then predict the relationship type with a softmax classifier."
        },
        {
            "heading": "3 Experiments",
            "text": "We hereby evaluate GEOLM on the aforementioned four downstream tasks. All compared models (except GPT3.5) are finetuned on task-specific datasets for toponym recognition, geo-entity typing, and geospatial relation extraction.4\n4Toponym linking is an unsupervised task."
        },
        {
            "heading": "3.1 Toponym Recognition",
            "text": "Task Setup. We adopt GEOLM for toponym recognition on the GeoWebNews (Gritta et al., 2020) dataset. This dataset contains 200 news articles with 2,601 toponyms.5 The annotation includes the start and end character positions (i.e., the spans) of the toponyms in paragraphs. We use 80% for training and 20% for testing.\nEvaluation Metrics. We report precision, recall, and F1, and include both token-level scores and entity-level scores. For entity-level scores, a prediction is considered correct only when it matches exactly with the ground-truth mention (i.e., no missing or partial overlap).\nModels in Comparison. We compare GEOLM with fine-tuned PLMs including BERT, SimCSEBERT6 (Gao et al., 2021) , SpanBERT (Joshi et al., 2020) and SapBERT (Liu et al., 2021). SpanBERT is a BERT-based model with span prediction pretraining. Instead of masking out random tokens, SpanBERT masks out a continuous span and tries to recover the missing parts using the tokens right next to the masking boundary. SapBERT learns to align biomedical entities through self-supervised contrastive learning.7 We compare all models in base versions.\nResults and Discussion. Tab. 1 shows that our GEOLM yields the highest entity-level F1 score, with BERT being the close second. Since GEOLM\u2019s weights are initialized from BERT, the improvement over BERT shows the effectiveness of in-domain training with geospatial grounding. For token-level results, SpanBERT has the best F1 score for I-topo, showing that span prediction during pretraining is beneficial for predicting the continuation of toponyms. However, GEOLM is better at predicting the start token of the toponym, which SpanBERT does not perform as well."
        },
        {
            "heading": "3.2 Toponym Linking",
            "text": "Task Setup. We run unsupervised toponym linking on two benchmark datasets: Local Global Corpus (LGL; Lieberman et al. 2010) and Wikipedia To-\n5We only consider the place names associated with valid geocoordinates as toponyms and do not count the literal expression (e.g., the word \u201cstreet\u201d, \u201cblocks\u201d and \u201cintersection\u201d) as toponyms.\n6We use the unsupervised pretrained weights. 7Although SapBERT is trained with biomedical corpus, it generalizes well on geo-entity recognition and linking since the study of diseases often relates to places and regions.\nponym Retrieval (WikToR; Gritta et al. 2018b). LGL contains 588 news articles with 4,462 toponyms that have been linked to the GeoNames database. This dataset has ground-truth geocoordinates and GeoNames ID annotations. This dataset has only one sample with a unique name (among 4,462 samples in total). On average, each toponym corresponds to 27.52 geo-entities with the same in GeoNames. WikToR is derived from 5,000 Wikipedia pages, where each page describes a unique place. This dataset also contains many toponyms that share the same lexical form but refer to distinct geo-entities. For example, Santa Maria, Lima, and Paris are the top ambiguous place names. For WikToR, there is no sample with a unique name. The least ambiguous one has four candidate geo-entities with the same name in GeoNames. On\naverage, each toponym corresponds to 70.45 geoentities with the same name in GeoNames. This dataset is not linked with the GeoNames database, but the ground-truth geocoordinates of the toponym are provided instead.\nEvaluation Metrics. For LGL, we evaluate model performance using two metrics. 1) R@k is a standard retrieval metric that computes the recall of the top-k prediction based on the ground-truth GeoNames ID. 2) P@D, following previous studies (Gritta et al., 2018b), computes the distance-based approximation rate of the top-ranked prediction. If the prediction falls within the distance threshold D from the ground truth geocoordinates, we consider the prediction as a correct approximation.8\n8Here, P@D161 is the same as Acc@161km (or Acc@100miles ) reported by Gritta et al. (2018b).\nFor WikToR, the ground-truth geocoordinates are given, and we follow Gritta et al. (2020, 2018b) to report P@D metrics with various D values.\nModels in Comparison. We compare GEOLM with multiple PLMs, including BERT, RoBETRa, SpanBERT (Joshi et al., 2020) and SapBERT (Liu et al., 2021). In the experiments, we use all base versions of the above models. For a fair comparison, to calculate the representation of the candidate entity, we concatenate the center entity\u2019s name and its neighbors\u2019 names then input the concatenated sequence (i.e., pseudo-sentence) to all baseline PLMs to provide the linguistic context, following the same unsupervised procedure as in \u00a72.4.\nResults and Discussion. One challenge of this task is that the input sentences may not have any information about their neighbor entities; thus, instead of only considering the linguistic context or relying on neighboring entities to appear in the sentence, GEOLM\u2019s novel approach aligns the linguistic and geospatial context so that GEOLM can effectively map the geo-entity embeddings learned from the linguistic context in articles to the embeddings learned from the geospatial context in geographic data and perform linking. The contrastive learning process during pretraining is designed to help the context alignment. From Tab. 2, GEOLM offers more reliable performance than baselines. On WikTor and LGL datasets, GEOLM obtains the best or second-best scores in all metrics. On LGL, GEOLM demonstrates more precise top-1 retrieval (R@1) than other models, and also the highest\nscores on P@D161. Since baselines are able to harness only the linguistic context, while GEOLM can use the linguistic and geospatial context when taking sentences as input. The improvement of R@1 and P@D161 from BERT shows the effectiveness of the geospatial context. On WikToR, GEOLM performs the best on all metrics. Since WikToR has many geo-entities with the same names, the scores of GEOLM indicate strong disambiguation capability obtained from aligning the linguistic and geospatial context.\nFig. 4 shows the visualization of the toponym linking results given \u201cParis\u201d mentioned in two sentences. The input sentences are provided in the figures. Apparently, the first Paris should be linked to Paris, France, and the second Paris goes to Paris, AK, US. However, the BERT model fails to ground these two mentions into the correct locations. BERT ranks the candidate geo-entities only slightly differently for the two input sentences, indicating that BERT relies more on the geo-entity name rather than the linguistic context when performing prediction. On the other hand, our model could predict the correct location of geo-entities. This is because even though the lexical forms of the geo-entity names (i.e., Paris) are the same, the linguistic context describing the geo-entity are distinct. Fig. 4 demonstrate that the contrastive learning helps GEOLM to map the linguistic context to the geospatial context in the embedding space."
        },
        {
            "heading": "3.3 Geo-entity Typing",
            "text": "Task Setup. We apply GEOLM on the supervised geo-entity typing dataset released by Li et al. (2022b). The goal is to classify the type of the center geo-entity providing the geospatial neighbors as context. We linearize the set of geo-entities to a pseudo-sentence that represent the geospatial context for the center geo-entity then feed the pseudosentence into GEOLM. There are 33,598 pseudosentences for nine amenity classes, with 80% for training and 20% for testing. We train multiple language models to perform amenity-type classification for the center geo-entity.\nEvaluation Metric. Following (Li et al., 2022b), we report the F1 score for each class and the micro F1 for all the samples.\nModels in Comparison. In addition to BERT, SpanBERT and SimCSE-BERT, this task also takes LUKE (Yamada et al., 2020) and SpaBERT (Li\net al., 2022b) into comparison. LUKE is designed to solve entity-related tasks with a specially designed entity tokenizer. SpaBERT generates geoentity representations given small geographical regions as input. In addition, BERT, SpanBERT, SimCSE-BERT, and LUKE rely only on linguistic information, and SpaBERT relies only on geospatial context to make predictions.\nResults and Discussion. Tab. 3 shows that the performance of GEOLM surpasses the baseline models using only linguistic or geospatial information. The experiment demonstrates that combining the information from both modalities and aligning the context is useful for geo-entity type inference. Compared to the second best model, SpaBERT, GEOLM has robust improvement on seven types. The result indicates that the contrastive learning during pretraining helps GEOLM to align the linguistic context and geospatial context, and although only geospatial context is provided as input during inference, GEOLM can still employ the aligned linguistic context to facilitate the type prediction."
        },
        {
            "heading": "3.4 Geospatial Relation Extraction",
            "text": "Task Setup. We apply GEOLM to the SpatialML topological relation extraction dataset released by Mani et al. (2010). Given a sentence containing multiple entities, the model classifies the relations between each pair of entities into six types9 (includ-\n9The types come from the RCC8 relations, which contain eight types of geospatial relations. The SpatialML dataset merges four of the relations (TPP, TPPi, NTTP, NTTPi) into the \u201cIN\u201d relation thus resulting in five geo-relations in the dataset.\ning an NA class indicating that there is no relation). The dataset consists of 1,528 sentences, with 80% for training and 20% for testing. Within the dataset, there are a total of 10,592 entity pairs, of which 10,232 pairs do not exhibit any relation.\nEvaluation Metric. We report the micro F1 score on the test dataset. We exclude the NA instances when calculating the F1 score.\nModels in Comparison. We compare GEOLM with other pretrained language models, including BERT, RoBERTa, and SpanBERT, as well as a large language model (LLM), GPT-3.5.\nResults and Discussion. Fig. 5 shows that GEOLM achieves the best F1. The results suggest that GEOLM effectively captures the topological relationships between entities. Furthermore, GEOLM demonstrates a better ability to learn geospatial grounding between the mentioned entities."
        },
        {
            "heading": "3.5 Ablation Study",
            "text": "We conduct two ablation experiments to validate the model design using toponym recognition and toponym linking: 1) removing the spatial coordinate embedding layer that takes the geo coordinates as input during the training; 2) removing the contrastive loss that encourages the model to learn similar geo-entity embeddings from two types of context (i.e., linguistic context and geospatial context) and only applying MLM on the NL corpora.\nFor the toponym recognition task, we compare\nthe entity-level precision, recall, and F1 scores on the GeoWebNews dataset. Tab. 4 shows that removing either component could cause performance degradation. For the toponym linking task, in the first ablation experiment, the linking accuracy (P@D161, i.e., Acc@161km or Acc@100miles) drops from 0.358 to 0.321 after removing the spatial coordinate embedding, indicating that the geocoordinate information is beneficial and our embedding layer design is effective. In the second ablation experiment, the linking accuracy (P@D161) drops from 0.358 to 0.146, showing that contrastive learning is essential."
        },
        {
            "heading": "4 Related Work",
            "text": "Geospatial NLU. Understanding geospatial concepts in natural language has long been a topic of interest. Previous studies (Liu et al., 2022; Hu, 2018; Wang and Hu, 2019) have used generalpurpose NER tools such as Stanford NER (Finkel et al., 2005) and NeuroNER (Dernoncourt et al., 2017) to identify toponyms in text. In the geographic information system (GIS) domain, tools such as the Edinburgh geoparser (Grover et al., 2010; Tobin et al., 2010a), Yahoo! Placemaker10 and Mordecai (Halterman, 2017, 2023) have been developed to detect toponyms and link them to geographical databases. Also, several heuristicsbased approaches have been proposed (Woodruff and Plaunt, 1994; Amitay et al., 2004; Tobin et al., 2010b) to limit the spatial range of gazetteers and associate the toponym with the most likely geo-entity (e.g., most populous ones). More recently, deep learning models have been adopted to establish the connection between extracted toponyms and geographical databases (Gritta et al., 2017, 2018a; DeLozier et al., 2015). For instance, TopoCluster (DeLozier et al., 2015) learns the association between words and geographic locations, deriving a geographic likelihood for each word in the vocabulary. Similarly, CamCoder (Gritta et al., 2018a) introduces an algorithm that encodes toponym mentions in a geodesic vector space, predicting the final location based on geodesic and lexical features. These models utilize supervised training, which assumes that the testing and training data cover the same region. However, this assumption may limit their applicability in scenarios where the testing and training regions differ. Furthermore, Yu\n10Yahoo! placemaker: https://simonwillison.net/ 2009/May/20/placemaker/\nand Lu (2015) use keyword extraction approach and Yang et al. (2022) use language model based (e.g., BERT) classification to solve the geospatial relation extraction problem. However, these models often struggle to incorporate the geospatial context of the text during inference. Our previous work SpaBERT (Li et al., 2022b) is related to GEOLM in terms of representing the geospatial context. It is a language model trained on geographical datasets. Although SpaBERT can learn geospatial information, it does not fully employ linguistic information during inference.\nGEOLM is specifically designed to align the linguistic and geospatial context within a joint embedding space through MLM and contrastive learning.\nLanguage Grounding. Language grounding involves mapping the NL component to other modalities, and it encompasses several areas of research. In particular, vision-language grounding has gained significant attention, with popular approaches including contrastive learning (Radford et al., 2021; Jia et al., 2021; You et al., 2022; Li et al., 2022a) and (masked) vision-language model on distantly parallel multi-modal corpora (Chen et al., 2020, 2021; Su et al., 2020; Li et al., 2020). Additionally, knowledge graph grounding has been explored with similar strategies (He et al., 2021; Wang et al., 2021). GEOLM leverages both contrastive learning and MLM on distantly parallel geospatial and linguistic data, and it represents a pilot study on the grounded understanding of these two modalities."
        },
        {
            "heading": "5 Conclusion",
            "text": "In this paper, we propose GEOLM, a PLM for geospatially grounded language understanding. This model can handle both natural language inputs and geospatial data inputs, and provide connected representation for both modalities. Technically, GEOLM conducts contrastive and MLM pretraining on a massive collection of distantly parallel language and geospatial corpora, and incorporates a geocoordinate embedding mechanism for improved spatial characterization. Through evaluations on four important downstream tasks, toponym recognition, toponym linking, geo-entity typing and geospatial relation extraction, GEOLM has demonstrated competent and comprehensive abilities for geospatially grounded NLU. In the future, we plan to extend the work to question answering and autoregressive tasks.\nLimitations\nThe current version of our model only uses point geometry to represent the geospatial context, ignoring polygons and polylines. Future work can expand the model\u2019s capabilities to handle those complex geometries. Also, it is important to note that the OpenStreetMap data were collected through crowdsourcing, which introduces possible labeling noise and bias. Lastly, model pre-training was conducted on a GPU with at least 24GB of memory. Attempting to train the model on GPUs with smaller memory may lead to memory constraints and degraded performance.\nEthics Statement\nThe model weights in our research are initialized from a pretrained BERT model for English. In addition, the training data used in our research are primarily extracted from crowd-sourced databases, including OpenStreetMap (OSM), Wikipedia, and Wikidata. Although these sources provide extensive geographical coverage, the geographical distribution of training data exhibits significant disparities, with Europe having the most abundant data. At the same time, Central America and Antarctica are severely underrepresented, with less than 1% of the number of samples compared to Europe. This uneven training data distribution may introduce biases in the model\u2019s performance, particularly in regions with limited annotated samples."
        },
        {
            "heading": "Acknowledgements",
            "text": "We appreciate the reviewers for their insightful comments and suggestions. We thank the Minnesota Supercomputing Institute (MSI) for providing resources that contributed to the research results reported in this article. Zekun Li and Yao-Yi Chiang were supported by the University of Minnesota Computer Science & Engineering Faculty startup funds. Wenxuan Zhou and Muhao Chen were supported by the NSF Grant IIS 2105329, the NSF Grant ITE 2333736, and the DARPA MCS program under Contract No. N660011924033 with the United States Office of Naval Research, a Cisco Research Award, two Amazon Research Awards, and a Keston Research Award."
        },
        {
            "heading": "A Distribution of Geo-entities",
            "text": "We analyze the distribution of geo-entities in the pretraining corpora and the downstream datasets. When considering only the name (without geocoordinates), the overlapping percentages of the pretraining data and downstream datasets are shown in Tab. 5. Ablation experiments show that for the geoentities already included during the pretraining, the average P@D161 score is 0.362. For the ones that are not included during the pretraining, the average P@D161 score is 0.341, which is not significantly different from the prior one. This indicates that the improved performance of GEOLM comparing with other models benefits from enhancing the geospatial representations."
        },
        {
            "heading": "B Comparison with Other Methods",
            "text": "For the toponym recognition task, we compare the results of GEOLM with other existing models designed specifically for this problem. According to (Gritta et al., 2020), the token-level F1 achieved by Yahoo! Placemaker, Edinburgh Geoparser, Spacy NLP, Google Cloud Natural Language, and NCRF++ are 63.2%, 63.6%, 74.9%, 83.2% and 88.6% respectively. GEOLM has a token-level F1 of 86.3%, which is better than all existing ones except NCRF++. The reason is that NCRF++ uses fine-grained toponym taxonomy to boost the toponym recognition performance. However, the finegrained labels can sometimes be difficult to collect for large-scale datasets. In addition, NCRF++ is a specific toponym recognition model that does not support other geospatial-inference tasks. With GEOLM, we can generate representations useful for various tasks.\nFor the toponym linking task, we compare our model with the other existing geoparser models mentioned in EUPEG (Hu, 2018) and Voting (Hu et al., 2023), including CLAVIN11, TopoCluster (DeLozier et al., 2015), CamCoder (Gritta et al., 2018a), Modecai (Halterman, 2017), GENRE (De Cao et al., 2020), and Voting (Hu et al., 2023). Since EUPEG evaluates the toponym resolution and toponym linking together and does not provide\n11CLAVIN:https://github.com/Novetta/CLAVIN\nthe scores for linking only, we use the scores reported in Voting, which assumes gold toponyms as inputs for toponym linking (same as ours).\nThe scores in P@D161 (or Accuracy@161km) are shown in Tab. 6. The models that perform better than GEOLM are all supervised learning models (i.e. CamCoder, GENRE and Voting) while GEOLM is unsupervised. Within the unsupervised group, GEOLM performs the best. The benefit of the unsupervised nature of GEOLM is that GEOLM can handle new samples without the need for extra training data, which is often difficult to gather. Also, new geo-entity names can appear in documents and the way people call the same geo-entity can change over time, the unsupervised approach has the advantage of handling ever-changing documents, e.g., online text, while the supervised approach focuses on existing names of geo-entities. We acknowledge that there is a gap between GEOLM and the domain-specific geoparsers, and we will aim to narrow this gap in the future."
        },
        {
            "heading": "C Trie",
            "text": "Trie supports a tree structure for efficient search for geo-entity names, and it helps distinguish between two geo-entities with shared substrings in their names. For example, Trie helps extract the geo-entity \u201cLos Angeles High School\u201d from the sentence \u201cI work at the Los Angeles High School,\u201d instead of extracting the geo-entity \u201cLos Angeles\u201d. Fig. 6 shows an example Trie constructed from a set of geo-entity names.\nWe use all geo-entity names in the worldwide OpenStreetMap database to construct a worldwide Trie, where each node is a single word in the name. When using Trie to preprocess the Wikipedia documents, we apply the Trie searching to find all\nthe mentioned geo-entity names. To mitigate the disambiguation error, we use the Wikipedia page title to filter out the mentions that do not describe the \u201centity-of-interest\u201d. After this step, the disambiguation error only occurs when two distinct geo-entities with the same name occur on the same Wikipedia page, which is pretty rare. This does not fully resolve the disambiguation error issue, and there may still be noises in the training data. However, as long as most of the data is clean, the model can still learn meaningful information from the data. Thus we have compiled a quite large pretraining NL corpora with 1,458,150 sentences/paragraphs describing 472,067 geo-entities."
        },
        {
            "heading": "D GPT-3.5 Prompt",
            "text": "We use GPT-3.5-turbo to help predict the relations between two geo-entities. With the system role, we prompt the model with the general task description and ask the model to choose from one of the possible relationships. With the user role, we provide the input sentence and the two geo-entity names. The example prompt is shown below.\n\u2022 System:Given a sentence, and two entities within the sentence, classify the relationship between the two entities based on the provided sentence. All possible Relationships are\nlisted below: [ disconnected (DC): Entity A and B have no spatial intersection, both in terms of interiors and boundaries; externally connected (EC): Entity A and B touch each other only at their boundaries; equal (EQ): Entity A and B are identical; partially overlapping (PO): Interiors of entity A and B overlap but neither is completely contained within the other; within (IN): One entity is part of the other entity; Others: No relation between entities]\n\u2022 User: Sentence: {input-sentence} Entity1: {entity1-name} Entity2: {entity2-name} Relationship:\nTo address the randomness in the GPT outputs and robustly evaluate the performance, we only look for some particular keywords from the GPT outputs. If the output contains the desired keyword, we consider the prediction as correct. Tab. 7 lists the keywords for each relation type."
        }
    ],
    "title": "GeoLM: Empowering Language Models for Geospatially Grounded Language Understanding",
    "year": 2023
}