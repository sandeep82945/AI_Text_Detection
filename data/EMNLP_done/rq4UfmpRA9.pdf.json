{
    "abstractText": "Large language models (LLMs) exhibit impressive emergent abilities in natural language processing, but their democratization is hindered due to huge computation requirements and closed-source nature. Recent research on advancing open-source smaller LMs by distilling knowledge from black-box LLMs has obtained promising results in the instructionfollowing ability. However, the reasoning ability which is more challenging to foster, is relatively rarely explored. In this paper, we propose a tailored learning approach to distill such reasoning ability to smaller LMs to facilitate the democratization of the exclusive reasoning ability. In contrast to merely employing LLM as a data annotator, we exploit the potential of LLM as a reasoning teacher by building an interactive multi-round learning paradigm. This paradigm enables the student to expose its deficiencies to the black-box teacher who then can provide customized training data in return. Further, to exploit the reasoning potential of the smaller LM, we propose selfreflection learning to motivate the student to learn from self-made mistakes. The learning from self-reflection and LLM are all tailored to the student\u2019s learning status, thanks to the seamless integration with the multi-round learning paradigm. Comprehensive experiments and analysis on mathematical and commonsense reasoning tasks demonstrate the effectiveness of our method. The code will be available at https://github.com/Raibows/Learn-to-Reason.",
    "authors": [
        {
            "affiliations": [],
            "name": "Zhaoyang Wang"
        },
        {
            "affiliations": [],
            "name": "Shaohan Huang"
        },
        {
            "affiliations": [],
            "name": "Yuxuan Liu"
        },
        {
            "affiliations": [],
            "name": "Jiahai Wang"
        },
        {
            "affiliations": [],
            "name": "Minghui Song"
        },
        {
            "affiliations": [],
            "name": "Zihan Zhang"
        },
        {
            "affiliations": [],
            "name": "Haizhen Huang"
        },
        {
            "affiliations": [],
            "name": "Furu Wei"
        },
        {
            "affiliations": [],
            "name": "Weiwei Deng"
        },
        {
            "affiliations": [],
            "name": "Feng Sun"
        },
        {
            "affiliations": [],
            "name": "Qi Zhang"
        }
    ],
    "id": "SP:723580e956b1c7aa9ff18b940fc39fa1483e115f",
    "references": [
        {
            "authors": [
                "Sid Black",
                "Gao Leo",
                "Phil Wang",
                "Connor Leahy",
                "Stella Biderman"
            ],
            "title": "GPT-Neo: Large Scale Autoregressive Language Modeling with MeshTensorflow",
            "year": 2021
        },
        {
            "authors": [
                "Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Morikawa",
                "Alec Radford",
                "Matthew Knight",
                "Miles Brundage",
                "Mira Murati",
                "Katie Mayer",
                "Peter Welinder",
                "Bob McGrew",
                "Dario Amodei",
                "Sam McCandlish",
                "Ilya Sutskever",
                "Wojciech Zaremba"
            ],
            "title": "Evaluating large language models trained on code",
            "year": 2021
        },
        {
            "authors": [
                "Cheng-Han Chiang",
                "Hung-yi Lee"
            ],
            "title": "Can large language models be an alternative to human evaluations? In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
            "year": 2023
        },
        {
            "authors": [
                "Wei-Lin Chiang",
                "Zhuohan Li",
                "Zi Lin",
                "Ying Sheng",
                "Zhanghao Wu",
                "Hao Zhang",
                "Lianmin Zheng",
                "Siyuan Zhuang",
                "Yonghao Zhuang",
                "Joseph E. Gonzalez",
                "Ion Stoica",
                "Eric P. Xing"
            ],
            "title": "Vicuna: An OpenSource Chatbot Impressing",
            "year": 2023
        },
        {
            "authors": [
                "Aakanksha Chowdhery",
                "Sharan Narang",
                "Jacob Devlin",
                "Maarten Bosma",
                "Gaurav Mishra",
                "Adam Roberts",
                "Paul Barham",
                "Hyung Won Chung",
                "Charles Sutton",
                "Sebastian Gehrmann"
            ],
            "title": "Palm: Scaling language modeling with pathways. arXiv, abs/2204.02311",
            "year": 2022
        },
        {
            "authors": [
                "Hyung Won Chung",
                "Le Hou",
                "Shayne Longpre",
                "Barret Zoph",
                "Yi Tay",
                "William Fedus",
                "Eric Li",
                "Xuezhi Wang",
                "Mostafa Dehghani",
                "Siddhartha Brahma"
            ],
            "title": "Scaling instruction-finetuned language models. arXiv, abs/2210.11416",
            "year": 2022
        },
        {
            "authors": [
                "Karl Cobbe",
                "Vineet Kosaraju",
                "Mohammad Bavarian",
                "Mark Chen",
                "Heewoo Jun",
                "Lukasz Kaiser",
                "Matthias Plappert",
                "Jerry Tworek",
                "Jacob Hilton",
                "Reiichiro Nakano"
            ],
            "title": "Training verifiers to solve math word problems. arXiv, abs/2110.14168",
            "year": 2021
        },
        {
            "authors": [
                "Jinlan Fu",
                "See-Kiong Ng",
                "Zhengbao Jiang",
                "Pengfei Liu."
            ],
            "title": "Gptscore: Evaluate as you desire",
            "venue": "arXiv, abs/2302.04166.",
            "year": 2023
        },
        {
            "authors": [
                "Shipeng Fu",
                "Zhen Li",
                "Zitao Liu",
                "Xiaomin Yang."
            ],
            "title": "Interactive knowledge distillation for image classification",
            "venue": "Neurocomputing, 449:411\u2013421.",
            "year": 2021
        },
        {
            "authors": [
                "Yao Fu",
                "Hao Peng",
                "Litu Ou",
                "Ashish Sabharwal",
                "Tushar Khot."
            ],
            "title": "Specializing smaller language models towards multi-step reasoning",
            "venue": "Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine",
            "year": 2023
        },
        {
            "authors": [
                "Leo Gao",
                "Stella Biderman",
                "Sid Black",
                "Laurence Golding",
                "Travis Hoppe",
                "Charles Foster",
                "Jason Phang",
                "Horace He",
                "Anish Thite",
                "Noa Nabeshima"
            ],
            "title": "The pile: An 800gb dataset of diverse text for language modeling. arXiv, abs/2101.00027",
            "year": 2020
        },
        {
            "authors": [
                "Mor Geva",
                "Daniel Khashabi",
                "Elad Segal",
                "Tushar Khot",
                "Dan Roth",
                "Jonathan Berant."
            ],
            "title": "Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies",
            "venue": "Transactions of the Association for Computational Linguistics, 9:346\u2013",
            "year": 2021
        },
        {
            "authors": [
                "Arnav Gudibande",
                "Eric Wallace",
                "Charles Burton Snell",
                "Xinyang Geng",
                "Hao Liu",
                "P. Abbeel",
                "S. Levine",
                "Dawn Song."
            ],
            "title": "The False Promise of Imitating Proprietary LLMs",
            "venue": "arXiv, abs/2305.15717.",
            "year": 2023
        },
        {
            "authors": [
                "Geoffrey Hinton",
                "Oriol Vinyals",
                "Jeff Dean."
            ],
            "title": "Distilling the knowledge in a neural network",
            "venue": "arXiv, abs/1503.02531.",
            "year": 2015
        },
        {
            "authors": [
                "Namgyu Ho",
                "Laura Schmid",
                "Se-Young Yun."
            ],
            "title": "Large language models are reasoning teachers",
            "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 14852\u201314882, Toronto, Canada.",
            "year": 2023
        },
        {
            "authors": [
                "Edward J Hu",
                "yelong shen",
                "Phillip Wallis",
                "Zeyuan AllenZhu",
                "Yuanzhi Li",
                "Shean Wang",
                "Lu Wang",
                "Weizhu Chen"
            ],
            "title": "LoRA: Low-rank adaptation of large language models",
            "venue": "In International Conference on Learning Representations",
            "year": 2022
        },
        {
            "authors": [
                "Zhiqiang Hu",
                "Yihuai Lan",
                "Lei Wang",
                "Wanyu Xu",
                "EePeng Lim",
                "Roy Ka-Wei Lee",
                "Lidong Bing",
                "Soujanya Poria."
            ],
            "title": "Llm-adapters: An adapter family for parameter-efficient fine-tuning of large language models",
            "venue": "arXiv, abs/2304.01933.",
            "year": 2023
        },
        {
            "authors": [
                "Jiaxin Huang",
                "Shixiang Shane Gu",
                "Le Hou",
                "Yuexin Wu",
                "Xuezhi Wang",
                "Hongkun Yu",
                "Jiawei Han."
            ],
            "title": "Large language models can self-improve",
            "venue": "arXiv, abs/2210.11610.",
            "year": 2022
        },
        {
            "authors": [
                "Ves Stoyanov"
            ],
            "title": "Opt-iml: Scaling language model instruction meta learning through the lens of generalization. arXiv, abs/2212.12017",
            "year": 2023
        },
        {
            "authors": [
                "Gou Jianping",
                "Yu Baosheng",
                "Stephen J Maybank",
                "Tao Dacheng."
            ],
            "title": "Knowledge distillation: A survey",
            "venue": "International Journal of Computer Vision, 129(6):1789\u20131819.",
            "year": 2021
        },
        {
            "authors": [
                "Takeshi Kojima",
                "Shixiang Shane Gu",
                "Machel Reid",
                "Yutaka Matsuo",
                "Yusuke Iwasawa."
            ],
            "title": "Large language models are zero-shot reasoners",
            "venue": "Advances in Neural Information Processing Systems.",
            "year": 2022
        },
        {
            "authors": [
                "Takeshi Kojima",
                "Shixiang Shane Gu",
                "Machel Reid",
                "Yutaka Matsuo",
                "Yusuke Iwasawa."
            ],
            "title": "Large language models are zero-shot reasoners",
            "venue": "Advances in Neural Information Processing Systems.",
            "year": 2022
        },
        {
            "authors": [
                "Hunter Lightman",
                "V. Kosaraju",
                "Yura Burda",
                "Harrison Edwards",
                "Bowen Baker",
                "Teddy Lee",
                "J. Leike",
                "J. Schulman",
                "Ilya Sutskever",
                "Karl Cobbe."
            ],
            "title": "Let\u2019s Verify Step by Step",
            "venue": "arXiv, abs/2305.20050.",
            "year": 2023
        },
        {
            "authors": [
                "Yuxuan Liu",
                "Tianchi Yang",
                "Shaohan Huang",
                "Zihan Zhang",
                "Haizhen Huang",
                "Furu Wei",
                "Weiwei Deng",
                "Feng Sun",
                "Qi Zhang."
            ],
            "title": "Calibrating llmbased evaluator",
            "venue": "arXiv, abs/2309.13308.",
            "year": 2023
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter."
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "International Conference on Learning Representations.",
            "year": 2019
        },
        {
            "authors": [
                "Clark."
            ],
            "title": "Self-refine: Iterative refinement with self-feedback",
            "venue": "arXiv, abs/2303.17651.",
            "year": 2023
        },
        {
            "authors": [
                "Lucie Charlotte Magister",
                "Jonathan Mallinson",
                "Jakub Adamek",
                "Eric Malmi",
                "Aliaksei Severyn."
            ],
            "title": "Teaching small language models to reason",
            "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short",
            "year": 2023
        },
        {
            "authors": [
                "Sewon Min",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Hannaneh Hajishirzi."
            ],
            "title": "MetaICL: Learning to learn in context",
            "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-",
            "year": 2022
        },
        {
            "authors": [
                "OpenAI."
            ],
            "title": "Gpt-4 technical report",
            "venue": "arXiv, abs/2303.08774.",
            "year": 2023
        },
        {
            "authors": [
                "Liangming Pan",
                "Michael Saxon",
                "Wenda Xu",
                "Deepak Nathani",
                "Xinyi Wang",
                "William Yang Wang."
            ],
            "title": "Automatically correcting large language models: Surveying the landscape of diverse self-correction strategies",
            "venue": "arXiv, abs/2308.03188.",
            "year": 2023
        },
        {
            "authors": [
                "Arkil Patel",
                "Satwik Bhattamishra",
                "Navin Goyal"
            ],
            "title": "Are NLP models really able to solve simple math word problems",
            "venue": "In Proceedings of the 2021 Conference of the North American Chapter of the Association",
            "year": 2021
        },
        {
            "authors": [
                "Wei Li",
                "Peter J Liu"
            ],
            "title": "Exploring the limits",
            "year": 2020
        },
        {
            "authors": [
                "Yuan",
                "Mu Li"
            ],
            "title": "Tailoring instructions to stu",
            "year": 2023
        },
        {
            "authors": [
                "Denny Zhou"
            ],
            "title": "2023a. Self-consistency improves",
            "year": 2023
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "Brian Ichter",
                "Fei Xia",
                "Ed H. Chi",
                "Quoc V. Le",
                "Denny Zhou."
            ],
            "title": "Chain-of-thought prompting elicits reasoning in large language models",
            "venue": "Advances in Neural Information Processing Systems.",
            "year": 2022
        },
        {
            "authors": [
                "Teven Le Scao",
                "Sylvain Gugger",
                "Mariama Drame",
                "Quentin Lhoest",
                "Alexander M. Rush."
            ],
            "title": "Huggingface\u2019s transformers: State-of-the-art natural language processing",
            "venue": "arXiv, abs/1910.03771.",
            "year": 2020
        },
        {
            "authors": [
                "Canwen Xu",
                "Daya Guo",
                "Nan Duan",
                "Julian McAuley."
            ],
            "title": "Baize: An open-source chat model with parameter-efficient tuning on self-chat data",
            "venue": "arXiv, abs/2304.01196.",
            "year": 2023
        },
        {
            "authors": [
                "Eric Zelikman",
                "Yuhuai Wu",
                "Jesse Mu",
                "Noah Goodman."
            ],
            "title": "Star: Bootstrapping reasoning with reasoning",
            "venue": "Advances in Neural Information Processing Systems, volume 35, pages 15476\u201315488.",
            "year": 2022
        },
        {
            "authors": [
                "Zihao Zhao",
                "Eric Wallace",
                "Shi Feng",
                "Dan Klein",
                "Sameer Singh."
            ],
            "title": "Calibrate before use: Improving few-shot performance of language models",
            "venue": "Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings",
            "year": 2021
        },
        {
            "authors": [
                "lowing Ho"
            ],
            "title": "MultiArith (Roy and Roth, 2015) and StrategyQA (Geva et al., 2021) is split with a ratio of 70 : 30 for the training and evaluation, while GSM8K (Cobbe et al., 2021) and CSQA (Talmor",
            "venue": "SVAMP (Patel et al.,",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Large language models (LLMs) with emergent abilities have achieved remarkable success across a wide range of tasks, deeply changed the landscape of both research and applications in natural language processing (Brown et al., 2020; Chen et al.,\n\u2020 Work done during internship at Microsoft. \u2217 Corresponding author.\n2021; Chowdhery et al., 2022; OpenAI, 2023). And Wei et al. (2022a,b) argue that emergent abilities particularly in reasoning only exist in LLMs whose parameters are commonly larger than 100B. Nevertheless, a line of research (Touvron et al., 2023a,b; Taori et al., 2023; Zeng et al., 2023) has indicated that smaller LMs with about 7B parameters after supervised fine-tuning such as Vicuna (Chiang et al., 2023) can be comparable to LLMs in following human instructions, while still falling short of reasoning. In this paper, we aim to harness the untapped reasoning potential of smaller LMs to democratize this important emergent ability.\nChain-of-Thought (CoT) prompts LMs to generate intermediate reasoning steps (i.e., rationale) to reach the final answer, significantly improving the complex reasoning ability (Wei et al., 2022b; Kojima et al., 2022a; Chung et al., 2022; Wang et al., 2023a). However, it is challenging to prompt smaller LMs to generate reasoning steps, since such ability appears to be exclusive to LLMs (Wei et al., 2022a,b; Chowdhery et al., 2022), which indicates the necessity of utilizing data annotated with rationales to cultivate smaller LMs\u2019 reasoning ability. Unfortunately, most existing reasoning datasets lack high-quality rationale annotations, and manual labeling them can be costly. Inspired by the success of collecting instruction data from LLMs (e.g., ChatGPT) for instruction tuning\nsmaller LMs (Wang et al., 2023b; Taori et al., 2023; Touvron et al., 2023a,b), we propose to leverage the rationales generated by LLMs to train smaller LMs to learn to use CoT towards reasoning.\nRecently, teaching smaller LMs towards reasoning with the help of LLMs has gained increasing attention. Most of these works (Ho et al., 2023; Magister et al., 2023; Fu et al., 2023b; Shridhar et al., 2023) can be summarized in two main steps: (1) employing LLMs to generate rationales for annotating the training data. (2) Fine-tuning smaller LMs on these data to enable reasoning with CoT. This approach can be viewed as a distant variant of black-box knowledge distillation (Jianping et al., 2021). However, these methods only employ LLMs to annotate the data for training smaller LMs, without leveraging the smaller LMs to assist LLMs in return. As a consequence, the LLMs are not aware of the weaknesses of the smaller LMs, thereby hindering their powerful ability to analyze and provide targeted feedback, which undermines the effectiveness of the reasoning distillation.\nTo this end, we propose a multi-round interactive learning paradigm to exploit the potential of black-box LLM as a reasoning teacher. In each round of learning, the student (i.e., smaller LM) first provides its learning status to the teacher LLM who then can provide customized rationales as the feedback to the student. The data annotated with these rationales serves as our customized training data. Such a paradigm is natural as it is in inline with how we human beings learn from teachers.\nBeyond learning from the teacher, another crucial paradigm for human learning lies in selfreflection on self-made mistakes. In parallel, recent studies (Huang et al., 2022; Shinn et al., 2023; Madaan et al., 2023; Pan et al., 2023) have also shown that LLMs can self-improve by reflecting on their own mistakes. Therefore, we exploit the reasoning potential of smaller LM by eliciting it to take self-reflection on the mistakes. These mistakes can complement correct rationales collected from the teacher LLM to teach the student LM to distinguish bad and good reasoning steps, thereby enhancing its reasoning ability.\nPutting them together, as briefly presented in Fig. 1, we propose a tailored multi-round learning paradigm based on the student\u2019s learning status and deficiencies, including learning from LLM\u2019s customized training data and self-reflection. In summary, our contributions are three-fold:\n1) A multi-round learning paradigm is introduced to enable the student LM to provide feedback to the teacher LLM who then can offer customized training data in response, building the interaction between smaller LM and black-box LLM.\n2) We propose self-reflection learning that motivates the student to learn from mistakes. Together with learning from customized training data, it can be seamlessly integrated into the multi-round learning paradigm.\n3) Experiments and analysis on mathematical and commonsense reasoning tasks demonstrate the effectiveness of our method in distilling the reasoning ability from LLMs to smaller LMs."
        },
        {
            "heading": "2 Related Work",
            "text": "Emergence in LLM LLMs show emergent abilities in a wide range of NLP tasks (Brown et al., 2020; Chowdhery et al., 2022; Wei et al., 2022a,b; OpenAI, 2023), among which the reasoning ability is the most noteworthy as it requires the model to perform multi-hop reasoning like human beings. Smaller LMs (< 100B) are often considered to be falling significantly short in reasoning, highlighting the superiority of LLMs in this aspect (Wei et al., 2022a). In this paper, we aim to democratize such emergent reasoning ability to smaller LMs.\nCoT Prompting CoT prompts LMs to solve reasoning tasks by generating intermediate rationales to reach the answer, which has greatly improved the reasoning performance (Wei et al., 2022b; Kojima et al., 2022b; Wang et al., 2023a). However, according to the reasoning performance curve (Wei et al., 2022a), the CoT reasoning performance of smaller LMs is far from satisfactory, since the generation of rationales is challenging for them. Chung et al. (2022) reveal that smaller LMs can partially master the CoT skill by training on data with rationales. We show that the CoT performance of smaller LMs can be further improved via tailored learning from LLM\u2019s customized training data and self-reflection.\nDistilling Knowledge from LLM Fine-tuning smaller LMs to follow instructions with highquality data collected from LLMs shows the feasibility of distilling knowledge from LLMs (Taori et al., 2023; Chiang et al., 2023; Xu et al., 2023). This procedure can also be viewed as a distant variant of black-box distillation (Hinton et al., 2015; Jianping et al., 2021). However, these works aim to improve the instruction-following ability of smaller\nLMs, while the reasoning ability that we focus on is often overlooked. Some recent studies (Ho et al., 2023; Fu et al., 2023b; Shridhar et al., 2023) propose to employ LLMs to annotate rationales for training smaller student LMs towards reasoning, not considering the student\u2019s feedback to the teacher. In contrast, we exploit the potential of the black-box LLM as the teacher instead of the data annotator by proposing a multi-round learning paradigm. This paradigm enables the mutual feedback between the LLM and smaller LM, thus can make the teacher LLM offer training data tailored for the student LM\u2019s learning status. Besides, we propose self-reflection learning to motivate the student LM to learn from mistakes."
        },
        {
            "heading": "3 Method",
            "text": "As shown in Fig. 2, we propose a multi-round learning paradigm that motivates the student LM and the teacher LLM to learn feedback from each other in an interactive manner. Specifically, each round of learning consists of three key steps: (1) The student LM undergoes an \u201cexam\u201d on the training set for collecting mistakes which are the wrong generated rationales. Existing works (Fu et al., 2023b; Ho et al., 2023; Shridhar et al., 2023; Magister et al., 2023) merely provide the sample question for the LLM to collect annotated rationales, neglecting the importance of the student\u2019s feedback. However, the student\u2019s feedback is crucial in knowledge distillation (Fu et al., 2021; Pham et al., 2021; Ren et al., 2023). (2) Therefore, we propose to curate a prompt integrated with the student\u2019s wrong rationale to ask the teacher LLM to generate customized feedback for the student. (3) In the last step, the student learns to reason via training on the tailored training data collected from the LLM, and selfreflection on its self-made mistakes. These steps are iterated to improve the reasoning ability of the student LM until convergence."
        },
        {
            "heading": "3.1 Undertaking an Exam",
            "text": "Given a dataset Dtrain = {(x, y)}, where x is the question and y is the answer, the correct rationale r is often not provided. During inference of CoT, the input is the question x, and the student LM\u2019s generated output f(x) = [r\u0302, y\u0302] is the concatenation of the generated rationale r\u0302 and answer y\u0302. The answer is often at the end of the output.\nThe student LM undertakes an \u201cexam\u201d on the training set Dtrain for evaluating the learning sta-\ntus, and collecting the mistakes Dneg which are the samples with wrong rationales and answers1:\nDneg = {(x, r\u0302, y\u0302) | y\u0302 \u0338= y, (x, y) \u2208 Dtrain}, (1)\nfor each question, we collect up to 4 wrong rationales through the decoding with sampling strategy. The collected mistake set Dneg reflecting the student\u2019s learning status and weakness are used for the following two purposes:\n(1) As the feedback for the teacher LLM to generate rationales tailored for the student.\n(2) As the negative contrastive samples for the student to learn from self-reflection."
        },
        {
            "heading": "3.2 Student\u2019s Feedback to LLM",
            "text": "We expect the black-box LLM to be a reasoning teacher instead of a data annotator. Thus, we propose to provide the student\u2019s feedback to help the teacher LLM generate customized training data to effectively target the student\u2019s weakness. In detail, we devise a prompt template T shown in Fig. 3, which integrates both the question x and the student\u2019s feedback (i.e., the wrong rationale r\u0302). The student\u2019s feedback can not only (1) assist teacher in identifying deficiencies in student\u2019s reasoning, but also (2) as the wrong demonstration example to help LLM increase the chance of generating correct rationales. Besides, to improve the LLM\u2019s accuracy and reduce the costs of calling APIs, we follow Zelikman et al. (2022) by adding a hint to explicitly tell LLM the golden answer of the question.\nFor each sample (x, r\u0302, y\u0302) \u2208 Dneg, we request the LLM with T (x, r\u0302, y\u0302) to generate 4 rationales, and only those containing correct answers are retained, since training with diverse reasoning paths can boost the reasoning performance of smaller LMs (Ho et al., 2023; Fu et al., 2023b). The collected rationale together with its question and answer is denoted as (x, r, y), which extends the original data to the customized training data Dtrain."
        },
        {
            "heading": "3.3 Tailored Learning",
            "text": "The reasoning ability of student LM f can be improved via tailored learning from both selfreflection and teacher\u2019s customized training data.\nLearning from Self-Reflection We propose to learn from the mistakes Dneg to simulate the selfreflection process of humans, which can help the\n1Following most existing works, we simply judge the quality of the generated rationale by the correctness of its answer.\nstudent LM to identify the quality of different rationales. The utilization can be defined in multiple forms (e.g., likelihood ranking), here we adopt a simple triplet-loss to encourage the model to learn different representations for good and bad rationales. Specifically, the wrong reasoning path [x, r\u0302, y\u0302] \u2208 Dneg, and the correct reasoning path [x, r\u2032, y] \u2208 Dtrain are utilized as the negative and positive contrastive samples, respectively. The hidden state of the last token is used as the representation of the whole reasoning path, which is denoted as h(r,y)x . Finally, the form of self-reflection learning is defined as follows:\nLcl = EDtrain max { 0, \u03c1\u2212 cos(h(r,y)x , h(r \u2032,y) x )\n+ cos(h(r,y)x , h (r\u0302,y\u0302) x )\n} , (2)\nwhere cos denotes the cosine similarity function, and \u03c1 set to 1.0 is the margin. (x, r, y) \u2208 Dtrain is the anchor sample whose positive and negative samples are randomly sampled from Dtrain and Dneg with the same question x, respectively2.\nLearning from Customized Feedback LLM\u2019s generated rationales are tailored to the student\u2019s weakness, thanks to the previous student\u2019s feedback. These collected rationales merged into the training set Dtrain as the customized feedback for the student, which is used to fine-tune the student LM f . In addition, we add several fixed demonstrations \u201cdemo\u201d listed in Table 15 to the prefix of each input sample, since recent research (Min et al., 2022; Zelikman et al., 2022; Fu et al., 2023b) have shown that training with demonstration examples can improve the in-context learning ability of LMs. The training objective is as follows:\nLlm = EDtrain log Pf ([demo, x, r, y]) , (3)\nwhere the square brackets represent the string concatenation. This process can directly help the student LM learn to generate intermediate reasoning steps and master the CoT skill.\n2Recall that we collect up to 4 unique correct and wrong rationales for each question in Dtrain and Dneg, respectively.\nAlgorithm 1 Multi-round learning paradigm. Require: the student LM f , the teacher LLM, the training\ndata Dtrain, the template T in Fig. 3 1: Initialize f0 with pre-trained weights and set the learning\nround count r \u2190 0 2: repeat 3: r \u2190 r + 1; fr \u2190 fr\u22121 4: Infer on Dtrain with f and collects the mistakes\n(x, r\u0302, y\u0302) \u223c Dneg by Eq. (1) 5: if r \u2264 1 then 6: Collect the rationale r for each sample of Dtrain\nfrom teacher LLM with T (x, null, y) 7: else 8: Collect the rationale r for each sample of Dneg\nfrom teacher LLM with T (x, r\u0302, y) 9: end if\n10: Optimize weights of fr using Eq. (4) 11: until Converges\nJoint Learning The final optimization incorporates the learning from both self-reflection and LLM\u2019s customized feedback. The contrastive learning loss in Eq. (2) and the language modeling loss in Eq. (3) are combined as follows:\nL = Llm + \u03bbLcl, (4)\nwhere \u03bb controls the impacts of self-reflection learning, balancing the two learning objectives."
        },
        {
            "heading": "3.4 Multi-round Learning",
            "text": "As depicted in Fig. 2, we adopt a multi-round learning paradigm to iteratively cultivate the reasoning ability of the student LM. Multiple rounds of learning can assist the teacher LLM in staying updated on the student\u2019s learning status, and thus offer more customized training data. Based on the student\u2019s learning status, the customized training data and self-made mistakes are adjusted in each round and tailored to the student\u2019s specific deficiencies.\nThe untrained student LM nearly has no reasoning ability, resulting in the noisy generations which are unhelpful as the feedback to the teacher LLM. Consequently, to prepare the data required by the initial round, we directly request the teacher LLM to generate rationales for the entire training set excluding the noisy feedback from the student. In the subsequent rounds, we adhere to the procedures outlined in Sections 3.1 to 3.3: (1) the student LM takes an \u201cexam\u201d to reveal self deficiencies and collect mistakes. (2) The teacher LLM is requested to generate customized training data based on the student\u2019s feedback. (3) The student is trained via learning both from self-reflection and teacher\u2019s customized feedback. These steps are repeated until\nthe student\u2019s performance reaches a plateau. The whole paradigm is summarized in Algorithm 1."
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Tasks & Datasets",
            "text": "Mathematical Task We adopt three math word problem datasets to evaluate the mathematical reasoning ability. GSM8k is a primary school level mathematical dataset (Cobbe et al., 2021). MultiArith is a multi-step arithmetic reasoning dataset (Roy and Roth, 2015). SVAMP is created by applying chosen variations over examples sampled from existing datasets (Patel et al., 2021).\nCommonsense Task We use two closed-ended question answering datasets to evaluate the commonsense reasoning ability. CSQA (Talmor et al., 2019) is a multi-choice commonsense question answering dataset. StrategyQA dataset (Geva et al., 2021) which implicitly requires reasoning steps and strategies to answer the yes-no questions."
        },
        {
            "heading": "4.2 Models & Baselines",
            "text": "Models Following previous works (Ho et al., 2023; Zelikman et al., 2022; Hu et al., 2023), we mainly utilize a publicly available LM GPTJ (Wang and Komatsuzaki, 2021) as our student LM which has about 6B parameters. Considering the pricing and availability, we select ChatGPT3, a popular black-box 175B LLM provided by OpenAI, as our teacher LLM.\nBaselines To demonstrate the effectiveness of our method, we compare with the following baselines: (1) the teacher LLM and student LM (w/o fine-tuning), for showing the effectiveness of distilling reasoning ability from the LLM. (2) Methods without the help of LLMs, including the student fine-tuned to directly generate answers without rationales, and STaR (Zelikman et al., 2022) which self-iteratively trains the LM to generate rationales and answers with very few annotated data. They are compared to highlight the importance of highquality rationales in teaching smaller LMs. (3) Three concurrent works which all use LLMs to help train smaller LMs to reason, including LM fine-tuned on CoT data (Magister et al., 2023), Specializing smaller LMs for mathematical reasoning (Fu et al., 2023b), and the LLM-adapter (Hu et al., 2023) which utilizes adapters for efficiently\n3https://chat.openai.com/chat. Most experiments are conducted between February and April of 2023.\ntuning on CoT data. (4) Our one-round distillation method, for demonstrating the superiority of the proposed multi-round learning paradigm."
        },
        {
            "heading": "4.3 Experimental Setup",
            "text": "The student is fine-tuned with a learning rate of 1e\u22126 in 10 epochs using AdamW (Loshchilov and Hutter, 2019) in default. Without any heavy tuning, \u03bb in Eq. (4) is set to 0.5 to control the impact of self-reflection. The CoT prompt accompanied by a fixed 3-shot demonstration is used for most datasets to balance the efficiency and performance. Some prompts are referred to previous research (Zelikman et al., 2022). And we use greedy decoding to generate the rationale and answer for evaluation. More implementation details are in Appendix A."
        },
        {
            "heading": "4.4 Main Results",
            "text": "The evaluation results are presented in Table 1.\nEffect of Distillation From the results of smaller LM with or without distillation, it is evident that the reasoning performance of smaller LM can be significantly improved by distilling the reasoning ability from LLM. Although the student LM falls short in mathematical reasoning, it can achieve comparable performance in commonsense reasoning with the teacher LLM while being 20x smaller in size.\nImportance of Rationales CoT can significantly improve reasoning performance which shows the necessity of high-quality rationales in teaching smaller LMs. Though STaR performs well in CSQA which often only involves single-step reasoning, the self-generated rationales encounter dif-\nficulties when applied to other multi-step reasoning tasks. Conversely, nearly all distillation methods can beat STaR in mathematical reasoning, which indicates that LLM\u2019s generated rationales can often better guide the smaller LM to reason.\nComparison with Concurrent Works Compared to concurrent distillation works (Hu et al., 2023; Fu et al., 2023b; Magister et al., 2023), our method consistently achieves better performance across all datasets, which demonstrates the success of customized feedback from the black-box LLM. For GSM8K, in contrast to training an 11B model with 130k rationales used by Specializing, our method can yield better performance with a 6B model and only 54k rationales, significantly reducing the cost of model training and data collection.\nEffect of Multi-round & Self-reflection Compared with our one-round distillation approach, multi-round learning leads to an average improvement of 5.1 in accuracy, indicating the success of building the interaction between teacher and student. Besides, the self-reflection learning can further exploit the reasoning potential of the student LM. Another advantage is that the self-reflection can be seamlessly integrated into multi-round learning, pushing the boundaries of reasoning."
        },
        {
            "heading": "5 Analysis",
            "text": ""
        },
        {
            "heading": "5.1 Contribution of Student\u2019s Feedback",
            "text": "To validate the contribution of student\u2019s feedback to LLM, an ablation study is conducted by removing this feedback of the requesting prompt template\n(Fig. 3). Results in Table 3 show that student feedback to LLM can first help the teacher LLM to generate more accurate and tailored rationales (larger # Success), which is then beneficial to the student\u2019s learning (higher Accuracy). Note that cooperating with our multi-round learning paradigm, the cumulative gains of student\u2019s feedback can be substantial. Further, we take a case study of the teacher LLM\u2019s generated rationales in Table 2 which shows that the LLM can often response improved rationales when the student\u2019s feedback is taken into account. For StrategyQA, the teacher LLM even gives a counterexample to the student\u2019s wrong answer, indicating the LLM can provide customized training data based on the student\u2019s feedback."
        },
        {
            "heading": "5.2 Effect of Self-Reflection",
            "text": "First, to intuitive understand the effect of selfreflection learning, Fig. 4 visualizes the latent space representations of generated rationales. It shows that the self-reflection could effectively cluster correct rationales and wrong ones respectively, helping the model to distinguish each other. Moreover, we compare the distance and preference differences in Table 4 which indicates that the self-reflection contributes to aligning the preference of the student\nLM with correct reasoning paths, while away from self-made wrong ones.\nFig. 5 illustrates the effect of the self-reflection learning on the reasoning performance. The observation is consistent with findings in Table 1 that self-reflection learning can help improve the reasoning ability when \u03bb < 0.5. However, excessive emphasis on self-reflection learning (i.e., a larger value of \u03bb) typically leads to poorer performance and instability, especially for the MultiArith dataset. We conjecture that it has a negative impact on the learning of teacher\u2019s training data.\nTo verify the above hypothesis, we plot the loss curve in Fig. 6. It shows that the excessive emphasis on self-reflection learning (higher \u03bb) can result in underfitting of the these training data within a limited number of training steps. Consequently, the reasoning performance of the student LM could be significantly decreased due to not fully converged. In general, a small value of \u03bb is preferred to achieve a balanced learning approach that incorporates both the teacher\u2019s rationales and self-made mistakes."
        },
        {
            "heading": "5.3 Analysis of Multi-round Learning",
            "text": "We examine each learning round of the student LM, as detailed in Table 5. The error rate and accuracy are typically gradually decreased and increased with the learning rounds, respectively. This is because of each round of learning aims to enhance the student LM in solving the questions that were not learned well in previous round. Additionally, inspired by recent research on employing the LLM as the evaluator (Chiang and Lee, 2023; Fu et al., 2023a; Liu et al., 2023), we instruct GPT-4 (OpenAI, 2023) to automatically evaluate the quality of generated rationales. From the results in Table 6, we find that there is an enhancement in the quality of both generated correct rationales and wrong ones as the learning rounds progress. However, the gains in reasoning performance reach a plateau after several rounds of training. This can be attributed as follows: (1) For GSM8K, the most challenging task, the student is reaching its capacity after 3 rounds of learning, still not performing well (49.2 ER). (2) For SVAMP and CSQA, relatively easy tasks, the student achieves a good performance on the training set after the 2nd round, leading to a small ER. Consequently, the prepared data for the next round will be relatively scarce, which is unlikely to further help improve the student.\nWe conduct the 4th round learning on GSM8K for justifying the above analysis, where the ER remains unsatisfactory (51.8 ER) despite a marginal improvement (+1.4\u2206) in accuracy. Besides, the results of the 3rd round on SVAMP and CSQA datasets show that there are no more gains after the 2nd round. Thus, we suggest to take early stopping\nin the multi-round learning if the student can nearly reach its plateau. By prior estimation of the task difficulty and observing performance gains in each round, we can avoid excessive parameter tuning on the number of learning rounds and balance the reasoning performance and training costs."
        },
        {
            "heading": "5.4 Feasibility Study",
            "text": "To further benefit the community concerning about individual affordable computation resources, we conduct a feasibility study by using different LMs spanning from 760M to 2.7B parameters. The tested models include two common LM architectures, i.e., encoder-decoder and decoder-only. The results shown in Table 7 first suggest that the reasoning abilities of these small LMs can all be en-\nhanced with the proposed self-reflection learning. With self-reflection, student LMs often achieve satisfying performance with just one round of learning for commonsense tasks. Moreover, we find that our multi-round learning can generally further improve the performance in mathematical reasoning. However, there are no more gains for StrategyQA, as it heavily relies on the memorization of commonsense knowledge mostly acquired from the pretraining stage, rather than on complex reasoning. Another evidence is that increasing the model size seems not to have contribution to the performance on this dataset. Besides, the relatively limited capacity of these smaller LMs may also restrict the gains from additional rounds of learning."
        },
        {
            "heading": "6 Conclusion",
            "text": "In this paper, we propose a tailored learning approach to cultivate the reasoning ability of the smaller LM, aiming to democratize the emergent reasoning ability of the LLM. First, we propose a multi-round interactive learning paradigm that enables the teacher LLM to provide customized training data according to the student\u2019s feedback. Next, we propose the self-reflection learning to motivate the student to distinguish correct rationales from wrong ones. Further, the integration of learning from LLM\u2019s customized feedback and self-reflection can complement each other within the proposed multi-round learning paradigm. The empirical results from mathematical and commonsense reasoning tasks demonstrate the success of unleashing the reasoning potential of smaller LMs. We believe that these findings can benefit the opensource and NLP communities in the era of LLM.\nLimitations\nIn this section, we discuss the limitations of our method with integrity while offering potentially useful advice for future research.\n1) Our experiments primarily utilize ChatGPT and GPT-J (Wang and Komatsuzaki, 2021) as the teacher LLM and student LM, respectively, due to the considerations of availability and costs. Although fine-tuning GPT-J on the outputs of ChatGPT boosts their reasoning performance, a substantial gap still remains between them. It is valuable to validate our findings using more powerful LMs (e.g., LLaMA (Touvron et al., 2023a,b)). And training better foundation LMs should be the primary task for the open-source community, since imitating proprietary LLMs may be a false promise (Gudibande et al., 2023).\n2) We have demonstrated the importance of student\u2019s feedback in distilling the knowledge from the black-box LLM, but without extensive engineering the feedback prompt templates (e.g., explicitly instructing the LLM to act as a teacher). And the interactions (e.g., use reinforcement learning to connect LLM and smaller LM) can be explored in the future.\n3) Our self-reflection learning currently is defined in a straightforward triplet-loss form. However, the core of self-reflection is learning from mistakes. Thus, the training objectives or forms can be defined in various ways, such as ranking loss or verbal critic are expected to further help the smaller LMs to reflect and learn from mistakes.\n4) Evaluating the correctness of generated rationale is mainly based on the final answer. Though most existing works (Zelikman et al., 2022; Ho et al., 2023; Fu et al., 2023b; Shridhar et al., 2023) in this field adopt this simple criterion, we call attention to develop more trustworthy criteria to evaluate the quality of rationales. Potential methods can be using GPT-4 (OpenAI, 2023) or a process reward model (Lightman et al., 2023) for automatic evaluation.\nEthics Statement\nRisk in using closed-source LLMs Though the datasets used for evaluation is publicly available, the annotated rationales in this paper are collected from close-source ChatGPT provided by OpenAI.\nOpen-source LLMs (e.g., LLaMA) have boomed in recent months, it is noteworthy that many of them use the outputs from closed-source LLMs (e.g., Alpaca and Vicuna are trained on ChatGPT\u2019s outputs) for further improvements. According to the Sec. 2 \"Usage Requirements\", within OpenAI\u2019s terms of use4, there exists a prohibition against \"use output from the Services to develop models that compete with OpenAI\". However, beyond its terms of use, the crucial matter lies in determining \"ownership of the copyright pertaining to the outputs of generative AI\". As of today, there remains an ambiguity regarding the copyright status of generative AI outputs, both in scholarly circles and legal contexts. Compelling evidence indicates that these closed-source LLMs undergo training using numerous copyrighted materials, such as books, academic publishings, etc. Thus, we think at least the authors of the training data that directly supports LLM\u2019s outputs hold the copyright, as opposed to the LLM service provider. The prompt creators may also hold the copyright if their prompts substantially influence LLM\u2019s outputs. For open-source and research communities, we call for a responsible discussion about data collection.\nSocial Impact This paper explores how to utilize the LLM as a teacher to enhance the reasoning performance of smaller LMs, which can help democratize these emergent abilities for the benefit of broader communities (e.g., math education). Furthermore, we firmly believe that the utilization of LLMs can be a significant area of interest in natural language processing applications and research."
        },
        {
            "heading": "Acknowledgements",
            "text": "We thank the anonymous reviewers for their insightful and valuable comments. This work is supported by the National Natural Science Foundation of China (62072483), and the Guangdong Basic and Applied Basic Research Foundation (2022A1515011690, 2021A1515012298)."
        },
        {
            "heading": "B Generalization Results",
            "text": "Generalization experiments are conducted to evaluate the generalization of the student LM, as shown in Table 11. The results reveal the following insights: (1) the in-domain generalization performance is enhanced after the reasoning distillation, while the out-of-domain (OOD) performance is usually slightly decreased. This finding is consistent with Fu et al. (2023b) although our method is better than theirs in terms of OOD performance. (2) The in-domain performance can be further improved by employing our multi-round learning paradigm. And we surprisingly find that, for some cases, the OOD performance can also be improved via multi-round learning. This can be attributed to that the customized training data of the following rounds may assists the model in generalizing its reasoning abilities to other domains. (3) The student LM trained on the GSM8K dataset exhibits the most significant improvements in in-domain reasoning performance. Note that the GSM8K dataset is the most challenging one among these mathematical datasets. Consequently, it is reasonable to expect gains on the other datasets if the student can already tackle the difficult problems."
        },
        {
            "heading": "C Case Study",
            "text": "Contribution of Student\u2019s Feedback Additional examples of the LLM\u2019s generated rationales are presented in Table 13. We observe that the teacher LLM, ChatGPT, is capable of generating more detailed and precise reasoning steps when provided with student\u2019s feedback (i.e., wrong solution). These detailed reasoning steps can help address the student\u2019s deficiencies and thereby improve the reasoning performance in the subsequent round of learning. Although both rationales, with and without feedback, are correct, their quality can vary. More precise and customized rationales can help the student better understand its own mistakes, especially coupled with our self-reflection learning, which is beneficial for student\u2019s reasoning learning.\nMulti-round Learning To better understand the impact of each learning round, we conduct a case study in Table 14. First, it is clear that the student LM initialized with pre-trained weights (i.e., the 0th round) is powerless to generate meaningful answers for the mathematical reasoning task, which may confuse the teacher LLM. Thus, we tend not to utilize these noisy feedback for preparing the train-\ning data of the initial round. Second, the LLM\u2019s generated response is often tailored to student\u2019s current deficiencies, thus effectively improving student\u2019s reasoning performance in the next round of learning. Third, a single round of distillation may not enable the student to solve challenging questions. However, with the help of our multi-round learning paradigm, the student can have the opportunity to tackle such challenging questions.\nQuestion: Is Europa linked to Viennese waltzes? Wrong Solution: Europa is a moon of Jupiter, and it is named after a Phoenician queen of Crete. Viennese waltzes are a type of dance that originated in Austria. So, the answer is No. Please correct the wrong solution by using better reasoning steps. Hint: The final answer should be yes. Better Reasoning: While Europa is a moon of Jupiter and named after a Phoenician queen of Crete, it is also the title of a famous waltz composed by Austrian composer Johann Strauss II in 1889. The waltz is named after the mythical princess Europa from Greek mythology, who was abducted by Zeus and taken to Crete. So, the answer is Yes.\nQuestion: Would a honey badger fit inside an oven? Wrong Solution: Honey badgers are large animals that can grow up to be about the size of a small dog. An oven is a small, enclosed space. So, the answer is No. Please correct the wrong solution by using better reasoning steps. Hint: The final answer should be yes. Better Reasoning: The answer is actually Yes, a honey badger could fit inside an oven, but the size of a honey badger can vary and an oven can also vary in size. So, the answer is yes.\nQuestion: Would a pacifist be opposed to hunting? Wrong Solution: Pacifists believe in non-violence and do not support the use of violence. Hunting is a form of violence. So, the answer is Yes. Please correct the wrong solution by using better reasoning steps. Hint: The final answer should be no. Better Reasoning: While pacifists do believe in non-violence, their opposition is generally focused on human-on-human violence. Some pacifists may be opposed to hunting due to their belief in non-violence towards animals, but it is not necessarily a universal position among pacifists. So, the answer is No.\nQuestion: { question x } Wrong Solution: { mistake r\u0302 } Please correct the wrong solution by using better reasoning steps. Hint: The final answer should be { answer y }. Better Reasoning: { Expected LLM\u2019s customized feedback }\nTable 12: An example of the prompt template introduced in Fig. 3 for asking the LLM to generate customized rationales, for a given question x, student\u2019s mistake r\u0302, and the ground truth answer y from the StrategyQA dataset.\nGSM8K Example#1\nQuestion: The number of short students in a class is 2/5 of the total number of students. If there are 90 tall students, and the class has 400 students, calculate the total number of students with average height. Ground Truth: 150\nRound Student\u2019s Generation Correctness: %\n0th The number of short students is 2/5 of the total number of students.The number of tall students is 2/5 of the total number of students. The number of students with average height is 2/5 of the total number of students.\nTeacher\u2019s Feedback\n1st The number of short students in the class is 2/5 of the total number of students, which means there are (2/5) * 400 = 160 short students. If there are 90 tall students, then the number of students with average height can be calculated by subtracting the number of short and tall students from the total number of students. The total number of students with average height is 400 - 160 - 90 = 150 students.\nStudent\u2019s Generation Correctness: %\n1st There are 90 tall students in the class. There are 2/5 of 400 students, which is 2/5 * 90 = 36 short students. So the total number of students in the class is 90 + 36 = 126.\nTeacher\u2019s Feedback\n2nd There are 90 tall students in the class. Short students are 2/5 of the total number of students. The total number of students in the class is 400. Short students: 2/5 * 400 = 160. We know the total number of students in the class is 400, so 90 + 160 = 250 is the number of tall and short students. Therefore, the total number of students with average height is 400 - 250 = 150.\nStudent\u2019s Generation Correctness: %\n2nd The number of short students in a class is 2/5 of the total number of students. So, 2/5 * 400 = 160. The number of tall students in a class is 90. So, the total number of students with average height is 400 - 160 - 90 = 170.\nTeacher\u2019s Feedback\n3rd The total number of students in the class is 400. The number of short students is 2/5 of the total number of students, 2/5 * 400 = 160. The number of tall students is 90. The total number of short and tall students is 160 + 90 = 250. So the total number of students with average height is 400 - 250 = 150.\nStudent\u2019s Generation Correctness: \"\n4th The number of short students is 2/5 of the total number of students, which is 2/5 * 400 = 160. The total number of students with short height is 160 + 90 = 250. The total number of students with average height is 400 - 250 = 150.\nCSQA Example#1\nQuestion: Where do students keep rubber bands? (a) trees (b) drug store (c) trash (d) beside table (e) classroom Ground Truth: (e)\n3-shot demonstrations of GSM8K dataset\nQuestion: Albert is wondering how much pizza he can eat in one day. He buys 2 large pizzas and 2 small pizzas. A large pizza has 16 slices and a small pizza has 8 slices. If he eats it all, how many pieces does he eat that day? \\n Reasoning: A large pizza has 16 slices, so 2 large pizzas have 32 slices. A small pizza has 8 slices, so 2 small pizzas have 16 slices. If we add 32 slices and 16 slices, we get 48 slices. Therefore, he will eat 48 slices of pizza in that day. \\n Answer: 48 Question: Mary does her grocery shopping on Saturday. She does her shopping only at a specific store where she is allowed a credit of 100, which must be paid in full before her next shopping trip. That week she spent the full credit limit and paid 15 of it on Tuesday and 23 of it on Thursday. How much credit will Mary need to pay before her next shopping trip? \\n Reasoning: Mary spends her entire credit limit of 100 on Saturday. On Tuesday, she pays 15 towards her debt. On Thursday, she pays 23 towards her debt. This leaves her with a remaining balance of 100 - 15 - 23, which is equal to 62. \\n Answer: 62 Question: Ralph is going to practice playing tennis with a tennis ball machine that shoots out tennis balls for Ralph to hit. He loads up the machine with 175 tennis balls to start with. Out of the first 100 balls, he manages to hit 2/5 of them. Of the next 75 tennis balls, he manages to hit 1/3 of them. Out of all the tennis balls, how many did Ralph not hit? \\n Reasoning: Ralph hits 2/5 of the first 100 balls, so he hits 40 balls. Then, Ralph hits 1/3 of the next 75 balls, so he hits 25 more balls. In total, Ralph hits 40 + 25 = 65 balls. Finally, we know that Ralph started with 175 balls, so 175 - 65 = 110 balls not hitted. \\n Answer: 110\n3-shot demonstrations of MultiArith dataset\nQuestion: There are 64 students trying out for the school\u2019s trivia teams. If 36 of them didn\u2019t get picked for the team and the rest were put into 4 groups, how many students would be in each group? \\n Reasoning: The number of students who got picked for the team is 64 - 36 = 28. To find how many students would be in each group, we need to divide the number of students by the number of groups, which is 28 / 4 = 7. \\n Answer: 7 Question: Cody bought 7 boxes of chocolate candy and 3 boxes of caramel candy. If each box has 8 pieces inside it, how much candy did he have total? \\n Reasoning: First, we need to find the total number of boxes Cody bought, which is 7 + 3 = 10 boxes. Then, we can multiply the number of boxes by the number of pieces of candy in each box to find the total amount of candy. Therefore, Cody had 10 x 8 = 80 pieces of candy in total. \\n Answer: 80 Question: For Halloween Robin scored 23 pieces of candy. She ate 7 pieces the first night and then her sister gave her 21 more pieces. How many pieces of candy does Robin have now? \\n Reasoning: We need to add the number of pieces of candy she had after the first night to the number of pieces her sister gave her. Therefore, the total number of pieces of candy Robin has now is 23 - 7 + 21 = 37. \\n Answer: 37\n3-shot demonstrations of SVAMP dataset\nQuestion: Paul had 50 books. After buying some in a garage sale he had 151 left. How many books did he buy? \\n Reasoning: The number of books Paul bought can be found by subtracting the final number of books from the initial number of books: 151 - 50 = 101. Therefore, Paul bought 101 books in the garage sale. \\n Answer: 101 Question: Luke played a trivia game and scored 154 points. If he gained the 11 points in each round. How many rounds did he play? \\n Reasoning: We need to divide Luke\u2019s total score by the number of points he gained in each round. Therefore, the number of rounds Luke played is 154 / 11 = 14. \\n Answer: 14 Question: Julia played tag with 17 kids on monday, 15 kids on tuesday and 2 kids on wednesday. How many kids did she play with altogether? \\n Reasoning: To find the total number of kids Julia played with, we need to add the number of kids she played with on each day. Therefore, the total number of kids Julia played with is 17 + 15 + 2 = 34. \\n Answer: 34\n5-shot demonstrations of CSQA dataset\nQuestion: What do people use to absorb extra ink from a fountain pen? \\n Answer Choices: \\n (a) shirt pocket \\n (b) calligrapher\u2019s hand \\n (c) inkwell \\n (d) desk drawer \\n (e) blotter \\n Answer: The answer must be used to absorb extra ink. Blotters are designed to absorb liquids. Therefore, the answer is blotter (e). Question: What home entertainment equipment requires cable? \\n Answer Choices: \\n (a) radio shack \\n (b) substation \\n (c) television \\n (d) cabinet \\n (e) desk \\n Answer: The answer must require cable. Cable is used to provide satellite channels to televisions. Therefore, the answer is television (c). Question: Sammy wanted to go to where the people were. Where might he go? \\n Answer Choices: \\n (a) populated areas \\n (b) race track \\n (c) desert \\n (d) apartment \\n (e) roadblock \\n Answer: The answer must be a place with many people. Populated areas, by definition, have a lot of people. Therefore, the answer is populated areas (a). Question: Where do you put your grapes just before checking out? \\n Answer Choices: \\n (a) mouth \\n (b) grocery cart \\n (c) super market \\n (d) fruit basket \\n (e) fruit market \\n Answer: The answer should be the place where grocery items are placed before checking out. Of the above choices, grocery cart makes the most sense for holding grocery items. Therefore, the answer is grocery cart (b). Question: Google Maps and other highway and street GPS services have replaced what? \\n Answer Choices: \\n (a) united states \\n (b) mexico \\n (c) countryside \\n (d) atlas \\n (e) oceans \\n Answer: The answer must be something that used to do what Google Maps and GPS services do, which is give directions. Atlases were also used to give directions. Therefore, the answer is atlas (d).\n4-shot demonstrations of StrategyQA dataset\nQuestion: Are chinchillas cold-blooded? \\n Reasoning: Chinchillas are rodents, which are mammals. All mammals are warm-blooded. So, the answer is No. \\n Answer: No Question: Would Janet Jackson avoid a dish with ham? \\n Reasoning: Janet Jackson follows an Islamic practice. Islamic culture avoids eating pork. Ham is made from pork. So, the answer is Yes. \\n Answer: Yes Question: Can a honey bee sting a human more than once? \\n Reasoning: Human skin is tough, and the bee\u2019s stinger gets lodged in the skin. The stinger becomes separated from the bee which dies soon after. So, the answer is No. \\n Answer: No Question: Is average number of peas in a pod enough commas for a billion? \\n Reasoning: The average number of peas in a pod is 6 or 7. A billion is a number that has only 3 commas in it. So, the answer is Yes. \\n Answer: Yes\nTable 15: The demonstrations used for each dataset. The \u201c\\n\u201d indicates a line break. The key token is marked in bold for clear view. The prompt for CSQA is slightly different from others since we adopt the original prompt template of STaR (Zelikman et al., 2022). And we only use 5 out of 7 demonstrations from STaR."
        }
    ],
    "title": "Democratizing Reasoning Ability: Tailored Learning from Large Language Model",
    "year": 2023
}