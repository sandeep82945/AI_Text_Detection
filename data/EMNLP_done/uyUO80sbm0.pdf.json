{
    "abstractText": "This work explores the use of self-generated natural language explanations as an intermediate step for code-to-code translation with language models. Across three types of explanations and 19 programming languages constructed from the MultiPL-E dataset (Cassano et al., 2022), we find the explanations to be particularly effective in the zero-shot case, improving performance by 12% on average. Improvements with natural language explanations are particularly pronounced on difficult programs. We release our dataset, code, and canonical solutions in all 19 languages.1",
    "authors": [
        {
            "affiliations": [],
            "name": "Zilu Tang"
        },
        {
            "affiliations": [],
            "name": "Mayank Agarwal"
        },
        {
            "affiliations": [],
            "name": "Alex Shypula"
        },
        {
            "affiliations": [],
            "name": "Bailin Wang"
        },
        {
            "affiliations": [],
            "name": "Derry Wijaya"
        },
        {
            "affiliations": [],
            "name": "Jie Chen"
        },
        {
            "affiliations": [],
            "name": "Yoon Kim"
        }
    ],
    "id": "SP:13df3b98cf73a03b3d8d383e56423ebb1adb6bfc",
    "references": [
        {
            "authors": [
                "Wasi Ahmad",
                "Saikat Chakraborty",
                "Baishakhi Ray",
                "Kai-Wei Chang."
            ],
            "title": "Unified pre-training for program understanding and generation",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics:",
            "year": 2021
        },
        {
            "authors": [
                "Loubna Ben Allal",
                "Raymond Li",
                "Denis Kocetkov",
                "Chenghao Mou",
                "Christopher Akiki",
                "Carlos Munoz Ferrandis",
                "Niklas Muennighoff",
                "Mayank Mishra",
                "Alex Gu",
                "Manan Dey"
            ],
            "title": "Santacoder: don\u2019t reach for the stars! arXiv preprint arXiv:2301.03988",
            "year": 2023
        },
        {
            "authors": [
                "Ben Athiwaratkun",
                "Sanjay Krishna Gouda",
                "Zijian Wang",
                "Xiaopeng Li",
                "Yuchen Tian",
                "Ming Tan",
                "Wasi Uddin Ahmad",
                "Shiqi Wang",
                "Qing Sun",
                "Mingyue Shang"
            ],
            "title": "Multi-lingual evaluation of code generation models",
            "year": 2022
        },
        {
            "authors": [
                "Jacob Austin",
                "Augustus Odena",
                "Maxwell Nye",
                "Maarten Bosma",
                "Henryk Michalewski",
                "David Dohan",
                "Ellen Jiang",
                "Carrie Cai",
                "Michael Terry",
                "Quoc Le"
            ],
            "title": "Program synthesis with large language models. arXiv preprint arXiv:2108.07732",
            "year": 2021
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners. Advances in neural information processing",
            "year": 2020
        },
        {
            "authors": [
                "Federico Cassano",
                "John Gouwar",
                "Daniel Nguyen",
                "Sydney Nguyen",
                "Luna Phipps-Costin",
                "Donald Pinckney",
                "Ming Ho Yee",
                "Yangtian Zi",
                "Carolyn Jane Anderson",
                "Molly Q Feldman"
            ],
            "title": "A scalable and extensible approach to benchmarking nl2code",
            "year": 2022
        },
        {
            "authors": [
                "Aaron Chan",
                "Shaoliang Nie",
                "Liang Tan",
                "Xiaochang Peng",
                "Hamed Firooz",
                "Maziar Sanjabi",
                "Xiang Ren."
            ],
            "title": "Frame: Evaluating rationale-label consistency metrics for free-text rationales",
            "venue": "arXiv preprint arXiv:2207.00779.",
            "year": 2022
        },
        {
            "authors": [
                "Angelica Chen",
                "J\u00e9r\u00e9my Scheurer",
                "Tomasz Korbak",
                "Jon Ander Campos",
                "Jun Shern Chan",
                "Samuel R Bowman",
                "Kyunghyun Cho",
                "Ethan Perez."
            ],
            "title": "Improving code generation by training with natural language feedback",
            "venue": "arXiv preprint arXiv:2303.16749.",
            "year": 2023
        },
        {
            "authors": [
                "Morikawa",
                "Alec Radford",
                "Matthew Knight",
                "Miles Brundage",
                "Mira Murati",
                "Katie Mayer",
                "Peter Welinder",
                "Bob McGrew",
                "Dario Amodei",
                "Sam McCandlish",
                "Ilya Sutskever",
                "Wojciech Zaremba."
            ],
            "title": "Evaluating large language models trained on code",
            "venue": "arXiv",
            "year": 2021
        },
        {
            "authors": [
                "Xinyun Chen",
                "Maxwell Lin",
                "Nathanael Sch\u00e4rli",
                "Denny Zhou."
            ],
            "title": "Teaching large language models to self-debug",
            "venue": "arXiv preprint arXiv:2304.05128.",
            "year": 2023
        },
        {
            "authors": [
                "Yi Chen",
                "Rui Wang",
                "Haiyun Jiang",
                "Shuming Shi",
                "Ruifeng Xu."
            ],
            "title": "Exploring the use of large language models for reference-free text quality evaluation: A preliminary empirical study",
            "venue": "arXiv preprint arXiv:2304.00723.",
            "year": 2023
        },
        {
            "authors": [
                "Aakanksha Chowdhery",
                "Sharan Narang",
                "Jacob Devlin",
                "Maarten Bosma",
                "Gaurav Mishra",
                "Adam Roberts",
                "Paul Barham",
                "Hyung Won Chung",
                "Charles Sutton",
                "Sebastian Gehrmann"
            ],
            "title": "Palm: Scaling language modeling with pathways",
            "year": 2022
        },
        {
            "authors": [
                "Jinlan Fu",
                "See-Kiong Ng",
                "Zhengbao Jiang",
                "Pengfei Liu."
            ],
            "title": "Gptscore: Evaluate as you desire",
            "venue": "arXiv preprint arXiv:2302.04166.",
            "year": 2023
        },
        {
            "authors": [
                "Marjan Ghazvininejad",
                "Hila Gonen",
                "Luke Zettlemoyer."
            ],
            "title": "Dictionary-based phrase-level prompting of large language models for machine translation",
            "venue": "arXiv preprint arXiv:2302.07856.",
            "year": 2023
        },
        {
            "authors": [
                "Mareike Hartmann",
                "Daniel Sonntag."
            ],
            "title": "A survey on improving nlp models with human explanations",
            "venue": "Proceedings of the First Workshop on Learning with Natural Language Supervision, pages 40\u201347.",
            "year": 2022
        },
        {
            "authors": [
                "Peter Hase",
                "Mohit Bansal."
            ],
            "title": "When can models learn from explanations? a formal framework for understanding the roles of explanation data",
            "venue": "Proceedings of the First Workshop on Learning with Natural Language Supervision, pages 29\u201339.",
            "year": 2022
        },
        {
            "authors": [
                "Ari Holtzman",
                "Jan Buys",
                "Li Du",
                "Maxwell Forbes",
                "Yejin Choi."
            ],
            "title": "The curious case of neural text degeneration",
            "venue": "International Conference on Learning Representations.",
            "year": 2020
        },
        {
            "authors": [
                "Cheng-Yu Hsieh",
                "Chun-Liang Li",
                "Chih-Kuan Yeh",
                "Hootan Nakhost",
                "Yasuhisa Fujii",
                "Alexander Ratner",
                "Ranjay Krishna",
                "Chen-Yu Lee",
                "Tomas Pfister"
            ],
            "title": "Distilling step-by-step! outperforming larger language models with less training data and smaller",
            "year": 2023
        },
        {
            "authors": [
                "Xue Jiang",
                "Yihong Dong",
                "Lecheng Wang",
                "Qiwei Shang",
                "Ge Li."
            ],
            "title": "Self-planning code generation with large language model",
            "venue": "arXiv preprint arXiv:2303.06689.",
            "year": 2023
        },
        {
            "authors": [
                "Brihi Joshi",
                "Aaron Chan",
                "Ziyi Liu",
                "Xiang Ren."
            ],
            "title": "Er-test evaluating explanation regularization methods for nlp models",
            "venue": "Proceedings of the 2nd Workshop on Trustworthy Natural Language Processing (TrustNLP 2022), pages 93\u2013109.",
            "year": 2022
        },
        {
            "authors": [
                "Tom Kocmi",
                "Christian Federmann."
            ],
            "title": "Large language models are state-of-the-art evaluators of translation quality",
            "venue": "arXiv preprint arXiv:2302.14520.",
            "year": 2023
        },
        {
            "authors": [
                "Takeshi Kojima",
                "Shixiang Shane Gu",
                "Machel Reid",
                "Yutaka Matsuo",
                "Yusuke Iwasawa."
            ],
            "title": "Large language models are zero-shot reasoners",
            "venue": "ICML 2022 Workshop on Knowledge Retrieval and Language Models.",
            "year": 2022
        },
        {
            "authors": [
                "Satyapriya Krishna",
                "Jiaqi Ma",
                "Dylan Slack",
                "Asma Ghandeharioun",
                "Sameer Singh",
                "Himabindu Lakkaraju."
            ],
            "title": "Post hoc explanations of language models can improve language models",
            "venue": "arXiv preprint arXiv:2305.11426.",
            "year": 2023
        },
        {
            "authors": [
                "Marie-Anne Lachaux",
                "Baptiste Roziere",
                "Lowik Chanussot",
                "Guillaume Lample."
            ],
            "title": "Unsupervised translation of programming languages",
            "venue": "arXiv preprint arXiv:2006.03511.",
            "year": 2020
        },
        {
            "authors": [
                "Marie-Anne Lachaux",
                "Baptiste Roziere",
                "Marc Szafraniec",
                "Guillaume Lample."
            ],
            "title": "Dobf: A deobfuscation pre-training objective for programming languages",
            "venue": "Advances in Neural Information Processing Systems, 34:14967\u201314979.",
            "year": 2021
        },
        {
            "authors": [
                "Andrew Lampinen",
                "Ishita Dasgupta",
                "Stephanie Chan",
                "Kory Mathewson",
                "Mh Tessler",
                "Antonia Creswell",
                "James McClelland",
                "Jane Wang",
                "Felix Hill"
            ],
            "title": "Can language models learn from explanations in context? In Findings of the Association for Computa",
            "year": 2022
        },
        {
            "authors": [
                "Raymond Li",
                "Loubna Ben Allal",
                "Yangtian Zi",
                "Niklas Muennighoff",
                "Denis Kocetkov",
                "Chenghao Mou",
                "Marc Marone",
                "Christopher Akiki",
                "Jia Li",
                "Jenny Chim"
            ],
            "title": "Starcoder: may the source be with you! arXiv preprint arXiv:2305.06161",
            "year": 2023
        },
        {
            "authors": [
                "Wang Ling",
                "Dani Yogatama",
                "Chris Dyer",
                "Phil Blunsom."
            ],
            "title": "Program induction by rationale generation: Learning to solve and explain algebraic word problems",
            "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics",
            "year": 2017
        },
        {
            "authors": [
                "Hongyuan Lu",
                "Haoyang Huang",
                "Dongdong Zhang",
                "Haoran Yang",
                "Wai Lam",
                "Furu Wei."
            ],
            "title": "Chainof-dictionary prompting elicits translation in large language models",
            "venue": "arXiv preprint arXiv:2305.06575.",
            "year": 2023
        },
        {
            "authors": [
                "Shuai Lu",
                "Daya Guo",
                "Shuo Ren",
                "Junjie Huang",
                "Alexey Svyatkovskiy",
                "Ambrosio Blanco",
                "Colin Clement",
                "Dawn Drain",
                "Daxin Jiang",
                "Duyu Tang"
            ],
            "title": "Codexglue: A machine learning benchmark dataset for code understanding and generation",
            "year": 2021
        },
        {
            "authors": [
                "Yao Lu",
                "Max Bartolo",
                "Alastair Moore",
                "Sebastian Riedel",
                "Pontus Stenetorp."
            ],
            "title": "Fantastically ordered prompts and where to find them: Overcoming fewshot prompt order sensitivity",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Compu-",
            "year": 2022
        },
        {
            "authors": [
                "Tim Miller."
            ],
            "title": "Explanation in artificial intelligence: Insights from the social sciences",
            "venue": "Artificial intelligence, 267:1\u201338.",
            "year": 2019
        },
        {
            "authors": [
                "Sewon Min",
                "Mike Lewis",
                "Hannaneh Hajishirzi",
                "Luke Zettlemoyer."
            ],
            "title": "Noisy channel language model prompting for few-shot text classification",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1:",
            "year": 2022
        },
        {
            "authors": [
                "Sewon Min",
                "Xinxi Lyu",
                "Ari Holtzman",
                "Mikel Artetxe",
                "Mike Lewis",
                "Hannaneh Hajishirzi",
                "Luke Zettlemoyer"
            ],
            "title": "Rethinking the role of demonstrations: What makes in-context learning work? arXiv preprint arXiv:2202.12837",
            "year": 2022
        },
        {
            "authors": [
                "Subhabrata Mukherjee",
                "Arindam Mitra",
                "Ganesh Jawahar",
                "Sahaj Agarwal",
                "Hamid Palangi",
                "Ahmed Awadallah."
            ],
            "title": "Orca: Progressive learning from complex explanation traces of gpt-4",
            "venue": "arXiv preprint arXiv:2306.02707.",
            "year": 2023
        },
        {
            "authors": [
                "Erik Nijkamp",
                "Hiroaki Hayashi",
                "Caiming Xiong",
                "Silvio Savarese",
                "Yingbo Zhou."
            ],
            "title": "Codegen2: Lessons for training llms on programming and natural languages",
            "venue": "arXiv preprint arXiv:2305.02309.",
            "year": 2023
        },
        {
            "authors": [
                "Erik Nijkamp",
                "Bo Pang",
                "Hiroaki Hayashi",
                "Lifu Tu",
                "Huan Wang",
                "Yingbo Zhou",
                "Silvio Savarese",
                "Caiming Xiong."
            ],
            "title": "Codegen: An open large language model for code with multi-turn program synthesis",
            "venue": "arXiv preprint arXiv:2203.13474.",
            "year": 2022
        },
        {
            "authors": [
                "Maxwell Nye",
                "Anders Johan Andreassen",
                "Guy Gur-Ari",
                "Henryk Michalewski",
                "Jacob Austin",
                "David Bieber",
                "David Dohan",
                "Aitor Lewkowycz",
                "Maarten Bosma",
                "David Luan"
            ],
            "title": "Show your work: Scratchpads for intermediate computation with language",
            "year": 2021
        },
        {
            "authors": [
                "Long Ouyang",
                "Jeffrey Wu",
                "Xu Jiang",
                "Diogo Almeida",
                "Carroll Wainwright",
                "Pamela Mishkin",
                "Chong Zhang",
                "Sandhini Agarwal",
                "Katarina Slama",
                "Alex Ray"
            ],
            "title": "Training language models to follow instructions with human",
            "year": 2022
        },
        {
            "authors": [
                "Baptiste Rozi\u00e8re",
                "Jonas Gehring",
                "Fabian Gloeckle",
                "Sten Sootla",
                "Itai Gat",
                "Xiaoqing Ellen Tan",
                "Yossi Adi",
                "Jingyu Liu",
                "Tal Remez",
                "J\u00e9r\u00e9my Rapin"
            ],
            "title": "Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950",
            "year": 2023
        },
        {
            "authors": [
                "Baptiste Roziere",
                "Jie M Zhang",
                "Francois Charton",
                "Mark Harman",
                "Gabriel Synnaeve",
                "Guillaume Lample."
            ],
            "title": "Leveraging automated unit tests for unsupervised code translation",
            "venue": "arXiv preprint arXiv:2110.06773.",
            "year": 2021
        },
        {
            "authors": [
                "Teven Le Scao",
                "Angela Fan",
                "Christopher Akiki",
                "Ellie Pavlick",
                "Suzana Ili\u0107",
                "Daniel Hesslow",
                "Roman Castagn\u00e9",
                "Alexandra Sasha Luccioni",
                "Fran\u00e7ois Yvon",
                "Matthias Gall\u00e9"
            ],
            "title": "Bloom: A 176bparameter open-access multilingual language model",
            "year": 2022
        },
        {
            "authors": [
                "Mirac Suzgun",
                "Nathan Scales",
                "Nathanael Sch\u00e4rli",
                "Sebastian Gehrmann",
                "Yi Tay",
                "Hyung Won Chung",
                "Aakanksha Chowdhery",
                "Quoc V Le",
                "Ed H Chi",
                "Denny Zhou"
            ],
            "title": "Challenging big-bench tasks and whether chain-of-thought",
            "year": 2022
        },
        {
            "authors": [
                "Miles Turpin",
                "Julian Michael",
                "Ethan Perez",
                "Samuel R Bowman."
            ],
            "title": "Language models don\u2019t always say what they think: Unfaithful explanations in chain-of-thought prompting",
            "venue": "arXiv preprint arXiv:2305.04388.",
            "year": 2023
        },
        {
            "authors": [
                "Bailin Wang",
                "Zi Wang",
                "Xuezhi Wang",
                "Yuan Cao",
                "Rif A Saurous",
                "Yoon Kim."
            ],
            "title": "Grammar prompting for domain-specific language generation with large language models",
            "venue": "arXiv preprint arXiv:2305.19234.",
            "year": 2023
        },
        {
            "authors": [
                "Jiaan Wang",
                "Yunlong Liang",
                "Fandong Meng",
                "Haoxiang Shi",
                "Zhixu Li",
                "Jinan Xu",
                "Jianfeng Qu",
                "Jie Zhou."
            ],
            "title": "Is chatgpt a good nlg evaluator? a preliminary study",
            "venue": "arXiv preprint arXiv:2303.04048.",
            "year": 2023
        },
        {
            "authors": [
                "Ruocheng Wang",
                "Eric Zelikman",
                "Gabriel Poesia",
                "Yewen Pu",
                "Nick Haber",
                "Noah D Goodman."
            ],
            "title": "Hypothesis search: Inductive reasoning with language models",
            "venue": "arXiv preprint arXiv:2309.05660.",
            "year": 2023
        },
        {
            "authors": [
                "Xuezhi Wang",
                "Jason Wei",
                "Dale Schuurmans",
                "Quoc Le",
                "Ed Chi",
                "Denny Zhou."
            ],
            "title": "Self-consistency improves chain of thought reasoning in language models",
            "venue": "arXiv preprint arXiv:2203.11171.",
            "year": 2022
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "Ed Chi",
                "Quoc Le",
                "Denny Zhou."
            ],
            "title": "Chain of thought prompting elicits reasoning in large language models",
            "venue": "arXiv preprint arXiv:2201.11903.",
            "year": 2022
        },
        {
            "authors": [
                "Sarah Wiegreffe",
                "Ana Marasovi\u0107",
                "Noah A Smith."
            ],
            "title": "Measuring association between labels and free-text rationales",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 10266\u201310284.",
            "year": 2021
        },
        {
            "authors": [
                "Frank F Xu",
                "Uri Alon",
                "Graham Neubig",
                "Vincent Josua Hellendoorn."
            ],
            "title": "A systematic evaluation of large language models of code",
            "venue": "Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming, pages 1\u201310.",
            "year": 2022
        },
        {
            "authors": [
                "Shunyu Yao",
                "Dian Yu",
                "Jeffrey Zhao",
                "Izhak Shafran",
                "Thomas L Griffiths",
                "Yuan Cao",
                "Karthik Narasimhan."
            ],
            "title": "Tree of thoughts: Deliberate problem solving with large language models",
            "venue": "arXiv preprint arXiv:2305.10601.",
            "year": 2023
        },
        {
            "authors": [
                "Eric Zelikman",
                "Qian Huang",
                "Gabriel Poesia",
                "Noah D Goodman",
                "Nick Haber."
            ],
            "title": "Parsel: A (de-) compositional framework for algorithmic reasoning with language models",
            "venue": "arXiv preprint arXiv:2212.10561.",
            "year": 2023
        },
        {
            "authors": [
                "Eric Zelikman",
                "Yuhuai Wu",
                "Jesse Mu",
                "Noah Goodman."
            ],
            "title": "Star: Bootstrapping reasoning with reasoning",
            "venue": "Advances in Neural Information Processing Systems, volume 35, pages 15476\u201315488. Curran Associates, Inc.",
            "year": 2022
        },
        {
            "authors": [
                "Tianyi Zhang",
                "Tao Yu",
                "Tatsunori B Hashimoto",
                "Mike Lewis",
                "Wen-tau Yih",
                "Daniel Fried",
                "Sida I Wang."
            ],
            "title": "Coder reviewer reranking for code generation",
            "venue": "arXiv preprint arXiv:2211.16490.",
            "year": 2022
        },
        {
            "authors": [
                "Haiyan Zhao",
                "Hanjie Chen",
                "Fan Yang",
                "Ninghao Liu",
                "Huiqi Deng",
                "Hengyi Cai",
                "Shuaiqiang Wang",
                "Dawei Yin",
                "Mengnan Du."
            ],
            "title": "Explainability for large language models: A survey",
            "venue": "arXiv preprint arXiv:2309.01029.",
            "year": 2023
        },
        {
            "authors": [
                "Denny Zhou",
                "Nathanael Sch\u00e4rli",
                "Le Hou",
                "Jason Wei",
                "Nathan Scales",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Olivier Bousquet",
                "Quoc Le",
                "Ed Chi."
            ],
            "title": "Least-to-most prompting enables complex reasoning in large language models",
            "venue": "arXiv preprint",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Program translation (i.e., translating code from one language to another) has significant value in reallife applications, including in legacy software modernization and enabling programmers to quickly adapt to new languages. Within prompt-based approaches to code translation, Chen et al. (2023b) recently found that simply prompting an LLM to generate explanations of the source program before generating the target program can improve performance. However, this conclusion is drawn on a single translation direction from C++ to Python (Lachaux et al., 2020), and lacks evaluation on a broader set of programming languages including low-resource languages\u2014a key component of code-to-code translation tasks in a software modernization setting.\nThis paper systemtically evaluates this \u201cExplainthen-Translate\u201d approach to code translation through the MultiPL-E dataset (Cassano et al., 2022). As the original dataset was constructed for the NL-to-code setting, we repurpose this dataset into a code-to-code, \u201cMultiPL-C2C\u201d dataset. We analyze our results in 36 different translation directions over different types of explanations. We find\n1https://github.com/PootieT/ explain-then-translate\nthat Explain-then-Translate improves zero-shot performance consistently in 18 Python-to-X translation directions, but much less so in the few-shot setting. We observe detailed explanations to be more useful when translating into high-resource PLs and from low-resource into other low-resource PLs. In contrast, translating from high- to lowresource PL\u2019s benefits from more abstract explanations. To aid future research in code-to-code translation across diverse language, we release our evaluation system, as well as canonical solutions in all languages, providing a 19-way parallel program translation evaluation set."
        },
        {
            "heading": "2 Explain-then-Translate for Code Translation",
            "text": "In code translation, we are given code x in a source language and must generate a program y in a target language that is functionally equivalent to x. In this paper we are interested in whether a selfgenerated natural language explanation z can be used to improve this translation process.2"
        },
        {
            "heading": "2.1 Prompt Variations",
            "text": "Fig 1 shows an example of our prompts for program translation. In addition to the direct translation baseline (Fig 1, left), we experiment with 3 types of explanations (full prompts are given in Apx C):\n1. exp: We ask the model to explain the source program in a few sentences (Fig 1, right). 2. exp-lbl: We ask the model to explain the source program line by line. This roughly mirrors the setup in Chen et al. (2023b). 3. exp-lbl-d: We ask the model to explain the source program line by line in additional detail. In particular if an individual line is complicated, we ask it to break it down, ex-\n2While we focus on natural language explanations in our main experiments, in Apx J we show results with other types of \u201cexplanations\u201d, such as another pivot language, pseudocode, etc.\ntranslating. Blue highlights are model completions, and red highlights point out the crucial difference between the two translations. Example prompts and explanations for exp-lbl and exp-lbl-d in Apx C, E.19\nplain individual fragment of the line, and then summarize the purpose for the entire line. This prompting method allows us to decompose compositionally difficult fragments of the code down, re-use individual fragments of explanation before explaining the whole line, similar to Zhou et al. (2022).\nWhen generating explanations, we treat the token sequence \\n# as a stopping sequence in order to prevent models from generating target translations (since we condition target program with translated signatures in addition to explanations). Sometimes, a model might generate target-language-specific details (equivalent classes, attempted translation, etc.). In order to control for inconsistencies caused by the target-language-specific explanations, we re-use the same explanations (from Python-Java) for all Python-to-X translation experiments (Section 3.1). Before reusing, we also remove any target specific information with programmatic rules so it can be reused across experiments. For completeness, in Apx G.1 we show the impact of removing target-language-specific details for the exp experiments: the effects are generally insignificant, but are more pronounced in low-resource languages.\nAdditional details on language-specific stop tokens and how few-shot programs are selected are described in Apx E and Apx F, respectively."
        },
        {
            "heading": "2.2 Dataset: MultiPL-C2C",
            "text": "MultiPL-E (Cassano et al., 2022) is a benchmark that was recently introduced in an effort to evaluate NL-to-code generation capabilities of language models in 19 different programming languages.3 Cassano et al. (2022) groups these languages by resource level:\n\u2022 High-resource: JavaScript (js), Python(py), Java*4 (jv), C++* (cpp), TypeScript* (ts) \u2022 Medium-resource: PHP (php), Ruby (rb), C#* (cs), Go* (go) \u2022 Low-resource: Perl (pl), R (r), Rust* (rs), Scala* (sc), Swift* (sw) \u2022 Extremely-low-resource: Bash (sh), Lua (lua), Racket (rkt), Julia* (jl), D* (d)\n3Concretely, taking the original HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021) datasets (where models are prompted with problem description and are tasked to generate a Python program that solves the problem), MultiPL-E built transpilers for the unit tests as well as code generation prompts such that models can be evaluated from NL-to-code in 19 different languages (Python + 18 additional languages).\n4*: indicates statically typed language (vs. dynamically)\nTo repurpose MultiPL-E into a code-to-code translation dataset, we change the task formulation by including canonical Python programs in the prompt and removing the NL problem descriptions. We dub this version of the dataset as MultiPL-C2C, and release it for future work in this area.5"
        },
        {
            "heading": "2.3 Metrics",
            "text": "We evaluate our methods using unit test pass rate (Chen et al. 2021; Cassano et al. 2022) as string match-based evaluations do not capture the diverse ways in which a program can be translated and still be functionally equivalent to the source. We calculate the pass rate as:\npass@k := E Problems\n[ 1\u2212 ( n\u2212c k )( n k ) ]\nwhere n is the total number of generations, and c is the number of correct generations. The best sampling temperature t (or top-p) (Holtzman et al., 2020) is often dependent on n/k, where smaller temperatures are best for small n/k, while larger temperatures increase the generation diversity (better recall) and can improve pass rate with large n/k. We prioritize precision and calculate pass@1 with n = 20, t = 0.2, and top-p= 0.95 following Cassano et al. (2022)."
        },
        {
            "heading": "2.4 Models",
            "text": "We evaluated four models of varying sizes. We main report the results from GPT-3.5 (gpt3.5-turbo-0301) in the main paper unless otherwise specified, and defer the results from open source models (CodeGen2-1B, CodeGen216B, and Llama2CodeInstruct-34B (Nijkamp et al., 2023; Rozi\u00e8re et al., 2023)) to the appendix.\n5We also considered into CodeXGLUE (Lu et al., 2021) and TransCoder (Lachaux et al., 2020) for the unit tests evaluations, but initial studies suggested a significant number of examples (more than 25%) contain mistakes in gold programs or inadequate unit tests (see Apx A, B)."
        },
        {
            "heading": "3 Experiments and Discussion",
            "text": "In our study we focus on two main sets of translation directions: Python-to-X, where we translate from Python to 18 other target languages ranging from high to extremely-low-resource (\u00a73.1), and Xto-X, where we target a representative set of translation directions varying source and target language resource levels and typing characteristics (\u00a73.2). We analyze translation improvements across models of 4 different sizes (\u00a73.3) and discuss improving individual explanations through heuristics (\u00a73.4). Finally we show our method improves more on difficult-to-translate examples (\u00a73.5) and provide ablations to understand what NL explanations improves performance and whether alternative selfgenerated contexts could help (\u00a73.6)."
        },
        {
            "heading": "3.1 Python-to-X Translation",
            "text": "In Table 1 we present results of the Python-to-X experiments in the zero- and four-shot settings with GPT-3.5. Results with open-source models results show similar trends and are shown in Apx G.9.\nNatural language explanations improve performance in the zero-shot setting, and this effect is more pronounced in low-resource languages. Providing explanations improves relative performance by 11.5% on average across 18 target languages. Regardless of the target language resource level, the best explanation improves translation with average relative improvement of 6.2% in highresource languages and 14.5% in extremely-lowresource languages. There is no significant difference between improvements on translating into statically vs. dynamically typed languages. Selfgenerated explanations even slightly outperform human-written doc-string instructions that are part of the original HumanEval dataset (see Apx J).\nHigh-resource target languages benefit from detailed explanations while low-resource alternatives benefit from abstract explanations. We\nhypothesize that high-resource languages benefit from more detailed explanations due to higher cooccurrences of NL and PL in the pretraining corpora; whereas in low-resource languages we speculate the additional detail may introduce spurious correlations. Since we re-use explanations across translation directions, the translation performance difference can be attributed only to the code generation step.\nNatural language explanations are less helpful in the few-shot setting, but good few-shot examples are crucial. In the four-shot setting, the average improvement is much less at 1.1%, although some language pairs observe as much as a 10.1% improvement. Average improvement in high-resource languages (1.2%) is smaller than that in extremely-low-resource languages (3.4%). The most detailed explanations perform the best in 12 out of 18 language directions amongst explanation types. This is likely due to the carefully curated few-shot examples, which are semantically and syntactically complex enough to benefit from decomposition and explanations (see in Apx F for more details).\nFew-shot explanations result in worse performance than zero-shot explanations. The most abstract explanation (exp) performs the worst (best in only 3 out of 18 directions) in the few-shot setting. Since we source the few-shot explanations from minimally modified zero-shot explanations, including these self-generated explanations simply restricts the model\u2019s explanation to follow stylistic patterns and decreases the diversity of the explanations. In Apx G.2, we disentangle the effect of target specific explanation and zero/four-shot setting to provide further evidence to this point.\nImprovements in the zero-shot setting correlate with improvements in the few-shot setting. Except for a few outliers, Fig 2 shows a good correlation. This is interesting because few-shot is manually curated and written in PL, while explanation is self-generated and written in NL. In our ablation 3.6 and Apx J we further analyze to what extent the source of information provides the structure of the output, and whether the correctness of the sequence actually matters.\nAdditional analyses. In the appendix we provide the breakdown of error types (Apx G.4, G.5), source program lengths (Apx G.6), qualitative\nanalysis of explanations (Apx E.19), pass@10 (Apx G.7), correlation between NL-to-code vs. translation results (Apx G.8). and improvements vs. translation difficulty (Sec 3.5, Apx I)."
        },
        {
            "heading": "3.2 Alternative Translation Directions",
            "text": "To understand whether our findings only hold for Python (or only high-resource languages), we experiment on additional translation directions from different resource groups and typing characteristics, and present our results in Table 2. Since source languages are different, we do not re-use explanations. In the four-shot explanation, we use zero-shot generated explanations (3.1). In the following section, we have High=high-resource languages and ExtLow=extremely-low-resource languages. Results from open-source model results are in Apx H.\nHigh-to-ExtLow and High-to-High follow a similar patterns as Python-to-X. In zero-shot, Highto-High has varied performance across different explanation types, whereas High-to-ExtLow benefits mostly from simple explanations (exp). In fourshot, there is little to no improvements in High-toHigh, but some improvements in High-to-ExtLow.\nExtLow-to-High: Models are poor at explaining low-resource language programs. The improvement in ExtLow-to-High trials is limited in both zero- and four-shot directions. Across explanation methods, we can see a general decrease in performance from high-level (exp) to detailed (exp-lbl-d) explanations. We speculate that LLMs generally struggle to understand and explain lower-resource PLs; more details may introduce more errors which\nmay compound into the translation phase."
        },
        {
            "heading": "3.3 Comparisons Across Different LMs",
            "text": "Improvements are robust across models. From Fig 3 and 9, we can see that in general, the larger the model, the larger the absolute improvement with self-generated explanations. In terms of improvement over resource levels, our method improves low-resource language generations more with larger models, while improving high-resource languages more with smaller models. See detailed result tables in Apx H and G.9. CodeGen2-16B is the only model that does not improve consistently with explanations.\nBetter explanations are transferable and lead to better translations. We also experimented with CodeGen2-1B by using GPT-3.5 generated explanations (Fig 3) and found it to improve performance further, outperforming self-generated explanations in 12 out of 18 directions. Comparing absolute improvements against CodeGen2-1B with self-explanations, we find that GPT-3.5-generated explanations improve more when generating higher resource than lower resource languages, indicating that smaller models are less sensitive to improvements. More analyses are given in Apx G.9."
        },
        {
            "heading": "3.4 Explanation Selection with Heuristics",
            "text": "In the context of chain-of-thought prompting, Wang et al. (2022) demonstrate the importance of sampling diverse \u201creasoning\u201d paths. It is difficult to ensemble sampled programs from language models, but we find sampling diverse explanations, where we first sample 20 explanations and then sample one program each, to improve recall for correct programs (pass@10) than sampling 20 programs from 1 explanation, or direct translation in zero/four-shot settings. This indicates that there is significant room for improvement if we are to be able to select the best explanation that can gener-\nate a correct program with the highest probability (oracle column in Table 3).\nMotivated by the potential of diverse explanations to improve translation results, we explore five explanation re-ranking heuristics: 1) length of explanation (in characters) excluding code; 2) lines of source code explained (line-e); 3) number of lines of explanations; 4) number of code fragments enclosed in \u2018\u2018;6 5) with logprob (Zhang et al. 2022; Min et al. 2022a), ranking the explanations with a weighted combination of \u03b1\u2217p(code|explanation)+ (1\u2212\u03b1)\u2217p(explanation|code) using CodeGen2 (Nijkamp et al., 2023) (more details in Apx L).7\nFor each explanation type, we generate 20 explanations and 1 program from each explanation (train set). We estimate each heuristics\u2019 performance by averaging the pass rates of its selected (argmax) explanations for each individual problem in the train set.8 For random baseline, we select 1 explanation for each program randomly;9 and for oracle, we select the explanations with the highest pass rates in the train set. For each explanation type, we pick the heuristics with the best estimated pass@1 (n = 1), and generate 20 programs from these explanations for pass@1 (n = 20) score (right most column in Table 3). We use zero-shot explanations for exp (see Sec 3.1) and four-shot for exp-lbl and exp-lbl-d. Our main results are shown in Table 3, from which we observe the following.\nHeuristics can improve performance, and this is robust across different target languages. With exp, logprob improves upon random by absolute 2.54% (p = 0.055),10 and frag improves explain-lbl-d upon random baseline by absolute 2.2% (p = 0.033) with simulation. Both improvements can be reproduced with pass@1, so we include these heuristically selected explanations as two additional rows in Table 1. With logprob selected exp, we improve or match performance in 15/18 directions, with an average improvement of 1.7% (p < 0.001). With frag selected exp-lblsimp, we improve or match performance in 13/18 directions, averaging 0.48% (p = 0.022). See Apx G.3 for more comparisons.\n6Markdown pattern for referencing code 7We tried scoring with GPT-3.5 directly as well but found it to not outperform the random baseline (Apx K) 8We discuss the trade-offs of alternative settings (sampling 4 explanations and 5 programs from each) in Apx M. 9Repeated 100 times to obtain mean and variance.\n10The resulting p-values are from a one-tailed paired t-test.\nSome heuristics generalize across explanation types. Only frag and logprob perform robustly. Intuitively, frag makes sense because data containing a piece of code and an explanation is more likely to be correct if the author refers to the code more frequently. With logprob, since we are effectively measuring the mutual information between codes and explanations (Zhang et al., 2022).\nThere is still ample room for improvement. As we can see in the difference between oracle and pass@1, no heuristics is able to to come close to this oracle upper bound. This gap is much larger in high-to-low-resource translation direction (py-rkt). Qualitatively, we could not distinguish a good explanation from a bad one manually (Apx L.3 and L.4), suggesting that the distinction between \u201cgood\u201d and \u201cbad\u201d explanations may be hidden due to stylistic noise (wording, spacing, etc.), or potentially due to chance."
        },
        {
            "heading": "3.5 Which programs benefit from explanations?",
            "text": "To understand where natural language explanations benefit most, we investigate how exp improvement varies across problem hardness, which is approximated through direct translation pass@1. Through Fig 4, we discovered that explanation improves difficult problems (left of x-axis), but could hurt easy problems (right of x-axis). This potentially suggests we could determine which program to explain using a hardness threshold, and improve performance further. We verified the validity of such approach through our oracle metric (direct pass@1),\nand show the full results in Apx I. We found selective explanation to improve performance over direct and exp in 35/36 directions. We leave building such difficulty-based problem selector for future work."
        },
        {
            "heading": "3.6 Ablation Studies",
            "text": "We perform additional ablation studies to understand what aspects of the explanations improve translation (\u00a73.6.1), whether explanations are robust to variations in surface semantics/readability of the source code (\u00a73.7, Apx N), and if selfgenerated context in PL could help few-shot examples (\u00a73.8, Apx J). Additionally, we explore the relationship between context length and improvements in Apx G.6."
        },
        {
            "heading": "3.6.1 Explanation Ablation",
            "text": "We select 4 target languages of different resource levels where explanations provide the most improvements (zero-shot) for Python-to-X. With each explanation, we ablate in following ways:\nswap-s: We randomly reorder sentences (exp) or code-explanation segments (exp-lbl) to test if explanation provides structural supervision.\nobf-exp: We obfuscate source programs (see examples in Apx N), where function and variable names are replaced systematically with templates (FUNC_0, VAR_1, etc.). This tests whether an explanation uses specific variable references (structural supervision at token level).\nret-exp, rand-exp, no-exp: We replace the explanation with another program\u2019s explanation randomly (rand-exp), or through retrieval (ret-exp, details in Apx O), or with an empty string (no-exp) to verify if explanations need to be correct/relevant.\ndel-w: We randomly remove some of the words and see if fluency (i.e. high logprob) is necessary.\ndel-s: We randomly remove a percentage of sentences (exp) or code-explanation paragraphs (explbl) to see the dependency of the translation on the completeness of the explanation.\nExplanation needs to be coherent, relevant, and correct. From what we can observe in Table 4, explanations do not provide much structural guidance (swap-s), and models do not overly rely on their completeness (del-s). Models do not rely on the surface form to redirect attention as much (obfexp), but they do require explanations to be fluent\n(del-w). Lastly, when models receive completely irrelevant explanations (rand-exp), they are able to recover performance to some extent; but if the explanations are convincingly misleading (ret-exp) performance deterioates.\nModels rely on semantics of explanations less when generating lower-resource languages. Different types of ablations affect lower-resource languages more uniformly than higher-resource languages. Relative to exp/exp-lbl, ablations that completely alter the semantics of the explanations (delw) decreases improvements less in lower-resource languages than higher counterparts, while ablations that keep overall semantics of the explanation (swap-s) decreases improvements less in higherresource languages.\nSemantics of explanation is not the only picture. Despite explanations having completely wrong semantics (rand-exp, ret-exp), models still improve from the added context. CoT self-generated reasoning has been found to follow unmentioned/hidden biases within the context (Turpin et al., 2023). It would be interesting to investigate further what remaining biases (if any) contribute to the improvements in program translation."
        },
        {
            "heading": "3.7 Source Program Ablation",
            "text": "To test whether our explanation methods work with a different distribution of source programs, we ob-\nfuscate variables and funciton names source programs, removing all surface form semantic information (Apx N). When comparing direct translation vs. exp, in Table 18, we find explanations to be robust regardless of surface semantics of the code. In half the trials, relative improvements using explanation are even higher for obfuscated source code than original code. This is potentially due to the fact that explanations become more reliant on actual syntax of the program, rather than hallucinating on the program semantics from surface variable names. This is promising because when using models in the real world, such as for app modernization, there is no guarantee of having readable code."
        },
        {
            "heading": "3.8 Few-shot Ablations",
            "text": "Since few-shot improvements correlate with explanation improvements (\u00a73.1) we conduct ablations to check how sensitive the models are to the correctness of few-shot examples, and whether unverified self-generated few-shots can also improve as well as explanation does. Here, we replace our correct few-shot examples with naturally generated programs from GPT-3.5 (high logprob, but formally unverified (unk) or incorrect (bad)), and observe how much self-generated few-shots improve translation and models\u2019 sensitivity towards their correctness. We experiment with a fixed one-shot example as well as retrieval one-shot to observe the improvement/sensitivity when the exemple program is similar or different from the testing program.\nWhen the few-shot program is similar, verification is important. In Table 5, we observe that when the retrieved one-shot is paired with a wrong target program, the decrease in performance is much more significant than in the fixed-shot setting.11 In other words, curated few-shots are robust to label noise. This is consistent with the earlier conclusion in Table 4 that an \u201calmost-correct\u201d explanation (ret-exp) could influence generation more than when it is obviously incorrect (randexp). If verification is available, retrieve-gold shows that a formally correct (similar) program is more useful than a natural language explanation. However, on average, self-generated unverified explanations (exp) still outperform one-shot in all directions (fixed-unk by 0.7-4.5%; retrieve-unk by 0.5-2.0%), indicating that NL generations often have higher quality than programs and can serve as a better medium for intermediate reasoning step.\nTo further compare NL explanations with other formal/non-formal reasoning steps, in Apx J, we experiment with translating to a pivot programming language before translating to the target language (e.g. Python-Java-PHP). By controlling the pivot language correctness, we can more closely verify the model\u2019s translation performance sensitivity to correctness in context. The result indicates mistakes in intermediate PL steps corrupt translation performance more than imperfect NL explanations. This indicates that using self-generated NL as an intermediate step often helps more than self-generated PL, and reasoning in the NL space is advantageous for language models."
        },
        {
            "heading": "4 Related Work",
            "text": "Explanation in deep learning. Many works have explored using explanations to improve language models. Hase and Bansal (2022) investigate various ways explanations can be introduced in modeling and find it most useful for retrieving similar data. Joshi et al. (2022) find explanation regularization to improve OOD performance. Most works in LLMs generate explanation using zeroshot, few-shot, or finetuning, before generating the target response (Ling et al., 2017; Nye et al., 2021; Wei et al., 2022; Mukherjee et al., 2023; Hsieh et al., 2023). A few works have also explored posthoc explanations (Lampinen et al., 2022; Krishna\n11retrieval-bad (Table 5) should be taken lightly since the subset of problems evaluated for bad also likely contains harder problems.\net al., 2023). Wiegreffe et al. (2021) and Chan et al. (2022) propose metrics to quantify rationale-label association. We refer readers with further interest to surveys (Miller, 2019; Hartmann and Sonntag, 2022; Zhao et al., 2023).\nLanguage model for code. Much work has been dedicated to applying transformer-based language models to NL and PL generation (Brown et al., 2020; Ahmad et al., 2021; Chen et al., 2021; Li et al., 2022; Ouyang et al., 2022). TransCoder leverages unsupervised pretraining and supervised finetuning to build one of the first neural transpilers (Lachaux et al., 2020, 2021; Roziere et al., 2021). Later works obtain impressive zero and few-shot learners by simply pretraining on NL and PL data with language modeling or derivative training objective (Ahmad et al., 2021; Nijkamp et al., 2022; Chen et al., 2021; Scao et al., 2022; Xu et al., 2022; Nijkamp et al., 2023; Allal et al., 2023; Li et al., 2023; Rozi\u00e8re et al., 2023; Athiwaratkun et al., 2022).\nIntermediate state prompting. The Explainthen-Translate approach is an instance of chainof-thought prompting (Wei et al. 2022; Nye et al. 2021), where the model is prompted to generate reasoning steps before the final answer. Follow-up works have found it to be useful on numerous tasks outside of niche reasoning tasks (Wang et al., 2022; Zhou et al., 2022; Chowdhery et al., 2022; Suzgun et al., 2022; Yao et al., 2023). In our setting, we find most improvements to come from the zeroshot setting (Kojima et al., 2022). Different from previous works, our task focuses on program translation, with significant token level correspondence between the source and target. Ghazvininejad et al. (2023) and Lu et al. (2023) improve NL translation by augmenting prompts with dictionary translations, but their contexts are not self-generated. It would be interesting to explore whether other forms of \u201cexplanations\u201d (e.g., BNF grammars (Wang et al., 2023a)) could further improve performance, especially on low-resource languages which may not have been encountered frequently during pretraining.\nCode prompting and feedback. In the codegeneration space, Zelikman et al. (2022) incorporate model-generated rationales given questionanswer pairs as part of fine-tuning to improve model reasoning capabilities. Jiang et al. (2023) use few-shot examples to teach models to create\nNL steps from NL instructions before generating the code. Zelikman et al. (2023) and decomposes complex problems in NL and generated/verified subproblems to achieve high performance in NLto-code. Chen et al. (2023a) finetune policy models to correct code given human critique. Wang et al. (2023c) searches multiple hypothesis in NL before generating PL targets. Our method uses selfgenerated context without overly relying on feedback, few-shot examples, or complicated frameworks, and is targeting code-translation specifically instead of NL-to-code generation. Chen et al. (2023b) briefly mentions in their ablation studies that explanation improves translation in Python\u2192C++, but our analysis reveals a more nuanced settings in which explanations improve code translation."
        },
        {
            "heading": "5 Conclusion",
            "text": "This work conducts a thorough analysis of the performance of large language models in program translation by using different types of selfgenerated explanations as an intermediate step. Models generate higher quality detailed explanations for high-resource languages, while still generating good enough abstract explanations for lowresource languages. With simple heuristics, we have also demonstrated the potential to improve explanation quality and consequently translation quality. We identify key requirements for explanation and find that on average, mistakes in NL are less detrimental to performance, and do not require verification to perform well compared to using PL as self-generated contexts."
        },
        {
            "heading": "Limitations",
            "text": "There are several limitations to our work. First, while we focus on the (adapted) MultiPL-E benchmark due to its widespread use, it is unclear whether programs in this benchmark are representative of programs that are targets for code-tocode translation. Second, while we saw our conclusions to largely hold across GPT-3.5 and other open-source models, it is unclear whether they will hold for more powerful LLMs such as GPT-4. Finally, somewhat disappointingly we found natural language explanations to be not as helpful in the few-shot setting, and insofar as obtaining several demonstration examples for each language pair is quite practical, natural language explanations for code-to-code translation may not actually be useful for many applications of interest."
        },
        {
            "heading": "Acknowledgements",
            "text": "We thank all anonymous reviewers. We also thank Justin Weisz, Afra Feyza Aky\u00fcrek, Najoung Kim, Aditya Yetetore, Garry Kuwanto, Reuben Tan, and Yuwen Pu for their helpful discussions and suggestions. This project is supported by the MIT-IBM Watson AI lab and DARPA HR001118S0044 (the LwLL program). ZT is supported by Lu Lingzi Fellowship from Boston University. Any opinions, findings, conclusions, or recommendations expressed here are those of the authors and do not necessarily reflect the view of the sponsor."
        },
        {
            "heading": "A Transcoder Evaluation Clean-up",
            "text": "TransCoder (Lachaux et al., 2020) and other followups (Chowdhery et al., 2022; Chen et al., 2023b) evaluated their translation model using mined programs from \"GeeksforGeek\", an online programming practice website. Within the evaluation (valid and test) set, we find more than 25% of the data points were erroneous due to mistakes in gold programs or inadequate unit tests. After\ncleaning, we find the dataset to be too easy for our baseline model GPT-3.5 (Apx B) and seek more challenging benchmarks (MultiPL-E).\nWithin Transcoder validation and test set, in the direction of C++ to Python, and we find several type of errors that are hindering existing evaluations:\nInadequate test: exact comparison Python tests on integers and float types are often inadequate in establishing equality. Depending on the library and specific function called, integers or floats can be rounded with different precision. Because tests in TransCoder evaluation dataset work by comparing a gold Python program\u2019s output with that of the generated Python program, if the generated program does not use the exact function call, exact equality (==) is not sufficient. Thus, we changed all cases of such violation from checking exact equality to approximate equality like below: # Old way of evaluating exact equality if f_gold(*parameters_set) == f_filled(* parameters_set):\nn_success+=1\n# New way of evaluating approximate equality, borrowed from other instances of float comparison tests in TransCoder. if abs(1 - (0.0000001 + abs(f_gold(* parameters_set))) / (abs(f_filled(* parameters_set)) + 0.0000001)) < 0.001:\nn_success+=1\nThere are 40+ examples of such case.\nInadequate test: default large number In C++ programs, integer variables are often initialized with INT_MAX and INT_MIN as default values when comparing with other values in a loop. When such values are not replaced and returned, comparing the exact values of such placeholders does not make sense, especially when there is not a single correct way of translating it into Python. Hence, to accommodate such programs, we check for approximate equality between input-output or check that the output of gold and generated program both return an extremely large/small placeholder number. # Old way if abs(1 - (0.0000001 + abs(f_gold(* parameters_set))) / (abs(f_filled(* parameters_set)) + 0.0000001)) < 0.001:\nn_success+=1\n# New way of evaluating approximate equality, as well as including cases where outputs are extremely large/small value placeholders. if abs(1 - (0.0000001 + abs(f_gold(* parameters_set))) / (abs(f_filled(* parameters_set)) + 0.0000001)) < 0.001 or ((\nf_gold(*parameters_set) > 10e7) and (f_filled (*parameters_set) > 10e7)):\nn_success+=1\nThere are about 12 programs for such cases.\nInadequate test: non-sensible test inputs Original unit tests sometimes contain non-sensible test inputs. For instance, some programs are intended to process string representation of numbers, but the unit test input would contain non-digit strings. This is especially harmful for translation based on semantics because extreme edge-cases like this should not be used as regular tests. Therefore, we replace these tests inputs with values from the appropriate space. There are 7 instances of such programs.\nWrong source program Sometimes, C++ programs are incorrect or non-standardized (such as returning 0/1 vs returning true/false, or missing return statements). We standardized all C++ programs. There are around 5 instances of such program we fixed.\nWrong gold program Of all errors, this is the most devastating type of error. Without a correct gold program to compare output with, no matter what the generation is there will be no chance of success. Errors range vastly from in-correct range value, wrong indentation, missing return statements, to using wrong/undefined variables. One of the most frequent errors that spans more than 60 programs is the in-correct translation of integer division from C++ to Python. Often it is translated to / when it should be //. We found 100+ examples of this type of error.\nWe stress that we have only fixed existing issues we have noticed in C++ to Python direction. With sampled programs from Java, we have also observed errors in gold programs and tests frequently.\nLeakage to training data One of the other reasons that we do not formally evaluate our methods on TransCoder dataset is because that the TransCoder dataset is very likely to have been included in GPTs\u2019 training corpora. We are often able to generate the entire program in test/valid split by only providing the signature. The unique detokenized program pattern makes it easy for the model to detect and regurgitate from training sequences."
        },
        {
            "heading": "B TransCoder Evaluation",
            "text": "We report the full evaluations on TransCoder eval+test set with GPT-3.5 in Table 6. The model\nimproves around (absolute) 10% on the fixed dataset, indicating the lack of difficulty in the program. Through manual inspection we find the syntax of the programs to be rudimentary. We conducted three type of evaluations regarding the amount of target program signature specification we provided:\n\u2022 no-sig: we only prompt the beginning of Python program generation with def. This signals the beginning of python program without specifying any part of the signature. This is what we use to compare with Self-debug (Chen et al., 2023b) baselines.\n\u2022 gold-sig: we use the gold programs signature to prompt the rest of Python generation. Some of the programs are translated incorrectly after being prepended with signature because a few Python gold programs contain non-equivalent program name and input variable name. This is the same evaluation setting in which we conducted other experiments in HumanEval, and is our main source of comparison.\n\u2022 typed-sig: In addition to prompting target program with gold programs, which do not contain Python type hints, we built rule-based transpiler to translate the C++ program signatures to Python with type hints, and prompt the rest of the program.\nWe do not have direct measurement of the selfdebug method on the fixed dataset. We can, however, infer that self-debug method\u2019s improvements over baseline is from the additional access to unit\ntest and compiler feedback. When unit tests or gold programs are wrong, relying on these feedback information is the only way to improve translation performance. In another word, translation models start to deviate from a faithful translation and \"over-correct\" itself to pass unit tests. Our methods, however, assumes no such access to such feedback signals, and slightly under-perform.\nWhen looking at gold-sig trials, we see that by asking model to explain the program and then translate consistently improve over baseline. In no-sig trials, we see a consistent under-performance of our methods. Most of the errors are result of generated program having incorrect signature (incorrect number of input variables). Often, these source C++ programs contain non-intuitive or redundant input variables. For example, in the following example FREQUENT_ELEMENT_ARRAY_1, the input variable n is non-intuitive. If a program were to find the most frequent element in a array, it should normally process the whole input array arr.\n# program generated through direct translation def f_gold ( arr , n ) :\nHash = dict ( ) for i in range ( n ) :\nif arr [ i ] in Hash.keys ( ) : Hash [ arr [ i ] ] += 1 else : Hash [ arr [ i ] ] = 1\nmax_count = 0 res = - 1 for i in Hash :\nif ( max_count < Hash [ i ] ) : res = i max_count = Hash [ i ]\nreturn res\n# program generated with explanation def f_filled ( arr ) :\nhash = { } for i in arr :\nif i in hash : hash [ i ] += 1 else : hash [ i ] = 1\nmax_count = 0 res = - 1 for key , value in hash.items ( ) :\nif max_count < value : res = key max_count = value\nreturn res\nBy explaining before translating, the model aligns the generation to the explanation, which often follows the distribution of \"typical programs\" in the wild.\nIn typed-sig, we see general improvements over other trials. This is expected because we can\nprovided more accurate information regarding the input-output types. The slight under-performance of exp compared to direct could be due to small experiment trial size."
        },
        {
            "heading": "C Full Prompts",
            "text": ""
        },
        {
            "heading": "C.1 Python-Java direct (four-shot)",
            "text": "You are a helpful and faithful compiler that transpiles Python code to Java code. Please translate the following Python code to Java?\n### Python version\nfrom typing import Tuple\ndef even_odd_palindrome(n: int) -> Tuple[int, int]:\ndef is_palindrome(n): return str(n) == str(n)[::-1]\neven_palindrome_count = 0 odd_palindrome_count = 0\nfor i in range(1, n+1): if i%2 == 1 and is_palindrome(i\n): odd_palindrome_count += 1 elif i%2 == 0 and is_palindrome (i):\neven_palindrome_count += 1 return (even_palindrome_count,\nodd_palindrome_count)\n### Java version\nimport java.util.*; import java.lang.reflect.*; import org.javatuples.*; import java.security.*; import java.math.*; import java.io.*; import java.util.stream.*; class Problem {\npublic static Pair<Long, Long> evenOddPalindrome(long n) {\nlong evenPalindromeCount = 0; long oddPalindromeCount = 0;\nfor (long i = 1; i <= n; i++) { if (i % 2 == 1 & & isPalindrome(i)) {\noddPalindromeCount++; } else if (i % 2 == 0 & & isPalindrome(i)) {\nevenPalindromeCount++; }\n} return Pair.with( evenPalindromeCount, oddPalindromeCount);\n}\nprivate static boolean isPalindrome (long n) {\nreturn Long.toString(n).equals( new StringBuilder(Long.toString (n)).reverse().toString());\n} }\n### Python version\nfrom typing import List\ndef is_sorted(lst: List[int]) -> bool: count_digit = dict([(i, 0) for i in\nlst]) for i in lst:\ncount_digit[i]+=1 if any(count_digit[i] > 2 for i in\nlst): return False\nif all(lst[i-1] <= lst[i] for i in range(1, len(lst))):\nreturn True else:\nreturn False\n### Java version\nimport java.util.*; import java.lang.reflect.*; import org.javatuples.*; import java.security.*; import java.math.*; import java.io.*; import java.util.stream.*; class Problem {\npublic static boolean isSorted( ArrayList<Long> lst) {\nHashMap<Long, Long> count_digit = new HashMap<Long, Long>(); for (long i : lst) {\nif (count_digit.containsKey (i)) {\ncount_digit.put(i, count_digit.get(i) + 1) ; } else { count_digit.put(i, 1L);\n} } for (long i : lst) {\nif (count_digit.get(i) > 2) {\nreturn false; }\n} for (int i = 1; i < lst.size(); i++) {\nif (lst.get(i - 1) > lst. get(i)) {\nreturn false; }\n} return true;\n} }\n### Python version\nfrom typing import List\ndef separate_paren_groups(paren_string: str) -> List[str]:\nresult = [] current_string = [] current_depth = 0\nfor c in paren_string: if c == \u2019(\u2019:\ncurrent_depth += 1 current_string.append(c) elif c == \u2019)\u2019: current_depth -= 1 current_string.append(c)\nif current_depth == 0: result.append(\u2019\u2019.join(\ncurrent_string)) current_string.clear()\nreturn result\n### Java version\nimport java.util.*; import java.lang.reflect.*; import org.javatuples.*; import java.security.*; import java.math.*; import java.io.*; import java.util.stream.*; class Problem {\npublic static ArrayList<String> separateParenGroups(String paren_string) {\nArrayList<String> result = new ArrayList<String>(); ArrayList<Character> current_string = new ArrayList< Character>(); int current_depth = 0;\nfor (int i = 0; i < paren_string.length(); i++) {\nchar c = paren_string. charAt(i); if (c == \u2019(\u2019) {\ncurrent_depth += 1; current_string.add(c); } else if (c == \u2019)\u2019) { current_depth -= 1; current_string.add(c);\nif (current_depth == 0) {\nStringBuilder sb = new StringBuilder( current_string.size ()); for (Character ch : current_string) {\nsb.append(ch); } result.add(sb. toString()); current_string. clear();\n} }\n}\nreturn result; }\n}\n### Python version\nfrom typing import List\ndef sort_array(array: List[int]) -> List[int]:\nreturn [] if len(array) == 0 else sorted(array, reverse= (array[0]+array [-1]) % 2 == 0)\n### Java version\nimport java.util.*; import java.lang.reflect.*; import org.javatuples.*; import java.security.*; import java.math.*; import java.io.*; import java.util.stream.*; class Problem {\npublic static ArrayList<Long> sortArray(ArrayList<Long> array) {\nif (array.size() == 0) { return new ArrayList<Long >(); } else { boolean reverse = (array. get(0) + array.get(array. size() - 1)) % 2 == 0; Collections.sort(array); if (reverse) {\nCollections.reverse( array);\n} return array;\n} }\n}\n### Python version\n<insert Python program here>\n### Java version\n<insert Java completion here>"
        },
        {
            "heading": "C.2 Python-Java exp (four-shot)",
            "text": "You are an helpful AI assistant who understands all programming languages and can translate between them at ease. Can you explain what this Python program does with a couple of sentences ? The goal with the explanation, is so that a reader can easily rewrite the program in Java?\n### Python version\nfrom typing import Tuple\ndef even_odd_palindrome(n: int) -> Tuple[int, int]:\ndef is_palindrome(n): return str(n) == str(n)[::-1]\neven_palindrome_count = 0 odd_palindrome_count = 0\nfor i in range(1, n+1): if i%2 == 1 and is_palindrome(i\n): odd_palindrome_count += 1 elif i%2 == 0 and is_palindrome (i):\neven_palindrome_count += 1 return (even_palindrome_count,\nodd_palindrome_count)\n### Explanation\nThis function takes an integer \u2018n\u2018 as input and returns a tuple of two integers. The first integer is the number of even palindromes between 1 and \u2018n\u2018, and the second integer is the number of odd palindromes between 1 and \u2018n\u2018. A palindrome is a number that reads the same forwards and backwards.\nThe function first defines a helper function \u2018is_palindrome\u2018 that takes a number and returns \u2018True\u2018 if it is a palindrome and \u2018False\u2018 otherwise. This is done by converting the number to a string and checking if the string is equal to its reverse.\nThe function then initializes two counters, \u2018even_palindrome_count\u2018 and \u2018 odd_palindrome_count\u2018, to 0. It then loops through all numbers between 1 and \u2018n\u2018 (inclusive) and checks if each number is a palindrome and even or odd. If the number is a palindrome and odd, it increments \u2018odd_palindrome_count\u2018. If the number is a palindrome and even, it increments \u2018even_palindrome_count\u2018.\nFinally, the function returns a tuple of the two counters.\n### Java version\nimport java.util.*; import java.lang.reflect.*; import org.javatuples.*; import java.security.*; import java.math.*; import java.io.*; import java.util.stream.*; class Problem {\npublic static Pair<Long, Long> evenOddPalindrome(long n) {\nlong evenPalindromeCount = 0;\nlong oddPalindromeCount = 0;\nfor (long i = 1; i <= n; i++) { if (i % 2 == 1 & & isPalindrome(i)) {\noddPalindromeCount++; } else if (i % 2 == 0 & & isPalindrome(i)) {\nevenPalindromeCount++; }\n} return Pair.with( evenPalindromeCount, oddPalindromeCount);\n}\nprivate static boolean isPalindrome (long n) {\nreturn Long.toString(n).equals( new StringBuilder(Long.toString (n)).reverse().toString());\n} }\n### Python version\nfrom typing import List\ndef is_sorted(lst: List[int]) -> bool: count_digit = dict([(i, 0) for i in\nlst]) for i in lst:\ncount_digit[i]+=1 if any(count_digit[i] > 2 for i in\nlst): return False\nif all(lst[i-1] <= lst[i] for i in range(1, len(lst))):\nreturn True else:\nreturn False\n### Explanation\nThis function takes in a list of integers and returns a boolean indicating whether the list is sorted in non-descending order and has no more than two occurrences of any integer. The function first creates a dictionary where the keys are the integers in the list and the values are the number of times that integer appears in the list. It then checks if any integer appears more than twice in the list, and returns False if so. Finally, it checks if the list is sorted in nondescending order, and returns True if so, and False otherwise.\n### Java version\nimport java.util.*; import java.lang.reflect.*; import org.javatuples.*; import java.security.*; import java.math.*; import java.io.*;\nimport java.util.stream.*; class Problem {\npublic static boolean isSorted( ArrayList<Long> lst) {\nHashMap<Long, Long> count_digit = new HashMap<Long, Long>(); for (long i : lst) {\nif (count_digit.containsKey (i)) {\ncount_digit.put(i, count_digit.get(i) + 1) ; } else { count_digit.put(i, 1L);\n} } for (long i : lst) {\nif (count_digit.get(i) > 2) {\nreturn false; }\n} for (int i = 1; i < lst.size(); i++) {\nif (lst.get(i - 1) > lst. get(i)) {\nreturn false; }\n} return true;\n} }\n### Python version\nfrom typing import List\ndef separate_paren_groups(paren_string: str) -> List[str]:\nresult = [] current_string = [] current_depth = 0\nfor c in paren_string: if c == \u2019(\u2019:\ncurrent_depth += 1 current_string.append(c) elif c == \u2019)\u2019: current_depth -= 1 current_string.append(c)\nif current_depth == 0: result.append(\u2019\u2019.join(\ncurrent_string)) current_string.clear()\nreturn result\n### Explanation\nThis function takes a string of parentheses and returns a list of strings, where each string is a group of parentheses. A group is defined as a set of parentheses that are balanced, meaning that each opening parenthesis has a corresponding closing parenthesis\n. The function works by iterating over each character in the input string, and keeping track of the current depth of nested parentheses. When an opening parenthesis is encountered, the depth is incremented and the character is added to the current string. When a closing parenthesis is encountered, the depth is decremented and the character is added to the current string. If the depth reaches 0, it means that a complete group has been found, so the current string is added to the result list and cleared for the next group. The function returns the list of groups .\n### Java version\nimport java.util.*; import java.lang.reflect.*; import org.javatuples.*; import java.security.*; import java.math.*; import java.io.*; import java.util.stream.*; class Problem {\npublic static ArrayList<String> separateParenGroups(String paren_string) {\nArrayList<String> result = new ArrayList<String>(); ArrayList<Character> current_string = new ArrayList< Character>(); int current_depth = 0;\nfor (int i = 0; i < paren_string.length(); i++) {\nchar c = paren_string. charAt(i); if (c == \u2019(\u2019) {\ncurrent_depth += 1; current_string.add(c); } else if (c == \u2019)\u2019) { current_depth -= 1; current_string.add(c);\nif (current_depth == 0) {\nStringBuilder sb = new StringBuilder( current_string.size ()); for (Character ch : current_string) {\nsb.append(ch); } result.add(sb. toString()); current_string. clear();\n} }\n}\nreturn result; }\n} ### Python version\nfrom typing import List\ndef sort_array(array: List[int]) -> List[int]:\nreturn [] if len(array) == 0 else sorted(array, reverse= (array[0]+array [-1]) % 2 == 0)\n### Explanation\nThis function takes a list of integers \u2018array\u2018 as input and returns a sorted list of integers. The function first checks if the input list is empty. If it is, it returns an empty list. Otherwise, it sorts the list using the \u2018sorted\u2018 function. The \u2018reverse\u2018 parameter is set to \u2018True\u2018 if the sum of the first and last elements of the array is even (and the array will be sorted in descending order), otherwise, it is set to \u2018False\u2018 (and the array will be sorted in ascending order).\n### Java version\nimport java.util.*; import java.lang.reflect.*; import org.javatuples.*; import java.security.*; import java.math.*; import java.io.*; import java.util.stream.*; class Problem {\npublic static ArrayList<Long> sortArray(ArrayList<Long> array) {\nif (array.size() == 0) { return new ArrayList<Long >(); } else { boolean reverse = (array. get(0) + array.get(array. size() - 1)) % 2 == 0; Collections.sort(array); if (reverse) {\nCollections.reverse( array);\n} return array;\n} }\n}\n### Python version\n<insert Python program here>\n### Explanation\nThis<insert Python program explanation here>\n### Java version\n<insert Java completion here>"
        },
        {
            "heading": "C.3 Python-Java exp-lbl (four-shot)",
            "text": "You are an helpful AI assistant who understands all programming languages and can translate between them at ease. Can you explain what this Python program does line by line? The goal with the explanation, is so that a reader can easily rewrite the program in Java?\n### Python version\nfrom typing import Tuple\ndef even_odd_palindrome(n: int) -> Tuple[int, int]:\ndef is_palindrome(n): return str(n) == str(n)[::-1]\neven_palindrome_count = 0 odd_palindrome_count = 0\nfor i in range(1, n+1): if i%2 == 1 and is_palindrome(i\n): odd_palindrome_count += 1 elif i%2 == 0 and is_palindrome (i):\neven_palindrome_count += 1 return (even_palindrome_count,\nodd_palindrome_count)\n### Explanation\nThe code is an implementation of finding the number of even and odd palindrome numbers between 1 and \u2018n\u2018 ( inclusive). It takes an integer \u2018n\u2018 as input and returns a tuple of two integers: even and odd palindrome number counts. A palindrome is a number that reads the same forwards and backwards.\ndef is_palindrome(n): return str(n) == str(n)[::-1] The function first defines a helper function \u2018is_palindrome\u2018 that takes a number and returns \u2018True\u2018 if it is a palindrome and \u2018False\u2018 otherwise. This is done by converting the number to a string and checking if the string is equal to its reverse.\neven_palindrome_count = 0 odd_palindrome_count = 0 The counter variable \u2018 even_palindrome_count\u2018 and \u2018 odd_palindrome_count\u2018 are used to record the result.\nfor i in range(1, n+1): The loops goes through all numbers between 1 and \u2018n\u2018 (inclusive)\nif i%2 == 1 and is_palindrome(i): odd_palindrome_count += 1\nelif i%2 == 0 and is_palindrome(i): even_palindrome_count += 1 Within the for-loop, the program checks if each number is a palindrome and even or odd. If the number is a palindrome and odd, it increments \u2018 odd_palindrome_count\u2018. If the number is a palindrome and even, it increments \u2018 even_palindrome_count\u2018.\nreturn (even_palindrome_count, odd_palindrome_count) Finally, the function returns a tuple of the two counters.\n### Java version\nimport java.util.*; import java.lang.reflect.*; import org.javatuples.*; import java.security.*; import java.math.*; import java.io.*; import java.util.stream.*; class Problem {\npublic static Pair<Long, Long> evenOddPalindrome(long n) {\nlong evenPalindromeCount = 0; long oddPalindromeCount = 0;\nfor (long i = 1; i <= n; i++) { if (i % 2 == 1 & & isPalindrome(i)) {\noddPalindromeCount++; } else if (i % 2 == 0 & & isPalindrome(i)) {\nevenPalindromeCount++; }\n} return Pair.with( evenPalindromeCount, oddPalindromeCount);\n}\nprivate static boolean isPalindrome (long n) {\nreturn Long.toString(n).equals( new StringBuilder(Long.toString (n)).reverse().toString());\n} }\n### Python version\nfrom typing import List\ndef is_sorted(lst: List[int]) -> bool: count_digit = dict([(i, 0) for i in\nlst]) for i in lst:\ncount_digit[i]+=1 if any(count_digit[i] > 2 for i in\nlst): return False\nif all(lst[i-1] <= lst[i] for i in range(1, len(lst))):\nreturn True else:\nreturn False\n### Explanation\nThe code is an implementation of checking whether a list is sorted in non-descending order and has no more than two occurrences of any integer. takes in a list of integers and returns a boolean.\ncount_digit = dict([(i, 0) for i in lst ]) for i in lst:\ncount_digit[i]+=1 The function first creates a dictionary where the keys are the integers in the list and the values are the number of times that integer appears in the list.\nif any(count_digit[i] > 2 for i in lst) :\nreturn False It then checks if any integer appears more than twice in the list, and returns False if so.\nif all(lst[i-1] <= lst[i] for i in range(1, len(lst))):\nreturn True else:\nreturn False Finally, it checks if the list is sorted in non-descending order, and returns True if so, and False otherwise .\n### Java version\nimport java.util.*; import java.lang.reflect.*; import org.javatuples.*; import java.security.*; import java.math.*; import java.io.*; import java.util.stream.*; class Problem {\npublic static boolean isSorted( ArrayList<Long> lst) {\nHashMap<Long, Long> count_digit = new HashMap<Long, Long>(); for (long i : lst) {\nif (count_digit.containsKey (i)) {\ncount_digit.put(i, count_digit.get(i) + 1) ; } else { count_digit.put(i, 1L);\n} } for (long i : lst) {\nif (count_digit.get(i) > 2) {\nreturn false; }\n}\nfor (int i = 1; i < lst.size(); i++) {\nif (lst.get(i - 1) > lst. get(i)) {\nreturn false; }\n} return true;\n} }\n### Python version\nfrom typing import List\ndef separate_paren_groups(paren_string: str) -> List[str]:\nresult = [] current_string = [] current_depth = 0\nfor c in paren_string: if c == \u2019(\u2019:\ncurrent_depth += 1 current_string.append(c) elif c == \u2019)\u2019: current_depth -= 1 current_string.append(c)\nif current_depth == 0: result.append(\u2019\u2019.join(\ncurrent_string)) current_string.clear()\nreturn result\n### Explanation\nThe code is an implementation of a function \u2018separate_paren_groups\u2018 which takes a string of parentheses and returns a list of strings containing the groups of parentheses.\nresult = [] current_string = [] current_depth = 0 The function initializes an empty list \u2018result\u2018 to store the groups of parentheses. It also initializes an empty list \u2018current_string\u2018 to store the current group of parentheses being processed. The variable \u2018current_depth\u2018 is initialized to 0 to keep track of the depth of the current group of parentheses.\nfor c in paren_string: The function iterates through each character \u2018c\u2018 in the input string \u2018 paren_string\u2018.\nif c == \u2019(\u2019: current_depth += 1 current_string.append(c) If the character is an opening parenthesis \u2018\u2019(\u2019\u2018, the function\nincrements the \u2018current_depth\u2018 variable and appends the character to the \u2018 current_string\u2018 list.\nelif c == \u2019)\u2019: current_depth -= 1 current_string.append(c) If the character is a closing parenthesis \u2018\u2019)\u2019\u2018, the function decrements the \u2018current_depth\u2018 variable and appends the character to the \u2018 current_string\u2018 list.\nif current_depth == 0: result.append(\u2019\u2019.join( current_string)) current_string.clear() If the \u2018current_depth\u2018 variable reaches 0, it means that the current group of parentheses has been fully processed, so the function joins the \u2018 current_string\u2018 list into a string and appends it to the \u2018result\u2018 list. The \u2018 current_string\u2018 list is then cleared to start processing the next group of parentheses.\nreturn result The function returns the \u2018result\u2018 list containing the groups of parentheses.\n### Java version\nimport java.util.*; import java.lang.reflect.*; import org.javatuples.*; import java.security.*; import java.math.*; import java.io.*; import java.util.stream.*; class Problem {\npublic static ArrayList<String> separateParenGroups(String paren_string) {\nArrayList<String> result = new ArrayList<String>(); ArrayList<Character> current_string = new ArrayList< Character>(); int current_depth = 0;\nfor (int i = 0; i < paren_string.length(); i++) {\nchar c = paren_string. charAt(i); if (c == \u2019(\u2019) {\ncurrent_depth += 1; current_string.add(c); } else if (c == \u2019)\u2019) { current_depth -= 1; current_string.add(c);\nif (current_depth == 0) {\nStringBuilder sb = new StringBuilder( current_string.size ());\nfor (Character ch : current_string) {\nsb.append(ch); } result.add(sb. toString()); current_string. clear();\n} }\n}\nreturn result; }\n}\n### Python version\nfrom typing import List\ndef sort_array(array: List[int]) -> List[int]:\nreturn [] if len(array) == 0 else sorted(array, reverse= (array[0]+array [-1]) % 2 == 0)\n### Explanation\nThe code is an implementation of sorting an array of integers. It takes a list of integers \u2018array\u2018 as input and returns a sorted list of integers.\nreturn [] if len(array) == 0 else sorted(array, reverse= (array[0]+array [-1]) % 2 == 0) The function first checks if the input list is empty. If it is, it returns an empty list. Otherwise, it sorts the list using the \u2018sorted\u2018 function. The \u2018 reverse\u2018 parameter is set to \u2018True\u2018 if the sum of the first and last elements of the array is even (and the array will be sorted in descending order), otherwise, it is set to \u2018False\u2018 (and the array will be sorted in ascending order).\n### Java version\nimport java.util.*; import java.lang.reflect.*; import org.javatuples.*; import java.security.*; import java.math.*; import java.io.*; import java.util.stream.*; class Problem {\npublic static ArrayList<Long> sortArray(ArrayList<Long> array) {\nif (array.size() == 0) { return new ArrayList<Long >(); } else { boolean reverse = (array. get(0) + array.get(array. size() - 1)) % 2 == 0; Collections.sort(array);\nif (reverse) { Collections.reverse( array); } return array;\n} }\n}\n### Python version\n<insert Python program here>\n### Explanation\nThe code is an implementation of<insert Python program explanation here>\n### Java version\n<insert Java completion here>"
        },
        {
            "heading": "C.4 Python-Java exp-lbl-d (four-shot)",
            "text": "You are an helpful AI assistant who understands all programming languages and can translate between them at ease. Can you explain what this Python program does line by line? If a line is too long or too complicated, simplify it and explain what individual parts of the line mean first before explaining the whole line. The goal with the explanation, is so that a reader can easily rewrite the program in Java?\n### Python version\nfrom typing import Tuple\ndef even_odd_palindrome(n: int) -> Tuple[int, int]:\ndef is_palindrome(n): return str(n) == str(n)[::-1]\neven_palindrome_count = 0 odd_palindrome_count = 0\nfor i in range(1, n+1): if i%2 == 1 and is_palindrome(i\n): odd_palindrome_count += 1 elif i%2 == 0 and is_palindrome (i):\neven_palindrome_count += 1 return (even_palindrome_count,\nodd_palindrome_count)\n### Explanation\nThe code is an implementation of finding the number of even and odd palindrome numbers between 1 and \u2018n\u2018 ( inclusive). It takes an integer \u2018n\u2018 as input and returns a tuple of two\nintegers: even and odd palindrome number counts. A palindrome is a number that reads the same forwards and backwards.\ndef is_palindrome(n): return str(n) == str(n)[::-1] The function defines a helper function. Let\u2019s break it down step by step: str(n) The function first converts the input from integers to strings, because Python does not allow one to easily reverse an integer by its digits. str(n)[::-1] It then uses string manipulation \u2018[::-1]\u2018 to reverse the string. return str(n) == str(n)[::-1] It then checks if the string version of the integer is equal to its reversed string version, thereby determining whether the input integer is a palindrome or not. Hence, the helper function\u2018 is_palindrome\u2018 takes a number and returns \u2018True\u2018 if it is a palindrome and \u2018False\u2018 otherwise.\neven_palindrome_count = 0 odd_palindrome_count = 0 The counter variable \u2018 even_palindrome_count\u2018 and \u2018 odd_palindrome_count\u2018 are used to record the result.\nfor i in range(1, n+1): The loops goes through all numbers between 1 and \u2018n\u2018 (inclusive)\nif i%2 == 1 and is_palindrome(i): odd_palindrome_count += 1 elif i%2 == 0 and is_palindrome(i): even_palindrome_count += 1 Within the for-loop, the program defines a set of if-else statements. Let\u2019s break it down step by step: i%2 == 1 This checks if the number is odd by checking if the mod 2 of the number has remainder of 1 is_palindrome(i) Here the program uses previously defined helper function to determine if the number is a palindrome or not. if i%2 == 1 and is_palindrome(i):\nodd_palindrome_count += 1 Putting it together, both of above components check if the number is odd and a palindrome, and if so, increment the counter \u2018odd_palindrome_count\u2018. The same logic is applied to even palindrome numbers. Hence, the if-else conditions checks if each number is a palindrome and even/ odd and increment respective counters.\nreturn (even_palindrome_count, odd_palindrome_count)\nFinally, the function returns a tuple of the two counters. Even palindrome count first, odd palindrome count next.\n### Java version\nimport java.util.*; import java.lang.reflect.*; import org.javatuples.*; import java.security.*; import java.math.*; import java.io.*; import java.util.stream.*; class Problem {\npublic static Pair<Long, Long> evenOddPalindrome(long n) {\nlong evenPalindromeCount = 0; long oddPalindromeCount = 0;\nfor (long i = 1; i <= n; i++) { if (i % 2 == 1 & & isPalindrome(i)) {\noddPalindromeCount++; } else if (i % 2 == 0 & & isPalindrome(i)) {\nevenPalindromeCount++; }\n} return Pair.with( evenPalindromeCount, oddPalindromeCount);\n}\nprivate static boolean isPalindrome (long n) {\nreturn Long.toString(n).equals( new StringBuilder(Long.toString (n)).reverse().toString());\n} }\n### Python version\nfrom typing import List\ndef is_sorted(lst: List[int]) -> bool: count_digit = dict([(i, 0) for i in\nlst]) for i in lst:\ncount_digit[i]+=1 if any(count_digit[i] > 2 for i in\nlst): return False\nif all(lst[i-1] <= lst[i] for i in range(1, len(lst))):\nreturn True else:\nreturn False\n### Explanation\nThe code is an implementation of checking whether a list is sorted in non-descending order and has no more than two occurrences of any integer. It takes in a list of integers and returns a boolean.\ncount_digit = dict([(i, 0) for i in lst ]) for i in lst:\ncount_digit[i]+=1 The function first creates a dictionary where the keys are the integers in the list and the values are the number of times that integer appears in the list.\nif any(count_digit[i] > 2 for i in lst) :\nreturn False It then checks if any integer appears more than twice in the list, and returns False if so.\nif all(lst[i-1] <= lst[i] for i in range(1, len(lst))):\nreturn True else:\nreturn False Let\u2019s break it down step by step: lst[i-1] <= lst[i] Within the list comprehension, this checks whether the current element at index \u2018i\u2018 is greater or equal to the previous element at index \u2018i-1\u2018. if all(lst[i-1] <= lst[i] for i in range(1, len(lst))): The whole expression then checks if every element is greater or equal to the element before in the list. In another word, this checks if the list is sorted in non-descending order.\nreturn True else:\nreturn False If the list is sorted in non-descending order the function returns True, otherwise it returns False. Hence, the if-else statements returns True if the list is sorted in nondescending order, and False otherwise.\n### Java version\nimport java.util.*; import java.lang.reflect.*; import org.javatuples.*; import java.security.*; import java.math.*; import java.io.*; import java.util.stream.*; class Problem {\npublic static boolean isSorted( ArrayList<Long> lst) {\nHashMap<Long, Long> count_digit = new HashMap<Long, Long>(); for (long i : lst) {\nif (count_digit.containsKey (i)) {\ncount_digit.put(i, count_digit.get(i) + 1) ; } else { count_digit.put(i, 1L);\n} }\nfor (long i : lst) { if (count_digit.get(i) > 2) {\nreturn false; }\n} for (int i = 1; i < lst.size(); i++) {\nif (lst.get(i - 1) > lst. get(i)) {\nreturn false; }\n} return true;\n} }\n### Python version\nfrom typing import List\ndef separate_paren_groups(paren_string: str) -> List[str]:\nresult = [] current_string = [] current_depth = 0\nfor c in paren_string: if c == \u2019(\u2019:\ncurrent_depth += 1 current_string.append(c) elif c == \u2019)\u2019: current_depth -= 1 current_string.append(c)\nif current_depth == 0: result.append(\u2019\u2019.join(\ncurrent_string)) current_string.clear()\nreturn result\n### Explanation\nThe code is an implementation of a function \u2018separate_paren_groups\u2018 which takes a string of parentheses and returns a list of strings containing the groups of parentheses.\nresult = [] current_string = [] current_depth = 0 The function initializes an empty list \u2018result\u2018 to store the groups of parentheses. It also initializes an empty list \u2018current_string\u2018 to store the current group of parentheses being processed. The variable \u2018current_depth\u2018 is initialized to 0 to keep track of the depth of the current group of parentheses.\nfor c in paren_string: The function iterates through each character \u2018c\u2018 in the input string \u2018 paren_string\u2018.\nif c == \u2019(\u2019: current_depth += 1 current_string.append(c) If the character is an opening parenthesis \u2018\u2019(\u2019\u2018, the function increments the \u2018current_depth\u2018 variable and appends the character to the \u2018 current_string\u2018 list.\nelif c == \u2019)\u2019: current_depth -= 1 current_string.append(c) If the character is a closing parenthesis \u2018\u2019)\u2019\u2018, the function decrements the \u2018current_depth\u2018 variable and appends the character to the \u2018 current_string\u2018 list.\nif current_depth == 0: result.append(\u2019\u2019.join( current_string)) current_string.clear() Let\u2019s break it down step by step: \u2019\u2019.join(current_string) The program joins the \u2018current_string\u2018 (which is represented as a list) into a string Hence, the if-statement does the following: if the \u2018current_depth\u2018 variable reaches 0, it means that the current group of parentheses has been fully processed, so the function joins the \u2018current_string\u2018 list into a string and appends it to the \u2018result\u2018 list. The \u2018current_string\u2018 list is then cleared to start processing the next group of parentheses.\nreturn result The function returns the \u2018result\u2018 list containing the groups of parentheses.\n### Java version\nimport java.util.*; import java.lang.reflect.*; import org.javatuples.*; import java.security.*; import java.math.*; import java.io.*; import java.util.stream.*; class Problem {\npublic static ArrayList<String> separateParenGroups(String paren_string) {\nArrayList<String> result = new ArrayList<String>(); ArrayList<Character> current_string = new ArrayList< Character>(); int current_depth = 0;\nfor (int i = 0; i < paren_string.length(); i++) {\nchar c = paren_string. charAt(i); if (c == \u2019(\u2019) {\ncurrent_depth += 1; current_string.add(c); } else if (c == \u2019)\u2019) { current_depth -= 1; current_string.add(c);\nif (current_depth == 0) {\nStringBuilder sb = new StringBuilder( current_string.size ()); for (Character ch : current_string) {\nsb.append(ch); } result.add(sb. toString()); current_string. clear();\n} }\n}\nreturn result; }\n}\n### Python version\nfrom typing import List\ndef sort_array(array: List[int]) -> List[int]:\nreturn [] if len(array) == 0 else sorted(array, reverse= (array[0]+array [-1]) % 2 == 0)\n### Explanation\nThe code is an implementation of a function that sorts an array of integers. The function takes an array of integers as input and returns a sorted array of integers.\nreturn [] if len(array) == 0 else sorted(array, reverse= (array[0]+array [-1]) % 2 == 0) This return statement contains an inline if-else statement. Let\u2019s break it down step by step: [] if len(array) == 0 This inline if statement checks if the length of the input array is zero. If the length of the input array is zero, the function returns an empty array. If the length of the input array is not zero, the function proceeds to else statement. sorted(array, reverse= (array[0]+array [-1]) % 2 == 0) This function sorts the list \u2018array\u2018. Let\u2019s break it down step by step: (array[0]+array[-1]) % 2 == 0 The reverse parameter of the sorted function is set to True if the sum of the first and last elements of the\ninput array is even. If the sum of the first and last elements of the input array is odd, the reverse parameter of the sorted function is set to False. Hence, the else statement sorts the input array. If the sum of the first and last elements of the input array is even, it is sorted with reverse=True ( descending order), otherwise, it is sorted with reverse=False (ascending order). Hence, the entire inline if-else statement (and the return statement) returns an empty array if the length of the input array is zero. Otherwise, if the sum of the first and last element of the array is even, the array is sorted with reverse parameter set to True. If the sum of the first and last element of the array is odd, the reverse parameter is set to False.\n### Java version\nimport java.util.*; import java.lang.reflect.*; import org.javatuples.*; import java.security.*; import java.math.*; import java.io.*; import java.util.stream.*; class Problem {\npublic static ArrayList<Long> sortArray(ArrayList<Long> array) {\nif (array.size() == 0) { return new ArrayList<Long >(); } else { boolean reverse = (array. get(0) + array.get(array. size() - 1)) % 2 == 0; Collections.sort(array); if (reverse) {\nCollections.reverse( array);\n} return array;\n} }\n}\n### Python version\n<insert Python program here>\n### Explanation\nThe code is an implementation of<insert Python program explanation here>\n### Java version\n<insert Java completion here>"
        },
        {
            "heading": "D Computational Resources",
            "text": "All completion queries are made to GPT-3.5 (gpt3.5-turbo-3010) between March - June 2023. We use the Azure completion endpoint (as opposed to the OpenAI chat completion endpoint). Compute credits are provided by MIT-IBM Watson AI Lab. The average query time for a single experiment (e.g. exp, Python \u2192 Julia) takes around 10-40 minutes with one API key (of which we only used one). For Tables 1 and 2, the total query time is between 21 - 84 hours. The experiments for CodeGen2-1B are conducted on BU SCC and MIT Satori clusters with NVIDIA RTX V100, A100, and A6000. Each single experiment takes 12-24 hours so the whole compute time for Tables 1 and 2 completions is around 72-144 GPU days. Experiments for CodeGen2-16B and Llama2CodeInstruct-34B are done through IBM hosted model inference APIs. Query time is about 2-3X to that of GPT-3.5. Code execution/evaluation is done locally on the same MacBook Pro 2015."
        },
        {
            "heading": "E Language Specific Stop Token and Post-Processing",
            "text": "To stop model generations and extract the relevant code needed for the problem, we use a few language-specific tokens. In the case of OpenAI APIs, such token size limit is 4. In order to accommodate multi-function and remove irrelevant generations, we modify the stop tokens from MultiPL-E and add post processing to some languages. There are two types of post-processing we use to truncate completions:\ntruncate_after_function_ends requires two function implementations from each language translator: is_end_of_function and is_function_signature. The function works by greedily search the completion line-byline. As soon as we encounter a end of a function, we start looking for the next non-empty line. We remove the rest if the next non-empty line is not a function signature. Otherwise, we keep the line and continue. Below is the Python implementation of truncate_after_function_ends: def truncate_after_function_ends(completion, translator):\nlines = completion.split(\"\\n\") in_function = True for i, l in enumerate(lines):\nif in_function: # if encounter end of function,\nstart looking out for next non empty line\nif translator.is_end_of_function( l):\nin_function = False else: # if we are not in function,\ncheck if line is signature if len(l.strip()) != 0: if translator. is_function_signature(l):\nin_function = True else:\nlines = lines[:i] break\ntruncated_completion = \"\\n\".join(lines) return truncated_completion\ntruncate_after_additional_stops requires one additional implementation get_function_name and additional_stops property. This function is used to deal with languages that are difficult to determine the end of functions without using lexers (hard to implement is_end_of_function). In this case, we put additional stop tokens that indicate for certain that the line is not inside a function (usually \\n with every single letter in the alphabet along with special symbols such as \\n#, \\n!.) We additionally add the main completion function\u2019s name to make sure the completion doesn\u2019t call the function itself at the base level."
        },
        {
            "heading": "E.1 JavaScript",
            "text": "\u2022 original stops: \\nfunction, /*, //, console.log\n\u2022 modified stops: \\n}, /*, //, console.log. Instead of stopping at the beginning of the next irrelevant code segment, we stop right before where the function ends, and add closing bracket back with test strings. This speeds up experiments by removing all possible additional un-expected completions.\nE.2 C++ \u2022 original stops: \\n}.\n\u2022 no modification."
        },
        {
            "heading": "E.3 Java",
            "text": "\u2022 original stops: \\n }\\n\n\u2022 modified stops: public static void main, ###, \\n}. Nested functions are illegal in Java. In order to allow models to generate multiple functions, instead of stopping at the end of\na function, we stop right before the main function, or end of class."
        },
        {
            "heading": "E.4 TypeScript",
            "text": "\u2022 original stops: \\nfunction, /*, //, console.log\n\u2022 modified stops: \\n}, /*, //, console.log. Same reason as JavaScript"
        },
        {
            "heading": "E.5 PHP",
            "text": "\u2022 original stops: \\nfunction, \\n?>, \\n//, \\n#\n\u2022 modified stops: \\n}, \\n?>, \\n//, \\n#. Same reason as JavaScript"
        },
        {
            "heading": "E.6 Ruby",
            "text": "\u2022 original stops: \\nclass, \\ndef, \\n#, \\n\\n\n\u2022 modified stops: \\nclass, \\ndef, \\n#, \\nputs. The original stop \\n\\n is problematic because it prematurely stops translation as soon as the source program has an extra empty line in the program. Additionally, \\nputs to ensure we don\u2019t have extra generations (selfcalls) at the end.\n\u2022 post-processing: truncate_after_function_ends"
        },
        {
            "heading": "E.7 C#",
            "text": "\u2022 original stops: \\n }\\n\n\u2022 modified stops: public static void Main, static void Main, \\n#, \\n}. Same reason as Java. Although nested/local function is allowed in C#, this increases the variety of generations which can be accepted by unit tests."
        },
        {
            "heading": "E.8 Go",
            "text": "\u2022 original stops: \\nfunc, struct, \\n//\n\u2022 modified stops: \\nfunc, struct, \\n// , \\n}. Same reason as JavaScript."
        },
        {
            "heading": "E.9 Perl",
            "text": "\u2022 original stops: \\nsub, \\n#, \\n\\n.\n\u2022 modified stops: \\nsub, \\n#, \\n}. Same reason as Ruby."
        },
        {
            "heading": "E.10 R",
            "text": "\u2022 original stops: \\n#, \\n\u2018\u2018\u2018\n\u2022 modified stops: \\n}. Same reason as JavaScript."
        },
        {
            "heading": "E.11 Rust",
            "text": "\u2022 original stops: \\n}.\n\u2022 no modification."
        },
        {
            "heading": "E.12 Scala",
            "text": "\u2022 original stops: \\n }\\n.\n\u2022 no modification."
        },
        {
            "heading": "E.13 Swift",
            "text": "\u2022 original stops: \\n}.\n\u2022 no modification."
        },
        {
            "heading": "E.14 Bash",
            "text": "\u2022 original stops: \\n}\n\u2022 modified stops: \\n#, \\nAnswer, \\necho, \\n\u2018\u2018\u2018. Although nested functions are technically allowed, it is quite conventional to write helper functions in a separate function. We also use post-processing to truncate additional unwanted generations.\n\u2022 post-processing: truncate_after_function_ends"
        },
        {
            "heading": "E.15 Lua",
            "text": "\u2022 original stops: \\nlocal, \\nfunction, \\n--, \\n\\n\n\u2022 modified stops: \\n--, \\n#, \\nend. The original stop \\n\\n is problematic because it prematurely stops translation as soon as the source program has an extra empty line in the program. We add base indentation level \\nend to truncate after the function ends."
        },
        {
            "heading": "E.16 Racket",
            "text": "\u2022 original stops: \\n(define , \\n#|, \\n;, \\n(\n\u2022 modified stops: \\n(define , \\n#|, \\n;, \\n(\n\u2022 additional stops: \\n#\", \\n\u2018\u2018\u2018, and \"\\n\" with all letters in alphabet.\n\u2022 post-processing: truncate_after_additional_ stops"
        },
        {
            "heading": "E.17 Julia",
            "text": "\u2022 original stops: \\nfunction, \\nmacro, \\n\\n,\n\u2022 modified stops: \\nend, \\n#. Same as JavaScript: \\nend is a stricter way of stopping multi-function generation since Julia allows nested functions."
        },
        {
            "heading": "E.18 D",
            "text": "\u2022 original stops: \\n\\n, \\nvoid, \\nbool, \\nint\n\u2022 modified stops: \\nvoid, \\nbool, \\nint, \\n}. More strict stopping at the end of function, and remove problematic \\n\\n\nE.19 Explanation Qualitative Analysis\n### Python version\ndef find_max(words: List[str]) -> str: return sorted(words, key = lambda x : (-len(set(x)), x))[0]\n### Explanation (exp)\nThis function receives a list of strings and returns the string with the highest number of unique characters, and if there are ties, the string that comes first in lexicographic order.\n### Explanation (exp-lbl)\nThe code is an implementation of finding the longest word in a list of words that has the fewest unique characters. It takes a list of strings \u2018words\u2018 as input and returns the longest string in the list that has the fewest unique characters.\nsorted(words, key = lambda x: (-len(set (x)), x))[0] The function first sorts the list of words using the \u2018sorted\u2018 function. The \u2018key\u2018 parameter is set to a lambda function that returns a tuple of two values: the negative length of the set of characters in the word (to sort by fewest unique characters first), and the word itself (to break ties by alphabetical order).\nThe \u2018[0]\u2018 at the end of the line returns the first (i.e., longest) word in the sorted list.\n### Explanation (exp-lbl-d)\nThe code is an implementation of a function that finds the word with the maximum number of unique characters.\nThe function takes a list of strings as input and returns the string with the maximum number of unique characters.\nsorted(words, key = lambda x: (-len(set (x)), x))[0] This statement sorts the input list \u2018 words\u2018. Let\u2019s break it down step by step: set(x) The \u2018set\u2018 function returns a set of unique characters in the string \u2018x\u2018. len(set(x)) The \u2018len\u2018 function returns the number of unique characters in the string \u2018x\u2018. -len(set(x)) The \u2018-\u2018 sign is used to sort the list in descending order of the number of unique characters. x The \u2018x\u2018 parameter is used to sort the list in ascending order of the string itself. Hence, the \u2018key\u2018 parameter of the \u2018 sorted\u2018 function is set to a lambda function that returns a tuple of two values: the negative length of the set of unique characters in the string and the string itself. The \u2018sorted\u2018 function sorts the input list based on this tuple. The \u2018[0]\u2018 at the end of the statement returns the first element of the sorted list, which is the string with the maximum number of unique characters.\nAbove is an example explanation of a program in Humaneval (humaneval_158_find_max). With exp, the explanation is very high-level and does not go into details with the Python implementation. With exp-lbl, each line of the program is referenced along with an explanation. However, we can see that in the second paragraph exp-lbl explains that words sorted \"by fewest unique characters first\" when it should be the other way. This led to the wrong translation where the negative sign is ignored. On the other hand, with exp-lbl-d, complicated line like ones above is decomposed first, explained separately and then combined together. This particular explanation emphasizes the negative sign and improves translation pass rate.\nF How did we select with few-shot programs and write explanations\nTo be consistent across trials, and due to the fact that not every program has gold translations, we use fixed few-shot in our main experiments. In this case, the few-shot examples we pick is crucial to the performance: they need to be representative of\nthe dataset characteristics and contain features that may demonstrate the usefulness of explanations. Once we select the few-shot programs, we simply use the model to generate zero-shot explanations, and modify for correctness and structural preferences (e.g. code lines followed by explanations with exp-lbl).\nOur \"development\" translation direction is Python \u2192 Java. We quickly notice that GPT-3.5 is bad at translating nested functions, which occur many times in the canonical solutions in HumanEval. Compounded with the fact that the original MultiPL-E stop tokens do not allow models to generate multiple functions and that Java does not allow nested functions, the only way for GPT-3.5 to generate a correct translation of these program is to use lambda expressions, which can be extremely convoluted if the nested function is long. Therefore, after loosening the constraint of single function only (see Java section in Apx E), we decide to use the first few-shot example humaneval_107_even_odd_palindrome as a demonstration to show the model how to translate these type of functions (by adding private helper function after the main function).\nSecond and third few-shot examples were selected quite arbitrarily. The only criteria we had in-mind was that these programs need to be somewhat difficult, on the longer side length-wise, and that the semantics of the program is not immediately clear after looking at the function name or first sentence explanation of the function. Lastly, the programs also should not be close to each other in the problem sequence (just in case when designing the dataset, (Chen et al., 2021) decided to stress-test different aspect of code-generation in batches). Hence, we picked humaneval_126_is_sorted and humaneval_1_separate_paren_groups.\nLast example is often the most influential due to the proximity to test examples (Lu et al., 2022). We decide on this program after careful error analysis of exp and exp-lbl. With few-shot exp-lbl, we found the explanations to be of great qualities already: the explanation chunks the model into several semantically independent segments and explain each of them separately. We notice that the quality of the explanations (through manual inspection) are worse when the segments are longer or more complicated. In many cases, these coincides with programs that contain one extremely\nsemantically complicated line. This is because the model has to spend paragraphs explaining these lines, and often produces confusing/wrong statements. Many of the remaining 17 assertion errors (semantic errors) in four-shot exp-lbl are due to in-correct/insufficient explanation). Section E.19 shows an example program.\nThese errors, however, can be effectively mitigated if they were further decomposed into smaller chunks of code statements. Due to the inherent tree-like structure (AST) of program, asking models to decompose is another way of learning finegrain parsing of the code. Surprisingly, models like GPT-3.5 is able to parsing very well. Hence, we developed our third explanation method exp-lbld: we ask the model to explain line-by-line, and breakdown the line into smaller parts if the line is too long or complicated. To demonstrate the usefulness of such method, we pick a program that is short but contains a long and complicated line (humaneval_88_sort_array). See E.19 for the result of explanations generated with few-shots across 3 explanation methods. With exp-lbl-d fourshot, we find the explanations generated for almost all 17 previously failing programs (in exp-lbl) to be correct, and the remaining 12 assertion errors are all results of wrong program generation given correct explanations."
        },
        {
            "heading": "G Python-to-X Detailed Result Investigation",
            "text": "All experiments detailed here are conducted with GPT-3.5 unless otherwise specified.\nG.1 Removing target language specific information in zero-shot explanation\n### Python version\ndef strlen(string: str) -> int: return len(string)\n### Explanation\nThis function takes a string as input and returns the length of the string. It does this by using the built-in \u2018len \u2018 function in Python. The function is defined with the name \u2018strlen\u2018 and takes a single argument, which is a string. The return type of the function is an integer.\nTo rewrite this program in Java, you would need to define a function with the same name and argument type, and\nJava Lua\nexp 0.829 0.593 exp (remove) 0.829 0.592\nexp-lbl 0.825 0.615 exp-lbl (remove) 0.825 0.615\nexp-lbl-d 0.846 0.595 exp-lbl-d (remove) 0.84 0.595\nAbove is an example of what target specific information in generated explanations looks like.\nSince different types of explanations generate different types of kind of information, we remove all of them in Python-to-Java direction to disentangle the effects of target-specific information vs the level of detail in the explanation. The removal is done through a script, where we remove from the first sentence that mentions anything about the target language.\nHere, we compare the pass@1 before and after removing such information for all explanation types. As seen, there is only a slight decrease in the performance. When translating into low-resource languages, the occurrence of target-specific information is much less frequent, leading to much less difference between regular explanations vs. explanations with target information removed.\nIn addition, we compare using targetindependent explanations (exp (java)) vs. target-dependent explanations (exp (tgt-specific)) in Table 8. Target specific explanations do not impact performance significantly in zero-shot (p = 0.090). Target-specific explanations tend to decrease performance in lower-resource languages."
        },
        {
            "heading": "G.2 Four-shot explanation variations",
            "text": "To observe the difference between the Pythonto-Java four-shot explanation, four-shot target language-specific explanation, and re-using zeroshot target-language specific explanation in fourshot translation, we compare their Python-to-X translation performance in 18 target languages (Table 9). Just by observing best trials across language directions in Table 9, exp (java) wins in 6/18 directions, whereas exp (tgt-specific) and exp (zero-shot) each win 5/18 and 11/18 directions. 2-tail paired t-test indicates that both alternative trials are significantly different from re-using 4 shot examples from Java (ptgt_specific = 0.036) and pzero\u2212shot = 0.024)). Four-shot explanations are worse than zero-shot generated explanation. This is intuitive because the explanations with exp method in the zero-shot setting are good enough. By incorporating mostly its own explanation in a few-shot setting, the model is not obtaining more information, but restricting its potential to generate diverse types of explanation."
        },
        {
            "heading": "G.3 heuristically selected explanations",
            "text": "For more fair comparisons, we include here the heuristically selected explanations with respective baselines. Since exp selection was done over zero-shot explanations, we compare exp (logprob) against exp (zero-shot). There is still a mean improvement of 0.73% with standard deviation of 1.8%.\nG.4 Error types breakdown in Python-to-X For each program, in addition to determine unit tests pass rate, we also use string matching on stderr and stdout to categorize the error type. In order to generalize across different languages, we group the errors into the following 6 types:\n\u2022 Type Error includes all errors related to interactions between variables with the wrong types. For example, in Python, floats cannot be used to index list, and a string cannot be used to multiply with another string.\n\u2022 Undeclared Error includes all errors calling methods or variables that do not exists. It ranges from undeclared variable, to unable to find equivalent built-in function such as string.swapcase() in Python\n\u2022 Assertion Error catches all cases where the function output does not match the expected\noutput. This indicates that the program runs, but is not functionally the same as the source.\n\u2022 Runtime Error generally includes all errors that do not occur for every unit tests. For instance, index out of bound error may only occur with input of long lists.\n\u2022 Other Syntax Errors includes all other type of errors not captured by a specific groups from above.\n\u2022 Unhelpful includes cases where the generated program contain exclusively comments like TODO, Your Code Here.\nFor better generalization, we also combine assertion error and unhelpful into semantic error and Type, Undeclared and other syntax errors into syntactic error. Here are some of our main conclusions:\nLess syntax error across the board in zero-shot In general we see a decrease of syntactic error across all target language resource level (Fig 5). Specifically, there is a significant decrease in unhelpful generations in trials with explanations. This is similar to effects of having few shot examples Min et al. (2022b), except in this case we do not actually provide the actual format of target translation. Other than reducing unhelpful generations, self-generated explanations also decrease undeclared and type error (more so in higher resource directions). This is intuitive because as model reasons through program explanations, it may generated sequences that specify variable type or specific methods used within source program, which in term provides more information for the\ntranslation step to generate appropriate method to call. Surprisingly, there is no sign of decrease in semantic error. This is likely due to the fact that by resolving syntactic errors, those programs switched to having semantic errors. In Fig 6 we look-into this phenomenon specifically.\nNo significant difference in four-shot Errors seem to be distributed very similarly across all trials. There are two exceptions. First, in highresource target languages, other syntax errors seem to drop significantly in exp-lbl-d than the other explanations, which both contain more error than direct baseline. In extremely low-resource target language, there also seem to be a somewhat significant drop in other syntax errors.\nG.5 Error conversion between direct translation and with explanations\nTo observe the program status with and without using explanations, we track each problem\u2019s status in direct and explanation trials. In Fig 6, we plot direct status on the x-axis and corresponding status with explanation on the y-axis. Here are some key take-aways:\nMore detailed explanations decrease semantic error rate In the top two rows of Fig 6, we can observe the differences between three explanation methods. In zero-shot setting (row 1), we can see that exp-lbl converts more semantic errors in direct to pass, and slightly more syntactic errors to semantic error, which are both indication of improvements. In four-shot setting (row 2), with more detailed explanations, we see consistent decrease in pass\u2192semantic error (explanation misleading translation), semantic error->pass but an increase\nof pass\u2192syntactic error. These all indicate that a more detailed explanation indeed decreases the amount of semantic errors\nHigher target language resource, proportionally more improvement with explanation, less misleads In the bottom two rows of Fig 6, we can see the effectiveness of explanation across target languages of different resource level. In zero-shot (row 3), we see a majority of the improved cases (not pass->pass) come from improving syntactic error. However, if we count the improvements of syntactic error->semantic error, the effect becomes similar. Proportionally, high-resource benefits the most in improving syntactic errors. In lower resources, there\u2019s proportionally more chance of explanation misleading the translation (pass->no pass. In four-shot (row 4), the effect of explanation is much smaller (pass->no pass or no pass->pass)\nG.6 Translation pass rate with different program lengths\nTypically, generating longer programs is harder. We look into the success rate of each our trials with respect to the source program length to observe if there are any patterns. We find that explanation affects translation across length uniformly, with better performance in high-resource long programs. In the top left box plot of Fig 7, we can see a more significant improvement for longest set of programs with explanation. This effect dampens slightly as we translate to lower-resource languages."
        },
        {
            "heading": "G.7 Python-to-X translation Pass@10 (n=20)",
            "text": "In the main result table 1, we presented Pass@1 results with GPT-3.5. For convenience and cost, we also report @10 results from the same trial (Table 11), but note that for more accurate and optimal estimation @10 should be estimated with n = 200 and t = 0.8 (Cassano et al., 2022).\nLess relative improvements than pass@1 Overall, from Table 11, we can see less improvements in pass@10. This is reasonable because ultimately adding explanation restricts the generation space and lowers the diversity of the output generations. Still, we see consistent improvements with explanations.\nZero-shot exp provide best coverage In the top 4 rows of Table 11, we can see exp outperforms the rest in the most directions (9/19). This is probably because there are countless ways of explaining\na program in free-form natural language, and abstract explanation provide the least constraints on generating a diverse set of programs (better recall)\nFour-shot better/detailed explanation leads better coverage In the bottom 4 row of Table 11, we can see that either direct translation, exp-lbl-d, or heuristically selected explanation wins. Indicating that with a good quality explanation, we can still obtain improvements in few-shot setting.\nG.8 Python-to-X translation performance vs. NL-to-Code performance\nTo investigate whether NL-to-code performance correlates to python-to-X translation performance, we compare our zero-shot results with Cassano et al. (2022) with code-davinci-002. In Fig 8, we can see a strong correlation between the two. On top of direct translation, explanations (best explanation for each target language) improve translations (absolute difference) uniformly across source languages, and a higher relative improvements in languages which are hard for NL-to-code task (lowerresource languages)"
        },
        {
            "heading": "G.9 Python-to-X for Opensource models",
            "text": "H X-to-X Opensource Model Results\nAll experiments detailed here are conducted with GPT-3.5 unless otherwise specified.\nThere are still improvements with self-generated explanations across most directions In weaker opensource models CodeGen2-1B improves more consistently (than 16B) using self-generated explanations (baseline is the worst in all X-to-X directions, and in 17/18 Python-to-X directions), as much as 300%+ improvement (12, Lua\u2192Python, Python\u2192JavaScript). In Python\u2192Ruby, model with explanations obtains a pass rate of 5.9, while baseline does not generate any single correct translation (pass rate of 0). CodeGen2-16B shows weaker results, with baseline outperforming in 10/18 directions in Table 1 and 2. Perhaps it has a weaker alignment between natural language and programming language, resulting in worse explanations generated for each problem. The majority of errors from translation with explanation are syntactic. For Llama2CodeInstruct-34B, there are consistent absolute improvements of 5%-10% and maximum relative improvements of up to 40% (Java \u2192 C++ in table 2).\nBetter explanation leads to better translation even in smaller model cases. We compare CodeGen2-1B performance given self-generated explanation vs. GPT-3.5 generated explanation and see that the better explanation outperforms selfgenerated explanation in 12/18 Python-to-X directions, with a maximum improvement of 400%+ in Python\u2192 JavaScript."
        },
        {
            "heading": "In smaller/weaker models, detailed explanations",
            "text": "(exp-lbl or exp-lbl-d) do not improve as much as exp does Often this is due to lower quality explanations generated when the model is asked to do something it is not capable of. The line-byline explanations often lead to repetitive content when source programs contain several repetitive lines (library import in C++ \u2192 Python direction, with CodeGen2-1B)\nExplanation\u2019s effectiveness at improving downstream translation is transferable between models Explanations that lead to higher pass rates in GPT-3.5 also tend to lead to higher success rates in CodeGen2-1b. If we compare Table12 exp, exp-lbl, exp-lbl-d trials with CodeGen2-1b with GPT-3.5 against the same three rows in Table 1, we see the best explanation type in each translation direction (e.g. Python \u2192TypeScript) are typically the same between two models. This is an indication of the \u201crobustness\u201d of the explanations."
        },
        {
            "heading": "I Explanation Improvement correlation with problem difficulty",
            "text": "To understand whether self-generated explanation improve more difficult problems (or vice versa), we\nplot per-problem direct pass@1 and whether exp improves over direct (Fig 4). direct pass@1 rate serves as a good approximation of how difficult the problem is given the model. In main text we discuss that the exp improves difficult problems more often than easy problems. For easy problems (the right-most column), explanations can often decrease performance. Perhaps this is a result of redundant or inconsistent information leading to confusion. This indicates that a potential way to improve performance further is to automatically pick the difficult problems to explain.\nTo show that such strategy works, we assume access to oracle metric (direct pass@1) and leverage our cached generations from direct and exp translations. For each problem, if the direct pass@1 rate is smaller than threshold (i.e. difficult problem), we use explanation, otherwise we use direct translation. We repeat this for all 36 translation directions in Python-to-X and X-to-X and present full results in Table 14. Immediately we can see that 1) low-resource languages typically require more explanations. 2) select almost always outperform direct and exp (only lost in 1 case with D\u2192C++). In best case scenario (Racket \u2192 Julia), we see as much as 9.6% relative improvement over exp with select, while explaining less than half of the problems.\nThis is however still impractical for inference during test time. Having to approximate hardness through direct translation requires more computation than generating a single explanation. Ideally, one could build classifiers or use heuristics to select programs to translate. We leave this for future directions."
        },
        {
            "heading": "J Alternative latent language guidance",
            "text": "In addition to asking the model to generate explanations, we experimented with various forms of latent languages (in the order of more structurally formal to more free-form natural language). We report here their pass@1 (n=1) for Python-Java\n\u2022 Pivot language: Instead of generating target program language directly, we also asked the program to translate to a pivot language and then translate to the target language. For initial experiment, we take the first generation from direct translation to the pivot language as intermediate step regardless of their accuracy. pass@1C++ = 0.732, pass@1Bash = 0.81, pass@1R = 0.703. More experiments in the next section.\n\u2022 Pseudocode: An intermediate form of program sketch described with a mix of mathematical operations and natural language. To ensure the format of the pseudocode, we prompt with \\\\ begin{algorithm} and use \\\\ end{algorithm} as stop token. pass@1 = 0.861\n\u2022 CoT: In Chain-of-thought (CoT) prompting, we break down the the input program space and translate each sub-components before combining all results together as a whole. In the decomposition phase, we try decomposing through model\u2019s perception of \"steps\" within algorithm, as well as programmatically extracting function calls within source programs that are often hard to translate (especially in low resource languages) pass@1 = 0.734\n\u2022 Steps: ordered list of natural language steps describing major steps of the program following work by Jiang et al. (2023). pass@1 = 0.824\n\u2022 Summary: free-form natural language sentences summary of what the program does. pass@1 = 0.854\n\u2022 Gold summary: We use the original human written docstring instructions (from HumanEval) as gold summaries for the program and ask the model to translate given the program and summary. pass@1 = 0.813\nJ.1 model\u2019s dependency on pivot program accuracy\nWithin pivot program experiments (Python-C++Java, Python-Bash-Java, Python-R-Java), we further analyzed the Java accuracy by measuring the subset accuracy: we split the set of source problems into those with a correct pivot translation, vs those with an incorrect pivot translation. Here is the result:\nAs seen in Table 15, there is no clear differences between the subset in which the pivot language passes or fails:\n\u2022 For C++ and R, regardless of the pivot accuracy, Java translation accuracy drops with pivot.\n\u2022 For Bash, regardless of the pivot accuracy, Java translation accuracy improves with pivot.\nAlthough the improvement given pivot language seem monotonic on an aggregated level, this is not to say that pivot language has no effects on translation accuracy because the programs that are correctly translated in pivot language likely has some characteristic that can confound the translation accuracy.\nTo further investigate whether we can change individual behavior in an individual problem setting, we pick either only correct or incorrect programs sampled from ChatGPT and observe translation performance (Table 16). If we do not have such a correct/incorrect pivot program we discard that problem. Since there could be bias in the dataset where for a specific language, harder problems might have more likelihood of having incorrect problems than correct problems, we experiment with various high/low resource combinations of target language and pivot, to be able to make conclusions overcoming such bias.\nFormal language as intermediate step can achieve equivalent or better results than natural language. In Table 16, comparing exp (bottom row) against Correct Pass@1, we can see that sometimes using formal language as intermediate step can indeed reach or surpass using explanation as intermediate steps. Using higher-resource pivot language than the target language always seem to help more than using lower-resource language, except for rkt-java, which could just be evaluated on\nan easier subset. This is intuitive because higher resource language generations in general are of better quality, and if the benefit of obtaining more information and generation length out-weights the noise, this is a valid way of boosting performance. Natural language can be thought of an extreme case of this with highest level of resource, with high probability of quality self-generated context.\nFormal intermediate steps are highly unpredictable. By glancing at the difference between Correct Pass@1 and Incorrect Pass@1, we can see incorrect pivot programs lead to drastically worse performance. If we observe the breakdown of the errors, we see a lot of Incorrect Unhelpful@1, indicating that the pivot programs themselves are unhelpful. Even if we assume models do not generate any unhelpful generations and combine Incorrect Unhelpful@1 with Incorrect Pass@1, we still see a significant gap between incorrect and correct pivot programs. Specifically, Incorrect Semantic Error@1 tends to be much higher than Correct Semantic Error@1. In ablation studies Table 4 we learned that when the wrong intermediate step is highly related to the source program in semantics, it decreases the translation performance more. In this experiment, since the semantics of source program and pivot program is almost identical, the mistakes in pivot program can have deleterious effects on translation.\nNatural language mistakes are taken less seriously To compare the effect of having mistakes in natural language vs pivot programs, we included Incorrect retrieved exp Pass@1 from Table 4. Since swapping an explanation with a closely related that of a similar problem guarantees the explanation to be wrong, we can compare this with Incorrect Pass@1. We found that on average, mistakes in natural language explanations do not decrease translation performance as much as programming language mistakes do."
        },
        {
            "heading": "K GPT-3.5 score heuristics",
            "text": "In addition to two heuristics mentioned in Sec 3.4, we also try prompting GPT-3.5 to select the best explanation. We follow works in automatic generation evaluation with LLMs (Kocmi and Federmann, 2023; Wang et al., 2023b; Chen et al., 2023c; Fu et al., 2023; Wang et al., 2023b) and experiment with multiple-choice, direct assessment (generating a score between 0 and 100), and summarizing from\nmultiple explanations. None of these methods outperformed random selection, so we do not include this method in Table 3. GPT-3.5-scores (direct assessment) of the explanations almost always fall between 90-100."
        },
        {
            "heading": "L Coder-reviewer details",
            "text": "Coder-Reviwer is a re-ranking method introduced by (Zhang et al., 2022) to re-rank code generations without verifying through symbolic tools (i.e. compilers) in NL-code tasks. The method found that averaging the logprob score from \"coder\" (which estimates a length-normalized p(code|NL)) and \"reviewer\" (which estimates a length-normalized p(NL|code)) can be used as a good metric to rerank code generations. Formally the score is defined as:\n\u03b1 logp(x|y) |x| + (1\u2212 \u03b1) logp(y|x) |y| (1)\nwhere x represents the natural language description of the code, y represents generated code, and \u03b1 is the hyperparameter that weighs the importance between the two terms.\nIn our problem, we have the inverse task of trying to find the best explanation x given y. Since the score is symmetric, we use the same formula during re-ranking.\nTo calculate the logprobs, we used CodeGen212B(Nijkamp et al., 2023). We use prompt in L.1 and L.2. To obtain the best performance in estimated pass rate (Table 3), we try 0,1, or 2-shots (if GPU memory allows), and vary \u03b1 between 0-1 with 0.1 interval (except for Python-Racket exp, which we tried 0.02 between 0.8-1.0 in addition to the rest). In Figure 10 we plot the best performing setting for each experiment trials across \u03b1.\nIn section L.3 and section L.4, we show an example of correct and incorrect selection of explanations by coder-reviewer (To provide an idea of what explanations look like, we include only 5 out of 20 total explanations)."
        },
        {
            "heading": "L.1 Coder prompt",
            "text": "Can you write a Python program given this explanation?\n### Explanation\nThis function takes in a list of integers and returns a boolean\nOtherwise, it sorts the list using the \u2018sorted\u2018 function. The \u2018reverse\u2018 parameter is set to \u2018True\u2018 if the sum of the first and last elements of the array is even (and the array will be sorted in descending order), otherwise, it is set to \u2018False\u2018 (and the array will be sorted in ascending order).\n### Python version\nfrom typing import List\ndef sort_array(array: List[int]) -> List[int]:\nreturn [] if len(array) == 0 else sorted(array, reverse= (array[0]+array [-1]) % 2 == 0)\n### Explanation\n<insert explanation here>\n### Python version\n<insert Python program here, calculate normalized log p on this sequence>"
        },
        {
            "heading": "L.2 Reviewer prompt",
            "text": "Can you explain what this Python program does in a couple of sentences?\n### Python version\nfrom typing import List\ndef is_sorted(lst: List[int]) -> bool: count_digit = dict([(i, 0) for i in\nlst]) for i in lst:\ncount_digit[i]+=1 if any(count_digit[i] > 2 for i in\nlst): return False\nif all(lst[i-1] <= lst[i] for i in range(1, len(lst))):\nreturn True else:\nreturn False\n### Explanation\nThis function takes in a list of integers and returns a boolean indicating whether the list is sorted in non-descending order and has no more than two occurrences of any integer. The function first creates a dictionary where the keys are the integers in the list and the values are the number of times that integer appears in the list. It then checks if any integer appears more than twice in the list, and returns False if so. Finally, it checks\nif the list is sorted in nondescending order, and returns True if so, and False otherwise.\n### Python version\nfrom typing import List\ndef sort_array(array: List[int]) -> List[int]:\nreturn [] if len(array) == 0 else sorted(array, reverse= (array[0]+array [-1]) % 2 == 0)\n### Explanation\nThis function takes a list of integers \u2018array\u2018 as input and returns a sorted list of integers. The function first checks if the input list is empty. If it is, it returns an empty list. Otherwise, it sorts the list using the \u2018sorted\u2018 function. The \u2018reverse\u2018 parameter is set to \u2018True\u2018 if the sum of the first and last elements of the array is even (and the array will be sorted in descending order), otherwise, it is set to \u2018False\u2018 (and the array will be sorted in ascending order).\n### Python version\n<insert Python program here>\n### Explanation\n<insert explanation here, calculate normalized log p on this sequence>\nL.3 Coder-Reviewer correct explanation selection\nYou are an helpful AI assistant who understands all programming languages and can translate between them at ease. Can you explain what this Python program does with a couple of sentences ? The goal with the explanation, is so that a reader can easily rewrite the program in Java.\n### Python version\nfrom typing import List\ndef match_parens(lst: List[str]) -> str :\ndef check(s): val = 0 for i in s:\nif i == \u2019(\u2019: val = val + 1 else: val = val - 1 if val < 0:\nreturn False return True if val == 0 else\nFalse\nS1 = lst[0] + lst[1] S2 = lst[1] + lst[0] return \u2019Yes\u2019 if check(S1) or check(\nS2) else \u2019No\u2019\n### Explanation 1 (pass 0/1\nThis Python program checks if it is possible to create a valid string of parentheses by concatenating two strings of parentheses. The function \u2018 match_parens\u2018 takes a list of two strings of parentheses as input and returns \u2019Yes\u2019 if it is possible to create a valid string of parentheses by concatenating the two strings, and \u2019No \u2019 otherwise.\nThe function \u2018check\u2018 checks if a given string of parentheses is valid. It does this by iterating over the string and keeping a count of the number of opening and closing parentheses. If the count of closing parentheses ever exceeds the count of opening parentheses, the string is invalid. If the count of opening and closing parentheses is equal at the end of the iteration, the string is valid.\nThe function \u2018match_parens\u2018 concatenates the two input strings in two different orders and checks if either of the resulting strings is valid using the \u2018check\u2018 function. If one of the resulting strings is valid, the function returns \u2019Yes\u2019, otherwise it returns \u2019No\u2019.\n### Explanation 2 (pass 0/1)\nThis program checks if it is possible to form a string of balanced parentheses by concatenating two strings from a given list of strings. It does this by checking if the sum of the number of opening parentheses is equal to the sum of the number of closing parentheses. If the sum of the opening parentheses is greater than the sum of the closing parentheses, then it is not possible to form a string of balanced parentheses. The program returns \u2019Yes\u2019 if it is possible to form a string of balanced parentheses and \u2019 No\u2019 otherwise.\n### Explanation 3 (pass 0/1)\nThis program checks if two strings concatenated together form a valid string of balanced parentheses. It does this by iterating through each character of the concatenated string\nand keeping track of the number of open and closed parentheses. If the number of closed parentheses ever exceeds the number of open parentheses, the string is invalid. If the number of open and closed parentheses is equal at the end of the string, the string is valid. The function returns \u2019Yes\u2019 if either the original concatenated string or the reversed concatenated string are valid, and \u2019No\u2019 otherwise.\n### Explanation 4 (pass 1/1, Coder-reviewer selected, actual pass 20/20)\nThis Python program takes a list of two strings as input, and checks if the two strings can be concatenated in any order to form a string with balanced parentheses. If the parentheses are balanced, the function returns \u2019Yes\u2019, otherwise it returns \u2019No\u2019.\nThe function \u2018check\u2018 takes a string as input and checks if the parentheses in the string are balanced. It does this by iterating through the string and keeping track of the number of open and closed parentheses encountered so far. If the number of closed parentheses encountered is greater than the number of open parentheses encountered, the function returns False. If the number of open and closed parentheses is equal at the end of the string, the function returns True, otherwise it returns False.\nThe function then concatenates the two input strings in both possible orders and checks if either of the resulting strings have balanced parentheses. If at least one of the strings has balanced parentheses, the function returns \u2019Yes\u2019, otherwise it returns \u2019No \u2019.\nTo rewrite this program in Java, you would need to replace the \u2018List\u2018 type with an array of strings, and use a for loop to iterate through the characters in the input strings. You would also need to use the \u2018StringBuilder\u2018 class to concatenate the strings. Here is an example Java implementation:\n\u2018\u2018\u2018java public static String matchParens(String [] lst) {\nStringBuilder s1 = new StringBuilder(lst[0]).append(lst[1]);\nStringBuilder s2 = new StringBuilder(lst[1]).append(lst[0]);\nif (check(s1.toString()) || check( s2.toString())) {\nreturn \"Yes\"; } else {\nreturn \"No\"; }\n}\npublic static boolean check(String s) { int val = 0; for (int i = 0; i < s.length(); i ++) { if (s.charAt(i) == \u2019(\u2019) {\nval++; } else {\nval--; } if (val < 0) {\nreturn false; }\n} return val == 0;\n} \u2018\u2018\u2018<|im_end|>\n### Explanation 5 (pass 1/1)\nThis program checks if it is possible to create a valid string of parentheses by concatenating two strings from a list of strings. The function \u2018 match_parens\u2018 takes a list of strings as input and returns \u2019Yes\u2019 if it is possible to create a valid string of parentheses by concatenating two strings from the list, and \u2019No\u2019 otherwise. The function \u2018check\u2018 checks if a string of parentheses is valid by counting the number of opening and closing parentheses. If the number of closing parentheses is greater than the number of opening parentheses at any point, the string is invalid. If the number of opening and closing parentheses is equal at the end of the string, the string is valid. The function \u2018match_parens\u2018 concatenates the first two strings in the list in two different orders and checks if either of the resulting strings is valid. If either string is valid, the function returns \u2019Yes\u2019, otherwise it returns \u2019No\u2019.\nL.4 Coder-Reviewer incorrect explanation selection\nYou are an helpful AI assistant who understands all programming languages and can translate between them at ease. Can you explain what this Python program does with a couple of sentences ? The goal with the explanation, is so that a reader can easily rewrite the program in Java.\n### Python version\nfrom typing import List\ndef words_string(s: str) -> List[str]: if not s:\nreturn []\ns_list = []\nfor letter in s: if letter == \u2019,\u2019:\ns_list.append(\u2019 \u2019) else:\ns_list.append(letter)\ns_list = \"\".join(s_list) return s_list.split()\n### Explanation 1 (pass 1/1)\nThis program takes a string as input and returns a list of words. The input string is first converted into a list of characters. If a comma is encountered in the string, it is replaced with a space. The list of characters is then joined back into a string and split into a list of words using the default whitespace separator. If the input string is empty, an empty list is returned.\n### Explanation 2 (pass 1/1)\nThis program takes a string as input, and returns a list of words. It does this by first checking if the string is empty, and if it is, it returns an empty list. If the string is not empty, it creates an empty list called \u2018 s_list\u2018, and then iterates over each letter in the input string. If the letter is a comma, it appends a space to \u2018s_list\u2018, otherwise it appends the letter itself. After iterating over all the letters in the input string, it joins all the elements in \u2018s_list\u2018 into a single string, and then splits that string into a list of words using the default whitespace separator.\n### Explanation 3 (pass 0/1, Coder-reviewer selected, actual pass 0/20)\nThis Python program takes a string as input and returns a list of words in the string. The function \u2018words_string\u2018 first checks if the input string is empty, and if it is, returns an empty list. If the input string is not empty, the function creates an empty list \u2018 s_list\u2018, and iterates over each character in the input string. If the character is a comma, the function appends a space to \u2018s_list\u2018, otherwise it appends the character. The function then joins \u2018s_list\u2018 into a single string, and splits the string into a list of words using the \u2018split()\u2018 method.\n### Explanation 4 (pass 0/1)\nThis program takes in a string as input and returns a list of words. The input string is first split into a list of characters. If a character is a comma, it is replaced with a space. The list of characters is then joined back into a string and split into a list of words. If the input string is empty, an empty list is returned.\n### Explanation 5 (pass 0/1)\nThis function takes a string as input and returns a list of words. It does this by first converting all commas in the string to spaces, and then splitting the string into a list of words. If the input string is empty, it returns an empty list."
        },
        {
            "heading": "M Alternative explanation-selection setting",
            "text": "In Sec 3.4, we sample 20 explanations for each problem and generate 1 program from each explanation. By sampling more than 1 programs for each explanation, one could obtain variances of the performance estimates (by simulating pass status of each program according to the pass rates of the selected explanations in the train set), but we find sampling one program to be good enough at estimating final performance. Plus, given the same budget, it is also much better to sample 20 X 1 (Table 3) than 4 X 5 to maximize explanations diversity, and have the potential of sampling the best explanation. Table 17 shows the result for 4 X 5 experiments.\nComparing results in Table 17 to the main results in Table 3, we see much less improvements using heuristics. It is likely that 4 is not a large enough sample size to obtain the the correct explanation for some problems, resulting in low coverage and small improvements from heuristics."
        },
        {
            "heading": "N Program obfuscation",
            "text": "### Original Python version\ndef make_palindrome(string: str) -> str :\nif not string: return \u2019\u2019\nbeginning_of_suffix = 0\nwhile string[beginning_of_suffix:] != string[beginning_of_suffix:][::-1]:\nbeginning_of_suffix += 1\nreturn string + string[: beginning_of_suffix][::-1]\n### Original Explanation\nThis function takes a string as input and returns a palindrome by appending the reverse of the string\u2019s suffix to the string. The suffix is the smallest substring that needs to be appended to the string to make it a palindrome. If the input string is empty, the function returns an empty string.\n### Obfuscated Python version\ndef FUNC_0(VAR_0: str) -> str: if (not VAR_0):\nreturn \u2019\u2019 VAR_1 = 0 while (VAR_0[VAR_1:] != VAR_0[VAR_1\n:][::(- 1)]): VAR_1 += 1\nreturn (VAR_0 + VAR_0[:VAR_1][::(1)])\n### Obfuscated Explanation\nThis function takes a string as input and returns a string. If the input string is empty, it returns an empty string. Otherwise, it finds the longest suffix of the input string that is also a prefix of the reverse of the input string. It then returns the input string concatenated with the reverse of the remaining part of the input string.\nAbove is an example of the program (humaneval_10_make_palindrome) before and after obfuscation using tools from (Lachaux et al., 2020). After obfuscation, function and variable names are all replaced with respective surface forms, as the functionality of the program remains unchanged. As the example indicates, explanation quality does not really decrease. In fact, explanations often become more detailed just because there is not a generic way of describing some operation/term like \"palindrome\".\nTo qualitatively examine explanations\u2019 effect on translating semantically confusing programs, we translated obfuscated Python programs using direct, exp, and exp-lbl (Table 18). Similar to Python-to-X experiment, we generate explanations with Python-Java, remove Java specific explanations, and re-use explanations across the rest of the\ndirections. We find that explanation is robust regardless of surface semantics / readability of source code. In direct, we see consistent drops in performance across all translation directions. However, explanations still provide consistent improvements across all four languages. The relative improvements from explanation is even larger in 2 out of 4 directions for obfuscated programs vs. nonobfuscated programs."
        },
        {
            "heading": "O Program Retrieval",
            "text": "### Query Python code\ndef correct_bracketing(brackets: str) -> bool:\ndepth = 0 for b in brackets:\nif b == \"\"(\"\": depth += 1 else: depth -= 1 if depth < 0: return False\nreturn depth == 0\n### Retrieved Python code]\ndef correct_bracketing(brackets: str) -> bool:\ndepth = 0 for b in brackets:\nif b == \"\"<\"\": depth += 1 else: depth -= 1\nif depth < 0: return False\nreturn depth == 0\n### Query Python code]\ndef fib(n: int) -> int: if n == 0:\nreturn 0 if n == 1:\nreturn 1 return fib(n - 1) + fib(n - 2)\n### Retrieved Python code]\ndef fibfib(n: int) -> int: if n == 0:\nreturn 0 if n == 1:\nreturn 0 if n == 2:\nreturn 1 return fibfib(n - 1) + fibfib(n -\n2) + fibfib(n - 3)\nAbove are examples of retrieved Python programs given query programs. For retrieval, we tokenize the python program the same way as TransCoder using \u2018tokenize\u2018 library 12. We then use BM25Okapi algorithm in rank_bm25 library to retrieve for the most similar program within the HumanEval Dataset. Although one can improve retrieval similarity by using more sophisticated methods (dense embeddings such as UniXCoder or syntax aware similarity metrics such as CodeBLEU), we find BM25 to be cheap and effective at retrieving similar programs for our ablation studies.\nP Intermediate step lengths\nIn Table 19, we report the length of the intermediate steps (explanation, pivot programs) and their respective ratio to the source program length. One could argue, that the increased length in intermediate step could lead to more computation in attention, decoding, which leads to improvement in downstream translation. Here, we note several observations and leave a detailed investigation as a direction for future work.\n12https://docs.python.org/3/library/tokenize.html\nMore detailed explanation is longer However, as we have noted in Table 1, more detailed explanations do not always lead to more improvements. In high-low-resource directions, more generic (shorter) explanations often works better. This is one of the examples where length does not correlate well with performance.\nTarget-specific information improves performance In zero-shot or four-shot settings (Table G.1, G.2), we see slight improvement with target specific explanations. However, length-wise, we do not see a pattern between target-specific explanation vs. target-independent explanation in 0 and four-shot setting.\nHeuristically selected explanations are longer Compare Python-to-X exp (four-shot, coderreviewer) vs. Python-to-X exp (four-shot) and Python-to-X exp-lbl-d (four-shot, frag) vs. Python-to-X exp-lbl-d (four-shot), we can see both heuristically selected explanations are longer than their random baselines. However, as seen in Table 3, len heuristics do not do nearly as well as winning heuristics. This indicates that length is important, but is not all the signal in determining the success in translation.\nFormal intermediate steps can be more efficient In Table 16, we see a similar scale improvements from using correct pivot programs as intermediate steps. We conclude from the table that using higher resource language as pivot works better, and in this case we do see higher-resource language tend to be longer than lower-resource languages. It would be\ninteresting to understand how does the verbosity of a language correlate to their usefulness as an intermediate reasoning step."
        }
    ],
    "title": "Explain-then-Translate: An Analysis on Improving Program Translation with Self-generated Explanations",
    "year": 2023
}