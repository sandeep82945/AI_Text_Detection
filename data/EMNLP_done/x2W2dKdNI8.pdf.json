{
    "abstractText": "Trustworthy language models should abstain from answering questions when they do not know the answer. However, the answer to a question can be unknown for a variety of reasons. Prior research has focused on the case in which the question is clear and the answer is unambiguous but possibly unknown. But the answer to a question can also be unclear due to uncertainty of the questioner\u2019s intent or context. We investigate question answering from this perspective, focusing on answering a subset of questions with a high degree of accuracy, from a set of questions in which many are inherently ambiguous. In this setting, we find that the most reliable approach to decide when to abstain involves quantifying repetition within sampled model outputs, rather than the model\u2019s likelihood or self-verification as used in prior work. We find this to be the case across different types of uncertainty and model scales, and with or without instruction tuning. Our results suggest that sampling-based confidence scores help calibrate answers to relatively unambiguous questions, with more dramatic improvements on ambiguous questions.",
    "authors": [
        {
            "affiliations": [],
            "name": "Jeremy R. Cole"
        },
        {
            "affiliations": [],
            "name": "Michael JQ Zhang"
        },
        {
            "affiliations": [],
            "name": "Daniel Gillick"
        },
        {
            "affiliations": [],
            "name": "Julian Martin Eisenschlos"
        },
        {
            "affiliations": [],
            "name": "Bhuwan Dhingra"
        }
    ],
    "id": "SP:50cacec1002e5a472b5a2efaa70e7c73ba566b74",
    "references": [
        {
            "authors": [
                "Ayush Agrawal",
                "Lester Mackey",
                "Adam Tauman Kalai"
            ],
            "title": "Do language models know when they\u2019re hallucinating references? arXiv preprint arXiv:2305.18248",
            "year": 2023
        },
        {
            "authors": [
                "Aryaman Arora",
                "Clara Meister",
                "Ryan Cotterell."
            ],
            "title": "Estimating the entropy of linguistic distributions",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 175\u2013195, Dublin, Ire-",
            "year": 2022
        },
        {
            "authors": [
                "Jannis Bulian",
                "Christian Buck",
                "Wojciech Gajewski",
                "Benjamin B\u00f6rschinger",
                "Tal Schuster."
            ],
            "title": "Tomayto, tomahto",
            "venue": "beyond token-level answer equivalence for question answering evaluation. In Proceedings of the 2022 Conference on Empirical",
            "year": 2022
        },
        {
            "authors": [
                "Chi-Keung Chow."
            ],
            "title": "An optimum character recognition system using decision functions",
            "venue": "IRE Transactions on Electronic Computers, EC-6(4):247\u2013254.",
            "year": 1957
        },
        {
            "authors": [
                "Aakanksha Chowdhery",
                "Sharan Narang",
                "Jacob Devlin",
                "Maarten Bosma",
                "Gaurav Mishra",
                "Adam Roberts",
                "Paul Barham",
                "Hyung Won Chung",
                "Charles Sutton",
                "Sebastian Gehrmann"
            ],
            "title": "Palm: Scaling language modeling with pathways",
            "year": 2022
        },
        {
            "authors": [
                "Hyung Won Chung",
                "Le Hou",
                "Shayne Longpre",
                "Barret Zoph",
                "Yi Tay",
                "William Fedus",
                "Eric Li",
                "Xuezhi Wang",
                "Mostafa Dehghani",
                "Siddhartha Brahma"
            ],
            "title": "Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416",
            "year": 2022
        },
        {
            "authors": [
                "Shrey Desai",
                "Greg Durrett."
            ],
            "title": "Calibration of pre-trained transformers",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 295\u2013302, Online. Association for Computational Linguistics.",
            "year": 2020
        },
        {
            "authors": [
                "Li Dong",
                "Chris Quirk",
                "Mirella Lapata."
            ],
            "title": "Confidence modeling for neural semantic parsing",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 743\u2013753, Melbourne, Australia. As-",
            "year": 2018
        },
        {
            "authors": [
                "Ran El-Yaniv"
            ],
            "title": "On the foundations of noisefree selective classification",
            "venue": "Journal of Machine Learning Research, 11(5).",
            "year": 2010
        },
        {
            "authors": [
                "Angela Fan",
                "Mike Lewis",
                "Yann Dauphin."
            ],
            "title": "Hierarchical neural story generation",
            "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 889\u2013898, Melbourne, Australia. Association",
            "year": 2018
        },
        {
            "authors": [
                "Chuan Guo",
                "Geoff Pleiss",
                "Yu Sun",
                "Kilian Q Weinberger."
            ],
            "title": "On calibration of modern neural networks",
            "venue": "International conference on machine learning, pages 1321\u20131330. PMLR.",
            "year": 2017
        },
        {
            "authors": [
                "Ari Holtzman",
                "Jan Buys",
                "Li Du",
                "Maxwell Forbes",
                "Yejin Choi."
            ],
            "title": "The curious case of neural text degeneration",
            "venue": "International Conference on Learning Representations.",
            "year": 2020
        },
        {
            "authors": [
                "Abhyuday Jagannatha",
                "Hong Yu."
            ],
            "title": "Calibrating structured output predictors for natural language processing",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2078\u20132092, Online. Association for Computa-",
            "year": 2020
        },
        {
            "authors": [
                "Zhengbao Jiang",
                "Jun Araki",
                "Haibo Ding",
                "Graham Neubig."
            ],
            "title": "How can we know when language models know? on the calibration of language models for question answering",
            "venue": "Transactions of the Association for Computational Linguistics, 9:962\u2013977.",
            "year": 2021
        },
        {
            "authors": [
                "Mandar Joshi",
                "Eunsol Choi",
                "Daniel Weld",
                "Luke Zettlemoyer."
            ],
            "title": "TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension",
            "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics",
            "year": 2017
        },
        {
            "authors": [
                "Saurav Kadavath",
                "Tom Conerly",
                "Amanda Askell",
                "Tom Henighan",
                "Dawn Drain",
                "Ethan Perez",
                "Nicholas Schiefer",
                "Zac Hatfield Dodds",
                "Nova DasSarma",
                "Eli Tran-Johnson"
            ],
            "title": "Language models (mostly) know what they know",
            "year": 2022
        },
        {
            "authors": [
                "Amita Kamath",
                "Robin Jia",
                "Percy Liang."
            ],
            "title": "Selective question answering under domain shift",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5684\u2013 5696, Online. Association for Computational Lin-",
            "year": 2020
        },
        {
            "authors": [
                "Lorenz Kuhn",
                "Yarin Gal",
                "Sebastian Farquhar."
            ],
            "title": "Clam: Selective clarification for ambiguous questions with large language models",
            "venue": "arXiv preprint arXiv:2212.07769.",
            "year": 2022
        },
        {
            "authors": [
                "Lorenz Kuhn",
                "Yarin Gal",
                "Sebastian Farquhar."
            ],
            "title": "Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation",
            "venue": "arXiv preprint arXiv:2302.09664.",
            "year": 2023
        },
        {
            "authors": [
                "Volodymyr Kuleshov",
                "Percy S Liang."
            ],
            "title": "Calibrated structured prediction",
            "venue": "Advances in Neural Information Processing Systems, volume 28. Curran Associates, Inc.",
            "year": 2015
        },
        {
            "authors": [
                "Aviral Kumar",
                "Sunita Sarawagi."
            ],
            "title": "Calibration of encoder decoder models for neural machine translation",
            "venue": "arXiv preprint arXiv:1903.00802.",
            "year": 2019
        },
        {
            "authors": [
                "Jakob Uszkoreit",
                "Quoc Le",
                "Slav Petrov."
            ],
            "title": "Natural questions: A benchmark for question answering research",
            "venue": "Transactions of the Association for Computational Linguistics, 7:452\u2013466.",
            "year": 2019
        },
        {
            "authors": [
                "Kenton Lee",
                "Ming-Wei Chang",
                "Kristina Toutanova."
            ],
            "title": "Latent retrieval for weakly supervised open domain question answering",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6086\u20136096, Florence,",
            "year": 2019
        },
        {
            "authors": [
                "Stephanie Lin",
                "Jacob Hilton",
                "Owain Evans."
            ],
            "title": "Teaching models to express their uncertainty in words",
            "venue": "arXiv preprint arXiv:2205.14334.",
            "year": 2022
        },
        {
            "authors": [
                "Zhen Lin",
                "Shubhendu Trivedi",
                "Jimeng Sun."
            ],
            "title": "Generating with confidence: Uncertainty quantification for black-box large language models",
            "venue": "arXiv eprints, pages arXiv\u20132305.",
            "year": 2023
        },
        {
            "authors": [
                "Yao Lu",
                "Max Bartolo",
                "Alastair Moore",
                "Sebastian Riedel",
                "Pontus Stenetorp."
            ],
            "title": "Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for",
            "year": 2022
        },
        {
            "authors": [
                "Potsawee Manakul",
                "Adian Liusie",
                "Mark JF Gales."
            ],
            "title": "SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models",
            "venue": "arXiv preprint arXiv:2303.08896.",
            "year": 2023
        },
        {
            "authors": [
                "Clara Meister",
                "Afra Amini",
                "Tim Vieira",
                "Ryan Cotterell."
            ],
            "title": "Conditional Poisson stochastic beams",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 664\u2013681, Online and Punta Cana, Dominican Re-",
            "year": 2021
        },
        {
            "authors": [
                "Clara Meister",
                "Tiago Pimentel",
                "Gian Wiher",
                "Ryan Cotterell."
            ],
            "title": "Locally typical sampling",
            "venue": "Transactions of the Association for Computational Linguistics, 11:102\u2013121.",
            "year": 2023
        },
        {
            "authors": [
                "Sewon Min",
                "Julian Michael",
                "Hannaneh Hajishirzi",
                "Luke Zettlemoyer."
            ],
            "title": "AmbigQA: Answering ambiguous open-domain questions",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 5783\u2013",
            "year": 2020
        },
        {
            "authors": [
                "Ian Osband",
                "Zheng Wen",
                "Seyed Mohammad Asghari",
                "Vikranth Dwaracherla",
                "Morteza Ibrahimi",
                "Xiuyuan Lu",
                "Benjamin Van Roy."
            ],
            "title": "Epistemic neural networks",
            "venue": "arXiv preprint arXiv:2107.08924.",
            "year": 2021
        },
        {
            "authors": [
                "Jie Ren",
                "Jiaming Luo",
                "Yao Zhao",
                "Kundan Krishna",
                "Mohammad Saleh",
                "Balaji Lakshminarayanan",
                "Peter J Liu."
            ],
            "title": "Out-of-distribution detection and selective generation for conditional language models",
            "venue": "The Eleventh International Conference on Learn-",
            "year": 2023
        },
        {
            "authors": [
                "Ivan Stelmakh",
                "Yi Luan",
                "Bhuwan Dhingra",
                "MingWei Chang."
            ],
            "title": "ASQA: Factoid questions meet long-form answers",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 8273\u20138288, Abu Dhabi, United",
            "year": 2022
        },
        {
            "authors": [
                "Neeraj Varshney",
                "Chitta Baral."
            ],
            "title": "Model cascading: Towards jointly improving efficiency and accuracy of NLP systems",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 11007\u201311021, Abu",
            "year": 2022
        },
        {
            "authors": [
                "Neeraj Varshney",
                "Swaroop Mishra",
                "Chitta Baral."
            ],
            "title": "Investigating selective prediction approaches across several tasks in IID, OOD, and adversarial settings",
            "venue": "Findings of the Association for Computational Linguistics: ACL 2022, pages 1995\u20132002,",
            "year": 2022
        },
        {
            "authors": [
                "Xuezhi Wang",
                "Jason Wei",
                "Dale Schuurmans",
                "Quoc Le",
                "Ed Chi",
                "Denny Zhou."
            ],
            "title": "Self-consistency improves chain of thought reasoning in language models",
            "venue": "arXiv preprint arXiv:2203.11171.",
            "year": 2022
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "brian ichter",
                "Fei Xia",
                "Ed H. Chi",
                "Quoc V Le",
                "Denny Zhou"
            ],
            "title": "2022. Chain of thought prompting elicits reasoning in large language models",
            "venue": "In Advances in Neural Information Processing Systems",
            "year": 2022
        },
        {
            "authors": [
                "Allen R. Wilcox."
            ],
            "title": "Indices of qualitative variation and political measurement",
            "venue": "The Western Political Quarterly, 26(2):325\u2013343.",
            "year": 1973
        },
        {
            "authors": [
                "John M Zelle",
                "Raymond J Mooney."
            ],
            "title": "Learning to parse database queries using inductive logic programming",
            "venue": "Proceedings of the National Conference on Artificial Intelligence, pages 1050\u20131055.",
            "year": 1996
        },
        {
            "authors": [
                "Hugh Zhang",
                "Daniel Duckworth",
                "Daphne Ippolito",
                "Arvind Neelakantan."
            ],
            "title": "Trading off diversity and quality in natural language generation",
            "venue": "Proceedings of the Workshop on Human Evaluation of NLP Systems (HumEval), pages 25\u201333, Online. As-",
            "year": 2021
        },
        {
            "authors": [
                "Michael Zhang",
                "Eunsol Choi."
            ],
            "title": "SituatedQA: Incorporating extra-linguistic contexts into QA",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7371\u20137387, Online and Punta Cana, Dominican Re-",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Any practical Question Answering (QA) system must be able to assess its own confidence so that it can (1) avoid making up incorrect, incomplete, or misleading answers, and (2) request clarification to resolve ambiguity arising from unclear questions or missing context. In recognition of this desirable property, recent work has addressed confidence estimation of Large Language Models (LLMs) by evaluating QA tasks in unambiguous settings (Jiang et al., 2021; Kadavath et al., 2022). In these experiments, questions are posed to the model with multiple-choice answers so the probability of each answer can be computed. Here we extend the study of confidence to practical scenarios, requiring freeform text answers to arbitrary questions which may be underspecified or ambiguous.\nWe describe a range of experiments to help disentangle uncertainty about the world from uncertainty about the question. While we focus on the question answering capabilities of large pretrained language models, these two forms of uncertainty can be more clearly distinguished in an idealized scenario in which a symbolic question-answering system first converts the question into a formal denotation (such as an SQL query or lambda expression), which is then applied to a knowledge base (e.g., Zelle and Mooney, 1996). The system may be uncertain about the meaning of the question \u2014 denotational uncertainty \u2014 if the question is underspecified because of assumed background knowledge or context. For any given denotation, the system may also be uncertain about the correct answer \u2014 epistemic uncertainty \u2014 if the knowledge base is incomplete1. In Figure 1, denotational uncertainty is shown in the upper fork and epistemic uncertainty is shown in the lower set of forks.\nThe most competitive systems today do not construct explicit denotations of questions. As a result, it can be difficult to clearly separate these two forms of uncertainty. Nonetheless, our general approach is to attempt to first approximately disambiguate the question (in natural language) and then selectively answer user questions with sufficiently high confidence. This allows the model to more effectively represent ambiguous user inputs. This scheme is represented in Figure 1. Further, we argue that repeated sampling within our disambiguate-then-answer framework provides reliable confidence estimates of the model.\nWe summarize our contributions as follows:\n1. We reframe the discussion of model confidence with respect to denotational and epistemic uncertainty and address each in turn.\n1Gruber et al. (2023) present an overview of uncertainty in Machine Learning, making a related distinction between epistemic and aleatoric uncertainty. We do not address aleatoric uncertainty in this work.\nOur experiments suggest that the answer probabilities given by current LLMs are somewhat helpful for assessing epistemic uncertainty but cannot detect denotational uncertainty.\n2. We present two simple and effective approaches for measuring confidence under all types of uncertainty, based on answer counts within repeated samples from the LLM. These approaches are better calibrated than the model\u2019s answer probabilities and a self-verification prompt, and are especially effective for ambiguous questions and for instruction-tuned model variants."
        },
        {
            "heading": "2 Calibration for Question Answering",
            "text": "A predictor is said to be well-calibrated if its predictions are accompanied by confidence scores and those scores are informative of the likelihood that the prediction is correct. Such confidence scores can be used to support applications such as selective prediction, in which the model can abstain from predicting when its confidence is low (Chow, 1957; El-Yaniv et al., 2010). In selective question answering (e.g., Kamath et al. 2020; Zhang et al. 2021b), we assign each question-answer pair (q, a) a confidence score s(q, a). The system\u2019s output is then parameterized by a threshold \u03c4 ,\ny\u0302\u03c4 (q) = { arg maxa s(q, a), maxa s(q, a) > \u03c4 \u2205, else,\n(1)\nwith \u2205 representing abstention. Given a probabilistic model P (a | q), a natural choice for the confidence score s(q, a) is the conditional probability of the answer. This and other scoring functions are discussed in Section 3.\nFor information-seeking queries such as those found in Natural Questions (Kwiatkowski et al., 2019), the askers have no answer in mind, which makes it more likely that the questions are accidentally ambiguous. The task of answering an ambiguous question requires solving at least two subtasks: determining the meaning of the question (its denotation), and then identifying the answer. This pipeline is shown in Figure 1; in a probabilistic model, this can be represented mathematically as P (a | q) = \u2211 d P (a | d)P (d | q), with d indicating the denotation. While prior work has investigated the calibration of answer probabilities given by pretrained language models (e.g., Kadavath et al., 2022), it is not clear whether the relatively positive findings of these studies extend to questions with significant denotational ambiguity, as represented by datasets such as AmbigQA (Min et al., 2020) and SituatedQA (Zhang and Choi, 2021). We address this question in Section 6.4."
        },
        {
            "heading": "3 Confidence Scores",
            "text": "Before proposing confidence scores for question answering, it will be helpful to review how these answers are generated in practical settings. A language model over a finite vocabulary \u03a3 defines a probability distribution X over sequences of to-\nkens in \u03a3\u2217, by modeling the marginal probability of a sequence p(\u03c31, \u00b7 \u00b7 \u00b7 , \u03c3n) using the conditional probabilities of individual tokens via the chain rule\u220fn i=1 p(\u03c3i|\u03c3i\u22121, \u00b7 \u00b7 \u00b7 , \u03c3n). For QA, answers are obtained by conditioning on the question q and first sampling from X (or a version of X adjusted with modifications such as temperature) and then applying a normalization or extraction process to it in order to map it to an answer space A. This function f : \u03a3\u2217 \u2192 A can be as simple as punctuation removal and lower-casing, or as complex as code execution or extracting the answer from a chain-of-thought (Wei et al., 2022).\nUsing this background, we can describe various methods for estimating the scores of predictions from a language model (Figure 2).\nLikelihood The most basic method we can use is likelihood-based calibration by computing p(\u03c3|q) = \u220fn i=1 p(\u03c3i|\u03c3i\u22121, \u00b7 \u00b7 \u00b7 , \u03c3n, q) for a sequence sampled from X . This may be used to rank the answers from most confident to least confident based on the model\u2019s produced likelihood of that answer. However, in practical settings this method may not accurately reflect the probability of observing an answer for several reasons:\n(1) Language models typically incorporate several inference-time biases to improve the quality of the outputs, such as nucleus sampling (Holtzman et al., 2020), top-k sampling (Fan et al., 2018), length penalties, length truncation, and temperature. These techniques affect the output distribution in ways that are hard or impossible to capture in the likelihood score (Zhang et al., 2021a).\n(2) Even for decoders that do not incorporate inference-time biases, the likelihood function as defined by the auto-regressive decomposition might leak probability mass to infinite sequences (see Meister et al., 2023, Proposition 2.4). This implies that the model that we sample from might not be a valid probability distribution.\n(3) Finally, the likelihood fails to account for the extraction step f , which requires an intractable marginalization step over the model outputs.\nIn view of these limitations, it is preferable to use sampling to estimate properties of the resulting marginal distribution. Formally, we can use an Index of Qualitative Variation (IQV) (Wilcox, 1973) to measure the statistical dispersion of discrete samples produced by the model. Practically, there are a\nmany possible IQVs, so we have chosen two fairly different approaches for our experiments. In each case, we generate 10 sampled outputs (at a temperature of 0.5) and use exact match (after lowercasing and removing puncutation) for comparison among outputs. Naturally, there are many other ways to compare outputs, including token-level overlap or an answer-span equivalence model, but we leave these alternatives for future work. See also Kuhn et al. (2023) and Lin et al. (2023) for concurrent work investigating alternative IQVs.\nSampling Repetition Our first approach is based on Wilcox\u2019s classic Variation Ratio which measures deviation from the mode. Instead, we simply compute the fraction of times that the sampled outputs match the greedy output (Temperature = 0.0). The more samples match the greedy output, the more confident we are in the answer. We count the number of samples which exactly match the greedy answer. We experimented briefly with more sophisticated approaches based on the BERT based answer equivalence model from Bulian et al. (2022) as well as the summed F1 score across sampled answers and the greedy answers, but these approaches did not meaningfully improve the calibration.\nSampling Diversity Our second approach is based on the diversity of the samples, computed by 1\u2212 num_uniquenum_samples . Here, our confidence is inversely proportional to the number of distinct samples and is estimated as zero if all samples are different. Note that while this works relatively well in practice, it is heavily dependent on the number of samples, as we do not expect the number of unique answers to scale linearly with the number of samples.\nWe note that some recent work (Meister et al., 2021; Arora et al., 2022) has also described estimating statistics of random variables induced by language models. This involved sampling without replacement and using importance weighting, but they seem to surpass Monte-Carlo estimates only with many samples and peaky distributions. We leave further exploration of these methods on our evaluations as well as other IQVs for future work.\nSelf-Verification Finally, we consider selfverification, in which the language model assesses its own confidence (Kadavath et al., 2022). This is a two-step process, where first the model is prompted to provide a list of possible answers given a question. Then, this list is fed back into the model,\nwhere the task is to determine if each answer is correct or incorrect. In practice, we re-use the sampled outputs from the previous run as the list of possible answers, and the greedy answer as the proposed answer. Then, the model scores the token \u201cTrue\u201d, which we use to obtain a confidence score. In practice, these possible answers come from sampling with replacement, similar to the Sampling Repetition and Diversity methods, so the list of possible answers may contain duplicates. While these duplicates may affect the usefulness of the brainstorming exercise, the model is not choosing among them; instead, it is only scoring whether the greedy answer is correct, using the list of possible answers as context."
        },
        {
            "heading": "4 Evaluation setup",
            "text": "Our evaluations test the utility of the confidence scoring methods described in Section 3. We focus mainly on the pretrained language model PaLM, which obtains high accuracy on several question answering datasets using few-shot in-context learning (Chowdhery et al., 2022)."
        },
        {
            "heading": "4.1 Metrics",
            "text": "There are many metrics for evaluating selective question answering systems, but fundamentally we want systems that (1) frequently return correct answers, and (2) rarely return incorrect answers.\nThese criteria are in conflict because any system can achieve zero error rate by always abstaining from answering. When the confidence score is a probability, we can also ask whether this probability is well-calibrated, in the sense that a confidence score of s(q, a) = \u03b1 implies \u03b1 is the probability of a being the correct answer to q.\nExpected Calibration Error (ECE) Predictions are grouped into ten equally sized bins, ranked by the evaluated system\u2019s assigned confidence scores. We compute the mean absolute distance between the average confidence score and the accuracy of predictions in each bin, averaging across all bins. If we interpret a confidence score to represent a probability, this corresponds to the difference in the predicted probability of correctness from the actual probability of correctness.\nROC-AUC Area under the receiver operating characteristic curve evaluates the uncertainty estimate\u2019s diagnostic ability as a binary classifier for correct and incorrect predictions by integrating over the tradeoff curve between the rates of true and false positives.\nCoverage@Acc While ECE and ROC-AUC assess absolute and relative calibration respectively, we want a metric closely aligned with a practical use case: selective answering above a confidence threshold. To this end, we measure the fraction\nof questions the system can answer correctly if it needs to maintain a certain accuracy. Specifically, C@ Acc is the maximum coverage such that the accuracy on the C% of most-confident predictions is at least Acc %. For example, if C@80 = 20, then the system achieves at least 80% accuracy on its 20% most-confident predictions (and lower than 80% accuracy on its X% most-confident predictions for any X > 20). With user-facing systems in mind, we set an accuracy of 80% (C@80)."
        },
        {
            "heading": "5 Unambiguous Questions",
            "text": "To start, we explore the case of unambiguous questions with a single answer. In this scenario, all uncertainty should be of the epistemic form; the denotation of the questions should be clear. Previous work with this scenario has focused on the domain shift setting (Kamath et al., 2020) where the goal is to avoid answering questions from other datasets. Other work has looked at reading comprehension with relatively small models (Zhang et al., 2021b; Jiang et al., 2021). Open-domain question answering is substantially more difficult than reading comprehension, which likewise makes calibration more challenging."
        },
        {
            "heading": "5.1 Datasets",
            "text": "We explore this setting using two datasets: TriviaQA (Joshi et al., 2017) and NQOpen (Kwiatkowski et al., 2019; Lee et al., 2019) because they are widely used for few-shot question answering. However, while it might be assumed that each question in these datasets has only one possible interpretation, this is often not the case. In fact, AmbigQA (Min et al., 2020) finds that over 50% of NQ-Open questions are underspecified. As such, we limit ourselves to unambiguous questions as annotated in AmbigQA. We will discuss the extended setting that includes underspecified queries in Section 6."
        },
        {
            "heading": "5.2 Experiment Setup",
            "text": "Our investigation focuses on few-shot in-context learning with large language models (LLMs). Our primary prompt is composed of four question and answer pairs from the training sets of Natural Questions and TriviaQA, respectively (see Figure 2). To reduce variance across experiments, the example QA pairs are selected randomly so that each input to the model has a different version of the prompt. We compute metrics over the entire evaluation set\nfor both datasets. We discuss investigation of other prompts in Appendix B."
        },
        {
            "heading": "5.3 Calibration Results",
            "text": "Results can be found in Table 1 and Figure 3 . First, we observe that the verification method used in prior work is inferior across nearly all calibration metrics. For unambiguous Natural Questions, depending on the metric used, we see that likelihood and sample repetition both work relatively well as calibration metrics. For TriviaQA, we see that the sample repetition method works notably better: this is potentially due to the presence of ambiguous questions in TriviaQA, which we do not attempt to remove. This hypothesis is strengthened by experiments in the next section that show that sampling methods tend to improve calibration more in the presence of more ambiguity."
        },
        {
            "heading": "6 Ambiguous Questions",
            "text": "Questions can be ambiguous for a variety of reasons. Language is highly context-sensitive and meaning can be lost when context is missing. Often, interlocutors fail to clearly translate their intent into unambiguous language. Alternatively, ambiguity occurs due to a lack of alignment between interlocutors at a higher level of abstraction. Regardless of its origin, ambiguous questions give rise to denotational uncertainty, which we can conceptualize as a latent distribution over interpretations.\nRealistic scenarios such as search and chat frequently contain ambiguous questions, which makes calibration more difficult because uncertainty may be epistemic, denotational, or both. Our approach will again involve sampling: this time over both interpretations and answers. Note that in this case, the approach has some similarity to selfconsistency (Wang et al., 2022), as we compute sampling-based confidence scores over the final answers regardless of the exact interpretation."
        },
        {
            "heading": "6.1 Datasets",
            "text": "We evaluate using two datasets of underspecified queries that are derived from NQ-Open: AmbigQA and SituatedQA.\nAmbigQA AmbigQA is created from a subset of Natural Questions (Kwiatkowski et al., 2019) that raters assess as ambiguous. After a rater deemed a question ambiguous, various unambiguous interpretations of the question and their respective answers are written. Thus, AmbigQA measures a model\u2019s ability to disambiguate-and-answer ambiguous questions. For instance, given a query like \u201cWhere does the new fallout game take place?\u201d, the model should be able to produce the disambiguated question \u201cWhere does the fallout 76 game take place?\u201d and answer with \u201cAppalachia\u201d. Models are evaluated based on the similarity between the generated question and the closest reference question as well as by producing the correct answer for the generated question.\nSituatedQA SituatedQA is likewise created from a subset of existing questions and uses a relatively similar process, but focuses more narrowly on temporal and geographic ambiguity. As the type of ambiguity and the process used to create them is different, we generally evaluate them separately. The temporal questions are annotated with the time range that a question\u2019s answer is valid. Afterward,\nadditional time ranges and their corresponding answers are crowdsourced. The geographic questions are largely created by removing references to location and then crowdsourcing locations and corresponding answers. Note that SituatedQA does not provide metrics assessing the precision or possible recall of the disambiguations (though the recall would be particularly difficult to measure)."
        },
        {
            "heading": "6.2 Experiment Setup",
            "text": "As above, we use prompts of question-answer pairs, with each example having six exemplars drawn from the training data. For AmbigQA, we use only the examples that have been further annotated by Stelmakh et al. (2022). For SituatedQA, we evaluate the Geographic and Temporal sets separately. For each example, three of the exemplars will be ambiguous; in those cases, the prompt will first include an interpretation of the question which is one of the provided disambiguations. In practice, we use the first disambiguation for each example."
        },
        {
            "heading": "6.3 Ambiguity Prediction",
            "text": "Before trying to assess calibration, we first seek to explore whether our method can predict whether a question is ambiguous according to AmbigQA. SituatedQA is not suitable for this task because its questions tend to follow templated patterns where the ambiguity arises from a missing time or place reference, while AmbigQA has more diverse expressions of ambiguity. We are not aware of prior work that evaluates ambiguity classification on AmbigQA or other datasets.\nWe use a first transformations of Section 3 to predict whether the model predicts the question is ambiguous. We discuss this in more detail in Appendix A. In short, none of the methods are particularly capable, with the best method achieving 58% accuracy when 53% of the questions are labeled as ambiguous.\nWhy is question ambiguity so difficult to predict? One possible explanation is that all queries are somewhat ambiguous. Queries are typically not intentionally underspecified: the questioner believes they have provided enough information to answer. In a conversation, this may fail and require developing common ground; in an information retrieval system, this may take the form of query refinement. All of the questions in the Natural Questions dataset were asked of the Google search engine and are answerable using Wikipedia\n(Kwiatkowski et al., 2019), yet many are labeled as ambiguous by AmbigQA.\nFor instance, \u201cWhere is the world cup going to be?\u201d might typically refer to the host country, but fans from the host country that year might instead refer to the cities the games will be played. Even binary choice questions like \u201cWho won more games between the Packers and the Bears?\u201d may appear unambiguous, but this question has different answers depending on the date it is asked.\nIn this sense, we propose that ambiguity is a function of both the interpreter and the query. However, measuring whether a model (or indeed, any interlocutor) understood a question is challenging. Often in dialogue, interlocutors may assume their counterpart understood if they receive a response similar to what they expected; alternatively, they may ask explicitly if they were understood.\nThis leads us to our disambiguate-then-answer paradigm: if there is no fundamental difference between ambiguous and unambiguous queries when taken out of the original context, then the model should first establish precisely what question it is attempting to answer."
        },
        {
            "heading": "6.4 Calibration Results",
            "text": "We evaluate calibration on ambiguous questions using the same methods described in Section 5 with results in Table 2 and Figure 4. We exclude the self-verification setup here, because its calibration is much worse on the unambiguous questions, and it is unclear how to translate the prompt when there are multiple interpretations of the question. Note that we consider the answer to be correct if it matches the answer of any of the disambiguations. We also present results for a \u201cstrict\u201d matching setup, where we only consider the answer to be correct if it matches the answer of the closest disambiguation to the provided interpretation, as measured by token overlap.\nOverall, the ambiguous questions are much more difficult than the unambiguous ones. In general, likelihood seems to become a worse method of measuring model uncertainty when the questions are ambiguous and sample repetition appears to improve calibration significantly.2 The strict matching setup does not affect the ordering of the results, though numbers are lower overall.\n2Note that the interpretations sometimes consist of longer sequences, thus confounding the usability of likelihood"
        },
        {
            "heading": "7 Instuction Tuning and Scaling",
            "text": "Pretrained language models are typically fine-tuned before being used in applications. However, finetuning and other alignment techniques may disturb the model\u2019s calibration by emphasizing high performance on specific sets of tasks rather than the full pretraining distribution.3 Table 4 shows that for Flan-PaLM (Chung et al., 2022), instruction tuning dramatically improves exact match accuracy while simultaneously worsening performance on confidence-based ranking (ROC-AUC) and calibration (ECE), when using model likelihood as the confidence score. However, this miscalibration can be mitigated by using sample-based confidence scores, which dramatically improves the calibration on ambiguous questions (AmbigQA). For the selective-prediction metric C@80, the instructiontuned model with sampling is far ahead of any other method investigated. Note that we investigate Natural Questions and AmbigQA here as they are not in the instruction tuning training data.\nWe examine the effect of model scale on calibration, finding that in general, accuracy declines substantially in closed book question answering, but calibration stays roughly constant. See Appendix D for full results.\nAs sampling-based approaches require a linear increase in compute for the number of samples, we also examine how calibration scales with the number. In particular, we test three, five, and eight samples and compare that to the original results containing ten samples. Results can be found in Appendix E. Unsurprisingly, more samples seems to improve calibration, though it seems on the unambiguous Natural Questions slice, sampling diversity with a small number of samples works relatively well for the cost."
        },
        {
            "heading": "8 Related Work",
            "text": "This paper investigates calibration, selective question answering, and ambiguity within a single model. It builds on work across these topics. While AmbigQA (Min et al., 2020), ASQA (Stelmakh et al., 2022) and SituatedQA (Zhang and Choi, 2021) introduce new annotations for question answering datasets, neither these papers nor\n3The GPT-4 technical report (https://cdn.openai. com/papers/gpt-4.pdf) shows that calibration is made significantly worse by the combination of instruction-tuning and reinforcement learning, but does not distinguish between these two factors.\nany follow-up work investigate how the uncertainty in interpreting a question interacts with uncertainty about the answer."
        },
        {
            "heading": "8.1 Calibration and Selective QA",
            "text": "Calibration of modern machine learning approaches is difficult due to their non-linearity, scale, and architecture details (Guo et al., 2017). Generation-like tasks are even more difficult, containing a sequence of tokens with individual confidence estimates and a special end of sentence marker that may be poorly calibrated (Kuleshov and Liang, 2015; Kumar and Sarawagi, 2019; Jagannatha and Yu, 2020). A common approach for various natural language processing tasks is to train a separate classifier to estimate the model\u2019s confidence (Kumar and Sarawagi, 2019; Jiang et al., 2021; Zhang et al., 2021b; Kamath et al., 2020; Desai and Durrett, 2020; Dong et al., 2018) using various featurizations of the model\u2019s input, output, and likelihoods. Alternatively, Ren et al. (2023) use embedding distances and Osband et al. (2021) use Bayesian-inspired approaches that try to predict the full distribution over labels.\nWithin the specific setting of question answering, Kamath et al. (2020) and Zhang et al. (2021b) both address a similar selective question answering framework to ours, but they do not explore ambiguous questions. Varshney et al. (2022) investigate selective prediction for various tasks, and Varshney and Baral (2022) aim to improve coverage by re-answering questions that the model originally abstained on. Kuhn et al. (2023) and Lin et al. (2023) use similar sampling-based methods over free-form question answering, using slightly different formulations of confidence scores, but they do not investigate ambiguous questions. Kuhn et al. (2022) examine synthetically-created ambiguous questions, but focus on multi-turn interactions.\nQuestion ambiguity can be formalized as uncertainty about the denotation of the question, relating to prior work on uncertainty in semantic parsing (Dong et al., 2018). However, rather the expressing candidate denotations in a formal semantic representation, we express them informally in natural language, as a waypoint toward quantifying the uncertainty of candidate answers."
        },
        {
            "heading": "8.2 Self-Calibrating Language Models",
            "text": "A number of papers propose to quantify confidence and detect hallucinations by (a) directly asking the model to rate its own confidence or correctness (e.g., Kadavath et al., 2022; Lin et al., 2022), (b) drawing repeated samples (Wang et al., 2022; Manakul et al., 2023), or (c) requesting additional supporting information about proposed outputs (Agrawal et al., 2023). As discussed in Section 5 and Appendix C, we found that neither selfverification nor offering the option of \u201cunknown\u201d were effective strategies for selective question answering. Wang et al. (2022) focus on accuracy rather than calibration and Manakul et al. (2023) focus on fact-checking long-form answers."
        },
        {
            "heading": "8.3 Prompting Strategies",
            "text": "In in-context learning, the choice of prompt and the order of exemplars can affect downstream performance (Lu et al., 2022). Other work suggests that giving more long-form answers and explanations in the prompt may encourage the model to do the same, increasing its likelihood of arriving at the correct answer (Wei et al., 2022). In our work, we find the choice of prompt plays little role and that longer answers or explanations do not seem to improve calibration (see Appendix B). Zhou et al. (2023) investigate how adding words of uncertainty interacts\nwith calibration, noting that additional expressions of uncertainty improve calibration without hurting accuracy. This is an interesting direction for future research into its interaction with ambiguity."
        },
        {
            "heading": "9 Conclusion",
            "text": "We investigate the calibration of large language models, extending prior work by distinguishing uncertainty about the answer (epistemic uncertainty) from uncertainty about the meaning of the question (denotational uncertainty). We propose a disambiguate-and-answer paradigm, where the model first attempts to rephrase the question before providing its answer. This paradigm enables straightforward techniques for quantifying model confidence by counting the frequency of answers within repeated samples from the language model. These sample-based confidence metrics are particularly effective for ambiguous questions, and are significantly better calibrated than traditional likelihood-based measures and self-verification. For ambiguous questions, the proposed method is similar to calibration through self-consistency."
        },
        {
            "heading": "10 Limitations",
            "text": "In this work, we explore selectively answering ambiguous questions and calibrating such models, but we do so only within the context of a single model: PaLM. We do not explore alternative paradigms, such as supervised fine-tuning, or other large language models for in-context learning with which to replicate our results. Furthermore, we only explore calibration in a closed-book setting, despite that open-book question answering (i.e., augmented with retrieval) generally has higher performance and may pose additional calibration challenges. Moreover, our exploration of various confidence scores was relatively brief, and while we explored some additional prompts, we did not iterate heavily on prompt tuning. Lastly, while the proposed method has some advantages, especially for instruction-tuned models on ambiguous questions, it also increases the compute linearly with the number of samples."
        },
        {
            "heading": "Acknowledgements",
            "text": "We thank William W. Cohen, Urvashi Khandelwal and Clara Meister for their valuable comments and feedback while discussing this work."
        },
        {
            "heading": "A Ambiguity Prediction",
            "text": "A full description of how we transformed our method into ambiguity prediction is below.\nDisambiguate and Answer In the prompting setup described, the model should produce an interpretation of an ambiguous question before answering it. Thus, if the model produces an interpretation, it predicts that it is more like the ambiguous questions in the prompt than the unambiguous ones. We can thus use this as a prediction of ambiguity. We use both the greedy output (Greedy Disambig.) and the sampled output as a voting mechanism (Voting Disambig.).\nDisagreements and Unique Answers These methods are simply the inverse of Sampling Repetition and Sampling Diversity described in Section 3. Num Disagreements refers to the fraction of sampled answers that are not the same as the greedy answer; Num Answers refers to the fraction of unique answers produced in the sampling procedure. As previously discussed, a more diverse set of answers could reflect either question ambiguity (denotational uncertainty) or epistemic uncertainty.\nDirect Prediction We also experiment with a different few-shot prompt where instead of predicting an answer, the task is to predict either the string Ambiguous or Unambiguous. We can then use the greedy output as a binary label (Greedy Direct) or use sampling as a voting mechanism (Voting Direct).\nAs stated in the main text, none of the methods are very effective. An exploration of the precision/recall tradeoff can be found in Figure 5."
        },
        {
            "heading": "B Chain of Thought",
            "text": "Chain-of-thought prompts can improve performance by providing examples of the reasoning patterns that yield valid answers (Wei et al., 2022). As a simple chain-of-thought, we have the model\nfirst produce the long answer and then the short answer, or vice versa. These long answers are taken from ASQA for AmbigQA or the provided long answers for Natural Questions, and we refer to these conditions as \u2019Long + Short\u2019 and \u2019Short + Long\u2019, depending on whether the short answer is placed before or after the long answer. From Table 6, we can see this has small and mostly negative effects on Natural Questions: the strongest effect comes from the decline in accuracy when the long answer is produced first, because the model sometimes hits its max decode length. For ambiguous questions, found in Table 7, there is a small gain in accuracy at the cost of every calibration metric.\nIn the direction of chain of thought, does the choice of exemplars in the prompts matter? Here, we select the questions by hand and write clear long answers and use these as the exemplars in the prompt. This also did not have much impact on performance, so we only do this investigation for the unambiguous questions, and call it \u2019Static Short + Long\u2019, \u2019Static Long + Short\u2019, and \u2019Static Short Only\u2019 (for the original case but with the hand selected questions). We find that the prompt largely does not matter, achieving relatively similar results regardless of prompt, though many better prompts may exist. Results can be found in Table 6 for Natural Questions.\nNatural Questions"
        },
        {
            "heading": "C Text as Calibration",
            "text": "Can language models simply tell us when they don\u2019t know the answer? Here, we use a prompt where two of the questions are given the answer \u201cUnknown\u201d, instead of the given answer. The method follows the above prompting strategy, except two of the answers are randomly replaced with Unknown, and the unambiguous and ambiguous both use six exemplars. Results can be found in Table 8. Overall, on the unambiguous portion of Natural Questions, the model chooses to abstain a relatively large fraction of the time for only slightly\nAmbigQA\nhigher accuracy. The sampling repetition method is able to answer 86.1% of the questions at 80% accuracy, which is substantially better.\nThe ambiguous results are more interesting, where the model very rarely outputs unknown even though the questions are supposedly more challenging. The model is able to improve its accuracy somewhat by not answering a small number of questions; however, it cannot reliably abstain when it is wrong, thus leading to an overall low percentage of accuracy. While not computed above, the standard model\u2019s C@45 would be approximately 99.6%.\nMethod Acc Cov\nNatural Questions\nLong + Short 49.8 65.3 Short + Long 62.1 65.4 Short Only 60.4 73.0\nStatic L+S 61.9 53.7 Static S+L 63.9 53.7 Static SO 63.9 63.7\nAmbigQA"
        },
        {
            "heading": "D Scaling",
            "text": "See Table 5 for results. Note that calibration as defined by error is roughly constant, the proportion of the questions that can be answered with reasonable accuracy declines dramatically, approximating zero on the smallest model sizes."
        },
        {
            "heading": "E Sampling",
            "text": "See ?? for results."
        }
    ],
    "title": "Selectively Answering Ambiguous Questions",
    "year": 2023
}