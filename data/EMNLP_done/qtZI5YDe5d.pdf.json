{
    "abstractText": "Text is ubiquitous in our visual world, conveying crucial information, such as in documents, websites, and everyday photographs. In this work, we propose UReader, a first exploration of universal OCR-free visually-situated language understanding based on the Multimodal Large Language Model (MLLM). By leveraging the shallow text recognition ability of the MLLM, we only finetuned 1.2% parameters and the training cost is much lower than previous work following domain-specific pretraining and finetuning paradigms. Concretely, UReader is jointly finetuned on a wide range of Visually-situated Language Understanding tasks via a unified instruction format. To enhance the visual text and semantic understanding, we further apply two auxiliary tasks with the same format, namely text reading and key points generation tasks. We design a shape-adaptive cropping module before the encoder-decoder architecture of MLLM to leverage the frozen low-resolution vision encoder for processing high-resolution images. Without downstream finetuning, our \u2217 Equal contribution \u2020 Corresponding authors single model achieves state-of-the-art ocr-free performance in 8 out of 10 visually-situated language understanding tasks, across 5 domains: documents, tables, charts, natural images, and webpage screenshots. Codes and instruction-tuning datasets are released at https://github.com/LukeForeverYoung/UReader.",
    "authors": [
        {
            "affiliations": [],
            "name": "Jiabo Ye"
        },
        {
            "affiliations": [],
            "name": "Anwen Hu"
        },
        {
            "affiliations": [],
            "name": "Haiyang Xu"
        },
        {
            "affiliations": [],
            "name": "Qinghao Ye"
        },
        {
            "affiliations": [],
            "name": "Ming Yan"
        },
        {
            "affiliations": [],
            "name": "Guohai Xu"
        },
        {
            "affiliations": [],
            "name": "Chenliang Li"
        },
        {
            "affiliations": [],
            "name": "Junfeng Tian"
        },
        {
            "affiliations": [],
            "name": "Qi Qian"
        },
        {
            "affiliations": [],
            "name": "Ji Zhang"
        },
        {
            "affiliations": [],
            "name": "Qin Jin"
        },
        {
            "affiliations": [],
            "name": "Liang He"
        },
        {
            "affiliations": [],
            "name": "Xin Lin"
        },
        {
            "affiliations": [],
            "name": "Fei Huang"
        }
    ],
    "id": "SP:015444c5fc341f1e98a93ac13515667f51c6e6b7",
    "references": [
        {
            "authors": [
                "Emily M Bender",
                "Timnit Gebru",
                "Angelina"
            ],
            "title": "McMillanMajor, and Shmargaret Shmitchell",
            "venue": "In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency,",
            "year": 2021
        },
        {
            "authors": [
                "Ali Furkan Biten",
                "Rub\u00e8n Tito",
                "Andr\u00e9s Mafla",
                "Llu\u00eds G\u00f3mez i Bigorda",
                "Mar\u00e7al Rusi\u00f1ol",
                "C.V. Jawahar",
                "Ernest Valveny",
                "Dimosthenis Karatzas."
            ],
            "title": "Scene text visual question answering",
            "venue": "ICCV, pages 4290\u20134300. IEEE.",
            "year": 2019
        },
        {
            "authors": [
                "Szyndler",
                "Filip Gralinski."
            ],
            "title": "DUE: end-toend document understanding benchmark",
            "venue": "NeurIPS Datasets and Benchmarks.",
            "year": 2021
        },
        {
            "authors": [
                "Wenhu Chen",
                "Hongmin Wang",
                "Jianshu Chen",
                "Yunkai Zhang",
                "Hong Wang",
                "Shiyang Li",
                "Xiyou Zhou",
                "William Yang Wang."
            ],
            "title": "Tabfact : A large-scale dataset for table-based fact verification",
            "venue": "International Conference on Learning Representations",
            "year": 2020
        },
        {
            "authors": [
                "Xingyu Chen",
                "Zihan Zhao",
                "Lu Chen",
                "JiaBao Ji",
                "Danyang Zhang",
                "Ao Luo",
                "Yuxuan Xiong",
                "Kai Yu."
            ],
            "title": "Websrc: A dataset for web-based structural reading comprehension",
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language",
            "year": 2021
        },
        {
            "authors": [
                "Wenliang Dai",
                "Junnan Li",
                "Dongxu Li",
                "Anthony Meng Huat Tiong",
                "Junqi Zhao",
                "Weisheng Wang",
                "Boyang Li",
                "Pascale Fung",
                "Steven C.H. Hoi."
            ],
            "title": "Instructblip: Towards general-purpose visionlanguage models with instruction tuning",
            "venue": "CoRR,",
            "year": 2023
        },
        {
            "authors": [
                "Brian L. Davis",
                "Bryan S. Morse",
                "Brian L. Price",
                "Chris Tensmeyer",
                "Curtis Wigington",
                "Vlad I. Morariu."
            ],
            "title": "End-to-end document recognition and understanding with dessurt",
            "venue": "ECCV Workshops (4), volume 13804 of Lecture Notes in Computer Science,",
            "year": 2022
        },
        {
            "authors": [
                "Hao Feng",
                "Zijian Wang",
                "Ji Tang",
                "Jinghui Lu",
                "Wen gang Zhou",
                "Houqiang Li",
                "Can Huang."
            ],
            "title": "Unidoc: A universal large multimodal model for simultaneous text detection, recognition, spotting and understanding",
            "venue": "ArXiv, abs/2308.11592.",
            "year": 2023
        },
        {
            "authors": [
                "Anwen Hu",
                "Shizhe Chen",
                "Qin Jin."
            ],
            "title": "Questioncontrolled text-aware image captioning",
            "venue": "ACM Multimedia, pages 3097\u20133105. ACM.",
            "year": 2021
        },
        {
            "authors": [
                "Edward J Hu",
                "Yelong Shen",
                "Phillip Wallis",
                "Zeyuan Allen-Zhu",
                "Yuanzhi Li",
                "Shean Wang",
                "Lu Wang",
                "Weizhu Chen."
            ],
            "title": "LoRA: Low-rank adaptation of large language models",
            "venue": "International Conference on Learning Representations.",
            "year": 2022
        },
        {
            "authors": [
                "Furu Wei."
            ],
            "title": "Language is not all you need: Aligning perception with language models",
            "venue": "CoRR, abs/2302.14045.",
            "year": 2023
        },
        {
            "authors": [
                "Yupan Huang",
                "Tengchao Lv",
                "Lei Cui",
                "Yutong Lu",
                "Furu Wei."
            ],
            "title": "Layoutlmv3: Pre-training for document AI with unified text and image masking",
            "venue": "ACM Multimedia, pages 4083\u20134091. ACM.",
            "year": 2022
        },
        {
            "authors": [
                "Kushal Kafle",
                "Brian L. Price",
                "Scott Cohen",
                "Christopher Kanan."
            ],
            "title": "DVQA: understanding data visualizations via question answering",
            "venue": "CVPR, pages 5648\u20135656. Computer Vision Foundation / IEEE Computer Society.",
            "year": 2018
        },
        {
            "authors": [
                "Samira Ebrahimi Kahou",
                "Vincent Michalski",
                "Adam Atkinson",
                "\u00c1kos K\u00e1d\u00e1r",
                "Adam Trischler",
                "Yoshua Bengio."
            ],
            "title": "Figureqa: An annotated figure dataset for visual reasoning",
            "venue": "ICLR (Workshop). OpenReview.net.",
            "year": 2018
        },
        {
            "authors": [
                "Geewook Kim",
                "Teakgyu Hong",
                "Moonbin Yim",
                "JeongYeon Nam",
                "Jinyoung Park",
                "Jinyeong Yim",
                "Wonseok Hwang",
                "Sangdoo Yun",
                "Dongyoon Han",
                "Seunghyun Park."
            ],
            "title": "Ocr-free document understanding transformer",
            "venue": "ECCV (28), volume 13688 of",
            "year": 2022
        },
        {
            "authors": [
                "Gal Chechik",
                "David Cai",
                "Zheyun Feng",
                "Dhyanesh Narayanan",
                "Kevin Murphy."
            ],
            "title": "Openimages: A public dataset for large-scale multi-label and multiclass image classification",
            "venue": "Dataset available from https://storage.googleapis.com/openimages/web/index.html.",
            "year": 2017
        },
        {
            "authors": [
                "Kenton Lee",
                "Mandar Joshi",
                "Iulia Turc",
                "Hexiang Hu",
                "Fangyu Liu",
                "Julian Eisenschlos",
                "Urvashi Khandelwal",
                "Peter Shaw",
                "Ming-Wei Chang",
                "Kristina Toutanova."
            ],
            "title": "Pix2struct: Screenshot parsing as pretraining for visual language understanding",
            "venue": "CoRR,",
            "year": 2022
        },
        {
            "authors": [
                "Junnan Li",
                "Dongxu Li",
                "Silvio Savarese",
                "Steven C.H. Hoi."
            ],
            "title": "BLIP-2: bootstrapping language-image pre-training with frozen image encoders and large language models",
            "venue": "CoRR, abs/2301.12597.",
            "year": 2023
        },
        {
            "authors": [
                "Haotian Liu",
                "Chunyuan Li",
                "Qingyang Wu",
                "Yong Jae Lee."
            ],
            "title": "Visual instruction tuning",
            "venue": "CoRR, abs/2304.08485.",
            "year": 2023
        },
        {
            "authors": [
                "Yuliang Liu",
                "Zhang Li",
                "Hongliang Li",
                "Wenwen Yu",
                "Mingxin Huang",
                "Dezhi Peng",
                "Mingyu Liu",
                "Mingrui Chen",
                "Chunyuan Li",
                "Lianwen Jin"
            ],
            "title": "On the hidden mystery of ocr in large multimodal models. arXiv preprint arXiv:2305.07895",
            "year": 2023
        },
        {
            "authors": [
                "Ahmed Masry",
                "Do Xuan Long",
                "Jia Qing Tan",
                "Shafiq R. Joty",
                "Enamul Hoque."
            ],
            "title": "Chartqa: A benchmark for question answering about charts with visual and logical reasoning",
            "venue": "ACL (Findings), pages 2263\u20132279. Association for Computational Linguis-",
            "year": 2022
        },
        {
            "authors": [
                "Minesh Mathew",
                "Viraj Bagal",
                "Rub\u00e8n Tito",
                "Dimosthenis Karatzas",
                "Ernest Valveny",
                "C.V. Jawahar."
            ],
            "title": "Infographicvqa",
            "venue": "WACV, pages 2582\u20132591. IEEE.",
            "year": 2022
        },
        {
            "authors": [
                "Minesh Mathew",
                "Dimosthenis Karatzas",
                "C.V. Jawahar."
            ],
            "title": "Docvqa: A dataset for VQA on document images",
            "venue": "WACV, pages 2199\u20132208. IEEE.",
            "year": 2021
        },
        {
            "authors": [
                "Nitesh Methani",
                "Pritha Ganguly",
                "Mitesh M. Khapra",
                "Pratyush Kumar."
            ],
            "title": "Plotqa: Reasoning over scientific plots",
            "venue": "WACV, pages 1516\u20131525. IEEE.",
            "year": 2020
        },
        {
            "authors": [
                "Anand Mishra",
                "Shashank Shekhar",
                "Ajeet Kumar Singh",
                "Anirban Chakraborty."
            ],
            "title": "OCR-VQA: visual question answering by reading text in images",
            "venue": "ICDAR, pages 947\u2013952. IEEE.",
            "year": 2019
        },
        {
            "authors": [
                "Kishore Papineni",
                "Salim Roukos",
                "Todd Ward",
                "WeiJing Zhu."
            ],
            "title": "Bleu: a method for automatic evaluation of machine translation",
            "venue": "Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311\u2013318.",
            "year": 2002
        },
        {
            "authors": [
                "Panupong Pasupat",
                "Percy Liang."
            ],
            "title": "Compositional semantic parsing on semi-structured tables",
            "venue": "ACL (1), pages 1470\u20131480. The Association for Computer Linguistics.",
            "year": 2015
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark",
                "Gretchen Krueger",
                "Ilya Sutskever"
            ],
            "title": "Learning transferable visual models from natural language",
            "year": 2021
        },
        {
            "authors": [
                "Oleksii Sidorov",
                "Ronghang Hu",
                "Marcus Rohrbach",
                "Amanpreet Singh."
            ],
            "title": "Textcaps: A dataset for image captioning with reading comprehension",
            "venue": "ECCV (2), volume 12347 of Lecture Notes in Computer Science, pages 742\u2013758. Springer.",
            "year": 2020
        },
        {
            "authors": [
                "Amanpreet Singh",
                "Vivek Natarajan",
                "Meet Shah",
                "Yu Jiang",
                "Xinlei Chen",
                "Dhruv Batra",
                "Devi Parikh",
                "Marcus Rohrbach."
            ],
            "title": "Towards VQA models that can read",
            "venue": "CVPR, pages 8317\u20138326. Computer Vision Foundation / IEEE.",
            "year": 2019
        },
        {
            "authors": [
                "Tomasz Stanislawek",
                "Filip Gralinski",
                "Anna Wr\u00f3blewska",
                "Dawid Lipinski",
                "Agnieszka Kaliska",
                "Paulina Rosalska",
                "Bartosz Topolski",
                "Przemyslaw Biecek."
            ],
            "title": "Kleister: Key information extraction datasets involving long documents with complex layouts",
            "venue": "ICDAR",
            "year": 2021
        },
        {
            "authors": [
                "Ryota Tanaka",
                "Kyosuke Nishida",
                "Sen Yoshida."
            ],
            "title": "Visualmrc: Machine reading comprehension on document images",
            "venue": "AAAI, pages 13878\u201313888. AAAI Press.",
            "year": 2021
        },
        {
            "authors": [
                "Zineng Tang",
                "Ziyi Yang",
                "Guoxin Wang",
                "Yuwei Fang",
                "Yang Liu",
                "Chenguang Zhu",
                "Michael Zeng",
                "Cha Zhang",
                "Mohit Bansal."
            ],
            "title": "Unifying vision, text, and layout for universal document processing",
            "venue": "Proceedings of the IEEE/CVF Conference on Com-",
            "year": 2023
        },
        {
            "authors": [
                "Hugo Touvron",
                "Thibaut Lavril",
                "Gautier Izacard",
                "Xavier Martinet",
                "Marie-Anne Lachaux",
                "Timoth\u00e9e Lacroix",
                "Baptiste Rozi\u00e8re",
                "Naman Goyal",
                "Eric Hambro",
                "Faisal Azhar"
            ],
            "title": "Llama: Open and efficient foundation language models",
            "year": 2023
        },
        {
            "authors": [
                "Ramakrishna Vedantam",
                "C. Lawrence Zitnick",
                "Devi Parikh."
            ],
            "title": "Cider: Consensus-based image description evaluation",
            "venue": "CVPR, pages 4566\u20134575. IEEE Computer Society.",
            "year": 2015
        },
        {
            "authors": [
                "Vicuna."
            ],
            "title": "Vicuna: An open chatbot impressing gpt-4",
            "venue": "https://github.com/lm-sys/ FastChat.",
            "year": 2023
        },
        {
            "authors": [
                "Yang Xu",
                "Yiheng Xu",
                "Tengchao Lv",
                "Lei Cui",
                "Furu Wei",
                "Guoxin Wang",
                "Yijuan Lu",
                "Dinei A.F. Flor\u00eancio",
                "Cha Zhang",
                "Wanxiang Che",
                "Min Zhang",
                "Lidong Zhou"
            ],
            "title": "Layoutlmv2: Multi-modal pretraining for visually-rich document understanding",
            "year": 2021
        },
        {
            "authors": [
                "Zhengyuan Yang",
                "Yijuan Lu",
                "Jianfeng Wang",
                "Xi Yin",
                "Dinei Flor\u00eancio",
                "Lijuan Wang",
                "Cha Zhang",
                "Lei Zhang",
                "Jiebo Luo."
            ],
            "title": "TAP: text-aware pretraining for text-vqa and text-caption",
            "venue": "CVPR, pages 8751\u20138761. Computer Vision Foundation /",
            "year": 2021
        },
        {
            "authors": [
                "Qinghao Ye",
                "Haiyang Xu",
                "Guohai Xu",
                "Jiabo Ye",
                "Ming Yan",
                "Yiyang Zhou",
                "Junyang Wang",
                "Anwen Hu",
                "Pengcheng Shi",
                "Yaya Shi",
                "Chenliang Li",
                "Yuanhong Xu",
                "Hehong Chen",
                "Junfeng Tian",
                "Qian Qi",
                "Ji Zhang",
                "Fei Huang"
            ],
            "title": "mplug-owl: Modularization",
            "year": 2023
        },
        {
            "authors": [
                "Liang Zhang",
                "Anwen Hu",
                "Jing Zhang",
                "Shuo Hu",
                "Qin Jin."
            ],
            "title": "MPMQA: multimodal question answering on product manuals",
            "venue": "CoRR, abs/2304.09660.",
            "year": 2023
        },
        {
            "authors": [
                "Deyao Zhu",
                "Jun Chen",
                "Xiaoqian Shen",
                "Xiang Li",
                "Mohamed Elhoseiny"
            ],
            "title": "Minigpt-4: Enhancing vision-language understanding with advanced large language models",
            "year": 2023
        },
        {
            "authors": [
                "David Karp"
            ],
            "title": "Tumblr is launched",
            "year": 2010
        }
    ],
    "sections": [
        {
            "text": "single model achieves state-of-the-art ocr-free performance in 8 out of 10 visually-situated language understanding tasks, across 5 domains: documents, tables, charts, natural images, and webpage screenshots. Codes and instruction-tuning datasets are released at https://github.com/LukeForeverYoung/UReader."
        },
        {
            "heading": "1 Introduction",
            "text": "Leveraging strong Large Language Models as the language decoder, some recent works propose Multimodal Large Language Models (MLLMs) (Zhu et al., 2023; Liu et al., 2023a; Ye et al., 2023; Li et al., 2023) and achieve promising vision-andlanguage understanding performance. Surprisingly, without in-domain training, these MLLMs exhibit shallow zero-shot visual text recognition ability when fed a low-resolution image with salient text information (Ye et al., 2023; Liu et al., 2023b). However, due to the variety of image types and the wide range of image sizes, they are still far from universal visually-situated language understanding, such as extracting information from documents, reading texts from webpages, and visual question and answering on tables, as shown in Figure 1.\nExisting works for visually-situated language understanding can be categorized into two-stage (Xu et al., 2021; Huang et al., 2022; Yang et al., 2021) and end-to-end (Davis et al., 2022; Kim et al., 2022; Lee et al., 2022) methods according to whether relying on an off-the-shelf OCR model or API. These works all follow a domain-specific pretraining and finetuning paradigm, thus leading to high training costs, e.g. end-to-end model Donut (Kim et al., 2022) costs more than 192 A100 days.\nInspired by the shallow text recognition ability of existing MLLMs, in this work, we propose UReader for universal OCR-free visually-situated language understanding, which leverages the multimodal Large Language Model via low-cost instruction tuning (Dai et al., 2023). Different from previous works, we forgo pretraining tasks by leveraging the existing MLLM and directly finetune MLLM by taking full advantage of various Visually-situated Language Understanding datasets. To make the most of the strong language understanding ability of MLLM, we convert all tasks into the visionlanguage instruction tuning format. Besides, to enhance text recognition and semantic understanding ability across diverse domains, we design auxiliary text reading and key points generation tasks in the same instruction format. To utilize the lowresolution encoder of MLLM for processing highresolution images and avoid blurry and distortion problems due to resizing, we propose a shapeadaptive cropping module to cut a high-resolution image into multiple local images. Each image is firstly independently encoded with the frozen visual encoder and a trainable visual abstractor and then concatenated to feed into the language decoder. Moreover, we add learnable crop position encoding to help the model correlate local images and add a resized global image to alleviate salient information loss due to cropping.\nOur contributions in this work are four-fold: \u2022 We first propose instruction tuning with Multi-\nmodal Large Language Models for OCR-free Visually-situated Language Understanding.\n\u2022 We build an instruction-tuning dataset covering 5 domains of visually-situated language understanding: document, table, chart, natural image, and webpage screenshot.\n\u2022 We design a shape-adaptive cropping module to utilize the frozen low-resolution vision encoder for processing high-resolution images.\n\u2022 UReader achieves state-of-the-art OCR-free\nperformance in 8 out of 10 tasks, across 5 domains."
        },
        {
            "heading": "2 Related Work",
            "text": "Visually-situated Language Understanding aims to comprehend images containing rich text information. The image types are quite diverse, covering document (Mathew et al., 2021, 2022; Stanislawek et al., 2021; Svetlichnaya, 2020; Zhang et al., 2023), table (Pasupat and Liang, 2015; Chen et al., 2020), chart (Masry et al., 2022; Methani et al., 2020; Kafle et al., 2018; Kahou et al., 2018), natural image (Singh et al., 2019; Mishra et al., 2019; Biten et al., 2019; Hu et al., 2021), webpage screenshot (Tanaka et al., 2021; Chen et al., 2021), etc. Tasks of Visually-situated Language Understanding range from visual question answering, image captioning, information extraction to natural language inference.\nAccording to whether using off-the-shelf OCR models or APIs to recognize texts from images, existing work can be divided into two-stage models (Xu et al., 2021; Huang et al., 2022; Tang et al., 2023; Yang et al., 2021) and end-to-end models (Kim et al., 2022; Davis et al., 2022; Lee et al., 2022). Two-stage work always designs pretrianing tasks to learn cross-modality alignment between visual inputs and text inputs. For example, for document understanding, UDOP (Tang et al., 2023) design a Joint Text-Layout Reconstruction task to recover masked texts and layout information given the visual inputs and retained text inputs. LayoutLMv3 (Huang et al., 2022) applies a Masked Image Modeling task to recover masked image tokens with the context of their surrounding text and image tokens. Without the help of an off-theshelf OCR model, end-to-end models need to learn text recognition with a high-resolution image encoder during the pretraining stage. For example, Pix2Struct (Lee et al., 2022) proposes a Screenshot Parsing pretraining task, where the model needs to generate the complete HTML DOM tree with only a masked webpage screenshot as the input. Donut (Kim et al., 2022) designs a pretraining task to generate all texts in the document image. These work all follow a domain-specific pretraining and finetuning paradigm and therefore ask for high training costs, e.g. Donut is trained for more than 192 A100 days. In this work, by leveraging the shallow text recognition ability of Multimodal Large Language Models, we propose to directly perform\ninstruction tuning across various types of images and greatly reduce the training cost for universal visually-situated Language Understanding. Multimodal Large Language Model is developed to empower the Large Language Model with multimodality understanding ability, especially for vision information. These work (Huang et al., 2023; Zhu et al., 2023; Liu et al., 2023a; Ye et al., 2023; Li et al., 2023; Dai et al., 2023) mainly connect a pre-trained vision encoder (usually CLIP VIT-L/14 (Radford et al., 2021)) with a strong large language model, such as LLaMA (Touvron et al., 2023). These MLLMs show some emergent abilities, including shallow zero-shot text recognition ability (Liu et al., 2023b). However, they are still far from universal visually-situated language understanding. Firstly, due to the pretraining data for the vision encoder being mostly natural images, MLLMs show barely acceptable text understanding performance on natural images but bad performance on other types, such as document (Liu et al., 2023b). Secondly, most images for visuall-situated language understanding are high-resolution. Rescaling them to low resolution to adapt to the vision encoder can cause the texts blurry and distorted. In this work, we propose to fully leverage the shallow text recognition ability of MLLMs and perform instruction tuning to enhance its universal understanding ability across 5 domains. Besides, we design a shape-adaptive cropping module to alleviate the text blur and distortion problem."
        },
        {
            "heading": "3 UReader",
            "text": "The primary goal of UReader is to efficiently utilize existing MLLMs for Visually-situated Language Understanding tasks. In this work, we utilize but are not limited to, the mPLUG-Owl (Ye et al., 2023) as our basic MLLM. Figure 2 presents an overall architecture of UReader. The input image is firstly pre-processed by a shape-adaptive cropping module (in Section 3.1). The resulting sub-images are then simultaneously passed through the visual encoder and visual abstractor. To enable the large language model to correlate multiple cropped subimages, we apply a crop position encoding module to introduce spatial information across sub-images. (in Section 3.2)."
        },
        {
            "heading": "3.1 Shape-Adaptive Cropping Module",
            "text": "Images with texts have various aspect ratios and a great range of resolutions. Simply resizing the im-\nage to Hv,Wv (raw resolution of the MLLM) can result in text being blurred, distorted, and unrecognizable. Thus we propose a shape-adaptive cropping module. Specifically, as shown in Figure 3, we pre-define grids {g = (nh \u00d7 nw)|nh \u00b7 nw \u2264 Nc, nh \u2208 N, nw \u2208 N} with various shapes, where nh and nw denote the number of rows and columns of the grid g and Nc denotes the maximum number of the cells (sub-images). To select a suitable grid for an image I with shape H\u00d7W , two rules should be followed: (1) The grid should preserve the resolution of the image as much as possible, and (2) the grid should fit the aspect ratio of the input image. To measure the resolution coherence and shape similarity between the image and each grid, we calculate the resolution-related and resolution-agnostic insection over union Srr and Sra as follows:\nSrr(I, g) = IoU ((H,W ), (nhHv, nwWv))\nSra(I, g) = IoU\n( ( nwH\nW ,nw), (nh, nw)\n) (1)\nwhere IoU denotes the insection over the union between two rectangles centered and aligned with each other. The matched grid is selected by maximizing the matching score:\ng\u2217 = argmax g Sra(I, g) + Srr(I, g) (2)\nwhere g\u2217 is the selected grid. Then, we resize the input image to (nhHv, nwWv) and crop it to nh \u00b7 nw local images. To maintain the global structure information of the image, we also resize the input\nimage to (Hv,Wv) as a global image. All images are then passed on to the visual encoder and visual abstractor in parallel.\nThe visual encoder extracts visual feature V \u2208 RN\u00d7(H\u2032\u00b7W \u2032)\u00d7dv from the input images I \u2208 RN\u00d7H\u00d7W\u00d73, where N = (nh \u00b7 nw) + 1, H \u2032 \u00b7W \u2032 and dv denote the number and dimension of the extracted visual features, respectively. The visual abstractor further summarizes visual information and obtains higher semantic visual representations V l \u2208 RN\u00d7Nq\u00d7dl in language feature space by several learnable queries, where dl denotes the dimension of language feature space and Nq denotes the number of learnable queries."
        },
        {
            "heading": "3.2 Cropped Images Modeling with LLM",
            "text": "MLLMs are mostly trained with a single image as the input. Due to the cropping module, we need to input visual features from multiple images into the language model. The 1-dimensional position embeddings of LLM can not reflect the spatial position of each sub-image, which is critical to correlate local images. Therefore, we incorporate a 2-dimensional crop position encoding to help the language model to understand the spatial relationship between cropped images. Specifically, we assign a location index (i, j) for each cell of the selected grid and obtain their row embedding and column embedding by two auxiliary embedding layers as follows:\nerowi,j = Embeddingrow(i)\necolumni,j = Embeddingcolumn(j)\nei,j = e row i,j + e column i,j\n(3)\nwhere ei,j \u2208 RDl denotes the crop position embedding of the cell (ci, cj). We add the embedding to the visual feature of each cell in the language space via broadcasting along the dimension of learnable queries: V\u0304 li,j = V l i,j + ei,j . We then reshape the visual features into V\u0304l \u2208 R(N \u00b7Nq)\u00d7dl . The resulting spatial-aware visual features and word embeddings of the input sentences are concatenated at sequence dimension and sent to the large language model.\nIn order to enhance the language model\u2019s ability to effectively model multiple images while keeping low training costs, we freeze the origin language model and adopt the low-rank adaptation approach (LoRA) (Hu et al., 2022)."
        },
        {
            "heading": "4 Instruction Tuning",
            "text": "For developing a universal visually-situated language understanding model that could process various types of images and perform different comprehension tasks, we conduct low-cost instruction tuning with a Multimodal Large Language Model. Without introducing any large-scale pretraining datasets, we directly ensemble multiple downstream datasets and perform joint training. Different downstream tasks are all reorganized to the unified instruction format (Dai et al., 2023). Besides, we design auxiliary text reading and key points generation tasks to enhance text recognition and semantic understanding ability."
        },
        {
            "heading": "4.1 Tuning Tasks",
            "text": "Unified downstream task. Downstream tasks of Visuall-situated Language Understanding cover Visual Question Answering, Information Extraction, Natural Language Inference, and Image Captioning. For developing a universal model, we reorganize all tasks into the instruction tuning format (Dai et al., 2023). Concretely, for the Visual Question Answering task, the question is directly used as the instruction: \"Human: {question} AI: {answer}\". For the Information Extraction task, each category and value pair is expressed with a prompt as \"Human: What is the value for the {category}? AI: {value}\". If some categories don\u2019t exist in the image, the value is \u2018None\u2019. In the raw annotation of the Natural Language Inference task, \u20181\u2019 means \u2018Entailed\u2019 and \u20180\u2019 means \u2018Refuted\u2019. We reorganize the NLI task by constructing the instruction \"Human: {statement}, Yes or No? AI: {answer}\", where \u2018Yes\u2019 means \u2018Entailed\u2019. For the Image captioning task, we refer to 11 prompts from LLaVa\n(Liu et al., 2023a) to instruct the model to briefly describe the image and randomly choose 1 prompt for each caption, such as \"Human: Provide a brief description of the given image. AI: {caption}\". Text Reading task. Text Recognition is a basic ability for OCR-free Visuall-situated Language Understanding. Therefore, we apply an auxiliary Text Reading task to strengthen text recognition ability across different domains. With the text and position information in the image, we organize the texts in the common reading order: from top to down, from left to right. Directly utilizing all texts as targets (Kim et al., 2022) will result in the model focusing on generating the starting texts and neglecting others to reduce the loss. Instead, we randomly choose a split position p from {0, L6 , 2L 6 , ..., 5L 6 }, where L is the text sequence length. The left part is used as the input and the right one is the target. p = 0 means to generate all texts while other cases ask the model to continue reading following the input texts. Such a design could enforce the model to read different parts of texts with the context. Starting texts always convey key information about the image, such as the chart title. Therefore, we apply a bigger sample rate (0.5) for the \u20180\u2019 position and 0.1 for other positions. To distinguish reading from the beginning and continuing reading, we design two groups of prompts and randomly choose 1 prompt for each sample. For example, an instruction of reading from the beginning can be \"Human: Recognize text in the image. AI: {all texts}\" and an instruction of continuing reading can be \"Human: The words on this picture are {left texts}. Continue reading the text. AI: {right texts}\". Key Points Generation task. Large Language Models learn strong understanding ability from the tough language modeling task. Therefore, for stronger vision-and-language semantic comprehension ability, we propose to design an auxiliary Key Points Generation task, which requires the model to give some key points about the image. To support this task, we collect QA pairs of each image and convert them to declarative sentences with Vicuna (Vicuna, 2023). These declarative sentences are finally regarded as key points about the image. We also build a set of templates to instruct this task, such as \"Human: Identify some key points in this picture. AI: {key points}\".\nAll templates for Text Reading and Key Points Generation tasks can be found in Appendix D."
        },
        {
            "heading": "4.2 Instruction Data Resources",
            "text": "Document. DocVQA (Mathew et al., 2021) comprises 50k question and answer(QA) paris on 12k document images from UCSF Industry Documents Library. InfographicsVQA (InfoVQA) (Mathew et al., 2022) collects 5k diverse infographics from the internet and annotates 30k QA pairs. DeepForm\u22171 (Svetlichnaya, 2020) and Kleister Charity (KLC) (Stanislawek et al., 2021) are two Information Extraction datasets. DeepForm\u2217 contains 1.1k documents related to election spending. 2.7k documents of KLC come from published reports of charity organizations. Table. WikiTableQuestions (WTQ\u2217) (Pasupat and Liang, 2015) comprises 2.1k table images from Wikipedia and is annotated with 23k question and answer pairs demanding comparison and arithmetic operations. TabFact\u2217 (Chen et al., 2020) is a Natural Language Inference dataset, which contains 112k \u2018entailed\u2019 or \u2018refuted\u2019 statements about 16k Wikipedia tables. Chart. ChartQA (Masry et al., 2022) collects various topics and types of charts from four sources: Statista (statista.com), The Pew research (pewresearch.org), OWID (ourworldindata.org) and OECD (oecd.org). It totally contains 21k chart images and 32k QA pairs. Natural Images. TextVQA (Singh et al., 2019) filters 28k natural images with texts from Open Images V3 (Krasin et al., 2017) and annotates 45k QA pairs. To support image captioning with reading comprehension, TextCaps (Sidorov et al., 2020) further collects 145k captions based on TextVQA. WebPage Screenshot. VisualMRC (Tanaka et al., 2021) collects 5k full screenshots of webpages from 35 websites. There are 30k annotated QA pairs where answers are expressed in fluent sentences (avg. 9.53 words) and much longer than the ones of QA datasets mentioned above."
        },
        {
            "heading": "5 Experiments",
            "text": ""
        },
        {
            "heading": "5.1 Implementation Details",
            "text": "We conduct experiments on a recently proposed MLLM named mPLUG-Owl (Ye et al., 2023) without modifying its hyperparameters. The number of learnable queries of visual abstractor is 65. The dimension of hidden states dv and dl are 1024. For the shape-adaptive cropping module, we set the\n1Superscript \u2217 means the reformulated or modified version in DUE-benchmark (Borchmann et al., 2021)\nmaximum number of cells Nc to 20 by default. During instruction tuning, the maximum sequence length is limited to 2048, and Hv,Wv are set to 224 to match the pretrained resolution of the visual encoder. For LoRA, we set the rank r = 8. The learning rate schedule uses a linear warmup of 36 steps to 1e\u22124, followed by cosine decay to 0. The batch size is set to 256. For better convergence of each dataset, DocVQA is up-sampled 3 times, InfoVQA, WTQ, DeepForm, and KLC are up-sampled 2 times. The instruction tuning process takes 16 A100 days for 20k training steps (10 epochs)."
        },
        {
            "heading": "5.2 Evaluation",
            "text": "We use official training splits as tuning data and evaluate models on test splits. Following previous works (Borchmann et al., 2021; Lee et al., 2022), DocVQA and InfoVQA are evaluated by ANLS (Biten et al., 2019), DeepForm and KLC are evaluated by F1 score. WTQ, TabFact and TextVQA are evaluated by accuracy. ChartQA is evaluated with the relaxed accuracy (Methani et al., 2020). TextCaps and VisualMRC are measured by CIDEr (Vedantam et al., 2015). Evaluation of TextVQA and TextCaps are performed with the official challenge website."
        },
        {
            "heading": "5.3 Main Results",
            "text": "We first compare UReader with state-of-the-art ocrfree models on 10 datasets. For fair and consistent comparison across all datasets, we finetune the strong and accessible baseline Dount on unreported datasets. As shown in Table 1, UReader achieves state-of-the-art performance in 8 tasks across 5 domains, covering Visual Question Answering, Information Extraction, Natural Language Inference and Image Captioning tasks. With much fewer trainable parameters (86M vs 1.3B) and without a specific finetuning stage, UReader outperforms\nthe strong pretriaining model Pix2Structlarge in InfoVQA, ChartQA, and TextCaps. Considering that Pix2Structlarge is trained more than 170k steps with a batch size of 1024 on 128 TPUs, this validates that with the help of open-domain Multimodal Large Language Models, learning costs for universal visually-situated language understanding can be greatly reduced. More detailed analysis can be found in Appendix B."
        },
        {
            "heading": "5.4 Ablation Study",
            "text": "We perform comprehensive ablation experiments to validate the contribution of two auxiliary tasks, trainable architectures, cross-domain joint training and the design of shape-adaptive cropping module.\nAuxiliary Tasks. As shown in Table 2, dropping the Key Points Generation task (r10 vs r2) causes a performance decrease on all domains of datasets, demonstrating that this task helps the model better understand the vision-and-language semantic. Further removing the Text Reading task (r2 vs r1) causes more significant performance degradation, which validates the importance of enhancing text recognition ability across different domains.\nTrainable Architectures. Both the visual abstractor and LoRA in LLM are finetuned in UReader (r10). Freezing either the visual abstractor (r3) or LoRA (r4) causes performance decrease, which demonstrates that both the vision and language parts should be finetuned for adjusting to Visually-situated Language Understanding.\nCross-domain Joint Training. After removing 4 document datasets from the training data, UReader achieves worse performance (r10 vs r5) on the table, natural image, and webpage domains, validating that images of different domains share some common characteristics and cross-domain joint training improves the universal performance. Besides, although trained without document data,\nTable 2: Ablation study about auxiliary training tasks, trainable model architectures, cross-domain joint training and shape-adaptive cropping. \u2018KPG\u2019 and \u2018TR\u2019 refer to Key Points Generation and Text Reading tasks, respectively. \u2018Abs\u2019 refers to the visual abstractor. \u2018Doc Data\u2019 means using 4 document datasets as training data or not. \u2018Global\u2019 means using a resized global image as input. \u2018Crops\u2019 refers to Nc, the maximum number of local images after cropping. \u2018CropPos\u2019 refers to the crop position embedding.\nTasks Trainable Doc Shape-adaptive Cropping DocVQA WTQ ChartQA TextVQA VisualKPG TR Abs LoRA Data Global CropPos Crops MRC\nr1 \u2713 \u2713 \u2713 \u2713 \u2713 20 56.7 22.9 56.7 54.3 205.0 r2 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 20 64.3 28.1 58.6 56.0 213.5\nr3 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 20 52.4 20.5 43.5 54.9 194.9 r4 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 20 59.5 23.5 58.5 53.3 177.0\nr5 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 20 46.2 27.4 59.8 54.0 185.6\nr6 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 - 22.0 13.4 24.2 34.4 157.4 r7 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 9 58.0 24.7 58.9 55.5 215.3 r8 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 20 64.1 27.6 60.7 56.5 210.7 r9 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 20 62.8 26.7 58.7 55.4 181.1\nr10 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 20 65.4 29.4 59.3 57.6 221.7\nFigure 4: Visualization of the frequency of selected grid with shape-adaptive cropping module. The cell at row i and column j denotes the selected frequency of grid (nh = i, nw = j). Deeper colors represent higher selection frequencies.\nour model achieves a 46.2 score on the DocVQA dataset, showing the potential out-of-domain understanding ability of our training paradigm.\nShape-adaptive Cropping. The r6 in Table 2 represents directly tuning the mPLUG-Owl without any model revisions. With the shape-adaptive cropping, UReader achieves significantly better performance (r7 vs r6), showing that our cropping module is indispensable to leverage pretrained lowresolution vision encoder for universal visuallysituated language understanding. Besides, increasing the cropping numbers (r8 vs r7) improves the model\u2019s performance. Due to the resolution of each local image being constant (224x224), more crops mean higher overall resolution and therefore achieves better performance. Furthermore, adding a resized global image bring a slight improvement in most datasets (r10 vs r8), validating that a complete image could alleviate possible information loss due to image cropping. Finally, dropping crop position encoding also hurts the model\u2019s perfor-\nmance (r10 vs r9), proving the effectiveness of crop position encoding for correlating local images.\nFor alleviating the distortion problem due to resizing, we propose to crop images according to their raw aspect ratio. Figure 4 shows the frequency distribution of grids selected by our shape-adaptive cropping module on DocVQA, VisualMRC and WikiTableQuestions (the distribution on more datasets can be found in the Appendix A). For aesthetic purposes, we present the distribution with Nc = 9. Apparently, different domains of images have different shape distributions. For most document images in DocVQA, their height is greater than the width, while table images are the opposite. As webpages are scrollable, their screenshots are always in the form of a long rectangular shape. With the shape-adaptive cropping design, our model can easily adapt to various image shapes without domain-specific fine-tuning.\nText distortion may pose little influence on visual question answering because they are always about partial text information. But it is harmful for reading texts in the image because every text matters. For quantitative analysis of the influence of shape-adaptive design, we directly evaluate the performance of reading all texts. We choose the Bleu (Papineni et al., 2002) as the metric because it directly measures the n-gram overlap between the ground-truth and predicted text sequence. The evaluation set is built by combining 100 randomlyselected test images from each dataset. As shown in Table 3, compared with cropping all images with a fixed grid, UReader could better recognize texts\nin the image due to our shape-adaptive design that alleviates the text distortion problem."
        },
        {
            "heading": "5.5 Qualitative Results",
            "text": "Figure 5 show some qualitative results produced by our UReader on different types of images. UReader could not only extract information from the document (case a), but also understand different instructions and provide corresponding answers by attending to different regions (case b). Table understanding always involves layout comprehension and statistics. As shown in case c, given a table image, UReader could well relate different columns to answer the \u2018first movie\u2019 and perform simple statistics about the \u2018total number\u2019. As for images with multiple paragraphs of text, e.g. webpage screenshot in case e, UReader could also locate the relevant paragraph, understand the texts and answer the question accurately. Case d shows the text reading performance. With the help of the Text Reading task, UReader is able to read texts from top left to bottom right. But, due to the language decoding\nmanner, when given an image with rich texts, such as a page of a book, the model often reads the beginning texts and then continues writing without watching the image. More qualitative results can be found in Appendix C. Finally, as shown in case f, UReader is able to list some key points about the chart by combining the title and line information. Listing key points in this work is just a superficial attempt at open-ended generation, and its performance is far from promising, e.g., UReader makes a mistake about the lowest line. More effort is needed towards a comprehensive understanding of images with rich text."
        },
        {
            "heading": "6 Conclusion",
            "text": "We first propose to leverage existing Multimodal Large Language Models for universal ocr-free visually-situated language understanding through low-cost instruction tuning. All downstream tasks are reorganized into a unified instruction-tuning format. Besides, we design the Text Reading task and Key Points Generation task to enhance text recognition and vision-and-language semantic comprehension abilities. To utilize the pre-trained vision encoder for processing high-resolution images, we design a shape-adaptive cropping module, which cuts the image into multiple local images considering its raw aspect ratio and resolution. UReader achieve state-of-the-art ocr-free performance in 8 out of 10 datasets, ranging from documents, tables, charts, and natural images to webpage screenshots.\nLimitations\nOur experiments validate that UReader is able to correlate local images after cropping a highresolution image. However, UReader struggles to understand multi-page documents (e.g. books and papers) due to lacking ability to correlate different pages and the limited sequence length of the decoder. Besides, UReader feeds an equal number of features for each local image into the language decoder. But, not all local images contain rich vision or text information. In the future, we will explore a more efficient way to encode different crops. Furthermore, the open-ended generation about Visually-situated Language understanding is far from well studied. We try developing key points generation ability in this work but more difficult generation tasks are not currently considered, such as giving the chain-of-the-thought of the answer. How to simulate such abilities through instruction tuning is a topic worth studying. Finally, the Text Reading task helps the model recognize texts, but the text reading performance with the LLM as the decoder is far from satisfactory due to the hallucination problem. Instructing the LLM to read texts strictly according to images is a challenging topic.\nEthics Statement\nOur UReader relies on multi-modal large language models that are trained on large-scale image and text data from the web and therefore may be subject to issues such as toxic language and bias (Bender et al., 2021). However, our model is further finetuned on publicly available datasets and is used specifically in the domain of visually-situated language understanding, where these issues have minimal impact."
        },
        {
            "heading": "A Grid Distribution on Downstream Datasets",
            "text": "We visualize the frequency distribution of grids selected by our shape-adaptive cropping module on all ten datasets in Figure 6. The wide variety of image shapes in downstream tasks highlights the crucial role of the shape-adaptive cropping module."
        },
        {
            "heading": "B Detailed Analysis on Performance",
            "text": "B.1 Underperforms Ocr-Free Baselines on DocVQA and DeepForm\nIt can be seen that UReaderunderperforms ocr-free baselines on DocVQA and DeepForm. There are two main factors: (1) Donut performs the pretraining on large-scale document dataset IIT-CDIP (11M document images), which is the same domain as DocVQA and DeepForm. But UReader does no have a pretraining process and is just instruction finetuned on ensembled datasets (less than 0.5M assorted images). Training with more document images brings better performance. (2) The pretraining task of Pix2struct is to predict the HTML dom tree of a masked web screenshot, which requires the model to fully understand the layout information of the image. But UReader is trained to read texts from top to down, from left to right, which requires a weaker layout understanding ability. The pretraining on layout understanding also leads to improved performance on DocVQA.\nThe conclusion can also be substantiated by the observations on the other two datasets (i.e., InfoVQA and KLC) included in the document domain as previous work (Tang et al., 2023). For the InfoVQA dataset, the image is poster style and the layout is not as important as DocVQA and DeepForm but the relationship between text and vision objects matters more, like natural image and chart image. As for the KLC dataset, ocr-free models are only fed with the first page (always the cover of a report) , where the layout is much simpler than DocVQA and DeepForm. Therefore, UReadercan outperform baselines on these two document datasets.\nIn summary, compared with ocr-free model Donut and Pix2Struct, due to the pretrianing of MLMM on open-domain datasets, UReaderis better at understanding cross-modality relationships in the image but weaker at comprehending text layout information without large-scale document pretraining and specific layout understanding tasks.\nB.2 Compared with Pipeline Methods\nWe list the performance of state-of-the-art pipeline models in Table 4. We can summarize from the results that there are two distinct aspects. Firstly, our model achieves comparable or slightly worse results compared to the pipeline methods on TextVQA, ChartQA, InfoVQA, TextCaps and TabFact. Secondly, there is a obvious gap between our model and pipeline methods on DocVQA, DeepForm, KLC, WTQ and VisualMRC.\nFor the first aspect, there are two reasons for the similarity performance: (1) Modeling the diverse relationship between visual objects and text presents challenges for both pipeline-based methods and OCR-free methods. TextVQA, TextCaps and InfoVQA requires the relation understanding between text and visual objects (i.e. logos, icons and common objects). ChartQA asks for trend comprehension of lines. Understanding such complex cross-modality relation is challenging for both ocrfree and pipeline methods. (2) The simplicity of task formats can reduces performance gaps. Tabfact is a simply binary classification task resulting the small performance gap.\nFor this second aspect, the main performance gap appears in three categories of datasets: document, table, and webpage screenshot. The reasons are two folds: (1) The gap in terms of text recognition and layout extraction. In document, table and website, text is the dominant information source and the layout(e.g. row and column layout in table) is relatively uniformer than the chart and natural images. Therefore, with pre-extracted texts and layout information, it is more easy to understand the image. But for OCR-Free models, such as our UReader and Donut, it\u2019s still challenging to fully recognize all texts. (2) The gap in terms of modeling capacity on multi-page document input. for multiple-page document datasets KLC (98% > 4 pages) and DeepForm (75% > 1 pages), OCR-Free models only input the first page and lose much information.\nB.3 Zero-shot Performance\nWe test the zero-shot performance of UReader on unseen dataset OCR-VQA. With the same evaluation metrics, UReader outperforms mPLUG-Owl (41.1 vs 28.6) and a recent work UniDoc (Feng et al., 2023) (41.1 vs 34.5) with the training of layout prediction. The results show that the zero-shot performance of our method on unseen domains is\nacceptable."
        },
        {
            "heading": "C More Qualitative Results",
            "text": "C.1 Downstream Results\nMore qualitative results on natural images, charts, tables, documents and webpage screenshots are shown in Figure 7-11.\nFigure 11 show a sample of Text Reading and Visual Question Answering about a webpage screenshot from VisualMRC. As mentioned in Section 5.5, when given an instruction about reading all texts in the image, UReader can read the beginning texts but sometimes is easy to continue to generate vision-irrelevant texts. With appropriate instructions, UReader could indeed recognize texts in other regions, such as \u2018exercise increases cellular recycling\u2019. Therefore, the hallucination problem during text reading is not because UReader cannot recognize texts, but the generating manner of LLM decoder. When beginning texts are read from the image, the decoder may generate the following texts according to the closer text context rather than the image.\nC.2 Open-domain Results\nWe present open-domain examples in Figure 12. We use randomly collected images and freely ask questions to the model based on the content of\nthese images. The original mPLUG-Owl is used for comparison.\nIn Figure 12 (a), UReader is able to accurately recognize and answer questions about the small text in natural images (\"Name of passenger\" and \"MORRIS/KARLA\"). In contrast, mPLUG-Owl does not respond with the name in the first round and gives an incorrect answer even with a prompt in the second round.\nIn Figure 12 (b), we raise a query consisting of two cascaded questions, which requires the model to simultaneously understand the spatial position of the non-textual objects referred to by the query and locate the corresponding fields. It can be seen that the UReader completes this task well, while mPLUG-Owl answers incorrectly in both object recognition and price extraction.\nIn Figure 12 (c), we conduct multi-turn conversions with UReader on a screenshot. The questions included references to the history of the conversation. Some questions also require a certain amount of common sense. For example, the time of account creation is equivalent to the time of joining Twitter, and the inactive state of the Follow button indicates that the user has not followed Iron Musk. UReader answers these questions well. mPLUG-Owl can correctly recognize the Iron Mush, but is it prone to generating content that is unrelated to the image,\nleading to some erroneous statements. In Figure 12 (d), we ask the UReader about the price and its components based on an image consisting of multiple forms. Although UReader wrongly includes the header in the answer and does not list\nthe prices for each component, we notice that it proactively filters out the components with a price of $0, making the answer more consistent with the user\u2019s intention. It indicates that UReader can find the form related to the question and compre-\nHuman: Identify some key points in this picture. UReader: The document is dated March 22, 1991. The document is from the United States.\nHuman: What is the name of the Dealer ? UReader: A. C. Monk\nHuman: How many grass/straw pieces of matter is found in the core samples ? UReader: 2\nHuman: How many lint/string pieces of matter is found in the core samples ? UReader: 2\n(a)\n(b)\nHuman: Parse text from imagery. UReader: A BRIEF HISTORY OF SOCIAL MEDIA PLATFORMS 1978 The first public Bulletin Board System went online. 1979 UseNet created by Duke University students Tom Truscott and Jim Ellis. 2. 1982 Six Degrees, the first social media website, is launched. 2002 LinkedIn is created by Reid Hoffman. 2003 MySpace is launched by several eUniverse employees. 2004 f Mark Zuckerberg officially launches Twitter. 2006 Twitter is launched by Jack Dorsey used by employees at Oden. 2007 2008 The photo-sharing app Instagram is launched. Tumblr is launched by David Karp. 2010 The blogging site Tumblr is launched by David Karp. 9. CREATED BY: Carly Racklin / Ghost Tweeting ghostweeting\nHuman: Which social media site was launched in 2010? UReader: Instagram.\nFigure 10: Text Reading, Key Points Generation and Visual Question Answering performance of UReader on documents from DocVQA and InfoVQA. Correct and wrong answers are colored green and red, respectively.\nhensively understand the meaning of each field in the form. In contrast, mPLUG-Owl generates re-\nsponses that are full of illusions due to the loss of textual information in the image.\nThese results reveal that UReader maintains some interactive ability of MLMM in the open domain and shows stronger visually-situated language understanding ability.\nD Instruction Templates\nThe instruction templates of the auxiliary Text Reading and Key Points Generation tasks are shown in Table 5."
        }
    ],
    "title": "UReader: Universal OCR-free Visually-situated Language Understanding with Multimodal Large Language Model",
    "year": 2023
}