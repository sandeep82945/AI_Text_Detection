{
    "abstractText": "Embodied task completion is a challenge where an agent in a simulated environment must predict environment actions to complete tasks based on natural language instructions and egocentric visual observations. We propose a variant of this problem where the agent predicts actions at a higher level of abstraction called a plan, which helps make agent actions more interpretable and can be obtained from the appropriate prompting of large language models. We show that multimodal transformer models can outperform language-only models for this problem but fall significantly short of oracle plans. Since collecting human-human dialogues for embodied environments is expensive and time-consuming, we propose a method to synthetically generate such dialogues, which we then use as training data for plan prediction. We demonstrate that multimodal transformer models can attain strong zero-shot performance from our synthetic data, outperforming language-only models trained on humanhuman data.",
    "authors": [
        {
            "affiliations": [],
            "name": "Aishwarya Padmakumar"
        },
        {
            "affiliations": [],
            "name": "Mert \u0130nan"
        },
        {
            "affiliations": [],
            "name": "Spandana Gella"
        },
        {
            "affiliations": [],
            "name": "Patrick L. Lange"
        },
        {
            "affiliations": [],
            "name": "Dilek Hakkani-Tur"
        }
    ],
    "id": "SP:9914e83bbb8c92ceb2e21fb028322a1d5e3f3b05",
    "references": [
        {
            "authors": [
                "Michael Ahn",
                "Anthony Brohan",
                "Noah Brown",
                "Yevgen Chebotar",
                "Omar Cortes",
                "Byron David",
                "Chelsea Finn",
                "Keerthana Gopalakrishnan",
                "Karol Hausman",
                "Alex Herzog"
            ],
            "title": "Do as i can, not as i say: Grounding language in robotic affordances",
            "year": 2022
        },
        {
            "authors": [
                "Ahmed Al-Moadhen",
                "Renxi Qiu",
                "Michael Packianather",
                "Ze Ji",
                "Rossi Setchi."
            ],
            "title": "Integrating robot task planner with common-sense knowledge base to improve the efficiency of planning",
            "venue": "Procedia Computer Science, 22:211\u2013220. 17th International Conference",
            "year": 2013
        },
        {
            "authors": [
                "Peter Anderson",
                "Angel Chang",
                "Devendra Singh Chaplot",
                "Alexey Dosovitskiy",
                "Saurabh Gupta",
                "Vladlen Koltun",
                "Jana Kosecka",
                "Jitendra Malik",
                "Roozbeh Mottaghi",
                "Manolis Savva"
            ],
            "title": "On evaluation of embodied navigation agents",
            "year": 2018
        },
        {
            "authors": [
                "Peter Anderson",
                "Qi Wu",
                "Damien Teney",
                "Jake Bruce",
                "Mark Johnson",
                "Niko S\u00fcnderhauf",
                "Ian Reid",
                "Stephen Gould",
                "Anton van den Hengel"
            ],
            "title": "2018b. Visionand-Language Navigation: Interpreting visuallygrounded navigation instructions in real environ",
            "year": 2018
        },
        {
            "authors": [
                "Sugato Bagchi",
                "Gautam Biswas",
                "Kazuhiko Kawamura."
            ],
            "title": "Interactive task planning under uncertainty and goal changes",
            "venue": "Robotics and Autonomous Systems, 18(1-2):157\u2013167.",
            "year": 1996
        },
        {
            "authors": [
                "Dhruv Batra",
                "Aaron Gokaslan",
                "Aniruddha Kembhavi",
                "Oleksandr Maksymets",
                "Roozbeh Mottaghi",
                "Manolis Savva",
                "Alexander Toshev",
                "Erik Wijmans."
            ],
            "title": "Objectnav revisited: On evaluation of embodied agents navigating to objects",
            "venue": "arXiv preprint",
            "year": 2020
        },
        {
            "authors": [
                "Valts Blukis",
                "Chris Paxton",
                "Dieter Fox",
                "Animesh Garg",
                "Yoav Artzi."
            ],
            "title": "A persistent spatial semantic representation for high-level natural language instruction execution",
            "venue": "Conference on Robot Learning, pages 706\u2013717. PMLR.",
            "year": 2022
        },
        {
            "authors": [
                "Angel Chang",
                "Angela Dai",
                "Thomas Funkhouser",
                "Maciej Halber",
                "Matthias Niessner",
                "Manolis Savva",
                "Shuran Song",
                "Andy Zeng",
                "Yinda Zhang."
            ],
            "title": "Matterport3d: Learning from rgb-d data in indoor environments",
            "venue": "International Conference on 3D Vision",
            "year": 2017
        },
        {
            "authors": [
                "Howard Chen",
                "Alane Suhr",
                "Dipendra Misra",
                "Noah Snavely",
                "Yoav Artzi."
            ],
            "title": "Touchdown: Natural language navigation and spatial reasoning in visual street environments",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pat-",
            "year": 2019
        },
        {
            "authors": [
                "Xiaoping Chen",
                "Jianmin Ji",
                "Jiehui Jiang",
                "Guoqiang Jin",
                "Feng Wang",
                "Jiongkun Xie."
            ],
            "title": "Developing high-level cognitive functions for service robots",
            "venue": "AAMAS, volume 10, pages 989\u2013996.",
            "year": 2010
        },
        {
            "authors": [
                "Abhishek Das",
                "Satwik Kottur",
                "Jos\u00e9 MF Moura",
                "Stefan Lee",
                "Dhruv Batra."
            ],
            "title": "Learning cooperative visual dialog agents with deep reinforcement learning",
            "venue": "Proceedings of the IEEE international conference on computer vision, pages 2951\u20132960.",
            "year": 2017
        },
        {
            "authors": [
                "Richard E Fikes",
                "Nils J Nilsson."
            ],
            "title": "Strips: A new approach to the application of theorem proving to problem solving",
            "venue": "Artificial intelligence, 2(3-4):189\u2013 208.",
            "year": 1971
        },
        {
            "authors": [
                "Chuang Gan",
                "Jeremy Schwartz",
                "Seth Alter",
                "Martin Schrimpf",
                "James Traer",
                "Julian De Freitas",
                "Jonas Kubilius",
                "Abhishek Bhandwaldar",
                "Nick Haber",
                "Megumi Sano"
            ],
            "title": "Threedworld: A platform for interactive multi-modal physical simulation",
            "year": 2020
        },
        {
            "authors": [
                "Michael Gelfond",
                "Yulia Kahl."
            ],
            "title": "Knowledge representation, reasoning, and the design of intelligent agents: The answer-set programming approach",
            "venue": "Cambridge University Press.",
            "year": 2014
        },
        {
            "authors": [
                "Spandana Gella",
                "Aishwarya Padmakumar",
                "Patrick Lange",
                "Dilek Hakkani-Tur."
            ],
            "title": "Dialog Acts for Task-Driven Embodied Agents",
            "venue": "Proceedings of the 23nd Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDial), pages",
            "year": 2022
        },
        {
            "authors": [
                "N Gopalan",
                "E Rosen",
                "GD Konidaris",
                "S Tellex."
            ],
            "title": "Simultaneously learning transferable symbols and language groundings from perceptual data for instruction following",
            "venue": "Robotics: Science and Systems XVI.",
            "year": 2020
        },
        {
            "authors": [
                "Peter E Hart",
                "Nils J Nilsson",
                "Bertram Raphael."
            ],
            "title": "A formal basis for the heuristic determination of minimum cost paths",
            "venue": "IEEE transactions on Systems Science and Cybernetics, 4(2):100\u2013107.",
            "year": 1968
        },
        {
            "authors": [
                "Malte Helmert."
            ],
            "title": "A planning heuristic based on causal graph analysis",
            "venue": "ICAPS, volume 16, pages 161\u2013170.",
            "year": 2004
        },
        {
            "authors": [
                "Zhiwei Jia",
                "Kaixiang Lin",
                "Yizhou Zhao",
                "Qiaozi Gao",
                "Govind Thattai",
                "Gaurav Sukhatme."
            ],
            "title": "Learning to act with affordance-aware multimodal neural slam",
            "venue": "arXiv preprint arXiv:2201.09862.",
            "year": 2022
        },
        {
            "authors": [
                "Yuqian Jiang",
                "Nick Walker",
                "Justin Hart",
                "Peter Stone."
            ],
            "title": "Open-world reasoning for service robots",
            "venue": "Proceedings of the International Conference on Automated Planning and Scheduling, volume 29, pages 725\u2013733.",
            "year": 2019
        },
        {
            "authors": [
                "Yash Kant",
                "Arun Ramachandran",
                "Sriram Yenamandra",
                "Igor Gilitschenski",
                "Dhruv Batra",
                "Andrew Szot",
                "Harsh Agrawal."
            ],
            "title": "Housekeep: Tidying virtual households using commonsense reasoning",
            "venue": "arXiv preprint arXiv:2205.10712.",
            "year": 2022
        },
        {
            "authors": [
                "Piyush Khandelwal",
                "Shiqi Zhang",
                "Jivko Sinapov",
                "Matteo Leonetti",
                "Jesse Thomason",
                "Fangkai Yang",
                "Ilaria Gori",
                "Maxwell Svetlik",
                "Priyanka Khante",
                "Vladimir Lifschitz"
            ],
            "title": "Bwibots: A platform for bridging the gap between ai and human\u2013robot interaction",
            "year": 2017
        },
        {
            "authors": [
                "Hyounghun Kim",
                "Abhaysinh Zala",
                "Graham Burri",
                "Hao Tan",
                "Mohit Bansal."
            ],
            "title": "Arramon: A joint navigation-assembly instruction interpretation task in dynamic environments",
            "venue": "Findings of EMNLP.",
            "year": 2020
        },
        {
            "authors": [
                "Eric Kolve",
                "Roozbeh Mottaghi",
                "Winson Han",
                "Eli VanderBilt",
                "Luca Weihs",
                "Alvaro Herrasti",
                "Daniel Gordon",
                "Yuke Zhu",
                "Abhinav Gupta",
                "Ali Farhadi."
            ],
            "title": "Ai2-thor: An interactive 3d environment for visual ai",
            "venue": "arXiv preprint arXiv:1712.05474.",
            "year": 2017
        },
        {
            "authors": [
                "George Konidaris",
                "Leslie Pack Kaelbling",
                "Tomas Lozano-Perez."
            ],
            "title": "From skills to symbols: Learning symbolic representations for abstract high-level planning",
            "venue": "J. Artif. Int. Res., 61(1):215\u2013289.",
            "year": 2018
        },
        {
            "authors": [
                "S\u00e9verin Lemaignan",
                "Mathieu Warnier",
                "E Akin Sisbot",
                "Aur\u00e9lie Clodic",
                "Rachid Alami."
            ],
            "title": "Artificial cognition for social human\u2013robot interaction: An implementation",
            "venue": "Artificial Intelligence, 247:45\u201369.",
            "year": 2017
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Veselin Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "Bart: Denoising sequence-to-sequence pre-training for natural language",
            "year": 2020
        },
        {
            "authors": [
                "Nir Lipovetzky."
            ],
            "title": "Structure and inference in classical planning",
            "venue": "Lulu. com.",
            "year": 2014
        },
        {
            "authors": [
                "Bing Liu",
                "Ian Lane."
            ],
            "title": "Iterative policy learning in end-to-end trainable task-oriented neural dialog models",
            "venue": "2017 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), pages 482\u2013 489. IEEE.",
            "year": 2017
        },
        {
            "authors": [
                "Qi Liu",
                "Zihuiwen Ye",
                "Tao Yu",
                "Phil Blunsom",
                "Linfeng Song."
            ],
            "title": "Augmenting multi-turn textto-sql datasets with self-play",
            "venue": "arXiv preprint arXiv:2210.12096.",
            "year": 2022
        },
        {
            "authors": [
                "Lajanugen Logeswaran",
                "Yao Fu",
                "Moontae Lee",
                "Honglak Lee."
            ],
            "title": "Few-shot subgoal planning with language models",
            "venue": "arXiv preprint arXiv:2205.14288.",
            "year": 2022
        },
        {
            "authors": [
                "Drew V McDermott."
            ],
            "title": "A heuristic estimator for means-ends analysis in planning",
            "venue": "AIPS, volume 96, pages 142\u2013149.",
            "year": 1996
        },
        {
            "authors": [
                "So Yeon Min",
                "Devendra Singh Chaplot",
                "Pradeep Kumar Ravikumar",
                "Yonatan Bisk",
                "Ruslan Salakhutdinov."
            ],
            "title": "Film: Following instructions in language with modular methods",
            "venue": "International Conference on Learning Representations.",
            "year": 2021
        },
        {
            "authors": [
                "So Yeon Min",
                "Hao Zhu",
                "Ruslan Salakhutdinov",
                "Yonatan Bisk."
            ],
            "title": "Don\u2019t copy the teacher: Data and model challenges in embodied dialogue",
            "venue": "arXiv preprint arXiv:2210.04443.",
            "year": 2022
        },
        {
            "authors": [
                "Anjali Narayan-Chen",
                "Prashant Jayannavar",
                "Julia Hockenmaier."
            ],
            "title": "Collaborative dialogue in minecraft",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5405\u20135415.",
            "year": 2019
        },
        {
            "authors": [
                "Aishwarya Padmakumar",
                "Jesse Thomason",
                "Ayush Shrivastava",
                "Patrick Lange",
                "Anjali Narayan-Chen",
                "Spandana Gella",
                "Robinson Piramuthu",
                "Gokhan Tur",
                "Dilek Hakkani-Tur."
            ],
            "title": "Teach: Task-driven embodied agents that chat",
            "venue": "Proceedings of the AAAI",
            "year": 2022
        },
        {
            "authors": [
                "Alexander Pashevich",
                "Cordelia Schmid",
                "Chen Sun."
            ],
            "title": "Episodic transformer for vision-and-language navigation",
            "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 15942\u2013 15952.",
            "year": 2021
        },
        {
            "authors": [
                "Baolin Peng",
                "Xiujun Li",
                "Lihong Li",
                "Jianfeng Gao",
                "Asli Celikyilmaz",
                "Sungjin Lee",
                "Kam-Fai Wong."
            ],
            "title": "Composite task-completion dialogue policy learning via hierarchical deep reinforcement learning",
            "venue": "arXiv preprint arXiv:1704.03084.",
            "year": 2017
        },
        {
            "authors": [
                "Michael A Peshkin",
                "J Edward Colgate",
                "Wit Wannasuphoprasit",
                "Carl A Moore",
                "R Brent Gillespie",
                "Prasad Akella."
            ],
            "title": "Cobot architecture",
            "venue": "IEEE Transactions on Robotics and Automation, 17(4):377\u2013 390.",
            "year": 2001
        },
        {
            "authors": [
                "Malik"
            ],
            "title": "Habitat: A platform for embodied",
            "year": 2019
        },
        {
            "authors": [
                "Steve Young"
            ],
            "title": "A survey of statistical user",
            "year": 2006
        },
        {
            "authors": [
                "Zettlemoyer",
                "Dieter Fox"
            ],
            "title": "Alfred: A bench",
            "year": 2020
        },
        {
            "authors": [
                "Jesse Thomason",
                "Michael Murray",
                "Maya Cakmak",
                "Luke Zettlemoyer."
            ],
            "title": "Vision-and-dialog navigation",
            "venue": "Conference on Robot Learning, pages 394\u2013406. PMLR.",
            "year": 2020
        },
        {
            "authors": [
                "Marc Toussaint",
                "Christian Goerick."
            ],
            "title": "Probabilistic inference for structured planning in robotics",
            "venue": "2007 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 3068\u20133073. IEEE.",
            "year": 2007
        },
        {
            "authors": [
                "Claudia Yan",
                "Dipendra Misra",
                "Andrew Bennnett",
                "Aaron Walsman",
                "Yonatan Bisk",
                "Yoav Artzi"
            ],
            "title": "Chalet: Cornell house agent learning environment",
            "year": 2018
        },
        {
            "authors": [
                "Steve Young",
                "Milica Ga\u0161i\u0107",
                "Blaise Thomson",
                "Jason D Williams."
            ],
            "title": "Pomdp-based statistical spoken dialog systems: A review",
            "venue": "Proceedings of the IEEE, 101(5):1160\u20131179.",
            "year": 2013
        },
        {
            "authors": [
                "Yichi Zhang",
                "Jianing Yang",
                "Jiayi Pan",
                "Shane Storks",
                "Nikhil Devraj",
                "Ziqiao Ma",
                "Keunwoo Peter Yu",
                "Yuwei Bao",
                "Joyce Chai."
            ],
            "title": "Danli: Deliberative agent for following natural language instructions",
            "venue": "arXiv preprint arXiv:2210.12485.",
            "year": 2022
        },
        {
            "authors": [
                "Kaizhi Zheng",
                "Kaiwen Zhou",
                "Jing Gu",
                "Yue Fan",
                "Jialu Wang",
                "Zonglin Li",
                "Xuehai He",
                "Xin Eric Wang."
            ],
            "title": "Jarvis: A neuro-symbolic commonsense reasoning framework for conversational embodied agents",
            "venue": "arXiv preprint arXiv:2208.13266.",
            "year": 2022
        },
        {
            "authors": [
                "2017 Kolve et al",
                "2019 Savva et al",
                "2018 Puig et al",
                "2017 Chang et al",
                "Yan"
            ],
            "title": "2018) have been used over recent years to explore the efficacy of deep learning methods that directly use egocentric visual observations",
            "year": 2018
        },
        {
            "authors": [
                "E.T. D"
            ],
            "title": "Episodic Transformer (E.T.) Model (Pashevich et al., 2021) along with our modifications in greater detail. For convenience, we include a diagram of the model",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Embodied task completion (Shridhar et al., 2020; Padmakumar et al., 2022) is a challenge where an agent in a simulated environment (Kolve et al., 2017; Savva et al., 2019; Chang et al., 2017) is given natural language context in the form of instructions or dialogue and needs to take actions in the environment to complete a desired task, additionally making use of egocentric visual observations. This typically requires the agent to predict actions directly executable in the simulated environment. For example, an action sequence to make coffee could start with actions to move forward two steps, turn left, and pick up a mug identified by a semantic segmentation mask. In contrast, physical robot systems tend to be more modular with a\n\u2217 Contributions from Mert I\u0307nan and Dilek Hakkani-Tur were provided when they were employed at Amazon.\ndedicated component for task planning - composing a sequence of fine-grained motor skills into a more complex task (Chen et al., 2010; Lemaignan et al., 2017; Jiang et al., 2019). In such a system, the coffee task considered above would likely start by invoking a semantic navigation module to find the mug and a grasping module to pick it up. Some prior work has been on embodied AI benchmarks suggesting that more modular models can outperform monolithic models (Min et al., 2021; Jia et al., 2022; Zheng et al., 2022; Min et al., 2022). However, these do not evaluate and explore the limitations of individual modules.\nIn this work, we formulate and explore the problem of task planning for embodied task completion. We improve upon existing plan prediction models and demonstrate the potential for improvement by comparing them with human plans. We adapt the Execution from Dialogue History (EDH) benchmark from the TEACh dataset (Padmakumar et al., 2022) to evaluate models at the level of a plan \u2013 a sequence of object interaction actions paired with the type of object on which the action needs to be executed \u2013 which are evaluated using task success upon execution with the aid of a heuristic plan execution module. Plan prediction is more challenging in TEACh compared to other embodied AI datasets, as tasks can be hierarchical and parameterized, environments are cluttered, and objects may be hidden inside closed receptacles. We evaluate variants of the multimodal Episodic Transformer (E.T.) model, previously used to directly predict low-level actions in embodied task completion (Shridhar et al., 2020; Padmakumar et al., 2022) and find that these outperform a finetuned language-only baseline.\nData collection for embodied AI tasks involving natural language is expensive and time-consuming to collect (Padmakumar et al., 2022), motivating the need for methods that require less humanhuman data. We develop the first framework for ex-\npanding agenda-based dialogue simulation (Schatzmann and Young, 2009) to a multimodal embodied agent setup by augmenting agenda-based dialogue act prediction with a rule-based module for identifying action sequences in the environment to complete tasks. We demonstrate that the E.T.based models trained only on synthetic data can achieve about 85% of the performance of the same models trained on real data, obtaining a zero-shot success rate of 17.09%, which outperforms the full shot success rate of plans generated by the language-only baseline at 10.27%.\nTo summarize, our contributions include:\n\u2022 We formulate the problem of plan prediction for the TEACh dataset and evaluate a language-only baseline, variants of a multimodal transformer model (E.T.), and establish oracle performance on this problem.\n\u2022 We are the first to design a framework for synthesizing embodied dialogues involving both utterances and environment actions to complete a task.\n\u2022 We demonstrate that the synthetic data generated by our framework results in competitive zero-shot performance in our problem."
        },
        {
            "heading": "2 Task Setup",
            "text": "The TEACh dataset (Padmakumar et al., 2022) is an embodied dialogue dataset consisting of inter-\nactions between human annotators role-playing a Commander and Follower collaborating in a simulated home environment to complete household tasks. Only the Commander has access to task information, and only the Follower can take actions in the environment requiring them to communicate to complete the task. An effective Follower must engage in dialogue with the Commander, obtain relevant information such as details of the task to be completed and locations of objects, and reason about environment actions that can accomplish relevant state changes to make progress in the task. We focus on the EDH benchmark from the TEACh dataset where given some dialogue history, as well as past actions and image observations, the Follower must predict subsequent actions in the environment to make progress with the task. This is evaluated by comparing environmental state changes arising from gold and predicted action sequences. We modify the expected prediction from a model to be a plan, which we define as a sequence of object interaction actions paired with the object category of the object they are to be executed upon 1. An example of the task of boiling a potato is included in Figure 1. Note that in plan prediction, the model needs to reason about physical state changes - that the act of boiling requires\n1While it is possible to specify more abstract plans, we choose this level of abstraction as the training data can be generated automatically from the TEACh EDH instances.\nplacing the potato in a container filled with water, which is then heated using a stove in the example. Other aspects of execution, such as navigating to required objects and fine-grained position adjustments, can be carried out by a separate execution module, which, in our case, is heuristic-based but can also be learned. This problem is non-trivial in datasets such as TEACh where tasks are parameterized, and hence highly variable, and diverse initial states can add or remove task steps. This is borne out in our experiments by the significant gap between model generated and human plans.\nDuring inference, at each time step, a plan prediction model is expected to predict one object interaction action and the category of the object on which it is to be executed. This is then executed by one of two possible plan execution modules described in section 4. Execution terminates either when a model predicts a special Stop action, reaches a limit of 100 plan steps, or results in 30 simulator execution failures. A plan step may fail execution for a variety of reasons. It may be infeasible (e.g., trying to pick up a cabinet), a prerequisite step may not be completed (e.g., the Slice action is only feasible if the agent is holding a knife), or the agent may be poorly positioned (e.g., too close to the fridge to open it)."
        },
        {
            "heading": "3 Plan Prediction Models",
            "text": "We adapt the Episodic Transformer (E.T.) model (Pashevich et al., 2021) for plan prediction. This is a multimodal transformer model which, in our case, receives the EDH dialogue history as language input and egocentric image observations as visual input and predicts plan steps consisting of an object interaction action and the type of object the action is to be taken on2. We obtain training data by filtering EDH action sequences to contain only object interaction actions and train the model as in Padmakumar et al. (2022), where the model receives images and plan steps from the EDH action history as input and predicts the entire output plan at once. At inference time, the last plan step predicted is executed, and the input for the next time step is updated with an image observation after executing this plan step. We use three variants of this model.\n\u2022 E.T. : E.T.model as described above. \u2022 E.T. Hierarchical: E.T. is modified to\npass output from the action prediction head as input to the object prediction head. \u2022 E.T. + Mask: Uses predefined constraints to determine whether the predicted action is\n2See Appendix D for a more detailed explanation and a figure.\nfeasible to execute on the predicted object, and if not, backs up to the action with the highest probability that is feasible."
        },
        {
            "heading": "4 Plan Execution",
            "text": "While we can compare predicted plans with a ground truth plan using surface metrics such as edit distance, we believe a stronger test of predicted plans is executing them in the environment and measuring task success. To do this, we pair our models with heuristic plan execution modules:\n\u2022 Direct Plan Execution: Given a predicted object interaction action and object type, we use object coordinates from the simulator to identify the closest object of the type 3, use the navigation graph to navigate to it and attempt to execute the action.\n\u2022 Assisted Plan Execution: Direct plan execution can fail for various reasons. For example, if the sink is full and something needs to be placed in it, the sink needs to be cleared first. Although we train our plan prediction model with data that should enable it to predict such intermediate steps, we wish to explore whether models perform better if they abstract out such details. To do this, we analyze common execution failure cases and implement a set of heuristics, detailed in Appendix C, to increase their success.\nIn future work, we hope to replace these with models for plan executions (Zheng et al., 2022)."
        },
        {
            "heading": "5 Synthetic Dialogue Generation",
            "text": "Collecting embodied dialogue examples is expensive and time-consuming (Padmakumar et al., 2022). However, it would be desirable for embodied agents to adapt to new tasks without requiring human interaction data. Ideally, given a task definition, we would like to be able to bootstrap a model for the task, which can then be further refined using techniques such as reinforcement learning. This work proposes a method to generate synthetic embodied dialogues to train an initial model without human interaction data. Our process for simulating synthetic embodied dialogues is outlined in Figure 2 and in the following sections. Additional details are included in Appendix G. We plan to release our synthetic data for future research.\n3Note that using the closest object is a heuristic and can fail. In our experiments, we evaluate two oracle plan prediction methods to quantify the limitations of this."
        },
        {
            "heading": "5.1 Next Task Step Identification",
            "text": "Our dialogue simulation process involves breaking down a task into task steps corresponding to a single desired object state change, around which dialogue utterances and environment actions are constructed. Tasks in the TEACh dataset are defined as sets of object properties that must be satisfied for the task to be complete (Padmakumar et al., 2022). The public TEACh simulation wrapper 4 also includes a task checker that, when given a task definition and the current state of the simulator, can provide pending object state changes that need to be accomplished for the task to be considered complete. For example, Figure 2 demonstrates the synthesis of a dialogue session related to making coffee, which requires a mug in the environment to be clean and filled with coffee. As the agent acts in the environment, the task checker examines mugs in the environment and identifies the one closest to completion. The task checker can indicate to the agent the object state changes that still need to be accomplished on this mug; for example, in Figure 2, it identifies a face that needs to be cleaned. This is a single Task Step that can be used as a focus for dialogue utterances and environment actions. Once this Task Step is completed, the simulation process proceeds to the next Task Step of filling coffee, after which the task checker will indicate task completion, ending the dialogue simulation."
        },
        {
            "heading": "5.2 Agenda Based Dialogue Simulation",
            "text": "Given the next Task Step, we build a semantic outline for a snippet of synthetic dialogue related to this Task Step that includes dialogue acts exchanging information related to this Task Step and predicting a special action ExecuteStep that indicates a transition to predicting environment actions that accomplish this Task Step 5.\nWe do this by building an agenda-based dialogue simulator (Schatzmann and Young, 2009) over dialogue acts relevant to the TEACh dataset combined with the ExecuteStep action. We use a subset of the dialogue acts annotated for the TEACh dataset in Gella et al. (2022), focusing on requesting and receiving instructions related to the task and how to complete it, as well as the locations of objects. Our agenda-based simulator consists of 9 dialogue states, each computed as\n4https://github.com/alexa/teach 5Note that human-human dialogue in TEACh is much more free-form and we hope to achieve more versatility in future work.\na boolean function of 8 binary dialogue features. We pre-define probabilities for sampling dialogue acts, ExecuteStep, and DoNothing actions in each state. We then generate a dialogue session by sampling a sequence of actions, expanding each dialogue act into an utterance using templates filled in with task and simulator information, and each ExecuteStep into a sequence of environment actions as described in section 5.3. Appendix G includes more details of this process."
        },
        {
            "heading": "5.3 Plan Synthesis",
            "text": "When we predict that the session should transition from dialogue to environment actions, we use a rule-based system to identify an action sequence in the environment that is likely to accomplish the next Task Step. We hard code plans for each possible object state change, detailed in Appendix G. For example, if the object state change requires an object to be cleaned, the plan will involve moving the object to the sink, turning on and off the tap, picking it up, and pouring out water accumulated from cleaning. These hard-coded plans do not account for handling difficulties arising from the current state of the environment, for example, clearing out the sink if it is too full to place the object to be cleaned. Hence, we execute these synthesized plans using Assisted Execution (Section 4) to improve our success rate at completing task steps using these hard-coded plans."
        },
        {
            "heading": "6 Experiments",
            "text": ""
        },
        {
            "heading": "6.1 Plan Prediction Models",
            "text": "We evaluate our proposed plan prediction models on the EDH task of the TEACh dataset (Padmakumar et al., 2022). We experiment with each of the models in section 3 with each execution method in section 4. Additionally, we evaluate the following baseline and oracle conditions 6:\nBaseline: Our baseline is a language-only BART model (Lewis et al., 2020), finetuned to take in the EDH dialogue history and predicts the entire plan as a sequence of tokens that are post-processed for validity and executed as in (Gella et al., 2022).\n6We do not compare to TEACh EDH models in this paper (Padmakumar et al., 2022) as our execution methods access information that the TEACh baseline models cannot access. We do this to ensure that we are evaluating only the process of plan prediction without additional complications arising from navigation and simulator behavior\nOracle: As an upper bound to the success rate obtainable with each of our plan execution methods, we obtain oracle plans using the ground truth actions in the EDH instance. We filter these action sequences, retaining only object interaction steps and converting object IDs to object types to match the plan representation used by our models.\nOracle with Object IDs (CorefOracle): To further test the limitations of our plan representation combined with the heuristic of selecting the closest object of a particular type, we produce plans from human action sequences containing object IDs instead of object types to avoid ambiguity during plan execution.\nWe additionally include our best zero-shot model and our best model trained on both real and synthetic data. These models use synthetic data generated according to the method outlined in Section 5 using the initial states corresponding to the TEACh train set. The zero-shot model is trained only on synthetic data, and the data-augmented model is trained on a combination of real and synthetic data.\nWe evaluate models based on the success rate (SR) and goal condition success rate (GC) as defined in the original TEACh paper (Padmakumar et al., 2022). Success rate, which measures the fraction of EDH instances for which predicted plans produced all expected object state changes, and GC, which measures the fraction of such object state changes across all instances, were calculated. Since the TEACh test set is not public, we follow the standard protocol proposed in the TEACh codebase 7 of using a standardized division of the original validation sets into validation and test sets called the divided validation and divided test sets, each of which is further divided into Seen and Unseen splits.\nWe present our results in Table 1. For a subset of these conditions, we train and perform inference with three random seeds and perform 2- sided Welch t-tests. Allowing for Bonferroni corrections over four tests, we find that E.T. + Mask is trending to be significantly better than the baseline with p = 0.0381 on the divided_val_seen split and p = 0.0164 on the divided_test_seen split. We did not find any statistically significant difference between the E.T. Hierarchical and E.T. + Mask models 8.\n7https://github.com/alexa/teach 8We did not perform statistical comparisons across all pairs of conditions as it is expensive and time-consuming to run an\nSince oracle plans do not obtain a 100% success rate, we observe the limitations of our plan execution method, which can only handle 78.43% of unseen test instances even with object coreference resolved (CorefOracle). We believe this is due to the difficulty in obtaining perfect positioning and placement even with ground truth simulator information, further supported by the gap between direct and assisted execution of oracle plans. We additionally see that there is considerable scope for improvement from resolving ambiguity related to which object instance to manipulate, which accounts for an improvement of about 17% between Oracle and CorefOracle.\nWe observe that while the E.T., E.T. Hierarchical and E.T. + Mask models substantially improve over the baseline, there is also a considerable gap between them and Oracle which uses the same plan representation, which demonstrates that there is considerable scope for improvement in understanding the details of the task to be completed from dialogue, and reasoning about actions to take to achieve the corresponding state changes. Qualitatively, we find that multimodal input\u2019s main benefits are breaking down complex\ninference with enough random seeds to allow for Bonferroni corrections as the number of tests grows.\ntasks such as watering a plant, for which detailed steps are rarely provided in the dialogue, and identifying how much of the task has already been completed. Failures of the E.T. models mainly arise from not learning when to stop, which is a limitation of the current inference procedure. Other causes of failure include performing unrelated manipulations on easily visible objects or ignoring small objects in favour of larger ones.\nOn comparing models trained on real data with synthetic data, we find that the zero-shot models perform surprisingly well, outperforming the baseline trained on real data and approaching the performance of the models that have the same architecture but are trained on real data. This suggests that when generalizing to new tasks for this application, it might be reasonable to train a model purely on synthetic data and expect reasonable performance from interaction with human users."
        },
        {
            "heading": "6.2 Zero Shot Training Ablation",
            "text": "We perform further ablations to identify how zeroshot model performance varies according to data size in Table 2. While we see a trend towards improvement in performance with increasing data size, and the best results are obtained at higher data sizes, the improvement is not perfectly con-\nsistent with data size. We believe this sub-linear scaling with the increase in data is due to a combination of limitations in the range of TEACh initial states in which our synthetic plan execution method can generate a valid action sequence successfully, limitations of the diversity in synthetic templates and limitations in the ability of the E.T. model to model the TEACh plan prediction task effectively. We believe that through engineering improvements or a learned Reinforcement Learning policy, we can\nimprove the range of initial states covered; with the assistance of better LLMs, we can produce more diverse synthetic dialogue, and using neural SLAM models, we can overcome the limitations of what a particular model can learn. We plan to explore these directions in future work.\nNote that we did not perform other hyperparameter tuning for models in Table 2 besides the changes in training data. The best condition, E.T. Hierarchical with a synthetic training set of size 3x as large as the human-human training set from Table 2 has been reported in Table 1 for comparison with models trained on human-human data. We find that this model trained purely on synthetic data obtains a success rate of 17.09% on the divided unseen test split, outperforming the language-only baseline trained on real data at 8.87%, and approaches close to the performance of E.T. Hierarchical trained on human-human data at 19.70%."
        },
        {
            "heading": "6.3 Data Augmentation Training Ablation",
            "text": "In Table 3, we ablate different sizes of synthetic data when used in data augmentation. We find that when both real and synthetic data are included, larger sizes of the synthetic training set are less beneficial than when trained only on synthetic data. The best condition E.T. + Mask with a combination of human-human and synthetic data of equal size has been included in Table 1 as \"Best Augmented.\" We find that at 22.32% on the divided unseen test split, this slightly outperforms the same model condition trained on human-human data at 20.07%, and is much stronger than the best condition using only synthetic data at 17.09%."
        },
        {
            "heading": "7 Related Work",
            "text": "Task Planning: Interactive systems on physical robots typically have a modular structure in which task planning plays a significant role (Chen et al., 2010; Khandelwal et al., 2017; Peshkin et al., 2001). In simulated environments, Logeswaran et al. (2022) propose a language-only finetuned GPT-2 model for task planning on ALFRED . Some end-to-end ALFRED models also have task planning as a component (Min et al., 2021; Jia et al., 2022; Blukis et al., 2022). However, this is a simpler dataset where task planning can be cast as a 7-way classification problem. Prior work has also explored language-only task planning using finetuned BART models in TEACh (Gella et al., 2022;\nZheng et al., 2022; Zhang et al., 2022), which we compare to as a baseline.\nDialogue Simulation: User simulation in the dialogue community originally consisted of rule-based systems designed using linguistic knowledge to enable finetuning dialogue systems to individual user preferences and subsequently evolved into trainable probabilistic models that can be used to bootstrap a dialogue system in the initial development phase and further finetune it through reinforcement learning (Schatzmann et al., 2006; Young et al., 2013). A common method for building user simulators is agenda-based simulation (Schatzmann and Young, 2009), which uses a predefined set of transition probabilities between dialogue acts in combination with goal information to sample subsequent dialogue acts. This has been used to bootstrap a range of dialogue models ranging from probabilistic POMDP models (Schatzmann et al., 2007) and text-to-SQL models (Liu et al., 2022) to hierarchical deep reinforcement learning methods (Peng et al., 2017). In this work, we augment a standard agenda-based simulator with an additional intent to determine transitions to acting in the environment to generate situated dialogues. Another popular paradigm for dialogue simulation is to develop two models - one for the user and one for the agent side and train them simultaneously using reinforcement learning (Liu and Lane, 2017; Shah et al., 2018). This has also been used for some multimodal dialogue domains (Das et al., 2017), but we choose not to adapt it in our domain as the time to generate a single dialogue is higher in situated applications due to the latency from executing environment actions in the simulator."
        },
        {
            "heading": "8 Conclusions and Future Work",
            "text": "We develop a model for multi-modal plan prediction for the TEACh dataset using the Episodic Transformer architecture and evaluate end-to-end performance on the TEACh EDH task in conjunction with heuristic plan execution modules. We additionally experiment with training this model using only synthetic data generated using an agendabased dialogue simulator combined with an environment action generator that uses a combination of rules and simulator information to identify sequences of actions in the environment that can make progress with the task. We find that our E.T. plan prediction models outperform a BART baseline, even when BART is finetuned on human-\nhuman embodied dialogue data but E.T. is finetuned only on synthetic data, suggesting that our dialogue simulation approach is a viable alternative to expensive data collection for bootstrapping an embodied task completion model on new tasks. We also find a considerable performance gap between models and humans in plan prediction that cannot be easily closed by techniques such as data augmentation.\nThe recent success of large language models in a wide variety of structured prediction tasks, including robotic planning tasks similar to ours, is a promising future direction for our work. Some preliminary attempts prompting large language models indicate that it is non-trivial to design an appropriate representation of the dialogue history and plans for LLMs to guess at the remaining objects required for subsequent actions effectively. Additionally, while it is likely that recent long context LLMs can likely take in a significant amount of state information of the environment, further work is required to determine the best way to provide this in environments such as AI2-THOR, especially if a proposed method much scale up to more realistic home environments with larger numbers of objects. It is likely also possible to improve the diversity of the generated simulated data by paraphrasing generated utterances using LLMs."
        },
        {
            "heading": "9 Limitations",
            "text": "In this paper, we explore the problem of plan prediction for embodied task completion, and conduct our experiments in the TEACh dataset, which is set in the AI2-THOR simulator. While we hypothesize that models developed for plan prediction will transfer better to physical robots as they align better with levels of abstraction at which robotics systems are currently implemented, further experimentation is needed to evaluate such transferability. In the short term, such experiments will likely need to work on problems with a simpler action space as some of the Follower actions supported in AI2THOR, such as slicing, are not supported in most robots available currently. Additionally, it would be beneficial to test plan prediction models and our dialogue simulation method on similar tasks set in other simulators. This is a direction we plan to pursue in future work. We believe this is beyond the scope of this publication due to the considerable engineering effort involved in adapting models across different embodied AI simulators and task\nrepresentations. In this work, we describe a method to generate synthetic dialogues for embodied task completion by augmenting an agenda-based dialogue simulator with modules that break up an embodied AI task into individual object state changes, and rulebased methods to identify actions that complete them. Additionally, we currently manually define transition probabilities between dialogue acts for simulation, which would also need to be modified for a new dataset. While our approach is effective at bootstrapping plan prediction models without any human-human interaction data, it requires simulator-specific engineering, particularly when defining heuristics for assisted plan execution. Additionally, for both plan prediction inference and dialogue simulation, every action must be executed in the simulator. This results in considerable compute time, as discussed in the appendix. While dialogue simulation does not require the use of a GPU, the plan prediction models do - both for training and inference. We have additionally found that the Episodic Transformer models used in the paper show a noticeable variance in performance when trained with the same hyperparameters but with different random seeds. We attempt to strengthen our conclusions by training models with multiple seeds for statistical analysis where it is beneficial, but we would also like to highlight the development of models with less variance as an important direction for future research.\nFinally, our work is currently limited to English as we are not familiar with datasets in other languages that provide language instructions for tasks that require complex reasoning over multi-step action sequences."
        },
        {
            "heading": "10 Ethics Statement",
            "text": "This work is part of a broader tradition of building natural language interfaces to control various devices. Natural language interfaces such as language-based search and intelligent personal assistants provide convenience and have the potential to make multiple forms of technology ranging from mobile phones and computers, as well as robots or other machines such as ATMs or self-checkout counters more accessible and less intimidating to users who are unfamiliar or uncomfortable with other interfaces on such devices such as command shells, button-based interfaces or changing visual user interfaces. Spoken language interfaces can\nalso be used to make such devices more accessible for the visually impaired or users who have difficulty with fine motor control.\nUser trust in such interfaces is essential. Depending on the circumstances, therefore, some considerations to keep in mind are: (1) whether the collection of personal data benefits the user; (2) whether the collection of personal data is transparent to the user; (3) whether the user understands whether and how they can control the collection of personal data; and (4) whether the user has the ability to elect whether to use such interfaces, opt into or out of the collection of certain personal data, access and update certain personal data, and delete certain personal data. Additionally, safeguards may be required as embodied agents become capable of interacting with arbitrary objects in the world to reduce the likelihood of accidents or malicious misuse.\nIn this work, we also experiment using simulated embodied dialogue data to train models. On one hand, the use of simulated data can limit the collection of personal data. On the other hand, simulators may not be designed to represent the full range of user behavior and may perform better for some users than others."
        },
        {
            "heading": "A Licensing and Responsible Use",
            "text": "In this section, we discuss the licensing and usage of existing scientific artifacts in this paper:\n\u2022 TEACh dataset: The TEACh dataset is released under a CDLA-Sharing V 1.0 license, with images released under Apache 2.0 and code under an MIT license. We believe our usage does not violate any of these license terms. We do not redistribute any of these as part of our work as they are publicly available. The TEACh dataset was created to study models for translating natural language instructions combined with egocentric visual observations into action sequences. Our use case is a subtask of this intended use case. We also believe using the TEACh code to generate synthetic dialogue sessions is consistent with this goal.\n\u2022 AI2-THOR simulator: The AI2-THOR simulator (Kolve et al., 2017) is necessary to use the TEACh dataset and generate synthetic dialogue sessions. We believe our usage of AI2THOR does not violate the license. We plan to release our data under the CDLA-Sharing V 1.0 license, with images released under Apache 2.0, which we believe will be consistent with AI2-THOR and TEACh license\nterms. We believe our work is consistent with the intended usage of AI2-THOR, which is to further embodied AI research in the household domain.\n\u2022 BART model: We finetune BART (Lewis et al., 2020) as a baseline for our task, which is released under the Apache 2.0 license. We believe our usage is consistent with the license and do not intend to redistribute it. We believe that our use is consistent with the intended usage of BART as a general purpose sequence to sequence language model.\n\u2022 Episodic Transformer Model: We use the Episodic Transformer model (Pashevich et al., 2021), which is released under an MIT License, for plan prediction with some architectural modifications. We believe our usage is consistent with the intent of this model, which was designed for a very similar embodied AI application in a similar dataset.\nThe TEACh dataset was manually inspected by the original authors to remove identifying information and offensive utterances (Padmakumar et al., 2022). Since our generated data is templated, we do not believe this can contain personally identifying information or offensive utterances. Our synthetic dialogues are set in the TEACh environment and only cover the tasks listed in the TEACh dataset. The dialogues are in English and are mainly intended to cover requesting and informing of task steps (in the form of object state changes) and locations of objects. They support a limited breakage of strict turn-taking by allowing an agent to not generate an utterance at some time steps in the generation procedure."
        },
        {
            "heading": "B Additional Related Work",
            "text": "Task Planning on Physical Robots Task planning has long been a standard component of physical robot architectures (Chen et al., 2010), particularly with general purpose service robots (Khandelwal et al., 2017; Peshkin et al., 2001). Classical task planners include a symbolic representation of the state of the world, a goal, and skills the robot can execute. They are expected to find a sequence of skills that, when executed, will transform the world into the goal state, typically using heuristic search algorithms (Lipovetzky, 2014). Over the years, research in planning has improved the symbolic\nrepresentations used in planners (Fikes and Nilsson, 1971; McDermott, 1996; Gelfond and Kahl, 2014; Konidaris et al., 2018; Gopalan et al., 2020), search algorithms (Hart et al., 1968; Helmert, 2004; Richter and Westphal, 2010) and handling uncertainty via probabilistic methods (Toussaint and Goerick, 2007; Bagchi et al., 1996; Ponzoni Carvalho Chanel et al., 2019). More recent work has focused on expanding beyond fully defined world representations by expanding to use common sense (Al-Moadhen et al., 2013) and open worlds (Jiang et al., 2019). Some interesting efforts in this direction use Large Language Models to perform planning (Ahn et al., 2022).\nEmbodied AI Tasks in Simulation Simulated environments (Kolve et al., 2017; Savva et al., 2019; Puig et al., 2018; Chang et al., 2017; Yan et al., 2018) have been used over recent years to explore the efficacy of deep learning methods that directly use egocentric visual observations instead of data from expensive sensors. While there is a challenge in transferring results from simulated to real environments, simulated environments are more accessible, less expensive, and allow for the testing of technologies that may not be sufficiently safe for use in the real world (Savva et al., 2019). Additionally, while simulated environments can be used for tasks that do not require the use of language (Anderson et al., 2018a; Batra et al., 2020; Gan et al., 2020; Kant et al., 2022), they play a particularly valuable role in developing language understanding and reasoning capabilities over actions that are currently difficult for physical robots to complete, but we hope it will become a reality in the future (Kolve et al., 2017). Much of the work in language understanding for embodied AI happens using vision and language navigation, where an agent must learn to navigate through a previously unseen environment purely based on natural language route instructions (Anderson et al., 2018b; Chen et al., 2019; Thomason et al., 2020). Embodied task completion additionally requires performing object manipulation actions (Shridhar et al., 2020; Padmakumar et al., 2022; Suhr et al., 2019; Kim et al., 2020; Narayan-Chen et al., 2019). In this work, we specifically focus on the TEACh dataset (Padmakumar et al., 2022) as it involves more complex tasks that require non-trivial task planning."
        },
        {
            "heading": "C Assisted Plan Execution",
            "text": "This section outlines the full set of heuristics involved in assisted plan execution:\n\u2022 For all actions, if the target object property change is already complete, do nothing to avoid an execution failure.\n\u2022 Pickup: If the object is inside a receptacle (container), open the receptacle. After pickup, if a receptacle was opened, close it.\n\u2022 Place: If the target receptacle is in a receptacle, take it out and place it on the counter first (for example, if we need to place something on a plate that is inside a drawer). If the target receptacle needs to be opened, open it and close it after placement (for example, a drawer or microwave needs to be opened to place something inside). If a placement attempt fails, try removing the existing contents of the receptacle one by one to make more space.\n\u2022 Open, Close: Toggle off the target object if relevant (for example, microwaves need to be turned off to open them).\n\u2022 ToggleOn, ToggleOff: If the target is open, close it first (for example, microwaves need to be closed to turn them on).\n\u2022 Slice: If the target is in a receptacle, move it to the counter first.\nAdditionally, we also attempt position adjustments to increase the chance of success.\nD E.T. model\nThis section discusses the Episodic Transformer (E.T.) Model (Pashevich et al., 2021) along with our modifications in greater detail. For convenience, we include a diagram of the model in Figure 3. The E.T. model receives language (in our case, EDH dialogue history) and egocentric image observations of size 900 x 900 as input. Visual observations are first resized to 224 x 244 and then encoded using a visual encoder that is based on a Faster R-CNN model trained on 325K frames of expert demonstrations from the ALFRED train fold (which comprises seen splits of TEACh ) and not finetuned in any of our experiments. This encoder average-pools ResNet features four times and adds\na dropout of 0.3 to obtain feature maps of 512 x 7 x 7. These are then fed into two convolutional layers with 256 and 64 filters of size 1 x 1, respectively, and mapped using a fully connected layer to size 768.\nThe language input is tokenized using revtok 9 and encoded using two transformer layers with 12 attention heads and an embedding size of 768, which are trained from scratch. The language and visual encodings are concatenated and passed through two multimodal transformer layers with 12 attention heads, which are also trained from scratch in our experiments.\nTo predict actions and objects, the final embedding corresponding to each image from the input is projected using fully connected layers and then passed to two independent softmax prediction heads over the action and object space, respectively. At inference time, the last action, which is not a padding token, is identified and used for prediction, along with the object, at the same time step. The E.T. Hierarchical model connects the two prediction heads by passing the output of the action head as an additional input to the object head. The E.T. + Mask model examines the action-object\n9https://pypi.org/project/revtok/\npair predicted at inference time, and if they are found to be incompatible, replaces the action with the action of highest probability with the predicted object. The object is assumed to be the more reliable prediction as it is more likely to be directly visible."
        },
        {
            "heading": "E Plan Prediction Model Hyperparameters",
            "text": "For training the E.T., E.T. + Mask and E.T. Hierarchical methods, we retained hyperparameters from the original TEACh paper (Padmakumar et al., 2022), except the batch size, without further hyperparameter tuning, and used the largest batch size that could fit in a single GPU of a p3.8xlarge AWS EC2 instance. Note that the following hyperparameters were kept constant for all experiments reported in this paper, and results in different tables arise only from changes in training data, model choice (between E.T., E.T. Hierarchical and E.T. + Mask), and plan execution method. We used the AdamW optimizer with 0.33 weight decay with a learning rate of 1e 4 for the first 10 epochs and 1e 5 for the last 10 epochs. We trained all models for 20 epochs with a batch size of 3 and reported results from the final epoch. We replace sampling with rotation permutations of our training dataset per epoch, ensuring that every training example is seen exactly once in our dataset. For the language decoder in the transformer, we use a dropout of 0.1, and for the encoder, we use a dropout of 0.1. The different E.T. models required 4 hours for preprocessing (extracting image features using the ResNet-50 backbone) and about 2 hours per model for training using 4 GPUs of a p3.8xlarge AWS EC2 instance. At inference time, we could use a maximum of 3 GPUs for inference as one GPU was required by the simulator. When using 3 GPUs of a p3.8xlarge AWS EC2 instance, E.T. models took about 11 hours to complete inference jointly on the divided_val_seen and divided_test_seen splits and about 35 hours to complete inference jointly on the divided_val_unseen and divided_test_unseen splits. The time difference is due to the size of the various splits.\nFor the baseline BART model, we retain hyperparameters from the model presented in (Gella et al., 2022). We take the pretrained BART-base model from the Huggingface library 10 and finetune for 20 epochs using a batch size of 2 per\n10https://huggingface.co/\nGPU. The training was done using gradient accumulation across 4 GPUs of a p3.8xlarge AWS EC2 instance. We use the AdamW optimizer with \u03b21 = 0.9, \u03b22 = 0.99, \u03f5 = 1e08 and weight decay of 0.01. We use a learning rate of 5e05 with a linear warmup over 500 steps. The BART model can be finetuned in under an hour using all 4 GPUs of a p3.8xlarge AWS EC2 instance. We first performed inference on the BART model and saved the predicted plans to file before separately executing them in the AI2-THOR simulator. This process can also be completed in under an hour. Executing stored plans either in the case of the BART model or the oracle conditions took about 2.5 hours using 3 GPUs of a p3.8xlarge AWS EC2 instance for the combined divided_val_seen and divided_test_seen splits and about 8 hours for the combined divided_val_unseen and divided_test_unseen splits."
        },
        {
            "heading": "F Data Statistics",
            "text": "The number of games and EDH instances in the TEACh data splits and batches of synthetic data used in this paper are included in Table 4."
        },
        {
            "heading": "G Dialogue Simulation Details",
            "text": "This section provides a more detailed description of the dialogue simulation process. Dialogue simulation begins by sampling which agent starts the interaction - the Commander or the Follower, each\nwith 50% While it is possible to create new initial states, we iterate over each initial state in the train split of the TEACh dataset, each of which is associated with a task to be completed in that state. For each agent, the Commander and the Follower we maintain a state for the agent that is factored into binary state features. We then use predefined probabilities for sampling different dialogue acts in each state and alternate taking turns between the two agents. In this implementation, we use the following dialogue acts defined in Gella et al. (2022):\n\u2022 RequestForInstruction\n\u2022 Instruction\n\u2022 RequestForObjectLocationAndOtherDetails\n\u2022 InfoOnObjectLocationAndOtherDetails\n\u2022 Acknowledge\n\u2022 FeedbackPositive\n\u2022 FeedbackNegative\nHowever, we use both FeedbackPositive and FeedbackNegative only to end the dialogue either as a success or failure respectively. We additionally divide the Instruction dialogue act into two sub-types for convenience:\n\u2022 Instruction: For communicating the task and its parameters\n\u2022 Step: For communicating a single desired object state change that would result in progress towards completion of the task, for example, cleaning a mug to fill it with coffee eventually.\nWe correspondingly also create a special RequestStep action. Besides this, we have two non-dialogue actions that an agent can also choose to perform:\n\u2022 ExecuteStep: This is a cue to transition to actions in the environment. This is only performed by the Follower which identifies a rule-based plan to accomplish the desired state change and executes it with assisted plan execution described in section 4.\n\u2022 DoNothing: This allows an agent to skip a turn and hence avoids rigid turn-taking in the resultant dialogue and introduces variability in the amount of information communicated in the dialogue to mimic real dialogues better.\nThe state features used are:\n\u2022 dialogue_started: Agents start with this feature set to False, indicating that the agent is in the initial state before any dialogue has taken place and must initiate dialogue. It gets set to True once an initial utterance has been exchanged.\n\u2022 goal_communicated: This feature is False initially and set to True after an Instruction utterance has been sent from the Commander to the Follower communicating the high level task.\n\u2022 cur_step_requested: This feature is False initially, gets set to True when the Follower sends a RequestStep action to the Commander and reset to False when environment actions are executed.\n\u2022 cur_step_sent: This feature is False initially, gets set to True when the Commander sends the current step to the Follower through a Step dialogue act and reset to False when environment actions are executed.\n\u2022 cur_step_obj_requested: This feature is False initially, gets set to True when the Follower requests the location of an object and reset to False when environment actions are executed.\n\u2022 cur_step_obj_sent: This feature is False initially, gets set to True when the Commander sends the location of an object and reset to False when environment actions are executed.\n\u2022 task_complete: This feature is False initially and gets set to True when the task is completed successfully.\n\u2022 follower_stuck: This feature is used to identify failed dialogues. It is False initially and set to true if the Follower attempts environment actions but is unable to accomplish the intended object state change. When this is identified, the dialogue is terminated early.\nGiven the value of the state features, the next state of the agent is computed as boolean functions over state features included in Table 5. Given the dialogue acts we then sample dialogue acts according to Table 6 for the Commander and Table 7 for the Follower.\nGiven a dialogue act, we use the following templates to get utterances:\n\u2022 RequestForInstruction: Hello, what can I help you with today?\n\u2022 Instruction: This is filled in with the description field from the task definition, for example Make coffee or Put all Forks in any Sink.\n\u2022 RequestStep: Done. What should I do next?\n\u2022 Step: This is filled it from the condition_failure_desc field from the TEACh task definition. For example, The Mug does not have coffee. or The Fork must be placed in a Sink.\n\u2022 RequestForObjectLocationAndOtherDetails: Where can I find a/an \u27e8object\u27e9? where \u27e8object\u27e9 is identified from the task step.\n\u2022 InfoOnObjectDetails: You can find a/ and \u27e8object\u27e9 in/on a/an \u27e8object\u27e9 near a/an \u27e8object\u27e9 where the reference objects are identified using location information in the simulator.\n\u2022 FeedbackPositive: Great! We\u2019re all done.\nIf an ExecuteStep action is sampled, we then synthesize the plan for the property to be changed as part of the current task step using Table 8. This is then executed using assisted execution (Section 4, Appendix C) to obtain the final sequence of environment actions. If this produces the desired object state change, dialogue simulation continues; otherwise, it is terminated by entering the follower_stuck state. Our final implementation of dialogue simulation successfully simulates a dialogue that completes the task in 35.4% of the initial states in the TEACh dataset.\nDesired Outcome Plan\nCleaning\n(Pickup, \u27e8TARGET\u27e9), (Place, Sink), (ToggleOn, Faucet), (ToggleOff, Faucet), (Pickup, \u27e8TARGET\u27e9), (Pour, Sink)\nMaking Coffee (Pickup, \u27e8TARGET\u27e9), (Place, CoffeeMachine), (ToggleOn, CoffeeMachine)\nSlicing (Pickup, Knife), (Slice, \u27e8TARGET\u27e9), (Place, CounterTop)\nToasting Bread\n(Pickup, \u27e8TARGET\u27e9), (Place, Toaster), (ToggleOn, Toaster), (Pickup, \u27e8TARGET\u27e9), (Place, CounterTop)\nCooking\n(Pickup, \u27e8TARGET\u27e9), (ToggleOff, Microwave), (Open, Microwave), (Place, Microwave), (Close, Microwave), (ToggleOn, Microwave), (ToggleOff, Microwave), (Open, Microwave), (Pickup, \u27e8TARGET\u27e9), (Close, Microwave), (Place, CounterTop)\nPlacing (Pickup, \u27e8TARGET\u27e9), (Place, \u27e8DESIRED_VALUE\u27e9)\nBoiling\n(Pickup, \u27e8TARGET\u27e9), (Place, Bowl), (Pickup, Bowl), (Place, Sink), (ToggleOn, Faucet), (ToggleOff, Faucet), (Pickup, Bowl), (ToggleOff, Microwave), (Open, Microwave), (Place, Microwave), (Close, Microwave), (ToggleOn, Microwave), (ToggleOff, Microwave), (Open, Microwave), (Pickup, \u27e8TARGET\u27e9), (Close, Microwave), (Place, CounterTop)\nFill With Water\n(Pickup, Cup), (Place, Sink), (ToggleOn, Faucet), (ToggleOff, Faucet), (Pickup, Cup), (Pour, \u27e8TARGET\u27e9)\nTurn On (ToggleOn, \u27e8TARGET\u27e9)\nOpen (Open, \u27e8TARGET\u27e9)\nTable 8: Plans to accomplish object state changes"
        }
    ],
    "title": "Multimodal Embodied Plan Prediction Augmented with Synthetic Embodied Dialogue",
    "year": 2023
}