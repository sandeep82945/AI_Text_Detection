{
    "abstractText": "Large language models (LLMs) excel in many tasks in NLP and beyond, but most open models have very limited coverage of smaller languages and LLM work tends to focus on languages where nearly unlimited data is available for pretraining. In this work, we study the challenges of creating LLMs for Finnish, a language spoken by less than 0.1% of the world population. We compile an extensive dataset of Finnish combining web crawls, news, social media and eBooks. We pursue two approaches to pretrain models: 1) we train seven monolingual models from scratch (186M to 13B parameters) dubbed FinGPT, 2) we continue the pretraining of the multilingual BLOOM model on a mix of its original training data and Finnish, resulting in a 176 billion parameter model we call BLUUMI. For model evaluation, we introduce FIN-bench, a version of BIG-bench with Finnish tasks. We also assess other model qualities such as toxicity and bias. Our models and tools are openly available at https://turkunlp.org/gpt3-finnish.",
    "authors": [
        {
            "affiliations": [],
            "name": "Risto Luukkonen"
        },
        {
            "affiliations": [],
            "name": "Ville Komulainen"
        },
        {
            "affiliations": [],
            "name": "Jouni Luoma"
        },
        {
            "affiliations": [],
            "name": "Anni Eskelinen"
        },
        {
            "affiliations": [],
            "name": "Jenna Kanerva"
        },
        {
            "affiliations": [],
            "name": "Hanna-Mari Kupari"
        },
        {
            "affiliations": [],
            "name": "Filip Ginter"
        },
        {
            "affiliations": [],
            "name": "Veronika Laippala"
        },
        {
            "affiliations": [],
            "name": "Niklas Muennighoff"
        },
        {
            "affiliations": [],
            "name": "Aleksandra Piktus"
        },
        {
            "affiliations": [],
            "name": "Thomas Wang"
        },
        {
            "affiliations": [],
            "name": "Nouamane Tazi"
        },
        {
            "affiliations": [],
            "name": "Teven Le Scao"
        },
        {
            "affiliations": [],
            "name": "Thomas Wolf"
        },
        {
            "affiliations": [],
            "name": "Osma Suominen"
        },
        {
            "affiliations": [],
            "name": "Samuli Sairanen"
        },
        {
            "affiliations": [],
            "name": "Mikko Merioksa"
        },
        {
            "affiliations": [],
            "name": "Jyrki Heinonen"
        },
        {
            "affiliations": [],
            "name": "Aija Vahtola"
        },
        {
            "affiliations": [],
            "name": "Samuel Antao"
        },
        {
            "affiliations": [],
            "name": "Sampo Pyysalo"
        }
    ],
    "id": "SP:875c2f6073923bf880e9a1f9fb2774c73f967ce3",
    "references": [
        {
            "authors": [
                "Loubna Ben Allal",
                "Raymond Li",
                "Denis Kocetkov",
                "Chenghao Mou",
                "Christopher Akiki",
                "Carlos Munoz Ferrandis",
                "Niklas Muennighoff",
                "Mayank Mishra",
                "Alex Gu",
                "Manan Dey"
            ],
            "title": "Santacoder: don\u2019t reach for the stars",
            "year": 2023
        },
        {
            "authors": [
                "Amanda Askell",
                "Yuntao Bai",
                "Anna Chen",
                "Dawn Drain",
                "Deep Ganguli",
                "Tom Henighan",
                "Andy Jones",
                "Nicholas Joseph",
                "Ben Mann",
                "Nova DasSarma"
            ],
            "title": "A general language assistant as a laboratory",
            "year": 2021
        },
        {
            "authors": [
                "Giusepppe Attardi."
            ],
            "title": "Wikiextractor",
            "venue": "https:// github.com/attardi/wikiextractor.",
            "year": 2015
        },
        {
            "authors": [
                "Adrien Barbaresi."
            ],
            "title": "Trafilatura: A web scraping library and command-line tool for text discovery and extraction",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Nat-",
            "year": 2021
        },
        {
            "authors": [
                "Emily M Bender",
                "Timnit Gebru",
                "Angelina"
            ],
            "title": "McMillanMajor, and Shmargaret Shmitchell",
            "venue": "In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency,",
            "year": 2021
        },
        {
            "authors": [
                "Douglas Biber."
            ],
            "title": "Variation across speech and writing",
            "venue": "Cambridge University Press, Cambridge.",
            "year": 1988
        },
        {
            "authors": [
                "Tolga Bolukbasi",
                "Kai-Wei Chang",
                "James Y Zou",
                "Venkatesh Saligrama",
                "Adam T Kalai."
            ],
            "title": "Man is to computer programmer as woman is to homemaker? debiasing word embeddings",
            "venue": "Advances in neural information processing systems, 29.",
            "year": 2016
        },
        {
            "authors": [
                "Aakanksha Chowdhery",
                "Sharan Narang",
                "Jacob Devlin",
                "Maarten Bosma",
                "Gaurav Mishra",
                "Adam Roberts",
                "Paul Barham",
                "Hyung Won Chung",
                "Charles Sutton",
                "Sebastian Gehrmann"
            ],
            "title": "Palm: Scaling language modeling with pathways",
            "year": 2022
        },
        {
            "authors": [
                "Guzm\u00e1n",
                "Edouard Grave",
                "Myle Ott",
                "Luke Zettlemoyer",
                "Veselin Stoyanov."
            ],
            "title": "Unsupervised cross-lingual representation learning at scale",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8440\u2013",
            "year": 2020
        },
        {
            "authors": [
                "Alice Coucke",
                "Alaa Saade",
                "Adrien Ball",
                "Th\u00e9odore Bluche",
                "Alexandre Caulier",
                "David Leroy",
                "Cl\u00e9ment Doumouro",
                "Thibault Gisselbrecht",
                "Francesco Caltagirone",
                "Thibaut Lavril"
            ],
            "title": "Snips voice platform: an embedded spoken language understanding",
            "year": 2018
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association",
            "year": 2019
        },
        {
            "authors": [
                "Kaustubh D Dhole",
                "Varun Gangal",
                "Sebastian Gehrmann",
                "Aadesh Gupta",
                "Zhenhao Li",
                "Saad Mahamood",
                "Abinaya Mahendiran",
                "Simon Mille",
                "Ashish Shrivastava",
                "Samson Tan"
            ],
            "title": "Nl-augmenter: A framework for task-sensitive",
            "year": 2021
        },
        {
            "authors": [
                "Anni Eskelinen",
                "Laura Silvala",
                "Filip Ginter",
                "Sampo Pyysalo",
                "Veronika Laippala."
            ],
            "title": "Toxicity detection in Finnish using machine translation",
            "venue": "Proceedings of the 24th Nordic Conference on Computational Linguistics (NoDaLiDa), pages 685\u2013697, T\u00f3r-",
            "year": 2023
        },
        {
            "authors": [
                "Samuel Gehman",
                "Suchin Gururangan",
                "Maarten Sap",
                "Yejin Choi",
                "Noah A. Smith."
            ],
            "title": "RealToxicityPrompts: Evaluating neural toxic degeneration in language models",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages",
            "year": 2020
        },
        {
            "authors": [
                "V\u00e4in\u00f6 Hatanp\u00e4\u00e4."
            ],
            "title": "A generative pre-trained transformer model for Finnish",
            "venue": "Master\u2019s thesis, Aalto University. School of Science.",
            "year": 2022
        },
        {
            "authors": [
                "Kenneth Heafield."
            ],
            "title": "KenLM: Faster and smaller language model queries",
            "venue": "Proceedings of the sixth workshop on statistical machine translation, pages 187\u2013197.",
            "year": 2011
        },
        {
            "authors": [
                "Jordan Hoffmann",
                "Sebastian Borgeaud",
                "Arthur Mensch",
                "Elena Buchatskaya",
                "Trevor Cai",
                "Eliza Rutherford",
                "Diego de Las Casas",
                "Lisa Anne Hendricks",
                "Johannes Welbl",
                "Aidan Clark"
            ],
            "title": "Training compute-optimal large language models",
            "year": 2022
        },
        {
            "authors": [
                "Jenna Kanerva",
                "Filip Ginter",
                "Li-Hsin Chang",
                "Iiro Rastas",
                "Valtteri Skantsi",
                "Jemina Kilpel\u00e4inen",
                "HannaMari Kupari",
                "Jenna Saarni",
                "Maija Sev\u00f3n",
                "Otto Tarkka."
            ],
            "title": "Finnish paraphrase corpus",
            "venue": "Proceedings of the 23rd Nordic Conference on Compu-",
            "year": 2021
        },
        {
            "authors": [
                "Hugo Lauren\u00e7on",
                "Lucile Saulnier",
                "Thomas Wang",
                "Christopher Akiki",
                "Albert Villanova del Moral",
                "Teven Le Scao",
                "Leandro Von Werra",
                "Chenghao Mou",
                "Eduardo Gonz\u00e1lez Ponferrada",
                "Huu Nguyen"
            ],
            "title": "The BigScience ROOTS corpus: A",
            "year": 2022
        },
        {
            "authors": [
                "Raymond Li",
                "Loubna Ben Allal",
                "Yangtian Zi",
                "Niklas Muennighoff",
                "Denis Kocetkov",
                "Chenghao Mou",
                "Marc Marone",
                "Christopher Akiki",
                "Jia Li",
                "Jenny Chim"
            ],
            "title": "Starcoder: may the source be with you! arXiv preprint arXiv:2305.06161",
            "year": 2023
        },
        {
            "authors": [
                "Juhani Luotolahti",
                "Jenna Kanerva",
                "Veronika Laippala",
                "Sampo Pyysalo",
                "Filip Ginter."
            ],
            "title": "Towards universal web parsebanks",
            "venue": "International Conference on Dependency Linguistics.",
            "year": 2015
        },
        {
            "authors": [
                "Tomas Mikolov",
                "Ilya Sutskever",
                "Kai Chen",
                "Greg S Corrado",
                "Jeff Dean."
            ],
            "title": "Distributed representations of words and phrases and their compositionality",
            "venue": "Advances in Neural Information Processing Systems, volume 26. Curran Associates, Inc.",
            "year": 2013
        },
        {
            "authors": [
                "Niklas Muennighoff",
                "Qian Liu",
                "Armel Zebaze",
                "Qinkai Zheng",
                "Binyuan Hui",
                "Terry Yue Zhuo",
                "Swayam Singh",
                "Xiangru Tang",
                "Leandro von Werra",
                "Shayne Longpre."
            ],
            "title": "Octopack: Instruction tuning code large language models",
            "venue": "arXiv preprint",
            "year": 2023
        },
        {
            "authors": [
                "Niklas Muennighoff",
                "Alexander M Rush",
                "Boaz Barak",
                "Teven Le Scao",
                "Aleksandra Piktus",
                "Nouamane Tazi",
                "Sampo Pyysalo",
                "Thomas Wolf",
                "Colin Raffel."
            ],
            "title": "Scaling data-constrained language models",
            "venue": "arXiv preprint arXiv:2305.16264.",
            "year": 2023
        },
        {
            "authors": [
                "Niklas Muennighoff",
                "Thomas Wang",
                "Lintang Sutawika",
                "Adam Roberts",
                "Stella Biderman",
                "Teven Le Scao",
                "M Saiful Bari",
                "Sheng Shen",
                "Zheng-Xin Yong",
                "Hailey Schoelkopf"
            ],
            "title": "Crosslingual generalization through multitask finetuning",
            "year": 2022
        },
        {
            "authors": [
                "Emily \u00d6hman",
                "Marc P\u00e0mies",
                "Kaisla Kajava",
                "J\u00f6rg Tiedemann."
            ],
            "title": "XED: A multilingual dataset for sentiment analysis and emotion detection",
            "venue": "arXiv preprint arXiv:2011.01612.",
            "year": 2020
        },
        {
            "authors": [
                "Long Ouyang",
                "Jeff Wu",
                "Xu Jiang",
                "Diogo Almeida",
                "Carroll L Wainwright",
                "Pamela Mishkin",
                "Chong Zhang",
                "Sandhini Agarwal",
                "Katarina Slama",
                "Alex Ray"
            ],
            "title": "Training language models to follow instructions with human feedback",
            "year": 2022
        },
        {
            "authors": [
                "Jan Pomik\u00e1lek."
            ],
            "title": "Removing boilerplate and duplicate content from web corpora",
            "venue": "Ph.D. thesis, Masaryk university, Faculty of informatics, Brno, Czech Republic.",
            "year": 2011
        },
        {
            "authors": [
                "Ofir Press",
                "Noah A Smith",
                "Mike Lewis."
            ],
            "title": "Train short, test long: Attention with linear biases enables input length extrapolation",
            "venue": "arXiv preprint arXiv:2108.12409.",
            "year": 2021
        },
        {
            "authors": [
                "Sampo Pyysalo",
                "Jenna Kanerva",
                "Antti Virtanen",
                "Filip Ginter."
            ],
            "title": "Wikibert models: Deep transfer learning for many languages",
            "venue": "Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa), pages 1\u201310.",
            "year": 2021
        },
        {
            "authors": [
                "Alec Radford",
                "Karthik Narasimhan",
                "Tim Salimans",
                "Ilya Sutskever"
            ],
            "title": "Improving language understanding by generative pre-training",
            "year": 2018
        },
        {
            "authors": [
                "Alec Radford",
                "Jeffrey Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "venue": "OpenAI blog,",
            "year": 2019
        },
        {
            "authors": [
                "Jeff Rasley",
                "Samyam Rajbhandari",
                "Olatunji Ruwase",
                "Yuxiong He."
            ],
            "title": "Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters",
            "venue": "Proceedings of the 26th ACM SIGKDD International Conference on",
            "year": 2020
        },
        {
            "authors": [
                "Teven Le Scao",
                "Angela Fan",
                "Christopher Akiki",
                "Ellie Pavlick",
                "Suzana Ili\u0107",
                "Daniel Hesslow",
                "Roman Castagn\u00e9",
                "Alexandra Sasha Luccioni",
                "Fran\u00e7ois Yvon"
            ],
            "title": "2022a. Bloom: A 176b-parameter openaccess multilingual language model",
            "year": 2022
        },
        {
            "authors": [
                "Teven Le Scao",
                "Thomas Wang",
                "Daniel Hesslow",
                "Lucile Saulnier",
                "Stas Bekman",
                "M Saiful Bari",
                "Stella Bideman",
                "Hady Elsahar",
                "Niklas Muennighoff",
                "Jason Phang"
            ],
            "title": "What language model to train if you have one million gpu hours",
            "year": 2022
        },
        {
            "authors": [
                "Mohammad Shoeybi",
                "Mostofa Patwary",
                "Raul Puri",
                "Patrick LeGresley",
                "Jared Casper",
                "Bryan Catanzaro."
            ],
            "title": "Megatron-lm: Training multi-billion parameter language models using model parallelism",
            "venue": "arXiv preprint arXiv:1909.08053.",
            "year": 2019
        },
        {
            "authors": [
                "Valtteri Skantsi",
                "Veronika Laippala."
            ],
            "title": "Analyzing the unrestricted web: The finnish corpus of online registers",
            "venue": "Nordic Journal of Linguistics.",
            "year": 2022
        },
        {
            "authors": [
                "Adri\u00e0 Garriga-Alonso"
            ],
            "title": "Beyond the imitation game: Quantifying and extrapolating the capabilities of language models",
            "venue": "arXiv preprint arXiv:2206.04615",
            "year": 2022
        },
        {
            "authors": [
                "Erich Strohmaier",
                "Jack Dongarra",
                "Horst Simon",
                "Martin Meuer",
                "Hans Meuer."
            ],
            "title": "Top500 - the list",
            "venue": "https://www.top500.org/.",
            "year": 2023
        },
        {
            "authors": [
                "Hugo Touvron",
                "Thibaut Lavril",
                "Gautier Izacard",
                "Xavier Martinet",
                "Marie-Anne Lachaux",
                "Timoth\u00e9e Lacroix",
                "Baptiste Rozi\u00e8re",
                "Naman Goyal",
                "Eric Hambro",
                "Faisal Azhar"
            ],
            "title": "Llama: Open and efficient foundation language models",
            "year": 2023
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin."
            ],
            "title": "Attention is all you need",
            "venue": "Advances in Neural Information Processing Systems, 30.",
            "year": 2017
        },
        {
            "authors": [
                "Viljami Venekoski",
                "Jouko Vankka."
            ],
            "title": "Finnish resources for evaluating language model semantics",
            "venue": "Proceedings of the 21st Nordic Conference on Computational Linguistics, NoDaLiDa, 22-24 May 2017, Gothenburg, Sweden, 131, pages 231\u2013236.",
            "year": 2017
        },
        {
            "authors": [
                "Antti Virtanen",
                "Jenna Kanerva",
                "Rami Ilo",
                "Jouni Luoma",
                "Juhani Luotolahti",
                "Tapio Salakoski",
                "Filip Ginter",
                "Sampo Pyysalo."
            ],
            "title": "Multilingual is not enough: Bert for finnish",
            "venue": "arXiv preprint arXiv:1912.07076.",
            "year": 2019
        },
        {
            "authors": [
                "Laura Weidinger",
                "John Mellor",
                "Maribeth Rauh",
                "Conor Griffin",
                "Jonathan Uesato",
                "Po-Sen Huang",
                "Myra Cheng",
                "Mia Glaese",
                "Borja Balle",
                "Atoosa Kasirzadeh"
            ],
            "title": "Ethical and social risks of harm from language models. arXiv preprint arXiv:2112.04359",
            "year": 2021
        },
        {
            "authors": [
                "Linting Xue",
                "Noah Constant",
                "Adam Roberts",
                "Mihir Kale",
                "Rami Al-Rfou",
                "Aditya Siddhant",
                "Aditya Barua",
                "Colin Raffel."
            ],
            "title": "mt5: A massively multilingual pre-trained text-to-text transformer",
            "venue": "Proceedings of the 2021 Conference of the North",
            "year": 2021
        },
        {
            "authors": [
                "Zheng-Xin Yong",
                "Hailey Schoelkopf",
                "Niklas Muennighoff",
                "Alham Fikri Aji",
                "David Ifeoluwa Adelani",
                "Khalid Almubarak",
                "M Saiful Bari",
                "Lintang Sutawika",
                "Jungo Kasai",
                "Ahmed Baruwa"
            ],
            "title": "BLOOM+ 1: Adding language support",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Neural language models based on the Transformer architecture (Vaswani et al., 2017) have revolutionized Natural Language Processing (NLP) in recent years, advancing the state of the art in tasks ranging from text classification to open-ended text generation. Generative, decoder-only language models such as the Generative Pretrained Transformer (GPT) (Radford et al., 2018) series have been a particular focus of interest in part due to their multitask and few-shot capabilities (Radford et al., 2019; Brown et al., 2020). The ability of such models to implicitly learn to perform tasks that they have not been directly trained on has been considered to be closely tied to the scale of the model (Brown et al., 2020; Chowdhery et al., 2022) and, perhaps even\nmore importantly, to the number of training tokens (Hoffmann et al., 2022; Muennighoff et al., 2023b; Touvron et al., 2023). Most work on such models focuses on English, often entirely excluding other languages, and assumes that hundreds of billions of tokens of text are readily available for model training.\nIn this study, we consider the challenges of introducing large generative models for Finnish, a Uralic language natively spoken by fewer than 6 million people. While the language is comparatively well represented in online resources relative to this number, less than 1% of texts available in e.g. Wikipedia and Common Crawl are Finnish (Pyysalo et al., 2021; Xue et al., 2021). As the other members in the language family are either even smaller and lesser-resourced or quite distant, the resources for creating models for the language are quite limited. Finnish has been represented to some degree in Transformer-based models since the release of the original multilingual BERT model (Devlin et al., 2019), and a dedicated monolingual BERT for the language was previously created by Virtanen et al. (2019). Also some generative models for Finnish have been previously introduced by the \"Finnish-NLP\" group1 and Hatanp\u00e4\u00e4 (2022), but as training LLMs is very expensive and Finnish is constrained by the size of available data, models exceeding a billion parameters have been so far missing from the Finnish NLP landscape.\nWe compile a broad-coverage dataset of Finnish and train monolingual models up to 13 billion parameters for 300 billion tokens (approx. 8 epochs). We also perform continued pretraining of the 176- billion parameter BLOOM model (Scao et al., 2022a) to extend its coverage of Finnish, introduce novel evaluation datasets, and assess multiple\n1https://huggingface.co/Finnish-NLP\naspects of the resulting models. While the details of our data collection and processing are somewhat specific to Finnish, we believe that our study can serve as a template for training large models for other small languages."
        },
        {
            "heading": "2 Models",
            "text": "Our models are based on the GPT architecture (Radford et al., 2019) and we follow the pretraining approach developed for the BLOOM family of large multilingual language models (Scao et al., 2022a). We train monolingual Finnish models with up to 13 billion parameters from scratch, following GPT-3 (Brown et al., 2020) in terms of the number of layers, dimensionality, and number of attention heads (Table 1), and BLOOM in terms of both the software implementation as well as specific design choices such as the use of Alibi position embeddings (Press et al., 2021) and layer normalization (Scao et al., 2022b). We additionally continue the pretraining of the original 176-billion parameter BLOOM model with a mix of its original pretraining corpus and Finnish data to create a model we call BLUUMI. While the BLOOM models were trained on data from 46 different languages, the training did not include Finnish. Prior work has investigated extending smaller BLOOM models to new languages not included during pretraining (Yong et al., 2022) and found parameter-efficient finetuning methods and (to a lesser degree) continued pretraining to be effective approaches. Due to the fact that the 176- billion parameter BLOOM model has been significantly undertrained for its parameter count (Hoffmann et al., 2022; Muennighoff et al., 2023b), we focus on continued pretraining in this study."
        },
        {
            "heading": "3 Data",
            "text": "We next present the sources of training data, preprocessing steps, data statistics and analysis."
        },
        {
            "heading": "3.1 Data sources",
            "text": "We draw on a broad range of text sources, aiming to cover a wide range of linguistic variation across genres, registers, authors and time periods. The pretraining data sources are listed in Table 2 and described below, and a summary of the timespans they cover is given in Appendix A. Parsebank The Finnish Internet Parsebank (Luotolahti et al., 2015) is a 6 billion token corpus of Finnish collected in 2015-2016 from Common Crawl and a targeted Internet crawl seeded by the .fi domain registry content and all URLs of Finnish material in Common Crawl. The texts have been deduplicated at the paragraph level using Onion (Pomik\u00e1lek, 2011) and cleaned using the jusText library.2 mC4 The multilingual colossal, cleaned version of Common Crawl\u2019s web crawl corpus (mC4) was introduced by Xue et al. (2021) for training the mT5 models. mC4 was derived from the 71 web scrapes (2013-2020) released by Common Crawl prior to the creation of the corpus. We use the Finnish subset of mC4 as identified by cld33, which contains 8 billion tokens across 19 million documents. CC-Fi To maximize coverage of Finnish text in Common Crawl resources, we applied a custom extraction process to all crawls from 2013-2022, emphasizing recall of Finnish.4 We extracted texts using Trafilatura (Barbaresi, 2021) and performed exact document-level deduplication using MurmurHash prior to the general preprocessing steps described below. This processing produced 55 million documents totaling 20 billion tokens. Fiwiki The Finnish portion of the Wikipedia free encyclopedia consists of approximately 180,000 openly licensed articles created by volunteer editors. For this work, we extracted text from the 20221120 dump of the Finnish Wikipedia using WikiExtractor (Attardi, 2015), producing a dataset of 110 million tokens. L\u00f6nnrot Projekti L\u00f6nnrot5 is a project digitizing out-of-copyright Finnish and Swedish literature. For this work, we used the 2574 Finnish works that were published by Projekti L\u00f6nnrot by the start of pretraining, which contain a total of 125 million tokens. Yle Archives of the national public broadcasting\n2https://github.com/miso-belica/jusText 3https://github.com/google/cld3 4Appendix B provides a comparison of the two datasets\nderived from Common Crawl. 5http://www.lonnrot.net/\ncompany of Finland (Yle) are available for research through the Language Bank of Finland6. We use the complete Yle archives available at the start of our model pretraining, which consist of approximately 800,000 articles (220 million tokens) from 2011-2020, of which 0.3% are easy-to-read news. STT As for Yle, archives of the Finnish News Agency (Suomen Tietotoimisto or STT) are provided for research through the Language Bank of Finland. The collection available at the start of this study spans publications from 1992-2018 and contains 2.8 million newswire articles which total approximately 300 million tokens. ePub The National Library of Finland maintains a collection of electronically published books in Finland. For the purposes of this project, the library granted access to its ePub collection of approximately 30,000 Finnish eBook contents. As these books remain copyrighted, it is not possible to redistribute texts from this dataset. Lehdet The Lehdet dataset is based on archived HTML material collected by the National Library of Finland and includes daily, weekly and monthly crawls of newspaper internet sites and also a yearly .fi-domain crawl covering years from 2015 to 2021. The total cleaned dataset consists of 85 billion characters from 60 million HTML documents. The dataset was provided by the National Library and can not be redistributed due to copyright. Suomi24 Archives of the largest social networking site in Finland, Suomi24,7 are available for research via the Language Bank of Finland. For this study, we downloaded the complete archives\n6https://www.kielipankki.fi/ 7https://www.suomi24.fi\navailable at the time, consisting of 95 million comments and 5 billion words from 2001-2020. Reddit-Fi The social site Reddit includes a few predominantly Finnish-language discussion forums. For this work, we downloaded Reddit archives8 and extracted text from posts to r/Suomi,9 the largest such forum. The dataset contains over 150,000 submissions and nearly 4 million comments (in total 150 million tokens) from 2009-2022. ROOTS The Responsible Open-science Opencollaboration Text Sources (ROOTS) dataset (Lauren\u00e7on et al., 2022) consists of 1.6 terabytes of text data spanning 59 languages used for pretraining BLOOM (Scao et al., 2022a). While Finnish was not included as an official language, a contamination analysis found 0.03% of ROOTS to be Finnish (Muennighoff et al., 2022). We use ROOTS in the continued pretraining of the BLOOM model, but not for the monolingual Finnish models."
        },
        {
            "heading": "3.2 Preprocessing",
            "text": "We next briefly describe the preprocessing steps performed for the source datasets. All processing scripts, parameters, and models are available along with detailed statistics at https://github.com/ TurkuNLP/finngen-tools. Deduplication In addition to the deduplication steps already performed for some of the datasets (see Section 3.1), we performed approximate Ngram overlap-based deduplication using Onion (Pomik\u00e1lek, 2011) separately for all datasets. We run Onion with default parameters, marking as duplicate any line of text (paragraph, title, etc.) where at least 50% of N-grams have appeared previously.\n8https://files.pushshift.io/reddit/ 9https://www.reddit.com/r/Suomi\nWe then trim duplicate lines from the beginning and end of each document. Finally, if at least 50% of the remaining lines in the document are duplicates, we discard the entire document.\nHeuristic filtering To filter out texts that are unlikely to be Finnish prose text, we apply a set of rule-based filters, extending on the heuristics introduced by Virtanen et al. (2019). In short, these filters remove texts that have e.g. an unusually high ratio of punctuation or digits to alphabetic characters, a high ratio of non-Finnish to Finnish alphabetic characters, a low type-token ratio, or a low average line length. This step removed only a small proportion of texts, with more than 95% of texts remaining in most resources.\nN-gram model filtering To further remove texts that have the surface characteristics of prose text but are unlikely to represent standard Finnish, we applied a perplexity filter using an N-gram model. We first trained a KenLM (Heafield, 2011) model on the set of known good Finnish texts prepared by Virtanen et al. (2019) for training their FinBERT model and then applied this model to documents, removing lines with perplexity > 100 000. This filter was not applied to sources estimated to be predominantly well-edited text (news, L\u00f6nnrot, and Wikipedia). For the three web crawl datasets, the filter removed 15-20% of text; for the social media datasets, this proportion was 2-5%.\nToxicity filtering To reduce the proportion of texts that contain e.g. obscenities or identity attacks, we applied the Finnish toxicity detection classifier introduced by Eskelinen et al. (2023). The classifier is a FinBERT model (Virtanen et al., 2019) fine-tuned on a machine-translated version of the\nJigsaw Toxicity dataset10. The filter was not applied to news, L\u00f6nnrot books, or Wikipedia. Toxicity filtering removed 1-5% of sources other than CC-Fi, but as much as 23% of the CC-Fi text. This effect may be explained by the fact that CC-Fi was the only web source that had not previously been filtered for e.g. obscenity.\nMasking personal data We applied a set of high-recall regular expressions and rule-based scripts to mask personal data such as email addresses and potential phone numbers. These scripts impacted approximately 0.2% of characters in total.\nTokenization We train a new monolingual Finnish tokenizer on a sample of the pretraining data using the tokenizers library11. We follow the BLOOM recipe for the tokenizer, creating a bytelevel BPE tokenizer without Unicode normalization and use the same regular expression-based pre-tokenization as in BLOOM. As Finnish is an agglutinative language with complex morphology and thus a high number of word forms, we chose to create a comparatively large vocabulary for a monolingual tokenizer of 131,072 tokens."
        },
        {
            "heading": "3.3 Data statistics",
            "text": "The statistics of the final dataset after preprocessing are presented in Table 3. We oversample open and high-quality resources such as L\u00f6nnrot and Wikipedia. In total, the final pretraining dataset (including oversampling) consists of 38 billion tokens when processed with our Finnish tokenizer.\n10https://www.kaggle.com/c/ jigsaw-toxic-comment-classification-challenge\n11https://github.com/huggingface/tokenizers"
        },
        {
            "heading": "3.4 Register analysis",
            "text": "We characterize the contents of the Web-based datasets (mC4, CC-Fi and Parsebank) by automatically analyzing their distribution of text registers (or genres) (Biber, 1988). To this end, we apply a register identification model based on the FinCORE corpus, trained using XLM-R (Conneau et al., 2020). The model and corpus were both presented by Skantsi and Laippala (2022). The register categories present text varieties with different characteristics and communicative objectives, such as narrative, interactive discussion and lyrical. Table 4 presents the proportions of the registers in the three datasets. We see a broadly similar register distribution across the datasets, with narrative clearly most frequent in all three and categories such as how-to, spoken and lyrical representing only small fractions of the total."
        },
        {
            "heading": "4 Pretraining",
            "text": "This work leverages the LUMI supercomputer,12 as of this writing the third-largest and seventh greenest in the world (Strohmaier et al., 2023). The LUMI data center allows power consumption to be fully supplied with hydroelectricity, and waste heat produced by LUMI is utilized by the city of Kajaani, providing up to 20% of the district heating.\nTraining was done on up to 192 nodes, each consisting of 4 AMD Instinct MI250X GPUs, a single 64-core AMD Trento CPU and 512GB of memory. Since the MI250X GPU is a multi-chip module with two Graphics Compute Dies (GCDs), each node can be considered to have 8 GPUs in total. In this perspective, the training utilized up to 1536 GPUs. The 64-core CPU is configured as 4 NUMA nodes linked to the GPUs. Because of a \u201clow noise\u201d mode used on the nodes, only 63 cores were available for training.\n12https://www.lumi-supercomputer.eu/\nLo ss\n3.0\n3.5\n4.0\n4.5\n5.0\nTokens 0K 50B 100B 150B 200B 250B 300B\nSmall Medium Large XL 3B 8B 13B 0K 0.000 0.000 0.000 0.000 0.000 0.000 0.000\n0.000\n2.000\n4.000\n0K 0K 0K\nSmall Medium Large XL 3B 8B 13B\nFigure 1: Validation losses with 5-point moving average smoothing.\nWe train our models on an adapted version of BLOOM\u2019s pretraining framework, MegatronDeepSpeed.13 By combining features from Megatron (Shoeybi et al., 2019) and DeepSpeed (Rasley et al., 2020), the Megatron-DeepSpeed framework can be used for training large language models with pipeline, tensor and data parallelization across GPUs and compute nodes. Our changes to the framework involve making the codebase, including its optimized CUDA kernels, usable on AMD MI250X GPUs using PyTorch ROCm. To leverage the capabilities of MI250X, ROCm enables the use of GPU matrix cores through its rocBLAS and MIOpen library implementations that, in turn, are leveraged by PyTorch. PyTorch also leverages the RCCL library to implement distributed collectives. RCCL also uses a HIP port of the AWS OpenFabrics Interface (OFI) plugin 14 to enable communication directly through to the Slingshot fabric provider for improved performance at scale.\nFor the monolingual Finnish models trained from scratch, we follow Brown et al. (2020) also in setting the batch size and maximum learning rate in addition to the model architecture parameters. For the continued pretraining of BLOOM to create the BLUUMI model, we retain the original BLOOM parameters (Scao et al., 2022a). The pretraining parameter values are shown in Table 5.\nFigure 1 shows the loss curves for held-out validation data for the models trained from scratch, showing a stable pretraining process for all models and the expected pattern of larger models achieving lower loss.\n13https://github.com/TurkuNLP/ Megatron-DeepSpeed\n14https://github.com/ROCmSoftwarePlatform/ aws-ofi-rccl"
        },
        {
            "heading": "5 Evaluation",
            "text": "We next present a few-shot evaluation dataset for Finnish and compare the capability of the models using this data. We additionally assess model alignment, bias, and toxicity in separate evaluations."
        },
        {
            "heading": "5.1 FIN-bench dataset",
            "text": "BIG-bench (Srivastava et al., 2022) is a collection of tasks created to assess various aspects of model capabilities. For this study, we created a similar Finnish evaluation dataset, FIN-bench,15 based on a BIG-bench subset augmented with newly introduced tasks. The tasks were primaly generated by machine translating the text of the equivalent BIGbench tasks and subsequently correcting any translation errors as well as assuring that the questions remain culturally relevant to Finnish. Exceptions include the Arithmetic tasks (generated data) and new tasks (Paraphrase, Analogy, Emotions). The FIN-bench dataset contains 3919 examples in total, divided over the tasks described briefly below. Examples of the tasks can be found from Appendix G. Analogy Analogies of the type Paris is to France as Helsinki is to . . . represent a well-established approach for evaluating language models. We created an analogy dataset using templates to reformulate analogy quadruples into natural language questions. We created 130 examples from the dataset of Venekoski and Vankka (2017) and the data of Mikolov et al. (2013) translated to Finnish. Arithmetic tests the degree to which a model has acquired an ability to perform basic one- to fivedigit addition, subtraction, multiplication and division. The Finnish variant of the task was automatically generated by manually translating the templates in the scripts for the corresponding BIGbench task and consists of 1923 examples in total. Cause and effect evaluates a model\u2019s ability to reason about the causality of two events. Each example states two events, the cause and the effect, and the model is asked to select the correct ordering. The task consists of 153 examples. Emotions evaluates the ability of a model to classify sentences according to the emotion that they express. The task is derived from the XED dataset (\u00d6hman et al., 2020) by selecting examples of at least five words that have exactly one emotion label and then manually filtering a random selection of these to identify 160 examples that a human an-\n15https://github.com/TurkuNLP/FIN-bench\nnotator without refrerence to specific annotation instructions would be expected to label correctly. Empirical judgments measures how well a model can distinguish sentences that express a causal relation from ones that express a correlative relation. The task also contains neutral passages of text that mimic the structure of the sentences containing a correlative or causal relation, but do not contain either. There are 33 examples of each category in the task, i.e. 99 in total. General knowledge measures the ability of models to answer simple questions which can easily be answered by most people, such as \u201cHow many legs does a horse have?\u201d. The task is a translation of the 70 examples in the BIG-bench original for all but three questions regarding imperial unit conversion, which we replace with questions on metric units. Intent recognition tests the logical reasoning of models by measuring how well they can recognize the correct intent from an input. The task may be a good predictor of performance in task-oriented dialogue systems. It includes 693 translated examples originally from the dataset introduced by Coucke et al. (2018). Misconceptions assesses a model\u2019s ability to distinguish popular misconceptions from facts; models trained on increasingly bigger datasets of mixedquality internet data may not discern between common assertions and ones that are true. Translations of this task were heavily filtered by our annotators due to being considered culturally too U.S.-centric. Approximately 40% of the original questions were removed from the dataset, resulting in a task with 134 examples. Paraphrase tests whether a model can distinguish full paraphrases from sentences that are merely similar. The task was created by selecting 100 positive and 100 negative examples from the Finnish Paraphrase Corpus (Kanerva et al., 2021), emphasizing cases that people can categorize without reference to the specifics of the corpus annotation guidelines. Sentence ambiguity evaluates to what degree a model can identify whether sentences with intentionally introduced ambiguous aspects state a true or false claim. The task consists of 60 examples translated from BIG-bench. Similarities abstraction measures a model\u2019s ability to identify human-like abstract associations between objects: for example, a dog and a parakeet are similar in that they are both pets. The data consists of 76 multiple-choice questions."
        },
        {
            "heading": "5.2 Few-shot results",
            "text": "We evaluate models on FIN-bench in zero- to threeshot settings and summarize results using mean accuracy across all tasks. For tasks that are organized into subtasks (Cause and effect and Arithmetic), we first average over the subtasks before taking the overall average. Primary evaluation results are visualized in Figure 2.\nWe find that our monolingual models at least match and in most instances outperform the results of previously released Finnish models of comparable sizes, lending support to the choices we have made for data selection and preprocessing as well as the model architecture and pretraining process. The best performance of the models released previously for Finnish, 38.5%, is achieved by the largest model introduced by Hatanp\u00e4\u00e4 (2022). Our best monolingual model outperforms this result by over 10% points and the BLUUMI model by over 20% points, representing a substantial advance in the state of the art in the capability of generative models trained for Finnish.\nAs expected, overall performance generally increases with the number of in-context examples (zero to three shots) as well as with model size, with some exceptions. First, some small models break the expected pattern, showing better zero-shot performance than one- to three-shot. This could be related to a tendency of less capable models to simply repeat patterns from preceding context, which can lead the models to copy whatever appears after \u201cAnswer:\u201d (or equivalent) in the preceding few-shot\nexamples. Second, we notice a consistent drop in performance between our 8B and 13B parameter models. This may be caused by overfitting due to an excessive number of parameters and training steps compared to a relatively small amount of (non-repeated) text, which can lead to decreasing performance (Muennighoff et al., 2023b). Based on these results, we estimate that the 8B parameter model may be our most capable monolingual model and, more generally, that approximately 10B parameters may represent a limit for effectively training monolingual models of this type for languages whose resources are broadly comparable to those available for Finnish.\nTo further evaluate the BLUUMI model, we compared its performance to that of the original BLOOM model on FIN-bench (Figure 3) and on English tasks from the EleutherAI evaluation har-\nness (Gao et al., 2021) (Figure 4). We find that BLUUMI performs notably better than BLOOM on FIN-bench tasks on all the few-shot evaluation tests, with a 12-18% point accuracy difference in favor of BLUUMI. On the English tasks, we find no significant difference in performance between the original BLOOM and BLUUMI (two-sided ttest). These results indicate that the continued pretraining has succeeded in substantially improving the Finnish capabilities of the model without compromising the existing English capabilities of the original model."
        },
        {
            "heading": "5.3 Alignment",
            "text": "We assess model alignment using the BIG-bench HHH alignment task (Askell et al., 2021), which includes four categories: harmlessness, honesty, helpfulness, and other. In contrast to most other tasks in BIG-bench, both of the two choices in each example can be considered correct: for instance, when assessing harmlessness, it is undesirable for a model to provide instructions for violent acts, and refusing to help is considered the correct answer. We create a Finnish version of the HHH alignment task through initial machine tranlation and manual correction, and evaluate models using the same process as for the other BIG-bench tasks. Results are shown in Figure 5. We find that all models perform poorly at these tasks, only exceeding the random baseline for the other category and measuring par-\nticularly low for helpfulness. While it is not surprising that base models that have not been specifically trained to follow instructions or operate in a dialogue context score low at this task, the results emhasize the need to align the models to assure that their output is helpful, harmless, and more factually accurate. We note that although there appear to be some correlations between model size and HHH performance, all differences remain within one standard deviation and are not significant."
        },
        {
            "heading": "5.4 Bias",
            "text": "Language models have an established tendency to repeat or amplify biases present in training data. As one example of bias, female/male gender stereotypes in models is a concern because their widespread use can result in further amplifying these biases (Bolukbasi et al., 2016). We assessed the occurrence of such bias using prompts with the structure \u201cThe name of the [professional or occupation holder] was\u201d and categorized predicted names into male or female when the name had that association in 95% of cases in national statistics. The distribution predicted by the model was then compared to the distribution in the most recent published labor data records published by Statistics Finland in 2020.16 As illustrated in Figure 6 and detailed in Appendix C, the model broadly reflects the actual labor distribution, indicating that\n16https://tilastokeskus.fi/julkaisu/ cktws35s04dru0b553lzi7aci\nit has learned this bias from the pretraining data. We note that while this is just one example of a type of bias that our models (as well as most other present-day models) can learn in their pretraining, it demonstrates why such models should not be naively applied e.g. for hiring decisions (see also Limitations below)."
        },
        {
            "heading": "5.5 Toxicity",
            "text": "To test to what degree our models are prone to generating toxic content, we follow the unprompted generation approach of Gehman et al. (2020), prompting the models with only their endof-sequence (EOS) token to signal the start of a new context.17 The unprompted generations were then classified for toxic content using the model introduced by Eskelinen et al. (2023) (see also Section 3.2) and a small sample manually assessed to assure labeling quality. The results of this evaluation are summarized in Figure 7. We find that our models more than halve the fraction of generated toxic content when compared to models from Hatanp\u00e4\u00e4 (2022), which were trained without filtering pretraining texts for toxicity. Our models nevertheless produce unprompted toxic generations approx. 2% of the time, reflecting remaining challenges in their alignment."
        },
        {
            "heading": "6 Discussion and conclusions",
            "text": "In this study, we compiled an extensive dataset of Finnish and created in total eight new large lan-\n17FinnishNLP-models were left out of this evaluation as they appear to have been trained without an EOS token.\nguage models: seven monolingual Finnish models ranging from 185 million to 13 billion parameters and a multilingual 176-billion parameter model, BLUUMI. We additionally introduced a new evaluation dataset, FIN-bench, and evaluated the models in few-shot settings as well as specifically assessed their alignment, bias and toxicity. We found that our models are substantially more capable than prior Finnish models and that continued pretraining has greatly improved the Finnish capability of BLUUMI without compromising its existing English capabilities. We also demonstrated limitations of the models in terms of their alignment, incorporation of bias, and remaining tendency to generate toxic content, which we aim to address in future work. We hope our models will serve as foundation models for Finnish that can be used in research and leveraged through instruction finetuning and other alignment methods (Ouyang et al., 2022) to create a range of capable tools for processing Finnish text. In future work, we hope to continue our study of efficient and environmentally sustainable approaches for creating capable open foundation models for lesser-resourced languages."
        },
        {
            "heading": "Acknowledgments",
            "text": "The authors wish to acknowledge CSC \u2013 IT Center for Science, Finland, for generous computational resources on the LUMI supercomputer. This project has received funding from the European Union\u2019s Horizon Europe research and innovation programme under Grant agreement No 101070350 and the Finnish Research Council, grant number 331297. The contents of this publication are the sole responsibility of its authors and do not necessarily reflect the opinion of the European Union.\nLimitations\nThe models introduced in this work are trained predominantly on data sourced from the internet, and despite our efforts to remove potentially harmful texts from the pretraining data, they carry many of the well-established limitations of such models (Bender et al., 2021; Weidinger et al., 2021). In our evaluation, we have experimentally demonstrated specific limitations in terms of model alignment (Section 5.3), bias (Section 5.4), and toxicity (Section 5.5). While the introduced models notably improve over the capabilities of previously released models in a range of Finnish tasks, due to these and other limitations the models should primarily be considered resources for research and a potential foundation for tools and applications, but they should not be used as-is for user-facing applications or for any task with potential for high impact on people\u2019s rights or well-being, such as hiring decisions. Substantial further work is likely to be required to create versions of the models that can be assured to be well aligned, free of bias, and not prone to generating toxic output.\nOur work focuses on large models for a lesserresourced language, and the amount of Finnish text available for model pretraining is a fundamental limitation of our work. Despite drawing on a broad range of sources, it was not possible to assemble enough text to avoid multiple epochs over the data to match the GPT-3 pretraining process, and the repetition of data may be reflected in reduced capability, especially for the largest monolingual model (Section 5.2). The challenges of collecting sufficient high-quality Finnish text for large model training also forced us to make a choice between data quality and quantity on the one hand and replicability on the other. We chose to partly train on texts provided by the National Library of Finland as part of a research collaboration. While these are some of the highest-quality texts in our dataset, they cannot be readily redistributed, and complete replication of our work is thus impossible without the involvement of the national library. While we regret this limitation, we note that lack of access to complete pretraining data is a negative aspect that our models share with many other present-day models. Future work may consider increasing the available data via augmentation techniques (Dhole et al., 2021) or mixing with data from a different modality such as code (Muennighoff et al., 2023b,a; Allal et al., 2023; Li et al., 2023)."
        },
        {
            "heading": "A Timespan covered by Finnish datasets",
            "text": "The rough timespan covered by the Finnish datasets is summarized in the following figure, excluding the L\u00f6nnrot dataset (0.4% of the data), which covers out-of-copyright literature and mostly consists of books published before 1950. Due to the difficulty of assigning a publication date to web-based materials that may be continuously edited, for these resources we report the timespan of their retrieval.\nDataset\n1990 1991 1992 1993 1994 1995 1996 1997 1998 1999 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023\nParsebank Retrieved mC4 Published CC-Fi Fiwiki Yle STT ePub Lehdet Suomi24 Reddit-Fi"
        },
        {
            "heading": "B Comparison of mC4-Fi and CC-Fi datasets",
            "text": "The mC4-Fi and CC-Fi datasets are both derived from Common Crawl data, but cover different sets of crawls and apply different selection criteria and text extraction and filtering pipelines. To assess the overlap of these two datasets after preprocessing, we first compared the sets of URLs in the metadata of the two datasets, finding that 65% of the mC4-Fi URLs are also found in CC-Fi, while only 29% of CC-Fi URLs are also in mC4-Fi, indicating substantial differences in which documents are included and suggesting that the processing to create the CC-Fi dataset was successful in increasing coverage of Finnish documents selected from Common Crawl resources compared to mC4-Fi.\nTo further assess textual overlap, we first sampled 100,000 random URLs found in both datasets. For each URL we created the set of 5-grams from the document texts in mC4-Fi and CC-Fi as well as their intersection. We found that 73% of 5-grams in mC4-Fi overlap with those of the corresponding document in CC-Fi, and 84% of CC-Fi 5-grams appeared also in the mC4-Fi document. This indicates that while the texts extracted from each matching document are highly similar in the two resources, they are not identical, and the redundancy of these resources is thus lower than suggested by simple URL overlap."
        },
        {
            "heading": "C Full gender bias results on 13B model",
            "text": "Occupation Ammatti STurkuNLPce M F M (%) F (%) seller myyj\u00e4 (s) Employment stats 35206 66315 34.68% 65.32%\nPredicted 243 68 78.14% 21.86% practical nurse l\u00e4hihoitaja (s) Employment stats 8925 70851 11.19% 88.81%\nPredicted 0 370 0.00% 100.00% registered nurse sairaanhoitaja (s) Employment stats 6342 66692 8.68% 91.32%\nPredicted 17 422 3.87% 96.13% office cleaner toimistosiivooja (s) Employment stats 10915 53098 17.05% 82.95%\nPredicted 334 156 68.16% 31.84% home aid kodinhoitaja (s) Employment stats 6252 36482 14.63% 85.37%\nPredicted 25 337 6.91% 93.09% nanny lastenhoitaja (s) Employment stats 2013 38010 5.03% 94.97%\nPredicted 39 427 8.37% 91.63% sales representative myyntiedustaja (s) Employment stats 25534 13096 66.10% 33.90%\nPredicted 383 90 80.97% 19.03% cargo handler rahdink\u00e4sittelij\u00e4 (s) Employment stats 29129 7450 79.63% 20.37% Predicted 350 64 84.54% 15.46% house builder talonrakentaja Employment stats 32032 1976 94.19% 5.81%\nPredicted 502 3 99.41% 0.59% restaurant attendant ravintolaty\u00f6ntekij\u00e4 Employment stats 11332 21799 34.20% 65.80%\nPredicted 173 137 55.81% 44.19% secretary yleissihteeri Employment stats 4285 27767 13.37% 86.63%\nPredicted 265 74 78.17% 21.83%\nsoftware engineer sovellussuunnittelija Employment stats 25110 5705 81.49% 18.51% Predicted 433 71 85.91% 14.09% kindergarten teacher lastentarhanopettaja Employment stats 656 21077 3.02% 96.98% Predicted 69 431 13.80% 86.20%\nsoftware architect sovellusarkkitehti Employment stats 15220 5348 74.00% 26.00% Predicted 291 35 89.26% 10.74%\nagriculture machinist maatalouskoneasentaja Employment stats 18090 479 97.42% 2.58% Predicted 423 8 98.14% 1.86%\naccountant tilintarkastaja Employment stats 6445 11208 36.51% 63.49% Predicted 230 5 97.87% 2.13% teaching assistant koulunk\u00e4yntiavustaja Employment stats 2314 14038 14.15% 85.85% Predicted 1 386 0.26% 99.74%\ncarpenter kirvesmies Employment stats 15870 448 97.25% 2.75% Predicted 228 11 95.40% 4.60%\ndriver autonkuljettaja Employment stats 14006 2303 85.88% 14.12% Predicted 281 11 96.23% 3.77%\nbuilding electrician rakennus s\u00e4hk\u00f6asentaja Employment stats 14084 364 97.48% 2.52% Predicted 513 0 100.00% 0.00%\nplumber putkiasentaja Employment stats 13618 271 98.05% 1.95% Predicted 455 0 100.00% 0.00%\nsenior physician ylil\u00e4\u00e4k\u00e4ri Employment stats 5505 8354 39.72% 60.28% Predicted 204 21 90.67% 9.33%\nstore manager myym\u00e4l\u00e4esimies Employment stats 4661 8004 36.80% 63.20% Predicted 371 62 85.68% 14.32%\nmachinist koneistaja Employment stats 11868 793 93.74% 6.26% Predicted 217 17 92.74% 7.26%\nfarmer maanviljelij\u00e4 Employment stats 10331 2137 82.86% 17.14% Predicted 295 54 84.53% 15.47% study advisor opinto-ohjaaja Employment stats 3498 8737 28.59% 71.41% Predicted 7 509 1.36% 98.64%\nhairdresser kampaaja Employment stats 867 10473 7.65% 92.35% Predicted 1 379 0.26% 99.74%\nmailman postinkantaja Employment stats 6503 4258 60.43% 39.57% Predicted 163 17 90.56% 9.44% coffee shop worker kahvilamyyj\u00e4 Employment stats 1927 8824 17.92% 82.08% Predicted 51 153 25.00% 75.00%\nreal estate agent kiinteist\u00f6nv\u00e4litt\u00e4j\u00e4 Employment stats 6496 4176 60.87% 39.13% Predicted 114 129 46.91% 53.09%\nbus driver linja-autonkuljettaja Employment stats 9099 1078 89.41% 10.59% Predicted 335 32 91.28% 8.72% guardsman vartija Employment stats 7496 2292 76.58% 23.42% Predicted 160 15 91.43% 8.57%\nbank worker pankkitoimihenkil\u00f6 Employment stats 2145 7531 22.17% 77.83% Predicted 274 51 84.31% 15.69%\nelectrician s\u00e4hk\u00f6asentaja Employment stats 9343 312 96.77% 3.23% Predicted 480 0 100.00% 0.00%\nphysiotherapist fysioterapeutti Employment stats 2008 7502 21.11% 78.89% Predicted 73 174 29.55% 70.45%\nsales engineer myynti-insin\u00f6\u00f6ri Employment stats 6422 2362 73.11% 26.89% Predicted 434 32 93.13% 6.87%\nwaiter tarjoilija Employment stats 2191 6125 26.35% 73.65% Predicted 52 69 42.98% 57.02%\nspecial education teacher erityisopettaja Employment stats 1223 7027 14.82% 85.18% Predicted 48 405 10.60% 89.40%\ncareers adviser urasuunnittelija Employment stats 1584 6445 19.73% 80.27% Predicted 233 179 56.55% 43.45%\nstorekeeper kauppias Employment stats 4678 3326 58.45% 41.55% Predicted 309 75 80.47% 19.53%\nphysical education instructor liikunnanohjaaja Employment stats 2829 5025 36.02% 63.98% Predicted 96 396 19.51% 80.49%\noffice secretary toimistosihteeri Employment stats 230 7393 3.02% 96.98% Predicted 150 347 30.18% 69.82% purchasing agent sis\u00e4\u00e4nostaja Employment stats 4066 3456 54.05% 45.95% Predicted 140 44 76.09% 23.91%\nphysician yleisl\u00e4\u00e4k\u00e4ri Employment stats 2882 4522 38.92% 61.08% Predicted 251 45 84.80% 15.20%"
        },
        {
            "heading": "D Toxicity scores",
            "text": "Model Identity attack Insult Obscene Severe toxicity Threat Toxicity\nHatanp\u00e4\u00e4/small 0.149 % 1.471 % 2.132 % 0.070 % 0.026 % 5.377 % Hatanp\u00e4\u00e4/xl 0.185 % 1.344 % 2.055 % 0.109 % 0.015 % 5.241 % TurkuNLP/small 0.039 % 0.208 % 0.435 % 0.004 % 0.008 % 1.658 % TurkuNLP/medium 0.048 % 0.248 % 0.410 % 0.002 % 0.011 % 1.896 % TurkuNLP/large 0.039 % 0.280 % 0.490 % 0.001 % 0.011 % 1.981 % TurkuNLP/xl 0.061 % 0.272 % 0.546 % 0.002 % 0.011 % 2.211 % TurkuNLP/3B 0.069 % 0.343 % 0.618 % 0.004 % 0.021 % 2.290 % TurkuNLP/8B 0.058 % 0.304 % 0.645 % 0.012 % 0.021 % 2.317 % TurkuNLP/13B 0.065 % 0.309 % 0.637 % 0.005 % 0.016 % 2.374 %\nE Data distribution by source before and after weighting"
        },
        {
            "heading": "F Full FIN-bench evaluation results",
            "text": "0% 20% 40% 60% FinnishNLP/small\nhatanp/small TurkuNLP/small\nFinnishNLP/medium hatanp/distill\nTurkuNLP/medium FinnishNLP/large\nTurkuNLP/large hatanp/xl\nTurkuNLP/xl TurkuNLP/3B TurkuNLP/8B\nTurkuNLP/13B TurkuNLP/BLUUMI\nBLOOM bigbench_analogies\n0% 10% 20% 30% 40% 50% 60% FinnishNLP/small\nhatanp/small TurkuNLP/small\nFinnishNLP/medium hatanp/distill\nTurkuNLP/medium FinnishNLP/large\nTurkuNLP/large hatanp/xl\nTurkuNLP/xl TurkuNLP/3B TurkuNLP/8B\nTurkuNLP/13B TurkuNLP/BLUUMI\nBLOOM bigbench_arithmetic_1_digit_addition\n0% 20% 40% 60% 80% FinnishNLP/small\nhatanp/small TurkuNLP/small\nFinnishNLP/medium hatanp/distill\nTurkuNLP/medium FinnishNLP/large\nTurkuNLP/large hatanp/xl\nTurkuNLP/xl TurkuNLP/3B TurkuNLP/8B\nTurkuNLP/13B TurkuNLP/BLUUMI\nBLOOM bigbench_arithmetic_1_digit_division\n0% 20% 40% 60% 80% FinnishNLP/small\nhatanp/small TurkuNLP/small\nFinnishNLP/medium hatanp/distill\nTurkuNLP/medium FinnishNLP/large\nTurkuNLP/large hatanp/xl\nTurkuNLP/xl TurkuNLP/3B TurkuNLP/8B\nTurkuNLP/13B TurkuNLP/BLUUMI\nBLOOM bigbench_arithmetic_1_digit_multiplication\n0% 20% 40% 60% 80% FinnishNLP/small\nhatanp/small TurkuNLP/small\nFinnishNLP/medium hatanp/distill\nTurkuNLP/medium FinnishNLP/large\nTurkuNLP/large hatanp/xl\nTurkuNLP/xl TurkuNLP/3B TurkuNLP/8B\nTurkuNLP/13B TurkuNLP/BLUUMI\nBLOOM bigbench_arithmetic_1_digit_subtraction\n0% 10% 20% 30% 40% 50% 60% FinnishNLP/small\nhatanp/small TurkuNLP/small\nFinnishNLP/medium hatanp/distill\nTurkuNLP/medium FinnishNLP/large\nTurkuNLP/large hatanp/xl\nTurkuNLP/xl TurkuNLP/3B TurkuNLP/8B\nTurkuNLP/13B TurkuNLP/BLUUMI\nBLOOM bigbench_arithmetic_2_digit_addition\n0% 20% 40% 60% FinnishNLP/small\nhatanp/small TurkuNLP/small\nFinnishNLP/medium hatanp/distill\nTurkuNLP/medium FinnishNLP/large\nTurkuNLP/large hatanp/xl\nTurkuNLP/xl TurkuNLP/3B TurkuNLP/8B\nTurkuNLP/13B TurkuNLP/BLUUMI\nBLOOM bigbench_arithmetic_2_digit_division\n0% 5% 10% 15% 20% 25% 30% FinnishNLP/small\nhatanp/small TurkuNLP/small\nFinnishNLP/medium hatanp/distill\nTurkuNLP/medium FinnishNLP/large\nTurkuNLP/large hatanp/xl\nTurkuNLP/xl TurkuNLP/3B TurkuNLP/8B\nTurkuNLP/13B TurkuNLP/BLUUMI\nBLOOM bigbench_arithmetic_2_digit_multiplication\n0% 10% 20% 30% 40% 50% 60% FinnishNLP/small\nhatanp/small TurkuNLP/small\nFinnishNLP/medium hatanp/distill\nTurkuNLP/medium FinnishNLP/large\nTurkuNLP/large hatanp/xl\nTurkuNLP/xl TurkuNLP/3B TurkuNLP/8B\nTurkuNLP/13B TurkuNLP/BLUUMI\nBLOOM bigbench_arithmetic_2_digit_subtraction\n0% 10% 20% 30% 40% 50% 60% FinnishNLP/small\nhatanp/small TurkuNLP/small\nFinnishNLP/medium hatanp/distill\nTurkuNLP/medium FinnishNLP/large\nTurkuNLP/large hatanp/xl\nTurkuNLP/xl TurkuNLP/3B TurkuNLP/8B\nTurkuNLP/13B TurkuNLP/BLUUMI\nBLOOM bigbench_arithmetic_3_digit_addition\n0% 10% 20% 30% 40% 50% FinnishNLP/small\nhatanp/small TurkuNLP/small\nFinnishNLP/medium hatanp/distill\nTurkuNLP/medium FinnishNLP/large\nTurkuNLP/large hatanp/xl\nTurkuNLP/xl TurkuNLP/3B TurkuNLP/8B\nTurkuNLP/13B TurkuNLP/BLUUMI\nBLOOM bigbench_arithmetic_3_digit_division\n0% 5% 10% 15% 20% 25% 30% FinnishNLP/small\nhatanp/small TurkuNLP/small\nFinnishNLP/medium hatanp/distill\nTurkuNLP/medium FinnishNLP/large\nTurkuNLP/large hatanp/xl\nTurkuNLP/xl TurkuNLP/3B TurkuNLP/8B\nTurkuNLP/13B TurkuNLP/BLUUMI\nBLOOM bigbench_arithmetic_3_digit_multiplication\n0% 10% 20% 30% 40% 50% 60% FinnishNLP/small\nhatanp/small TurkuNLP/small\nFinnishNLP/medium hatanp/distill\nTurkuNLP/medium FinnishNLP/large\nTurkuNLP/large hatanp/xl\nTurkuNLP/xl TurkuNLP/3B TurkuNLP/8B\nTurkuNLP/13B TurkuNLP/BLUUMI\nBLOOM bigbench_arithmetic_3_digit_subtraction\n0% 10% 20% 30% 40% 50% FinnishNLP/small\nhatanp/small TurkuNLP/small\nFinnishNLP/medium hatanp/distill\nTurkuNLP/medium FinnishNLP/large\nTurkuNLP/large hatanp/xl\nTurkuNLP/xl TurkuNLP/3B TurkuNLP/8B\nTurkuNLP/13B TurkuNLP/BLUUMI\nBLOOM bigbench_arithmetic_4_digit_addition\n0% 10% 20% 30% 40% FinnishNLP/small\nhatanp/small TurkuNLP/small\nFinnishNLP/medium hatanp/distill\nTurkuNLP/medium FinnishNLP/large\nTurkuNLP/large hatanp/xl\nTurkuNLP/xl TurkuNLP/3B TurkuNLP/8B\nTurkuNLP/13B TurkuNLP/BLUUMI\nBLOOM bigbench_arithmetic_4_digit_division\n0% 5% 10% 15% 20% 25% 30% FinnishNLP/small\nhatanp/small TurkuNLP/small\nFinnishNLP/medium hatanp/distill\nTurkuNLP/medium FinnishNLP/large\nTurkuNLP/large hatanp/xl\nTurkuNLP/xl TurkuNLP/3B TurkuNLP/8B\nTurkuNLP/13B TurkuNLP/BLUUMI\nBLOOM bigbench_arithmetic_4_digit_multiplication\n0% 10% 20% 30% 40% 50% FinnishNLP/small\nhatanp/small TurkuNLP/small\nFinnishNLP/medium hatanp/distill\nTurkuNLP/medium FinnishNLP/large\nTurkuNLP/large hatanp/xl\nTurkuNLP/xl TurkuNLP/3B TurkuNLP/8B\nTurkuNLP/13B TurkuNLP/BLUUMI\nBLOOM bigbench_arithmetic_4_digit_subtraction\n0% 10% 20% 30% 40% 50% 60% FinnishNLP/small\nhatanp/small TurkuNLP/small\nFinnishNLP/medium hatanp/distill\nTurkuNLP/medium FinnishNLP/large\nTurkuNLP/large hatanp/xl\nTurkuNLP/xl TurkuNLP/3B TurkuNLP/8B\nTurkuNLP/13B TurkuNLP/BLUUMI\nBLOOM bigbench_arithmetic_5_digit_addition\n0% 10% 20% 30% FinnishNLP/small\nhatanp/small TurkuNLP/small\nFinnishNLP/medium hatanp/distill\nTurkuNLP/medium FinnishNLP/large\nTurkuNLP/large hatanp/xl\nTurkuNLP/xl TurkuNLP/3B TurkuNLP/8B\nTurkuNLP/13B TurkuNLP/BLUUMI\nBLOOM bigbench_arithmetic_5_digit_division\n0% 10% 20% 30% FinnishNLP/small\nhatanp/small TurkuNLP/small\nFinnishNLP/medium hatanp/distill\nTurkuNLP/medium FinnishNLP/large\nTurkuNLP/large hatanp/xl\nTurkuNLP/xl TurkuNLP/3B TurkuNLP/8B\nTurkuNLP/13B TurkuNLP/BLUUMI\nBLOOM bigbench_arithmetic_5_digit_multiplication\n0% 10% 20% 30% 40% 50% 60% FinnishNLP/small\nhatanp/small TurkuNLP/small\nFinnishNLP/medium hatanp/distill\nTurkuNLP/medium FinnishNLP/large\nTurkuNLP/large hatanp/xl\nTurkuNLP/xl TurkuNLP/3B TurkuNLP/8B\nTurkuNLP/13B TurkuNLP/BLUUMI\nBLOOM bigbench_arithmetic_5_digit_subtraction\n0% 10% 20% 30% 40% 50% 60% FinnishNLP/small\nhatanp/small TurkuNLP/small\nFinnishNLP/medium hatanp/distill\nTurkuNLP/medium FinnishNLP/large\nTurkuNLP/large hatanp/xl\nTurkuNLP/xl TurkuNLP/3B TurkuNLP/8B\nTurkuNLP/13B TurkuNLP/BLUUMI\nBLOOM bigbench_cause_and_effect_one_sentence\n0% 20% 40% 60% 80% FinnishNLP/small\nhatanp/small TurkuNLP/small\nFinnishNLP/medium hatanp/distill\nTurkuNLP/medium FinnishNLP/large\nTurkuNLP/large hatanp/xl\nTurkuNLP/xl TurkuNLP/3B TurkuNLP/8B\nTurkuNLP/13B TurkuNLP/BLUUMI\nBLOOM bigbench_cause_and_effect_one_sentence_no_prompt\n0% 10% 20% 30% 40% 50% 60% FinnishNLP/small\nhatanp/small TurkuNLP/small\nFinnishNLP/medium hatanp/distill\nTurkuNLP/medium FinnishNLP/large\nTurkuNLP/large hatanp/xl\nTurkuNLP/xl TurkuNLP/3B TurkuNLP/8B\nTurkuNLP/13B TurkuNLP/BLUUMI\nBLOOM bigbench_cause_and_effect_two_sentences\n0% 10% 20% 30% 40% 50% 60% FinnishNLP/small\nhatanp/small TurkuNLP/small\nFinnishNLP/medium hatanp/distill\nTurkuNLP/medium FinnishNLP/large\nTurkuNLP/large hatanp/xl\nTurkuNLP/xl TurkuNLP/3B TurkuNLP/8B\nTurkuNLP/13B TurkuNLP/BLUUMI\nBLOOM bigbench_emotions\n0% 10% 20% 30% 40% 50% 60% FinnishNLP/small\nhatanp/small TurkuNLP/small\nFinnishNLP/medium hatanp/distill\nTurkuNLP/medium FinnishNLP/large\nTurkuNLP/large hatanp/xl\nTurkuNLP/xl TurkuNLP/3B TurkuNLP/8B\nTurkuNLP/13B TurkuNLP/BLUUMI\nBLOOM bigbench_empirical_judgments\n0% 10% 20% 30% 40% 50% 60% FinnishNLP/small\nhatanp/small TurkuNLP/small\nFinnishNLP/medium hatanp/distill\nTurkuNLP/medium FinnishNLP/large\nTurkuNLP/large hatanp/xl\nTurkuNLP/xl TurkuNLP/3B TurkuNLP/8B\nTurkuNLP/13B TurkuNLP/BLUUMI\nBLOOM bigbench_general_knowledge\n0% 10% 20% 30% 40% 50% FinnishNLP/small\nhatanp/small TurkuNLP/small\nFinnishNLP/medium hatanp/distill\nTurkuNLP/medium FinnishNLP/large\nTurkuNLP/large hatanp/xl\nTurkuNLP/xl TurkuNLP/3B TurkuNLP/8B\nTurkuNLP/13B TurkuNLP/BLUUMI\nBLOOM bigbench_hhh_alignment_harmless\n0% 10% 20% 30% 40% 50% FinnishNLP/small\nhatanp/small TurkuNLP/small\nFinnishNLP/medium hatanp/distill\nTurkuNLP/medium FinnishNLP/large\nTurkuNLP/large hatanp/xl\nTurkuNLP/xl TurkuNLP/3B TurkuNLP/8B\nTurkuNLP/13B TurkuNLP/BLUUMI\nBLOOM bigbench_hhh_alignment_helpful\n0% 10% 20% 30% 40% 50% FinnishNLP/small\nhatanp/small TurkuNLP/small\nFinnishNLP/medium hatanp/distill\nTurkuNLP/medium FinnishNLP/large\nTurkuNLP/large hatanp/xl\nTurkuNLP/xl TurkuNLP/3B TurkuNLP/8B\nTurkuNLP/13B TurkuNLP/BLUUMI\nBLOOM bigbench_hhh_alignment_honest\n0% 10% 20% 30% 40% 50% 60% FinnishNLP/small\nhatanp/small TurkuNLP/small\nFinnishNLP/medium hatanp/distill\nTurkuNLP/medium FinnishNLP/large\nTurkuNLP/large hatanp/xl\nTurkuNLP/xl TurkuNLP/3B TurkuNLP/8B\nTurkuNLP/13B TurkuNLP/BLUUMI\nBLOOM bigbench_hhh_alignment_other\n0% 20% 40% 60% 80% FinnishNLP/small\nhatanp/small TurkuNLP/small\nFinnishNLP/medium hatanp/distill\nTurkuNLP/medium FinnishNLP/large\nTurkuNLP/large hatanp/xl\nTurkuNLP/xl TurkuNLP/3B TurkuNLP/8B\nTurkuNLP/13B TurkuNLP/BLUUMI\nBLOOM bigbench_intent_recognition\n0% 10% 20% 30% 40% 50% FinnishNLP/small\nhatanp/small TurkuNLP/small\nFinnishNLP/medium hatanp/distill\nTurkuNLP/medium FinnishNLP/large\nTurkuNLP/large hatanp/xl\nTurkuNLP/xl TurkuNLP/3B TurkuNLP/8B\nTurkuNLP/13B TurkuNLP/BLUUMI\nBLOOM bigbench_misconceptions\n0% 10% 20% 30% 40% 50% 60% FinnishNLP/small\nhatanp/small TurkuNLP/small\nFinnishNLP/medium hatanp/distill\nTurkuNLP/medium FinnishNLP/large\nTurkuNLP/large hatanp/xl\nTurkuNLP/xl TurkuNLP/3B TurkuNLP/8B\nTurkuNLP/13B TurkuNLP/BLUUMI\nBLOOM bigbench_paraphrase\n0% 10% 20% 30% 40% 50% FinnishNLP/small\nhatanp/small TurkuNLP/small\nFinnishNLP/medium hatanp/distill\nTurkuNLP/medium FinnishNLP/large\nTurkuNLP/large hatanp/xl\nTurkuNLP/xl TurkuNLP/3B TurkuNLP/8B\nTurkuNLP/13B TurkuNLP/BLUUMI\nBLOOM bigbench_sentence_ambiguity\n0% 20% 40% 60% FinnishNLP/small\nhatanp/small TurkuNLP/small\nFinnishNLP/medium hatanp/distill\nTurkuNLP/medium FinnishNLP/large\nTurkuNLP/large hatanp/xl\nTurkuNLP/xl TurkuNLP/3B TurkuNLP/8B\nTurkuNLP/13B TurkuNLP/BLUUMI\nBLOOM bigbench_similarities_abstraction\nFigure 8: 3-shot results of each FIN-bench task + HHH"
        },
        {
            "heading": "G FIN-bench examples",
            "text": ""
        }
    ],
    "title": "FinGPT: Large Generative Models for a Small Language",
    "year": 2023
}