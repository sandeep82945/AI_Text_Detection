{
    "abstractText": "Event Causality Extraction (ECE) aims to extract the cause-effect event pairs from the given text, which requires the model to possess a strong reasoning ability to capture event causalities. However, existing works have not adequately exploited the interactions between the cause and effect event that could provide crucial clues for causality reasoning. To this end, we propose an Implicit Cause-Effect interaction (ICE) framework, which formulates ECE as a template-based conditional generation problem. The proposed method captures the implicit intraand inter-event interactions by incorporating the privileged information (ground truth event types and arguments) for reasoning, and a knowledge distillation mechanism is introduced to alleviate the unavailability of privileged information in the test stage. Furthermore, to facilitate knowledge transfer from teacher to student, we design an event-level alignment strategy named Cause-Effect Optimal Transport (CEOT) to strengthen the semantic interactions of cause-effect event types and arguments. Experimental results indicate that ICE achieves state-of-the-art performance on the ECE-CCKS dataset.",
    "authors": [
        {
            "affiliations": [],
            "name": "Jintao Liu"
        },
        {
            "affiliations": [],
            "name": "Zequn Zhang"
        },
        {
            "affiliations": [],
            "name": "Kaiwen Wei"
        },
        {
            "affiliations": [],
            "name": "Zhi Guo"
        },
        {
            "affiliations": [],
            "name": "Xian Sun"
        },
        {
            "affiliations": [],
            "name": "Li Jin"
        },
        {
            "affiliations": [],
            "name": "Xiaoyu Li"
        }
    ],
    "id": "SP:f7d91dcd8018d5b9aaac8759cca0d9f1d8ac8795",
    "references": [
        {
            "authors": [
                "Jonathan Berant",
                "Vivek Srikumar",
                "Pei-Chun Chen",
                "Abby Vander Linden",
                "Brittany Harding",
                "Brad Huang",
                "Peter Clark",
                "Christopher D. Manning."
            ],
            "title": "Modeling biological processes for reading comprehension",
            "venue": "Proceedings of the 2014 Conference on",
            "year": 2014
        },
        {
            "authors": [
                "Pengfei Cao",
                "Xinyu Zuo",
                "Yubo Chen",
                "Kang Liu",
                "Jun Zhao",
                "Yuguang Chen",
                "Weihua Peng."
            ],
            "title": "Knowledge-enriched event causality identification via latent structure induction networks",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for",
            "year": 2021
        },
        {
            "authors": [
                "Liqun Chen",
                "Guoyin Wang",
                "Chenyang Tao",
                "Dinghan Shen",
                "Pengyu Cheng",
                "Xinyuan Zhang",
                "Wenlin Wang",
                "Yizhe Zhang",
                "Lawrence Carin."
            ],
            "title": "Improving textual network embedding with global attention via optimal transport",
            "venue": "Proceedings of the 57th",
            "year": 2019
        },
        {
            "authors": [
                "Shiyao Cui",
                "Jiawei Sheng",
                "Xin Cong",
                "Quangang Li",
                "Tingwen Liu",
                "Jinqiao Shi."
            ],
            "title": "Event causality extraction with event argument correlations",
            "venue": "Proceedings of the 29th International Conference on Computational Linguistics, COLING 2022,",
            "year": 2022
        },
        {
            "authors": [
                "Xinya Du",
                "Claire Cardie."
            ],
            "title": "Document-level event role filler extraction using multi-granularity contextualized encoding",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020,",
            "year": 2020
        },
        {
            "authors": [
                "Lei Gao",
                "Prafulla Kumar Choubey",
                "Ruihong Huang."
            ],
            "title": "Modeling document-level causal structures for event causal relation identification",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics:",
            "year": 2019
        },
        {
            "authors": [
                "Chikara Hashimoto."
            ],
            "title": "Weakly supervised multilingual causality extraction from wikipedia",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro-",
            "year": 2019
        },
        {
            "authors": [
                "Christopher Hidey",
                "Kathy McKeown."
            ],
            "title": "Identifying causal relations using parallel wikipedia articles",
            "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume",
            "year": 2016
        },
        {
            "authors": [
                "Geoffrey E. Hinton",
                "Oriol Vinyals",
                "Jeffrey Dean."
            ],
            "title": "Distilling the knowledge in a neural network",
            "venue": "CoRR, abs/1503.02531.",
            "year": 2015
        },
        {
            "authors": [
                "Zhichao Hu",
                "Marilyn A. Walker."
            ],
            "title": "Inferring narrative causality between event pairs in films",
            "venue": "Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue, Saarbr\u00fccken, Germany, August 15-17, 2017, pages 342\u2013351. Association for",
            "year": 2017
        },
        {
            "authors": [
                "Xiaoqi Jiao",
                "Yichun Yin",
                "Lifeng Shang",
                "Xin Jiang",
                "Xiao Chen",
                "Linlin Li",
                "Fang Wang",
                "Qun Liu."
            ],
            "title": "Tinybert: Distilling BERT for natural language understanding",
            "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020, Online Event,",
            "year": 2020
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Veselin Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "BART: denoising sequence-to-sequence pre-training for natural language",
            "year": 2020
        },
        {
            "authors": [
                "Jianqiao Li",
                "Chunyuan Li",
                "Guoyin Wang",
                "Hao Fu",
                "YuhChen Lin",
                "Liqun Chen",
                "Yizhe Zhang",
                "Chenyang Tao",
                "Ruiyi Zhang",
                "Wenlin Wang",
                "Dinghan Shen",
                "Qian Yang",
                "Lawrence Carin."
            ],
            "title": "Improving text generation with student-forcing optimal transport",
            "venue": "In",
            "year": 2020
        },
        {
            "authors": [
                "Zhuoran Li",
                "Chunming Hu",
                "Xiaohui Guo",
                "Junfan Chen",
                "Wenyi Qin",
                "Richong Zhang."
            ],
            "title": "An unsupervised multiple-task and multiple-teacher model for cross-lingual named entity recognition",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for",
            "year": 2022
        },
        {
            "authors": [
                "Jian Liu",
                "Yubo Chen",
                "Jun Zhao."
            ],
            "title": "Knowledge enhanced event causality identification with mention masking generalizations",
            "venue": "Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI 2020, pages 3608\u2013",
            "year": 2020
        },
        {
            "authors": [
                "Jintao Liu",
                "Zequn Zhang",
                "Zhi Guo",
                "Li Jin",
                "Xiaoyu Li",
                "Kaiwen Wei",
                "Xian Sun."
            ],
            "title": "KEPT: knowledge enhanced prompt tuning for event causality identification",
            "venue": "Knowl. Based Syst., 259:110064.",
            "year": 2023
        },
        {
            "authors": [
                "Shulin Liu",
                "Yubo Chen",
                "Kang Liu",
                "Jun Zhao."
            ],
            "title": "Exploiting argument information to improve event detection via supervised attention mechanisms",
            "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1:",
            "year": 2017
        },
        {
            "authors": [
                "Xiao Liu",
                "Heyan Huang",
                "Ge Shi",
                "Bo Wang."
            ],
            "title": "Dynamic prefix-tuning for generative template-based event extraction",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin,",
            "year": 2022
        },
        {
            "authors": [
                "Xiao Liu",
                "Yanan Zheng",
                "Zhengxiao Du",
                "Ming Ding",
                "Yujie Qian",
                "Zhilin Yang",
                "Jie Tang."
            ],
            "title": "GPT understands, too",
            "venue": "CoRR, abs/2103.10385.",
            "year": 2021
        },
        {
            "authors": [
                "Giulia Luise",
                "Alessandro Rudi",
                "Massimiliano Pontil",
                "Carlo Ciliberto."
            ],
            "title": "Differential properties of sinkhorn approximation for learning with wasserstein distance",
            "venue": "Advances in Neural Information Processing Systems 31: Annual Conference on Neu-",
            "year": 2018
        },
        {
            "authors": [
                "Yubo Ma",
                "Zehao Wang",
                "Yixin Cao",
                "Mukai Li",
                "Meiqi Chen",
                "Kun Wang",
                "Jing Shao."
            ],
            "title": "Prompt for extraction? PAIE: prompting argument interaction for event argument extraction",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Compu-",
            "year": 2022
        },
        {
            "authors": [
                "Jong-Hoon Oh",
                "Kentaro Torisawa",
                "Canasai Kruengkrai",
                "Ryu Iida",
                "Julien Kloetzer."
            ],
            "title": "Multicolumn convolutional neural networks with causalityattention for why-question answering",
            "venue": "Proceedings of the Tenth ACM International Conference on",
            "year": 2017
        },
        {
            "authors": [
                "Minh Tran Phu",
                "Thien Huu Nguyen."
            ],
            "title": "Graph convolutional networks for event causality identification with rich document-level structures",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Lin-",
            "year": 2021
        },
        {
            "authors": [
                "Charlotte Rudnik",
                "Thibault Ehrhart",
                "Olivier Ferret",
                "Denis Teyssou",
                "Rapha\u00ebl Troncy",
                "Xavier Tannier."
            ],
            "title": "Searching news articles using an event knowledge graph leveraged by wikidata",
            "venue": "Companion proceedings of the 2019 world wide web conference,",
            "year": 2019
        },
        {
            "authors": [
                "Ruslan Salakhutdinov."
            ],
            "title": "Learning deep generative models",
            "venue": "Annual Review of Statistics and Its Application, 2:361\u2013385.",
            "year": 2015
        },
        {
            "authors": [
                "Victor Sanh",
                "Lysandre Debut",
                "Julien Chaumond",
                "Thomas Wolf."
            ],
            "title": "Distilbert, a distilled version of BERT: smaller, faster, cheaper and lighter",
            "venue": "CoRR, abs/1910.01108.",
            "year": 2019
        },
        {
            "authors": [
                "Shirong Shen",
                "Heng Zhou",
                "Tongtong Wu",
                "Guilin Qi."
            ],
            "title": "Event causality identification via derivative prompt joint learning",
            "venue": "Proceedings of the 29th International Conference on Computational Linguistics, COLING 2022, Gyeongju, Republic of Korea,",
            "year": 2022
        },
        {
            "authors": [
                "Xiaozhi Wang",
                "Xu Han",
                "Zhiyuan Liu",
                "Maosong Sun",
                "Peng Li."
            ],
            "title": "Adversarial training for weakly supervised event detection",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics:",
            "year": 2019
        },
        {
            "authors": [
                "Yucheng Wang",
                "Bowen Yu",
                "Yueyang Zhang",
                "Tingwen Liu",
                "Hongsong Zhu",
                "Limin Sun."
            ],
            "title": "Tplinker: Single-stage joint extraction of entities and relations through token pair linking",
            "venue": "Proceedings of the 28th International Conference on Computational Lin-",
            "year": 2020
        },
        {
            "authors": [
                "Kaiwen Wei",
                "Xian Sun",
                "Zequn Zhang",
                "Li Jin",
                "Jingyuan Zhang",
                "Jianwei Lv",
                "Zhi Guo."
            ],
            "title": "Implicit event argument extraction with argument-argument relational knowledge",
            "venue": "IEEE Trans. Knowl. Data Eng., 35(9):8865\u20138879.",
            "year": 2023
        },
        {
            "authors": [
                "Kaiwen Wei",
                "Xian Sun",
                "Zequn Zhang",
                "Jingyuan Zhang",
                "Zhi Guo",
                "Li Jin."
            ],
            "title": "Trigger is not sufficient: Exploiting frame-aware knowledge for implicit event argument extraction",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational",
            "year": 2021
        },
        {
            "authors": [
                "Kaiwen Wei",
                "Yiran Yang",
                "Li Jin",
                "Xian Sun",
                "Zequn Zhang",
                "Jingyuan Zhang",
                "Xiao Li",
                "Linhao Zhang",
                "Jintao Liu",
                "Guo Zhi."
            ],
            "title": "Guide the many-toone assignment: Open information extraction via iou-aware optimal transport",
            "venue": "Proceedings of the",
            "year": 2023
        },
        {
            "authors": [
                "Zhepei Wei",
                "Jianlin Su",
                "Yue Wang",
                "Yuan Tian",
                "Yi Chang."
            ],
            "title": "A novel cascade binary tagging framework for relational triple extraction",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, On-",
            "year": 2020
        },
        {
            "authors": [
                "Scao",
                "Sylvain Gugger",
                "Mariama Drame",
                "Quentin Lhoest",
                "Alexander M. Rush."
            ],
            "title": "Transformers: State-of-the-art natural language processing",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demon-",
            "year": 2020
        },
        {
            "authors": [
                "Chuhan Wu",
                "Fangzhao Wu",
                "Yongfeng Huang."
            ],
            "title": "One teacher is enough? pre-trained language model distillation from multiple teachers",
            "venue": "Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021, Online Event, August 1-6,",
            "year": 2021
        },
        {
            "authors": [
                "Yujia Xie",
                "Xiangfeng Wang",
                "Ruijia Wang",
                "Hongyuan Zha."
            ],
            "title": "A fast proximal point method for computing exact wasserstein distance",
            "venue": "Proceedings of the Thirty-Fifth Conference on Uncertainty in Artificial Intelligence, UAI 2019, Tel Aviv,",
            "year": 2019
        },
        {
            "authors": [
                "Chen Xu",
                "Quan Li",
                "Junfeng Ge",
                "Jinyang Gao",
                "Xiaoyong Yang",
                "Changhua Pei",
                "Fei Sun",
                "Jian Wu",
                "Hanxiao Sun",
                "Wenwu Ou."
            ],
            "title": "Privileged features distillation at taobao recommendations",
            "venue": "Proceedings of the 26th ACM SIGKDD International Conference on",
            "year": 2020
        },
        {
            "authors": [
                "Jingjing Xu",
                "Hao Zhou",
                "Chun Gan",
                "Zaixiang Zheng",
                "Lei Li."
            ],
            "title": "Vocabulary learning via optimal transport for neural machine translation",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International",
            "year": 2021
        },
        {
            "authors": [
                "Hailin Zhang",
                "Defang Chen",
                "Can Wang."
            ],
            "title": "Confidence-aware multi-teacher knowledge distillation",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2022, Virtual and Singapore, 23-27 May 2022, pages 4498\u20134502.",
            "year": 2022
        },
        {
            "authors": [
                "Suncong Zheng",
                "Feng Wang",
                "Hongyun Bao",
                "Yuexing Hao",
                "Peng Zhou",
                "Bo Xu."
            ],
            "title": "Joint extraction of entities and relations based on a novel tagging scheme",
            "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics,",
            "year": 2017
        },
        {
            "authors": [
                "Bo Zhou",
                "Yubo Chen",
                "Kang Liu",
                "Jun Zhao",
                "Jiexin Xu",
                "Xiaojian Jiang",
                "Qiuxia Li."
            ],
            "title": "Generating temporally-ordered event sequences via event optimal transport",
            "venue": "Proceedings of the 29th International Conference on Computational Linguistics,",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Event Causality Extraction (ECE) is an emerging yet challenging natural language processing (NLP) task, which requires extracting the whole event structure of cause and effect events, including event type and event arguments. As shown in Figure 1, given the input text, an ECE system is expected to identify the cause event (i.e., the event type is Price Increase, the argument oil plays the role of Product, the argument worldwide as Region), and similarly identify the corresponding effect event. Recognizing event causality can provide support for many downstream NLP tasks, including machine reading comprehension (Berant et al., 2014),\n*Corresponding author.\nintelligent search (Rudnik et al., 2019), future event forecasting (Hashimoto, 2019), and why-question answering (Oh et al., 2017).\nExisting studies mainly regard this task as a classification-based problem. Du and Cardie (2020) and Wang et al. (2019) extracted the events and classified the causal relation in a pipelined manner. Wang et al. (2020) leveraged grid tagging to simultaneously extract the events and causality pairs. Cui et al. (2022) also designed a dual grid tagging scheme, which aims at modeling the correlations between different event arguments. Compared to the classification-based methods, generationbased models can take full advantage of Pre-trained Language Models (PLMs) by designing flexible prompt templates (Ma et al., 2022; Liu et al., 2022). As a result, there is a trend to cast the extraction task as a conditional generation problem.\nAlthough generation-based approaches have\nachieved remarkable success, the limited interactions between the cause-effect events impede the model\u2019s ability to reason effectively between events. Intuitively, the privileged information (Xu et al., 2020), which stands for the ground truth information of event types or event arguments, could provide valuable knowledge for inferring causal clues. Take Figure 1 as an example, if the model has already known the cause event is Price Increase, and the Product and Region are oil and worldwide in this event, the inter-event interactions could benefit the extraction of effect event. Similarly, if the model has known Price Increase causes Demand Increase during training, the intra-event interactions may help the model to capture event arguments more accurately. By incorporating different kinds of privileged information, the model could make full use of the implicit interactions and be guided to extract causal clues. However, due to the unavailability of such privileged information in practice, incorporating it naively will result in inconsistencies between training and test phases and may affect the model performance. Several methods (Liu et al., 2017; Wei et al., 2021) in other NLP fields have provided solutions to overcome this problem, but they are not applicable when ECE is modeled as a generative problem.\nMoreover, generation-based methods are typically trained via maximum likelihood estimation (MLE) (Salakhutdinov, 2015), which maximizes the likelihood of the next word conditioned on its previous ground truth words. Then, it leverages cross-entropy loss to measure the difference at each position of the target sequence. Nevertheless, since MLE only emphasizes strict word-level alignment, it struggles to consider the semantic information from the perspective of event type or event arguments. For instance, as shown in Figure 1, when training the word new from the effect event argument new energy, MLE ignores new energy is a whole unit as the Industry, which results in a partial loss of semantics. The event type and event arguments from the cause-effect event pairs could also be regarded as different wholes, and by interacting with each other, the model could implicitly incorporate such event-level semantic information.\nIn this paper, we propose an Implicit CauseEffect interaction (ICE) framework for ECE to address the above issues. Specifically, we formulate the ECE task as a template-based conditional generation problem, which takes the context and prompt\ntemplate as the input, and decodes event causality and event structure from the generated sequence. To capture the implicit intra- and inter-event interactions, we feed different privileged information to the input template and train two well-informed teacher models. Then a student model is driven by imitating the behaviors of teachers to narrow the input difference of training and test phases through knowledge distillation (Hinton et al., 2015). Furthermore, to facilitate knowledge transfer and strengthen the interactions between cause and effect events, we design a Cause-Effect Optimal Transport (CEOT) mechanism by treating the event type and event arguments as model units, which could implicitly incorporate the event-level semantic information. In summary, the contributions of this paper are as follows:\n1) This work proposes an ICE framework, which models event causality extraction in a generative paradigm and incorporates privileged knowledge for reasoning.\n2) The proposed method implicitly captures the intra- and inter-event interactions through knowledge distillation, and employs a CEOT strategy to strengthen the semantic interactions of cause and effect events.\n3) Experimental results show that our model achieves state-of-the-art performance, improving the F1-score by 8.39% on the ECE evaluation benchmark."
        },
        {
            "heading": "2 Related Work",
            "text": "Event Causality Extraction. ECE is derived from the previous event causality identification (ECI), which aims to recognize the causal relations between the given events in text (Zuo et al., 2021; Phu and Nguyen, 2021). Early methods mainly focus on syntactic and lexical features (Gao et al., 2019), causality patterns (Hidey and McKeown, 2016), and statistical causal clues (Hu and Walker, 2017). Recent works seek to employ external knowledge (Liu et al., 2020; Cao et al., 2021) or prompt-based models (Shen et al., 2022; Liu et al., 2023) for this task. But these methods only identify the causality of events expressed by a word or phrase, without considering the event type and event arguments. Cui et al. (2022) first proposed the ECE task and exploited the argument correlations to extract event causality and event structure. Some variant methods from relation extraction have also been applied to this task (Wang et al., 2020; Wei et al., 2020).\nHowever, these works fail to model the implicit cause-effect interactions, making it difficult to extract causal clues. Knowledge Distillation. This technique is first proposed by Hinton et al. (2015), which aims to transfer knowledge from a well-trained teacher to a student model. Jiao et al. (2020) and Sanh et al. (2019) used knowledge distillation for compressing large-scale pre-trained language models. Wu et al. (2021) and Li et al. (2022) adopted multipleteacher knowledge distillation to improve the effectiveness of distillation. Wei et al. (2023a) proposed to incorporate related arguments knowledge through knowledge distillation for event argument extraction. Nevertheless, they are designed for classification-based methods and struggle to migrate to the ECE task under the generative pattern. Optimal Transport. OT has a wide range of applications in NLP domains (Chen et al., 2019; Xu et al., 2021; Wei et al., 2023b). Li et al. (2020) proposed using optimal transport to tackle the exposure bias issue in training generative models by maximum likelihood estimation. Zhou et al. (2022) modeled events in the sequence as units and adopted optimal transport to explicitly extract the event semantics for generating temporally-ordered event sequences. In this paper, we employ optimal transport to improve the semantic interactions of event type and event argument for cause and effect events."
        },
        {
            "heading": "3 Methodology",
            "text": "Our ICE framework formulates ECE as a templatebased generation problem and implicitly incorporates privileged information for reasoning. Under this paradigm, we train two well-informed teacher models by incorporating different privileged information into model inputs. Then we adopt a knowledge distillation mechanism to drive a student model to capture implicit cause-effect interactions, which could alleviate the difference of unavailable privileged information in the test stage. During the training phase, a CEOT strategy is adopted to improve the semantic interactions of cause-effect events and promote the training of the student. The overview of ICE is shown in Figure 2."
        },
        {
            "heading": "3.1 Task Formulation",
            "text": "The goal of ECE is to extract event causality and event structure from the text. Formally, given a context, ECE aims to extract a set of cause-effect\nevent pairs {Ecai , Eefi}si=1, where Ecai and Eefi indicate the cause and effect event of the i-th pair, respectively. The event structure E = (t, A) contains the event type t and the argument set A. Each argument in A corresponds to a role."
        },
        {
            "heading": "3.2 Generative Template-based ECE Model",
            "text": "Template Creation. At the input stage, we first construct a specific task-related template for ECE. Following Ma et al. (2022) and Liu et al. (2021), we design a soft template that contains learnable pseudo tokens and slots for all components we require extracting. Figure 2(c) shows the ECE template in our model, where <Cause>, <Effect>, <type>, </type>, etc. are specific learnable pseudo tokens. Then we concatenate the context and template, and feed them into a Transformer-based model to generate the output sequence, where the slots in the template will be filled with concrete event type or event arguments of the cause event and effect event. Target Output Sequence. For the cause-effect event pair in context, we construct the target output sequence Y for conditional generation by filling the ground truth cause event and effect event into the template. Note that when there is more than one argument corresponding to a role, they will be concatenated by a special token <and>. If some roles of the event have no arguments, the corresponding positions in the target output sequence will be filled with <None>. Training. In the training process, we adopt the Transformer-based pre-trained language model BART (Lewis et al., 2020) as our basic model architecture, which consists an encoder and a decoder.\nHe = Encoder(X) Hd = Decoder(Y ;He) (1)\nwhere X denotes the concatenation of context and template. The training target is to maximize the likelihood of the next token conditioned on the previous ground truth tokens in the sequence:\nLgen = \u2212 L\u2211 l=1 log p(Yl|Y<l, X) (2)\nwhere L is the length of the target output sequence. Inference. After obtaining the generated sequence, we decode the event type and event arguments of cause-effect events from corresponding slots with a rule-based matching algorithm. Then we check\nwhether the event type is in the predefined event type set and whether each event argument is a span of the context."
        },
        {
            "heading": "3.3 Teacher-Student Distillation Learning",
            "text": "Based on the generative paradigm, we adopt a teacher-student distillation learning framework to capture the implicit intra- and inter-event interactions, which could implicitly incorporate the privileged information for event causality reasoning and narrow the inconsistencies between training and test phases.\nSpecifically, we train two well-informed teachers, including an event argument extractor and an effect event extractor. The event argument extractor aims to extract event arguments with the event type of cause event and effect event given, and the effect event extractor seeks to recognize event type and event arguments of effect event with knowing the information of cause event. With templatebased generation formulation, we could introduce the privileged information into the model in a flexible way. As shown in Figure 2, for the event argument extractor, we construct a knowledgeenriched template by filling the ground truth event types of the cause and effect event in the raw template. Likewise, for the effect event extractor, we fill in the ground truth event type and arguments of the cause event to form a new knowledgeenriched template. We give an example for the\nconstruction of knowledge-enriched templates in Appendix B. Next, we concatenate the context and the knowledge-enriched templates as inputs to train teachers.\nIn the knowledge distillation stage, we use HSdi to denote the hidden states of the i-th decoder layer of the student model, and HTdi to denote the teacher\u2019s. We adopt the mean squared error (MSE) to encourage the student model to match the hidden states of corresponding layers of the decoder in the teacher:\nLmse = L\u2211 l=1 N\u2211 i=1 MSE(HSdi ,H T di ) (3)\nwhere N is the number of decoder layers. We also employ KL-Divergence to encourage the student to match the probability distribution of the teacher over the next possible word at each position:\nLkl = L\u2211 l=1 KL(pSl , p T l ) (4)\nwhere pSl and p T l are probability distributions of student and teacher over the next possible token at position l. Please note that the teachers and student exploit the same model architecture and training objectives, but do not share model parameters. And the parameters of teachers are fixed during the training of the student. The overall loss for training\nstudent model with a single teacher is:\nLkd = Lgen + \u03b1Lmse + \u03b2Lkl (5)\nwhere \u03b1 and \u03b2 are weight coefficients."
        },
        {
            "heading": "3.4 Cause-Effect Optimal Transport",
            "text": "To improve knowledge transfer, we seek to model semantic information from the perspective of event type or event arguments and introduce a CEOT strategy to promote interactions of cause and effect events, which is achieved by event-level alignment of teacher and student representations.\nOptimal transport defines a distance metric between two probability measures on a domain. Given two discrete probability measures \u00b5 =\u2211n\ni=1 ui\u03b4xi and \u03bd = \u2211m\nj=1 vj\u03b4yj , where \u03b4xi is the Dirac function centered on x, the weights u = {ui}ni=1 \u2208 \u2206n and v = {vj}mj=1 \u2208 \u2206m satisfy the constraints \u2211n i=1 ui = \u2211m j=1 vj = 1. Under this setting, the OT distance is formalized as the following problem (Luise et al., 2018):\nLot(\u00b5,\u03bd) = min M\u2208\u03a0(u,v) n\u2211 i=1 m\u2211 j=1 Mij \u00b7 c(xi,yj)\n= min M\u2208\u03a0(u,v)\n\u27e8M ,C\u27e9\n(6) where \u03a0(u,v) = {M \u2208 Rn\u00d7m+ |M1m = u,M\u22a41n = v}, 1n denotes an n-dimensional all-one vector, C is cost matrix defined as Cij = c(xi,yj), M is the transportation plan, and \u27e8M ,C\u27e9 = Tr(M\u22a4C) denotes the Frobenius inner product. Xie et al. (2019) proposed an approximate algorithm IPOT to solve Eq. 6, which is illustrated in Appendix A. After solving M , we use OT distance as loss to update model parameters.\nSpecifically, the representation of the template for student is denoted as HSe , which is obtained from the last hidden state of the BART encoder of student corresponding to the template. We first partition HSe into n groups, where each group corresponds to the representation of the slot together with specific tokens before and behind it (e.g., <type> TypeOfCause </type> belong to a group). Then we average the representations of tokens in each group to obtain the sequence KSe = {hSei} n i=1, where hei denotes the representation of the i-th group, n is the number of groups. Similarly, we can get the sequence KTe = {hTei} m i=1 by group and average the template representations of teacher.\nWe use cosine distance as the cost function and adopt the IPOT algorithm to compute the OT loss:\nLeot = IPOT(KSe ,KTe ) (7)\nMeanwhile, the last hidden state of the BART decoder for teacher and student is denoted as HTd and HSd . We first use a linear layer followed by an argmax function to decode the output sequence. Then the representations are divided into several groups based on the event type or event argument together with specific tokens before and behind it. Note that when some specific tokens are missing in the output, we remove the corresponding groups. Likewise, we can obtain two sequences KSd and KTd via an average operation and compute the OT loss:\nLdot = IPOT(KSd ,KTd ) (8)\nThe overall loss of the model is defined as:\nLo = Lkd + \u03bbLeot + \u03b7Ldot (9)\nwhere \u03bb and \u03b7 are weight coefficients."
        },
        {
            "heading": "3.5 Training Objectives",
            "text": "Since the two teachers exhibit different difficulties in extracting the same sample, motivated by Zhang et al. (2022), we use adaptive weights to control the importance of different teachers for a specific sample:\nwk = 1\u2212 exp(LkgenT )\u2211 j exp(L j genT )\n(10)\nwhere LkgenT denotes the prediction loss of the kth teacher. The total loss under the multi-teacher knowledge distillation framework is calculated as:\nL = \u2211 k wkLko (11)"
        },
        {
            "heading": "4 Experiments",
            "text": ""
        },
        {
            "heading": "4.1 Experiment Setup",
            "text": "Dataset. Our experiments are conducted on the ECE-CCKS (Cui et al., 2022) dataset, which is\nderived from the corpus released by Tianchi (2021). The dataset is annotated with 39 event types and 3 event roles, and the statistic information is listed in Table 1. Evaluation Metric. We use precision (P), recall (R), and F1-score (F1) as evaluation metrics. A predicted cause-effect event pair is considered correct when the event type and event arguments of the cause event and effect event are correctly extracted. To prove a fair comparison with previous methods (Cui et al., 2022), we also report results on the following two tasks: Event Argument Extraction (EAE), which measures the model\u2019s ability to extract event arguments; Cause-Effect Type extraction (CET), which aims to recognize whether the predicted event types of cause and effect event are correct. Implementation Details. All experiments are conducted on NVIDIA Tesla V100 GPU with Pytorch framework. We use the pre-trained BARTbase from Hugging-Face\u2019s Transformers library (Wolf et al., 2020) as the encoder-decoder language model. Our model is optimized by the AdamW weight decay strategy with a learning rate of 3e-5. The coefficient \u03bb is set to 0.1 and \u03b7 is set to 0.1. We set \u03b1 and \u03b2 to 1e-3 and 0.5. The model is trained for 60 epochs with a batch size of 16."
        },
        {
            "heading": "4.2 Baseline Methods",
            "text": "Classification-based Method. (1) Novel-tagging is introduced to ECE by combining causality, event types, event roles, and argument span into a unified label space (Zheng et al., 2017). (2) CasECE is a pipelined method inspired by Wei et al. (2020),\nwhich first extracts the cause event and then recognizes the effect event conditioned on the former prediction. (3) Pair-linking is a grid tagging method based on Wang et al. (2020), which uses eventtype-level pair linking as conditional information for token-pair linking to extract event arguments. (4) DualCor (Cui et al., 2022) designed a dual grid tagging scheme to capture the argument correlations for ECE. Generation-based Method. Since there is no generative method for ECE in existing studies, we implement the following baselines: (1) BART-ECE achieves event causality extraction with a templatebased conditional generation method in natural language form. (2) Student is the generative model introduced in Section 3.2. (3) MKD employs a multiteacher knowledge distillation framework where each teacher is trained without privileged information. (4) CE-Pipeline is a pipelined generation model, which first extracts the cause event and then predicts the effect event based on the cause. (5) TA-Pipeline is also a pipelined generation model that first identifies the event type of the cause and effect event, conditioned on which to detect the event arguments."
        },
        {
            "heading": "4.3 Overall Performance",
            "text": "The experimental results are reported in Table 2. We can draw the following conclusions: (1) The proposed method achieves the best performance, outperforming the previous state-of-the-art model, DualCor, by 8.90%, 5.46%, and 8.39% on EAE, CET, and ECE in terms of F1-score. The significant improvements demonstrate the effectiveness of our\nmodel. (2) The generation-based methods generally produce better performance than classificationbased methods. This suggests that the generationbased model could make full use of the knowledge in PLMs and exhibit strong advantages on ECE. (3) Compared with Student, our method performs better on the three tasks. We credit the reason to that the student model has difficulty in extracting implicit causal clues, while ICE can equip the model with more implicit event causality reasoning knowledge via a multi-teacher knowledge distillation framework, thus boosting the model performance. (4) With the same model architecture, the performance of ICE exceeds MKD. The improvements indicate that the privileged information could help to train well-informed teachers, which guide the student to capture the intra- and inter-event interactions.(5) CE-Pipeline and TA-Pipeline perform poorly among the generation-based methods. The results illustrate that the pipelined methods suffer from error accumulation problems, while ICE is an end-to-end method with privileged information as the supervision, which could avoid introducing noise or irrelevant information."
        },
        {
            "heading": "4.4 Further Discussion",
            "text": "Ablation Study. To evaluate the contribution of each component, we conduct ablation studies by removing event argument extractor (w/o EAE), effect event extractor (w/o EEE), Cause-Effect Optimal Transport (w/o CEOT), and adaptive weights (w/o WA). The experimental results are shown in Table 3. We observe that: (1) Removing the event argument extractor or effect event extractor will result in performance decay, demonstrating that both extractors are beneficial for event causality extraction. This is because they could capture the implicit intra- and inter-event interactions to perform event causality reasoning, and the knowledge is transferred to the student via a teacher-student\ndistillation framework. (2) The model performance decreases after removing CEOT, which illustrates this strategy could promote the training process by enhancing the event-level semantic information interactions of cause event and effect event. (3) The adaptive weights mechanism can assign different reliability for each teacher according to the characteristics of the samples, further improving the performance of the student. Effect of the Number of Cause-Effect Pairs. To evaluate the effect of different numbers of event causality pairs in a sample, we conduct experiments by dividing the test set into two subsets: Single, which indicates the subset with one pair in a sample; Multiple, which denotes the subset with more than one pairs. The results are reported in Table 4. It can be observed that: (1) ICE achieves the best performance among all the baselines on Single and Multiple subsets, which shows the generalization ability and robustness of ICE. (2) The methods on Multiple generally suffer from weak performance. The reason may be that multiple event causality pairs impose difficulties in capturing implicit causal clues, leading to extremely low recall. (3) The performance gaps between Student and ICE on the two subsets further demonstrate that the knowledge distillation framework could utilize the privileged information to improve the\nevent causality reasoning ability of the model, thus driving a more effective model. Performance in Low-resource Scenarios. To investigate the model performance in low-resource scenarios, we adopt different low proportions of training data to conduct experiments on the three tasks. As shown in Figure 3, it can be observed that: (1) ICE achieves the best performance under different ratios of training data on three tasks. This suggests that our model can extract event causalities and event structure effectively with a small scale of annotated data, which is more practical to use. (2) ICE using a small amount of data can even exceed DualCor using full data, and the performance gap becomes larger with the decrease of training data. These observations indicate that ICE can elicit knowledge in PLMs and has a strong ability to capture causal clues, which is beneficial for the model to perform the ECE task. Effect of the Prompt Template. We study how different types of prompt templates affect the model performance by conducting experiments with the following templates: Concatenation Template, Manual Template, and Soft Template. The creation of the three templates is shown in Appendix C and the results are listed in Table 5. We\nfind that the soft template is superior to the manual template and concatenation template, which illustrates the effectiveness of our template. Although the manual template can elicit pre-training knowledge in a cloze formulation, it is labor-intensive and hard to achieve optimal. However, the soft template can avoid this laborious process and take the best advantage of the PLMs.\nCase Study. We present case studies to further illustrate the performance of the proposed method. As shown in Table 6, for Example1, BART-GEN gives wrong predictions about the event type of the cause event and effect event. The reason may be that BART-ECE has difficulty capturing implicit cause-effect interactions, so it fails to recognize the causality between Demand Drop and Price Drop. For Example2, both methods produce the correct cause-effect event type, while BART-GEN fails to predict the event argument monosilicon of the cause event. We credit the reason to that our model could leverage the event argument extractor and effect event extractor trained with privileged information to guide the training process of the student, thus obtaining better performance in extracting event causality and event structure."
        },
        {
            "heading": "5 Conclusions",
            "text": "In this paper, we propose an Implicit Cause-Effect interaction (ICE) framework to improve the reasoning ability of the model, which tackles ECE in a generative manner. The proposed method incorporates privileged information for reasoning to capture implicit intra- and inter-event interactions, and utilizes a teacher-student learning framework to bridge the gap between training and test stages. Besides, we introduce a Cause-Effect Optimal Transport (CEOT) strategy to improve the event-level semantic interactions of cause and effect events. Experimental results indicate that ICE outperforms all the baselines on the ECE-CCKS dataset, demonstrating the effectiveness of this work.\nLimitations\nThe multi-teacher knowledge distillation mechanism utilized in the ICE framework may increase the computational time during the training process. However, only the student model is leveraged during the test process, and the test time is identical to regular generation-based models. The problem of relatively long training time can be mitigated by strategies such as GPU parallelization. Considering the significant improvement brought by the ICE framework, we believe the cost is acceptable."
        },
        {
            "heading": "Acknowledgements",
            "text": "The research is supported by the National Natural Science Foundation of China under Grant 62206267."
        },
        {
            "heading": "A The details of the IPOT algorithm",
            "text": "The details of the IPOT algorithm is shown in Algorithm 1.\nAlgorithm 1 IPOT algorithm Input: Feature vectors S = {hi}n1 , S \u2032 = {h\u2032j}m1 , Generalized stepsize 1\u03b2 Output: \u27e8M ,C\u27e9\n1: \u03c3 = 1m1m, M (1) = 1n1 \u22a4 m 2: Cij = c(hi,h \u2032 j), Aij = e \u2212 Cij \u03b2 3: for t = 1, 2, 3 \u00b7 \u00b7 \u00b7 do 4: Q = A\u2299M (t) // \u2299 is Hadamart product 5: for k = 1, 2, \u00b7 \u00b7 \u00b7 ,K do //K=1 in practice 6: \u03b4 = 1nQ\u03c3 ,\u03c3 = 1 mQ\u22a4\u03b4 7: end for 8: M (t+1) = diag(\u03b4) Q diag(\u03c3) 9: end for\n10: return \u27e8M ,C\u27e9"
        },
        {
            "heading": "B An example of the construction of knowledge-enriched templates",
            "text": "We show an example of the construction of knowledge-enriched templates for EAE and EEE in Table 7. The knowledge-enriched template for EAE is constructed by filling the ground truth event types of the cause and effect event in the raw template; The knowledge-enriched template for EEE is constructed by filling the ground truth event type and arguments of the cause event in the raw template."
        },
        {
            "heading": "C The creation of different types of templates",
            "text": "We construct three types of templates for comparison: (1) Concatenation Template, where all slots for event type and event arguments are concatenated; (2) Manual Template, where event types and event arguments are integrated into templates in natural language form; (3) Soft Template, which is used in our method. And detailed information of the templates is shown in Table 8."
        },
        {
            "heading": "D Compare with ChatGPT",
            "text": "In this section, we conduct experiments to evaluate the performance of ChatGPT on the ECE task. A well-designed prompt template is as follows:\nSuppose you are now an event causality extraction model. Given a sentence, please give the cause event and result event respectively, where the event contains the event type and the arguments corresponding to each role. The list of event types is: [\u2019Typhoon\u2019, \u2019Demand Increase\u2019, \u2019Price Decrease\u2019, \u2019Cold Wave\u2019, \u2019Price Increase\u2019, \u2019Other Natural Disasters\u2019, \u2019Supply Decrease\u2019, \u2019Supply Increase\u2019, \u2019Sales Decrease\u2019, \u2019Demand Drop\u2019, \u2019Import Decrease\u2019, \u2019Flood\u2019, \u2019Other Trade Frictions\u2019, \u2019Negative Impact\u2019, \u2019Swine Fever\u2019, \u2019Sales Increase\u2019, \u2019Limited Production\u2019, \u2019Operating Costs Increased \u2019, \u2019Other Livestock Epidemics\u2019, \u2019Positive Impact\u2019, \u2019Drought\u2019, \u2019Operating Cost Decrease\u2019, \u2019Export Decrease\u2019, \u2019Frost\u2019, \u2019Other or Unclear\u2019, \u2019Import Increase\u2019, \u2019Bird Flu\u2019, \u2019Earthquake \u2019, \u2019Anti-dumping Against China\u2019, \u2019Exports Increase\u2019, \u2019Add Tariffs to China\u2019, \u2019Decrease in Product Profits\u2019, \u2019Increase in Product Profits\u2019, \u2019Foot-and-mouth Disease of Pigs\u2019, \u2019Anti-dumping Against Other Countries\u2019, \u2019Unsalable\u2019, \u2019Cattle Foot and Mouth Disease\u2019, \u2019Flash Flood\u2019, \u2019Hail\u2019]. The list of event argument roles is: [\u2019Region\u2019, \u2019Industry\u2019, \u2019Product\u2019]. Given a sentence:\"The worldwide rise of oil prices stimulates the demand for new energy such as Ammonia fuel.\", please extract the event type and arguments corresponding to cause and effect event. If no argument corresponds to a role, the argument content returns \"None\". If multiple arguments corresponds to a role, the arguments are connected with \"and\".\nAs shown in Table 9, we can observe that ICE outperforms ChatGPT by a large margin on EAE, CET, and ECE tasks. The results indicate that ChatGPT has difficulty solving such complex event causality extraction tasks without any fine-tuning or training to update parameters."
        }
    ],
    "title": "Event Causality Extraction via Implicit Cause-Effect Interactions",
    "year": 2023
}