{
    "abstractText": "Large Language Models (LLMs) prompted to generate chain-of-thought (CoT) exhibit impressive reasoning capabilities. Recent attempts at prompt decomposition toward solving complex, multi-step reasoning problems depend on the ability of the LLM to simultaneously decompose and solve the problem. A significant disadvantage is that foundational LLMs are typically not available for finetuning, making adaptation computationally prohibitive. We believe (and demonstrate) that problem decomposition and solution generation are distinct capabilites, better addressed in separate modules, than by one monolithic LLM. We introduce DaSLaM, which uses a decomposition generator to decompose complex problems into subproblems that require fewer reasoning steps. These subproblems are answered by a solver. We use a relatively small (13B parameters) LM as the decomposition generator, which we train using policy gradient optimization to interact with a solver LM (regarded as blackbox) and guide it through subproblems, thereby rendering our method solver-agnostic. Evaluation on multiple different reasoning datasets reveal that with our method, a 175 billion parameter LM (text-davinci-003) can produce competitive or even better performance, compared to its orders-of-magnitude larger successor, GPT-4. Additionally, we show that DaSLaM is not limited by the solver\u2019s capabilities as a function of scale; e.g., solver LMs with diverse sizes give significant performance improvement with our solver-agnostic decomposition technique. Exhaustive ablation studies evince the superiority of our modular finetuning technique over exorbitantly large decomposer LLMs, based on prompting alone.",
    "authors": [
        {
            "affiliations": [],
            "name": "Gurusha Juneja"
        },
        {
            "affiliations": [],
            "name": "Subhabrata Dutta"
        },
        {
            "affiliations": [],
            "name": "Soumen Chakrabarti"
        },
        {
            "affiliations": [],
            "name": "Tanmoy Chakraborty"
        }
    ],
    "id": "SP:596da0f11e62d9a3916c116951845506d77c42ac",
    "references": [
        {
            "authors": [
                "Daman Arora",
                "Himanshu Gaurav Singh",
                "Mausam"
            ],
            "title": "Have llms advanced enough? a challenging problem solving benchmark for large language models",
            "year": 2023
        },
        {
            "authors": [
                "Karl Cobbe",
                "Vineet Kosaraju",
                "Mohammad Bavarian",
                "Mark Chen",
                "Heewoo Jun",
                "Lukasz Kaiser",
                "Matthias Plappert",
                "Jerry Tworek",
                "Jacob Hilton",
                "Reiichiro Nakano"
            ],
            "title": "Training verifiers to solve math word problems",
            "venue": "arXiv preprint arXiv:2110.14168",
            "year": 2021
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Payal Dhar."
            ],
            "title": "The carbon impact of artificial intelligence",
            "venue": "Nature Machine Intelligence, 2(8):423\u2013425.",
            "year": 2020
        },
        {
            "authors": [
                "Dheeru Dua",
                "Shivanshu Gupta",
                "Sameer Singh",
                "Matt Gardner."
            ],
            "title": "Successive Prompting for Decomposing Complex Questions",
            "venue": "Empirical Methods in Natural Language Processing (EMNLP).",
            "year": 2022
        },
        {
            "authors": [
                "Evelina Fedorenko",
                "Rosemary A. Varley."
            ],
            "title": "Language and thought are not the same thing: evidence from neuroimaging and neurological patients",
            "venue": "Annals of the New York Academy of Sciences, 1369.",
            "year": 2016
        },
        {
            "authors": [
                "Guhao Feng",
                "Bohang Zhang",
                "Yuntian Gu",
                "Haotian Ye",
                "Di He",
                "Liwei Wang"
            ],
            "title": "Towards revealing the mystery behind chain of thought: a theoretical perspective",
            "year": 2023
        },
        {
            "authors": [
                "Mor Geva",
                "Daniel Khashabi",
                "Elad Segal",
                "Tushar Khot",
                "Dan Roth",
                "Jonathan Berant."
            ],
            "title": "Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies",
            "venue": "Transactions of the Association for Computational Linguistics, 9:346\u2013",
            "year": 2021
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Collin Burns",
                "Saurav Kadavath",
                "Akul Arora",
                "Steven Basart",
                "Eric Tang",
                "Dawn Song",
                "Jacob Steinhardt."
            ],
            "title": "Measuring mathematical problem solving with the math dataset",
            "venue": "NeurIPS.",
            "year": 2021
        },
        {
            "authors": [
                "Edward J Hu",
                "yelong shen",
                "Phillip Wallis",
                "Zeyuan AllenZhu",
                "Yuanzhi Li",
                "Shean Wang",
                "Lu Wang",
                "Weizhu Chen"
            ],
            "title": "LoRA: Low-rank adaptation of large language models",
            "venue": "In International Conference on Learning Representations",
            "year": 2022
        },
        {
            "authors": [
                "Omar Khattab",
                "Keshav Santhanam",
                "Xiang Lisa Li",
                "David Hall",
                "Percy Liang",
                "Christopher Potts",
                "Matei Zaharia."
            ],
            "title": "Demonstrate-searchpredict: Composing retrieval and language models for knowledge-intensive nlp",
            "venue": "arXiv preprint",
            "year": 2022
        },
        {
            "authors": [
                "Omar Khattab",
                "Keshav Santhanam",
                "Xiang Lisa Li",
                "David Hall",
                "Percy Liang",
                "Christopher Potts",
                "Matei Zaharia."
            ],
            "title": "Demonstrate-searchpredict: Composing retrieval and language models for knowledge-intensive NLP",
            "venue": "arXiv preprint",
            "year": 2022
        },
        {
            "authors": [
                "Tushar Khot",
                "Harsh Trivedi",
                "Matthew Finlayson",
                "Yao Fu",
                "Kyle Richardson",
                "Peter Clark",
                "Ashish Sabharwal."
            ],
            "title": "Decomposed prompting: A modular approach for solving complex tasks",
            "venue": "The Eleventh International Conference on Learning Representa-",
            "year": 2023
        },
        {
            "authors": [
                "Wang Ling",
                "Dani Yogatama",
                "Chris Dyer",
                "Phil Blunsom."
            ],
            "title": "Program induction by rationale generation: Learning to solve and explain algebraic word problems",
            "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics",
            "year": 2017
        },
        {
            "authors": [
                "Pan Lu",
                "Swaroop Mishra",
                "Tanglin Xia",
                "Liang Qiu",
                "KaiWei Chang",
                "Song-Chun Zhu",
                "Oyvind Tafjord",
                "Peter Clark",
                "Ashwin Kalyan."
            ],
            "title": "Learn to explain: Multimodal reasoning via thought chains for science question answering",
            "venue": "Advances in Neural Information",
            "year": 2022
        },
        {
            "authors": [
                "Kyle Mahowald",
                "Anna A. Ivanova",
                "Idan A. Blank",
                "Nancy Kanwisher",
                "Joshua B. Tenenbaum",
                "Evelina Fedorenko"
            ],
            "title": "Dissociating language and thought in large language models: a cognitive perspective",
            "year": 2023
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J. Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "J. Mach. Learn. Res., 21(1).",
            "year": 2020
        },
        {
            "authors": [
                "Timo Schick",
                "Jane Dwivedi-Yu",
                "Roberto Dess\u00ec",
                "Roberta Raileanu",
                "Maria Lomeli",
                "Luke Zettlemoyer",
                "Nicola Cancedda",
                "Thomas Scialom."
            ],
            "title": "Toolformer: Language models can teach themselves to use tools",
            "venue": "arXiv preprint arXiv:2302.04761.",
            "year": 2023
        },
        {
            "authors": [
                "Kumar Shridhar",
                "Alessandro Stolfo",
                "Mrinmaya Sachan."
            ],
            "title": "Distilling multi-step reasoning capabilities of large language models into smaller models via semantic decompositions",
            "venue": "arXiv preprint arXiv:2212.00193.",
            "year": 2022
        },
        {
            "authors": [
                "Rohan Taori",
                "Ishaan Gulrajani",
                "Tianyi Zhang",
                "Yann Dubois",
                "Xuechen Li",
                "Carlos Guestrin",
                "Percy Liang",
                "Tatsunori B. Hashimoto."
            ],
            "title": "Stanford alpaca: An instruction-following llama model",
            "venue": "https:// github.com/tatsu-lab/stanford_alpaca.",
            "year": 2023
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "brian ichter",
                "Fei Xia",
                "Ed H. Chi",
                "Quoc V Le",
                "Denny Zhou"
            ],
            "title": "2022. Chain of thought prompting elicits reasoning in large language models",
            "venue": "In Advances in Neural Information Processing Systems",
            "year": 2022
        },
        {
            "authors": [
                "Stephen Wolfram"
            ],
            "title": "ChatGPT gets its \u201cWolfram superpowers\u201d! (blog)",
            "year": 2023
        },
        {
            "authors": [
                "Yunhu Ye",
                "Binyuan Hui",
                "Min Yang",
                "Binhua Li",
                "Fei Huang",
                "Yongbin Li."
            ],
            "title": "Large language models are versatile decomposers: Decompose evidence and questions for table-based reasoning",
            "venue": "arXiv preprint arXiv:2301.13808.",
            "year": 2023
        },
        {
            "authors": [
                "Chuanyang Zheng",
                "Zhengying Liu",
                "Enze Xie",
                "Zhenguo Li",
                "Yu Li"
            ],
            "title": "Progressive-hint prompting improves reasoning in large language models",
            "year": 2023
        },
        {
            "authors": [
                "Denny Zhou",
                "Nathanael Sch\u00e4rli",
                "Le Hou",
                "Jason Wei",
                "Nathan Scales",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Claire Cui",
                "Olivier Bousquet",
                "Quoc V Le",
                "Ed H. Chi."
            ],
            "title": "Least-to-most prompting enables complex reasoning in large language models",
            "venue": "The",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "In recent years, an astounding variety of text and NLP tasks have been accomplished by language\n\u2217Equal contribution as first author. \u2020Correspondence: tanchak@iitd.ac.in\nmodels (LMs) (Devlin et al., 2019) \u2014 in essence, fitting continuous feature vectors to tokens and modeling smooth conditional distributions over thousands of token positions with multi-task objectives. The next generation of large LMs (LLMs) such as T5, GPT4 and Bard (Raffel et al., 2020; OpenAI, 2023) developed protean capabilities, extending to mathematical and logical ability, based on prompting and in-context learning. Chain-ofthought (CoT) prompting has been a key enabler (Wei et al., 2022; Feng et al., 2023). LLMs can solve middle-school word problems and equations reasonably well. It has also acquired the ability to invoke specialized external tools such as Wolfram Alpha (Wolfram, 2023; Schick et al., 2023).\nRecent advances in LLMs have arisen largely from cleverly-engineered, ever-growing training data, rather than any significant change in network structure, which remains relatively regular, but with rapidly increasing network size and number of parameters. One outcome of such giant monolithic LLMs is unsustainable levels of hardware and energy (Dhar, 2020) to train them. Meanwhile, neurologists and brain scientists have known, via fMRI scans, inter alia, that cerebral functions are specialized and spatially localized (Fedorenko and Varley, 2016; Mahowald et al., 2023).\nMany recent complex reasoning challenges thrown at LLMs have a two-level character \u2013 the input task needs to be decomposed into subtasks, then the subtasks need to be solved, and finally, subtask solutions have to be consolidated and combined to solve the input task. Existing approaches use the same LLM to both decompose and solve the task, sometimes in tangled and uninterpretable ways. Because the sharing of an LLM across these functions cannot be closely controlled, very large models are needed for this double ability (decompose and solve) to emerge.\nStaying entirely inside the LLM regime, and avoiding the possibility of specialized tools, we\net al., 2023). In this example, the solver LM is text-davinci-003. In step 1 , the solver is prompted to answer the question (blue textbox) and it fails to answer correctly (red textbox). A problem decomposing LM generates subproblems (violet textboxes) conditioned on the original question and the initial response of the solver in step 2 . In step 3 , the solver answers these subproblems iteratively and appends to the prompt. Finally, the original problem is appended to the prompt in step 4 , and the solver answers it correctly (green textbox).\nask a simple question \u2013 is it possible to offload the ability of problem decomposition to a dedicated, relatively smaller scale model, which is specialized and can act in synergy with any solver model of choice? To incorporate flexibility and better generalization, an immediate requirement of such a setup would be to enable a model-agnostic communication between the decomposer and the solver.\nOur contributions. To study this research question, we develop DaSLaM (Decomposition And Solution LAnguage Models), in which we separate the decomposer from the solver, as shown in Figure 1. The solver LM can be conventionally trained or fine-tuned. In the illustration, when it answers a question incorrectly, the decomposer LM takes over to produce sub-questions. The partial solutions are appended and resubmitted to the solver LM, which solves the question correctly. The decomposer LM regards the solver as a black box, and uses reinforcement learning (RL) to become a specialized expert at decomposition, informed by the solver\u2019s mistakes.\nExtensive experiments with three reasoning datasets (MATH, AQuA, and JEEBench) show that the proposed specialization improves the perfor-\nmance of OpenAI GPT-3 text-davinci-003 to outperform GPT-3.5 and even begins to compete with GPT-4, outperforming other similar methods. DaSLaM boosts text-davinci-003 from an exact match accuracy of 41.6 to 54.5 in zero-shot regime, which is 3.9 points higher than few-shot GPT-4. Similarly, on Physics problems from JEEBench dataset, DaSLaM-augmented text-davinci-003 scores only 0.58 points short of GPT-4 while outperforming GPT-3.5. The decomposer LM in DaSLaM reduces decomposition errors, and generalizes well across diverse small-scale LMs. It is also more robust in the face of difficult datasets, where the solver gives near-random performance.\nThese results support our founding hypothesis that heterogeneous functional specialization improves model efficiency and robustness of LLMs. A crucial findings from our experiments is that finetuning the decomposer is much more powerful choice than finetuning the solver. Moreover, a finetuned decomposer is largely superior compared to an orders of magnitude larger LLM prompted to act as a decomposer. Given the prohibitive cost of finetuning LLMs like GPT 3, 3.5, or 4, we hope this method would provide us a promising direction to-\nwards future development of task-expert models.1"
        },
        {
            "heading": "2 Related Work",
            "text": "Eliciting superior reasoning abilities in LM through specially designed prompts has found its popularity through CoT prompting (Wei et al., 2022) \u2013 asking the LM to explain the reasoning steps improves overall performance. Decomposing a complex reasoning requirement into multiple, simple steps results in superior reasoning capabilities across modalities other than free-form natural language text as well, e.g., reasoning over tabular data (Ye et al., 2023), visual question answering (Lu et al., 2022), etc. These methods generally solicit a single run of LM inference with no intermediate prompting interactions. Consequently, the LM often misses key reasoning steps or hallucinates irrelevant ones.\nOn the other hand, a prototypical prompter, sequentially interacting with the LM, has shown impressive performance. Progressive Hint Prompting (Zheng et al., 2023) uses such a setting; first, the LM is asked to provide a base answer. The prompt then uses the answer as a hint to the LM that progressively guides it to the final answer. Zhou et al. (2023) followed a similar direction by breaking down the problem itself. Their method, Least-tomost prompting, asks the LM to generate simpler, related problems from a complex problem. The final solution to the original question is generated by the LM conditioned upon the solution of the subproblems. A major bottleneck then becomes the solver\u2019s ability to identify the critical subproblems. Decomposing a complex task and then solving each task via multiple LLMs with their own in-context examples have been attempted as well (Dua et al., 2022; Khot et al., 2023). Recently, Shridhar et al. (2022) explored subquestion generation from complex questions as means of distilling reasoning abilities from larger LMs to smaller ones.\nOur proposed method, DaSLaM makes a departure from these mentioned approaches in three particular features: (i) we seek to separate out the decomposer from the solver to get rid of the solver\u2019s limitations affecting decomposition, (ii) the decomposer acts as a plug-and-play module that can generalize to any solver, and (iii) the decomposition actuates with complete knowledge of the solver\u2019s actions.\n1The codebase is given at: https://github.com/ LCS2-IIITD/DaSLaM\n3 A General Overview of DaSLaM\nGiven an LM \u03b8 and a question Q as a sequence of tokens, a standard zero-shot prompting can be described as,\nA\u0302 = argmax A\u2208A\np\u03b8(A|Q)\nwhere A\u0302 is the inferred answer, and A is the set of possible answers (e.g., numerical values, multiplechoice options, True/False, etc.). With a CoT prompting, the LM generates a sequence of tokens explaining the steps S to reach the answer given the question. The modified process can be described as,\nA\u0302 = argmax A\u2208A\n[ p\u03b8(A|S,Q) argmax\nS p\u03b8(S|Q)\n] (1)\nWhen answering Q requires multistep reasoning, one can conceptualize S as a sequence of smaller steps {S\u20321, S\u20322, \u00b7 \u00b7 \u00b7 , S\u2032n} such that the LM iteratively answers a sequence of subproblems {Q\u20321, \u00b7 \u00b7 \u00b7 , Q\u2032n} to finally reach the desired answer to the original question. Eq. 1 can then be rewritten as,\nA\u0302 = argmax A\u2032n\u2208A \u220f i p\u03b8(A \u2032 i|S\u2032i, Q\u2032i) argmax S\u2032i p\u03b8(S \u2032 i|Q\u2032i) (2) where A\u2032i is the answer to the subproblem Q \u2032 i. In the usual regime of CoT, the subproblems Q\u2032i are implicit; the LM discovers them on its own and generates the reasoning steps and the answers. Due to the repeated multiplication in Eq. 2, any error in the initial stages quickly propagates along the chain of steps.\nIn DaSLaM, we seek to alleviate this problem by offloading the task of inferring the subproblems {Q\u2032i} to a decomposer LM \u03d5. For a more guided problem decomposition, DaSLaM uses the answer and the steps generated by the solver LM, \u03b8 in the naive CoT regime as described in Eq. 1 to generate a set of subproblems {Q\u0302i}i=1,...,n as follows:\nQ\u0302i = argmax Q\u2032i\np\u03d5(Q \u2032 i|{Q\u0302j : j \u2208 [1, i\u2212 1]},\nQ, A\u03020, S0), (3)\nfor i \u2208 [1, n], where A\u03020 and S0 are the initial answer and reasoning steps, respectively generated by \u03b8. The solver LM \u03b8 then solves the subproblem set one-by-one similar to Eq. 2. However, instead of seeking to generate the final answer as a response to the last subproblem Q\u0302n, we append the original question at the end and let \u03b8 answer it directly given the context generated by the subproblems, their answers, and the corresponding CoTs. The four stages of workflow with DaSLaM, as described\nin Figure 1, can then be summarized as follows:\nA\u03020 = argmax A\u2208A\n[ p\u03b8(A|S0, Q) argmax\nS0\np\u03b8(S0|Q) ]\nQ\u0302i = argmax Q\u2032i\np\u03d5(Q \u2032 i|{Q\u0302j}j\u2208{1,i\u22121}, Q, A\u03020, S0)\nA\u0302i = argmax Ai\u2208A\n[ p\u03b8(Ai|Si, Q\u0302i) argmax\nSi\np\u03b8(Si|Q\u0302i) ]\nA\u0302 = argmax A\u2208A\n[ p\u03b8(A|S,Q)\nargmax S p\u03b8(S|{A\u0302i, Si, Q\u0302i}i\u2208{1,N}, Q) ] (4)"
        },
        {
            "heading": "4 Learning to Decompose from Feedback",
            "text": "Typical LMs are not suitable to reliably perform the problem decomposition stage in Eq. 3. We seek to finetune the decomposer LM, \u03d5 for this task. Specifically, we use the LLaMA 13 billion model. Instead of full LM-tuning, we use LoRA adapters (Hu et al., 2022) for parameter-efficient training. For brevity, we will denote the adapteraugmented LM as \u03d5, although only the adapter weights are being updated while training. The whole process of teaching the LM to perform the problem decomposition comprises two successive stages: (i) subproblem construction from the original question and CoT, and (ii) policy optimization in conjunction with the solver LM. Details of the supervised data curation required for these three steps are described in Section 5. Each data sample in the supervised dataset D can be conceptualized as a sequence of triplets \u27e8Qgold, Sgold, Agold,Q\u2032gold\u27e9, where Qgold denotes the original question, Sgold denotes reasoning steps in form of CoT, Agold denotes the answer to the question, and Q\u2032gold is a sequence of subproblems generated by decomposing Qgold (see Section 5 and Appendix A for further details on supervised data curation process).\nThe first stage is straightforward with the decomposer LM being finetuned using language modeling objective. In the first stage, we seek to optimize the following objective:\nmin \u03d5\n[\u2212 log(p\u03d5(Q\u2032gold|Q,S))] (5)\nThis step is somewhat similar to instruction tuning, where an LM is asked to generate some text conditioned on a context, instead of following the usual sentence completion-style behavior of the LM. Intuitively, the role of this stage is to align the LM to the task of the decomposer.\nDecomposition via policy network. The previous stage inculcates the ability to decompose a\nproblem within \u03d5. However, it is still blind to the actual errors made by the solver LM. In the next stage, we seek to make the decomposer LM work in synergy with any solver LM of choice. This solver agnosticism restrains \u03d5 to observe the internal representations computed by \u03b8 while solving the problem. To handle this imposed blackbox characteristics of the solver, we resort to policy gradient optimization of \u03d5 assuming \u03b8 to be part of the environment. We formalize the setup as follows.\nElements of the environment: The solver LM \u03b8 constitutes the core component of the environment. Given a question Q, the solver-generated CoT S and the answer A as sequences of tokens define the observation of the policy.\nState and action space: We define the state space as S. The initial state, s0 \u2208 S, is defined by the original question Q and the initial response from the solver, S0, A0, that are provided to the decomposer LM \u03d5 as input. A single timestep is defined on generation of a single token, i.e., the action at at time t. Given the autoregressive nature of LM, we define the state at t-th timestep as st = (st\u22121, {at\u22121}), i.e., the token generated at t \u2212 1 appended to the tokens generated till t \u2212 1. Trivially, the action space is the same as V , the vocabulary of \u03d5.\nPolicy: The decomposer LM is conceptualized as a policy network \u03c0\u03d5 : S \u2212\u2192 V , i.e., it generates a token given the inputs and the token generated hitherto, till the end of episode T . Inspired by the recent success in Reinforcement Learning from Human Feedback (RLHF) with autoregressive LMs, we choose the Proximal Policy Optimization (PPO) algorithm to train the policy \u03c0\u03d5(a|s). In a typical PPO setup, we define the advantage function as follows:\nG\u0302t = T\u2212t+1\u2211 i=0 (\u03b3\u03bb)i [rt+i + \u03b3V (st+i+1)\u2212 V (st+i)] (6)\nwhere rt is the reward at step t, V (st) : S \u2212\u2192 R is the value function determining the reward associated to state st, and \u03b3, \u03bb are hyperparameters. We use the policy model augmented with a randomly initialized feedforward layer as the value function. A crucial component in on-policy learning regime is the reward function rt. While the end goal of the learning is to have the solver answering correctly, the decomposer LM should receive some incremental signal aligned to its generation as well for it to stably converge to the optimal policy. With this\ngoal in mind, we construct the reward function as, rt = R1 +R2 +R3 +R4 +R5 (7)\nwhere R1 to R5 are defined as follows (see Appendix B for a detailed treatment on the reward computation method). Here cos-sim represents cosine similarity. I(x) = 1 if x is true is the indicator function. Entity coverage: R1 =\n|EQ\u2032 | |EQ| , where EQ\u2032 and EQ\nare the sets of distinct entities in the generated subproblems and the original question, respectively.\nConsistency of answers to subproblems: R2 = \u2211 i ( I(ei = e\u0302i) + cos-sim(Q \u2032 i, Ai) ) (8)\nwhere e\u0302i is the entity whose value has been asked in the subproblem Q\u2032i, and ei is the entity answered. This reward penalizes the decomposer LM for generating questions whose answers are not consistent. Order of operations: R3 = lm , where l is the number of operations matched in order between S and Sgold, and m is the total number of operations in Sgold. CoT proximity: To ensure that the distance of reasoning produced by the model after prompting S to the gold reasoning Sgold is less than the distance of reasoning produced without prompt S0 to the gold reasoning steps Sgold, we design a reward based on the cosine similarity of each step of Sgold. We break S and S0 at new-line token to form reasoning steps. At each step j, we compute c1j = cos-sim(Sj , S j gold) and\nc2j = cos-sim(S j 0, S j gold). The reward is\nR4 = m\u2211 j=0 I(c1j > c2j)c1j + I(c2j > c1j)(\u22121\u2212 c2j),\n(9)\nCorrectness of final answer: R5=I(A\u0302=Agold). Now, we can define the PPO objective as follows:\nmax \u03d5\n( Et [ \u03c0\u03d5(at|st) \u03c0ref(at|st) G\u0302t ] \u2212 \u03b2Et [Kt] ) (10)\nwhere \u03c0ref is the reference model that is initialized with supervised finetuned \u03d5. Kt = KL[\u03c0ref(\u00b7|st), \u03c0\u03d5(\u00b7|st)] is the KL-divergence between the reference model and the policy model.\nThe resulting decomposer LM \u03d5 optimized using the above mentioned three stages of finetuning can then be used with DaSLaM."
        },
        {
            "heading": "5 Experiments",
            "text": "Training data curation. The training process of DaSLaM consists of two stages as mentioned previ-\nously. In the first stage, we require the subproblems along with the reasoning steps for a given problem. We use samples from four existing datasets \u2014 MATH (Hendrycks et al., 2021), AQuA (Ling et al., 2017), GSM8K (Cobbe et al., 2021), and StrategyQA (Geva et al., 2021). Each question in these four datasets contains a question Qgold, a step-bystep illustration of the reasoning process Sgold, and the final answer Agold. We sample 7, 000 examples from the training splits of these datasets and employ OpenAI\u2019s text-davinci-003 model to generate the corresponding subquestions. We provide the model with one-shot example illustrating how to decompose a question into subquestions based on the reasoning. In the second stage of training, we utilize the remaining training data from MATH and AQuA datasets to conduct the policy optimization since this step does not require any supervised examples of subproblems.\nLMs used. We use LLaMA 13 billion (Touvron et al., 2023) as the decomposer LM. For the solver LM, we primarily use text-davinci-003 (henceforth, we denote it as GPT-3.5 for brevity). We also experiment with the LLaMA 13 bilion and LLaMA 33 billion models as solvers to test the model-agnostic generalizability of DaSLaM.\nBaselines. We compare DaSLaM with four existing methods of prompting: Chain-of-thought prompting (CoT) (Wei et al., 2022), Least-to-most prompting (L2M) (Zhou et al., 2023), Progressive Hint Prompting (PHP) (Zheng et al., 2023), and, Demonstrate-Search-Predict (DSP) (Khattab et al., 2022a). The original setting of PHP requires an 8-shot prompting; however, since all other methods including DaSLaM predict in the zero-shot setting, we use PHP in 1-shot for a fairer comparison. Additionally, we experiment with three ablation variants: DaSLaM-NF does not take the solver feedback into account while generating the subproblems; Finetuned is the solver LM (LLaMA 13B in this case, we could not finetune 33B variant due to computational constraints) finetuned without any decomposer; GPT-3.5 decomposer does away with the finetuned LLaMA 13B decomposer and uses pretrained GPT-3.5 as the prompted decomposer.\nTest datasets. For evaluation purposes, we use three datasets \u2013 MATH (Hendrycks et al., 2021), AQuA (Ling et al., 2017), and JEEBench (Arora et al., 2023). For the first two datasets, only the test splits are used during evaluation since their\ntraining splits are used while finetuning the decomposer. The MATH dataset contains mathematical problems on multiple different domains. We report the results on each of them separately and use the following abbreviations \u2013 ALG, I-ALG, and PALG for Algebra, Intermediate Algebra, and PreAlgebra, respectively; Calc for Calculus, Geom for Geometry, PnC for Probability and Combinatorics, NT for Number theory. From the JEEBench dataset, we use the problems in Physics (Phy) and Mathematics (Math). Each of these two subjects has three types of problems \u2013 single-answer multiple-choice questions (MCQ), numerical problems (Num), and multi-answer multiple-choice questions (Multi). For all these datasets, we use exact match criteria to evaluate the correctness of the model-inferred answers. Details of training and inference hyperparameters and compute resource usage are provided in Appendix C."
        },
        {
            "heading": "6 Experimental Results",
            "text": "The tasks used to evaluate the performance of DaSLaM contain questions that can be answered either of the three types \u2013 numerical, single correct answer MCQ, and multiple correct answer MCQ. DaSLaM is better than pure prompting We start with DaSLaM augmented with GPT-3.5 as the solver LM on MATH and AQuA datasets (see Table 1). The improvement achieved with DaSLaM prompting compared to standard CoT is staggering across all types of problems in the MATH dataset: +11.7 on Pre-Algebra, +8.4 on Intermediate Algebra, +7.7 on Number Theory, +7.2 on Geometry, +5.0 on Probability and Combinatorics, +5.8 on Algebra, and +4.2 on Calculus. The absolute improvement is even larger on the AQuA dataset, i.e., +12.9 over CoT. It is noticeable that the effects of DaSLaM are\nstronger across tasks containing algebraic reasoning (AQuA, Pre- and Intermediate-Algebra, etc.) compared to Probability and Combinatorics or Calculus, which require more implicit knowledge. The performance gain achieved via DaSLaM is significantly better compared to methods like L2M or PHP. The latter methods often fail to improve over standard CoT (e.g., on Probability and combinatorics, Number Theory, and Algebra problems, L2M shows a drop in accuracy). Even when improving over CoT, their improvement is meager compared to DaSLaM. This trend entails our earlier argument in support of offloading the problem decomposition task to a specialized LM; methods that prompt the solver LM to decompose the problem lack the expertise achieved via dedicated finetuning in DaSLaM.\nFinetuned decomposer is essential. Despite being orders of magnitude smaller, a finetuned LLaMA 13B model delivers better performance compared to GPT-3.5 as a decomposer (DaSLaM vs. GPT-3.5 generator in Table 1 and 2). This further justifies our choice of separately finetuning the decomposer and the added flexibility that it offers. In fact, finetuning the decomposer is far effective compared to finetuning the solver (DaSLaM vs Finetuned solver in Table 2).\nFeedback from the solver is important. In the preceding paragraph, we attributed the superiority of DaSLaM over other methods to the usage of a specialized LM for problem decomposition. However, manipulating the problem decomposition upon feedback from the solver is also an important factor here. None of the existing methods does so, and therefore, remains blind towards what reasoning (and possible errors) is followed by the solver model. This is further manifested when we\nMethod DatasetPnC NT ALG iALG Geom Cal Palg AQuA LLaMA 13 billion CoT 2.05 4.0 3.12 2.4 3.2 2.08 5.0 17.7 L2M 1.66 3.2 3.33 2.8 2.0 3.33 4.54 16.6 Finetuned 2.8 3.6 3.57 3.2 4.1 3.05 6.04 19.4 GPT3.5 Decomposer 2.05 5.0 4.68 2.8 2.08 4.0 6.66 20.4 DaSLaM-NF 2.93 4.8 4.68 3.2 4.0 3.9 6.2 21.6 DaSLaM 4.0 5.6 4.70 3.4 4.3 4.1 8.33 22.0 LLaMA 33 billion CoT 2.4 4.16 4.54 3.7 4.0 4.0 5.2 20.0 L2M 2.38 4.16 4.2 6.0 4.25 5.71 5.55 21.6 DaSLaM-NF 3.2 5.83 5.6 5.6 5.1 5.71 5.2 22.5 DaSLaM 4.0 7.36 9.09 6.02 5.3 6.03 8.44 26.8\nTable 2: Performance on MATH and AQuA with LLaMA 13 billion and LLaMA 33 billion as solvers. PHP is not reported as one-shot PHP generated randomly with both LLaMA variants. DaSLaM provides consistent improvement across all the tasks while other baseline methods mostly fail.\nMethod DatasetPhy MCQ Math MCQ Phy Multi Math Multi Phy Num Math Num Phy Int Math Int CoT 33.33 21.9 6.25 12.0 3.03 1.69 12.5 20.0 PHP 22.22 17.07 6.25 7.59 3.03 1.69 0* 4.0 L2M 22.22 21.9 6.25 12.5 3.03 3.38 10.0 20.0 DaSLaM-NF 20.8 31.7 7.5 10.12 3.03 3.38 12.5 16.0 DaSLaM 55.55 36.5 18.75 16.0 6.06 10.16 22.5 24.0 GPT-4 55.55 34.14 27.5 21.5 15.15 11.8 25.0 20.0\nTable 3: Performance comparison on the JEE benchmark dataset with GPT-3.5 as the solver LM. 0* signifies that the model was not able to answer any problem in the task correctly.\ncompare DaSLaM with itself without the feedback module, DaSLaM-NF. While DaSLaM-NF is able to improve upon basic CoT and other prompting methods, it falls short of a decomposer LM that has access to the initial response of the solver.\nDaSLaM generalizes to smaller solvers. An important aspect of a prompting method is its ability to work with LMs of different scales. Despite being finetuned with GPT-3.5 responses only, DaSLaM is able to improve upon the base performance of smaller scale LLaMA models as well (see Table 2). L2M prompting generally fails with both LLaMA 13 billion and 33 billion variants. On the other hand, DaSLaM, with or without feedback, almost doubles the performance of the base CoT across multiple tasks of the MATH dataset. It shows substantial improvement on AQuA as well. The importance of feedback from the solver LM usually manifests strongly in proportion to the scale of the solver.\nDaSLaM generalizes to harder problems. Since the decomposer LM \u03d5 is trained using a subset of the training data of MATH and AQuA, we opt for a harder (in terms of benchmark performance of different LMs) reasoning evaluation on the JEEBench dataset. Table 3 summarizes the performance of the baselines and DaSLaM with GPT-3.5 as the solver LM on Mathematics and Physics questions of the JEEBench dataset. We notice that the superiority of DaSLaM manifests even more profoundly on this task compared to the former ones. Both PHP and L2M prompting absolutely fail to improve upon basic CoT prompting, often with a sharp fall in performance (e.g., Physics MCQ questions). On the other hand, DaSLaM boosts the LMs performance, very often over 100% relative improvement (all three types of problems in Physics and numerical problems in Mathematics). Aggregated across question types, DaSLaM boosts the performance of GPT-3.5 to 22.420 in Physics and 22.07 in Mathematics. It is noteworthy that the same LM in its base setting performs near-random, i.e., 10.4 and 10.7 in Physics and Mathematics, respectively, whereas a random selection baseline gives scores of 9.6 and 10.3, respectively (Arora et al., 2023). Furthermore, GPT-3.5 with DaSLaM outperforms a better optimized candidate of the GPT series, GPT3.5 on both these subjects (note that Arora et al. (2023) reported 18.9 and 15.7 scores with GPT-3.5 on Physics and Mathematics, respectively). Comparison with GPT-4. The colossal compute used by GPT-4 makes the comparison with any of its predecessors like GPT-3.5 quite unfair. However, it is tempting to observe that DaSLaM boosts the performance of GPT-3.5 often to the level of GPT4. For example, on arithmetic problems of the AQuA dataset, DaSLaM surprisingly outperforms both zero-shot and few-shot GPT-4 (40.6 and 50.4 respectively, compared to 54.5 with GPT-3.5 and DaSLaM). On MATH dataset, DaSLaM augmented GPT-3.5 scores an aggregate of 30.23, which is better than ChatGPT (26.4) and close to GPT-4 (35.7). On JEEBench Mathematics problems, GPT-\n4 comes up with an aggregate score of 23.1, which is pretty close to our 22.42. In Physics and Math MCQ questions, DaSLaM with GPT-3.5 outperforms GPT-4. These results definitely do not claim any assumed superiority of DaSLaM-boosted GPT-3.5 over GPT-4 since there are multiple other cases that state otherwise. Instead, we seek to demonstrate how much leftover potential these LMs possess that can be unleashed via our proposed method of feedback-guided automatic problem decomposition."
        },
        {
            "heading": "7 Case Study",
            "text": "To this point, we have compared the numbers produced by DaSLaM-boosted models across different datasets. While they provide an overall assessment, deeper analyses are needed to comprehend the actual reasoning steps adopted by these different methods. Figure 2 shows the reasoning steps generated by GPT-3.5 given an example problem from the MATH dataset with three different prompting methods \u2013 vanilla CoT, L2M, and DaSLaM. Note that DaSLaM uses the CoT output to decompose the problem.\nBoth CoT and L2M end up with the model an-\nswering incorrectly. With CoT, the solver wrongly assumes that the given equation must have two real roots though it should not have any real roots. Also, it mistakes the value of a2 as a. The effect is prominent in the subproblems generated by DaSLaM as it asks to find the value of a explicitly. Furthermore, the solver LM specifically announces that y \u2264 0 to answer the first subproblem generated by DaSLaM. This helps to correct the reasoning about the sign of the discriminant.\nWith L2M, the confusion around the value of a and a2 persists, as the solver LM substitutes a in the given equation by 49 (which is the value of a2) twice in the answering process. Although it substituted the correct value of a once while answering the second question, it is not explicitly declared like in DaSLaM. We observe multiple similar failure cases with L2M. It is quite likely that prompting the model to generate the final answer after each subproblem accumulates the erroneous reasoning steps that the model falls prey to.\nWith DaSLaM, the reasoning steps followed by the solver remains robust throughout. It reaches the final answer much earlier (third and fourth subproblems). In the final answer, the solver simply\nreiterates the steps that it earlier generated to answer the subproblems. This is a common behavior that we observed across multiple problems from multiple datasets. In Appendix D (see Figures 3 and 4), we provide similar case studies on LLaMA 13B and 33B with different prompting methods. With reduced solver capacity, the difference between Least-to-most and CoT generated reasoning steps further diminishes with both leading to incorrect answers; DaSLaM, on the other hand, still guides the solver through correct steps.\nAn interesting observation can be made by comparing how the solver behaves with CoT vs. with DaSLaM. With DaSLaM, we do not provide any new knowledge to the solver. Yet, the same model can rectify its errors made in CoT response. This may point to the intuition that current LLMs are actually underutilized, and one can unfold even more impressive performance with cleverly composed guidance.\nWe further provide case analyses with when DaSLaM fails to guide the model (GPT 3.5 in this case) to successful final answers, in Appendix E. While we do not find any obvious pattern of errors, one can see that the decomposer generates questions that are not readily answerable within that context. DaSLaM does not use any method to trace back the error or generate subproblems based on the answers to the previous subproblems. This might raise such issues where the subproblems generated are not actually helping the solver in the right order."
        },
        {
            "heading": "8 Conclusion",
            "text": "We challenged the design of ever-larger monolithic LLMs as homogeneous network structures, where diverse aspects of problem decomposition and solution are stored in a tangled and opaque manner. The formidable general-purpose problem-solving capabilities of LLMs are exceedingly resource-hungry, dependent on immense data engineering. Inspired by brain science, we took a first step toward heterogeneity \u2014 let two different LLMs evolve independently and adapt to their roles of decomposing and solving complex reasoning problems. Through extensive experiments on several benchmarks, we showed that such a heterogeneous network can match or exceed some of the largest contemporary LLMs, at a much smaller parameter count.\nLimitations\nA potential limitation of DaSLaM, as with many system that uses an LLM-as-a-service API charging per token exchange, is the increased token usage because of the RL exploration. Asserting a token budget on the decomposer LM is left as an avenue for future exploration. Ideally, the decomposer LM should seamlessly invoke solvers of many forms, such as retrievers (Khattab et al., 2022b) or mathematical calculators (Schick et al., 2023; Wolfram, 2023). Future work may extend DaSLaM to such tools. DaSLaM is limited to purely text-based subproblem decomposition; it is not possible at present to incorporate reasoning through other modalities (e.g., visual inputs for geometric reasoning) into DaSLaM in its current form."
        },
        {
            "heading": "Acknowledgments",
            "text": "The authors acknowledge the financial support of DYSL-AI."
        },
        {
            "heading": "A Supervised Fine-tuning Dataset",
            "text": "The training data for supervised fine-tuning stage was generated using text-davinci-003. Each data point in the dataset consisted of a tuple \u27e8Qgold, Sgold, Q\u2032gold\u27e9, where Qgold represents the reasoning question, Sgold represents the gold reasoning steps, and Q\u2032gold represent the sub-questions generated by text-davinci-003 in a one-shot setting. An example of a single data point is given below.\nQgold\nJohn borrowed 3 soccer boots from Jake, and forgot them on the field, if peter came across a total of 15 boots on the field Jake\u2019s boots inclusive, and he took 4 boots at random, what is the probability that Jake\u2019s boots were not amongst the 4 taken?. (A) 12\n91 , (B) 3 15 , (C) 12 15 (D) 33 91 , (E) 3 91\nSgold\nSince Jake owns 3 of the boots, the subset from which the 4 boots should be chosen are the 12 boots not owned by Jake from the universe of 15.The first boot can be one of the 12 from the 15 with probability 12\n15 . The second boot can be\none of the 11 from the 14 remaining with probability 11 14 .The third boot can be one of the 10 from the 13 remaining with probability 10\n13 . The fourth boot can be one of the 9 from the\n12 remaining with probability 9 12 .The total probability will be 12\n15 \u00b7 11 14 \u00b7 10 13 \u00b7 9 12 . On cancellation, this comes to 33 91\nQ\u2032gold\n1. How many boots did Jake own? 2. How many boots were on the field? 3. How many boots did Peter take? 4. What is the probability of choosing one of the 12 boots\nnot owned by Jake from the universe of 15? 5. What is the probability of choosing the second, third\nand fourth boots not owned by Jake? 6. What is the total probability?"
        },
        {
            "heading": "B Reward Calculation",
            "text": "During policy gradient optimization of the decomposer LM \u03d5, rewards were provided in an incremental fashion so that the generated sub-questions align well with the initial question, the answer generated by the solver LM \u03b8 and the sub-question solving capabilities of \u03b8. For this, the reward was designed as a combination of five terms. The definitions of these terms are given in Section 4. Here we provide an example of the reward calculation.\nEntity coverage Reward (R1). Here, we find distinct nouns and numbers in the question and the sub-questions using the nltk library. The reward is calculated using R1 =\n|EQ\u2032 | |EQ| , where EQ\u2032 and\nEQ are the sets of distinct nouns and numbers in the sub-questions and questions, respectively. An example of the same is shown below.\nQgold: Each good worker can paint my new house alone in 12 hours. Each bad worker can paint my house alone in 36 hours. I need my house painted in 3 hours. If I can only find 3 good workers, how many bad workers must I also find in order to have my house painted on time?\nQ\u2032gold: 1. How many good workers are needed to paint the house\nin 3 hours? 2. How many bad workers are needed to paint the house\nin 3 hours? 3. What is the total number of workers needed to paint\nthe house in 3 hours?\n|EQ\u2032 | = 4 + 1 = 5 (11) |EQ\u2032 | = 6 + 3 = 9 (12)\nR1 = |EQ\u2032 | |EQ| = 5 9 = 0.55 (13)\nConsistency of answers to subproblems reward (R2). To ensure that the sub-question is such that the solver LLM \u03d5 does not produce random answers, we compare the entity in the solution to the entity asked in the subquestion along with the cosine similarity of the embeddings of sub-question and sub-answers. To find the embeddings, we use the pre-trained model in the spacy library. R2 = \u2211 i(I(ei = e\u0302i) + cosine-similarity(Q\u2032i, Ai)), where I(x) = 1 if x is true, e\u0302i is entity whose value has been asked in the subproblem Q\u2032i, and ei is the entity answered. An example for the reward calculation is given below.\nQ\u20321: How many good workers are needed to paint the house in 3 hours? A1: We can calculate the amount of work done in 3 hours by a good worker by dividing 12 hours by 3 hours. We get 4 as the answer. We know that a bad worker can paint the house in 36 hours. We can calculate the amount of work done in 3 hours by a bad worker by dividing 36 hours by 3 hours. We get 12 as the answer. We need to find out how many good workers are needed to paint the house in 3 hours. We can calculate the number of good workers needed by dividing 12 by 4. Answer: 3 good workers can paint the house in 3 hours. cos-sim(Q\u2032i, Ai) = 0.70457435 I(ei = e\u0302i) = 1 R2 = 1 + 0.70457435 = 1.70457435\nOrder of operations reward (R3). To evaluate the correctness of the mathematical solution produced by the model, we compare the order of operations. R3 = lm , where l is the number of operations matched in order between S and Sgold, and m is the total number of operations in Sgold. Below is an example of R3 calculation.\nSmodel: Original Price = x Discounted Price = 0.78x Price after coupon = 0.78x - 20 Price paid = 0.78x - 20 + 1.90 0.78x - 20 + 1.90 = 0.5x 0.28x = 21.90 x = 78.21 Sgold : Let x be the original price of item Discounted price = 0.78x Payment made by the customer after using the $20 coupon = 0.78x - 20 0.78x - 20 = x/2 + 1.9 x = 78.20 operations in Smodel: [-, -, +, -, +] operations in Sgold: [-, -, /, +] R3 =\n3 4 = 0.75\nCoT Proximity reward (R4). As defined in Section 4, R4 = \u2211m j=0 I(c1j > c2j)c1 + I(c2j > c1j)(\u22121\u2212c2j), where I(\u00b7) is the indicator function. Here we present an example to illustrate the same.\nSgold 1. : We first consider the possible neighbors of the\nchief\u2019s mother, one must be the chief, the other is one of the remaining 5 natives. That native can, in turn, have one of 4 neighbors in addition to the chief\u2019s mother 2. Continuing around, there are 5! = 120 possible arrangements 3. The chief can be on his mother\u2019s left or right, for a total of 2 \u00b7 120 = 240 arrangements 4. Alternatively, we know that the seating arrangement is the same if it is rotated, once the chief\u2019s spot is chosen, the rotations are eliminated 5. There are 2 ways to seat his wife and his mother, and then there are 5 distinct positions for the remaining 5 natives, which makes for 2 \u00b7 5! = 240 possible arrangements S0: 1. We can see that the chief must be seated between his\nmother and his wife. 2. This means that there are 3 possible positions for the\nchief, and 5 possible positions for the other 6 people. 3. Therefore, there are 3! \u00b7 5! = 1800 different seating\narrangements. S:\n1. The chief must sit between his mother and his wife. 2. There are 8 people in the tribe. 3. So, the chief\u2019s mother and wife can be arranged in 2!\nways. 4. The remaining 5 people can be arranged in 5! ways. 5. Therefore, the total number of possible seating ar-\nrangements after removing rotation = 5! = 240 Cosine similarity calculation per step:\n1. c1j = 0.47673503, c2j = 0.44773823 2. c1j = 0.45063934, c2j = 0.47917843 3. c1j = 0.5173945, c2j = 0.20383504 4. c1j = 0.46866685, c2j = 0 5. c1j = 0.47825924, c2j = 0\nHence, R4 = 0.47673503 + (\u22121 \u2212 0.47917843) + 0.5173945 + 0.46866685 + 0.47825924 = 0.46187719\nCorrectness of final answer(R5), R5 = I(A\u0302 = Agold). This reward checks if the final answer matches the gold answer. A negative example of the same is given below.\nQgold: Three friends Alan, Roger and Peter attempt to answer a question on an exam. Alan randomly guesses the answer, giving him a 1\n5 probability of guessing correctly. Roger\ncheats by looking at the paper of the student in front of him, giving him a 2\n3 probability of answering correctly. And Peter\ndutifully performs the calculations, then marks the answer, giving him a 5\n6 probability of a correct answer. What is the\nprobability that the question is answered correctly, but not via cheating? Sgold : Prob(Alan) = 1\n5\nProb(Roger) without cheating = 2 3 \u2212 1 = 1 3 Prob(Peter) = 5 6\nTotal Probability = 1 5 \u00b7 1 3 \u00b7 5 6 =\n1\n18\nSmodel : Alan has a 1\n5 chance of getting the answer correct.\nRoger has a 2 3 chance of getting the answer correct. Peter has a 5\n6 chance of getting the answer correct.\nThe probability that the question is answered correctly is 1 5 + 2 3 + 5 6 = 13 12 . The probability that the question is answered correctly, but\nnot via cheating is1\u2212 ( 1 5 + 2 3 + 5 6 ) = 1\u2212 13 12 = \u22121 12\nAgold: 118 Amodel: \u2212112\nR5 = 0"
        },
        {
            "heading": "C Hyperparameter Selection",
            "text": "We performed hyperparameter tuning on a subset of 250 examples from the training dataset. Based on the improvement in accuracy and compute resources, we converged at the following values.\nFor the supervised fine-tuning stage, we used LoRA r = 16, LoRA \u03b1 = 32, LoRA droput = 0.05. For the RLMF stage, we finetuned the last 3 layers of LoRA adapters, using a batch size of 8, gradient accumulation steps=8, init kl coef=0.01, target=4. For inference, we used the following generation parameters: temperature = 0.95, top p=0.18, pad token id = 0, do sample = False, number of beams = 1, maxi length = 2048.\nAll the models were implemented using Huggingface with PyTorch, and loaded in int_8 precision. For solver-LLaMA 13B model, we instruction finetuned the LLaMA13B model using the Alpaca dataset by stanford (Taori et al., 2023). For solver-LLaMA 33 billion model, we used an instruction-finetuned version of LLAMA-33 billion from Huggingface. To implement the PPO algorithm, we used the TRL library. We used a single NVIDIA A100 (80GB) GPU for all training and inference purposes."
        },
        {
            "heading": "D Case Study on LLAMA Models",
            "text": "Similar to Figure 2, we performed example-wise case study on the answering process of the LLaMA solvers (13 billion and 33 billion models) with different prompting methods (see Figures 3 and 4). The example problem was selected from the MATH dataset. As we can see, with relatively small scale solvers like LLaMA, Least-to-most fails to add any improvement over basic CoT prompting. DaSLaM is able to guide the model to the correct answer in both cases.\nE Failure cases with DaSLaM\nE.1 Case I Question A train running at a speed of 100 miles/hour, takes 10 hours to reach its destination. After covering quarter of the distance, it starts raining and the train has to be slowed to speed of 75 miles/hour. What is the total journey duration? A)10 B)11.5 C)12.5 D)13.5 E)15\nInitial Answer The train covers the first quarter of the distance at a speed of 100 miles/hour. So, the time taken to cover the first quarter of the distance = (1/4) x 10 hours = 2.5 hours The train covers the remaining three quarters of the distance at a speed of 75 miles/hour. So, the time taken to cover the remaining three quarters of the distance = (3/4) x 10 hours = 7.5 hours Therefore, the total journey duration = 2.5 + 7.5 = 10 hours\nSub problem 1 What is the total distance to be covered?\nSub Answer 1 Let the total distance to be covered be d. Time taken to cover the first quarter of the distance = (1/4)d/100 = 10/400 hours Time taken to cover the remaining distance = (3/4)d/75 = 300/400 hours Total time taken = 10/400 + 300/400 = 10/4 hours = 2.5 hours Since the train takes 10 hours to reach its destination, Total distance to be covered = 10 x 100 = 1000 miles\nSub problem 2 What is the speed of the train?\nSub Answer 2 After covering quarter of the distance, the train has to be slowed to speed of 75 miles/hour.\nLet the remaining distance be x. Therefore, x = d - (d/4) = 1000 - (1000/4) = 750 miles. The time taken to cover the remaining distance x = 750 miles at a speed of 75 miles/hour = (750/75) hours = 10 hours.\nTherefore, the speed of the train = x/time taken = 750/10 = 75 miles/hour.\nSub problem 3 What is the time taken to cover the distance? Sub Answer 3 Time taken to cover the first quarter of the distance = (d/4)/100 hours = d/400 hours\nTime taken to cover the remaining distance = (3d/4)/75 hours = d/300 hours\nTotal time taken = d/400 + d/300 = (5d + 4d)/1200 hours = 9d/1200 hours.\nPutting d=1000, Final answer: The time taken to cover the distance is 9 x 1000/1200 hours. Sub problem 4 What is the total duration of the journey? Sub Answer 4 Time taken to cover the first quarter of the distance = (d/4)/100 hours = d/400 hours\nTime taken to cover the remaining three quarters of the distance = (3d/4)/75 hours = 4d/300 hours\nTotal time taken = d/400 + 4d/300 = (5d + 4d)/(400 + 300) = 9d/700 hours\nTotal duration of the journey = 9d/700 hours = 9 hours and (d/700) hours = 9 hours and (100/700) hours = 9 hours and 14.29 minutes\nFinal Answer Step 1: The train covers the first quarter of the journey at a speed of 100 miles/hour. Step 2: The time taken to cover the first quarter of the journey is given by: Time = Distance/Speed Time = (1/4) x 100/100 Time = 1/4 hour Step 3: The train covers the remaining three quarters of the journey at a speed of 75 miles/hour. Step 4: The time taken to cover the remaining three quarters of the journey is given by: Time = Distance/Speed Time = (3/4) x 75/100 Time = 3/4 hour Step 5: The total duration of the journey is the sum of the time taken to cover the first quarter and the remaining three quarters of the journey. Total Time = 1/4 + 3/4 Total Time = 4/4 Total Time = 1 hour Therefore, the total duration of the journey is 1 hour.\nE.2 Case II Question Consider there is an staircase elevator and you are coming down. If you walk 20 steps and stop, then you reach bottom in 10 minutes. If you walk 10 steps and stop, you reach to the ground in 20 minutes. What is the speed of the elevator? A)1 step/minute B)2 step/minute C)3 step/minute D)4 step/minute E)None of the above Initial Answer . Sub Question 1 What is the total number of steps in the elevator? . Sub Question 2 What is the speed of the elevator? . Sub Question 3 What is the time taken to cover 20 steps? Sub Answer 3 Time taken to cover 20 steps is 10 minutes Sub Question 4 What is the time taken to cover 10 steps? Sub Answer 4 Time taken to cover 10 steps is 20 minutes Final Answer Let the speed of the elevator be x. Time taken to reach the bottom when walking 20 steps and stopping = 10 minutes Time taken to reach the bottom when walking 10 steps and stopping = 20 minutes Therefore, 20x = 10 10x = 20 x = 2 Therefore, the speed of the elevator is 2 steps/minute.\nE.3 Case III Question Let \u2212\u03c06 < \u03b8 < \u2212 \u03c0 12 . Suppose \u03b11 and \u03b21 are the roots of the equation x2 \u2212 2x sec \u03b8+1 = 0 and \u03b12 and \u03b22 are the roots of the equation x2 + 2x tan \u03b8 \u2212 1 = 0. If \u03b11 > \u03b21 and \u03b12 > \u03b22, then \u03b11 + \u03b22 equals\n(A) 2(sec \u03b8 \u2212 tan \u03b8) (B) 2 sec \u03b8 (C) \u22122 tan \u03b8 (D) 0 Initial Answer\nWe can solve the equations for alpha1 and alpha2 to get alpha1 = sec \u03b8 + tan \u03b8 alpha2 = \u2212 sec \u03b8 + tan \u03b8 Since \u03b11\u03b21 and \u03b12\u03b22, we have sec \u03b8 + tan \u03b8 > \u2212 sec \u03b8 sec \u03b8 + tan \u03b8 > \u2212 tan \u03b8 Adding the two equations, we get 2 sec \u03b8 > 0 Therefore, \u03b11\u03b22 = 2 sec \u03b8. The correct answer is (B) 2 sec \u03b8. What is the equation of the line x2 \u2212 2x sec \u03b8 + 1 = 0? Given, \u2212\u03c06 < \u03b8 < \u2212 \u03c0 12 We need to find the equation of the line x2\u22122x sec \u03b8+1 = 0 We know that the equation of a line is given by y = mx+ c Now, let us substitute the given equation in the equation of a line. x2 \u2212 2x sec \u03b8 + 1 = 0 y = mx+ c x2 \u2212 2x sec \u03b8 + 1 = mx+ c mx+ c\u2212 x2 + 2x sec \u03b8 \u2212 1 = 0 mx\u2212 x2 + 2x sec \u03b8 = 1 Comparing the coefficients of x on both sides, m = 2 sec \u03b8 Comparing the constant terms on both sides, c = 1 Therefore, the equation of the line x2 \u2212 2x sec \u03b8 + 1 = 0 is given by y = 2 sec \u03b8x+ 1 Hence, the answer is y = 2 sec \u03b8x+ 1.\nWhat is the equation of the line x2 + 2x tan \u03b8 \u2212 1 = 0? Given, \u2212\u03c06 < \u03b8 < \u2212 \u03c0 12 We know that, sec \u03b8 = 1cos \u03b8 tan \u03b8 = sin \u03b8cos \u03b8 Therefore, x2 \u2212 2x sec \u03b8 + 1 = x2 \u2212 2x 1cos \u03b8 + 1 = 0 x2 + 2x tan \u03b8 \u2212 1 = x2 + 2x sin \u03b8cos \u03b8 \u2212 1 = 0\nHence, the equation of the line x2+2x tan \u03b8\u22121 = 0 is x2 + 2x sin \u03b8cos \u03b8 \u2212 1 = 0. Final Answer: x2 + 2x sin \u03b8cos \u03b8 \u2212 1 = 0\nWhat is the equation of the line x2 \u2212 2x sec \u03b8 + 1 = 0 minus the equation of the line x2 + 2x tan \u03b8 \u2212 1 = 0? The equation of the line x2 \u2212 2x sec \u03b8 + 1 = 0 is given by x2 \u2212 2x sec \u03b8 + 1 = 0 The equation of the line x2 + 2x tan \u03b8 \u2212 1 = 0 is given by x2 + 2x tan \u03b8 \u2212 1 = 0 Subtracting the two equations, we get x2 \u2212 2x sec \u03b8 + 1\u2212 x2 \u2212 2x tan \u03b8 + 1 = 0 Simplifying the equation, we get \u22122x(sec \u03b8 + tan \u03b8) = 0 Therefore, the equation of the line x2\u22122x sec \u03b8+1 = 0 minus the equation of the line x2 + 2x tan \u03b8 \u2212 1 = 0 is \u22122x(sec \u03b8 + tan \u03b8) = 0 Hence, the answer is \u22122x(sec \u03b8 + tan \u03b8) = 0.\nWhat is the equation of the line x2 \u2212 2x sec \u03b8 + 1 = 0 plus the equation of the line x2 Given, \u2212\u03c06 < \u03b8 < \u2212 \u03c0 12 \u03b11 and \u03b21 are the roots of the equation x2 \u2212 2x sec \u03b8 + 1 = 0 \u03b12 and \u03b22 are the roots of the equation x2 + 2x tan \u03b8 \u2212 1 = 0 \u03b11 > \u03b21 and \u03b12 > \u03b22 We need to find the value of \u03b11 + \u03b22 Let us consider the equation x2\u2212 2x sec \u03b8+1 = 0 \u03b11 and \u03b21 are the roots of the equation. Therefore, \u03b11 + \u03b21 = \u22122 sec \u03b8 Let us consider the equation x2+2x tan \u03b8\u22121 = 0 \u03b12 and \u03b22 are the roots of the equation. Therefore, \u03b12 + \u03b22 = 2 tan \u03b8 Now, \u03b11 + \u03b22 = \u03b11 + \u03b22 + \u03b12 + \u03b22 \u2212 \u03b12 \u2212 \u03b22 = \u03b11 + \u03b21 \u2212 \u03b12 \u2212 \u03b22 = \u22122 sec \u03b8 + 2 tan \u03b8 = 2(sec \u03b8 \u2212 tan \u03b8) Hence, the answer is (A) 2(sec \u03b8 \u2212 tan \u03b8).\nFinal Answer Given, \u2212\u03c06 < \u03b8 < \u2212 \u03c0 12 \u03b11 and \u03b21 are the roots of the equation x2 \u2212 2x sec \u03b8 + 1 = 0 \u03b12 and \u03b22 are the roots of the equation x2 + 2x tan \u03b8 \u2212 1 = 0\n\u03b11 > \u03b21 and \u03b12 > \u03b22 We need to find the value of \u03b11 + \u03b22 Let us consider the equation x2\u2212 2x sec \u03b8+1 = 0 \u03b11 and \u03b21 are the roots of the equation. Therefore, \u03b11 + \u03b21 = \u22122 sec \u03b8 Let us consider the equation x2+2x tan \u03b8\u22121 = 0 \u03b12 and \u03b22 are the roots of the equation. Therefore, \u03b12 + \u03b22 = 2 tan \u03b8 Now, \u03b11 + \u03b22 = \u03b11 + \u03b22 + \u03b12 + \u03b22 \u2212 \u03b12 \u2212 \u03b22 = \u03b11 + \u03b21 \u2212 \u03b12 \u2212 \u03b22 = \u22122 sec \u03b8 + 2 tan \u03b8 = 2(sec \u03b8 \u2212 tan \u03b8) Hence, the answer is (A) 2(sec \u03b8 \u2212 tan \u03b8)."
        }
    ],
    "title": "Small Language Models Fine-tuned to Coordinate Larger Language Models improve Complex Reasoning",
    "year": 2023
}