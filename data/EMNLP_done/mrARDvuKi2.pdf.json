{
    "abstractText": "Prompt-based learning has emerged as a powerful technique in natural language processing (NLP) due to its ability to leverage pre-training knowledge for downstream few-shot tasks. In this paper, we propose 2INER, a novel textto-text framework for Few-Shot Named Entity Recognition (NER) tasks. Our approach employs instruction finetuning based on InstructionNER (Wang et al., 2022) to enable the model to effectively comprehend and process task-specific instructions, including both main and auxiliary tasks. We also introduce a new auxiliary task, called Type Extraction, to enhance the model\u2019s understanding of entity types in the overall semantic context of a sentence. To facilitate in-context learning, we concatenate examples to the input, enabling the model to learn from additional contextual information. Experimental results on four datasets demonstrate that our approach outperforms existing Few-Shot NER methods and remains competitive with state-of-the-art standard NER algorithms.",
    "authors": [
        {
            "affiliations": [],
            "name": "Jiasheng Zhang"
        },
        {
            "affiliations": [],
            "name": "Xikai Liu"
        },
        {
            "affiliations": [],
            "name": "Xinyi Lai"
        },
        {
            "affiliations": [],
            "name": "Yan Gao"
        },
        {
            "affiliations": [],
            "name": "Shusen Wang"
        },
        {
            "affiliations": [],
            "name": "Yao Hu"
        },
        {
            "affiliations": [],
            "name": "Yiqing LIN"
        }
    ],
    "id": "SP:374014a06cd6d5daec86c5a9302bfef53be5ffbd",
    "references": [
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners. Advances in neural information processing",
            "year": 2020
        },
        {
            "authors": [
                "Jiawei Chen",
                "Qing Liu",
                "Hongyu Lin",
                "Xianpei Han",
                "Le Sun."
            ],
            "title": "Few-shot named entity recognition with self-describing networks",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages",
            "year": 2022
        },
        {
            "authors": [
                "Xiang Chen",
                "Lei Li",
                "Shumin Deng",
                "Chuanqi Tan",
                "Changliang Xu",
                "Fei Huang",
                "Luo Si",
                "Huajun Chen",
                "Ningyu Zhang."
            ],
            "title": "LightNER: A lightweight tuning paradigm for low-resource NER via pluggable prompting",
            "venue": "Proceedings of the 29th Inter-",
            "year": 2022
        },
        {
            "authors": [
                "Yanru Chen",
                "Yanan Zheng",
                "Zhilin Yang."
            ],
            "title": "Prompt-based metric learning for few-shot ner",
            "venue": "arXiv preprint arXiv:2211.04337.",
            "year": 2022
        },
        {
            "authors": [
                "Jason P.C. Chiu",
                "Eric Nichols."
            ],
            "title": "Named entity recognition with bidirectional LSTM-CNNs",
            "venue": "Transactions of the Association for Computational Linguistics, 4:357\u2013370.",
            "year": 2016
        },
        {
            "authors": [
                "Hyung Won Chung",
                "Le Hou",
                "Shayne Longpre",
                "Barret Zoph",
                "Yi Tay",
                "William Fedus",
                "Eric Li",
                "Xuezhi Wang",
                "Mostafa Dehghani",
                "Siddhartha Brahma"
            ],
            "title": "Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416",
            "year": 2022
        },
        {
            "authors": [
                "Leyang Cui",
                "Yu Wu",
                "Jian Liu",
                "Sen Yang",
                "Yue Zhang."
            ],
            "title": "Template-based named entity recognition using BART",
            "venue": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 1835\u20131845, Online. Association for Computational",
            "year": 2021
        },
        {
            "authors": [
                "Leyang Cui",
                "Yue Zhang."
            ],
            "title": "Hierarchicallyrefined label attention network for sequence labeling",
            "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Lan-",
            "year": 2019
        },
        {
            "authors": [
                "Xiang Dai",
                "Sarvnaz Karimi",
                "Ben Hachey",
                "Cecile Paris."
            ],
            "title": "An effective transition-based model for discontinuous NER",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5860\u20135870, Online. Association",
            "year": 2020
        },
        {
            "authors": [
                "Sarkar Snigdha Sarathi Das",
                "Arzoo Katiyar",
                "Rebecca Passonneau",
                "Rui Zhang."
            ],
            "title": "CONTaiNER: Few-shot named entity recognition via contrastive learning",
            "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics",
            "year": 2022
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova."
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
            "year": 2019
        },
        {
            "authors": [
                "Alexander Fritzler",
                "Varvara Logacheva",
                "Maksim Kretov."
            ],
            "title": "Few-shot classification in named entity recognition task",
            "venue": "Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing, pages 993\u20131000.",
            "year": 2019
        },
        {
            "authors": [
                "Tao Gui",
                "Jiacheng Ye",
                "Qi Zhang",
                "Zhengyan Li",
                "Zichu Fei",
                "Yeyun Gong",
                "Xuanjing Huang."
            ],
            "title": "Uncertainty-aware label refinement for sequence labeling",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
            "year": 2020
        },
        {
            "authors": [
                "Dilek Hakkani-T\u00fcr",
                "G\u00f6khan T\u00fcr",
                "Asli Celikyilmaz",
                "YunNung Chen",
                "Jianfeng Gao",
                "Li Deng",
                "Ye-Yi Wang."
            ],
            "title": "Multi-domain joint semantic frame parsing using bi-directional rnn-lstm",
            "venue": "Interspeech, pages 715\u2013719.",
            "year": 2016
        },
        {
            "authors": [
                "Xu Han",
                "Zhengyan Zhang",
                "Ning Ding",
                "Yuxian Gu",
                "Xiao Liu",
                "Yuqi Huo",
                "Jiezhong Qiu",
                "Yuan Yao",
                "Ao Zhang",
                "Liang Zhang"
            ],
            "title": "Pre-trained models: Past, present and future",
            "venue": "AI Open,",
            "year": 2021
        },
        {
            "authors": [
                "Yucheng Huang",
                "Kai He",
                "Yige Wang",
                "Xianli Zhang",
                "Tieliang Gong",
                "Rui Mao",
                "Chen Li."
            ],
            "title": "COPNER: Contrastive learning with prompt guiding for few-shot named entity recognition",
            "venue": "Proceedings of the 29th International Conference on Computational",
            "year": 2022
        },
        {
            "authors": [
                "Sarvnaz Karimi",
                "Alejandro Metke-Jimenez",
                "Madonna Kemp",
                "Chen Wang."
            ],
            "title": "Cadec: A corpus of adverse drug event annotations",
            "venue": "Journal of biomedical informatics, 55:73\u201381.",
            "year": 2015
        },
        {
            "authors": [
                "J-D Kim",
                "Tomoko Ohta",
                "Yuka Tateisi",
                "Jun\u2019ichi Tsujii"
            ],
            "title": "Genia corpus\u2014a semantically annotated corpus for bio-textmining. Bioinformatics, 19(suppl_1):i180\u2013i182",
            "year": 2003
        },
        {
            "authors": [
                "Dong-Ho Lee",
                "Akshen Kadakia",
                "Kangmin Tan",
                "Mahak Agarwal",
                "Xinyu Feng",
                "Takashi Shibuya",
                "Ryosuke Mitani",
                "Toshiyuki Sekiya",
                "Jay Pujara",
                "Xiang Ren"
            ],
            "title": "2022a. Good examples make a faster learner: Simple demonstration-based learning for low-resource NER",
            "year": 2022
        },
        {
            "authors": [
                "Dong-Ho Lee",
                "Akshen Kadakia",
                "Kangmin Tan",
                "Mahak Agarwal",
                "Xinyu Feng",
                "Takashi Shibuya",
                "Ryosuke Mitani",
                "Toshiyuki Sekiya",
                "Jay Pujara",
                "Xiang Ren"
            ],
            "title": "2022b. Good examples make a faster learner: Simple demonstration-based learning for low-resource NER",
            "year": 2022
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Veselin Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "BART: Denoising sequence-to-sequence pre-training for natural language",
            "year": 2020
        },
        {
            "authors": [
                "Xiaoya Li",
                "Jingrong Feng",
                "Yuxian Meng",
                "Qinghong Han",
                "Fei Wu",
                "Jiwei Li."
            ],
            "title": "A unified MRC framework for named entity recognition",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5849\u20135859, On-",
            "year": 2020
        },
        {
            "authors": [
                "Xiaoya Li",
                "Jingrong Feng",
                "Yuxian Meng",
                "Qinghong Han",
                "Fei Wu",
                "Jiwei Li."
            ],
            "title": "A unified MRC framework for named entity recognition",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5849\u20135859, On-",
            "year": 2020
        },
        {
            "authors": [
                "Jingjing Liu",
                "Panupong Pasupat",
                "Scott Cyphers",
                "Jim Glass."
            ],
            "title": "Asgard: A portable architecture for multilingual dialogue systems",
            "venue": "2013 IEEE International Conference on Acoustics, Speech and Signal Processing, pages 8386\u20138390. IEEE.",
            "year": 2013
        },
        {
            "authors": [
                "Kun Liu",
                "Yao Fu",
                "Chuanqi Tan",
                "Mosha Chen",
                "Ningyu Zhang",
                "Songfang Huang",
                "Sheng Gao."
            ],
            "title": "Noisy-labeled NER with confidence estimation",
            "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computa-",
            "year": 2021
        },
        {
            "authors": [
                "Yijin Liu",
                "Fandong Meng",
                "Jinchao Zhang",
                "Jinan Xu",
                "Yufeng Chen",
                "Jie Zhou."
            ],
            "title": "GCDT: A global context enhanced deep transition architecture for sequence labeling",
            "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Lin-",
            "year": 2019
        },
        {
            "authors": [
                "Jie Ma",
                "Miguel Ballesteros",
                "Srikanth Doss",
                "Rishita Anubhai",
                "Sunil Mallya",
                "Yaser Al-Onaizan",
                "Dan Roth."
            ],
            "title": "Label semantics for few shot named entity recognition",
            "venue": "Findings of the Association for Computational Linguistics: ACL 2022, pages 1956\u2013",
            "year": 2022
        },
        {
            "authors": [
                "Dublin",
                "Ireland"
            ],
            "title": "Association for Computational Linguistics",
            "year": 1971
        },
        {
            "authors": [
                "Tingting Ma",
                "Huiqiang Jiang",
                "Qianhui Wu",
                "Tiejun Zhao",
                "Chin-Yew Lin."
            ],
            "title": "Decomposed metalearning for few-shot named entity recognition",
            "venue": "Findings of the Association for Computational Linguistics: ACL 2022, pages 1584\u20131596, Dublin, Ire-",
            "year": 2022
        },
        {
            "authors": [
                "Xuezhe Ma",
                "Eduard Hovy."
            ],
            "title": "End-to-end sequence labeling via bi-directional LSTM-CNNs-CRF",
            "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1064\u20131074, Berlin, Germany.",
            "year": 2016
        },
        {
            "authors": [
                "Alejandro Metke-Jimenez",
                "Sarvnaz Karimi."
            ],
            "title": "Concept identification and normalisation for adverse drug event discovery in medical forums",
            "venue": "BMDID@ ISWC.",
            "year": 2016
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J Liu."
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2020
        },
        {
            "authors": [
                "Victor Sanh",
                "Albert Webson",
                "Colin Raffel",
                "Stephen H Bach",
                "Lintang Sutawika",
                "Zaid Alyafeai",
                "Antoine Chaffin",
                "Arnaud Stiegler",
                "Teven Le Scao",
                "Arun Raja"
            ],
            "title": "Multitask prompted training enables zero-shot task generalization",
            "year": 2021
        },
        {
            "authors": [
                "Jake Snell",
                "Kevin Swersky",
                "Richard Zemel."
            ],
            "title": "Prototypical networks for few-shot learning",
            "venue": "Advances in neural information processing systems, 30.",
            "year": 2017
        },
        {
            "authors": [
                "Emma Strubell",
                "Patrick Verga",
                "David Belanger",
                "Andrew McCallum."
            ],
            "title": "Fast and accurate entity recognition with iterated dilated convolutions",
            "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages",
            "year": 2017
        },
        {
            "authors": [
                "Buzhou Tang",
                "Jianglu Hu",
                "Xiaolong Wang",
                "Qingcai Chen."
            ],
            "title": "Recognizing continuous and discontinuous adverse drug reaction mentions from social media using lstm-crf",
            "venue": "Wireless Communications & Mobile Computing (Online), 2018.",
            "year": 2018
        },
        {
            "authors": [
                "Erik F. Tjong Kim Sang",
                "Fien De Meulder."
            ],
            "title": "Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition",
            "venue": "Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003, pages 142\u2013",
            "year": 2003
        },
        {
            "authors": [
                "Hugo Touvron",
                "Thibaut Lavril",
                "Gautier Izacard",
                "Xavier Martinet",
                "Marie-Anne Lachaux",
                "Timoth\u00e9e Lacroix",
                "Baptiste Rozi\u00e8re",
                "Naman Goyal",
                "Eric Hambro",
                "Faisal Azhar"
            ],
            "title": "Llama: Open and efficient foundation language models",
            "year": 2023
        },
        {
            "authors": [
                "Jue Wang",
                "Lidan Shou",
                "Ke Chen",
                "Gang Chen."
            ],
            "title": "Pyramid: A layered model for nested named entity recognition",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5918\u20135928, Online. Association for Computa-",
            "year": 2020
        },
        {
            "authors": [
                "Liwen Wang",
                "Rumei Li",
                "Yang Yan",
                "Yuanmeng Yan",
                "Sirui Wang",
                "Wei Wu",
                "Weiran Xu."
            ],
            "title": "Instructionner: A multi-task instruction-based generative framework for few-shot ner",
            "venue": "arXiv preprint arXiv:2203.03903.",
            "year": 2022
        },
        {
            "authors": [
                "Jason Wei",
                "Maarten Bosma",
                "Vincent Y Zhao",
                "Kelvin Guu",
                "Adams Wei Yu",
                "Brian Lester",
                "Nan Du",
                "Andrew M Dai",
                "Quoc V Le."
            ],
            "title": "Finetuned language models are zero-shot learners",
            "venue": "arXiv preprint arXiv:2109.01652.",
            "year": 2021
        },
        {
            "authors": [
                "Ikuya Yamada",
                "Akari Asai",
                "Hiroyuki Shindo",
                "Hideaki Takeda",
                "Yuji Matsumoto."
            ],
            "title": "LUKE: Deep contextualized entity representations with entityaware self-attention",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Lan-",
            "year": 2020
        },
        {
            "authors": [
                "Hang Yan",
                "Tao Gui",
                "Junqi Dai",
                "Qipeng Guo",
                "Zheng Zhang",
                "Xipeng Qiu."
            ],
            "title": "A unified generative framework for various NER subtasks",
            "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International",
            "year": 2021
        },
        {
            "authors": [
                "Jie Yang",
                "Shuailong Liang",
                "Yue Zhang."
            ],
            "title": "Design challenges and misconceptions in neural sequence labeling",
            "venue": "Proceedings of the 27th International Conference on Computational Linguistics, pages 3879\u20133889, Santa Fe, New Mexico, USA. As-",
            "year": 2018
        },
        {
            "authors": [
                "Yi Yang",
                "Arzoo Katiyar."
            ],
            "title": "Simple and effective few-shot named entity recognition with structured nearest neighbor learning",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6365\u20136375,",
            "year": 2020
        },
        {
            "authors": [
                "Juntao Yu",
                "Bernd Bohnet",
                "Massimo Poesio."
            ],
            "title": "Named entity recognition as dependency parsing",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6470\u2013 6476, Online. Association for Computational Lin-",
            "year": 2020
        },
        {
            "authors": [
                "Juntao Yu",
                "Bernd Bohnet",
                "Massimo Poesio."
            ],
            "title": "Named entity recognition as dependency parsing",
            "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6470\u2013 6476, Online. Association for Computational Lin-",
            "year": 2020
        },
        {
            "authors": [
                "Ningyu Zhang",
                "Shumin Deng",
                "Zhen Bi",
                "Haiyang Yu",
                "Jiacheng Yang",
                "Mosha Chen",
                "Fei Huang",
                "Wei Zhang",
                "Huajun Chen."
            ],
            "title": "OpenUE: An open toolkit of universal extraction from text",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods in Nat-",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 Introduction",
            "text": "Named Entity Recognition (NER) has been a fundamental task of Natural Language Processing (NLP) and there are three types of sub-tasks in NER: flat NER (Tjong Kim Sang and De Meulder, 2003), nested NER (Kim et al., 2003) and discontinuous NER (Karimi et al., 2015). All three sub-tasks aim to locate named entities, extract the entity spans, and classify each span into pre-defined label categories. In terms of the flat NER which is the main focus of this paper, it can be formulated as a sequence labeling paradigm by assigning labels to each token in the sentence through token-classification models. The dominant methods include combining Pre-trained Language Models(PLMs) (Devlin et al., 2019) with label-specific classifier (LC) (Strubell et al., 2017; Cui and Zhang, 2019). However, the fixed shape of the output LC\nlayer necessitates a consistent label set for both the training and testing data, which poses a challenge for knowledge transfer. Therefore, these models need to be trained from scratch to adapt to a new domain with a different label set, highlighting the requirement for a large amount of data for these methods.\nDue to the high cost of sequence labeling annotation in real-world scenarios, labeled data for NER is often limited. As a result, few-shot NER has gained significant attention due to its practical applications. Meanwhile, applying prompt-base learning (Han et al., 2021) on PLMs is an effective way to solve few-shot problems (Brown et al., 2020). PLMs can learn a lot of knowledge regarding human languages by training on a large amount of self-supervised corpus. In order to explore the potential of PLMs, prompt-based learning reformulate the downstream tasks to text-totext framework with additional prompt indicating task descriptions (e.g. instruction fine-tuning (Wei et al., 2021; Chung et al., 2022; Sanh et al., 2021)). Through this approach, the model can effectively leverage the knowledge present in PLMs to enhance downstream skills without the need for additional large amounts of downstream data. This enables the model to achieve remarkable performance in few-shot settings.\nRecently, many prompt-based NER methods have emerged to address the limitations of traditional few-shot NER approaches. TemplateNER (Cui et al., 2021) treats original sentence and predicted template filled by entity spans as source and target sequence, respectively, but all candidate spans must be enumerated during inference, leading to a high computational cost. BARTNER (Yan et al., 2021) proposed a pointer mechanism to unified all NER sub-tasks into one sequenceto-sequence (seq2seq) framework. BARTNER utilizes the raw sentence as input and outputs pointer index and tag index which represent the location\nof the span and the corresponding label index in the category, respectively. To further adapt BARTNER for few-shot settings, LightNER (Chen et al., 2022b) proposed a lightweight tuning approach for low-resource settings by adding a unified learnable verbalizer and incorporating learnable parameters into the self-attention layers. Nonetheless, due to the fact that pointer mechanism only outputs the indexes of entities and labels, the model encounters challenges in effectively leveraging the capabilities of PLMs to directly comprehend the semantic meaning between entities and labels. Thus instead of using a pointer mechanism, InstructionNER (Wang et al., 2022) directly generates entity spans and types in the target sequence and applies instruction fine-tuning with two auxiliary tasks to further mining the capabilities of PLMs, which leads to significant few-shot improvements.\nIn terms of the auxiliary tasks in InstructionNER, they propose two auxiliary tasks from two perspectives: span recognition (Entity Extraction) and entity labeling (Entity Typing). However, we argue that NER can be further divided into three parts: 1) understand the the relationship between the label and semantic meaning of the sentence. 2) extract the spans. 3) annotate the given spans. We believe that both span recognition and entity labeling can be benefit from having a deeper understanding of the label semantics. Therefore, we proposed a new auxiliary task, called Type Extraction, to help the model to acquire this ability.\nMeanwhile, none of the above methods take the additional external knowledge into account. Current literature related to utilize external knowledge in NER involve (Chen et al., 2022a) and (Lee et al., 2022a). SDNet (Chen et al., 2022a) proposes a self-describing mechanism to leverage external resources by self-describing both entity types and mentions, while (Lee et al., 2022a) uses a demonstration-based method by incorporating examples to the input but without a text-to-text framework. Therefore, to the best of our knowledge, there is currently no existing literature that combines in-context external knowledge with instruction fine-tuning for few-shot NER.\nIn this paper, we propose 2INER(Instructive and In-Context Learning on Few-Shot NER). We build upon the work of InstructionNER by incorporating in-context examples and a novel auxiliary task. Specifically, we first reformulate the NER tasks into a text-to-text framework and then employ T5\n(Raffel et al., 2020) for natural language generation. In terms of the source sentence, we use instructions to distinguish between tasks by giving a comprehensive task description and include an alternative field to identify the entity type that requires detection. Moreover, we suggest incorporating in-context demonstration examples into the source sentence to enable the model to learn from external knowledge. For the target sentence, we use natural language to represent entity spans and types instead of pointer mechanism. In addition to the two auxiliary tasks used in InstructionNER, we propose a new task called type extraction to further explore the potential of PLMs to understand label semantics. Type Extraction task requires the model to identify all the entity types presented in the original sentence and learn to understand the meaning of entity types at the overall semantic level of the sentence. Our contributions can be summarized as follows: \u2022 To utilize external knowledge, we apply demonstration-based in-context learning examples to the instruction template. The in-context examples enable the model to directly learn which spans correspond to which types from these additional information, leading to better few-shot abilities. \u2022 We expand the NER capabilities by dividing them into three components instead of two. And we propose a novel auxiliary task for instructions fine-tuning, called type extraction, to address the existing gap. It can enable the model to understand the meaning of the entity types through the overall semantic level of the sentence, which will improve span recognition and entity labeling abilities. \u2022 We conduct extensive experiments on four datasets, demonstrating that 2INER outperforms existing few-shot NER methods and remains competitive with SOTA standard NER algorithms."
        },
        {
            "heading": "2 Related Work",
            "text": ""
        },
        {
            "heading": "2.1 Named Entity Recognition",
            "text": "Currently, NER tasks can be divided into flat NER (Tjong Kim Sang and De Meulder, 2003), nested NER (Kim et al., 2003) and discontinuous NER (Karimi et al., 2015), while in this paper, we mainly focus on the flat NER task. The current dominant method to solve flat NER is using token-level classification by turning it into a sequence labeling problem (Chiu and Nichols, 2016; Liu et al., 2019; Zhang et al., 2020; Liu et al., 2021), which apply a text encoder and CRF (Ma and Hovy, 2016) in\nsequence. Recently, BARTNER (Yan et al., 2021) formulate all three NER tasks into a text-to-text framework to solve them concurrently. BARTNER generate entity span sequences by a pointer-based model based on BART (Lewis et al., 2020) so that special design of tagging schema or spans postprocessing are no longer needed."
        },
        {
            "heading": "2.2 Prompt-based Learning",
            "text": "With the emergence of GPT-3 (Brown et al., 2020), prompt-based learning has gained increasing attention. It can better stimulate the knowledge model learned in pre-training stages and integrate different tasks together compared to the paradigm of fine-tuning separate model for each task, especially in few-shot settings (Han et al., 2021). To push prompt-based learning further, instruction-based learning (Wei et al., 2021) is proposed to fine-tune the PLMs on a collection of task descriptions which enables the model to better follow human instructions and generalize to unseen tasks with better zero-shot and few-shot abilities (Chung et al., 2022; Sanh et al., 2021)."
        },
        {
            "heading": "2.3 Few-Shot NER Methods",
            "text": "One line of work in few-shot NER is to apply contrastive learning to assign the labels by searching for the closest token (Das et al., 2022; Chen et al., 2022c), prototype (Snell et al., 2017; Fritzler et al., 2019; Ma et al., 2022b) or label semantic (Ma et al., 2022a; Huang et al., 2022) in the support set. Another line of researches is prompt-based learning using a unified text-to-text framework to make full use of the PLMs abilities. (Cui et al., 2021) applies span classification using BART and (Chen et al., 2022b; Yan et al., 2021) use a pointer mechanism to generate indexes of spans and types. (Wang et al., 2022) utilizes instruction fine-tuning and two auxiliary tasks to train T5. Meanwhile, to apply external knowledge to the model, (Chen et al., 2022a) introduces a self-describing mechanism and (Lee et al., 2022a) uses a demonstration-based method. Therefore, our methods introduce in-context learning via instruction fine-tuning together to achieve better few-shot NER abilities, which haven\u2019t been fully discussed yet in seq2seq NER settings."
        },
        {
            "heading": "3 Methodology",
            "text": ""
        },
        {
            "heading": "3.1 NER Definition",
            "text": "NER aims to predict all spans in the input sentence as well as the entity types associated with the spans.\nThe standard flatten-NER can be formulated as follows, given the input sentence containing n tokens X = [x1, x2, ..., xn], the model have to predict the target sentence Y = [l1, l2, ..., ln]. We use VBIO to denote the BIO label set, so \u2200li, li \u2208 VBIO. While in the sequence-to-sequence modeling scenario, the input sentence is still X but instead of predicting Y , the model predict each entity yi = (ei, si) directly, where si represents the entity span in X . And ei \u2208 V represents the entity type of si, where V is the set of entity types.\nMore specifically, we use l and r to indicate the left and right boundary of an entity span in X , so si can be simplified as si = xl:r, where xl:r = [xl, xl+1, ..., xr]. Therefore, the NER model have to predict each yi in X , indicating that the span si belongs to the ei entity type."
        },
        {
            "heading": "3.2 Convert NER to Text-to-text Task",
            "text": "Using language models like T5 (Raffel et al., 2020) to solve most NLP tasks in a unified text-to-text framework can not only fully utilize the knowledge model learned in the pre-training stage but also simplify the training by using same data format, same loss and same model architecture. Moreover, Compared to using simple prompts, using instruction finetuning can further explore the capabilities of the model (Chung et al., 2022; Sanh et al., 2021). Besides, utilizing in-context learning can further enhance the model\u2019s few-shot capabilities in general (Brown et al., 2020) and specifically NER abilities (Lee et al., 2022b). Therefore, we transform the NER task into a text-to-text format and employ instruction finetuning and in-context learning to unleash the model\u2019s few-shot capabilities, as shown in Figure 1. The backbone we used is T5.\nThe basic text-to-text format of the main NER tasks consists of the following three parts, which is inspired by InstructionNER (Wang et al., 2022) 1:\nInstruction The instruction is a prompt that informs the model about the current task it needs to perform. The model is expected to follow the instructions provided within the prompt and complete the task accordingly. The instruction for the main NER task is: Please extract entities and their types from the Sentence, choose entity types from Alternatives.\nSentence The sentence is the input X from which entities need to be extracted.\n1The templates of auxiliary tasks and in-context Example will be discussed in 3.4 and 3.3 respectively.\nAlternatives Alternatives is a list of entity types (V ) split by comma, from which the model needs to select the corresponding type to annotate the corresponding span. Alternatives serves as a constraint and a guiding factor, informing the model that it can only select entity types from within this list.\nIn order to formulate the NER output to natural language, for each NER output yi = (ei, si), we use the following template to convert it to text: si is a/an ei, and we use dot to concatenate all detected entity occurrences yi to form the output text. In terms of the entity types ei, we use natural language to represent the entity instead of adding special tokens to the model 2."
        },
        {
            "heading": "3.3 Auxiliary Tasks",
            "text": "To enhance the NER performance, in addition to the main task, we need to introduce several auxiliary tasks. In InstructionNER (Wang et al., 2022), they employed two auxiliary tasks: entity extraction and entity typing. Moreover, in this paper, a new auxiliary task called type extraction will be introduced. During training, the auxiliary task will also be in the form of text-to-text data, trained alongside the main task data.\nThe auxiliary task primarily aims to improve NER capabilities from three perspectives: understand label semantic, span recognition and entity labeling, since NER can be decomposed into three steps: understand the relationship between the label and semantic meaning of the sentence, then extract\n2e.g. \"Character _Name\" will be represented as \"Character Name\" instead of adding a special token named \"Character _Name\"\nthe spans and finally annotate the given spans. We will discuss the configuration of the auxiliary task in detail from these three perspectives."
        },
        {
            "heading": "3.3.1 Understand label semantic",
            "text": "Type Extraction The goal of the Type Extraction task is to identify all the entity types present in the original sentence. The Instruction is changed to: Please extract all entity types appeared in the Sentence. We will remove the Alternatives in this case, which means that there will be no constraints or hints regarding entity types in the input text, aiming to increase the difficulty of the task. And the output template is: ei type exists in the sentence. The Type Extraction task involves detecting whether a specific entity type appears in the sentence, without focusing on specific spans or associating spans with entity types. This task will assist the model in understanding the meaning of entity types at the overall semantic level of the sentence. We believe that once the model gains a deeper understanding of entity types, it will be able to comprehend the relationship between spans and types more accurately. As a result, it will enhance both span recognition and entity labeling capabilities simultaneously."
        },
        {
            "heading": "3.3.2 Span recognition",
            "text": "Entity Extraction The goal of the entity extraction task is to extract useful entity spans from the original sentence without the need for annotating the extracted spans. The instruction has been modified to: Please extract entities from the Sentence. Because the model doesn\u2019t need to type spans, the Alternatives field is deleted. And the output template has been changed to: si is an entity word, since ei is no longer needed. Because the entity ex-\ntraction task only require the model to predict useful spans regardless of the associated entity types, this task will guide the model to extract correct spans, enhancing the span-F1 accuracy, moreover, overall main task F1 as well (Wang et al., 2022).\nThe original InstructionNER (Wang et al., 2022) paper only employed span concatenation as the output(e.g. s1, s2, s3.). However, we believe that since the output of the main task consists of complete sentences with subject-verb-object structures, it would be more cohesive to follow the same pattern for the auxiliary tasks. And more structured output can fully utilize the PLMs\u2019s understanding of the task as well."
        },
        {
            "heading": "3.3.3 Entity Labeling",
            "text": "Entity Typing The entity typing task aims to type the given span with the correct label. The instruction has been modified to: Please type these entities according to the Sentence: <the given spans>. The Alternatives prompt and output template is the same as those in main task. During training, the given spans in the Instruction is the exact entity spans that have labels on. In entity typing task, since the spans are given, the model doesn\u2019t need to worry about the correctness of the span extracted, so the model can focus more on learning how to label the entity accurately, enhancing the main task NER ability."
        },
        {
            "heading": "3.4 In-Context Learning",
            "text": "In-context learning will be applied to further enhance few-shot NER capabilities. The main approach of in-context learning is to append Examples at the end of the input sentence, hoping that the model can directly learn which spans correspond to which types from these Examples, without the need for additional gradient updates. Besides, the in-context examples are also presented in natural language format, which closely resembles the output text format, serving as a reminder for the model about the desired format it should generate and making it easier for PLMs to understand. This similarity helps bridge the gap and facilitates the model\u2019s comprehension.\nThe in-context example format in NER is inspired by (Lee et al., 2022b). All examples in this context follow the template: span is a/an entitytype. And we will concatenate an additional prompt (based on the knowledge in Examples) after the Instruction to hint the model to learn from the Examples. During training stage, in-context Examples\nwill only be added to the main NER tasks and there will be no Examples added in auxiliary tasks, which will be discussed in detail in Analysis 5.2.\nIn terms of the choices of the samples in Examples, we randomly choose some spans appeared in the train set as well as their corresponding entity types to create Examples. Since we are uncertain about the entity types present in the sentence, we will provide at least one example for each entity type in the Alternatives list within the Examples. The number of samples of each entity types in Examples will also be the same 3(e.g. in terms of MIT Movie dataset, there are 12 entity types. If we set the number of examples to 5, there will be 5 examples for each entity types, resulting in a total of 5*12 examples in the field)."
        },
        {
            "heading": "3.5 Inference",
            "text": "During inference time, we first use the template of the main NER task to wrap the input sentence X , and then feed the sentence to 2INER to get the predicted output text. In terms of the Example field, the example spans are sampled from the training support set, so the model won\u2019t see the groundtruth in the Examples during evaluation, avoiding information leakage. After the output text is generated, a decoding strategy will be applied to get the predicted entity (ei, si): (1) We use dot to split the whole output text to obtain individual sub-texts. (2) We use \"is a\" or \"is an\" to split each sub-text if they can be found. (3) The span is the part before \"is a/an\" and the entity type is the part after it. Once we get the (ei, si), we will check whether si is in the input sentence X and ei is in the set of entity types V . If it doesn\u2019t pass the check, then it isn\u2019t a valid entity and will be deleted. And if any of the three steps result in a match failure, then the sub-text will be skipped."
        },
        {
            "heading": "4 Experiment",
            "text": ""
        },
        {
            "heading": "4.1 Dataset",
            "text": "We conduct NER experiments in standard and low-resources settings. For the rich-resources domain, we use CoNLL-2003 (Tjong Kim Sang and De Meulder, 2003) and for the low-resource domain, we use three datasets: MIT Movie Review, MIT Restaurant Review (Liu et al., 2013) and Airline Travel Information Systems (ATIS) (Hakkani-\n3We refer to \"the number of samples per entity types\" as \"the number of examples\" in the rest of the paper for convenience.\nT\u00fcr et al., 2016), following (Wang et al., 2022; Chen et al., 2022b; Cui et al., 2021; Yan et al., 2021)."
        },
        {
            "heading": "4.2 Implementation settings",
            "text": "In Few-Shot NER scenario, in order to guarantee that each entity type has equal number of instances in the training set, we can\u2019t sample k sentences for each entity type directly because a single sentence may contain multiple entities, so the actual shot will exceed k. Following (Wang et al., 2022), we will apply a greedy sampling strategy (Yang and Katiyar, 2020) instead, to sample the few-shot training set for each setting and due to the randomness of the sampling, we will repeat 3 times for each experiment. We use T5-large 4 as the backbone model for fair comparision with (Wang et al., 2022). In terms of the number of examples in in-context Example field, we set the number to 5 for MIT Movie and MIT Restaurant dataset, and 1 for ATIS dataset as default 5. We only add in-context Example field on main-task, and don\u2019t include them in auxiliary tasks. The ratio of auxiliary tasks is set to 1.0 6. We set the batch size to 2/4/8, learning-rate to 2e5/5e-5 for 10/20/50 Shot settings respectively, and set batch size to 32, learning-rate to 1e-4 for the abundant data setting. The optimizer is Adam and beam search is set to 2. For evaluation, we use F1 score as the metric for NER.\nThe names InstructionNER in the tables mean training with main-task data only, indicating the base model, and the subscript words in the tables indicate addition to the base model: +ET, +EE, +TE, +EX means adding Entity Typing, Entity Extraction, Type Extraction, incontext examples, respectively. And we named InstructionNER+ET,EE,TE,EX as 2INER, which is our final model."
        },
        {
            "heading": "4.3 Standard NER Setting",
            "text": "We use CoNLL-2003 dataset to conduct standard NER experiment. We combine the train and validation set as described in (Yan et al., 2021) to train the model. The result is in Table 1, which shows that even though our method mainly focuses on few-shot NER settings, it remains competitive with\n4https://huggingface.co/t5-large 5ATIS has 79 entity types so we set the number to 1 to avoid excessively long token lengths. 6The data size ratio between main task and each auxiliary tasks. 1.0 means that each sample will be extended into 4 samples: one for main task, one for EE, ET, TE, respectively.\nSOTA algorithm under standard NER setting and even outperform BARTNER (Yan et al., 2021) in span-F1, which is designed for rich-resource NER settings. The performances of 2INER in data abundant nested and discontinuous NER settings are in Appendix A."
        },
        {
            "heading": "4.4 Few-Shot NER Setting",
            "text": "Under Few-Shot NER setting, we only use K-Shot training samples to finetune our model and the results are in Table 2. According to the table, we can find that: (1) Our models consistently outperform InstructionNER as well as other baselines on all three datasets under 10/20/50 Shot settings (except 50Shot in ATIS, which is slightly lower than BARTNER). Especially in MIT Movie dataset, our models have 7.33%, 6.76%, 5.39% improvements compared to InstructionNER under 10/20/50 settings. (2) Our 10Shot model even outperforms TemplateNER\u2019s 50Shot model by 20.73% and 7.06% in MIT Movie and MIT Restaurant respectively, which highlights the superiority and capability of our model. (3) We have the same finding as InstructionNER (Wang et al., 2022) that F1 improvements are much more significant on MIT Movie than on MIT Restaurant (7.33% / 6.76% / 5.39% v.s. 6.86% / 3.24% / 3.3% under 10/20/50 Shot settings), which indicates that although MIT Movie has more entity types, text-to-text framework and instruction-tuning can better utilize pre-training knowledge, and through in-context learning, the model can learn more about the relationships between entities. (4) In ATIS dataset, the improve-\nment of our model is less significant compared to other two dataset. We argue that this is because ATIS contains 79 entity types and even if we only provide one sample span for each entity types in in-context Example field, the average token length is 1099 compared to 368 with or without examples, where the token length of the Alternative filed is 327. So the actual input Sentence X only accounts for 3.7% of the total token length, which increases the difficulty for the model to extract key information from lengthy sentences. 7 So too many entity types may potentially reduce model improvements."
        },
        {
            "heading": "4.5 Ablation Study",
            "text": "In order to find out the influence of our proposed type extraction task and in-context examples on model\u2019s few-shot abilities, we conduct ablation studies in Figure 2. The results indicate that adding type extraction task and in-context examples can further enhance the model\u2019s few-shot NER abilities. We set InstructionNER as the baseline here which only trains on main-task data without any auxiliary tasks. Then we add type extraction task (InstructionNER+TE) or in-context examples (InstructionNER+EX) respectively on the baseline model to explore their influences. The results from Figure 2 shows that in terms of 10/20/50 Shot settings in few-shot NER, type extraction task achieves an average improvements of 7.21%, 4.86%, 4.35% F1 and in-context example achieves an average improvements of 6.76%, 3.84%, 3.84% F1 in MIT Movie and MIT Restaurant dataset.\nMoreover, adding type extraction task can greatly increase the Span-F1 as well. Because Span-F1 indicates the model\u2019s ability to locate\n7We try to use special-tokens to represent the entity types, but the F1 is slightly lower than without using special-tokens and the proportion of X to the total number of tokens is 4.5%.\nspans, the results reveal that through training on type extraction task, span recognition can be benefit from having a deeper understanding of the labels from the overall semantic level of sentence. Therefore, it proves the effectiveness of three steps of NER abilities we proposed in 3.3, and shows that type extraction task can simultaneously improve span recognition and entity labeling abilities through understanding label semantic."
        },
        {
            "heading": "5 Analysis",
            "text": ""
        },
        {
            "heading": "5.1 Increase Example Number",
            "text": "In this section, we will focus on how the number of examples in in-context Example field influence the model performance. We will sequentially change the number of examples to 1, 3, 5, 10, and 15, and train corresponding models to observe the change of F1 on MIT Restaurant dataset. We train our model with main-task and in-context example without any auxiliary tasks (InstructionNER+EX) in this section. The results are in Table 3.\nAs the number of examples increases, F1 score continues to increase and the largest improvement in F1 score occurs when going from zero examples to one example. As the number of examples increases further, the F1 will continue to increase but the rate of improvement gradually slows down. This suggests that when only one in-context example is provided, the model can quickly learn the specific meanings of each entity type from the example. While more examples may lead to repetitive cues to the model so a balance should be made between model performance and computational cost."
        },
        {
            "heading": "5.2 Effect of In-Context Example on Auxiliary task",
            "text": "In this section, we will discuss whether to add incontext examples on auxiliary task. The model is 2INER (InstructionNER+ET,EE,TE,EX) and we will compare two settings: add examples only on maintask, add examples on main-task as well as three auxiliary tasks. The results in Table 4 indicates that adding examples on auxiliary task will slightly decrease the F1 performance. Because adding examples to auxiliary tasks may potentially reduce their difficulty and make it too easy for the model, thereby reducing the auxiliary tasks\u2019 effectiveness in aiding the main task. So adding examples only to the main task is a better approach."
        },
        {
            "heading": "5.3 Increase Shot",
            "text": "In this section, we will discuss the model performance under relatively abundant settings. We increase the shots to 100, 200 and 500 in MIT Movie and MIT Restaurant datasets using 2INER (InstructionNER+ET,EE,TE,EX). As shown in Table 5, compared to InstructionNER, 2INER achieves 5.43%, 3.98%, 3.19% improvements in F1 under 100/200/500 shots settings respectively.\nAnd 2INER outperforms LightNER in all settings except 500-shots in MIT Restaurant, which shows that 2INER has great NER abilities under data abundant scenario as well. We argue that the in-context Example field may help the model to learn from more diverse samples from the abundant training set and turn the general knowledge into specialized capabilities, leading to the improvement in F1."
        },
        {
            "heading": "6 Conclusion",
            "text": "In this paper, we propose 2INER for few-shot NER using both instruction finetuning and in-context learning by converting NER into a text-to-text framework. Based on InstructionNER, we create a template to concatenate task-specific instructions, input sentence and entity alternatives to make full use of the pre-training knowledge. Besides, we decompose NER into three steps and introduce another auxiliary tasks, called type extraction, to help the model better understand the general semantic meaning of the entity types, which can improve both span recognition and entity labeling abilities. Moreover, we apply the in-context examples to enable the model to learn from additional contextual information, enhancing few-shot abilities. Multiple experiments on four NER datasets prove 2INER\u2019s effectiveness in few-shot NER scenario by consistently outperforming other baselines.\nLimitations\nOne limitation of our work is the extensive length of the Example and Alternative field when there are too many existed entity types. While incorporating in-context examples in the input sentence can improve few-shot NER performance, it poses a challenge when the Example field becomes too long because we add at least one examples for each potential entity type, especially when the Alternative list contains numerous entity types. This can result in less improvement gains and more computational costs. To address this issue, we assume that larger PLMs such as the recently proposed LLaMA (Touvron et al., 2023) could potentially be explored in future research as a means of resolution.\nEthics Statement\nIn consideration of ethical concerns, we would make the following descriptions: (1) All of our experiments are conducted using existing datasets sourced from publicly available scientific papers. (2) Our few-shot methods don\u2019t require a lot of computational resources. (3) Our text generation models will generate texts based on existing templates, so it won\u2019t generate harmful sentences."
        },
        {
            "heading": "A Appendix",
            "text": "In this section, we will discuss the remaining two NER settings: nested NER and discontinuous NER. Because the text-to-text structure of our proposed method can be easily adapted to all three NER settings, which will result in a unified structure for solving NER problems. Here, we mainly discuss standard NER scenarios with abundant data.\nFor data abundant nested NER, We conduct experiments on Genia (Kim et al., 2003). We follow BARTNER (Yan et al., 2021) to use five entities types and split the train, dev, test as 8.1:0.9:1.0. The results are in Table 6.\nFor data abundant discontinuous NER, we conduct experiments on CADEC (Karimi et al., 2015).\nFollowing BARTNER (Yan et al., 2021), since only the Adverse Drug Events (ADEs) entities include discontinuous data, only these entities were considered. The results are in Table 7.\nThe experiment settings are the same as flat NER. We use T5-large as the backbone model and report span-level F1. The results show that in data abundant nested and discontinuous NER setting, our proposed method greatly outperforms BARTNER (Yan et al., 2021) and other SOTA methods, which demonstrates that our methods do have a potential to handle different NER settings in a unified framework."
        }
    ],
    "title": "2INER: Instructive and In-Context Learning on Few-Shot Named Entity Recognition",
    "year": 2023
}