{
    "abstractText": "Contrastive learning is a powerful self-supervised learning method, but we have a limited theoretical understanding of how it works and why it works. In this paper, we prove that contrastive learning with the standard InfoNCE loss is equivalent to spectral clustering on the similarity graph. Using this equivalence as the building block, we extend our analysis to the CLIP model and rigorously characterize how similar multi-modal objects are embedded together. Motivated by our theoretical insights, we introduce the Kernel-InfoNCE loss, incorporating mixtures of kernel functions that outperform the standard Gaussian kernel on several vision datasets\u2021.",
    "authors": [
        {
            "affiliations": [],
            "name": "SIMILARITY GRAPH"
        },
        {
            "affiliations": [],
            "name": "Zhiquan Tan"
        },
        {
            "affiliations": [],
            "name": "Yifan Zhang"
        },
        {
            "affiliations": [],
            "name": "Jingqin Yang"
        },
        {
            "affiliations": [],
            "name": "Yang Yuan"
        }
    ],
    "id": "SP:4db09f5f41275e32f6a5613e8dc1e1bbb35eb773",
    "references": [
        {
            "authors": [
                "Sanjeev Arora",
                "Hrishikesh Khandeparkar",
                "Mikhail Khodak",
                "Orestis Plevrakis",
                "Nikunj Saunshi"
            ],
            "title": "A theoretical analysis of contrastive unsupervised representation learning",
            "venue": "arXiv preprint arXiv:1902.09229,",
            "year": 1902
        },
        {
            "authors": [
                "Randall Balestriero",
                "Yann LeCun"
            ],
            "title": "Contrastive and non-contrastive self-supervised learning recover global and local spectral embedding methods",
            "venue": "arXiv preprint arXiv:2205.11508,",
            "year": 2022
        },
        {
            "authors": [
                "Suzanna Becker",
                "Geoffrey E Hinton"
            ],
            "title": "Self-organizing neural network that discovers surfaces in random-dot stereograms",
            "venue": "Nature, 355(6356):161\u2013163,",
            "year": 1992
        },
        {
            "authors": [
                "Ting Chen",
                "Simon Kornblith",
                "Mohammad Norouzi",
                "Geoffrey Hinton"
            ],
            "title": "A simple framework for contrastive learning of visual representations",
            "venue": "In International conference on machine learning,",
            "year": 2020
        },
        {
            "authors": [
                "Xinlei Chen",
                "Haoqi Fan",
                "Ross B. Girshick",
                "Kaiming He"
            ],
            "title": "Improved baselines with momentum contrastive learning, 2020b",
            "year": 2020
        },
        {
            "authors": [
                "Jiequan Cui",
                "Zhisheng Zhong",
                "Shu Liu",
                "Bei Yu",
                "Jiaya Jia"
            ],
            "title": "Parametric contrastive learning",
            "venue": "In Proceedings of the IEEE/CVF international conference on computer vision,",
            "year": 2021
        },
        {
            "authors": [
                "Jia Deng",
                "Wei Dong",
                "Richard Socher",
                "Li-Jia Li",
                "Kai Li",
                "Li Fei-Fei"
            ],
            "title": "Imagenet: A large-scale hierarchical image database",
            "venue": "IEEE conference on computer vision and pattern recognition,",
            "year": 2009
        },
        {
            "authors": [
                "Zhijie Deng",
                "Jiaxin Shi",
                "Hao Zhang",
                "Peng Cui",
                "Cewu Lu",
                "Jun Zhu"
            ],
            "title": "Neural eigenfunctions are structured representation learners",
            "venue": "arXiv preprint arXiv:2210.12637,",
            "year": 2022
        },
        {
            "authors": [
                "Lijie Fan",
                "Dilip Krishnan",
                "Phillip Isola",
                "Dina Katabi",
                "Yonglong Tian"
            ],
            "title": "Improving clip training with language rewrites",
            "venue": "arXiv preprint arXiv:2305.20088,",
            "year": 2023
        },
        {
            "authors": [
                "Raia Hadsell",
                "Sumit Chopra",
                "Yann LeCun"
            ],
            "title": "Dimensionality reduction by learning an invariant mapping",
            "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR\u201906),",
            "year": 2006
        },
        {
            "authors": [
                "Jeff Z HaoChen",
                "Colin Wei",
                "Adrien Gaidon",
                "Tengyu Ma"
            ],
            "title": "Provable guarantees for self-supervised deep learning with spectral contrastive loss",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Kaveh Hassani",
                "Amir Hosein Khasahmadi"
            ],
            "title": "Contrastive multi-view representation learning on graphs",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Kaiming He",
                "Haoqi Fan",
                "Yuxin Wu",
                "Saining Xie",
                "Ross Girshick"
            ],
            "title": "Momentum contrast for unsupervised visual representation learning",
            "venue": "arXiv preprint arXiv:1911.05722,",
            "year": 2019
        },
        {
            "authors": [
                "Kaiming He",
                "Haoqi Fan",
                "Yuxin Wu",
                "Saining Xie",
                "Ross Girshick"
            ],
            "title": "Momentum contrast for unsupervised visual representation learning",
            "venue": "arXiv preprint arXiv:1911.05722,",
            "year": 1911
        },
        {
            "authors": [
                "Tianyang Hu",
                "Zhili Liu",
                "Fengwei Zhou",
                "Wenjia Wang",
                "Weiran Huang"
            ],
            "title": "Your contrastive learning is secretly doing stochastic neighbor embedding",
            "venue": "arXiv preprint arXiv:2205.14814,",
            "year": 2022
        },
        {
            "authors": [
                "Sergey Ioffe",
                "Christian Szegedy"
            ],
            "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
            "venue": "In International Conference on Machine Learning,",
            "year": 2015
        },
        {
            "authors": [
                "Yannis Kalantidis",
                "Mert Bulent Sariyildiz",
                "Noe Pion",
                "Philippe Weinzaepfel",
                "Diane Larlus"
            ],
            "title": "Hard negative mixing for contrastive learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Alex Krizhevsky",
                "Geoffrey Hinton"
            ],
            "title": "Learning multiple layers of features from tiny images",
            "year": 2009
        },
        {
            "authors": [
                "Ya Le",
                "Xuan Yang"
            ],
            "title": "Tiny imagenet visual recognition challenge",
            "venue": "CS 231N,",
            "year": 2015
        },
        {
            "authors": [
                "Liam Li",
                "Kevin Jamieson",
                "Afshin Rostamizadeh",
                "Ekaterina Gonina",
                "Moritz Hardt",
                "Benjamin Recht",
                "Ameet Talwalkar"
            ],
            "title": "Massively parallel hyperparameter tuning",
            "venue": "arXiv preprint arXiv:1810.05934,",
            "year": 2018
        },
        {
            "authors": [
                "Richard Liaw",
                "Eric Liang",
                "Robert Nishihara",
                "Philipp Moritz",
                "Joseph E Gonzalez",
                "Ion Stoica"
            ],
            "title": "Tune: A research platform for distributed model selection and training",
            "venue": "arXiv preprint arXiv:1807.05118,",
            "year": 2018
        },
        {
            "authors": [
                "Vinod Nair",
                "Geoffrey E Hinton"
            ],
            "title": "Rectified linear units improve restricted boltzmann machines",
            "venue": "In Icml,",
            "year": 2010
        },
        {
            "authors": [
                "Aaron van den Oord",
                "Yazhe Li",
                "Oriol Vinyals"
            ],
            "title": "Representation learning with contrastive predictive coding",
            "venue": "arXiv preprint arXiv:1807.03748,",
            "year": 2018
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Yonglong Tian",
                "Chen Sun",
                "Ben Poole",
                "Dilip Krishnan",
                "Cordelia Schmid",
                "Phillip Isola"
            ],
            "title": "What makes for good views for contrastive learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Yuandong Tian"
            ],
            "title": "Understanding deep contrastive learning via coordinate-wise optimization",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Hugues Van Assel",
                "Thibault Espinasse",
                "Julien Chiquet",
                "Franck Picard"
            ],
            "title": "A probabilistic graph coupling view of dimension reduction",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Tongzhou Wang",
                "Phillip Isola"
            ],
            "title": "Understanding contrastive representation learning through alignment and uniformity on the hypersphere",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Yifei Wang",
                "Qi Zhang",
                "Yisen Wang",
                "Jiansheng Yang",
                "Zhouchen Lin"
            ],
            "title": "Chaos is a ladder: A new theoretical understanding of contrastive learning via augmentation overlap",
            "venue": "arXiv preprint arXiv:2203.13457,",
            "year": 2022
        },
        {
            "authors": [
                "Yang You",
                "Igor Gitman",
                "Boris Ginsburg"
            ],
            "title": "Scaling sgd batch size to 32k for imagenet training",
            "venue": "arXiv preprint arXiv:1708.03888,",
            "year": 2017
        },
        {
            "authors": [
                "Yuning You",
                "Tianlong Chen",
                "Yongduo Sui",
                "Ting Chen",
                "Zhangyang Wang",
                "Yang Shen"
            ],
            "title": "Graph contrastive learning with augmentations",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Contrastive learning has emerged as one of the most prominent self-supervised learning methods, especially in the realm of vision tasks (Chen et al., 2020a; He et al., 2019b). This approach trains a neural network to map a set of objects into an embedding space, ensuring that similar objects are closely positioned while dissimilar objects remain distanced. The InfoNCE loss, exemplified by SimCLR (Chen et al., 2020a), is a widely employed loss function for achieving this goal.\nIn their inspiring work, HaoChen et al. (2021) demonstrated that by replacing the standard InfoNCE loss with their spectral contrastive loss, contrastive learning performs spectral clustering on the population augmentation graph. However, the spectral contrastive loss is seldom utilized in practice and is not applicable for analyzing the performance of various similarity functions in the embedding space. Furthermore, when employing the spectral contrastive loss, the final embedding constitutes a combination of standard spectral clustering and an additional linear transformation. Consequently, existing results do not establish a connection between the original InfoNCE loss and standard spectral clustering.\nIn this paper, we prove that SimCLR, the standard contrastive learning method, performs spectral clustering without modifying the InfoNCE loss or applying additional transformations to the embeddings. Our analysis involves a collection of n objects X = [X1, \u00b7 \u00b7 \u00b7 ,Xn] within space X . For these objects, we define a similarity graph with an adjacency matrix \u03c0, such that \u03c0i,j represents the probability of Xi and Xj being paired together in the data augmentation step of contrastive learning. Notice that \u03c0 can be in general asymmetric.\nGiven this similarity graph, we aim to discover an embedding function f : X \u2192 Z . Denote Z \u225c f(X) as the embedding of X, and our objective is to ensure that the Gram matrix KZ with kernel k representing the similarities for Z closely approximates \u03c0. Please refer to Figure 1 for an illustration.\nHowever, directly comparing \u03c0 with KZ can be difficult, as there are too many edges in both graphs. Therefore, we define two Markov random fields (MRFs) based on \u03c0 and KZ and compare the MRFs instead. Each MRF introduces a probability distribution of unweighted directed subgraphs on n\n\u2217Equal contribution. \u2020Corresponding author. \u2021The code is available at https://github.com/yifanzhang-pro/Kernel-InfoNCE.\nobjects (Van Assel et al., 2022), denoted as WX and WZ respectively. As a natural approximation to the ideal loss between \u03c0 and KZ, we employ the cross-entropy loss between WX and WZ. Our paper\u2019s surprising discovery is that the InfoNCE loss is equivalent to the cross-entropy loss when each subgraph is constrained to have an out-degree of exactly one. Furthermore, when k is the Gaussian kernel, optimizing the cross-entropy loss corresponds to executing spectral clustering on \u03c0. By combining these two observations, we conclude that employing the InfoNCE loss is equivalent to performing spectral clustering.\nOur characterization of contrastive learning hinges on two crucial factors: the augmentation step that defines a similarity graph, and the InfoNCE loss that measures the distance between two MRFs. Consequently, any other models incorporating these two factors can be similarly analyzed. Notably, the CLIP (Radford et al., 2021) model for multi-modal learning fits within this paradigm. Utilizing the same framework, we establish a representation theorem for CLIP, demonstrating that it performs spectral clustering on the bipartite graph induced by the paired training data.\nIs it possible to improve the InfoNCE loss by using a different kernel? Based on the maximum entropy principle, we demonstrate that the exponential kernels are the natural choices for capturing the local similarity structure for contrastive learning. Empirically, we observe that taking the mixture of Gaussian and Laplacian kernels, which maintain the aforementioned properties, can achieve better performance than the Gaussian kernel on several benchmark vision datasets.\nIn summary, our main contributions include:\n\u2022 We prove the equivalence of SimCLR and spectral clustering on the similarity graph. \u2022 We extend our analysis to the multi-modal setting and prove the equivalence of CLIP and spectral\nclustering on the multi-modal similarity graph. \u2022 Inspired by theory, we propose a new Kernel-InfoNCE loss with mixture of kernel functions\nthat achieves better performance than the standard Gaussian kernel (SimCLR) empirically on the benchmark vision datasets."
        },
        {
            "heading": "2 BACKGROUND",
            "text": "In this section, we will introduce the basic knowledge we will use throughout the paper. In this paper, we use objects to denote data points like images or texts. Given a matrix X, we use Xi to denote its i-th row, and Xi,j to denote its (i, j)-th entry. Same holds for matrices like WX, where we use WX,i and WX,i,j , respectively."
        },
        {
            "heading": "2.1 CONTRASTIVE LEARNING: SIMCLR",
            "text": "Given a query object q \u2208 X , one similar object (positive samples) p1 for q, and N \u2212 1 other objects {pi}Ni=2, SimCLR finds a function f (usually a neural network) that maps these objects to Z , to minimize the InfoNCE loss of q:\nL(q,p1, {pi}Ni=2) = \u2212 log exp(sim(f(q),f(p1))/\u03c4)\u2211N i=1 exp(sim(f(q),f(pi))/\u03c4)\n(1)\nHere, the actual loss of f takes the summation over different qs, and \u03c4 is a temperature hyperparameter. The sim(Zi,Zj) function measures the similarity between Zi,Zj in Z , and is commonly defined as sim(Zi,Zj) = Z\u22a4i Zj \u2225Zi\u2225\u2225Zj\u2225 , or Z \u22a4 i Zj , or \u2212\u2225Zi \u2212 Zj\u22252/2. In this paper, we consider the case that Z is the unit sphere, i.e., \u2225Zi\u2225 = \u2225Zj\u2225 = 1. This is because both SimCLR and CLIP have a normalization step in the implementation (Chen et al., 2020a; Radford et al., 2021). Hence, Z \u22a4 i Zj\n\u2225Zi\u2225\u2225Zj\u2225 = Z \u22a4 i Zj , and\n\u2212\u2225Zi \u2212 Zj\u22252/2 = \u2212Z2i /2\u2212 Z2j/2 + Z\u22a4i Zj = \u22121 + Z\u22a4i Zj . (2) Therefore, these losses are the same up to a constant."
        },
        {
            "heading": "2.2 MULTI-MODAL LEARNING: CLIP",
            "text": "CLIP (Radford et al., 2021) is a multi-modal model with a dataset containing millions of (image, text) pairs. During pretraining, for each batch of N pairs of data points, CLIP uses an image encoder and a text encoder to get N pairs of embeddings, and uses the InfoNCE loss to compute the correct N pairs out of N \u00d7 N possible connections. Specifically, given an image ai, we compare the its matching score of the paired text bi, with the matching scores of other N \u2212 1 texts {bj}j \u0338=i, using the loss L(ai,bi, {bj}j \u0338=i) defined in Eqn. (1) by setting sim(Zi,Zj) = Z \u22a4 i Zj\n\u2225Zi\u2225\u2225Zj\u2225 . One can define the loss similarly for text, and the actual loss of the embedding network f takes the summation over all the images and texts."
        },
        {
            "heading": "2.3 REPRODUCING KERNEL HILBERT SPACE",
            "text": "Given two objects Zi,Zj \u2208 Z , consider a feature map \u03c6 : Z \u2192 H, where the feature space H is usually much larger than Z . We may define a kernel k that measures the similarity of Zi,Zj as k(Zi,Zj) \u225c \u27e8\u03c6(Zi), \u03c6(Zj)\u27e9H, i.e., the inner product between the two objects after mapping them to the feature space. For any vector h \u2208 H, it also corresponds to a function h(\u00b7) : Z \u2192 R, defined as h(Zi) = \u27e8h, \u03c6(Zi)\u27e9H. Specifically, \u03c6(Zj) as a vector in H represents the function k(\u00b7,Zj) : Z \u2192 R, because for any Zi \u2208 Z , we have k(Zi,Zj) = \u27e8\u03c6(Zi), \u03c6(Zj)\u27e9H. Formally, we have: Definition 2.1 (Reproducing kernel Hilbert space). Let H be a Hilbert space of R-valued functions defined on a non-empty set Z . A function k : Z \u00d7 Z \u2192 R is called a reproducing kernel of H, and H is a reproducing kernel Hilbert space, if k satisfies\n\u2022 \u2200Zi \u2208 Z, k(\u00b7,Zi) \u2208 H, \u2022 \u2200Zi \u2208 Z,\u2200h \u2208 H, \u27e8h, k(\u00b7,Zi)\u27e9H = h(Zi).\nWe focus on the translation-invariant kernel in our paper, where the kernel k(Zi,Zj) can always be written as k\u2032(Zi \u2212 Zj) for k\u2032 \u2208 Z \u2192 R. The Moore\u2013Aronszajn\u2019s theorem states that if k is a symmetric, positive definite kernel on Z , there is a unique Hilbert space of functions H on Z for which k is a reproducing kernel.\nFor instance, the Gaussian kernel is a symmetric, positive definite kernel that yields an RKHS with infinite dimensions. One of the advantages of a reproducing kernel is that the similarity can be computed directly in Z without using the feature map to go to the potentially infinite dimensional Hilbert space. However, a reproducing kernel\u2019s similarity structure should ideally align with the semantic meanings of specific tasks. For example, it is unlikely to calculate the semantic similarity of two images directly using a predefined reproducing kernel in the pixel space.\nConsequently, we ask if it is possible to find an embedding function f : X \u2192 Z , where Z can compute the similarity of two objects in X with a predefined kernel function, i.e., whether KZ matches with \u03c0 in Figure 1. In other words, we hope to map the objects to a space where the semantic similarity in X is naturally embedded. This is the starting point of our paper."
        },
        {
            "heading": "2.4 MARKOV RANDOM FIELD",
            "text": "In this subsection, we present the framework (without proofs) of MRF for dimension reduction (Van Assel et al., 2022). We have modified some definitions and lemmas for our learning scenarios, and the readers may check the paper for more details on this framework.\nConsider n objects Z = [Z1, \u00b7 \u00b7 \u00b7 ,Zn] in Z . We use a symmetric and translation invariant kernel k : Z \u2192 R+ to represent the similarities in Z , where symmetric means k(x) = k(\u2212x). Given Z and k, we define the gram matrix as KZ \u225c (k(Zi \u2212 Zj))(i,j)\u2208[n]2 , which is also the adjacency matrix representing the similarities of objects in Z.\nDue to the large size of \u03c0 and in practice \u03c0 is usually formed by using positive samples sampling and hard to explicitly construct, directly comparing KZ and \u03c0 can be difficult, so we treat them as MRFs and compare the induced probability distributions on subgraphs instead. In our paper, subgraphs are directed unweighted graphs from the set SW \u225c {W \u2208 {0, 1}n\u00d7n | \u2200(i, j) \u2208 [n]2,Wi,i = 0}. The distribution of W is generally defined as follows. Definition 2.2 (Distribution of W). Let \u03c0 \u2208 Rn\u00d7n+ , we define the distribution P(W;\u03c0) \u221d \u2126(W)\u03a0(i,j)\u2208[n]2\u03c0 Wi,j i,j , where \u2126(W) \u225c \u03a0iI\u2211j Wi,j=1 is called a unitary out-degree filter.\nTo provide a clearer interpretation of the definition, we can break down the expression \u2126(W)\u03a0(i,j)\u2208[n]2\u03c0 Wi,j i,j into two parts. Firstly, if we view W as the adjacency of a graph, the unitary out-degree filter \u2126(W) checks if each node i of the graph has exactly one out-going edge. Therefore, only subgraphs with a unitary out-degree will be preserved, while subgraphs with other out-degree values will be filtered out. As we will see later, this exactly corresponds to the setting that the InfoNCE loss uses exactly one positive neighbor. Secondly, \u03a0(i,j)\u2208[n]2\u03c0 Wi,j i,j multiplies the scores of each edge in \u03c0 compared with Wi,j . This multiplication results in the un-normalized likelihood of W under \u03c0, it is noticeable that the constraint of single outgoing edge of each node that ensures that the multiplication results reflect the consistent of W and \u03c0. See Figure 2 for an illustration.\nBy applying Definition 2.2 to KZ, we obtain the following expression for P(W;KZ): P(W;KZ) \u221d \u2126(W)\u03a0(i,j)\u2208[n]2k(Zi \u2212 Zj)Wi,j . This expression represents the prior probability of W under KZ.\nDue to the unitary out-degree filter, P(W;\u03c0) has the following property. Lemma 2.3. For W \u223c P(\u00b7;\u03c0), \u2200i \u2208 [n],Wi \u223c M(1,\u03c0i/ \u2211 j \u03c0i,j), where M is the multinomial distribution. Moreover, given any i, i\u2032 \u2208 [n], Wi is independent to Wi\u2032 . Where Wi is the i-th row of W, \u03c0i is the i-th row of \u03c0.\nBelow we define the cross entropy loss given distribution \u03c0 and the similarity matrix KZ.\nHk\u03c0(Z) \u225c \u2212EWX\u223cP(\u00b7;\u03c0)[logP(WZ = WX;KZ)] (3) The following lemma will be helpful in analyzing the cross-entropy loss, which states that when the two distributions can be aligned and decomposed, their cross-entropy loss can also be decomposed. Lemma 2.4. Assume X = X1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 Xk and there are two probability distributions P and Q supported on X . Suppose P = P1 \u2297 \u00b7 \u00b7 \u00b7 \u2297Pk and Q = Q1 \u2297 \u00b7 \u00b7 \u00b7 \u2297Qk, with Pi and Qi supported on Xi. Let H(P,Q) \u225c \u2212Ex\u223cP[logQ(x)]. Then H(P,Q) = \u2211k i=1 H(Pi,Qi).\nThe next lemma shows that the cross-entropy loss can be converted to the combination of repulsion and attraction terms.\nLemma 2.5. minZ Hk\u03c0(Z) is equivalent to\nmin Z\n\u2212 \u2211\n(i,j)\u2208[n]2 Pi,j log k(Zi \u2212 Zj) + logR(Z), (4)\nwhere P = EWX\u223cP(\u00b7;\u03c0)[WX], and R(Z) = \u2211\nW\u2208SW P(Z,W) with P(Z,W) \u221d fk(Z,W)\u2126(W).\nThe second term in Eqn. (4) punishes trivial solutions like Z = 0, as 0 is a mode for fk(\u00b7,W) for any W, which incurs large logR(Z). The first term can be interpreted using the graph Laplacian operator, defined below.\nDefinition 2.6 (Graph Laplacian operator). The graph Laplacian operator is a function L that maps a n\u00d7 n non-negative matrix to a positive semi-definite matrix such that:\n\u2200i, j \u2208 [n]2,L(W)i,j =  \u2212Wi,j if i \u0338= j\u2211\nk\u2208[n]\nWi,k o.w.\nWe will then introduce the definition of spectral clustering used in our paper.\nDefinition 2.7 (Spectral Clustering). Let W \u2208 Rn\u00d7n+ be the adjacency matrix of a graph and L be the graph Laplacian operator. Then the following optimization problem is called performing spectral clustering on the graph whose adjacency matrix is W:\nmin Z\ntr(Z\u22a4L(W)Z) + E(Z), (5)\nwhere E(Z) is a regularization term.\nBy simple calculation, when k is the Gaussian kernel, the first term in Eqn. (4) becomes tr(Z\u22a4L\u2217Z) where L\u2217 = EWX\u223cP(\u00b7;\u03c0)[L(WX)]. In other words, Eqn. (4) is equivalent to doing spectral clustering with a repulsion regularizer logR(Z)."
        },
        {
            "heading": "3 CONSTRASTIVE LEARNING: SIMCLR",
            "text": "In this section, we will prove our main theorem that contrastive learning is spectral clustering on a similarity graph. We assume that there are finitely many objects in X , denoted as n. This is the same assumption used by HaoChen et al. (2021), who also demonstrated that the finite case can be easily extended to the infinite case by replacing sum by integral, adjacency matrix by adjacency operator, etc. For continuous augmentation methods like adding Gaussian noise, we can discretize it in a natural way. Assuming a finite number of objects can help us avoid non-essential technical jargon.\nWith n objects in X , consider a similarity graph defined on these objects, which gives a similarity matrix \u03c0 of size n \u00d7 n. However, for real scenarios like learning images, it is extremely difficult to obtain such \u03c0 from human labeling. Therefore, we compute \u03c0 using the prior knowledge of the dataset. For example, in the original SimCLR paper (Chen et al., 2020a), there are 9 different augmentation methods. Each augmentation method may generate many different augmented images that look similar to the original image. For every original image Xi, we define a probability distribution \u03c0i, such that each object Xj gets sampled with probability \u03c0i,j . For example, during the augmentation process, suppose Xj has a probability, say 1/9, to be an augmentation of Xi, then \u03c0i,j = 1/9. Therefore, \u03c0i can be represented as a vector in Rn+.\nStacking all probability distributions \u03c0i together for i \u2208 [n], we get a matrix \u03c0 \u2208 Rn\u00d7n+ . In this section, we assume the sampling process is symmetric, i.e., \u03c0i,j = \u03c0j,i. The stochastic data augmentation samples WX based on \u03c0, i.e., WX \u223c P(\u00b7;\u03c0)."
        },
        {
            "heading": "3.1 MAIN THEOREM",
            "text": "Theorem 3.1. For the SimCLR algorithm, denote f as the neural network, Z := f(X), and \u03c0 as the similarity graph defined by the data augmentation process where objects are connected iff they are positive samples of each other. Then SimCLR is equivalent to solving the following program:\nmin Z\ntr(Z\u22a4L(\u03c0)Z) + logR(Z),\nwhich runs spectral clustering on \u03c0.\nProof. Please refer to Appendix A.\nDiscussions. Empirically, the InfoNCE loss is applied to a large batch of the object, rather than all the n objects that Theorem 3.1 requires. This explains why SimCLR benefits from larger batch size, e.g. Chen et al. (2020a) use a batch size of 4096, and He et al. (2019b) use an even large memory bank for storing more samples.\nWhile using the same framework from (Van Assel et al., 2022), our Theorem 3.1 is significantly different from their results on dimension reduction from at least two aspects. Firstly, in the object space X , we have a predefined similarity graph \u03c0, but they were using a kernel matrix KX based on X and a kernel kX . This is because in the dimension reduction setting, the input objects are assumed to be well-structured data points, but in the self-supervised learning setting, the input objects are images or texts, where a translation invariant kernel cannot be used for computing the similarities. Secondly, their cross-entropy loss is directly computed from KX, while WX is never explicitly sampled. In contrast, in our method, \u03c0 is never explicitly used, and the cross entropy loss is indirectly computed from the randomly sampled WX.\nThe equivalence we proved is exact. Therefore, after learning, the embedding space contains different components, corresponding to various (sub-) classes of the objects. This characterization naturally explains why contrastive learning works well for classification-related downstream tasks."
        },
        {
            "heading": "4 MULTI-MODAL LEARNING: CLIP",
            "text": "In this subsection, we extend Theorem 3.1 to the multi-modal setting by analyzing CLIP, which applies the contrastive loss to the image-text pairs. The image-text pairs can be represented with the following pair graph. Definition 4.1 (Pair graph). Consider two modalities of objects A,B, and undirected unit-weight edges E = {(ai,bi) | ai \u2208 A,bi \u2208 B}Mi=1. The pair graph between A,B is a directed bipartite graph \u03c0A,B = (A,B,E), with the weight of each outgoing edge normalized by the out-degree of the node.\nBy definition, \u03c0A,B is not necessarily symmetric. Consider the case where the dataset contains 10 images of \u201cdog\u201d, all of them are connected to the same text \u201cdog\u201d. In this case, the text dog has 1/10 probability to each image, while each image has only one edge with 100% probability to the text. However, since each row of \u03c0A,B is still a probability distribution, we still have the next theorem. Theorem 4.2 (CLIP\u2019s objective). For the CLIP algorithm, denote \u03c0A,B as the pair graph. Then CLIP is equivalent to running the generalized spectral clustering on \u03c0A,B.\nProof. Please refer to Appendix A.\nDiscussions. In Theorem 4.2, we say CLIP runs the generalized spectral clustering because L(\u03c0A,B) is not necessarily the Laplacian of a symmetric graph, although one can still compute the optimal embedding Z following Eqn. (4). The pair graph may contain a huge number of isolated edges. Empirically, CLIP picks strong image and text encoders with good prior knowledge about the dataset. Such prior knowledge may bias towards a better embedding for grouping the isolated edges with more semantics.\nTheorem 4.2 also assumes that all the objects are sampled in WX, while empirically a really big batch size of 32,768 is used in Radford et al. (2021). Moreover, the probability distribution P(\u00b7;\u03c0A,B)\nused in Theorem 4.2 is slightly different from the implementation of CLIP, in the sense that CLIP uniformly samples the edges in E, but here we uniformly sample the objects in A \u222a B. When the image-text pairs dataset has high quality, the difference between these two sampling schemes becomes negligible as the variance of object out-degrees is extremely small."
        },
        {
            "heading": "4.1 APPLYING TO LACLIP",
            "text": "Due to computation resources limitations, we haven\u2019t implemented an improved CLIP algorithm ourselves. Interestingly, a direct improvement to CLIP based on our theory was recently conducted. We shall present this algorithm LaCLIP carefully (Fan et al., 2023) and discuss why it can be seen as a direct application of our theory.\nRoughly speaking, LaCLIP is a direct extension of CLIP by not only incorporating image augmentations but using text augmentation as well. Specifically, we can treat language rewrites as text augmentations. For each image text pair (xI , xT ), the text augmentation can be derived as follows:\naugT (xT ) \u223c Uniform ([xT0 , xT1 . . . , xTM ]) , (6) where xTi is the text xT itself or its rewrite.\nThen training loss over the images in LaCLIP becomes:\nLI := \u2212 N\u2211 i=1 log exp\n( sim ( fI ( augI ( xiI )) ,fT ( augT ( xiT ))) /\u03c4 )\u2211N\nk=1 exp ( sim ( fI ( augI ( xiI )) ,fT ( augT ( xkT ))) /\u03c4 ) ,\nwhere fI and fT are image and text encoders respectively.\nFrom the pair graph point of view, LaCLIP expands the nodes in the \u201ctext side\u201d of the pair graph by including the nodes of the rewrite (augmented) texts. Moreover, as the augmented images are connected to augmented texts, this augmented pair graph will have more clusters between similar objects than the original CLIP pair graph. Thus, from the spectral clustering, it will be natural to expect LaCLIP shall cluster similar objects across modalities better than CLIP. Indeed, the zero-shot transfer ability of LaCLIP significantly improves (Fan et al., 2023)."
        },
        {
            "heading": "5 USING NEW KERNELS",
            "text": ""
        },
        {
            "heading": "5.1 MAXIMUM ENTROPY PRINCIPLE",
            "text": "In this subsection, we offer an interpretation of InfoNCE-like loss, suggesting that exponential kernels are natural choices to use in this type of loss. Given a query sample q, let \u03c8i represent the similarity between q and the contrastive sample pi for i \u2208 [n], computed by a kernel k. Without loss of generality, assume p1 is the (positive) neighbor of q according to prior knowledge, but \u03c81 is not necessarily the largest value in \u03c8i. Ideally, we desire \u03c81 to be the largest or at least among the few largest similarities, which indicates that our kernel properly aligns with the prior knowledge of X . To optimize toward this goal, we must design a loss function that captures the ranking of \u03c81. Since the ordering function is discrete and lacks gradient information, we need to convert it into a soft and continuous function that enables gradient-based optimization. Specifically, we employ a probability distribution \u03b1 to represent the neighborhood structure of q in relation to \u03c81, satisfying \u03c81 \u2264 \u2211n i=1 \u03b1i\u03c8i, and \u2200i, \u03b1i \u2265 0. If \u03c81 is the largest, \u03b1 = e1 is the sole solution; otherwise, \u03b1 can be more diverse. For instance, when all \u03c8i values are equal, \u03b1 can be a uniform distribution.\nIntuitively, if there are numerous other \u03c8i values similar to \u03c81, the neighborhood structure of q is not as optimal as when \u03c81 is the only object close to q. Formally, this means \u03b1 should have fewer non-zero entries or at least concentrate on \u03b11. We use its entropy H(\u03b1) = \u2212 \u2211n i=1 \u03b1i log\u03b1i to represent this diversity, which results in the following optimization problem.\n(P1) max\u03b1 H(\u03b1) s.t. \u03b1\u22a41n = 1, \u03b11, . . . , \u03b1n \u2265 0 \u03c81 \u2212 \u2211n i=1 \u03b1i\u03c8i \u2264 0\nBy minimizing the solution of (P1), we can discover an embedding that more accurately approximates the prior knowledge. However, how can we solve (P1)? By introducing the Lagrangian dual variable\n\u03c4 > 0, we obtain the subsequent program (P2)\u2019s solution upper bound \u03c4 times the solution of (P1). Consequently, minimizing (P2) simultaneously produces a smaller upper bound of (P1) as well, indirectly aiding us in achieving our objective.\n(P2) max\u03b1 \u2212E(\u03b1) s.t. \u03b1\u22a41n = 1, \u03b11, . . . , \u03b1n \u2265 0\nwhere E(\u03b1) = \u03c81 \u2212 \u2211n i=1 \u03b1i\u03c8i + \u03c4 \u2211n i=1 \u03b1i log\u03b1i.\nWe present the following theorem for solving (P2). Theorem 5.1 (Exponential kernels are natural). The solution of (P2) satisfies:\n\u2212E (\u03b1\u2217) = \u2212\u03c4 log exp\n( 1 \u03c4 \u03c81 )\u2211n i=1 exp ( 1 \u03c4 \u03c8i ) .\nProof. Please refer to Appendix A.\nUsing this framework, we can derive results akin to the max-min optimization formulation from Tian (2022) as a corollary."
        },
        {
            "heading": "5.2 KERNEL-INFONCE LOSS",
            "text": "In this subsection, we will show how to use the derivation above to improve InfoNCE loss. Theorem 5.1 suggests that the loss function of the form \u2212\u03c4 log exp( 1 \u03c4 \u03c81)\u2211n\ni=1 exp( 1\u03c4 \u03c8i) is a natural choice for\ncharacterizing the neighborhood similarity structure. When the similarity between the query sample p and neighbourhood sample pi is defined as \u03c8i = C\u2212\u2225f(q)\u2212f(pi)\u2225\u03b3 , where C is a large positive constant and \u03b3 > 0. We find it recovers the exponential kernels defined as follows:\nK\u03b3,\u03c4exp(x,y) := exp\n( \u2212\u2225x\u2212 y\u2225 \u03b3\n\u03c4\n) (\u03b3, \u03c4 > 0) (7)\nWe then define our kernel-based contrastive loss, Kernel-InfoNCE, as follows:\nL\u03b3,\u03c4Kernel-InfoNCE(q,p1, {pi} N i=2) := \u2212 log K\u03b3,\u03c4exp(q,p1)\u2211N i=1K \u03b3,\u03c4 exp(q,pi).\n(8)\nNote equation (8) can be easily derived by setting the kernel k in equation (3) to exponential kernel. Our framework in Section 3 is suitable for explaining losses that are adapted from InfoNCE by changing kernels. We consider generalizing the exponential kernel a bit. We propose to use the mixture of two exponential kernels as potential candidates for replacing the Gaussian kernel. There are two kinds of mixing methods. The first one is taking the weighted average of two positive definite kernels.\nThe other mixing method is concatenation, which splits the input vectors into two parts, where the first part uses the first kernel, and the second part uses the second kernel. It is easy to see that both mixing methods maintain the strictly positive definite property of the base kernels. We list the two types of kernel mixtures below.\nSimple Sum Kernel:\nK(xi, xj) := exp(\u2212\u2225f(xi)\u2212 f(xj)\u222522/\u03c42) + exp(\u2212\u2225f(xi)\u2212 f(xj)\u222512/\u03c41)\nConcatenation Sum Kernel:\nK(xi, xj) := exp(\u2212\u2225f(xi)[0 : n]\u2212 f(xj)[0 : n]\u222522/\u03c42) + exp(\u2212\u2225f(xi)[n : 2n]\u2212 f(xj)[n : 2n]\u222512/\u03c41)"
        },
        {
            "heading": "6 EXPERIMENTS",
            "text": "In our experiments, we reproduce the baseline algorithm SimCLR (Chen et al., 2020a), and replace SimCLR\u2019s Gaussian kernel with other kernels. We then test against SimCLR using Kernel-InfoNCE\nloss on various benchmark vision datasets, including CIFAR-10/100 (Krizhevsky et al., 2009) and TinyImageNet (Le & Yang, 2015).\nFor each algorithm, we first train an encoder f on the training dataset to minimize the empirical loss function generated by the kernel. Then, following the standard linear evaluation protocol, we freeze the encoder f and train a supervised linear classifier, which takes the output representation of f as input. Additional experimental details, dataset information, and results can be found in Appendix B.\nExperimental Results. We summarize our empirical results on various benchmark datasets in Table 1. It is evident that we have achieved better performance than SimCLR on all three benchmark datasets, with the Simple Sum Kernel reaching the best average performance."
        },
        {
            "heading": "7 RELATED WORK",
            "text": "Contrastive learning constitutes a classical method extensively employed in representation learning (Hadsell et al., 2006; Becker & Hinton, 1992). Owing to its recent applications in self-supervised learning, contrastive learning has garnered widespread attention and achieved state-of-the-art results in numerous downstream tasks within computer vision (Tian et al., 2020; Cui et al., 2021), graph representation learning (You et al., 2020; Hassani & Khasahmadi, 2020; Deng et al., 2022), multi-modality (Radford et al., 2021), and beyond. Contrastive predictive coding (Oord et al., 2018) represents one of the pioneering methods to incorporate the concept of contrastive learning in self-supervised learning. Subsequently, various methods have sought to enhance performance. SimCLR (Chen et al., 2020a) and MoCo (Chen et al., 2020b) advocate for utilizing a large batch size and momentum update mechanism to guarantee effective learning. Moreover, the hard negative sampling method (Kalantidis et al., 2020) has been explored to mitigate the impact of false negative sampling.\nDespite the empirical success of contrastive learning, the theoretical comprehension of its underlying mechanisms remains limited. Oord et al. (2018) demonstrate that the InfoNCE loss can be regarded as a surrogate loss for maximizing mutual information. Arora et al. (2019) give the generalization bound for contrastive learning under a latent class assumption. HaoChen et al. (2021) incorporate the concept of augmentation graph to facilitate the analysis of contrastive learning and propose a surrogate loss spectral contrastive loss. They show that this surrogate loss is equivalent to spectral clustering on augmentation graph. Wang et al. (2022) propose that aggressive data augmentations lead to overlapping support of intra-class samples, allowing for the clustering of positive samples and the gradual learning of class-separated representations, providing new insights of understanding contrastive learning. Balestriero & LeCun (2022) link a variant of contrastive loss to the ISOMAP algorithm. Hu et al. (2022) connect contrastive learning with stochastic neighbor embedding. Wang & Isola (2020) reveal that the quality of embedding can be decomposed into an alignment component and a uniformity component, considering both the loss function and the embedding space."
        },
        {
            "heading": "8 CONCLUSION",
            "text": "In this paper, we take the probabilistic perspective of contrastive learning, and prove that it is essentially running spectral clustering on the predefined similarity graph. Extending this result to multi-modal learning, we show that CLIP is also doing the generalized spectral clustering on the pair graph. Based on the maximum entropy principle and other useful properties, we propose to use the mixtures of exponential kernels (Kernel-InfoNCE loss) to replace the Gaussian kernel, which has achieved better performance empirically.\nREPRODUCIBILITY STATEMENT\nFor reproducibility, we share our code at https://github.com/yifanzhang-pro/ Kernel-InfoNCE. The experiment results can be reproduced following the instructions in the README document. We also provide our experiment details in Appendix B."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "The authors would like to thank Van Assel for clarifying a derivation step in his paper, and anonymous reviewers and ACs for their helpful suggestions. This work is supported by the Ministry of Science and Technology of the People\u2019s Republic of China, the 2030 Innovation Megaprojects \u201cProgram on New Generation Artificial Intelligence\u201d (Grant No. 2021AAA0150000).\nAUTHOR CONTRIBUTIONS\nYifan Zhang suggests the relationship between MRF and contrastive learning. Zhiquan Tan discovered the MRF framework for dimension reduction (Van Assel et al., 2022), and applied this framework to prove Theorem 3.1. He also introduced the viewpoint of the maximum entropy principle for contrastive learning. Yifan and Zhiquan proposed Kernel-InfoNCE loss and refined the paper. Jingqin Yang did comprehensive experiments on Kernel-InfoNCE loss. Yang Yuan extended Theorem 3.1 to CLIP, and wrote most of the paper."
        },
        {
            "heading": "A APPENDIX FOR PROOFS",
            "text": "Proof of Theorem 3.1.\nProof. Our proof has two steps. In Step 1, we will show that SimCLR is equivalent to minimizing the cross entropy loss defined in Eqn. (3). In Step 2, we will show that minimizing the cross-entropy loss is equivalent to spectral clustering on \u03c0. Combining the two steps together, we have proved our theorem.\nStep 1: SimCLR is equivalent to minimizing the cross entropy loss.\nThe cross entropy loss takes expectation over WX \u223c P(\u00b7;\u03c0), which means WX has exactly one non-zero entry in each row i. By Lemma 2.3, we know every row i of WX is independent of other rows. Moreover, WX,i \u223c M(1,\u03c0i/ \u2211 j \u03c0i,j) = M(1,\u03c0i), because \u03c0i itself is a probability distribution. Similarly, we know WZ also has the row-independent property by sampling over P(\u00b7;KZ). Therefore, by Lemma 2.4, we know Eqn. (3) is equivalent to:\n\u2212 n\u2211 i=1 EWX,i [logP(WZ,i = WX,i;KZ)],\nThis expression takes expectation over WX,i for the given row i. Notice that WX,i has exactly one non-zero entry, which equals 1 (same for WZ,i). As a result we expand the above expression to be:\n\u2212 n\u2211 i=1 \u2211 j \u0338=i Pr(WX,i,j = 1) log Pr(WZ,i,j = 1). (9)\nBy Lemma 2.3, Pr(WZ,i,j = 1) = KZ,i,j/\u2225KZ,i\u22251 for j \u0338= i. Recall that KZ = (k(Zi \u2212 Zj))(i,j)\u2208[n]2 , which means KZ,i,j/\u2225KZ,i\u22251 = exp(\u2212\u2225Zi\u2212Zj\u22252/2\u03c4)\u2211 k \u0338=i exp(\u2212\u2225Zi\u2212Zk\u22252/2\u03c4)\nfor j \u0338= i, when k is the Gaussian kernel with variance \u03c4 .\nNotice that Zi = f(Xi), so we know\n\u2212 log Pr(WZ,i,j = 1) = \u2212 log exp(\u2212\u2225f(Xi)\u2212 f(Xj)\u22252/2\u03c4)\u2211 k \u0338=i exp(\u2212\u2225f(Xi)\u2212 f(Xk)\u22252/2\u03c4),\n(10)\nThe right hand side is exactly the InfoNCE loss defined in Eqn. (1). Inserting Eqn. (10) into Eqn. (9), we get the SimCLR algorithm, which first samples augmentation pairs (i, j) with Pr(WX,i,j = 1) for each row i, and then optimize the InfoNCE loss.\nStep 2: minimizing the cross entropy loss is equivalent to spectral clustering on \u03c0.\nBy Lemma 2.5, we may further convert the loss to\nmin Z\n\u2212 \u2211\n(i,j)\u2208[n]2 Pi,j log k(Zi \u2212 Zj) + logR(Z). (11)\nSince k is the Gaussian kernel, this reduces to\nmin Z\ntr(Z\u22a4L(\u03c0)Z) + logR(Z),\nwhere we use the fact that EWX\u223cP(\u00b7;\u03c0)[L(WX)] = L(\u03c0), because the Laplacian operator is linear and EWX\u223cP(\u00b7;\u03c0)(WX) = \u03c0.\nProof of Theorem 4.2.\nProof. Since WX \u223c P(\u00b7;\u03c0A,B), we know WX has exactly one non-zero entry in each row, denoting the pair that got sampled. A notable difference compared to the previous proof is we now have nA + nB objects in our graph. CLIP deals with this by taking a mini-batch of size 2N , such that\nnA = nB = N , and adding the 2N InfoNCE losses together. We label the objects in A as [nA], and the objects in B as {nA + 1, \u00b7 \u00b7 \u00b7 , nA + nB}. Notice that \u03c0A,B is a bipartite graph, so the edges of objects in A will only connect to object in B and vice versa. We can define the similarity matrix in Z as KZ, where KZ(i, j+nA) = KZ(j+nA, i) = k(Zi \u2212 Zj) for i \u2208 [nA], j \u2208 [nB], and otherwise we set KZ(i, j) = 0. The rest is same as the previous proof.\nProof of Theorem 5.1.\nProof. Since the objective function consists of a linear term combined with an entropy regularization, which is a strongly concave function, the maximization problem is a convex optimization problem. Owing to the implicit constraints provided by the entropy function, the problem is equivalent to having only the equality constraint. We then introduce the Lagrangian multiplier \u03bb and obtain the following relaxed problem:\nE\u0303(\u03b1) = \u03c81 \u2212 n\u2211 i=1 \u03b1i\u03c8i + \u03c4 n\u2211 i=1 \u03b1i log\u03b1i + \u03bb ( \u03b1\u22a41n \u2212 1 ) .\nAs the relaxed problem is unconstrained, taking the derivative with respect to \u03b1i yields\n\u2202E\u0303(\u03b1)\n\u2202\u03b1i = \u2212\u03c8i + \u03c4\n( log\u03b1i + \u03b1i 1\n\u03b1i\n) + \u03bb = 0.\nSolving the above equation implies that \u03b1i takes the form \u03b1i = exp ( 1 \u03c4 \u03c8i ) exp (\u2212\u03bb \u03c4 \u2212 1 ) . Since \u03b1i lies on the probability simplex, the optimal \u03b1i is explicitly given by \u03b1\u2217i = exp( 1\u03c4 \u03c8i)\u2211n\ni\u2032=1 exp( 1 \u03c4 \u03c8i\u2032)\n.\nSubstituting the optimal point into the objective function, we obtain\nE (\u03b1\u2217) = \u03c81 \u2212 n\u2211 i=1 exp ( 1 \u03c4 \u03c8i )\u2211n i\u2032=1 exp ( 1 \u03c4 \u03c8i\u2032 )\u03c8i + \u03c4 n\u2211 i=1 exp ( 1 \u03c4 \u03c8i )\u2211n i\u2032=1 exp ( 1 \u03c4 \u03c8i\u2032 ) log exp ( 1\u03c4 \u03c8i)\u2211n i\u2032=1 exp ( 1 \u03c4 \u03c8i\u2032\n) = \u03c81 \u2212 \u03c4 log\n( n\u2211 i=1 exp ( 1 \u03c4 \u03c8i )) .\nThus, the Lagrangian dual function is given by\n\u2212E (\u03b1\u2217) = \u2212\u03c4 log exp\n( 1 \u03c4 \u03c81 )\u2211n i=1 exp ( 1 \u03c4 \u03c8i ) ."
        },
        {
            "heading": "B MORE ON EXPERIMENT DETAILS",
            "text": "CIFAR-10 and CIFAR-100. CIFAR-10 (Krizhevsky et al., 2009) and CIFAR-100 (Krizhevsky et al., 2009) are well-known classic image classification datasets. Both CIFAR-10 and CIFAR-100 contain a total of 60k 32\u00d7 32 labeled images of different classes, with 50k for training and 10k for testing. CIFAR-10 is similar to CIFAR-100, except there are 10 different classes in CIFAR-10 and 100 classes in CIFAR-100.\nTinyImageNet. TinyImageNet (Le & Yang, 2015) is a subset of ImageNet (Deng et al., 2009). There are 200 different object classes in TinyImageNet, with 500 training images, 50 validation images, and 50 test images for each class. All the images in TinyImageNet are colored and labeled with a size of 64\u00d7 64. Pseudo-code. Algorithm 1 presents the pseudo-code for our empirical training procedure.\nWe also provide the pseudo-code for our core loss function used in the training procedure in Algorithm 2. The pseudo-code is almost identical to SimCLR\u2019s loss function, with the exception of an extra parameter \u03b3.\nAlgorithm 1 Training Procedure Require: trainable encoder network f , batch size N , augmentation strategy aug, loss function L\nwith hyperparameters args 1: for sampled minibatch xiNi=1 do 2: for all i \u2208 1, ..., N do 3: draw two augmentations ti = aug (xi), t\u2032i = aug (xi) 4: zi = f (ti), z\u2032i = f (t \u2032 i) 5: end for 6: compute loss L = L(N, z, z\u2032, args) 7: update encoder network f to minimize L 8: end for 9: Return encoder network f\nAlgorithm 2 Core loss function C Require: batch size N , two encoded minibatches z1, z2, \u03b3, temperature \u03c4\n1: z = concat (z1, z2) 2: for i \u2208 1, ..., 2N, j \u2208 1, ..., 2N do 3: si,j = \u2225zi \u2212 zj\u2225\u03b32 4: end for 5: define l(i, j) as l(i, j) = \u2212 log exp(si,j/\u03c4)\u22112N\nk=1 1[k \u0338=i]exp(si,j/\u03c4) 6: Return 12N \u2211N k=1 [l(i, i+N) + l(i+N, i)]\nUtilizing the core loss function C, we can define all kernel loss functions used in our experiments in Table 2. For all zi \u2208 z with even dimensions n, we define zLi = zi [0 : n/2] and zRi = zi [n/2 : n].\nBaselines. We reproduce the SimCLR algorithm using PyTorch Lightning (Team, 2022).\nEncoder details. The encoder f consists of a backbone network and a projection network. We employ ResNet50 (He et al., 2016) as the backbone and a 2-layer MLP (connected by a batch normalization (Ioffe & Szegedy, 2015) layer and a ReLU Nair & Hinton (2010) layer) with hidden dimensions 2048 and output dimensions 128 (or 256 in the concatenation kernel case).\nEncoder hyperparameter tuning. For each encoder training case, we randomly sample 500 hyperparameter groups (sample details are shown in Table 3) and train these samples simultaneously using Ray Tune (Liaw et al., 2018), with the ASHA scheduler (Li et al., 2018). Ultimately, the hyperparameter group that maximizes the online validation accuracy (integrated in PyTorch Lightning) within 5000 validation steps is chosen for the given encoder training case.\nEncoder training. We train each encoder using the LARS optimizer (You et al., 2017), LambdaLR Scheduler in PyTorch, momentum 0.9, weight decay 10\u22126, batch size 256, and the aforementioned hyperparameters for 400 epochs on a single A-100 GPU.\nImage transformation. The image transformation strategy, including augmentation, is identical to the default transformation strategy provided by PyTorch Lightning.\nLinear evaluation. The linear head is trained using the SGD optimizer with a cosine learning rate scheduler, batch size 64, and weight decay 10\u22126 for 100 epochs. The learning rate starts at 0.3 and ends at 0.\nMoco Experiments. We also tested our method based on MoCo (He et al., 2019a). The results are summarized in Table 4. Here we choose ResNet18 (He et al., 2016) as the backbone and set a temperature of 0.1 as default. For our simple sum kernel, we set \u03bb = 0.8. The results show that our method outperforms the original MoCo method."
        },
        {
            "heading": "C EXPERIMENTS ON SYNTHETIC DATA",
            "text": "Consider a scenario with n clusters, each containing k vertices. Let the probability of vertices u and v from the same cluster belonging to \u03c0 be p. Conversely, for vertices u and v from different clusters, let the probability of belonging to \u03c0 be q. We generate the graph \u03c0 randomly, based on p and q. We experiment with values of k = 100 and n = 6 for ease of visualization, embedding all points in a two-dimensional space. Each vertex\u2019s initial position originates from a normal distribution. In each iteration, we sample a subgraph of \u03c0 uniformly, ensuring each vertex has an out-degree of 1. We then optimize the corresponding vectors using InfoNCE loss with an SGD optimizer and iterate until convergence. Our experimental setup consists of an SGD learning rate of 1, an InfoNCE loss temperature of 0.5, and a batch size of 50. We evaluate two scenarios with different p and q values: p = 1, q = 0, and p = 0.75, q = 0.2. The results of these experiments are visualized in Figure 3. The obtained embeddings exhibit the hallmark pattern of spectral clustering of graph \u03c0."
        }
    ],
    "year": 2024
}