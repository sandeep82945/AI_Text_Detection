{
    "abstractText": "In this work, we explore regions as a potential visual analogue of words for selfsupervised image representation learning. Inspired by Masked Autoencoding (MAE), a generative pre-training baseline, we propose masked region autoencoding to learn from groups of pixels or regions. Specifically, we design an architecture which efficiently addresses the one-to-many mapping between images and regions, while being highly effective especially with high-quality regions. When integrated with MAE, our approach (R-MAE) demonstrates consistent improvements across various pre-training datasets and downstream detection and segmentation benchmarks, with negligible computational overheads. Beyond the quantitative evaluation, our analysis indicates the models pre-trained with masked region autoencoding unlock the potential for interactive segmentation.1",
    "authors": [],
    "id": "SP:a6f50c6ceda6a1c4d25a62bde82cf1321307cd76",
    "references": [
        {
            "authors": [
                "Pablo Arbel\u00e1ez",
                "Jordi Pont-Tuset",
                "Jonathan T Barron",
                "Ferran Marques",
                "Jitendra Malik"
            ],
            "title": "Multiscale combinatorial grouping",
            "venue": "In CVPR,",
            "year": 2014
        },
        {
            "authors": [
                "Yuki M Asano",
                "Christian Rupprecht",
                "Andrea Vedaldi"
            ],
            "title": "A critical analysis of self-supervision, or what we can learn from a single image",
            "year": 1904
        },
        {
            "authors": [
                "Roman Bachmann",
                "David Mizrahi",
                "Andrei Atanov",
                "Amir Zamir"
            ],
            "title": "Multimae: Multi-modal multi-task masked autoencoders",
            "venue": "In ECCV,",
            "year": 2022
        },
        {
            "authors": [
                "Yutong Bai",
                "Xinlei Chen",
                "Alexander Kirillov",
                "Alan Yuille",
                "Alexander C. Berg"
            ],
            "title": "Point-level region contrast for object detection pre-training",
            "year": 2022
        },
        {
            "authors": [
                "Hangbo Bao",
                "Li Dong",
                "Furu Wei"
            ],
            "title": "BEiT: BERT pre-training of image transformers",
            "venue": "In ICLR,",
            "year": 2022
        },
        {
            "authors": [
                "Dario Amodei"
            ],
            "title": "Language models are few-shot learners",
            "venue": "In NeurIPS,",
            "year": 2020
        },
        {
            "authors": [
                "Nicolas Carion",
                "Francisco Massa",
                "Gabriel Synnaeve",
                "Nicolas Usunier",
                "Alexander Kirillov",
                "Sergey Zagoruyko"
            ],
            "title": "End-to-end object detection with transformers",
            "venue": "In ECCV,",
            "year": 2020
        },
        {
            "authors": [
                "Mathilde Caron",
                "Hugo Touvron",
                "Ishan Misra",
                "Herv\u00e9 J\u00e9gou",
                "Julien Mairal",
                "Piotr Bojanowski",
                "Armand Joulin"
            ],
            "title": "Emerging properties in self-supervised vision transformers",
            "venue": "In ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "Jun Chen",
                "Ming Hu",
                "Boyang Li",
                "Mohamed Elhoseiny"
            ],
            "title": "Efficient self-supervised vision pretraining with local masked reconstruction",
            "venue": "arXiv preprint arXiv:2206.00790,",
            "year": 2022
        },
        {
            "authors": [
                "Kai Chen",
                "Zhili Liu",
                "Lanqing Hong",
                "Hang Xu",
                "Zhenguo Li",
                "Dit-Yan Yeung"
            ],
            "title": "Mixed autoencoder for self-supervised visual representation learning",
            "year": 2023
        },
        {
            "authors": [
                "Mark Chen",
                "Alec Radford",
                "Rewon Child",
                "Jeff Wu",
                "Heewoo Jun",
                "Prafulla Dhariwal",
                "David Luan",
                "Ilya Sutskever"
            ],
            "title": "Generative pretraining from pixels",
            "venue": "In ICML,",
            "year": 2020
        },
        {
            "authors": [
                "Ting Chen",
                "Simon Kornblith",
                "Mohammad Norouzi",
                "Geoffrey Hinton"
            ],
            "title": "A simple framework for contrastive learning of visual representations",
            "venue": "In ICML,",
            "year": 2020
        },
        {
            "authors": [
                "Xiaokang Chen",
                "Mingyu Ding",
                "Xiaodi Wang",
                "Ying Xin",
                "Shentong Mo",
                "Yunhao Wang",
                "Shumin Han",
                "Ping Luo",
                "Gang Zeng",
                "Jingdong Wang"
            ],
            "title": "Context autoencoder for self-supervised representation learning",
            "venue": "arXiv preprint arXiv:2202.03026,",
            "year": 2022
        },
        {
            "authors": [
                "Xinlei Chen",
                "Kaiming He"
            ],
            "title": "Exploring simple siamese representation learning",
            "venue": "In CVPR,",
            "year": 2021
        },
        {
            "authors": [
                "Xinlei Chen",
                "Saining Xie",
                "Kaiming He"
            ],
            "title": "An empirical study of training self-supervised vision transformers",
            "venue": "In ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "Bowen Cheng",
                "Anwesa Choudhuri",
                "Ishan Misra",
                "Alexander Kirillov",
                "Rohit Girdhar",
                "Alexander G Schwing"
            ],
            "title": "Mask2former for video instance segmentation",
            "venue": "arXiv preprint arXiv:2112.10764,",
            "year": 2021
        },
        {
            "authors": [
                "Mostafa Dehghani",
                "Josip Djolonga",
                "Basil Mustafa"
            ],
            "title": "Scaling vision transformers to 22 billion parameters",
            "venue": "In ICML,",
            "year": 2023
        },
        {
            "authors": [
                "Jia Deng",
                "Wei Dong",
                "Richard Socher",
                "Li-Jia Li",
                "Kai Li",
                "Li Fei-Fei"
            ],
            "title": "Imagenet: A large-scale hierarchical image database",
            "venue": "In CVPR,",
            "year": 2009
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova"
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "In ACL,",
            "year": 2019
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "Lucas Beyer",
                "Alexander Kolesnikov",
                "Dirk Weissenborn",
                "Xiaohua Zhai",
                "Thomas Unterthiner",
                "Mostafa Dehghani",
                "Matthias Minderer",
                "Georg Heigold",
                "Sylvain Gelly"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
            "venue": "arXiv preprint arXiv:2010.11929,",
            "year": 2020
        },
        {
            "authors": [
                "Pedro F Felzenszwalb",
                "Daniel P Huttenlocher"
            ],
            "title": "Efficient graph-based image segmentation",
            "year": 2004
        },
        {
            "authors": [
                "Wouter Van Gansbeke",
                "Simon Vandenhende",
                "Stamatios Georgoulis",
                "Luc Van Gool"
            ],
            "title": "Unsupervised semantic segmentation by contrasting object mask proposals",
            "venue": "In ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "Xinyang Geng",
                "Hao Liu",
                "Lisa Lee",
                "Dale Schuurams",
                "Sergey Levine",
                "Pieter Abbeel"
            ],
            "title": "Multimodal masked autoencoders learn transferable representations",
            "venue": "arXiv preprint arXiv:2205.14204,",
            "year": 2022
        },
        {
            "authors": [
                "Ross Girshick"
            ],
            "title": "Fast r-cnn",
            "venue": "In ICCV,",
            "year": 2015
        },
        {
            "authors": [
                "Ross Girshick",
                "Jeff Donahue",
                "Trevor Darrell",
                "Jitendra Malik"
            ],
            "title": "Rich feature hierarchies for accurate object detection and semantic segmentation",
            "venue": "In CVPR,",
            "year": 2014
        },
        {
            "authors": [
                "Jean-Bastien Grill",
                "Florian Strub",
                "Florent Altch\u00e9",
                "Corentin Tallec",
                "Pierre Richemond",
                "Elena Buchatskaya",
                "Carl Doersch",
                "Bernardo Avila Pires",
                "Zhaohan Guo",
                "Mohammad Gheshlaghi Azar"
            ],
            "title": "Bootstrap your own latent-a new approach to self-supervised learning",
            "venue": "NeurIPS,",
            "year": 2020
        },
        {
            "authors": [
                "Agrim Gupta",
                "Piotr Dollar",
                "Ross Girshick"
            ],
            "title": "LVIS: A dataset for large vocabulary instance segmentation",
            "year": 2019
        },
        {
            "authors": [
                "Kaiming He",
                "Haoqi Fan",
                "Yuxin Wu",
                "Saining Xie",
                "Ross Girshick"
            ],
            "title": "Momentum contrast for unsupervised visual representation learning",
            "venue": "In CVPR,",
            "year": 2020
        },
        {
            "authors": [
                "Kaiming He",
                "Xinlei Chen",
                "Saining Xie",
                "Yanghao Li",
                "Piotr Doll\u00e1r",
                "Ross Girshick"
            ],
            "title": "Masked autoencoders are scalable vision learners",
            "year": 2022
        },
        {
            "authors": [
                "Ronghang Hu",
                "Shoubhik Debnath",
                "Saining Xie",
                "Xinlei Chen"
            ],
            "title": "Exploring long-sequence masked autoencoders",
            "venue": "arXiv preprint arXiv:2210.07224,",
            "year": 2022
        },
        {
            "authors": [
                "Olivier J. H\u00e9naff",
                "Skanda Koppula",
                "Jean-Baptiste Alayrac",
                "Aaron van den Oord",
                "Oriol Vinyals",
                "Jo\u00e3o Carreira"
            ],
            "title": "Efficient visual pretraining with contrastive detection",
            "venue": "In ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "Olivier J. H\u00e9naff",
                "Skanda Koppula",
                "Evan Shelhamer",
                "Daniel Zoran",
                "Andrew Jaegle",
                "Andrew Zisserman",
                "Jo\u00e3o Carreira",
                "Relja Arandjelovi\u0107"
            ],
            "title": "Object discovery and representation networks",
            "year": 2022
        },
        {
            "authors": [
                "Alexander Kirillov",
                "Eric Mintun",
                "Nikhila Ravi",
                "Hanzi Mao",
                "Chloe Rolland",
                "Laura Gustafson",
                "Tete Xiao",
                "Spencer Whitehead",
                "Alexander C. Berg",
                "Wan-Yen Lo",
                "Piotr Doll\u00e1r",
                "Ross Girshick"
            ],
            "title": "Segment anything",
            "venue": "In ICCV,",
            "year": 2023
        },
        {
            "authors": [
                "Alex Krizhevsky",
                "Ilya Sutskever",
                "Geoffrey E Hinton"
            ],
            "title": "Imagenet classification with deep convolutional neural networks",
            "year": 2017
        },
        {
            "authors": [
                "Gang Li",
                "Heliang Zheng",
                "Daqing Liu",
                "Chaoyue Wang",
                "Bing Su",
                "Changwen Zheng"
            ],
            "title": "Semmae: Semantic-guided masking for learning masked autoencoders",
            "venue": "NeurIPS,",
            "year": 2022
        },
        {
            "authors": [
                "Yanghao Li",
                "Saining Xie",
                "Xinlei Chen",
                "Piotr Doll\u00e1r",
                "Kaiming He",
                "Ross Girshick"
            ],
            "title": "Benchmarking detection transfer learning with vision transformers",
            "venue": "arXiv preprint arXiv:2111.11429,",
            "year": 2021
        },
        {
            "authors": [
                "Yanghao Li",
                "Hanzi Mao",
                "Ross Girshick",
                "Kaiming He"
            ],
            "title": "Exploring plain vision transformer backbones for object detection",
            "venue": "In ECCV,",
            "year": 2022
        },
        {
            "authors": [
                "Tsung-Yi Lin",
                "Michael Maire",
                "Serge J. Belongie",
                "Lubomir D. Bourdev",
                "Ross B. Girshick",
                "James Hays",
                "Pietro Perona",
                "Deva Ramanan",
                "Piotr Doll\u00e1r",
                "C. Lawrence Zitnick"
            ],
            "title": "Microsoft COCO: common objects in context",
            "venue": "In ECCV,",
            "year": 2014
        },
        {
            "authors": [
                "Duy-Kien Nguyen",
                "Jihong Ju",
                "Olaf Booij",
                "Martin R Oswald",
                "Cees GM Snoek"
            ],
            "title": "Boxer: Boxattention for 2d and 3d transformers",
            "year": 2022
        },
        {
            "authors": [
                "Deepak Pathak",
                "Ross Girshick",
                "Piotr Doll\u00e1r",
                "Trevor Darrell",
                "Bharath Hariharan"
            ],
            "title": "Learning features by watching objects move",
            "year": 2017
        },
        {
            "authors": [
                "Zhiliang Peng",
                "Li Dong",
                "Hangbo Bao",
                "Qixiang Ye",
                "Furu Wei"
            ],
            "title": "Beit v2: Masked image modeling with vector-quantized visual tokenizers",
            "venue": "arXiv preprint arXiv:2208.06366,",
            "year": 2022
        },
        {
            "authors": [
                "Pedro O. Pinheiro",
                "Amjad Almahairi",
                "Ryan Y. Benmalek",
                "Florian Golemo",
                "Aaron Courville"
            ],
            "title": "Unsupervised learning of dense visual representations",
            "venue": "NeurIPS,",
            "year": 2020
        },
        {
            "authors": [
                "Alec Radford",
                "Karthik Narasimhan",
                "Tim Salimans",
                "Ilya Sutskever"
            ],
            "title": "Improving language understanding by generative pretraining",
            "venue": "OpenAI preprint,",
            "year": 2018
        },
        {
            "authors": [
                "Alec Radford",
                "Jeffrey Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "venue": "OpenAI preprint,",
            "year": 2019
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark",
                "Gretchen Krueger",
                "Ilya Sutskever"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "year": 2021
        },
        {
            "authors": [
                "Shaoqing Ren",
                "Kaiming He",
                "Ross Girshick",
                "Jian Sun"
            ],
            "title": "Faster R-CNN: Towards real-time object detection with region proposal networks",
            "venue": "NeurIPS,",
            "year": 2015
        },
        {
            "authors": [
                "Byungseok Roh",
                "Wuhyun Shin",
                "Ildoo Kim",
                "Sungwoong Kim"
            ],
            "title": "Spatilly consistent representation learning",
            "venue": "In CVPR,",
            "year": 2021
        },
        {
            "authors": [
                "Amanpreet Singh",
                "Ronghang Hu",
                "Vedanuj Goswami",
                "Guillaume Couairon",
                "Wojciech Galuba",
                "Marcus Rohrbach",
                "Douwe Kiela"
            ],
            "title": "Flava: A foundational language and vision alignment model",
            "year": 2022
        },
        {
            "authors": [
                "Konstantin Sofiiuk",
                "Ilia Petrov",
                "Olga Barinova",
                "Anton Konushin"
            ],
            "title": "f-brs: Rethinking backpropagating refinement for interactive segmentation",
            "venue": "In CVPR,",
            "year": 2020
        },
        {
            "authors": [
                "Jasper RR Uijlings",
                "Koen EA Van De Sande",
                "Theo Gevers",
                "Arnold WM Smeulders"
            ],
            "title": "Selective search for object recognition",
            "year": 2013
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N. Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "NeurIPS,",
            "year": 2017
        },
        {
            "authors": [
                "Pascal Vincent",
                "Hugo Larochelle",
                "Yoshua Bengio",
                "Pierre-Antoine Manzagol"
            ],
            "title": "Extracting and composing robust features with denoising autoencoders",
            "venue": "In ICML,",
            "year": 2008
        },
        {
            "authors": [
                "Xinlong Wang",
                "Rufeng Zhang",
                "Chunhua Shen",
                "Tao Kong",
                "Lei Li"
            ],
            "title": "Dense contrastive learning for self-supervised visual pre-training",
            "venue": "In CVPR,",
            "year": 2021
        },
        {
            "authors": [
                "Chen Wei",
                "Haoqi Fan",
                "Saining Xie",
                "Chao-Yuan Wu",
                "Alan Yuille",
                "Christoph Feichtenhofer"
            ],
            "title": "Masked feature prediction for self-supervised visual pre-training",
            "year": 2022
        },
        {
            "authors": [
                "Fangyun Wei",
                "Yue Gao",
                "Zhirong Wu",
                "Han Hu",
                "Stephen Lin"
            ],
            "title": "Aligning pretraining for detection via object-level contrastive learning",
            "venue": "NeurIPS,",
            "year": 2021
        },
        {
            "authors": [
                "Sanghyun Woo",
                "Shoubhik Debnath",
                "Ronghang Hu",
                "Xinlei Chen",
                "Zhuang Liu",
                "In So Kweon",
                "Saining Xie"
            ],
            "title": "Convnext v2: Co-designing and scaling convnets with masked autoencoders",
            "year": 2023
        },
        {
            "authors": [
                "Tete Xiao",
                "Colorado J Reed",
                "Xiaolong Wang",
                "Kurt Keutzer",
                "Trevor Darrell"
            ],
            "title": "Region similarity representation learning",
            "venue": "In ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "Enze Xie",
                "Jian Ding",
                "Wenhai Wang",
                "Xiaohang Zhan",
                "Hang Xu",
                "Peize Sun",
                "Zhenguo Li",
                "Ping Luo"
            ],
            "title": "Detco: Unsupervised contrastive learning for object detection",
            "venue": "In ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "Enze Xie",
                "Jian Ding",
                "Wenhai Wang",
                "Xiaohang Zhan",
                "Hang Xu",
                "Peize Sun",
                "Zhenguo Li",
                "Ping Luo"
            ],
            "title": "Unsupervised object-level representation learning from scene images",
            "venue": "NeurIPS,",
            "year": 2021
        },
        {
            "authors": [
                "Zhenda Xie",
                "Yutong Lin",
                "Zheng Zhang",
                "Yue Cao",
                "Stephen Lin",
                "Han Hu"
            ],
            "title": "Propagate yourself: Exploring pixel-level consistency for unsupervised visual representation learning",
            "venue": "In CVPR,",
            "year": 2021
        },
        {
            "authors": [
                "Zhenda Xie",
                "Zheng Zhang",
                "Yue Cao",
                "Yutong Lin",
                "Jianmin Bao",
                "Zhuliang Yao",
                "Qi Dai",
                "Han Hu"
            ],
            "title": "Simmim: A simple framework for masked image modeling",
            "year": 2022
        },
        {
            "authors": [
                "Ceyuan Yang",
                "Zhirong Wu",
                "Bolei Zhou",
                "Stephen Lin"
            ],
            "title": "Instance localization for self-supervised detection pretraining",
            "venue": "In CVPR,",
            "year": 2021
        },
        {
            "authors": [
                "Xiaohua Zhai",
                "Alexander Kolesnikov",
                "Neil Houlsby",
                "Lucas Beyer"
            ],
            "title": "Scaling vision transformers",
            "venue": "In CVPR,",
            "year": 2022
        },
        {
            "authors": [
                "Xiao Zhang",
                "Michael Maire"
            ],
            "title": "Self-supervised visual representation learning from hierarchical grouping",
            "venue": "In NeurIPS,",
            "year": 2020
        },
        {
            "authors": [
                "Jinghao Zhou",
                "Chen Wei",
                "Huiyu Wang",
                "Wei Shen",
                "Cihang Xie",
                "Alan Yuille",
                "Tao Kong"
            ],
            "title": "ibot: Image bert pre-training with online tokenizer",
            "venue": "In ICLR,",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "There has been a significant progress of self-supervised pre-training in Natural Language Processing (NLP) over a short period of time, showing the potential of pre-trained language models from huge amounts of data. This progress has been mainly brought about two lines of research, the autogressive language model in GPT (Radford et al., 2018; 2019) and the masked language model in BERT (Devlin et al., 2019). While being different in the design of pre-text task, both approaches learn to predict missing words given the available content. Such reconstructive pre-training enables language models to capture complex and long-range context in documents, resulting in a general learner for various NLP downstream tasks (Brown et al., 2020).\nInspired by the reconstructive design of masked language modeling in NLP, recent self-supervised learning approaches in computer vision also propose to directly predict masked patches from visible image content (Peng et al., 2022; He et al., 2022; Xie et al., 2022). Indeed, the idea of masked autoencoding in vision proves its effectiveness in learning visual representations, reaching state-ofthe-art performance in image recognition (He et al., 2022; Woo et al., 2023). Among these methods, Masked Autoencoding (MAE) (He et al., 2022) that employs an asymmetric design and a high masking ratio proves to be a simple and powerful vision learner. Notably, Li et al. (2021) show that unlike supervised or contrastive learning, MAE improves the upper bound of object detection and segmentation compared to a long and optimal recipe of training from scratch.\nHowever, for visual understanding, MAE has not yet reached the same performance level as language models. Despite the benefit of learning from unlabeled data, MAE still lags behind in its scalability (Zhai et al., 2022; Dehghani et al., 2023) and other emergent properties (e.g., one that explicitly capture human-relatable segments (Caron et al., 2021)). This may come from the fact that the raw pixel values are continuous signals of the visual world, whereas words are discrete human creations. Motivated by this, we examine the concept of \u2018region\u2019 (Girshick et al., 2014) as a potential visual analogue of words for pre-training, as regions offer similarly discrete information about which group of pixels belong together. By learning from regions in the image, the model can hopefully be less biased towards raw pixels and focus more on the grouping of pixels that encode parts, objects, and scenes. Thus it can further advance the performance on tasks like object detection and segmentation.\nSpecifically, we propose \u2018masked Region Autoencoding\u2019 (RAE), as a reconstructive pre-text task to learn from regions. In RAE, each region is represented as a binary region \u2018map\u2019, with each value indicating whether a pixel belongs to the current region or not. We can then follow a similar procedure in MAE to learn a region-aware representation by predicting masked portions of the input regions.\n1The code is provided as part of the supplementary material.\nHowever, unlike MAE that reconstructs a single input image in its decoder, learning from regions requires our pipeline to efficiently deal with one-to-many mappings. This is because a pixel in the image can belong to an unknown number of regions. In addition, different from color channels in pixels that appear in a pre-defined order (e.g., RGB), the reconstruction of multiple regions needs to maintain permutation equivariance \u2013 a swap of two regions in the input should automatically lead to a swap in the output. To address these challenges, we explore several architecture variants for RAE and converge to a \u2018length\u2019 variant that compresses each spatial region to a single query vector. We show our final design is both efficient and effective.\nRAE is fully compatible with MAE. When integrated, we name our approach R-MAE, short for Region-aware Masked Autoencoding. Since we use regions which are fully computable from mere images, R-MAE enjoys the same range of applicability as MAE. Empirically, we find R-MAE can generate useful representations for dense vision tasks such as object detection and segmentation, which we thoroughly study with our experiments. Specifically, we highlight:\n\u2022 RAE alone reaches strong performance, especially when fed with high-quality, off-the-shelf regions (Kirillov et al., 2023) \u2013 better than MAE;\n\u2022 Even with regions from a simple clustering algorithm (Felzenszwalb & Huttenlocher, 2004), R-MAE offers consistent improvements over MAE on multiple settings, and reaches stateof-the-art performance without compromising pre-training efficiency;\n\u2022 Qualitative visualizations show our pre-training is indeed more region-aware, or instanceaware compared to others;\n\u2022 As a final demonstration, pre-trained R-MAE models can be potentially used as a promptable, \u2018interactive segmenter\u2019 beyond representation learning."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "We first review two intrinsic properties of regions, which have driven their popularity:\nLocal. In machine learning algorithms images are typically treated as holistic entities (Krizhevsky et al., 2017; Chen et al., 2020b), but real-world photos have rich spatial structures and local contents can vary across the same scene (Asano et al., 2019). This became a strong motivation for the well-known R-CNN series (Girshick et al., 2014; Girshick, 2015; Ren et al., 2015; He et al., 2017), especially with Region-of-Interest (RoI) operations on local feature maps (Girshick, 2015). The same holds for contrastive or Siamese learning (Chen et al., 2020b; He et al., 2020; Radford et al., 2021; Grill et al., 2020; Chen & He, 2021; Caron et al., 2021), where 2D signals are generally suppressed into global vectors for inter-image contrast. Realizing its potential downside for localization, many follow-up works (Xie et al., 2021c; Pinheiro et al., 2020; Roh et al., 2021; Xiao et al., 2021; Xie et al., 2021a;b; Yang et al., 2021; Gansbeke et al., 2021; Wei et al., 2021; H\u00e9naff et al., 2022) have shifted focus on intra-image contrast, which use features from local geometric entities (e.g. points (Wang et al., 2021), regions (H\u00e9naff et al., 2021) or both (Bai et al., 2022)). Meanwhile, reconstructive methods (He et al., 2022; Bao et al., 2022; Wei et al., 2022; Chen et al., 2022b) as denoising autoencoders (Vincent et al., 2008) preserve the 2D structure. It is therefore unclear how regions can further help in this regard.\nObject-centric. Reconstructive learning is the dominating paradigm in pre-training natural language representations (Devlin et al., 2019; Brown et al., 2020), and while steady progress is made (Chen et al., 2020a; He et al., 2022), computer vision models are still lagging behind. One crucial difference between the two fields is that language consists of semantically meaningful discrete words, while images are raw continuous signals recorded in pixels. Meanwhile, in vision, objects can serve as a natural counterpart to words \u2013 they are constantly referred and manipulated as we interact with the visual world (Koffka, 2013; Zhang & Maire, 2020), and they can often be captured, albeit not perfectly, by regions (Uijlings et al., 2013; Arbel\u00e1ez et al., 2014). By enhancing MAE\u2019s region awareness, we hope to uncover novel ways to bridge the gap between vision and language.\nNext we discuss how regions are generated and utilized:\nSource of regions. Regions can come from various sources (e.g. human annotations (Lin et al., 2014), spatial heuristics (H\u00e9naff et al., 2021), clustering/segmentation (Felzenszwalb & Huttenlocher, 2004; Achanta et al., 2010), object proposals (Uijlings et al., 2013; Arbel\u00e1ez et al., 2014), or motion\nsegmentation (Pathak et al., 2017)). Most recently, the Segment Anything Model (SAM) proposed by Kirillov et al. (2023) stands out as a universal model for generating region proposals. As an initial exploration, our study mainly focuses on pre-computed, clustering-based regions (Felzenszwalb & Huttenlocher, 2004), but we also verify the effectiveness of R-MAE using regions generated from SAM. Moreover, regions can be jointly discovered (H\u00e9naff et al., 2022) or updated (Bai et al., 2022) with representation learning, which is left for future work.\nUse of regions. There are at least three other ways to leverage regions in MAE. One is to bias the random masking strategy (Li et al., 2022a), which is less general and can be sensitive to region qualities (Li et al., 2022a). Second is to revisit the RoI operation (Ren et al., 2015) and contrastive learning, which is costly with Siamese encoders (He et al., 2020; Chen & He, 2021), and has been extensively studied (H\u00e9naff et al., 2021; Xiao et al., 2021; Xie et al., 2021b; Wei et al., 2021) even with MAE (Zhou et al., 2022). Third is to view regions as an extra modality, and treat the task as a multi-modal learning one (e.g. with text (Geng et al., 2022; Singh et al., 2022) or a depth map (Bachmann et al., 2022)). This is closest to our work, yet the lightweight design of R-MAE makes it especially well-suited to learn representations using regions."
        },
        {
            "heading": "3 APPROACH",
            "text": "Background on Masked Autoencoding. Since Masked Autoencoding (MAE) (He et al., 2022) is the foundation and baseline of our approach, we first summarize it as background knowledge. As the name suggests, MAE uniformly masks out a portion of an image and learns to reconstruct by directly predicting raw pixel values. To provide a meaningful and challenging task for images, a high mask ratio \u03b2I (e.g. 75%) is used by default. The reconstruction is compared against the ground-truth with a simple \u21132 loss.\nAs an autoencoder (Vincent et al., 2008), MAE instantiates its encoder and decoder with vision transformers (ViTs) (Dosovitskiy et al., 2020). ViTs directly \u2018tokenize\u2019 images as sequences of patches, which paves the way for MAE\u2019s efficient encoder pre-training that removes (and not replaces) masked tokens. Given visible tokens from the pixel encoder, the fixed-sized (8-block, 512- dimensional) pixel decoder then reconstruct masked patches via pixel regression. After pre-training, the pixel encoder is transferred as a visual backbone for downstream tasks (Li et al., 2022b)."
        },
        {
            "heading": "3.1 RAE: MASKED REGION AUTOENCODING",
            "text": "Region maps. To perform masked region autoencoding, we first simply follow MAE and prepare them to be \u2018image-like\u2019. Specifically, each region can be represented by a binary-valued region map similar in size to the image. Each element on the map, with a value of either in 0 or 1, indicates whether the corresponding location belongs to the region or not. Now, given any partially visible region map (mask ratio \u03b2R), we can ask the model to complete it, the same as MAE does for pixels.\nArchitecture. Similar to MAE, the proposed architecture contains an encoder and decoder for region autoencoding. We follow MAE and simply use ViT blocks (Dosovitskiy et al., 2020) for both. However, just a region encoder-decoder pair is insufficient, as our ultimate goal is to obtain a pre-trained pixel encoder. Therefore, we maintain the pixel encoder, and use a neck of a single ViT block to match dimensions and (optionally) propagate information before feeding into the region decoder. Such a configuration also makes effective use of the abundant contextual information available in the pixels to pre-train the encoder. See Fig. 1 for an overview.\nOne-to-many mapping. While regions can be considered as an additional modality to pixel-based MAE, the problem addressed here presents a distinctive challenge that cannot be fully captured by this view alone. Compared to other modalities (e.g. depth or semantic maps (Bachmann et al., 2022)) for which there is a one-to-one correspondence to pixels, the mapping between images and regions is one-to-many: one pixel can belong to an unknown number of regions.\nOne na\u00efve implementation is to merge the k regions in the channel axis. In this way, they can be viewed as a single image, and the computations are shared in the intermediate blocks. But unlike natural images which have fixed channel orders (e.g., RGB), randomly sampled regions can appear in any order. It would be ideal if the solution preserves permutation equivariance.\nFortunately, this happens to be the very problem encountered in object detection. The mainstream solution, as promoted by R-CNN (Girshick et al., 2014), is to sample and stack regions in the batch axis, and process each of them separately. In masked region autoencoding, this means each region map will go through the encoder-decoder in isolation: If there are b images and k regions per image, the network must be applied b\u00d7k times. This is expensive \u2013 so how to reduce the cost?\nRegions as queries \u2013 the length variant. Our final idea is inspired by DETR series (Carion et al., 2020; Nguyen et al., 2022), which uses \u2018object queries\u2019 as substrates to decode objects. In a nutshell, each region is first encoded and pooled into a 1D embedding; then multiple region embeddings are concatenated along the sequence length (Dosovitskiy et al., 2020) axis to form \u2018region queries\u2019; and finally, these region queries will decode region maps from the output of the pixel encoder (through the neck, see Fig. 1 for details). Since ViT blocks are set operations w.r.t. the input (Vaswani et al., 2017), this solution is permutation equivariant by design.\nThe last decoder block is responsible for expanding region queries spatially. Note that because the decoder has two sets of inputs, its blocks follow the three-layer design (Carion et al., 2020), with an extra cross-attention layer that uses outputs from the neck to generate keys and values. Different from standard attention layers that\ncompute a weighted sum (with keys) over values to produce the output (Fig. 2, left), we expand the query by directly adding it to all the values (Fig. 2, right). A small MLP head is attached afterwards to predict region maps on these spatially expanded features. Since this variant alleviates the linear complexity w.r.t. number of regions k, and still maintains the desired property w.r.t. permutation, we choose it as the default for RAE. Since this variant alleviates the linear complexity w.r.t. the number of regions k, and still maintains the desired property w.r.t. the permutation, we choose it as the default for masked region autoencoding.\nLoss. While the \u21132 loss fits real-valued pixel predictions, by default we use the cross-entropy loss for binary-valued regions which is effective for binary classification."
        },
        {
            "heading": "3.2 R-MAE: REGIONS MEET MAE",
            "text": "As masked region autoencoding is fully compatible with MAE, they can be trained in conjunction by simply restoring the pixel encoder and applying a joint loss with an equal weight (see Fig. 1). Note that: (i) The pixel branch feeds to the region branch, but not vice versa; (ii) The mask is shared between two branches which prevents information leak and creates a more challenging pre-text task. We name this framework R-MAE, short for Region-aware Masked Autoencoding.\nInterestingly, Fig. 3 shows that when pre-trained with R-MAE using unsupervised, image-computable region maps (Felzenszwalb & Huttenlocher, 2004), ViT features are shown to be more instanceaware. In particular, its attention map focuses more on the objects given the query compared to the reconstructive (MAE (He et al., 2022)) and contrastive (MoCo v3 (Chen et al., 2021)) baselines. The ViT features pre-trained with R-MAE reveal its localization capabilities through the attention map, with strong focus on objects across different locations."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "4.1 EXPERIMENTAL SETUPS",
            "text": "Source of regions. By default, we use regions generated from the unsupervised, image-computable Felzenswalb-Huttenlocher (FH) algorithm (Felzenszwalb & Huttenlocher, 2004). It is fast, efficient and covers the whole image that underlies classic object proposal methods (e.g. selective search (Uijlings et al., 2013)). The use of FH region maps allows our self-supervised method to inherit the wide applicability on multiple domains. In addition, we also ablate regions from different sources such as, panoptic regions \u2013 ground-truth annotations from the COCO dataset (Lin et al., 2014), and regions generated by the SAM model (Kirillov et al., 2023).\nthe asymmetric design that only feeds pixels to regions works best. Default settings are shaded in gray .\nPre-training datasets. Deviating from prior practices (Bao et al., 2022; He et al., 2022), we develop RAE and R-MAE by pre-training on COCO train2017 (Lin et al., 2014). This is due to the scene-centric nature of the images in COCO and the presence of ground-truth regions which can serve as useful oracles. Following (H\u00e9naff et al., 2021), FH is run at three scales: {500, 1000, 1500}, which also set the minimum cluster sizes. Since this dataset (118k images) is significantly smaller than ImageNet (1.4m), we pre-train for 4k epochs instead of 800 (He et al., 2022). For fairness, we also pre-train ViTs with R-MAE on ImageNet (Deng et al., 2009) for 800/1600 epochs. In this case, we extract FH region maps with a single scale of 1000 as in H\u00e9naff et al. (2021).\nOther pre-training details. Unless otherwise specified, we exactly follow MAE (He et al., 2022) for hyper-parameters. Our base learning rate is set to 1e-4, which offers better stability during training and maintains the baseline performance (see Appendix). The length variant is used. ViT-B (Dosovitskiy et al., 2020) is set as the pixel backbone, and a 1-block, 128-dimensional ViT is used for the neck, the region encoder and the region decoder. A 3-layer MLP acts as the region predictor after the decoder block. k=8 regions are randomly sampled per image with replacement, and a mask ratio of \u03b2R=0.75. When combined with pixel regression in MAE in R-MAE framework, the pixel branch feeds the region branch, and the random masks are shared.\nDownstream transfers. The pre-trained vision transformers serve as the backbone for downstream tasks. We simply use the recipe from ViTDet (Li et al., 2022b) for object evaluation on COCO, and report mean Average Precision (AP) for both box detection (APb) and instance segmentation (APm). Specifically, the learning rate is linearly warmed up for the first 250 iterations and decayed at 0.9 and 0.95 fractions of the total number of training steps by a factor 10. The input image size is 1024\u00d71024 with large-scale jitter between a scale range of [0.1, 2.0]. We finetune for 100 epochs with batch size of 64. For semantic segmentation, we evaluate on ADE20K and report mean Intersection-over-Union (mIoU) as the main metric following MAE (He et al., 2022) (e.g., run each setting 3 times and take the mean). In sum, all hyper-parameters here are following standard practices for fair comparisons of pre-training settings."
        },
        {
            "heading": "4.2 EXPERIMENTAL RESULTS",
            "text": "Ablation studies. In Tab. 1, we ablate the most important design choices in RAE and R-MAE: Tab. 1a shows the performance of the RAE alone w.r.t. the number of regions. RAE improves when more\nregions per image are sampled during pre-training. From Tabs. 1b and 1c, we conclude the channel variant is efficient due to the share of computation in the intermediate blocks of the architecture, but lags behind in the performance. This proves that learning permutation equivariance of multiple region maps within an image is non-trivial. While the batch variant effectively deals with the permutation of regions and demonstrates strong performance, it comes with high computational cost (see Tab. 1b). By treating regions as queries, the length variant provides the best trade-off between speed and accuracy, which is important to process multiple regions per image.\nIn Tab. 1d, we compare the performance of RAE with regions from different sources: FH regions as our default setting, panoptic regions from COCO ground-truth (Lin et al., 2014) and regions generated by SAM (Kirillov et al., 2023). While panoptic regions only improve on semantic segmentation, region maps from SAM contribute to boost the performance of RAE by a large margin on all tasks compared to the default FH regions. Surprisingly, RAE alone with SAM regions outperforms MAE (50.6 vs. 50.1 for APbox and 46.8 vs. 45.9 for mIoU) with less computational requirements (more details in Appendix). This validates that masked region autoencoding is an effective pre-text task especially when fed with high-quality regions.\nWhile SAM regions are superior in accuracy, we still focus on regions generated by FH (Felzenszwalb & Huttenlocher, 2004) \u2013 a fast and simple clustering algorithm. Unlike SAM, FH algorithm is fully unsupervised and therefore best aligned with the notion of self-supervised learning \u2013 our focus of research.\nWith FH regions, we show in Tab. 1e the results of our full pre-training pipeline, R-MAE, by integrating RAE and MAE. Specifically, we jointly optimize masked region autoencoding with pixel reconstruction from MAE in R-MAE. The asymmetric design that only feeds representation from pixels to regions (\u2190) achieves the best results compared to joint (\u2194) and regions to pixels (\u2192). Thanks to the lightweight length variant of our RAE, the improvement comes with very minor computational costs: the region branch only adds \u223c1% FLOPs to the MAE baseline (9.8b vs. 9.7b).2\nMask ratios. We study mask ratio as the most important hyper-parameter from MAE in Figs. 4a and 4b. Starting from the default value 0.75, we either vary the region ratio alone, or jointly with the image one. In both cases, we share the random masks whenever possible (among the image and its regions) to minimize the information leak. The results suggest that a high mask ratio (\u223c0.75) is still required.\nNext, we generalize our finding and show that R-MAE performs well with high-quality regions from SAM, with more pre-training data, and on the long-tailed object detection task.\nPre-training on high-quality regions. We show in Tab. 1d that RAE alone is an effective task when provided high-quality regions. Similarly, when integrated with MAE, R-MAE demonstrates the same behaviour as shown in Tab. 2, improving over the strong baseline from MAE.\nMore data on COCO. The second generalization is on pre-training data scale \u2013 if adding more data changes our observation. To this end, we add COCO unlabeled2017, and again pre-train\n2Technically for R-MAE, computing regions is also an overhead. Yet FH is O(n log (n)) w.r.t. number of pixels (Felzenszwalb & Huttenlocher, 2004) and pre-computable using cpus. Empirically we find it\u2019s negligible.\nmethod train2017 only +unlabeled2017 APb APm mIoU APb APm mIoU MAE 50.1 44.6 45.9 51.5 45.9 48.4 R-MAE 50.6 45.0 46.8 52.1 46.1 48.7\nR-MAE with FH regions for 4k epochs following Hu et al. (2022). Results are summarized in Tab. 3. Without changing any hyper-parameters, R-MAE continues to outperform MAE.\nComparison on LVIS detection. As a generalization of the downstream task, we further report the evaluation of R-MAE and MAE on the LVIS benchmark (Gupta et al., 2019). This dataset includes \u223c2m high-quality instance segmentation labels for 1203 categories that exhibit a natural, long-tailed object distribution. Unlike COCO, there is a significant imbalance in class distribution with many rare classes having very few (e.g., <10) training examples. We use the same training recipe as Li et al. (2022b) for LVIS. We directly evaluate the backbones pre-trained with FH regions on COCO train2017. The results are presented in Tab. 4, where we observe a similar gain as on COCO detection. Notably, R-MAE shows a bigger improvement on the rare, or tail classes, suggesting the priors learned in R-MAE is more decoupled from category labels.\nR-MAE 1\u00d7 52.3 46.4 47.5\nmethod APb APm mIoU supervised 47.9 42.9 47.4 MoCo v3 47.9 42.7 47.3 BEiT 49.8 44.4 47.1 R-MAE 52.3 46.4 47.5\nTable 6: Comparison with other pre-training methods.\nState-of-the-art comparison with ImageNet pre-training. In Tab. 5 we summarize our comparison among latest MAE variants (Hu et al., 2022; Li et al., 2022a; Chen et al., 2022a; Bachmann et al., 2022; Chen et al., 2023) on COCO object detection and instance segmentation, along with ADE20K semantic segmentation. The transferring recipe follows ViTDet (Li et al., 2022b) for COCO object detection and instance segmentation (i.e., 100 epochs with batch size of 64), and MAE (He et al., 2022) for ADE20K semantic segmentation (i.e., 100 epochs with batch size of 16). All methods are pre-trained on ImageNet (Chen et al., 2020b; He et al., 2022).\nR-MAE is the most efficient among all MAE variants in terms of computation in FLOPs. For example, Long-Seq MAE (Hu et al., 2022) and SemMAE (Li et al., 2022a) are more than 4\u00d7 as expensive due to a longer sequence length.\nIt should also be noted that MultiMAE (Bachmann et al., 2022) employs regions extracted from a state-of-the-art detector (i.e., Mask2Former (Cheng et al., 2021)) and SemMAE (Li et al., 2022a) utilizes regions generated by a variant of iBot (Zhou et al., 2022). In contrast, R-MAE simply learns to reconstruct FH regions which can be generated by an efficient clustering algorithm.\nAcross all the methods compared, R-MAE achieves the best results on object detection and instance segmentation. For semantic segmentation, it comes as a second, only behind the most recent MixedAE work (Chen et al., 2023) which is more expensive in compute.\nTo complete our picture for comparison, we also included results with other types of ImageNet-based pre-training in Tab. 6. This incudes supervised learning with labels, contrastive learning (Chen et al., 2021), and masked token prediction (Bao et al., 2022). We outperform on all the benchmarks.\nQualitative results. Fig. 5 shows the region reconstruction of R-MAE pre-trained with FH regions. R-MAE for interactive segmentation. Since the pre-training task is to complete regions, our pre-trained R-MAE model can naturally act as \u2018interactive segmenter\u2019 (Sofiiuk et al., 2020). In fact,\nif we view the visible foreground/background patches as prompts (Kirillov et al., 2023), then RAE shares the same task nature as SAM. While our focus is on representation learning, not on generation quality \u2013 which leads to distinctions in design (e.g., efficient decoding of multiple regions with the length variant), R-MAE can still perform interactive segmentation, which we show next.\nSpecifically, we ask it to take the image along with some patches-of-interest as its inputs after pre-training. In an interactive segmentation setting, these patches can be provided by user clicks or eye gazing. A reasonable model can then predict the object corresponding to the given patches. From Fig. 6, we can see that the pre-trained model can indeed predict high-quality regions even with 90% of the patches masked, and continue to refine when more hints are supplied (from left to right)."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "In this work, we presented a simple yet effective approach (R-MAE) to explore an important vision concept \u2013 region in MAE (He et al., 2022). Through quantitative and qualitative results, we showed R-MAE is indeed more \u2018region-aware\u2019, and can consistently help downstream performance on localization-related tasks (e.g. detection and segmentation).\nLimitations. While regions share resemblances to words (e.g., in being discrete), there are other aspects of words that regions may still lack (e.g., it\u2019s debatable if they provide enough semantics). Therefore, our work is still a first step towards truly closing the gap to words for large language models in NLP. Nevertheless, we believe our exploration is valuable towards uncovering the visual analogue of words in computer vision, and can inspire more future efforts along this direction.\nWhile regions from SAM (Kirillov et al., 2023) significantly boost the performance of R-MAE, SAM itself initializes from MAE, is computationally expensive, and requires large-scale learning with human in the loop. A possible next step is to nail down the true reason why SAM regions are helpful, and minimize the complexities in this pipeline."
        }
    ],
    "title": "R-MAE: REGIONS MEET MASKED AUTOENCODERS",
    "year": 2023
}