{
    "abstractText": "The full potential of large pretrained models remains largely untapped in control domains like robotics. This is mainly due to data scarcity and computational challenges associated with training or fine-tuning large models for such applications. Prior work mainly emphasizes either effective pretraining of large models for decision-making or single-task adaptation. But real-world problems will require data-efficient, continual adaptation for new control tasks. Recognizing these constraints, we introduce TAIL (Task-specific Adapters for Imitation Learning), a framework for efficient adaptation to a stream of new control tasks. Inspired by recent advancements in parameter-efficient fine-tuning in language domains, we explore efficient fine-tuning techniques\u2014e.g., Bottleneck Adapters, P-Tuning, and Low-Rank Adaptation (LoRA)\u2014in TAIL to adapt large pretrained models for new tasks with limited demonstration data. Our extensive experiments comparing prevalent parameter-efficient fine-tuning techniques and adaptation baselines suggest that TAIL with LoRA can achieve the best post-adaptation performance with only 1% of the trainable parameters of full fine-tuning while avoiding catastrophic forgetting and preserving adaptation plasticity in continual learning settings.",
    "authors": [
        {
            "affiliations": [],
            "name": "Zuxin Liu"
        },
        {
            "affiliations": [],
            "name": "Jesse Zhang"
        },
        {
            "affiliations": [],
            "name": "Kavosh Asadi"
        },
        {
            "affiliations": [],
            "name": "Yao Liu"
        },
        {
            "affiliations": [],
            "name": "Ding Zhao"
        },
        {
            "affiliations": [],
            "name": "Shoham Sabach"
        },
        {
            "affiliations": [],
            "name": "Rasool Fakoor"
        }
    ],
    "id": "SP:12e2119a98ef96b8e2eae4ba8b088f326789d9ba",
    "references": [
        {
            "authors": [
                "Rishabh Agarwal",
                "Max Schwarzer",
                "Pablo Samuel Castro",
                "Aaron C Courville",
                "Marc Bellemare"
            ],
            "title": "Reincarnating reinforcement learning: Reusing prior computation to accelerate progress",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Armen Aghajanyan",
                "Luke Zettlemoyer",
                "Sonal Gupta"
            ],
            "title": "Intrinsic dimensionality explains the effectiveness of language model fine-tuning",
            "venue": "arXiv preprint arXiv:2012.13255,",
            "year": 2020
        },
        {
            "authors": [
                "Christopher M Bishop"
            ],
            "title": "Mixture density networks",
            "year": 1994
        },
        {
            "authors": [
                "Konstantinos Bousmalis",
                "Giulia Vezzani",
                "Dushyant Rao",
                "Coline Devin",
                "Alex X Lee",
                "Maria Bauza",
                "Todor Davchev",
                "Yuxiang Zhou",
                "Agrim Gupta",
                "Akhil Raju"
            ],
            "title": "Robocat: A self-improving foundation agent for robotic manipulation",
            "venue": "arXiv preprint arXiv:2306.11706,",
            "year": 2023
        },
        {
            "authors": [
                "Stevo Bozinovski",
                "Ante Fulgosi"
            ],
            "title": "The influence of pattern similarity and transfer learning upon training of a base perceptron b2",
            "venue": "In Proceedings of Symposium Informatica,",
            "year": 1976
        },
        {
            "authors": [
                "Anthony Brohan",
                "Noah Brown",
                "Justice Carbajal",
                "Yevgen Chebotar",
                "Joseph Dabis",
                "Chelsea Finn",
                "Keerthana Gopalakrishnan",
                "Karol Hausman",
                "Alex Herzog",
                "Jasmine Hsu"
            ],
            "title": "Rt-1: Robotics transformer for real-world control at scale",
            "venue": "arXiv preprint arXiv:2212.06817,",
            "year": 2022
        },
        {
            "authors": [
                "Anthony Brohan",
                "Noah Brown",
                "Justice Carbajal",
                "Yevgen Chebotar",
                "Xi Chen",
                "Krzysztof Choromanski",
                "Tianli Ding",
                "Danny Driess",
                "Avinava Dubey",
                "Chelsea Finn",
                "Pete Florence",
                "Chuyuan Fu",
                "Montse Gonzalez Arenas"
            ],
            "title": "Rt-2: Vision-language-action models transfer web knowledge to robotic control",
            "venue": "In arXiv preprint arXiv:2307.15818,",
            "year": 2023
        },
        {
            "authors": [
                "Tom B. Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell",
                "Sandhini Agarwal",
                "Ariel Herbert-Voss",
                "Gretchen Krueger",
                "Tom Henighan",
                "Rewon Child",
                "Aditya Ramesh"
            ],
            "title": "Language models are few-shot learners, 2020",
            "year": 2020
        },
        {
            "authors": [
                "Massimo Caccia",
                "Jonas Mueller",
                "Taesup Kim",
                "Laurent Charlin",
                "Rasool Fakoor"
            ],
            "title": "Task-agnostic continual reinforcement learning: Gaining insights and overcoming challenges",
            "venue": "In Conference on Lifelong Learning Agents,",
            "year": 2023
        },
        {
            "authors": [
                "Arslan Chaudhry",
                "Marcus Rohrbach",
                "Mohamed Elhoseiny",
                "Thalaiyasingam Ajanthan",
                "Puneet K Dokania",
                "Philip HS Torr",
                "Marc\u2019Aurelio Ranzato"
            ],
            "title": "On tiny episodic memories in continual learning",
            "year": 1902
        },
        {
            "authors": [
                "Yevgen Chebotar",
                "Karol Hausman",
                "Yao Lu",
                "Ted Xiao",
                "Dmitry Kalashnikov",
                "Jake Varley",
                "Alex Irpan",
                "Benjamin Eysenbach",
                "Ryan Julian",
                "Chelsea Finn",
                "Sergey Levine"
            ],
            "title": "Actionable models: Unsupervised offline reinforcement learning of robotic skills",
            "venue": "arXiv preprint arXiv:2104.07749,",
            "year": 2021
        },
        {
            "authors": [
                "Baiming Chen",
                "Zuxin Liu",
                "Jiacheng Zhu",
                "Mengdi Xu",
                "Wenhao Ding",
                "Liang Li",
                "Ding Zhao"
            ],
            "title": "Context-aware safe reinforcement learning for non-stationary environments",
            "venue": "IEEE International Conference on Robotics and Automation (ICRA),",
            "year": 2021
        },
        {
            "authors": [
                "Lili Chen",
                "Kevin Lu",
                "Aravind Rajeswaran",
                "Kimin Lee",
                "Aditya Grover",
                "Misha Laskin",
                "Pieter Abbeel",
                "Aravind Srinivas",
                "Igor Mordatch"
            ],
            "title": "Decision transformer: Reinforcement learning via sequence modeling",
            "venue": "Advances in neural information processing systems,",
            "year": 2021
        },
        {
            "authors": [
                "Shoufa Chen",
                "Chongjian Ge",
                "Zhan Tong",
                "Jiangliu Wang",
                "Yibing Song",
                "Jue Wang",
                "Ping Luo"
            ],
            "title": "Adaptformer: Adapting vision transformers for scalable visual recognition",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Open X-Embodiment Collaboration",
                "Abhishek Padalkar",
                "Acorn Pooley",
                "Ajinkya Jain",
                "Alex Bewley",
                "Alex Herzog",
                "Alex Irpan",
                "Alexander Khazatsky",
                "Anant Rai",
                "Anikait Singh",
                "Anthony Brohan",
                "Antonin Raffin",
                "Ayzaan Wahid"
            ],
            "title": "Open X-Embodiment: Robotic learning datasets and RT-X models",
            "venue": "https://robotics-transformer-x.github.io,",
            "year": 2023
        },
        {
            "authors": [
                "Thomas G Dietterich",
                "Lorien Pratt",
                "Sebastian Thrun"
            ],
            "title": "Special issue on inductive transfer",
            "venue": "Machine Learning,",
            "year": 1997
        },
        {
            "authors": [
                "Danny Driess",
                "Fei Xia",
                "Mehdi S.M. Sajjadi",
                "Corey Lynch",
                "Aakanksha Chowdhery",
                "Brian Ichter",
                "Ayzaan Wahid",
                "Jonathan Tompson",
                "Quan Vuong",
                "Tianhe Yu",
                "Wenlong Huang"
            ],
            "title": "Palm-e: An embodied multimodal language model",
            "venue": "In arXiv preprint arXiv:2303.03378,",
            "year": 2023
        },
        {
            "authors": [
                "Haotian Fu",
                "Shangqun Yu",
                "Michael Littman",
                "George Konidaris"
            ],
            "title": "Model-based lifelong reinforcement learning with bayesian exploration",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Kristen Grauman",
                "Andrew Westbury",
                "Eugene Byrne",
                "Zachary Chavis",
                "Antonino Furnari",
                "Rohit Girdhar",
                "Jackson Hamburger",
                "Hao Jiang",
                "Miao Liu",
                "Xingyu Liu"
            ],
            "title": "Ego4d: Around the world in 3,000 hours of egocentric video",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Abhishek Gupta",
                "Corey Lynch",
                "Brandon Kinman",
                "Garrett Peake",
                "Sergey Levine",
                "Karol Hausman"
            ],
            "title": "Demonstration-bootstrapped autonomous practicing via multi-task reinforcement learning",
            "year": 2022
        },
        {
            "authors": [
                "Nicklas Hansen",
                "Zhecheng Yuan",
                "Yanjie Ze",
                "Tongzhou Mu",
                "Aravind Rajeswaran",
                "Hao Su",
                "Huazhe Xu",
                "Xiaolong Wang"
            ],
            "title": "On pre-training for visuo-motor control: Revisiting a learning-from-scratch baseline",
            "venue": "arXiv preprint arXiv:2212.05749,",
            "year": 2022
        },
        {
            "authors": [
                "Junxian He",
                "Chunting Zhou",
                "Xuezhe Ma",
                "Taylor Berg-Kirkpatrick",
                "Graham Neubig"
            ],
            "title": "Towards a unified view of parameter-efficient transfer learning",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Neil Houlsby",
                "Andrei Giurgiu",
                "Stanislaw Jastrzebski",
                "Bruna Morrone",
                "Quentin De Laroussilhe",
                "Andrea Gesmundo",
                "Mona Attariyan",
                "Sylvain Gelly"
            ],
            "title": "Parameter-efficient transfer learning for nlp",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Edward J Hu",
                "Yelong Shen",
                "Phillip Wallis",
                "Zeyuan Allen-Zhu",
                "Yuanzhi Li",
                "Shean Wang",
                "Lu Wang",
                "Weizhu Chen"
            ],
            "title": "Lora: Low-rank adaptation of large language models",
            "venue": "arXiv preprint arXiv:2106.09685,",
            "year": 2021
        },
        {
            "authors": [
                "Eric Jang",
                "Alex Irpan",
                "Mohi Khansari",
                "Daniel Kappler",
                "Frederik Ebert",
                "Corey Lynch",
                "Sergey Levine",
                "Chelsea Finn"
            ],
            "title": "BC-z: Zero-shot task generalization with robotic imitation learning",
            "venue": "In 5th Annual Conference on Robot Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Yunfan Jiang",
                "Agrim Gupta",
                "Zichen Zhang",
                "Guanzhi Wang",
                "Yongqiang Dou",
                "Yanjun Chen",
                "Li Fei-Fei",
                "Anima Anandkumar",
                "Yuke Zhu",
                "Linxi Fan"
            ],
            "title": "Vima: General robot manipulation with multimodal prompts",
            "venue": "In Fortieth International Conference on Machine Learning,",
            "year": 2023
        },
        {
            "authors": [
                "Dmitry Kalashnkov",
                "Jake Varley",
                "Yevgen Chebotar",
                "Ben Swanson",
                "Rico Jonschkowski",
                "Chelsea Finn",
                "Sergey Levine",
                "Karol Hausman"
            ],
            "title": "Mt-opt: Continuous multi-task robotic reinforcement learning at scale",
            "year": 2021
        },
        {
            "authors": [
                "Liyiming Ke",
                "Sanjiban Choudhury",
                "Matt Barnes",
                "Wen Sun",
                "Gilwoo Lee",
                "Siddhartha Srinivasa"
            ],
            "title": "Imitation learning as f -divergence minimization",
            "year": 1905
        },
        {
            "authors": [
                "James Kirkpatrick",
                "Razvan Pascanu",
                "Neil Rabinowitz",
                "Joel Veness",
                "Guillaume Desjardins",
                "Andrei A Rusu",
                "Kieran Milan",
                "John Quan",
                "Tiago Ramalho",
                "Agnieszka Grabska-Barwinska"
            ],
            "title": "Overcoming catastrophic forgetting in neural networks",
            "venue": "Proceedings of the national academy of sciences,",
            "year": 2017
        },
        {
            "authors": [
                "Ananya Kumar",
                "Aditi Raghunathan",
                "Robbie Matthew Jones",
                "Tengyu Ma",
                "Percy Liang"
            ],
            "title": "Finetuning can distort pretrained features and underperform out-of-distribution",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Saurabh Kumar",
                "Henrik Marklund",
                "Benjamin Van Roy"
            ],
            "title": "Maintaining plasticity via regenerative regularization",
            "venue": "arXiv preprint arXiv:2308.11958,",
            "year": 2023
        },
        {
            "authors": [
                "Yoonho Lee",
                "Annie S Chen",
                "Fahim Tajwar",
                "Ananya Kumar",
                "Huaxiu Yao",
                "Percy Liang",
                "Chelsea Finn"
            ],
            "title": "Surgical fine-tuning improves adaptation to distribution shifts",
            "venue": "International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Brian Lester",
                "Rami Al-Rfou",
                "Noah Constant"
            ],
            "title": "The power of scale for parameter-efficient prompt tuning",
            "venue": "arXiv preprint arXiv:2104.08691,",
            "year": 2021
        },
        {
            "authors": [
                "Xiang Lisa Li",
                "Percy Liang"
            ],
            "title": "Prefix-tuning: Optimizing continuous prompts for generation",
            "venue": "arXiv preprint arXiv:2101.00190,",
            "year": 2021
        },
        {
            "authors": [
                "Anthony Liang",
                "Ishika Singh",
                "Karl Pertsch",
                "Jesse Thomason"
            ],
            "title": "Transformer adapters for robot learning",
            "venue": "CoRL",
            "year": 2022
        },
        {
            "authors": [
                "Bo Liu",
                "Yifeng Zhu",
                "Chongkai Gao",
                "Yihao Feng",
                "Qiang Liu",
                "Yuke Zhu",
                "Peter Stone"
            ],
            "title": "Libero: Benchmarking knowledge transfer for lifelong robot learning",
            "venue": "arXiv preprint arXiv:2306.03310,",
            "year": 2023
        },
        {
            "authors": [
                "Zuxin Liu",
                "Zijian Guo",
                "Yihang Yao",
                "Zhepeng Cen",
                "Wenhao Yu",
                "Tingnan Zhang",
                "Ding Zhao"
            ],
            "title": "Constrained decision transformer for offline safe reinforcement learning",
            "venue": "arXiv preprint arXiv:2302.07351,",
            "year": 2023
        },
        {
            "authors": [
                "David Lopez-Paz",
                "Marc\u2019Aurelio Ranzato"
            ],
            "title": "Gradient episodic memory for continual learning",
            "venue": "In Advances in Neural Information Processing Systems (NIPS),",
            "year": 2017
        },
        {
            "authors": [
                "Yao Lu",
                "Karol Hausman",
                "Yevgen Chebotar",
                "Mengyuan Yan",
                "Eric Jang",
                "Alexander Herzog",
                "Ted Xiao",
                "Alex Irpan",
                "Mohi Khansari",
                "Dmitry Kalashnikov",
                "Sergey Levine"
            ],
            "title": "Aw-opt: Learning robotic skills with imitation andreinforcement at scale",
            "venue": "In 5th Annual Conference on Robot Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Clare Lyle",
                "Mark Rowland",
                "Will Dabney"
            ],
            "title": "Understanding and preventing capacity loss in reinforcement learning",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Yecheng Jason Ma",
                "Shagun Sodhani",
                "Dinesh Jayaraman",
                "Osbert Bastani",
                "Vikash Kumar",
                "Amy Zhang"
            ],
            "title": "Vip: Towards universal visual reward and representation via value-implicit pre-training",
            "venue": "arXiv preprint arXiv:2210.00030,",
            "year": 2022
        },
        {
            "authors": [
                "Yecheng Jason Ma",
                "William Liang",
                "Vaidehi Som",
                "Vikash Kumar",
                "Amy Zhang",
                "Osbert Bastani",
                "Dinesh Jayaraman"
            ],
            "title": "Liv: Language-image representations and rewards for robotic control",
            "venue": "arXiv preprint arXiv:2306.00958,",
            "year": 2023
        },
        {
            "authors": [
                "Arjun Majumdar",
                "Karmesh Yadav",
                "Sergio Arnaud",
                "Yecheng Jason Ma",
                "Claire Chen",
                "Sneha Silwal",
                "Aryan Jain",
                "Vincent-Pierre Berges",
                "Pieter Abbeel",
                "Jitendra Malik",
                "Dhruv Batra",
                "Yixin Lin",
                "Oleksandr Maksymets",
                "Aravind Rajeswaran",
                "Franziska Meier"
            ],
            "title": "Where are we in the search for an artificial visual cortex for embodied intelligence? 2023a",
            "year": 2023
        },
        {
            "authors": [
                "Arjun Majumdar",
                "Karmesh Yadav",
                "Sergio Arnaud",
                "Yecheng Jason Ma",
                "Claire Chen",
                "Sneha Silwal",
                "Aryan Jain",
                "Vincent-Pierre Berges",
                "Pieter Abbeel",
                "Jitendra Malik"
            ],
            "title": "Where are we in the search for an artificial visual cortex for embodied intelligence",
            "venue": "arXiv preprint arXiv:2303.18240,",
            "year": 2023
        },
        {
            "authors": [
                "Arun Mallya",
                "Svetlana Lazebnik"
            ],
            "title": "Packnet: Adding multiple tasks to a single network by iterative pruning",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2018
        },
        {
            "authors": [
                "Ajay Mandlekar",
                "Danfei Xu",
                "Josiah Wong",
                "Soroush Nasiriany",
                "Chen Wang",
                "Rohun Kulkarni",
                "Li FeiFei",
                "Silvio Savarese",
                "Yuke Zhu",
                "Roberto Mart\u00edn-Mart\u00edn"
            ],
            "title": "What matters in learning from offline human demonstrations for robot manipulation",
            "venue": "arXiv preprint arXiv:2108.03298,",
            "year": 2021
        },
        {
            "authors": [
                "Yuning Mao",
                "Lambert Mathias",
                "Rui Hou",
                "Amjad Almahairi",
                "Hao Ma",
                "Jiawei Han",
                "Wen-tau Yih",
                "Madian Khabsa"
            ],
            "title": "Unipelt: A unified framework for parameter-efficient language model tuning",
            "year": 2022
        },
        {
            "authors": [
                "Michael McCloskey",
                "Neal J Cohen"
            ],
            "title": "Catastrophic interference in connectionist networks: The sequential learning problem",
            "venue": "In Psychology of learning and motivation,",
            "year": 1989
        },
        {
            "authors": [
                "Suraj Nair",
                "Aravind Rajeswaran",
                "Vikash Kumar",
                "Chelsea Finn",
                "Abhinav Gupta"
            ],
            "title": "R3m: A universal visual representation for robot manipulation",
            "venue": "arXiv preprint arXiv:2203.12601,",
            "year": 2022
        },
        {
            "authors": [
                "Ethan Perez",
                "Florian Strub",
                "Harm de Vries",
                "Vincent Dumoulin",
                "Aaron Courville"
            ],
            "title": "Film: Visual reasoning with a general conditioning layer, 2017",
            "year": 2017
        },
        {
            "authors": [
                "Jonas Pfeiffer",
                "Aishwarya Kamath",
                "Andreas R\u00fcckl\u00e9",
                "Kyunghyun Cho",
                "Iryna Gurevych"
            ],
            "title": "Adapterfusion: Non-destructive task composition for transfer learning",
            "venue": "arXiv preprint arXiv:2005.00247,",
            "year": 2020
        },
        {
            "authors": [
                "Jonas Pfeiffer",
                "Andreas R\u00fcckl\u00e9",
                "Clifton Poth",
                "Aishwarya Kamath",
                "Ivan Vuli\u0107",
                "Sebastian Ruder",
                "Kyunghyun Cho",
                "Iryna Gurevych"
            ],
            "title": "Adapterhub: A framework for adapting transformers",
            "venue": "arXiv preprint arXiv:2007.07779,",
            "year": 2020
        },
        {
            "authors": [
                "Alec Radford",
                "Jeff Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners. 2019a",
            "year": 2019
        },
        {
            "authors": [
                "Alec Radford",
                "Jeffrey Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "venue": "OpenAI blog,",
            "year": 2019
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "venue": "In International conference on machine learning,",
            "year": 2021
        },
        {
            "authors": [
                "S. Rebuffi",
                "A. Vedaldi",
                "H. Bilen"
            ],
            "title": "Efficient parametrization of multi-domain deep neural networks",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2018
        },
        {
            "authors": [
                "Scott Reed",
                "Konrad Zolna",
                "Emilio Parisotto",
                "Sergio G\u00f3mez Colmenarejo",
                "Alexander Novikov",
                "Gabriel Barth-maron",
                "Mai Gim\u00e9nez",
                "Yury Sulsky"
            ],
            "title": "A generalist agent",
            "venue": "Transactions on Machine Learning Research,",
            "year": 2835
        },
        {
            "authors": [
                "David Rolnick",
                "Arun Ahuja",
                "Jonathan Schwarz",
                "Timothy Lillicrap",
                "Gregory Wayne"
            ],
            "title": "Experience replay for continual learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Stephane Ross",
                "Geoffrey Gordon",
                "Drew Bagnell"
            ],
            "title": "A reduction of imitation learning and structured prediction to no-regret online learning",
            "venue": "In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,",
            "year": 2011
        },
        {
            "authors": [
                "J\u00fcrgen Schmidhuber"
            ],
            "title": "Learning complex, extended sequences using the principle of history compression",
            "venue": "Neural Computation,",
            "year": 1992
        },
        {
            "authors": [
                "Thomas Schmied",
                "Markus Hofmarcher",
                "Fabian Paischer",
                "Razvan Pascanu",
                "Sepp Hochreiter"
            ],
            "title": "Learning to modulate pre-trained models in rl",
            "venue": "arXiv preprint arXiv:2306.14884,",
            "year": 2023
        },
        {
            "authors": [
                "Nur Muhammad Mahi Shafiullah",
                "Zichen Jeff Cui",
                "Ariuntuya Altanzaya",
                "Lerrel Pinto"
            ],
            "title": "Behavior transformers: Cloning $k$ modes with one stone",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Mohit Sharma",
                "Claudio Fantacci",
                "Yuxiang Zhou",
                "Skanda Koppula",
                "Nicolas Heess",
                "Jon Scholz",
                "Yusuf Aytar"
            ],
            "title": "Lossless adaptation of pretrained vision models for robotic manipulation",
            "venue": "arXiv preprint arXiv:2304.06600,",
            "year": 2023
        },
        {
            "authors": [
                "Mohit Shridhar",
                "Lucas Manuelli",
                "Dieter Fox"
            ],
            "title": "Cliport: What and where pathways for robotic manipulation",
            "venue": "In Conference on Robot Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Sebastian Thrun",
                "Tom M Mitchell"
            ],
            "title": "Lifelong robot learning",
            "venue": "In The biology and technology of intelligent autonomous agents,",
            "year": 1995
        },
        {
            "authors": [
                "Hugo Touvron",
                "Thibaut Lavril",
                "Gautier Izacard",
                "Xavier Martinet",
                "Marie-Anne Lachaux",
                "Timoth\u00e9e Lacroix",
                "Baptiste Rozi\u00e8re",
                "Naman Goyal",
                "Eric Hambro",
                "Faisal Azhar",
                "Aurelien Rodriguez",
                "Armand Joulin",
                "Edouard Grave",
                "Guillaume Lample"
            ],
            "title": "Llama: Open and efficient foundation language models, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Ren\u00e9 Traor\u00e9",
                "Hugo Caselles-Dupr\u00e9",
                "Timoth\u00e9e Lesort",
                "Te Sun",
                "Guanghang Cai",
                "Natalia D\u00edaz Rodr\u00edguez",
                "David Filliat"
            ],
            "title": "Discorl: Continual reinforcement learning via policy distillation",
            "year": 1907
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141 ukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Mengdi Xu",
                "Yikang Shen",
                "Shun Zhang",
                "Yuchen Lu",
                "Ding Zhao",
                "Joshua Tenenbaum",
                "Chuang Gan"
            ],
            "title": "Prompting decision transformer for few-shot policy generalization",
            "venue": "In international conference on machine learning,",
            "year": 2022
        },
        {
            "authors": [
                "Mengdi Xu",
                "Yuchen Lu",
                "Yikang Shen",
                "Shun Zhang",
                "Ding Zhao",
                "Chuang Gan"
            ],
            "title": "Hyper-decision transformer for efficient online policy adaptation",
            "venue": "arXiv preprint arXiv:2304.08487,",
            "year": 2023
        },
        {
            "authors": [
                "Yihang Yao",
                "Zuxin Liu",
                "Zhepeng Cen",
                "Jiacheng Zhu",
                "Wenhao Yu",
                "Tingnan Zhang",
                "Ding Zhao"
            ],
            "title": "Constraint-conditioned policy optimization for versatile safe reinforcement learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2024
        },
        {
            "authors": [
                "Jesse Zhang",
                "Karl Pertsch",
                "Jiahui Zhang",
                "Joseph J. Lim"
            ],
            "title": "Sprint: Scalable policy pre-training via language instruction relabeling",
            "venue": "arXiv preprint arXiv:2306.11886,",
            "year": 2023
        },
        {
            "authors": [
                "Jesse Zhang",
                "Jiahui Zhang",
                "Karl Pertsch",
                "Ziyi Liu",
                "Xiang Ren",
                "Minsuk Chang",
                "Shao-Hua Sun",
                "Joseph J Lim"
            ],
            "title": "Bootstrap your own skills: Learning to solve new tasks with large language model guidance",
            "venue": "In 7th Annual Conference on Robot Learning,",
            "year": 2023
        },
        {
            "authors": [
                "Agarwal et al",
                "Lyle"
            ],
            "title": "2022), preventing the weights from being trapped into local optimum. There is a potential to better utilize the trained adapter weights on preceding tasks. We outline several promising exploration directions in Appendix Section B.4. B.3 TRAINING HYPERPARAMETERS AND EXPERIMENT CONFIGURATIONS Following similar setup as in the LIBERO benchmark (Liu et al., 2023a), we perform data augmenta",
            "year": 2023
        },
        {
            "authors": [
                "Kumar"
            ],
            "title": "A noteworthy observation from Fig. 7 is the behavior of FFT. Despite achieving the lowest training loss across all stages, its validation loss spikes significantly after just a few epochs. This pattern suggests severe overfitting when FFT is applied to the entire parameter space using limited data. Intriguingly, this overfitting intensifies in the later adaptation phases, potentially signifying a distortion of pretrained features",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "A desired property of an autonomous agent is the ability to adapt efficiently to novel tasks. In vision and language domains, large pretrained models have demonstrated adaptation to new tasks with just a few examples through prior knowledge obtained from internet-scale datasets (Brown et al., 2020; Radford et al., 2021; Touvron et al., 2023). Similar methods have also been applied in decision-making and control applications (Brohan et al., 2022; Driess et al., 2023; Brohan et al., 2023). However, new control tasks are more difficult to adapt to than the aforementioned vision and language domains due to (1) the lack of internet-scale control data and (2) how optimal actions can vary significantly from task-to-task, even under shared observation spaces. As such, these large-scale decision-making models still rely on a close alignment between training and testing tasks.\nIn contrast, agents deployed in challenging environments need to adapt to major task variations\u2014take, for example, a general household robot. Equipped with a factory-pretrained policy, the robot will be employed in unique ways by every household. Thus, the robot will need to continually adapt in order to best serve each one, e.g., by fine-tuning its capabilities on a few demonstrations (Chebotar et al., 2021; Lu et al., 2021; Kalashnkov et al., 2021; Chen et al., 2021a; Yao et al., 2024). Because most prior decision-making papers adapt to new tasks by fine-tuning the entire model (Gupta et al., 2022; Bousmalis et al., 2023; Zhang et al., 2023a;b; Collaboration et al., 2023; Liu et al., 2023c), mastering each new skill requires great computational cost and often leads to catastrophic forgetting of old ones. An alternative approach would be to store a separate policy per new task, which leads to unreasonable storage requirements. Some prior work investigates efficient adaptation of large models to a single task suite (Liang et al., 2022; Schmied et al., 2023; Sharma et al., 2023), but this realistic continual learning setting brings out additional problems to consider, warranting further investigation. What would be the best way for agents to efficiently adapt to a stream of novel tasks without having to trade off computation, storage, and performance on older tasks?\nTo answer this question, we propose Task-specific Adapters for Imitation Learning, shown in Fig. 1, a framework for efficient adaptation to new control tasks. Through TAIL we (1) effectively incorporate lightweight adapter modules into pretrained decision-making models and (2) comprehensively com-\npare efficient adaptation techniques implemented in TAIL in a continual imitation learning setting. Notably, we examine parameter-efficient adaptation techniques (PEFT) used for large language models; we explore the potential of adapters (Houlsby et al., 2019), prefix tuning (Li & Liang, 2021), and low-rank adaptation (LoRA) (Hu et al., 2021) in fostering efficient and continual adaptation in large pretrained decision-making models. These works stand out as they introduce a small number of new parameters which help: avoid catastrophic forgetting, maintain training plasticity for continual learning, avoid overfitting with limited adaptation data, and reduce computational and memory burden. Investigating these works in control tasks for a realistic continual learning setup specifically is important because, unlike in language domains, test task losses are often not proportional to test task performance (Ross et al., 2011; Ke et al., 2020)\u2014efficient adaptation insights from language models may not transfer to decision-making ones. Thus, independent investigation of these adaptation techniques for decision-making is crucial for deploying continually adapting agents in the real world.\nWe compare PEFT techniques implemented in TAIL against commonly used adaptation methods in the imitation learning literature. In our experiments, we discover that TAIL with LoRA leads to the best post-adaptation performance as it preserves the original pretrained representations while being resilient against overfitting in the limited-data regime. These capabilities are especially important for agents operating in new, challenging environments, such as the aforementioned household robots. Our analysis also reveals important insights into the strengths and limitations of each adaptation strategy. Instead of performing full fine-tuning of the entire model, TAIL only introduces a small number of additional parameters without making changes to the original model. These additional parameters make up a mere 1.17% of the size of the original model. Importantly, this results in approximately 23% less GPU memory consumption to achieve 22% higher forward adaptation success rate than full fine-tuning while avoiding catastrophic forgetting. Notably, these results are contrary to many results from the vision and language model literature which show that full fine-tuning works better (He et al., 2022; Mao et al., 2022; Chen et al., 2022; Schmied et al., 2023).\nIn summary, this work bridges a crucial gap in research into efficient and continual adaptation for pretrained decision models by introducing a framework for continual imitation learning, TAIL, and thoroughly analyzing the effects of different efficient adaptation methods. Comprehensive experiments demonstrate that TAIL outperforms standard continual learning and prior single-task adaptation baselines."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Pretrained Models for Control. Researchers have long studied the use of pretrained models for better downstream transfer to related tasks (Bozinovski & Fulgosi, 1976; Schmidhuber, 1992; Dietterich et al., 1997). Recent works have examined using the representations learned by pretrained visual models for control (Shridhar et al., 2022; Nair et al., 2022; Ma et al., 2022; 2023; Majumdar et al., 2023a). These methods leverage representations acquired from large task-agnostic datasets, such as Ego4D (Grauman et al., 2022), or through self-supervised objectives. However, there\u2019s evidence that simply utilizing these pretrained features may not be as useful for downstream task performance (Hansen et al., 2022). Meanwhile, another recent line of work directly trains large pretrained models for control (Brohan et al., 2022; Reed et al., 2022; Driess et al., 2023; Jiang et al., 2023; Brohan et al., 2023; Bousmalis et al., 2023). These methods either do not attempt adaptation to new tasks, or perform expensive full-fine-tuning for adaptation. In contrast, our method, TAIL, is a framework for efficient adaptation of decision-making models, like the aforementioned large pretrained control models, and investigates ways to adapt such models efficiently to multiple new tasks.\nParameter-Efficient Fine-Tuning (PEFT). PEFT has gained traction as a way to adapt pretrained models without significantly increasing parameters. Rebuffi et al. (2018) demonstrated that residual adapters for smaller, CNN-based vision models are effective in non-control supervised learning settings. More recently, transformer-focused techniques such as transformer adapter modules (Houlsby et al., 2019), LoRA (Hu et al., 2021), and prompt tuning (Li & Liang, 2021) incorporate lightweight modules or prompts optimized for downstream tasks, all while preserving the original model weights. PEFT offers several advantages over full fine-tuning: it\u2019s faster, less susceptible to overfitting, retains prior capabilities, and facilitates efficient task-switching. While PEFT has been successful in both language and vision domains (Chen et al., 2022; Schmied et al., 2023), its continuous adaptation for large decision-making models is not yet thoroughly examined. Liang et al. (2022); Sharma et al. (2023), Xu et al. (2022), and Xu et al. (2023) propose the use of adapters, prompt-tuning, and hyper-network in robotics settings, but they do not examine other PEFT methods and focus on\nadaptation to a single task suite. We instead examine the performance of various state-of-the-art PEFT techniques implemented with TAIL in the continual learning scenario.\nContinual Learning. Continual learning in control (Thrun & Mitchell, 1995; McCloskey & Cohen, 1989; Fu et al., 2022) is a long-studied problem with applications to many real-world situations. In general, agents should be able to transfer knowledge (e.g., by continually fine-tuning) or experience (e.g., training data) from previously learned tasks to new tasks (Lopez-Paz & Ranzato, 2017; Traor\u00e9 et al., 2019; Fakoor et al., 2020; Caccia et al., 2023). However, with large pretrained models trained on large datasets, fine-tuning the entire model is computationally costly yet risks catastrophic forgetting, and transferring training data from other tasks is too memory inefficient in the face of a large stream of new tasks. Therefore, we present a study into efficient fine-tuning techniques which, when integrated with TAIL, can help inform future research of continual learning."
        },
        {
            "heading": "3 PRELIMINARIES",
            "text": "In this section, we introduce our problem setting (Sec. 3.1), review large, pretrained models for decision-making (Sec. 3.2), and discuss traditional adaptation methods in this area (Sec. 3.3)."
        },
        {
            "heading": "3.1 CONTINUAL IMITATION LEARNING",
            "text": "The agent encounters a sequence of K tasks, denoted as {T1, . . . , TK}. Each task Tk = (\u00b50k, gk) is characterized by an initial state distribution \u00b50k and a goal predicate gk. Goals for tasks can be specified using language instructions, providing clear context (Jang et al., 2021; Zhang et al., 2023a). For every task Tk, the agent receives N demonstration trajectories Dk = {\u03c41k , . . . , \u03c4Nk }. In this paper, we use the standard behavioral cloning loss to optimize the agent\u2019s policy \u03c0 over these demonstrations, however we note that TAIL can be used with other training objectives as well:\n\u03b8\u0302 = min \u03b8 K\u2211 k=1 E st,at\u223cDk [ lk\u2211 t=0 L ( \u03c0(a|s\u2264t, Tk;\u03b8), atk )] . (1)\nHere, L is a supervised action prediction (e.g., mean squared error or negative log likelihood) loss, lk is the length of demonstrations for task Tk, and \u03b8 refers to the learnable parameters of the network. Notably, after learning task Tk, the agent cannot access additional data from preceding tasks. This presents a continual learning challenge, emphasizing the importance of transferring knowledge across tasks without the risk of catastrophic forgetting (McCloskey & Cohen, 1989)."
        },
        {
            "heading": "3.2 PRETRAINED DECISION-MAKING MODELS",
            "text": "Here, we briefly describe common features of large pretrained decision-making model architectures used for embodied agents. We incorporate key components shared amongst these models into the architecture of the model that we pretrain to evaluate efficient adaptation, pictured in Fig. 1(a).\nTransformer Backbone. Most recent work training large-scale decision-making models (Brohan et al., 2022; Shafiullah et al., 2022; Brohan et al., 2023) utilize a transformer backbone (Vaswani et al., 2017) that attends to tokenized observations from prior timesteps. We adopt a standard GPT2 (Radford et al., 2019a) transformer decoder (Fig. 1(a), temporal decoder) with separate encoders for each input modality and continuous action distribution outputs.\nPretrained Input Encoders. Encoders pretrained on large, diverse datasets can produce rich, wellstructured embeddings which make it easier to learn the downstream tasks (Jang et al., 2021; Brohan et al., 2022). Therefore, we utilize pretrained CLIP image and textual encoders (Radford et al., 2021).\nInput Modality Fusion. The idea of explicitly \u201cfusing\u201d different input modalities has seen great success not only in domains like vision and language (Perez et al., 2017), but also in agent learning (Jang et al., 2021; Brohan et al., 2022). Similarly, we utilize FiLM layers (Perez et al., 2017) (Fig. 1(a), input fusion module) to fuse language task specifications with observations."
        },
        {
            "heading": "3.3 ADAPTING PRETRAINED MODELS FOR NEW TASKS",
            "text": "One standard adaptation method in prior research is full fine-tuning (FFT) of all model parameters (Fig 1(b), top left). Though straightforward, it is resource-intensive and prone to overfitting with limited data (Bousmalis et al., 2023). There is also a risk of distorting pretrained features, resulting in the loss of prior tasks\u2014a phenomenon known as catastrophic forgetting (McCloskey & Cohen, 1989). Evidence also suggests that extensive fine-tuning might undermine a model\u2019s rapid adaptability to new tasks, an effect referred to as the loss of model plasticity and capacity (Kumar et al., 2022; Lyle et al., 2022; Kumar et al., 2023). Such issues become more prominent in continual learning contexts (Lopez-Paz & Ranzato, 2017). Moreover, duplicating a sizable model for each subsequent task is neither efficient nor practical due to storage limitations.\nAnother standard adaptation method is the use of frozen pretrained features (FPF, Fig 1(b) top right). FPF ensures the retention of knowledge acquired from previous tasks by tuning a task-specific head. However, as noted in Sharma et al. (2023), it is not expressive enough for out-of-distribution or especially complex tasks. Given these challenges, there\u2019s a clear need for a more advanced fine-tuning paradigm that addresses catastrophic forgetting while maintaining model plasticity for adapting to new tasks, all in a data and computationally resource-efficient manner."
        },
        {
            "heading": "4 TASK-SPECIFIC ADAPTERS FOR IMITATION LEARNING",
            "text": "In this section, we outline how we perform efficient adaptation on pretrained models through our Task-specific Adapters for Imitation Learning framework, depicted in Fig 1(b). Different from the FPF approach which simply substitutes the policy head for every new task, TAIL introduces a small set of new weights, serving as a lightweight plugin to address specific tasks. This concept draws inspiration from parameter-efficient adaptation techniques prevalent in the language model area. These methods offer several advantages as they: (1) add a few parameters (typically between 0.1% \u223c 2%) to preserve the original features, thereby enhancing model plasticity for continual learning and avoiding catastrophic forgetting (Kumar et al., 2023), (2) are resilient to overfitting when adaptation data is scarce, (3) are more computationally and storage-efficient than FFT.\nNext, we delve into three prominent weight integration techniques for Transformer-based pretrained models in Sec. 4.1, followed by a case study illustrating the application of this framework in continual imitation learning scenarios in Sec. 4.2."
        },
        {
            "heading": "4.1 ADAPTER WEIGHTS INTEGRATION",
            "text": "The concept of an adapter can be best conceptualized as a modular plugin to the base model, customized for specific downstream tasks, that does not affect the model\u2019s pretrained representations. We mainly explore three prevalent styles of integration for TAIL: Parallel (Hu et al., 2021), Sequential (Houlsby et al., 2019; Sharma et al., 2023), and Prefix Token (Li & Liang, 2021; Lester et al., 2021; Liu et al., 2023b), all of which are showcased with a Transformer block in Fig. 2. Parallel and sequential integration techniques are generally applicable to any model with feedforward layers, while the prefix token style method is especially tailored for Transformers.\nGiven a pretrained model, let\u2019s consider one layer weight matrix in it, denoted as W \u2208 Rd\u00d7k. Its input and output hidden states are hin \u2208 Rd and hout \u2208 Rk, respectively. We have hout = W\u22a4hin. Next, we detail how to apply parallel and sequential insertions to the pretrained weight matrix.\nParallel Integration (LoRA). This integration method, often associated with Low-Rank Adaptation (LoRA) (Hu et al., 2021), introduces trainable low-rank matrices Wdown \u2208 Rd\u00d7r and Wup \u2208 Rr\u00d7k. Here, r \u226a min(d, k) represents the rank and is usually much smaller than the dimensions of the original matrix. These matrices are typically integrated in parallel with the original weight matrix W through addition, as shown as LoRA in Fig. 2:\nhout = W \u22a4hin + \u03b1W \u22a4 upW \u22a4 downhin, (2)\nwith \u03b1 being a hyperparameter to modulate task-specific adjustments. The above equation can also be formulated as: hout = (W + \u03b1WdownWup)\u22a4hin = (W + \u03b1\u2206W )\u22a4hin, where \u2206W denotes the weight modifications for new tasks, and thus the columns of Wdown and Wup can be interpreted as a new basis that contains task-specific knowledge. As observed by Aghajanyan et al. (2020), despite projecting to a condensed subspace with small \u201cintrinsic dimensions,\u201d pretrained models can still learn effectively. By introducing the two low-rank matrices, the original weight matrices W can be adeptly tailored with a minimal increase in parameters. Though LoRA was originally crafted for large language models\u2014specifically for the query and value projections matrices WQ and WV in multi-head attention (Hu et al., 2021)\u2014it is easily applied to other linear layers as well, such as the Transformer\u2019s feedforward layers (Chen et al., 2022).\nSequential Integration (Bottleneck Adapter). Renowned in the language model domain, the Bottleneck Adapter introduces bottleneck layers within the model (Houlsby et al., 2019; Sharma et al., 2023) by appending a trainable bottleneck layer after the feedforward network in each Transformer layer. Similar to LoRA, this bottleneck consists of down and up projections, Wdown and Wup, which first shrink then restore the dimensions of token hidden states. Formally, for the feedforward network\u2019s input hin and a bottleneck size r, the output hout is:\nhout = W \u22a4 up\u03d5 ( W\u22a4down(W \u22a4hin) ) , (3)\nwhere \u03d5 denotes a nonlinear activation function. The Bottleneck Adapter (Fig. 2) acts as a filter, isolating relevant information for specific tasks. Yet, filtering often requires a larger bottleneck size compared to that of LoRA, leading to more parameters. Additionally, the sequential insertion can increase latency compared to the parallel nature of LoRA (Hu et al., 2021).\nPrefix Token Integration (Prefix & Prompt-Tuning). In this style, a set of learnable prefix tokens are appended or prepended to the input sequence (Li & Liang, 2021; Lester et al., 2021; Liu et al., 2023b). Let\u2019s consider an input sequence s \u2208 Rn\u00d7d, where n is the sequence length and d is the embedding dimension. The prefix tokens can be represented as p \u2208 Rm\u00d7d, where m denotes the number of prefix tokens. These vectors act like virtual tokens which the original tokens can attend to. They are initialized and learned during the task-specific adaptation phase. The modified input sequence, after appending the prefix tokens, can be expressed as S = [p; s] \u2208 R(m+n)\u00d7d. The model then processes this extended sequence. These prefix tokens can be viewed as task descriptors that are designed to guide the model towards the desired task-specific behavior (see in Fig. 2).\nWith adapters, we can treat the optimization from Eq. 1 as one over adapter weights instead, where the model is parametrized by \u03b8\u0302 = {\u03b8,\u03c9} and \u03c9 is the set of adapter weights we are optimizing for."
        },
        {
            "heading": "4.2 TAIL FOR CONTINUAL IMITATION LEARNING",
            "text": "We consider the continual imitation learning problem as a typical application of the proposed TAIL adaptation paradigm. The goal of continual imitation learning is to ensure that the model performs effectively on the current task and without significant degradation of performance in past tasks. Given pretrained model weights, denoted as \u03b8, and a new task Tk with demonstrations Dk = {\u03c41k , . . . , \u03c4Nk }, we initialize the task-specific adapter weight \u03c9k with far less parameters than the base model: |\u03c9k| \u226a |\u03b8|. The adapter weights are inserted into the model through the integration methods introduced in Sec. 4.1. By optimizing the behavior cloning loss in Eq. 1 w.r.t \u03c9k while keeping the pretrained weights frozen, the policy adapts to Tk without interfering with previous tasks. To execute a task, the corresponding lightweight adapters are loaded as a plugin of the pretrained network weights. For example, when revisiting a prior task Tj , where j \u2264 k, the model is configured to solely activate the j-th adapter \u03c9j . This entire procedure can be streamlined as follows:\n1. For an incoming task Tk, acquire the training set Dk, initialize a task-specific adapter \u03c9k.\n2. Combine adapter \u03c9k with the base model \u03b8 using either parallel, sequential, or prefix token.\n3. Train the adapter on Dk to optimize Eq. 1 for \u03c9k, keeping pretrained parameters \u03b8 frozen.\nIn essence, TAIL ensures task-specific knowledge is contained within the adapters, thereby enabling efficient adaptation without catastrophic forgetting. It\u2019s also worth noting that the TAIL framework is flexible. The choice of integration method or the specific architecture of the adapter can be tailored based on the complexity of the task or the available computational resources."
        },
        {
            "heading": "5 EXPERIMENTS",
            "text": "In this section, we evaluate TAIL on a wide range of tasks and benchmark its performance against other fine-tuning approaches. We mainly aim to answer the following questions: (1) Which efficient adaptation methods in TAIL work best? (2) Can TAIL prevent catastrophic forgetting of previously learned tasks, while allowing more efficient forward adaptation to new tasks over standard adaptation methods? (3) What are the computational efficiencies gained by using TAIL? Addressing them requires a set of diverse tasks in realistic environments, as we describe in the following section."
        },
        {
            "heading": "5.1 DATASETS AND BENCHMARK SUITES",
            "text": "We utilize the LIBERO robotic manipulation continual learning benchmark (Liu et al., 2023a), which features a diverse range of tasks that mirror human daily activities, such as turning on a stove, moving books, and opening drawers. Each task is specified via natural language instructions, for instance, \"Open the top drawer of the cabinet, and put the bowl in it.\"\nWe craft a pretraining task suite, named Kitchen, involving 40 diverse tasks sourced from the LIBERO-90 dataset\u2019s kitchen scenes. We then evaluate adaptation to 5 separate task suites. LIBERO contains 3 task suites tailored for continual learning, focusing on evaluating different aspects of knowledge adaptation: the Spatial task contains the same objects in each scene but with different spatial layouts; each task in the Goal suite has distinct goals (such as open the drawer, or turn on the stove), while keeping the objects and layout fixed; the Object suite contains pick-and-place tasks for different objects in the scene but with the same layout. To create a more comprehensive experimental setting, we also create 2 additional task suites (from LIBERO-90): Living Room, and Study Room. We adopt 8 tasks from each of the 5 adaptation task suites, respectively. Finally, we separately evaluate each task sequentially in LIBERO-10, a benchmark with 10 challenging long-horizon tasks. See Fig. 3 for task suite examples and Appendix Sec. D for more details."
        },
        {
            "heading": "5.2 EXPERIMENT SETUP",
            "text": "Evaluation metrics. The primary metric we report is average per-task suite success rate, measured by checking if current state aligns with pre-defined goal states. For continual learning, we also assess Forward Transfer (FWT) and Backward Transfer (BWT) across the curriculum of suites. Following the metric proposed in LIBERO (Liu et al., 2023a), FWT is computed by the maximum success rate one algorithm can achieve when adapting to a new task. We denote FWT at task k as Fk. Meanwhile, BWT measures the success rate increase on previous tasks. Namely, when adapting to the k-th task, we record the best FWT model on this task and then evaluate this model on all previous k \u2212 1 tasks, obtaining success rate Si, 1 \u2264 i \u2264 k \u2212 1. Then we compute the success rate difference between the new model and the best FWT of the previous k \u2212 1 tasks and then average among them to obtain the BWT metric: Bk = 1k\u22121 \u2211k\u22121 i=1 (Si \u2212 Fi). For both metrics, higher is better.\nModel architecture. We adopt the CLIP-base model (Radford et al., 2021) as both the spatial encoder and the language instruction encoder, each with 12 transformer layers. A 6-layer GPT2 structure (Radford et al., 2019b) serves as our temporal encoder, with the FiLM module (Perez et al., 2017) handling input fusion. These components are well-regarded in the literature (Chen et al., 2021b; Brohan et al., 2022; Jiang et al., 2023). Further architectural details can be found in Appendix A.\nContinual Learning Baselines. We adopt four baselines: Full Fine-Tuning (FFT), Frozen Pretrained Features (FPF) which mirrors the linear probing method (Kumar et al., 2022) but also tunes both the policy head and fusion module per task, Experience Replay (ER) (Chaudhry et al., 2019) which uses a 50-50 data split between new and previous task data while adapting to a new task (Rolnick et al., 2019), Elastic Weight Consolidation (EWC) (Kirkpatrick et al., 2017) which regularizes updates of crucial parameters from earlier tasks based on their Fisher information, and PackNet (Mallya & Lazebnik, 2018) which prunes parameters to then be re-learned for every new task. These all use the same model and task conditioning, i.e., language, as TAIL. Further baseline details in Appendix B.1.\nTAIL Adapters. We use LoRA (Hu et al., 2021), Bottleneck Adapter (Houlsby et al., 2019), and Prefix Tuning (Li & Liang, 2021) to represent parallel, sequential, and prefix integration styles. RoboAdapter (Sharma et al., 2023), a specific implementation for decision-making, stands as another sequential integration style. Unlike the Bottleneck Adapter that applies weights at every transformer layer, it introduces weights only at specific transformer layers and exclusively after the feedforward layer. Configuration specifics and more details for these adapters are available in Appendix B.2.\nTraining, Adaptation, and Evaluation. Each task provides 50 successful human demonstrations. These are divided into 40 training trajectories and 10 for validation. We report success rates over 10 scenes with initial states that are unseen in training. This limited demonstration setup offers an opportunity to determine which technique is less prone to overfitting in data-restricted conditions. Given our focus on evaluating the adaptation of large pretrained models, we further increase adaptation difficulty by training on and evaluating adaptation performance on all tasks within a task suite simultaneously.1 We pretrain on Kitchen until performance convergence (100 epochs). Subsequent adaptations follow two setups: (1) sequential adaptation across the Spatial, Goal, Object, Living Room, and Study Room task suites for 100 epochs each, and (2) adaptation to each long-horizon task within the LIBERO-10 benchmark over 50 epochs. Each experiment is conducted with 3 different random seeds. Except for the Experience Replay (ER) method, data from earlier tasks remains unavailable in later stages. Our diverse adaptation setup provides a thorough\n1We use one adapter per task suite. LIBERO (Liu et al., 2023a) originally evaluated on a per-task basis.\nand in-depth examination of knowledge transfer across a spectrum of domains, including spatial, procedural, visual, and compositional.\nIn the pretraining phase for TAIL, we add trainable adapters to the CLIP spatial and instruction encoders while freezing the encoder weights. All other model weights are fully learnable. During adaptation, the CLIP encoders and the GPT2 decoder are frozen, while adapters for them, the fusion module, and the policy head are tuned. Adapter weights are initialized from previous adapters with minor random noise. A fusion module and policy head copy are maintained during the adaptation for both TAIL and FPF. The detailed hyperparameters are presented in Appendix B."
        },
        {
            "heading": "5.3 RESULTS AND ANALYSIS",
            "text": "Comparison of TAIL Integration Styles. Fig. 4 showcases the continual adaptation success rates for different TAIL methods. The efficacy of LoRA suggests that a well-pretrained model has a surprisingly low intrinsic dimension for imitation learning tasks (Aghajanyan et al., 2020). This implies the existence of a low-rank reparameterization that is just as adept for fine-tuning as the full parameter space. Further, the prefix tuning method outperforms the bottleneck-based approach (Houlsby et al., 2019), indicating that the sequential integration style may not be the optimal choice for continual learning, potentially due to its inherent \"filtering\" mechanism. Surprisingly, RoboAdapter (Sharma et al., 2023) generally performs the worst, potentially due to only introducing weights after the feedforward layer as opposed to after (Houlsby et al., 2019) or within (Li & Liang, 2021; Hu et al., 2021) the attention layer. Due to LoRA\u2019s pronounced effectiveness, it is predominantly employed as our TAIL integration method in subsequent experiments.\nTAIL vs. Conventional Fine-tuning. Across all evaluations, TAIL vastly outperforms all baselines in both forward and backward transfer, demonstrating that conventional fine-tuning methods are weak in data-scarce continual learning. In Fig. 5 we plot continual learning success rates over 6 task suites, where TAIL outperforms the best baselines by over 3x in some comparisons and generally achieves the best success rates. We display additional results on LIBERO-10, long-horizon tasks, in Table 1. Here, TAIL again performs best, with perfect backward transfer and forward transfer capabilities significantly better than the baselines: FFT not only exhibits marked catastrophic forgetting of earlier tasks\u2014evidenced by poor BWT\u2014but also compromises the model\u2019s adaptability to new tasks. This decline in forward transfer is characterized by a steady descent in success rates as training progresses, displayed in Appendix Table 7. Such deterioration in flexibility has been recognized in other studies as well (Lyle et al., 2022; Kumar et al., 2023). PackNet is able to adapt well on some task suites as it learns new parameters within different parts of the model, but overall is still outperformed by TAIL.\nAdaptation Plasticity. Exhaustive fine-tuning on specialized domains has been found to distort pretrained features (Kumar et al., 2022), undermining model adaptability. Our circle-back experiments\nin Table 2, where a full fine-tuned model is re-trained on prior task suites, demonstrate a steep performance drop upon re-visiting previously learned tasks. Additional experiments in Appendix C.3 further highlight this issue.\nTable 2: The success rate of initial training and revisiting previous tasks with FFT. FFT suffers from catastrophic forgetting and performs worse on re-visits despite re-training on the same data.\nType LIBERO Task Suite Spatial Goal Object Initial 0.79 0.42 0.42 Re-visit 0.53 ( \u2193 0.26) 0.20 (\u21930.22) 0.27 (\u21930.15)\nThe training and validation losses, detailed in Appendix C.1 and Fig. 7, highlight FFT\u2019s propensity to overfit. This translates to a notable decline in success rates, reinforcing the challenges FFT faces in balancing retention of prior tasks with the assimilation of new ones.\nWhile ER and the regularization-based method EWC exhibit some potential in mitigating catastrophic forgetting, they were detrimental to forward transfer performance. Their downsides are also reflected in storage and computing costs: ER requires more storage for previous data than TAIL LoRA adapter weights (e.g., Kitchen dataset at 28GB vs 7.8MB for TAIL\u2019s LoRA adapter). Furthermore, EWC presents significant challenges for larger models because of the increased GPU memory consumption from maintaining a copy of the entire weights of the old model in memory. We also found it to exhibit unstable training due to the regularization loss. More discussions are presented in Appendix B.1.\nWhen does TAIL work best? The efficacy of TAIL hinges significantly on the base model\u2019s features. We compare TAIL under different pretraining strategies and models in Appendix Sec. C.2 and C.3. In short, TAIL works best with our pretraining architecture and frozen CLIP visual/language encoders, and performance drops when we fine-tune the pretrained encoders, likely as FFT contaminates the rich CLIP features when fine-tuned in a niche domain with sparse data.\nAnalysis Summary. We argue in favor of a large pretrained base model augmented with numerous lightweight plugins tailored for different downstream tasks. This framework, TAIL, holds considerable promise for advancing embodied intelligence in real-world applications; the storage footprint of our entire model is about 660MB, and duplicating this model for each task in a stream of oncoming tasks is impractical. Meanwhile, the space occupied by one such model can accommodate as many as 84 task-specific adapters, which, as our experiments show, can outperform full fine-tuning regardless. Moreover, the features of the pretrained weights remain intact, ensuring their applicability across a broad array of domains. In summary, TAIL offers a promising avenue for the efficient adaptation of large decision-making models. Despite the fact that our method requires significantly less computation and memory (and storage), our experiments show that it consistently outperforms all prior approaches in the continual learning setting. We would also like to highlight that the TAIL framework is not restricted to imitation learning, but also other learning methods such as reinforcement learning."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "In this study, we examined the challenges of efficiently adapting large pretrained models for decisionmaking and robotics applications. We proposed TAIL, an efficient adaptation framework for pretrained decision-making models. Through a comprehensive exploration of parameter-efficient fine-tuning (PEFT) techniques in TAIL, especially Low-Rank Adaptation (LoRA), we demonstrated their potential in enhancing adaptation efficiency, mitigating catastrophic forgetting, and ensuring robust performance across diverse tasks. Our empirical evaluations on the LIBERO benchmark further underscored the advantages of these techniques in continual learning scenarios. As the demand for adaptive, intelligent agents grows across various domains, the insights from this research offer a promising direction for the future of efficient model adaptation in decision-making contexts."
        },
        {
            "heading": "A MODEL ARCHITECTURE DETAILS",
            "text": ""
        },
        {
            "heading": "A.1 PRETRAINED INPUT ENCODERS",
            "text": "We utilize pretrained CLIP image and textual encoders (Radford et al., 2021) to encode image observations and language goal descriptions, respectively. Note that we do not use a pre-trained encoder for the low-dimensional state; the state encoder is learned from scratch.\nA.2 INPUT MODALITY FUSION\nWe utilize Feature-wise Linear Modulation (FiLM) layers (Perez et al., 2017) (Fig. 1(a), input fusion module) to fuse language task specifications with image observations. FiLM is a technique in multi-modal deep learning which modulates the intermediate activations of a neural network based on external information. Rather than explicitly designing architectures for conditional computation, FiLM layers simply use the features from one network to modulate the features of another.\nLet\u2019s consider a neural network f with intermediate activations x and an external network g which outputs modulation parameters \u03b3 and \u03b2. The modulated features x\u2032 are given by:\n\u03b3, \u03b2 = g(z) (4)\nx\u2032 = \u03b3 \u2299 x+ \u03b2, (5)\nwhere z is the input to the external network g; \u2299 represents element-wise multiplication; \u03b3 and \u03b2 are vectors having the same size as x, with each element modulating a corresponding feature in x.\nThus, FiLM layers allow for a dynamic and feature-wise conditional computation without needing explicit architectural changes. As such, task token (language) embeddings are given as input to a fully connected feedforward network, which outputs scale and translation parameters for the image and state embeddings. These parameters modulate the image and state embeddings before they are passed to the transformer backbone."
        },
        {
            "heading": "A.3 TEMPORAL TRANSFORMER BACKBONE",
            "text": "We utilize a standard GPT-2 (Radford et al., 2019a) transformer backbone for our policy. Its input is a sequence of image and low-dim state encodings (robot joint states in LIBERO) and it outputs an action distribution. Following the literature (Mandlekar et al., 2021; Liu et al., 2023a), we adopt a stochastic policy parametrization based on a Gaussian-Mixture-Model (GMM) (Bishop, 1994). Therefore, for every decision-making step, the transformer produces a latent vector of Gaussian means and variances, one for each of the GMM modes. We optimize the parameters of the model with the negative log-likelihood loss on the ground truth actions based on the parameters of the GMM distribution. At evaluation time, we deterministically select the next action parameterized by the mean of the Gaussian model with the highest density.\nThe environment configuration and the temporal decoder (GPT-2) hyperparameters are presented in Table 4."
        },
        {
            "heading": "Environment Configuration GPT2 Temporal Encoder Configuration",
            "text": "B IMPLEMENTATION AND TRAINING DETAILS"
        },
        {
            "heading": "B.1 BASELINE DETAILS",
            "text": "Experience Replay (ER). ER (Chaudhry et al., 2019; Rolnick et al., 2019) is a rehearsal-based approach that retains a buffer of samples from previous tasks to facilitate the learning of new tasks. After completing the learning process for a task, a subset of the data is saved into this buffer. During the training of subsequent tasks, ER draws samples from this buffer and mixes them with current task data. This process ensures that the training data closely resembles the distribution of data across all tasks. In our setup, we store all the previous trajectories in a replay buffer. For each training iteration on a new task, we uniformly sample 50% trajectories from this buffer and 50% from the new task\u2019s training data, respectively.\nElastic Weight Consolidation (EWC). EWC (Kirkpatrick et al., 2017) is a regularization method that adds a term to the standard single-task learning objective to constrain the updates of the neural network. This constraint uses the Fisher information matrix to gauge the significance of each network parameter. The loss function for task k is represented as:\nLEWCk(\u03b8) = LBCK (\u03b8) + \u2211 i \u03bb 2 Fi(\u03b8i \u2212 \u03b8\u2217k\u22121,i)2\nHere, \u03bb is a hyperparameter penalty, and Fi is the diagonal of the Fisher information matrix given by:\nFk = Es\u223cDk,a\u223cp\u03b8(\u00b7|s) (\u2207\u03b8k log p\u03b8k(a|s)) 2\nFor our experiments, we adopt the online version of EWC. It updates the Fisher information matrix using an exponential moving average throughout the lifelong learning process. The actual Fisher Information Matrix estimate used is:\nF\u0303k = \u03b3Fk\u22121 + (1\u2212 \u03b3)Fk with Fk = E(s,a)\u223cDk (\u2207\u03b8k log p\u03b8k(a|s))\n2 and k representing the task number. Following the benchmark implementation (Liu et al., 2023a), the hyperparameters are set as \u03b3 = 0.9 and \u03bb = 5\u00d7 104.\nDiscussions. Both Experience Replay (ER) and Elastic Weight Consolidation (EWC) demonstrate potential in mitigating catastrophic forgetting. However, they each come with notable limitations, particularly with respect to forward transfer performance, storage, and computational efficiency.\nStorage Overhead: ER demands significant storage space to maintain samples from prior tasks. This becomes particularly evident when comparing the storage needs of ER for larger datasets, such as the Kitchen dataset which requires 28GB, with the lightweight LoRA adapter occupies only 7.8MB. The vast difference in storage demands underscores the inefficiency of the ER approach.\nComputational Challenges: EWC, by design, necessitates the maintenance of a copy of the weights of the previous model in GPU memory. This leads to escalated GPU memory consumption, making EWC tends to reduce the training batch size, subsequently slowing down the training process.\nTraining Instability: The regularization approach of EWC can introduce instability during training, owing to the regularization loss. This is also reflected by the poor forward transfer capability, as shown in Table 1.\nScalability Concerns: While EWC might be manageable for smaller networks, it is ill-suited for the fine-tuning of larger decision models due to its computational and storage challenges.\nGiven these outlined limitations, we advocate TAIL for alternative approaches that are both storageefficient and computationally scalable, especially for large pretrained model adaptation."
        },
        {
            "heading": "B.2 TAIL ADAPTER CONFIGURATIONS",
            "text": "To establish our TAIL adapter configurations, we primarily draw from the AdapterHub implementation, setup and hyperparameters (Pfeiffer et al., 2020b).\nWe utilize the default hyperparameters for LoRA, with the rank r = 8 and scaling factor \u03b1 = 8. These low-rank matrices are applied in parallel to the Transformer\u2019s query and value matrices (Hu et al., 2021). We also adopt the default for prefix token length of 30 for the prefix tuning (Li & Liang, 2021) method across all tasks. To improve the training stability, Low-rank matrices (r = 16) are employed during training to represent the prefix tokens. The Bottleneck Adapter (Houlsby et al., 2019) employs the bottleneck size of 32, and is applied to both the output layer of the attention and the intermediate feedforward layers. The RoboAdapter method (Sharma et al., 2023), as the closest work to us, also applies the sequential adapters to the decision-making domain. It differs from the Bottleneck Adapter in that they adopt a special insertion of weights to specific layers of the Transformer, namely, layers 0, 1, 5, 6, 10, 11. They selectively skip certain layers, aiming to increase the bottleneck size on the remaining layers. Therefore, the bottleneck size is doubled to 64 for this approach, such that all methods share similar amount of parameters.\nIn order to maintain balanced adapter parameters number between the two CLIP-based (spatial and instruction) encoders, and the temporal transformer GPT2 decoder, the rank size for the GPT2 decoder is doubled across all methodologies. This adjustment compensates for the GPT2 decoder\u2019s fewer layers relative to the encoders.\nFor the continual learning setup, we use the previous stage\u2019s adapter weight (if any) plus a small random Gaussian noise with standard deviation 0.001 as an initialization of the current stage. The goal for adding a minor random noise aims to improve the adapter weight capacity (Kumar et al., 2022; Agarwal et al., 2022; Lyle et al., 2022), preventing the weights from being trapped into local optimum. There is a potential to better utilize the trained adapter weights on preceding tasks. We outline several promising exploration directions in Appendix Section B.4."
        },
        {
            "heading": "B.3 TRAINING HYPERPARAMETERS AND EXPERIMENT CONFIGURATIONS",
            "text": "Following similar setup as in the LIBERO benchmark (Liu et al., 2023a), we perform data augmentation for the image observation data for all methods. We adopt the color, affine, and random erase augmentations to improve the robustness. The hyperparameters are presented in Table 5.\nFor our training process, we employed the AdamW optimizer combined with a linear learning rate scheduler. The majority of our task suites\u2014Kitchen, Spatial, Goal, Object, Living Room, and Study Room\u2014underwent training for 100 epochs. Notably, each suite encompasses multiple tasks, with Kitchen having 40 and the others containing 8 each. In contrast, the 10 long-horizon adaptation tasks,\ntermed LIBERO-10, were trained for 50 epochs, with each task trained sequentially. We performed evaluations after every 5 training epochs over 8 episodes (unseen in training) for each task.\nComputing machine. Our experimental platform was powered by an AMD EPYC 7R32 CPU running Ubuntu 20.04.06. All trainings utilized 8 NVIDIA A10G GPUs, each with a memory of 22731 MiB, equipped with driver version 470.199.02 and CUDA version 11.4. We employ Distributed Data Parallel (DDP) for parallel training across 8 GPUs, and utilize the 16-bit floating point precision (FP16) training mode to accelerate the training process. To ensure reproducibility, we adopted 3 distinct random seeds: 0, 21, and 42.\nTraining time. For a holistic perspective on training duration: FFT and ER methods demanded between 120 \u223c 140 hours per experiment (1.5 \u223c 1.75 hours per task) for the 6 task suites shown in Fig. 5, including the evaluation time. In stark contrast, TAIL-based techniques slashed this to 60 \u223c 66 hours (0.75 \u223c 0.825 hours per task). Hence, TAIL would also be much cheaper to train, considering its significantly shorter training time under identical computing machines.\nBatch sizes varied by training method. EWC employed a batch size of 10, given its added memory demands to store a distinct full parameter set. FFT and ER utilized batch sizes of 14. Owing to TAIL\u2019s more efficient memory utilization\u2014detailed in Table 3\u2014a larger batch size of 18 was feasible, which can maximize GPU resource usage on our machine, reducing training duration and cost."
        },
        {
            "heading": "B.4 MORE DISCUSSION AND FUTURE DIRECTIONS",
            "text": "The TAIL framework paves the way for a myriad of research opportunities:\n1. Better Weight Allocation Method Across Layers: An interesting question within this framework is discerning which layers, early or later, derive the most benefit from weight modifications. This can offer insights into the adaptability of neural architectures (Lee et al., 2023).\n2. Enhanced Reusability of Trained Adapters: Exploring methods to efficiently reuse adapters from prior tasks, especially in scenarios with limited data, is a promising direction. AdapterFusion techniques (Pfeiffer et al., 2020a) can be potentially useful, enabling the composition of knowledge from multiple pre-existing adapters.\n3. Building on Knowledge with Parallel Integration: The parallel integration method, particularly with LoRA weights, offers the capability to merge trained weights back into the main model. This iterative buildup of knowledge makes the approach valuable for continual learning, allowing new adapters to capitalize on the expertise of their predecessors.\n4. Combining with Established Continual Learning Strategies: The potential to merge the TAIL framework with existing continual learning methods, like Experience Replay and EWC, can be a beneficial avenue. Such integrations can accommodate the strengths of each method, crafting models that are both efficient in memory and robust against forgetting.\n5. Extension beyond the Imitation Learning Domain: Taking the TAILframework into other decision-making domains, such as reinforcement learning (RL), is also promising. TAIL has the potential to address the model capacity loss issue in RL (Agarwal et al., 2022; Lyle et al., 2022). Leveraging the TAIL framework can also aid in multitask learning, meta-learning, and efficiently adapting offline-trained RL models to new tasks without the necessity of vast amounts of data or extensive fine-tuning, thereby potentially accelerating convergence to optimal policies.\nThe avenues above elucidate the adaptability and potential of the TAIL framework, setting the stage for future research in this domain."
        },
        {
            "heading": "C MORE EXPERIMENT RESULTS",
            "text": ""
        },
        {
            "heading": "C.1 OVERFITTING",
            "text": "For each task, we used 40 demonstrations for training and 10 for validation. We are interested in the following question: In scenarios where data is limited, how resilient is TAIL against overfitting compared to traditional fine-tuning methods? To answer this, we present the training and validation loss cross the Kitchen, Spatial, Goal, Object, Living Room and Study Room task suites, each with 100 epochs, in Fig. 7.\nA noteworthy observation from Fig. 7 is the behavior of FFT. Despite achieving the lowest training loss across all stages, its validation loss spikes significantly after just a few epochs. This pattern suggests severe overfitting when FFT is applied to the entire parameter space using limited data. Intriguingly, this overfitting intensifies in the later adaptation phases, potentially signifying a distortion of pretrained features as alluded to by Kumar et al. (2022). Such distortion could be a contributor to the suboptimal success rate observed in Fig. 5, and the loss of learning capacity when revisiting a previous task, as presented in Table 2.\nIn constrast, TAIL-based methods shows strong resilience against overfitting. Drawing from the Occam\u2019s razor principle, TAIL leverages fewer trainable parameters, inherently reducing its potential to overfit with scarce data. Additional, different integration styles provide the flexibility to effectively utilize the features from pretrained models while preserving them across all the adaptation stages.\nThis observation underscores the disparities between our decision-making problem, characterized by its limited data, and the traditional language or vision domains, which have data in abundance. Prior studies utilizing parameterefficient fine-tuning techniques for language or vision tasks often reported superior performance with full finetuning due to its low training loss (He et al., 2022; Mao\net al., 2022; Chen et al., 2022; Sharma et al., 2023). However, as our results demonstrate, a lower training loss does not invariably translate to superior performance, especially in the context of a data-scarce sequential decision-making tasks."
        },
        {
            "heading": "C.2 ANALYSIS OF PRETRAINED WEIGHTS\u2019 INFLUENCE",
            "text": "We aim to answer the following question: how does the underlying pretrained base model influence the performance of TAIL, and are certain pretrained weights more conducive to this kind of adaptation? We initiated our investigation by analyzing the success rates of 40 Kitchen tasks using different pretrained weights for the spatial encoder. Apart from the CLIP-ViT pretrained encodings as we adopted in our main results, two other initialization of weights were considered: one sourced from the Visual Cortex 1 (VC-1) (Majumdar et al., 2023b), recognized for being a leading pretrained\nmodel for embodied agent tasks, and another using randomly initialized weights. The language instruction encoder consistently utilized the CLIP text model. From the results in Fig. 8, the VC-1 pretrained weights delivered performance on par with the CLIP-ViT encodings. Both considerably outperformed the randomly initialized weights, suggesting that large-scale pretraining can indeed enhance downstream fine-tuning. We then study how does the pretrained base model influence the performance of TAIL."
        },
        {
            "heading": "C.3 FURTHER EVALUATIONS ON TAIL WITH DIFFERENT BASE MODELS",
            "text": "To understand the influence of the base model\u2019s features on the performance of TAIL, we conducted additional evaluations. In Table 6, the methods column showcases different configurations:\n\u2022 LoRA (CLIP): The main setup we adopted in the experiment section 5, which keeps the pretrained CLIP encodings frozen across all the adaptation stages.\n\u2022 LoRA (CLIP with FFT): Starting with the CLIP model, we applied FFT pretraining on the Kitchen task before using LoRA for subsequent adaptations. This helps us test out whether adaptation plasticity suffers after full fine-tuning as the only difference between this and the above method is the addition of full fine-tuning before using LoRA.\n\u2022 LoRA (VC-1 with FFT): The VC-1 model, after FFT pretraining on the Kitchen task, was adapted using LoRA.\n\u2022 LoRA (Random with FFT): A model with randomly initialized weights underwent FFT pretraining on the Kitchen task, followed by adaptation with LoRA.\nAll the pretrained encodings implemented in the same model architecture as described in Appendix Section A.\nObservations from Table 6 highlight several findings:\n\u2022 Dominance of Original CLIP: The pure CLIP base model, when combined with LoRA, yielded the highest success rates across all task suites, suggesting the inherent quality and robustness of the original CLIP features for these tasks.\n\u2022 FFT\u2019s Mixed Impact: While FFT pretraining aids in task-specific fine-tuning, when combined with CLIP, it leads to a degradation in performance. This could be attributed to FFT potentially diluting the comprehensive and rich features within CLIP while also reducing adaptation plasticity (Kumar et al., 2022), especially when exposed to a more constrained domain with limited data.\n\u2022 VC-1\u2019s Comparable Performance: The VC-1 model, though renowned in the domain of embodied agent tasks, delivered results that were only marginally better than the randomly initialized weights when both were subjected to FFT pretraining and then adapted with LoRA. This emphasizes the unique advantages of the original CLIP features.\nInterestingly, it is observed that CLIP is pretrained on the most comprehensive dataset, followed by VC-1. In contrast, the model with random weights only underwent pretraining on the 40 Kitchen tasks. The success rates mirror this order, underscoring the idea that the efficacy of TAIL is closely tied to a base model pretrained with rich features on extensive datasets. So in summary, the choice of base model significantly affects the performance of TAIL, with CLIP\u2019s original features showing remarkable compatibility and resilience across various task suites"
        },
        {
            "heading": "C.4 RANK SIZE ABLATION STUDY",
            "text": "In order to understand the impact of rank-size on adaptation performance, we conducted experiments using varying rank sizes for the LoRA and Bottleneck Adapter methods. The results, illustrated in Fig. 9, present the average success rates across the Spatial, Goal, and Object task suites. It is evident that increasing the rank size generally enhances performance up to a certain point. Beyond this optimal threshold, further increasing the rank size does not necessarily lead to higher success rates, potentially because of overfitting. Notably, in our continual learning context, the parallel insertion approach of LoRA consistently surpasses the sequential style of the Bottleneck Adapter method.\nAdditionally, we would like to note that our TAIL framework exhibits data adaptivity, suggesting that the rank size could be adjusted based on the quantity of adaptation data. In scenarios with smaller datasets, a smaller rank size could be more effective, and vice versa."
        },
        {
            "heading": "C.5 COMPARISON BETWEEN TRAINING FROM SCRATCH AND USING PRETRAINED MODELS",
            "text": "Fig. 10 compares the success rates across task suites for TAIL-LoRA, sequential FFT with pre-trained CLIP weights, and FFT-In-Domain. Unlike FFT-CLIP, FFT-In-Domain is trained from scratch with task-suite demonstration data only, i.e., we need to maintain a copy of the entire model for each task suite. There are three observations:\n1. Pretrained Weights Advantage: In the initial Kitchen and Spatial task suites, FFT with CLIP pretrained weights demonstrates a higher success rate compared to FFT trained from scratch. This indicates the effectiveness of leveraging pretrained models, particularly in the context of the Kitchen suite where the benefit is more pronounced.\n2. Decline in Model Adaptability: Despite the initial advantage, sequential FFT with CLIP shows a marked decline in performance in the remaining four task suites - Goal, Object, Living, and Study. This trend may be indicative of a loss in model plasticity, where the pre-trained model performs well in the early stages but struggles to adapt to new tasks after the pre-trained weights are contaminated.\n3. TAIL-LoRA\u2019s Consistent Performance: Throughout all the task suites, TAIL-LoRA with pretrained CLIP consistently outperforms the other methods. This suggests that the LoRA approach, combined with the advantages of pretrained CLIP weights, provides a robust and adaptable framework capable of handling a variety of tasks with greater efficiency."
        },
        {
            "heading": "C.6 ABLATION STUDY FOR DIFFERENT INTEGRATION STYLE COMBINATIONS",
            "text": "It\u2019s noteworthy that our method allows for the simultaneous use of multiple integration techniques (Mao et al., 2022). This flexibility lets us explore the performance impact of combining LoRA (parallel integration), bottleneck adapter (sequential integration), and prefix token (concatenation). To this end, we conduct an ablation study for each of the combinations over the Spatial, Goal, and Object task suites. The experiment result is shown in Fig. 11, where the y-axis is the averaged success rate.\nA key finding is the critical role of LoRA (parallel integration) in enhancing adaptation performance. Combinations involving LoRA consistently outperform those without it. For instance, the standalone use of LoRA yields a comparable success rate w.r.t the combination with others. This pattern underscores LoRA\u2019s effectiveness, either used alone or in conjunction with other methods. In contrast, the combination of Prefix and Adapter without LoRA results in a notably lower success rate (0.6641), highlighting LoRA\u2019s indispensability.\nThe integration of all three methods\u2014Prefix, Adapter, and LoRA\u2014achieves a success rate that is comparable to LoRA\u2019s standalone performance. This outcome suggests that while the combination of different integration methods does not detract from performance, LoRA remains the\nprimary driver of successful adaptation. These findings emphasize the importance of LoRA in adapter weight integration strategies and provide valuable guidance for future approaches in this domain."
        },
        {
            "heading": "C.7 DETAILED PER-TASK RESULTS IN THE LIBERO-LONG TASK SUITE",
            "text": ""
        },
        {
            "heading": "D EVALUATION TASK DETAILS",
            "text": "We list all the language instructions describing the tasks we adopted in our experiments below. Note that while certain tasks may share similar descriptions, they are not the same due to variations in the environment configurations (e.g., different spatial layouts, objects, or goal positions)."
        }
    ],
    "title": "TAIL: TASK-SPECIFIC ADAPTERS FOR IMITATION LEARNING WITH LARGE PRETRAINED MODELS",
    "year": 2024
}