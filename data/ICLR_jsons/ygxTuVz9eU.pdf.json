{
    "abstractText": "The role of data in building AI systems has recently been emphasized by the emerging concept of data-centric AI. Unfortunately, in the real-world, datasets may contain dirty samples, such as poisoned samples from backdoor attack, noisy labels in crowdsourcing, and even hybrids of them. The presence of such dirty samples makes the DNNs vunerable and unreliable. Hence, it is critical to detect dirty samples to improve the quality and realiability of dataset. Existing detectors only focus on detecting poisoned samples or noisy labels, that are often prone to weak generalization when dealing with dirty samples from other fields. In this paper, we find a commonality of various dirty samples is visual-linguistic inconsistency between images and associated labels. To capture the semantic inconsistency between modalities, we propose versatile data cleanser (VDC) leveraging the surpassing capabilities of multimodal large language models (MLLM) in cross-modal alignment and reasoning. It consists of three consecutive modules: the visual question generation module to generate insightful questions about the image; the visual question answering module to acquire the semantics of the visual content by answering the questions with MLLM; followed by the visual answer evaluation module to evaluate the inconsistency. Extensive experiments demonstrate its superior performance and generalization to various categories and types of dirty samples.",
    "authors": [
        {
            "affiliations": [],
            "name": "VISUAL-LINGUISTIC INCONSIS"
        }
    ],
    "id": "SP:529bfd2f75dfe3f0da7928d1e1a840effb3ccf4c",
    "references": [
        {
            "authors": [
                "Dara Bahri",
                "Heinrich Jiang",
                "Maya Gupta"
            ],
            "title": "Deep k-nn for noisy labels",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Mauro Barni",
                "Kassem Kallas",
                "Benedetta Tondi"
            ],
            "title": "A new backdoor attack in cnns by training set corruption without label poisoning",
            "venue": "IEEE International Conference on Image Processing,",
            "year": 2019
        },
        {
            "authors": [
                "Antonin Berthon",
                "Bo Han",
                "Gang Niu",
                "Tongliang Liu",
                "Masashi Sugiyama"
            ],
            "title": "Confidence scores make instance-dependent label-noise learning possible",
            "venue": "In International conference on machine learning,",
            "year": 2021
        },
        {
            "authors": [
                "Yupeng Chang",
                "Xu Wang",
                "Jindong Wang",
                "Yuan Wu",
                "Kaijie Zhu",
                "Hao Chen",
                "Linyi Yang",
                "Xiaoyuan Yi",
                "Cunxiang Wang",
                "Yidong Wang"
            ],
            "title": "A survey on evaluation of large language models",
            "venue": "arXiv preprint arXiv:2307.03109,",
            "year": 2023
        },
        {
            "authors": [
                "Bryant Chen",
                "Wilka Carvalho",
                "Nathalie Baracaldo",
                "Heiko Ludwig",
                "Benjamin Edwards",
                "Taesung Lee",
                "Ian Molloy",
                "Biplav Srivastava"
            ],
            "title": "Detecting backdoor attacks on deep neural networks by activation clustering",
            "venue": "arXiv preprint arXiv:1811.03728,",
            "year": 2018
        },
        {
            "authors": [
                "Weixin Chen",
                "Baoyuan Wu",
                "Haoqian Wang"
            ],
            "title": "Effective backdoor defense by exploiting sensitivity of poisoned samples",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Xinyun Chen",
                "Chang Liu",
                "Bo Li",
                "Kimberly Lu",
                "Dawn Song"
            ],
            "title": "Targeted backdoor attacks on deep learning systems using data poisoning",
            "venue": "arXiv preprint arXiv:1712.05526,",
            "year": 2017
        },
        {
            "authors": [
                "Hao Cheng",
                "Zhaowei Zhu",
                "Xingyu Li",
                "Yifei Gong",
                "Xing Sun",
                "Yang Liu"
            ],
            "title": "Learning with instancedependent label noise: A sample sieve approach",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Wenliang Dai",
                "Junnan Li",
                "Dongxu Li",
                "Anthony Meng Huat Tiong",
                "Junqi Zhao",
                "Weisheng Wang",
                "Boyang Li",
                "Pascale Fung",
                "Steven Hoi"
            ],
            "title": "Instructblip: Towards general-purpose vision-language models with instruction tuning, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Yansong Gao",
                "Change Xu",
                "Derui Wang",
                "Shiping Chen",
                "Damith C Ranasinghe",
                "Surya Nepal"
            ],
            "title": "Strip: A defence against trojan attacks on deep neural networks",
            "venue": "In Proceedings of the 35th Annual Computer Security Applications Conference,",
            "year": 2019
        },
        {
            "authors": [
                "Aritra Ghosh",
                "Himanshu Kumar",
                "P Shanti Sastry"
            ],
            "title": "Robust loss functions under label noise for deep neural networks",
            "venue": "In Proceedings of the AAAI conference on artificial intelligence,",
            "year": 2017
        },
        {
            "authors": [
                "Tianyu Gu",
                "Kang Liu",
                "Brendan Dolan-Gavitt",
                "Siddharth Garg"
            ],
            "title": "Badnets: Evaluating backdooring attacks on deep neural networks",
            "venue": "IEEE Access,",
            "year": 2019
        },
        {
            "authors": [
                "Jonathan Hayase",
                "Weihao Kong",
                "Raghav Somani",
                "Sewoong Oh"
            ],
            "title": "Spectre: Defending against backdoor attacks using robust statistics",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Max Jaderberg",
                "Karen Simonyan",
                "Andrew Zisserman"
            ],
            "title": "Spatial transformer networks",
            "venue": "Advances in neural information processing systems,",
            "year": 2015
        },
        {
            "authors": [
                "Youngdong Kim",
                "Junho Yim",
                "Juseung Yun",
                "Junmo Kim"
            ],
            "title": "Nlnl: Negative learning for noisy labels",
            "venue": "In Proceedings of the IEEE/CVF international conference on computer vision,",
            "year": 2019
        },
        {
            "authors": [
                "Alex Krizhevsky",
                "Geoffrey Hinton"
            ],
            "title": "Learning multiple layers of features from tiny images",
            "year": 2009
        },
        {
            "authors": [
                "Bo Li",
                "Yuanhan Zhang",
                "Liangyu Chen",
                "Jinghao Wang",
                "Fanyi Pu",
                "Jingkang Yang",
                "Chunyuan Li",
                "Ziwei Liu"
            ],
            "title": "Mimic-it: Multi-modal in-context instruction tuning",
            "venue": "arXiv preprint arXiv:2306.05425,",
            "year": 2023
        },
        {
            "authors": [
                "Yuezun Li",
                "Yiming Li",
                "Baoyuan Wu",
                "Longkang Li",
                "Ran He",
                "Siwei Lyu"
            ],
            "title": "Invisible backdoor attack with sample-specific triggers",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Haotian Liu",
                "Chunyuan Li",
                "Qingyang Wu",
                "Yong Jae Lee"
            ],
            "title": "Visual instruction tuning",
            "venue": "arXiv preprint arXiv:2304.08485,",
            "year": 2023
        },
        {
            "authors": [
                "Yingqi Liu",
                "Shiqing Ma",
                "Yousra Aafer",
                "Wen-Chuan Lee",
                "Juan Zhai",
                "Weihang Wang",
                "Xiangyu Zhang"
            ],
            "title": "Trojaning attack on neural networks",
            "venue": "In 25th Annual Network and Distributed System Security Symposium,",
            "year": 2018
        },
        {
            "authors": [
                "Tuan Anh Nguyen",
                "Anh Tuan Tran"
            ],
            "title": "Wanet - imperceptible warping-based backdoor attack",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Curtis Northcutt",
                "Lu Jiang",
                "Isaac Chuang"
            ],
            "title": "Confident learning: Estimating uncertainty in dataset labels",
            "venue": "Journal of Artificial Intelligence Research,",
            "year": 2021
        },
        {
            "authors": [
                "Xiangyu Qi",
                "Tinghao Xie",
                "Jiachen T Wang",
                "Tong Wu",
                "Saeed Mahloujifar",
                "Prateek Mittal"
            ],
            "title": "Towards a proactive {ML} approach for detecting backdoor poison samples",
            "venue": "In 32nd USENIX Security Symposium (USENIX Security",
            "year": 2023
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "venue": "In International conference on machine learning,",
            "year": 2021
        },
        {
            "authors": [
                "Olaf Ronneberger",
                "Philipp Fischer",
                "Thomas Brox"
            ],
            "title": "U-net: Convolutional networks for biomedical image segmentation",
            "venue": "MICCAI 2015: 18th International Conference,",
            "year": 2015
        },
        {
            "authors": [
                "Olga Russakovsky",
                "Jia Deng",
                "Hao Su",
                "Jonathan Krause",
                "Sanjeev Satheesh",
                "Sean Ma",
                "Zhiheng Huang",
                "Andrej Karpathy",
                "Aditya Khosla",
                "Michael Bernstein"
            ],
            "title": "Imagenet large scale visual recognition challenge",
            "venue": "International journal of computer vision,",
            "year": 2015
        },
        {
            "authors": [
                "Di Tang",
                "XiaoFeng Wang",
                "Haixu Tang",
                "Kehuan Zhang"
            ],
            "title": "Demon in the variant: Statistical analysis of {DNNs} for robust backdoor contamination detection",
            "venue": "In 30th USENIX Security Symposium (USENIX Security",
            "year": 2021
        },
        {
            "authors": [
                "Brandon Tran",
                "Jerry Li",
                "Aleksander Madry"
            ],
            "title": "Spectral signatures in backdoor attacks",
            "venue": "Advances in neural information processing systems,",
            "year": 2018
        },
        {
            "authors": [
                "Steven Euijong Whang",
                "Yuji Roh",
                "Hwanjun Song",
                "Jae-Gil Lee"
            ],
            "title": "Data collection and quality challenges in deep learning: A data-centric ai perspective",
            "venue": "The VLDB Journal,",
            "year": 2023
        },
        {
            "authors": [
                "Baoyuan Wu",
                "Hongrui Chen",
                "Mingda Zhang",
                "Zihao Zhu",
                "Shaokui Wei",
                "Danni Yuan",
                "Chao Shen"
            ],
            "title": "Backdoorbench: A comprehensive benchmark of backdoor learning",
            "venue": "In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track,",
            "year": 2022
        },
        {
            "authors": [
                "Baoyuan Wu",
                "Li Liu",
                "Zihao Zhu",
                "Qingshan Liu",
                "Zhaofeng He",
                "Siwei Lyu"
            ],
            "title": "Adversarial machine learning: A systematic survey of backdoor attack, weight attack and adversarial example",
            "venue": "arXiv preprint arXiv:2302.09457,",
            "year": 2023
        },
        {
            "authors": [
                "Zhewei Yao",
                "Cheng Li",
                "Xiaoxia Wu",
                "Stephen Youn",
                "Yuxiong He"
            ],
            "title": "A comprehensive study on post-training quantization for large language models",
            "venue": "arXiv preprint arXiv:2303.08302,",
            "year": 2023
        },
        {
            "authors": [
                "Chenglin Yu",
                "Xinsong Ma",
                "Weiwei Liu"
            ],
            "title": "Delving into noisy label detection with clean data",
            "venue": "In Proceedings of the 40th International Conference on Machine Learning,",
            "year": 2023
        },
        {
            "authors": [
                "Yi Zeng",
                "Won Park",
                "Z Morley Mao",
                "Ruoxi Jia"
            ],
            "title": "Rethinking the backdoor attacks\u2019 triggers: A frequency perspective",
            "venue": "In Proceedings of the IEEE/CVF international conference on computer vision,",
            "year": 2021
        },
        {
            "authors": [
                "Daochen Zha",
                "Zaid Pervaiz Bhat",
                "Kwei-Herng Lai",
                "Fan Yang",
                "Xia Hu"
            ],
            "title": "Data-centric ai: Perspectives and challenges",
            "venue": "In Proceedings of the 2023 SIAM International Conference on Data Mining (SDM),",
            "year": 2023
        },
        {
            "authors": [
                "Zhaowei Zhu",
                "Zihao Dong",
                "Yang Liu"
            ],
            "title": "Detecting corrupted labels without training a model to predict",
            "venue": "In International conference on machine learning,",
            "year": 2022
        },
        {
            "authors": [
                "Blended Blended Chen"
            ],
            "title": "2017) firstly adopted the blended injection strategy",
            "year": 2017
        },
        {
            "authors": [],
            "title": "2018) starts by choosing a trigger mask, which is a subset of the input variables that are used to inject the trigger. Then it searches for value assignment of the input variables in the trigger mask so that the selected neuron(s) of the target model",
            "year": 2018
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "The emerging concept of data-centric AI (DCAI) highlights the pivotal role of data in constructing advanced AI systems (Zha et al., 2023). The quality and reliability of data are crucial factors influencing model performance. Nevertheless, in the real world, dataset can be susceptible to undesirable flaws (Whang et al., 2023).\nFor instance, dirty samples may be introduced into the datasets intentionally or unintentionally. In this paper, we comprehensively examine three categories of dirty samples as follows: Category I: Poisoned Samples. In the context of backdoor attack, malicious attackers intentionally manipulate partical clean samples by embedding triggers and changing the ground-truth labels to target labels, thereby generating poisoned samples. Deep neural networks (DNNs) trained on the dataset with such poisoned samples will be injected with backdoor, i.e., predict any poisoned sample as the target label during the inference stage, while maintain accuracy on the clean samples. Category II: Noisy Labels. In scenarios of crowdsourcing or web crawling, human annotators or automatic annotation robots may make mistakes accidentally, resulting in the presence of dirty samples with corrupted labels. Training DNNs using the dataset with such noisy labels will significantly degrade the overall performance. Category III: Hybrid Dirty Samples. An even more critical concern arises when the attackers poison datasets that initially contain noisy labels. In this case, the datasets comprise both poisoned samples and noisy labels. Models trained on such datasets will encounter both malicious backdoor attack and performance degradation simultaneously.\nThe presence of above dirty samples makes the DNNs vulnerable and unreliable. To enhance the robustness and performance of DNNs, the detection of dirty samples is crucial in the lifecycle of\nDCAI. Recent research have been proposed on the noisy label detection (Northcutt et al., 2021; Zhu et al., 2022; Yu et al., 2023) or poisoned sample detection (Hayase et al., 2021; Tang et al., 2021; Qi et al., 2023) respectively. However, they frequently exhibit limitations in terms of generalization: 1). Inconsistent generalization across different categories of dirty samples. We empirically find that detectors designed for detecting poisoned samples are ineffective when applied to datasets with noisy labels, and vice versa. Moreover, both types of detectors prove inadequate for hybrid dirty samples. (See Table 5 in Sec 5.2.3). 2). Inconsistent generalization across different types of dirty samples in the same category. For noisy label detection, research has shown that symmetric noisy labels are more readily detectable than asymmetric ones (Cheng et al., 2021). Likewise, for poisoned sample detection, sensitivity to various triggers has been demonstrated in Wu et al. (2022). Therefore, developing a universal framework capable of detecting multiple types of dirty samples concurrently, including noisy labels and poisoned samples, is an urgent challenge for DCAI.\nWe find a notable commonality of noisy labels and poisoned samples lies in visual-linguistic inconsistency between visual contents and associated labels, i.e., the semantics of visual modality and that of language modality of label do not match, even when the poisoned samples are embedded with triggers. Given the exceptional capabilities of multimodal large language models (MLLM) in cross-modal alignment and reasoning, we resort to MLLM to measure this semantic inconsistency between modalities. To this end, we propose a universal detection framework called Versatile Data Cleanser (VDC). It consists of three consecutive modules: the visual question generation (VQG) module to generate insightful visual questions about the image based on the associated label; the visual question answering (VQA) module to obtain the semantic information of the image by answering the generated questions with MLLM; followed by the visual answer evaluation (VAE) module to measure the inconsistency by evaluating the matching score between the semantics of the image and labels. Since VDC does not involve the training process with specific dirty samples, it is endowed with the universal capacity to detect various categories and types of dirty samples.\nWe summarize our main contributions: 1). We identify the commonality of various dirty samples is visual-linguistic inconsistency between visual contents and associated labels. 2). To quantify this inconsistency, we propose a versatile data cleanser that leverages the impressive capabilities of multimodal large language models. 3). Experiments show that VDC consistently exhibits superior performance for detecting poisoned samples, noisy labels, and hybrids of them."
        },
        {
            "heading": "2 RELATED WORKS",
            "text": "Poisoned Sample Detection. The rise of backdoor attacks in machine learning has posed a significant security threat, including embedding malicious triggers into clean training samples (Wu et al., 2023). Several recent studies have explored detecting and mitigating the presence of poisoned samples in datasets. Chen et al. (2018) proposes to use K-means to separate the clean and poison clusters in the latent space. Tran et al. (2018) and Hayase et al. (2021) utilize robust statistics to detects poisoned samples based on spectral signature. Gao et al. (2019) observes the randomness of predicted classes for perturbed inputs. Zeng et al. (2021) proposes to detect artifacts of poison samples in the frequency domain. Chen et al. (2022) focuses on sensitivity metrics for distinguishing poisoned samples from clean ones. Qi et al. (2023) proposes confusion training to decouple benign correlations while exposing backdoor patterns to detection. Most of these approaches require training on the poisoned dataset or external clean subset, which depends on the types of poisoned samples, while our proposed method is more robust and generalizable to various types of poisoned samples.\nNoisy Label Detection. Human-annotated labels are often prone to noise, and the presence of such noisy labels will degrade the performance of the DNNs. Several approaches have been proposed to detect noisy labels (Ghosh et al., 2017; Bahri et al., 2020; Berthon et al., 2021). Northcutt et al. (2021) proposes to exploit confident learning to estimate the uncertainty of dataset labels. CORES (Cheng et al., 2021) progressively sieves out corrupted examples via a proposed confidence regularizer. Zhu et al. (2022) proposes a data-centric solution based on neighborhood information to detect noisy labels. BHN (Yu et al., 2023) leverages clean data by framing the problem of noisy label detection with clean data as a multiple hypothesis testing problem.\nExisting poisoned sample detection and noisy label detection methods are limited to performing well in their respective domain. Instead, our paper proposes a universal detection framework capable of detecting various types of dirty samples simultaneously."
        },
        {
            "heading": "3 PRELIMINARIES: DIRTY SAMPLE DETECTION",
            "text": "In this section, we first define the setup of dirty sample detection task, including poisoned samples and noisy labels, and then clarify the goals of this paper.\nSetup. We consider a standard classification problem given the dataset D = {(xi, yi)}Ni=1 that contains N samples i.i.d sampled from X \u00d7 Y , where xi \u2208 X denotes the input feature, yi \u2208 Y = {1, . . . ,K} is the label of xi. The classification task aims to learn a classifier f\u03b8 : X \u2192 Y . In the real-world, however, when collecting a dataset, some samples may be corrupted due to human mistakes or malicious goals, thereby generating dirty samples with corrupted labels in the dataset. Therefore, in the real-world, D is the fusion of dirty dataset D\u0303 = {(x\u0303i, y\u0303i)}Mi=1 and clean dataset D\u0302 = {(x\u0302i, y\u0302i)}N\u2212Mi=1 , i.e., D\u2032 = D\u0303 \u222a D\u0302, where (x\u0303i, y\u0303i) is a dirty sample and M is the number of dirty samples, (x\u0302i, y\u0302i) is a clean sample. We formulate two types of dirty sample in the following:\n\u2022 Poisoned Sample: Poisoned sample denotes the one that its visual feature is maliciously manipulated by the attacker, i.e., x\u0303i := g(xi) \u0338= x\u0302i, where g(\u00b7) is the generation function, such as blending (Chen et al., 2017) and wrapping-based transformation (Nguyen & Tran, 2021). Meanwhile, the label is changed to the target label by the attacker, i.e., y\u0303i = yt \u0338= y\u0302i.\n\u2022 Noisy Label: Noisy label represents the sample that its label is annotated incorrectly, while its visual feature remains unchanged, i.e., x\u0303i = x\u0302i, y\u0303i \u2208 Y\u0303 \u0338= y\u0302i, where Y\u0303 represents noisy version of Y . Following Yu et al. (2023); Zhu et al. (2022), we focus on the closed-set label noise that Y and Y\u0303 are assumed to be in the same label space. This situation is common when human annotators are asked to select the most appropriate label from a preset label set.\nGoal. Unlike most existing works that can only detect noisy labels or poisoned samples, our goal is to design a universal detection framework that can be applied to various categories of dirty samples."
        },
        {
            "heading": "4 METHODOLOGY: VERSATILE DATA CLEANSER",
            "text": "We find that what poisoned samples and noisy labels have in common is that the visual features of the poisoned samples are inconsistent with their given labels. For example, an image containing \u2018cat\u2019 is wrongly labeled as a \u2018dog\u2019, which can be detected by comparing the semantics of the visual content of the image and that of the given label. For the poisoned sample, although the trigger is embedded into the image, its underlying semantics has not been modified. We refer this commonality as\n\u201cvisual-linguistic inconsistency\u201d. Thanks to the surpassing abilities of multimodal understanding and reasoning of MLLM, we propose Versatile Data Cleanser, called VDC, to capture the visuallinguistic inconsistency based on MLLM. To the best of our knowledge, VDC is the first universal framework that is capable of detecting both noisy labels and poisoned samples simultaneously. As shown in Figure 1, it consists of the following consecutive modules:\n\u2022 Visual Question Generation (VQG): VQG module first generates insightful visual questions related to the given labels based on the template and LLM, which is detailed in Sec 4.1.\n\u2022 Visual Question Answering (VQA): Then VQA module resorts to MLLM to answer the generated visual questions about the image to acquire the semantics of the visual content, which is detailed in Sec 4.2.\n\u2022 Visual Answer Evaluation (VAE): The VAE module assesses visual-linguistic inconsistency by evaluating the matching score between the semantics of the image and label, detailed in Sec 4.3."
        },
        {
            "heading": "4.1 VISUAL QUESTION GENERATION",
            "text": "We propose to obtain semantic information of the visual content by asking MLLM visual questions. Therefore, the first step is how to design insightful questions based on the given label yi, which is formulated as follows:\n\u03a6i = {(Qji , A j i )} Nq j=1 := Fvqg(yi) (1)\nwhere yi might be corrupted label y\u0303i or ground-truth label y\u0302i, Q j i denotes the j-th question and A j i denotes expected answer, and Nq denotes the number of questions. In order to comprehensively and fully understand the semantics of images, two different types of questions are considered in VDC, including coarse-grained general questions and fine-grained label-specific questions.\nGeneral Questions. General questions can serve as a means to acquire holistic semantic understanding of an image from a global perspective, such as \u201cPlease describe the image briefly.\u201d. The expected answers to these general questions align with the given label. Since the general questions remain consistent across various labels, they are generated by random selection from a set of predefined templates, as outlined in Table 10 in Appendix E.\nLabel-specific Questions. Besides, the label-specific questions related to the given labels aim to extract more localized semantics from the image, encompassing aspects of common sense features, attributions, functions, geography, history, culture, and etc . For example, given the label \u201cairplane\u201d, an apt question is \u201cIs the object in the image designed for flying in the air?\u201d. Designing most labelspecific questions necessitates a level of expertise about the label that may exceed the capacity of a human annotator. When dealing with a multitude of labels, such as ImageNet with 1,000 classes, manually designing for each label becomes impractical. Hence, we utilize LLM like ChatGPT (OpenAI) to automatically generate these questions, depending on its expansive open-world knowledge. The well-designed prompts and generated questions are detailed in Appendix D and E."
        },
        {
            "heading": "4.2 VISUAL QUESTION ANSWERING",
            "text": "The next step involves responding to the generated questions in Sec 4.1 based on the input image xi to acquire the semantics of the visual content. This process is often referred to as the visual question answering (VQA) task, which can be formulated as follows:\nRji := Fvqa(Q j i ,xi) (2)\nwhere Rji indicates the response of VQA model for the question Q j i . Answering these questions necessitates the capabilities of natural language generation and external knowledge beyond the visible content of image. Therefore, we resort to MLLM as our VQA model owing to its remarkable capabilities of visual and language understanding and reasoning, which has been demonstrated in a wide range of visual-language tasks."
        },
        {
            "heading": "4.3 VISUAL ANSWER EVALUATION",
            "text": "Afterward, for a suspicious input sample (xi, yi), we obtain a set of questions, expected answers, and responses, i.e., {Qji , A j i , R j i} Nq j=1. The subsequent step is to assess visual-linguistic consistency\nby evaluating the matching score between the semantics of the image and label. We first judge the correctness of the response of MLLM, i.e., whether it aligns with the expected answer, which can be formulated as follows:\neji := Fvae(A j i , R j i ) (3)\nwhere ei denotes the correctness, i.e., true or false. For label-specific questions with deterministic expected answers, we use string matching to evaluate the response. If the word \u201cyes\u201d is present in the response, the result should be true, otherwise if the response contains the word \u201cno\u201d, the result should be false. Nevertheless, for general questions, string matching is insufficient to determine correctness. In such cases, we employ ChatGPT as a specialized evaluator through meticulously designed prompts, which is a commonly adopted approach in the evaluation of LLM Chang et al. (2023).\nVote-based Ensemble. Then the matching score si of sample (xi, yi) is computed as the proportion of questions answered correctly, which are formulated as follows:\nsi =\n\u2211Nq j=1 1(ei = true)\nNq (4)\nwhere 1(\u00b7) denotes identity function. If the score is less than the threshold \u03b1, sample (xi, yi) is detected as a dirty sample and then removed from the dataset."
        },
        {
            "heading": "5 EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "5.1 EXPERIMENTAL SETTINGS",
            "text": "Datasets. We evaluate ASRon three benchmark datasets, CIFAR-10 (Krizhevsky et al., 2009) and two ImageNet (Russakovsky et al., 2015) subsets: (1) For ImageNet-100, we randomly choose 100 classes from ImageNet, in which 500 images per class for training and 100 images per class for testing. (2) For ImageNet-Dog, to evaluate the effect of similarity of classes, we randomly choose 10 classes of dogs from ImageNet, in which 800 images per class for training and 200 images per class for testing.\nDirty Samples Generation. Denote the ratio of dirty samples in the whole dataset by \u03b7. Two types of dirty samples are considered in the evaluation, which are illstrated as follows:\n\u2022 Poisoned Samples. We consider six representative backdoor attacks to generate poisoned samples: (1) Visible triggers: BadNets (Gu et al., 2019), Blended (Chen et al., 2017), TrojanNN (Liu et al., 2018). (2) Invisible triggers: SIG (Barni et al., 2019), SSBA Li et al. (2021), WaNet Nguyen & Tran (2021). For all attacks, we randomly choose the same number of images from all classes except target class to add trigger, and then change the labels as target label. The example and settings of each attack are detailed in Appendix C.2.\n\u2022 Noisy Labels. We experiment with two popular synthetic noisy model models: the symmetric and asymmetric noise: (1) Symmetric noisy label is generated by uniform flipping, i.e., randomly flipping a ground-truth label to all other possible classes (Kim et al., 2019). (2) Asymmetric noisy label is generated by flipping the ground-truth label to the next class, i.e., (i mod K) + 1, where K denotes the number of classes.\nEvaluation Metrics. We report the detection results with two key metrics: true positive rate (TPR) and false positive rate (FPR) following Qi et al. (2023). TPR means the recall of detected dirty samples, representing the capacity to successfully detect dirty samples within the dataset. FPR denotes the ratio of clean samples erroneously identified as dirty samples, highlighting the susceptibility to produce false alarms. An ideal detection method should exhibit a higher TPR and lower FPR. Let vi = 1 indicate that the i-th sample is detected as dirty sample. Then the TPR and FPR can be calculated as follows:\nTPR = \u2211N\ni=1 1(vi = 1, yi \u0338= y\u0302i)\u2211N i=1 1(yi \u0338= y\u0302i)\n, FPR = \u2211N\ni=1 1(vi = 1, yi = y\u0302i)\u2211N i=1 1(yi = y\u0302i)\n(5)\nMoreover, when retraining on the purified dataset, we report the attack success rate (ASR) and the clean accuracy (ACC) of the retrained model.\nImplemented Details. We adopt ChatGPT based on GPT-3.5-turbo (OpenAI) as LLM and InstructBLIP (Dai et al., 2023) as MLLM in VDC. For all datasets, we generate two general questions. The number of label-specific questions is six for ImageNet-100 and four for CIFAR-10 and ImageNetDog. The threshold \u03b1 is set as 0.5 across all experiments. The noisy ratio \u03b7 for noisy labels is set as 0.4. We poison 50 and 500 samples per class for CIFAR-10, 5 and 50 per class for ImageNet-100, and 80 per class for ImageNet-Dog. We retrain on the purified dataset with ResNet-18 (He et al., 2016). Additional details can be found in Appendix C.1.\nCompared Baselines. For poisoned sample detection, we compare with 7 baselines, in which STRIP (Gao et al., 2019), SS (Tran et al., 2018), SCAn (Tang et al., 2021), Frequency (Zeng et al., 2021) and CT (Qi et al., 2023) require external clean subset to execute, while SPECTRE (Hayase et al., 2021) and D-BR (Chen et al., 2022) do not require any clean subset. For noisy label detection, we compare with 5 baselines, including BHN (Yu et al., 2023), CL (Northcutt et al., 2021), CORES Cheng et al. (2021), SimiFeat-V and SimiFeat-R (Zhu et al., 2022), in which BHN relies on a clean subset to perform. The detailed settings of each baseline can be found in Appendix C.3,C.4."
        },
        {
            "heading": "5.2 EXPERIMENTAL RESULTS",
            "text": ""
        },
        {
            "heading": "5.2.1 RESULTS ON DETECTING POISONED SAMPLES",
            "text": "In this section, we first conduct a comprehensive evaluation on the poisoned samples detection. The results on CIFAR-10, ImageNet-100 and ImageNet-Dog with different poisoning ratios are presented in Tables 1,2,13,14 (Refer Tables 13,14 in Appendix F.2). For a fair comparison, all baselines requiring clean data utilize 4% clean subset. The results demonstrate the effectiveness of our proposed method from the following aspects:\nConsistent Effectiveness Against Various Types of Poisoned Samples. From the results on CIFAR-10 in Table 1, we find that VDC consistently exhibits superior performance under various types of poisoned samples without relying on any clean subset, demonstrating the generalization of VDC. In contrast, other detectors are sensitive to different types of triggers. For example, VDC achieves average TPR of 99.91% against all backdoor attacks, while SPECTER experiences a significant fluctuation with a difference of 87.15% between its highest and lowest TPR. Additionally, VDC achieves competitive results in terms of FPR, averaging only 2.75%, which indicates that VDC has a low propensity to incorrectly identify clean samples as dirty samples.\nConsistent Effectiveness Across Datasets. Comparing the results of ImageNet-100 in Table 2 and CIFAR-10 in Table 1, when facing a larger dataset with more labels, VDC maintains performance with the average TPR still reaching 99.94%. On the contrary, other baselines are unstable on different datasets, such as SPECTRE decreases from 77.20% to 45.58%. To explore the effect of the similarity of classes, we evaluate on a fine-grained dataset ImageNet-Dog. From the results in Table 3 in Appendix F.1, VDC shows evident improvement compared to other baselines.\nConsistent Effectiveness Across Poisoning Ratios. We also evaluate with lower poisoning ratios on CIFAR-10 and ImageNet-100 to study the effect of poisoning ratios. Compare Table 1 with \u03b7 = 0.09 and Table 13 with \u03b7 = 0.009 on CIFAR-10, we find that the performance of VDC has almost no fluctuation, while other methods are greatly affected by the poisoning ratio. A similar phenomenon on ImageNet-100 can be found in Table 2 and 14."
        },
        {
            "heading": "5.2.2 RESULTS ON DETECTING NOISY LABELS.",
            "text": "In this section, we evaluate VDC on noisy label detection, another common type of dirty samples. The results on CIFAR-10, ImageNet-100 and ImageNet-Dog are shown in Table 4, verifying that VDC also performs well on detecting noisy labels from the following points:\nConsistent Effectiveness Against Various Types of Noisy Labels. By comparing the performance on the symmetric and the asymmetric noisy labels, we note that asymmetric is a more challenging setting. Even though some baselines behave well on detecting symmetric noisy labels, such as SimiFeat-V and SimiFeat-R, they may reach low TPR on the symmetric noisy labels. However, VDC consistently works well on the asymmetric noisy label. For example, VDC achieves 99.60% TPR on detecting asymmetric noisy labels on CIFAR-10, while SimiFeat-V only has 59.67% TPR.\nConsistent Effectiveness Across Datasets. From the results on the three datasets in Table 4, we note that VDC performs consistently well on different datasets, while other methods perform worse on ImageNet-100 and Imagenet-Dog, which indicates the robustness of our proposed method."
        },
        {
            "heading": "5.2.3 RESULTS ON DETECTING HYBRID DIRTY SAMPLES",
            "text": "In the real world, when an attacker poisons a realistic dataset, the dataset may already contain noisy labels. Therefore, in this section, we further evaluate the effectiveness of detectors when the dataset contains both poisoned samples and noisy samples, in which poisoning ratio is 0.09 and noisy ratio\nis 0.1 The results on CIFAR-10 are shown in Table 5. The following insightful points can be found from the results:\nConsistent Effectiveness Against Hybrids of Poisoned Samples and Noisy Labels. In this more challenging scenario, VDC still shows leading advantages compared with other methods, with average TPR reaching 99.41%. However, methods designed only for poisoned sample detection perform poorly when detecting a mixture of various dirty samples, such as SCAn decreasing from 95.46% to 46.29%. In the meantime, methods designed only to detect noisy samples also underperform in this case, such as CL decreasing from 85.05% to 36.33%, which further illustrates the effectiveness and robustness of our proposed method."
        },
        {
            "heading": "5.2.4 TRAINING ON THE PURIFIED DATASETS",
            "text": "After detecting and removing dirty samples from the origin dataset, we normally train DNNs on the purified datasets to verify the detection effect. The results on the purified datasets initially contain poisoned samples, noisy labels, and hybrid dirty samples are shown in Tables 6,15,16,17. By accurately detecting dirty samples, VDC indeed prevents the trained model from being interfered by dirty samples, i.e., maintaining low ASR and high ACC compared with other detectors."
        },
        {
            "heading": "6 A CLOSER LOOK AT VDC",
            "text": "In this section, we provide further analysis and ablation studies of VDC and show some limitations.\nEffect of the Type of Visual Questions. Figure 2a illustrates the influence of visual question types generated in VDC. We conducted experiments separately only using general questions or labelspecific questions while keeping all other settings constant. We observe that using only one type of question makes the model perform worse. In addition, label-specific questions are slightly more important than general questions.\nEffect of the Number of Visual Questions. We investigate the effect of the number of visual questions generated in VDC. Figure 2b shows the detection results w.r.t. various number of questions. We find that VDC\u2019s performance improves as the number of questions increases. But more questions also lead to more inference time. Therefore, it becomes crucial to strike a balance between these two factors.\nEffect of the Multimodal Large Language Model. In Figure 2c, we substitute the multimodal large language model in the VDC with Otter (Li et al., 2023), another recently open-sourced MLLM, to investigate the impact of MLLM. Although the performance differs from those obtained with InstructBLIP, it still outperforms the majority of baselines. with the TPR for all poisoned samples consistently exceeding 96%, which further verifies the effectiveness of VDC.\nComputational Complexity. Unlike other baselines that require training, VDC requires only inference of LLM and MLLM. Let K represent the number of classes, Nqg and Nqs denote the number of general questions and label-specific questions respectively, T and T \u2032 denote the time of one inference of LLM and MLLM. The overall time complexity can be expressed as O(TKNqs) + O(T\n\u2032(Nqg + Nqs)N) + O(TNqgN), in which three terms correspond to the complexities of VQG, VQA, and VQE respectively. With the development of lightweight LLM, such as quantization (Yao et al., 2023), the inference speed of LLM will increase, leading to a further reduction in the computational cost of VDC.\nLimitations. We show two primary limitations of VDC: 1) VDC hinges on the inconsistency between visual content and labels, making it inappropriate for detecting samples without corrupted labels, such as clean-label backdoor attack. 2) Although ensembling technique has been employed in our framework to mitigate the risk of abnormal questions and answers, LLM and MLLM may still yield incorrect replies. However, as LLM progresses (e.g. GPT-4V (OpenAI, 2023) that released recently), the performance of VDC will also improve in the future."
        },
        {
            "heading": "7 CONCLUSION",
            "text": "In this paper, we propose to detect dirty samples with corrupted labels by exploiting semantic inconsistency between visual content and associated labels. To this end, we design versatile data cleanser (VDC), a universal detection framework harnessing the surpassing capabilities of large language models and multimodal large language models, which is capable of detecting various categories and types of dirty samples. Experimental results validate the consistent superior performance of VDC in poisoned sample detection and noisy label detection. In addtion, VDC still maintains effectiveness even when the dataset contains the hybrid dirty samples. Furthermore, we anticipate that as large language models continue to evolve at a rapid pace, VDC will demonstrate further enhanced performance in the future."
        },
        {
            "heading": "A APPENDIX OVERVIEW",
            "text": "The overall structure of the Appendix is listed as follows:\n\u2022 Appendix B: A Naive Approach with CLIP\n\u2022 Appendix C: More implementation details.\n\u2013 Appendix C.1: Details of training on the purified datasets. \u2013 Appendix C.2: Details of poisoned sample generation. \u2013 Appendix C.3: Details of baseline Poisoned sample detectors. \u2013 Appendix C.4: Details of baseline Noisy label detectors\n\u2022 Appendix D: Prompts used in ChatGPT.\n\u2022 Appendix E: Examples of generated questions.\n\u2013 Appendix E.1: Examples of general questions. \u2013 Appendix E.2: Examples of label-specific questions.\n\u2022 Appendix F: Additional experimental results.\n\u2013 Appendix F.1: More results of poisoned sample detection. \u2013 Appendix F.2: More results of training on the purified datasets."
        },
        {
            "heading": "B A NAIVE APPROACH WITH CLIP",
            "text": "As we identified in the manuscript, how to measure the visual-linguistic inconsistency between the visual content and associated labels is the key to detect dirty samples. A naive approach to quantify such semantic inconsistency is directly using CLIP (Radford et al., 2021). We first encode the input image using image encoder in the CLIP, and get the image representation I. The associated label is transformed into sentences, \u201ca photo of {label}\u201d. Then the text representation T is extracted from the sentence via text encoder in the CLIP. Cosine similarity between I and T is treated as the matching score. If the matching score is less than a certain threshold, the input sample can be considered as dirty sample. In the implementation, we choose ViT-B/32 as the image encoder and the threshold is set as 0.2. The results are shown in Table 7, where VDC-CLIP represents the naive approach with CLIP. We find that the TPR using only CLIP is far from our proposed VDC, indicating the need for more advanced detection frameworks instead of only using CLIP."
        },
        {
            "heading": "C MORE IMPLEMENTATION DETAILS",
            "text": "C.1 DETAILS OF TRAINING ON THE PURIFIED DATASETS\nAfter successfully detecting dirty samples in the dataset, we need to normally training on the purified dataset t further verify the effectiveness of detectors. In our experiments, we choose ResNet-18 as the target model. For all datasets, the training epochs is set as 100 and adpot SGD optimizer. For\nCIFAR-10, we set the batch size of 128 and the inital learning rate of 0.1 and decreases it by the factor of 10 after 50, 75 epochs. For ImageNet-100 and ImageNet-Dog, the batch size is 64, the inital learning rate is 0.1 and decreases by the factor of 10 after 30, 60 epochs.\nC.2 DETAILS OF POISONED SAMPLE GENERATION\nIn this section, we present the settings for generating poisoned samples in backdoor attacks that are evaluated in the main manuscript. For all backdoor attacks, we choose class 0 as the target label.\nBadNets BadNets (Gu et al., 2019) stands as a seminal work in the realm of backdoor attacks, which introduces the concept of substituting specific pixels within a clean image with a welldesigned trigger, thus yielding a poisoned image. In our experiments, for a 32 \u00d7 32 image in CIFAR-10, we select a 3 \u00d7 3 white square patch located in the lower-right corner of the image to serve as the trigger. In the case of images with dimensions 224 \u00d7 224 from both ImageNet-100 and ImageNet-Dog datasets, we utilize a white square patch with dimensions 21\u00d7 21 as the trigger.\nBlended Blended Chen et al. (2017) firstly adopted the blended injection strategy to generate poisoned samples by blending a benign input instance with the key pattern. The choice of the key pattern can be an arbitrary image. In our experiments, we use a \u201cHello Kitty\u201d cartoon image (see Figure 3) as a trigger, and the blending ratio is set as 0.1.\nSIG SIG (Barni et al., 2019) proposes a horizontal sinusoidal signal designed by v(i, j) = \u2206 sin(2\u03c0jf/m), 1 \u2264 j \u2264 m, 1 \u2264 i \u2264 l, for a certain frequency f , on the clean image, where m is the number of columns of the image and l the number of rows. In the evaluation, we set \u2206 = 20, f = 6 for all datasets. The overlay backdooor signal is applied on all the channels. In this case, the backdoor is almost, though not perfectly, invisible.\nTrojanNN TrojanNN attack Liu et al. (2018) starts by choosing a trigger mask, which is a subset of the input variables that are used to inject the trigger. Then it searches for value assignment of the input variables in the trigger mask so that the selected neuron(s) of the target model can achieve the maximum values. The identified input values are essentially the trigger. In our evaluation, as shown in Figure 4, we choose to use the Apple logo as the trigger mask and ResNet-18 as target model.\nSSBA SSBA (Li et al., 2021) generates sample-specific invisible additive noises as backdoor triggers by encoding an attacker-specified string into clean images through an encoder-decoder network. Following the settings in (Li et al., 2021), we use a U-Net (Ronneberger et al., 2015) style DNN as the encoder, a spatial transformer network (Jaderberg et al., 2015) as the decoder. The encoderdecoder is trained for 140,000 iterations and batch size is set as 16.\nWaNet WaNet (Nguyen & Tran, 2021) uses a small and smooth warping field in generating poisoned images, making the modification unnoticeable. In our experiments, we adopt elastic image warping proposed in (Nguyen & Tran, 2021).\nExamples of Various Poisoned Samples As shown in Figure 5, we choose one image from ImageNet-100 and visualize the examples of various poisoned samples mentioned above.\nC.3 DETAILS OF BASELINE POISONED SAMPLE DETECTORS\nIn this section, we present the settings of 7 poisoned sample detection baselines compared in our experiments.\nSTRIP STRIP (Gao et al., 2019) detects a poisoned sample by checking whether superimposing the input image over a set of randomly selected images makes those new image\u2019s class label harder to predict. If so, the input is considered to be normal and otherwise. In our evaluation, the FRR is preset to be 0.1\nSS SS (Tran et al., 2018) identifies spectral signatures of all known backdoor attacks to utilize tools from robust statistics to thwart the attacks. The upper bound on number of poisoned training set examples \u03b5 is set as 0.1.\nSCAn SCAn (Tang et al., 2021) utilizes several statistical methods to estimate the most likely parameters for the decomposition and untangling models and then detect an infected label through a likelihood ratio test. The threshold user for split clean samples in each classes is set as Euler\u2019s number e.\nFrequency Frequency-based detection (Zeng et al., 2021) trains a binary classifier based on a training set that contains DCT transformations of clean samples and samples with digital manipulations. For CIFAR-10, We directly use their provided pretrained detection model. For ImageNet-100 and ImageNet-Dog, we train a 6 layer CNN with the same settings as CIFAR-10.\nCT CT (Qi et al., 2023) proposes confusion training that applies an additional poisoning attack to the already poisoned dataset, actively decoupling benign correlation while exposing backdoor patterns to detection. In our experiments, we set confusion factor \u03bb = 20, the number of confusion iterations m = 6000, the number of confusion training rounds K = 6.\nD-BR We only use the sample-distinguishment (SD) module in D-BR. SD module splits the whole training set into clean, poisoned and uncertain samples, according to the FCT metric. In our evaluation, we set \u03b1c = 0.2, \u03b1p is set as the true poisoning ratio.\nSPECTRE SPECTRE (Hayase et al., 2021) uses robust covariance estimation to amplify the spectral signature of corrupted data. In our experiments, \u03b1 is set as 4, poison fraction \u03b5 is set as 0.1.\nC.4 DETAILS OF BASELINE NOISY LABEL DETECTORS\nIn this section, we present the settings of 5 noisy label detection baselines compared in our experiments.\nBHN BHN (Yu et al., 2023) defines the p-values based on the neural network with the clean data. The p-values are then applied to the multiple hypothesis testing to detect corrupted examples. In our evaluation, we set leave ratio as 0.4. We use ResNet-18 for all datasets, and training epochs is set to be 200.\nCORES CORES (Cheng et al., 2021) trains ResNet-34 on the noisy dataset and uses its proposed sample sieve to filter out the corrupted examples. In our experiments, we adopt its default setting during training and calculate the F1 of the sieved out corrupted examples. The training epochs is set as 40.\nCL CL (Northcutt et al., 2021) detects corrupted labels by firstly estimating probabilistic thresholds to characterize the label noise, ranking examples based on model predictions, then filtering out corrupted examples based on ranking and thresholds.In our experiments, we train ResNet-18 on the noisy dataset and call the functions of Cleanlab1 to detect noisy labels.\nSimiFeat-V and SimiFeat-R SimiFeat-V (Zhu et al., 2022) uses \u201clocal voting\u201d via checking the noisy label consensuses of nearby features to determine if the example is corrupted. SimiFeatR (Zhu et al., 2022) scores and ranks each instance based on the neighborhood information and filters out a guaranteed number of instances that are likely to be corrupted. In the evaluation, the KNN paprameter k is set as 10 and epochs is set as 21."
        },
        {
            "heading": "D PROMPTS USED IN CHATGPT",
            "text": "In this section, we present the prompts that we used to query ChatGPT in our paper. Table 8 shows the prompts used for the generation of label-specific visual questions for different datasets. Table 9 shows the prompts used for the evaluation of the response of MLLM."
        },
        {
            "heading": "E EXAMPLES OF GENERATED QUESTIONS",
            "text": "In this section, we show some examples of generated visual questions in the visual question generation module of VDC.\nE.1 EXAMPLES OF GENERAL QUESTIONS\nTable 10 shows the general questions used for acquiring holistic descriptions of the image, with some prompts sourced from (Liu et al., 2023).\nE.2 EXAMPLES OF LABEL-SPECIFIC QUESTIONS\nTable 11 and 12 show the examples of label-specific visual questions on ImageNet-100.\n1https://github.com/cleanlab/cleanlab"
        },
        {
            "heading": "F ADDITIONAL EXPERIMENTAL RESULTS",
            "text": "In this section, we provide more experimental results that mentioned in the manuscript.\nF.1 MORE POISONED SAMPLE DETECTION RESULTS\n\u2022 Table 13 shows the detection results on CIFAR-10 with poisoning ratio \u03b7 = 0.009, i.e., 50 poisoned samples per class.\n\u2022 Table 14 shows the detection results on ImageNet-100 with poisoning ratio \u03b7 = 0.0099, i.e., 5 poisoned samples per class.\nThe results show the consistent effectiveness of VDC across different datasets and poisoning ratios.\nF.2 RESULTS OF TRAINING ON THE PURIFIED DATASETS.\n\u2022 Table 15 shows the normally training results on the purified CIFAR-10 with poisoning ratio \u03b7 = 0.009, i.e., 50 poisoned samples per class.\n\u2022 Table 17 shows the normally training results on the purified CIFAR-10 with poisoning ratio \u03b7 = 0.09 noisy ratio \u03b72 = 0.1.\nThe results show that our proposed VDC can indeed improve the reliability and usability of DNNs trained with dirty samples."
        },
        {
            "heading": "CT 0.79 93.24 0.71 93.94 0.12 93.7 3.96 93.17 0.57 93.76 1.32 93.55 1.25 93.56",
            "text": ""
        },
        {
            "heading": "SS 0.97 92.62 0.98 92.9 0.41 92.77 99.94 92.74 1.21 92.76 0.89 93.15 17.40 92.82",
            "text": ""
        }
    ],
    "year": 2023
}