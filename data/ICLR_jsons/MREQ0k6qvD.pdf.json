{
    "abstractText": "Exposing meaningful and interpretable neural interactions is critical to understanding neural circuits. Inferred neural interactions from neural signals primarily reflect functional connectivity. In a long experiment, subject animals may experience different stages defined by the experiment, stimuli, or behavioral states, and hence functional connectivity can change over time. To model dynamically changing functional connectivity, prior work employs state-switching generalized linear models with hidden Markov models (i.e., HMM-GLMs). However, we argue they lack biological plausibility, as functional connectivities are shaped and confined by the underlying anatomical connectome. Here, we propose two novel priorinformed state-switching GLMs, called Gaussian HMM-GLM (Gaussian prior) and one-hot HMM-GLM (Gumbel-Softmax one-hot prior). We show that the learned prior should capture the state-invariant interaction, shedding light on the underlying anatomical connectome and revealing more likely physical neuron interactions. The state-dependent interaction modeled by each GLM offers traceability to capture functional variations across multiple brain states. Our methods effectively recover true interaction structures in simulated data, achieve the highest predictive likelihood, and enhance the interpretability of interaction patterns and hidden states when applied to real neural data. The code is available at https://github.com/JerrySoybean/onehot-hmmglm.",
    "authors": [
        {
            "affiliations": [],
            "name": "Chengrui Li"
        },
        {
            "affiliations": [],
            "name": "Soon Ho Kim"
        },
        {
            "affiliations": [],
            "name": "Chris Rodgers"
        },
        {
            "affiliations": [],
            "name": "Hannah Choi"
        },
        {
            "affiliations": [],
            "name": "Anqi Wu"
        }
    ],
    "id": "SP:0233637b2acaf6b8d4a50e1eb0a74c4087e03204",
    "references": [
        {
            "authors": [
                "Guy Ackerson",
                "K Fu"
            ],
            "title": "On state estimation in switching environments",
            "venue": "IEEE transactions on automatic control,",
            "year": 1970
        },
        {
            "authors": [
                "Jeffrey Anderson",
                "Ilan Lampl",
                "Iva Reichova",
                "Matteo Carandini",
                "David Ferster"
            ],
            "title": "Stimulus dependence of two-state fluctuations of membrane potential in cat visual cortex",
            "venue": "Nature Neuroscience,",
            "year": 2000
        },
        {
            "authors": [
                "Zoe C Ashwood",
                "Nicholas A Roy",
                "Iris R Stone",
                "International Brain Laboratory",
                "Anne E Urai",
                "Anne K Churchland",
                "Alexandre Pouget",
                "Jonathan W Pillow"
            ],
            "title": "Mice alternate between discrete strategies during perceptual decision-making",
            "venue": "Nature Neuroscience,",
            "year": 2022
        },
        {
            "authors": [
                "Christopher M Bishop",
                "Nasser M Nasrabadi"
            ],
            "title": "Pattern Recognition and Machine Learning, volume",
            "year": 2006
        },
        {
            "authors": [
                "Chaw-Bing Chang",
                "Michael Athans"
            ],
            "title": "State estimation for discrete systems with switching parameters",
            "venue": "IEEE Transactions on Aerospace and Electronic Systems,",
            "year": 1978
        },
        {
            "authors": [
                "Martin Desch\u00eanes",
                "Jeffrey Moore",
                "David Kleinfeld"
            ],
            "title": "Sniffing and whisking in rodents",
            "venue": "Current Opinion in Neurobiology,",
            "year": 2012
        },
        {
            "authors": [
                "Tatiana A Engel",
                "Nicholas A Steinmetz",
                "Marc A Gieselmann",
                "Alexander Thiele",
                "Tirin Moore",
                "Kwabena Boahen"
            ],
            "title": "Selective modulation of cortical state during spatial",
            "venue": "attention. Science,",
            "year": 2016
        },
        {
            "authors": [
                "Sean Escola",
                "Alfredo Fontanini",
                "Don Katz",
                "Liam Paninski"
            ],
            "title": "Hidden markov models for the stimulus-response relationships of multistate neural systems",
            "venue": "Neural computation,",
            "year": 2011
        },
        {
            "authors": [
                "Emily Fox",
                "Erik Sudderth",
                "Michael Jordan",
                "Alan Willsky"
            ],
            "title": "Nonparametric bayesian learning of switching linear dynamical systems",
            "venue": "Advances in neural information processing systems,",
            "year": 2008
        },
        {
            "authors": [
                "Erhan Gen\u00e7",
                "Marieke Louise Sch\u00f6lvinck",
                "Johanna Bergmann",
                "Wolf Singer",
                "Axel Kohler"
            ],
            "title": "Functional connectivity patterns of visual cortex reflect its anatomical organization",
            "venue": "Cerebral Cortex,",
            "year": 2016
        },
        {
            "authors": [
                "Zoubin Ghahramani",
                "Geoffrey E Hinton"
            ],
            "title": "Switching state-space models. Technical report, Technical Report CRG-TR-96-3 DRAFT",
            "venue": "Dept. of Computer Science,",
            "year": 1996
        },
        {
            "authors": [
                "Zoubin Ghahramani",
                "Geoffrey E Hinton"
            ],
            "title": "Variational learning for switching state-space models",
            "venue": "Neural computation,",
            "year": 2000
        },
        {
            "authors": [
                "Joshua Glaser",
                "Matthew Whiteway",
                "John P Cunningham",
                "Liam Paninski",
                "Scott Linderman"
            ],
            "title": "Recurrent switching dynamical systems models for multiple interacting neural populations",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Clive WJ Granger"
            ],
            "title": "Investigating causal relations by econometric models and cross-spectral methods",
            "venue": "Econometrica: Journal of the Econometric Society,",
            "year": 1969
        },
        {
            "authors": [
                "Bilal Haider",
                "Alvaro Duque",
                "Andrea R Hasenstaub",
                "Yuguo Yu",
                "David A McCormick"
            ],
            "title": "Enhancement of visual responsiveness by spontaneous local network activity in vivo",
            "venue": "Journal of Neurophysiology,",
            "year": 2007
        },
        {
            "authors": [
                "James D Hamilton"
            ],
            "title": "Analysis of time series subject to changes in regime",
            "venue": "Journal of econometrics,",
            "year": 1990
        },
        {
            "authors": [
                "Conor Houghton"
            ],
            "title": "Calculating the mutual information between two spike trains",
            "venue": "Neural Computation,",
            "year": 2019
        },
        {
            "authors": [
                "Eric Jang",
                "Shixiang Gu",
                "Ben Poole"
            ],
            "title": "Categorical reparameterization with gumbel-softmax",
            "venue": "arXiv preprint arXiv:1611.01144,",
            "year": 2016
        },
        {
            "authors": [
                "Xiaoxuan Jia",
                "Joshua H Siegle",
                "S\u00e9verine Durand",
                "Greggory Heller",
                "Tamina K Ramirez",
                "Christof Koch",
                "Shawn R Olsen"
            ],
            "title": "Multi-regional module-based signal transmission in mouse",
            "venue": "visual cortex. Neuron,",
            "year": 2022
        },
        {
            "authors": [
                "Weihan Li",
                "Yu Qi",
                "Gang Pan"
            ],
            "title": "Online neural sequence detection with hierarchical dirichlet point process",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Scott Linderman",
                "Ryan P Adams",
                "Jonathan W Pillow"
            ],
            "title": "Bayesian latent structure discovery from multi-neuron recordings",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2016
        },
        {
            "authors": [
                "Scott Linderman",
                "Matthew Johnson",
                "Andrew Miller",
                "Ryan Adams",
                "David Blei",
                "Liam Paninski"
            ],
            "title": "Bayesian learning and inference in recurrent switching linear dynamical systems",
            "venue": "In Artificial Intelligence and Statistics,",
            "year": 2017
        },
        {
            "authors": [
                "Chris J Maddison",
                "Andriy Mnih",
                "Yee Whye Teh"
            ],
            "title": "The concrete distribution: A continuous relaxation of discrete random variables",
            "venue": "arXiv preprint arXiv:1611.00712,",
            "year": 2016
        },
        {
            "authors": [
                "Maxime Morariu-Patrichi",
                "Mikko S Pakkanen"
            ],
            "title": "State-dependent hawkes processes and their application to limit order book modelling",
            "venue": "Quantitative Finance,",
            "year": 2022
        },
        {
            "authors": [
                "Kevin P Murphy"
            ],
            "title": "Switching kalman filters",
            "year": 1998
        },
        {
            "authors": [
                "Namrata Nadagouda",
                "Mark A Davenport"
            ],
            "title": "Switched hawkes processes",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2021
        },
        {
            "authors": [
                "Ramon Nogueira",
                "Chris C Rodgers",
                "Randy M Bruno",
                "Stefano Fusi"
            ],
            "title": "The geometry of cortical representations of touch in rodents",
            "venue": "Nature Neuroscience,",
            "year": 2023
        },
        {
            "authors": [
                "A Peyrache",
                "M Khamassi",
                "K Benchenane",
                "SI Wiener",
                "F Battaglia"
            ],
            "title": "Replay of rule-learning related neural patterns in the prefrontal cortex during sleep",
            "venue": "Nature Neuroscience,",
            "year": 2009
        },
        {
            "authors": [
                "A Peyrache",
                "M Khamassi",
                "K Benchenane",
                "SI Wiener",
                "F Battaglia"
            ],
            "title": "Activity of neurons in rat medial prefrontal cortex during learning and sleep",
            "year": 2018
        },
        {
            "authors": [
                "Jonathan W Pillow",
                "Jonathon Shlens",
                "Liam Paninski",
                "Alexander Sher",
                "Alan M Litke",
                "EJ Chichilnisky",
                "Eero P Simoncelli"
            ],
            "title": "Spatio-temporal correlations and visual signalling in a complete neuronal population",
            "year": 2008
        },
        {
            "authors": [
                "CC Rodgers",
                "R Nogueira",
                "B Christina Pil",
                "EA Greeman",
                "JM Park",
                "YK Hong",
                "S Fusi",
                "RM Bruno"
            ],
            "title": "Sensorimotor strategies and neuronal representations for shape discrimination",
            "year": 2021
        },
        {
            "authors": [
                "Chris C Rodgers"
            ],
            "title": "A detailed behavioral, videographic, and neural dataset on object recognition in mice",
            "venue": "Scientific Data,",
            "year": 2022
        },
        {
            "authors": [
                "Maria V Sanchez-Vives",
                "David A McCormick"
            ],
            "title": "Cellular and network mechanisms of rhythmic recurrent activity in neocortex",
            "venue": "Nature Neuroscience,",
            "year": 2000
        },
        {
            "authors": [
                "Thomas Schreiber"
            ],
            "title": "Measuring information transfer",
            "venue": "Physical Review Letters,",
            "year": 2000
        },
        {
            "authors": [
                "S Murray Sherman"
            ],
            "title": "Tonic and burst firing: dual modes of thalamocortical relay",
            "venue": "Trends in neurosciences,",
            "year": 2001
        },
        {
            "authors": [
                "Joshua H Siegle",
                "Xiaoxuan Jia",
                "S\u00e9verine Durand",
                "Sam Gale",
                "Corbett Bennett",
                "Nile Graddis",
                "Greggory Heller",
                "Tamina K Ramirez",
                "Hannah Choi",
                "Jennifer A Luviano"
            ],
            "title": "Survey of spiking in the mouse visual system reveals functional",
            "venue": "hierarchy. Nature,",
            "year": 2021
        },
        {
            "authors": [
                "Feng Zhou",
                "Quyu Kong",
                "Yixuan Zhang",
                "Cheng Feng",
                "Jun Zhu"
            ],
            "title": "Nonlinear hawkes processes in time-varying system",
            "venue": "arXiv preprint arXiv:2106.04844,",
            "year": 2021
        },
        {
            "authors": [
                "Glaser"
            ],
            "title": "2020) consider state switching on underlying linear dynamics governing the observed spike trains. Besides, mp-rSLDS also considers incorporating prior information (such as anatomy) into the linear mappings in different states. However, its shared prior is a known hyperparameter, but the shared global prior in our GHG and OHG are learnable",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Unveiling meaningful and interpretable neural interaction structures is vital for comprehending neural circuits. Extensive research has investigated these interactions using statistical and informationtheoretic methods like cross-correlogram (Jia et al., 2022), mutual information (Houghton, 2019), Granger causality (Granger, 1969), transfer entropy (Schreiber, 2000), generalized linear methods on top of dynamical systems (Linderman et al., 2016; 2017; Glaser et al., 2020), and Hawkes process (Li et al., 2022). Typically, the inferred neural interaction reflects dynamic functional connectivity, influenced by variations in neural activity. Direct observation or inference of the anatomical connectome, encompassing axons, dendrites, and synapses that establish neural communication, is usually not feasible. Moreover, unlike anatomical connectome, which remains relatively stable over a period of time, function connectivity varies with behavioral states and on much faster time scales. Functional connectivities of neurons, therefore, reflect dynamic modes of computation shaped by task and sensory inputs. Existing experimental evidence suggests that neural systems can exhibit diverse firing patterns associated with different sensory, perceptual, and behavioral states (Sherman, 2001; Haider et al., 2007; Anderson et al., 2000; Sanchez-Vives & McCormick, 2000; Escola et al., 2011).\nTo capture such time-varying functional connectivities in multi-state systems, prior studies explored state-switching generalized linear models (GLMs) with hidden Markov models (HMMs), referred to as HMM-GLMs (Escola et al., 2011; Nadagouda & Davenport, 2021; Zhou et al., 2021; MorariuPatrichi & Pakkanen, 2022). These models introduce a discrete hidden variable representing the\nstate of each time point, with each state equipped with its own GLM to capture neural interactions. However, we argue that such methods are not biologically plausible enough, since an interaction between a pair of neurons inferred from neural signals can reflect not only functional connectivity but also anatomical connectome or synaptic connectivity. There exists experimental evidence manifesting degrees of correlations between functional and anatomical networks (Genc\u0327 et al., 2016; Siegle et al., 2021). It is thus plausible to assume that functional connectivity is dynamically modulated by brain states while also being shaped and confined by the underlying anatomical connectome.\nIncorporating these more biologically plausible assumptions, we introduce a novel approach for capturing time-varying functional connectivity in multi-state neural systems using an HMM-GLM framework. Unlike previous HMM-GLM methods that assume complete independence among GLMs in different states, we introduce a learnable prior for all states, constraining the search space for the interaction weight of each GLM derived from neural activity. We first provide a solution using a shared Gaussian prior over the weight matrices of GLMs for all states, denoted as Gaussian HMM-GLM. However, this Gaussian prior is relatively naive and does not explicitly connect functional connectivities to the anatomical connectome. Accordingly, we provide a second solution that decomposes each GLM\u2019s weight matrix into an adjacency matrix and a strength matrix, with the adjacency matrix modeled by a one-hot encoding mechanism. The prior is then imposed solely on the adjacency, not the entire weight matrix. We argue that the regulated adjacency matrices, guided by their prior, shed light on the underlying anatomical connectome, revealing more likely physical interactions between neurons. Meanwhile, less restricted strength matrices offer flexibility to capture functional variations across multiple brain states. Our experimental results demonstrate that when compared to alternatives, one-hot HMM-GLM accurately recovers true interaction structures in simulated data and achieves the highest predictive likelihood on test spike trains from two real neural datasets. Moreover, the uncovered interaction structures and hidden states are more interpretable compared with alternatives in real neural datasets."
        },
        {
            "heading": "2 METHOD",
            "text": "Classic GLM: Denote a spike train data as X \u2208 NT\u00d7N recorded from N neurons across T time bins, xt,n as the number of spikes generated by the n-th neuron in the t-th time bin, and xt \u2208 RN\u00d71 as the vector of spikes for all neurons at time t. When provided with X , a classic GLM, with pre-defined basis functions, predicts the firing rates of the n-th neuron at the time bin t as\nft,n = \u03c3 ( bn +\nN\u2211 n\u2032=1 wn\u2190n\u2032 \u00b7\n( K\u2211\nk=1\nxt\u2212k,n\u2032\u03d5k )) , with spike xt,n \u223c Poisson(ft,n), (1)\nwhere \u03c3 : R \u2192 R+ is a non-linear function (e.g., Softplus); bn is the background intensity of the n-th neuron; wn\u2190n\u2032 is the influence weight from the n\u2032-th neuron to the n-th neuron whose matrix form is W \u2208 RN\u00d7N ; \u03d5 \u2208 RK+ is the basis function summarizing history spikes from t\u2212K to t\u22121. The GLM finds the optimal W by maximizing the Poisson log-likelihood of the observed spikes.\nHMM-GLM (HG): First, we extend the GLM with a hidden Markov model (HMM). We assume there exist S states underlying the functional connectivities of neural activity. For each time t, we introduce a discrete latent variable zt \u2208 {1, . . . , S}, whose transition probability is p(zt+1|zt) = \u03c0zt,zt+1 with a matrix form \u03a0 \u2208 [0, 1]S\u00d7S . Given a latent state zt, the weight matrix at time t is selected as Wzt \u2208 Rn\u00d7n from {Ws} S s=1. Then the emission model is p(xt,n|zt,x1, . . . ,xt\u22121) = Poisson(ft,n):\nft,n = \u03c3 ( bn +\nN\u2211 n\u2032=1 wzt,n\u2190n\u2032 \u00b7\n( K\u2211\nk=1\nxt\u2212k,n\u2032\u03d5k\n)) . (2)\nNote that the traditional HMM framework assumes that the emission probability distributions, similar to the transition probability distributions, are time-homogeneous, i.e., the emission model does not depend on any previous observations. Here we relax the assumption by introducing the dependence over the spike history, similar to the previous HMM-GLMs (Escola et al., 2011). Next, we will introduce two variants of HG, in which a single shared global prior is imposed over all Ws in different states.\nGaussian HMM-GLM (GHG): To impose the assumption that functional connectivities across different states should share some common structure, a straightforward approach is imposing a Gaussian prior N (w0,n\u2190n\u2032 , \u03c32) on the weight ws,n\u2190n\u2032 with hyperparameter \u03c32, \u2200s \u2208 {1, . . . , S}. An\nA) B)\n\u2299 \u2299 \u2299\n...\n...\nHMM hidden state\nPoisson observation\nGLM latent\nshared prior\n\u2299)) + += weight inhibitory no excitatory strength\nFigure 1: A) A descriptive schematic of the weight matrix decomposition. B) The graphical model of the one-hot HMM-GLM.\nHG with such a Gaussian prior is referred to as Gaussian HMM-GLM (GHG), and the shared common information of state-dependent weights Ws is stored in W0.\nOne-hot HMM-GLM (OHG): One-hot HMM-GLM (OHG) decomposes the weight matrices Ws in Eq. 2 into a discrete adjacency matrix and a positive-valued strength matrix, i.e.,\nws,n\u2190n\u2032 = [(\u22121)as,n\u2190n\u2032,inh + (+1)as,n\u2190n\u2032,exc] \u00b7 w\u0303s,n\u2190n\u2032 , \u2200s \u2208 {1, . . . , S} . (3) w\u0303s,n\u2190n\u2032 \u2208 R+ is the strength of the weight in state s. We define as,n\u2190n\u2032 = [as,n\u2190n\u2032,inh, as,n\u2190n\u2032,no, as,n\u2190n\u2032,exc] \u2208 \u22062 to be the adjacency (weight type) from neuron n\u2032 to neuron n in state s corresponding to {inhibitory, no connection, excitatory}. as,n\u2190n\u2032 is a soft one-hot encoding vector over a simplex \u22062 := {a \u2208 [0, 1]3| \u22113 i=1 ai = 1}, so its scaler representation as,n\u2190n\u2032 = (\u22121)as,n\u2190n\u2032,inh + (+1)as,n\u2190n\u2032,exc should be close to either \u22121 or 0 or +1. The matrix and tensor forms are denoted as W\u0303s \u2208 RN\u00d7N+ and As \u2208 [0, 1]N\u00d7N\u00d73 respectively, \u2200s \u2208 {1, . . . , S}. Fig. 1A shows a schematic of the one-hot decomposition. To impose the assumption that functional connectivities across different states should share some common structure informing us about the underlying anatomical connectome, we impose a GumbelSoftmax prior over as,n\u2190n\u2032 , i.e., as,n\u2190n\u2032 \u223c Gumbel-Softmax(a0,n\u2190n\u2032 , \u03c4), \u2200s \u2208 {1, . . . , S}, written out as as,n\u2190n\u2032,type =\nexp [(ln a0,n\u2190n\u2032,type + gs,n\u2190n\u2032,type)/\u03c4 ]\u2211 type\u2032\u2208{inh,no,exc} exp [(ln a0,n\u2190n\u2032,type\u2032 + gs,n\u2190n\u2032,type\u2032)/\u03c4 ] ,\u2200 type \u2208 {inh, no, exc}\n(4) where gs,n\u2190n\u2032,type\ni.i.d.\u223c Gumbel(0, 1). In practice, we can sample g by sampling u from Uniform(0, 1) and computing g = \u2212 ln(\u2212 ln(u)). \u03c4 > 0 is a temperature hyperparameter forcing as,n\u2190n\u2032 to be a soft one-hot representation of the weight type. For each as,n\u2190n\u2032 , only one of the three types is significantly hotter than the other two, representing the type of the connection. The tensor form of a0,n\u2190n\u2032 is denoted as A0 \u2208 RN\u00d7N\u00d73, which is a free-parameter matrix imposing the biological structure similarity over different states. Consequently, if the synaptic (anatomical) connection type is excitatory, its functional connection type is likely to be excitatory; and vice versa. The log density of the Gumbel-Softmax distribution Jang et al. (2016); Maddison et al. (2016) is:\nln p(as,n\u2190n\u2032 |a0,n\u2190n\u2032) = ln 2 + 2\u03c4 \u2212 2 ln ( \u2211\ntype\u2208{inh,no,exc}\na0,n\u2190n\u2032,type (as,n\u2190n\u2032,type)\u03c4 ) +\n\u2211 type\u2208{inh,no,exc} (ln a0,n\u2190n\u2032,type \u2212 (\u03c4 + 1) ln(as,n\u2190n\u2032,type)) . (5)\nBy introducing a Gumbel-Softmax prior over the connection matrix As, we turn the parameter As into a latent variable. We can also assume the strength W\u0303s and the background intensity bn are random variables from some prior distributions. We put a Gaussian prior over the log of W\u0303s to ensure its positivity and a Gaussian prior over bn, finally obtaining the generative model of OHG:\nzt+1|zt \u223c Categorical(\u03c0zt,1, . . . , \u03c0zt,S), \u2200t \u2208 {1, . . . , T} as,n\u2190n\u2032 \u223c Gumbel-Softmax(a0,n\u2190n\u2032 , \u03c4), \u2200s \u2208 {1, . . . , S} , \u2200n, n\u2032 \u2208 {1, . . . , N}\nln w\u0303s,n\u2190n\u2032 \u223c N (\u00b5w, \u03c32w), \u2200s \u2208 {1, . . . , S} , \u2200n, n\u2032 \u2208 {1, . . . , N} (6) bn \u223c N (\u00b5b, \u03c32b ), \u2200n \u2208 {1, . . . , N} xt,n \u223c Poisson(ft,n(x1, . . . ,xt\u22121,Azt , W\u0303zt , bn)),\u2200t \u2208 {1, . . . , T} ,\u2200n, n\u2032 \u2208 {1, . . . , N} . A schematic diagram representing this generative model is shown in Fig. 1B.\nRelationships between GHG and OHG: GHG is similar to OHG in the sense that they both assume that the state-dependent weights Ws share some common information (A0 for OHG and W0\nfor GHG). The main difference is that GHG does not differentiate the adjacency from the strength (Eq. 3). Therefore, the shared W0 incorporates both. While in OHG, thanks to the decomposition, A0 only imposes similarity over the adjacency, not the strength. The regulated adjacency matrices As with their prior A0 should inform us about the underlying anatomical connectome. The less restricted strength matrices W\u0303s provide us with sufficient flexibility to capture functional variations across multiple brain states. Compared with OHG, GHG serves as an intermediate model with a straightforward prior directly on the weights representing shared global connectivity but without the one-hot decomposition. In the experimental evaluation section, we will show that a biologically plausible constraint like A0 in OHG is critical to obtaining meaningful inference and learning results."
        },
        {
            "heading": "3 INFERENCE",
            "text": "The generative model has four latent variables {zt,As, ln W\u0303s, bn}. It requires a complex fully Bayesian inference approach to infer all the latent variables, which is usually very time-consuming and highly computationally intensive. We provide a Baum-Welch algorithm to solve the inference problem. In our Baum-Welch, we derive the posterior of zt in the E-step, and do maximum likelihood estimation for all other latent variables given the estimated posterior distribution of zt in the M-step, i.e., we jointly optimize model parameters and latent variables (except zt) in the M-step. The rationale is that the calculation of the posterior for zt is straightforward via forward-backward message passing, while the calculation of the posterior for As is very challenging and has no closedform expression. We can certainly resort to a variational distribution to approximate the posterior for As. However, since the prior of As is a Gumbel-Softmax distribution, it is unclear what parametric density function we should choose to serve as the approximated posterior distribution. Given these challenges, we only do the E-step for zt with forward-backward message passing. In the M-step, we optimize the model parameters {\u03a0,A0} with {As, lnW\u0303s, bn}, denoted as \u03b8 altogether. The hyperparemeter set is \u03b6 = {\u00b5w, \u03c32w, \u00b5b, \u03c32b , \u03c4}, which is pre-defined, detailed later. We also pre-define the basis function \u03d5 \u2208 RK+ .\nFirst, we infer the hidden state given \u03b8old with the forward-backward algorithm (E-step). In this step, we will omit \u03b8old for simplicity. We define \u03b3zt(t) := p(zt|X; \u03b8old), \u03bezt\u22121,zt(t) := p(zt\u22121, zt|X; \u03b8old), and define \u03b1zt(t) := p(x1, . . . ,xt, zt), \u03b2zt(t) := p(zt+1, . . . , zT |x1, . . . ,xt, zt). Then, we can obtain the relationship \u03b3zt(t) = \u03b1zt (t)\u03b2zt (t) p(X) , \u03bezt\u22121,zt(t) = \u03b2zt (t)p(xt|x1,...,xt\u22121,zt)\u03b1zt\u22121 (t\u22121)p(zt|zt\u22121)\np(X) . \u03b1zt(t) and \u03b2zt(t) can be computed iteratively as{\n\u03b1zt(t) = p(xt|x1, . . . ,xt\u22121, zt) \u2211S zt\u22121=1 \u03b1zt\u22121(t)p(zt|zt\u22121), \u03b1z1(1) = p(z1)p(x1|z1)\n\u03b2zt(t) = \u2211S zt+1=1 \u03b2zt+1(t+ 1)p(xt+1|x1, . . . ,xt, zt+1)p(zt+1|zt), \u03b2zT (T ) = 1\nresulting in p(X) = \u2211S\nzT=1 \u03b1zT (T ). With this inferred posterior for z, we can update \u03b8 in the\nM-step by maximizing Q(\u03b8, \u03b8old) =Ep(z|X;\u03b8old) ln p(X, z; \u03b8) = \u2211 z p(z|X; \u03b8old) ln p(X, z; \u03b8)\n= S\u2211 z1=1 \u03b3z1(1) ln p(z1; \u03b8) + T\u2211 t=2 S\u2211 zt\u22121=1 S\u2211 zt=1 \u03bezt\u22121,zt(t) ln p(zt|zt\u22121; \u03b8)\n+ T\u2211 t=1 S\u2211 zt=1 \u03b3zt(t) ln p(xt|x1, . . . ,xt\u22121, zt; \u03b8).\n(7)\nMore details about the inference can be found in Appendix A.1.1.\nThere are several key hyperparameters in \u03b6 requiring pre-defining before inference. (1) GumbelSoftmax temperature \u03c4 : It is common to choose the temperature \u03c4 in Gumbel-Softmax from [0.1, 1]. If \u03c4 is too large, the relaxation will be too soft; if \u03c4 is too small, numerical issues could arise. In our model, \u03c4 is used to force the soft one-hot close to one corner of the simplex, so we tried \u03c4 \u2208 {0.1, 0.2, 0.5}, and found that the result of the one-hot HMM-GLM is not sensitive to \u03c4 in this range. Given that the selection of \u03c4 is insensitive to different datasets, we fix \u03c4 = 0.2, which is a common moderate choice. (2) Generative hyperparameters {\u00b5w, \u03c32w, \u00b5b, \u03c32b}: we chose \u00b5w = \u22125, \u03c3w = 2 and \u00b5b = 0, \u03c3b = 2 since this set provides noninformative priors for the weight strength and the background intensity in GLMs, and hence the inference is insensitive to different datasets."
        },
        {
            "heading": "4 EXPERIMENTAL EVALUATION",
            "text": "Models for comparison. We will compare our methods and state-of-the-art baseline methods on one simulated data and two real neural datasets: \u2022 GLM (Pillow et al., 2008): The most original model for discovering neural interactions, without the multiple-state assumption. \u2022 HMM Corr (Engel et al., 2016): An HMM for discovering state switches from spike train data. Since this method cannot find neural connectivities but only the latent states, we use a correlationbased method, i.e., cross-correlogram (CCG) to find the connectivity in each inferred state. \u2022 HMM Bern (Ashwood et al., 2022): Similar to the HMM Corr, but uses the Bernoulli rather than Poisson distribution to model the spike count in each time bin. \u2022 HG (Escola et al., 2011): The naive HMM-GLM (HG), which is the only existing model that both infers latent states and learns neural connectivities. \u2022 GHG (our method): We abbreviate Gaussian HMM-GLM as GHG. \u2022 OHG (our method): We abbreviate one-hot HMM-GLM as OHG. \u2022 HG-L1 and GHG-L1: Given that the one-hot mechanism implicitly imposes sparsity on the weight matrix, concerns may arise regarding whether the imposition of sparsity solely accounts for OHG\u2019s superiority. To address this, we add two more models: one by adding an L1 penalty to the weight of HG (HG-L1), and another to the weight of GHG (GHG-L1). The L1 penalty coefficients are determined through validation.\nMetrics. We use the following metrics to report performances from different models: \u2022 LL. The log-likelihood on the test set. A better model should have a stronger ability to predict future spiking events. Note that this is the only metric that can be used on real-world datasets, since there are usually no true states and neural connectivities available for real-world datasets. \u2022 State accuracy. The average accuracy of the inferred states across all time bins. This is only applicable to the simulated dataset where we know the true hidden states. \u2022 Weight error. The error of the learned weight matrices in all states. Note that there is no weight error for HMM Corr and HMM Bern. Since their learned weights are from CCG, the weights cannot be compared with the weights in the GLM model. This is only applicable to the simulated dataset. \u2022 Adjacency accuracy. The balanced accuracy of the learned adjacency matrices in all states. This is only applicable to the simulated dataset. For models without adjacency matrices explicitly modeled, we use\nas,n\u2190n\u2032 =  ( 0, 1\u2212 ws,n\u2190n\u2032maxs,n,n\u2032 ws,n\u2190n\u2032 ,\nws,n\u2190n\u2032\nmaxs,n,n\u2032 ws,n\u2190n\u2032 ) , ws,n\u2190n\u2032 \u2a7e 0(\nws,n\u2190n\u2032 mins,n,n\u2032 ws,n\u2190n\u2032 , 1\u2212 ws,n\u2190n\u2032mins,n,n\u2032 ws,n\u2190n\u2032 , 0\n) , ws,n\u2190n\u2032 < 0\n(8)\nto obtain the adjacency matrix from the learned weight matrix. We choose Eq. 8 since it is an automatic way with a reasonable rationale. We can also use a pre-defined threshold to obtain the adjacency matrix, but the accuracy of the connection matrices is very sensitive to the thresholding technique (see Appendix A.2). In real neural data analysis, when we don\u2019t have the ground-truth adjacency matrices, we cannot even use such an accuracy metric to select the optimal threshold value. This demonstrates that the explicit adjacency matrices from the OHG provide a succinct expression requiring no pre-defined thresholds but rendering satisfactory estimation. \u2022 Adjacency prior accuracy. This is only applicable to the simulated dataset. Except for OHG, the adjacency prior is obtained by first averaging the weight matrices across all states and then applying the averaged weight to Eq. 8."
        },
        {
            "heading": "4.1 APPLICATION TO SIMULATED DATA",
            "text": "Dataset. We first compare different models on a 5-state-20-neuron synthetic dataset with 10 independent trials. For each trial, we generate 20 spike sequences of length T = 5000. Each spike sequence is generated from the generative model in Eq. 6, with \u03c0s,s\u2032 = 0.005 + 0.975 \u00b7 1[s = s\u2032], \u03c4 = 0, \u00b5w = \u22125, \u03c32w = 1.5, and \u00b5b = 0, \u03c32b = 0.0008. We sample a0,n\u2032\u2190n from Dirichlet(0.1, 0.8, 0.1), \u2200n, n\u2032 \u2208 {1, . . . , 20}. Note that when \u03c4 = 0, all As are hard one-hot encodings i.i.d. sampled from A0. This is equivalent to sample as,n\u2190n\u2032 from a categorical distribution, i.e., as,n\u2190n\u2032 \u223c Categorical(a0,n\u2190n\u2032), \u2200s \u2208 {1, . . . , 5} , \u2200n, n\u2032 \u2208 {1, . . . , 20}. This introduces some mismatching generative procedures compared with Eq. 6. For each trial, we train different models on the first 10 sequences and test on the remaining 10 sequences.\nThe quantitative results in Tab. 1 show that OHG is the best in terms of all five metrics. GHGs are the second best since GHGs impose shared global prior on the weights. However, they are still worse than OHG, validating that the one-hot component accounts for the better performance of OHG.\nNext, we analyze the neural connectivities learned by different models (Fig. 2). Although there are S = 5 different states, one-state GLM only captures an \u201caverage\u201d estimation across the 5 states. For HMM Corr and HMM Bern, the learning procedure is decoupled into two steps, inferring hidden states and estimating the neural connectivity in each inferred state. Although the inferred hidden states from HMM Bern are acceptable, the estimated adjacency in each state and the adjacency prior are still bad. For HG, the poor performance is mainly from an incorrect estimation of the transition matrix, which leads to a bad inference of the hidden state sequence (Fig. 7 in Appendix 7) and hence results in a wrong weight and adjacency estimation. Comparing HG with GHG and OHG, we conclude that a constraint (i.e., a prior) on different states is necessary for a stable result, since the shared information between different states can help prevent the inferred states and the weights in different states from falling into extremes or bad local optima. Adding an L1 penalty could suppress some of the noisy weights but is still not helpful for estimating the adjacency in each state and the shared adjacency prior, as L1 does not enhance discrimination between a weak and no connection. The main difference between GHG and OHG is their weight and adjacency estimations. GHG still has many noisy non-zero weights. With the one-hot setting in OHG, the sparsity of the network is easily learned, and connections with zero interactions are successfully suppressed, which leads to a lower weight error and better adjacency accuracy (the weights, adjacencies, and the adjacency prior learned by OHG match the true the best in Fig. 2)."
        },
        {
            "heading": "4.2 APPLICATIONS TO ELECTROPHYSIOLOGY DATA",
            "text": ""
        },
        {
            "heading": "4.2.1 PREFRONTAL CORTEX DURING A CONTINGENCY TASK",
            "text": "Dataset. We first apply different models to a prefrontal cortex (PFC-6) dataset (Peyrache et al., 2018; 2009)1. Neural spike trains were collected while a rat learned a behavioral contingency task. During recording, the animal performed a trial for about 4 secs and then took a short break for about 24 secs. The spike train data used for learning and testing is segmented from the long session. Each sequence starts from 5 seconds before a behavior start and lasts for 10 seconds after the start. Hence, each sequence corresponds to a behavioral trial. We use 23 of the neural sequences as the training set and the remaining 13 as the test set. The neural spikes are binned into 750 time bins with bin size = 20 ms. Since we do not know the true number of hidden states, we try S \u2208 {2, 3, 4, 5}.\n1https://crcns.org/data-sets/pfc/pfc-6\nTab. 2 shows that the test log-likelihoods of OHG with all different numbers of states are consistently better than others. Fig. 3 shows an example of the weights and adjacencies estimated by different models. For HG, the learned weight matrices are pretty dense and noisy, resulting in a bad test log-likelihood. For GHG, the weights are less dense but still noisy. Adding L1 penalties to HG and GHG helps reduce some noisy weight entries, but does not help discriminate between a weak connection and no connection. Using OHG, we can get a much clearer strength-adjacency decomposition and also obtain an adjacency prior. The global restriction provided by the adjacency prior shapes the functional connectivities as the anatomical connectome does, which improves OHG\u2019s test log-likelihood. Note that GLM actually achieves a reasonably good result, only worse than OHG. This indicates that in such real-world scenarios, functional connectivities in different states indeed share a global static structure (may reflect the anatomical connectome), outweighing the functional differences between different states and hence should be taken into account. For S > 5, the performance does not increase significantly and becomes flat (Fig. 10 in Appendix A.4).\nAlthough there is no ground truth of hidden states, we can integrate the behavioral data to analyze the inferred hidden states from different models. Pick 4 states as an example. In Fig. 4, we plot the hidden state prediction of one incorrect trial (Fig. 4A) and one correct trial (Fig. 4B). We also plot the corresponding rat movement on the right-hand side and color it according to the inferred states from OHG. As previously observed, HG continues to yield a state prediction characterized by significant noise and limited interpretability. Although the number of hidden states is set as S = 4, GHG only infers two effective hidden states. The transition from state 4 to state 3 typically happens when the rat turns back at the wrong target location. However, OHG is able to find four explainable effective hidden states. Before each trial, the rat goes back to the root of the Y-shaped maze (starting point), corresponding to state 4. Then the rat turns around at the starting point and goes forward to the turning point of the Y-shaped maze, corresponding to state 3. After making the decision, the rat enters into state 2 in one arm of the Y-shaped maze, to reach the destination. If the rat goes to the correct target location, it gets a reward at the target and will stay in state 4 for a long while. But if the rat goes to the incorrect target location, there is no reward and the rat will go back immediately, corresponding to state 1. This state interpretation is reflected in the colored rat trajectory in Fig. 4. Note that these state patterns are not from cherry-picking. We do observe similar state transitions\namong other more correct and incorrect trials, which can be checked and validated in Fig. 8 in Appendix A.4. We also perform an analysis of task information decoding from the inferred hidden states (Fig. 9 in Appendix A.4) to further validate the rationality this interpretation."
        },
        {
            "heading": "4.2.2 BARREL CORTEX DURING WHISKING",
            "text": "Dataset. We next apply different models to electrode recordings of the somatosensory (barrel) cortex in mice during a shape discrimination task (Rodgers et al., 2021; Rodgers, 2022; Nogueira et al., 2023) (Fig. 5A). Mice were trained to discriminate concave from convex shapes using only their whiskers. In particular, the mice are required to actively whisk in order to make contact with the object; a high-speed video of whisker motion was collected, allowing analysis of the active movement of the whiskers to sense the environment. Here we use 27 sessions from 5 different mice. The number of recorded neurons varies from 10 to 44 across sessions. Six seconds from each trial is included in the analysis, and spike trains are discretized with a time bin of 3 ms. The first 30 trials are used in the analysis of each session, of which 10 randomly selected trials form the test set when evaluating the test log-likelihood, and the remaining 20 trials are used for training the model.\nGiven that we do not have good knowledge about the behavioral states, we try different numbers of hidden states for the barrel cortex data, i.e., S = {2, 3, 4, 5}. The log-likelihoods of the models fit to the barrel cortex dataset show similar trends to the PFC-6 dataset. OHG consistently has the highest log-likelihood, and GHG generally exhibits greater log-likelihood compared to other baseline models across different numbers of hidden states (See Fig. 5B).\nFig. 5C shows the whisker positions, contacts, and inferred hidden states of each model. We select the case of S = 2 hidden states here for visualization. While the log-likelihood of OHG increases as S increases to 5, for S > 2, there are many sessions with rarely occupied states, and the distinction between different states becomes subtle. Results for 3-5 states are shown in Appendix A.5. When two states are assumed, it is typically observed that one of the states inferred by GHG and OHG coincides with active whisking events during which contacts occurred, while the states inferred from HG switch very frequently.\nWhile GHG and OHG correlated with whisking events similarly, the durations of the inferred states are different (Fig. 5C). OHG infers a stable state with a duration over 1 s that persists over whisking cycles, while the inferred states from GHG switch rapidly with short durations (< 0.1 s). OHG thus better captures sustained whisking cycles (Desche\u0302nes et al. (2012); Rodgers et al. (2021)). Fig. 5D further shows the weight and adjacency matrices, and the adjacency prior estimated by each model for the same session shown in Fig. 5C. As in the PFC-6 dataset, we observe that only OHG learns sparse and clear weight matrices, while the ones learned by HG and GHG are denser and noisier.\nWe further test the idea that the states inferred from GHG and OHG are related to the active whisking events statistically. We compute the frequency with which whisker contacts are initiated in each state, and perform a \u03c72 test against the expected frequencies if no relation between the states and contacts is assumed. Among 11 sessions where all three models result in inferred state frequencies that are not completely skewed (the least frequent state was inferred in at least 5% of the time steps), the null hypothesis is rejected (p < 0.001) in 6 sessions (54%) for HG and in 8 sessions (73%) for both GHG and OHG. Furthermore, across all sessions, we compute the sum of all elements in the weight matrix W of the state associated with whisker contacts and that of the other state. When\ncomparing the distribution of total weight between whisking and non-whisking states, OHG results in a significant increase of the weights during whisking states (p = 0.008, two-sided Wilcoxon ranksum test), while HG and GHG do not (p > 0.1). This suggests that OHG is capable of detecting shifts in functional interaction tied to switching behavioral states."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "We develop a novel one-hot HMM-GLM (OHG) to estimate time-varying functional connectivity in multi-state neural systems. The newly proposed OHG decomposes the traditional weight matrix in GLMs into a discrete adjacency matrix representing the connection type and a positive-valued strength matrix. When building OHG, we place a common Gumbel-Softmax prior over the adjacency matrices for all states, enforcing the adjacency matrices to learn shared information. We argue that the regulated adjacency matrices with their shared prior should inform us about underlying anatomical connectome and thus uncover the \u201cmore likely\u201d physical interactions between neurons. The less restricted strength matrices are allowed to change freely without a shared prior across different states, and hence can provide us with sufficient flexibility to capture functional variations across multiple brain states. We argue that OHG is more biologically plausible given the aforementioned benefits. Gaussian HMM-GLM (GHG) serves as an intermediate model with shared prior directly on weight matrices without (one-hot) strength-adjacency decomposition, confirming that such a decomposition is critical for the success of multi-state inference. The experiments show that when compared with alternatives, OHG gets better connectivities and hidden states. It not only accurately recovers the true connectivities for simulated data but also achieves the best predictive likelihood on test spike trains for a PFC dataset and a barrel cortex dataset. The uncovered connectivities and hidden state sequence from OHG are more interpretable for these real neural datasets."
        },
        {
            "heading": "A APPENDIX",
            "text": "A.1 INFERENCE AND LEARNING ALGORITHMS FOR HMM-GLM"
        },
        {
            "heading": "A.1.1 FORWARD-BACKWARD INFERENCE",
            "text": "In this part, we compute the posterior probability given the old parameter \u03b8old, which is the E-step of the EM algorithm. Define {\n\u03b3zt(t) := p(zt|X; \u03b8old) \u03bezt\u22121,zt(t) := p(zt\u22121, zt|X; \u03b8old) , (9)\nwhere zt indexes one of the S different states. Define { \u03b1zt(t) := p(x1, . . . ,xt, zt)\n\u03b2zt(t) := p(zt+1, . . . , zT |x1, . . . ,xt, zt) , (10)\nand we have\n\u03b3zt(t) = p(X, zt)\np(X)\n= p(x1, . . . ,xt, zt)p(xt+1, . . . ,xT |x1, . . . ,xt, zt)\np(X)\n= \u03b1zt(t)\u03b2zt(t)\np(X) .\n(11)\n\u03b1zt(t) =p(x1, . . . ,xt, zt)\n=p(xt|x1, . . . ,xt\u22121, zt)p(x1, . . . ,xt\u22121, zt) =p(xt|x1, . . . ,xt\u22121, zt) \u2211 zt\u22121 p(x1, . . . ,xt\u22121, zt\u22121, zt)\n=p(xt|x1, . . . ,xt\u22121, zt) \u2211 zt\u22121 p(x1, . . . ,xt\u22121, zt|zt\u22121)p(zt\u22121)\n=p(xt|x1, . . . ,xt\u22121, zt) \u2211 zt\u22121 p(x1, . . . ,xt\u22121|zt, zt\u22121)p(zt|zt\u22121)p(zt\u22121)\n=p(xt|x1, . . . ,xt\u22121, zt) \u2211 zt\u22121 p(x1, . . . ,xt\u22121|zt\u22121)p(zt\u22121)p(zt|zt\u22121)\n=p(xt|x1, . . . ,xt\u22121, zt) \u2211 zt\u22121 \u03b1zt\u22121(t)p(zt|zt\u22121)\n(12)\nwith the initial condition \u03b1z1(1) = p(x1, z1) = p(z1)p(x1|z1). (13)\n\u03b2zt(t) =p(xt+1, . . . ,xT |x1, . . . ,xt, zt) = \u2211 zt+1 p(xt+1, . . . ,xT , zt+1|x1, . . . ,xt, zt)\n= \u2211 zt+1 p(xt+1, . . . ,xT |x1, . . . ,xt, zt+1, zt)p(zt+1|x1, . . . ,xt, zt)\n= \u2211 zt+1 p(xt+1, . . . ,xT |x1, . . . ,xt, zt+1)p(zt+1|zt)\n= \u2211 zt+1 p(xt+2, . . . ,xT |x1, . . . ,xt,xt+1, zt+1)p(xt+1|x1, . . . ,xt, zt+1)p(zt+1|zt)\n= \u2211 zt+1 \u03b2zt+1(t+ 1)p(xt+1|x1, . . . ,xt, zt+1)p(zt+1|zt)\n(14)\nwith the initial condition \u03b2zT (T ) = 1, since in Eq. 11\n\u03b3zT (T ) = p(zT |X) = p(X, zT )\u03b2z+T (T ) p(X) \u2261 p(X, zT ) p(X) . (15)\nIf we sum both sides of Eq. 11,\n1 = \u2211 zt \u03b3zt(t) = \u2211 zt \u03b1zt(t)\u03b2zt(t) p(X) =\u21d2 p(X) = \u2211 zt \u03b1zt(t)\u03b2zt(t), (16)\nand we can simply use p(X) = \u2211\nzT \u03b1zT (T ) when t = T .\n\u03bezt\u22121,zt(t) =p(zt\u22121, zt|X)\n= p(X|zt\u22121, zt)p(zt\u22121, zt)\np(X)\n= p(X|zt\u22121, zt)p(zt|zt\u22121)p(zt\u22121)\np(X)\n= p(xt, . . . ,xT |x1, . . . ,xt\u22121, zt\u22121, zt)p(x1, . . . ,xt\u22121|zt\u22121, zt)p(zt|zt\u22121)p(zt\u22121)\np(X)\n= p(xt+1, . . . ,xT |x1, . . . ,xt, zt\u22121, zt)p(xt|x1, . . . ,xt\u22121, zt\u22121, zt)\u03b1zt\u22121(t\u2212 1)p(zt|zt\u22121)\np(X)\n= p(xt+1, . . . ,xT |x1, . . . ,xt, zt)p(xt|x1, . . . ,xt\u22121, zt)\u03b1zt\u22121(t\u2212 1)p(zt|zt\u22121)\np(X)\n= \u03b2zt(t)p(xt|x1, . . . ,xt\u22121, zt)\u03b1zt\u22121(t\u2212 1)p(zt|zt\u22121)\np(X) .\n(17)"
        },
        {
            "heading": "A.1.2 BAUM\u2013WELCH ALGORITHM",
            "text": "Now, we already have the posterior, and we proceed to the M-step of the EM algorithm.\np(X, z; \u03b8) = p(z1; \u03b8)\n[ T\u220f\nt=2\np(zt|zt\u22121; \u03b8)\n] T\u220f\nt=1\np(xt|x1, . . . ,xt\u22121, zt; \u03b8). (18)\nln p(X, z; \u03b8) = ln p(z1; \u03b8) + T\u2211\nt=2\nln p(zt|zt\u22121; \u03b8) + T\u2211\nt=1\nln p(xt|x1, . . . ,xt\u22121, zt; \u03b8). (19)\nNotice that\nQ(\u03b8, \u03b8old) = S\u2211 z1=1 \u03b3z1(1) ln p(z1; \u03b8) + T\u2211 t=2 S\u2211 zt\u22121=1 S\u2211 zt=1 \u03bezt\u22121,zt(t) ln p(zt|zt\u22121; \u03b8)\n+ T\u2211 t=1 S\u2211 zt=1 \u03b3zt(t) ln p(xt|x1, . . . ,xt\u22121, zt; \u03b8).\n(20)\nProblems regarding the scaling factor in the forward-backward algorithm for numerical stability and the Viterbi algorithm for predicting the most probable hidden sequence are identical to the plain HMM, which can be found in (Bishop & Nasrabadi, 2006)."
        },
        {
            "heading": "A.2 THRESHOLD",
            "text": "Fig. 6 shows the balanced accuracy of the adjacency matrices and the adjacency prior matrix by thresholding as a function of the threshold varying from 0 to 0.5. The plots demonstrate that, in general, the accuracy is very sensitive to the threshold."
        },
        {
            "heading": "A.3 SYNTHETIC DATASET",
            "text": "Fig. 7 shows the state inference of all models on one of the synthetic spike trains."
        },
        {
            "heading": "A.4 PFC-6 DATASET",
            "text": ""
        },
        {
            "heading": "A.4.1 STATES VISUALIZATION OF CONSECUTIVE TRIALS",
            "text": "Fig. 8 shows the inferred state sequences of all models on trials 15-24."
        },
        {
            "heading": "A.4.2 DECODING TASK INFORMATION FROM INFERRED LATENT STATES",
            "text": "To further confirm our interpretation regarding the inferred states from OHG, we use logistic regression to decode the correctness of each trial from the inferred states. Specifically, each trial is viewed as a data point in logistic regression. The input variable is the one-hot representation of the inferred hidden states of size S \u00d7 T . We use the one-hot representation since the state in each time bin is a categorical variable. The output is a binary variable, representing the correctness of a trial. We use 2/3 trials to train the logistic regression and test on the remaining 1/3 trials.\nFig. 9 shows that the logistic regression fitted to the inferred states from OHG obtains the highest decoding accuracy. This means the inferred states from OHG do include enough information related to the correctness of each trial. From the interpretation in the main content, we mention that the animal will enter state 4 if it is a correct trial because of obtaining the reward at the correct target location. This is consistent with the positive coefficients of state 4 at the end period of trials, indicated by the black square in Fig. 9.\nTo further confirm the irreplaceable role of the inferred latent states in predicting trials\u2019 correctness, we train a multilayer perceptron (MLP) neural network with one hidden layer of size 100 (we tried different MLP configurations and selected the best one) to predict the correctness of each trial directly using the neural spike train as the input. The test accuracy is only 0.72, which is significantly worse than OHG. This implies that OHG plays a very important role in summarizing the task information from the neural spike train, similar to the irreplaceable role of the convolutional layers in CNN."
        },
        {
            "heading": "A.4.3 OHG WITH UP TO 20 STATES",
            "text": ""
        },
        {
            "heading": "A.5 BARREL CORTEX DATA WITH 5 HIDDEN STATES",
            "text": "As noted in the main text, OHG exhibits increasing test log-likelihood with an increasing number of states S. When S = 5, there were typically 2 or 3 dominant states inferred from OHG, with the other states being inferred only rarely across the sessions. Fig. 12 shows an example of a trial with S = 5. OHG exhibits one dominant hidden state (blue) with the other states being inferred for short intervals of duration 0.1-0.3 s, showing complex activation patterns in the vicinity of whisker contacts. The corresponding weight and adjacency matrices are shown in Fig. 13. Further analysis is needed to determine the significance of such states."
        },
        {
            "heading": "A.6 SUBPOPULATION (MULTI-REGION) PROBLEMS",
            "text": "Although all HMM-GLMs in this paper including our newly proposed GHG and OHG only work for one global prior, it is still possible to apply the model to multi-region data. For example, C groups of neurons N1, N2, . . . , NC evolve largely independently and only rarely communicate. Further, assume each group of neurons has its own global structure and its own number of states S1, S2, . . . , SC . Then, we can reduce such a problem into a model with one single globally shared structure diag(A1,0,A2,0, . . . ,AC,0). Then, there will be S = S1 \u00d7 S2 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 SC states for the whole groups of neurons in total.\nFor better illustration, we run a simple example on C = 3 subpopulations (groups) and each group has 5 neurons and 2 states. Therefore, we need a model of 15 neurons and 8 states to accommodate this configuration.\nFig. 14A shows that OHG is better than HG and GHG in terms of LL and weight error. For state accuracy, adjacency accuracy, and adjacency prior accuracy both GHG and OHG are better than\nHG. However, since these models are not targeted tasks including subpopulations, the overall state accuracies for all models are worse than usual. The reason is that the state complexity of the reduced problem increases exponentially w.r.t. the number of groups. Fig. 14B shows the true weight matrices in all 8 combination states (using binary encoding) and the learned weight matrices from HG, GHG, and OHG. Consistent with the weight error reported in Fig. 14A, OHG is better than GHG and is better than HG. Particularly, OHG learns meaningful weight matrices in each group. For example, the top-left block of OHG in states 1, 2, and 3 corresponds to the state 0 weight matrix of group c = 1. The top-left block of OHG in states 5 and 6 corresponds to the state 1 weight matrix of group c = 1. Fig. 14C qualitatively shows that the inferred states from GHG and OHG are at an acceptable level compared with the true overall state transitions."
        },
        {
            "heading": "A.7 RELATIONSHIPS TO SLDS, RSLDS, AND MP-RSLDS",
            "text": "Instead of considering state switching on neural connectivities as our HMM-GLM-based models, SLDS (Ackerson & Fu, 1970; Chang & Athans, 1978; Hamilton, 1990; Ghahramani & Hinton, 1996; 2000; Murphy, 1998; Fox et al., 2008), rSLDS (Linderman et al., 2016; 2017), and mp-rSLDS (Glaser et al., 2020) consider state switching on underlying linear dynamics governing the observed spike trains. Besides, mp-rSLDS also considers incorporating prior information (such as anatomy) into the linear mappings in different states. However, its shared prior is a known hyperparameter, but the shared global prior in our GHG and OHG are learnable.\nAlthough SLDS, rSLDS, and mp-rSLDS don\u2019t learn neural connectivities in different states explicitly, both SLDS-based models and HMM-GLM-based models learn discrete state switches from spike trains. Since we don\u2019t consider multi-region or subpopulations in this work, we compare the inferred states from OHG with those from SLDS and rSLDS on the PFC-6 dataset.\nFig. 15 shows the inferred states from SLDS and rSLDS exhibit fast-switching phenomena, which hinders their interpretability. This result might imply that state switching over neural connectivities could be an important assumption that should be taken into account when dealing with spike train data collected from multi-stage experiments."
        },
        {
            "heading": "A.8 SCALABILITY TO LARGE NUMBERS OF NEURONS",
            "text": "From the synthetic dataset, we observe that the state accuracies of all HMM-GLM-based models start to drop when there are 40-50 neurons. To this scale, there will be more than S \u00d7 [1600, 2500] edges (connections) that need to be learned in the network, which introduces challenges in determining the correct state switches. Under such situations, the effectiveness of both the E-step and the M-step in the algorithm would be mutually influenced by each other. However, this does not mean that the HMM-GLM-based models cannot be applied to real-world datasets. It might still be worth trying HMM-GLM-based models even if the data consists of a large number of neurons, since Fig. 16 shows that the per-neuron log-likelihood does not drop when increasing the number of neurons. This means that HMM-GLM-based models (especially OHG) can still predict firing rates effectively for spike train data reconstruction. We would like to view this as an important future direction."
        },
        {
            "heading": "A.9 PRIOR ON THE TRANSITION MATRIX",
            "text": "An intuitive way of suppressing the fast state switches in HG is to add an L2 regularization term on the transition matrix. However, Fig. 17 shows that when the number of state switches is suppressed, the inferred states are still meaningless. Specifically, most switches happen within a short duration, which looks like nothing but noisy state switches. Only one major state governs the whole trial."
        }
    ],
    "title": "ONE-HOT GENERALIZED LINEAR MODEL FOR SWITCHING BRAIN STATE DISCOVERY",
    "year": 2024
}