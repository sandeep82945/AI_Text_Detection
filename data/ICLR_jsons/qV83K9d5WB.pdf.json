{
    "abstractText": "Recent research has highlighted the potential of large language models (LLMs) to improve their problem-solving capabilities with the aid of suitable external tools. In our work, we further advance this concept by introducing a closedloop framework, referred to as LLMs As Tool Makers (LATM), where LLMs create their own reusable tools for problem-solving. Our approach consists of two phases: 1) tool making: an LLM acts as the tool maker that crafts tools for a set of tasks, where a tool is implemented as a Python utility function. 2) tool using: another LLM acts as the tool user, which applies the tool built by the tool maker for problem-solving. The tool user can be either the same or a different LLM from the tool maker. On the problem-solving server side, tool-making enables continual tool generation and caching as new requests emerge. This framework enables subsequent requests to access cached tools via their corresponding APIs, enhancing the efficiency of task resolution. Beyond enabling LLMs to create their own tools, our framework also uncovers intriguing opportunities to optimize the serving cost of LLMs: Recognizing that tool-making requires more sophisticated capabilities, we assign this task to a powerful, albeit resource-intensive, model. Conversely, the simpler tool-using phase is delegated to a lightweight model. This strategic division of labor allows the once-off cost of tool-making to be spread over multiple instances of tool-using, significantly reducing average costs while maintaining strong performance. Furthermore, our method offers a functional cache through the caching and reuse of tools, which stores the functionality of a class of requests instead of the natural language responses from LLMs, thus extending the applicability of the conventional cache mechanism. We evaluate our approach across various complex reasoning tasks, including Big-Bench tasks. With GPT-4 as the tool maker and GPT-3.5 as the tool user, LATM demonstrates performance equivalent to using GPT-4 for both roles, but with a significantly reduced inference cost. The codebase can be found in https://github.com/ ctlllll/LLM-ToolMaker.",
    "authors": [
        {
            "affiliations": [],
            "name": "TOOL MAKERS"
        },
        {
            "affiliations": [],
            "name": "Tianle Cai"
        },
        {
            "affiliations": [],
            "name": "Xuezhi Wang"
        },
        {
            "affiliations": [],
            "name": "Tengyu Ma"
        },
        {
            "affiliations": [],
            "name": "Xinyun Chen"
        },
        {
            "affiliations": [],
            "name": "Denny Zhou"
        }
    ],
    "id": "SP:672b412679ad69e9f258339a50988ca7f5ed186a",
    "references": [
        {
            "authors": [
                "Aida Amini",
                "Saadia Gabriel",
                "Peter Lin",
                "Rik Koncel-Kedziorski",
                "Yejin Choi",
                "Hannaneh Hajishirzi"
            ],
            "title": "Mathqa: Towards interpretable math word problem solving with operation-based formalisms",
            "year": 1905
        },
        {
            "authors": [
                "Simran Arora",
                "Brandon Yang",
                "Sabri Eyuboglu",
                "Avanika Narayan",
                "Andrew Hojel",
                "Immanuel Trummer",
                "Christopher R\u00e9"
            ],
            "title": "Language models enable simple systems for generating structured views of heterogeneous data lakes, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Jacob Austin",
                "Augustus Odena",
                "Maxwell Nye",
                "Maarten Bosma",
                "Henryk Michalewski",
                "David Dohan",
                "Ellen Jiang",
                "Carrie Cai",
                "Michael Terry",
                "Quoc Le",
                "Charles Sutton"
            ],
            "title": "Program synthesis with large language models, 2021",
            "year": 2021
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "S\u00e9bastien Bubeck",
                "Varun Chandrasekaran",
                "Ronen Eldan",
                "Johannes Gehrke",
                "Eric Horvitz",
                "Ece Kamar",
                "Peter Lee",
                "Yin Tat Lee",
                "Yuanzhi Li",
                "Scott Lundberg"
            ],
            "title": "Sparks of artificial general intelligence: Early experiments with gpt-4",
            "venue": "arXiv preprint arXiv:2303.12712,",
            "year": 2023
        },
        {
            "authors": [
                "Charlie Chen",
                "Sebastian Borgeaud",
                "Geoffrey Irving",
                "Jean-Baptiste Lespiau",
                "Laurent Sifre",
                "John Jumper"
            ],
            "title": "Accelerating large language model decoding with speculative sampling",
            "venue": "February 2023a. doi: 10.48550/ARXIV.2302.01318",
            "year": 2023
        },
        {
            "authors": [
                "Lingjiao Chen",
                "Matei Zaharia",
                "James Zou"
            ],
            "title": "Frugalgpt: How to use large language models while reducing cost and improving performance, 2023b",
            "year": 2023
        },
        {
            "authors": [
                "Wenhu Chen",
                "Xueguang Ma",
                "Xinyi Wang",
                "William W. Cohen"
            ],
            "title": "Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Xinyun Chen",
                "Maxwell Lin",
                "Nathanael Sch\u00e4rli",
                "Denny Zhou"
            ],
            "title": "Teaching large language models to self-debug",
            "venue": "ARXIV.ORG, 2023c. doi: 10.48550/arXiv.2304.05128",
            "year": 2023
        },
        {
            "authors": [
                "Aakanksha Chowdhery",
                "Sharan Narang",
                "Jacob Devlin",
                "Maarten Bosma",
                "Gaurav Mishra",
                "Adam Roberts",
                "Paul Barham",
                "Hyung Won Chung",
                "Charles Sutton",
                "Sebastian Gehrmann"
            ],
            "title": "Palm: Scaling language modeling with pathways",
            "venue": "arXiv preprint arXiv:2204.02311,",
            "year": 2022
        },
        {
            "authors": [
                "Karl Cobbe",
                "Vineet Kosaraju",
                "Mohammad Bavarian",
                "Mark Chen",
                "Heewoo Jun",
                "Lukasz Kaiser",
                "Matthias Plappert",
                "Jerry Tworek",
                "Jacob Hilton",
                "Reiichiro Nakano"
            ],
            "title": "Training verifiers to solve math word problems",
            "venue": "arXiv preprint arXiv:2110.14168,",
            "year": 2021
        },
        {
            "authors": [
                "David Dohan",
                "Winnie Xu",
                "Aitor Lewkowycz",
                "Jacob Austin",
                "David Bieber",
                "Raphael Gontijo Lopes",
                "Yuhuai Wu",
                "Henryk Michalewski",
                "Rif A. Saurous",
                "Jascha Sohl-dickstein",
                "Kevin Murphy",
                "Charles Sutton"
            ],
            "title": "Language model cascades, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Daniel Conrad Halbert"
            ],
            "title": "Programming by example",
            "venue": "University of California, Berkeley,",
            "year": 1984
        },
        {
            "authors": [
                "Jordan Hoffmann",
                "Sebastian Borgeaud",
                "Arthur Mensch",
                "Elena Buchatskaya",
                "Trevor Cai",
                "Eliza Rutherford",
                "Diego de Las Casas",
                "Lisa Anne Hendricks",
                "Johannes Welbl",
                "Aidan Clark"
            ],
            "title": "Training compute-optimal large language models",
            "venue": "arXiv preprint arXiv:2203.15556,",
            "year": 2022
        },
        {
            "authors": [
                "Geunwoo Kim",
                "P. Baldi",
                "S. McAleer"
            ],
            "title": "Language models can solve computer",
            "venue": "tasks. ARXIV.ORG,",
            "year": 2023
        },
        {
            "authors": [
                "Michal Kosinski"
            ],
            "title": "Theory of mind may have spontaneously emerged in large language models",
            "venue": "arXiv preprint arXiv:2302.02083,",
            "year": 2023
        },
        {
            "authors": [
                "Yaniv Leviathan",
                "Matan Kalman",
                "Yossi Matias"
            ],
            "title": "Fast inference from transformers via speculative decoding",
            "venue": "doi: 10.48550/ARXIV.2211.17192",
            "year": 2022
        },
        {
            "authors": [
                "Yaobo Liang",
                "Chenfei Wu",
                "Ting Song",
                "Wenshan Wu",
                "Yan Xia",
                "Yu Liu",
                "Yang Ou",
                "Shuai Lu",
                "Lei Ji",
                "Shaoguang Mao"
            ],
            "title": "Taskmatrix. ai: Completing tasks by connecting foundation models with millions of apis",
            "venue": "arXiv preprint arXiv:2303.16434,",
            "year": 2023
        },
        {
            "authors": [
                "Wang Ling",
                "Dani Yogatama",
                "Chris Dyer",
                "Phil Blunsom"
            ],
            "title": "Program induction by rationale generation: Learning to solve and explain algebraic word problems. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
            "year": 2017
        },
        {
            "authors": [
                "Ruibo Liu",
                "Jason Wei",
                "Shixiang Shane Gu",
                "Te-Yen Wu",
                "Soroush Vosoughi",
                "Claire Cui",
                "Denny Zhou",
                "Andrew M. Dai"
            ],
            "title": "Mind\u2019s eye: Grounded language model reasoning through simulation",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Pan Lu",
                "Baolin Peng",
                "Hao Cheng",
                "Michel Galley",
                "Kai-Wei Chang",
                "Ying Nian Wu",
                "Song-Chun Zhu",
                "Jianfeng Gao"
            ],
            "title": "Chameleon: Plug-and-play compositional reasoning with large language models",
            "venue": "arXiv preprint arXiv:2304.09842,",
            "year": 2023
        },
        {
            "authors": [
                "Aman Madaan",
                "Niket Tandon",
                "Prakhar Gupta",
                "Skyler Hallinan",
                "Luyu Gao",
                "Sarah Wiegreffe",
                "Uri Alon",
                "Nouha Dziri",
                "Shrimai Prabhumoye",
                "Yiming Yang"
            ],
            "title": "Self-refine: Iterative refinement with self-feedback",
            "venue": "arXiv preprint arXiv:2303.17651,",
            "year": 2023
        },
        {
            "authors": [
                "Maxwell Nye",
                "Anders Johan Andreassen",
                "Guy Gur-Ari",
                "Henryk Michalewski",
                "Jacob Austin",
                "David Bieber",
                "David Dohan",
                "Aitor Lewkowycz",
                "Maarten Bosma",
                "David Luan"
            ],
            "title": "Show your work: Scratchpads for intermediate computation with language models",
            "venue": "arXiv preprint arXiv:2112.00114,",
            "year": 2021
        },
        {
            "authors": [
                "Bhargavi Paranjape",
                "Scott Lundberg",
                "Sameer Singh",
                "Hannaneh Hajishirzi",
                "Luke Zettlemoyer",
                "Marco Tulio Ribeiro"
            ],
            "title": "Art: Automatic multi-step reasoning and tool-use for large language models",
            "venue": "arXiv preprint arXiv:2303.09014,",
            "year": 2023
        },
        {
            "authors": [
                "Aaron Parisi",
                "Yao Zhao",
                "Noah Fiedel"
            ],
            "title": "Talm: Tool augmented language models, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Cheng Qian",
                "Chi Han",
                "Yi R Fung",
                "Yujia Qin",
                "Zhiyuan Liu",
                "Heng Ji"
            ],
            "title": "Creator: Disentangling abstract and concrete reasonings of large language models through tool creation",
            "venue": "arXiv preprint arXiv:2305.14318,",
            "year": 2023
        },
        {
            "authors": [
                "Yujia Qin",
                "Shengding Hu",
                "Yankai Lin",
                "Weize Chen",
                "Ning Ding",
                "Ganqu Cui",
                "Zheni Zeng",
                "Yufei Huang",
                "Chaojun Xiao",
                "Chi Han"
            ],
            "title": "Tool learning with foundation models",
            "venue": "arXiv preprint arXiv:2304.08354,",
            "year": 2023
        },
        {
            "authors": [
                "Timo Schick",
                "Jane Dwivedi-Yu",
                "Roberto Dess\u00ec",
                "Roberta Raileanu",
                "Maria Lomeli",
                "Luke Zettlemoyer",
                "Nicola Cancedda",
                "Thomas Scialom"
            ],
            "title": "Toolformer: Language models can teach themselves to use tools, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Yongliang Shen",
                "Kaitao Song",
                "Xu Tan",
                "Dongsheng Li",
                "Weiming Lu",
                "Yueting Zhuang"
            ],
            "title": "Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface",
            "venue": "arXiv preprint arXiv:2303.17580,",
            "year": 2023
        },
        {
            "authors": [
                "Freda Shi",
                "Mirac Suzgun",
                "Markus Freitag",
                "Xuezhi Wang",
                "Suraj Srivats",
                "Soroush Vosoughi",
                "Hyung Won Chung",
                "Yi Tay",
                "Sebastian Ruder",
                "Denny Zhou"
            ],
            "title": "Language models are multilingual chain-of-thought reasoners",
            "venue": "arXiv preprint arXiv:2210.03057,",
            "year": 2022
        },
        {
            "authors": [
                "Aarohi Srivastava",
                "Abhinav Rastogi",
                "Abhishek Rao",
                "Abu Awal Md Shoeb",
                "Abubakar Abid",
                "Adam Fisch",
                "Adam R Brown",
                "Adam Santoro",
                "Aditya Gupta",
                "Adri\u00e0 Garriga-Alonso"
            ],
            "title": "Beyond the imitation game: Quantifying and extrapolating the capabilities of language models",
            "venue": "arXiv preprint arXiv:2206.04615,",
            "year": 2022
        },
        {
            "authors": [
                "Mirac Suzgun",
                "Nathan Scales",
                "Nathanael Sch\u00e4rli",
                "Sebastian Gehrmann",
                "Yi Tay",
                "Hyung Won Chung",
                "Aakanksha Chowdhery",
                "Quoc V Le",
                "Ed H Chi",
                "Denny Zhou"
            ],
            "title": "Challenging big-bench tasks and whether chain-of-thought can solve them",
            "venue": "arXiv preprint arXiv:2210.09261,",
            "year": 2022
        },
        {
            "authors": [
                "Guanzhi Wang",
                "Yuqi Xie",
                "Yunfan Jiang",
                "Ajay Mandlekar",
                "Chaowei Xiao",
                "Yuke Zhu",
                "Linxi Fan",
                "Anima Anandkumar"
            ],
            "title": "Voyager: An open-ended embodied agent with large language models",
            "venue": "arXiv preprint arXiv:2305.16291,",
            "year": 2023
        },
        {
            "authors": [
                "Xuezhi Wang",
                "Jason Wei",
                "Dale Schuurmans",
                "Quoc Le",
                "Ed Chi",
                "Denny Zhou"
            ],
            "title": "Self-consistency improves chain of thought reasoning in language models",
            "venue": "arXiv preprint arXiv:2203.11171,",
            "year": 2022
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "Ed Chi",
                "Quoc Le",
                "Denny Zhou"
            ],
            "title": "Chain of thought prompting elicits reasoning in large language models",
            "year": 2022
        },
        {
            "authors": [
                "Tongshuang Wu",
                "Michael Terry",
                "Carrie Jun Cai"
            ],
            "title": "Ai chains: Transparent and controllable human-ai interaction by chaining large language model prompts",
            "venue": "In Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Heming Xia",
                "Tao Ge",
                "Si-Qing Chen",
                "Furu Wei",
                "Zhifang Sui"
            ],
            "title": "Speculative decoding: Lossless speedup of autoregressive translation, 2023. URL https://openreview.net/forum?id= H-VlwsYvVi",
            "year": 2023
        },
        {
            "authors": [
                "Zhengyuan Yang",
                "Linjie Li",
                "Jianfeng Wang",
                "Kevin Lin",
                "Ehsan Azarnasab",
                "Faisal Ahmed",
                "Zicheng Liu",
                "Ce Liu",
                "Michael Zeng",
                "Lijuan Wang"
            ],
            "title": "Mm-react: Prompting chatgpt for multimodal reasoning and action",
            "venue": "arXiv preprint arXiv:2303.11381,",
            "year": 2023
        },
        {
            "authors": [
                "Shunyu Yao",
                "Jeffrey Zhao",
                "Dian Yu",
                "Nan Du",
                "Izhak Shafran",
                "Karthik R Narasimhan",
                "Yuan Cao"
            ],
            "title": "React: Synergizing reasoning and acting in language models",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Susan Zhang",
                "Stephen Roller",
                "Naman Goyal",
                "Mikel Artetxe",
                "Moya Chen",
                "Shuohui Chen",
                "Christopher Dewan",
                "Mona Diab",
                "Xian Li",
                "Xi Victoria Lin"
            ],
            "title": "Opt: Open pre-trained transformer language models",
            "venue": "arXiv preprint arXiv:2205.01068,",
            "year": 2022
        },
        {
            "authors": [
                "Denny Zhou",
                "Nathanael Sch\u00e4rli",
                "Le Hou",
                "Jason Wei",
                "Nathan Scales",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Olivier Bousquet",
                "Quoc Le",
                "Ed Chi"
            ],
            "title": "Least-to-most prompting enables complex reasoning in large language models",
            "venue": "arXiv preprint arXiv:2205.10625,",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Large language models (LLMs) have demonstrated outstanding capabilities across a broad array of NLP tasks (Brown et al., 2020; Chowdhery et al., 2022; Zhang et al., 2022; Hoffmann et al., 2022; OpenAI, 2023; Google, 2023) and have even shown promising signs of achieving certain aspects of artificial general intelligence (Bubeck et al., 2023; Kosinski, 2023). Moreover, analogous to the evolution of human intelligence, recent research has unveiled the potential of augmenting LLMs with external tools, thereby significantly enhancing their problem-solving capacities and efficiencies (Yao et al., 2023; Liu et al., 2023; Parisi et al., 2022; Schick et al., 2023).\nHowever, the applicability of these tool-using methods is largely contingent on the availability of suitable tools. According to the lessons learned from the evolutionary milestones of humans, a crucial turning point was that humans got the ability to fabricate their own tools to address emerging challenges. Inspired by the importance of tool-making for humans, in this work, we embark on an initial exploration to apply this evolutionary concept to the realm of LLMs. We propose a closed-loop framework, which we term as LLMs As Tool Makers (LATM), enables LLMs to generate their own reusable tools to tackle new tasks. Our approach comprises two key stages: 1) tool making: an LLM,\n\u2217Work done as a Student Researcher at Google Deepmind. \u2020Work done as a Visiting Researcher at Google Deepmind.\nknown as the tool maker, designs tools (implemented as Python functions) specifically for a given task. 2) tool using: another LLM referred to as the tool user, which can be the same as the tool maker, applies the tools to handle new requests. The two-stage design allows LATM to allocate jobs in each stage to the most suitable LLM. Specifically, the tool-making process, which requires a high degree of capability, can be assigned to a powerful albeit resource-intensive model (e.g., GPT-4). On the other hand, the tool-using process, which is comparatively simpler, can be assigned to a lightweight and cost-effective model (e.g., GPT-3.5 Turbo). This approach not only enhances the problem-solving capabilities of LLMs, but also significantly reduces the average computational cost of addressing a series of tasks.\nAs the tool-making process needs to be executed only once for a given functionality, the resulting tools can be reused across different task instances. This approach paves the way for a scalable and cost-efficient solution for handling complex task. For instance, consider a task where a user ask the LLM to schedule a meeting that works for everyone (e.g., in email conversations). Lightweight models like GPT-3.5 Turbo often struggle with such tasks that involve complex arithmetic reasoning. In contrast, more powerful models (e.g., GPT-4) can find the correct solutions, despite that the inference costs become much higher. LATM overcomes these hurdles by employing a powerful yet expensive model as the tool maker, and passing it to a cost-effective model as the tool user, for subsequent usage. After the tool has been forged, the lightweight tool user can use it to solve the task efficiently with high performance. This paradigm can similarly be applied to recurring tasks in various workflows, such as parsing and analyzing web documents into specific data formats or formulating routing plans that satisfy several custom requirements, or being used to solve popular games like the 24-game, Sudoku.\nIn the context of serving cost reduction, LATM introduces the opportunity of creating a functional cache for the LLM server. Specifically, consider a streaming setting where the LLM server continuously receives a sequence of requests. Traditional cache systems, such as GPTCache (Zilliz, 2023), store the responses generated by the LLMs and reuse them for textually similar requests. However, with the capacity for tool-making that LATM introduces, the system can store tools crafted by the tool maker and reuse them for functionally analogous requests. This novel approach, combined with the strategic division of labor between the tool maker and tool user, has the potential to considerably reduce the average cost of serving a sequence of requests while maintaining high performance.\nOur experiments validate the effectiveness of this approach on a range of complex reasoning tasks, including several challenging Big-Bench tasks (Srivastava et al., 2022). The results show that LATM can achieve performance on par with more resource-intensive models while being more cost-effective. This novel approach to LLMs, which mimics the evolutionary leap of humans in creating and using tools, opens up exciting possibilities for a growing community with LLM-generated tools."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Chain of thought (CoT). Recently, significant progress has been made in enhancing the problemsolving abilities of large language models (LLMs) for complex tasks. For instance, CoT prompting (Wei et al., 2022; Wang et al., 2022) has been proposed to bolster LLM reasoning capabilities, demonstrating improved performance across various reasoning and natural language processing tasks. CoT is typically articulated through natural languages (Ling et al., 2017; Cobbe et al., 2021; Suzgun et al., 2022; Shi et al., 2022; Zhou et al., 2022), yet it might also be effectively represented using programming languages (Amini et al., 2019; Austin et al., 2021; Nye et al., 2021; Chowdhery et al., 2022; Gao et al., 2023; Chen et al., 2022). More recently, Arora et al. (2023) proposed using LLMs to generate structured views over documents, balancing quality and cost by ensembling extractions from multiple synthesized functions. Our method shares a similar spirit with Arora et al. (2023) in managing cost and quality trade-offs but focuses on more general use cases.\nAugmenting language models with tools. Recent works have explored the potential of using external tools to supplement LLMs\u2019 capabilities for complex tasks. Yao et al. (2023); Yang et al. (2023) proposed augmenting reasoning traces with task-specific actions in LLMs, enabling models to reason and act synergistically. Various studies (Liu et al., 2023; Parisi et al., 2022; Schick et al., 2023; Shen et al., 2023; Lu et al., 2023; Paranjape et al., 2023; Liang et al., 2023) have demonstrated that supplementing LLMs with tools, such as calculators, search engines, translation systems, calendars, or even API calls on other models, can help solve tasks that are not easily addressed by LLMs alone.\nSimilar to LATM, methods like Chameleon (Lu et al., 2023) also incorporate Python executors in the pipeline. However, their primary focus is on using Python executors to accurately solve sub-steps involving arithmetic reasoning, similar to Gao et al. (2023); Chen et al. (2022). In contrast, we use Python executors to create reusable tools for addressing other task instances. Furthermore, the separation of the tool maker and tool user enables the use of a lightweight model for most inferences, thus enhancing efficiency and cost-effectiveness in LATM.\nAdaptive generation in language models. In addition, recent research has proposed methods to adaptively control decoding in LLMs to improve text generation efficiency (Leviathan et al., 2022; Chen et al., 2023a; Xia et al., 2023). Speculative decoding is based on the notion that generating text tokens (a more expensive process) can be expedited with a faster yet less powerful model while approximating the performance of larger, costlier models by using them to score generated tokens (a much faster process). Our approach of passing tools from a more expensive model to a smaller, faster model also shares a similar spirit of adaptive computing. Instead of altering the decoding procedure, we transfer newly generated tools between models to boost both the performance and efficiency of an LLM in solving tasks.\nLanguage model cascades. There is recent evidence that LLMs can enable repeated interactions and that multiple LLMs can be combined to extend their capabilities further (Wu et al., 2022; Zhou et al., 2022; Dohan et al., 2022; Chen et al., 2023c). Also, Chen et al. (2023b) demonstrated that identifying optimal LLM combinations can help reduce costs while improving accuracy. Our motivation aligns with these findings; however, rather than merely cascading LLMs, we identify task categories that can be better addressed using new tools generated by a larger model and assign each individual inference within that task category to a smaller model.\nEarly attempts on tool-making. Concurrent and independent to our work, several early attempts have been made towards using LLMs to make tools. Wang et al. (2023) conducted research within the Minecraft environment and demonstrated the ability of an LLM-powered agent to acquire new skills in the form of programs. Similarly, Qian et al. (2023) proposes a method of decomposing problem-solving for each individual instance into an abstract tool creation phase and a concrete tool\napplication phase. Our work aligns with the spirit of both Wang et al. (2023) and Qian et al. (2023) in the aim to let LLMs to generate their own tools for problem-solving. However, we also underscore the significance of tool reusability and cost-effectiveness stemming from the division of labor. The idea of tool making is also mentioned in a recent survey paper (Qin et al., 2023)."
        },
        {
            "heading": "3 LLM AS TOOL MAKER (LATM)",
            "text": ""
        },
        {
            "heading": "3.1 MAKING NEW TOOLS AND REUSE THEM",
            "text": "In the LATM paradigm, the main process can be split into two stages: Tool Making and Tool Using. Each stage utilizes different types of Large Language Models (LLMs) to balance performance and cost-effectiveness. All the prompts used in our experiments are shown in Appendix C.\nTool Making. This stage employs a powerful yet more expensive model, such as GPT-4, to serve as the tool maker. Tool maker\u2019s role is to create a generic and reusable tool (implemented as a Python function) from a few demonstrations of a task. This stage can be further divided into three sub-stages:\n\u2022 Tool Proposing: In this stage, tool maker attempts to generate a Python function to solve the demonstrations from the given task. This process follows the \u201cprogramming by example\u201d (PbE)\nparadigm (Halbert, 1984) where several concrete demonstrations are provided, and the model is required to write programs that produce the demonstrated behaviors. In our experiments, we use 3 demonstrations for this stage. If the proposed tool is unexecutable or encounters errors, tool maker appends the error messages to the history and makes another attempt.\n\u2022 Tool Verification: In this stage, the tool maker generates unit tests using validation samples and subsequently executes these tests on the proposed tool. We utilize 3 validation samples in our experiments. If the tool fails any of these tests, the tool maker records the error in its history and makes an attempt to rectify the issues within the unit tests (this procedure will only correct the function calls in the unit test part and will not correct the function). The ability of LLMs to self-debug has been demonstrated effectively in recent research (Madaan et al., 2023; Chen et al., 2023c; Lu et al., 2023; Kim et al., 2023). However, within the LATM pipeline, the verification stage serves a slightly different usage. This stage fulfills two key roles: 1) it provides examples that demonstrate how to convert natural language questions into function calls, and 2) it verifies the tool\u2019s reliability, enabling the entire process to be fully automated.\n\u2022 Tool Wrapping: If the execution or verification fails over a preset threshold, the Tool Making stage is viewed as failed. Otherwise, tool maker is ready to prepare the wrapped tool for tool user. This step involves wrapping up the function code and providing demonstrations of how to convert a task into a function call. These demonstrations are extracted from the Tool Verification step, which converts questions into unit tests. This final product is then ready for use by the tool user. Please see Appendix D for examples of the wrapped tools.\nTool Using. This second stage involves a lightweight and cost-effective model, such as GPT-3.5 Turbo, to serve as the tool user. The tool user\u2019s role is to utilize the verified tool to solve various instances of the task. The prompt for this stage is the wrapped tool which contains the function for solving the task and demonstrations of how to convert a task query into a function call. With the demonstrations, tool user can then generate the required function call in an in-context learning fashion. The function calls are then executed to solve the task. Optionally, postprocessing can be applied to convert the output to match the required format of the task, such as options for multiple-choice questions.\nThe tool-making stage, including tool proposing, verification, and wrapping, only needs to be performed once for each type of task. The resulting tools can then be reused for all instances of that task. This makes LATM significantly more efficient and cost-effective than using a powerful model alone. Furthermore, the Python function tools are a more generic form of Chain-of-Thought, enhancing the overall utility and flexibility of the LLMs, as they can be used to solve questions that involve algorithmic reasoning ability (Velic\u030ckovic\u0301 and Blundell, 2021)."
        },
        {
            "heading": "4 LATM FOSTERS A FUNCTIONAL CACHE MECHANISM FOR LLM SERVING",
            "text": "In real-world scenarios, tasks often arrive in a sequential stream. To address this, we introduce a third LLM, the dispatcher, that decides whether to engage the tool user or tool maker for each incoming task. While this tool selection function mirrors existing works (Lu et al., 2023; Shen et al., 2023; Schick et al., 2023; Paranjape et al., 2023), our dispatcher distinctively contributes to creating a functional cache\u2014it discerns new tasks that cannot be resolved with existing tools, thereby triggering the tool maker to generate appropriate tools for these tasks.\nThe dispatcher maintains a repository of existing tools crafted by the tool maker in the format of function APIs. Upon receipt of a new task instance, the dispatcher first attempts to locate a compatible tool within the cache. If such a tool is present, the dispatcher assigns the instance and corresponding tool to the tool user for resolution. However, if no suitable tool is available, the dispatcher identifies this as a novel task, either solving it with a powerful model or, if necessary, invoking a human labeler. These new instances are then cached until a sufficient number are amassed to craft a new tool, further enriching the functional cache. This mechanism allows for the functionally similar tasks to reuse these tools, expanding the coverage of the classic cache mechanism and reducing the overall serving cost. Given the simplicity of the dispatching task, a lightweight model equipped with appropriate prompts (See Appendix C) can efficiently serve as the dispatcher, adding only a marginal cost to the entire pipeline."
        },
        {
            "heading": "5 EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "5.1 EXPERIMENTAL SETUP",
            "text": "Datasets. We evaluate our approach on six datasets from diverse domains, including Logical Deduction, Tracking Shuffled Objects, Dyck Language, Word Sorting, Chinese Remainder Theorem, and Scheduling Meeting. The first five datasets are sourced from BigBench (Srivastava et al., 2022). We take the 5 objects version of the Logical Deduction and Tracking Shuffled Objects tasks, referred to as Logical Deduction (5) and Tracking Shuffled Objects (5) in the paper. We also constructed the Scheduling Meeting task to demonstrate the effectiveness of LATM in real-world scenarios. Detailed information on dataset generation can be found in Appendix E. We divide each dataset into training, validation, and test sets, containing 3, 3, and 240 instances, respectively.\nModel settings. During the tool-making stage, we set the temperature to 0.3 to introduce randomness to the generation process, allowing for retries if necessary. For this stage, we conduct experiments using GPT-4 and GPT-3.5 Turbo models with the ChatCompletion API, always appending the response to the chat history to create an interactive experience. In the tool-using stage, the LLM API call is made only once, and we also perform ablation studies on GPT-3-type models with the standard Completion API. When using the tools, we consistently set the temperature to 0.0. We set the maximal retry times to be 3 for the tool-proposing and tool-verification stages."
        },
        {
            "heading": "5.2 EFFECTIVENESS OF THE TOOL-MAKING STAGE",
            "text": "In the tool-making stage, we use a powerful yet slower model to generate generic Python functions tailored to a specific task. This step is performed only once for each task, and the overhead is amortized across all instances of that task. In our experiments, we use GPT-4 (OpenAI, 2023) as a\nrepresentative tool maker, while we explore other models\u2019 tool-making capabilities in Section 5.5. We provide several few-shot exemplars for the language model, guiding it to generate generic Python programs, as illustrated in Figure 3.\nOur observations indicate that when GPT-4 is employed as the tool maker, the model frequently devises suitable algorithms for solving tasks. For instance, as shown in Table 1, the tool maker creates code to solve the logical deduction task by searching through all permutations and selecting the correct one that satisfies the given constraints. In our experiment, the tool-verification stage is mainly used to provide examples that demonstrate how to convert natural language questions into function calls, and we only observe 2 cases out of the 60 trials that the tool maker can correct its mistakes with the guide of error messages. See Section 5.5 for more discussions on the tool maker."
        },
        {
            "heading": "5.3 LATM IMPROVES THE PERFORMANCE OF LIGHTWEIGHT LLMS",
            "text": "In Table 2, we compare the performance of Chain-of-Thought prompting (Wei et al., 2022) with our method, LATM. We employ GPT-4 as the tool maker to generate tools for the six tasks, and evaluate the performance of both GPT-3.5 Turbo and GPT-4 as tool user. The results demonstrate that with the help of the tool, a lightweight model like GPT-3.5 Turbo can achieve performance on par with GPT-4, significantly outperforming CoT prompting. Additionally, the average cost of using GPT-3.5 Turbo with the tool is much lower compared to using GPT-4. This highlights the effectiveness of LATM in enhancing the performance of lightweight models and therefore reducing the cost compared to employing expensive models. Intriguingly, for the Dyck Language task, GPT-3.5 Turbo as the tool user even surpasses GPT-4 in its role as the tool user. Upon investigating the failure cases, we find that when converting the question into a function call, GPT-4 occasionally superfluously closes some brackets within the argument instead of leaving the argument unchanged and letting the function solve it, which leads to incorrect function output."
        },
        {
            "heading": "5.4 ADAPTING LATM TO A DYNAMIC STREAM OF DIVERSE TASKS",
            "text": "As discussed in Section 4, we can adapt LATM to handle a dynamic stream where instances from potentially different tasks emerge in real-time. In this setting, we introduce an additional model, the dispatcher, tasked with identifying the task to which each incoming instance pertains. We employ GPT-3.5 Turbo for this role, evaluating its effectiveness in two key functions: 1) Identifying and employing existing tools from the functional cache to resolve an incoming instance, and 2)\nDetecting unseen tasks and triggering the tool maker to create appropriate tools for these tasks. This experimental setup helps assess how effectively our system can reduce serving costs by reusing and extending the functional cache in a dynamic, multi-tasking scenario.\nIdentifying existing tools. The first part of our evaluation assesses the dispatcher\u2019s capability to recognize existing tools within the functional cache that correspond to a given instance, analogous to the cache fetching phase of traditional cache systems. To this end, we generate a test set of 100 samples, randomly mixed from the six tasks discussed in Section 5.1. For each instance, the dispatcher is tasked to determine the appropriate tool from existing ones, utilizing prompts containing task examples associated with these tools (See Appendix C). Success is measured by the correct identification of the tool. Over five random constructions of the test set, the accuracy in correctly determining the suitable tool is 95%\u00b1 2%.\nRequesting tool-making. The second part of our evaluation tests the dispatcher\u2019s ability to request tool-making for instances originating from an unseen task. This situation is akin to enqueuing a new instance into the cache when a cache miss happens. We randomly designate four tasks as existing tasks with readily available tools and select four other tasks for testing\u2014two of these are unseen, and the other two fall within the realm of existing tasks. Again, a test set of 100 samples is generated. For each instance in the test set, the dispatcher determines whether it needs to request tool-making or if an existing tool can solve the instance. Over multiple runs, the accuracy of making the correct decision stands at 96%\u00b1 3%, demonstrating the robustness of our approach in efficiently managing the functional cache.\nThe above results illustrate that the dispatcher can effectively recognize existing tools and accurately request tool-making for unseen tasks, all while maintaining high performance. These findings highlight the potential of LATM to be seamlessly adapted to a streaming environment encompassing a diverse range of tasks. This validation serves to fortify the viability of our framework in real-world applications, particularly where the efficient management of functional cache is paramount."
        },
        {
            "heading": "5.5 ABLATION STUDY",
            "text": "Capacity required for the tool-making language model. We investigate the capacity requirements for the language model used in the tool-making stage (See Table 3). Generally, we found that a more powerful and expensive model better serves the purpose, as this stage is performed only once for each task, and high accuracy is crucial for effectively passing tools to a smaller model. Specifically, on hard tasks like Logical Deduction and Tracking Shuffled Objects, GPT-3.5 Turbo fails in all the 5 trails. And the major failure reason is that the tool is not general enough and may only work on the training samples. On the other hand, we also discovered that for easy tasks, the tool maker can be a lightweight language model. For simple tasks like Word Sorting, GPT-3.5 Turbo can effortlessly generate a program that solves the task. Another limitation that may contribute to the tool maker\u2019s failure is the context length constraints. Since we use the entire history in each step of tool-making to enhance the reliability of the tool-making stage, this also introduces a longer context. In this case GPT-4 with 8192 context length is preferable.\nCapacity required for the tool-using language model. In this section, we investigate the capacity requirements for the tool-using model. The results are presented in Table 4. We observed that GPT-3.5\nTurbo offers the best balance between performance and cost among all the models tested. Regarding the older GPT-3 series of models (ada, babbage, curie, davinci), we found that models that before instruction tuning often perform better than their counterparts post instruction tuning (text-ada-001, etc.). We hypothesize that the instruction tuning phase in these models may adversely impact the in-context learning ability, which is crucial for the tool-using stage.\nCoT as a tool does not help. In addition to LATM, we investigate if we can improve task performance by reusing Chain-of-Thought (CoT) from a larger model to a smaller model similar to LATM pipeline. Specifically, we use the same larger model (GPT-4) in the \u201cCoT-making\u201d stage, using zero-shot prompting \u201cLet\u2019s think step by step.\u201d to elicit the intermediate thought steps, and then use the generated CoT to the same smaller tool-using model (GPT-3.5 Turbo). We test this on two tasks and report the results Table 5. We observe that using CoT from a large model has a similar or even worse performance than human-written CoT, which is much worse than LATM."
        },
        {
            "heading": "6 CONCLUSION AND FUTURE WORK",
            "text": "We introduced LATM, a closed-loop framework empowering large language models (LLMs) to create and utilize their own tools for diverse tasks. Our approach, inspired by human\u2019s evolutionary strides in tool creation, employs two key stages: Tool Making and Tool Using. This division of labor allows us to harness the capabilities of advanced LLMs while significantly reducing computational costs. Our experiments confirmed the efficacy of LATM across various complex tasks, demonstrating that our framework performs comparably to resource-intensive models while being more cost-effective. In addition, we show that adding another dispatcher LLM can further provide flexibility to our framework, enabling on-the-fly tool creation and usage.\nIn our evaluation process, we identified a significant lack of high-quality datasets that authentically represent daily human-computer interactions, including recurring tasks such as scheduling meetings or booking flights over email or phone calls, in their raw natural language format. We anticipate that our work will stimulate the research community to create such datasets, which could prove instrumental in cultivating the next generation of AI systems. These systems, capable of generating and applying their own tools, will be equipped to tackle complex tasks more effectively. An exciting avenue for future research is enabling the tool maker to refine and upgrade existing tools to manage new problem instances, much like in software development. This adaptability could further catalyze the evolution of the AI ecosystem, unlocking a wealth of opportunities."
        },
        {
            "heading": "B BROADER IMPACT AND LIMITATIONS",
            "text": "This paper explores the potential of enabling Large Language Models (LLMs) to create their own tools, thus allowing them greater autonomy in developing their ecosystem. While this avenue of research is promising, it also raises important ethical, safety, and control considerations that need to be carefully addressed.\nOne of the most significant impacts of our work lies in the potential for LLMs to grow and achieve unprecedented capabilities automatically. This could significantly enhance the range and complexity of tasks these models can handle, potentially revolutionizing fields such as customer service, technical support, and even areas of research and development. It could lead to more efficient use of computational resources and a reduction in human intervention, especially for routine or repetitive tasks.\nHowever, this newfound autonomy of LLMs is a double-edged sword. As we endow LLMs with the ability to generate their own tools, we also create a scenario where the quality of the tools they develop may not always meet the standards or expectations set by human developers. Without proper safeguards, there\u2019s a risk that these models could generate solutions that are suboptimal, incorrect, or even potentially harmful. Furthermore, as LLMs become more autonomous, the potential for loss of control increases. If these models are widely used without appropriate regulation, there could be unforeseen consequences, potentially even leading to scenarios where humans lose control over the AI systems.\nIn this study, we have not addressed these control and safety issues in depth, and our work has some limitations. Our proposed framework, LLM As Tool Maker, while effective in the tested scenarios, is still in its early stages of development. It is crucial to note that the real-world performance and safety of the system may vary based on the complexity and nature of the tasks it is applied to. Additionally, the evaluation and validation of the tools created by the tool maker in a real-world setting is a challenge that needs to be addressed."
        },
        {
            "heading": "C LATM PROMPTS",
            "text": "Tool Maker Prompt\nPlease write a generic Python function to solve this type of problems using only standard python libraries. The output of the function can later be converted to the answer (option for multiple choice question). All the function should be wrapped by \u21aa\u2192 \u21aa\u2192 \u21aa\u2192 \u21aa\u2192 ```python ```\nTool Verifier Prompt\nWrite unit tests to verify the correctness of the function on the questions above using the following format:\u21aa\u2192 ```python {parse the question into the arguments of the function} {call the function and save the return value in a variable\nnamed \"ret\"}\u21aa\u2192 {for multiple choice question, parse the options} {convert the return value \"ret\" to the answer (if the\nquestion is a multiple choice question, convert to an option) and save it in a variable named \"ans\", otherwise} \u21aa\u2192 \u21aa\u2192\n{assert ans == the provided answer (if the question is a multiple choice question, assert ans == option)}\u21aa\u2192 ```\nTool Wrapper Prompt\nSuccess! The function is correct. We will need to summarize the function and use cases up for further use. Please extract the information from the history in the following format: \u21aa\u2192 \u21aa\u2192 \u21aa\u2192\nHere is a function to solve a class of problems: ```python {the function, including necessary imports} ```\nUse cases: Question: {question (including options)} Solution: ```python {parse the question into the arguments of the function} {call the function and save the return value in a variable\nnamed \"ret\"}\u21aa\u2192 {for multiple choice question, parse the options} {convert the return value \"ret\" to the answer (if the\nquestion is a multiple choice question, convert to an option) and save it in a variable named \"ans\", otherwise} \u21aa\u2192 \u21aa\u2192\n``` Do this for all the questions in the verification step.\nDispatcher Prompt\nHere are several functions that can be used to solve some task:\u21aa\u2192\nTask: logical_deduction_five_objects\nAPI: find_order(objects, constraints): Finds the order of objects that satisfies a given set of\nconstraints.\u21aa\u2192 objects: A list of unique objects (strings) to be ordered. constraints: A list of lambda functions that represent the\nconstraints on the order of objects. Each constraint should take the order of objects as input and return a boolean value (True if the constraint is satisfied, False otherwise). \u21aa\u2192 \u21aa\u2192 \u21aa\u2192 \u21aa\u2192\nreturn: A tuple representing the order of objects that satisfies all the constraints. If no such order exists, the function returns None. \u21aa\u2192 \u21aa\u2192\n===\nTask: tracking_shuffled_objects_five_objects\nAPI: square_dance(initial_partners, switches): This function takes an initial list of pairs and a list of\nswitches, and returns a dictionary representing the final state of the pairs after performing the switches. \u21aa\u2192 \u21aa\u2192\ninitial_partners: A list of tuples, where each tuple contains two elements representing a pair (e.g., [(\"Alice\", \"goalkeeper\"), (\"Bob\", \"left midfielder\"), ...]). The elements can be any type (e.g., strings, integers, etc.). \u21aa\u2192 \u21aa\u2192 \u21aa\u2192\nswitches: A list of tuples, where each tuple contains two elements representing a pair of elements from the initial_partners list that will be switched (e.g., [(\"Alice\", \"Claire\"), (\"Alice\", \"Bob\"), ...]). The elements should match the types used in the initial_partners list. \u21aa\u2192 \u21aa\u2192 \u21aa\u2192 \u21aa\u2192 \u21aa\u2192\nreturn: A dictionary representing the final state of the pairs after performing the switches. The keys are the first elements of the pairs in the initial_partners list, and the values are the corresponding second elements after performing the switches (e.g., {\"Alice\": \"right winger\", \"Bob\": \"center midfielder\", ...}). \u21aa\u2192 \u21aa\u2192 \u21aa\u2192 \u21aa\u2192 \u21aa\u2192\n===\nSkip other tasks\nHere is a question:\\n{question}\\n\\nAccoding to the API documents above, you may find some functions that can be used to solve the task, or, sometimes there does not exist proper function to solve the task. Figure out if there is function to solve the task and reply in the format:\\nTask: {{task}} (reply unknown if no function can solve the question) \u21aa\u2192 \u21aa\u2192 \u21aa\u2192 \u21aa\u2192 \u21aa\u2192 \u21aa\u2192"
        },
        {
            "heading": "D WRAPPED TOOLS",
            "text": "Tool for Logical Deduction\nHere is a function to solve a class of problems:\n```python from itertools import permutations\ndef find_order(objects, constraints): for order in permutations(objects):\nvalid = True for constraint in constraints:\nif not constraint(order): valid = False break\nif valid: return order\n```\nUse cases:\nQuestion: The following paragraphs each describe a set of five objects arranged in a fixed order. The statements are logically consistent within each paragraph. On a shelf, there are five books: a white book, a green book, a brown book, a gray book, and an orange book. The gray book is to the right of the orange book. The green book is the second from the right. The brown book is to the right of the white book. The brown book is to the left of the orange book. \u21aa\u2192 \u21aa\u2192 \u21aa\u2192 \u21aa\u2192 \u21aa\u2192 \u21aa\u2192 \u21aa\u2192 \u21aa\u2192\nOptions: (A) The white book is the third from the left (B) The green book is the third from the left (C) The brown book is the third from the left (D) The gray book is the third from the left (E) The orange book is the third from the left Solution:\n```python objects = [\"white\", \"green\", \"brown\", \"gray\", \"orange\"]\nconstraints = [ lambda order: order.index(\"gray\") >\norder.index(\"orange\"),\u21aa\u2192 lambda order: order.index(\"green\") == len(order) - 2, lambda order: order.index(\"brown\") >\norder.index(\"white\"),\u21aa\u2192 lambda order: order.index(\"brown\") <\norder.index(\"orange\")\u21aa\u2192 ]\nret = find_order(objects, constraints) options = {\n\"A\": \"white\", \"B\": \"green\", \"C\": \"brown\", \"D\": \"gray\", \"E\": \"orange\"\n} ans = [k for k, v in options.items() if v == ret[2]][0] ``` Skip two more questions... 17\nTool for Tracking Shuffled Objects\nHere is a function to solve a class of problems:\n```python def square_dance(initial_partners, switches):\n# Create a dictionary to store the current partners current_partners = dict(initial_partners)\n# Iterate through the switches and update the current partners\u21aa\u2192\nfor switch in switches: dancer1, dancer2 = switch partner1 = current_partners[dancer1] partner2 = current_partners[dancer2]\n# Swap the partners current_partners[dancer1] = partner2 current_partners[dancer2] = partner1\nreturn current_partners ```\nUse cases:\nQuestion: Alice, Bob, Claire, Dave, and Eve are on the same team in a soccer match. At the start of the match, they are each assigned to a position: Alice is playing goalkeeper, Bob is playing left midfielder, Claire is playing right winger, Dave is playing striker, and Eve is playing center midfielder. \u21aa\u2192 \u21aa\u2192 \u21aa\u2192 \u21aa\u2192 \u21aa\u2192\nAs the game progresses, pairs of players occasionally swap positions. First, Alice and Claire trade positions. Then, Alice and Bob trade positions. Then, Dave and Bob trade positions. Then, Bob and Eve trade positions. Finally, Dave and Eve trade positions. At the end of the match, Eve is playing \u21aa\u2192 \u21aa\u2192 \u21aa\u2192 \u21aa\u2192 \u21aa\u2192\nOptions: (A) goalkeeper (B) left midfielder (C) right winger (D) striker (E) center midfielder Answer: (C)\nSolution: ```python initial_positions = [(\"Alice\", \"goalkeeper\"), (\"Bob\", \"left\nmidfielder\"), (\"Claire\", \"right winger\"), (\"Dave\", \"striker\"), (\"Eve\", \"center midfielder\")] \u21aa\u2192 \u21aa\u2192\nswitches = [(\"Alice\", \"Claire\"), (\"Alice\", \"Bob\"), (\"Dave\", \"Bob\"), (\"Bob\", \"Eve\"), (\"Dave\", \"Eve\")]\u21aa\u2192\nret = square_dance(initial_positions, switches) options = [\"goalkeeper\", \"left midfielder\", \"right winger\",\n\"striker\", \"center midfielder\"]\u21aa\u2192 ans = options.index(ret[\"Eve\"]) + 1 # Convert the return\nvalue to an option index (1-based)\u21aa\u2192 ``` Skip two more questions...\nTool for Dyck Language\nHere is a function to solve a class of problems:\n```python def complete_sequence(input_str):\nstack = [] closing_map = {'(': ')', '[': ']', '<': '>', '{': '}'} result = []\nfor char in input_str: if char in closing_map.keys():\nstack.append(char) elif char in closing_map.values():\nif stack and closing_map[stack[-1]] == char: stack.pop()\nelse: return \"Invalid sequence\"\nelse: return \"Invalid character\"\nwhile stack: result.append(closing_map[stack[-1]]) stack.pop()\nreturn ''.join(result) ```\nUse cases:\nQuestion: Complete the rest of the sequence, making sure that the parentheses are closed properly. Input: ([[[{}]]{<[<[{}]>]>} \u21aa\u2192 \u21aa\u2192\nAnswer: ])\nSolution: ```python input_str = \"([[[{}]]{<[<[{}]>]>}\" ret = complete_sequence(input_str) ans = ret ``` Skip two more questions...\nTool for Word Sorting\nHere is a function to solve a class of problems: ```python def sort_words_alphabetically(word_list):\nreturn sorted(word_list) ```\nUse cases:\nQuestion: Sort the following words alphabetically: List: conference apparition ignore dutton layperson coupe superstitious westward turnoff messenger copra floruit primitive implement \u21aa\u2192 \u21aa\u2192 \u21aa\u2192\nAnswer: apparition conference copra coupe dutton floruit ignore implement layperson messenger primitive superstitious turnoff westward \u21aa\u2192 \u21aa\u2192\nSolution: ```python words1 = [\"conference\", \"apparition\", \"ignore\", \"dutton\",\n\"layperson\", \"coupe\", \"superstitious\", \"westward\", \"turnoff\", \"messenger\", \"copra\", \"floruit\", \"primitive\", \"implement\"] \u21aa\u2192 \u21aa\u2192 \u21aa\u2192\nret1 = sort_words_alphabetically(words1) ans1 = \" \".join(ret1) ``` Skip two more questions...\nTool for Chinese Remainder Theorem\nHere is a function to solve a class of problems:\n```python def find_number(max_limit, divisors, remainders):\nfor num in range(max_limit + 1): if all((num - remainder) % divisor == 0 for divisor,\nremainder in zip(divisors, remainders)):\u21aa\u2192 return num\nreturn None ```\nUse cases:\nQuestion: There is a basket of no more than 1188877 durians. If we divide them equally among 41 penguins, we have 17 left; if we divide them equally among 107 dinosaurs, we have 42 left; if we divide them equally among 271 elephants, we have 260 left. How many durians are in the basket? \u21aa\u2192 \u21aa\u2192 \u21aa\u2192 \u21aa\u2192 \u21aa\u2192\nSolution: ```python max_limit = 1188877 divisors = [41, 107, 271] remainders = [17, 42, 260] ret = find_number(max_limit, divisors, remainders) ans = ret ``` Skip two more questions...\nTool for Schedule Meeting\nHere is a function to solve a class of problems:\n```python from datetime import datetime, timedelta\ndef find_earliest_time_slot(a_availability, b_availability, meeting_duration):\u21aa\u2192 a_availability = [(datetime.strptime(start, '%H:%M'),\ndatetime.strptime(end, '%H:%M')) for start, end in a_availability] \u21aa\u2192 \u21aa\u2192\nb_availability = [(datetime.strptime(start, '%H:%M'), datetime.strptime(end, '%H:%M')) for start, end in b_availability] \u21aa\u2192 \u21aa\u2192\nfor a_start, a_end in a_availability: for b_start, b_end in b_availability:\nlatest_start = max(a_start, b_start) earliest_end = min(a_end, b_end)\nif earliest_end - latest_start >= timedelta(minutes=meeting_duration):\u21aa\u2192 return latest_start.strftime('%H:%M'),\n(latest_start + timedelta(minutes=meeting_duration)).strftime('%H:%M') \u21aa\u2192 \u21aa\u2192\nreturn None ```\nUse cases: Question: A and B want to schedule a 1-hour meeting together.\nA's availability: 12:00 - 12:30, 13:00 - 13:30, 14:30 - 15:30, 17:30 - 18:00. B's availability: 09:00 - 11:00, 12:00 - 12:30, 13:00 - 13:30, 15:30 - 16:30, 17:30 - 18:00. What time slot works best? (if multiple, choose the earliest one) \u21aa\u2192 \u21aa\u2192 \u21aa\u2192 \u21aa\u2192 \u21aa\u2192\nAnswer: No time slot works.\nSolution: ```python a_availability = [('12:00', '12:30'), ('13:00', '13:30'),\n('14:30', '15:30'), ('17:30', '18:00')]\u21aa\u2192 b_availability = [('09:00', '11:00'), ('12:00', '12:30'),\n('13:00', '13:30'), ('15:30', '16:30'), ('17:30', '18:00')] \u21aa\u2192 \u21aa\u2192\nmeeting_duration = 60\nret = find_earliest_time_slot(a_availability, b_availability, meeting_duration)\u21aa\u2192\nans = ret if ret else \"No time slot works.\" ``` Skip two more questions..."
        },
        {
            "heading": "E DATASET CONSTRUCTION",
            "text": "For the \u201cschedule meeting\u201d task, we use the following template to generate the dataset:\nquestion_format = \"\"\"A and B want to schedule a {interval}-hour meeting together.\u21aa\u2192\nA's availability: {A_availability} B's availability: {B_availability} What time slot works best? (if multiple, choose the earliest\none)\"\"\"\u21aa\u2192\nwhere the interval is randomly sampled from {0.5, 1, 1.5}, and the availability of A and B are randomly sampled from 8:00-18:00 with 30 minutes as the granularity. The answer is computed by computing the intersection of the two availability sets and then find the earliest time slot that is at least as long as the meeting duration. If there is no such time slot, we return \u201cNo time slot works.\u201d."
        }
    ],
    "year": 2024
}