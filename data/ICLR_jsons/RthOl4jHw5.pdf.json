{
    "abstractText": "We investigate the problem of transferring an expert policy from a source robot to multiple different robots. To solve this problem, we propose a method named Meta-Evolve that uses continuous robot evolution to efficiently transfer the policy to each target robot through a set of tree-structured evolutionary robot sequences. The robot evolution tree allows the robot evolution paths to be shared, so our approach can significantly outperform naive one-to-one policy transfer. We present a heuristic approach to determine an optimized robot evolution tree. Experiments have shown that our method is able to improve the efficiency of one-to-three transfer of manipulation policy by up to 3.2\u00d7 and one-to-six transfer of agile locomotion policy by 2.4\u00d7 in terms of simulation cost over the baseline of launching multiple independent one-to-one policy transfers. Supplementary videos available at the project website: https://sites.google.com/view/meta-evolve.",
    "authors": [
        {
            "affiliations": [],
            "name": "Xingyu Liu"
        },
        {
            "affiliations": [],
            "name": "Deepak Pathak"
        },
        {
            "affiliations": [],
            "name": "Ding Zhao"
        }
    ],
    "id": "SP:c9e390ae6753024089aea967d9d6579c29899b64",
    "references": [
        {
            "authors": [
                "Haitham Bou Ammar",
                "Eric Eaton",
                "Paul Ruvolo",
                "Matthew E Taylor"
            ],
            "title": "Unsupervised cross-domain transfer in policy gradient reinforcement learning via manifold alignment",
            "venue": "In Twenty-Ninth AAAI Conference on Artificial Intelligence,",
            "year": 2015
        },
        {
            "authors": [
                "Yu-Wei Chao",
                "Wei Yang",
                "Yu Xiang",
                "Pavlo Molchanov",
                "Ankur Handa",
                "Jonathan Tremblay",
                "Yashraj S Narang",
                "Karl Van Wyk",
                "Umar Iqbal",
                "Stan Birchfield"
            ],
            "title": "Dexycb: A benchmark for capturing hand grasping of objects",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "David Cheriton",
                "Robert Endre Tarjan"
            ],
            "title": "Finding minimum spanning trees",
            "venue": "SIAM journal on computing,",
            "year": 1976
        },
        {
            "authors": [
                "Erwin Coumans",
                "Yunfei Bai"
            ],
            "title": "Pybullet, a python module for physics simulation for games, robotics and machine learning, 2016",
            "year": 2016
        },
        {
            "authors": [
                "Dima Damen",
                "Hazel Doughty",
                "Giovanni Maria Farinella",
                "Sanja Fidler",
                "Antonino Furnari",
                "Evangelos Kazakos",
                "Davide Moltisanti",
                "Jonathan Munro",
                "Toby Perrett",
                "Will Price"
            ],
            "title": "Scaling egocentric vision: The epic-kitchens dataset",
            "venue": "In Proceedings of the European Conference on Computer Vision (ECCV),",
            "year": 2018
        },
        {
            "authors": [
                "Yan Duan",
                "Marcin Andrychowicz",
                "Bradly Stadie",
                "OpenAI Jonathan Ho",
                "Jonas Schneider",
                "Ilya Sutskever",
                "Pieter Abbeel",
                "Wojciech Zaremba"
            ],
            "title": "One-shot imitation learning",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Marcia Fampa",
                "Jon Lee",
                "Nelson Maculan"
            ],
            "title": "An overview of exact algorithms for the euclidean steiner tree problem in n-space",
            "venue": "International Transactions in Operational Research,",
            "year": 2016
        },
        {
            "authors": [
                "Chelsea Finn",
                "Pieter Abbeel",
                "Sergey Levine"
            ],
            "title": "Model-agnostic meta-learning for fast adaptation of deep networks",
            "venue": "In International conference on machine learning,",
            "year": 2017
        },
        {
            "authors": [
                "Chelsea Finn",
                "Kelvin Xu",
                "Sergey Levine"
            ],
            "title": "Probabilistic model-agnostic meta-learning",
            "venue": "Advances in neural information processing systems,",
            "year": 2018
        },
        {
            "authors": [
                "Edgar N Gilbert",
                "Henry O Pollak"
            ],
            "title": "Steiner minimal trees",
            "venue": "SIAM Journal on Applied Mathematics,",
            "year": 1968
        },
        {
            "authors": [
                "Kristen Grauman",
                "Andrew Westbury",
                "Eugene Byrne",
                "Zachary Chavis",
                "Antonino Furnari",
                "Rohit Girdhar",
                "Jackson Hamburger",
                "Hao Jiang",
                "Miao Liu",
                "Xingyu Liu"
            ],
            "title": "Ego4d: Around the world in 3,000 hours of egocentric video",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Abhishek Gupta",
                "Coline Devin",
                "YuXuan Liu",
                "Pieter Abbeel",
                "Sergey Levine"
            ],
            "title": "Learning invariant feature spaces to transfer skills with reinforcement learning",
            "venue": "arXiv preprint arXiv:1703.02949,",
            "year": 2017
        },
        {
            "authors": [
                "Agrim Gupta",
                "Linxi Fan",
                "Surya Ganguli",
                "Li Fei-Fei"
            ],
            "title": "Metamorph: Learning universal controllers with transformers",
            "venue": "arXiv preprint arXiv:2203.11931,",
            "year": 2022
        },
        {
            "authors": [
                "Donald Hejna",
                "Lerrel Pinto",
                "Pieter Abbeel"
            ],
            "title": "Hierarchically decoupled imitation for morphological transfer",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Jonathan Ho",
                "Stefano Ermon"
            ],
            "title": "Generative adversarial imitation learning",
            "venue": "Advances in neural information processing systems,",
            "year": 2016
        },
        {
            "authors": [
                "Sunghoon Hong",
                "Deunsol Yoon",
                "Kee-Eung Kim"
            ],
            "title": "Structure-aware transformer policy for inhomogeneous multi-task reinforcement learning",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Wenlong Huang",
                "Igor Mordatch",
                "Deepak Pathak"
            ],
            "title": "One policy to control them all: Shared modular policies for agent-agnostic control",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Oussama Khatib"
            ],
            "title": "A unified approach for motion and force control of robot manipulators: The operational space formulation",
            "venue": "IEEE Journal on Robotics and Automation,",
            "year": 1987
        },
        {
            "authors": [
                "Pradeep K Khosla",
                "Takeo Kanade"
            ],
            "title": "Parameter identification of robot dynamics",
            "venue": "24th IEEE conference on decision and control,",
            "year": 1985
        },
        {
            "authors": [
                "George Konidaris",
                "Andrew Barto"
            ],
            "title": "Autonomous shaping: Knowledge transfer in reinforcement learning",
            "venue": "In Proceedings of the 23rd international conference on Machine learning,",
            "year": 2006
        },
        {
            "authors": [
                "Vikash Kumar",
                "Zhe Xu",
                "Emanuel Todorov"
            ],
            "title": "Fast, strong and compliant pneumatic actuation for dexterous tendon-driven hands",
            "venue": "IEEE international conference on robotics and automation,",
            "year": 2013
        },
        {
            "authors": [
                "Fangchen Liu",
                "Zhan Ling",
                "Tongzhou Mu",
                "Hao Su"
            ],
            "title": "State alignment-based imitation learning",
            "venue": "arXiv preprint arXiv:1911.10947,",
            "year": 2019
        },
        {
            "authors": [
                "Xingyu Liu",
                "Deepak Pathak",
                "Kris M. Kitani"
            ],
            "title": "HERD: Continuous Human-to-Robot Evolution for Learning from Human Demonstration",
            "venue": "In The Conference on Robot Learning (CoRL),",
            "year": 2022
        },
        {
            "authors": [
                "Xingyu Liu",
                "Deepak Pathak",
                "Kris M. Kitani"
            ],
            "title": "REvolveR: Continuous Evolutionary Models for Robot-to-robot Policy Transfer",
            "venue": "In The International Conference on Machine Learning (ICML),",
            "year": 2022
        },
        {
            "authors": [
                "Viktor Makoviychuk",
                "Lukasz Wawrzyniak",
                "Yunrong Guo",
                "Michelle Lu",
                "Kier Storey",
                "Miles Macklin",
                "David Hoeller",
                "Nikita Rudin",
                "Arthur Allshire",
                "Ankur Handa"
            ],
            "title": "Isaac gym: High performance gpu-based physics simulation for robot learning",
            "venue": "arXiv preprint arXiv:2108.10470,",
            "year": 2021
        },
        {
            "authors": [
                "Mayank Mittal",
                "Calvin Yu",
                "Qinxi Yu",
                "Jingzhou Liu",
                "Nikita Rudin",
                "David Hoeller",
                "Jia Lin Yuan",
                "Ritvik Singh",
                "Yunrong Guo",
                "Hammad Mazhar"
            ],
            "title": "Orbit: A unified simulation framework for interactive robot learning environments",
            "venue": "IEEE Robotics and Automation Letters,",
            "year": 2023
        },
        {
            "authors": [
                "Anusha Nagabandi",
                "Ignasi Clavera",
                "Simin Liu",
                "Ronald S Fearing",
                "Pieter Abbeel",
                "Sergey Levine",
                "Chelsea Finn"
            ],
            "title": "Learning to adapt in dynamic, real-world environments through meta-reinforcement learning",
            "venue": "arXiv preprint arXiv:1803.11347,",
            "year": 2018
        },
        {
            "authors": [
                "Andrew Y Ng",
                "Daishi Harada",
                "Stuart Russell"
            ],
            "title": "Policy invariance under reward transformations: Theory and application to reward shaping",
            "venue": "In Icml,",
            "year": 1999
        },
        {
            "authors": [
                "Andrew Y Ng",
                "Stuart J Russell"
            ],
            "title": "Algorithms for inverse reinforcement learning",
            "venue": "In Icml,",
            "year": 2000
        },
        {
            "authors": [
                "Adam Paszke",
                "Sam Gross",
                "Francisco Massa",
                "Adam Lerer",
                "James Bradbury",
                "Gregory Chanan",
                "Trevor Killeen",
                "Zeming Lin",
                "Natalia Gimelshein",
                "Luca Antiga"
            ],
            "title": "Pytorch: An imperative style, high-performance deep learning library",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Deepak Pathak",
                "Chris Lu",
                "Trevor Darrell",
                "Phillip Isola",
                "Alexei A Efros"
            ],
            "title": "Learning to control self-assembling morphologies: a study of generalization via modularity",
            "venue": "NeurIPS,",
            "year": 2019
        },
        {
            "authors": [
                "Xue Bin Peng",
                "Marcin Andrychowicz",
                "Wojciech Zaremba",
                "Pieter Abbeel"
            ],
            "title": "Sim-to-real transfer of robotic control with dynamics randomization",
            "venue": "IEEE international conference on robotics and automation (ICRA),",
            "year": 2018
        },
        {
            "authors": [
                "Ilija Radosavovic",
                "Xiaolong Wang",
                "Lerrel Pinto",
                "Jitendra Malik"
            ],
            "title": "State-only imitation learning for dexterous manipulation",
            "venue": "arXiv preprint arXiv:2004.04650,",
            "year": 2020
        },
        {
            "authors": [
                "Aravind Rajeswaran",
                "Kendall Lowrey",
                "Emanuel Todorov",
                "Sham Kakade"
            ],
            "title": "Towards generalization and simplicity in continuous control",
            "venue": "arXiv preprint arXiv:1703.02660,",
            "year": 2017
        },
        {
            "authors": [
                "Aravind Rajeswaran",
                "Vikash Kumar",
                "Abhishek Gupta",
                "Giulia Vezzani",
                "John Schulman",
                "Emanuel Todorov",
                "Sergey Levine"
            ],
            "title": "Learning complex dexterous manipulation with deep reinforcement learning and demonstrations",
            "venue": "In Proceedings of Robotics: Science and Systems (RSS),",
            "year": 2018
        },
        {
            "authors": [
                "Aravind Rajeswaran",
                "Chelsea Finn",
                "Sham M Kakade",
                "Sergey Levine"
            ],
            "title": "Meta-learning with implicit gradients",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Gabriel Robins",
                "Alexander Zelikovsky"
            ],
            "title": "Improved steiner tree approximation in graphs",
            "venue": "In SODA, pp",
            "year": 2000
        },
        {
            "authors": [
                "St\u00e9phane Ross",
                "Geoffrey Gordon",
                "Drew Bagnell"
            ],
            "title": "A reduction of imitation learning and structured prediction to no-regret online learning",
            "venue": "In Proceedings of the fourteenth international conference on artificial intelligence and statistics,",
            "year": 2011
        },
        {
            "authors": [
                "Andrei A Rusu",
                "Sergio Gomez Colmenarejo",
                "Caglar Gulcehre",
                "Guillaume Desjardins",
                "James Kirkpatrick",
                "Razvan Pascanu",
                "Volodymyr Mnih",
                "Koray Kavukcuoglu",
                "Raia Hadsell"
            ],
            "title": "Policy distillation",
            "venue": "arXiv preprint arXiv:1511.06295,",
            "year": 2015
        },
        {
            "authors": [
                "Steind\u00f3r S\u00e6mundsson",
                "Katja Hofmann",
                "Marc Peter Deisenroth"
            ],
            "title": "Meta reinforcement learning with latent variable gaussian processes",
            "venue": "arXiv preprint arXiv:1803.07551,",
            "year": 2018
        },
        {
            "authors": [
                "Gerrit Schoettler",
                "Ashvin Nair",
                "Juan Aparicio Ojea",
                "Sergey Levine",
                "Eugen Solowjow"
            ],
            "title": "Metareinforcement learning for robotic industrial insertion tasks",
            "venue": "In 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),",
            "year": 2020
        },
        {
            "authors": [
                "Naveed A Sherwani"
            ],
            "title": "Algorithms for VLSI physical design automation",
            "venue": "Springer Science & Business Media,",
            "year": 2012
        },
        {
            "authors": [
                "Warren D Smith"
            ],
            "title": "How to find steiner minimal trees in euclidean d-space",
            "venue": "Algorithmica, 7:137\u2013177,",
            "year": 1992
        },
        {
            "authors": [
                "Josh Tobin",
                "Rachel Fong",
                "Alex Ray",
                "Jonas Schneider",
                "Wojciech Zaremba",
                "Pieter Abbeel"
            ],
            "title": "Domain randomization for transferring deep neural networks from simulation to the real world",
            "venue": "IEEE/RSJ international conference on intelligent robots and systems (IROS),",
            "year": 2017
        },
        {
            "authors": [
                "Emanuel Todorov",
                "Tom Erez",
                "Yuval Tassa"
            ],
            "title": "Mujoco: A physics engine for model-based control",
            "venue": "In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems,",
            "year": 2012
        },
        {
            "authors": [
                "Brandon Trabucco",
                "Mariano Phielipp",
                "Glen Berseth"
            ],
            "title": "Anymorph: Learning transferable polices by inferring agent morphology",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Tingwu Wang",
                "Renjie Liao",
                "Jimmy Ba",
                "Sanja Fidler"
            ],
            "title": "Nervenet: Learning structured policy with graph neural networks",
            "year": 2018
        },
        {
            "authors": [
                "Eric Wiewiora",
                "Garrison W Cottrell",
                "Charles Elkan"
            ],
            "title": "Principled methods for advising reinforcement learning agents",
            "venue": "In Proceedings of the 20th international conference on machine learning",
            "year": 2003
        },
        {
            "authors": [
                "Huaxiu Yao",
                "Linjun Zhang",
                "Chelsea Finn"
            ],
            "title": "Meta-learning with fewer tasks through task interpolation",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Yuke Zhu",
                "Josiah Wong",
                "Ajay Mandlekar",
                "Roberto Mart\u00edn-Mart\u00edn",
                "Abhishek Joshi",
                "Soroush Nasiriany",
                "Yifeng Zhu"
            ],
            "title": "robosuite: A modular simulation framework and benchmark for robot learning",
            "year": 2009
        },
        {
            "authors": [
                "Liu"
            ],
            "title": "Robotiq-140 Gripper, the 140mm variation of Robotiq\u2019s multi-purpose two-finger gripper. We follow the high-fidelity robot arm models introduced in Zhu et al. (2020) for the detailed physical specifications of the target robots to minimize the sim-to-real gap. Note that in the simulation model of the source ADROIT hand, the robot is rootless, i.e. connected to a virtual mount base via translation and rotation joints",
            "year": 2022
        },
        {
            "authors": [
                "Isaac (Makoviychuk"
            ],
            "title": "2021) allows highly accurate physics simulation to be performed. Futhermore, simulation frameworks built upon these engines such as robosuite (Zhu et al., 2020) and Orbit (Mittal et al., 2023) can simulate robot actions and robot-object interactions with very high fidelity. It is true that as long as simulation is used, there is always Sim-to-real gap",
            "year": 2018
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "The robotics industry has designed and developed a large number of commercial robots deployed in various applications. How to efficiently learn robotic skills on diverse robots in a scalable fashion? A popular solution is to train a policy for every new robot on every new task from scratch. This is not only inefficient in terms of sample efficiency but also impractical for complex robots due to a large exploration space. Inter-robot imitation by statistic matching methods that optimize to match the distribution of actions (Ross et al., 2011), transitioned states (Liu et al., 2019; Radosavovic et al., 2020), or reward (Ng et al., 2000; Ho & Ermon, 2016) could be possible solutions. However, they can only be applied to robots with similar dynamics to yield optimal performance.\nRecent advances in evolution-based imitation learning (Liu et al., 2022a;b) inspire us to view this problem from the perspective of policy transferring from one robot to another. The core idea is to interpolate two different robots by producing a large number of intermediate robots between them which gradually evolve from the source robot toward the target robot. These continuously and gradually evolving robots act as the bridge for transferring the policy from the source to the\ntarget robot. The source robot is usually selected as a robot such that it is easy to collect sufficient demonstrations to train a high-performance expert policy, e.g. a Shadow Hand robot that can be trained from large-scale human hand demonstration data (Grauman et al., 2022; Damen et al., 2018). While continuous robot evolution has shown success in learning challenging robot manipulation tasks (Liu et al., 2022a), the policy transfer is limited to being between a pair of robots. As illustrated in\nFigure 1(a), for N different target robots where N > 1, it requires launching N independent runs of robot-to-robot policy transfer and is not scalable. How can one efficiently transfer a well-trained policy from one source robot to multiple different target robots?\nThe history of biodiversity provides inspirations for this problem. In the biological world, similar creatures usually share the same ancestors in their evolution history before splitting their ways to form diverse species (Darwin, 1859). The same holds true for the robotic world. When robots are designed to complete certain tasks, they often share similar forms of morphology and dynamics to interact with other objects in similar ways. Examples include robot grippers that are all designed to close their fingers to grasp objects and multi-legged robots that are all designed to stretch their legs for agile locomotion. Therefore, to transfer the policy to N different target robots, it may be possible to find common robot \u201cancestors\u201d and share some parts of the robot evolution paths among the target robots before splitting their ways to each target robot. In this way, the cost of exploration and training during policy transfer can be significantly reduced. The idea is illustrated in Figure 1(b).\nWe propose a method named Meta-Evolve to instantiate the above idea. Given the source and multiple target robots, our method first matches their kinematic tree topology. This allows the source robot and all target robots to be represented in the same high-dimensional continuous space of physical parameters and also allows generating new intermediate robots. To share the evolution paths among multiple target robots, it requires defining a set of robot evolution sequences that are organized in a tree structure with the source robot being the root node. We propose a heuristic approach to determine the robot evolution tree within the parameter space by minimizing the total cost of training and exploration during policy transfer. We formulate the problem as finding a Steiner tree (Steiner, 1881; Gilbert & Pollak, 1968) in the robot parameter space that interconnects the source and all target robots. Our algorithm then decides evolution path splitting based on the evolution Steiner tree.\nWe showcase our Meta-Evolve on three Hand Manipulation Suite manipulation tasks (Rajeswaran et al., 2018) where the source robot is a five-finger dexterous hand and the target robots are three robot grippers with two, three, and four fingers respectively. Our Meta-Evolve reduces the total number of simulation epochs by up to 3.2\u00d7 compared to pairwise robot policy transfer baselines (Liu et al., 2022a;b) to reach the same performance on the three target robots. When applied to one-to-six policy transfer on four-legged agile locomotion robots, our Meta-Evolve can improve the total simulation cost by 2.4\u00d7. This shows that our Meta-Evolve allows more scalable inter-robot imitation learning."
        },
        {
            "heading": "2 PRELIMINARY",
            "text": "Notation We use bold letters to denote vectors. Specially, 0 and 1 are the all-zero and all-one vectors with proper dimensions respectively. We use \u2299 to denote the element-wise product between vectors. We use \u03b8 with subscripts to denote robot physical parameters and \u03b1 and \u03b2 with subscripts to denote evolution parameters. We use MAX(\u00b7) and MIN(\u00b7) to denote element-wise maximum and minimum of a set of vectors respectively. || \u00b7 ||p denotes the Lp vector norm. | \u00b7 | denotes the set cardinality. MDP Preliminary We consider a continuous control problem formulated as Markov Decision Process (MDP). It is defined by a tuple (S,A, T ,R, \u03b3), where S \u2286 RS is the state space, A \u2286 RA is the action space, T : S \u00d7 A \u2192 S is the transition function, R : S \u00d7 A \u2192 R is the reward function, and \u03b3 \u2208 [0, 1] is the discount factor. A policy \u03c0 : S \u2192 A maps a state to an action where \u03c0(a|s) is the probability of choosing action a at state s. Suppose M is the set of all MDPs and \u03c1\u03c0,M = \u2211\u221e t=0 \u03b3\ntR(st, at) is the episode discounted reward with policy \u03c0 on MDP M \u2208 M. The optimal policy \u03c0\u2217M on MDP M is the one that maximizes the expected value of \u03c1 \u03c0,M .\nREvolveR and HERD Preliminary Liu et al. (2022b) proposed a technique named REvolveR for transferring policies from one robot to a different robot. Given a well-trained expert policy \u03c0\u2217MS on a source robot MS \u2208 M, its goal is to find the optimal policy \u03c0\u2217MT on another target robot MT \u2208 M. The core idea is to define a sequence of intermediate robots through linear interpolation of physical parameters and sequentially fine-tune the policy on each intermediate robot in the sequence. Liu et al.\n(2022a) proposed HERD, which extends the idea to robots represented in high-dimensional parameter space, and proposes to optimize the robot evolution path together with the policy. Concretely, given source and target robots MS,MT \u2208 M that are parameterized in D-dimensional space, HERD defines a continuous function F : [0, 1]D \u2192 M where F (0) = MS,F (1) = MT. Given the physical parameters of the source and target robots \u03b8S,\u03b8T \u2208 RD respectively, for any evolution parameter \u03b1 \u2208 [0, 1]D, F (\u03b1) defines an intermediate robot whose physical parameters are \u03b8 = (1\u2212\u03b1)\u2299\u03b8S+\u03b1\u2299\u03b8T. Then an expert policy \u03c0\u2217F (0) on the source robot F (0) is optimized by sequentially interacting with each intermediate robot in the sequence F (\u03b11),F (\u03b12), . . . ,F (\u03b1K) where \u03b1K = 1, until the policy is able to act (near) optimally on each intermediate robot. At robot F (\u03b1k), the optimization objective for finding the next best intermediate robot F (\u03b1k+1) := F (\u03b1k + lk) is\nmax ||lk||2=\u03be max \u03c0\nE[\u03c1\u03c0,F (\u03b1k+lk)]\u2212 \u03bb/2 \u00b7 ||1\u2212 (\u03b1k + lk)||22 (1)\nwhich optimizes both the expected reward E[\u03c1\u03c0,F (\u03b1k+lk)] and the L2 distance to the target robot ||1\u2212 (\u03b1k + lk)||2. For all k, the evolution step size \u03be = ||lk||2 is small enough so that each policy fine-tuning step is a much easier task. The idea is illustrated in Figure 1(a)."
        },
        {
            "heading": "3 ONE-TO-MANY ROBOT-TO-ROBOT POLICY TRANSFER",
            "text": ""
        },
        {
            "heading": "3.1 PROBLEM STATEMENT",
            "text": "We investigate a new problem of transferring an expert policy from one source robot to multiple target robots. Formally, we consider a source robot MS \u2208 M and N target robots MT,1,MT,2, . . . ,MT,N \u2208 M respectively. We assume the state space S, and action space A, reward function R and discount factor \u03b3 of MS and all MT,i are shared and the difference is their transition dynamics T . Given a well-trained expert policy \u03c0\u2217MS on a source robot MS, the goal is to find the optimal policies \u03c0 \u2217 MT,i\non each of the target robot MT,i. We would like to investigate using the information in \u03c0\u2217MS to improve the learning of \u03c0MT,i and study how the learning of each individual \u03c0MT,i can help each other.\nWe approach this problem by defining multiple meta robots MMeta,l \u2208 M that shares the same state and action space as MS and all MT,i. The meta robots MMeta,l are designed such that MMeta,l interconnects MS and all MT,i in an efficient way. Therefore, instead of repeating the process of one-to-one policy transferring for N times, we can transfer source robot policy \u03c0S by going through the interconnection formed by meta robots MMeta,l to reach each individual MT,i."
        },
        {
            "heading": "3.2 MULTI-ROBOT MORPHOLOGY MATCHING AND INTERMEDIATE ROBOT GENERATION",
            "text": "Our problem setting and our proposed solution are based on an assumption that even if the source robot MS and target robots MT,i are different in action and state spaces, they can still be mapped to the same state and action space based on which the intermediate robots can be defined. The assumption is true for a pair of robots as shown in Liu et al. (2022a) and Liu et al. (2022b) where the intermediate robots are produced by robot morphology matching and kinematic interpolation. In this subsection, we show that this assumption can be extended to more than two robots.\nKinematic Tree Matching The topology of the kinematic tree of a robot describes the connection of the bodies and joints and reflects the kinematic behavior of the robot. Given two different robots with different kinematic tree, it has been shown in Liu et al. (2022b) that their kinematic trees can be matched by adding extra nodes and edges. This step can be extended to N robots when N > 2. As illustrated in Figure 2(a), by matching proper root and leaf nodes, the matched kinematic tree is essentially a graph union of the kinematic trees of all N robots. This means each robot needs to create additional bodies, joints and motors, though they may be all zero in numbers at the beginning.\nThe above kinematic tree matching process can be automated by an algorithm that achieves the best matching of nodes and edges across N robots. However, in practice, we would like the matching process to include reasonable human intervention with enough robotics knowledge, e.g. matching human hand fingers to robot gripper fingers such that the knuckle joints are matched correctly.\nPhysical Parameter Interpolation After kinematic tree matching, the state and action space of the robots are matched. The difference in the robot transition dynamics is now only due to the differences in physical parameters, such as shapes and mass of robot bodies, gain and armature of joint motors,\netc. Suppose the kinematic-matched robots have D physical parameters. Then each \u03b8 \u2208 RD uniquely defines a new robot. Suppose the physical parameters of the source robot and the N target robots are \u03b8S \u2208 RD and \u03b8T,1,\u03b8T,2, . . . ,\u03b8T,N \u2208 RD respectively. On each dimension, we compute the upper and lower bounds of the physical parameters\n\u03b8U = MAX({\u03b8S,\u03b8T,1,\u03b8T,2, . . . ,\u03b8T,N}) \u03b8L = MIN({\u03b8S,\u03b8T,1,\u03b8T,2, . . . ,\u03b8T,N})\n(2)\nwhere \u03b8U and \u03b8L essentially defines the convex hull of the set of robot physical parameters in RD that encompasses the source and all target robots. We can now use continuous function F : [0, 1]D \u2192 M to define an intermediate robot by interpolation between all pairs of physical parameters\n\u03b8 = (1\u2212\u03b1)\u2299 \u03b8L +\u03b1\u2299 \u03b8U (3) where \u03b1 \u2208 [0, 1]D is the evolution parameter that describes the normalized position of a robot in the convex hull. Note that by limiting \u03b1 to be between 0 and 1, we assume that the convex hull is the set of all possible intermediate robots. This is a reasonable assumption since an out-of-range parameter can be physically dangerous and is also unlikely to be useful in robot continuous interpolation. The convex hull also serves as the metric space that measures the hardware difference between two robots.\nIn HERD (Liu et al., 2022a), the source robot and the target robot are always represented as 0 and 1 respectively because when there are only two robots, one of them must either be the lower bound or the upper bound. Different from HERD, the source and target robots in our problem are not necessarily 0 or 1. An example of the resulting robot evolution space and the positions of the source and target robots in the evolution space are illustrated in Figure 2(b)."
        },
        {
            "heading": "3.3 ONE-TO-MANY ROBOT EVOLUTION FOR POLICY TRANSFER",
            "text": "Suppose the source and the N target robots are represented by F (\u03b20) := MS and F (\u03b21) := MT,1, . . . ,F (\u03b2N ) := MT,N respectively where \u03b2i \u2208 [0, 1]D. Similar to HERD (Liu et al., 2022a), we employ N robot evolution paths \u03c4i = (F (\u03b1i,1),F (\u03b1i,2), . . . ,F (\u03b1i,Ki)), i = 1, 2, . . . ,N where F (\u03b1i,1) = F (\u03b20) is the source robot and F (\u03b1i,Ki) = F (\u03b2i) is the i-th target robot. Following HERD, we use Ki phases of policy optimizations. At phase k, the policy is trained on policy rollouts on robots sampled from the line \u03b1i,k\u03b1i,k+1. The sampling window gradually converges to \u03b1i,k+1 during training until the policy is able to achieve sufficient performance on F (\u03b1i,k+1) before moving on to phase k+ 1. For all k, we set the evolution step size ||\u03b1i,k \u2212\u03b1i,k+1||p = \u03be to be small enough so that each training phase is an easy sub-task.\nNaively following HERD would require training through all the N evolution paths \u03c4i. However, if some target robots are mutually similar, at the beginning of the transfer, the robot evolution could be in roughly similar directions. Therefore, the robot evolution paths could be close to each other near the start of the paths and the policy optimization might be redundant, as illustrated in Figure 1(a).\nAlgorithm 1 Meta-Evolve Input: source robot \u03b20 and the expert policy \u03c0F (\u03b20) on it; target robot set B = {\u03b21,\u03b22, . . . ,\u03b2N}; Output: policies {\u03c0F (\u03b21),\u03c0F (\u03b22), . . . ,\u03c0F (\u03b2N )} on target robots; {\u03c0F (\u03b21),\u03c0F (\u03b22), . . . ,\u03c0F (\u03b2N )} \u2190 Meta_Evolve(\u03b20,\u03c0F (\u03b20),B) 1: \u03b1\u2190 \u03b20, \u03c0 \u2190 \u03c0F (\u03b20), \u03a0\u2190 \u2205; // initialization 2: \u03b2Meta,P \u2190 Evolution_Tree(\u03b1,B); // temporary meta robot \u03b2Meta; target robot partition P \u2286 2B ; 3: while ||\u03b1\u2212 \u03b2Meta||2 \u2265 \u03be do 4: \u03c0, l\u2190 argmax\u03c0,||l||2=\u03be E[\u03c1 \u03c0,F (\u03b1+l)]\u2212 1 2 \u03bb||\u03b2Meta \u2212 (\u03b1+ l)||2p; // optimize both path and reward\n5: \u03b1\u2190 MIN({MAX({\u03b1+ l,0}),1}); // move towards meta robot, and make sure to stay within [0, 1] 6: \u03b2Meta,P \u2190 Evolution_Tree(\u03b1,B); // update \u03b2Meta and P after robot evolution 7: if |B| = 1 then // if there is only one target robot, then the meta robot is simply the target robot 8: return {\u03c0}; // reaching the meta robot means policy transfer completes: return the policy 9: for p in P do\n10: \u03a0\u2190 \u03a0 \u222aMeta_Evolve(\u03b1,\u03c0,p); // recursively transfer policy in each subtree from meta robot 11: return \u03a0;\nWe propose a method named Meta-Evolve that designs the N evolution paths by forcing the first mi,j \u2208 Z+ intermediate robots to be shared between the paths towards target robots F (\u03b2i) and F (\u03b2j) to address the redundancy issue at the start of the training. Formally, we enforce\n\u2200k \u2264 mi,j ,\u03b1i,k = \u03b1j,k (4) This means that the two paths towards F (\u03b2i) and F (\u03b2j) will first reach a shared robot F (\u03b1i,mi,j ) = F (\u03b1j,mi,j ) before splitting their ways. In this way, both exploration and training overhead during policy transfer can be significantly saved due to path sharing. Theoretically, if all N target robots are close enough to each other, we could expect our Meta-Evolve method to yield a speedup up to O(N) compared to launching multiple one-to-one policy transfers such as HERD (Liu et al., 2022a).\nAs illustrated in Figure 1(b), for N target robots, sharing their evolution paths essentially forms an \u201cevolution tree\u201d with its root node being the source robot and N leaf nodes being the N target robots. The N \u2212 1 internal tree nodes are the intermediate robots that are last shared by the paths before the split. We name these internal tree nodes as \u201cmeta robots\u201d. Given these N \u2212 1 meta robots, our Method-Evolve method first transfers the expert policy \u03c0F (\u03b20) from the source robot \u03b20 to the closest meta robot \u03b2Meta to obtain a well-trained policy, and then recursively transfer the policy towards the target robots in each sub-tree respectively. The overall idea is illustrated in Algorithm 1."
        },
        {
            "heading": "3.4 EVOLUTION TREE DETERMINATION",
            "text": "Given the source and target robots, the structure of the evolution tree and the choice of meta robots significantly impacts the overall performance of the policy transfer. However, due to the huge complexity of the robots\u2019 physical parameter and its relation to the actual MDP transition dynamics in the physical world, it is extremely difficult to develop a universal solution for the optimal evolution tree. We hereby propose the following heuristics for determining the evolution tree and meta robots.\nEvolution Tree as Steiner Tree We aim to minimize the total Lp travel distance in robot evolution parameter space from the source robot to all target robots. Mathematically, an undirected graph that interconnects a set of points and minimizes the total Lp travel distance is called the Lp Steiner tree or p-Steiner tree (Steiner, 1881; Gilbert & Pollak, 1968) of the point set. Then the evolution tree can be selected as the p-Steiner tree of the evolution parameter set of the source and all target robots:\n(VST,EST) = argmin (V ,E):\u03ba((V ,E))=1,{\u03b20,\u03b21,...,\u03b2N}\u2286V \u2211 (v1,v2)\u2208E ||v1 \u2212 v2||p (5)\nwhere VST and EST are the vertex and edge sets of the p-Steiner tree and \u03ba(\u00b7) denotes the graph connectivity. The neighbor(s) of the source robot acts as the initial goal(s) of the evolution. If the source robot has more than one neighbor in the tree, i.e. deg(VST,EST)(\u03b20) > 1, it means the evolution paths should already be split at the source robot and the policy should be transferred in each subtree respectively. The idea is illustrated in Figure 1(b) and Algorithm 2.\nNote that by using p-Steiner tree, we assume that the training cost of transferring the policy from robot F (\u03b1i,k) to robot F (\u03b1i,k+1) is proportional to ||\u03b1i,k \u2212 \u03b1i,k+1||p. We believe this is a reasonable\nAlgorithm 2 Determination of Evolution Tree and Meta Robots Input: target robot set B = {\u03b21,\u03b22, . . . ,\u03b2N}; current intermediate robot \u03b1; Output: meta robot \u03b2Meta; target robot partition P \u2286 2B where \u22c3 p\u2208P p = B and \u2200p1,p2 \u2208 P ,p1\u2229p2 = \u2205; \u03b2Meta,P \u2190 Evolution_Tree(\u03b1,B) 1: (VST,EST)\u2190 argmin(V ,E):\u03ba((V ,E))=1,{\u03b1}\u222aB\u2286V \u2211 (v1,v2)\u2208E ||v1 \u2212 v2||p; // p-Steiner tree\n2: if deg(VST,EST)(\u03b1) = 1 then // the current intermediate robot has only one neighbor in the tree 3: \u03b2Meta \u2190 argminv\u2208VST ||v \u2212\u03b1||2; // meta robot should be the neighbor 4: P \u2190 {B}; // there is no partition in the target robot set yet 5: else // the current intermediate robot has more than one neighbor, so should split paths toward each subtree 6: \u03b2Meta \u2190 \u03b1; // meta robot is the current intermediate robot itself 7: P \u2190 {p\u2032 \u2286 B | p\u2032 \u2286 V \u2032 \u2286 VST,E\u2032 \u2286 EST, \u03ba((V \u2032,E\u2032)) = 1, deg(V \u2032,E\u2032)(\u03b2Meta) = 1}; // partition 8: return \u03b2Meta,P ;\nassumption since the training cost should be locally proportional to the distribution difference of the MDP transition dynamics of the two robots measured in e.g. KL divergence, and should be locally proportional to the robot hardware difference ||\u03b1i,k \u2212\u03b1i,k+1||p when ||\u03b1i,k \u2212\u03b1i,k+1||p \u2192 0, i.e. DKL(F (\u03b1i,k),F (\u03b1i,k+1)) = o(||\u03b1i,k \u2212\u03b1i,k+1||p). Implementation Details At each training phase of the policy transfer, the algorithm should aim to only reduce the expected future cost of training instead of including the past. So a more optimized implementation of our method is that, at training phase k, the algorithm acts greedily to minimize the total Lp travel distance from the current robot \u03b1i,k to the meta robots \u03b2Meta and then to all target robots {\u03b2i} through the evolution tree. It can be implemented by replacing source robot \u03b20 in Equation (5) with the current intermediate robot \u03b1:\n(VST,EST) = argmin (V ,E):\u03ba((V ,E))=1,{\u03b1}\u222a{\u03b21,\u03b22,...,\u03b2N}\u2286V \u2211 (v1,v2)\u2208E ||v1 \u2212 v2||p (6)\nThis means the evolution tree and the meta robots are temporary and are updated at the start of every training phase when the robot evolution progresses, as illustrated in Algorithm 1. In practice, instead of keeping track of the entire evolution tree, we only keep the partition of the target robot set P \u2286 2B when paths splits into subtrees and re-compute each evolution subtree after path splitting."
        },
        {
            "heading": "3.5 DISCUSSIONS",
            "text": "Can the Target Robots be Very Different? It is possible that target robots are in opposite directions. One extreme example is that the source robot is a five-finger hand while the two target robots are a ten-finger hand and a two-finger gripper. Our Meta-Evolve will still be able to handle such cases correctly, but the meta robot may simply be the source robot itself. This means the evolution paths are split at the start and our Meta-Evolve will be reduced to multiple runs of independent one-to-one policy transfers and does not yield any speedup in performance, which is reasonable. Fortunately, in practice, most commercial robots such as Sawyer, Panda and UR5e are indeed mutually similar in morphology and kinematics. So our Meta-Evolve can still be useful in these cases.\nCan the Meta Robots be Learned or Optimized? We envision the learning or optimization of the evolution tree and the meta robots being very challenging. Policy transfer through robot evolution relies on local optimization of the robot evolution. On the other hand, optimizing the evolution tree requires optimizing the robot evolution paths globally and needs an accurate \u201cguess\u201d of the future cost of policy transfer. In fact, our proposed heuristics can be viewed as using Lp distance of evolution parameters to roughly guess the future policy transfer cost for constructing evolution tree. We leave the problem of finding the optimal evolution tree and meta robots as future work."
        },
        {
            "heading": "4 RELATED WORK",
            "text": "Imitation Learning across Different Robots Traditional imitation learning is designed for learning on the same robots (Ross et al., 2011; Ng et al., 2000; Ho & Ermon, 2016; Duan et al., 2017). However, due to a huge mismatch in transition dynamics, these works often struggle in learning across different robots. Compared to previous imitation learning methods that aim to learn across\ndifferent robots directly (Radosavovic et al., 2020; Liu et al., 2019; Rusu et al., 2015; Trabucco et al., 2022), we aim to employ robot evolution to gradually adapt the policy. Furthermore, our Meta-Evolve focuses on one-to-many imitation where the transferred policies must work on multiple target robots.\nLearning Controllers for Diverse Robot Morphology Recent work has studied the problem of learning a policy/controller for diverse robots. For instance, Wang et al. (2018), Huang et al. (2020) and Pathak et al. (2019) use graph neural networks to control and develop robots with different morphology that can generalize to new scenarios. Hierarchical controllers (Hejna et al., 2020) and transformers (Gupta et al., 2022; Hong et al., 2021) are also shown to be effective across diverse robot morphology. In contrast to these works, we do not co-develop the controller with morphology but transfer the policy from a source robot to multiple target robots.\nMeta-Learning Our Meta-Evolve is closely related to the formulation of meta-learning (Finn et al., 2017; 2018; Rajeswaran et al., 2019; Nagabandi et al., 2018; S\u00e6mundsson et al., 2018; Schoettler et al., 2020). Different from meta reinforcement learning where only the policy \u03c0 is meta learned, our formulation can be viewed as the continuous update of both the policy \u03c0 and the transition dynamics T instantiated by setting different robot hardware parameters \u03b8. Moreover, while meta-learning aims to learn a meta policy from scratch, in our problem, the source expert policy is given and used in policy transfer. Closely related to our approach is task interpolation for meta-learning (Yao et al., 2021). Different from task interpolation, our method does not require the policy to work on a range of robots at the same time but only needs each transferred policy to work on each target robot.\nTransfer Learning in RL Previous works on RL transfer learning have explored transferring policies by matching certain quantities across multiple tasks. Examples include learning inter-task mappings of states and actions (Gupta et al., 2017; Konidaris & Barto, 2006; Ammar et al., 2015) and cross-task reward shaping (Ng et al., 1999; Wiewiora et al., 2003). Different from these works, we do not aim to directly find the matching between different robots but gradually evolve one robot to multiple target robots through an evolution tree and transfer the expert policy along the way."
        },
        {
            "heading": "5 EXPERIMENTS",
            "text": "The design of our Meta-Evolve method is motivated by the hypothesis that by sharing the evolution paths among multiple robots through the design of the evolution tree, the overall cost of one-to-many policy transfer can be reduced compared to multiple one-to-one transfers. To show this, we apply our Meta-Evolve on the policy transfer on two types of robot learning tasks: one-to-three policy transfer on the three manipulation tasks in Hand Manipulation Suite (HMS) (Rajeswaran et al., 2018), and one-to-six policy transfer on an agile locomotion task in a maze."
        },
        {
            "heading": "5.1 ONE-TO-THREE MANIPULATION POLICY TRANSFER",
            "text": "Source and Target Robots We utilize the five-finger ADROIT dexterous hand (Kumar et al., 2013) as the source robot and follow Rajeswaran et al. (2018) for the initial settings. The target robots are three robot grippers with two, three, and four fingers respectively. The target robots can be produced by gradually shrinking the fingers of the source robot, as illustrated in Figures 1 and 2(b).\nTask and RL Algorithm We use the three tasks from the the task suite in Rajeswaran et al. (2018): Door, Hammer and Relocate illustrated in Figure 3. In Door task, the goal is to turn the door handle and fully open the door; n Hammer task, the goal is to pick up the hammer and smash the nail into the board; in Relocate task, the goal is to pick up the ball and take it to the target position. We use a challenging sparse reward function where only the task completion is rewarded. We use NPG (Rajeswaran et al., 2017) as the RL algorithm in all compared methods. The source expert policy was trained by learning from human demonstrations collected from VR-empowered sensor glove.\nBaselines We compare our Meta-Evolve against three baselines: (1) DAPG: we launch multiple independent direct one-to-one imitation learning using DAPG (Rajeswaran et al., 2018); (2) HERD: we launch multiple independent one-to-one robot policy transfer with HERD; (3) Geom-Median: in this baseline, we allow only one meta robot \u03b2Meta in the evolution tree. Mathematically, a point that minimizes the sum of Lp distances to a set of points is called the Lp geometric median of the point set. When there is only one meta robot in the Steiner tree, the meta robot \u03b2Meta is the Lp geometric median of the source and target robot evolution parameter set, i.e. \u03b2Meta = argmin\u03b2\u2208[0,1]D \u2211N i=0 ||\u03b2\u2212\u03b2i||p.\nEvaluation Metrics For each compared method, the goal is to reach 80% success rate on all three target robots. Due to the nature of one-to-many policy transfer, the total number of RL iterations or simulation epochs it takes to reach this goal cannot be set beforehand. So we instead report the number of policy training iterations and simulation epochs needed to reach the desired success rate.\nResults and Analysis The topology of the resulting evolution tree is illustrated in Figure 3(f). As illustrated in Table 1, in terms of the total number of training iterations needed, our Meta-Evolve method is able to achieve 2.35\u00d7, 2.95\u00d7 and 2.29\u00d7 improvement on the three tasks respectively compared to one-to-one policy transfer using HERD. In terms of simulation epochs needed, the improvement is 2.73\u00d7, 3.23\u00d7 and 3.17\u00d7 respectively on the three tasks. Direct policy transfer with DAPG never successfully completes the task, therefore the policy was never able to be trained.\nBreaking down each part of the evolution paths, we observed that in our method, the paths from source robot to the meta robots are usually the most costly and constitutes the largest portion of simulation epochs and training. The cost after splitting the path at the meta robots is smaller which is the reason for smaller total cost. The baseline of using only one meta robot in the evolution tree can also yield significant improvements, however, its performance is still inferior to using an evolution tree with multiple meta robots, which shows the general tree-structured evolution paths is necessary.\nA more interesting observation is that, for some target robots and tasks, e.g. two- and three-finger target robots on Hammer task, the total cost of transferring the policy by going through multiple meta robots in the evolution tree is even smaller than the cost of directly transferring the policy to the target robot using HERD (Liu et al., 2022a). It shows that, transferring the policies to multiple related target through an evolution tree determined by our heuristic approach can possibly help each robot improve their own learning efficiency. We believe this phenomena deserve more future research attention.\nAblation Studies On Hammer and Relocate tasks, we provide ablation studies on the design choice of the distance measure used to construct the evolution tree, i.e. L1 vs. L2 distance in evolution parameter space. As illustrated in Table 1, L1 distance achieves better performance than L2 distance. A possible reason is that the hardware parameters mostly mutually independent, so when empirically estimating the robot transition dynamics difference, directly adding up element-wise difference may be better than using Euclidean distance which intertwines the difference on each dimension."
        },
        {
            "heading": "5.2 ONE-TO-SIX AGILE LOCOMOTION POLICY TRANSFER",
            "text": "Experiment Settings To show that our Meta-Evolve can generalize to diverse tasks and robot morphology, we conduct additional policy transfer experiments on an agile locomotion task illustrated in Figure 4. The goal of the robot is to move out of the maze from the starting position. The source robot is the Ant-v2 robot used in MuJoCo Gym (Brockman et al., 2016). The six target robots are four-legged agile locomotion robots with different lengths of torsos, thickness of legs, and widths of hips and shoulders. The reward function is also sparse task completion reward. We use NPG (Rajeswaran et al., 2017) as the RL algorithm. We report the number of training iterations and simulation epochs needed to reach 90% success rate on the task.\nResults and Analysis The experiment results are illustrated in Table 2. Our Meta-Evolve method is able to achieve 2.20\u00d7 improvement in terms of the total training cost and 2.43\u00d7 total simulation cost compared to launching multiple HERD. The improvement is less compared to manipulation policy transfer. A possible reason is that locomotion tasks are less sensitive to the morphological changes of robots than manipulation tasks, therefore benefit less from our Meta-Evolve."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "In this paper, we introduce a new research problem of transferring an expert policy from a source robot to multiple target robots. To solve this new problem, we introduce a new method named Meta-Evolve that utilizes continuous robot evolution to efficiently transfer the policy through an robot evolution tree defined by the interconnection of multiple meta robots and then to each target robot. We present a heuristic approach to determine the robot evolution tree. We conduct experiments on Hand Manipulation Suite tasks and an agile locomotion task and show that our Meta-Evolve can significantly outperform the one-to-one policy transfer baselines.\nAcknowledgment Deepak Pathak is supported in part by NSF IIS-2024594 and AFOSR FA9550-231-0747."
        },
        {
            "heading": "A ADDITIONAL EXPERIMENTS ON REAL COMMERCIAL ROBOTS",
            "text": "To show that our Meta-Evolve can be applied to real robots and real-world tasks, we conduct an additional set of experiments of transferring an object manipulation policy to multiple real commercial robots.\nSource and Target Robots The source robot is the same ADROIT hand robot (Kumar et al., 2013) used in Section 5.1. The three target robots are as follows and are illustrated in Figure 5(b)(c)(d):\n\u2022 Jaco: Jaco is a 7-DoF robot produced by Kinova Robotics. It is equipped with the Jaco Three-Finger Gripper, a three-finger gripper with multi-jointed fingers. \u2022 Kinova3: Kinova3 is a 7-DoF robot produced by Kinova Robotics. It is equipped with the Robotiq-85 Gripper, the 85mm variation of Robotiq\u2019s multi-purpose two-finger gripper\n\u2022 IIWA: IIWA is an industrial-grade 7-DoF robot produced by KUKA. It is equipped with the Robotiq-140 Gripper, the 140mm variation of Robotiq\u2019s multi-purpose two-finger gripper.\nWe follow the high-fidelity robot arm models introduced in Zhu et al. (2020) for the detailed physical specifications of the target robots to minimize the sim-to-real gap. Note that in the simulation model of the source ADROIT hand, the robot is rootless, i.e. connected to a virtual mount base via translation and rotation joints. We adopt the same procedure introduced in Liu et al. (2022a) to attach the ADROIT virtual mount base to the end-effector of the 7-DoF robot arm. The end-effector of the robot arm is controlled by an Operational Space Controller (OSC) (Khatib, 1987) that moves the end-effector to its desired 6D pose with PD control schema. We follow Zhu et al. (2020) for the implementation of OSC. Apart from the states of the ADROIT hand, the 6D pose of the robot end-effector is additionally included in the state of the robot. During robot evolution, the original ADROIT arm shrinks and the five-finger ADROIT hand gradually changes to be the target gripper. Please refer to Liu et al. (2022a) for more details on the idea behind this implementation. The resultant L1 evolution tree is illustrated in Figure 5 and is used in our experiments.\nTask and RL Algorithms The task setup is illustrated in Figure 6. The goal of the robot is to pick up the object and take it to the desired goal position. The task is considered success if the distance from the object to the goal is sufficiently small. The reward function is sparse task completion reward. We use NPG (Rajeswaran et al., 2017) as the RL algorithm. The source expert policy is trained by learning from the human hand demonstrations in DexYCB dataset (Chao et al., 2021). We report the number of training iterations and simulation epochs needed to reach 80% success rate on the task.\nResults and Analysis The experiment results are illustrated in Table 3. Our Meta-Evolve method is able to achieve 1.91\u00d7 improvement in terms of the total training cost and 2.07\u00d7 total simulation cost compared to launching multiple HERD. This shows that our Meta-Evolve can be applied to real commercial robots. Moreover, to show that the transferred policy can be used on real target robots, we conduct real-world experiment and deploy the target robot policies on Kinova3 on the corresponding real machine as illustrated in Figure 6(h) . Please refer to the our project website for more details."
        },
        {
            "heading": "B ADDITIONAL DISCUSSIONS",
            "text": "Can the physical parameters of all robots be known? It is easy to obtain all necessary physical parameters of a robot. The ultimate goal of our method is to train a policy that can be deployed on real robots. In order to do such real-world experiments, we need to obtain the robot physically. At that time, we will have access to every specification of the robot:\n\u2022 If we purchase, borrow or rent a commercial robot: When selling their robots, licensed robot manufacturing companies would release detailed parameters of their robots. Besides, the controller software such as Operational Space Controller (OSC) (Khatib, 1987) is usually also released for the robot by the manufacturers. These controller software can only work correctly when all the necessary physical parameters of the robots are matched with the actual hardware, including the inertia and mass of every robot body, and damping and gain of every motor etc. Users with sufficient robotics expertise can easily infer the accurate physical parameters from the released control software or the manual of the robot provided to the users. \u2022 If we create our own robot: Nowadays, the mechanical components of new robots are manufactured by printing 3D CAD models. Therefore, all physical parameters of the mechanical parts of the robot can be easily calculated using the 3D CAD design software. This is also how the robot manufacturing companies obtain the parameters for their own commercial robots. \u2022 If the robot we obtained has missing or unknown parameters: It is not recommended to use a robot with missing or unknown parameters because it might be dangerous to do so. In the rare and extreme case where we are forced to use a robot with missing or unknown parameters, the methods for accurately measuring the parameters of unknown real robots were already developed in the 1980s (Khosla & Kanade, 1985) and have been maturally used in the robotics industry for decades since then. \u2022 If we attach external components to the robot: External components may introduce additional physical parameters such as friction coefficient of the auxiliary gripper finger parts etc. These external parameters can be easily and accurately measured in lab experiments. On the other hand, it is not recommended to use an external components without knowing its detailed parameters\nbecause it might be dangerous to do so. Please refer to related literature on mechanical or materials engineering for more details on how to measure these parameters in the lab.\nReal-robot vs. Simulation Experiments? If we want to take advantage of the power of deep reinforcement learning in solving robotics tasks, it is imperative to collect sufficient amount of data, and performing large-scale simulation is the most convenient way to do so. On the other hand, modern simulation engines such as MuJoCo (Todorov et al., 2012), Pybullet (Coumans & Bai, 2016) and Isaac (Makoviychuk et al., 2021) allows highly accurate physics simulation to be performed. Futhermore, simulation frameworks built upon these engines such as robosuite (Zhu et al., 2020) and Orbit (Mittal et al., 2023) can simulate robot actions and robot-object interactions with very high fidelity. It is true that as long as simulation is used, there is always Sim-to-real gap. Fortunately, there has been numerous works on reducing the Sim-to-real gap in robotic control (Peng et al., 2018; Tobin et al., 2017). Sim-to-real transfer is not the focus of our work, therefore is not discussed in depth in our paper.\nScaling/transformations on the evolution parameters \u03b1 and its effect on Meta-Evolve? There could be many ways to transform the physical parameter \u03b8S and \u03b8T,i to be evolution parameters \u03b2i by \u03b20 = f(\u03b8S) and \u03b2i = f(\u03b8T,i). In the main paper, we present a simple and intuitive way by normalizing each element of \u03b8 to [0, 1] through a linear transformation. If L2 distance is used to construct the evolution Steiner tree, the meta robots depend on the choice of the transformation function f . However, if L1 distance is used instead, the meta robots will not depend on the choice of f , as long as f is monotonic on every input dimension. As introduced in Equation (8), the L1 distance of two vectors is the sum of element-wise absolute difference of the two vectors. Given the current intermediate robot \u03b1 \u2208 RD and target robots \u03b21,\u03b22, . . . ,\u03b2N \u2208 RD, the first meta robot in the Steiner tree to reach from \u03b1 is given by an element-wise clamp operation on \u03b1 to the convex hull boundaries of the target robots:\n\u03b2Meta = MAX(\u03b2L,MIN(\u03b1,\u03b2U )) (7)\nwhere \u03b2U = MAX({\u03b21,\u03b22, . . . ,\u03b2N}) and \u03b2L = MIN({\u03b21,\u03b22, . . . ,\u03b2N}) are the element-wise upper and lower bounds of the target robot evolution parameter convex hull. This is because the convex hull defines the spanning range of the target robot parameters and should be used as the indicator of where to split the evolution path. As long as f is monotonic on every dimension of the parameter, the meta robot given by Equation (7) is invariant to f .\nCan Meta-Evolve generalize to any known robot configurations? Our Meta-Evolve can generalize to any known robot configurations. For any source robot and N target robots, we can always use the method described in Section 3.2 to match the robot kinematic trees and state/action spaces. Whether the policy transfer can work on that set of source and target robots depends on the task as well as the policy. For example, we do not expect a manipulation policy on a five-finger gripper can be transferred to some four-legged agile locomotion robots, though we can still define the intermediate robots between them. As long as the task and robot settings are reasonable, e.g. transferring a manipulation policy from one robot gripper to some other robot grippers, our Meta-Evolve should be able to deal with these cases, as shown by the experiments in our paper.\nCan Meta-Evolve generalize to unknown robot configurations? Our Meta-Evolve does require knowing the configurations of the source and target robots. This is a reasonable assumption because we only deal with predetermined source and target robots. When we deploy the trained policies on the robots in the real world, we would need to first obtain these robots physically. Then at that time we will have access to everything about the robots. Please refer to the discussions on how to obtain the physical parameters of a robot, i.e. the first paragraph of Section B, for more details.\nC INTRODUCTION TO STEINER TREE\nIn RD, the Steiner tree problem is to find a network of minimum length interconnecting a set B of N given points. Such networks can be represented by a tree. The set of the tree nodes can consist of the points in B, known as terminals, and possibly of additional points, known as Steiner points. The length of the network is defined as the sum of the lengths of all the edges in the tree. Without allowing Steiner points, the problem is reduced to the well-known minimum spanning tree problem (Cheriton & Tarjan, 1976). Allowing additional Steiner points can possibly reduce the length of the network, but can also make the problem harder.\nThe Steiner trees can be classified based on the metric used to measure the edge lengths. When the edge length is measured with Lp norm, the Steiner tree is known as p-Steiner tree. In the main paper, we did ablation studies on both L1 and L2 norms for constructing the Steiner tree. In this section, we provide more detailed background on 1-Steiner tree and 2-Steiner tree.\nC.1 1-STEINER TREE\nThe L1 distance of two points \u03b1 = [\u03b11,\u03b12, . . . ,\u03b1D] \u2208 RD and \u03b2 = [\u03b21,\u03b22, . . . ,\u03b2D] \u2208 RD, also known as Manhattan distance, is the sum of element-wise absolute difference and is defined as\n||\u03b1\u2212 \u03b2||1 = D\u2211 i=1 |\u03b1i \u2212 \u03b2i| (8)\nBy using L1 distance of evolution parameters of two robots to measure their MDP difference as the guideline for constructing Steiner tree, we not only assume the changes of each robot hardware parameter contribute equally to the change of MDP, but also assume their effect on MDP is independent and can be summed up directly. Without additional information on the actual robot hardware and task, we believe this is a reasonable assumption.\nA 1-Steiner tree of a point set B, also known as Rectilinear Steiner tree, is the undirected graph that interconnects B and minimizes the total L1 lengths of its edges. Finding the L1 Steiner tree is one of the core problems in the physical design of electronic design automation (EDA). In VLSI circuits, wire routing is only carried out by metal wires running in either vertical or horizontal directions (Sherwani, 2012).\nL1 Steiner tree problem is known to be an NP-hard problem. However, multiple approximate and heuristic algorithms have been introduced and used in VLSI design. Using algorithms introduced by Robins & Zelikovsky (2000), for N terminal points, a good approximate solution to L1 Steiner tree can be found in O(N logN) time. The solution to L1 Steiner tree of a specific set of points may not be unique. Examples of L1 Steiner tree are illustrated in Figure 7(a)(b).\nC.2 2-STEINER TREE\nThe L2 distance of two points \u03b1 = [\u03b11,\u03b12, . . . ,\u03b1D] \u2208 RD and \u03b2 = [\u03b21,\u03b22, . . . ,\u03b2D] \u2208 RD, also known as Euclidean distance, is the square root of element-wise summation of the difference squares and is defined as\n||\u03b1\u2212 \u03b2||2 = \u221a\u221a\u221a\u221a D\u2211 i=1 (\u03b1i \u2212 \u03b2i)2 (9)\nA 2-Steiner tree of a point set B, also known as Euclidean Steiner tree, is the undirected graph that interconnects B and minimizes the total L2 lengths of its edges. L2 distance of evolution parameters of two robots intertwines the effect of the difference on each dimension. We believe this is one of the reasons that L2 Steiner tree as the evolution tree shows worse performance in one-to-many policy transfer than L1 Steiner tree.\nIn an L2 Steiner tree, a terminal has degree between 1 and 3 and a Steiner point has degree of 3. An L2 Steiner tree of N terminals have at most N \u2212 2 Steiner points and all Steiner points must lie in the convex hull of the terminals. The Steiner point and its three neighbors in the tree must lie in a plane, and the angles between the edges connecting the Steiner point to its neighbors are all 120\u25e6.\nL2 Steiner tree problem is also known to be an NP-hard problem. Similar to L1 Steiner tree, multiple approximate and heuristic algorithms have been introduced for L2 Steiner tree (Fampa et al., 2016) where a good solution can be found in O(N logN) time. The Euclidean Steiner tree of three vertices of a triangle is also known as the Fermat point of the triangle, illustrated in Figure 7(c)(d). The solution to L2 Steiner tree of a specific set of points may not be unique.\nC.3 OUR IMPLEMENTATION\nThough both L1 and L2 Steiner tree problems are NP-hard, fortunately, there exist multiple heuristic algorithms for approximate solutions in O(N logN) time for N target robots. We used Robins & Zelikovsky (2000) for computing L1 Steiner tree and Smith (1992) for computing L2 Steiner tree in our implementation of finding evolution trees. In practice, we do not expect to deal with an extremely huge number of target robots. We expect the number of robots being dealt with to be under 20, which means the CPU time spent to compute both L1 and L2 Steiner trees is negligible."
        },
        {
            "heading": "D ROBOT EVOLUTION SPECIFICS",
            "text": "For robots with different state and action spaces, Meta-Evolve converts different state and action spaces into the same state and action space with different transition dynamics. Specifically, the kinematic trees of all robots are unified by adding additional bodies and joints, though the new bodies and joints may be zero in their physical parameters, e.g. zero mass, zero sizes, zero motors etc, so that the original expert policy remains intact. The zeros are inserted to the correct positions of the state vectors of different robots to map them to the same state space. In this section, we provide more details on the evolution of the robots used in our experiments.\nManipulation Policy Transfer Experiments We illustrate the kinematic tree of the source ADROIT robot Kumar et al. (2013) used in our manipulation policy transfer experiments in Figure 8. During evolution, all revolute joints gradually freeze to have a motion range of 0. On the other hand, the prismatic joints are initially frozen with a motion range of 0, and some of their ranges gradually increase until the same full range.\nDuring robot evolution, the body of the ring finger gradually shrinks to be zero-size and disappears for all target robots. Besides, other fingers may also gradually shrink and disappear for certain target robots, e.g. the middle finger and the little finger will shrink and disappear for the two-finger target robot. Our evolution solution includes the changing of D = 65 independent robot parameters resulting in an evolution parameter space of [0, 1]65.\nAgile Locomotion Policy Transfer Experiments During robot evolution of the agile locomotion policy transfer experiments, the lengths of the body, thickness of the legs, and the widths of the shoulder and hip change. This is implemented by changing the sizes of the torso frame, the size of legs as well as leg mounting positions. The solution includes the changing of D = 5 independent robot parameters resulting in an evolution parameter space of [0, 1]5."
        },
        {
            "heading": "E TRAINING DETAILS",
            "text": "Hyperparameter Selection. We present the hyperparameters of our robot evolution and policy optimization in Table 4. To fairly compare against HERD (Liu et al., 2022a), the two methods should be compared under their respective optimal performance. Fortunately, our Meta-Evolve and HERD share the same set of hyperparameters while achieving their own optimal performance. This is discovered by searching the optimal combinations of hyperparameters both methods. Another baseline method DAPG (Rajeswaran et al., 2018) uses the same RL hyperparameters illustrated in Table 4.\nPerformance Threshold for Moving to the Next Intermediate Robot. We used success rate as the indicator for deciding whether to move on to the next intermediate robot during policy transfer.\nSpecifically, the policy transfer moves on to the next intermediate robot if and only if the success rate on the current intermediate robot exceeded a certain threshold. As illustrated in Table 4, 66.7% is roughly the best option for both HERD and Meta-Evolve. Using a higher success rate threshold may waste training overhead on intermediate robots and slow down policy transfer, while using a lower success rate threshold may sacrifice sample efficiency in later stages of policy transfer due to sparse-reward settings.\nEvaluation Metrics. We followed HERD (Liu et al., 2022a) and adopted the training overhead needed to reach 80% success rate as our evaluation metric. The core idea of using this evaluation metric is to compare the efficiency of the policy transfer when the difficulty of transferring each policy is unknown beforehand. Using an alternative success rate higher than 80% as the evaluation metric could also be feasible. However, since the success rate threshold for moving on to the next intermediate robot is 66.7%, the success rate increase from 80% to a higher one can only happen after the policy transfer is completed. Therefore, the remaining part of training for reaching a higher success rate is simply vanilla reinforcement learning on the target robots and is irrelevant to our problem of inter-robot policy transfer, so should not be included in the evaluation.\nTraining Platforms. We use PyTorch (Paszke et al., 2019) as our deep learning framework and NPG (Rajeswaran et al., 2017) as the RL algorithm in all manipulation policy transfer and agile locomotion transfer experiments. We used MuJoCo (Todorov et al., 2012) as the physics simulation engine.\nF VISUALIZATIONS\nOn the three Hand Manipulation Suite (Rajeswaran et al., 2018) tasks, we provide visualizations for the expert policy on the source robot and the transferred policies on three target robots in Figure 9. As shown in the visualizations, during policy transfers, the original behaviors can be generally\nmaintained in the original expert policy and also transfer it to each target robot. Please refer to the attached supplementary video or our project website for more details on the visualizations."
        }
    ],
    "title": "ONE-TO-MANY POLICY TRANSFER",
    "year": 2024
}