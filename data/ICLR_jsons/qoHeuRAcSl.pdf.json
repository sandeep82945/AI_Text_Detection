{
    "abstractText": "Grounding the abstract knowledge captured by Large Language Models (LLMs) in physical domains remains a pivotal yet unsolved problem. Whereas prior works have largely focused on leveraging LLMs for generating abstract plans in symbolic spaces, this work uses LLMs to guide the learning for structures and constraints in robot manipulation tasks. Specifically, we borrow from manipulation planning literature the concept of mode families, defining specific types of motion constraints among sets of objects, to serve as an intermediate layer that connects high-level language representations with low-level physical trajectories. By locally perturbing a small set of successful human demonstrations, we augment the dataset with additional successful executions as well as counterfactuals that fail the task. Our explanation-based learning framework trains neural network-based classifiers to differentiate success task executions from failures and as a by-product learns classifiers that ground low-level states into mode families without dense labeling. This further enables us to learn structured policies for the target task. Experimental validation in both 2D continuous-space and robotic manipulation environments demonstrates the robustness of our mode-based imitation methods under external perturbations.",
    "authors": [],
    "id": "SP:252f2b120c1ef44e94b7fd708526ef3201aa3451",
    "references": [
        {
            "authors": [
                "Michael Ahn",
                "Anthony Brohan",
                "Noah Brown",
                "Yevgen Chebotar",
                "Omar Cortes",
                "Byron David",
                "Chelsea Finn",
                "Chuyuan Fu",
                "Keerthana Gopalakrishnan",
                "Karol Hausman"
            ],
            "title": "Do as I Can, Not as I Say: Grounding Language in Robotic Affordances",
            "year": 2022
        },
        {
            "authors": [
                "Jacob Andreas",
                "Dan Klein"
            ],
            "title": "Alignment-Based Compositional Semantics for Instruction Following",
            "venue": "In EMNLP,",
            "year": 2015
        },
        {
            "authors": [
                "Jacob Andreas",
                "Dan Klein",
                "Sergey Levine"
            ],
            "title": "Modular Multitask Reinforcement Learning with Policy Sketches",
            "venue": "In ICML,",
            "year": 2017
        },
        {
            "authors": [
                "Manel Baradad Jurjo",
                "Jonas Wulff",
                "Tongzhou Wang",
                "Phillip Isola",
                "Antonio Torralba"
            ],
            "title": "Learning to see by looking at noise",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Christopher Bradley",
                "Adam Pacheck",
                "Gregory J. Stein",
                "Sebastian Castro",
                "Hadas Kress-Gazit",
                "Nicholas Roy"
            ],
            "title": "Learning and planning for temporally extended tasks in unknown environments, 2021",
            "year": 2021
        },
        {
            "authors": [
                "Ruth MJ Byrne"
            ],
            "title": "Counterfactuals in explainable artificial intelligence (xai): Evidence from human reasoning",
            "venue": "In IJCAI,",
            "year": 2019
        },
        {
            "authors": [
                "Rodolfo Corona",
                "Daniel Fried",
                "Coline Devin",
                "Dan Klein",
                "Trevor Darrell"
            ],
            "title": "Modular Networks for Compositional Instruction Following",
            "venue": "In NAACL-HLT,",
            "year": 2021
        },
        {
            "authors": [
                "Gerald DeJong",
                "Raymond Mooney"
            ],
            "title": "Explanation-based learning: An alternative view",
            "venue": "Machine learning,",
            "year": 1986
        },
        {
            "authors": [
                "Eoin Delaney",
                "Derek Greene",
                "Mark T Keane"
            ],
            "title": "Instance-based counterfactual explanations for time series classification",
            "venue": "In International Conference on Case-Based Reasoning,",
            "year": 2021
        },
        {
            "authors": [
                "Divyansh Garg",
                "Skanda Vaidyanath",
                "Kuno Kim",
                "Jiaming Song",
                "Stefano Ermon"
            ],
            "title": "Lisa: Learning interpretable skill abstractions from language",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Kris Hauser",
                "Jean-Claude Latombe"
            ],
            "title": "Multi-modal motion planning in non-expansive spaces",
            "venue": "The International Journal of Robotics Research,",
            "year": 2010
        },
        {
            "authors": [
                "Wenlong Huang",
                "Pieter Abbeel",
                "Deepak Pathak",
                "Igor Mordatch"
            ],
            "title": "Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents",
            "venue": "In ICML,",
            "year": 2022
        },
        {
            "authors": [
                "Wenlong Huang",
                "Chen Wang",
                "Ruohan Zhang",
                "Yunzhu Li",
                "Jiajun Wu",
                "Li Fei-Fei"
            ],
            "title": "Voxposer: Composable 3d value maps for robotic manipulation with language models",
            "year": 2023
        },
        {
            "authors": [
                "Yiding Jiang",
                "Shixiang Shane Gu",
                "Kevin P Murphy",
                "Chelsea Finn"
            ],
            "title": "Language as an Abstraction for Hierarchical Deep Reinforcement Learning",
            "venue": "In NeurIPS,",
            "year": 2019
        },
        {
            "authors": [
                "Amir-Hossein Karimi",
                "Gilles Barthe",
                "Bernhard Sch\u00f6lkopf",
                "Isabel Valera"
            ],
            "title": "A survey of algorithmic recourse: definitions, formulations, solutions, and prospects",
            "venue": "arXiv preprint arXiv:2010.04050,",
            "year": 2020
        },
        {
            "authors": [
                "Steven LaValle"
            ],
            "title": "Rapidly-Exploring Random Trees: A New Tool for Path Planning",
            "venue": "Research Report 9811,",
            "year": 1998
        },
        {
            "authors": [
                "Shuang Li",
                "Xavier Puig",
                "Chris Paxton",
                "Yilun Du",
                "Clinton Wang",
                "Linxi Fan",
                "Tao Chen",
                "De-An Huang",
                "Ekin Aky\u00fcrek",
                "Anima Anandkumar"
            ],
            "title": "Pre-trained language models for interactive decision-making",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Bo Liu",
                "Yuqian Jiang",
                "Xiaohan Zhang",
                "Qiang Liu",
                "Shiqi Zhang",
                "Joydeep Biswas",
                "Peter Stone"
            ],
            "title": "LLM+ P: Empowering Large Language Models with Optimal Planning Proficiency",
            "year": 2023
        },
        {
            "authors": [
                "Zhezheng Luo",
                "Jiayuan Mao",
                "Jiajun Wu",
                "Tom\u00e1s Lozano-P\u00e9rez",
                "Joshua B Tenenbaum",
                "Leslie Pack Kaelbling"
            ],
            "title": "Learning Rational Subgoals from Demonstrations and Instructions",
            "venue": "In AAAI,",
            "year": 2023
        },
        {
            "authors": [
                "Corey Lynch",
                "Pierre Sermanet"
            ],
            "title": "Language conditioned imitation learning over unstructured data",
            "venue": "arXiv preprint arXiv:2005.07648,",
            "year": 2020
        },
        {
            "authors": [
                "Corey Lynch",
                "Ayzaan Wahid",
                "Jonathan Tompson",
                "Tianli Ding",
                "James Betker",
                "Robert Baruch",
                "Travis Armstrong",
                "Pete Florence"
            ],
            "title": "Interactive language: Talking to robots in real time",
            "venue": "IEEE Robotics and Automation Letters,",
            "year": 2023
        },
        {
            "authors": [
                "Matthew T Mason"
            ],
            "title": "Mechanics of robotic manipulation",
            "venue": "MIT press,",
            "year": 2001
        },
        {
            "authors": [
                "Alberto Segre",
                "Gerald DeJong"
            ],
            "title": "Explanation-based manipulator learning: Acquisition of planning ability through observation",
            "venue": "In Proceedings",
            "year": 1985
        },
        {
            "authors": [
                "Pratyusha Sharma",
                "Antonio Torralba",
                "Jacob Andreas"
            ],
            "title": "Skill Induction and Planning with Latent Language",
            "venue": "In ACL,",
            "year": 2022
        },
        {
            "authors": [
                "Shao-Hua Sun",
                "Te-Lin Wu",
                "Joseph J Lim"
            ],
            "title": "Program guided agent",
            "venue": "In ICLR,",
            "year": 2020
        },
        {
            "authors": [
                "Stefanie Tellex",
                "Thomas Kollar",
                "Steven Dickerson",
                "Matthew Walter",
                "Ashis Banerjee",
                "Seth Teller",
                "Nicholas Roy"
            ],
            "title": "Understanding Natural Language Commands for Robotic Navigation and Mobile Manipulation",
            "venue": "In AAAI,",
            "year": 2011
        },
        {
            "authors": [
                "Rodrigo Toro Icarte",
                "Toryn Q. Klassen",
                "Richard Valenzano",
                "Sheila A. McIlraith"
            ],
            "title": "Teaching multiple tasks to an rl agent using ltl",
            "venue": "In AAMAS,",
            "year": 2018
        },
        {
            "authors": [
                "Guanzhi Wang",
                "Yuqi Xie",
                "Yunfan Jiang",
                "Ajay Mandlekar",
                "Chaowei Xiao",
                "Yuke Zhu",
                "Linxi Fan",
                "Anima Anandkumar"
            ],
            "title": "Voyager: An Open-Ended Embodied Agent with Large Language Models",
            "year": 2023
        },
        {
            "authors": [
                "Yanwei Wang",
                "Nadia Figueroa",
                "Shen Li",
                "Ankit Shah",
                "Julie Shah"
            ],
            "title": "Temporal logic imitation: Learning plan-satisficing motion policies from demonstrations",
            "venue": "arXiv preprint arXiv:2206.04632,",
            "year": 2022
        },
        {
            "authors": [
                "Yanwei Wang",
                "Ching-Yun Ko",
                "Pulkit Agrawal"
            ],
            "title": "Visual pre-training for navigation: What can we learn from noise",
            "venue": "arXiv preprint arXiv:2207.00052,",
            "year": 2022
        },
        {
            "authors": [
                "Yuke Zhu",
                "Josiah Wong",
                "Ajay Mandlekar",
                "Roberto Mart\u0131\u0301n-Mart\u0131\u0301n",
                "Abhishek Joshi",
                "Soroush Nasiriany",
                "Yifeng Zhu"
            ],
            "title": "robosuite: A modular simulation framework and benchmark for robot learning",
            "venue": "arXiv preprint arXiv:2009.12293,",
            "year": 2009
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Language is a flexible medium that people use to communicate about physical interactions. We use nouns and adjectives to identify objects, and verbs to describe ways that they interact. Language models, in particular, pretrained large language models (LLMs) have acquired a large amount of knowledge about physical interactions in an abstract space. However, a grand open challenge lies in extracting such knowledge and grounding it in physical domains to build robust and flexible systems.\nPrevious methods, given the symbolic and abstract nature of language, primarily focus on leveraging LLMs to propose abstract actions or policies in purely symbolic spaces or on top of manually defined high-level primitive abstractions (Liu et al., 2023; Ahn et al., 2022; Wang et al., 2023). By contrast, this work aims to ground of such knowledge in low-level manipulation trajectories, leveraging the idea of mode families (Hauser & Latombe, 2010; Mason, 2001) from manipulation planning literature. Our key idea is that many verbs, such as reach, grasp, release, and align, are all grounded on top of mode families in the configuration spaces. A mode family defines a specific type of motion constraint among a set of objects and can be chained together to represent complex manipulation behaviors, such as reaching the nut, grasping the nut, aligning the nut with the peg, and finally inserting the nut.\nLeveraging language models and grounding them in mode families in robot motion brings several advantages. First, understanding demonstration trajectories as a sequence of segments where each lies in a specific mode family allows us to recover an interpretable representation of the environmental constraints and the task specification. Second, in natural setups where we only have access to a limited number of successful demonstrations from human experts, the inductive biases that each segment corresponds to a single motion constraint and that modes can be chained together to achieve goals provide a principled way for us to automatically generate counterfactual experiments in the physical environment and guide the learning of the agent. Third, during inference time, by leveraging classifiers for high-level mode families and chaining per-mode policies, we can improve the policy robustness at both the local motion level and at the global task level (e.g., manually dropping objects).\nTo this end, we propose our framework, Manipulation Modes from Language Plans (MMLP) for learning manipulation mode families from language plans and counterfactual perturbations. Illustrated\nin Fig. 1, given a small number of human demonstrations for the task and a small language description of the task, our goal is to automatically reconstruct the sequence of mode switches required to achieve the task, and learn classifiers and control policies for each individual mode families. The mode classification task can be cast as predicting discrete labels from continuous-valued states. This is a challenging task. First, accurate learning of the boundaries of modes naturally requires dense annotations of the states inside and outside the mode. However, when humans demonstrate, they usually only provide a small number of trajectories and without annotating exactly the boundaries where the transitions occur. To address both challenges, we employ the idea of counterfactual perturbations imposed on a few successful human demonstrations to generate additional trajectories, and use the execution results of these counterfactual trajectories (whether the execution results in task success or not) as distant supervisions for learning.\nIn particular, MMLP uses an explanation-based learning paradigm (DeJong & Mooney, 1986; Segre & DeJong, 1985) that recovers a mode family and transition-based explanation for successful and failed trajectories. MMLP works in four stages. First, given a short task description, we prompt large language models to generate a multi-step physical plan in the language space. Second, given an initial (small) set of successful demonstrations, we generate a large amount of counterfactual perturbed trajectories based on them, and explore them in the environment to get reward feedback. Third, we learn a classifier for each mode family denoted in the language plan. Finally, we use the learned classifier to segment all trajectories and learn mode-specific policies for each individual mode family.\nWe showcase our framework in two environments: a simple 2D continuous-space domain and Robosuite (Zhu et al., 2020) a simulated robot manipulation environment, which involves rich motion constraints across different modes. Experiments show that our learning paradigm can successfully identify each mode from the demonstration data without any human segmentation annotations, and from only a small number of expert-generated demonstrations. Based on the mode family classifiers and the corresponding policies, we show that our model significantly improves the baseline methods in terms of robustness, both at the low-level motion and the high-level task levels. We also show that by learning an interpretable explanation of the trajectories, we can automatically generate visualizations that help humans to understand the failure modes of the current policies and optionally provide an active learning protocol to collect trajectories."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Learning abstractions from demonstrations. A large body of work has been focusing on learning action abstractions from language and interaction. This includes the grounding of natural language (Corona et al., 2021; Andreas et al., 2017; Andreas & Klein, 2015; Jiang et al., 2019; Sharma et al., 2022; Luo et al., 2023), programs (Sun et al., 2020), and linear temporal logic (LTL) formulas (Bradley et al., 2021; Toro Icarte et al., 2018; Tellex et al., 2011). In contrast to learning policies for individual action terms, this paper focuses on learning mode families in robot manipulation domains. These learned mode families enable us to construct robust policies under perturbation (Wang et al., 2022a). Furthermore, our framework is capable of recovering the underlying modes from a small number of unsegmented demonstrations. Through the use of synthetic noise (Baradad Jurjo et al., 2021; Wang\net al., 2022b), we relieve humans from the burden of providing negative demonstrations that fail a task by automatically generating positive and negative variations of task executions.\nGrounding language in robot behavior. With the rise of LLMs that can decompose high-level commands into sequences of actions, there has been much recent interest in the ability to ground these commands in embodied agents, such as robots. Given that data from environment interactions (including human demonstrations) do not explicitly identify constraints and success criteria of a task, previous work has investigated how to infer affordances directly from observations Ahn et al. (2022). Compared to prior work, our method does not require dense labels to learn a grounding operator (e.g., Lynch et al. (2023)). We are also not directly using large language models for planning Huang et al. (2022); Li et al. (2022); Huang et al. (2023). Rather, we are using LLM to guide the discovery of mode abstractions in demonstrations, and as a result, we can also get a grounding operator for high-level language commands. In contrast to the discovery of language-conditioned skills Lynch & Sermanet (2020); Garg et al. (2022), which can consist of multiple modes, our mode decomposition happens at a lower level and can explain why certain trajectories fail a task execution.\nCounterfactuals. Counterfactuals describe hypothetical situations of what could have happened instead of the original data (Byrne, 2019). In other words, they are fake (non-human generated) data with a different outcome (Karimi et al., 2020). In this paper, we define counterfactual perturbations as non-human-generated synthetic probes that test which parts of the time-series trajectory data (Delaney et al., 2021) demonstrated by humans have implicit constraints, the violation of which will change the outcome of the successful human demonstrations."
        },
        {
            "heading": "3 GROUNDING LANGUAGE PLANS",
            "text": "Our goal is to reconstruct the sequence of mode families and switches necessary for accomplishing each task, and to learn classifiers and control policies for each mode family, given a small number of human demonstrations and a brief language description of the task. MMLP adopts an explanationbased learning paradigm and operates in four stages: prompting LLMs with a short task description to create a multi-step physical plan (Section 3.1); generating a vast amount of counterfactual perturbed trajectories based on a small set of successful demonstrations, and subsequently, learning a classifier for each mode family outlined in the plan (Section 3.2); and using the learned classifiers to segment all trajectories and derive mode-specific policies (Section 3.3)."
        },
        {
            "heading": "3.1 GENERATING LANGUAGE PLANS",
            "text": "Shown in Fig. 2, given a description of the task inclduing the objects involved, and the language description of the goal) we prompt LLMs to generate three important pieces of information: mode families and switches, keypoints on objects to track, and relevant features for each individual modes.\nIn particular, first and foremost, we ask LLMs to generate a sequence of mode families and how they should be chained together given the task description. In the nut insertion example, it produces four\nmode families. We did not explicitly use the language description of each mode in the generated description. Instead, we only use the sequential structure in LLM generations to construct 1) the number of modes involved in the task and 2) the feasibility matrix of transitions among these modes\u2014 in a plain sequential structure, transitions are only allowed between mode i \u2212 1 and i. We use sequential structures throughout the paper, but our learning algorithm can accept any directed acyclic mode transition graphs. In particular, we represent the transition graph as a matrix where each entry Ti,j = max(i\u2212 j + 1, 0), encoding a transition feasibility weight from mode i to mode j. Second, for each mode, we further prompt LLMs to generate a set of keypoints on objects to track, and a set of key features for each mode. In particular, each keypoint definition contains 1) the object name and 2) a short description of the name of the keypoint. One could imagine using vision-language models to identify the keypoints on perceptual inputs as in Huang et al. (2023). In our case, because we are only tackling a small number of objects, we simply manually label them based on the LLM suggestions. The generated features are defined over the keypoints produced by the LLMs. Each mode can be associated with one feature, and it can either be an absolute location of a keypoint (on the object or on the robot gripper), or a relative position between two keypoints. The keypoint features will be used as additional state representations for our model (as well as all baselines). In addition, later we will use this feature for each mode family to define a pseudo attractor in the configuration space in order to improve the robustness of the learned policies."
        },
        {
            "heading": "3.2 MODE GROUNDING FROM COUNTERFACTUAL PERTURBATIONS",
            "text": "Given the abstract language plan generated by the LLMs, our next step is to ground each mode family defined by the LLM on physical states. This is a challenging task given only a small number of (unsegmented) human demonstrations because the accurate identification of the boundaries between modes naturally requires dense annotation. Illustrated in Fig. 3, the high-level idea we are going to explore in this section is to leverage the definitions and essential principles of modes and mode transitions to 1) develop a novel counterfactual perturbation strategy that can efficiently generate trajectories to explore that can potentially helps mode classification learning, and 2) a loss function that can recover mode families from unsegmented trajectories based the binary task success signal and the mode transition feasibility matrix generated from LLMs.\nCounterfactual perturbation. We generate counterfactual trajectories by perturbing the original successful demonstration data. A key challenge is determining what perturbations are likely to expose novel task failures. The conventional practice for adding robot trajectory perturbation is to add Gaussian noises to the action space of the agent (e.g., the end-effector pose). However, the scale of the noise can be very hard to tune: if the scale is too small, then we end up not gathering signals for the boundaries of the modes. On the contrary, if the scale is too large, it becomes unlikely for the robot to recover from the perturbations. Based on the idea that mode families are defined by motion constraints and that task success can be defined by whether the robot has successfully traversed all mode families, we propose the following perturbations:\n\u2022 End-effector perturbations: Illustrated in Fig. 4a and b, given a successful demonstration, we first sample two points on the trajectory, namely A and B. Next, we randomly sample a third point C in the robot configuration space that is close to both points. In the counterfactual\nperturbation, instead of following the trajectory from A to B, we directly interpolate between A and C, and C and B, and stitch two trajectories together to replace the AB segment. This allows us to probe for whether two modes A and B can be directly connected or have to go through an intermediate mode switch. Shown in Fig. 4a, since our counterfactual interpolation between two points A and B (connected by the red dots) misses the important mode switch where the robot grasps the target object, this perturbation yields a failed execution. Fig. 4b illustrates a scenario where the local perturbation between two points in the same mode family yields another successful trajectory.\n\u2022 Gripper perturbations: Illustrated in Fig. 4c, we randomly toggle the gripper state while otherwise adhering to the original trajectory. This allows us to probe for mode transitions related to manipulated objects.\nGiven the perturbed trajectories, we execute them using a trajectory following controller in the environment, and collect a binary task success signal for each trajectory. Essentially, this gives us a dataset of paired trajectories and their task success labels: \u27e8traji, succi\u27e9. Our next step is to learn a classifier for each individual mode, given this dataset.\nLearning mode family classifiers. Our method learns a neural network-based classifier for each mode family, and it is agnostic to the particular architecture of the classifiers. Since we are using state-space representations composed of gripper poses and object poses, across all our experiments, we are using simple Multi-Layer Perceptrons (MLPs) as the model for mode classification. In particular, the input to the mode family classifier \u03d5 is the state s, and the output is a categorical distribution of the mode at this state. The number of modes K is chosen based on the sequence length of the LLM-generated language plan. For a randomly sampled trajectory traj = {s0, s1, \u00b7 \u00b7 \u00b7 , sT }, let \u03d5(sj)k] be the probability that state sj is classified as belonging to mode k.\nThe design of the training loss is based on two important principles: motion constraints within each mode and mode transitions across the trajectory.\nFirst, consider an underactuated environment where a world configuration q is composed of the robot configuration qrobot and the external object configuration qexternal. Here, qrobot consists of the degree of freedoms (DoFs) that we can control and qexternal consists of the DoFs that we can not directly control. A motion constraint within a single mode can therefore be written as c(qrobot, qexternal) = 0. In the quasi-static manipulation settings, this constraint is essentially the kinematic constraints (Mason, 2001). For example, when the robot is rigidly holding an object with a grasping pose g, the relative transformation between the end-effector pose and the object pose will be fixed as g as long as the robot stays in this mode. This motivates the following loss term Lmotion. For two consecutive steps j and j + 1, if they are in the same mode, then we can perfectly reconstruct the external object configuration at step j + 1: qj+1external based on the current robot and object configuration qjrobot, q j+1 external, and the robot configuration at j + 1. Therefore, for each mode k, we additionally train a regression model \u03c8k by:\nLmotion = T\u22121\u2211 j=0 K\u2211 k=1 \u03d5(sj)k\u03d5(sj+1)k \u00b7 \u2225\u03c8k(qjrobot, q j+1 robot, q j external)\u2212 q j+1 external\u2225 2 2\nOur next loss considers mode transitions between states in the trajectory. Based on the assumption, we know that we can only enter mode k+1 if we are already at mode k (or in general, as specified in the mode transition feasibility matrix T ). Our transition loss term Ltrans is defined as:\nLtrans = \u03c4(succ) T\u22121\u2211 j=0 K\u2211 k=1 K\u2211 \u2113=1 \u03d5(sj)k\u03d5(sj+1)\u2113 \u00b7 Tk,\u2113,\nwhere \u03c4(succ) = 0 if the trajectory is a successful trajectory and \u03c4(succ) = \u22121 if it failed. Essentially, if a trajectory is successful, we ensure that each pair of consecutive states forms a feasible transition. Otherwise, some of the pairs should be infeasible.\nWe define two additional loss terms Lstart and Lend that enforce the first state s0 belongs to mode k = 1 and the last state sT belongs to mode k = K, respectively. The total loss is the summation of four loss terms: L = Lmotion + Ltrans + Lstart + Lend. Since all loss terms are differentiable with respect to \u03d5 and \u03c8, at each training iteration, we randomly sample a batch of trajectories and use stochastic gradient descent to optimize all learnable parameters."
        },
        {
            "heading": "3.3 MODE-SPECIFIC POLICY LEARNING",
            "text": "Given the mode classifiers, our next step is to construct a per-mode control policy that can map from states to robot action commands (e.g., end-effector position control commands). In spaces where the state space is of low dimensions, and there is no motion constraint, we can directly construct controllers using sampling-based or optimization-based motion planners (LaValle, 1998). In more complex scenarios, one can train a neural network-based policy to move from the current mode to the next mode. Our algorithm is agnostic to the actual choice of the neural network architecture.\nAs an illustration of the usage of the learned modes, we construct our per-mode policy \u03c0k(a|s) that maps from state s to action a in the following way. First, we use the learned mode classifier \u03d5 to segment each successful trajectory into pieces corresponding to different modes. Therefore, for each mode k, we essentially obtains a dataset of state-action pairs {(si, ai)}. We then use this dataset to directly supervise the learning of an MLP-based policy \u03c0k using behavior cloning.\nTo further improve the robustness of the learned policy, we use the mode feature identified by the LLM (e.g., the relative pose between the robot end-effector and the nut) to construct a pseudo-attractor for each mode. We first segment the successful trajectories, and for each mode k we collect the first state that belongs to k. Then, we compute the pseudo-attractor for mode k as the mean configuration for the mode feature, across all first states (by taking the average in the configuration space along all DoFs). If the mode feature is an absolute pose of the robot end-effector, then we use this mean configuration as the pseudo-attractor; if it is a relative pose, we transform that into an absolute pose of the end-effector at test time. We use this pseudo-attractor to construct a potential field that guides the robot to move towards the next mode at inference time. Specifically, the final policy output \u03c0\u2217k(a|s) is a weighted sum of the original \u03c0k(a|s), and a control command that moves the end-effector towards the pseudo-attractor for mode k + 1. We only apply the pseudo-attractor term when the distance between the current state and the pseudo-attractor is greater than a threshold. Intuitively, when the robot receives a large perturbation that leads to out-of-distribution states, the potential field term will drive the end-effector to a position that is close to the next goal and then switches back \u03c0k."
        },
        {
            "heading": "4 EXPERIMENT",
            "text": "We evaluate our methods on two benchmarks: a synthetic 2D polygon domain with a 2D configuration space so that we can systematically evaluate and visualize learned modes and policies, and RoboSuite (Zhu et al., 2020) for simulated robot manipulation tasks."
        },
        {
            "heading": "4.1 2D POLYGON DOMAIN",
            "text": "Setup. Each 2D polygon environment contains a 2D continuous configuration space [\u22128, 8]2 and K \u2212 1 modes generated as connected convex polygons in the 2D space. The special k = 1 mode corresponds to any area outside the K \u2212 1 polygons. See Fig. 5a-b for examples. The goal is to start from a random point in the k = 1 mode, traverse through all K modes, and finally, reach a configuration inside the last mode. There is no motion constraint within each mode. At each step, the agent at state (x, y) can take an action (dx, dy) and it will move to (x+ dx, y + dy). We restrict the action space that that \u2225(dx, dy)\u222522 \u2264 0.1. For all methods, we use 20 successful demonstrations.\nMethod. We compare our approach with a plain behavior cloning (BC) baseline that is unaware of the underlying mode. In particular, instead of training a set of mode-specific policies, the BC baseline learns a single policy \u03c0(a|s) from all successful trajectories. By contrast, our mode-conditional BC method first segments the demonstration trajectories and then learns mode-specific policies. Since this is a low-dimensional configuration space, we also implemented a variant of our method (MMLP-Stable) that implements a stable policy: it uses the trajectory segmentation to compute pseudo-attractors for all mode families. Next, it uses RRT to compute waypoints and then uses potential fields to construct a stable policy. If the mode classification is accurate, this will essentially produce a stable policy that is guaranteed to reach the goal region starting at any initial configuration. For simplicity, here we do not add the pseudo-attractor term for mode-conditional BC.\nResult. Table 1 summarizes the result on the task success rate. We study two evaluation settings for all models: without perturbation and with perturbation. In the with-perturbation setting, at each step, there is a small chance (p = 0.05) that the robot will be teleported to a randomly generated configuration (x, y), for at most 5 times in an episode. The BC baseline sees the most significant drop under perturbation, especially for 4-mode and 5-mode scenarios. Fig. 5a-b shows the learned mode families and compares them with the groundtruth. The accuracy is greater than 97% across all three settings. We provide additional classification accuracy analysis in Appendix A.1.\nOn the contrary, adding a mode-conditional policy significantly improves the robustness of the policy. Recall that in this experiment, we are not using pseudo-attractor for the mode-conditional policy. Therefore, this performance gain is completely brought by the introduction of mode conditioning. This can be further validated by visualizing the policy constructed by each method (Fig. 5c-d): the mode-conditional policy learns to avoid entering later modes directly from mode 1. The grounding of modes further allows us to construct a stable policy that yields a nearly perfect success rate.\nInterpretability. In the 2D environment, the learned mode families are directly interpretable, and we can also use the policy visualization to find areas that require more demonstrations."
        },
        {
            "heading": "4.2 ROBOSUITE",
            "text": "Setup. We tested our method across three tasks from Robosuite: placing a can in a bin (can), lifting a block (lift), and inserting a square nut into a peg (square). In general, we used the default action and observation space of each environment, other than in cases where the prompting of the LLM indicated that particular variables (e.g., distance relative to an object or keypoints) would be helpful\nfor mode classification. For each environment, we also use heuristic rules to define the groundtruth modes, which we include details in Appendix A.2.\nMethod. Again, we compare our approach to a behavior cloning (BC) baseline, \u03c0(a|s), that is trained on all successful trajectories and unaware of the underlying mode. We compared this baseline against the full method described in Section 3.3, where the same BC policy is augmented with the pseudo attractor term. While this input blending strategy is insufficient to recover from all potential failures in a manipulation domain, our goal was to demonstrate how even basic knowledge of the underlying mode families can benefit policy learning in robotics.\nMode classification result. Table 2 shows the results related to mode classification accuracy for different combinations of the proposed loss functions (ablating components of the loss). The results indicate the utility of including all of the loss terms (in terms of mode alignment) and show that our method can generally detect the ground truth modes. Our mode classification accuracy is lowest in the square task where several modes (e.g., transporting and aligning) have similar state information. We show qualitative examples in Appendix A.3.\nResult. Table 3 summarizes the result on the task success rate. We again study the models without and with perturbation. In the with-perturbation setting, at each step, there is a small chance that the end-effector will be displaced over future steps to create large net displacements. We see that for both methods, adding perturbations introduces some amount of performance drop. We find that the performance degradation for the BC baseline is much higher than with MMLP-Conditional.\nInterpretability. In manipulation environments, it is challenging to directly visualize the mode families given the high-dimensiontal state space. However, exposing the mode families allows us to easily identify mode transition failures which can be used to generate post-hoc explanations of failures (e.g., a video that shows the invalid mode transition associated with a task failure)."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "In conclusion, this work introduces a groundbreaking framework, Manipulation Modes from Language Plans (MMLP), effectively grounding the knowledge within large language models into physical domains, via mode families. Given a small number of human demonstrations and task descriptions, MMLP successfully recovers mode families and their transitions required in the task and enables the learning of robust control policies.\nLimitations and future work. While MMLP does not need a large number of human demonstrations, it requires a large number of trial-and-errors and an environment with a reset capability in order to collect task success labels of a trajectory. This data inefficiency, however, can be addressed through active learning where the current belief of mode segmentation can be used to probe demonstrations only in regions with high uncertainty. Additionally, prompting the LLM to find a suitable state representation for learning the classifier also requires skill. In future work, we would like to learn the state representation in conjunction with the mode classifiers in an end-to-end fashion."
        },
        {
            "heading": "A APPENDIX",
            "text": "A.1 MODE CLASSIFICATION IN THE POLYGON ENVIRONMENT\nMode classification result. Table 4 shows the result on the mode classification accuracy. Our model is capable of accurately identifying mode families in the 2D environment, yielding a nearly perfect classification accuracy.\nAblation: the necessity of negative data. Another possible way to use perturbation is to simply generate new successful trajectories by slightly perturbing given demonstrations. We implement this baseline, namely MMLP (w/o. fail). It uses the same counterfactual perturbation generation protocol as ours, but only retains those successful trajectories for training. Here, we evaluate them on the mode classification accuracy. Shown in Table 4, using perturbation to generate both successful and failed trajectories significantly outperforms the variant that uses only successful trajectories. In particular, through visualization, we found that in the 4-Mode example, MMLP (w/o. fail) even fails to find 4 different modes and generates a mode segmentation that only contains 3 modes.\nA.2 HEURISTIC RULES FOR MODE FAMILY GROUNDTRUTH IN ROBOSUITE\nWe use the following heuristic rules to define the groundtruth mode families for evaluation purposes.\n\u2022 Can (3 modes): the ground truth modes are reaching for the can (until the end effector makes contact with the can), transporting the can to the bin, and finally hovering about the target bin. \u2022 Lift (3 modes): the ground truth modes are reaching for the cube (until the end effector makes contact with the cube), lifting the cube off the table, and finally moving to a certain height above the table. \u2022 Square (4 modes): the ground truth modes are reaching for the nut (until the end effector makes contact with the nut), transporting the nut to the peg, aligning the nut above the peg, and finally lowering the nut into the assembled position.\nA.3 VISUALIZATION OF MODE SEGMENTATION IN ROBOSUITE\nFig. 6 shows the visualization of the mode segmentation from MMLP, compared to the groundtruth on the can placing task. Our model faithfully identifies all the modes and yields a high consistency with the modes defined by human-crafted rules. For the last mode (highlighted in yellow), MMLP only associates it with the very last few frames in the trajectory. Therefore it\u2019s barely visible in the visualization."
        }
    ],
    "title": "GROUNDING LANGUAGE PLANS IN DEMONSTRATIONS THROUGH COUNTER-FACTUAL PERTURBATIONS",
    "year": 2023
}