{
    "abstractText": "Numerous explainability methods have been proposed to shed light on the inner workings of GNNs. Despite the inclusion of empirical evaluations in all the proposed algorithms, the interrogative aspects of these evaluations lack diversity. As a result, various facets of explainability pertaining to GNNs, such as a comparative analysis of counterfactual reasoners, their stability to variational factors such as different GNN architectures, noise, stochasticity in non-convex loss surfaces, feasibility amidst domain constraints, and so forth, have yet to be formally investigated. Motivated by this need, we present a benchmarking study on perturbation-based explainability methods for GNNs, aiming to systematically evaluate and compare a wide range of explainability techniques. Among the key findings of our study, we identify the Pareto-optimal methods that exhibit superior efficacy and stability in the presence of noise. Nonetheless, our study reveals that all algorithms are affected by stability issues when faced with noisy data. Furthermore, we have established that the current generation of counterfactual explainers often fails to provide feasible recourses due to violations of topological constraints encoded by domain-specific considerations. Overall, this benchmarking study empowers stakeholders in the field of GNNs with a comprehensive understanding of the state-of-the-art explainability methods, potential research problems for further enhancement, and the implications of their application in real-world scenarios.",
    "authors": [
        {
            "affiliations": [],
            "name": "IN-DEPTH BENCHMARKING"
        },
        {
            "affiliations": [],
            "name": "Mert Kosan"
        },
        {
            "affiliations": [],
            "name": "Samidha Verma"
        },
        {
            "affiliations": [],
            "name": "Burouj Armgaan"
        },
        {
            "affiliations": [],
            "name": "Khushbu Pahwa"
        },
        {
            "affiliations": [],
            "name": "Ambuj Singh"
        },
        {
            "affiliations": [],
            "name": "Sourav Medya"
        },
        {
            "affiliations": [],
            "name": "Sayan Ranu"
        }
    ],
    "id": "SP:fe9165b85d0f02a8e84ea223fef76decb44fb541",
    "references": [
        {
            "authors": [
                "Carlo Abrate",
                "Francesco Bonchi"
            ],
            "title": "Counterfactual graphs for explainable classification of brain networks",
            "venue": "In KDD,",
            "year": 2021
        },
        {
            "authors": [
                "Chirag Agarwal",
                "Owen Queen",
                "Himabindu Lakkaraju",
                "Marinka Zitnik"
            ],
            "title": "Evaluating explainability for graph neural networks",
            "year": 2023
        },
        {
            "authors": [
                "Miltiadis Allamanis",
                "Earl T Barr",
                "Premkumar Devanbu",
                "Charles Sutton"
            ],
            "title": "A survey of machine learning for big code and naturalness",
            "venue": "ACM Computing Surveys (CSUR),",
            "year": 2018
        },
        {
            "authors": [
                "Kenza Amara",
                "Zhitao Ying",
                "Zitao Zhang",
                "Zhichao Han",
                "Yang Zhao",
                "Yinan Shan",
                "Ulrik Brandes",
                "Sebastian Schemm",
                "Ce Zhang"
            ],
            "title": "Graphframex: Towards systematic evaluation of explainability methods for graph neural networks",
            "venue": "In The First Learning on Graphs Conference,",
            "year": 2022
        },
        {
            "authors": [
                "Steve Azzolin",
                "Antonio Longa",
                "Pietro Barbiero",
                "Pietro Lio",
                "Andrea Passerini"
            ],
            "title": "Global explainability of GNNs via logic combination of learned concepts",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Mohit Bajaj",
                "Lingyang Chu",
                "Zi Yu Xue",
                "Jian Pei",
                "Lanjun Wang",
                "Peter Cho-Ho Lam",
                "Yong Zhang"
            ],
            "title": "Robust counterfactual explanations on graph neural networks",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Federico Baldassarre",
                "Hossein Azizpour"
            ],
            "title": "Explainability techniques for graph convolutional networks",
            "venue": "arXiv preprint arXiv:1905.13686,",
            "year": 2019
        },
        {
            "authors": [
                "Ravinder Bhattoo",
                "Sayan Ranu",
                "NM Krishnan"
            ],
            "title": "Learning articulated rigid body dynamics with lagrangian graph neural network",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Ravinder Bhattoo",
                "Sayan Ranu",
                "NM Anoop Krishnan"
            ],
            "title": "Learning the dynamics of particle-based systems with lagrangian graph neural networks",
            "venue": "Machine Learning: Science and Technology,",
            "year": 2023
        },
        {
            "authors": [
                "Suresh Bishnoi",
                "Ravinder Bhattoo",
                "Sayan Ranu",
                "NM Krishnan"
            ],
            "title": "Enhancing the inductive biases of graph neural ode for modeling dynamical systems",
            "year": 2023
        },
        {
            "authors": [
                "Karsten M Borgwardt",
                "Cheng Soon Ong",
                "Stefan Sch\u00f6nauer",
                "SVN Vishwanathan",
                "Alex J Smola",
                "Hans-Peter Kriegel"
            ],
            "title": "Protein function prediction via graph kernels",
            "venue": "i47\u2013i56,",
            "year": 2005
        },
        {
            "authors": [
                "Yuwei Cao",
                "Hao Peng",
                "Jia Wu",
                "Yingtong Dou",
                "Jianxin Li",
                "Philip S Yu"
            ],
            "title": "Knowledge-preserving incremental social event detection via heterogeneous gnns",
            "venue": "In Proceedings of the Web Conference",
            "year": 2021
        },
        {
            "authors": [
                "Pritish Chakraborty",
                "Sayan Ranu",
                "Krishna Sri Ipsit Mantri",
                "Abir De"
            ],
            "title": "Learning and maximizing influence in social networks under capacity constraints",
            "venue": "In Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining,",
            "year": 2023
        },
        {
            "authors": [
                "Asim Kumar Debnath",
                "Rosa L Lopez de Compadre",
                "Gargi Debnath",
                "Alan J Shusterman",
                "Corwin Hansch"
            ],
            "title": "Structure-activity relationship of mutagenic aromatic and heteroaromatic nitro compounds. correlation with molecular orbital energies and hydrophobicity",
            "venue": "Journal of medicinal chemistry,",
            "year": 1991
        },
        {
            "authors": [
                "Paul D Dobson",
                "Andrew J Doig"
            ],
            "title": "Distinguishing enzyme structures from non-enzymes without alignments",
            "venue": "Journal of molecular biology,",
            "year": 2003
        },
        {
            "authors": [
                "Junfeng Fang",
                "Xiang Wang",
                "An Zhang",
                "Zemin Liu",
                "Xiangnan He",
                "Tat-Seng Chua"
            ],
            "title": "Cooperative explanations of graph neural networks",
            "venue": "In Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining,",
            "year": 2023
        },
        {
            "authors": [
                "Nikhil Goyal",
                "Harsh Vardhan Jain",
                "Sayan Ranu"
            ],
            "title": "Graphgen: a scalable approach to domainagnostic labeled graph generation",
            "venue": "In Proceedings of The Web Conference",
            "year": 2020
        },
        {
            "authors": [
                "Mridul Gupta",
                "Hariprasad Kodamana",
                "Sayan Ranu"
            ],
            "title": "FRIGATE: Frugal spatio-temporal forecasting on road networks",
            "venue": "In 29th SIGKDD Conference on Knowledge Discovery and Data Mining,",
            "year": 2023
        },
        {
            "authors": [
                "William L. Hamilton",
                "Rex Ying",
                "Jure Leskovec"
            ],
            "title": "Inductive representation learning on large graphs",
            "venue": "In Proceedings of the 31st International Conference on Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Qiang Huang",
                "Makoto Yamada",
                "Yuan Tian",
                "Dinesh Singh",
                "Yi Chang"
            ],
            "title": "Graphlime: Local interpretable model explanations for graph neural networks",
            "venue": "IEEE Transactions on Knowledge and Data Engineering,",
            "year": 2022
        },
        {
            "authors": [
                "Zexi Huang",
                "Mert Kosan",
                "Sourav Medya",
                "Sayan Ranu",
                "Ambuj Singh"
            ],
            "title": "Global counterfactual explainer for graph neural networks",
            "venue": "In Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining,",
            "year": 2023
        },
        {
            "authors": [
                "Sergei Ivanov",
                "Sergei Sviridov",
                "Evgeny Burnaev"
            ],
            "title": "Understanding isomorphism bias in graph data sets",
            "venue": "Geometric Learning and Graph Representations ICLR Workshop,",
            "year": 2019
        },
        {
            "authors": [
                "Jayant Jain",
                "Vrittika Bagadia",
                "Sahil Manchanda",
                "Sayan Ranu"
            ],
            "title": "Neuromlr: Robust & reliable route recommendation on road networks",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Jaykumar Kakkad",
                "Jaspal Jannu",
                "Kartik Sharma",
                "Charu Aggarwal",
                "Sourav Medya"
            ],
            "title": "A survey on explainability of graph neural networks",
            "venue": "arXiv preprint arXiv:2306.01958,",
            "year": 2023
        },
        {
            "authors": [
                "Jeroen Kazius",
                "Ross McGuire",
                "Roberta Bursi"
            ],
            "title": "Derivation and validation of toxicophores for mutagenicity prediction",
            "venue": "Journal of medicinal chemistry,",
            "year": 2005
        },
        {
            "authors": [
                "Thomas N Kipf",
                "Max Welling"
            ],
            "title": "Semi-supervised classification with graph convolutional networks",
            "venue": "arXiv preprint arXiv:1609.02907,",
            "year": 2016
        },
        {
            "authors": [
                "Mert Kosan",
                "Arlei Silva",
                "Sourav Medya",
                "Brian Uzzi",
                "Ambuj Singh"
            ],
            "title": "Event detection on dynamic graphs",
            "venue": "arXiv preprint arXiv:2110.12148,",
            "year": 2021
        },
        {
            "authors": [
                "Mert Kosan",
                "Arlei Silva",
                "Ambuj Singh"
            ],
            "title": "Robust ante-hoc graph explainer using bilevel optimization",
            "venue": "arXiv preprint arXiv:2305.15745,",
            "year": 2023
        },
        {
            "authors": [
                "Xiucheng Li",
                "G. Cong",
                "Yun Cheng"
            ],
            "title": "Spatial transition learning on road networks with deep probabilistic models",
            "venue": "IEEE 36th International Conference on Data Engineering (ICDE),",
            "year": 2020
        },
        {
            "authors": [
                "Wanyu Lin",
                "Hao Lan",
                "Baochun Li"
            ],
            "title": "Generative causal explanations for graph neural networks",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Wanyu Lin",
                "Hao Lan",
                "Baochun Li"
            ],
            "title": "Generative causal explanations for graph neural networks",
            "venue": "In ICML,",
            "year": 2021
        },
        {
            "authors": [
                "Ana Lucic",
                "Maartje A Ter Hoeve",
                "Gabriele Tolomei",
                "Maarten De Rijke",
                "Fabrizio Silvestri"
            ],
            "title": "Cfgnnexplainer: Counterfactual explanations for graph neural networks",
            "venue": "In AISTATS,",
            "year": 2022
        },
        {
            "authors": [
                "Dongsheng Luo",
                "Wei Cheng",
                "Dongkuan Xu",
                "Wenchao Yu",
                "Bo Zong",
                "Haifeng Chen",
                "Xiang Zhang"
            ],
            "title": "Parameterized explainer for graph neural network. Advances in neural information processing",
            "year": 1962
        },
        {
            "authors": [
                "Sahil Manchanda",
                "Akash Mittal",
                "Anuj Dhawan",
                "Sourav Medya",
                "Sayan Ranu",
                "Ambuj Singh"
            ],
            "title": "Gcomb: Learning budget-constrained combinatorial algorithms over billion-sized graphs",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Siqi Miao",
                "Mia Liu",
                "Pan Li"
            ],
            "title": "Interpretable and generalizable graph learning via stochastic attention mechanism",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Phillip E Pope",
                "Soheil Kolouri",
                "Mohammad Rostami",
                "Charles E Martin",
                "Heiko Hoffmann"
            ],
            "title": "Explainability methods for graph convolutional neural networks",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Mario Alfonso Prado-Romero",
                "Bardh Prenkaj",
                "Giovanni Stilo",
                "Fosca Giannotti"
            ],
            "title": "A survey on graph counterfactual explanations: Definitions, methods, evaluation, and research challenges",
            "venue": "ACM Computing Surveys,",
            "year": 2023
        },
        {
            "authors": [
                "Ladislav Ramp\u00e1\u0161ek",
                "Mikhail Galkin",
                "Vijay Prakash Dwivedi",
                "Anh Tuan Luu",
                "Guy Wolf",
                "Dominique Beaini"
            ],
            "title": "Recipe for a General, Powerful, Scalable Graph Transformer",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Rishabh Ranjan",
                "Siddharth Grover",
                "Sourav Medya",
                "Venkatesan Chakaravarthy",
                "Yogish Sabharwal",
                "Sayan Ranu"
            ],
            "title": "Greed: A neural framework for learning graph distance functions",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Kaspar Riesen",
                "Horst Bunke"
            ],
            "title": "Iam graph database repository for graph based pattern recognition and machine learning. In Joint IAPR International Workshops on Statistical Techniques in Pattern Recognition (SPR) and Structural and Syntactic Pattern Recognition (SSPR)",
            "year": 2008
        },
        {
            "authors": [
                "Thomas Schnake",
                "Oliver Eberle",
                "Jonas Lederer",
                "Shinichi Nakajima",
                "Kristof T Sch\u00fctt",
                "Klaus-Robert M\u00fcller",
                "Gr\u00e9goire Montavon"
            ],
            "title": "Higher-order explanations of graph neural networks via relevant walks",
            "venue": "IEEE transactions on pattern analysis and machine intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Caihua Shan",
                "Yifei Shen",
                "Yao Zhang",
                "Xiang Li",
                "Dongsheng Li"
            ],
            "title": "Reinforcement learning enhanced explainer for graph neural networks",
            "venue": "NeurIPS",
            "year": 2021
        },
        {
            "authors": [
                "Juntao Tan",
                "Shijie Geng",
                "Zuohui Fu",
                "Yingqiang Ge",
                "Shuyuan Xu",
                "Yunqi Li",
                "Yongfeng Zhang"
            ],
            "title": "Learning and evaluating graph neural network explanations based on counterfactual and factual reasoning",
            "venue": "In Proceedings of the ACM Web Conference",
            "year": 2022
        },
        {
            "authors": [
                "Abishek Thangamuthu",
                "Gunjan Kumar",
                "Suresh Bishnoi",
                "Ravinder Bhattoo",
                "NM Krishnan",
                "Sayan Ranu"
            ],
            "title": "Unravelling the performance of physics-informed graph neural networks for dynamical systems",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Petar Veli\u010dkovi\u0107",
                "Guillem Cucurull",
                "Arantxa Casanova",
                "Adriana Romero",
                "Pietro Li\u00f2",
                "Yoshua Bengio"
            ],
            "title": "Graph attention networks",
            "venue": "In International Conference on Learning Representations,",
            "year": 2018
        },
        {
            "authors": [
                "Clement Vignac",
                "Igor Krawczuk",
                "Antoine Siraudin",
                "Bohan Wang",
                "Volkan Cevher",
                "Pascal Frossard"
            ],
            "title": "Digress: Discrete denoising diffusion for graph generation",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Philippe Vismara",
                "Claude Lauren\u00e7o"
            ],
            "title": "An abstract representation for molecular graphs, pp. 343\u2013366",
            "year": 2000
        },
        {
            "authors": [
                "Minh Vu",
                "My T Thai"
            ],
            "title": "Pgm-explainer: Probabilistic graphical model explanations for graph neural networks",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Nikil Wale",
                "Ian A Watson",
                "George Karypis"
            ],
            "title": "Comparison of descriptor spaces for chemical compound retrieval and classification",
            "venue": "Knowledge and Information Systems,",
            "year": 2008
        },
        {
            "authors": [
                "Xingchen Wan",
                "Henry Kenlay",
                "Binxin Ru",
                "Arno Blaas",
                "Michael Osborne",
                "Xiaowen Dong"
            ],
            "title": "Adversarial attacks on graph classifiers via bayesian optimisation",
            "venue": "In Thirty-Fifth Conference on Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Geemi P Wellawatte",
                "Aditi Seshadri",
                "Andrew D White"
            ],
            "title": "Model agnostic generation of counterfactual explanations for molecules",
            "venue": "Chemical science,",
            "year": 2022
        },
        {
            "authors": [
                "Hao Wu",
                "Ziyang Chen",
                "Weiwei Sun",
                "Baihua Zheng",
                "Wei Wang"
            ],
            "title": "Modeling trajectories with recurrent neural networks",
            "venue": "In Proceedings of the 26th International Joint Conference on Artificial Intelligence,",
            "year": 2017
        },
        {
            "authors": [
                "Yaochen Xie",
                "Sumeet Katariya",
                "Xianfeng Tang",
                "Edward Huang",
                "Nikhil Rao",
                "Karthik Subbian",
                "Shuiwang Ji"
            ],
            "title": "Task-agnostic graph explanations",
            "venue": "NeurIPS,",
            "year": 2022
        },
        {
            "authors": [
                "Keyulu Xu",
                "Weihua Hu",
                "Jure Leskovec",
                "Stefanie Jegelka"
            ],
            "title": "How powerful are graph neural networks",
            "venue": "In International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "Han Xuanyuan",
                "Pietro Barbiero",
                "Dobrik Georgiev",
                "Lucie Charlotte Magister",
                "Pietro Li\u00f3"
            ],
            "title": "Global concept-based interpretability for graph neural networks via neuron analysis",
            "year": 2023
        },
        {
            "authors": [
                "Pinar Yanardag",
                "SVN Vishwanathan"
            ],
            "title": "Deep graph kernels",
            "venue": "In Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining,",
            "year": 2015
        },
        {
            "authors": [
                "Chengxuan Ying",
                "Tianle Cai",
                "Shengjie Luo",
                "Shuxin Zheng",
                "Guolin Ke",
                "Di He",
                "Yanming Shen",
                "Tie-Yan Liu"
            ],
            "title": "Do transformers really perform badly for graph representation",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Rex Ying",
                "Dylan Bourgeois",
                "Jiaxuan You",
                "Marinka Zitnik",
                "Jure Leskovec"
            ],
            "title": "Gnnexplainer: Generating explanations for graph neural networks",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Zhitao Ying",
                "Dylan Bourgeois",
                "Jiaxuan You",
                "Marinka Zitnik",
                "Jure Leskovec"
            ],
            "title": "Gnnexplainer: Generating explanations for graph neural networks",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Jiaxuan You",
                "Rex Ying",
                "Xiang Ren",
                "William L. Hamilton",
                "Jure Leskovec"
            ],
            "title": "Graphrnn: Generating realistic graphs with deep auto-regressive models",
            "venue": "Proceedings of the 35th International Conference on Machine Learning,",
            "year": 2018
        },
        {
            "authors": [
                "Hao Yuan",
                "Jiliang Tang",
                "Xia Hu",
                "Shuiwang Ji"
            ],
            "title": "Xgnn: Towards model-level explanations of graph neural networks",
            "venue": "In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining,",
            "year": 2020
        },
        {
            "authors": [
                "Hao Yuan",
                "Haiyang Yu",
                "Jie Wang",
                "Kang Li",
                "Shuiwang Ji"
            ],
            "title": "On explainability of graph neural networks via subgraph explorations",
            "venue": "In ICML,",
            "year": 2021
        },
        {
            "authors": [
                "Hao Yuan",
                "Haiyang Yu",
                "Shurui Gui",
                "Shuiwang Ji"
            ],
            "title": "Explainability in graph neural networks: A taxonomic survey",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "Yue Zhang",
                "David Defazio",
                "Arti Ramesh"
            ],
            "title": "Relex: A model-agnostic relational model explainer",
            "venue": "In Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society,",
            "year": 2021
        },
        {
            "authors": [
                "TREE-GRID Ying"
            ],
            "title": "The base graph used in this dataset is a binary tree, and the motif is a 3\u00d7 3 grid connected to random nodes in the tree (Figure D(b)). Similar to the tree-cycles dataset, the nodes are labeled with binary classes (0 for the non-motif nodes and 1 for the motif nodes)",
            "year": 2019
        },
        {
            "authors": [
                "BA-SHAPES Ying"
            ],
            "title": "The base graph in this dataset is a Barabasi-Albert (BA) graph. The dataset includes house-shaped structures composed of 5 nodes (Figure D (c)). Non-motif nodes are assigned class 0, while nodes at the top, middle, and bottom of the motif are assigned classes",
            "year": 2019
        },
        {
            "authors": [
                "\u2022 MUTAG Ivanov"
            ],
            "title": "Mutagenicity Riesen ",
            "venue": "Kazius et al",
            "year": 2008
        },
        {
            "authors": [
                "AIDS: Ivanov"
            ],
            "title": "2019) This dataset contains small molecules. The nodes and edges are atoms and chemical bonds, respectively. The molecules are classified by whether they are active against the HIV virus",
            "year": 2019
        },
        {
            "authors": [
                "Proteins Borgwardt"
            ],
            "title": "These datasets are comprised of proteins categorized into enzymes and non-enzymes. The nodes represent amino acids, and an edge exists between two nodes if their distance is less than 6 Angstroms",
            "year": 2003
        },
        {
            "authors": [
                "Wale"
            ],
            "title": "This dataset is derived from cheminformatics and represents chemical compounds as input graphs. Vertices in the graph correspond to atoms, while edges represent bonds between atoms. This dataset focuses on anti-cancer screenings for cell lung cancer, with chemicals labeled as positive or negative",
            "year": 2008
        },
        {
            "authors": [
                "Yuan"
            ],
            "title": "The Graph-SST2 dataset is a graph-based dataset derived from the SST2 dataset Socher et al. (2013), which contains movie review sentences labeled with positive or negative sentiment. Each sentence in the Graph-SST2 dataset is transformed into a graph representation, with words as nodes and edges representing syntactic relationships capturing the sentence\u2019s grammatical structure. The sentiment labels from the original SST2 dataset",
            "year": 2022
        },
        {
            "authors": [
                "\u2022 ogbg-molhiv Allamanis"
            ],
            "title": "ogbg-molhiv is a molecule dataset with nodes representing atoms and edges representing chemical bonds. The node features represent various properties of the atoms like chirality, atomic number, formal charge etc. Edge attributes represent the bond type. We study binary classification task on this dataset. The task is to achieve the most accurate predictions of specific molecular properties",
            "year": 2018
        },
        {
            "authors": [
                "Veli\u010dkovi\u0107"
            ],
            "title": "2017)) for each graph classification dataset is presented in Table J. Table J: Test accuracy of black-box GNN \u03a6 trained for the graph classification task, averaged over 10 runs with random seeds. We train multiple GNNS for this task to test explainers for stability against GNN architectures",
            "venue": "GNN",
            "year": 2017
        },
        {
            "authors": [],
            "title": "https://github.com/divelab/DIG/tree/main/dig/xgraph/SubgraphX A.7 FEASIBILITY Counterfactual explanations: As shown in Table L, we observe statistically significant deviations from the expected values in two out of three molecular datasets",
            "year": 2021
        },
        {
            "authors": [
                "counter-factual Yuan"
            ],
            "title": "2022); a value close to 1 is desired. In node classification, we compute this proportion for edges in the N l",
            "year": 2022
        },
        {
            "authors": [
                "I FACTUAL"
            ],
            "title": "EXPLAINERS ON OGBG-MOLHIV Figure R demonstrates a new dataset OGBG-Molhiv for three factual explainers. On this dataset, three factual methods are close to each other in terms of sufficiency performance",
            "venue": "J EXISTING BENCHMARKING STUDIES ON GNN EXPLAINABILITY GraphFrameX Amara et al",
            "year": 2022
        },
        {
            "authors": [
                "plainer Ying"
            ],
            "title": "Empirical investigations: How susceptible are the explanations to topological noise, variations",
            "venue": "PGExplainer Luo et al. (2020), and SubgraphX Yuan et al. (2021)",
            "year": 2019
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION AND RELATED WORK",
            "text": "GNNs have shown state-of-the-art performance in various domains including social networks Manchanda et al. (2020); Chakraborty et al. (2023), biological sciences Ying et al. (2021); Ramp\u00e1\u0161ek et al. (2022); Ranjan et al. (2022), modeling of physical systems Thangamuthu et al. (2022); Bhattoo et al. (2022; 2023); Bishnoi et al. (2023), event detection Cao et al. (2021); Kosan et al. (2021) and traffic modeling Gupta et al. (2023); Jain et al. (2021); Wu et al. (2017); Li et al. (2020). Unfortunately, like other deep-learning models, GNNs are black boxes due to lacking transparency and interpretability. This lack of interpretability is a significant barrier to their adoption in critical domains such as healthcare, finance, and law enforcement. In addition, the ability to explain predictions is critical towards understanding potential flaws in the model and generate insights for further refinement. To impart interpretability to GNNs, several algorithms to explain the inner workings of GNNs have been proposed. The diversified landscape of GNN explainability research is visualized in Fig. 1. We summarize each of the categories below:\n\u2022 Model-level: Model-level or global explanations are concerned with the overall behavior of the model and search for patterns in the set of predictions made by the model. XGNN Yuan et al. (2020), GLG-Explainer Azzolin et al. (2023), Xuanyuan et al. Xuanyuan et al. (2023), GCFExplainer Huang et al. (2023). \u2217Both authors contributed equally to this research. \u2020Work done prior to joining Visa Inc."
        },
        {
            "heading": "Factual Counterfactual",
            "text": "\u2022 Instance-level: Instance-level or local explainers provide explanations for specific predictions made by a model. For instance, these explanations reason why a particular instance or input is classified or predicted in a certain way. \u2022 Gradient-based: They follow the idea of the rate of change being represented by gradients. Additionally, the gradient of the prediction with respect to the input represents the prediction sensitivity to the input. This sensitivity gives the importance scores and helps in finding explanations. SA and Guided-BP Baldassarre & Azizpour (2019), Grad-CAM Pope et al. (2019). \u2022 Decomposition-based: They consider the prediction of the model to be decomposed and distributed backward in a layer-by-layer fashion and the score of different parts of the input can be construed as its importance to the prediction. CAM and Excitation-BP Pope et al. (2019), GNN-LRP Schnake et al. (2021). \u2022 Perturbation-based: They utilize input perturbations to identify important subgraphs serving as factual or counterfactual explanations. GNNExplainer Ying et al. (2019b), PGExplainer Luo et al. (2020), SubgraphX Yuan et al. (2021), GEM Lin et al. (2021a), TAGExplainer Xie et al. (2022), CF2 Tan et al. (2022), RCExplainer Bajaj et al. (2021), CF-GNNexplainer Lucic et al. (2022), CLEAR Ma et al. (2022), Shan et al. (2021); Abrate & Bonchi (2021); Wellawatte et al. (2022) \u2022 Surrogate: They use the generic intuition that in a smaller range of input values, the relationship between input and output can be approximated by interpretable functions. The methods fit a simple and interpretable surrogate model in the locality of the prediction. GraphLime Huang et al. (2022), Relex Zhang et al. (2021), PGM-Explainer Vu & Thai (2020).\nThe type of explanation offered represents a crucial component. Explanations can be broadly classified into two categories: factual reasoning and counterfactual reasoning.\n\u2022 Factual explanations provide insights into the rationale behind a specific prediction by identifying the minimal subgraph that is sufficient to yield the same prediction as the entire input graph. \u2022 Counterfactual explanations elucidate why a particular prediction was not made by presenting alternative scenarios that could have resulted in a different decision. In the context of graphs, this involves identifying the smallest perturbation to the input graph that alters the prediction of the GNN. Perturbations typically involve the removal of edges or modifications to node features."
        },
        {
            "heading": "1.1 CONTRIBUTIONS",
            "text": "In this benchmarking study, we systematically study perturbation-based factual and counterfactual explainers and identify their strengths and limitations in terms of their ability to provide accurate, meaningful, and actionable explanations for GNN predictions. The proposed study surfaces new insights that have not been studied in existing benchmarking literature Amara et al. (2022); Agarwal et al. (2023)(See. App. J for details). Overall, we make the following key contributions:\n\u2022 Comprehensive evaluation encompassing counterfactual explainers: The benchmarking study encompasses seven factual explainers and four counterfactual explainers. The proposed work is the first benchmarking study on counterfactual explainers for GNNs. \u2022 Novel insights: The findings of our benchmarking study unveil stability to noise and variational factors and generating feasible counterfactual recourses as two critical technical deficiencies that naturally lead us towards open research challenges. \u2022 Codebase: As a by-product, a meticulously curated, publicly accessible code base is provided (https://github.com/Armagaan/gnn-x-bench/)."
        },
        {
            "heading": "2 PRELIMINARIES AND BACKGROUND",
            "text": "We use the notation G = (V, E) to represent a graph, where V denotes the set of nodes and E denotes the set of edges. Each node vi \u2208 V is associated with a feature vector xi \u2208 Rd. We assume there exists a GNN \u03a6 that has been trained on G (or a set of graphs). The literature on GNN explainability has primarily focused on graph classification and node classification, and hence the output space is assumed to be categorical. In graph classification, we are given a set of graphs as input, each associated with a class label. The task of the GNN \u03a6 is to correctly predict this label. In the case of node classification, class labels are associated with each node and the predictions are performed on nodes. In a message passing GNN of \u2113 layers, the embedding on a node is a function of its \u2113-hop neighborhood. We use the term inference subgraph to refer to this \u2113-hop neighborhood. Henceforth, we will assume that graph refers to the inference subgraph for node classification. Factual and counterfactual reasoning over GNNs are defined as follows.\nDefinition 1 (Perturbation-based Factual Reasoning) Let G be the input graph and \u03a6(G) the prediction on G. Our task is to identify the smallest subgraph GS \u2286 G such that \u03a6(G) = \u03a6(GS). Formally, the optimization problem is expressed as follows:\nGS = arg min G\u2032\u2286G, \u03a6(G)=\u03a6(G\u2032)\n||A(G\u2032)|| (1)\nHere, A(GS) denotes the adjacency matrix of GS , and ||A(GS)|| is its L1 norm which is equivalent to the number of edges. Note that if the graph is undirected, the number of edges is half of the L1 norm. Nonetheless, the optimization problem remains the same.\nWhile subgraph generally concerns only the topology of the graph, since graphs in our case may be annotated with features, some algorithms formulate the minimization problem in the joint space of topology and features. Specifically, in addition to identifying the smallest subgraph, we also want to minimize the number of features required to characterize the nodes in this subgraph.\nDefinition 2 (Counterfactual Reasoning) Let G be the input graph and \u03a6(G) the prediction on G. Our task is to introduce the minimal set of perturbations to form a new graph G\u2217 such that \u03a6(G) \u0338= \u03a6(G\u2217). Mathematically, this entails to solving the following optimization problem.\nG\u2217 = arg min G\u2032\u2208G, \u03a6(G) \u0338=\u03a6(G\u2032) dist(G,G\u2032) (2)\nwhere dist(G,G\u2032) quantifies the distance between graphs G and G\u2032 and G is the set of all graphs one may construct by perturbing G. Typically, distance is measured as the number of edge perturbations while keeping the node set fixed. In case of multi-class classification, if one wishes to switch to a target class label(s), then the optimization objective is modified as G\u2217 = argminG\u2032\u2208G, \u03a6(G\u2032)=C dist(G,G\u2032), where C is the set of desired class labels."
        },
        {
            "heading": "2.1 REVIEW OF PERTURBATION-BASED GNN REASONING",
            "text": "Factual (Yuan et al. (2022); Kakkad et al. (2023)): The perturbation schema for factual reasoning usually consists of two crucial components: the subgraph extraction module and the scoring function module. Given an input graph G, the subgraph extraction module extracts a subgraph Gs; and the scoring function module evaluates the model predictions \u03a6(Gs) for the subgraphs, comparing them with the actual predictions \u03a6(G). For instance, while GNNExplainer Ying et al. (2019a) identifies an explanation in the form of a subgraph that have the maximum influence on the prediction, PGExplainer Luo et al. (2020) assumes the graph to be a random Gilbert graph. Unlike the existing explainers, TAGExplainer Xie et al. (2022) takes a two-step approach where the first step has an\nembedding explainer trained using a self-supervised training framework without any information of the downstream task. On the other hand, GEM Lin et al. (2021a) uses Granger causality and an autoencoder for the subgraph extraction strategy where as SubgraphX Yuan et al. (2021) employes a monte carlo tree search. The scoring function module uses mutual information for GNNExplainer, PGExplainer, and TAGExplainer. This module is different for GEM and SubgraphX, and uses casual contribution and Shapley value respectively. Table 1 summarizes the key highlights. Counterfactual (Yuan et al. (2022)): The four major counterfactual methods are CF-GNNExplainer Lucic et al. (2022), CF2 Tan et al. (2022), RCExplainer Bajaj et al. (2021), and CLEAR Ma et al. (2022). They are instance-level explainers and apply to both graph and node classification tasks except for CF-GNNExplainer which is only applied to node classification. In terms of method, CF-GNNExplainer aims to perturb the computational graph by using a binary mask matrix. The corresponding loss function quantifies the accuracy of the produced counterfactual and captures the distance (or similarity) between the counterfactual graph and the original graph, whereas, CF2 Tan et al. (2022) extends this method by including a contrastive loss that jointly optimizes the quality of both the factual and the counterfactual explanation. Both of the above methods are transductive. As an inductive method, RCExplainer Bajaj et al. (2021), aims to identify a resilient subset of edges to remove such that it alters the prediction of the remaining graph while CLEAR Ma et al. (2022) generates counterfactual graphs by using a graph variational autoencoder. Table 2 summarizes the key highlights."
        },
        {
            "heading": "3 BENCHMARKING FRAMEWORK",
            "text": "In this section, we outline the investigations we aim to conduct and the rationale behind them. The mathematical formulation of the various metrics are summarized in Table 3. Comparative Analysis: We evaluate algorithms for both factual and counterfactual reasoning and identify the pareto-optimal methods. The performance is quantified using explanation size and sufficiency Tan et al. (2022). Sufficiency encodes the ratio of graphs for which the prediction derived from the explanation matches the prediction obtained from the complete graph Tan et al. (2022). Its value spans between 0 and 1. For factual explanations, higher values indicate superior performance, while in counterfactual lower is better since the objective is to flip the class label. Stability: Stability of explanations, when faced with minor variations in the evaluation framework, is a crucial aspect that ensures their reliability and trustworthiness. Stability is quantified by taking the Jaccard similarity between the set of edges in the original explanation vs. those obtained after introducing the variation (details in \u00a7 4). In order to evaluate this aspect, we consider the following perspectives:\n\u2022 Perturbations in topological space: If we inject minor perturbations to the topology through a small number of edge deletions or additions, then that should not affect the explanations. \u2022 Model parameters: The explainers are deep-learning models themselves and optimize a nonconvex loss function. As a consequence of non-convexity, when two separate instances of the explainer starting from different seeds are applied to the same GNN model, they generate dissimilar\nexplanations. Our benchmarking study investigates the impact of this stochasticity on the quality and consistency of the explanations produced. \u2022 Model architectures: Message-passing GNNs follow a similar computation framework, differing mainly in their message aggregation functions. We explore the stability of explanations under variations in the model architecture.\nNecessity: Factual explanations are necessary if the removal of the explanation subgraph from the graph results in counterfactual graph (i.e., flipping the label). Reproducibility: We measure two different aspects related to how central the explanation is towards retaining the prediction outcomes. Specifically, Reproducibility+ measures if the GNN is retrained on the explanation graphs alone, can it still obtain the original predictions? On the other hand, Reproducibility\u2212 measures if the GNN is retrained on the residual graph constructed by removing the explanation from the original graph, can it still predict the class label? The mathematical quantification of these metrics is presented in Fig. 3. Feasibility: One notable characteristic of counterfactual reasoning is its ability to offer recourse options. Nonetheless, in order for these recourses to be effective, they must adhere to the specific domain constraints. For instance, in the context of molecular datasets, the explanation provided must correspond to a valid molecule. Likewise, if the domain involves consistently connected graphs, the recourse must maintain this property. The existing body of literature on counterfactual reasoning with GNNs has not adequately addressed this aspect, a gap we address in our benchmarking study."
        },
        {
            "heading": "4 EMPIRICAL EVALUATION",
            "text": "In this section, we execute the investigation plan outlined in \u00a7 3. Unless mentioned specifically, the base black-box GNN is a GCN. Details of the set up (e.g., hardware) are provided in App. A. Datasets: Table 4 showcases the principal statistical characteristics of each dataset employed in our experiments, along with the corresponding tasks evaluated on them. The TREE-CYCLES, TREEGRID, and BA-SHAPES datasets serve as benchmark graph datasets for counterfactual analysis. These datasets incorporate ground-truth explanations Tan et al. (2022); Lin et al. (2021a); Lucic et al. (2022). Each dataset contains an undirected base graph to which predefined motifs are attached to random nodes, and additional edges are randomly added to the overall graph. The class label assigned to a node determines its membership in a motif."
        },
        {
            "heading": "4.1 COMPARATIVE ANALYSIS",
            "text": "Factual Explainers: Fig. 2 illustrates the sufficiency analysis of various factual reasoners in relation to size. Each algorithm assigns a score to edges, indicating their likelihood of being included in the factual explanation. To control the size, we adopt a greedy approach by selecting the highest-scoring edges. Both CF2 and RCEXPLAINER necessitate a parameter to balance factual and counterfactual explanations. We set this parameter to 1, corresponding to solely factual explanations.\nInsights: No single technique dominates across all datasets. For instance, while RCEXPLAINER performs exceptionally well in the MUTAG dataset, it exhibits subpar performance in IMDB-B and GRAPH-SST2. Similar observations are also made for GNNEXPLAINER in REDDIT-B vs. MUTAG and NCI1. Overall, we recommend using either RCEXPLAINER or GNNEXPLAINER as the preferred choices. The spider plot in Fig. Q more prominently substantiates this suggestion. GNNEXPLAINER is transductive, wherein it trains the parameters on the input graph itself. In contrast, inductive\nmethods use pre-trained weights to explain the input. Consequently, transductive methods, such as GNNEXPLAINER, at the expense of higher computation cost, has an inherent advantage in terms of optimizing sufficiency. Compared to other transductive methods, GNNEXPLAINER utilizes a loss function that aims to increase sufficiency directly. This makes the method a better candidate for sufficiency compared to other inductive and transductive explainers. On the other hand, for RCEXPLAINER, we believe that calculation of decision regions for classes helps to increase its generalizability as well as robustness.\nIn Fig. 2, the sufficiency does not always increase monotonically with explanation size (such as PGEXPLAINER in Mutag). This behavior arises due to the combinatorial nature of the problem. Specifically, the impact of adding an edge to an existing explanation on the GNN prediction is a function of both the edge being added and the edges already included in the explanation. An explainer seeks to learn a proxy function that mimics the true combinatorial output of a set of edges. When this proxy function fails to predict the marginal impact of adding an edge, it could potentially select an edge that exerts a detrimental influence on the explanation\u2019s quality.\nCounterfactual Explainers: Tables 5 and 6 present the results on graph and node classification.\nInsights on graph classification (Table 5): RCEXPLAINER is the best-performing explainer across the majority of the datasets and metrics. However, it is important to acknowledge that RCEXPLAINER\u2019s sufficiency, when objectively evaluated, consistently remains high, which is undesired. For instance, in the case of AIDS, the sufficiency of RCEXPLAINER reaches a value of 0.9, signifying its inability to generate counterfactual explanations for 90% of the graphs. This\nobservation suggests that there exists considerable potential for further enhancement. We also note that while CLEAR achieves the best (lowest) sufficiency in AIDS, the number of perturbations it requires (size) is exorbitantly high to be useful in practical use-cases.\nInsights on node classification (Table 6): We observe that CF-GNNEXPLAINER consistently outperforms CF2 (\u03b1 = 0 indicates the method to be entirely counterfactual). We note that our result contrasts with the reported results in CF2 Tan et al. (2022), where CF2 was shown to outperform CF-GNNEXPLAINER. A closer examination reveals that in Tan et al. (2022), the value of \u03b1 was set to 0.6, placing a higher emphasis on factual reasoning. It was expected that with \u03b1 = 0, counterfactual reasoning would be enhanced. However, the results do not align with this hypothesis. We note that in CF2, the optimization function is a combination of explanation complexity and explanation strength. The contribution of \u03b1 is solely in the explanation strength component, based on its alignment with factual and counterfactual reasoning. The counterintuitive behavior observed with \u03b1 is attributed to the domination of explanation complexity in the objective function, thereby diminishing the intended impact of \u03b1. Finally, when compared to graph classification, the sufficiency produced by the best methods in the node classification task is significantly lower indicating that it is an easier task. One possible reason might be the space of counterfactuals is smaller in node classification."
        },
        {
            "heading": "4.2 STABILITY",
            "text": "We next examine the stability of the explanations against topological noise, model parameters, and the choice of GNN architecture. In App. C, we present the impact of the above mentioned factors on other metrics of interest such as sufficiency and explanation size. In addition, we also present impact of feature perturbation and topological adversarial attack in App. C.\nInsights on factual-stability against topological noise: Fig. 3 illustrates the Jaccard coefficient as a function of the noise volume. Similar to Fig.2, edge selection for the explanation involves a greedy approach that prioritizes the highest score edges. A clear trend that emerges is that inductive methods consistently outperform the transductive methods (such as CF2 and GNNEXPLAINER). This is expected since transductive methods lack generalizable capability to unseen data. Furthermore, the stability is worse on denser datasets of IMDB-B since due to the presence of more edges, the search space of explanation is larger. RCEXPLAINER (executed at \u03b1 = 1) and PGEXPLAINER consistently exhibit higher stability. This consistent performance reinforces the claim that RCEXPLAINER is the preferred factual explainer. The stability of RCEXPLAINER can be attributed to its strategy of selecting a subset of edges that is resistant to changes, such that the removal of these edges\nsignificantly impacts the prediction made by the remaining graph Bajaj et al. (2021). PGEXPLAINER also incorporates a form of inherent stability within its framework. It builds upon the concept introduced in GNNEXPLAINER through the assumption that the explanatory graph can be modeled as a random Gilbert graph, where the probability distribution of edges is conditionally independent and can be parameterized. This generic assumption holds the potential to enhance the stability of the method. Conversely, TAGEXPLAINER exhibits the lower stability than RCEXPLAINER and PGEXPLAINER, likely due to its reliance solely on gradients in a task-agnostic manner Xie et al. (2022). The exclusive reliance on gradients makes it more susceptible to overfitting, resulting in reduced stability.\nInsights on factual-stability against explainer instances: Table 7 presents the stability of explanations provided across three different explainer instances on the same black-box GNN. A similar trend is observed, with RCEXPLAINER remaining the most robust method, while GNNEXPLAINER exhibits the least stability. For GNNEXPLAINER, the Jaccard coefficient hovers around 0.5, indicating significant variance in explaining the same GNN. Although the explanations change, their quality remains stable (as evident from small standard deviation in Fig. 2). This result indicates that multiple explanations of similar quality exist and hence a single explanation fails to complete explain the data signals. This component is further emphasized when we delve into reproducibility (\u00a7 4.3).\nInsights on factual-stability against GNN architectures: Finally, we explore the stability of explainers across different GNN architectures in Table 8, which has not yet been investigated in the existing literature. For each combination of architectures, we assess the stability by computing the Jaccard coefficient between the explained predictions of the indicated GNN architecture and the default GCN model. One notable finding is that the stability of explainers exhibits a strong correlation with the dataset used. Specifically, in five out of six datasets, the best performing explainer across all architectures is unique. However, it is important to highlight that the Jaccard coefficients across architectures consistently remain low indicating stability against different architectures is the hardest objective due to the variations in their message aggregating schemes."
        },
        {
            "heading": "4.3 NECESSITY AND REPRODUCIBILITY",
            "text": "We aim to understand the quality of explanations in terms of necessity and reproducibility. The results are presented in App. D and E. Our findings suggest that necessity is low but increases with the removal of more explanations, while reproducibility experiments reveal that explanations do not provide a comprehensive explanation of the underlying data, and even removing them and retraining the model can produce a similar performance to the original GNN model."
        },
        {
            "heading": "4.4 FEASIBILITY",
            "text": "Counterfactual explanations serve as recourses and are expected to generate graphs that adhere to the feasibility constraints of the pertinent domain. We conduct the analysis of feasibility on molecular graphs. It is rare for molecules to be constituted of multiple connected components Vismara & Lauren\u00e7o (2000). Hence, we study the distribution of molecules that are connected in the original dataset and its comparison to the distribution in counterfactual recourses. We measure the p-value of this deviation. App. A.7 presents the results."
        },
        {
            "heading": "4.5 VISUALIZATION-BASED ANALYSIS",
            "text": "We include visualizations of the explanations in App. F. Our analysis reveals that a statistically good performance does not always align with human judgment indicating an urgent need for datasets annotated with ground truth explanations. Furthermore, the visualization analysis reinforces the need to incorporate feasibility as a desirable component in counterfactual reasoning."
        },
        {
            "heading": "5 CONCLUDING INSIGHTS AND POTENTIAL SOLUTIONS",
            "text": "Our benchmarking study has yielded several insights that can streamline the development of explanation algorithms. We summarize the key findings below (please also see the App. K for our recommendations of explainer for various scenarios).\n\u2022 Performance and Stability: Among the explainers evaluated, RCEXPLAINER consistently outperformed others in terms of efficacy and stability to noise and variational factors (\u00a7 4.1 and \u00a7 4.2). \u2022 Stability Concerns: Most factual explainers demonstrated significant deviations across explainer instances, vulnerability to topological perturbations and produced significantly different set of explanations across different GNN architectures. These stability notions should therefore be embraced as desirable factors along with other performance metrics. \u2022 Model Explanation vs. Data Explanation: Reproducibility experiments (\u00a7 4.3) revealed that retraining with only factual explanations cannot reproduce the predictions fully. Furthermore, even without the factual explanation, the GNN model predicted accurately on the residual graph. This suggests that explainers only capture specific signals learned by the GNN and do not encompass all underlying data signals. \u2022 Feasibility Issues: Counterfactual explanations showed deviations in topological distribution from the original graphs, raising feasibility concerns (\u00a7 4.4).\nPotential Solutions: The aforementioned insights raise important shortcomings that require further investigation. Below, we explore potential avenues of research that could address these limitations.\n\u2022 Feasible recourses through counterfactual reasoning: Current counterfactual explainers predominantly concentrate on identifying the shortest edit path that nudges the graph toward the decision boundary. This design inherently neglects the feasibility of the proposed edits. Therefore, it is imperative to explicitly address feasibility as an objective in the optimization function. One potential solution lies in the vibrant research field of generative modeling for graphs, which has yielded impressive results Goyal et al. (2020); You et al. (2018); Vignac et al. (2023). Generative models, when presented with an input graph, can predict its likelihood of occurrence within a domain defined by a set of training graphs. Integrating generative modeling into counterfactual reasoning by incorporating likelihood of occurrence as an additional objective in the loss function presents a potential remedy. \u2022 Ante-hoc explanations for stability and reproducibility: We have observed that if the explanations are removed and the GNN is retrained on the residual graphs, the GNN is often able to recover the correct predictions from our reproducibilty experiments. Furthermore, the explanation exhibit significant instability in the face of minor noise injection. This incompleteness of explainers and instability is likely a manifestation of their post-hoc learning framework, wherein the explanations are generated post the completion of GNN training. In this pipeline, the explainers have no visibility to how the GNN would behave to perturbations on the input data, initialization seeds, etc. Potential solutions may lie on moving to an ante-hoc paradigm where the GNN and the explainer are jointly trained Kosan et al. (2023); Miao et al. (2022); Fang et al. (2023).\nThese insights, we believe, open new avenues for advancing GNN explainers, empowering researchers to overcome limitations and elevate the overall quality and interpretability of GNNs."
        },
        {
            "heading": "6 ACKNOWLEDGEMENTS",
            "text": "Samidha Verma acknowledges the generous grant received from Microsoft Research India to sponsor her travel to ICLR 2024. Additionally, this project was partially supported by funding from the National Science Foundation under grant #IIS-2229876."
        },
        {
            "heading": "APPENDIX",
            "text": ""
        },
        {
            "heading": "A EXPERIMENTAL SETUP",
            "text": "All experiments were conducted using the Ubuntu 18.04 operating system on an NVIDIA DGX Station equipped with four V100 GPU cards, each having 128GB of GPU memory. The system also included 256GB of RAM and a 20-core Intel Xeon E5-2698 v4 2.2 GHz CPU.\nThe datasets for factual and counterfactual explainers follow an 80:10:10 split for training, validation and testing. We explain some of our design choices below.\n\u2022 For factual explainers, the inductive explainers are trained on the training data and the reported results are computed on the entire dataset. We also report results only on test data (please see Sec. A.5) comparing only inductive methods. Transductive methods are run on the entire dataset. \u2022 For counterfactual explainers, the inductive explainers are trained on the training data, and the reported results are computed on the test data. Since transductive methods do not have the notion of training and testing separately, they are run only on the test data."
        },
        {
            "heading": "A.1 BENCHMARK DATASETS",
            "text": "Datasets for Node classification: The following datasets have node labels and are used for the node classification task.\n\u2022 TREE-CYCLES Ying et al. (2019b): The base graph used in this dataset is a binary tree, and the motifs consist of 6-node cycles (Figure D(a)). The motifs are connected to random nodes in the tree. Non-motif nodes are labeled 0, while the motif nodes are labeled 1. \u2022 TREE-GRID Ying et al. (2019b): The base graph used in this dataset is a binary tree, and the motif is a 3\u00d7 3 grid connected to random nodes in the tree (Figure D(b)). Similar to the tree-cycles dataset, the nodes are labeled with binary classes (0 for the non-motif nodes and 1 for the motif nodes). \u2022 BA-SHAPES Ying et al. (2019b): The base graph in this dataset is a Barabasi-Albert (BA) graph. The dataset includes house-shaped structures composed of 5 nodes (Figure D (c)). Non-motif nodes are assigned class 0, while nodes at the top, middle, and bottom of the motif are assigned classes 1, 2, and 3, respectively.\nFigure D: Motifs used in (a) Tree-Cycles, (b) Tree-Grid and (c) BA-Shapes datasets for the node classification task. Please note the following. (i) Tree-Cycles and Tree-Grid have labels 0 and 1 for the non-motif and the motif nodes, respectively. Hence, all nodes in (a) and (b) have label 1. (ii) BA-Shapes dataset has 4 classes. Non-motif nodes have labels 0; motif nodes have integral labels depending on the position in the house motif. The other labels are 1 (top node), 2 (middle nodes) and 3 (bottom nodes). They are represented in (c).\nDatasets for Graph Classification: The following datasets are used for the graph classification task and contain labeled graphs.\n\u2022 MUTAG Ivanov et al. (2019) and Mutagenicity Riesen & Bunke (2008); Kazius et al. (2005): These are graph datasets containing chemical compounds. The nodes represent atoms, and the edges represent chemical bonds. The binary labels depend on the mutagenic effect of the compound\non a bacterium, namely mutagenic or non-mutagenic. MUTAG and Mutagenicity datasets contain 188 and 4337 graphs, respectively. \u2022 AIDS: Ivanov et al. (2019) This dataset contains small molecules. The nodes and edges are atoms and chemical bonds, respectively. The molecules are classified by whether they are active against the HIV virus or not. \u2022 Proteins Borgwardt et al. (2005); Dobson & Doig (2003) and DD Dobson & Doig (2003): These datasets are comprised of proteins categorized into enzymes and non-enzymes. The nodes represent amino acids, and an edge exists between two nodes if their distance is less than 6 Angstroms. \u2022 NCI1 Wale et al. (2008): This dataset is derived from cheminformatics and represents chemical compounds as input graphs. Vertices in the graph correspond to atoms, while edges represent bonds between atoms. This dataset focuses on anti-cancer screenings for cell lung cancer, with chemicals labeled as positive or negative. Each vertex is assigned an input label indicating the atom type, encoded using a one-hot-encoding scheme. \u2022 IMDB-B Yanardag & Vishwanathan (2015): The IMDB-BINARY dataset is a collection of movie collaboration networks, encompassing the ego-networks of 1,000 actors and actresses who have portrayed roles in films listed on IMDB. Each network is represented as a graph, where the nodes correspond to the actors/actresses, and an edge is present between two nodes if they have shared the screen in the same movie. These graphs have been constructed specifically from movies in the Action and Romance genres, which are the class labels. \u2022 REDDIT-B Yanardag & Vishwanathan (2015): REDDIT-BINARY dataset encompasses graphs representing online discussions on Reddit. Each graph has nodes representing users, connected by edges when either user responds to the other\u2019s comment. The four prominent subreddits within this dataset are IAmA, AskReddit, TrollXChromosomes, and atheism. IAmA and AskReddit are question/answer-based communities, while TrollXChromosomes and atheism are discussion-based communities. Graphs are labeled based on their affiliation with either a question/answer-based or discussion-based community. \u2022 GRAPH-SST2 Yuan et al. (2022): The Graph-SST2 dataset is a graph-based dataset derived from the SST2 dataset Socher et al. (2013), which contains movie review sentences labeled with positive or negative sentiment. Each sentence in the Graph-SST2 dataset is transformed into a graph representation, with words as nodes and edges representing syntactic relationships capturing the sentence\u2019s grammatical structure. The sentiment labels from the original SST2 dataset are preserved, allowing for sentiment analysis tasks using the graph representations of the sentences. \u2022 ogbg-molhiv Allamanis et al. (2018): ogbg-molhiv is a molecule dataset with nodes representing atoms and edges representing chemical bonds. The node features represent various properties of the atoms like chirality, atomic number, formal charge etc. Edge attributes represent the bond type. We study binary classification task on this dataset. The task is to achieve the most accurate predictions of specific molecular properties. These properties are framed as binary labels, indicating attributes like whether a molecule demonstrates inhibition of HIV virus replication or not."
        },
        {
            "heading": "A.2 DETAILS OF GNN MODEL \u03a6 USED FOR NODE CLASSIFICATION",
            "text": "We use the same GNN model used in CF-GNNEXPLAINER and CF2. Specifically, it is a Graph Convolutional Networks Kipf & Welling (2016) trained on each of the datasets. Each model has 3 graph convolutional layers with 20 hidden dimensions for the benchmark datasets. The non-linearity used is relu for the first two layers and log softmax after the last layer of GCN. The learning rate is 0.01. The train and test data are divided in the ratio 80:20. The accuracy of the GNN model \u03a6 for each dataset is mentioned in Table I."
        },
        {
            "heading": "A.3 DETAILS OF BASE GNN MODEL \u03a6 FOR THE GRAPH CLASSIFICATION TASK",
            "text": "Our GNN models have an optional parameter for continuous edge weights, which in our case represents explanations. Each model consists of 3 layers with 20 hidden dimensions specifically designed for benchmark datasets. The models provide node embeddings, graph embeddings, and direct outputs from the model (without any softmax function). The output is obtained through a onelayer MLP applied to the graph embedding. We utilize the max pooling operator to calculate the graph embedding. The dropout rate, learning rate, and batch size are set to 0, 0.001, and 128, respectively. The train, validation, and test datasets are divided into an 80:10:10 ratio. The algorithms run for 1000 epochs with early stopping after 200 patience steps on the validation set. The performance analysis of the base GNN models ( Kipf & Welling (2016); Velic\u030ckovic\u0301 et al. (2018); Xu et al. (2019); Hamilton et al. (2017)) for each graph classification dataset is presented in Table J.\nTable J: Test accuracy of black-box GNN \u03a6 trained for the graph classification task, averaged over 10 runs with random seeds. We train multiple GNNS for this task to test explainers for stability against GNN architectures.\nDataset GCN GAT GIN GraphSAGE\nMutagenicity 0.8724\u00b1 0.0092 0.8685\u00b1 0.0111 0.8914\u00b1 0.0101 0.8749\u00b1 0.0059 Mutag 0.925\u00b1 0.0414 0.8365\u00b1 0.0264 0.9542\u00b1 0.0149 0.8323\u00b1 0.0445\nProteins 0.8418\u00b1 0.0144 0.8362\u00b1 0.0269 0.8352\u00b1 0.0165 0.8408\u00b1 0.0124 IMDB-B 0.8318\u00b1 0.0197 0.8292\u00b1 0.015 0.8554\u00b1 0.027 0.8373\u00b1 0.0093\nAIDS 0.999\u00b1 0.0005 0.9971\u00b1 0.0068 0.9797\u00b1 0.0099 0.9903\u00b1 0.0088 NCI1 0.8243\u00b1 0.028 0.8096\u00b1 0.015 0.8365\u00b1 0.0201 0.8303\u00b1 0.0137 Graph-SST2 0.957\u00b1 0.001 0.9603\u00b1 0.0009 0.9552\u00b1 0.0014 0.9611\u00b1 0.0011 DD 0.736\u00b1 0.0377 0.7312\u00b1 0.048 0.7693\u00b1 0.0238 0.7541\u00b1 0.0415 REDDIT-B 0.8984\u00b1 0.0247 0.8444\u00b1 0.0266 0.6886\u00b1 0.1231 0.8733\u00b1 0.0196 ogbg-molhiv 0.9729\u00b1 0.0002 0.9722\u00b1 0.0010 0.9726\u00b1 0.0003 0.9725\u00b1 0.0005"
        },
        {
            "heading": "A.4 DETAILS OF FACTUAL EXPLAINERS FOR THE GRAPH CLASSIFICATION TASK",
            "text": "In many cases, explainers generate continuous explanations that can be used with graph neural network (GNN) models, which can handle edge weights. To be able to use explanations in our GNN models, we map them into [0, 1] using a sigmoid function if not mapped. While generating performance results, we calculate top-k edges based on their scores instead of assigning a threshold value (e.g., 0.5). However, there are some approaches, such as GEM and SubgraphX, that do not rely on continuous edge explanations.\nGEM employs a variational auto-encoder to reconstruct ground truth explanations. As a result, the generated explanations can include negative values. While our experiments primarily focus on the order of explanations and do not require invoking the base GNN in the second stage of GEM, we can still use negative explanation edges.\nOn the other hand, SubgraphX ranks different subgraph explanations based on their scores. We select the top 20 explanations and, for each explanation, compute the subgraph. Then, we enhance the importance of each edge of a particular subgraph by incrementing its score by 1. Finally, we normalize the weights of the edges. This process allows us to obtain continuous explanations as well. Moreover, since SubgraphX employs tree search, its scalability is limited when dealing with large graphs. For instance, in the Mutagenicity dataset, obtaining explanations for 435 graphs requires approximately 26.5 hours. To address this challenge, we restricted our analysis to test graphs when calculating explanations using SubgraphX. It is important to include this disclaimer, working on only subset of graphs may introduce potential biases or noises in the results."
        },
        {
            "heading": "A.5 FACTUAL EXPLAINERS: INDUCTIVE METHODS ON TEST SET",
            "text": "The inductive factual explainers are run only on the test data and the results are reported in Figure E. The results are similar to the ones where the methods are run on the entire dataset (Figure 2). Consistent with the earlier results, PGEXPLAINER consistently delivers inferior results compared to other\nbaseline methods, and no single technique dominates across all datasets. Overall, RCEXPLAINER could be recommended as one of the preferred choices.\n0.4\n0.5\n0.6\n0.7 0.8 Su ffi cie nc y\nMutagenicity\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00 Proteins\n0.2\n0.4\n0.6\n0.8\n1.0\nMutag\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\nSu ffi\ncie nc\ny\nIMDB-B\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\nAIDS\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9 NCI1\n5 10 15 20 25\n0.88\n0.90\n0.92\n0.94\n0.96\n0.98\n1.00\nSu ffi\ncie nc\ny\nGraph-SST2\n5 10 15 20 25 0.3\n0.4\n0.5\n0.6\n0.7 DD\nPGExplainer TAGExplainer RCExplainer GEM\n5 10 15 20 25 0.40\n0.45\n0.50\n0.55\n0.60\nREDDIT-B\n0.0 0. 0.4 0.6 .8 1.0 size (#edges in the explanation)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFigure E: Sufficiency of the inductive factual explainers against the explanation size on only test data. For factual explanations, higher is better. We omit those methods for a dataset that throw an out-of-memory (OOM) error and are not scalable."
        },
        {
            "heading": "A.6 CODES AND IMPLEMENTATION",
            "text": "Table K shows the code bases we have used for the explainers. We have adapted the codes based on our base GNN models. Our repository, https://github.com/Armagaan/gnn-x-bench/, includes the adaptations of the methods to our base models.\nTable K: Reference of code repositories.\nMethod Repository\nPGExplainer Luo et al. (2020) https://github.com/LarsHoldijk/RE-ParameterizedExplainerForGraphNeuralNetworks/ TAGExplainer Xie et al. (2022) https://github.com/divelab/DIG/tree/main/dig/xgraph/TAGE/ CF2 Tan et al. (2022) https://github.com/chrisjtan/gnn_cff RCExplainer Bajaj et al. (2021) https://developer.huaweicloud.com/develop/aigallery/notebook/detail?id=e41f63d3-e346-4891-bf6a-40e64b4a3278 GNNExplainer Ying et al. (2019b) https://github.com/LarsHoldijk/RE-ParameterizedExplainerForGraphNeuralNetworks/ GEM Lin et al. (2021b) https://github.com/wanyu-lin/ICML2021-Gem/ SubgraphX Yuan et al. (2021) https://github.com/divelab/DIG/tree/main/dig/xgraph/SubgraphX"
        },
        {
            "heading": "A.7 FEASIBILITY",
            "text": "Counterfactual explanations: As shown in Table L, we observe statistically significant deviations from the expected values in two out of three molecular datasets. This suggests a heightened probability of predicting counterfactuals that do not correspond to feasible molecules. This finding underscores\nTable L: Assessing the statistical significance of deviations in the number of connected graphs between the test set and their corresponding counterfactual explanations on molecular datasets. Statistically significant deviations with p-value< 0.05 are highlighted.\nDataset RCEXPLAINER CF 2\nExpected Count Observed Count p-value Expected Count Observed Count p-value\nMutagenicity 233.05 70 < 0.00001 206.65 0 < 0.00001 Mutag 11 9 0.55 4 1 0.13 AIDS 17.6 8 < 0.00001 1.76 0 0.0001\na limitation of counterfactual explainers, which has received limited attention within the research community.\nFactual explanations: The feasibility metric is commonly used in the context of counterfactual graph explainers because it measures how feasible it is to achieve a specific counterfactual outcome. In other words, it assesses the likelihood of a counterfactual scenario being realized given the constraints and assumptions of the underlying base model. On the other hand, factual explainers aim to explain why a model makes a certain prediction based on the actual input data. They do not involve any counterfactual scenarios, so the feasibility metric is not relevant in this context. Instead, factual explainers may use other metrics such as sufficiency and reproducibility to provide insights into how the model is making its predictions. Therefore, we have not used feasibility metrics for factual explanations."
        },
        {
            "heading": "B SUFFICIENCY OF FACTUAL EXPLANATIONS UNDER TOPOLOGICAL NOISE",
            "text": "We check the sufficiency of the factual explanations under noise for four different datasets. Figure F demonstrates the results, including when there is no noise (i.e., when X = 0). We set the explanation size (i.e., the number of edges) to 10 units. We observe that, in most cases, increasing noise results in a decrease in the sufficiency metric for the Mutagenicity and AIDS datasets, which is expected. However, for the Proteins and IMDB-B datasets, even though there are still drops for some methods, others remain stable in sufficiency across different noise levels. This demonstrates that, despite the changes in explanations caused by noise, GNN may still predict the same class under noisy conditions.\n0 1 2 3 4 5\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\nSu ffi\ncie nc\ny\nMutagenicity\n0 1 2 3 4 5\n0.825\n0.850\n0.875\n0.900\n0.925\n0.950\n0.975\nProteins\n0 1 2 3 4 5\n0.70\n0.75\n0.80\n0.85\nIMDB-B\nPGExplainer TAGExplainer CF2 RCExplainer GNNExplainer GEM\n0 1 2 3 4 5 0.88\n0.90\n0.92\n0.94\n0.96\n0.98\n1.00 AIDS\n0.0 0.2 0.4 0.6 0.8 1.0 X (noise)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFigure F: Sufficiency of factual explainers under topological noise."
        },
        {
            "heading": "C STABILITY",
            "text": "In addition to stability against topological noise, different seeds, and different GNN architecture, we also analyze stability against feature pertubation and stability against topological adversarial attack.\nFor feature perturbation, we first select the percentage of nodes to be perturbed (X% \u2208 {10, 20, 30, 40, 50}). Then, perturbation operation varies depending on the nature of node features (continuous or discrete). For the Proteins dataset (continuous features); for each feature f , we compute its standard deviation \u03c3f . Then we sample a value uniformly at random from \u2206 \u223c [\u22120.1, 0.1]. The feature value fx is perturbed to fx + \u2206 \u00d7 \u03c3f . For other datasets (discrete features), for each selected node, we flip its feature to a randomly sampled feature.\nFor topology adversarial attack, we follow the flip edge method from Wan et al. (2021) with a query size of one and vary the number flip count across datasets."
        },
        {
            "heading": "C.1 FACTUAL EXPLAINERS",
            "text": "Stability against feature perturbation:\nFigure G illustrates the outcomes, demonstrating a continuation of the previously observed trends. Among these trends, we see that there is one clear winner for both datasets. However, PGEXPLAINER performs better than most datasets in both datasets (with discrete and continuous features). On the other hand, the transductive method GNNEXPLAINER performs very poorly in both datasets compared to other methods (i.e., inductive), which further provides evidence that transductive methods are poor in stability for factual explanations.\n10 20 30 40 50\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70 0.75 Ja cc ar d Si m ila rit y\nMutagenicity\n10 20 30 40 50\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0 Proteins\nPGExplainer TAGExplainer RCExplainer GNNExplainer\n0.0 0.2 0.4 0.6 0.8 1.0 Perturbation (%)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFigure G: Stability of factual explainers against feature perturbation in Jaccard similarity. The stability of explanations drops when the perturbation percentage increases. GNNEXPLAINER (transductive) is the worst method for these two datasets.\nAdversarial attack on topology: Figure H demonstrates the performance of four factual methods on these evasion attacks for four datasets. The behavior of the factual methods is similar to the topological noise attack explained in Section 4.2 and the feature perturbations results. When an adversarial attack is compared to random perturbations (Fig. 3), we observe higher deterioration in stability, which is expected since adversarial edge flip attack aims every possible edge in the graph rather than only considering nonexistent edges. Similar to feature perturbation, GNNExplainer (transductive) is affected more by the adversarial attack.\n1 2 3 4 5\n0.5\n0.6\n0.7\n0.8\nJa cc\nar d\nSi m\nila rit\ny\nMutagenicity\n1 2 3 4 5\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9 Proteins\n1 2 3 4 5\n0.2\n0.3\n0.4\n0.5\n0.6\nIMDB-B\nPGExplainer TAGExplainer RCExplainer GNNExplainer\n1 2 3 4 5 0.75\n0.80\n0.85\n0.90\n0.95\nAIDS\n0.0 0.2 0.4 0.6 0.8 1.0 X (flip count)\n0.0\n0.2\n0.4\n0.6\n0.\n1.0\nFigure H: Stability of factual explainers against random edge flip in Jaccard similarity. The stability of explanations drops when the flip count increases. GNNEXPLAINER (transductive) is the worst method for these two datasets."
        },
        {
            "heading": "C.2 COUNTERFACTUAL EXPLAINERS",
            "text": "Stability against topological noise: In this section, we investigate the influence of topological noise on datasets on both the performance and generated explanations of counterfactual explainers. For inductive methods (RCEXPLAINER and CLEAR), we utilize explainers trained on noise-free data and only infer on the noisy data. However, for the transductive method CF2, we retrain the model using the noisy data.\nFigure I presents the average Jaccard similarity results, indicating the similarity between the counterfactual graph predicted as an explanation for the original graph and the noisy graphs at varying levels of perturbations. Additionally, Figure J demonstrates the performance of different explainers in terms of sufficiency and size as the degree of noise increases. This provides insights into how these explainers handle higher levels of noise.\nRCExplainer outperforms other baselines by a significant margin in terms of size and sufficiency across datasets, as shown in Fig. J. However, the Jaccard similarity between RCExplainer and CF2 for counterfactual graphs is nearly identical, as shown in Fig. I. CF2 benefits from its transductive training on noisy graphs. CLEAR\u2019s results are not shown for Proteins and Mutagenicity datasets due to scalability issues. In the case of IMDB-B dataset, CLEAR is highly unstable in predicting counterfactual graphs, indicated by a low Jaccard index (Fig. I). Additionally, CLEAR demonstrates high sufficiency but requires a large number of edits, indicating difficulty in finding minimal-edit counterfactuals (Fig. J).\nOverall, RCExplainer seems to be the model of choice when topological noise is introduced, and it is significantly faster than CF2 because it is inductive. Further, it is better than CLEAR as the latter does not scale for larger datasets and is inferior in terms of sufficiency and size as well.\n1 2 3 4 5 0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nJa cc\nar d\nMutagenicity\n1 2 3 4 5 0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nIMDB-B\n1 2 3 4 5 0.0\n0.2\n0.4\n0.6\n0.8\n1.0 AIDS\n0.0 0.2 0.4 0.6 0.8 1.0 X (noise)\n0.\n0.2\n0.4\n0.6\n0.8\n1.0\nCF2 RCExplainer CLEAR\nFigure J: Performance evaluation of counterfactual explainers against topological noise. We omit CLEAR for Mutagenicity and Proteins as it throws an OOM error for these datasets. RCExplainer is more robust to noise in both metrics: (a) sufficiency and (b) size.\nStability against explainer instances: Table M provides an overview of the stability exhibited among explainer instances trained using three distinct seeds. Notably, we observe a substantial Jaccard index, indicating favorable stability, in the case of RCEXPLAINER and CF2 explainers. Conversely, CLEAR fails to demonstrate comparable stability. These findings align with the outcomes derived from Table 5. Specifically, when RCEXPLAINER and CF2 are successful in identifying a counterfactual, the resultant counterfactual graphs are obtained through a small number of perturbations. Consequently, the counterfactual graphs exhibit similarities to the original graph, rendering them akin to one another. However, this trend does not hold for CLEAR, as it necessitates a significantly greater number of perturbations.\nTable M: Stability against explainer instances. Note that stability with respect to a graph is computable only if both explainer instances find their counterfactual. \u201cNA\u201d indicates no such graph exists.\nRCExplainer CF2 CLEAR\nDataset / Seeds 1vs2 1vs3 2vs3 1vs2 1vs3 2vs3 1vs2 1vs3 2vs3\nMutagenicity 0.96 \u00b10.06 0.96 \u00b10.04 0.98 \u00b10.03 0.90 \u00b10.09 0.89 \u00b10.1 0.89 \u00b10.11 OOM OOM OOM Proteins 0.95 \u00b10.0 0.94 \u00b10.0 0.90 \u00b10.0 NA NA NA OOM OOM OOM Mutag 0.98 \u00b10.03 0.98 \u00b10.03 1.0 \u00b10.0 1.0 \u00b10.0 1.0 \u00b10.0 1.0 \u00b10.0 0.55 \u00b10.01 0.53 \u00b10.01 0.54 \u00b10.02 IMDB-B 0.99 \u00b10.01 1.0 \u00b10.0 0.99 \u00b10.01 0.96 \u00b10.05 0.95 \u00b10.06 0.94 \u00b10.07 0.28 \u00b10.0 0.27 \u00b10.0 0.28 \u00b10.0 AIDS 0.84 \u00b10.04 0.96 \u00b10.06 0.84 \u00b10.04 NA 1.0 \u00b10.0 NA 0.19 \u00b10.02 0.20 \u00b10.03 0.19 \u00b10.04 ogbg-molhiv 0.99 \u00b10.03 0.99 \u00b10.04 0.99 \u00b10.04 0.801 \u00b10.149 0.764 \u00b10.14 0.784 \u00b10.144 OOM OOM OOM\nSimilarly, as additional results, we also present the variations in terms of sufficiency (Table N) and the explanation size (Table O) produced by the methods using three different seeds. In terms of sufficiency, the methods show stability while varying the seeds. However, the results are drastically different for explanation size. Table O present the results. We observe that RCEXPLAINER is consistently the most stable method while CF2 is worse. The worst stability is shown by CLEAR and this observation is consistent with the previous results.\nTable N: Stability of sufficiency produced by counterfactual explainers against the explainer instances (seeds). The best explainers for each dataset (row) are highlighted in gray, yellow and cyan shading for seeds 1, 2, and 3, respectively. OOM indicates that the explainer threw an out-of-memory error.\nRCExplainer CF2 CLEAR\nDataset / Seeds 1 2 3 1 2 3 1 2 3\nMutagenicity 0.4 \u00b10.06 0.40 \u00b10.05 0.41 \u00b10.05 0.50 \u00b10.05 0.49 \u00b10.06 0.52 \u00b10.05 OOM OOM OOM Proteins 0.96 \u00b10.02 0.96 \u00b10.02 0.96 \u00b10.02 1.0 \u00b10.0 1.0 \u00b10.0 0.98 \u00b10.02 OOM OOM OOM Mutag 0.4 \u00b10.12 0.6 \u00b10.12 0.55 \u00b10.1 0.9 \u00b10.12 0.85 \u00b10.2 0.9 \u00b10.12 0.55 \u00b10.1 0.55 \u00b10.1 0.65 \u00b10.12\nIMDB-B 0.72 \u00b10.11 0.72 \u00b10.11 0.72 \u00b10.11 0.81 \u00b10.07 0.82 \u00b10.08 0.81 \u00b10.07 0.96 \u00b10.02 0.96 \u00b10.02 0.96 \u00b10.02 AIDS 0.91 \u00b10.04 0.91 \u00b10.04 0.91 \u00b10.04 0.98 \u00b10.02 1.0 \u00b10.0 0.99 \u00b10.01 0.84 \u00b10.03 0.82 \u00b10.03 0.83 \u00b10.04\nogbg-molhiv 0.90 \u00b10.02 0.88 \u00b10.01 0.90 \u00b10.01 0.96 \u00b10.01 0.97 \u00b10.00 0.96 \u00b10.00 OOM OOM OOM\nTable O: Stability of explanation size produced by explainers against the explainer instances (seeds). NA indicates the inability to find a counterfactual. OOM indicates that the explainer threw out-ofmemory error. The best explainers for each dataset (row) are highlighted in gray, yellow and cyan shading for seeds 1, 2, and 3, respectively.\nRCExplainer CF2 CLEAR\nDataset / Seeds 1 2 3 1 2 3 1 2 3\nMutagenicity 1.01 \u00b10.19 1.0 \u00b10.0 1.25 \u00b10.0 2.78 \u00b10.98 2.85 \u00b11.07 2.95 \u00b11.37 OOM OOM OOM Proteins 1.0 \u00b10.0 1.0 \u00b10.0 1.0 \u00b10.0 NA NA 3.0 \u00b10.0 OOM OOM OOM Mutag 1.1 \u00b10.22 1.0 \u00b10.0 1.0 \u00b10.0 1.0 \u00b10.0 1.25 \u00b10.35 1.0 \u00b10.0 17.15 \u00b11.62 15.6 \u00b11.86 19.05 \u00b11.31\nIMDB-B 1.0 \u00b10.0 1.0 \u00b10.0 1.0 \u00b10.0 8.57 \u00b14.99 8.29 \u00b14.50 9.01 \u00b15.58 218.62 \u00b10.0 182.25 \u00b10.0 181.38 \u00b10.0 AIDS 1.0 \u00b10.0 1.0 \u00b10.0 1.0 \u00b10.0 5.25 \u00b10.35 NA 6.0 \u00b10.0 164.95 \u00b147.93 162.32 \u00b145.70 185.29 \u00b178.92\nogbg-molhiv 1.0 \u00b10.0 1.0 \u00b10.0 1.02 \u00b10.42 10.45 \u00b14.43 9.69 \u00b14.18 10.24 \u00b14.87 OOM OOM OOM\nStability against GNN architectures: Table P shows the stability of the explainers across different GNN architectures. Similar to our factual setting (Table 8), we assess the stability by computing the Jaccard coefficient between the explained predictions of the indicated GNN architecture and the default GCN model. Unsurprisingly, the stability of the explainer highly depends on the dataset.\nRCEXPLAINER is also the most stable among all the explainers, and the produced high values indicate that the method is agnostic towards the variations in different message aggregating schemes of the architectures.\nWe further look into the stability of the counterfactual methods in terms of sufficiency (Table Q) and the explanation size (Table R) across different GNN architectures. The sufficiency results (Table Q) show large variations produced by the same method on the same dataset due to the different architectures and message passing schemes. For instance, RCEXPLAINER produces sufficiency of .10 and .93 on the AIDS dataset for GAT and GIN, respectively. In terms of explanation size(Table R), RCEXPLAINER is stable against different GNN architectures. However, consistent with previous stability results, CF2 is more unstable than RCEXPLAINER and the worst stability is shown by CLEAR.\nTable P: Stability of counterfactual explainers against the GNN architecture. We report the Jaccard coefficient of explanations obtained for GAT, GIN and GRAPHSAGE against the explanation provided over GCN. The higher the Jaccard, the more is the stability. The best explained for each dataset (row) are highlighted in gray, yellow and cyan shading for architectures GAT, GIN, and GRAPHSAGE, respectively. GRAPHSAGE is denoted by SAGE. NA indicates one or both of the architectures were unable to identify a counterfactual for the graphs. OOM indicates that the explainer threw an out-of-memory error.\nRCEXplainer CF2 CLEAR\nDataset / Architecture GAT GIN SAGE GAT GIN SAGE GAT GIN SAGE\nMutagenicity 0.95 \u00b10.05 0.94 \u00b10.06 0.95 \u00b10.03 0.79 \u00b10.13 0.75 \u00b10.16 0.84 \u00b10.10 OOM OOM OOM Proteins 0.88 \u00b10.0 NA 0.88 \u00b10.0 NA NA NA OOM OOM OOM Mutag 0.94 \u00b10.0 NA 0.90 \u00b10.02 NA NA NA 0.86 \u00b10.0 NA 0.72 \u00b10.04\nIMDB-B 0.99 \u00b10.01 0.98 \u00b10.0 0.98 \u00b10.01 NA 0.93 \u00b10.0 NA 0.60 \u00b10.0 0.70 \u00b10.0 0.76 \u00b10.0 AIDS 0.89 \u00b10.03 NA NA 0.74 \u00b10.0 0.73 \u00b10.11 0.72 \u00b10.12 0.25 \u00b10.04 0.54 \u00b10.04 0.66 \u00b10.04\nogbg-molhiv 0.96 \u00b10.02 0.96 \u00b10.01 0.96 \u00b10.02 0.63 \u00b10.12 0.13 \u00b10.14 0.61 \u00b10.16 OOM OOM OOM\nTable Q: Stability in terms of sufficiency of counterfactual explainers against the GNN architectures. OOM indicates that the explainer threw out-of-memory error. The best explainers for each dataset (row) are highlighted in gray, yellow, cyan, and pink shading for GCN, GAT, GIN, SAGE, respectively. RCExplainer outperforms other baselines on a majority of the datasets and architectures. CLEAR also is stable in terms of sufficiency but has a much larger explanation size compared to other baselines(Refer Table R).\nRCEXplainer CF2 CLEAR\nDataset / Architecture GCN GAT GIN SAGE GCN GAT GIN SAGE GCN GAT GIN SAGE\nMutagenicity 0.4 \u00b10.06 0.38 \u00b10.04 0.6 \u00b10.04 0.59 \u00b10.06 0.50 \u00b10.05 0.64 \u00b10.04 0.57 \u00b10.08 0.62 \u00b10.03 OOM OOM OOM OOM\nProteins 0.96 \u00b10.02 0.88 \u00b10.04 0.3 \u00b10.05 0.46 \u00b10.08 1.0 \u00b10.0 1.0 \u00b10.0 0.76 \u00b10.06 0.79 \u00b10.02 OOM OOM OOM OOM Mutag 0.4 \u00b10.12 0.7 \u00b10.19 1.0 \u00b10.0 0.45 \u00b10.19 0.9 \u00b10.12 0.9 \u00b10.12 0.45 \u00b10.33 0.7 \u00b10.19 0.55 \u00b10.1 0.8 \u00b10.19 1.0 \u00b10.0 0.05 \u00b10.1 IMDB-B 0.72 \u00b10.11 0.89 \u00b10.02 0.54 \u00b10.06 0.39 \u00b10.04 0.81 \u00b10.07 1.0 \u00b10.0 0.98 \u00b10.02 0.99 \u00b10.02 0.96 \u00b10.02 0.68 \u00b10.08 0.22 \u00b10.11 0.32 \u00b10.11\nAIDS 0.91 \u00b10.04 0.10 \u00b10.04 0.93 \u00b10.03 0.86 \u00b10.05 0.98 \u00b10.02 0.92 \u00b10.04 0.96 \u00b10.01 0.96 \u00b10.02 0.84 \u00b10.03 0.80 \u00b10.04 0.74 \u00b10.04 0.84 \u00b10.02\nogbg-molhiv 0.90 \u00b10.02 0.80 \u00b10.01 0.56 \u00b10.01 0.20 \u00b10.01 0.96 \u00b10.01 0.96 \u00b10.01 0.90 \u00b10.01 0.59 \u00b10.01 OOM OOM OOM OOM\nTable R: Stability of explanation size produced by explainers against different GNN architectures. OOM indicates that the explainer threw out-of-memory error. NA indicates that the explainer could not identify a counterfactual for the graphs. The best explainers for each dataset (row) are highlighted in gray, yellow, cyan, and pink shading for GCN, GAT, GIN, SAGE respectively. RCExplainer outperforms other counterfactual baselines.\nRCEXplainer CF2 CLEAR\nDataset / Architecture GCN GAT GIN SAGE GCN GAT GIN SAGE GCN GAT GIN SAGE\nMutagenicity 1.01\u00b1 0.18 1.33\u00b1 2.06 1.0\u00b1 0.0 1.03\u00b1 0.29 2.81\u00b1 1.12 3.90\u00b1 2.08 5.93\u00b1 2.97 3.32\u00b1 1.61 OOM OOM OOM OOM Proteins 1.0\u00b1 0.0 1.0\u00b1 0.0 1.0\u00b1 0.0 1.97\u00b1 7.75 3.5\u00b1 0.5 4.0\u00b1 0.0 2.62\u00b1 1.22 2.04\u00b1 1.3 OOM OOM OOM OOM Mutag 1.0\u00b1 0.0 NA NA 1.0\u00b1 0.0 2.0\u00b1 1.07 1.0\u00b1 0.0 20.36\u00b1 3.94 1.4\u00b1 0.8 38.12\u00b1 3.41 37.4\u00b1 3.61 NA 45.76\u00b1 7.94\nIMDB-B 1.0\u00b1 0.0 1.0\u00b1 0.0 1.0\u00b1 0.0 1.0\u00b1 0.0 7.78\u00b1 3.98 NA 6.0\u00b1 0.0 7.17\u00b1 3.89 424.0\u00b1 192.26 441.53\u00b1 70.97 475.42\u00b1 75.76 350.45\u00b1 86.62 AIDS 1.0\u00b1 0.0 1.0\u00b1 0.0 1.0\u00b1 0.0 1.0\u00b1 0.0 NA 4.21\u00b1 3.07 2.0\u00b1 0.0 3.22\u00b1 2.15 222.84\u00b1 47.02 667.01\u00b1 94.35 212.77\u00b1 43.18 201.09\u00b1 38.85\nStability to feature noise: Table S presents the impact of feature noise on counterfactual explanations in Mutag and Mutagenicity. We observe that CF2 and CLEAR are markedly more stable than\nRCEXPLAINER in Mutag. This outcome is not surprising, considering that RCEXPLAINER exclusively addresses topological perturbations, while both CF2 and CLEAR accommodate perturbations encompassing both topology and features. In Mutaganecity, RCEXPLAINER exhibits slightly higher stability than CF2.\nTable S: Stability of counterfactual explainers against feature perturbation on \u201cMutag\u201d and \u201cMutagenecity\u201d datasets. We do not report results for CLEAR on Mutagenicity since it runs out of GPU memory.\n(a) Mutag\nRCExplainer CF2 CLEAR\nNoise% / Metric Sufficiency Size Jaccard Sufficiency Size Jaccard Sufficiency Size Jaccard\n0 (no noise) 0.4 \u00b1 0.12 1.10 \u00b1 0.22 1.0 \u00b1 0.0 0.90 \u00b1 0.12 1.0 \u00b1 0.0 1.0 \u00b1 0.0 0.55 \u00b1 0.1 17.15 \u00b1 1.62 1.0 \u00b1 0.0\n10 1.0 \u00b1 0.0 NA NA 0.6 \u00b1 0.2 2.17 \u00b1 0.31 0.19 \u00b1 0.0 0.55 \u00b1 0.1 16.35 \u00b1 1.68 0.98 \u00b1 0.01 20 0.75 \u00b1 0.22 1.0 \u00b1 0.0 NA 0.25 \u00b1 0.16 1.95 \u00b1 0.80 NA 0.55 \u00b1 0.1 18.1 \u00b1 1.94 0.55 \u00b1 0.01 30 1.0 \u00b1 0.0 NA NA 0.6 \u00b1 0.3 1.0 \u00b1 0.0 0.29 \u00b1 0.0 0.55 \u00b1 0.1 19.1 \u00b1 2.91 0.57 \u00b1 0.02 40 1.0 \u00b1 0.0 NA NA 0.6 \u00b1 0.2 2.8 \u00b1 0.0 0.29 \u00b1 0.0 0.55 \u00b1 0.1 14.7 \u00b1 1.78 0.58 \u00b1 0.02 50 1.0 \u00b1 0.0 NA NA 0.8 \u00b1 0.1 1.5 \u00b1 0.0 NA 0.55 \u00b1 0.1 16.05 \u00b1 2.48 0.54 \u00b1 0.02\n(b) Mutagenicity\nRCEXplainer CF2\nNoise% / Metric Sufficiency Size Jaccard Sufficiency Size Jaccard\n0 (no noise) 0.4 \u00b1 0.06 1.01 \u00b1 0.19 1.0 \u00b1 0.0 0.50 \u00b1 0.05 2.78 \u00b1 0.98 1.0 \u00b1 0.0\n10 0.43 \u00b1 0.04 1.01 \u00b1 0.19 0.96 \u00b1 0.04 0.49 \u00b1 0.04 2.11 \u00b1 1.06 0.92 \u00b1 0.06 20 0.47 \u00b1 0.06 1.0 \u00b1 0.0 0.95 \u00b1 0.04 0.53 \u00b1 0.06 1.73 \u00b1 0.94 0.86 \u00b1 0.08 30 0.50 \u00b1 0.04 1.0 \u00b1 0.0 0.94 \u00b1 0.04 0.61 \u00b1 0.04 1.68 \u00b1 0.91 0.86 \u00b1 0.09 40 0.52 \u00b1 0.05 1.0 \u00b1 0.0 0.93 \u00b1 0.06 0.54 \u00b1 0.05 1.47 \u00b1 0.73 0.87 \u00b1 0.09 50 0.49 \u00b1 0.05 1.0 \u00b1 0.0 0.93 \u00b1 0.06 0.67 \u00b1 0.01 1.54 \u00b1 1.06 0.86 \u00b1 0.08\n0.1\n0.2\n0.3\n0.4\n0.5\nNe ce\nss ity\nMutagenicity\n0.00\n0.02\n0.04\n0.06\n0.08\nProteins\n0.2\n0.4\n0.6\n0.8\nMutag\n1 2 3 4 5 6 7 8 9 10 0.00\n0.05\n0.10\n0.15\n0.20\nNe ce\nss ity\nIMDB-B\n1 2 3 4 5 6 7 8 9 10 0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nAIDS\nPGExplainer TAGExplainer CF2 RCExplainer GNNExplainer GEM SubgraphX\n1 2 3 4 5 6 7 8 9 10 0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40\n0.45\nNCI1\n0.0 0.2 0.4 0.6 0.8 1.0 size (#edges in the explanation)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFigure K: Necessity of various factual explainers against the explanation size. The necessity increases with the removal of the explanations."
        },
        {
            "heading": "D NECESSITY: RESULTS FOR SEC. 4.3",
            "text": "For necessity, we remove explanations from graphs and measure the ratio of graphs for which the GNN prediction on the residual graph is flipped. We expect that removing more explanations will lead to more necessity; however, we do not necessarily expect necessity to be higher because our factual explainers are not trained to make the residual graph counterfactual. Fig. K presents the necessity performance on six datasets for all factual methods with varying explanation sizes from 1 to 10. The trend aligns with our expectations, as the removal of more explanations increases the necessity. The value of necessity varies between datasets. Proteins and IMDB-B datasets have graphs with larger sizes (in terms of the number of edges), which aligns with the small necessity score. On the other hand, datasets with relatively smaller graphs have higher necessity scores. Note that our factual methods do not optimize residual graphs to be counterfactuals; this might be another reason for the low values."
        },
        {
            "heading": "E REPRODUCIBILITY: RESULTS FOR SEC. 4.3",
            "text": "Reproducibility can be measured two different ways. (1) Retraining using only explanation graphs called Reproducibility+, (2) retraining using only residual graphs called Reproducibility\u2212. Both metrics is a ratio of an GNN accuracy compared to the original GNN accuracy. We provide the math definitions in Table 3. In our figures, we separated SubgraphX to an independent table, because we could only obtain explanations of test graphs for SubgraphX (refer to our disclaimer in Sec. A.4)\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\nRe pr\nod uc\nib ilit\ny+\nMutagenicity\n0.875\n0.900\n0.925\n0.950\n0.975\n1.000\n1.025\nProteins\n0.4\n0.6\n0.8\n1.0\nMutag\n1 2 3 4 5 6 7 8 9 10 0.75\n0.80\n0.85\n0.90\n0.95\n1.00\nRe pr\nod uc\nib ilit\ny+\nIMDB-B\n1 2 3 4 5 6 7 8 9 10 0.86\n0.88\n0.90\n0.92\n0.94\n0.96\n0.98\n1.00\nAIDS\nPGExplainer TAGExplainer CF2 RCExplainer GNNExplainer GEM\n1 2 3 4 5 6 7 8 9 10 0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95 NCI1\n0.0 0.2 0.4 0.6 0.8 1.0 size (#edges in the explanation)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFigure L: Reproducibility+ of factual explainers against size. Performance tends to rise with more edges in the explanations; however, a small number of edges does not guarantee a performance of 1.0.\nSize (#edges in the explanation)\nDatasets 1 2 3 4 5 6 7 8 9 10\nMutagenicity 1.11\u00b1 0.02 1.07\u00b1 0.03 1.08\u00b1 0.02 1.05\u00b1 0.02 1.05\u00b1 0.02 1.05\u00b1 0.02 1.03\u00b1 0.02 1.01\u00b1 0.02 0.97\u00b1 0.02 1.02\u00b1 0.02 Mutag 0.86\u00b1 0.43 0.43\u00b1 0.53 0.76\u00b1 0.5 1.08\u00b1 0.0 0.11\u00b1 0.32 0.22\u00b1 0.43 1.08\u00b1 0.0 0.86\u00b1 0.43 0.97\u00b1 0.32 1.08\u00b1 0.0\nIMDB-B 1.07\u00b1 0.16 1.1\u00b1 0.17 1.05\u00b1 0.15 1.08\u00b1 0.06 1.09\u00b1 0.08 1.07\u00b1 0.08 1.05\u00b1 0.06 1.06\u00b1 0.06 1.02\u00b1 0.05 1.05\u00b1 0.09 AIDS 1.0\u00b1 0.0 1.0\u00b1 0.0 1.0\u00b1 0.0 1.0\u00b1 0.0 1.0\u00b1 0.01 1.0\u00b1 0.01 0.99\u00b1 0.01 1.0\u00b1 0.0 1.0\u00b1 0.0 1.0\u00b1 0.0 NCI1 1.05\u00b1 0.04 1.09\u00b1 0.03 1.09\u00b1 0.03 1.08\u00b1 0.02 1.09\u00b1 0.02 1.09\u00b1 0.02 1.07\u00b1 0.03 1.07\u00b1 0.03 1.05\u00b1 0.03 1.04\u00b1 0.04\nTable T: Reproducibility+ in SubgraphX. Since we use small number of graphs for SubgraphX, the variance of the results are very high, thus unreliable.\nFigure L and Table T illustrate the Reproducibility+ performance of seven factual methods against the size of explanations for six datasets. Reproducibility increases with more edges in the explanations for the most cases, as expected. However, reaching a score of 1.0 is challenging even when selecting the most crucial edges. This suggests that explanations do not capture the full picture for GNN predictions.\nFigure M and Table U illustrate the Reproducibility\u2212 performance of seven factual methods against the size of explanations for six datasets. Reproducibility remains high even when the most crucial edges from the graphs are removed. This demonstrates that the explainers hardly capture the real cause of the GNN predictions.\n0.92\n0.94\n0.96\n0.98\n1.00\nRe pr\nod uc\nib ilit\ny\u2212\nMutagenicity\n0.96\n0.98\n1.00\n1.02\nProteins\n0.7\n0.8\n0.9\n1.0\nMutag\n1 2 3 4 5 6 7 8 9 10\n0.96\n0.98\n1.00\n1.02\nRe pr\nod uc\nib ilit\ny\u2212\nIMDB-B\n1 2 3 4 5 6 7 8 9 10 0.980\n0.985\n0.990\n0.995\n1.000\n1.005 AIDS\nPGExplainer TAGExplainer CF2 RCExplainer GNNExplainer GEM\n1 2 3 4 5 6 7 8 9 10 0.94\n0.96\n0.98\n1.00\n1.02\n1.04\n1.06\n1.08 NCI1\n0.0 0.2 0.4 0.6 0.8 1.0 size (#edges in the explanation)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.\nFigure M: Reproducibility\u2212 of factual explainers against size. Even when the most crucial edges are taken out, the performance still remains close to 1.0.\nSize (#edges in the explanation)\nDatasets 1 2 3 4 5 6 7 8 9 10\nMutagenicity 1.06\u00b1 0.03 1.01\u00b1 0.02 1.01\u00b1 0.02 1.01\u00b1 0.03 1.0\u00b1 0.03 1.0\u00b1 0.03 1.01\u00b1 0.03 0.99\u00b1 0.03 1.0\u00b1 0.03 0.99\u00b1 0.03 Mutag 1.08\u00b1 0.0 1.08\u00b1 0.0 0.97\u00b1 0.32 1.08\u00b1 0.0 0.97\u00b1 0.32 0.97\u00b1 0.32 1.08\u00b1 0.0 1.08\u00b1 0.0 1.08\u00b1 0.0 1.08\u00b1 0.0\nIMDB-B 0.82\u00b1 0.28 0.96\u00b1 0.14 0.86\u00b1 0.27 0.85\u00b1 0.28 0.86\u00b1 0.28 0.82\u00b1 0.28 0.84\u00b1 0.27 0.85\u00b1 0.28 0.85\u00b1 0.28 0.86\u00b1 0.28 AIDS 1.0\u00b1 0.0 1.0\u00b1 0.01 1.0\u00b1 0.01 1.0\u00b1 0.0 1.0\u00b1 0.0 1.0\u00b1 0.01 0.99\u00b1 0.01 1.0\u00b1 0.01 1.0\u00b1 0.01 1.0\u00b1 0.01 NCI1 0.9\u00b1 0.06 0.91\u00b1 0.05 0.89\u00b1 0.07 0.93\u00b1 0.07 0.92\u00b1 0.07 0.92\u00b1 0.06 0.91\u00b1 0.05 0.9\u00b1 0.04 0.91\u00b1 0.05 0.92\u00b1 0.05\nTable U: Reproducibility\u2212 in SubgraphX. Since we use small number of graphs for SubgraphX, the variance of the results are very high, thus unreliable.\nF VISUALIZATION OF EXPLANATIONS\nIn Figs. N and O, we engage in a visual analysis of the explanations provided by various GNN explainers. The graphs presented in these figures represent mutagenic molecules sourced from the Mutag dataset. Several insights emerge from this analysis. Factual: The mutagenic attribute of the molecule in Fig. N stems from the presence of the NO2 group attached to the benzene ringYing et al. (2019b); Debnath et al. (1991). As a result, the optimal explanation entails pinpointing this specific benzene ring in conjunction with the NO2 group. Notably, we observe that while certain explainers identify fragments of this subgraph, with CF2 achieving the highest overlap, many also highlight bonds originating from regions outside the authentic explanatory context. Adding to the intrigue, the explanation offered by RCEXPLAINER stands out due to its compactness, resulting in commendable statistical performance. However, this succinct explanation\nlacks meaning in the eyes of a domain expert. Consequently, a pressing need arises for real-world datasets endowed with ground truth explanations, a resource that the current field unfortunately lacks. Counterfactuals: Fig. O illustrates two molecules, with Molecule 1 (top row) being identical to the one shown in Fig. N. The optimal explanation involves eliminating the NO2 component, a task accomplished solely by CF2 in the case of Molecule 1. While the remaining explanation methods can indeed alter the GNN prediction by implementing the changes described in Figure O, two critical insights emerge. First, statistically, RCEXPLAINER is considered a better explanation than CF2 since its size is 1 compared to 3 of CF2. However, our interaction with multiple chemists clearly indicated their preference towards CF2 since eliminates the entire NO2 group. Second, chemically infeasible explanations are common as evident from CLEAR for both molecules and CF2 in molecule 2. Both fail to adhere to valency rules, a behavior also noted in Sec. 4.4.\nFigure N: Visualization of factual explanations on a mutagenic molecule from the Mutag dataset. The explanations contain the edges highlighted in red.\nFigure O: Visualization of counterfactual explanations on Mutag dataset. Edge additions and deletions are represented by green and red colors respectively."
        },
        {
            "heading": "G ADDITIONAL EXPERIMENTS ON SPARSITY METRIC",
            "text": "Counterfactual explainers: Sparsity is defined as the proportion of edges from the graph that is retained in the counter-factual Yuan et al. (2022); a value close to 1 is desired. In node classification, we compute this proportion for edges in the N \u2113v , i.e., the \u2113-hop neighbourhood of the target node v. We present the results on sparsity metric on node and graph classification tasks in Table V and W,\nrespectively. For node classification, we see CF-GNNEXPLAINER continues to outperform (Table V). The results are consistent with our earlier results in Table 6. Similarly, Table W shows that RCExplainer continues to outperform in the case of graph classification (earlier results show a similar trend in Table 5).\nFactual explainers: For experiments with factual explainers, we report results of the necessity metric on varying degrees of sparsity (Recall Fig. K). Note that size acts as a proxy for sparsity in this case. This is because sparsity only involves the normalized size where it is normalized by the number of total edges. So sparsity can be obtained by normalizing all perturbation sizes in the plots to get the sparsity metric. For counterfactual explainers, we do not supply explanation size as a parameter. Hence, computing the sparsity of the predicted counterfactual becomes relevant.\nTable W: Results on sparsity of counterfactual explainers for graph classification. Best results are shown in gray. RCEXPLAINER consistently outperforms the other methods.\nMethod/Dataset Mutagenicity Mutag Proteins AIDS IMDB-B ogbg-molhiv\nRCExplainer 0.96 \u00b10.00 0.94 \u00b10.01 0.94 \u00b10.02 0.91 \u00b10.00 0.98 \u00b10.00 0.96 +- 0.00 CF2 0.90 \u00b10.01 0.94 \u00b10.0 NA 0.99 \u00b10.01 0.89 \u00b10.04 0.62 \u00b10.05\nCLEAR OOM 0.88 \u00b10.07 OOM 0.66 \u00b10.04 0.87 \u00b10.06 OOM"
        },
        {
            "heading": "H TAGEXPLAINER VARIANTS",
            "text": "TAGExplainer has two stages. We define TAGExplainer (1) as when we apply only the first stage and get explanations, whereas TAGExplainer (2) applies both stages. Figure P compares performance of these two variants."
        },
        {
            "heading": "I FACTUAL EXPLAINERS ON OGBG-MOLHIV",
            "text": "Figure R demonstrates a new dataset OGBG-Molhiv for three factual explainers. On this dataset, three factual methods are close to each other in terms of sufficiency performance."
        },
        {
            "heading": "J EXISTING BENCHMARKING STUDIES ON GNN EXPLAINABILITY",
            "text": "GraphFrameX Amara et al. (2022) and GraphXAI Agarwal et al. (2023) represent two notable benchmarking studies. While both investigations have contributed valuable insights into GNN explainers, certain unresolved investigative aspects persist.\n\u2022 Inclusion of counterfactual explainability: GraphFrameX and GraphXAI have focused on factual explainers for GNNs. Prado-Romero et al. (2023) has discussed methods and challenges, but benchmarking on counterfactual explainers remains underexplored. \u2022 Achieving Comprehensive coverage: Existing literature encompasses seven perturbation-based factual explainers. However, GraphFrameX and GraphXAI collectively assess only GnnExplainer Ying et al. (2019b), PGExplainer Luo et al. (2020), and SubgraphX Yuan et al. (2021). \u2022 Empirical investigations: How susceptible are the explanations to topological noise, variations in GNN architectures, or optimization stochasticity? Do the counterfactual explanations provided align with the structural and functional integrity of the underlying domain? To what extent do these explainers elucidate the GNN model as opposed to the underlying data? Are there standout explainers that consistently outperform others in terms of performance? These are critical empirical inquiries that necessitate attention.\n0.65\n0.70\n0.75\n0.80\n0.85\nSu ffi\ncie nc\ny\nMutagenicity\n0.75\n0.80\n0.85\n0.90\n0.95\nProteins\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0 Mutag\n0.650\n0.675\n0.700\n0.725\n0.750\n0.775\n0.800\n0.825\nSu ffi\ncie nc\ny\nIMDB-B\n0.85\n0.90\n0.95\n1.00 AIDS\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\nNCI1\n5 10 15 20 25\n0.970\n0.975\n0.980\n0.985\n0.990\n0.995\n1.000\nSu ffi\ncie nc\ny\nGraph-SST2\n5 10 15 20 25 0.450\n0.475\n0.500\n0.525\n0.550\n0.575\n0.600\n0.625 DD\nTAGExplainer (1) TAGExplainer (2)\n5 10 15 20 25\n0.46\n0.48\n0.50\n0.52\n0.54 REDDIT-B\n0.0 0. 0.4 0.6 .8 1.0 size (#edges in the explanation)\n0.0\n0.2\n0.4\n.6\n0.8\n1.0\nFigure P: Sufficiency of TAGExplainer variants against size. Applying the second stage does not help much for TAGExplainer.\nMutagenicity\nProteins\nMutag\nIMDB-B\nAIDSNCI1\nGraph-SST2\nDD\nREDDIT-B\n0.2\n0.4\n0.6\n0.8\nPGExplainer TAGExplainer CF2 RCExplainer GNNExplainer GEM SubgraphX\nFigure Q: Spiderplot for sufficiency performance averaged for different sizes of explanations. Even though there is no clear winner method, GNNEXPLAINER and RCEXPLAINER appear among the top performers in the majority of the datasets. We omit those methods for a dataset that throw an out-of-memory (OOM) error and are not scalable.\n5 10 15 20 25 0.980\n0.982\n0.984\n0.986\n0.988\n0.990\nSu ffi\ncie nc\ny\nogbg_molhiv\nPGExplainer TAGExplainer RCExplainer GNNExplainer\n0.0 0.2 0.4 0.6 0.8 1.0 size (#edges in the explanation)\n0.0\n0.\n0.4\n0.6\n0.8\n1.0\nFigure R: Sufficiency of the factual explainers against the explanation size for ogbg-molhiv dataset.\nReasoning\nTask\nTask\nGeneralization\nGNNExplainer\nRCExplainer\nCF-GNNExplainer\nRCExplainer\nTAGExplainer\nCF F NC\nGC\nGC/ NC\n*\nInstanc e-speci\nfic\nData\nFigure S: Flowchart of our recommendations of explainers in different scenarios. Note that \u201cF\u201d, \u201cCF\u201d, \u201cNC\u201d, \u201cGC\u201d, and \u201c*\u201d denote factual, counterfactual, node classification, graph classification, and generalized tasks respectively."
        },
        {
            "heading": "K RECOMMENDATIONS IN PRACTICE",
            "text": "The choice of the explainer depends on various factors and we make the following recommendations.\n\u2022 For counter-factual reasoning, we recommend RCExplainer for graph classification and CF-GNNExplainer for node classification.\n\u2022 For factual reasoning, if the goal is to do node or graph classification, we need to first decide if we need an inductive reasoner or transductive. While an inductive reasoner is more suitable we want to generalize to large volumes of unseen graphs, transductive is suitable for explaining single instances. In case of inductive, we recommend RCExplainer, while for transductive GNNExplainer stands out as the method of choice. These two algorithms had the highest sufficiency on average. In addition, RCExplainer displayed stability in the face of noise injection. \u2013 For different task generalization beyond node and graph classification, one can consider using\nTAGExplainer. \u2013 In high-stakes applications, we recommend RCExplainer due to consistent results across\ndifferent runs and robustness to noise.\n\u2022 For both types of explainers, the transductive methods are slow. So, if the dataset is large, it is always better to use an inductive explainer over transductive ones.\nWhile the above is a guideline, we emphasize that there is no one-size-fits-all benchmark for selecting the ideal explainer. The choice depends on the characteristics of the application in hand and/or the dataset. The above flowchart takes these factors into account to streamline the decision process. We have now added a flowchart to help the user in selecting the most appropriate. This recommendation has now also been added as a flowchart (Fig. S)."
        }
    ],
    "title": "PERTURBATION-BASED GNN EXPLAINERS THROUGH",
    "year": 2024
}