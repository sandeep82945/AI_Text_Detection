{
    "abstractText": "Generative flow networks (GFlowNets) are sequential sampling models trained to match a given distribution. GFlowNets have been successfully applied to various structured object generation tasks, sampling a diverse set of high-reward objects quickly. We propose expected flow networks (EFlowNets), which extend GFlowNets to stochastic environments. We show that EFlowNets outperform other GFlowNet formulations in stochastic tasks such as protein design. We then extend the concept of EFlowNets to adversarial environments, proposing adversarial flow networks (AFlowNets) for two-player zero-sum games. We show that AFlowNets learn to find above 80% of optimal moves in Connect-4 via self-play and outperform AlphaZero in tournaments.",
    "authors": [],
    "id": "SP:34942db7fbb777d881021cc6f70c74c2efd3addf",
    "references": [
        {
            "authors": [
                "Victor Allis"
            ],
            "title": "A knowledge-based approach of connect-four",
            "venue": "J. Int. Comput. Games Assoc.,",
            "year": 1988
        },
        {
            "authors": [
                "Ioannis Antonoglou",
                "Julian Schrittwieser",
                "Sherjil Ozair",
                "Thomas K Hubert",
                "David Silver"
            ],
            "title": "Planning in stochastic environments with a learned model",
            "venue": "International Conference on Learning Representations (ICLR),",
            "year": 2022
        },
        {
            "authors": [
                "Marc G. Bellemare",
                "Will Dabney",
                "Remi Munos"
            ],
            "title": "A distributional perspective on reinforcement learning",
            "venue": "International Conference on Machine Learning (ICML),",
            "year": 2017
        },
        {
            "authors": [
                "Emmanuel Bengio",
                "Moksh Jain",
                "Maksym Korablyov",
                "Doina Precup",
                "Yoshua Bengio"
            ],
            "title": "Flow network based generative models for non-iterative diverse candidate generation",
            "venue": "Neural Information Processing Systems (NeurIPS),",
            "year": 2021
        },
        {
            "authors": [
                "YooJung Choi",
                "Antonio Vergari",
                "Guy Van den Broeck"
            ],
            "title": "Probabilistic circuits: A unifying framework for tractable probabilistic models, 2020",
            "venue": "URL http://starai.cs.ucla.edu/ papers/ProbCirc20.pdf",
            "year": 2020
        },
        {
            "authors": [
                "R\u00e9mi Coulom"
            ],
            "title": "Whole-history rating: A Bayesian rating system for players of time-varying strength",
            "venue": "Computers and Games,",
            "year": 2008
        },
        {
            "authors": [
                "Tristan Deleu",
                "Ant\u00f3nio G\u00f3is",
                "Chris Emezue",
                "Mansi Rankawat",
                "Simon Lacoste-Julien",
                "Stefan Bauer",
                "Yoshua Bengio"
            ],
            "title": "Bayesian structure learning with generative flow networks",
            "venue": "Uncertainty in Artificial Intelligence (UAI),",
            "year": 2022
        },
        {
            "authors": [
                "Tristan Deleu",
                "Mizu Nishikawa-Toomey",
                "Jithendaraa Subramanian",
                "Nikolay Malkin",
                "Laurent Charlin",
                "Yoshua Bengio"
            ],
            "title": "Joint Bayesian inference of graphical structure and parameters with a single generative flow network",
            "year": 1936
        },
        {
            "authors": [
                "Manfred Diaz",
                "Aur\u00e9lien"
            ],
            "title": "B\u00fcck-Kaeffer. PopRank: A rating library for population-based training, 2023",
            "venue": "URL https://github.com/poprl/poprank",
            "year": 2023
        },
        {
            "authors": [
                "Jianqing Fan",
                "Zhaoran Wang",
                "Yuchen Xie",
                "Zhuoran Yang"
            ],
            "title": "A theoretical analysis of deep qlearning",
            "venue": "In Learning for dynamics and control,",
            "year": 2020
        },
        {
            "authors": [
                "Jacob K Goeree",
                "Charles A Holt",
                "Thomas R Palfrey"
            ],
            "title": "Stochastic game theory for social science: A primer on quantal response equilibrium",
            "venue": "In Handbook of Experimental Game Theory,",
            "year": 2020
        },
        {
            "authors": [
                "Tuomas Haarnoja",
                "Haoran Tang",
                "Pieter Abbeel",
                "Sergey Levine"
            ],
            "title": "Reinforcement learning with deep energy-based policies",
            "venue": "International Conference on Machine Learning (ICML),",
            "year": 2017
        },
        {
            "authors": [
                "Tuomas Haarnoja",
                "Aurick Zhou",
                "Pieter Abbeel",
                "Sergey Levine"
            ],
            "title": "Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor",
            "venue": "International Conference on Machine Learning (ICML),",
            "year": 2018
        },
        {
            "authors": [
                "Edward J Hu",
                "Nikolay Malkin",
                "Moksh Jain",
                "Katie Everett",
                "Alexandros Graikos",
                "Yoshua Bengio"
            ],
            "title": "GFlowNet-EM for learning compositional latent variable models",
            "venue": "International Conference on Machine Learning (ICML),",
            "year": 2023
        },
        {
            "authors": [
                "Moksh Jain",
                "Emmanuel Bengio",
                "Alex Hernandez-Garcia",
                "Jarrid Rector-Brooks",
                "Bonaventure F.P. Dossou",
                "Chanakya Ekbote",
                "Jie Fu",
                "Tianyu Zhang",
                "Micheal Kilgour",
                "Dinghuai Zhang",
                "Lena Simine",
                "Payel Das",
                "Yoshua Bengio"
            ],
            "title": "Biological sequence design with GFlowNets",
            "venue": "International Conference on Machine Learning (ICML),",
            "year": 2022
        },
        {
            "authors": [
                "Salem Lahlou",
                "Tristan Deleu",
                "Pablo Lemos",
                "Dinghuai Zhang",
                "Alexandra Volokhova",
                "Alex Hern\u00e1ndez-Garc\u0131\u0301a",
                "L\u00e9na N\u00e9hale Ezzine",
                "Yoshua Bengio",
                "Nikolay Malkin"
            ],
            "title": "A theory of continuous generative flow networks",
            "venue": "International Conference on Machine Learning (ICML),",
            "year": 2023
        },
        {
            "authors": [
                "Wenqian Li",
                "Yinchuan Li",
                "Zhigang Li",
                "Jianye Hao",
                "Yan Pang"
            ],
            "title": "DAG Matters! GFlowNets enhanced explainer for graph neural networks",
            "venue": "International Conference on Learning Representations (ICLR),",
            "year": 2023
        },
        {
            "authors": [
                "Dianbo Liu",
                "Moksh Jain",
                "Bonaventure F.P. Dossou",
                "Qianli Shen",
                "Salem Lahlou",
                "Anirudh Goyal",
                "Nikolay Malkin",
                "Chris C. Emezue",
                "Dinghuai Zhang",
                "Nadhir Hassen",
                "Xu Ji",
                "Kenji Kawaguchi",
                "Yoshua Bengio"
            ],
            "title": "GFlowOut: Dropout with generative flow networks",
            "venue": "International Conference on Machine Learning (ICML),",
            "year": 2022
        },
        {
            "authors": [
                "R Duncan Luce"
            ],
            "title": "Individual Choice Behavior: A Theoretical Analysis",
            "year": 1959
        },
        {
            "authors": [
                "Kanika Madan",
                "Jarrid Rector-Brooks",
                "Maksym Korablyov",
                "Emmanuel Bengio",
                "Moksh Jain",
                "Andrei Nica",
                "Tom Bosc",
                "Yoshua Bengio",
                "Nikolay Malkin"
            ],
            "title": "Learning GFlowNets from partial episodes for improved convergence and stability",
            "venue": "International Conference on Machine Learning (ICML),",
            "year": 2023
        },
        {
            "authors": [
                "Nikolay Malkin",
                "Moksh Jain",
                "Emmanuel Bengio",
                "Chen Sun",
                "Yoshua Bengio"
            ],
            "title": "Trajectory balance: Improved credit assignment in GFlowNets",
            "venue": "Neural Information Processing Systems (NeurIPS),",
            "year": 2022
        },
        {
            "authors": [
                "Nikolay Malkin",
                "Salem Lahlou",
                "Tristan Deleu",
                "Xu Ji",
                "Edward Hu",
                "Katie Everett",
                "Dinghuai Zhang",
                "Yoshua Bengio"
            ],
            "title": "GFlowNets and variational inference",
            "venue": "International Conference on Learning Representations (ICLR),",
            "year": 2023
        },
        {
            "authors": [
                "Richard D. McKelvey",
                "Thomas R. Palfrey"
            ],
            "title": "Quantal response equilibria for normal form games",
            "venue": "Games and Economic Behavior,",
            "year": 1995
        },
        {
            "authors": [
                "Richard D. McKelvey",
                "Thomas R. Palfrey"
            ],
            "title": "A statistical theory of equilibrium in games",
            "venue": "Japanese Economic Review,",
            "year": 1996
        },
        {
            "authors": [
                "Richard D. McKelvey",
                "Thomas R. Palfrey"
            ],
            "title": "Quantal response equilibria for extensive form games",
            "venue": "Experimental Economics,",
            "year": 1998
        },
        {
            "authors": [
                "Ofir Nachum",
                "Mohammad Norouzi",
                "Kelvin Xu",
                "Dale Schuurmans"
            ],
            "title": "Bridging the gap between value and policy based reinforcement learning",
            "venue": "Neural Information Processing Systems (NIPS),",
            "year": 2017
        },
        {
            "authors": [
                "Ling Pan",
                "Dinghuai Zhang",
                "Moksh Jain",
                "Longbo Huang",
                "Yoshua Bengio"
            ],
            "title": "Stochastic generative flow networks",
            "venue": "Uncertainty in Artificial Intelligence (UAI),",
            "year": 2023
        },
        {
            "authors": [
                "Pascal Pons"
            ],
            "title": "Connect 4 game solver. https://github.com/PascalPons/connect4, 2019",
            "year": 2019
        },
        {
            "authors": [
                "Aditya Prasad",
                "Vish Abrams",
                "Anthony Young"
            ],
            "title": "Lessons from implementing alphazero. Medium, 2018",
            "venue": "URL https://medium.com/oracledevs/ lessons-from-implementing-alphazero-7e36e9054191",
            "year": 2023
        },
        {
            "authors": [
                "Jarrid Rector-Brooks",
                "Kanika Madan",
                "Moksh Jain",
                "Maksym Korablyov",
                "Cheng-Hao Liu",
                "Sarath Chandar",
                "Nikolay Malkin",
                "Yoshua Bengio"
            ],
            "title": "Thompson sampling for improved exploration in GFlowNets",
            "venue": "arXiv preprint arXiv:2306.17693,",
            "year": 2023
        },
        {
            "authors": [
                "Julian Schrittwieser",
                "Ioannis Antonoglou",
                "Thomas Hubert",
                "Karen Simonyan",
                "Laurent Sifre",
                "Simon Schmitt",
                "Arthur Guez",
                "Edward Lockhart",
                "Demis Hassabis",
                "Thore Graepel"
            ],
            "title": "Mastering atari, go, chess and shogi by planning with a learned model",
            "year": 2020
        },
        {
            "authors": [
                "David Silver",
                "Thomas Hubert",
                "Julian Schrittwieser",
                "Ioannis Antonoglou",
                "Matthew Lai",
                "Arthur Guez",
                "Marc Lanctot",
                "Laurent Sifre",
                "Dharshan Kumaran",
                "Thore Graepel",
                "Timothy Lillicrap",
                "Karen Simonyan",
                "Demis Hassabis"
            ],
            "title": "A general reinforcement learning algorithm that masters chess, shogi, and go through self-play",
            "venue": "science.aar6404. URL https://www.science.org/doi/abs/10.1126/science",
            "year": 2018
        },
        {
            "authors": [
                "Richard S Sutton",
                "Andrew G Barto"
            ],
            "title": "Reinforcement learning: An introduction",
            "venue": "MIT press,",
            "year": 2018
        },
        {
            "authors": [
                "Shantanu Thakoor",
                "Surag Nair",
                "Megha Jhunjhunwala"
            ],
            "title": "Learning to play Othello without human knowledge, 2016",
            "year": 2016
        },
        {
            "authors": [
                "Emile van Krieken",
                "Thiviyan Thanapalasingam",
                "Jakub Tomczak",
                "Frank van Harmelen",
                "Annette ten Teije"
            ],
            "title": "A-NeSI: A scalable approximate method for probabilistic neurosymbolic inference",
            "venue": "arXiv preprint arXiv:2212.12393,",
            "year": 2022
        },
        {
            "authors": [
                "Antonio Vergari",
                "YooJung Choi",
                "Anji Liu",
                "Stefano Teso",
                "Guy Van den Broeck"
            ],
            "title": "A compositional atlas of tractable circuit operations for probabilistic inference",
            "venue": "Neural Information Processing Systems (NeurIPS),",
            "year": 2021
        },
        {
            "authors": [
                "Sherry Yang",
                "Dale Schuurmans",
                "Pieter Abbeel",
                "Ofir Nachum"
            ],
            "title": "Dichotomy of control: Separating what you can control from what you",
            "year": 2023
        },
        {
            "authors": [
                "David Zhang",
                "Corrado Rainone",
                "Markus Peschl",
                "Roberto Bondesan"
            ],
            "title": "Robust scheduling with GFlowNets",
            "venue": "International Conference on Learning Representations (ICLR),",
            "year": 2023
        },
        {
            "authors": [
                "Dinghuai Zhang",
                "Nikolay Malkin",
                "Zhen Liu",
                "Alexandra Volokhova",
                "Aaron Courville",
                "Yoshua Bengio"
            ],
            "title": "Generative flow networks for discrete probabilistic modeling",
            "venue": "International Conference on Machine Learning (ICML),",
            "year": 2022
        },
        {
            "authors": [
                "Dinghuai Zhang",
                "L. Pan",
                "Ricky T.Q. Chen",
                "Aaron C. Courville",
                "Yoshua Bengio"
            ],
            "title": "Distributional GFlowNets with quantile flows",
            "venue": "arXiv preprint arXiv:2302.05793,",
            "year": 2023
        },
        {
            "authors": [
                "Heiko Zimmermann",
                "Fredrik Lindsten",
                "Jan-Willem van de Meent",
                "Christian A. Naesseth"
            ],
            "title": "A variational perspective on generative flow networks",
            "venue": "Transactions on Machine Learning Research (TMLR),",
            "year": 2023
        },
        {
            "authors": [
                "Prasad"
            ],
            "title": "As architecture, we use a convolutional neural network composed of residual blocks inspired by the AlphaZero architecture (Silver et al., 2018; Thakoor et al., 2016) with a few modifications. We remove the batch normalization layers as the population statistics varied too much between training and evaluation. We use only the policy head (using one for each side, e.g., playing as \u201dX",
            "year": 2018
        }
    ],
    "sections": [
        {
            "text": "Generative flow networks (GFlowNets) are sequential sampling models trained to match a given distribution. GFlowNets have been successfully applied to various structured object generation tasks, sampling a diverse set of high-reward objects quickly. We propose expected flow networks (EFlowNets), which extend GFlowNets to stochastic environments. We show that EFlowNets outperform other GFlowNet formulations in stochastic tasks such as protein design. We then extend the concept of EFlowNets to adversarial environments, proposing adversarial flow networks (AFlowNets) for two-player zero-sum games. We show that AFlowNets learn to find above 80% of optimal moves in Connect-4 via self-play and outperform AlphaZero in tournaments."
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "Generative flow networks (GFlowNets; Bengio et al., 2021; 2023; Lahlou et al., 2023) are a unifying algorithmic framework for training stochastic policies in Markov decision processes (MDPs; Sutton & Barto, 2018) to sample from a given distribution over terminal states. GFlowNets have been used as an efficient alternative to Monte Carlo methods for amortized sampling in applications that require finding diverse high-reward samples (Jain et al., 2022; Zhang et al., 2022; 2023a; Li et al., 2023, see \u00a75). This paper revisits and extends the view of GFlowNets as diversity-seeking learners in MDPs, enabling the training of robust agents in stochastic environments and two-player adversarial games.\nThe main application of GFlowNets to date has been generation of structured objects \ud835\udc65 \u2208 X \u2013 where X is the set of terminal states of an episodic MDP \u2013 given a reward function \ud835\udc45 : X \u2192 R>0 interpreted as an unnormalized density. The generation of \ud835\udc65 follows a trajectory \ud835\udc600 \u2192 \ud835\udc601 \u2192 \u00b7 \u00b7 \u00b7 \u2192 \ud835\udc60\ud835\udc5b = \ud835\udc65, representing iterative construction or refinement (e.g., building a graph by adding one edge at a time). These settings assume that applying an action to a partially constructed object \ud835\udc60\ud835\udc56 deterministically yields the object \ud835\udc60\ud835\udc56+1. It is natural to attempt to generalize the GFlowNet framework to stochastic environments, in which a given sequence of actions does not always produce the same final state.\nHowever, the common notions of GFlowNets must be modified to recover a useful stochastic generalization. An existing attempt (Pan et al., 2023) proposes to treat stochastic transitions in the environment as actions of the GFlowNet drawn from a fixed policy. One of the starting points for this paper is that this previous formulation sacrifices several desirable theoretical properties, inducing poor sampling performance in many practical settings. We propose an alternative notion of expected flow networks (EFlowNets), which provably do not suffer from the previous limitations (Fig. 1a).\nAs EFlowNets can perform inference via stochastic control in a fixed environment, they can be used to learn robust strategies against a stochastic opponent in a two-player game. We further define an adversarial flow network (AFlowNet) as a collection of EFlowNet players, each with its own reward function, taking actions in a shared environment. We show the existence and uniqueness of a joint optimum of the players\u2019 respective objectives. This stable point can be characterized via a probabilistic notion of game-theoretic equilibrium. We perform additional theoretical analysis and develop efficient training objectives for two-player zero-sum games.\nThe contributions of this work are as follows:\n(1) We propose expected flow networks (EFlowNets, \u00a73.1), a class of sequential sampling models and learning objectives generalizing GFlowNets on tree-structured state spaces. We demonstrate\ntheoretically and experimentally the advantages of the EFlowNet formulation over past attempts to generalize GFlowNets to stochastic environments (\u00a73.1, \u00a74.1). (2) We define adversarial flow networks (AFlowNets, \u00a73.2) for two-player games and prove the existence and uniqueness of equilibrium solutions. In Proposition 4 we exploit the zero-sum structure of two-player games to get a novel trajectory balance (TB) loss for AFlowNets. We believe this new loss is a major algorithmic novelty. We conduct extensive experiments on two-player games showing that AFlowNets are able to learn robust policies via self-play (\u00a74.2). (3) We connect GFlowNets, EFlowNets, and AFlowNets to models of imperfect agents in psychology and behavioral economics (\u00a7A)."
        },
        {
            "heading": "2 BACKGROUND",
            "text": ""
        },
        {
            "heading": "2.1 GFLOWNETS IN TREE-SHAPED ENVIRONMENTS",
            "text": "We review GFlowNets in deterministic environments, mainly following the conventions from Malkin et al. (2022). (Notably, we assume that terminating states have no children and rewards are strictly positive.) In keeping with past work, we use the language of directed acyclic graphs (DAGs), rather than the equivalent language of deterministic MDPs. We state all results only for tree-structured state spaces, but note that they are special cases of results for general DAGs.\nSetting. Let \ud835\udc3a = (S,A) be a directed tree, with finite sets of vertices (states) S and edges (actions) A \u2282 S \u00d7 S, oriented away from the root (initial state) \ud835\udc600 \u2208 S. Denote by Ch(\ud835\udc60) the set of children of a state \ud835\udc60 and by Pa(\ud835\udc60) the parent of \ud835\udc60, which exists unless \ud835\udc60 = \ud835\udc600. The set of childless (terminal) states is denoted X. A complete trajectory is a sequence \ud835\udf0f = (\ud835\udc600 \u2192 \ud835\udc601 \u2192 \u00b7 \u00b7 \u00b7 \u2192 \ud835\udc60\ud835\udc5b), where \ud835\udc60\ud835\udc5b \u2208 X and each \ud835\udc60\ud835\udc56 \u2192 \ud835\udc60\ud835\udc56+1 is an action. The set of complete trajectories is denoted T . A (forward) policy is a collection of distributions \ud835\udc43\ud835\udc39 (\u00b7 | \ud835\udc60) over Ch(\ud835\udc60) for every \ud835\udc60 \u2208 S \\ X. A policy induces a distribution over T , with \ud835\udc43\ud835\udc39 (\ud835\udc600 \u2192 \ud835\udc601 \u2192 \u00b7 \u00b7 \u00b7 \u2192 \ud835\udc60\ud835\udc5b) = \u220f\ud835\udc5b \ud835\udc56=1 \ud835\udc43\ud835\udc39 (\ud835\udc60\ud835\udc56 | \ud835\udc60\ud835\udc56\u22121). This in turn induces a terminating distribution \ud835\udc43\u22a4 \ud835\udc39\nover X, defined as the marginal distribution over the final state of a trajectory \ud835\udf0f \u223c \ud835\udc43\ud835\udc39 (\ud835\udf0f). One can sample \ud835\udc65 \u223c \ud835\udc43\u22a4\ud835\udc39 (\ud835\udc65) by running a chain starting at \ud835\udc600 and transitioning according to \ud835\udc43\ud835\udc39 until a terminal state \ud835\udc65 is reached.\nA reward function is a function \ud835\udc45 : X \u2192 R>0. A policy \ud835\udc43\ud835\udc39 is said to sample proportionally to the reward \ud835\udc45 if \ud835\udc43\u22a4\n\ud835\udc39 (\ud835\udc65) \u221d \ud835\udc45(\ud835\udc65), i.e., \ud835\udc43\u22a4 \ud835\udc39 (\ud835\udc65) = \ud835\udc45(\ud835\udc65)/\ud835\udc4d for all \ud835\udc65 \u2208 X, where \ud835\udc4d = \u2211\ud835\udc65\u2208X \ud835\udc45(\ud835\udc65). Given a\nreward function \ud835\udc45, GFlowNet algorithms aim to produce a policy \ud835\udc43\ud835\udc39 that samples proportionally to \ud835\udc45. This is, in essence, a generative modeling problem, but the setting is close to maximum-entropy reinforcement learning (RL; Haarnoja et al., 2017): one is not given samples from the target density,\nas in typical generative modeling settings, but must rather explore the reward landscape through sequential sampling.\nFM and DB objectives. We review the two GFlowNet objectives of flow matching (FM; Bengio et al., 2021) and detailed balance (DB; Bengio et al., 2023) in tree-structured state spaces.\nThe FM objective optimizes a function \ud835\udc39 : S \u2192 R>0, called the state flow. The objective enforces a pair of constraints, which in tree-structured DAGs are \ud835\udc39 (\ud835\udc60) = \u2211\ufe01\n\ud835\udc60\u2032\u2208Ch(\ud835\udc60) \ud835\udc39 (\ud835\udc60\u2032) \u2200\ud835\udc60 \u2208 S \\ X and \ud835\udc39 (\ud835\udc65) = \ud835\udc45(\ud835\udc65) \u2200\ud835\udc65 \u2208 X. (1)\nAny edge flow \ud835\udc39 induces a policy \ud835\udc43\ud835\udc39 , defined by \ud835\udc43\ud835\udc39 (\ud835\udc60\u2032 | \ud835\udc60) = \ud835\udc39 (\ud835\udc60 \u2032 ) \ud835\udc39 (\ud835\udc60) for all (\ud835\udc60, \ud835\udc60 \u2032) \u2208 A. If the flow satisfies the FM constraints (1), then it holds that \ud835\udc43\ud835\udc39 samples proportionally to the reward \ud835\udc45.\nThe DB objective avoids the explicit summation over children in (1) and jointly optimizes both \ud835\udc39 and the policy \ud835\udc43\ud835\udc39 , replacing the first constraint by \ud835\udc39 (\ud835\udc60)\ud835\udc43\ud835\udc39 (\ud835\udc60\u2032 | \ud835\udc60) = \ud835\udc39 (\ud835\udc60\u2032) \u2200(\ud835\udc60, \ud835\udc60\u2032) \u2208 A. (2) This constraint implies the first constraint of (1), as \ud835\udc43\ud835\udc39 sums to 1 over \ud835\udc60\u2032 \u2208 Ch(\ud835\udc60). The function \ud835\udc39 is typically parametrized as a neural network \ud835\udc39\ud835\udf03 with parameters \ud835\udf03, and (if using the DB objective) the policy \ud835\udc43\ud835\udc39 as a network producing logits of \ud835\udc43\ud835\udc39 (\ud835\udc60\u2032 | \ud835\udc60; \ud835\udf03) given \ud835\udc60 as input. The parameters \ud835\udf03 are optimized to minimize some discrepancy between the left and right sides of (1) or (2). A typical choice is the squared log-ratio; for example, the FM objective at a state \ud835\udc60 is\nLFM (\ud835\udc60) = ( log \ud835\udc39\ud835\udf03 (\ud835\udc60) \u2212 log \u2211\ufe01 \ud835\udc60\u2032\u2208Ch(\ud835\udc60) \ud835\udc39\ud835\udf03 (\ud835\udc60\u2032) )2 . (3)\nThe choice of states \ud835\udc60 at which this objective is evaluated and optimized is made by a training policy \ud835\udf0b. For example, \ud835\udf0b could select the states \ud835\udc60 seen in trajectories sampled from \ud835\udc43\ud835\udc39 (on-policy training), but could also use off-policy exploration techniques, such as tempering, replay buffers, or Thompson sampling (Rector-Brooks et al., 2023). Because the objective can be simultaneously minimized to zero at all \ud835\udc60 for a sufficiently expressive \ud835\udc39\ud835\udf03 , the global optimum of the objective is independent of the choice of training policy \ud835\udf0b, as long as \ud835\udf0b has full support. This capacity for off-policy training without differentiating through the sampling procedure is a key advantage of GFlowNets over onpolicy RL algorithms and over other hierarchical variational methods (Malkin et al., 2023).\nConnections with RL. In the case of tree-structured state spaces, GFlowNets are closely connected to entropy-regularized RL methods (soft Q-learning; Haarnoja et al., 2017): identifying the log-flow function with a value function, the FM/DB objectives are analogous to temporal difference learning (Sutton & Barto, 2018) and TB, along with its variant SubTB (Madan et al., 2023), to path consistency learning (Nachum et al., 2017). As a diversity-seeking agent, a GFlowNet can also be understood as way to train a quantal response agent; see \u00a7A for more discussion.\nHow restrictive is the tree structure? Any environment that has a non-tree DAG structure \u2013 i.e., where multiple trajectories may lead to the same state \u2013 can be converted to a tree-structured environment by augmenting each state with the history (the trajectory followed to reach the state). This implicitly multiplies the reward of each terminal state by the number of trajectories that lead to it (while keeping the optimal policy independent of the history). This alternative way to reward the state may be desired in some applications (e.g., zero-sum games) for which the path to the solution matters as much as the outcome."
        },
        {
            "heading": "2.2 PAST APPROACHES TO GFLOWNETS IN STOCHASTIC ENVIRONMENTS",
            "text": "Pan et al. (2023) propose a generalization of GFlowNets to stochastic environments (i.e., where the state and choice of action nondeterministically yield the subsequent state), following an approach described in Bengio et al. (2023). We now review their formulation, which we refer to as \u2018stochastic GFlowNets\u2019, restating it in a suitable language to motivate our method.\nIn stochastic environments, every state \ud835\udc60 is associated with a set of possible actions A\ud835\udc60 , and the environment provides a stochastic transition function \u2013 a distribution \ud835\udc43env (\ud835\udc60\u2032 | \ud835\udc60, \ud835\udc4e), understood as the likelihood of arriving in state \ud835\udc60\u2032 when taking action \ud835\udc4e at state \ud835\udc60. In stochastic GFlowNets, the state space S is augmented with a collection of hypothetical states (\ud835\udc60, \ud835\udc4e) for \ud835\udc60 \u2208 S \\ X and \ud835\udc4e \u2208 A\ud835\udc60 . The augmented DAG \ud835\udc3a contains two kinds of edges:\n\u2022 Edges \ud835\udc60\u2192 (\ud835\udc60, \ud835\udc4e) for each \ud835\udc60 \u2208 S and \ud835\udc4e \u2208 A\ud835\udc60 , which we call agent edges; \u2022 Edges (\ud835\udc60, \ud835\udc4e) \u2192 \ud835\udc60\u2032 for each hypothetical \ud835\udc60\u2032 in the support of \ud835\udc43env (\u00b7 | \ud835\udc60, \ud835\udc4e), \ud835\udc4e \u2208 A\ud835\udc60 , and \ud835\udc60\u2032 \u2208 Ch(\ud835\udc60),\nwhich we call environment edges.\nStochastic GFlowNets directly apply the training algorithms applicable to deterministic GFlowNets (e.g., DB) to the augmented DAG \ud835\udc3a, with the only modification being that the forward policy \ud835\udc43\ud835\udc39 is free to be learned only on agent edges, while on environment edges it is fixed to the transition function. Formally, for all \ud835\udc60 \u2208 S \\ X and \ud835\udc4e \u2208 A\ud835\udc60 , one learns \ud835\udc43\ud835\udc39 ((\ud835\udc60, \ud835\udc4e) | \ud835\udc60) (denoted \ud835\udc43\ud835\udc39 (\ud835\udc4e | \ud835\udc60) for short), while \ud835\udc43\ud835\udc39 (\ud835\udc60\u2032 | (\ud835\udc60, \ud835\udc4e)) is fixed to \ud835\udc43env (\ud835\udc60\u2032 | \ud835\udc60, \ud835\udc4e). The environment policy \ud835\udc43env, which appears in the loss, may be assumed to be known, but may also be approximated using a neural network trained by a maximum-likelihood objective on observed environment transitions, jointly with the agent policy.\nViolated desiderata in stochastic GFlowNets. By construction, if a stochastic GFlowNet satisfies the DB constraints, then the policy \ud835\udc43\ud835\udc39 samples proportionally to the reward \ud835\udc45. In this way, stochastic GFlowNets are a minimal modification of GFlowNets that can function in stochastic environments. However, there exist stochastic environments and reward functions for which no stochastic GFlowNet policy \ud835\udc43\ud835\udc39 (\ud835\udc4e | \ud835\udc60) can satisfy the constraints (Fig. 1a). Two consequences of this are the impossibility of minimizing the loss to zero for all transitions, even for a perfectly expressive policy model, and the resulting dependence of the global optimum on the choice of training policy \ud835\udf0b. Thus stochastic GFlowNets satisfy D0, but not D1 (as noted by Bengio et al. (2023)) and D2 below.\nThe generalization of GFlowNet constraints and objectives to stochastic environments that we propose satisfies the following desiderata:\nD0. If the environment\u2019s transition function \ud835\udc43env is deterministic, one should recover deterministic GFlowNet constraints and objectives. D1. Satisfiability: A perfectly expressive model should be able to minimize the generalized FM/DB losses to 0 for all states/actions in the DAG simultaneously. Consequently, the set of global optima of the loss should not depend on the choice of full-support training policy. D2. Uniqueness: If \ud835\udc3a is a tree, then the global optimum of the loss should be unique. D3. Equilibrium: In a game where two GFlowNet agents alternate actions, there should be a unique\npair of policies for the two players such that each policy is optimal for its respective loss.\nAs noted in \u00a72.1, deterministic GFlowNets satisfy D1 and D2. D0 is a common-sense property, as deterministic environments are special cases of stochastic environments. D1 (satisfiability) is essential for off-policy training, while D2 (uniqueness) is desirable in game-playing agents. The meaning of D3 will be detailed in \u00a73.2."
        },
        {
            "heading": "3 METHOD: EXPECTED AND ADVERSARIAL FLOW NETWORKS",
            "text": ""
        },
        {
            "heading": "3.1 EXPECTED FLOW NETWORKS",
            "text": "In this section, we define expected flow networks (EFlowNets) on tree-structured spaces, which encompasses the problems we study, in particular, two-player games with memory. We then show that EFlowNets satisfy the desiderata D0\u2013D2 above.\nExpected flow networks (EFlowNets) assume the following are given:\n\u2022 A tree \ud835\udc3a = (S,A), with initial state \ud835\udc600 and set of terminal states X, and a reward function \ud835\udc45 : X \u2192 R>0. \u2022 A partition of the nonterminal states into two disjoint sets, S \\ X = Sagent \u2294 Senv, called the agent states and environment states, respectively. \u2022 A distribution \ud835\udc43env (\u00b7 | \ud835\udc60) over the children of every environment state \ud835\udc60 \u2208 Senv. Observe that if Senv = \u2205, then the input data for an EFlowNet is the same as the input data for a GFlowNet on a tree-structured space. This setting also generalizes that of stochastic GFlowNets in \u00a72.2, where all transitions link agent states \ud835\udc60 to environment states (\ud835\udc60, \ud835\udc4e) \u2013 called \u2018hypothetical states\u2019 by Pan et al. (2023) \u2013 or vice versa.\nAn agent policy is a collection of distributions \ud835\udc43agent (\u00b7 | \ud835\udc60) over the children of every agent state \ud835\udc60 \u2208 Sagent. Together, \ud835\udc43agent and \ud835\udc43env determine a forward policy on \ud835\udc3a. We define the expected\ndetailed balance (EDB) constraints relating \ud835\udc43agent, \ud835\udc43env, and a state flow function \ud835\udc39 : S \u2192 R>0:\n\ud835\udc39 (\ud835\udc60)\ud835\udc43agent (\ud835\udc60\u2032 | \ud835\udc60) = \ud835\udc39 (\ud835\udc60\u2032) \u2200\ud835\udc60 \u2208 Sagent, \ud835\udc60\u2032 \u2208 Ch(\ud835\udc60), (4) \ud835\udc39 (\ud835\udc60) = E\ud835\udc60\u2032\u223c\ud835\udc43env (\ud835\udc60\u2032 |\ud835\udc60)\ud835\udc39 (\ud835\udc60\u2032) \u2200\ud835\udc60 \u2208 Senv, (5) \ud835\udc39 (\ud835\udc65) = \ud835\udc45(\ud835\udc65) \u2200\ud835\udc65 \u2208 X. (6)\nThese constraints satisfy the desiderata D0\u2013D2, as summarized in the following proposition.\nProposition 1. There exists a unique pair of state flow function \ud835\udc39 and agent policy \ud835\udc43agent satisfying constraints (4), (5), and (6). If Senv = \u2205, then this pair satisfies the detailed balance constraints (2).\nEFlowNets marginalize over the uncertainty of the environment\u2019s transitions: they aim to sample each action in proportion to the expected total reward available if the action is taken (see Prop. 5). A connection between EFlowNets and Luce quantal response agents is made in \u00a7A.\nTraining EFlowNets: From constraints to losses. Just as in deterministic environments, when training EFlowNets, we parametrize the state flow and agent policy as neural networks \ud835\udc39\ud835\udf03 and \ud835\udc43agent (\u00b7 | \u00b7; \ud835\udf03). The EDB constraints can be turned into squared log-ratio losses in the same manner that the FM constraint (1) is converted into the loss (3) and optimized by gradient descent.\nIn problems where the number of environment transitions is large and computing the expectation on the right side of (5) is costly, it may be replaced by an alternative constraint by introducing a distribution \ud835\udc44(\ud835\udc60\u2032 | \ud835\udc60) = \ud835\udc39 (\ud835\udc60\n\u2032 )\ud835\udc43env (\ud835\udc60\u2032 |\ud835\udc60) \ud835\udc39 (\ud835\udc60) . This quantity sums to 1 over the \ud835\udc60 \u2032 if and only if (5) is satisfied. Thus (5) is equivalent to the following constraint on \ud835\udc44:\n\ud835\udc39 (\ud835\udc60)\ud835\udc44(\ud835\udc60\u2032 | \ud835\udc60) = \ud835\udc39 (\ud835\udc60\u2032)\ud835\udc43env (\ud835\udc60\u2032 | \ud835\udc60). (7)\nEnforcing this constraint requires learning an additional distribution\ud835\udc44(\ud835\udc60\u2032 | \ud835\udc60; \ud835\udf03), but does not require summation over children. The conversion from (5) to (7) resembles that from FM (1) to DB (2).\nJust like deterministic GFlowNets, the globally optimal agent policy in an EFlowNet is unique and does not depend on the distribution of states at which the objectives are optimized, as long as it has full support. However, the choice of training policy can be an important hyperparameter that can affect the rate of convergence and the local minimum reached in a function approximation setting. We describe the choices we make in the experiment sections below.\nJust like in stochastic GFlowNets (\u00a72.2), the environment policy \ud835\udc43env can be either assumed to be known or learned, jointly with the policy, from observations of the environment\u2019s transitions."
        },
        {
            "heading": "3.2 ADVERSARIAL FLOW NETWORKS",
            "text": "We now consider the application of EFlowNets to multiagent settings. Although our experiments are in the domain of two-player games, we define adversarial flow networks (AFlowNets) in their full generality, with \ud835\udc5b agents. AFlowNets with \ud835\udc5b agents, or players, depend on the following information:\n\u2022 A tree \ud835\udc3a = (S,A), with initial state \ud835\udc600 and set of terminal states X, and a collection of reward functions \ud835\udc451, . . . , \ud835\udc45\ud835\udc5b : X \u2192 R>0. \u2022 A partition of the nonterminal states into disjoint sets, S \\ X = S1 \u2294 \u00b7 \u00b7 \u00b7 \u2294 S\ud835\udc5b. This data defines a fully observed sequential game, where \ud835\udc60 \u2208 S\ud835\udc56 means that player \ud835\udc56 is to play at \ud835\udc60. An agent policy for player \ud835\udc56 is a collection of distributions \ud835\udc43\ud835\udc56 (\u00b7 | \ud835\udc60) over Ch(\ud835\udc60) for every \ud835\udc60 \u2208 S\ud835\udc56 . The input data for an AFlowNet also defines a collection of EFlowNets, one for each player \ud835\udc56. The EFlowNet E\ud835\udc56 for player \ud835\udc56 has the same underlying graph \ud835\udc3a, with Sagent = S\ud835\udc56 and Senv = \u2294 \ud835\udc57\u2260\ud835\udc56 S \ud835\udc57 = S \\ (X \u222a S\ud835\udc56), and reward function \ud835\udc45\ud835\udc56 . That is, each player is viewed as an agent in an EFlowNet whose \u2018environment\u2019 is given by the other players\u2019 policies. (We also remark that the case \ud835\udc5b = 1 recovers a regular (deterministic) GFlowNet.)\nThe policy \ud835\udc43\ud835\udc56 of player \ud835\udc56 can be optimized using the EFlowNet training objective given fixed values of the other players\u2019 policies \ud835\udc43 \ud835\udc57 ( \ud835\udc57 \u2260 \ud835\udc56), and by Proposition 1, there is a unique global optimum for \ud835\udc43\ud835\udc56 . However, remarkably, there exists a unique collection of policies \ud835\udc431, . . . , \ud835\udc43\ud835\udc5b such that each \ud835\udc43\ud835\udc56 that jointly satisfy the EFlowNet constraints for each player.\nProposition 2. There exist unique agent policies \ud835\udc431, . . . , \ud835\udc43\ud835\udc5b and state flow functions \ud835\udc391, . . . , \ud835\udc39\ud835\udc5b : S \u2192 R>0 such that \ud835\udc43\ud835\udc56 and \ud835\udc39\ud835\udc56 satisfy the EDB constraints with respect to the EFlowNet E\ud835\udc56 for all \ud835\udc56.\nWe also have a characterization of the joint optimum in the case of two-agent AFlowNets: Proposition 3. Suppose that in a 2-player AFlowNet, the agent policies \ud835\udc431, \ud835\udc432 and state flow functions \ud835\udc391, \ud835\udc392 are jointly optimal in the sense of Prop. 2. Then the function \ud835\udc39 (\ud835\udc60) = \ud835\udc391 (\ud835\udc60)\ud835\udc392 (\ud835\udc60) is a flow on \ud835\udc3a, i.e., satisfies the FM constraint (1), with respect to the reward \ud835\udc45(\ud835\udc65) = \ud835\udc451 (\ud835\udc65)\ud835\udc452 (\ud835\udc65)."
        },
        {
            "heading": "3.3 BRANCH-ADJUSTED AFLOWNETS FOR TWO-PLAYER ZERO-SUM GAMES.",
            "text": "An important application of AFlowNets is two-player zero-sum games. We will now describe a way to turn the game outcomes into rewards that allows a simpler and more efficient training objective.\nSpecifically, we consider a two-player, complete-information game with tree-shaped state space \ud835\udc3a, in which player 1 moves first (i.e., \ud835\udc600 \u2208 S1) and the players alternate moves, i.e., every complete trajectory \ud835\udc600 \u2192 \ud835\udc601 \u2192 \u00b7 \u00b7 \u00b7 \u2192 \ud835\udc60\ud835\udc5b = \ud835\udc65 has \ud835\udc60\ud835\udc56 \u2208 S1 if and only if \ud835\udc56 is even. We assume that the game ends in a win for player 1, a win for player 2, or a draw. A na\u0131\u0308ve way to define the rewards for players 1 and 2 is the following, which ensures the log-rewards sum to zero at every terminal state:\n\ud835\udc45\u25e6\ud835\udc56 (\ud835\udc65) =  \ud835\udc52\ud835\udf06 if player \ud835\udc56 wins, 1 if the game ends in a draw, \ud835\udc52\u2212\ud835\udf06 if player \ud835\udc56 loses.\n(8)\nHowever, we find that AFlowNets trained with this reward often exhibit suboptimal behaviour in complex games: the agent may avoid a move that leads directly to a winning terminal state (high reward) in favour of a move with a large downstream subtree. We therefore define an alternative reward function that favours shorter winning trajectories. If \ud835\udc600 \u2192 \ud835\udc601 \u2192 \u00b7 \u00b7 \u00b7 \u2192 \ud835\udc60\ud835\udc5b = \ud835\udc65 is the trajectory leading to \ud835\udc65, then the branch-adjusted reward for player \ud835\udc56 is defined as\n\ud835\udc45\ud835\udc56 (\ud835\udc65) = \ud835\udc45\u25e6 \ud835\udc56 (\ud835\udc65)\n\ud835\udc35\ud835\udc56 (\ud835\udc65) , \ud835\udc35\ud835\udc56 (\ud835\udc65) := \u220f \ud835\udc58:\ud835\udc60\ud835\udc58 \u2208S\ud835\udc58 | Ch(\ud835\udc60\ud835\udc58) |. (9)\nThat is, \ud835\udc35\ud835\udc56 (\ud835\udc65) is the product of the branching factors (numbers of children) of the states \ud835\udc60\ud835\udc58 on the trajectory at which player \ud835\udc56 is to move.1 Besides delivering a higher reward for less selective trajectories and being empirically essential for good game-playing performance, this correction is necessary to derive the simplified objective below, which critically uses \ud835\udc45\u25e61 (\ud835\udc65)\ud835\udc45 \u25e6 2 (\ud835\udc65) = 1.\nA \u2018trajectory balance\u2019 for branch-adjusted AFlowNets. A limitation of objectives such as FM, DB, and their EFlowNet and AFlowNet generalizations are their slow credit assignment (propagation of a reward signal) over long trajectories, which results from these losses being local to a state or transition. This limitation motivated the trajectory balance (TB) loss for GFlowNets (Malkin et al., 2022), which delivers a gradient update to all policy probabilities along a complete trajectory.\nWhile the GFlowNet TB objective does not appear to generalize to expected flow networks, we derive an objective of a TB-like flavour for branch-adjusted AFlowNets. Proposition 4. In a 2-player AFlowNet with alternating moves satisfying \ud835\udc45\u25e61 (\ud835\udc65)\ud835\udc45 \u25e6 2 (\ud835\udc65) = 1:\n(a) Suppose that the agent policies \ud835\udc431, \ud835\udc432 and state flow functions \ud835\udc391, \ud835\udc392 are jointly optimal in the sense of Prop. 2. Then there exists a scalar \ud835\udc4d , independent of \ud835\udc65, such that for every complete trajectory \ud835\udc600 \u2192 \ud835\udc601 \u2192 \u00b7 \u00b7 \u00b7 \u2192 \ud835\udc60\ud835\udc5b = \ud835\udc65,\n\ud835\udc4d \u220f \ud835\udc56:\ud835\udc60\ud835\udc56\u2208S1 \ud835\udc431 (\ud835\udc60\ud835\udc56+1 | \ud835\udc60\ud835\udc56) = \ud835\udc451 (\ud835\udc65)\ud835\udc352 (\ud835\udc65) \u220f \ud835\udc56:\ud835\udc60\ud835\udc56\u2208S2 \ud835\udc432 (\ud835\udc60\ud835\udc56+1 | \ud835\udc60\ud835\udc56). (10)\n(b) Conversely, if the constraint (10) holds for some constant \ud835\udc4d and policies \ud835\udc431 and \ud835\udc432, then \ud835\udc431 and \ud835\udc432 are the jointly optimal AFlowNet policies.\nThe constraint (10) can be converted into a training objective LTB \u2013 the squared log-ratio between the left and right sides \u2013 and optimized with respect to the policy parameters and the scalar \ud835\udc4d (parametrized through log \ud835\udc4d for stability) for complete trajectories (game simulations) sampled from a training policy. Prop. 2 and Prop. 4(a) guarantee that the constraints are satisfiable, while Prop. 4(b) guarantees that the policies satisfying the constraints are unique.\n1While the branching factor appears large, it is counteracted by the fact that, on an AFlowNet agent\u2019s turn, its child flows are summed, e.g., \ud835\udc391 (\ud835\udc60) = \u2211 \ud835\udc60\u2032\u2208Ch(\ud835\udc60) \ud835\udc391 (\ud835\udc60\u2032) if \ud835\udc60 \u2208 S1.\nTraining AFlowNets. AFlowNets are trained by optimizing the EFlowNet objectives of each agent independently. The states at which the objectives are optimized are chosen by a training policy, which may either sample the agents\u2019 policies to produce training trajectories (on-policy selfplay) or use off-policy exploration. The joint global optimum, where all agents optimize their losses to 0, is unique and independent of the training policy due to Prop. 2. See Alg. 1.\nA significant benefit of AFlowNets over methods like Silver et al. (2018) is that they do not require expensive rollout procedures (i.e., MCTS) to generate games. MCTS performs a number of simulations \u2013 each of which requires a forward pass \u2013 for every state in a game. AFlowNets, on the other hand, only require a single forward pass per state. Consequently, AFlowNets can be trained on more games given a similar computational budget."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": "We conduct experiments to investigate whether EFlowNets can effectively learn in stochastic environments compared to related methods (\u00a74.1) and whether AFlowNets are effective learners of adversarial gameplay, as measured by their performance against contemporary approaches (\u00a74.2).\n4.1 GENERATIVE MODELING IN STOCHASTIC ENVIRONMENTS\nWe evaluate EFlowNets in a protein design task from Jain et al. (2022). The GFlowNet policy autoregressively generates an 8-symbol DNA sequence \ud835\udc65 and receives a reward of \ud835\udc45(\ud835\udc65) = \ud835\udc53 (\ud835\udc65)\ud835\udefd , where \ud835\udc53 (\ud835\udc65) is a proxy model estimating binding affinity to a target protein and \ud835\udefd is a hyperparameter controlling the reward distribution\u2019s entropy. In Pan et al. (2023), the problem was made stochastic by letting the environment replace the symbol appended by the policy to the right of a partial sequence with a uniformly random symbol with probability \ud835\udefc. Thus \ud835\udefc = 0 gives a deterministic environment and \ud835\udefc = 1 a fully stochastic environment, where the policy\u2019s actions have no effect.\nWe extend the published code of Pan et al. (2023) with an implementation of the EFlowNet objective. Besides the stochastic GFlowNet (Stoch-GFN) formulation from \u00a72.2, we compare with the two strongest\nbaselines considered in that work: a \u201cna\u0131\u0308ve\u201d GFlowNet that ignores the environment\u2019s transitions (GFN), and a discrete soft actor-critic (SAC; Haarnoja et al., 2018). We use the hyperparameters from the existing implementation for all methods (except SAC, which we reimplemented because code was not available) and report the same primary metrics: the mean reward of the top-100 sequences among 2048 sampled from a trained model and the number of diverse modes found, as measured by the sphere exclusion algorithm from Jain et al. (2022). A model of the environment\u2019s transition distribution is learned, consistent with Pan et al. (2023).\nThe results and error ranges, with different values of the stochasticity \ud835\udefc and reward exponent \ud835\udefd, are shown in Fig. 2. When the reward is peaky (larger \ud835\udefd), EFlowNets outperform other algorithms in both diversity and top-100 reward. This is consistent with situations such as those in Fig. 1a, where the Stoch-GFN constraints are unsatisfiable, being more common when the reward is peaky, as the environment\u2019s random actions place smoothness constraints on the sampling distribution. Of note, our implementation of SAC performs better than what is reported in Pan et al. (2023) and often better than Stoch-GFN, which was previously considered only with the flat-reward setting of \ud835\udefd = 3."
        },
        {
            "heading": "4.2 ADVERSARIAL GAMES",
            "text": "The game-playing capabilities of AFlowNets are evaluated in 2-player games. Rewards are modeled as described in \u00a73.3, and the AFlowNets trained with rewards defined by (8) with a given \ud835\udf06 are denoted AFlowNet\ud835\udf06. We evaluate how the efficacy of an agent changes with various values of \ud835\udf06.\nAlgorithm 1: Branch-adjusted AFlowNet Training Data: \ud835\udf06, batch size \ud835\udc5b, number of trajectories \ud835\udc3e , number of steps \ud835\udc3f, buffer capacity \ud835\udc40 ,\nmodel and training hyperparameters Initialize AFlowNet policies \ud835\udc431, \ud835\udc432, log \ud835\udc4d , and replay buffer \ud835\udc35 with capacity \ud835\udc40 for \ud835\udc56 = 1 to \ud835\udc41 do\nGenerate \ud835\udc3e trajectories (sampling from AFlowNet\u2019s policies \ud835\udc431, \ud835\udc432) Add trajectories to \ud835\udc35 // \ud835\udc35 is a FIFO queue for \ud835\udc56 = 1 to \ud835\udc3f do {\ud835\udf0f\ud835\udc57 }\ud835\udc5b\ud835\udc57=1 \u2190 Sample randomly from \ud835\udc35 L \u2190 1\n\ud835\udc5b \u2211\ud835\udc5b \ud835\udc57=1 LTB (log \ud835\udc4d, \ud835\udc431, \ud835\udc432, \ud835\udf0f\ud835\udc57 , \ud835\udf06)\nGradient update on \u2207L with respect to log \ud835\udc4d and policy parameters\n0 10 20 30 40 Steps (\u00d7103)\n100\n0\n100\n200\n300\n400\nEl o\nsc or\ne\nTic-tac-toe\n0 10 20 30 40 Steps (\u00d7103)\n0\n500\n1000\n1500\n2000 Connect-4\n0.0 0.5 1.0 1.5 2.0 Time (seconds \u00d7103)\n100\n0\n100\n200\n300\n400\nEl o\nsc or\ne\n0 10 20 30 40 Time (seconds \u00d7103)\n0\n500\n1000\n1500\n2000\nAFlowNet2 AFlowNet10\nAFlowNet15 AlphaZero\nAlphaZero+MCTS Tree Search (d=3)\nFigure 3: Elo as a function of training steps and training time. As a convention, random uniform baseline agents represent an Elo of 0. AFlowNets achieve similar Elo to AlphaZero in tic-tac-toe and AFlowNets quickly learn to outperform AlphaZero in Connect-4.\n0 10 20 30 40 Uniform Time (seconds) \u00d7103\n0\n20\n40\n60\n80\n100\n% o\nf m ov\nes\nOptimal Inaccuracy Blunder\nFigure 4: Move quality for the AFlowNet (for a set of 10240 randomly generated Connect-4 boards) over the course of training. An optimal move leads to the quickest win or slowest loss. An inaccuracy is a non-optimal move that has the same sign as the optimal move (e.g., leading to a win but not as quickly). A blunder leads from a winning state to either a drawing or losing state.\nAFlowNets are trained using the TB loss to play tic-tac-toe and Connect-4. For each, we run a tournament against an open-source AlphaZero implementation (Thakoor et al., 2016) and a uniform random agent. For tic-tac-toe we also include a tree-search agent which uses AlphaZero\u2019s value function, alpha-beta pruning, and a search depth of 3. To compare the agents, we compute their Elo over the course of training using BayesElo (Coulom, 2008; Diaz & Bu\u0308ck-Kaeffer, 2023). The training procedure is outlined in Alg. 1 and details are in \u00a7D.\nFigure 3 (left) illustrates the Elo of the agents in tic-tac-toe over training steps and time. It is clear that AFlowNets quickly achieve a competitive Elo with AlphaZero. It is worth noting that AFlowNet2 and AFlowNet15 achieved and Elo of 334.8 \u00b1 15.5 and 231.1 \u00b1 91.3, whereas AFlowNet10 achieved an Elo of 338.4 \u00b1 14.1. As such, it appears that \ud835\udf06 has a diminishing return on game performance in tic-tac-toe.\nThe parameter \ud835\udf06 has a large effect on the performance of AFlowNets in Connect-4 (cf. Fig. 3 (right)). The AFlowNets with \ud835\udf06 \u2208 {10, 15} achieve the highest Elo of all tested agents. AFlowNets win almost every game against AlphaZero and achieve an Elo score roughly 800 points higher. Additional tournament results and further analysis are available in \u00a7E.\nAs in Prasad et al. (2018), we take advantage of the fact that Connect-4 is solved (Allis, 1988) to obtain perfect values for arbitrary positions. We compare the moves selected by the AFlowNet with the values computed by a perfect Connect-4 solver (Pons (2019)) over the course of training. Fig. 4 shows an evaluation of the AFlowNet\u2019s performance using this metric (and baseline values for a random uniform agent). The AFlowNets learn to play optimal moves in > 80% of board states after 3 hours of training on one RTX8000 GPU.\nDifferences between AFlowNets and AlphaZero methodologies. Distinctions exist between our approach and AlphaZero that make direct comparisons between the methods challenging, summarized in Table 1. Most importantly, the batch-adjusted AFlowNet objective depends on an entire game simulation, while the AFlowNet value function updates are performed at individual states. In addition, the game simulations in AFlowNets are obtained using a single policy rollout, without Monte Carlo tree search. Thus, generation of training data with AFlowNets is faster than with AlphaZero, assuming the base model architectures are of a similar scale.\nDemo. We invite the reader to play Connect-4 anonymously against a pretrained AFlowNet agent at the following URL: https://bit.ly/demoafn."
        },
        {
            "heading": "5 RELATED WORK",
            "text": "Stochasticity in GFlowNets. GFlowNets have been used as diversity-seeking samplers in various settings with deterministic environment transitions. In particular, they have been interpreted as hierarchical variational inference algorithms (Malkin et al., 2023; Zimmermann et al., 2023) and correspondingly applied to modeling of Bayesian posteriors (Deleu et al., 2022; van Krieken et al., 2022; Hu et al., 2023). GFlowNets can be trained with stochastic rewards (Bengio et al., 2023), and Deleu et al. (2022; 2023); Liu et al. (2022) take advantage of this property to train samplers of Bayesian posteriors using small batches of observed data. Zhang et al. (2023b) proposed to match the uncertainty in a stochastic reward in a manner resembling distributional RL (Bellemare et al., 2017); however, the stochasticity is introduced only at terminal states, while we consider stochasticity in intermediate transitions. The stochastic modelling of Pan et al. (2023), as we have argued, is insufficient to capture desired sampling behaviours in the problems we consider.\nRL in stochastic environments and games. Learning in an environment with stochastic transition dynamics and against adversaries has long been a task of RL (Sutton & Barto, 2018). While AlphaZero (Silver et al., 2018) has achieved state-of-the-art performance in chess, Shogi, and Go, it does not explicitly model stochastic transition dynamics. Stochastic MuZero (Antonoglou et al., 2022) is a model-based stochastic planning method that learns a model of the environment and a policy at the same time, allowing it to perform well in a variety of stochastic games. Both AlphaZero and MuZero use Monte Carlo tree search for policy and value estimation (Silver et al., 2018; Antonoglou et al., 2022; Schrittwieser et al., 2020). Our EFlowNets bear similarities to a recently introduced approach that integrates over environment uncertainty in RL (Yang et al., 2023)."
        },
        {
            "heading": "6 DISCUSSION",
            "text": "This paper extends GFlowNets to stochastic and adversarial environments. Expected flow networks learn in settings with stochastic transitions while maintaining desirable convergence properties, and adversarial flow networks pit EFlowNets against themselves in self-play. We successfully applied these algorithms to a stochastic generative modeling problem and to two real-world zero-sum games.\nIn future work, we intend to scale these methods to larger game spaces (e.g., chess and Go). Such scaling is likely to require algorithmic improvements to address the limitations of our method. While we derived an efficient \u2018trajectory balance\u2019 for branch-adjusted AFlowNets, trajectory-level objectives suffer from high variance for long trajectories and have a high memory cost. Although these limitations did not surface in our experiments, it would be interesting to consider interpolations between expected DB and TB, akin to subtrajectory balance for GFlowNets (Madan et al., 2023).\nOther future work can consider generalizations to incomplete-information games and cooperative multi-agent settings. For games with continuous action spaces, one can analyze the continuous-time and infinite-agent (mean-field) limits. Beyond games, GFlowNets and EFlowNets, in which node flows are computed by aggregation over children \u2013 either summation (4) or expectation (5) \u2013 may fall into a more general class of probabilistic flow networks that encompass a range of samplers trainable by local consistency objectives, reminiscent of the manner in which the language of circuits unifies probabilistic models with tractable inference (Choi et al., 2020; Vergari et al., 2021)."
        },
        {
            "heading": "A GFLOWNETS AS QUANTAL RESPONSE AGENTS",
            "text": "A GFlowNet policy as a Luce agent. GFlowNets in tree-structured spaces are closely related to probabilistic models of imperfect agent behaviour in game theory known as quantal response agents (McKelvey & Palfrey, 1996; 1995; Goeree et al., 2020). A particular kind of quantal response agent uses the Luce ratio rule (Luce, 1959) to sample strategies in proportion to their expected payoff. GFlowNet trajectories \ud835\udf0f can be seen as (pure) strategies of an agent in a one-player game, and the reward \ud835\udc45(\ud835\udc65) as the payoff for a trajectory \ud835\udf0f leading to \ud835\udc65 \u2208 X. A GFlowNet that samples trajectories proportionally to the rewards of their last states, i.e., \ud835\udc43\ud835\udc39 (\ud835\udf0f = (\ud835\udc600 \u2192 \ud835\udc601 \u2192 \u00b7 \u00b7 \u00b7 \u2192 \ud835\udc60\ud835\udc5b = \ud835\udc65)) \u221d \ud835\udc45(\ud835\udc65), is thus a Luce quantal response agent. On the level of individual actions at a given state \ud835\udc60, the policy is that of a Luce agent that treats \ud835\udc39 (\ud835\udc60\u2032) \u2013 the total reward accessible from \ud835\udc60\u2032 \u2013 as the payoff for a transition \ud835\udc60\u2192 \ud835\udc60\u2032. EFlowNets learn a marginalized quantal response policy. Next, we show that EFlowNets are Bayesian (model-averaging) analogues of Luce agents that marginalize out the uncertainty of the environment\u2019s transitions.\nDefine a (pure) environment strategy as an induced subgraph \ud835\udc3aenv of \ud835\udc3a whose vertex set \ud835\udc49 (\ud835\udc3aenv) \u2286 S has the following properties: \u2022 If \ud835\udc60 \u2208 \ud835\udc49 (\ud835\udc3aenv) and \ud835\udc60 \u2260 \ud835\udc600, then the parent of \ud835\udc60 is in \ud835\udc49 (\ud835\udc3aenv). \u2022 If \ud835\udc60 \u2208 \ud835\udc49 (\ud835\udc3aenv) \u2229 Senv, then exactly one child of \ud835\udc60 is in \ud835\udc49 (\ud835\udc3aenv). \u2022 If \ud835\udc60 \u2208 \ud835\udc49 (\ud835\udc3aenv) \u2229 Sagent, then all children of \ud835\udc60 are in \ud835\udc49 (\ud835\udc3aenv). It is clear from the first property that any such \ud835\udc3aenv is a tree. An environment strategy thus amounts to a predetermined choice of action at every state that can be reached if the environment takes the actions chosen by the strategy. The environment policy \ud835\udc43env determines a distribution over environment strategies, where the child of each agent state \ud835\udc60 in \ud835\udc3aagent is sampled from the policy independently for each \ud835\udc60, i.e.,\n\ud835\udc43env (\ud835\udc3aenv) = \u220f\n\ud835\udc60\u2208\ud835\udc49 (\ud835\udc3aenv )\u2229Senv \ud835\udc60\u2032\u2208Ch(\ud835\udc60)\u2229\ud835\udc49 (\ud835\udc3aenv )\n\ud835\udc43env (\ud835\udc60\u2032 | \ud835\udc60). (11)\nThe environment strategy is a source of uncertainty for the agent. Any given \ud835\udc3aenv is a tree that contains \ud835\udc600 and some subset X\ud835\udc3aenv of the terminal states. Viewing \ud835\udc3aenv as a (deterministic) GFlowNet\nin the sense of \u00a72.1, there is a unique policy \ud835\udc43\ud835\udc3aenv \ud835\udc39 , and corresponding state flow \ud835\udc39\ud835\udc3aenv on \ud835\udc3aenv that samples proportionally to the reward \ud835\udc45 restricted to X\ud835\udc3aenv . The following proposition shows that the optimal EFlowNet policy averages the stepwise utilities (i.e., total accessible rewards) of deterministic-environment GFlowNets \ud835\udc43\ud835\udc3aenv\n\ud835\udc39 weighted by their\nlikelihood under \ud835\udc43env.\nProposition 5. Suppose that \ud835\udc43agent satisfies the EDB constraints. Then, for any \ud835\udc60 \u2208 Sagent and \ud835\udc60\u2032 \u2208 Ch(\ud835\udc60), \ud835\udc43agent (\ud835\udc60\u2032 | \ud835\udc60) \u221d E\ud835\udc3aenv\u223c\ud835\udc43env [ \ud835\udc39\ud835\udc3aenv (\ud835\udc60\u2032) | \ud835\udc60 \u2208 \ud835\udc49 (\ud835\udc3aenv) ] ,\nwhere the expectation is taken over the distribution over strategies determined by \ud835\udc43env, restricted to the strategies that contain the state \ud835\udc60.\nAFlowNets and agent quantal response equilibrium. In two-player games with unadjusted rewards \ud835\udc45\u25e61, \ud835\udc45 \u25e6 2 and rewards \ud835\udc451 and \ud835\udc452 defined using the branching factor adjustment (9), we also have the following characterization of the optimal state flows:\n\ud835\udc35\ud835\udc56 (\ud835\udc60)\ud835\udc39\ud835\udc56 (\ud835\udc60) = E \ud835\udc60=\ud835\udc601\u2192\ud835\udc602\u2192...\ud835\udc60\ud835\udc5b=\ud835\udc65\u2208X \ud835\udc60\ud835\udc58+1\u223cU[Ch(\ud835\udc60\ud835\udc58 ) ] if \ud835\udc60\ud835\udc58 \u2208 S\ud835\udc56\n\ud835\udc60\ud835\udc58+1\u223c\ud835\udc43\ud835\udc57 (\ud835\udc60\ud835\udc58+1 |\ud835\udc60\ud835\udc58 ) if \ud835\udc60\ud835\udc58 \u2208 S \ud835\udc57 ( \ud835\udc57 \u2260 \ud835\udc56)\n[ \ud835\udc45\u25e6\ud835\udc56 (\ud835\udc65) ] (12)\n= \u2211\ufe01\n\ud835\udc60=\ud835\udc601\u2192\ud835\udc602\u2192...\ud835\udc60\ud835\udc5b=\ud835\udc65\u2208X  \u220f \ud835\udc58:\ud835\udc60\ud835\udc58 \u2208S\ud835\udc56 1 | Ch(\ud835\udc60\ud835\udc58) | \u220f \ud835\udc58:\ud835\udc60\ud835\udc58 \u2208S \ud835\udc57 , \ud835\udc57\u2260\ud835\udc56 \ud835\udc43 \ud835\udc57 (\ud835\udc60\ud835\udc58+1 | \ud835\udc60\ud835\udc58)  \ud835\udc45\u25e6\ud835\udc56 (\ud835\udc65),\nwhere the notation \ud835\udc35\ud835\udc56 (\ud835\udc60) is extended to nonterminal states \ud835\udc60 using the same definition (9). This is easily derived by recursion from the EDB constraints and (9) in a similar way to the proof of Prop. 5. Because \ud835\udc43\ud835\udc56 (\ud835\udc60\u2032 | \ud835\udc60) \u221d \ud835\udc39\ud835\udc56 (\ud835\udc60\u2032) for \ud835\udc60 \u2208 S\ud835\udc56 , the expression (12) characterizes the policy \ud835\udc43\ud835\udc56 via a form of agent quantal response (McKelvey & Palfrey, 1998), in which the action probability of an agent is proportional to its expected reward under future actions of the opponent (sampled from its policy) and the agent itself (here, sampled uniformly)."
        },
        {
            "heading": "B PROOFS",
            "text": "Proposition 1. There exists a unique pair of state flow function \ud835\udc39 and agent policy \ud835\udc43agent satisfying constraints (4), (5), and (6). If Senv = \u2205, then this pair satisfies the detailed balance constraints (2).\nProof of Proposition 1. If the EDB constraints are satisfied, then \ud835\udc39 satisfies a recurrence: \ud835\udc39 (\ud835\udc60) =  \u2211 \ud835\udc60\u2032\u2208Ch(\ud835\udc60) \ud835\udc39 (\ud835\udc60\u2032) \ud835\udc60 \u2208 Sagent E\ud835\udc60\u2032\u223c\ud835\udc43env (\ud835\udc60\u2032 |\ud835\udc60)\ud835\udc39 (\ud835\udc60\u2032) \ud835\udc60 \u2208 Senv \ud835\udc45(\ud835\udc60) \ud835\udc60 \u2208 X , (13)\nwhere the first case (\ud835\udc60 \u2208 Sagent) follows from summing (4) over \ud835\udc60\u2032. The uniqueness of \ud835\udc39 (\ud835\udc60) can easily be seen, e.g., by induction on the length of the longest path from \ud835\udc60 to a terminal state.\nBecause \ud835\udc45(\ud835\udc65) > 0 for all \ud835\udc65 \u2208 X, and the recurrence preserves the positivity (i.e., \ud835\udc39 (\ud835\udc60\u2032) > 0 for all \ud835\udc60\u2032 \u2208 Ch(\ud835\udc60) implies \ud835\udc39 (\ud835\udc60) > 0), we have \ud835\udc39 (\ud835\udc60) > 0 for all \ud835\udc60. Therefore, one can recover the unique \ud835\udc43agent that satisfies (4) jointly with \ud835\udc39 via \ud835\udc43agent (\ud835\udc60\u2032 | \ud835\udc60) = \ud835\udc39 (\ud835\udc60 \u2032 ) \ud835\udc39 (\ud835\udc60) .\nFinally, if Senv = \u2205, then constraint (5) is vacuous, and the remaining constraints exactly recover (2). \u25a1\nProposition 2. There exist unique agent policies \ud835\udc431, . . . , \ud835\udc43\ud835\udc5b and state flow functions \ud835\udc391, . . . , \ud835\udc39\ud835\udc5b : S \u2192 R>0 such that \ud835\udc43\ud835\udc56 and \ud835\udc39\ud835\udc56 satisfy the EDB constraints with respect to the EFlowNet E\ud835\udc56 for all \ud835\udc56.\nProof of Proposition 2. As in the proof of Proposition 1, we give a recurrence on the flows:\n\ud835\udc39\ud835\udc56 (\ud835\udc60) =  \u2211 \ud835\udc60\u2032\u2208Ch(\ud835\udc60) \ud835\udc39\ud835\udc56 (\ud835\udc60\u2032) \ud835\udc60 \u2208 S\ud835\udc56\u2211 \ud835\udc60\u2032 \u2208Ch(\ud835\udc60) \ud835\udc39\ud835\udc56 (\ud835\udc60\u2032 )\ud835\udc39\ud835\udc57 (\ud835\udc60\u2032 )\u2211 \ud835\udc60\u2032 \u2208Ch(\ud835\udc60) \ud835\udc39\ud835\udc57 (\ud835\udc60\u2032 ) \ud835\udc60 \u2208 S \ud835\udc57 , \ud835\udc57 \u2260 \ud835\udc56 \ud835\udc45\ud835\udc56 (\ud835\udc60) \ud835\udc60 \u2208 X . (14)\nThis recurrence uniquely determines the state flows \ud835\udc39\ud835\udc56 and therefore the policies \ud835\udc43\ud835\udc56 . It remains to see that if the flows satisfy (14), then each \ud835\udc39\ud835\udc56 satisfies the recurrence (13). The cases \ud835\udc60 \u2208 S\ud835\udc56 and \ud835\udc60 \u2208 X are clear. For the case \ud835\udc60 \u2208 S \ud835\udc57 , \ud835\udc57 \u2260 \ud835\udc56, observe that\u2211 \ud835\udc60\u2032\u2208Ch(\ud835\udc60) \ud835\udc39\ud835\udc56 (\ud835\udc60\u2032)\ud835\udc39\ud835\udc57 (\ud835\udc60\u2032)\u2211\n\ud835\udc60\u2032\u2208Ch(\ud835\udc60) \ud835\udc39\ud835\udc57 (\ud835\udc60\u2032) = \u2211\ufe01 \ud835\udc60\u2032\u2208Ch(\ud835\udc60) \ud835\udc39\ud835\udc56 (\ud835\udc60\u2032)\ud835\udc39\ud835\udc57 (\ud835\udc60\u2032)\u2211 \ud835\udc60\u2032\u2032\u2208Ch(\ud835\udc60) \ud835\udc39\ud835\udc57 (\ud835\udc60\u2032\u2032) = \u2211\ufe01 \ud835\udc60\u2032\u2208Ch(\ud835\udc60) \ud835\udc39\ud835\udc56 (\ud835\udc60\u2032)\ud835\udc43 \ud835\udc57 (\ud835\udc60\u2032 | \ud835\udc60) = E\ud835\udc60\u2032\u223c\ud835\udc43\ud835\udc57 (\ud835\udc60\u2032 |\ud835\udc60)\ud835\udc39\ud835\udc56 (\ud835\udc60\u2032),\nwhich coincides with the second case of the recurrence (13). \u25a1\nProposition 3. Suppose that in a 2-player AFlowNet, the agent policies \ud835\udc431, \ud835\udc432 and state flow functions \ud835\udc391, \ud835\udc392 are jointly optimal in the sense of Prop. 2. Then the function \ud835\udc39 (\ud835\udc60) = \ud835\udc391 (\ud835\udc60)\ud835\udc392 (\ud835\udc60) is a flow on \ud835\udc3a, i.e., satisfies the FM constraint (1), with respect to the reward \ud835\udc45(\ud835\udc65) = \ud835\udc451 (\ud835\udc65)\ud835\udc452 (\ud835\udc65).\nProof of Proposition 3. Because \ud835\udc391 (\ud835\udc65) = \ud835\udc451 (\ud835\udc65) and \ud835\udc392 (\ud835\udc65) = \ud835\udc452 (\ud835\udc65) for all \ud835\udc65 \u2208 X, we have \ud835\udc39 (\ud835\udc65) = \ud835\udc45(\ud835\udc65) for all \ud835\udc65 \u2208 X. For \ud835\udc60 \u2208 S \\ X, we must show that \ud835\udc39 (\ud835\udc60) = \u2211\ud835\udc60\u2032\u2208Ch(\ud835\udc60) \ud835\udc39 (\ud835\udc60\u2032). Suppose without loss of generality that \ud835\udc60 \u2208 S1. Then\n\ud835\udc391 (\ud835\udc60)\ud835\udc392 (\ud835\udc60) = \ud835\udc391 (\ud835\udc60)E\ud835\udc60\u2032\u223c\ud835\udc431 (\ud835\udc60\u2032 |\ud835\udc60)\ud835\udc392 (\ud835\udc60\u2032) = \ud835\udc391 (\ud835\udc60) \u2211\ufe01\n\ud835\udc60\u2032\u2208Ch(\ud835\udc60)\n\ud835\udc391 (\ud835\udc60\u2032) \ud835\udc391 (\ud835\udc60)\n\ud835\udc392 (\ud835\udc60\u2032) = \u2211\ufe01\n\ud835\udc60\u2032\u2208Ch(\ud835\udc60) \ud835\udc391 (\ud835\udc60\u2032)\ud835\udc392 (\ud835\udc60\u2032),\nwhich completes the proof. \u25a1\nProposition 4. In a 2-player AFlowNet with alternating moves satisfying \ud835\udc45\u25e61 (\ud835\udc65)\ud835\udc45 \u25e6 2 (\ud835\udc65) = 1:\n(a) Suppose that the agent policies \ud835\udc431, \ud835\udc432 and state flow functions \ud835\udc391, \ud835\udc392 are jointly optimal in the sense of Prop. 2. Then there exists a scalar \ud835\udc4d , independent of \ud835\udc65, such that for every complete trajectory \ud835\udc600 \u2192 \ud835\udc601 \u2192 \u00b7 \u00b7 \u00b7 \u2192 \ud835\udc60\ud835\udc5b = \ud835\udc65,\n\ud835\udc4d \u220f \ud835\udc56:\ud835\udc60\ud835\udc56\u2208S1 \ud835\udc431 (\ud835\udc60\ud835\udc56+1 | \ud835\udc60\ud835\udc56) = \ud835\udc451 (\ud835\udc65)\ud835\udc352 (\ud835\udc65) \u220f \ud835\udc56:\ud835\udc60\ud835\udc56\u2208S2 \ud835\udc432 (\ud835\udc60\ud835\udc56+1 | \ud835\udc60\ud835\udc56). (10)\n(b) Conversely, if the constraint (10) holds for some constant \ud835\udc4d and policies \ud835\udc431 and \ud835\udc432, then \ud835\udc431 and \ud835\udc432 are the jointly optimal AFlowNet policies.\nProof of Proposition 4. Part (a). We first extend the definition of \ud835\udc35\ud835\udc56 to nonterminal states: if \ud835\udc600 \u2192 \ud835\udc601 \u2192 \u00b7 \u00b7 \u00b7 \u2192 \ud835\udc60\ud835\udc5a = \ud835\udc60 is any trajectory, define \ud835\udc35\ud835\udc56 (\ud835\udc60) := \u220f 0\u2264\ud835\udc56<\ud835\udc5a:\ud835\udc60\ud835\udc56\u2208S\ud835\udc56 | Ch(\ud835\udc60\ud835\udc56) |.\nWe claim that for all states \ud835\udc60, \ud835\udc391 (\ud835\udc60)\ud835\udc392 (\ud835\udc60) = 1\ud835\udc351 (\ud835\udc60)\ud835\udc352 (\ud835\udc60) . This holds at terminal states \ud835\udc65, since \ud835\udc391 (\ud835\udc65)\ud835\udc392 (\ud835\udc65) = \ud835\udc451 (\ud835\udc65)\ud835\udc452 (\ud835\udc65) = \ud835\udc45\u25e61 (\ud835\udc65 )\ud835\udc45 \u25e6 2 (\ud835\udc65 )\n\ud835\udc351 (\ud835\udc65 )\ud835\udc352 (\ud835\udc65 ) = 1 \ud835\udc351 (\ud835\udc65 )\ud835\udc352 (\ud835\udc65 ) . By Prop. 3, \ud835\udc391 (\ud835\udc60)\ud835\udc392 (\ud835\udc60) is a flow, so it suffices to show that 1\n\ud835\udc351 (\ud835\udc60)\ud835\udc352 (\ud835\udc60) also satisfies (1) for \ud835\udc60 \u2208 X \\ S. Without loss of generality, suppose \ud835\udc60 \u2208 S1 and let \ud835\udc600 \u2192 \u00b7 \u00b7 \u00b7 \u2192 \ud835\udc60\ud835\udc56 = \ud835\udc60 be the trajectory leading to \ud835\udc60. Then\u2211\ufe01\n\ud835\udc60\u2032\u2208Ch(\ud835\udc60)\n1 \ud835\udc351 (\ud835\udc60\u2032)\ud835\udc352 (\ud835\udc60\u2032)\n= \u2211\ufe01\n\ud835\udc60\u2032\u2208Ch(\ud835\udc60)\n1 \ud835\udc351 (\ud835\udc60) | Ch(\ud835\udc60) | \u00b7 \ud835\udc352 (\ud835\udc60) = 1 \ud835\udc351 (\ud835\udc60)\ud835\udc352 (\ud835\udc60) ,\nestablishing the claim.\nReturning to the proposition, rearranging factors and using the definition (9), it is necessary to show that \ud835\udc45\u25e61 (\ud835\udc65)\ud835\udc352 (\ud835\udc65) \u220f \ud835\udc56:\ud835\udc60\ud835\udc56\u2208S2 \ud835\udc432 (\ud835\udc60\ud835\udc56+1 | \ud835\udc60\ud835\udc56)\n\ud835\udc351 (\ud835\udc65) \u220f \ud835\udc56:\ud835\udc60\ud835\udc56\u2208S1 \ud835\udc431 (\ud835\udc60\ud835\udc56+1 | \ud835\udc60\ud835\udc56)\nis independent of \ud835\udc65. We have, using the above claim, \ud835\udc451 (\ud835\udc65)\ud835\udc352 (\ud835\udc65) \u220f \ud835\udc56:\ud835\udc60\ud835\udc56\u2208S2 \ud835\udc432 (\ud835\udc60\ud835\udc56+1 | \ud835\udc60\ud835\udc56)\u220f\n\ud835\udc56:\ud835\udc60\ud835\udc56\u2208S1 \ud835\udc431 (\ud835\udc60\ud835\udc56+1 | \ud835\udc60\ud835\udc56) = \ud835\udc451 (\ud835\udc65)\n\u220f \ud835\udc56:\ud835\udc60\ud835\udc56\u2208S2 | Ch(\ud835\udc60\ud835\udc56) |\n\ud835\udc392 (\ud835\udc60\ud835\udc56+1 ) \ud835\udc392 (\ud835\udc60\ud835\udc56 )\u220f\n\ud835\udc56:\ud835\udc60\ud835\udc56\u2208S1 \ud835\udc391 (\ud835\udc60\ud835\udc56+1 ) \ud835\udc391 (\ud835\udc60\ud835\udc56 )\n= \ud835\udc451 (\ud835\udc65)\n\u220f \ud835\udc56:\ud835\udc60\ud835\udc56\u2208S2 | Ch(\ud835\udc60\ud835\udc56) |\n\ud835\udc391 (\ud835\udc60\ud835\udc56 )\ud835\udc351 (\ud835\udc60\ud835\udc56 )\ud835\udc352 (\ud835\udc60\ud835\udc56 ) \ud835\udc391 (\ud835\udc60\ud835\udc56+1 )\ud835\udc351 (\ud835\udc60\ud835\udc56+1 )\ud835\udc352 (\ud835\udc60\ud835\udc56+1 )\u220f\n\ud835\udc56:\ud835\udc60\ud835\udc56\u2208S1 \ud835\udc391 (\ud835\udc60\ud835\udc56+1 ) \ud835\udc391 (\ud835\udc60\ud835\udc56 )\n= \ud835\udc451 (\ud835\udc65)\n\u220f \ud835\udc56:\ud835\udc60\ud835\udc56\u2208S2\n\ud835\udc391 (\ud835\udc60\ud835\udc56 ) \ud835\udc391 (\ud835\udc60\ud835\udc56+1 )\u220f\n\ud835\udc56:\ud835\udc60\ud835\udc56\u2208S1 \ud835\udc391 (\ud835\udc60\ud835\udc56+1 ) \ud835\udc391 (\ud835\udc60\ud835\udc56 ) = \ud835\udc451 (\ud835\udc65) \u220f\n\ud835\udc56 < \ud835\udc5b even\n\ud835\udc391 (\ud835\udc60\ud835\udc56) \ud835\udc391 (\ud835\udc60\ud835\udc56+1) \u220f \ud835\udc56 < \ud835\udc5b odd \ud835\udc391 (\ud835\udc60\ud835\udc56) \ud835\udc391 (\ud835\udc60\ud835\udc56+1)\n= \ud835\udc391 (\ud835\udc60\ud835\udc5b) \ud835\udc5b\u22121\u220f \ud835\udc56=0 \ud835\udc391 (\ud835\udc60\ud835\udc56) \ud835\udc391 (\ud835\udc60\ud835\udc56+1)\n= \ud835\udc391 (\ud835\udc600),\nwhich is independent of \ud835\udc65.\nPart (b). Because jointly optimal AFlowNet policies \ud835\udc431, \ud835\udc432 exist by Prop. 2, and they satisfy (10) by part (a) of this proposition, it suffices to show that the constraint (10) uniquely determines \ud835\udc431 and \ud835\udc432 for all pairs of reward functions (\ud835\udc451, \ud835\udc452) for which \ud835\udc451 (\ud835\udc65)\ud835\udc452 (\ud835\udc65) = 1\ud835\udc351 (\ud835\udc65 )\ud835\udc352 (\ud835\udc65 ) for all \ud835\udc65 \u2208 X.\nWe prove this by strong induction on the number of states in \ud835\udc3a. The base case |S| = 1 (there is a unique state which is both initial and terminal) is trivial: the products are empty and the constraint reads \ud835\udc4d = \ud835\udc451 (\ud835\udc65). Now suppose that the constraint uniquely determines \ud835\udc431 and \ud835\udc432 for all reward functions satisfying \ud835\udc451 (\ud835\udc65)\ud835\udc452 (\ud835\udc65) = 1\ud835\udc351 (\ud835\udc65 )\ud835\udc352 (\ud835\udc65 ) on graphs with fewer than \ud835\udc41 states, for some \ud835\udc41 > 1, and consider an AFlowNet \ud835\udc3a = (S,A) with \ud835\udc41 states, for which (10) holds and the reward functions satisfy \ud835\udc451 (\ud835\udc65)\ud835\udc452 (\ud835\udc65) = 1\ud835\udc351 (\ud835\udc65 )\ud835\udc352 (\ud835\udc65 ) . It is easy to see that there exists a state \ud835\udc60 \u2208 S \\X such that all children of \ud835\udc60 are terminal; select one such state \ud835\udc60.\nSuppose that \ud835\udc60 \u2208 S1. We construct a new graph \ud835\udc3a\u2032 and rewards \ud835\udc45\u20321, \ud835\udc45 \u2032 2 by deleting the children of \ud835\udc60, making \ud835\udc60 a terminal state, and modifying the reward function so that \ud835\udc45\u20321 (\ud835\udc60) = \u2211 \ud835\udc65\u2208Ch(\ud835\udc60) \ud835\udc451 (\ud835\udc60), setting \ud835\udc45\u20322 (\ud835\udc60) so as to preserve \ud835\udc45 \u2032 1 (\ud835\udc60)\ud835\udc45 \u2032 2 (\ud835\udc60) = 1 \ud835\udc351 (\ud835\udc65 )\ud835\udc352 (\ud835\udc65 ) , and setting \ud835\udc45 \u2032 \ud835\udc56 (\ud835\udc65) = \ud835\udc45\ud835\udc56 (\ud835\udc65) for all other terminal states \ud835\udc65. Thus the graph \ud835\udc3a\u2032 is a two-player AFlowNet with alternating turns and satisfying the constraint on rewards.\nWe claim that the constraint (10) for \ud835\udc3a, \ud835\udc451, \ud835\udc452 and a pair of policies \ud835\udc431, \ud835\udc432 on \ud835\udc3a implies the constraint for \ud835\udc3a\u2032, \ud835\udc45\u20321, \ud835\udc45 \u2032 2 and the same policies restricted to the states in \ud835\udc3a\n\u2032. For all terminal states in \ud835\udc3a\u2032 inherited from \ud835\udc3a, the constraint is unchanged. For the new terminal state \ud835\udc60, we sum the constraints on \ud835\udc3a for the children \ud835\udc651, . . . , \ud835\udc65\ud835\udc3e \u2208 Ch(\ud835\udc60). Letting \ud835\udc600 \u2192 \ud835\udc601 \u2192 \u00b7 \u00b7 \u00b7 \u2192 \ud835\udc60\ud835\udc5b = \ud835\udc60 be the trajectory leading to \ud835\udc60, we have:\n\ud835\udc3e\u2211\ufe01 \ud835\udc58=1 [ \ud835\udc4d \u220f \ud835\udc56<\ud835\udc5b:\ud835\udc60\ud835\udc56\u2208S1 \ud835\udc431 (\ud835\udc60\ud835\udc56+1 | \ud835\udc60\ud835\udc56)\ud835\udc431 (\ud835\udc65\ud835\udc58 | \ud835\udc60) ] = \ud835\udc3e\u2211\ufe01 \ud835\udc58=1 [ \ud835\udc451 (\ud835\udc65\ud835\udc58)\ud835\udc352 (\ud835\udc65\ud835\udc58) \u220f \ud835\udc56:\ud835\udc60\ud835\udc56\u2208S2 \ud835\udc432 (\ud835\udc60\ud835\udc56+1 | \ud835\udc60\ud835\udc56) ]\n( \ud835\udc4d \u220f \ud835\udc56<\ud835\udc5b:\ud835\udc60\ud835\udc56\u2208S1 \ud835\udc431 (\ud835\udc60\ud835\udc56+1 | \ud835\udc60\ud835\udc56) ) \ud835\udc3e\u2211\ufe01 \ud835\udc58=1 \ud835\udc431 (\ud835\udc65\ud835\udc58 | \ud835\udc60) = ( \ud835\udc352 (\ud835\udc60) \u220f \ud835\udc56:\ud835\udc60\ud835\udc56\u2208S2 \ud835\udc432 (\ud835\udc60\ud835\udc56+1 | \ud835\udc60\ud835\udc56) ) \ud835\udc3e\u2211\ufe01 \ud835\udc58=1 \ud835\udc451 (\ud835\udc65\ud835\udc58)\n\ud835\udc4d \u220f\n\ud835\udc56<\ud835\udc5b:\ud835\udc60\ud835\udc56\u2208S1 \ud835\udc431 (\ud835\udc60\ud835\udc56+1 | \ud835\udc60\ud835\udc56) = \ud835\udc352 (\ud835\udc60) \u220f \ud835\udc56:\ud835\udc60\ud835\udc56\u2208S2 \ud835\udc432 (\ud835\udc60\ud835\udc56+1 | \ud835\udc60\ud835\udc56)\ud835\udc45\u20321 (\ud835\udc60),\nwhich is precisely the constraint for \ud835\udc3a\u2032 at the state \ud835\udc60. So the constraint (10) is satisfied on \ud835\udc3a\u2032.\nSince \ud835\udc3a\u2032 has fewer than \ud835\udc41 states, by the induction hypothesis, \ud835\udc431 and \ud835\udc432 are uniquely determined on \ud835\udc3a\u2032 and are therefore equal to the jointly optimal AFlowNet policies. It remains to show that \ud835\udc431 (\u00b7 | \ud835\udc60) is uniquely determined. Indeed, the only factor on the left side of (10) that varies between\nchildren \ud835\udc65 of \ud835\udc60 is \ud835\udc431 (\ud835\udc65 | \ud835\udc60), while on the right side, the only such factor is \ud835\udc451 (\ud835\udc65). It follows that if the constraint is satisfied, then \ud835\udc431 (\ud835\udc65 | \ud835\udc60) \u221d \ud835\udc451 (\ud835\udc65), which uniquely determines \ud835\udc431 (\u00b7 | \ud835\udc60). The case \ud835\udc60 \u2208 S2 is analogous. \u25a1\nProposition 5. Suppose that \ud835\udc43agent satisfies the EDB constraints. Then, for any \ud835\udc60 \u2208 Sagent and \ud835\udc60\u2032 \u2208 Ch(\ud835\udc60), \ud835\udc43agent (\ud835\udc60\u2032 | \ud835\udc60) \u221d E\ud835\udc3aenv\u223c\ud835\udc43env [ \ud835\udc39\ud835\udc3aenv (\ud835\udc60\u2032) | \ud835\udc60 \u2208 \ud835\udc49 (\ud835\udc3aenv) ] ,\nwhere the expectation is taken over the distribution over strategies determined by \ud835\udc43env, restricted to the strategies that contain the state \ud835\udc60.\nProof of Proposition 5. We first note that the expression inside the expectation is well-defined, since if \ud835\udc60 \u2208 \ud835\udc49 (\ud835\udc3aenv) \u2229 Sagent, then all children of \ud835\udc60 are also in \ud835\udc49 (\ud835\udc3aenv). Now suppose that \ud835\udc43agent satisfies EDB jointly with a flow function \ud835\udc39. By (4), we have \ud835\udc43agent (\ud835\udc60\u2032 | \ud835\udc60) \u221d \ud835\udc39 (\ud835\udc60\u2032), and \ud835\udc60\u2032 \u2208 \ud835\udc49 (\ud835\udc3aenv) is equivalent to \ud835\udc60 \u2208 \ud835\udc49 (\ud835\udc3aenv) for \ud835\udc60 \u2208 Sagent and \ud835\udc60\u2032 \u2208 Ch(\ud835\udc60). Therefore, it would suffice to show that\n\ud835\udc39 (\ud835\udc60) = E\ud835\udc3aenv\u223c\ud835\udc43env [ \ud835\udc39\ud835\udc3aenv (\ud835\udc60) | \ud835\udc60 \u2208 \ud835\udc49 (\ud835\udc3aenv) ] for all \ud835\udc60 \u2208 S \\ X. To do so, we show that the expression on the right side satisfies the recurrence (13). We consider three cases:\n\u2022 If \ud835\udc60 \u2208 Sagent, then the child set of \ud835\udc60 in any \ud835\udc3aenv containing \ud835\udc60 is the same as its child set in \ud835\udc3a. It follows that\nE\ud835\udc3aenv\u223c\ud835\udc43env [ \ud835\udc39\ud835\udc3aenv (\ud835\udc60) | \ud835\udc60 \u2208 \ud835\udc49 (\ud835\udc3aenv) ] = E\ud835\udc3aenv\u223c\ud835\udc43env  \u2211\ufe01 \ud835\udc60\u2032\u2208Ch(\ud835\udc60) \ud835\udc39\ud835\udc3aenv (\ud835\udc60\u2032) | \ud835\udc60 \u2208 \ud835\udc49 (\ud835\udc3aenv)  =\n\u2211\ufe01 \ud835\udc60\u2032\u2208Ch(\ud835\udc60) E\ud835\udc3aenv\u223c\ud835\udc43env [ \ud835\udc39\ud835\udc3aenv (\ud835\udc60\u2032) | \ud835\udc60 \u2208 \ud835\udc49 (\ud835\udc3aenv) ] =\n\u2211\ufe01 \ud835\udc60\u2032\u2208Ch(\ud835\udc60) E\ud835\udc3aenv\u223c\ud835\udc43env [ \ud835\udc39\ud835\udc3aenv (\ud835\udc60\u2032) | \ud835\udc60\u2032 \u2208 \ud835\udc49 (\ud835\udc3aenv) ] ,\nshowing the first case of the recurrence. \u2022 If \ud835\udc60 \u2208 Senv, then in any \ud835\udc3aenv containing \ud835\udc60, \ud835\udc60 has a unique child \ud835\udc60\u2032 and \ud835\udc39\ud835\udc3aenv (\ud835\udc60) = \ud835\udc39\ud835\udc3aenv (\ud835\udc60\u2032). We\ndecompose the expectation into terms depending on the child of \ud835\udc60 that is present in \ud835\udc3aenv: E\ud835\udc3aenv\u223c\ud835\udc43env [ \ud835\udc39\ud835\udc3aenv (\ud835\udc60) | \ud835\udc60 \u2208 \ud835\udc49 (\ud835\udc3aenv) ] = E \ud835\udc60\u2032\u2208Ch(\ud835\udc60)\n\ud835\udc60\u2032\u223c\ud835\udc43env (\ud835\udc60\u2032\u2208\ud835\udc3aenv |\ud835\udc60\u2208\ud835\udc3aenv ) E\ud835\udc3aenv\u223c\ud835\udc43env\n[ \ud835\udc39\ud835\udc3aenv (\ud835\udc60) | \ud835\udc60\u2032 \u2208 \ud835\udc49 (\ud835\udc3aenv) ] = E\ud835\udc60\u2032\u223c\ud835\udc43env (\ud835\udc60\u2032 |\ud835\udc60)E\ud835\udc3aenv\u223c\ud835\udc43env [ \ud835\udc39\ud835\udc3aenv (\ud835\udc60\u2032) | \ud835\udc60\u2032 \u2208 \ud835\udc49 (\ud835\udc3aenv) ] ,\nwhich shows the second case of the recurrence. \u2022 The case \ud835\udc60 \u2208 X is simple, since \ud835\udc39 (\ud835\udc60\u2032) = \ud835\udc39\ud835\udc3aenv (\ud835\udc60\u2032) = \ud835\udc45(\ud835\udc60\u2032) for all \ud835\udc3aenv containing \ud835\udc60.\n\u25a1"
        },
        {
            "heading": "C GAME SPECIFICATION",
            "text": ""
        },
        {
            "heading": "C.1 TIC-TAC-TOE",
            "text": "Two players alternate between placing X tiles and O tiles in a 3 \u00d7 3 grid. If any player reaches a board with three of their pieces connected by a straight line, they win. Although simplistic, tic-tactoe has a small enough state space that the ground truth EDB-based flow values can be computed and compared to the learned values. This allows for the unique opportunity to verify that the AFlowNet has converged to the predicted stable point. The stable point can be found by recursively visiting each state in the game and backpropagating the rewards, flows, and probabilities."
        },
        {
            "heading": "C.2 CONNECT-4",
            "text": "There are two players who alternate between placing yellow and red tokens (for the sake of simplicity, we use X tiles and O tiles) in a 6\u00d7 7 grid (6 rows, 7 columns). Each token is placed at the top of the grid and falls to the lowest unoccupied point in the column. If any player reaches a board with four of their pieces connected in a straight line, they win. Connect-4 has a far larger state space than tic-tac-toe: it is quite difficult for even humans to learn, although the first player has been proven to have a winning strategy (Allis, 1988). As such, it is computationally infeasible to compute, or to store, the optimal policies at all states in the AFlowNet."
        },
        {
            "heading": "D TRAINING DETAILS",
            "text": "For 1, we collect trajectories as sequences of tuples (state, mask, curr player, action, done, log reward):\n\u2022 state: the current state of the environment \u2022 mask: binary mask of legal moves over the action space \u2022 curr player: the player whose turn it is to make the action \u2022 action: the sampled action at the given state \u2022 done: whether the action resulted in a terminal state \u2022 log reward: the log reward if done\nAs architecture, we use a convolutional neural network composed of residual blocks inspired by the AlphaZero architecture (Silver et al., 2018; Thakoor et al., 2016) with a few modifications. We remove the batch normalization layers as the population statistics varied too much between training and evaluation. We use only the policy head (using one for each side, e.g., playing as \u201dX\u201d or \u201dO\u201d) and increase the number of filters it has as recommended by Prasad et al. (2018). We replace ReLU activations by Leaky ReLU activations, reduce the number of residual blocks and reduce the number of filters in each block (128 instead of 256) as tic-tac-toe/Connect-4 are simpler than chess/Go. Finally, we include a single differentiable parameter log \ud835\udc4d .\nThe main training hyperparameters are:\n\u2022 num trajectories epoch: number of trajectories added to the buffer every epoch \u2022 batch size: batch size used for training and trajectory generation \u2022 num steps: number of optimization steps per epoch \u2022 replay buffer capacity: the maximum capacity of the replay buffer \u2022 learning rate: learning rate for the policy network \u2022 learning rate Z: learning rate for the log \ud835\udc4d (we find that a higher value helps training) \u2022 num residual blocks: number of residual blocks in the architecture\nSpecific values are included in Table 2."
        },
        {
            "heading": "D.1 TRAINING/EVALUATION POLICY",
            "text": "During training, to generate the training trajectories, we sample actions using the softmax of the policy logits of the AFlowNet with a temperature coefficient of 1.5. At test time, (i.e., for the tournaments), when it is the AFlowNet\u2019s turn to play, we select the move corresponding to arg max\ud835\udc60\u2032\u2208\ud835\udc36\u210e (\ud835\udc60) \ud835\udc43\ud835\udc56 (\ud835\udc60\u2032 | \ud835\udc60), where \ud835\udc56 is the index of the player to make a move at \ud835\udc60."
        },
        {
            "heading": "D.2 ALPHAZERO TRAINING",
            "text": "AlphaZero is trained as per the specifications of Thakoor et al. (2016). The batch size of AlphaZero is changed to 512 to match the batch size of AFlowNets. Training discrepancies include the number of examples gathered and retained over each iteration, which is significantly different between the AFlowNet implementation and AlphaZero. Naturally, it is quite difficult to compare AlphaZero and AFlowNets exactly because we do not have an MCTS analogue. Additionally, AlphaZero trains on single transitions whereas AFlowNets with TB loss train on entire trajectories. This leads to a discrepancy in what is considered a training example: a transition versus a trajectory. The number of Monte Carlo tree search iterations for AlphaZero is 25 for both tic-tac-toe and Connect-4."
        },
        {
            "heading": "E ADDITIONAL RESULTS FROM ADVERSARIAL GAMES",
            "text": ""
        },
        {
            "heading": "E.1 CONVERGENCE OF EFLOWNETS/AFLOWNETS TO UNIQUE OPTIMUM",
            "text": "In addition to testing game-playing performance, we aim to investigate whether EFlowNets/AFlowNets are capable of learning the correct flows (i.e., those corresponding to the unique optimum). The game tree of tic-tac-toe is small enough that ground truth flows and policies (for a fixed opponent and the stable-point optimum) can be computed algorithmically by backtracking recursively from terminal states.\nWe train neural networks, see \u00a7D for details, to evaluate the following configurations:\n(1) EFlowNet with a fixed stochastic opponent (policy consists of choosing each legal action with equal probability) from one perspective (e.g., always plays X). (2) EFlowNet versus a fixed stochastic opponent, learning both perspectives. (3) AFlowNet learning using the EDB objective with off-policy self-play.\nFigure 5 illustrates the learning performance of the three tested configurations. For all configurations, the ground truth flows/policies that satisfy the EDB constraints are learned, as evidenced by the left and middle graphs. Importantly, this is even the case for training through self-play, where the AFlowNet converges to the actual stable-point optimum.\nInterestingly, the AFlowNet does not need to learn the exact flows to achieve strong game-playing performance. For example, here the AFlowNet is able to always win or draw against a uniform agent after less than 5000 steps even though its MAE relative to the correct flows still decreases substantially in the next 20000 steps. Additionally, the EFlowNet formulation is not enough to obtain a robust game playing agent, particularly when the agent it is playing against does not play well."
        },
        {
            "heading": "E.2 COMPARISON TO DQN AND SOFTDQN",
            "text": "We include additional results comparing DQN and SoftDQN to AFlowNets on TicTacToe in Fig. 6. Getting standard RL algorithms to work in multi-agent settings (learning through self-play) is non trivial and does not work in many common RL libraries.\nTo remedy this, and to ensure as fair of a comparison as possible, we implement DQN and its soft equivalent inside our framework (i.e. using the same environment as AFN, the same architecture, etc.). Notably, to achieve an agent that could consistently beat a uniform opponent, it was necessary to use a minimax version of DQN similar to (Fan et al., 2020) for sequential games (i.e. the q-update is based on the negation of the maximum q-value of the opponent). For the soft version (denoted SoftDQN), we sample trajectories using the softmax of the Q-values and similarly use the softmax for the updates (once again in a minimax fashion)."
        },
        {
            "heading": "E.3 TIC-TAC-TOE TOURNAMENT",
            "text": "To test the performance of AFlowNets against state of the art methods such as AlphaZero, we train a popular open-source AlphaZero implementation Thakoor et al. (2016) to play tic-tac-toe, pitting the agents against each other and baselines in a tournament. In the set of baselines we include a uniform opponent and a tree-search agent2. By changing the value of \ud835\udf06 for the AFlowNet, we also test how the learned policy changes with varying rewards. We proceed to test the performance of EDB-based AFlowNets and TB-based AFlowNets.\nResults with EDB-based AFlowNets Some selected results of the tournament are listed in Table 3. Note that AlphaZero is trained with Monte Carlo tree search (MCTS), but is tested in the tournament with MCTS both on and off. This ensures a fair comparison of inference-time game-playing capabilities as AFlowNets have no such tree-search mechanism to generate a policy.\nIt is clear from the tournament results in Table 3 that AlphaZero and AFlowNet agents are capable of perfect play in tic-tac-toe, drawing nearly every game that was played. The biggest differences in performance come from playing against the uniform and tree-search agents. It is clear that AFlowNet10 performed the best, winning or drawing all games against the uniform and tree-search agents. Interestingly, while a higher \ud835\udf06 produces a better X-playing agent, the same is not true for agents playing second: against the tree-search and uniform agents, a lower \ud835\udf06 corresponds to a better score. As such, there may be a diminishing return with higher values of \ud835\udf06, perhaps encouraging overly-risky behaviour.\nTraining with MCTS seems to greatly improve the speed of convergence, as AlphaZero converges to a stable Elo after only a few training steps, see Figure 7. In comparison, AFlowNets require more training steps, optimization steps, and training examples to reach a similar level of performance\n2The tree-search agent uses AlphaZero\u2019s value function, a search depth of three, and alpha-beta pruning.\nto AlphaZero in terms of Elo. AFlowNet10 achieves the highest Elo, reinforcing its tournament performance in Table 3. Again, it appears that a larger \ud835\udf06 produces a worse AFlowNet agent, with AFlowNet15 achieving the lowest Elo of all AFlowNets. The AlphaZero agents also achieve high Elo scores, with AlphaZero+MCTS achieving the better score of the two.\nAlphaZero converges to a stable Elo after a small number of optimization steps whereas AFlowNets using the EDB constraint require an order of magnitude more steps to reach a similar Elo (about 50k steps versus 5k to 10k steps). A similar trend holds for training time, with AFlowNets requiring about 15 times longer to reach similar Elo scores (AlphaZero+MCTS took 38 seconds to reach an Elo of 46 whereas the first AFlowNet to reach a similar Elo of 51 took 558 seconds). While AFlowNets can certainly learn to play tic-tac-toe effectively, they clearly require far more training time and computation to achieve similar levels of performance to AlphaZero. We have not tested AlphaZero without MCTS in training, nor AFlowNets with tree search, so the comparison naturally favors AlphaZero given the power of tree search.\nResults with TB-based AFlowNets The results thus far have focused on EDB-trained agents, but it is important to demonstrate the performance of TB-based agents as well. Figure 3 illustrates the Elo over three training runs with different seeds of AFlowNet10 and the baseline models. Clearly, AFlowNet10 matches the Elo of AlphaZero and converges quickly. Compared to Figure 7, it appears that TB-based agents converge about as quickly as AlphaZero, about 10 times faster than the EDBbased agents. Interestingly, the Elo of the TB-based AFlowNets is lower than the Elo of the EDBbased AFlowNets.\nThere appears to be little difference in the performance of a TB-trained AFlowNet with different values of \ud835\udf06 in tic-tac-toe. This is in contrast to the EDB-trained AFlowNets which seemed to be affected by the setting of \ud835\udf06. Similarly to the EDB-based agents, almost every game in the tournament was a draw, with small differences between the performance of an agent against the baselines dictating the differences in Elo."
        },
        {
            "heading": "E.4 CONNECT-4 TOURNAMENT",
            "text": "We also run a tournament in Connect-4 against AlphaZero and a uniform random baseline. Again, we vary \ud835\udf06 to test how the reward structure changes agent performance. The tournament results in Table 4 indicate that the effect of \ud835\udf06 is similar to the experiments in tic-tac-toe. A higher \ud835\udf06 produces a better agent, however the diminishing return in these experiments relates to a reduction in benefit when increasing lambda rather than a decrease in Elo. This supports the idea that the reward structure affects the behaviour of an agent. The results of the Connect-4 tournament corroborate the Elo results of Figure 3. The Elos of agents AFlowNet2, AFlowNet10, and AFlowNet15 are 1190.8 \u00b1 64.2, 1700.1 \u00b1 60.0, and 1835.3 \u00b1 154.9. Clearly, AFlowNet15 is the best agent."
        }
    ],
    "year": 2023
}