{
    "abstractText": "Finetuning large language models (LLMs) on instructions leads to vast performance improvements on natural language tasks. We apply instruction tuning using code, leveraging the natural structure of Git commits, which pair code changes with human instructions. We compile COMMITPACK: 4 terabytes of Git commits across 350 programming languages. We benchmark COMMITPACK against other natural and synthetic code instructions (xP3x, Self-Instruct, OASST) on the 16B parameter StarCoder model, and achieve state-of-the-art performance among models not trained on OpenAI outputs, on the HumanEval Python benchmark (46.2% pass@1). We further introduce HUMANEVALPACK, expanding the HumanEval benchmark to a total of 3 coding tasks (Code Repair, Code Explanation, Code Synthesis) across 6 languages (Python, JavaScript, Java, Go, C++, Rust). Our models, OCTOCODER and OCTOGEEX, achieve the best performance across HUMANEVALPACK among all permissive models, demonstrating COMMITPACK\u2019s benefits in generalizing to a wider set of languages and natural coding tasks. Code, models and data are freely available at https://github.com/bigcode-project/octopack. import numpy as np import matplotlib.pyplot as plt # generate sample data x_data = np.linspace(-5, 5, 20) y_data = np.random.normal(0.0, 1.0, x_data.size) plt.plot(x_data, y_data, 'o') plt.show() Code Before Commit Message Code After Change to sin() function with noise import math import numpy as np import matplotlib.pyplot as plt # generate sample data x_data = np.linspace(-math.pi, math.pi, 30) y_data = np.sin(x_data) + np.random.normal(0.0, 0.1, x_data.size) plt.plot(x_data, y_data, 'o') plt.show() 1) CommitPack",
    "authors": [
        {
            "affiliations": [],
            "name": "Niklas Muennighoff"
        },
        {
            "affiliations": [],
            "name": "Qian Liu"
        },
        {
            "affiliations": [],
            "name": "Armel Zebaze"
        },
        {
            "affiliations": [],
            "name": "Qinkai Zheng"
        },
        {
            "affiliations": [],
            "name": "Binyuan Hui"
        },
        {
            "affiliations": [],
            "name": "Terry Yue Zhuo"
        },
        {
            "affiliations": [],
            "name": "Swayam Singh"
        },
        {
            "affiliations": [],
            "name": "Xiangru Tang"
        },
        {
            "affiliations": [],
            "name": "Leandro von Werra"
        },
        {
            "affiliations": [],
            "name": "Shayne Longpre"
        }
    ],
    "id": "SP:d65c7d5a268b54b95593b5e0e0ca1cb380b5a260",
    "references": [
        {
            "authors": [
                "Wasi Uddin Ahmad",
                "Saikat Chakraborty",
                "Baishakhi Ray",
                "Kai-Wei Chang"
            ],
            "title": "A transformer-based approach for source code summarization",
            "venue": "arXiv preprint arXiv:2005.00653,",
            "year": 2020
        },
        {
            "authors": [
                "Loubna Ben Allal",
                "Raymond Li",
                "Denis Kocetkov",
                "Chenghao Mou",
                "Christopher Akiki",
                "Carlos Munoz Ferrandis",
                "Niklas Muennighoff",
                "Mayank Mishra",
                "Alex Gu",
                "Manan Dey"
            ],
            "title": "Santacoder: don\u2019t reach for the stars",
            "venue": "arXiv preprint arXiv:2301.03988,",
            "year": 2023
        },
        {
            "authors": [
                "Ben Athiwaratkun",
                "Sanjay Krishna Gouda",
                "Zijian Wang",
                "Xiaopeng Li",
                "Yuchen Tian",
                "Ming Tan",
                "Wasi Uddin Ahmad",
                "Shiqi Wang",
                "Qing Sun",
                "Mingyue Shang"
            ],
            "title": "Multi-lingual evaluation of code generation models",
            "venue": "arXiv preprint arXiv:2210.14868,",
            "year": 2022
        },
        {
            "authors": [
                "Jacob Austin",
                "Augustus Odena",
                "Maxwell Nye",
                "Maarten Bosma",
                "Henryk Michalewski",
                "David Dohan",
                "Ellen Jiang",
                "Carrie Cai",
                "Michael Terry",
                "Quoc Le"
            ],
            "title": "Program synthesis with large language models",
            "venue": "arXiv preprint arXiv:2108.07732,",
            "year": 2021
        },
        {
            "authors": [
                "Hannah McLean Babe",
                "Sydney Nguyen",
                "Yangtian Zi",
                "Arjun Guha",
                "Molly Q Feldman",
                "Carolyn Jane Anderson"
            ],
            "title": "Studenteval: A benchmark of student-written prompts for large language models of code",
            "venue": "arXiv preprint arXiv:2306.04556,",
            "year": 2023
        },
        {
            "authors": [
                "Yuntao Bai",
                "Andy Jones",
                "Kamal Ndousse",
                "Amanda Askell",
                "Anna Chen",
                "Nova DasSarma",
                "Dawn Drain",
                "Stanislav Fort",
                "Deep Ganguli",
                "Tom Henighan"
            ],
            "title": "Training a helpful and harmless assistant with reinforcement learning from human feedback",
            "venue": "arXiv preprint arXiv:2204.05862,",
            "year": 2022
        },
        {
            "authors": [
                "Satanjeev Banerjee",
                "Alon Lavie"
            ],
            "title": "Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization",
            "year": 2005
        },
        {
            "authors": [
                "Antonio Valerio Miceli Barone",
                "Rico Sennrich"
            ],
            "title": "A parallel corpus of python functions and documentation strings for automated code documentation and code generation",
            "venue": "arXiv preprint arXiv:1707.02275,",
            "year": 2017
        },
        {
            "authors": [
                "Mohammad Bavarian",
                "Heewoo Jun",
                "Nikolas A. Tezak",
                "John Schulman",
                "Christine McLeavey",
                "Jerry Tworek",
                "Mark Chen"
            ],
            "title": "Efficient training of language models to fill in the middle",
            "venue": "arXiv preprint arXiv:2207.14255,",
            "year": 2022
        },
        {
            "authors": [
                "Loubna Ben Allal",
                "Niklas Muennighoff",
                "Logesh Kumar Umapathi",
                "Ben Lipkin",
                "Leandro von Werra"
            ],
            "title": "A framework for the evaluation of code generation models",
            "venue": "https://github.com/b igcode-project/bigcode-evaluation-harness,",
            "year": 2022
        },
        {
            "authors": [
                "Stella Biderman",
                "USVSN Sai Prashanth",
                "Lintang Sutawika",
                "Hailey Schoelkopf",
                "Quentin Anthony",
                "Shivanshu Purohit",
                "Edward Raf"
            ],
            "title": "Emergent and predictable memorization in large language models",
            "venue": "arXiv preprint arXiv:2304.11158,",
            "year": 2023
        },
        {
            "authors": [
                "Stella Biderman",
                "Hailey Schoelkopf",
                "Quentin Gregory Anthony",
                "Herbie Bradley",
                "Kyle O\u2019Brien",
                "Eric Hallahan",
                "Mohammad Aflah Khan",
                "Shivanshu Purohit",
                "USVSN Sai Prashanth",
                "Edward Raff"
            ],
            "title": "Pythia: A suite for analyzing large language models across training and scaling",
            "venue": "In International Conference on Machine Learning,",
            "year": 2023
        },
        {
            "authors": [
                "Sid Black",
                "Leo Gao",
                "Phil Wang",
                "Connor Leahy",
                "Stella Biderman"
            ],
            "title": "Gpt-neo: Large scale autoregressive language modeling with mesh-tensorflow",
            "year": 2021
        },
        {
            "authors": [
                "Sid Black",
                "Stella Biderman",
                "Eric Hallahan",
                "Quentin Anthony",
                "Leo Gao",
                "Laurence Golding",
                "Horace He",
                "Connor Leahy",
                "Kyle McDonell",
                "Jason Phang"
            ],
            "title": "Gpt-neox-20b: An open-source autoregressive language model",
            "venue": "arXiv preprint arXiv:2204.06745,",
            "year": 2022
        },
        {
            "authors": [
                "Herbie Bradley",
                "Honglu Fan",
                "Harry Saini",
                "Reshinth Adithyan",
                "Shivanshu Purohit",
                "Joel Lehman"
            ],
            "title": "Diff models - a new way to edit code",
            "venue": "CarperAI Blog,",
            "year": 2023
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D. Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners",
            "venue": "Conference on Neural Information Processing Systems (NeurIPS),",
            "year": 2020
        },
        {
            "authors": [
                "S\u00e9bastien Bubeck",
                "Varun Chandrasekaran",
                "Ronen Eldan",
                "Johannes Gehrke",
                "Eric Horvitz",
                "Ece Kamar",
                "Peter Lee",
                "Yin Tat Lee",
                "Yuanzhi Li",
                "Scott Lundberg"
            ],
            "title": "Sparks of artificial general intelligence: Early experiments with gpt-4",
            "venue": "arXiv preprint arXiv:2303.12712,",
            "year": 2023
        },
        {
            "authors": [
                "Nghi DQ Bui",
                "Hung Le",
                "Yue Wang",
                "Junnan Li",
                "Akhilesh Deepak Gotmare",
                "Steven CH Hoi"
            ],
            "title": "Codetf: One-stop transformer library for state-of-the-art code llm",
            "venue": "arXiv preprint arXiv:2306.00029,",
            "year": 2023
        },
        {
            "authors": [
                "Federico Cassano",
                "John Gouwar",
                "Daniel Nguyen",
                "Sydney Nguyen",
                "Luna Phipps-Costin",
                "Donald Pinckney",
                "Ming-Ho Yee",
                "Yangtian Zi",
                "Carolyn Jane Anderson",
                "Molly Q Feldman"
            ],
            "title": "Multipl-e: a scalable and polyglot approach to benchmarking neural code generation",
            "venue": "IEEE Transactions on Software Engineering,",
            "year": 2023
        },
        {
            "authors": [
                "Sahil Chaudhary"
            ],
            "title": "Code alpaca: An instruction-following llama model for code generation",
            "venue": "https: //github.com/sahil280114/codealpaca,",
            "year": 2023
        },
        {
            "authors": [
                "Bei Chen",
                "Fengji Zhang",
                "Anh Nguyen",
                "Daoguang Zan",
                "Zeqi Lin",
                "Jian-Guang Lou",
                "Weizhu Chen"
            ],
            "title": "Codet: Code generation with generated tests",
            "venue": "arXiv preprint arXiv:2207.10397,",
            "year": 2022
        },
        {
            "authors": [
                "Lingjiao Chen",
                "Matei Zaharia",
                "James Zou"
            ],
            "title": "How is chatgpt\u2019s behavior changing over time",
            "year": 2023
        },
        {
            "authors": [
                "Mark Chen",
                "Jerry Tworek",
                "Heewoo Jun",
                "Qiming Yuan",
                "Henrique Ponde de Oliveira Pinto",
                "Jared Kaplan",
                "Harri Edwards",
                "Yuri Burda",
                "Nicholas Joseph",
                "Greg Brockman"
            ],
            "title": "Evaluating large language models trained on code",
            "venue": "arXiv preprint arXiv:2107.03374,",
            "year": 2021
        },
        {
            "authors": [
                "Shouyuan Chen",
                "Sherman Wong",
                "Liangjian Chen",
                "Yuandong Tian"
            ],
            "title": "Extending context window of large language models via positional interpolation",
            "venue": "arXiv preprint arXiv:2306.15595,",
            "year": 2023
        },
        {
            "authors": [
                "Xinyun Chen",
                "Maxwell Lin",
                "Nathanael Sch\u00e4rli",
                "Denny Zhou"
            ],
            "title": "Teaching large language models to self-debug",
            "venue": "arXiv preprint arXiv:2304.05128,",
            "year": 2023
        },
        {
            "authors": [
                "Paul F Christiano",
                "Jan Leike",
                "Tom Brown",
                "Miljan Martic",
                "Shane Legg",
                "Dario Amodei"
            ],
            "title": "Deep reinforcement learning from human preferences",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Fenia Christopoulou",
                "Gerasimos Lampouras",
                "Milan Gritta",
                "Guchun Zhang",
                "Yinpeng Guo",
                "Zhongqi Li",
                "Qi Zhang",
                "Meng Xiao",
                "Bo Shen",
                "Lin Li"
            ],
            "title": "Pangu-coder: Program synthesis with function-level language modeling",
            "venue": "arXiv preprint arXiv:2207.11280,",
            "year": 2022
        },
        {
            "authors": [
                "Hyung Won Chung",
                "Le Hou",
                "Shayne Longpre",
                "Barret Zoph",
                "Yi Tay",
                "William Fedus",
                "Eric Li",
                "Xuezhi Wang",
                "Mostafa Dehghani",
                "Siddhartha Brahma"
            ],
            "title": "Scaling instruction-finetuned language models",
            "venue": "arXiv preprint arXiv:2210.11416,",
            "year": 2022
        },
        {
            "authors": [
                "Haotian Cui",
                "Chenglong Wang",
                "Junjie Huang",
                "Jeevana Priya Inala",
                "Todd Mytkowicz",
                "Bo Wang",
                "Jianfeng Gao",
                "Nan Duan"
            ],
            "title": "Codeexp: Explanatory code document generation",
            "venue": "arXiv preprint arXiv:2211.15395,",
            "year": 2022
        },
        {
            "authors": [
                "Zihang Dai",
                "Zhilin Yang",
                "Yiming Yang",
                "Jaime Carbonell",
                "Quoc V Le",
                "Ruslan Salakhutdinov"
            ],
            "title": "Transformer-xl: Attentive language models beyond a fixed-length context",
            "year": 1901
        },
        {
            "authors": [
                "Tri Dao",
                "Dan Fu",
                "Stefano Ermon",
                "Atri Rudra",
                "Christopher R\u00e9"
            ],
            "title": "Flashattention: Fast and memoryefficient exact attention with io-awareness",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Tim Dettmers",
                "Artidoro Pagnoni",
                "Ari Holtzman",
                "Luke Zettlemoyer"
            ],
            "title": "Qlora: Efficient finetuning of quantized llms",
            "venue": "arXiv preprint arXiv:2305.14314,",
            "year": 2023
        },
        {
            "authors": [
                "Kaustubh D Dhole",
                "Varun Gangal",
                "Sebastian Gehrmann",
                "Aadesh Gupta",
                "Zhenhao Li",
                "Saad Mahamood",
                "Abinaya Mahendiran",
                "Simon Mille",
                "Ashish Srivastava",
                "Samson Tan"
            ],
            "title": "Nl-augmenter: A framework for task-sensitive natural language augmentation",
            "venue": "arXiv preprint arXiv:2112.02721,",
            "year": 2021
        },
        {
            "authors": [
                "Yangruibo Ding",
                "Zijian Wang",
                "Wasi Uddin Ahmad",
                "Murali Krishna Ramanathan",
                "Ramesh Nallapati",
                "Parminder Bhatia",
                "Dan Roth",
                "Bing Xiang"
            ],
            "title": "Cocomic: Code completion by jointly modeling in-file and cross-file context",
            "venue": "arXiv preprint arXiv:2212.10007,",
            "year": 2022
        },
        {
            "authors": [
                "Yihong Dong",
                "Xue Jiang",
                "Zhi Jin",
                "Ge Li"
            ],
            "title": "Self-collaboration code generation via chatgpt",
            "venue": "arXiv preprint arXiv:2304.07590,",
            "year": 2023
        },
        {
            "authors": [
                "Dawn Drain",
                "Colin B Clement",
                "Guillermo Serrato",
                "Neel Sundaresan"
            ],
            "title": "Deepdebug: Fixing python bugs using stack traces, backtranslation, and code skeletons",
            "venue": "arXiv preprint arXiv:2105.09352,",
            "year": 2021
        },
        {
            "authors": [
                "Xueying Du",
                "Mingwei Liu",
                "Kaixin Wang",
                "Hanlin Wang",
                "Junwei Liu",
                "Yixuan Chen",
                "Jiayi Feng",
                "Chaofeng Sha",
                "Xin Peng",
                "Yiling Lou"
            ],
            "title": "Classeval: A manually-crafted benchmark for evaluating llms on class-level code generation",
            "venue": "arXiv preprint arXiv:2308.01861,",
            "year": 2023
        },
        {
            "authors": [
                "Jane Dwivedi-Yu",
                "Timo Schick",
                "Zhengbao Jiang",
                "Maria Lomeli",
                "Patrick Lewis",
                "Gautier Izacard",
                "Edouard Grave",
                "Sebastian Riedel",
                "Fabio Petroni"
            ],
            "title": "Editeval: An instruction-based benchmark for text improvements",
            "venue": "arXiv preprint arXiv:2209.13331,",
            "year": 2022
        },
        {
            "authors": [
                "Aryaz Eghbali",
                "Michael Pradel"
            ],
            "title": "Crystalbleu: precisely and efficiently measuring the similarity of code",
            "venue": "In Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering,",
            "year": 2022
        },
        {
            "authors": [
                "Kawin Ethayarajh",
                "Winnie Xu",
                "Niklas Muennighoff",
                "Dan Jurafsky",
                "Douwe Kiela"
            ],
            "title": "Kto: Model alignment as prospect theoretic optimization, 2024",
            "year": 2024
        },
        {
            "authors": [
                "Sarah Fakhoury",
                "Saikat Chakraborty",
                "Madan Musuvathi",
                "Shuvendu K Lahiri"
            ],
            "title": "Towards generating functionally correct code edits from natural language issue descriptions",
            "venue": "arXiv preprint arXiv:2304.03816,",
            "year": 2023
        },
        {
            "authors": [
                "Daniel Fried",
                "Armen Aghajanyan",
                "Jessy Lin",
                "Sida Wang",
                "Eric Wallace",
                "Freda Shi",
                "Ruiqi Zhong",
                "Wen-tau Yih",
                "Luke Zettlemoyer",
                "Mike Lewis"
            ],
            "title": "Incoder: A generative model for code infilling and synthesis",
            "venue": "arXiv preprint arXiv:2204.05999,",
            "year": 2022
        },
        {
            "authors": [
                "Jinlan Fu",
                "See-Kiong Ng",
                "Zhengbao Jiang",
                "Pengfei Liu"
            ],
            "title": "Gptscore: Evaluate as you desire",
            "venue": "arXiv preprint arXiv:2302.04166,",
            "year": 2023
        },
        {
            "authors": [
                "Deep Ganguli",
                "Liane Lovitt",
                "Jackson Kernion",
                "Amanda Askell",
                "Yuntao Bai",
                "Saurav Kadavath",
                "Ben Mann",
                "Ethan Perez",
                "Nicholas Schiefer",
                "Kamal Ndousse"
            ],
            "title": "Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned",
            "venue": "arXiv preprint arXiv:2209.07858,",
            "year": 2022
        },
        {
            "authors": [
                "Leo Gao",
                "Jonathan Tow",
                "Stella Biderman",
                "Sid Black",
                "Anthony DiPofi",
                "Charles Foster",
                "Laurence Golding",
                "Jeffrey Hsu",
                "Kyle McDonell",
                "Niklas Muennighoff",
                "Jason Phang",
                "Laria Reynolds",
                "Eric Tang",
                "Anish Thite",
                "Ben Wang",
                "Kevin Wang",
                "Andy Zou"
            ],
            "title": "A framework for few-shot language model evaluation, 2021",
            "venue": "URL https://doi.org/10.5281/zenodo.5371628",
            "year": 2021
        },
        {
            "authors": [
                "Luyu Gao",
                "Aman Madaan",
                "Shuyan Zhou",
                "Uri Alon",
                "Pengfei Liu",
                "Yiming Yang",
                "Jamie Callan",
                "Graham Neubig"
            ],
            "title": "Pal: Program-aided language models",
            "venue": "In International Conference on Machine Learning,",
            "year": 2023
        },
        {
            "authors": [
                "Deepanway Ghosal",
                "Yew Ken Chia",
                "Navonil Majumder",
                "Soujanya Poria"
            ],
            "title": "Flacuna: Unleashing the problem solving power of vicuna using flan fine-tuning",
            "venue": "arXiv preprint arXiv:2307.02053,",
            "year": 2023
        },
        {
            "authors": [
                "Zhibin Gou",
                "Zhihong Shao",
                "Yeyun Gong",
                "Yelong Shen",
                "Yujiu Yang",
                "Nan Duan",
                "Weizhu Chen"
            ],
            "title": "Critic: Large language models can self-correct with tool-interactive critiquing",
            "venue": "arXiv preprint arXiv:2305.11738,",
            "year": 2023
        },
        {
            "authors": [
                "Arnav Gudibande",
                "Eric Wallace",
                "Charlie Snell",
                "Xinyang Geng",
                "Hao Liu",
                "Pieter Abbeel",
                "Sergey Levine",
                "Dawn Song"
            ],
            "title": "The false promise of imitating proprietary llms",
            "venue": "arXiv preprint arXiv:2305.15717,",
            "year": 2023
        },
        {
            "authors": [
                "Jingxuan He",
                "Luca Beurer-Kellner",
                "Martin Vechev"
            ],
            "title": "On distribution shift in learning-based bug detectors",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Vincent J Hellendoorn",
                "Charles Sutton",
                "Rishabh Singh",
                "Petros Maniatis",
                "David Bieber"
            ],
            "title": "Global relational models of source code",
            "venue": "In International conference on learning representations,",
            "year": 2019
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Steven Basart",
                "Saurav Kadavath",
                "Mantas Mazeika",
                "Akul Arora",
                "Ethan Guo",
                "Collin Burns",
                "Samir Puranik",
                "Horace He",
                "Dawn Song"
            ],
            "title": "Measuring coding challenge competence with apps",
            "venue": "arXiv preprint arXiv:2105.09938,",
            "year": 2021
        },
        {
            "authors": [
                "Edward J Hu",
                "Yelong Shen",
                "Phillip Wallis",
                "Zeyuan Allen-Zhu",
                "Yuanzhi Li",
                "Shean Wang",
                "Lu Wang",
                "Weizhu Chen"
            ],
            "title": "Lora: Low-rank adaptation of large language models",
            "venue": "arXiv preprint arXiv:2106.09685,",
            "year": 2021
        },
        {
            "authors": [
                "Yi Hu",
                "Haotong Yang",
                "Zhouchen Lin",
                "Muhan Zhang"
            ],
            "title": "Code prompting: a neural symbolic method for complex reasoning in large language models",
            "venue": "arXiv preprint arXiv:2305.18507,",
            "year": 2023
        },
        {
            "authors": [
                "Srinivasan Iyer",
                "Ioannis Konstas",
                "Alvin Cheung",
                "Luke Zettlemoyer"
            ],
            "title": "Summarizing source code using a neural attention model",
            "venue": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
            "year": 2016
        },
        {
            "authors": [
                "Srinivasan Iyer",
                "Xi Victoria Lin",
                "Ramakanth Pasunuru",
                "Todor Mihaylov",
                "Daniel Simig",
                "Ping Yu",
                "Kurt Shuster",
                "Tianlu Wang",
                "Qing Liu",
                "Punit Singh Koura",
                "Xian Li",
                "Brian O\u2019Horo",
                "Gabriel Pereyra",
                "Jeff Wang",
                "Christopher Dewan",
                "Asli Celikyilmaz",
                "Luke Zettlemoyer",
                "Ves Stoyanov"
            ],
            "title": "Opt-iml: Scaling language model instruction meta learning through the lens of generalization",
            "venue": "URL https://arxiv.org/abs/2212.12017",
            "year": 2022
        },
        {
            "authors": [
                "Mingi Jeon",
                "Seung-Yeop Baik",
                "Joonghyuk Hahn",
                "Yo-Sub Han",
                "Sang-Ki Ko"
            ],
            "title": "Deep Learning-based Code Complexity Prediction",
            "year": 2022
        },
        {
            "authors": [
                "Nan Jiang",
                "Kevin Liu",
                "Thibaud Lutellier",
                "Lin Tan"
            ],
            "title": "Impact of code language models on automated program repair",
            "venue": "arXiv preprint arXiv:2302.05020,",
            "year": 2023
        },
        {
            "authors": [
                "Tae-Hwan Jung"
            ],
            "title": "Commitbert: Commit message generation using pre-trained programming language model",
            "venue": "arXiv preprint arXiv:2105.14242,",
            "year": 2021
        },
        {
            "authors": [
                "Mohammad Abdullah Matin Khan",
                "M Saiful Bari",
                "Xuan Long Do",
                "Weishi Wang",
                "Md Rizwan Parvez",
                "Shafiq Joty"
            ],
            "title": "xcodeeval: A large scale multilingual multitask benchmark for code understanding, generation, translation and retrieval",
            "venue": "arXiv preprint arXiv:2303.03004,",
            "year": 2023
        },
        {
            "authors": [
                "Douwe Kiela",
                "Hamed Firooz",
                "Aravind Mohan",
                "Vedanuj Goswami",
                "Amanpreet Singh",
                "Casey A Fitzpatrick",
                "Peter Bull",
                "Greg Lipstein",
                "Tony Nelli",
                "Ron Zhu"
            ],
            "title": "The hateful memes challenge: Competition report",
            "venue": "NeurIPS",
            "year": 2020
        },
        {
            "authors": [
                "Denis Kocetkov",
                "Raymond Li",
                "Loubna Ben Allal",
                "Jia Li",
                "Chenghao Mou",
                "Carlos Mu\u00f1oz Ferrandis",
                "Yacine Jernite",
                "Margaret Mitchell",
                "Sean Hughes",
                "Thomas Wolf"
            ],
            "title": "The stack: 3 tb of permissively licensed source code",
            "venue": "arXiv preprint arXiv:2211.15533,",
            "year": 2022
        },
        {
            "authors": [
                "Andreas K\u00f6pf",
                "Yannic Kilcher",
                "Dimitri von R\u00fctte",
                "Sotiris Anagnostidis",
                "Zhi-Rui Tam",
                "Keith Stevens",
                "Abdullah Barhoum",
                "Nguyen Minh Duc",
                "Oliver Stanley",
                "Rich\u00e1rd Nagyfi"
            ],
            "title": "Openassistant conversations\u2013democratizing large language model alignment",
            "venue": "arXiv preprint arXiv:2304.07327,",
            "year": 2023
        },
        {
            "authors": [
                "Yuhang Lai",
                "Chengxi Li",
                "Yiming Wang",
                "Tianyi Zhang",
                "Ruiqi Zhong",
                "Luke Zettlemoyer",
                "Wen-tau Yih",
                "Daniel Fried",
                "Sida Wang",
                "Tao Yu"
            ],
            "title": "Ds-1000: A natural and reliable benchmark for data science code generation",
            "venue": "In International Conference on Machine Learning,",
            "year": 2023
        },
        {
            "authors": [
                "Hugo Lauren\u00e7on",
                "Lucile Saulnier",
                "Thomas Wang",
                "Christopher Akiki",
                "Albert Villanova del Moral",
                "Teven Le Scao",
                "Leandro Von Werra",
                "Chenghao Mou",
                "Eduardo Gonz\u00e1lez Ponferrada",
                "Huu Nguyen"
            ],
            "title": "The bigscience roots corpus: A 1.6 tb composite multilingual dataset",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Joel Lehman",
                "Jonathan Gordon",
                "Shawn Jain",
                "Kamal Ndousse",
                "Cathy Yeh",
                "Kenneth O Stanley"
            ],
            "title": "Evolution through large models",
            "venue": "arXiv preprint arXiv:2206.08896,",
            "year": 2022
        },
        {
            "authors": [
                "Bo Li",
                "Yuanhan Zhang",
                "Liangyu Chen",
                "Jinghao Wang",
                "Jingkang Yang",
                "Ziwei Liu"
            ],
            "title": "Otter: A multi-modal model with in-context instruction tuning",
            "venue": "arXiv preprint arXiv:2305.03726,",
            "year": 2023
        },
        {
            "authors": [
                "Hongyu Li",
                "Seohyun Kim",
                "Satish Chandra"
            ],
            "title": "Neural code search evaluation dataset",
            "venue": "arXiv preprint arXiv:1908.09804,",
            "year": 2019
        },
        {
            "authors": [
                "Raymond Li",
                "Loubna Ben Allal",
                "Yangtian Zi",
                "Niklas Muennighoff",
                "Denis Kocetkov",
                "Chenghao Mou",
                "Marc Marone",
                "Christopher Akiki",
                "Jia Li",
                "Jenny Chim"
            ],
            "title": "Starcoder: may the source be with you! arXiv preprint arXiv:2305.06161, 2023b",
            "year": 2023
        },
        {
            "authors": [
                "Xueyang Li",
                "Shangqing Liu",
                "Ruitao Feng",
                "Guozhu Meng",
                "Xiaofei Xie",
                "Kai Chen",
                "Yang Liu"
            ],
            "title": "Transrepair: Context-aware program repair for compilation errors",
            "venue": "In Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering,",
            "year": 2022
        },
        {
            "authors": [
                "Yujia Li",
                "David Choi",
                "Junyoung Chung",
                "Nate Kushman",
                "Julian Schrittwieser",
                "R\u00e9mi Leblond",
                "Tom Eccles",
                "James Keeling",
                "Felix Gimeno",
                "Agustin Dal Lago"
            ],
            "title": "Competition-level code generation with alphacode",
            "year": 2022
        },
        {
            "authors": [
                "Chin-Yew Lin"
            ],
            "title": "Rouge: A package for automatic evaluation of summaries",
            "venue": "In Text summarization branches out,",
            "year": 2004
        },
        {
            "authors": [
                "Derrick Lin",
                "James Koppel",
                "Angela Chen",
                "Armando Solar-Lezama"
            ],
            "title": "Quixbugs: A multi-lingual program repair benchmark set based on the quixey challenge",
            "venue": "In Proceedings Companion of the 2017 ACM SIGPLAN international conference on systems,",
            "year": 2017
        },
        {
            "authors": [
                "Haotian Liu",
                "Chunyuan Li",
                "Qingyang Wu",
                "Yong Jae Lee"
            ],
            "title": "Visual instruction tuning",
            "venue": "arXiv preprint arXiv:2304.08485,",
            "year": 2023
        },
        {
            "authors": [
                "Jiawei Liu",
                "Chunqiu Steven Xia",
                "Yuyao Wang",
                "Lingming Zhang"
            ],
            "title": "Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation",
            "venue": "arXiv preprint arXiv:2305.01210,",
            "year": 2023
        },
        {
            "authors": [
                "Nelson F Liu",
                "Kevin Lin",
                "John Hewitt",
                "Ashwin Paranjape",
                "Michele Bevilacqua",
                "Fabio Petroni",
                "Percy Liang"
            ],
            "title": "Lost in the middle: How language models use long contexts",
            "venue": "arXiv preprint arXiv:2307.03172,",
            "year": 2023
        },
        {
            "authors": [
                "Tianyang Liu",
                "Canwen Xu",
                "Julian McAuley"
            ],
            "title": "Repobench: Benchmarking repository-level code auto-completion systems",
            "venue": "arXiv preprint arXiv:2306.03091,",
            "year": 2023
        },
        {
            "authors": [
                "Yang Liu",
                "Dan Iter",
                "Yichong Xu",
                "Shuohang Wang",
                "Ruochen Xu",
                "Chenguang Zhu"
            ],
            "title": "Gpteval: Nlg evaluation using gpt-4 with better human alignment",
            "venue": "arXiv preprint arXiv:2303.16634,",
            "year": 2023
        },
        {
            "authors": [
                "Shayne Longpre",
                "Le Hou",
                "Tu Vu",
                "Albert Webson",
                "Hyung Won Chung",
                "Yi Tay",
                "Denny Zhou",
                "Quoc V Le",
                "Barret Zoph",
                "Jason Wei"
            ],
            "title": "The flan collection: Designing data and methods for effective instruction tuning",
            "venue": "arXiv preprint arXiv:2301.13688,",
            "year": 2023
        },
        {
            "authors": [
                "Shayne Longpre",
                "Gregory Yauney",
                "Emily Reif",
                "Katherine Lee",
                "Adam Roberts",
                "Barret Zoph",
                "Denny Zhou",
                "Jason Wei",
                "Kevin Robinson",
                "David Mimno"
            ],
            "title": "A pretrainer\u2019s guide to training data: Measuring the effects of data age, domain coverage, quality, & toxicity",
            "venue": "arXiv preprint arXiv:2305.13169,",
            "year": 2023
        },
        {
            "authors": [
                "Shuai Lu",
                "Daya Guo",
                "Shuo Ren",
                "Junjie Huang",
                "Alexey Svyatkovskiy",
                "Ambrosio Blanco",
                "Colin Clement",
                "Dawn Drain",
                "Daxin Jiang",
                "Duyu Tang"
            ],
            "title": "Codexglue: A machine learning benchmark dataset for code understanding and generation",
            "venue": "arXiv preprint arXiv:2102.04664,",
            "year": 2021
        },
        {
            "authors": [
                "Ziyang Luo",
                "Can Xu",
                "Pu Zhao",
                "Qingfeng Sun",
                "Xiubo Geng",
                "Wenxiang Hu",
                "Chongyang Tao",
                "Jing Ma",
                "Qingwei Lin",
                "Daxin Jiang"
            ],
            "title": "Wizardcoder: Empowering code large language models with evol-instruct",
            "venue": "arXiv preprint arXiv:2306.08568,",
            "year": 2023
        },
        {
            "authors": [
                "Aman Madaan",
                "Alexander Shypula",
                "Uri Alon",
                "Milad Hashemi",
                "Parthasarathy Ranganathan",
                "Yiming Yang",
                "Graham Neubig",
                "Amir Yazdanbakhsh"
            ],
            "title": "Learning performance-improving code edits",
            "venue": "arXiv preprint arXiv:2302.07867,",
            "year": 2023
        },
        {
            "authors": [
                "Aman Madaan",
                "Niket Tandon",
                "Prakhar Gupta",
                "Skyler Hallinan",
                "Luyu Gao",
                "Sarah Wiegreffe",
                "Uri Alon",
                "Nouha Dziri",
                "Shrimai Prabhumoye",
                "Yiming Yang"
            ],
            "title": "Self-refine: Iterative refinement with self-feedback",
            "venue": "arXiv preprint arXiv:2303.17651,",
            "year": 2023
        },
        {
            "authors": [
                "Sewon Min",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Hannaneh Hajishirzi"
            ],
            "title": "MetaICL: Learning to learn in context",
            "venue": "Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL),",
            "year": 2022
        },
        {
            "authors": [
                "Martin Monperrus",
                "Matias Martinez",
                "He Ye",
                "Fernanda Madeiral",
                "Thomas Durieux",
                "Zhongxing Yu"
            ],
            "title": "Megadiff: A dataset of 600k java source code changes categorized by diff size",
            "venue": "arXiv preprint arXiv:2108.04631,",
            "year": 2021
        },
        {
            "authors": [
                "Niklas Muennighoff"
            ],
            "title": "Sgpt: Gpt sentence embeddings for semantic search",
            "venue": "arXiv preprint arXiv:2202.08904,",
            "year": 2022
        },
        {
            "authors": [
                "Niklas Muennighoff",
                "Nouamane Tazi",
                "Lo\u00efc Magne",
                "Nils Reimers"
            ],
            "title": "Mteb: Massive text embedding benchmark",
            "venue": "arXiv preprint arXiv:2210.07316,",
            "year": 2022
        },
        {
            "authors": [
                "Niklas Muennighoff",
                "Thomas Wang",
                "Lintang Sutawika",
                "Adam Roberts",
                "Stella Biderman",
                "Teven Le Scao",
                "M Saiful Bari",
                "Sheng Shen",
                "Zheng-Xin Yong",
                "Hailey Schoelkopf"
            ],
            "title": "Crosslingual generalization through multitask finetuning",
            "venue": "arXiv preprint arXiv:2211.01786,",
            "year": 2022
        },
        {
            "authors": [
                "Niklas Muennighoff",
                "Alexander M Rush",
                "Boaz Barak",
                "Teven Le Scao",
                "Aleksandra Piktus",
                "Nouamane Tazi",
                "Sampo Pyysalo",
                "Thomas Wolf",
                "Colin Raffel"
            ],
            "title": "Scaling data-constrained language models",
            "venue": "arXiv preprint arXiv:2305.16264,",
            "year": 2023
        },
        {
            "authors": [
                "Niklas Muennighoff",
                "Hongjin Su",
                "Liang Wang",
                "Nan Yang",
                "Furu Wei",
                "Tao Yu",
                "Amanpreet Singh",
                "Douwe Kiela"
            ],
            "title": "Generative representational instruction tuning, 2024",
            "year": 2024
        },
        {
            "authors": [
                "Ansong Ni",
                "Srini Iyer",
                "Dragomir Radev",
                "Veselin Stoyanov",
                "Wen-tau Yih",
                "Sida Wang",
                "Xi Victoria Lin"
            ],
            "title": "Lever: Learning to verify language-to-code generation with execution",
            "venue": "In International Conference on Machine Learning,",
            "year": 2023
        },
        {
            "authors": [
                "Erik Nijkamp",
                "Bo Pang",
                "Hiroaki Hayashi",
                "Lifu Tu",
                "Huan Wang",
                "Yingbo Zhou",
                "Silvio Savarese",
                "Caiming Xiong"
            ],
            "title": "Codegen: An open large language model for code with multi-turn program synthesis",
            "venue": "arXiv preprint arXiv:2203.13474,",
            "year": 2022
        },
        {
            "authors": [
                "Erik Nijkamp",
                "Hiroaki Hayashi",
                "Caiming Xiong",
                "Silvio Savarese",
                "Yingbo Zhou"
            ],
            "title": "Codegen2: Lessons for training llms on programming and natural languages",
            "venue": "arXiv preprint arXiv:2305.02309,",
            "year": 2023
        },
        {
            "authors": [
                "Maxwell Nye",
                "Anders Johan Andreassen",
                "Guy Gur-Ari",
                "Henryk Michalewski",
                "Jacob Austin",
                "David Bieber",
                "David Dohan",
                "Aitor Lewkowycz",
                "Maarten Bosma",
                "David Luan"
            ],
            "title": "Show your work: Scratchpads for intermediate computation with language models",
            "venue": "arXiv preprint arXiv:2112.00114,",
            "year": 2021
        },
        {
            "authors": [
                "Gabriel Orlanski",
                "Kefan Xiao",
                "Xavier Garcia",
                "Jeffrey Hui",
                "Joshua Howland",
                "Jonathan Malmaud",
                "Jacob Austin",
                "Rishah Singh",
                "Michele Catasta"
            ],
            "title": "Measuring the impact of programming language distribution",
            "year": 1973
        },
        {
            "authors": [
                "Long Ouyang",
                "Jeff Wu",
                "Xu Jiang",
                "Diogo Almeida",
                "Carroll L. Wainwright",
                "Pamela Mishkin",
                "Chong Zhang",
                "Sandhini Agarwal",
                "Katarina Slama",
                "Alex Ray"
            ],
            "title": "Training language models to follow instructions with human feedback",
            "venue": "In Conference on Neural Information Processing Systems (NeurIPS),",
            "year": 2022
        },
        {
            "authors": [
                "Kishore Papineni",
                "Salim Roukos",
                "Todd Ward",
                "Wei-Jing Zhu"
            ],
            "title": "Bleu: a method for automatic evaluation of machine translation",
            "venue": "In Proceedings of the 40th annual meeting of the Association for Computational Linguistics,",
            "year": 2002
        },
        {
            "authors": [
                "Bo Peng",
                "Eric Alcaide",
                "Quentin Anthony",
                "Alon Albalak",
                "Samuel Arcadinho",
                "Huanqi Cao",
                "Xin Cheng",
                "Michael Chung",
                "Matteo Grella",
                "Kranthi Kiran GV"
            ],
            "title": "Rwkv: Reinventing rnns for the transformer era",
            "venue": "arXiv preprint arXiv:2305.13048,",
            "year": 2023
        },
        {
            "authors": [
                "Ethan Perez",
                "Douwe Kiela",
                "Kyunghyun Cho"
            ],
            "title": "True few-shot learning with language models",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Luiza Amador Pozzobon",
                "Beyza Ermis",
                "Patrick Lewis",
                "Sara Hooker"
            ],
            "title": "On the challenges of using black-box apis for toxicity evaluation in research",
            "venue": "In ICLR 2023 Workshop on Trustworthy and Reliable Large-Scale Machine Learning Models,",
            "year": 2023
        },
        {
            "authors": [
                "Julian Aron Prenner",
                "Romain Robbes"
            ],
            "title": "Automatic program repair with openai\u2019s codex: Evaluating quixbugs",
            "venue": "arXiv preprint arXiv:2111.03922,",
            "year": 2021
        },
        {
            "authors": [
                "Julian Aron Prenner",
                "Romain Robbes"
            ],
            "title": "Runbugrun\u2013an executable dataset for automated program repair",
            "venue": "arXiv preprint arXiv:2304.01102,",
            "year": 2023
        },
        {
            "authors": [
                "Ofir Press",
                "Noah A Smith",
                "Mike Lewis"
            ],
            "title": "Train short, test long: Attention with linear biases enables input length extrapolation",
            "venue": "arXiv preprint arXiv:2108.12409,",
            "year": 2021
        },
        {
            "authors": [
                "Rafael Rafailov",
                "Archit Sharma",
                "Eric Mitchell",
                "Christopher D Manning",
                "Stefano Ermon",
                "Chelsea Finn"
            ],
            "title": "Direct preference optimization: Your language model is secretly a reward model",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2024
        },
        {
            "authors": [
                "Vipul Raheja",
                "Dhruv Kumar",
                "Ryan Koo",
                "Dongyeop Kang"
            ],
            "title": "Coedit: Text editing by task-specific instruction tuning",
            "venue": "arXiv preprint arXiv:2305.09857,",
            "year": 2023
        },
        {
            "authors": [
                "Machel Reid",
                "Graham Neubig"
            ],
            "title": "Learning to model editing processes",
            "venue": "arXiv preprint arXiv:2205.12374,",
            "year": 2022
        },
        {
            "authors": [
                "Ehud Reiter"
            ],
            "title": "A structured review of the validity of bleu",
            "venue": "Computational Linguistics,",
            "year": 2018
        },
        {
            "authors": [
                "Victor Sanh",
                "Albert Webson",
                "Colin Raffel",
                "Stephen Bach",
                "Lintang Sutawika",
                "Zaid Alyafeai",
                "Antoine Chaffin",
                "Arnaud Stiegler",
                "Teven Le Scao",
                "Arun Raja"
            ],
            "title": "Multitask prompted training enables zero-shot task generalization",
            "venue": "International Conference on Learning Representations (ICLR),",
            "year": 2022
        },
        {
            "authors": [
                "Teven Le Scao",
                "Angela Fan",
                "Christopher Akiki",
                "Ellie Pavlick",
                "Suzana Ili\u0107",
                "Daniel Hesslow",
                "Roman Castagn\u00e9",
                "Alexandra Sasha Luccioni",
                "Fran\u00e7ois Yvon",
                "Matthias Gall\u00e9"
            ],
            "title": "Bloom: A 176bparameter open-access multilingual language model",
            "venue": "arXiv preprint arXiv:2211.05100,",
            "year": 2022
        },
        {
            "authors": [
                "Teven Le Scao",
                "Thomas Wang",
                "Daniel Hesslow",
                "Lucile Saulnier",
                "Stas Bekman",
                "M Saiful Bari",
                "Stella Bideman",
                "Hady Elsahar",
                "Niklas Muennighoff",
                "Jason Phang"
            ],
            "title": "What language model to train if you have one million gpu hours",
            "venue": "arXiv preprint arXiv:2210.15424,",
            "year": 2022
        },
        {
            "authors": [
                "Timo Schick",
                "Jane Dwivedi-Yu",
                "Zhengbao Jiang",
                "Fabio Petroni",
                "Patrick Lewis",
                "Gautier Izacard",
                "Qingfei You",
                "Christoforos Nalmpantis",
                "Edouard Grave",
                "Sebastian Riedel"
            ],
            "title": "Peer: A collaborative language model",
            "venue": "arXiv preprint arXiv:2208.11663,",
            "year": 2022
        },
        {
            "authors": [
                "Natalie Schluter"
            ],
            "title": "The limits of automatic summarisation according to rouge",
            "venue": "In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics,",
            "year": 2017
        },
        {
            "authors": [
                "Noam M. Shazeer"
            ],
            "title": "Fast transformer decoding: One write-head is all you need",
            "venue": "arXiv preprint arXiv:1911.02150,",
            "year": 2019
        },
        {
            "authors": [
                "Bo Shen",
                "Jiaxin Zhang",
                "Taihong Chen",
                "Daoguang Zan",
                "Bing Geng",
                "An Fu",
                "Muhan Zeng",
                "Ailun Yu",
                "Jichuan Ji",
                "Jingyang Zhao",
                "Yuenan Guo",
                "Qianxiang Wang"
            ],
            "title": "Pangu-coder2: Boosting large language models for code with ranking feedback, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Ensheng Shi",
                "Yanlin Wang",
                "Lun Du",
                "Junjie Chen",
                "Shi Han",
                "Hongyu Zhang",
                "Dongmei Zhang",
                "Hongbin Sun"
            ],
            "title": "On the evaluation of neural code summarization",
            "venue": "In Proceedings of the 44th International Conference on Software Engineering,",
            "year": 2022
        },
        {
            "authors": [
                "Disha Shrivastava",
                "Denis Kocetkov",
                "Harm de Vries",
                "Dzmitry Bahdanau",
                "Torsten"
            ],
            "title": "Scholak. Repofusion: Training code models to understand your repository",
            "venue": "arXiv preprint arXiv:2306.10998,",
            "year": 2023
        },
        {
            "authors": [
                "Disha Shrivastava",
                "Hugo Larochelle",
                "Daniel Tarlow"
            ],
            "title": "Repository-level prompt generation for large language models of code",
            "venue": "In International Conference on Machine Learning,",
            "year": 2023
        },
        {
            "authors": [
                "Shivalika Singh",
                "Freddie Vargus",
                "Daniel Dsouza",
                "B\u00f6rje F Karlsson",
                "Abinaya Mahendiran",
                "Wei-Yin Ko",
                "Herumb Shandilya",
                "Jay Patel",
                "Deividas Mataciunas",
                "Laura OMahony"
            ],
            "title": "Aya dataset: An open-access collection for multilingual instruction tuning",
            "venue": "arXiv preprint arXiv:2402.06619,",
            "year": 2024
        },
        {
            "authors": [
                "Marta Skreta",
                "Naruki Yoshikawa",
                "Sebastian Arellano-Rubach",
                "Zhi Ji",
                "Lasse Bj\u00f8rn Kristensen",
                "Kourosh Darvish",
                "Al\u00e1n Aspuru-Guzik",
                "Florian Shkurti",
                "Animesh Garg"
            ],
            "title": "Errors are useful prompts: Instruction guided task programming with verifier-assisted iterative prompting",
            "venue": "arXiv preprint arXiv:2303.14100,",
            "year": 2023
        },
        {
            "authors": [
                "Dominik Sobania",
                "Martin Briesch",
                "Carol Hanna",
                "Justyna Petke"
            ],
            "title": "An analysis of the automatic bug fixing performance of chatgpt",
            "venue": "arXiv preprint arXiv:2301.08653,",
            "year": 2023
        },
        {
            "authors": [
                "Luke Zettlemoyer",
                "Noah A. Smith",
                "Hanna Hajishirzi",
                "Iz Beltagy",
                "Dirk Groeneveld",
                "Jesse Dodge",
                "Kyle Lo"
            ],
            "title": "Dolma: an open corpus of three trillion tokens for language model pretraining research",
            "year": 2024
        },
        {
            "authors": [
                "Aarohi Srivastava",
                "Abhinav Rastogi",
                "Abhishek Rao",
                "Abu Awal Md Shoeb",
                "Abubakar Abid",
                "Adam Fisch",
                "Adam R Brown",
                "Adam Santoro",
                "Aditya Gupta",
                "Adri\u00e0 Garriga-Alonso"
            ],
            "title": "Beyond the imitation game: Quantifying and extrapolating the capabilities of language models, 2022",
            "venue": "URL https://arxiv.org/abs/2206.04615",
            "year": 2022
        },
        {
            "authors": [
                "Simeng Sun",
                "Kalpesh Krishna",
                "Andrew Mattarella-Micke",
                "Mohit Iyyer"
            ],
            "title": "Do long-range language models actually use long-range context? ArXiv",
            "venue": "URL https://api. semanticscholar.org/CorpusID:237572264",
            "year": 2021
        },
        {
            "authors": [
                "Rohan Taori",
                "Ishaan Gulrajani",
                "Tianyi Zhang",
                "Yann Dubois",
                "Xuechen Li",
                "Carlos Guestrin",
                "Percy Liang",
                "Tatsunori B. Hashimoto"
            ],
            "title": "Stanford alpaca: An instruction-following llama model",
            "venue": "https://github.com/tatsu-lab/stanford_alpaca,",
            "year": 2023
        },
        {
            "authors": [
                "Ross Taylor",
                "Marcin Kardas",
                "Guillem Cucurull",
                "Thomas Scialom",
                "Anthony Hartshorn",
                "Elvis Saravia",
                "Andrew Poulton",
                "Viktor Kerkez",
                "Robert Stojnic"
            ],
            "title": "Galactica: A large language model for science",
            "venue": "arXiv preprint arXiv:2211.09085,",
            "year": 2022
        },
        {
            "authors": [
                "Hugo Touvron",
                "Louis Martin",
                "Kevin Stone",
                "Peter Albert",
                "Amjad Almahairi",
                "Yasmine Babaei",
                "Nikolay Bashlykov",
                "Soumya Batra",
                "Prajjwal Bhargava",
                "Shruti Bhosale"
            ],
            "title": "Llama 2: Open foundation and fine-tuned chat models",
            "venue": "arXiv preprint arXiv:2307.09288,",
            "year": 2023
        },
        {
            "authors": [
                "Lewis Tunstall",
                "Nathan Lambert",
                "Nazneen Rajani",
                "Edward Beeching",
                "Teven Le Scao",
                "Leandro von Werra",
                "Sheon Han",
                "Philipp Schmid",
                "Alexander Rush"
            ],
            "title": "Creating a coding assistant with starcoder",
            "venue": "Hugging Face Blog,",
            "year": 2023
        },
        {
            "authors": [
                "Ahmet \u00dcst\u00fcn",
                "Viraat Aryabumi",
                "Zheng-Xin Yong",
                "Wei-Yin Ko",
                "Daniel D\u2019souza",
                "Gbemileke Onilude",
                "Neel Bhandari",
                "Shivalika Singh",
                "Hui-Lee Ooi",
                "Amr Kayid"
            ],
            "title": "Aya model: An instruction finetuned open-access multilingual language model",
            "venue": "arXiv preprint arXiv:2402.07827,",
            "year": 2024
        },
        {
            "authors": [
                "Alex Wang",
                "Yada Pruksachatkun",
                "Nikita Nangia",
                "Amanpreet Singh",
                "Julian Michael",
                "Felix Hill",
                "Omer Levy",
                "Samuel Bowman"
            ],
            "title": "SuperGLUE: A stickier benchmark for general-purpose language understanding systems",
            "venue": "Conference on Neural Information Processing Systems (NeurIPS),",
            "year": 2019
        },
        {
            "authors": [
                "Ben Wang",
                "Aran Komatsuzaki"
            ],
            "title": "Gpt-j-6b: A 6 billion parameter autoregressive language model, 2021",
            "year": 2021
        },
        {
            "authors": [
                "Xuezhi Wang",
                "Jason Wei",
                "Dale Schuurmans",
                "Quoc V Le",
                "Ed H. Chi",
                "Sharan Narang",
                "Aakanksha Chowdhery",
                "Denny Zhou"
            ],
            "title": "Self-consistency improves chain of thought reasoning in language models",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2023
        },
        {
            "authors": [
                "Yizhong Wang",
                "Yeganeh Kordi",
                "Swaroop Mishra",
                "Alisa Liu",
                "Noah A Smith",
                "Daniel Khashabi",
                "Hannaneh Hajishirzi"
            ],
            "title": "Self-instruct: Aligning language model with self generated instructions",
            "venue": "arXiv preprint arXiv:2212.10560,",
            "year": 2022
        },
        {
            "authors": [
                "Yizhong Wang",
                "Swaroop Mishra",
                "Pegah Alipoormolabashi",
                "Yeganeh Kordi",
                "Amirreza Mirzaei",
                "Anjana Arunkumar",
                "Arjun Ashok",
                "Arut Selvan Dhanasekaran",
                "Atharva Naik",
                "David Stap"
            ],
            "title": "Super-naturalinstructions: Generalization via declarative instructions on",
            "venue": "nlp tasks. arXiv preprint arXiv:2204.07705,",
            "year": 2022
        },
        {
            "authors": [
                "Yizhong Wang",
                "Hamish Ivison",
                "Pradeep Dasigi",
                "Jack Hessel",
                "Tushar Khot",
                "Khyathi Raghavi Chandu",
                "David Wadden",
                "Kelsey MacMillan",
                "Noah A Smith",
                "Iz Beltagy"
            ],
            "title": "How far can camels go? exploring the state of instruction tuning on open resources",
            "venue": "arXiv preprint arXiv:2306.04751,",
            "year": 2023
        },
        {
            "authors": [
                "Yue Wang",
                "Hung Le",
                "Akhilesh Deepak Gotmare",
                "Nghi DQ Bui",
                "Junnan Li",
                "Steven CH Hoi"
            ],
            "title": "Codet5+: Open code large language models for code understanding and generation",
            "venue": "arXiv preprint arXiv:2305.07922,",
            "year": 2023
        },
        {
            "authors": [
                "Zhiruo Wang",
                "Grace Cuenca",
                "Shuyan Zhou",
                "Frank F Xu",
                "Graham Neubig"
            ],
            "title": "Mconala: a benchmark for code generation from multiple natural languages",
            "venue": "arXiv preprint arXiv:2203.08388,",
            "year": 2022
        },
        {
            "authors": [
                "Zhiruo Wang",
                "Shuyan Zhou",
                "Daniel Fried",
                "Graham Neubig"
            ],
            "title": "Execution-based evaluation for open-domain code generation",
            "venue": "arXiv preprint arXiv:2212.10481,",
            "year": 2022
        },
        {
            "authors": [
                "Jason Wei",
                "Maarten Bosma",
                "Vincent Y. Zhao",
                "Kelvin Guu",
                "Adams Wei Yu",
                "Brian Lester",
                "Nan Du",
                "Andrew M. Dai",
                "Quoc V. Le"
            ],
            "title": "Finetuned language models are zero-shot learners",
            "venue": "International Conference on Learning Representations (ICLR),",
            "year": 2022
        },
        {
            "authors": [
                "Jiayi Wei",
                "Greg Durrett",
                "Isil Dillig"
            ],
            "title": "Coeditor: Leveraging contextual changes for multi-round code auto-editing",
            "venue": "arXiv preprint arXiv:2305.18584,",
            "year": 2023
        },
        {
            "authors": [
                "Minghao Wu",
                "Alham Fikri Aji"
            ],
            "title": "Style over substance: Evaluation biases for large language models",
            "venue": "arXiv preprint arXiv:2307.03025,",
            "year": 2023
        },
        {
            "authors": [
                "Chunqiu Steven Xia",
                "Lingming Zhang"
            ],
            "title": "Conversational automated program repair",
            "venue": "arXiv preprint arXiv:2301.13246,",
            "year": 2023
        },
        {
            "authors": [
                "Mengzhou Xia",
                "Mikel Artetxe",
                "Chunting Zhou",
                "Xi Victoria Lin",
                "Ramakanth Pasunuru",
                "Danqi Chen",
                "Luke Zettlemoyer",
                "Ves Stoyanov"
            ],
            "title": "Training trajectories of language models across scales",
            "venue": "arXiv preprint arXiv:2212.09803,",
            "year": 2022
        },
        {
            "authors": [
                "Can Xu",
                "Qingfeng Sun",
                "Kai Zheng",
                "Xiubo Geng",
                "Pu Zhao",
                "Jiazhan Feng",
                "Chongyang Tao",
                "Daxin Jiang"
            ],
            "title": "Wizardlm: Empowering large language models to follow complex instructions",
            "venue": "arXiv preprint arXiv:2304.12244,",
            "year": 2023
        },
        {
            "authors": [
                "Frank F Xu",
                "Uri Alon",
                "Graham Neubig",
                "Vincent Josua Hellendoorn"
            ],
            "title": "A systematic evaluation of large language models of code",
            "venue": "In Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming,",
            "year": 2022
        },
        {
            "authors": [
                "Shengbin Xu",
                "Yuan Yao",
                "Feng Xu",
                "Tianxiao Gu",
                "Hanghang Tong"
            ],
            "title": "Combining code context and fine-grained code difference for commit message generation",
            "venue": "In Proceedings of the 13th Asia-Pacific Symposium on Internetware,",
            "year": 2022
        },
        {
            "authors": [
                "Zhiyang Xu",
                "Ying Shen",
                "Lifu Huang"
            ],
            "title": "Multiinstruct: Improving multi-modal zero-shot learning via instruction tuning, 2023b",
            "year": 2023
        },
        {
            "authors": [
                "Michihiro Yasunaga",
                "Percy Liang"
            ],
            "title": "Break-it-fix-it: Unsupervised learning for program repair",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "He Ye",
                "Matias Martinez",
                "Thomas Durieux",
                "Martin Monperrus"
            ],
            "title": "A comprehensive study of automatic program repair on the quixbugs benchmark",
            "venue": "Journal of Systems and Software,",
            "year": 2021
        },
        {
            "authors": [
                "Burak Yetistiren",
                "Isik Ozsoy",
                "Eray Tuzun"
            ],
            "title": "Assessing the quality of github copilot\u2019s code generation",
            "venue": "In Proceedings of the 18th International Conference on Predictive Models and Data Analytics in Software Engineering,",
            "year": 2022
        },
        {
            "authors": [
                "Pengcheng Yin",
                "Bowen Deng",
                "Edgar Chen",
                "Bogdan Vasilescu",
                "Graham Neubig"
            ],
            "title": "Learning to mine aligned code and natural language pairs from stack overflow",
            "venue": "In International Conference on Mining Software Repositories,",
            "year": 2018
        },
        {
            "authors": [
                "Zheng-Xin Yong",
                "Hailey Schoelkopf",
                "Niklas Muennighoff",
                "Alham Fikri Aji",
                "David Ifeoluwa Adelani",
                "Khalid Almubarak",
                "M Saiful Bari",
                "Lintang Sutawika",
                "Jungo Kasai",
                "Ahmed Baruwa"
            ],
            "title": "Bloom+ 1: Adding language support to bloom for zero-shot prompting",
            "venue": "arXiv preprint arXiv:2212.09535,",
            "year": 2022
        },
        {
            "authors": [
                "Hao Yu",
                "Bo Shen",
                "Dezhi Ran",
                "Jiaxin Zhang",
                "Qi Zhang",
                "Yuchi Ma",
                "Guangtai Liang",
                "Ying Li",
                "Tao Xie",
                "Qianxiang Wang"
            ],
            "title": "Codereval: A benchmark of pragmatic code generation with generative pre-trained models",
            "venue": "arXiv preprint arXiv:2302.00288,",
            "year": 2023
        },
        {
            "authors": [
                "Yan Zeng",
                "Hanbo Zhang",
                "Jiani Zheng",
                "Jiangnan Xia",
                "Guoqiang Wei",
                "Yang Wei",
                "Yuchen Zhang",
                "Tao Kong"
            ],
            "title": "What matters in training a gpt4-style language model with multimodal inputs",
            "venue": "arXiv preprint arXiv:2307.02469,",
            "year": 2023
        },
        {
            "authors": [
                "Chunyan Zhang",
                "Junchao Wang",
                "Qinglei Zhou",
                "Ting Xu",
                "Ke Tang",
                "Hairen Gui",
                "Fudong Liu"
            ],
            "title": "A survey of automatic source code",
            "venue": "summarization. Symmetry,",
            "year": 2022
        },
        {
            "authors": [
                "Fengji Zhang",
                "Bei Chen",
                "Yue Zhang",
                "Jin Liu",
                "Daoguang Zan",
                "Yi Mao",
                "Jian-Guang Lou",
                "Weizhu Chen"
            ],
            "title": "Repocoder: Repository-level code completion through iterative retrieval and generation",
            "venue": "arXiv preprint arXiv:2303.12570,",
            "year": 2023
        },
        {
            "authors": [
                "Hang Zhang",
                "Xin Li",
                "Lidong Bing"
            ],
            "title": "Video-llama: An instruction-tuned audio-visual language model for video understanding",
            "venue": "arXiv preprint arXiv:2306.02858,",
            "year": 2023
        },
        {
            "authors": [
                "Jialu Zhang",
                "Jos\u00e9 Cambronero",
                "Sumit Gulwani",
                "Vu Le",
                "Ruzica Piskac",
                "Gustavo Soares",
                "Gust Verbruggen"
            ],
            "title": "Repairing bugs in python assignments using large language models",
            "venue": "arXiv preprint arXiv:2209.14876,",
            "year": 2022
        },
        {
            "authors": [
                "Jiyang Zhang",
                "Sheena Panthaplackel",
                "Pengyu Nie",
                "Junyi Jessy Li",
                "Milos Gligoric"
            ],
            "title": "Coditt5: Pretraining for source code and natural language editing",
            "venue": "In 37th IEEE/ACM International Conference on Automated Software Engineering,",
            "year": 2022
        },
        {
            "authors": [
                "Tianyi Zhang",
                "Tao Yu",
                "Tatsunori Hashimoto",
                "Mike Lewis",
                "Wen-tau Yih",
                "Daniel Fried",
                "Sida Wang"
            ],
            "title": "Coder reviewer reranking for code generation",
            "venue": "In International Conference on Machine Learning,",
            "year": 2023
        },
        {
            "authors": [
                "Qinkai Zheng",
                "Xiao Xia",
                "Xu Zou",
                "Yuxiao Dong",
                "Shan Wang",
                "Yufei Xue",
                "Zihan Wang",
                "Lei Shen",
                "Andi Wang",
                "Yang Li"
            ],
            "title": "Codegeex: A pre-trained model for code generation with multilingual evaluations on humaneval-x",
            "venue": "arXiv preprint arXiv:2303.17568,",
            "year": 2023
        },
        {
            "authors": [
                "Chunting Zhou",
                "Pengfei Liu",
                "Puxin Xu",
                "Srini Iyer",
                "Jiao Sun",
                "Yuning Mao",
                "Xuezhe Ma",
                "Avia Efrat",
                "Ping Yu",
                "Lili Yu"
            ],
            "title": "Lima: Less is more for alignment",
            "venue": "arXiv preprint arXiv:2305.11206,",
            "year": 2023
        },
        {
            "authors": [
                "Shuyan Zhou",
                "Uri Alon",
                "Sumit Agarwal",
                "Graham Neubig"
            ],
            "title": "Codebertscore: Evaluating code generation with pretrained models of code",
            "venue": "arXiv preprint arXiv:2302.05527,",
            "year": 2023
        },
        {
            "authors": [
                "Yongchao Zhou",
                "Andrei Ioan Muresanu",
                "Ziwen Han",
                "Keiran Paster",
                "Silviu Pitis",
                "Harris Chan",
                "Jimmy Ba"
            ],
            "title": "Large language models are human-level prompt engineers",
            "year": 1910
        },
        {
            "authors": [
                "Ming Zhu",
                "Aneesh Jain",
                "Karthik Suresh",
                "Roshan Ravindran",
                "Sindhu Tipirneni",
                "Chandan K Reddy"
            ],
            "title": "Xlcost: A benchmark dataset for cross-lingual code intelligence",
            "venue": "arXiv preprint arXiv:2206.08474,",
            "year": 2022
        },
        {
            "authors": [
                "Terry Yue Zhuo"
            ],
            "title": "Large language models are state-of-the-art evaluators of code generation",
            "venue": "arXiv preprint arXiv:2304.14317,",
            "year": 2023
        },
        {
            "authors": [
                "Terry Yue Zhuo",
                "Armel Zebaze",
                "Nitchakarn Suppattarachai",
                "Leandro von Werra",
                "Harm de Vries",
                "Qian Liu",
                "Niklas Muennighoff"
            ],
            "title": "Astraios: Parameter-efficient instruction tuning code large language models",
            "venue": "arXiv preprint arXiv:2401.00788,",
            "year": 2024
        }
    ],
    "sections": [
        {
            "text": ""
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "Finetuning large language models (LLMs) on a variety of language tasks explained via instructions (instruction tuning) has been shown to improve model usability and general performance (Wei et al., 2022; Sanh et al., 2022; Min et al., 2022; Ouyang et al., 2022). The instruction tuning paradigm has also proven successful for models trained on visual (Liu et al., 2023a; Li et al., 2023a), audio (Zhang et al., 2023b) and multilingual (Muennighoff et al., 2022b; Wang et al., 2022b) data.\nIn this work, we instruction tune LLMs on the coding modality. While Code LLMs can already be indirectly instructed to generate desired code using code comments, this procedure is brittle and does not work when the desired output is natural language, such as explaining code. Explicit instructing tuning of Code LLMs may improve their steerability and enable their application to more tasks. Concurrently to our work, three instruction tuned Code LLMs have been proposed: PanGu-Coder2 (Shen et al., 2023), WizardCoder (Luo et al., 2023) and InstructCodeT5+ (Wang et al., 2023c). These models rely on more capable and closed models from the OpenAI API1 to create their instruction training data. This approach is problematic as (1) closed-source APIs keep changing and have unpredictable availability (Pozzobon et al., 2023; Chen et al., 2023a), (2) it relies on the assumption that a more capable model exists (3) it can reinforce model hallucination (Gudibande et al., 2023) and (4), depending on legal interpretation, OpenAI\u2019s terms of use2 forbid such models: \u201c...You may not...use output from the Services to develop models that compete with OpenAI...\u201d. Thus, we consider models trained on OpenAI outputs not usable for commercial purposes in practice and classify them as non-permissive in this work.\nWe focus on more permissively licensed data and avoid using a closed-source model to generate synthetic data. We benchmark four popular sources of code instruction data: (1) xP3x (Muennighoff et al., 2022b), which contains data from common code benchmarks, (2) Self-Instruct (Wang et al., 2023a) data we create using a permissive Code LLM, (3) OASST (K\u00f6pf et al., 2023), which contains mostly natural language data and few code examples and (4) COMMITPACK, our new 4TB dataset of Git commits. Instruction tuning\u2019s primary purpose is to expand models\u2019 generalization abilities to a wide variety of tasks and settings. Thus, we extend the code synthesis benchmark, HumanEval (Chen et al., 2021; Zheng et al., 2023), to create HUMANEVALPACK: A code benchmark covering code synthesis, code repair, and code explanation across six programming languages.\nInstruction tuning StarCoder (Li et al., 2023b) on a filtered variant of COMMITPACK and OASST leads to our best model, OCTOCODER, which surpasses all other openly licensed models (Figure 1), but falls short of the much larger GPT-4 (OpenAI, 2023). GPT-4 is close to maximum performance on the code synthesis variant, notably with a pass@1 score of 86.6% on Python HumanEval. However, it performs significantly worse on the code fixing and explanation variants of HUMANEVALPACK, which we introduce. This suggests that the original HumanEval benchmark may soon cease to be useful due to models reaching close to the maximum performance. Our more challenging evaluation variants provide room for future LLMs to improve on the performance of the current state-of-the-art.\nIn summary, we contribute:\n\u2022 COMMITPACK and COMMITPACKFT: 4TB of permissively licensed code commits across 350 programming languages for pretraining and a filtered 2GB variant containing highquality code instructions used for finetuning\n\u2022 HUMANEVALPACK: A benchmark for Code LLM generalization, spanning three scenarios (Code Repair, Code Explanation, Code Synthesis) and 6 programming languages (Python, JavaScript, Java, Go, C++, Rust)\n\u2022 OCTOCODER and OCTOGEEX: The best permissive Code LLMs"
        },
        {
            "heading": "2 COMMITPACK: CODE INSTRUCTION DATA",
            "text": "Prior work has shown that models can generalize to languages included in pretraining, but absent during instruction tuning (Muennighoff et al., 2022b). However, they also show that including such\n1 https://openai.com/blog/openai-api 2 https://openai.com/policies/terms-of-use\nlanguages during instruction tuning boosts their performance further. We hypothesize that code data exhibits the same behavior. To improve performance on code-related tasks, we thus construct a code instruction dataset leveraging the natural structure of Git commits.\nCOMMITPACK To create the dataset, we use commit metadata from the GitHub action dump on Google BigQuery.3 We apply quality filters, filter for commercially friendly licenses, and discard commits that affect more than a single file to ensure commit messages are very specific and to avoid additional complexity from dealing with multiple files. We use the filtered metadata to scrape the affected code files prior to and after the commit from GitHub. This leads to almost 4 terabytes of data covering 350 programming languages (COMMITPACK). As instruction tuning does not require so much data (Zhou et al., 2023a; Touvron et al., 2023), we apply several strict filters to\n3 https://www.gharchive.org/\nreduce the dataset to 2 gigabytes and 277 languages (COMMITPACKFT). These include filtering for samples where the commit message has specific words in uppercase imperative form at the start (e.g. \"Verify ...\"), consists of multiple words, and does not contain external references. All filters are detailed in Appendix D. Figure 2 depicts the distribution of both datasets and the tasks contained in COMMITPACKFT. For instruction tuning our models, we select 5,000 random samples from COMMITPACKFT across the 6 programming languages that we evaluate on. In Appendix G, we also experiment with pretraining on the entirety of COMMITPACK.\nAlternatives We consider three additional datasets for instruction tuning presented in Table 1. xP3x: xP3x is a large-scale collection of multilingual instruction data with around 532 million samples (Muennighoff et al., 2022b). We focus only on the code subset of xP3x, excluding NeuralCodeSearch (Li et al., 2019) which is not licensed permissively, and select 5,000 samples. Self-Instruct: Using the Self-Instruct method (Wang et al., 2022a) and the StarCoder model (Li et al., 2023b), we create 5,003 synthetic instructions and corresponding answers. OASST: OASST is a diverse dataset of multi-turn chat dialogues (K\u00f6pf et al., 2023). Only a few of the dialogues contain code. We reuse a filtered variant from prior work (Dettmers et al., 2023) and additionally filter out moralizing assistant answers (Appendix D) leading to 8,587 samples."
        },
        {
            "heading": "3 HUMANEVALPACK: EVALUATING INSTRUCTION TUNED CODE MODELS",
            "text": "When instruction tuning LLMs using natural language (NL) data, the input is an NL instruction with optional NL context and the target output is the NL answer to the task (Wei et al., 2022). When instruction tuning with code (C) data, code may either appear only in the input alongside the NL instruction (NL+C!NL, e.g. code explanation), only in the output (NL!C, e.g. code synthesis), or in both input and output (NL+C!C, e.g. code modifications like bug fixing). While prior benchmarks commonly only cover variants of code synthesis, users may want to use models in all three scenarios. Thus, we expand the code synthesis benchmark HumanEval (Chen et al., 2021; Zheng et al., 2023) to cover all three input-output combinations for six languages (Figure 3).\nHUMANEVALFIX (NL+C!C) Given an incorrect code function with a subtle bug and accompanying unit tests, the model is tasked to fix the function. We manually add a bug to each of the 164 HumanEval solutions across all 6 languages (984 total bugs). For a given sample, the bugs are as similar as possible across the 6 languages enabling meaningful comparison of scores across languages. Bugs are written such that the code still runs but produces an incorrect result leading to at least one unit test failing. Bug statistics and examples are in Appendix L. We also evaluate an easier variant of this task where instead of unit tests, models are provided with the correct function docstring as the source of truth to fix bugs, see Appendix K.\nHUMANEVALEXPLAIN (NL+C!NL) Given a correct code function, the model is tasked to generate an explanation of the code. Subsequently, the same model is tasked to regenerate the code given only its own explanation. The second step allows us to score this task via code execution and measure pass@k (Chen et al., 2021) instead of evaluating the explanation itself using heuristic-based metrics like BLEU (Papineni et al., 2002) or ROUGE (Lin, 2004) which have major limitations (Reiter, 2018; Schluter, 2017; Eghbali & Pradel, 2022; Zhou et al., 2023b). To prevent models from copying the solution into the description, we remove any solution overlap of at least 20 characters from the description. We further enforce a character length limit on the model-generated explanation equivalent to the length of the docstring describing the function. This limit is specified in the prompt for the model. Note that the function docstring itself is never provided to the model for this task.\nHUMANEVALSYNTHESIZE (NL!C) Given a natural language docstring or comment describing the desired code, the model is tasked to synthesize the correct code. This task corresponds to the original HumanEval benchmark (Chen et al., 2021). For instruction tuned models, we add an explicit instruction to the input explaining what the model should do. For models that have only gone through language model pretraining, we follow Chen et al. (2021) and provide the model with the function header and docstring to evaluate its completion of the function.\nFor all tasks we execute the code generations to compute performance using the pass@k metric (Chen et al., 2021): a problem is considered solved if any of k code generations passes every test case. We focus on the simplest version of pass@k, which is pass@1: the likelihood that the model solves a problem in a single attempt. Like Chen et al. (2021), we use a sampling temperature of 0.2 and topp = 0.95 to estimate pass@1. We generate n = 20 samples, which is enough to get reliable pass@1 estimates (Li et al., 2023b). For GPT-4, we generate n = 1 samples. Using n = 1 instead of n = 20 for GPT-4 only changed scores from 75.0% to 75.2% pass@1 on HUMANEVALSYNTHESIZE Python while providing 20x cost savings.\nPython HumanEval is the most widely used code benchmark and many training datasets have already been decontaminated for it (Kocetkov et al., 2022). By manually extending HumanEval, we ensure existing decontamination remains valid to enable fair evaluation. However, this may not hold for all models (e.g. GPT-4), thus results should be interpreted carefully."
        },
        {
            "heading": "4 OCTOCODER: BEST COMMERCIALLY LICENSED CODE LLM",
            "text": ""
        },
        {
            "heading": "4.1 ABLATING INSTRUCTION DATA CHOICES",
            "text": "We instruction tune the pretrained StarCoder model (Li et al., 2023b) on different combinations of our instruction datasets (\u00a72). We evaluate all models on the Python subset of HUMANEVALPACK as depicted in Figure 4. Similar to prior work (Taori et al., 2023), we format all instructions into a consistent schema to distinguish question and answer (see Figure 18).\nCOMMITPACKFT enables CodeLLMs to fix bugs COMMITPACKFT is critical for the performance boost on code repair (HUMANEVALFIX), where instruction tuning on only OASST or other variants results in a significantly lower score. This is likely due to COMMITPACKFT including around 20% of bug fixes among other code-related tasks (Figure 2).\nImportance of samples with natural language targets The pretrained StarCoder model, as well as the Self-Instruct variant, perform poorly on code explanation (HUMANEVALEXPLAIN). This is because both models are only conditioned to write code instead of natural language. We find that to\nperform well at explaining code, it is necessary to include samples with natural language as the target output during instruction tuning. Only relying on data with code as the target, such as the Self-Instruct data, will lead to models always outputting code even if the question requires a natural language output. Thus, we mix all other ablations with OASST, which contains many natural language targets. While the xP3x subset also contains samples with natural language output, many of its target outputs are short, which leads to models with a bias for short answers. This is impractical for the explanation task leading to the comparatively low score of mixing xP3x with OASST.\nCOMMITPACKFT+OASST yields best performance All instruction datasets provide similar boosts for code synthesis (HUMANEVALSYNTHESIZE), which has been the focus of all prior work on code instruction models (Wang et al., 2023c; Luo et al., 2023; Muennighoff et al., 2022b). We achieve the best average score by instruction tuning on COMMITPACKFT mixed with our filtered OASST data yielding an absolute 23% improvement over StarCoder. Thus, we select COMMITPACKFT+OASST for our final model dubbed OCTOCODER. Using the same data, we also instruction tune the 6 billion parameter CodeGeeX2 (Zheng et al., 2023) to create OCTOGEEX. Training hyperparameters for both models are in Appendix P."
        },
        {
            "heading": "4.2 COMPARING WITH OTHER MODELS",
            "text": "We benchmark OCTOCODER and OCTOGEEX with state-of-the-art Code LLMs on HUMANEVALPACK in Table 2. For all models, we use the prompt put forward by the model creators if applicable or else a simple intuitive prompt, see Appendix Q.\nOCTOCODER performs best among permissive models OCTOCODER has the highest average score across all three evaluation scenarios among all permissive models. With just 6 billion parameters, OCTOGEEX is the smallest model benchmarked, but still outperforms all prior permissive Code LLMs. GPT-4 (OpenAI, 2023) performs best among all models benchmarked with a significant margin. However, GPT-4 is closed-source and likely much larger than all other models evaluated.\nInstruction tuning generalizes to unseen programming languages Trained primarily on natural language, not code, BLOOMZ (Muennighoff et al., 2022b) performs worse than other models despite having 176 billion parameters. Go and Rust are not contained in BLOOMZ\u2019s instruction data, yet it performs much better than the random baseline of 0.0 for these two languages across most tasks. This confirms our hypothesis that models are capable of generalizing instructions to programming languages only seen at pretraining, similar to crosslingual generalization for natural languages (Muennighoff et al., 2022b). To improve programming language generalization further, we tune OCTOCODER and OCTOGEEX on many languages from COMMITPACKFT, and this generalization improvement is reflected in the performance on HUMANEVALPACK\u2019s new languages.\nPretraining weight correlates with programming language performance after instruction tuning Prior work has shown that the performance on natural languages after instruction tuning is correlated with the weight of these languages during pretraining (Muennighoff et al., 2022b). The more weight during pretraining, the better the performance after instruction tuning. We find the same to be\nModel (#) Python JavaScript Java Go C++ Rust Avg.\nHUMANEVALFIX\nNon-permissive models\nInstructCodeT5+\u2020 2.7 1.2 4.3 2.1 0.2 0.5 1.8 WizardCoder\u2020 31.8 29.5 30.7 30.4 18.7 13.0 25.7 GPT-4 47.0 48.2 50.0 50.6 47.6 43.3 47.8\nPermissive models\nBLOOMZ 16.6 15.5 15.2 16.4 6.7 5.7 12.5 StarChat- 18.1 18.1 24.1 18.1 8.2 3.6 11.2 CodeGeeX2\u21e4 15.9 14.7 18.0 13.6 4.3 6.1 12.1 StarCoder 8.7 15.7 13.3 20.1 15.6 6.7 13.4 OCTOGEEX\u21e4 28.1 27.7 30.4 27.6 22.9 9.6 24.4 OCTOCODER 30.4 28.4 30.6 30.2 26.1 16.5 27.0\nHUMANEVALEXPLAIN\nNon-permissive models\nInstructCodeT5+\u2020 20.8 0.0 0.0 0.0 0.1 0.0 3.5 WizardCoder\u2020 32.5 33.0 27.4 26.7 28.2 16.9 27.5 GPT-4 64.6 57.3 51.2 58.5 38.4 42.7 52.1\nPermissive models\nBLOOMZ 14.7 8.8 12.1 8.5 0.6 0.0 7.5 StarChat- 25.4 21.5 24.5 18.4 17.6 13.2 20.1 CodeGeeX2\u21e4 0.0 0.0 0.0 0.0 0.0 0.0 0.0 StarCoder 0.0 0.0 0.0 0.0 0.0 0.0 0.0 OCTOGEEX\u21e4 30.4 24.0 24.7 21.7 21.0 15.9 22.9 OCTOCODER 35.1 24.5 27.3 21.1 24.1 14.8 24.5\nHUMANEVALSYNTHESIZE\nthe case for programming languages. Python, Java, and JavaScript collectively make up around 30% of the pretraining data of StarCoder (Li et al., 2023b). After instruction tuning StarCoder to produce OCTOCODER, we see the best performance among these three languages, especially for HUMANEVALSYNTHESIZE. OCTOCODER performs weakest on Rust, which is the lowest resource language of StarCoder among the languages we benchmark (1.2% of pretraining data).\nModels struggle with small targeted changes HUMANEVALFIX is the most challenging task for most models. They commonly regenerate the buggy function without making any change (e.g. WizardCoder in Figure 34) or they introduce new bugs (e.g. GPT-4 in Figure 33). We analyze model performance by bug type in Appendix M and find bugs that require removing excess code are the most challenging. OCTOCODER performs comparatively well across all languages. Instruction tuning on COMMITPACKFT has likely taught OCTOCODER to make small, targeted changes to fix bugs.\nModels struggle switching between code and text Some models fail at HUMANEVALEXPLAIN, as they do not generate natural language explanations. We manually inspect explanations for the first ten samples of the Python split and disqualify a model if none of them are explanations. This is the case for StarCoder and CodeGeeX2, which generate code instead of natural language explanations. BLOOMZ and InstructCodeT5+ also occasionally generate code. Other models exclusively generate natural language explanations, not containing any code for inspected samples.\nModels struggle adhering to a specified output length HUMANEVALEXPLAIN instructs models to fit their explanation within a given character limit (\u00a73). Current models appear to have no understanding of how many characters they are generating. They commonly write very short and thus underspecified explanations (e.g. BLOOMZ in Figure 35) or excessively long explanations that end up being cut off (e.g. StarChat- in Figure 38). Future work could investigate how to enable models to be aware of their generated output length to improve HUMANEVALEXPLAIN performance.\nHumanEval code synthesis is close to saturation Pure code synthesis on HUMANEVALSYNTHESIZE is the easiest task for all models. With a pass rate of 86.6% for a single solution, GPT-4 is close to fully saturating the Python subset. GPT-4 was originally found to score 67% on Python HumanEval (OpenAI, 2023) and 81% in later work (Bubeck et al., 2023). Our score for GPT-4 is significantly higher, possibly due to improvements made to the API by OpenAI, contamination of HumanEval in GPT-4 training, or slightly different prompting and evaluation. An example of our prompt is depicted in Figure 3 (right). We perform very careful evaluation to ensure every generation is correctly processed. We reproduce the HumanEval score of WizardCoder (Luo et al., 2023; Xu et al., 2023a) and find it to also perform well across other languages. For BLOOMZ and InstructCodeT5+ our evaluation leads to a higher Python score than they reported, likely because of our more careful processing of generations. OCTOCODER has the highest performance for every language among permissively licensed models. With a pass@1 of 46.2% on the original Python split, OCTOCODER improves by a relative 38% over its base model, StarCoder."
        },
        {
            "heading": "5 RELATED WORK",
            "text": ""
        },
        {
            "heading": "5.1 CODE MODELS",
            "text": "There has been extensive work on code models tailored to a specific coding task, such as code summarization (Iyer et al., 2016; Ahmad et al., 2020; Zhang et al., 2022a; Shi et al., 2022) or code editing (Drain et al., 2021; Zhang et al., 2022c; He et al., 2022; Zhang et al., 2022b; Wei et al., 2023; Prenner & Robbes, 2023; Fakhoury et al., 2023; Skreta et al., 2023) (also see work on edit models more generally (Reid & Neubig, 2022; Schick et al., 2022; Dwivedi-Yu et al., 2022; Raheja et al., 2023)). These works use task-specific heuristics that limit the applicability of their methods to other tasks. In contrast, we aim to build models applicable to all kinds of tasks related to code and beyond.\nThrough large-scale pretraining more generally applicable code models have been developed (Nijkamp et al., 2022; 2023; Xu et al., 2022a; Christopoulou et al., 2022; Gunasekar et al., 2023; Li et al., 2023b; Bui et al., 2023; Scao et al., 2022a;b). However, these models only continue code making them hard to use for tasks such as explaining code with natural language (HUMANEVALEXPLAIN). Teaching them to follow human instructions is critical to make them applicable to diverse tasks."
        },
        {
            "heading": "5.2 INSTRUCTION MODELS",
            "text": "Training models to follow instructions has led to new capabilities in text (Ouyang et al., 2022; Wang et al., 2022b; Chung et al., 2022) and visual modalities (Xu et al., 2023b; OpenAI, 2023). Prior work has shown its benefits for traditional language tasks (Wei et al., 2022; Longpre et al., 2023a; Iyer et al., 2022), multilingual tasks (Muennighoff et al., 2022b; 2024; Yong et al., 2022; \u00dcst\u00fcn et al., 2024), and dialog (K\u00f6pf et al., 2023; Bai et al., 2022; Ganguli et al., 2022). For coding applications, PanGu-Coder2 (Shen et al., 2023), WizardCoder (Luo et al., 2023) and InstructCodeT5+ (Wang et al., 2023c) are recent models trained with coding instructions. However, they all use the CodeAlpaca dataset (Chaudhary, 2023), which is synthetically generated from OpenAI models. Using data from powerful closed-source models provides a strong advantage, but limits the model use and has other limitations highlighted in \u00a71. CoEditor (Wei et al., 2023) proposes an \u201cauto-editing\u201d task, trained on 1650 python commit history repositories. Our work expands this to more general coding tasks via instructions, more languages, and orders of magnitude more commit data."
        },
        {
            "heading": "5.3 CODE BENCHMARKS",
            "text": "Many code synthesis benchmarks have been proposed (Wang et al., 2022d;c; Yu et al., 2023; Lai et al., 2023; Du et al., 2023). HumanEval (Chen et al., 2021; Liu et al., 2023b) has emerged as the standard for this task. Prior work has extended HumanEval to new programming languages via automatic translation mechanisms (Athiwaratkun et al., 2022; Cassano et al., 2023; Orlanski et al., 2023). These approaches are error-prone and only translate tests, not the actual solutions, which are needed for tasks like code explanation. Thus, we rely only on humans to create all parts of HUMANEVALPACK including test cases, correct solutions, buggy solutions, and other metadata across 6 languages.\nCode repair is commonly evaluated on Quixbugs (Lin et al., 2017; Prenner & Robbes, 2021; Ye et al., 2021; Xia & Zhang, 2023; Jiang et al., 2023; Sobania et al., 2023) or Python bugs (He et al., 2022; Bradley et al., 2023). The latter does not support code execution, which limits its utility. While Quixbugs supports execution with unit tests, it only contains 40 samples in Python and Java. Further, the problems in Quixbugs are generic functions, such as bucket sort. This makes them easy to solve and hard to decontaminate training data for. Our benchmark, HUMANEVALFIX, contains 164 buggy functions for six languages with solutions and unit tests. Further, our coding problems, derived from HumanEval, are very specific, such as keeping track of a bank account balance (see Figure 14).\nPrior work on evaluating code explanations (Lu et al., 2021; Cui et al., 2022) has relied on metrics such as METEOR (Banerjee & Lavie, 2005) or BLEU (Papineni et al., 2002). By chaining code explanation with code synthesis, we can evaluate this task using the execution-based pass@k metric overcoming the major limitations of BLEU and other heuristics-based metrics (Reiter, 2018).\nLarge-scale benchmarking has proven useful in many areas of natural language processing (Wang et al., 2019; Kiela et al., 2021; Srivastava et al., 2022; Muennighoff et al., 2022a). By producing 18 scores (6 languages across 3 tasks) for 9 models, we take a step towards large-scale benchmarking of code models. However, we lack many models capable of generating code (Black et al., 2021; Fried et al., 2022; Black et al., 2022; Wang & Komatsuzaki, 2021; Biderman et al., 2023b). Future work may consider more models or extending HUMANEVALPACK to new languages or tasks, such as code efficiency (Madaan et al., 2023a; Yetistiren et al., 2022) or code classification (Khan et al., 2023)."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "This work studies training and evaluation of Code LLMs that follow instructions. We introduce COMMITPACK, a 4TB dataset of Git commits covering 350 programming languages. We filter this large-scale dataset to create COMMITPACKFT, 2GB of high-quality code with commit messages that assimilate instructions. To enable a comprehensive evaluation of instruction code models, we construct HUMANEVALPACK, a human-written benchmark covering 3 different tasks for 6 programming languages. We ablate several instruction datasets and find that COMMITPACKFT combined with natural language data leads to the best performance. While our models, OCTOCODER and OCTOGEEX, are the best permissively licensed Code LLMs available, they are outperformed by closed-source models such as GPT-4. In addition to improving the instruction tuning paradigm, future work should consider training more capable base models."
        },
        {
            "heading": "ACKNOWLEDGEMENTS",
            "text": "We thank Hugging Face for providing compute instances. We are extremely grateful to Rodrigo Garcia for the Rust translations, Dimitry Ageev and Calum Bird for help with GPT-4 evaluation, Loubna Ben Allal for help on evaluation, Arjun Guha for insightful discussions on chaining evaluation tasks to avoid evaluating with BLEU, Lewis Tunstall for help on the OASST data, Victor Sanh and Nadav Timor for discussions, Jiaxi Yang for logo editing and domain classification prompting design, Ghosal et al. (2023); Zeng et al. (2023) for design inspiration, Harm de Vries for feedback and all members of BigCode for general support. Finally, we thank every programmer who takes the time to write informative commit messages."
        }
    ],
    "title": "OCTOPACK: INSTRUCTION TUNING CODE LARGE LANGUAGE MODELS",
    "year": 2024
}