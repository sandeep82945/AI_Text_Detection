{
    "abstractText": "Mesh deformation plays a pivotal role in many 3D vision tasks including dynamic simulations, rendering, and reconstruction. However, defining an efficient discrepancy between predicted and target meshes remains an open problem. A prevalent approach in current deep learning is the set-based approach which measures the discrepancy between two surfaces by comparing two randomly sampled pointclouds from the two meshes with Chamfer pseudo-distance. Nevertheless, the set-based approach still has limitations such as lacking a theoretical guarantee for choosing the number of points in sampled point-clouds, and the pseudo-metricity and the quadratic complexity of the Chamfer divergence. To address these issues, we propose a novel metric for learning mesh deformation. The metric is defined by sliced Wasserstein distance on meshes represented as probability measures that generalize the set-based approach. By leveraging probability measure space, we gain flexibility in encoding meshes using diverse forms of probability measures, such as continuous, empirical, and discrete measures via varifold representation. After having encoded probability measures, we can compare meshes by using the sliced Wasserstein distance which is an effective optimal transport distance with linear computational complexity and can provide a fast statistical rate for approximating the surface of meshes. To the end, we employ a neural ordinary differential equation (ODE) to deform the input surface into the target shape by modeling the trajectories of the points on the surface. Our experiments on cortical surface reconstruction demonstrate that our approach surpasses other competing methods in multiple datasets and metrics.",
    "authors": [],
    "id": "SP:127a8af4fd52b1ad3bd658b4a1a17501a03a8ec7",
    "references": [
        {
            "authors": [
                "Panos Achlioptas",
                "Olga Diamanti",
                "Ioannis Mitliagkas",
                "Leonidas Guibas"
            ],
            "title": "Learning representations and generative models for 3d point clouds",
            "venue": "In International conference on machine learning,",
            "year": 2018
        },
        {
            "authors": [
                "Frederick J Almgren"
            ],
            "title": "Plateau\u2019s problem: an invitation to varifold geometry, volume 13",
            "venue": "American Mathematical Soc.,",
            "year": 1966
        },
        {
            "authors": [
                "Martin Arjovsky",
                "Soumith Chintala",
                "L\u00e9on Bottou"
            ],
            "title": "Wasserstein generative adversarial networks",
            "venue": "In International Conference on Machine Learning,",
            "year": 2017
        },
        {
            "authors": [
                "Vincent Arsigny"
            ],
            "title": "Processing data in lie groups: An algebraic approach. Application to non-linear registration and diffusion tensor MRI",
            "venue": "PhD thesis, Citeseer,",
            "year": 2004
        },
        {
            "authors": [
                "John Ashburner"
            ],
            "title": "A fast diffeomorphic image registration",
            "venue": "algorithm. Neuroimage,",
            "year": 2007
        },
        {
            "authors": [
                "Brian B Avants",
                "Charles L Epstein",
                "Murray Grossman",
                "James C Gee"
            ],
            "title": "Symmetric diffeomorphic image registration with cross-correlation: evaluating automated labeling of elderly and neurodegenerative brain",
            "venue": "Medical image analysis,",
            "year": 2008
        },
        {
            "authors": [
                "Guha Balakrishnan",
                "Amy Zhao",
                "Mert R Sabuncu",
                "John Guttag",
                "Adrian V Dalca"
            ],
            "title": "Voxelmorph: a learning framework for deformable medical image registration",
            "venue": "IEEE transactions on medical imaging,",
            "year": 2019
        },
        {
            "authors": [
                "Harry G Barrow",
                "Jay M Tenenbaum",
                "Robert C Bolles",
                "Helen C Wolf"
            ],
            "title": "Parametric correspondence and chamfer matching: Two new techniques for image matching",
            "venue": "Technical report, SRI INTERNATIONAL MENLO PARK CA ARTIFICIAL INTELLIGENCE CENTER,",
            "year": 1977
        },
        {
            "authors": [
                "Espen Bernton",
                "Pierre E Jacob",
                "Mathieu Gerber",
                "Christian P Robert"
            ],
            "title": "On parameter estimation with the Wasserstein distance",
            "venue": "Information and Inference: A Journal of the IMA,",
            "year": 2019
        },
        {
            "authors": [
                "Fabian Bongratz",
                "Anne-Marie Rickmann",
                "Sebastian P\u00f6lsterl",
                "Christian Wachinger"
            ],
            "title": "Vox2cortex: fast explicit reconstruction of cortical surfaces from 3d mri scans with geometric deep neural networks",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Nicolas Bonneel",
                "Julien Rabin",
                "Gabriel Peyr\u00e9",
                "Hanspeter Pfister"
            ],
            "title": "Sliced and radon Wasserstein barycenters of measures",
            "venue": "Journal of Mathematical Imaging and Vision,",
            "year": 2015
        },
        {
            "authors": [
                "Nicolas Charon"
            ],
            "title": "Analysis of geometric and functional shapes with extensions of currents: applications to registration and atlas estimation",
            "venue": "PhD thesis, E\u0301cole normale supe\u0301rieure de Cachan-ENS Cachan,",
            "year": 2013
        },
        {
            "authors": [
                "Nicolas Charon",
                "Alain Trouv\u00e9"
            ],
            "title": "The varifold representation of nonoriented shapes for diffeomorphic registration",
            "venue": "SIAM journal on Imaging Sciences,",
            "year": 2013
        },
        {
            "authors": [
                "Ricky TQ Chen",
                "Yulia Rubanova",
                "Jesse Bettencourt",
                "David K Duvenaud"
            ],
            "title": "Neural ordinary differential equations",
            "venue": "Advances in neural information processing systems,",
            "year": 2018
        },
        {
            "authors": [
                "Zhiqin Chen",
                "Hao Zhang"
            ],
            "title": "Learning implicit fields for generative shape modeling",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Christopher B Choy",
                "Danfei Xu",
                "JunYoung Gwak",
                "Kevin Chen",
                "Silvio Savarese"
            ],
            "title": "3d-r2n2: A unified approach for single and multi-view 3d object reconstruction",
            "venue": "In Computer Vision\u2013ECCV 2016: 14th European Conference,",
            "year": 2016
        },
        {
            "authors": [
                "Earl A Coddington",
                "Norman Levinson"
            ],
            "title": "Theory of ordinary differential equations",
            "venue": "Pure & Applied Mathematics S. McGraw-Hill Education,",
            "year": 1984
        },
        {
            "authors": [
                "Rodrigo Santa Cruz",
                "Leo Lebrat",
                "Pierrick Bourgeat",
                "Clinton Fookes",
                "Jurgen Fripp",
                "Olivier Salvado"
            ],
            "title": "Deepcsr: A 3d deep learning approach for cortical surface reconstruction",
            "venue": "In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Marco Cuturi"
            ],
            "title": "Sinkhorn distances: Lightspeed computation of optimal transport",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2013
        },
        {
            "authors": [
                "Adrian V Dalca",
                "Guha Balakrishnan",
                "John Guttag",
                "Mert R Sabuncu"
            ],
            "title": "Unsupervised learning for fast probabilistic diffeomorphic registration",
            "venue": "In International Conference on Medical Image Computing and Computer-Assisted Intervention,",
            "year": 2018
        },
        {
            "authors": [
                "Adrian V Dalca",
                "Guha Balakrishnan",
                "John Guttag",
                "Mert R Sabuncu"
            ],
            "title": "Unsupervised learning of probabilistic diffeomorphic registration for images and surfaces",
            "venue": "Medical image analysis,",
            "year": 2019
        },
        {
            "authors": [
                "Haowen Deng",
                "Tolga Birdal",
                "Slobodan Ilic"
            ],
            "title": "Ppf-foldnet: Unsupervised learning of rotation invariant 3d local descriptors",
            "venue": "In Proceedings of the European conference on computer vision (ECCV),",
            "year": 2018
        },
        {
            "authors": [
                "Chaojing Duan",
                "Siheng Chen",
                "Jelena Kovacevic"
            ],
            "title": "3d point cloud denoising via deep neural network based local surface estimation",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2019
        },
        {
            "authors": [
                "Jean Feydy",
                "Thibault S\u00e9journ\u00e9",
                "Fran\u00e7ois-Xavier Vialard",
                "Shun-ichi Amari",
                "Alain Trouve",
                "Gabriel Peyr\u00e9"
            ],
            "title": "Interpolating between optimal transport and MMD using Sinkhorn divergences",
            "venue": "In The 22nd International Conference on Artificial Intelligence and Statistics,",
            "year": 2019
        },
        {
            "authors": [
                "Joan Glaunes",
                "Alain Trouv\u00e9",
                "Laurent Younes"
            ],
            "title": "Diffeomorphic matching of distributions: A new approach for unlabelled point-sets and sub-manifolds matching",
            "venue": "In Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition,",
            "year": 2004
        },
        {
            "authors": [
                "Joan Glaun\u00e8s",
                "Anqi Qiu",
                "Michael I Miller",
                "Laurent Younes"
            ],
            "title": "Large deformation diffeomorphic metric curve mapping",
            "venue": "International journal of computer vision,",
            "year": 2008
        },
        {
            "authors": [
                "Ziv Goldfeld",
                "Kengo Kato",
                "Gabriel Rioux",
                "Ritwik Sadhu"
            ],
            "title": "Statistical inference with regularized optimal transport",
            "venue": "arXiv preprint arXiv:2205.04283,",
            "year": 2022
        },
        {
            "authors": [
                "Thibault Groueix",
                "Matthew Fisher",
                "Vladimir G Kim",
                "Bryan C Russell",
                "Mathieu Aubry"
            ],
            "title": "A papier-m\u00e2ch\u00e9 approach to learning 3d surface generation",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Kunal Gupta"
            ],
            "title": "Neural mesh flow: 3d manifold mesh generation via diffeomorphic flows",
            "year": 2020
        },
        {
            "authors": [
                "Kun Han",
                "Shanlin Sun",
                "Xiangyi Yan",
                "Chenyu You",
                "Hao Tang",
                "Junayed Naushad",
                "Haoyu Ma",
                "Deying Kong",
                "Xiaohui Xie"
            ],
            "title": "Diffeomorphic image registration with neural velocity field",
            "venue": "In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision,",
            "year": 2023
        },
        {
            "authors": [
                "Christian H\u00e4ne",
                "Shubham Tulsiani",
                "Jitendra Malik"
            ],
            "title": "Hierarchical surface prediction for 3d object reconstruction",
            "venue": "In 2017 International Conference on 3D Vision (3DV),",
            "year": 2017
        },
        {
            "authors": [
                "Hsi-Wei Hsieh",
                "Nicolas Charon"
            ],
            "title": "Diffeomorphic registration of discrete geometric distributions",
            "venue": "In Mathematics Of Shapes And Applications,",
            "year": 2020
        },
        {
            "authors": [
                "Clifford R Jack Jr.",
                "Matt A Bernstein",
                "Nick C Fox",
                "Paul Thompson",
                "Gene Alexander",
                "Danielle Harvey",
                "Bret Borowski",
                "Paula J Britson",
                "Jennifer L. Whitwell",
                "Chadwick Ward"
            ],
            "title": "The alzheimer\u2019s disease neuroimaging initiative (adni): Mri methods",
            "venue": "Journal of Magnetic Resonance Imaging: An Official Journal of the International Society for Magnetic Resonance in Medicine,",
            "year": 2008
        },
        {
            "authors": [
                "Chiyu Jiang",
                "Avneesh Sud",
                "Ameesh Makadia",
                "Jingwei Huang",
                "Matthias Nie\u00dfner",
                "Thomas Funkhouser"
            ],
            "title": "Local implicit grid representations for 3d scenes",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Irene Kaltenmark",
                "Benjamin Charlier",
                "Nicolas Charon"
            ],
            "title": "A general framework for curve and surface comparison and registration with oriented varifolds",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "arXiv preprint arXiv:1412.6980,",
            "year": 2014
        },
        {
            "authors": [
                "Alexander Korotin",
                "Lingxiao Li",
                "Aude Genevay",
                "Justin M Solomon",
                "Alexander Filippov",
                "Evgeny Burnaev"
            ],
            "title": "Do neural optimal transport solvers work? a continuous Wasserstein-2 benchmark",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Julian Krebs",
                "Herv\u00e9 Delingette",
                "Boris Mailh\u00e9",
                "Nicholas Ayache",
                "Tommaso Mansi"
            ],
            "title": "Learning a probabilistic model for diffeomorphic registration",
            "venue": "IEEE transactions on medical imaging,",
            "year": 2019
        },
        {
            "authors": [
                "Leo Lebrat",
                "Rodrigo Santa Cruz",
                "Frederic de Gournay",
                "Darren Fu",
                "Pierrick Bourgeat",
                "Jurgen Fripp",
                "Clinton Fookes",
                "Olivier Salvado"
            ],
            "title": "Corticalflow: a diffeomorphic mesh transformer network for cortical surface reconstruction",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "L\u00e9o Lebrat",
                "Rodrigo Santa Cruz",
                "Fr\u00e9d\u00e9ric de Gournay",
                "Darren Fu",
                "Pierrick Bourgeat",
                "Jurgen Fripp",
                "Clinton Fookes",
                "Olivier Salvado"
            ],
            "title": "Corticalflow: A diffeomorphic mesh deformation module for cortical surface reconstruction",
            "venue": "arXiv preprint arXiv:2206.02374,",
            "year": 2022
        },
        {
            "authors": [
                "William E Lorensen",
                "Harvey E Cline"
            ],
            "title": "Marching cubes: A high resolution 3d surface construction algorithm",
            "venue": "ACM siggraph computer graphics,",
            "year": 1987
        },
        {
            "authors": [
                "Jun Ma",
                "Michael I Miller",
                "Laurent Younes"
            ],
            "title": "A Bayesian generative model for surface template estimation",
            "venue": "Journal of Biomedical Imaging,",
            "year": 2010
        },
        {
            "authors": [
                "Qiang Ma",
                "Emma C Robinson",
                "Bernhard Kainz",
                "Daniel Rueckert",
                "Amir Alansary"
            ],
            "title": "Pialnn: a fast deep learning framework for cortical pial surface reconstruction. In Machine Learning in Clinical Neuroimaging: 4th International Workshop, MLCN 2021, Held in Conjunction with MICCAI 2021",
            "venue": "Proceedings 4,",
            "year": 2021
        },
        {
            "authors": [
                "Qiang Ma",
                "Liu Li",
                "Emma C Robinson",
                "Bernhard Kainz",
                "Daniel Rueckert",
                "Amir Alansary"
            ],
            "title": "Cortexode: Learning cortical surface reconstruction by Neural ODEs",
            "venue": "IEEE Transactions on Medical Imaging,",
            "year": 2022
        },
        {
            "authors": [
                "Julian Maclaren",
                "Zhaoying Han",
                "Sjoerd B Vos",
                "Nancy Fischbein",
                "Roland Bammer"
            ],
            "title": "Reliability of brain volume measurements: a test-retest dataset",
            "venue": "Scientific data,",
            "year": 2014
        },
        {
            "authors": [
                "Daniel S Marcus",
                "Tracy H Wang",
                "Jamie Parker",
                "John G Csernansky",
                "John C Morris",
                "Randy L Buckner"
            ],
            "title": "Open access series of imaging studies (oasis): cross-sectional mri data in young, middle aged, nondemented, and demented older adults",
            "venue": "Journal of cognitive neuroscience,",
            "year": 2007
        },
        {
            "authors": [
                "Gonzalo Mena",
                "Jonathan Weed"
            ],
            "title": "Statistical bounds for entropic optimal transport: sample complexity and the central limit theorem",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Lars Mescheder",
                "Michael Oechsle",
                "Michael Niemeyer",
                "Sebastian Nowozin",
                "Andreas Geiger"
            ],
            "title": "Occupancy networks: Learning 3d reconstruction in function space",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Khai Nguyen",
                "Nhat Ho"
            ],
            "title": "Control variate sliced Wasserstein estimators",
            "venue": "arXiv preprint arXiv:2305.00402,",
            "year": 2023
        },
        {
            "authors": [
                "Khai Nguyen",
                "Nhat Ho"
            ],
            "title": "Energy-based sliced Wasserstein distance",
            "venue": "arXiv preprint arXiv:2304.13586,",
            "year": 2023
        },
        {
            "authors": [
                "Khai Nguyen",
                "Nhat Ho",
                "Tung Pham",
                "Hung Bui"
            ],
            "title": "Distributional sliced-Wasserstein and applications to generative modeling",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Khai Nguyen",
                "Dang Nguyen",
                "Nhat Ho"
            ],
            "title": "Self-attention amortized distributional projection optimization for sliced Wasserstein point-cloud reconstruction",
            "venue": "Proceedings of the 40th International Conference on Machine Learning,",
            "year": 2023
        },
        {
            "authors": [
                "Trung Nguyen",
                "Quang-Hieu Pham",
                "Tam Le",
                "Tung Pham",
                "Nhat Ho",
                "Binh-Son Hua"
            ],
            "title": "Pointset distances for learning representations of 3d point clouds",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV),",
            "year": 2021
        },
        {
            "authors": [
                "Michael Niemeyer",
                "Lars Mescheder",
                "Michael Oechsle",
                "Andreas Geiger"
            ],
            "title": "Differentiable volumetric rendering: Learning implicit 3d representations without 3d supervision",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Junyi Pan",
                "Xiaoguang Han",
                "Weikai Chen",
                "Jiapeng Tang",
                "Kui Jia"
            ],
            "title": "Deep mesh reconstruction from single rgb images via topology modification networks",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2019
        },
        {
            "authors": [
                "Jeong Joon Park",
                "Peter Florence",
                "Julian Straub",
                "Richard Newcombe",
                "Steven Lovegrove"
            ],
            "title": "Deepsdf: Learning continuous signed distance functions for shape representation",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Songyou Peng",
                "Michael Niemeyer",
                "Lars Mescheder",
                "Marc Pollefeys",
                "Andreas Geiger"
            ],
            "title": "Convolutional occupancy networks",
            "venue": "In Computer Vision\u2013ECCV 2020: 16th European Conference,",
            "year": 2020
        },
        {
            "authors": [
                "Gabriel Peyr\u00e9",
                "Marco Cuturi"
            ],
            "title": "Computational optimal transport: With applications to data science",
            "venue": "Foundations and Trends\u00ae in Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Nikhila Ravi",
                "Jeremy Reizenstein",
                "David Novotny",
                "Taylor Gordon",
                "Wan-Yen Lo",
                "Justin Johnson",
                "Georgia Gkioxari"
            ],
            "title": "Accelerating 3d deep learning with pytorch3d",
            "year": 2007
        },
        {
            "authors": [
                "Islem Rekik",
                "Gang Li",
                "Weili Lin",
                "Dinggang Shen"
            ],
            "title": "Multidirectional and topography-based dynamic-scale varifold representations with application to matching developing cortical",
            "venue": "surfaces. NeuroImage,",
            "year": 2016
        },
        {
            "authors": [
                "David Ruelle",
                "Dennis Sullivan"
            ],
            "title": "Currents, flows and diffeomorphisms",
            "venue": "Topology, 14(4):319\u2013327,",
            "year": 1975
        },
        {
            "authors": [
                "Rodrigo Santa Cruz",
                "L\u00e9o Lebrat",
                "Darren Fu",
                "Pierrick Bourgeat",
                "Jurgen Fripp",
                "Clinton Fookes",
                "Olivier Salvado"
            ],
            "title": "Corticalflow++: Boosting cortical surface reconstruction accuracy, regularity, and interoperability",
            "venue": "In Medical Image Computing and Computer Assisted Intervention\u2013MICCAI 2022: 25th International Conference,",
            "year": 2022
        },
        {
            "authors": [
                "Thibault S\u00e9journ\u00e9",
                "Jean Feydy",
                "Fran\u00e7ois-Xavier Vialard",
                "Alain Trouv\u00e9",
                "Gabriel Peyr\u00e9"
            ],
            "title": "Sinkhorn divergences for unbalanced optimal transport",
            "venue": "arXiv preprint arXiv:1910.12958,",
            "year": 2019
        },
        {
            "authors": [
                "Edward J Smith",
                "Scott Fujimoto",
                "Adriana Romero",
                "David Meger"
            ],
            "title": "Geometrics: Exploiting geometric structure for graph-encoded objects",
            "venue": "arXiv preprint arXiv:1901.11461,",
            "year": 2019
        },
        {
            "authors": [
                "Shanlin Sun",
                "Kun Han",
                "Deying Kong",
                "Hao Tang",
                "Xiangyi Yan",
                "Xiaohui Xie"
            ],
            "title": "Topology-preserving shape reconstruction and registration via neural diffeomorphic flow",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Maxim Tatarchenko",
                "Alexey Dosovitskiy",
                "Thomas Brox"
            ],
            "title": "Octree generating networks: Efficient convolutional architectures for high-resolution 3d outputs",
            "venue": "In Proceedings of the IEEE international conference on computer vision,",
            "year": 2017
        },
        {
            "authors": [
                "Marc Vaillant",
                "Joan Glaunes"
            ],
            "title": "Surface matching via currents",
            "venue": "In Information Processing in Medical Imaging: 19th International Conference,",
            "year": 2005
        },
        {
            "authors": [
                "C\u00e9dric Villani"
            ],
            "title": "Topics in optimal transportation",
            "venue": "Number 58. American Mathematical Soc.,",
            "year": 2003
        },
        {
            "authors": [
                "C\u00e9dric Villani"
            ],
            "title": "Optimal transport: old and new, volume 338",
            "year": 2009
        },
        {
            "authors": [
                "Martin J Wainwright"
            ],
            "title": "High-dimensional statistics: A non-asymptotic viewpoint",
            "year": 2019
        },
        {
            "authors": [
                "Nanyang Wang",
                "Yinda Zhang",
                "Zhuwen Li",
                "Yanwei Fu",
                "Wei Liu",
                "Yu-Gang Jiang"
            ],
            "title": "Pixel2mesh: Generating 3d mesh models from single rgb images",
            "venue": "In Proceedings of the European conference on computer vision (ECCV),",
            "year": 2018
        },
        {
            "authors": [
                "Peng-Shuai Wang",
                "Chun-Yu Sun",
                "Yang Liu",
                "Xin Tong"
            ],
            "title": "Adaptive o-cnn: A patch-based deep representation of 3d shapes",
            "venue": "ACM Transactions on Graphics (TOG),",
            "year": 2018
        },
        {
            "authors": [
                "Weiyue Wang",
                "Duygu Ceylan",
                "Radomir Mech",
                "Ulrich Neumann"
            ],
            "title": "3dn: 3d deformation network",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Udaranga Wickramasinghe",
                "Edoardo Remelli",
                "Graham Knott",
                "Pascal Fua"
            ],
            "title": "Voxel2mesh: 3d mesh model generation from volumetric data",
            "venue": "Medical Image Computing and Computer Assisted Intervention\u2013MICCAI 2020: 23rd International Conference,",
            "year": 2020
        },
        {
            "authors": [
                "Qiangeng Xu",
                "Weiyue Wang",
                "Duygu Ceylan",
                "Radomir Mech",
                "Ulrich Neumann"
            ],
            "title": "Disn: Deep implicit surface network for high-quality single-view 3d reconstruction",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Lior Yariv",
                "Yoni Kasten",
                "Dror Moran",
                "Meirav Galun",
                "Matan Atzmon",
                "Basri Ronen",
                "Yaron Lipman"
            ],
            "title": "Multiview neural surface reconstruction by disentangling geometry and appearance",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Q Zhou"
            ],
            "title": "Pymesh\u2014geometry processing library for python. Software available for download at https://github",
            "venue": "com/PyMesh/PyMesh,",
            "year": 2019
        },
        {
            "authors": [
                "2017 H\u00e4ne et al",
                "2017 Tatarchenko et al",
                "2018b Wang et al",
                "Cruz"
            ],
            "title": "2021) and implicit surface-based (Mescheder et al., 2019; Park et al., 2019; Xu et al., 2019) methods can directly obtain surface by employing iso-surface extraction methods, such as Marching Cubes (Lorensen",
            "year": 1987
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Mesh deformation is a fundamental task in 3D computer vision and computer graphics. A wide range of shape reconstruction tasks (Chen & Zhang, 2019; Jiang et al., 2020; Mescheder et al., 2019; Niemeyer et al., 2020; Park et al., 2019; Peng et al., 2020; Yariv et al., 2020) and shape registration (Ashburner, 2007; Dalca et al., 2018; Balakrishnan et al., 2019; Krebs et al., 2019; Dalca et al., 2019; Avants et al., 2008; Sun et al., 2022; Han et al., 2023) leverages state-of-theart mesh deformation methodology. One popular approach for mesh deformation is to estimate the vertex displacement vectors (3D offsets) while keeping their connectivity (Wang et al., 2019; Bongratz et al., 2022). However, displacement-based methods cannot guarantee the manifoldness of the resulting mesh and often produce self-intersecting faces. To address this issue, diffeomorphic transformation (Ruelle & Sullivan, 1975; Arsigny, 2004) is one effective way to deform a mesh while preserving its topology. As an instance of diffeomorphic surface deformation, Neural Mesh Flow (NMF) (Gupta, 2020) learns a sequence of diffeomorphic flows between two meshes and models the trajectories of the mesh vertices as ordinary differential equations (ODEs). However, these methods have limited shape representation capacity and struggle to perform well on complex manifolds. To overcome this limitation, several diffeomorphic mesh deformation models have been proposed, such as CortexODE (Ma et al., 2022) and CorticalFlow (Lebrat et al., 2022), which aim to handle hard manifolds, such as the cortical surface. While CorticalFlow (Lebrat et al., 2022) introduces diffeomorphic mesh deformation (DMD) modules to learn a series of stationary velocity fields, CortexODE (Ma et al., 2022) encodes spatial information of the MRI images along with vertices features using an MLP model and employs neural ODEs (Chen et al., 2018) to model the trajectories\nof the points. Nonetheless, these approaches often rely on Chamfer distance as the objective function, which might have disadvantages.\nSelecting an appropriate metric to evaluate the dissimilarity between two meshes is a crucial step in learning deformation mesh models. Recent literature favors the approach of using set-based comparison due to its simplicity. In particular, the set-based approach first samples two sets of points on the surface meshes, and then use Chamfer distance (CD) (Deng et al., 2018; Duan et al., 2019; Groueix et al., 2018) to compare two meshes and optimize them. However, the CD loss tends to get trapped in local minima easily (Achlioptas et al., 2018; Nguyen et al., 2021b; 2023), failing to distinguish bad samples from the true ones, as demonstrated by our toy example in Fig. 1. Although a weighted CD has been proposed to prioritize fitting local regions with high curvatures in Vox2Cortex (Bongratz et al., 2022), the issue is only alleviated but not resolved completely, still resulting in suboptimal assignments between two sets of points. Therefore, we propose novel approaches to transform a mesh into a probability measure that generalizes the set-based approach. Furthermore, by relying on the probability measure approach, we can employ geometric measure theory to represent mesh as an oriented varifold (Almgren, 1966; Vaillant & Glaunes, 2005; Glaun\u00e8s et al., 2008) and get a better approximation of the mesh compared to the random sampling approach. After that, we adopt efficient optimal transport to compare these measures since optimal transport distances are naturally fitted to compare disjoint-support measures.\nWasserstein distance (Peyr\u00e9 & Cuturi, 2019; Villani, 2009) has been widely recognized as an effective optimal transport metric to compare two probability measures, especially when their supports are disjointed. Despite having a lot of appealing properties, the Wasserstein distance has high computational complexity. In particular, when dealing with discrete probability measures that have at most m supports, the time and memory complexities of the Wasserstein distance are O(m3 logm) and O(m2), respectively. The issue becomes more problematic when the Wasserstein distance is computed on different pairs of measures as in mesh applications, namely, each mesh can be treated as a probability measure.\nTo improve the computational complexities of the Wasserstein distance, by adding entropic regularization and using the Sinkhorn algorithm (Cuturi, 2013), an \u03f5-approximation of Wasserstein distance can be obtained in O(m2/\u03f52). However, this approach cannot reduce the memory complexity of O(m2) due to the storage of the cost matrix. Moreover, the entropic regularization approach cannot lead to a valid metric between probability measures since the resulting discrepancy does not satisfy the triangle inequality. A more efficient approach based on the closed-form solution of Wasserstein is sliced Wasserstein distance (SWD) (Bonneel et al., 2015), which is computed as the expectation of the Wasserstein distance between random one-dimensional push-forward measures from two original measures. SWD can be solved inO(m logm) time complexity while having a linear memory complexity O(m). In this paper, we propose a learning-based Diffeomorphic mesh Deformation framework via an efficient Optimal Transport metric, dubbed DDOT, that learns continuous dynamics to smoothly deform an initial mesh towards an intricate shape based on volumetric input. Specifically, given a 3D brain MRI volume, we aim to reconstruct the highly folded white matter surface region. We first extract the initial surface from the white matter segmentation mask of the brain MRI image, then we deform the initial surface to the target surface by modeling its vertices trajectory via neural ODE\n(Chen et al., 2018). Our deformation model is optimized via sliced Wasserstein distance loss by encoding the mesh as a probability measure. We further represent mesh as an oriented varifold and empirically show that our approach surpasses other related works on multiple datasets and metrics, namely, almost self-intersection-free while maintaining high geometric accuracy. It is worth noting that although our DDOT is developed within the scope of cortical surface reconstruction (CSR), the underlying concepts can be extended to other 3D deformation networks.\nContribution. In summary, our contributions are as follows:\n1. We propose to represent triangle meshes as probability measures that generalize the common set-based approach in a learning-based deformation network. Specifically, we present three forms of mesh as probability measure: continuous, empirical, and discrete measure via an oriented varifold.\n2. We propose a new metric for learning mesh deformation. Our metric utilizes the sliced Wasserstein distance (SWD), which operates on meshes represented as probability measures. We demonstrate that sliced Wasserstein distance (SWD) is a valid computationally fast metric between probability measures and provide the approximation bound between the SWD between empirical probability measures and the SWD between continuous probability measures.\n3. We conduct extensive experiments on white matter reconstruction by employing neural ODE (Chen et al., 2018) to deform the initial surface to the target surface. Our experiments on multiple brain datasets demonstrate that our method outperforms existing state-of-the-art related works in terms of geometric accuracy, self-intersection ratio, and consistency.\nOrganization. The paper\u2019s structure is as follows. We first provide background about the setbased approach for comparing two meshes and the definition of Wasserstein distance as well as diffeomorphic flows for deforming meshes in Section 2. In Section 3, we propose probability measure encoding to represent mesh and further employ SWD as an objective function for diffeomorphic deformation framework as well as analyze their relevant theoretical properties. Section 4 presents our experiments on reconstructing cortical surface and provides quantitative results compared to state-of-the-art methods. In Section 5, we identify the limitations of our approach and outline potential avenues for future research. We then draw concluding remarks in Section 6. Finally, we defer the proofs of key results, supplementary materials, and discussion on related works to Appendices.\nNotations. For any d \u2265 2, we denote Sd\u22121 := {\u03b8 \u2208 Rd | ||\u03b8||22 = 1} and U(Sd\u22121) as the unit hypersphere and its corresponding uniform distribution. We denote \u03b8\u266f\u00b5 as the push-forward measures of \u00b5 through the function f : Rd \u2192 R that is f(x) = \u03b8\u22a4x. Furthermore, we denote \u03b4x as Dirac distribution at a location x, and Pp(Rd) is the set of probability measures over Rd that has finite p-moment. By abuse of notations, we use capitalized letters for both random variables and sets."
        },
        {
            "heading": "2 BACKGROUND",
            "text": "In this section, we first review the set-based approach for comparing two meshes. After that, we review the definition of Wasserstein distance and diffeomorphic flows for deforming meshes."
        },
        {
            "heading": "2.1 THE SET-BASED APPROACH: MESH TO POINT-CLOUD",
            "text": "Mesh to a point-cloud. To sample a point p from a mesh, a face f = (x1, x2, x3) is first sampled with the probability proportional to the area of the face. Then, the position p can be sampled by setting p := w1x1 + w2x2 + w3x3, where w1 + w2 + w3 = 1 are random barycentric coordinates which are uniformly distributed over a triangle (Ravi et al., 2020; Wang et al., 2019). The process is repeated until getting the desired number of points.\nComparing two point-clouds. After having representative point-clouds from meshes, a discrepancy in the space of point-clouds (sets) is used. The most widely used discrepancy for point-cloud is the set-based Chamfer pseudo distance (Barrow et al., 1977). For any two point-clouds X and Y , the Chamfer distance is:\nCD(X,Y ) = 1 |X| \u2211 x\u2208X min y\u2208Y \u2225x\u2212 y\u222522 + 1 |Y | \u2211 y\u2208Y min x\u2208X \u2225x\u2212 y\u222522, (1)\nwhere |X| denotes the number of points in X ."
        },
        {
            "heading": "2.2 WASSERSTEIN DISTANCE",
            "text": "We now review the definition of the Wasserstein distance for comparing two probability measures \u00b5 \u2208 Pp(Rd) and \u03bd \u2208 Pp(Rd). The Wasserstein-p (Villani, 2003) distance between \u00b5 and \u03bd as follows:\nWpp(\u00b5, \u03bd) := inf \u03c0\u2208\u03a0(\u00b5,\u03bd) \u222b Rd\u00d7Rd \u2225x\u2212 y\u2225ppd\u03c0(x, y), (2)\nwhere \u03a0(\u00b5, \u03bd) is the set of joint distributions that have marginals are \u00b5 and \u03bd respectively. A benefit of Wasserstein distance is that it can handle two measures that have disjointed supports.\nWasserstein distance between continuous measures. Computing the Wasserstein distance accurately between continuous measures is still an open question (Korotin et al., 2021) due to the non-optimality and instability of the minimax optimization for continuous functions which are the Kantorovich potentials (Arjovsky et al., 2017). Hence, discrete representations of the continuous measures are often used as proxies to compare them. i.e., the plug-in estimator (Bernton et al., 2019). In particular, let X1, . . . , Xm i.i.d\u223c \u00b5 and Y1, . . . , Ym i.i.d\u223c \u03bd, the Wasserstein distance Wp(\u00b5, \u03bd) is ap-\nproximated by Wp(\u00b5\u0302m, \u03bd\u0302m) with \u00b5\u0302m = 1m \u2211m i=1 \u03b4Xi and \u03bd\u0302m = 1 m \u2211m i=1 \u03b4Yi are the corresponding empirical measures. However, the convergence rate of the Wasserstein distance isO(m\u22121/d) (Mena & Weed, 2019). Namely, we have E [|Wp(\u00b5\u0302m, \u03bd\u0302m)\u2212Wp(\u00b5, \u03bd)|] \u2264 Cm\u22121/d for an universal constant C, where \u00b5\u0302m = 1m \u2211m i=1 \u03b4Xi is the corresponding empirical measure. Therefore, the Wasserstein distance suffers from the curse of dimensionality i.e., the Wasserstein distance needs more samples to represent the true measure well when dealing with high-dimensional measures. In the setting of comparing meshes, the Wasserstein distance will be worse if we use more features for meshes e.g., normals, colors, and so on.\nWasserstein distance between discrete measures. When \u00b5 and \u03bd are two discrete probability measures that have at most m supports, the time complexity and memory complexity to compute the Wasserstein distance areO(m3logm) andO(m2) respectively. Therefore, using the plug-in estimator requires expensive computation since it requires a relative large value of m to the dimension."
        },
        {
            "heading": "2.3 DIFFEOMORPHIC FLOWS",
            "text": "Diffeomorphic flows can be established by dense point correspondences between source and target surfaces. Given an input surface, the trajectories of the points can be modeled by an ODE, where the derivatives of the points are parameterized by a deep neural network (DNN). Specifically, let \u03a6(p, t) : \u2126 \u2282 R3 \u00d7 [0, 1] 7\u2192 \u2126 \u2282 R3 be a continuous hidden state of the neural network that defines a trajectory from source position p = \u03a6(p, 0) to the target position p\u2032 = \u03a6(p, 1), and F\u03d5 be a DNN with parameters \u03d5. An ordinary differential equation (ODE) with the initial condition is defined as:\n\u2202\u03a6(p, t)\n\u2202t = F\u03d5(\u03a6(p, t), t) s.t. \u03a6(p, 0) = p, (3)\nIf F\u03d5 is Lipschitz, a solution to Eq. 3 exists and is unique in the interval [0, 1], which provides a theoretical guarantee that any two deformation trajectories do not intersect with each other (Coddington & Levinson, 1984)."
        },
        {
            "heading": "3 DIFFEOMORPHIC MESH DEFORMATION VIA AN EFFICIENT OPTIMAL TRANSPORT METRIC",
            "text": "In this section, we generalize the set-based mesh representation by proposing three ways of transforming mesh into probability measure encoding. We further employ sliced Wasserstein distance as an objective function in the diffeomorphic flow model for cortical surface reconstruction task."
        },
        {
            "heading": "3.1 THE MEASURE-APPROACH: MESH TO PROBABILITY MEASURE",
            "text": "We now discuss the approach that we rely on to compare meshes via probability metrics. In particular, we consider transforming a mesh into a probability measure.\nMesh to a continuous and hierarchical probability measure. Let a mesh M have a set of faces FM = {fM1 , . . . , fMN } (N > 0) where a face f is represented by its vertices\nVer(f) = {x1, . . . , xvf } (vf \u2265 3). We now can define a probability measure over faces, namely, \u00b5M(f) = \u2211N i=1 Vol(fMi )\u2211N j=1 Vol(f M j ) \u03b4fMi is the categorical distribution over the faces that has the weights proportional to the areas of the faces (volume of the convex hull of vertices). For example, in the case of triangle meshes, we have Ver(f) = (x1, x2, x3) and Vol(f) = 12\u2225(x2 \u2212 x1)\u00d7 (x3 \u2212 x1)\u2225. Given a face f , the conditional distribution for a point in the space is \u00b5M(x|f) = 1Vol(f) ,\u2200x \u2208 ConvexHull(Ver(f)). Therefore, the marginal distribution for a point in the space induced by a mesh M is \u00b5M(x) = \u2211N i=1 \u00b5\nM(x|fMi )\u00b5M(fMi ). It is worth noting that given two meshes M1 and M2, their corresponding probability measures \u00b5M1 and \u00b5M2 are likely to have disjoint supports. Therefore, the optimal transport distances are natural metrics for comparing them.\nMesh to an empirical probability measure. As discussed in the background, the most computationally efficient and stable approach to approximate the Wasserstein distance is through the plugin estimator (Bernton et al., 2019). In particular, let x1, . . . , xm be a set of points that are independently identically sampled from \u00b5M1 and y1, . . . , ym be a set of points that are independently identically sampled from \u00b5M2 . After that, we can define \u00b5\u0302M1m = 1 m \u2211m i=1 \u03b4xi and \u00b5\u0302 M2 m = 1 m \u2211m i=1 \u03b4yi as the represented empirical probability measure of the two meshesM1 andM2 respectively. Finally, the discrete Wasserstein distance is computed between \u00b5\u0302M1m and \u00b5\u0302 M2 m as the final discrepancy.\nMesh to a discrete probability measure. To have a richer representation of mesh, we consider the discrete probability measure representation through varifold. Let M be a smooth submanifold of dimension 2 embedded in the ambient space of Rn, e.g. n = 3 for surface, with finite total volume Vol(M) < \u221e. For every point p \u2208 M , there exists a tangent space TpM be a linear subspace of Rn. To establish an orientation of M , it is essential to orient the tangent space TpM for every p \u2208 M . This ensures that each oriented tangent space can be represented as an element of an oriented Grassmannian. Inspired from previous works (Glaunes et al., 2004; Vaillant & Glaunes, 2005; Charon & Trouv\u00e9, 2013; Kaltenmark et al., 2017), M can be associated as an oriented varifold \u00b5\u0303M , i.e. a distribution on the position space and tangent space orientation Rn \u00d7 Sn\u22121, as follows: \u00b5\u0303M = \u222b M\n\u03b4(p,n\u20d7(p))dVol(p), where n\u20d7(p) is the unit oriented normal vector to the surface at p. Once established the oriented varifold for a smooth surface, an oriented varifold for triangular meshM that approximates smooth shape M can be derived as follows:\n\u00b5\u0303M = |F |\u2211 i=1 \u00b5\u0303fi = |F |\u2211 i=1 \u222b fi \u03b4(pi,n\u20d7(pi))dVol(p) \u2248 |F |\u2211 i=1 \u03b1i\u03b4(pi,n\u20d7(pi)), (4)\nwhere pi is the barycenter of the vertices of face fi and \u03b1i := Vol(fi) is the area of the triangle. To ensure that \u00b5\u0303M possesses the characteristic of a discrete measure, we normalize \u03b1is\u2019 such that they sum up to 1. Note that provided the area of triangular is sufficiently small, \u00b5\u0303M gives an acceptable approximation of the discrete meshM in terms of oriented varifold (Kaltenmark et al., 2017)."
        },
        {
            "heading": "3.2 EFFECTIVE AND EFFICIENT MESH COMPARISON WITH SLICED WASSERSTEIN",
            "text": "Sliced Wasserstein distance. The sliced Wasserstein distance (Bonneel et al., 2015) (SWD) between two probability measures \u00b5 \u2208 Pp(Rd) and \u03bd \u2208 Pp(Rd) is defined as:\nSWpp(\u00b5, \u03bd) = E\u03b8\u223cU(Sd\u22121)[Wpp(\u03b8\u266f\u00b5, \u03b8\u266f\u03bd)], (5)\nThe benefit of SW is that Wpp(\u03b8\u266f\u00b5, \u03b8\u266f\u03bd) has a closed-form solution which is \u222b 1 0 |F\u22121\u03b8\u266f\u00b5(z)\u2212F \u22121 \u03b8\u266f\u03bd(z)|pdz with F\u22121 denotes the inverse CDF function. The expectation is often approximated by Monte Carlo sampling, namely, it is replaced by the average from \u03b81, . . . , \u03b8L (L is the number of projections) that are drawn i.i.d from U(Sd\u22121). In particular, we have:\nS\u0302W p p(\u00b5, \u03bd) = 1\nL L\u2211 l=1 Wpp(\u03b8l\u266f\u00b5, \u03b8l\u266f\u03bd). (6)\nThe computational complexity and memory complexity of the Monte Carlo estimation of SW are O(Lm logm) and O(Lm) (Nguyen & Ho, 2023a) respectively when \u00b5 and \u03bd are discrete measures with at most m supports. Therefore, the SW is naturally suitable for large-scale mesh comparison.\nConvergence rate. We now discuss the non-asymptotic convergence of the Monte Carlo estimation of the sliced Wasserstein between two empirical probability measures to the sliced Wasserstein between the corresponding two continuous probability measures on surface meshes.\nTheorem 1. For any two meshes M1 and M2, let X1, . . . , Xm i.i.d\u223c \u00b5M1(x), Y1, . . . , Ym i.i.d\u223c \u00b5M2(x), \u00b5\u0302M1m (x) = 1 m \u2211m i=1 \u03b4Xi and \u00b5\u0302 M2 m (x) = 1 m \u2211m i=1 \u03b4Yi be the corresponding empirical distribution. Assume that \u00b5M1 and \u00b5M2 have compact supports with the diameters that are at most R, we have the following approximation error :\nE [\u2223\u2223\u2223S\u0302Wpp(\u00b5\u0302M1m , \u00b5\u0302M2m ;L)\u2212 SWpp(\u00b5M1 , \u00b5M2)\u2223\u2223\u2223] \u2264 RCp,R \u221a (d+ 1) logm\nm + 1\u221a L E [ Var [ Wpp(\u03b8\u266f\u00b5\u0302 M1 m , \u03b8\u266f\u00b5\u0302 M2 m ) ]1/2 |X1:m, Y1:m] , for an universal constant Cp,R > 0. The variance is with respect to \u03b8 \u223c U(Sd\u22121).\nTheorem 1 suggests that when using empirical probability measure to approximate meshes, the error between the Monte Carlo estimation of sliced Wasserstein distance between empirical distributions over two sampled point-clouds and sliced Wasserstein distance between two continuous distributions on meshes surface is bounded by the rate of m\u22121/2 and L\u22121/2. It means that, when increasing the number of points and the number of projections, the error reduces by the square root of them. This rate is very fast since it does not depend exponentially on the dimension, hence, it is scalable to meshes with high-dimensional features at vertices, such as normals, colors, and so on. Leveraging the scaling property and the approximation of varifold to meshMmentioned in Sec 3.1, we can represent meshes as discrete measures \u00b5\u0303M and optimize S\u0302W p\np(\u00b5\u0303 M1 , \u00b5\u0303M2 ;L) as the objective function. The\nproof of Theorem 1 is given in Appendix C. It is worth noting that a similar property is not able to be derived for Chamfer since it is a discrepancy on sets that cannot be generalized to compare meshes."
        },
        {
            "heading": "3.3 SLICED WASSERSTEIN DIFFEOMORPHIC FLOW FOR CORTICAL SURFACE RECONSTRUCTION",
            "text": "Diffeomorphic deformation framework for reconstructing cortical surfaces. In this section, we present DDOT that incorporates diffeomorphic deformation and sliced Wasserstein distance to reconstruct the cortical surface. Specifically, our goal is to derive a high-resolution, 2D manifold of the white matter that is topologically accurate from a 3D brain MR image. Let I \u2208 RD\u00d7W\u00d7H be a MRI volume andM = (V,F) be a 3D triangle mesh. The corresponding vertices of the mesh are represented by v \u2208 R3. Firstly, we train a U-Net model to automatically predict the white matter segmentation mask from I. Then, a signed distance function (SDF) is extracted from the binary mask before employing Marching Cubes (Lorensen & Cline, 1987) to get the initial surface. After getting the initial surfaceM0, the trajectory of each coordinate v0 is modeled via the ODE with initial condition from Eq. 3. Inspired from (Ma et al., 2021; 2022), we concatenate the point features with the corresponding cube features sampling from I as a new feature vector. The new feature is passed through a multilayer perceptron F\u03d5 to learn the deformation. More implementation settings are included in the Appendix D.\nSliced Wasserstein distance as a loss function. To train the DDOT model, we minimize the distance between predicted mesh M\u0302 and the ground truth meshM\u2217. We adopt a novel way of transforming mesh into a probability measure and leveraging sliced Wasserstein distance (SWD) as a loss function to optimize two discrete meshes. As discussed, the SW is a valid metric on the space of distribution and can guarantee the convergence of the probability measure. Moreover, as shown in Theorem 1, the sample complexity of the SW is bounded with a parametric rate, hence, it is suitable to use the SW to compare empirical probability measures as the proxy for the continuous mesh probability measure. Therefore, we sample points on M\u0302 andM\u2217 as probability measures and compute SWD loss between these two measures without regularization terms. Additionally, we can represent mesh as discrete probability measures, i.e. oriented varifold, and utilize the same objective function. Based on our observations, the varifold representation has shown better performance compared to encoding using empirical probability measures, which we provide a more detailed comparison in our ablation study in Sec. 4.3. As a result, we assume that our experiments in the following sections will be conducted using the oriented varifold approach unless otherwise specified."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": "Within this section, we first provide detailed settings and conduct extensive experiments on multiple datasets. Furthermore, we also give a comprehensive ablation study to validate our findings."
        },
        {
            "heading": "4.1 EXPERIMENTAL SETTINGS",
            "text": "Datasets. We conduct experiments on three publicly available datasets: ADNI dataset (Jack Jr et al., 2008), OASIS dataset (Marcus et al., 2007), and TRT dataset (Maclaren et al., 2014). The pseudo-ground truth surfaces are obtained from Freesurfer v5.3 (Fischl, 2012). We want to emphasize that using pseudo-ground truth as a reference is a standard practice in brain image analysis, adopted by all of the methods we have included in our comparison (Cruz et al., 2021; Bongratz et al., 2022; Santa Cruz et al., 2022; Ma et al., 2022). Therefore, using pseudo-ground truth does not limit the significance of our contributions within the context of this paper. Regarding data split, we carefully stratified (at the patient-level) each dataset into train, valid, and test sets. We select the best checkpoint based on train and validation set, subsequently reporting the outcomes on the unseen test set. Additional dataset details are available in Appendix E.\nBaselines. We reproduce all competing methods using their official implementations and recommended experimental settings based on our split for a fair comparison. Specifically, we reproduce DeepCSR (Cruz et al., 2021) in both the occupancy field and signed distance function (SDF) and report SDF results due to better performance. For Vox2Cortex (Bongratz et al., 2022), we use the authors\u2019 suggestion with a high-resolution template with \u2248 168, 000 vertices for each structure to get the best performance. Regarding CorticalFlow (Lebrat et al., 2021), we reproduce with their improved version settings CFPP (Santa Cruz et al., 2022). Finally, we retrain CortexODE (Ma et al., 2022) with default settings.\nMetrics. We employ various metrics including earth mover\u2019s distance (EMD), sliced Wasserstein distance (SWD), average symmetric surface distance (ASSD), Chamfer normals (CN), and selfintersection faces ratio (SI). We sample 100K points over the predicted and target surface to compute EMD, SWD, ASSD, and CN. Due to the large number of sampled points, we estimate EMD using entropic regularization and the Sinkhorn algorithm from (Feydy et al., 2019). Furthermore, we determine SI faces using PyMesh (Zhou, 2019) library."
        },
        {
            "heading": "4.2 RESULTS & DISCUSSION",
            "text": ""
        },
        {
            "heading": "4.2.1 RESULTS",
            "text": "Geometric accuracy. As shown in Tab. 1, our DDOT provides more geometrically accurate surfaces than other competing methods in multiple metrics. Qualitative results from Fig. 2 also indicate that our proposed method is closer to the ground truth than competing methods.\nSelf-intersection. Compared to GNN deformation-based methods such as Vox2Cortex with no SI-free theoretical guarantee, diffeomorphic deformation-based methods such as CFPP, CortexODE, and our proposed approach has much less SI. However, despite the nice property of existence and uniqueness of the solution of ODE models, CFPP and CortexODE still introduce a certain amount of SI faces since they both rely on the optimization of Chamfer divergence on discretized vertices of the mesh. Our approach, on the other hand, represents mesh as probability measures and has strong theoretical optimization support by employing efficient optimal transport metric, thus can approximate the mesh much better with almost no SI faces, i.e. less than 10\u22124%. It is worth noting that DDOT is 100\u00d7 better in terms of SIF score compared to CortexODE, the current SOTA in CSR task. DeepCSR introduces no SI thanks to Marching Cubes (Lorensen & Cline, 1987) from the implicit surface but often has other artifacts and requires extensive topology correction.\nConsistency. We compare the consistency of our DDOT, Vox2Cortex, CortexODE (which are all trained on OASIS), and FreeSurfer on the TRT dataset. We reconstruct white matter cortical surfaces from MRI images of the same subject on the same day and evaluated the EMD, SWD, and ASSD of the resulting reconstructions. The expectation is that the brain morphology of two consecutive scans taken on the same day should be similar to each other, except for the variations caused by the imaging process. To align pairs of images, we utilized the iterative closest-point algorithm (ICP) following (Cruz et al., 2021). As presented in Tab. 2, we outperform in both EMD and ASSD, and only Freesurfer (Fischl, 2012) result has the better performance in SWD score than us."
        },
        {
            "heading": "4.2.2 RUNNING TIME ANALYSIS",
            "text": "We compare the running time of CD loss, CD loss with regularization, and SWD loss, as shown in Fig. 3. The regularization of CD loss includes mesh edge loss, normal consistency, and Laplacian smoothing, which are commonly employed in mesh deformation frameworks (Bongratz et al., 2022; Santa Cruz et al., 2022). Firstly, as the number of supports increases, SWD loss consistently\nexhibits significantly faster performance compared to CD losses. This empirical finding further substantiates our assertion regarding the theoretical running time complexities of SWD (O(m logm)) and CD (O(m2)), with m denotes the number of supports. Secondly, regarding high-dimensional measures, e.g. varifolds, while the CD losses rigorously scale w.r.t. the dimension of the supports, SWD loss shows minimal variation as the number of supports increases, further support Theorem 1.\nIn conclusion, our proposed metric is scalable w.r.t. both the number of supports and the dimension of those supports, thus demonstrating the efficiency of our methods in learning-based frameworks.\n4.3 ABLATION STUDY\nTable 2: White matter surface reconstruction consistency comparison in terms of EMD, SWD, ASSD on TRT dataset.\nMethod EMD SWD ASSD\nVox2Cortex .886 \u00b1.130 .485 \u00b1.176 .263 \u00b1.112 CortexODE .799 \u00b1.038 .444 \u00b1.201 .241 \u00b1.040 FreeSurfer .859 \u00b1.213 .358 \u00b1.275 .286 \u00b1.156 Ours .780 \u00b1.032 .403 \u00b1.184 .229 \u00b1.010\nSetups. We evaluate the individual optimization design choices and report the WM surface reconstruction on ADNI dataset. For fair comparisons, we conduct ablation studies on the same initial surface and the same number of supports, i.e. the number of faces on mesh. We train all of them for 300 epochs and get the best checkpoints on the validation set. The result is reported in Tab. 3 on the holdout test set.\nComparisons. To begin with, we conduct experiments where we independently identically sample points on the surface and used SWD loss to optimize two sets of points. The results showed that this approach did not perform as well as optimizing SWD on oriented varifold representation. This indicates that the varifold method provides better mesh approximation compared to using random points on the surface. In the second part of our ablations, we use CD to optimize two oriented varifolds. Our results show that SWD loss outperforms CD on varifold, which further supports our Theorem 1. Finally, we employ the Sinkhorn divergence, implemented by (S\u00e9journ\u00e9 et al., 2019), as the loss function to optimize two oriented varifolds. It is worth noting that Sinkhorn divergence is the approximation of Wasserstein distance, but cannot lead to a valid metric between probability measures since the resulting discrepancy does not satisfy the triangle inequality. Experiments on both left and right WM show that our SWD loss outperforms Sinkhorn in both metrics."
        },
        {
            "heading": "5 LIMITATIONS AND FUTURE WORKS",
            "text": "Our work is the first learning-based deformation approach that tackles the local optimality problem of Chamfer distance on mesh deformation by employing efficient optimal transport theory on meshes as probability measures. Yet, it is not without its limitations, which present intriguing avenues for future exploration. Unlike the set-based approach that predefines the number of sampling supports, our optimization settings work best on the deterministic supports correlated with the mesh resolution, thus introducing stochastic memory during training. Our future work will focus on mitigating this issue by either employ-\ning remeshing techniques or ensuring a consistent cutoff number of supports for both the predicted mesh and the target mesh. Secondly, though the underlying proposed techniques have potential applications in other deformation tasks beyond CSR, within the context of this paper, we only focus on this task. It is intriguing to explore the potential applications of our approach in diverse domains."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "In this paper, we introduce a learning-based diffeomorphic deformation network that employs sliced Wasserstein distance (SWD) as the objective function to deform an initial mesh to an intricate mesh based on volumetric input. Different from previous approaches that use point-clouds for approximating mesh, we represent a mesh as a probability measure that generalizes the common set-based methods. By lying on probability measure space, we can further exploit statistical shape analysis theory to approximate mesh as an oriented varifold. Our theorem shows that leveraging sliced Wasserstein distance to optimize probability measures can have a fast statistical rate for approximating the surfaces of the meshes. Finally, we extensively verify our proposed approach in the challenging brain cortical surface reconstruction problem. Our experiment results demonstrate that our method surpasses existing state-of-the-art competing works in terms of geometric accuracy, self-intersection ratio, and consistency."
        },
        {
            "heading": "7 ETHICS STATEMENT",
            "text": "Potential impacts. Our work experiments on the human brain MRI dataset. Our proposed model holds the potential to assist neuroradiologists in effectively visualizing brain surfaces. However, it is crucial to emphasize that our predictions should not be utilized for making clinical decisions. This is because our model has solely undergone testing using the data discussed within this research, and we cannot ensure its performance on unseen data in clinical practices."
        },
        {
            "heading": "A RELATED WORKS",
            "text": "Deformation network for surface reconstruction. 3D surface reconstruction can be obtained from various approaches such as volumetric, implicit surfaces, and geometric deep learning methods. While volumetric-based (Choy et al., 2016; H\u00e4ne et al., 2017; Tatarchenko et al., 2017; Wang et al., 2018b; Cruz et al., 2021) and implicit surface-based (Mescheder et al., 2019; Park et al., 2019; Xu et al., 2019) methods can directly obtain surface by employing iso-surface extraction methods, such as Marching Cubes (Lorensen & Cline, 1987), they often require extensive post-processing to capture the high-quality resulting mesh. In contrast, geometric deep learning approaches use mesh deformation to achieve the target mesh while maintaining vertex connectivity (Pan et al., 2019; Smith et al., 2019; Wang et al., 2018a; Wickramasinghe et al., 2020; Gupta, 2020; Bongratz et al., 2022). Among deformation-based approaches, diffeomorphic deformation demonstrates its capability to perform well on complex manifolds while keeping the \u2018manifoldness\u2019 property (Gupta, 2020; Ma et al., 2022; Lebrat et al., 2021). However, those methods often use Chamfer divergence as their objective optimization, which is sub-optimal, especially on intricate manifolds such as cortical surfaces, i.e. as illustrated in Fig. 4. Therefore, in this work, we address the problem by employing efficient optimal transport in optimizing mesh during training diffeomorphic deformation models.\nMesh as varifold representation. Varifolds were initially introduced in the realm of geometric measure theory as a practical approach to tackle Plateau\u2019s problem (Almgren, 1966), which involves determining surfaces with a specified boundary that has the least area. Specifically, varifolds provide a convenient representation of geometric shapes, including rectifiable curves and surfaces, and serve as an effective geometric measure for optimization-based shape matching problems (Charon & Trouv\u00e9, 2013; Charon, 2013; Kaltenmark et al., 2017; Hsieh & Charon, 2020; Rekik et al., 2016; Ma et al., 2010). In this work, we focus on employing varifold as a discrete measure approximating the mesh. To the best of our knowledge, we are the first to exploit oriented varifolds as discrete probability measures in the learning-based deformation framework."
        },
        {
            "heading": "B TOY EXAMPLES",
            "text": "Setups. In this toy example, we aim to deform the template circle to the target polygon in an optimization-based. We uniformly sample the template circle and the target polygon into 2D points. The number of sampled points on both the template circle and target polygon are 678 points. To optimize the position of predicted points and target points, we employ Chamfer loss implemented by (Ravi et al., 2020), and the sliced Wasserstein distance with p = 2 approximated by Monte Carlo estimation with 100 projections. We optimize the two sets of points with stochastic gradient descent (SGD) optimizer with a learning rate of 1.0 and momentum of 0.9 for 1000 iterations.\nDiscussion. As shown in Fig. 5, we can see that the set of points optimized by Chamfer distance often gets trapped in some specific region, e.g. the acute region of the polygon in this example. This confinement occurs due to the nature of Chamfer distance, which primarily focuses on optimizing nearest neighbors, inhibiting the points from escaping the local region during the optimization process. To alleviate this issue, practitioners often introduce multiple losses as regularizers to aid Chamfer distance in escaping local minima. However, determining the appropriate weights for each auxiliary loss is a challenging task, as they tend to vary across different tasks, thus making the optimization process harder. SWD loss, on the other hand, can find the optimal transport plan for the whole set of points, thus resulting in a better solution when compared to Chamfer distance."
        },
        {
            "heading": "C PROOF OF THEOREM 1",
            "text": "Using the triangle inequality of the L1 norm, we obtain:\nE [\u2223\u2223\u2223S\u0302Wpp(\u00b5\u0302M1m , \u00b5\u0302M2m ;L)\u2212 SWpp(\u00b5M1 , \u00b5M2)\u2223\u2223\u2223] \u2264 E [\u2223\u2223\u2223S\u0302Wpp(\u00b5\u0302M1m , \u00b5\u0302M2m ;L)\u2212 SWpp(\u00b5\u0302M1m , \u00b5\u0302M2m )\u2223\u2223\u2223]\n+ E [\u2223\u2223SWpp(\u00b5\u0302M1m , \u00b5\u0302M2m )\u2212 SWpp(\u00b5M1 , \u00b5M2)\u2223\u2223] .\nNow, we bound the first term E [\u2223\u2223\u2223S\u0302Wpp(\u00b5\u0302M1m , \u00b5\u0302M2m ;L)\u2212 SWpp(\u00b5\u0302M1m , \u00b5\u0302M2m )\u2223\u2223\u2223]. By the definitions of\nthe SW and its Monte Carlo estimation, we have:\nE [\u2223\u2223\u2223S\u0302Wpp(\u00b5\u0302M1m , \u00b5\u0302M2m ;L)\u2212 SWpp(\u00b5\u0302M1m , \u00b5\u0302M2m )\u2223\u2223\u2223] = E [\u2223\u2223\u2223\u2223\u2223 1L L\u2211\nl=1\nWpp(\u03b8l\u266f\u00b5\u0302 M1 m , \u03b8l\u266f\u00b5\u0302 M2 m )\u2212 E[Wpp(\u03b8\u266f\u00b5\u0302M1m , \u03b8\u266f\u00b5\u0302M2m )|X1:m, Y1:m] \u2223\u2223\u2223\u2223\u2223 ]\n\u2264 E ( 1\nL L\u2211 l=1 Wpp(\u03b8l\u266f\u00b5\u0302 M1 m , \u03b8l\u266f\u00b5\u0302 M2 m )\u2212 E[Wpp(\u03b8\u266f\u00b5\u0302M1m , \u03b8\u266f\u00b5\u0302M2m )|X1:m, Y1:m]\n)2 12\n= E ( 1\nL L\u2211 l=1 ( Wpp(\u03b8l\u266f\u00b5\u0302 M1 m , \u03b8l\u266f\u00b5\u0302 M2 m )\u2212 E[Wpp(\u03b8\u266f\u00b5\u0302M1m , \u03b8\u266f\u00b5\u0302M2m )|X1:m, Y1:m] ))2 12 , where the inequality is due to the Holder\u2019s inequality. Using the fact that E [ 1 L \u2211L l=1 W p p(\u03b8l\u266f\u00b5\u0302 M1 m , \u03b8l\u266f\u00b5\u0302 M2 m |X1:m, Y1:m) ] = E[Wpp(\u03b8\u266f\u00b5\u0302M1m , \u03b8\u266f\u00b5\u0302M2m )|X1:m, Y1:m] since\n\u03b81, . . . , \u03b8L i.i.d\u223c U(Sd\u22121), we have:\nE [\u2223\u2223\u2223S\u0302Wpp(\u00b5\u0302M1m , \u00b5\u0302M2m ;L)\u2212 SWpp(\u00b5\u0302M1m , \u00b5\u0302M2m )\u2223\u2223\u2223] \u2264 ( Var [( 1\nL L\u2211 l=1 Wpp(\u03b8l\u266f\u00b5\u0302 M1 m , \u03b8l\u266f\u00b5\u0302 M2 m )|X1:m, Y1:m]\n)]) 1 2\n= 1\u221a L\nVar [ Wpp(\u03b8\u266f\u00b5\u0302 M1 m , \u03b8\u266f\u00b5\u0302 M2 m )|X1:m, Y1:m] ] 1 2 .\nNow, we bound the second term E [\u2223\u2223SWpp(\u00b5\u0302M1m , \u00b5\u0302M2m )\u2212 SWpp(\u00b5M1 , \u00b5M2)\u2223\u2223]. Using the Jensen inequality, we obtain E [\u2223\u2223SWpp(\u00b5\u0302M1m , \u00b5\u0302M2m )\u2212 SWpp(\u00b5M1 , \u00b5M2)\u2223\u2223] = E [\u2223\u2223E[Wpp(\u03b8\u266f\u00b5\u0302M1m , \u03b8\u266f\u00b5\u0302M2m )]\u2212 E[Wpp(\u03b8\u266f\u00b5M1 , \u03b8\u266f\u00b5M2)]\u2223\u2223]\n\u2264 E [ |Wpp(\u03b8\u266f\u00b5\u0302M1m , \u03b8\u266f\u00b5\u0302M2m )\u2212Wpp(\u03b8\u266f\u00b5M1 , \u03b8\u266f\u00b5M2)| ] \u2264 C(1)p,RE[|W1(\u03b8\u266f\u00b5\u0302 M1 m , \u03b8\u266f\u00b5 M1) + W1(\u03b8\u266f\u00b5\u0302M2m , \u03b8\u266f\u00b5 M2)|]\n= C (1) p,R(E[W1(\u03b8\u266f\u00b5\u0302 M1 m , \u03b8\u266f\u00b5 M1)] + E[W1(\u03b8\u266f\u00b5\u0302M2m , \u03b8\u266f\u00b5M2)]),\nwhere the second inequality is due to Lemma 4 in (Goldfeld et al., 2022). We now show that:\nE[W1(\u03b8\u266f\u00b5\u0302M1m , \u03b8\u266f\u00b5M1)] \u2264 C (2) R\n\u221a (d+ 1) logm\nm .\nFollowing (Nguyen et al., 2021a; Nguyen & Ho, 2023b), we have:\nE[W1(\u03b8\u266f\u00b5\u0302M1m , \u03b8\u266f\u00b5M1)] \u2264 E [ max\n\u03b8\u2208Sd\u22121 W1(\u03b8\u266f\u00b5\u0302\nM1 m , \u03b8\u266f\u00b5 M1)|X1:m, Y1:m ]\n= E [\nmax \u03b8\u2208Rd|\u2225\u03b8\u22252\u22641\nW1(\u03b8\u266f\u00b5\u0302 M1 m , \u03b8\u266f\u00b5 M1)|X1:m, Y1:m ]\n= E [\nmax \u03b8\u2208Rd|\u2225\u03b8\u22252\u22641 \u222b 1 0 |F\u22121m,\u03b8(z)\u2212 F \u22121 \u03b8 (z)|dz|X1:m, Y1:m ] ,\nwhere F\u22121m,\u03b8(z) is the inverse CDF of \u00b5\u0302 M1 m , F \u22121 \u03b8 (z) is the inverse CDF of \u00b5 M1 . Using the assumption that the diameter of \u00b5M1 is at most R, we have:\nE [\nmax \u03b8\u2208Rd|\u2225\u03b8\u22252\u22641 \u222b 1 0 |F\u22121m,\u03b8(z)\u2212 F \u22121 \u03b8 (z)|dz|X1:m, Y1:m ] = E [ max\n\u03b8\u2208Rd|\u2225\u03b8\u22252\u22641 \u222b \u221e \u2212\u221e |Fm,\u03b8(y)\u2212 F\u03b8(y)|dy|X1:m, Y1:m ] \u2264 RE [ sup\ny\u2208R,\u03b8\u2208Rd|\u2225\u03b8\u22252\u22641 |Fm,\u03b8(y)\u2212 F\u03b8(y)|\n]\n= RE [ sup C\u2208B |\u00b5\u0302M1m (C)\u2212 \u00b5M1(C)| ] ,\nwhere B = {x \u2208 Rd|\u03b8\u22a4x \u2264 y} is set of half spaces for \u03b8 and y. Since the Vapnik-Chervonenkis (VC) dimension of B is upper bounded by d + 1 (Wainwright, 2019), applying the VC inequality results:\nE [ sup C\u2208B |\u00b5\u0302M1m (C)\u2212 \u00b5M1(C)| ] \u2264 C \u221a (d+ 1) logm m ,\nfor a constant C > 0. Absorbing constants, we obtain the final result. Therefore, we conclude the proof.\nD IMPLEMENTATION DETAILS\nNetwork architecture. First of all, for the white matter segmentation model, we train a vanilla 3D U-Net model with sequential 3D convolutional layers with kernel size 3 \u00d7 3 \u00d7 3. Secondly, for the deformation network, for each v0 \u2208 R3, we linearly transform it to a 128-dimensional feature vector. For each v0, we find a corresponding cube size 5\u00d7 5\u00d7 5 on I. This process can be repeated across multiple scales of I, resulting in the extraction of multiple cubes. Then, we apply a 3D convolution layer followed by a linear layer to transform the spatial cubes to a 128-dimensional feature vector, i.e. same as the feature of v0. Once having the v0\u2019s features and its corresponding spatial features, we concatenate them as a new feature before passing through an MLP, namely F\u03d5, to learn the deformation. As discussed, we represent the predicted mesh M\u0302 and the target meshM\u2217 as probability measures \u00b5M\u0302 and \u00b5M \u2217 , respectively. In practice, we can substitute discrete probability measures, e.g. oriented varifold, as \u00b5\u0303M\u0302 and \u00b5\u0303M \u2217 , respectively. The loss function is computed as follows:\nL(M\u0302,M\u2217) = S\u0302W p p(\u00b5\u0303 M\u0302, \u00b5\u0303M \u2217 ) = 1\nL L\u2211 l=1 Wpp(\u03b8l\u266f\u00b5\u0303 M\u0302, \u03b8l\u266f\u00b5\u0303 M\u2217),\nwhere Wpp(\u03b8l\u266f\u00b5\u0303 M\u0302, \u03b8l\u266f\u00b5\u0303 M\u2217) is the Wasserstein-p (Villani, 2003) distance between \u00b5\u0303M\u0302 and \u00b5\u0303M \u2217 . We fix L = 100, p = 2 for all of our experiments. In terms of oriented varifold, for each support, we concatenate the barycenter of vertices of the face and the unit normal vector as a single vector in R6. The training procedure is described in Algo. 1.\nTraining details. We optimize both segmentation and deformation networks with Adam optimizer (Kingma & Ba, 2014) with a fixed learning rate 10\u22124. We train the segmentation and the deformation networks for 100 and 300 epochs, respectively, and get the best checkpoint on the validation set. All experiments are implemented using Pytorch and executed on a system equipped with an NVIDIA RTX A6000 GPU and an Intel i7-7700K CPU."
        },
        {
            "heading": "E DATASET INFORMATION",
            "text": "Dataset split. As discussed in Sec. 4.1, we employ three publicly available datasets: the Alzheimer\u2019s Disease Neuroimaging Initiative (ADNI) dataset (Jack Jr et al., 2008), the Open Access Series of Imaging Studies (OASIS) dataset (Marcus et al., 2007), and the test-retest (TRT) dataset (Maclaren\nAlgorithm 1 Training cortical surface reconstruction with SWD distance Input: MRI volume I, initial meshM0 = (V0,F), learning rate \u03b7, max iter T , number projections L. Initialization: Deformation network F\u03d5(I,M0) .\nwhile \u03d5 not converge or reach T do \u2207\u03d5 \u2190 0 \u25b7 Zero gradient. V\u0302 \u2190 ODESolver(F\u03d5,V0) \u25b7 Deform source vertices V0 to V\u0302 . M\u0302 \u2190 (V\u0302,F) \u25b7 Get predicted mesh. \u00b5\u0303M\u0302 \u2190 ToMeasure(M\u0302); \u00b5\u0303M\u2217 \u2190 ToMeasure(M\u2217) \u25b7 Transform to discrete measures. \u2207\u03d5 \u2190 \u2207\u03d5 + 1L \u2211L l=1\u2207\u03d5W pp (\u03b8l\u266f\u00b5\u0303M\u0302, \u03b8l\u266f\u00b5\u0303M \u2217 ) \u25b7 Update gradient.\n\u03d5\u2190 \u03d5\u2212 \u03b7 \u00b7 \u2207\u03d5 \u25b7 Update parameters. end while Return: \u03d5M\u2192M\u2217\net al., 2014). A subset of the ADNI dataset (Jack Jr et al., 2008) is employed, consisting of 419 T1-weighted (T1w) brain MRI from subjects aged from 55 to 90 years old. The dataset is stratified into 299 scans for training (\u2248 70%), 40 scans for validation(\u2248 10%), and 80 scans for testing (\u2248 20%). Regarding the OASIS dataset (Marcus et al., 2007), all 416 T1-weighted (T1w) brain MRI images are included. We stratify the dataset into 292 scans for training (\u2248 70%), 44 scans for validation (\u2248 10%), and 80 scans for testing (\u2248 20%). As for the TRT dataset (Maclaren et al., 2014), it consists of 120 scans obtained from three distinct subjects, with each subject undergoing two scans within a span of 20 days.\nPreprocess. We strictly follow the pre-processing pipeline from (Bongratz et al., 2022). Specifically, we first register the MRIs to the MNI152 scan. After padding the input images to have shape 192\u00d7 208\u00d7 192, we resize them to 128\u00d7 144\u00d7 128. The intensity values are min-max-normalized to the range [0, 1].\nF VISUALIZATIONS\nWe provide more visualization of our work as in Fig. 6. We randomly select the prediction meshes from the test set and compute the point-to-surface distance. The color is encoded as how far the point is to the surface. The figures say that the darker color is, the further the predicted mesh to the pseudo-ground truth."
        }
    ],
    "year": 2023
}