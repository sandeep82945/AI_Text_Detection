{
    "abstractText": "Accurate 3D lane estimation is crucial for ensuring safety in autonomous driving. However, prevailing monocular techniques suffer from depth loss and lighting variations, hampering accurate 3D lane detection. In contrast, LiDAR points offer geometric cues and enable precise localization. In this paper, we present DV3DLane, a novel end-to-end Dual-View multi-modal 3D Lane detection framework that synergizes the strengths of both images and LiDAR points. We propose to learn multi-modal features in dual-view spaces, i.e., perspective view (PV) and bird\u2019s-eye-view (BEV), effectively leveraging the modal-specific information. To achieve this, we introduce three designs: 1) A bidirectional feature fusion strategy that integrates multi-modal features into each view space, exploiting their unique strengths. 2) A unified query generation approach that leverages laneaware knowledge from both PV and BEV spaces to generate queries. 3) A 3D dual-view deformable attention mechanism, which aggregates discriminative features from both PV and BEV spaces into queries for accurate 3D lane detection. Extensive experiments on the public benchmark, OpenLane, demonstrate the efficacy and efficiency of DV-3DLane. It achieves state-of-the-art performance. with a remarkable 11.2 gain in F1 score and a substantial 53.5% reduction in errors. The code is available at https://github.com/JMoonr/dv-3dlane.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yueru Luo"
        },
        {
            "affiliations": [],
            "name": "Shuguang Cui"
        },
        {
            "affiliations": [],
            "name": "Zhen Li"
        }
    ],
    "id": "SP:53545d859a7dcffe5c74fce0a130ed5f64cc55fa",
    "references": [
        {
            "authors": [
                "Jianyong Ai",
                "Wenbo Ding",
                "Jiuhua Zhao",
                "Jiachen Zhong"
            ],
            "title": "Ws-3d-lane: Weakly supervised 3d lane detection with 2d lane labels",
            "venue": "IEEE International Conference on Robotics and Automation (ICRA),",
            "year": 2023
        },
        {
            "authors": [
                "Min Bai",
                "Gellert Mattyus",
                "Namdar Homayounfar",
                "Shenlong Wang",
                "Shrinidhi Kowshika Lakshmikanth",
                "Raquel Urtasun"
            ],
            "title": "Deep multi-sensor lane detection",
            "venue": "In 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),",
            "year": 2018
        },
        {
            "authors": [
                "Xuyang Bai",
                "Zeyu Hu",
                "Xinge Zhu",
                "Qingqiu Huang",
                "Yilun Chen",
                "Hongbo Fu",
                "Chiew-Lan Tai"
            ],
            "title": "Transfusion: Robust lidar-camera fusion for 3d object detection with transformers",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Yifeng Bai",
                "Zhirong Chen",
                "Zhangjie Fu",
                "Lang Peng",
                "Pengpeng Liang",
                "Erkang Cheng"
            ],
            "title": "Curveformer: 3d lane detection by curve propagation with curve queries and attention",
            "venue": "arXiv preprint arXiv:2209.07989,",
            "year": 2022
        },
        {
            "authors": [
                "Nicolas Carion",
                "Francisco Massa",
                "Gabriel Synnaeve",
                "Nicolas Usunier",
                "Alexander Kirillov",
                "Sergey Zagoruyko"
            ],
            "title": "End-to-end object detection with transformers",
            "venue": "In Computer Vision\u2013ECCV 2020: 16th European Conference,",
            "year": 2020
        },
        {
            "authors": [
                "Li Chen",
                "Chonghao Sima",
                "Yang Li",
                "Zehan Zheng",
                "Jiajie Xu",
                "Xiangwei Geng",
                "Hongyang Li",
                "Conghui He",
                "Jianping Shi",
                "Yu Qiao",
                "Junchi Yan"
            ],
            "title": "Persformer: 3d lane detection via perspective transformer and the openlane benchmark",
            "venue": "In European Conference on Computer Vision (ECCV),",
            "year": 2022
        },
        {
            "authors": [
                "Xuanyao Chen",
                "Tianyuan Zhang",
                "Yue Wang",
                "Yilun Wang",
                "Hang Zhao"
            ],
            "title": "Futr3d: A unified sensor fusion framework for 3d detection",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Tianheng Cheng",
                "Xinggang Wang",
                "Shaoyu Chen",
                "Wenqiang Zhang",
                "Qian Zhang",
                "Chang Huang",
                "Zhaoxiang Zhang",
                "Wenyu Liu"
            ],
            "title": "Sparse instance activation for real-time instance segmentation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Netalee Efrat",
                "Max Bluvstein",
                "Shaul Oron",
                "Dan Levi",
                "Noa Garnett",
                "Bat El Shlomo"
            ],
            "title": "3d-lanenet+: Anchor free lane detection using a semi-local representation",
            "venue": "arXiv preprint arXiv:2011.01535,",
            "year": 2020
        },
        {
            "authors": [
                "Zhengyang Feng",
                "Shaohua Guo",
                "Xin Tan",
                "Ke Xu",
                "Min Wang",
                "Lizhuang Ma"
            ],
            "title": "Rethinking efficient lane detection via curve modeling",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Noa Garnett",
                "Rafi Cohen",
                "Tomer Pe\u2019er",
                "Roee Lahav",
                "Dan Levi"
            ],
            "title": "3d-lanenet: end-to-end 3d multiple lane detection",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2019
        },
        {
            "authors": [
                "Yuliang Guo",
                "Guang Chen",
                "Peitao Zhao",
                "Weide Zhang",
                "Jinghao Miao",
                "Jingao Wang",
                "Tae Eun Choe"
            ],
            "title": "Gen-lanenet: A generalized and scalable approach for 3d lane detection",
            "venue": "In European Conference on Computer Vision,",
            "year": 2020
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Yuenan Hou",
                "Zheng Ma",
                "Chunxiao Liu",
                "Chen Change Loy"
            ],
            "title": "Learning lightweight lane detection cnns by self attention distillation",
            "venue": "In Proceedings of the IEEE/CVF international conference on computer vision,",
            "year": 2019
        },
        {
            "authors": [
                "Shaofei Huang",
                "Zhenwei Shen",
                "Zehao Huang",
                "Zi-han Ding",
                "Jiao Dai",
                "Jizhong Han",
                "Naiyan Wang",
                "Si Liu"
            ],
            "title": "Anchor3dlane: Learning to regress 3d anchors for monocular 3d lane detection",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Eric Jang",
                "Shixiang Gu",
                "Ben Poole"
            ],
            "title": "Categorical reparameterization with gumbel-softmax",
            "venue": "arXiv preprint arXiv:1611.01144,",
            "year": 2016
        },
        {
            "authors": [
                "Dongkwon Jin",
                "Wonhui Park",
                "Seong-Gyun Jeong",
                "Heeyeon Kwon",
                "Chang-Su Kim"
            ],
            "title": "Eigenlanes: Data-driven lane descriptors for structurally diverse lanes",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "arXiv preprint arXiv:1412.6980,",
            "year": 2014
        },
        {
            "authors": [
                "Yeongmin Ko",
                "Younkwan Lee",
                "Shoaib Azam",
                "Farzeen Munir",
                "Moongu Jeon",
                "Witold Pedrycz"
            ],
            "title": "Key points estimation and point instance segmentation approach for lane detection",
            "venue": "IEEE Transactions on Intelligent Transportation Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Alex H Lang",
                "Sourabh Vora",
                "Holger Caesar",
                "Lubing Zhou",
                "Jiong Yang",
                "Oscar Beijbom"
            ],
            "title": "Pointpillars: Fast encoders for object detection from point clouds",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Seokju Lee",
                "Junsik Kim",
                "Jae Shin Yoon",
                "Seunghak Shin",
                "Oleksandr Bailo",
                "Namil Kim",
                "Tae-Hee Lee",
                "Hyun Seok Hong",
                "Seung-Hoon Han",
                "In So Kweon"
            ],
            "title": "Vpgnet: Vanishing point guided network for lane and road marking detection and recognition",
            "venue": "In Proceedings of the IEEE international conference on computer vision,",
            "year": 2017
        },
        {
            "authors": [
                "Chenguang Li",
                "Jia Shi",
                "Ya Wang",
                "Guangliang Cheng"
            ],
            "title": "Reconstruct from top view: A 3d lane detection approach based on geometry structure prior",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Xiang Li",
                "Jun Li",
                "Xiaolin Hu",
                "Jian Yang"
            ],
            "title": "Line-cnn: End-to-end traffic line detection with line proposal unit",
            "venue": "IEEE Transactions on Intelligent Transportation Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Yingwei Li",
                "Adams Wei Yu",
                "Tianjian Meng",
                "Ben Caine",
                "Jiquan Ngiam",
                "Daiyi Peng",
                "Junyang Shen",
                "Yifeng Lu",
                "Denny Zhou",
                "Quoc V Le"
            ],
            "title": "Deepfusion: Lidar-camera deep fusion for multimodal 3d object detection",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Yinhao Li",
                "Zheng Ge",
                "Guanyi Yu",
                "Jinrong Yang",
                "Zengran Wang",
                "Yukang Shi",
                "Jianjian Sun",
                "Zeming Li"
            ],
            "title": "Bevdepth: Acquisition of reliable depth for multi-view 3d object detection",
            "venue": "arXiv preprint arXiv:2206.10092,",
            "year": 2022
        },
        {
            "authors": [
                "Zhiqi Li",
                "Wenhai Wang",
                "Hongyang Li",
                "Enze Xie",
                "Chonghao Sima",
                "Tong Lu",
                "Yu Qiao",
                "Jifeng Dai"
            ],
            "title": "Bevformer: Learning bird\u2019s-eye-view representation from multi-camera images via spatiotemporal transformers",
            "venue": "In Computer Vision\u2013ECCV 2022: 17th European Conference,",
            "year": 2022
        },
        {
            "authors": [
                "James Liang",
                "Tianfei Zhou",
                "Dongfang Liu",
                "Wenguan Wang"
            ],
            "title": "Clustseg: Clustering for universal segmentation",
            "venue": "arXiv preprint arXiv:2305.02187,",
            "year": 2023
        },
        {
            "authors": [
                "Ming Liang",
                "Bin Yang",
                "Yun Chen",
                "Rui Hu",
                "Raquel Urtasun"
            ],
            "title": "Multi-task multi-sensor fusion for 3d object detection",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Tingting Liang",
                "Hongwei Xie",
                "Kaicheng Yu",
                "Zhongyu Xia",
                "Zhiwei Lin",
                "Yongtao Wang",
                "Tao Tang",
                "Bing Wang",
                "Zhi Tang"
            ],
            "title": "Bevfusion: A simple and robust lidar-camera fusion framework",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Bencheng Liao",
                "Shaoyu Chen",
                "Xinggang Wang",
                "Tianheng Cheng",
                "Qian Zhang",
                "Wenyu Liu",
                "Chang Huang"
            ],
            "title": "Maptr: Structured modeling and learning for online vectorized hd map construction",
            "venue": "arXiv preprint arXiv:2208.14437,",
            "year": 2022
        },
        {
            "authors": [
                "Tsung-Yi Lin",
                "Priya Goyal",
                "Ross Girshick",
                "Kaiming He",
                "Piotr Doll\u00e1r"
            ],
            "title": "Focal loss for dense object detection",
            "venue": "In Proceedings of the IEEE international conference on computer vision,",
            "year": 2017
        },
        {
            "authors": [
                "Huan Liu",
                "Qiang Chen",
                "Zichang Tan",
                "Jiang-Jiang Liu",
                "Jian Wang",
                "Xiangbo Su",
                "Xiaolong Li",
                "Kun Yao",
                "Junyu Han",
                "Errui Ding"
            ],
            "title": "Group pose: A simple baseline for end-to-end multi-person pose estimation",
            "venue": "arXiv preprint arXiv:2308.07313,",
            "year": 2023
        },
        {
            "authors": [
                "Rosanne Liu",
                "Joel Lehman",
                "Piero Molino",
                "Felipe Petroski Such",
                "Eric Frank",
                "Alex Sergeev",
                "Jason Yosinski"
            ],
            "title": "An intriguing failing of convolutional neural networks and the coordconv solution",
            "venue": "Advances in neural information processing systems,",
            "year": 2018
        },
        {
            "authors": [
                "Ruijin Liu",
                "Zejian Yuan",
                "Tie Liu",
                "Zhiliang Xiong"
            ],
            "title": "End-to-end lane shape prediction with transformers",
            "venue": "In Proceedings of the IEEE/CVF winter conference on applications of computer vision,",
            "year": 2021
        },
        {
            "authors": [
                "Ruijin Liu",
                "Dapeng Chen",
                "Tie Liu",
                "Zhiliang Xiong",
                "Zejian Yuan"
            ],
            "title": "Learning to predict 3d lane shape and camera pose from a single image via geometry constraints",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "Zhijian Liu",
                "Haotian Tang",
                "Alexander Amini",
                "Xinyu Yang",
                "Huizi Mao",
                "Daniela L Rus",
                "Song Han"
            ],
            "title": "Bevfusion: Multi-task multi-sensor fusion with unified bird\u2019s-eye view representation",
            "venue": "IEEE International Conference on Robotics and Automation (ICRA),",
            "year": 2023
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter"
            ],
            "title": "Sgdr: Stochastic gradient descent with warm restarts",
            "venue": "arXiv preprint arXiv:1608.03983,",
            "year": 2016
        },
        {
            "authors": [
                "Yueru Luo",
                "Xu Yan",
                "Chaoda Zheng",
                "Chao Zheng",
                "Shuqi Mei",
                "Tang Kun",
                "Shuguang Cui",
                "Zhen Li"
            ],
            "title": "M\u02c6 2-3dlanenet: Multi-modal 3d lane detection",
            "venue": "arXiv preprint arXiv:2209.05996,",
            "year": 2022
        },
        {
            "authors": [
                "Yueru Luo",
                "Chaoda Zheng",
                "Xu Yan",
                "Tang Kun",
                "Chao Zheng",
                "Shuguang Cui",
                "Zhen Li"
            ],
            "title": "Latr: 3d lane detection from monocular images with transformer, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Yuexin Ma",
                "Tai Wang",
                "Xuyang Bai",
                "Huitong Yang",
                "Yuenan Hou",
                "Yaming Wang",
                "Yu Qiao",
                "Ruigang Yang",
                "Dinesh Manocha",
                "Xinge Zhu"
            ],
            "title": "Vision-centric bev perception: A survey",
            "venue": "arXiv preprint arXiv:2208.02797,",
            "year": 2022
        },
        {
            "authors": [
                "Hanspeter A Mallot",
                "Heinrich H B\u00fclthoff",
                "JJ Little",
                "Stefan Bohrer"
            ],
            "title": "Inverse perspective mapping simplifies optical flow computation and obstacle detection",
            "venue": "Biological cybernetics,",
            "year": 1991
        },
        {
            "authors": [
                "Sergiu Nedevschi",
                "Rolf Schmidt",
                "Thorsten Graf",
                "Radu Danescu",
                "Dan Frentiu",
                "Tiberiu Marita",
                "Florin Oniga",
                "Ciprian Pocol"
            ],
            "title": "3d lane detection system based on stereovision",
            "venue": "In Proceedings. The 7th International IEEE Conference on Intelligent Transportation Systems (IEEE Cat. No. 04TH8749),",
            "year": 2004
        },
        {
            "authors": [
                "Davy Neven",
                "Bert De Brabandere",
                "Stamatios Georgoulis",
                "Marc Proesmans",
                "Luc Van Gool"
            ],
            "title": "Towards end-to-end lane detection: an instance segmentation approach",
            "venue": "IEEE intelligent vehicles symposium (IV),",
            "year": 2018
        },
        {
            "authors": [
                "Aaron van den Oord",
                "Yazhe Li",
                "Oriol Vinyals"
            ],
            "title": "Representation learning with contrastive predictive coding",
            "venue": "arXiv preprint arXiv:1807.03748,",
            "year": 2018
        },
        {
            "authors": [
                "X Pan",
                "J Shi",
                "P Luo",
                "X Wang",
                "X Tang"
            ],
            "title": "Spatial as deep: spatial cnn for traffic scene understanding. 2017",
            "venue": "Pan X Shi J Luo P Spatial As Deep: Spatial CNN for Traffic Scene Understanding,",
            "year": 2017
        },
        {
            "authors": [
                "Jonah Philion",
                "Sanja Fidler"
            ],
            "title": "Lift, splat, shoot: Encoding images from arbitrary camera rigs by implicitly unprojecting to 3d",
            "venue": "In Computer Vision\u2013ECCV 2020: 16th European Conference,",
            "year": 2020
        },
        {
            "authors": [
                "Zhan Qu",
                "Huan Jin",
                "Yang Zhou",
                "Zhen Yang",
                "Wei Zhang"
            ],
            "title": "Focus on local: Detecting lane marker from bottom up via key point",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Shaoqing Ren",
                "Kaiming He",
                "Ross Girshick",
                "Jian Sun"
            ],
            "title": "Faster r-cnn: Towards real-time object detection with region proposal networks",
            "venue": "Advances in neural information processing systems,",
            "year": 2015
        },
        {
            "authors": [
                "Guangsheng Shi",
                "Ruifeng Li",
                "Chao Ma"
            ],
            "title": "Pillarnet: Real-time and high-performance pillar-based 3d object detection",
            "venue": "In European Conference on Computer Vision,",
            "year": 2022
        },
        {
            "authors": [
                "Vishwanath A Sindagi",
                "Yin Zhou",
                "Oncel Tuzel"
            ],
            "title": "Mvx-net: Multimodal voxelnet for 3d object detection",
            "venue": "In 2019 International Conference on Robotics and Automation (ICRA),",
            "year": 2019
        },
        {
            "authors": [
                "Pei Sun",
                "Henrik Kretzschmar",
                "Xerxes Dotiwalla",
                "Aurelien Chouard",
                "Vijaysai Patnaik",
                "Paul Tsui",
                "James Guo",
                "Yin Zhou",
                "Yuning Chai",
                "Benjamin Caine"
            ],
            "title": "Scalability in perception for autonomous driving: Waymo open dataset",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Lucas Tabelini",
                "Rodrigo Berriel",
                "Thiago M Paixao",
                "Claudine Badue",
                "Alberto F De Souza",
                "Thiago Oliveira-Santos"
            ],
            "title": "Keep your eyes on the lane: Real-time attention-guided lane detection",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Lucas Tabelini",
                "Rodrigo Berriel",
                "Thiago M Paixao",
                "Claudine Badue",
                "Alberto F De Souza",
                "Thiago Oliveira-Santos"
            ],
            "title": "Polylanenet: Lane estimation via deep polynomial regression",
            "venue": "In 2020 25th International Conference on Pattern Recognition (ICPR),",
            "year": 2021
        },
        {
            "authors": [
                "Wouter Van Gansbeke",
                "Bert De Brabandere",
                "Davy Neven",
                "Marc Proesmans",
                "Luc Van Gool"
            ],
            "title": "Endto-end lane detection through differentiable least-squares fitting",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops,",
            "year": 2019
        },
        {
            "authors": [
                "Chunwei Wang",
                "Chao Ma",
                "Ming Zhu",
                "Xiaokang Yang"
            ],
            "title": "Pointaugmenting: Cross-modal augmentation for 3d object detection",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Jinsheng Wang",
                "Yinchao Ma",
                "Shaofei Huang",
                "Tianrui Hui",
                "Fei Wang",
                "Chen Qian",
                "Tianzhu Zhang"
            ],
            "title": "A keypoint-based global association network for lane detection",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Ruihao Wang",
                "Jian Qin",
                "Kaiying Li",
                "Yaochen Li",
                "Dong Cao",
                "Jintao Xu"
            ],
            "title": "Bev-lanedet: An efficient 3d lane detection based on virtual camera via key-points",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Ze Wang",
                "Weiqiang Ren",
                "Qiang Qiu"
            ],
            "title": "Lanenet: Real-time lane detection networks for autonomous driving",
            "venue": "arXiv preprint arXiv:1807.01726,",
            "year": 2018
        },
        {
            "authors": [
                "Zhirong Wu",
                "Yuanjun Xiong",
                "Stella X Yu",
                "Dahua Lin"
            ],
            "title": "Unsupervised feature learning via nonparametric instance discrimination",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Hang Xu",
                "Shaoju Wang",
                "Xinyue Cai",
                "Wei Zhang",
                "Xiaodan Liang",
                "Zhenguo Li"
            ],
            "title": "Curvelane-nas: Unifying lane-sensitive architecture search and adaptive point blending",
            "venue": "In Computer Vision\u2013 ECCV 2020: 16th European Conference,",
            "year": 2020
        },
        {
            "authors": [
                "Shenghua Xu",
                "Xinyue Cai",
                "Bin Zhao",
                "Li Zhang",
                "Hang Xu",
                "Yanwei Fu",
                "Xiangyang Xue"
            ],
            "title": "Rclane: Relay chain prediction for lane detection",
            "venue": "In European Conference on Computer Vision,",
            "year": 2022
        },
        {
            "authors": [
                "Fan Yan",
                "Ming Nie",
                "Xinyue Cai",
                "Jianhua Han",
                "Hang Xu",
                "Zhen Yang",
                "Chaoqiang Ye",
                "Yanwei Fu",
                "Michael Bi Mi",
                "Li Zhang"
            ],
            "title": "Once-3dlanes: Building monocular 3d lane detection",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Zeyu Yang",
                "Jiaqi Chen",
                "Zhenwei Miao",
                "Wei Li",
                "Xiatian Zhu",
                "Li Zhang"
            ],
            "title": "Deepinteraction: 3d object detection via modality interaction",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Chengtang Yao",
                "Lidong Yu",
                "Yuwei Wu",
                "Yunde Jia"
            ],
            "title": "Sparse point guided 3d lane detection",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2023
        },
        {
            "authors": [
                "Ruochen Yin",
                "Yong Cheng",
                "Huapeng Wu",
                "Yuntao Song",
                "Biao Yu",
                "Runxin Niu"
            ],
            "title": "Fusionlane: Multi-sensor fusion for lane marking semantic segmentation using deep neural networks",
            "venue": "IEEE Transactions on Intelligent Transportation Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Tianwei Yin",
                "Xingyi Zhou",
                "Philipp Krahenbuhl"
            ],
            "title": "Center-based 3d object detection and tracking",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Tianwei Yin",
                "Xingyi Zhou",
                "Philipp Kr\u00e4henb\u00fchl"
            ],
            "title": "Multimodal virtual point 3d detection",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Jin Hyeok Yoo",
                "Yecheol Kim",
                "Jisong Kim",
                "Jun Won Choi"
            ],
            "title": "3d-cvf: Generating joint camera and lidar features using cross-view spatial feature fusion for 3d object detection",
            "venue": "In Computer Vision\u2013 ECCV 2020: 16th European Conference,",
            "year": 2020
        },
        {
            "authors": [
                "Qihang Yu",
                "Huiyu Wang",
                "Siyuan Qiao",
                "Maxwell Collins",
                "Yukun Zhu",
                "Hartwig Adam",
                "Alan Yuille",
                "Liang-Chieh Chen"
            ],
            "title": "k-means mask transformer",
            "venue": "In European Conference on Computer Vision,",
            "year": 2022
        },
        {
            "authors": [
                "Jianfeng Zhang",
                "Yujun Cai",
                "Shuicheng Yan",
                "Jiashi Feng"
            ],
            "title": "Direct multi-view multi-person 3d pose estimation",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Xinyu Zhang",
                "Zhiwei Li",
                "Xin Gao",
                "Dafeng Jin",
                "Jun Li"
            ],
            "title": "Channel attention in lidar-camera fusion for lane line segmentation",
            "venue": "Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Tu Zheng",
                "Hao Fang",
                "Yi Zhang",
                "Wenjian Tang",
                "Zheng Yang",
                "Haifeng Liu",
                "Deng Cai"
            ],
            "title": "Resa: Recurrent feature-shift aggregator for lane detection",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Yin Zhou",
                "Oncel Tuzel"
            ],
            "title": "Voxelnet: End-to-end learning for point cloud based 3d object detection",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Xizhou Zhu",
                "Weijie Su",
                "Lewei Lu",
                "Bin Li",
                "Xiaogang Wang",
                "Jifeng Dai"
            ],
            "title": "Deformable detr: Deformable transformers for end-to-end object detection",
            "venue": "arXiv preprint arXiv:2010.04159,",
            "year": 2020
        }
    ],
    "sections": [
        {
            "text": "1 INTRODUCTION Faster\nBetter\nFPS\nF 1\nSc or\ne\nFigure 1: FPS vs. F1 score. All models are tested on a single V100 GPU, and F1-score is evaluated with a harsh distance threshold of 0.5m on the OpenLane-1K dataset. Our model sets a new state-of-the-art, and our tiny version surpasses all previous methods with the fastest FPS. More details can be found in Table 1 and our Appendix.\nAutonomous driving (AD) technology in recent years has made remarkable strides, bringing us closer to the realization of fully self-driving vehicles. Within this field, one of the key challenges is the accurate detection of 3D lanes, a critical component for ensuring safe and reliable navigation. 3D lane detection entails identifying the 3D positions of lane boundaries in the environment, providing essential data for tasks like path planning and vehicle control.\n3D lane detection is proposed to mitigate the limitations posed by the absence of depth information in 2D prediction. Currently, the majority of 3D lane detection methods rely on visioncentric approaches, i.e., monocular solutions, where some designs are naturally borrowed and benefit from advances in 2D lane methods. Taking the perspective-view (PV) image as input,\nthese monocular methods mainly utilize the inverse perspective mapping (IPM) Mallot et al. (1991) technique to warp the PV features into BEV. However, there are misalignment issues in the IPMbased methods when encountering non-flat roads, due to the rigid flat assumption of IPM Nedevschi et al. (2004); Yan et al. (2022). While some recent efforts have been made to address this issue and\n\u2217Corresponding author.\nhave shown promising results by directly predicting 3D lanes in PV Bai et al. (2022b); Huang et al. (2023); Luo et al. (2023), these monocular 3D approaches, as vision-centric solutions, inevitably get stuck in capturing the complexity of real-world driving scenarios, when encountering adverse weather and lighting conditions. In contrast, as an active sensor, LiDAR excels in spatial localization and 3D structure perception, complementing the capabilities of passive sensor cameras, and it gets more widely used thanks to hardware advancements. A bunch of recent works in 3D object detection have demonstrated the power of LiDARs Zhou & Tuzel (2018); Lang et al. (2019); Yin et al. (2021a) and multiple modalities Liang et al. (2019); Wang et al. (2021); Yang et al. (2022); Li et al. (2022b); Chen et al. (2023) in autonomous driving scenarios. Whereas, fewer endeavors Bai et al. (2018); Luo et al. (2022) have been made to exploit multi-modal strength for 3D lane detection. Albeit using extra LiDAR data, M2-3DLane Luo et al. (2022) failed to make full use of features in image space which is crucial to 3D lane performance. Besides, M2-3DLane employs a naive fusion to aggregate multi-modal features, resulting in inferior performance to the camera-only methods(e.g., Luo et al. (2023)).\nGiven the rich semantics inherent in images and the accurate positional information afforded by the BEV representation Philion & Fidler (2020); Li et al. (2022d), we strive to exploit the multimodal features to enhance the performance of 3D lane detection. Existing methods tend to fuse two modalities into a single space Liang et al. (2022); Liu et al. (2023b), e.g., BEV, for feature extraction and subsequent prediction. However, this approach constrains the model\u2019s capacity to harness modality-specific features. We contend that features represented in both PV space and BEV space bear significance, facilitating improved representation learning. Motivated by the above observation, we introduce DV-3DLane, a novel end-to-end multi-modal 3D lane detection framework.\nTo maintain a dual-view space representation, we adopt a symmetric backbone consisting of a PV branch and a BEV branch to extract features in PV and BEV spaces, respectively. To leverage the merits of both images and points for comprehensive feature learning in each view, we design a bidirectional feature fusion (BFF) strategy. Subsequently, to effectively facilitate query-based detection using the retained dual-view features, we devise a unified query generator (UQG). This generator initially produces two sets of lane-aware queries: one from the PV space and the other from the BEV space. These two query sets are compelled to capture lane knowledge regarding semantics and spatiality, guided by auxiliary 2D segmentation supervision. Further, these two sets are then combined into a unified set that serves the decoder. To achieve the unification of dual-view queries, we propose a lane-centric clustering technique. Besides, we employ a Transformer decoder to aggressively integrate discriminative features from both views into the unified queries. For effective feature aggregation across different view spaces, we introduce a 3D dual-view deformable attention mechanism that considers the inherent properties of 3D space, resulting in deformed 3D sample points. These 3D sample points are then projected onto the PV and BEV planes, yielding 2D sample points in each respective view space. These projected 2D points are utilized for feature sampling within their respective view spaces.\nIn summary, our contributions are threefold : \u2022 We introduce DV-3DLane, an end-to-end multi-modal 3D lane detection framework that har-\nnesses the power of dual-view representation.\n\u2022 We devise the BFF strategy to mutually fuse features across modalities, and design the UQG to merge lane-aware queries from dual views, yielding a unified query set. Further, a 3D dual-view deformation attention mechanism is introduced to aggregate dual-view features effectively.\n\u2022 We conduct thorough experiments on the OpenLane benchmark to validate the effectiveness of our method. Experimental results show that DV-3DLane surpasses previous methods significantly, achieving an impressive 11.2 gain in F1 score and a remarkable 53.5% reduction in errors. Moreover, a 3D dual-view deformation attention mechanism is introduced to aggregate dual-view features effectively."
        },
        {
            "heading": "2 RELATED WORK",
            "text": ""
        },
        {
            "heading": "2.1 2D LANE DETECTION",
            "text": "Recent works in 2D lane detection can be broadly categorized into four main approaches: 1) Segmentation-based methods Lee et al. (2017); Pan et al. (2017); Neven et al. (2018); Hou et al.\n(2019); Xu et al. (2020); Zheng et al. (2021) devote to classifying pixels into lanes or the background, necessitating further post-processing steps (e.g., grouping and curve fitting) to produce lane instances. 2) Anchor-based methods, inspired by region-based object detectors such as FasterRCNN Ren et al. (2015), employ line-like anchors to localize lanes Wang et al. (2018); Li et al. (2019); Tabelini et al. (2021a). To overcome the limitations of straight-line constraints, Jin et al. (2022) employ eigenlane space to produce diverse lane shape candidates. 3) Point-based methods Ko et al. (2021); Qu et al. (2021); Wang et al. (2022); Xu et al. (2022) attempt to flexibly localize key points along each lane instance and subsequently group the points belonging to the same lane. 4) Parametric methods Van Gansbeke et al. (2019); Tabelini et al. (2021b); Liu et al. (2021); Feng et al. (2022) formulate lane detection as a curve fitting problem, leveraging prior knowledge about lane shapes by representing them using various parametric forms, such as polynomials and splines.\nutilized. Different colored boxes denote queries targeting different lanes; dashed boxes represent the background, and box texture indicates features."
        },
        {
            "heading": "2.2 3D LANE DETECTION",
            "text": "Existing methods center on vision-centric solutions and draw inspiration from the 2D task. Typically, monocular approaches Garnett et al. (2019); Efrat et al. (2020); Guo et al. (2020); Chen et al. (2022); Wang et al. (2023); Liu et al. (2022); Li et al. (2022a); Ai et al. (2023); Yao et al. (2023) construct surrogate representations using inverse perspective mapping (IPM), and perform predictions in this surrogate space. Nonetheless, IPM inherently introduces discrepancies between the perspective and the surrogate view in non-flat areas due to its planar assumption. To address this limitation, recent efforts have endeavored to predict 3D lanes from the perspective view Yan et al. (2022); Bai et al. (2022b); Huang et al. (2023); Luo et al. (2023), or employ a depth-aware projection to enhance lane perception by incorporating LiDAR information Luo et al. (2022)."
        },
        {
            "heading": "2.3 MULTI-MODAL DETECTION",
            "text": "Despite advancements in lane detection, multi-modal methods remain relatively underexplored. Previous works typically utilize either BEV Bai et al. (2018); Yin et al. (2020); Luo et al. (2022) or PV Zhang et al. (2021b) as representation spaces for performing 2D lane segmentation Yin et al. (2020); Zhang et al. (2021b) or 3D lane detection Bai et al. (2018); Luo et al. (2022). For BEVbased methods, Bai et al. (2018) rasterizes LiDAR points to create a BEV image and transforms PV images into BEV using the estimated ground height derived from the LiDAR data. Similarly, M2-3DLane Luo et al. (2022) utilizes the BEV space to fuse multi-modal features. To project PV features into BEV space, they lift compact 2D features into 3D space guided by the depth map and\nfurther employ a pillar-based method Lang et al. (2019) to splat them into BEV. While these methods primarily focus on 3D tasks, Yin et al. (2020) leverages BEV space for fusing camera and LiDAR features, serving for 2D BEV lane segmentation. Conversely, Zhang et al. (2021b) adopts PV to fuse multi-modal features for 2D lane segmentation. In contrast to lane detection, multi-modal methods have been extensively studied in 3D object detection, with most previous multi-modal methods attempting to fuse image features into BEV space due to its compactness and interoperability for ambient perception Ma et al. (2022). These methods either adopt point-level fusion Sindagi et al. (2019); Wang et al. (2021); Yin et al. (2021b) to paint points, instance-level fusion to project 3D proposals to image space Yoo et al. (2020); Bai et al. (2022a), or feature-level fusion to transform features from PV space into BEV space Liu et al. (2023b); Liang et al. (2022). However, few works consider both the perspective view and BEV simultaneously."
        },
        {
            "heading": "3 METHODOLOGY",
            "text": "The overall framework of our DV-3DLane is depicted in Figure 2. Section 3.1 describes the bidirectional feature fusion module, which merges different modalities bidirectionally and constructs multi-modal features in both PV and BEV spaces. In Section 3.2, we present the unified query generator, which generates two lane-aware query sets from dual views and unifies them into a shared space in a lane-centric manner. Section 3.3 introduces the 3D dual-view deformable attention module, which effectively aggregates dual-view features into unified queries, serving for prediction."
        },
        {
            "heading": "3.1 BIDIRECTIONAL FEATURE FUSION",
            "text": "Instead of merging different views into one single space Bai et al. (2018); Luo et al. (2022); Liang et al. (2022); Li et al. (2022d); Liu et al. (2023b), we propose to retain features in both PV and BEV spaces while incorporating multi-modal features for each view. To achieve this, we employ a dual branch to extract features for each view, using images and points as input, respectively. Intermediately, we conduct bidirectional feature fusion between the symmetric branches to enhance each view with multiple modalities, as shown in Figure 3 and summarized in Algorithm 1.\nProject\nScatter\nSample\nConcat\nConcat\nPV Fe at ur e\nPo in tF ea tu re\nConcretely, we place points and images in their designated branches. After obtaining low-level features within each branch, we perform bidirectional feature fusion. By projecting 3D points Ppt = {(xi, yi, zi)|i \u2208 P} onto the PV plane, we obtain their corresponding 2D coordinates Ppt2pv = {(ui, vi)|i \u2208 P}, where P is the cardinality of the point set. 1) For points-to-pixels fusion, we utilize a Scatter operation to construct dense point feature grids Fpt2pv , (depicted in the upper part of Figure 3, with blue cells denoting positions hit by the projected 3D points). 2) For pixels-to-points fusion, we employ bilinear interpolation to sample features at 2D positions hit by the projection of 3D points, yielding Fpv2pt (shown in the lower part of Figure 3). The resulting cross-modal features in PV and BEV are concatenated with their respective original modal features. The fused multi-modal features in each view, i.e., PV and BEV, are then fed into subsequent modules in the corresponding branch, generating Fpv and Fbev , respectively. Notably, Fpv and Fbev encapsulate multi-modal information represented in distinct spaces."
        },
        {
            "heading": "3.2 UNIFIED QUERY GENERATOR",
            "text": "We introduce a unified query generator for end-to-end 3D lane detection. To this end, we first generate two distinct lane-aware query sets, termed dual-view queries, from the previously obtained multi-modal features, Fpv and Fbev . Then, we present a lane-centric clustering strategy to unify these dual-view queries into a cohesive set of queries.\nDual-view Query Generation. To effectively capture semantic and spatial features related to lanes, which are termed as \u201clane-aware\u201d knowledge, we utilize an instance activation map (IAM) Cheng et al. (2022)-assisted method to generate lane-aware queries in PV and BEV spaces. Taking PV branch as an example, we produce a set of IAMs, denoted as Apv , via the following equation:\nApv = \u03c3(F(Concat(Fpv,Spv))),\nwhere Apv \u2208 RN\u00d7Hpv\u00d7Wpv , Fpv \u2208 RC\u00d7Hpv\u00d7Wpv , N denotes query number, \u03c3 is the sigmoid function, Concat represents concatenation operation, and Spv comprises two-channel spatial localization features for each pixel Liu et al. (2018). The lane-aware query Qpv assisted by IAMs is generated via:\nQpv = Apv \u2297 FTpv,\nwhere Qpv \u2208 RN\u00d7C , \u2297 denotes the matrix product. Similarly, lane-aware BEV query Qbev \u2208 RN\u00d7C is formed using:\nQbev = \u03c3(F([Fbev,Sbev]))\u2297 FTbev. To force the query sets to learn lane-aware features, during training, we employ an auxiliary instance segmentation for each branch on top of the query set. Labels for the auxiliary segmentation are generated in pairs for these two branches, which are further assigned to predictions using maskbased bipartite matching Cheng et al. (2022), as illustrated in Figure 4 (a) and (b).\nDual-view Query Clustering. Given dual-view query sets Qpv and Qbev , we propose employing a lane-centric clustering technique to generate a unified query set for end-to-end lane detection. While kMax-DeepLab Yu et al. (2022) previously used k-means cross-attention to group pixels into distinct clusters, i.e., instance masks, our approach focuses on unifying queries from different views. Queries from Qpv and Qbev targeting the same lane are merged within the same cluster. Specifically, we initiate lane cluster centers C \u2208 RN\u00d7C with Qpv ,\nand assign each query in Qbev to its nearest cluster center among C. Notably, cluster centers can be chosen from either Qpv or Qbev . Empirically, we found that using Qpv produces better results. To achieve clustering, we perform attention between C (query) and Qbev (key), while applying argmax along the cluster center (query) dimension Yu et al. (2022) as follows:\nA = argmax N\n(C\u00d7QTbev), C\u0302 = A \u00b7Qbev +C,\nwhere C\u0302 \u2208 RN\u00d7C refers to updated centers unifying queries from dual views. In practise, we use gumbel-softmax Jang et al. (2016); Liang et al. (2023) to substitute argmax.\nConsidering the variation and slenderness of lanes, we employ a refined point query scheme Luo et al. (2023) to enhance lane detection. Instead of using a single query for each lane, multiplepoint queries are employed for more precise capture Luo et al. (2023); Liao et al. (2022); Zhang et al. (2021a); Liu et al. (2023a). Consequently, in the first layer, we construct point-based queries Q \u2208 RN\u00d7M\u00d7C with Q = C\u0302 \u2295 Epoints, where \u2295 denotes broadcast sum, Epoints \u2208 RM\u00d7C is the learnable point embedding, and in the subsequent layer, we update Q by Q = C\u0302\u2295Q. Supervision on Query Clustering. Given the critical importance of deep supervision for the clustering Yu et al. (2022), we leverage the InfoNCE loss Oord et al. (2018) to supervise the query\nclustering in a lane-centric manner, as illustrated in Figure 4 (c) and formulated as:\nLNCE= \u2212log exp(q \u00b7k+/\u03c4) exp(q \u00b7k+/\u03c4) + \u2211\nk\u2212\u2208N exp(q \u00b7k\u2212/\u03c4) ,\nwhere \u03c4 is a temperature hyper-parameter Wu et al. (2018), q denotes one query, k+ indicates the positive sample w.r.t. q, and N denotes the collection of all negative samples from the different query set relative to the one containing q. Notably, queries assigned to the background do not incur penalties in the clustering learning process. With this supervision, queries from different views are grouped together when matched to the same ground truth lane. Consequently, lane-aware knowledge residing in two view spaces is synergized into the unified query."
        },
        {
            "heading": "3.3 3D DUAL-VIEW DEFORMABLE ATTENTION",
            "text": "Apart from informative query generation, feature aggregation plays a crucial role in DV-3DLane. Instead of projecting points from densely sampled grids Chen et al. (2022) or their lifted pillars Li et al. (2022d) onto the PV plane for feature sampling, as shown in Figure 5 (a), we adopt sparse queries to sample features from different views. Moreover, our approach distinguishes itself from several existing sparse query methods, as depicted in Figure 5 (b) and (c). For instance, DeepInteration Yang et al. (2022) (Figure 5 (b)) employs a sequential method to sample PV and BEV features, while FUTR3D Chen et al. (2023) (Figure 5 (c)) projects 3D points into different spaces, sampling features individually for each space.\nIn contrast, as outlined in Algorithm 2, we leverage the inherent properties of 3D space by predicting both 3D reference points and their 3D offsets using queries, forming 3D deformed points. These 3D deformed points are then projected into each space, establishing a consistent feature sampling strategy across spaces, as depicted in Figure 5. Consequently, features corresponding to the same 3D points from different views are effectively sampled and integrated into the query.\nBEV Grids\nlayer \ud835\udc56 layer \ud835\udc56 + 1\nPV Space BEV Space Reference points Projection Offsets\n(a) (b)\n(c) (ours)"
        },
        {
            "heading": "3.4 PREDICTION AND LOSS",
            "text": "Auxiliary Tasks. During training, we incorporate two auxiliary tasks: 1) 2D instance segmentation Luo et al. (2023); Cheng et al. (2022) loss Lseg for both PV and BEV branches, aiding in extracting discriminative lane features in each view; 2) Depth estimation for the PV branch, which guides effective 3D structure-aware feature extraction of Fpv . Depth labels are generated from LiDAR points, and the loss Ldepth is calculated following BEVDepth Li et al. (2022c).\n3D Lane Prediction and Loss. As we adopt point-based queries Q \u2208 R(N\u00d7M)\u00d7C , each query naturally corresponds to a 3D point, and every group of M points constructs a complete 3D lane. Thus, we predict x, z, and visibility for each point query on the predefined y coordinates Chen et al. (2022); Luo et al. (2023) and a classification probability for each lane. Overall, the total loss is:\nLlane = wxLx + wzLz + wvLv + wcLc, Laux = wsegLseg + wdepthLdepth, Ltotal = Llane + Laux.\nwhere w\u2217 denotes different loss weights. We adopt the L1 loss Lx and Lz to learn the x, z positions, focal loss Lin et al. (2017) Lc to learn the lane category, and BCELoss Lv to learn visibility.\n4 EXPERIMENTS\n4.1 DATASETS\nWe evaluate our method on OpenLane Chen et al. (2022), the sole public 3D lane dataset featuring multi-modal sources, OpenLane is a large-scale dataset built on Waymo Open Dataset Sun et al. (2020), comprising 200K frames and 880K lanes across six driving scenarios and 14 lane categories. The LiDAR data, collected using 64-beam LiDARs, is sampled at 10Hz. This extensive dataset provides a solid foundation for evaluating 3D lane algorithms comprehensively."
        },
        {
            "heading": "4.2 METRICS",
            "text": "We adopt the evaluation metrics established by OpenLane Chen et al. (2022), framing 3D lane detection evaluation as a matching problem based on the edit distance between predictions and ground truth. Successful matching results in computed metrics, including F-Score, category accuracy, and error in X/Z-axes. A successful match for each predicted 3D lane is defined when at least 75% of its points have a distance to the ground truth below the predefined threshold Dthre."
        },
        {
            "heading": "4.3 IMPLEMENTATION DETAILS",
            "text": "Models. In the base version of DV-3DLane, we employ ResNet34 He et al. (2016) and PillarNet34 Shi et al. (2022) as the backbones for our camera and LiDAR branches, respectively. For the lite version, we utilize ResNet18 and PillarNet18. The base version features two decoder layers, while the lite version employs a single decoder layer. Following LATR Luo et al. (2023), we set the number of lane queries to 40, and we employ deformable attention with 4 heads, 8 sample points, and 256 embedding dimensions.\nTraining. We use the Adam optimizer Kingma & Ba (2014) with a weight decay of 0.01. The learning rate is set to 2e-4, and our models undergo training for 24 epochs with a batch size of 32. We employ the cosine annealing scheduler Loshchilov & Hutter (2016) with Tmax = 8. Our input images are of resolution 720\u00d7960, and we adopt a voxel size of (0.2m, 0.4m) for the X and Y axes."
        },
        {
            "heading": "4.4 MAIN RESULTS",
            "text": "It\u2019s important to note that the existing metrics use a rather lenient distance threshold of Dthre=1.5m. However, in the context of ensuring safety in AD, this value, although commonly used for assessment purposes, may be considered overly permissive. Following M2-3DLaneNet Luo et al. (2022), we extend our evaluation to include a more stringent threshold, Dthre=0.5m. Further, we illustrate the relationship between the F1 score performance and different distance thresholds for various models, as shown in Figure 6. Notably, our method consistently achieves superior results, even when evaluated under a much more stringent criterion of Dthre=0.1m. In contrast, other approaches experience a noticeable decline in performance as the distance threshold decreases. These findings confirm the robustness of our method across varying distance thresholds, particularly highlighting its advantage in precise localization.\nWe present the main results in Table 1, obtained from experiments conducted on the OpenLane-1K dataset. The evaluation uses both Dthre=1.5m and Dthre=0.5m criteria, allowing for a comprehensive and insightful comparison. It is evident that DV-3DLane consistently outperforms previous state-of-the-art (SoTA) methods across all metrics. Notably, when applying a more strict 0.5m threshold, DV-3DLane demonstrates a substantial 11.2% improvement in the F1 score. It is noteworthy that our method excels in localization accuracy, leading to significant performance improvements. Specifically, our method achieves remarkable reductions in localization errors: 52%/50% for\nX near/far, and 61%/52% for Z near/far. Due to space limitations, results in various scenarios and studies about robustness concerning calibration noise are included in our Appendix.\nEffect of Multiple Modalities. To explore the impact of individual modalities, we conduct experiments using single modalities, as outlined in the \u201cImage-Branch\u201d and \u201cLiDAR-Branch\u201d rows of Table 1. The results illustrate that DV-3DLane significantly enhances performance compared to using images alone or relying solely on LiDAR data. Notably, our method significantly surpasses configurations that simply equip LATR with LiDAR input across all metrics, underscoring the substantial improvements achieved by DV-3DLane in leveraging information from both modalities. Moreover, to evaluate the effect of dual-view, we conduct experiments using single-modality input but transforming features extracted from the backbone into another view, yielding single-modal dual-view features. Then, our dual-view decoder is applied, and the results are detailed in our Appendix. Additionally, we conduct experiments using our \u201cImage-Branch\u201d on the Apollo Guo et al. (2020) dataset, which exclusively contains image data. The results are provided in our Appendix.\nQualitative Results. We present a qualitative comparison between DV-3DLane and LATR Luo et al. (2023) in Figure 7, demonstrating that our method achieves more robust and accurate predictions across various scenarios. More visualization results are included in our Appendix."
        },
        {
            "heading": "4.5 ABLATION STUDIES",
            "text": "We conduct all ablation studies on OpenLane-300 following established practices Chen et al. (2022); Luo et al. (2023); Huang et al. (2023), while adopting a 0.5m threshold Dthre for evaluation.\nEffect of Bidirectional Feature Fusion. The corresponding experiments are included in our Appendix, due to space limitations. We kindly direct the readers to refer to the Appendix for details. The results confirm the effectiveness of the proposed bidirectional feature fusion approach.\nEffect of Unified Query. We study the effect of our unified queries generation strategy in Table 2, where \u201cRandom\u201d means random initialization using nn.Embedding, \u201cQpv\u201d denotes using only PV queries, and \u201cQbev\u201d refers to using only BEV queries. Replacing our unified queries with randomly initialized ones Carion et al. (2020); Zhu et al. (2020); Li et al. (2022d) results in a decrease of 1.0 in the F1 score compared to our approach. Interestingly, employing a single space instance-\naware query yields even lower F1 scores of 69.6%/69.1% for PV/BEV, respectively, than random initialization. This underscores the inadequacy of a single-space lane-aware query in capturing complex 3D lane features comprehensively existing in both PV and BEV spaces. However, our dual-view strategy, generating lane-aware queries w.r.t. both views, improves overall performance to 70.7, achieving the best result. This demonstrates that our method effectively integrates the strengths of features from two spaces, forming a cohesive query set.\nEffect of 3D Dual-view Deformable Attention. To evaluate the efficacy of our proposed Dual-view Deformable Attention, we conduct ablation studies in Table 3, where \u201cPV space\u201d and \u201cBEV space\u201d mean using single space in the decoder. \u201cDeepInteration\u201d Yang et al. (2022) denotes sequential fusion of features from different spaces, and \u201cFUTR3D\u201d Chen et al. (2023) refer to a modalityagnostic approach where sampling locations differ across views. We compare DV-3DLane against alternative approaches, including single-view fused method, as well as methods proposed in DeepInteration and FUTR3D, as described in Section 3.3. The results underscore the significance of our approach. In detail, sampling only PV space features leads to a notable drop (70.7\u219263.6) in performance, showing the importance of BEV space due to its advantages in localization. Besides, our method outperforms the sequential approach of DeepInteration with a substantial 2.0 gain in F1 score. Furthermore, compared to the modality-agnostic approach proposed in FUTR3D, our method achieves a 0.5 improvement, emphasizing the importance of consistent sampling locations in deformable attention across different spaces.\nTable 3: Effect of 3D dual-view deformable attention.\nMethods F1 X error (m) Z error (m)near | far near | far PV space 63.6 0.150 | 0.202 0.060 | 0.081 BEV space 68.5 0.127 | 0.151 0.064 | 0.087 DeepInteration 68.7 0.126 | 0.157 0.059 | 0.081 FUTR3D 70.2 0.118 | 0.145 0.057 | 0.077 Ours 70.7 0.123 | 0.146 0.058 | 0.078"
        },
        {
            "heading": "5 CONCLUSION",
            "text": "In this work, we introduce DV-3DLane, a novel end-to-end multi-modal 3D lane detection framework that leverages the strengths of both PV and BEV spaces. To this end, we propose three novel modules that effectively utilize dual-view representation on different levels, consistently enhancing performance. Extensive experiments substantiate the outstanding advancements achieved by DV3DLane, establishing a new state of the art on OpenLane."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "This work was supported by NSFC with Grant No. 62293482, by the Basic Research Project No. HZQB-KCZYZ-2021067 of Hetao Shenzhen HK S&T Cooperation Zone, by Shenzhen General Program No. JCYJ20220530143600001, by Shenzhen-Hong Kong Joint Funding No. SGDX20211123112401002, by the National Key R&D Program of China with grant No. 2018YFB1800800, by the Shenzhen Outstanding Talents Training Fund 202002, by Guangdong Research Project No. 2017ZT07X152 and No. 2019CX01X104, by the Guangdong Provincial Key Laboratory of Future Networks of Intelligence (Grant No. 2022B1212010001), by the Guangdong Provincial Key Laboratory of Big Data Computing, The Chinese University of Hong Kong, Shenzhen, by the NSFC 61931024&12326610, by the Shenzhen Key Laboratory of Big Data and Artificial Intelligence (Grant No. ZDSYS201707251409055), and the Key Area R&D Program of Guangdong Province with grant No. 2018B03033800, by Tencent&Huawei Open Fund."
        }
    ],
    "year": 2024
}