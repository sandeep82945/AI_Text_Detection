{
    "abstractText": "Despite their ubiquity in language generation, it remains unknown why truncation sampling heuristics like nucleus sampling are so effective. We provide a theoretical explanation for the effectiveness of the truncation sampling by proving that truncation methods that discard tokens below some probability threshold (the most common type of truncation) can guarantee that all sampled tokens have nonzero true probability. However, thresholds are a coarse heuristic, and necessarily discard some tokens with nonzero true probability as well. In pursuit of a more precise sampling strategy, we show that we can leverage a known source of model errors, the softmax bottleneck, to prove that certain tokens have nonzero true probability, without relying on a threshold. Based on our findings, we develop an experimental truncation strategy and the present pilot studies demonstrating the promise of this type of algorithm. Our evaluations show that our method outperforms its threshold-based counterparts under automatic and human evaluation metrics for low-entropy (i.e., close to greedy) open-ended text generation. Our theoretical findings and pilot experiments provide both insight into why truncation sampling works, and make progress toward more expressive sampling algorithms that better surface the generative capabilities of large language models.",
    "authors": [],
    "id": "SP:f3cdb58f6e06984856b014967c4ede732748de8b",
    "references": [
        {
            "authors": [
                "J. Aitchison"
            ],
            "title": "The statistical analysis of compositional data",
            "venue": "Journal of the Royal Statistical Society: Series B (Methodological),",
            "year": 1982
        },
        {
            "authors": [
                "MOSEK ApS"
            ],
            "title": "MOSEK Optimizer API for Python 9.3.22",
            "venue": "Version 10.0.,",
            "year": 2023
        },
        {
            "authors": [
                "Sourya Basu",
                "Nitish Shirish Keskar",
                "Lav R. Varshney"
            ],
            "title": "Mirostat: A perplexity-controlled neural text decoding algorithm",
            "venue": "In ICLR,",
            "year": 2021
        },
        {
            "authors": [
                "Stella Biderman",
                "Hailey Schoelkopf",
                "Quentin Gregory Anthony",
                "Herbie Bradley",
                "Kyle O\u2019Brien",
                "Eric Hallahan",
                "Mohammad Aflah Khan",
                "Shivanshu Purohit",
                "USVSN Sai Prashanth",
                "Edward Raff"
            ],
            "title": "Pythia: A suite for analyzing large language models across training and scaling",
            "venue": "In ICML,",
            "year": 2023
        },
        {
            "authors": [
                "Haw-Shiuan Chang",
                "Andrew McCallum"
            ],
            "title": "Softmax bottleneck makes language models unable to represent multi-mode word distributions",
            "venue": "In ACL,",
            "year": 2022
        },
        {
            "authors": [
                "Aakanksha Chowdhery",
                "Sharan Narang",
                "Jacob Devlin",
                "Maarten Bosma",
                "Gaurav Mishra",
                "Adam Roberts"
            ],
            "title": "PaLM: Scaling language modeling with pathways, 2022",
            "venue": "URL https: //arxiv.org/abs/2204.02311",
            "year": 2022
        },
        {
            "authors": [
                "Alexandra DeLucia",
                "Aaron Mueller",
                "Xiang Lisa Li",
                "Jo\u00e3o Sedoc"
            ],
            "title": "Decoding methods for neural narrative generation",
            "venue": "In Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021),",
            "year": 2021
        },
        {
            "authors": [
                "David Demeter",
                "Gregory Kimmel",
                "Doug Downey"
            ],
            "title": "Stolen probability: A structural weakness of neural language models",
            "venue": "In ACL, pp",
            "year": 2020
        },
        {
            "authors": [
                "Angela Fan",
                "Mike Lewis",
                "Yann Dauphin"
            ],
            "title": "Hierarchical neural story generation",
            "venue": "In ACL,",
            "year": 2018
        },
        {
            "authors": [
                "Markus Freitag",
                "Behrooz Ghorbani",
                "Patrick Fernandes"
            ],
            "title": "Epsilon sampling rocks: Investigating sampling strategies for minimum bayes risk decoding for machine translation, 2023",
            "venue": "URL https://arxiv.org/abs/2305.09860",
            "year": 2023
        },
        {
            "authors": [
                "O. Ganea",
                "S. Gelly",
                "Gary B\u00e9cigneul",
                "Aliaksei Severyn"
            ],
            "title": "Breaking the softmax bottleneck via learnable monotonic pointwise non-linearities",
            "venue": "In ICML,",
            "year": 2019
        },
        {
            "authors": [
                "Aaron Gokaslan",
                "Vanya Cohen",
                "Ellie Pavlick",
                "Stefanie Tellex"
            ],
            "title": "OpenWebText Corpus, 2019",
            "venue": "URL http://Skylion007.github.io/OpenWebTextCorpus",
            "year": 2019
        },
        {
            "authors": [
                "Andreas Grivas",
                "Nikolay Bogoychev",
                "Adam Lopez"
            ],
            "title": "Low-rank softmax can have unargmaxable classes in theory but rarely in practice",
            "venue": "In ACL,",
            "year": 2022
        },
        {
            "authors": [
                "Tatsunori B Hashimoto",
                "Hugh Zhang",
                "Percy Liang"
            ],
            "title": "Unifying human and statistical evaluation for natural language generation",
            "venue": "In NAACL-HLT,",
            "year": 2019
        },
        {
            "authors": [
                "John Hewitt",
                "Christopher Manning",
                "Percy Liang"
            ],
            "title": "Truncation sampling as language model desmoothing",
            "venue": "In EMNLP,",
            "year": 2022
        },
        {
            "authors": [
                "Ari Holtzman",
                "Jan Buys",
                "Li Du",
                "Maxwell Forbes",
                "Yejin Choi"
            ],
            "title": "The curious case of neural text degeneration",
            "venue": "In ICLR,",
            "year": 2020
        },
        {
            "authors": [
                "Daphne Ippolito",
                "Reno Kriz",
                "Jo\u00e3o Sedoc",
                "Maria Kustikova",
                "Chris Callison-Burch"
            ],
            "title": "Comparison of diverse decoding methods from conditional language models",
            "venue": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,",
            "year": 2019
        },
        {
            "authors": [
                "Fred Jelinek"
            ],
            "title": "Self-organized language modeling for speech recognition",
            "venue": "Readings in speech recognition,",
            "year": 1990
        },
        {
            "authors": [
                "Kalpesh Krishna",
                "Yapei Chang",
                "John Wieting",
                "Mohit Iyyer"
            ],
            "title": "RankGen: Improving text generation with large ranking models",
            "venue": "In EMNLP,",
            "year": 2022
        },
        {
            "authors": [
                "Xiang Lisa Li",
                "Ari Holtzman",
                "Daniel Fried",
                "Percy Liang",
                "Jason Eisner",
                "Tatsunori Hashimoto",
                "Luke Zettlemoyer",
                "Mike Lewis"
            ],
            "title": "Contrastive decoding: Open-ended text generation as optimization",
            "venue": "In ACL,",
            "year": 2023
        },
        {
            "authors": [
                "Clara Meister",
                "Ryan Cotterell",
                "Tim Vieira"
            ],
            "title": "If beam search is the answer, what was the question",
            "venue": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 2173\u20132185,",
            "year": 2020
        },
        {
            "authors": [
                "Clara Meister",
                "Tiago Pimentel",
                "L. Malagutti",
                "Ethan Gotlieb Wilcox",
                "Ryan Cotterell"
            ],
            "title": "On the efficacy of sampling adapters",
            "year": 2023
        },
        {
            "authors": [
                "Clara Meister",
                "Tiago Pimentel",
                "Gian Wiher",
                "Ryan Cotterell"
            ],
            "title": "Locally Typical Sampling",
            "venue": "TACL, 11:102\u2013121,",
            "year": 2023
        },
        {
            "authors": [
                "Krishna Pillutla",
                "Swabha Swayamdipta",
                "Rowan Zellers",
                "John Thickstun",
                "Sean Welleck",
                "Yejin Choi",
                "Zaid Harchaoui"
            ],
            "title": "MAUVE: Measuring the gap between neural text and human text using divergence frontiers",
            "year": 2021
        },
        {
            "authors": [
                "Raj Reddy"
            ],
            "title": "Speech understanding systems: summary of results of the five-year research effort at Carnegie-Mellon University",
            "year": 1977
        },
        {
            "authors": [
                "Teven Le Scao",
                "Angela Fan",
                "Christopher Akiki",
                "Ellie Pavlick",
                "Suzana Ili\u0107",
                "Daniel Hesslow",
                "Roman Castagn\u00e9",
                "Alexandra Sasha Luccioni",
                "Fran\u00e7ois Yvon",
                "Matthias Gall\u00e9"
            ],
            "title": "Bloom: A 176bparameter open-access multilingual language model, 2022",
            "venue": "URL https://arxiv.org/ abs/2211.05100",
            "year": 2022
        },
        {
            "authors": [
                "Rico Sennrich",
                "Jannis Vamvas",
                "Alireza Mohammadshahi"
            ],
            "title": "Mitigating hallucinations and offtarget machine translation with source-contrastive and language-contrastive decoding, 2023",
            "venue": "URL https://arxiv.org/abs/2309.07098",
            "year": 2023
        },
        {
            "authors": [
                "Felix Stahlberg",
                "Bill Byrne"
            ],
            "title": "On NMT search errors and model errors: Cat got your tongue",
            "venue": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),",
            "year": 2019
        },
        {
            "authors": [
                "Hugo Touvron",
                "Louis Martin",
                "Kevin Stone",
                "Peter Albert",
                "Amjad Almahairi",
                "Yasmine Babaei"
            ],
            "title": "Llama 2: Open foundation and fine-tuned chat models, 2023",
            "venue": "URL https://arxiv.org/ abs/2307.09288",
            "year": 2023
        },
        {
            "authors": [
                "Zhilin Yang",
                "Zihang Dai",
                "Ruslan Salakhutdinov",
                "William W. Cohen"
            ],
            "title": "Breaking the softmax bottleneck: A high-rank RNN language model",
            "venue": "In ICLR,",
            "year": 2018
        },
        {
            "authors": [
                "Zhilin Yang",
                "Thang Luong",
                "Russ R Salakhutdinov",
                "Quoc V Le"
            ],
            "title": "Mixtape: Breaking the softmax bottleneck efficiently",
            "venue": "In NeurIPS,",
            "year": 2019
        },
        {
            "authors": [
                "Holtzman"
            ],
            "title": "In developing algorithms for high-entropy generation, the afore-mentioned line of work attempts to maintain the learned distribution as much as possible (Holtzman et al., 2020; Hewitt et al., 2022); another significant design principle has related to the uniform information density principle that humans are observed to obey",
            "year": 2020
        },
        {
            "authors": [
                "Yang"
            ],
            "title": "2018), various algorithms were proposed for efficiently learning a high-rank language models (Yang et al., 2019; Ganea et al., 2019)",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Crucial to the remarkable generative capabilities of today\u2019s large language models (LLMs) (OpenAI, 2023; Touvron et al., 2023; Chowdhery et al., 2022) are the sampling algorithms responsible for selecting the next token at each timestep. The most common of these algorithms use a simple truncation strategy: sample only the tokens that have probability greater than some threshold (Holtzman et al., 2020; Fan et al., 2018). In the quest for high-entropy generation wherein one wants to be able to generate multiple good completions, it has been empirically established that the search for the highest-likelihood strings through e.g., beam search or greedy decoding led to low-quality generations (Hashimoto et al., 2019). Threshold-based truncation sampling presents a compelling alternative: by avoiding the tokens at the tail end of the distribution which correspond to degenerate text it produces significantly more coherent generations (Ippolito et al., 2019; Holtzman et al., 2020; DeLucia et al., 2021). However, beyond the intuition that language models tend to assign too much probability to tokens that should have 0 or near-0 probability (akin to smoothing (Hewitt et al., 2022)), prior work has been limited in establishing why truncation sampling is so essential in autoregressive generation.\nIn this paper, we provide a precise mathematical explanation to elucidate the extraordinary success of threshold-based truncation sampling (\u00a73). First, we prove via an argument about log-probability errors that threshold sampling is guaranteed to only sample tokens in the support of the true distribution, so long as the chosen threshold is larger than some bound (Corollary 1). Next, we present a method to more directly account for a likely source of tail errors: the softmax bottleneck (Yang et al., 2018), which states that the low-rank softmax matrix used at the output layer of language models causes probability errors in the model\u2019s output distribution (\u00a74). Specifically, we show how to leverage the restricted structure imposed by the softmax bottleneck to more precisely determine (relative to threshold-based truncation) which tokens are in the support of the true distribution (Theorem 2). At a high level, the idea is to declare a token to be in the support if its probability is nonzero not only in the predicted distribution but also in all distributions that are \u201csimilar\u201d to it (in a precise technical sense) from the perspective of the softmax matrix. This presents a more nuanced strategy compared\nto threshold-based truncation sampling: our algorithm does not rely on a threshold, thereby allowing higher probability tokens to be discarded while keeping some lower-probability tokens.\nWe conduct a pilot investigation (\u00a75) to empirically evaluate this basis-aware truncation sampling approach. Our results shows improvements on an open-ended generation task via both automatic and human evaluation metrics under low-entropy generation (i.e., close to greedy). Figure 1 illustrates our algorithm\u2019s more nuanced token selection strategy qualitatively (also see Figure 4). Unlike threshold-based truncation methods (each shown with a dotted vertical line), our method can selectively discard low-quality tokens while still keeping high-quality but lower-probability tokens. This is accomplished by taking into account linear dependencies between token embeddings.\nOverall our work provides theoretical insights which motivate a practical method and show how truncation sampling avoids errors in a language model by mitigating the softmax bottleneck."
        },
        {
            "heading": "2 BACKGROUND",
            "text": "Autoregressive language models Autoregressive language models (henceforth models) are trained as next-word-predictors: given a prefix, the model assigns a probability to each token in a vocabulary of size v as a prediction of which token comes next. Given an input prefix, a model produces a vector h \u2208 Rd, which we refer to as the hidden state, and hyperparameter d as the hidden size. The model then uses a linear map with matrix W \u2208 Rv\u00d7d to obtain logits Wh \u2208 Rv , to which it applies the softmax function to obtain a probability distribution over tokens in the vocabulary:\np\u0302 = softmax(Wh) = exp(Wh)\u2211v i=1 exp(Wh)i .\nThe matrix W is commonly referred to as the softmax matrix because it is applied directly before the softmax, or the embedding matrix. Generally models are trained to output the p\u0302 that minimizes the cross entropy with the conditional true distribution1 p\u2217: crossentropy(p\u2217, p\u0302) = \u2211v i=1 p \u2217 i log p\u0302i.\nGeneration via truncation sampling Language models can autoregressively generate text by sampling a token from p\u0302 at each time step. Unfortunately, sampling directly from p\u0302, i.e., ancestral sampling, often leads to quality issues with unnatural, low-probability tokens. Truncation sampling aims to solve this issue post-hoc by choosing a subset of the vocabulary to sample from, setting\n1In the case of natural language, it is not entirely clear what the \u201ctrue\u201d distribution p\u2217 means exactly. Nonetheless we can use the distribution from which internet text is implicitly sampled as a useful surrogate.\nall other tokens to have zero probability. Meister et al. (2023a) frame this strategy as reprioritizing precision over recall (i.e., removing some valid text from the distribution to avoid sampling unlikely text.) We focus on a class of truncation methods that select tokens by choosing a threshold at each timestep and truncating tokens with probability less than that threshold. This simple heuristic has been found to be effective and forms the basis of popular methods like nucleus (top-p) (Holtzman et al., 2020) and top-k (Fan et al., 2018) sampling.\nPrior work has introduced several heuristics for choosing truncation thresholds. For instance, the threshold can be fixed constant as in \u03f5 sampling, or chosen dynamically across different distributions, as in \u03b7, nucleus, top-k, and Mirostat sampling (Basu et al., 2021).2 \u03b7 sampling introduces the idea that the threshold should depend on the entropy of the distribution H(p\u0302) and sets the threshold3 to min(\u03b7, \u221a \u03b7H(p\u0302)). In the latter three, the threshold is chosen implicitly rather than explicitly, for\ninstance, in nucleus sampling with parameter \u03c0, the threshold is min { p\u0302i | \u2211 p\u0302j\u2265p\u0302i p\u0302j \u2264 \u03c0 } .\nIn the extreme case, truncating all but the most likely token results in greedy decoding. Though this strategy makes it unlikely to sample a token outside the true support, it often results in degenerative patterns like repetition (Holtzman et al., 2020). Furthermore, even for modern language models that suffer less from greedy decoding traps, non-deterministic sample-based decoding is useful for generating multiple completions and for more \u201ccreative\u201d generations. Thus, the best choice of threshold must strike a balance between diversity (i.e., including as many tokens as possible in the set of candidates) and coherence (i.e., avoiding sampling tokens outside the true support).\nThe softmax bottleneck The sources of the probability overestimation errors are likely many, but one source of error is particularly compelling and well defined mathematically: the softmax bottleneck (Yang et al., 2018). The softmax bottleneck refers to the limited expressivity of models with a small hidden size and large vocabulary. Recalling the notation from Yang et al. (2018), let A \u2208 Rv\u00d7n be the matrix where each entry Ai,j = log p\u2217(i | j) is the true log-probability of token i given a prefix j from some set of n > v prefixes. Also, let W \u2208 Rv\u00d7d be the softmax matrix for a model, and H \u2208 Rd\u00d7n be the matrix of model hidden states given each prefix. Finally, let J \u2208 Rv\u00d7n be the all-ones matrix. The rank of the model\u2019s log-probability matrix\nA\u2032 = log softmax(WH) = WH \u2212 Jdiag(log v\u2211\ni=1\nexp(WH)i) (1)\nis at most d+1 because WH has inner dimension d and therefore rank at most d, and the subtrahend has identical rows and therefore has rank at most 1. The rank of A is at most v. If the rank of A is much larger than d, then A\u2032 can be at best a low-rank approximation of A. From the Eckart\u2013Young\u2013Mirsk (EYM) theorem for low-rank approximations,\nmin A\u2032:rank(A\u2032)\u2264d+1\n\u2225A\u2212A\u2032\u22252F = v\u2211\ni=d+2\n\u03c32i (2)\nwhere \u2225\u00b7\u2225F denotes the Frobenius norm, and \u03c3 is the vector of singular values of A, ordered by decreasing size. Thus, there will always be some error in the model\u2019s log-probability estimations if there are more than d + 1 linearly independent columns in A. Yang et al. (2018) hypothesize that this is indeed the case.\nDespite these theoretical shortcomings, language models still seem to perform quite well. We hypothesize that the reason for this is that default truncation sampling is sufficient to approximately mitigate errors from the softmax bottleneck. For a deeper discussion, see Appendix A."
        },
        {
            "heading": "3 A THEORETICAL EXPLANATION OF TRUNCATION SAMPLING",
            "text": "Given some textual context as input, let p\u2217 denote the true next-token distribution of the language and p\u0302 the model\u2019s predicted next-token distribution. Intuitively, if the model\u2019s probability overestimation could be additively upper bounded, i.e., if we could show that p\u0302i \u2264 p\u2217i + \u03c4 for every token i,\n2Locally typical sampling (Meister et al., 2023b) truncates tokens whose log-probability diverges from LM\u2019s conditional entropy. This can truncate top-probability tokens which presumably have non-zero true probability.\n3Hewitt et al. (2022) instead set \u03b7 = min(\u03b5, \u221a \u03b5H(p\u0302)) for a parameter \u03b5. We diverge for simplicity.\nthen this would yield a natural way to avoid sampling tokens not in the support of p\u2217: only sample tokens i with p\u0302i > \u03c4 (which, along with the bound, would imply p\u2217i > 0). This is exactly what truncation sampling does. However, a difficulty in motivating truncation sampling via this argument is that it is unclear how to derive such an additive upper bound on probability overestimation.\nOur key observation is that A\u2032 being a low-rank approximation of A can be used to conclude that the model\u2019s log-probability underestimation is non-zero but additively upper bounded. Indeed, assuming A\u2032 is a reasonably good low-rank approximation of A, Equation 1 implies such an upper bound in the log-probability space, which yields a multiplicative upper bound in the probability space. We then combine this underestimation upper bound with basic properties of a probability distribution in order to derive the desired additive upper bound on the model\u2019s probability overestimation. Lastly, we show formally how this overestimation upper bound directly motivates truncation sampling."
        },
        {
            "heading": "3.1 BOUNDING LOG-PROBABILITY UNDERESTIMATION",
            "text": "We begin by proving bounds on models\u2019 log-probability errors. Specifically, we find bounds on the maximum log-probability underestimation error of the model, max(A\u2212A\u2032). We focus exclusively on underestimation errors because log-probability overestimation errors cannot be bounded above.4\nMaximum log-probability error upper bound. We begin by upper-bounding all model\u2019s logprobability underestimations. In particular, the underestimation errors A\u2212A\u2032 are upper-bouded by max(A \u2212 A\u2032) \u2264 maxA \u2212 minA\u2032 \u2264 \u2212minA\u2032, where the last inequality holds because maxA is a log-probability and hence upper-bounded by 0. In other words, the negative minimum logprobability prediction minA\u2032 upper bounds all underestimation. As an example, a uniform predicted distribution underestimates the log-probability of a token by at most \u2212 log(1/v).\nMaximum log-probability error lower bound. Next, we lower-bound maximum underestimation errors by showing that they are strictly positive. We conjecture that this lower-bound on error is loose, i.e., that the maximum error is bounded away from 0, depending on the singular values of A."
        },
        {
            "heading": "3.2 BOUNDING PROBABILITY OVERESTIMATION",
            "text": "Having established bounds on maximum log-probability underestimation, we now show that assuming such an upper bound implies an additive upper bound on maximum probability overestimation. As before, fix some input textual context and let p\u2217 and p\u0302 denote the true and model\u2019s predicted next-token distributions, respectively, for that context. Theorem 1. If log p\u0302i underestimates log p\u2217i by at most \u03b4 for all tokens i, then p\u0302i overestimates p\u2217i by at most 1\u2212 exp(\u2212\u03b4) for all tokens i.\nSee Appendix D for a proof. Note that the precondition log p\u2217i\u2212log p\u0302i \u2264 \u03b4 implies p\u0302i \u2265 p\u2217i exp(\u2212\u03b4). Intuitively, since p\u0302 is a valid probability distribution summing to 1, if it cannot underestimate token probabilities beyond a factor of exp(\u2212\u03b4), then it also cannot overestimate other tokens\u2019 probabilities beyond a certain additive factor. We compute this additive factor and find it to be 1\u2212 exp(\u2212\u03b4)."
        },
        {
            "heading": "3.3 EXPLAINING TRUNCATION SAMPLING",
            "text": "Recall that threshold-based truncation sampling works by only sampling tokens with probability greater than some threshold \u03c4 . Sampling methods that choose a different \u03c4 at every time step can be viewed as additional heuristics for guessing when model outputs will have smaller errors. Theorem 1 provides a direct explanation for why threshold-based truncation sampling might be successful: Corollary 1 (Threshold-based truncation works). Suppose log p\u0302 underestimates log p\u2217 by at most \u03b4. Then, for any threshold \u03c4 \u2265 1\u2212 exp(\u2212\u03b4), threshold-based truncation sampling discards all tokens that are not in the support of p\u2217.\nFurthermore, based on the above proof, we present an alternative formulation of truncation sampling. 4If the true distribution assigns zero probability to some tokens in some contexts (e.g., p\u2217(\u201cate\u201d | \u201cI went to the\u201d) = 0), then the corresponding log-probability is \u2212\u221e. Hence any finite log-probability estimate will have infinite error.\n0.0\n0.5\n1.0\np1\n0.0\n0.5\n1.0\np 2\n0. 0\n0. 5\n1. 0\np 3\nFigure 3: If the model outputs p\u0302 (the blue dot) within the space of possible outputs (blue line), then each token i might have zero true probability only if there is a distribution p with pi = 0 that satisfies both the BA constraints (orange line) and the threshold constraints (orange area). For example, the orange line and area coincide at the green dot where p1 = 0, therefore token 1 might have zero true probability. The other tokens must have nonzero true probability since there are no other such solutions.\nCorollary 2 (Threshold sampling reformulation). For a model with maximum log-probability underestimation error \u03b4, if the model outputs p\u0302 and there is no distribution p with pi = 0 such that pj \u2264 p\u0302j exp(\u03b4) for j \u2208 {1, 2, . . . , v}, then p\u2217i > 0.\nThis follows directly from Equation (8) from the proof in the appendix, and is the contrapositive of the more straightforward statement that if p\u2217i = 0 then there exists a distribution satisfying inequality conditions in the corollary, namely p\u2217. One can check that only sampling tokens based on Corollary 2 yields the same candidate sets as threshold sampling with 1\u2212 exp(\u2212\u03b4) as the parameter. This alternative formulation will become useful later on when we combine methods for proving certain tokens are in the support."
        },
        {
            "heading": "4 DIRECTLY ADDRESSING ERRORS FROM THE SOFTMAX BOTTLENECK",
            "text": "The previous section demonstrates that we can arrive at truncation sampling by making an assumption about the log-probability errors, which allows us to prove that certain tokens have true probability greater than zero. However, truncating via a threshold is an inherently limited approach: if a model assigns more probability to a \u201cbad\u201d (zero true probability) token than a \u201cgood\u201d (nonzero true probability) token, then there is no threshold that discards the bad token without discarding the good token. Na\u0131\u0308vely, it would seem that this type of issue is unsolvable, however, it turns out that if this error was is caused by the softmax bottleneck, we can actually recover the good token without risking sampling the bad token. By exploiting W , the low-rank basis for the model\u2019s outputs, and we can deduce exactly which tokens may have errors due to the softmax bottleneck, regardless of their relative probability. In this section we show mathematically how we can extend threshold sampling to take full advantage of our knowledge of the softmax bottleneck."
        },
        {
            "heading": "4.1 BASIS-AWARE SAMPLING",
            "text": "At a high level, we will motivate this approach by showing that the function used to transform the hidden state h to a probability distribution p\u0302 restricts model\u2019s outputs to a subset of the possible probability distributions. When the true distribution p\u2217 lies outside of this set, then we can expect the model to output the p\u0302 within the set that minimizes the model\u2019s training loss with respect to p\u2217. We can exploit this property to identify the set of distributions wherein the true distribution lies,\nnamely the set of distributions that p\u0302 minimizes loss with. If no distributions within this set assign zero probability to a particular token, then that token must have nonzero probability.\nTo build intuition for how a model\u2019s outputs are restricted, consider the toy model in Figure 2. We generalize this toy model to a model with hidden size d and vocabulary size v. Observe that the composed functions softmax\u25e6W define a linear map: first, the model\u2019s softmax matrix W \u2208 Rv\u00d7d defines a linear map Rd \u2192 Rv . Next, it is a lesser-known fact that the softmax function is a linear map from Rv \u2192 \u2206v , where \u2206v is the (v \u2212 1)-dimensional vector space of valid probability distributions over v variables (Aitchison, 1982) (see Appendix C for an explanation). Therefore, softmax \u25e6 W : Rd \u2192 \u2206v is a linear map from a d-dimensional space to a (v \u2212 1)-dimensional space, meaning the image of this function is an at-most d-dimensional subspace of \u2206v . In other words, the space of model outputs is restricted to a subset of all possible probability distributions over the vocabulary.5\nWhat distribution should a model output, given that the true distribution p\u2217 may not lie in the subspace of possible outputs? Typically, language models are trained to minimize cross-entropy with the true distribution. Therefore, a well-trained model can be expected to output the distribution p\u0302 within the image of softmax \u25e6 W that minimizes cross-entropy with p\u2217. In other words, we assume that the model will produce the hidden state h such that crossentropy(softmax(Wh),p\u2217) is minimized. The key insight of our method is that if h does not minimize cross entropy with any distribution p such that pi = 0, then p\u2217i \u0338= 0, i.e., token i is in the true support. Theorem 2 (Basis-aware sampling). If p\u0302 is the predicted distribution from a cross-entropyminimizing model with embedding matrix W , and if there is no valid probability distribution p such that pi = 0 and W\u22a4p = W\u22a4p\u0302, then the token\u2019s true probability p\u2217i is greater than 0.\nSee proof in Appendix D. This gives us a new way to prove that tokens are in the true support, similar to Corollary 2, but in a way that directly compensates for errors due to the softmax bottleneck."
        },
        {
            "heading": "4.2 COMBINING SAMPLING METHODS",
            "text": "Theorem 2 and Corollary 2 equip us with methods for proving tokens are in the true support. By combining the constraints specified from each method we can create a hybrid proof strategy to take advantage of both methods\u2019 insights. In particular, if there does not exist a distribution p with pi = 0 such that pj \u2264 p\u0302j exp(\u03b4) for all j (the truncation constraint) and W\u22a4p = W\u22a4p\u0302 (the basis-aware constraint), then p\u2217i > 0.\nThis hybrid proof strategy naturally yields a sampling method: sample only tokens that we can prove are in the support. We call this method basis-aware threshold (BAT) sampling. Fortunately, both the threshold constraint and basis-aware (BA) constraints are linear, so we can use an off-theshelf linear programming optimizer to verify whether a token is in the support. Concretely, if the optimizer determines that there does not exist a feasible solution p \u2208 Rv such that:\npi = 0, v\u2211 j=1 pj = 1, \u2200j : 0 \u2264 pj \u2264 p\u0302j exp(\u03b4), W\u22a4p = W\u22a4p\u0302, (3)\nthen p\u2217i > 0. Thus, our sampling strategy can be: sample a token i according to the model\u2019s output probabilities; if the optimizer finds a solution to (3), reject the token and re-sample; otherwise accept. See Algorithm 1 in the Appendix.\nWe expose \u03b4 as a parameter to tune the restrictiveness of the sampling method. For large \u03b4, BAT becomes more like greedy sampling, and for small \u03b4, more like ancestral sampling. The value of \u03b4 can be chosen on a per-context basis using any threshold sampling heuristic, be it \u03f5, \u03b7, or nucleus sampling. Given a threshold \u03c4 from the heuristic, set exp \u03b4 = 1/(1 \u2212 \u03c4). We call these variants of BAT sampling BA-\u03f5, BA-\u03b7, an BA-nucleus sampling.\nA toy example. Suppose our model has hidden size 1, vocabulary size 3, and embedding matrix W\u22a4 = [0.55 0.71 0.29]. We employ the truncation sampling assumption that our model\u2019s output distributions are somewhat close to the true distribution by saying p\u2217i \u2264 p\u0302i exp \u03b4 and choosing\n5One may correctly observe that the \u201cbottleneck\u201d is a consequence of the linear map W , not the softmax function. We keep our notation for the sake of consistency with Yang et al. (2018).\n\u03b4 = log 1.9 so that p\u2217i \u2264 1.9p\u0302i for all tokens i. Additionally, assume the model\u2019s outputs minimize cross-entropy with the true distribution, i.e., W\u22a4p\u2217 = W\u22a4p\u0302 for all p\u0302. Now suppose our model outputs h = [2.55]. The output distribution is therefore p\u0302 = softmax(Wh) = [0.33 0.50 0.17]\u22a4.\nOur strategy only samples tokens for which we can prove that the true probability is positive. Referring to Figure 3, we see that there are no probability distributions p that satisfy our assumptions with p2 = 0 or p3 = 0. However, p = [0 0.70 0.30] does satisfy our assumptions. Therefore, if we sample token 1 we should reject it, as we only have evidence that p\u22172 \u0338= 0 and p\u22173 \u0338= 0. Notice that this strategy is non-monotonic: p\u03021 > p\u03023, but we only reject token 1, not token 3.\nBasis-aware threshold sampling in practice. The proposed implementation of basis-aware sampling requires solving rather large linear programs, which tends to be too computationally expensive to be practical, even when using proprietary solvers. The long run times can mainly be attributed to the size of W . To make BAT feasible in practice, we approximate the full solution by replacing W with an much smaller matrix such that no additional tokens are accepted, and the set of rejected tokens minimally increases. More details are deferred to Appendix E. This shortens the run time from over a minute on a proprietary solver to about a second. We further reduce the generation run time by observing that whenever a token has probability greater than 1 \u2212 exp(\u2212\u03b4) we can safely accept it without running the program, since the program will be infeasible. Since high-probability tokens are most likely to be sampled, the program only needs to run once every few samples. The amortized cost of BAT sampling comes to only about 0.1 seconds per token as the program typically runs every 10 samples."
        },
        {
            "heading": "5 PILOT EXPERIMENTS WITH BASIS-AWARE TRUNCATION",
            "text": "We conduct several evaluations with GPT-2 to pilot BAT sampling as a viable alternative to threshold sampling. While more powerful language models exist, these models suffice since we are primarily interested in testing the effect of the BAT sampling on performance under controlled settings.\nAs baseline methods for comparison, we select \u03b7, \u03f5, and nucleus sampling (see \u00a72). We also use \u03b7 and \u03f5 as methods for selecting the \u03b4 parameter at each time step for BAT sampling. In preliminary experiments, we also tried BA-nucleus, but found it to be significantly worse. One possible intuition for why is that the methods for choosing the threshold \u03f5 and \u03b7 are similar to the formulation of threshold sampling used to develop BAT. Nucleus sampling on the other hand determines the threshold using a function that is somewhat inconsistent with our framework.\nWe evaluate models on open-ended generation using both human annotators and automatic metrics. For each model and sampling setting, we generate completions for 5000 35-token prefixes taken from the Open Web Text (OWT) (Gokaslan et al., 2019). We use OWT because it comes from a similar distribution to GPT-2\u2019s training data. We report MAUVE (Pillutla et al., 2021) similarity between human text and generated text for parameter selection and automatic evaluation.\nParameter Selection and Evaluation. We perform a parameter sweep for nucleus, \u03b7, and \u03f5 sampling and select the parameter that gives the highest MAUVE score on the OWT validation set (see Table 3 in the appendix). We control for the parameter choice in comparisons between BAT methods and their vanilla counterparts, by matching the parameters by selecting the BAT parameter that rejects the same proportion of tokens from corpus of human text as the vanilla method; see Appendix F for more details. Using these parameters, we generate completions on the OWT test set for automatic evaluation with MAUVE and human evaluation."
        },
        {
            "heading": "5.1 QUALITATIVE, AUTOMATIC, AND HUMAN EVALUATION",
            "text": "Qualitative analysis Figure 4 shows the effects of truncation methods on the next-token distributions from 6 prefixes, drawn from Hewitt et al. (2022). Unlike threshold sampling methods, BAT can reject low-quality high-probability tokens while accepting high-quality low-probability tokens.\nBA-\u03b7 outperforms all other methods for GPT-2-Large. We compare the MAUVE scores on OWT for each method and model size in Figure 5. The results show that no single method\nToken (ordered by probability)\nL o g\np ro\nb a b\nil it\ny\n0 2 4 6 8\n10\u22124\n10\u22123\n10\u22122\n10\u22121\n100 is\n, isn\nand\nMy name\n0 2 4 6 8\n10\u22123\n10\u22122\n10\u22121\n100 Trump\nJ\nGlover\nT C and\nDonald\n0 10 20\n10\u22122\n5\u00d7 10\u22123\n6\u00d7 10\u22123\n7\u00d7 10\u22123\n8\u00d7 10\u22123\n9\u00d7 10\u22123\nDavid\nMichael\n1160 1180\nPenny\nSk\nVern\nEb\nStar\nMy name is\n0 5\n10\u22122\n6\u00d7 10\u22123\nfirst\nNew\n4355 4360\nProfessor\nhackers\n5200 5210\nTu heavily\nThe\n0 2 4 6 8\n10\u22124\n10\u22123\n10\u22122\n10\u22121\n100 Acc\nAb\nKum\nCon Ghana\nK\nThe capital of of the USA is Washington D.C. The capital of India is New Delhi. The capital of the UK is London. The\ncapital of Ghana is\n0 2 4 6 8\n10\u22123\n10\u22122\n10\u22121\n100 The\nTHE I\nThe It\nThe feeling! The feeling! The feeling! The feeling! The feeling! The feeling! The feeling! The feeling! The feeling!\nThe feeling! The feeling!\nNucleus threshold Eta threshold Epsilon threshold BA-\u03b7 accept BA-\u03b7 reject\nFigure 4: Additional qualitative examples, following the same setup as Figure 1.\nSmall Medium Large XL\n85\n90\n95\nModel size\nM A\nU V\nE\nBA-\u03b7 \u03b7\nNucleus BA-\u03f5 \u03f5\nFigure 5: MAUVE scores for sampling methods on Open Web Text test set. No single sampling method consistently outperforms across sizes. BA-\u03b7 performs remarkably well for GPT-2-Large.\nconsistently performs best, with BAT methods sometimes out-performing and sometimes underperforming their vanilla counterparts. We do, however, see that BA-\u03b7 outperforms \u03b7 sampling for the two larger model sizes, and does particularly well against all methods for GPT-2-Large.\nBA-\u03b7 outperforms \u03b7 sampling in low-entropy decoding across model sizes. We compare BA-\u03b7 and \u03b7 sampling across different \u03b7 parameters, again matching our BA-\u03b7 parameter to reject the same proportion of human text as the \u03b7 parameter. As shown in Figure 6, we find that for more restrictive sampling (i.e., larger \u03b7, closer to greedy decoding), BA-\u03b7 consistently outperforms \u03b7 sampling. To verify our results (since we know from Figure 5 that model size effects which method is best) we show in Table 1 that this pattern holds across all model sizes.\nMore constraints improves BAT. Since we reduce the number of constraints in the linear program to make it run quickly, we can add constraints back into to program to verify that the basis-aware constraints are the reason for the gains in BAT sampling. We again adjust the BAT parameter to match the proportion of rejected human text to control for the additional tokens added to the support from the new constraints. Figure 7 shows that adding more BA constraints indeed increases the MAUVE score for our method. This is direct evidence that controlling for the softmax bottleneck helps reduce errors in the model distribution.\nHuman annotators narrowly favor BA-\u03b7 and prefer coherence to diversity. To support our automatic evaluations, we additionally use human annotators from Amazon Mechanical Turk to\ncompare both methods. Annotators are tasked with pairwise comparisons between generations from each method and generated from the same prefix. See Appendix F.1 for more details. Table 2 shows that, annotators narrowly prefer generations from BA-\u03b7 sampling to those from \u03b7 sampling. Furthermore we see that human annotators prefer lower entropy generations. This is likely because humans only see 1 generation per method, making it impossible to assess diversity in the generations."
        },
        {
            "heading": "5.2 DISCUSSION",
            "text": "Overall, our results provide empirical evidence that the softmax bottleneck is responsible for significant errors in language model next-token distributions, and show that BAT sampling offers a viable method for mitigating those errors. Under low-entropy generation, BAT offers clear advantages to threshold sampling, where only a few tokens are permissible.\nAlthough our pilot study shows promising results for BA-\u03b7 sampling in low-entropy generation settings, there remain a number of limitations. For instance, as mentioned in \u00a75, BAT does not pair well with nucleus sampling. Furthermore, we find that for certain prefixes and sufficiently lowentropy sampling parameters, BA-\u03f5 accepts no tokens. This is a non-issue for threshold sampling which can fall back to greedy sampling, but because BAT relies on rejection sampling, it is not known when to revert to greedy. Though it is possible to implement a max-retries guard, this remains computationally expensive and the generations themselves tend to degrade.\nA broader issue that BAT must deal with is the expensive computation associated with running the linear program. While this is generally not an issue for generation, certain tasks are infeasible, such as finding the exact set of candidate tokens, which would require running the linear program on the full vocabulary. We remain optimistic that further optimizations to the method can be made to allow this in future work, as well as enable BAT sampling with higher constraint counts."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "Our work fills a crucial gap in the theoretical understanding of truncation sampling methods and how they account for language model errors. These theoretical findings translate into a more direct method for mitigating errors due to the softmax bottleneck. As a result, our BAT sampling method can discard higher-probability tokens while keeping higher-quality but lower-probability tokens. Lastly, our pilot study with BAT sampling shows promising results in low-entropy generation."
        },
        {
            "heading": "A FURTHER RELATED WORK",
            "text": "Generating from autoregressive language distributions. Generating strings from autoregressive, optionally conditional, generative models of language has a long history in NLP; for decades, algorithms were developed for approximating the maximum-likelihood string under the model (Jelinek, 1990), e.g., beam search (Reddy, 1977), under the understanding that there is one best output for, e.g., speech recognition. As Transformers became used for general-purpose, and often highentropy generation wherein one wants to be able to generate multiple good completions, it was found that search for the highest-likelihood strings led to low-quality generations (Fan et al., 2018; Holtzman et al., 2020; Hashimoto et al., 2019; Stahlberg & Byrne, 2019; Meister et al., 2020). In developing algorithms for high-entropy generation, the afore-mentioned line of work attempts to maintain the learned distribution as much as possible (Holtzman et al., 2020; Hewitt et al., 2022); another significant design principle has related to the uniform information density principle that humans are observed to obey, motivating Meister et al. (2023b). This algorithm intentionally deviates from the overall distribution more, by sometimes truncating high-probability tokens in order to never generate any tokens that are too high probability relative to the overall entropy. Krishna et al. (2022) show that language models do not effectively make use of long-term context, finding that an explicitly trained re-ranker can help. Li et al. (2023) hypothesizes that language model errors are distributed similarly in small models as in large, showing that taking the difference of their logits can help improve the large model\u2019s generations. Some ideas from high-entropy generation, including this, and the \u03f5-sampling algorithm, have shown to be useful even in low-entropy generation where previously algorithms like beam search have performed best (Freitag et al., 2023; Sennrich et al., 2023).\nDid the softmax bottleneck turn out not to be a problem? After the demonstration of the softmax bottleneck by Yang et al. (2018), various algorithms were proposed for efficiently learning a high-rank language models (Yang et al., 2019; Ganea et al., 2019). Chang & McCallum (2022) showed that the softmax bottleneck makes certain multi-mode distributions difficult to model, while Demeter et al. (2020) demonstrated that the low-rank nature of language models means that it is possible for certain word tokens to be unable to be the argmax, but Grivas et al. (2022) demonstrated that this is rarely the case in practice. Overall, rank considerations have not been at the fore of language model development, as language models have scaled, their hidden state sizes have scaled as well, but stayed smaller than their vocabulary sizes (Scao et al., 2022; Biderman et al., 2023; Touvron et al., 2023). Throughout this time, when one generates from language models, one almost always lowers entropy and performs some kind of truncation sampling (or in the extreme, greedy decoding). Our results suggest that training high-rank language models may appear unnecessary because default truncation sampling mitigates errors stemming from the low-rank approximation."
        },
        {
            "heading": "B BAT SAMPLING ALGORITHM",
            "text": "Algorithm 1 gives the procedure for BAT sampling.\nAlgorithm 1 BAT sampling 1: procedure BAT(Threshold \u03c4 , Next-token distribution p\u0302) 2: repeat 3: Sample i \u223c p\u0302 4: until \u2204p such that pi = 0 \u2227W\u22a4p = W\u22a4p\u0302 \u2227 \u2200j, pj \u2264 p\u0302j/(1\u2212 \u03c4) \u25b7 Use LP solver 5: return i 6: end procedure"
        },
        {
            "heading": "C THE SOFTMAX FUNCTION IS LINEAR",
            "text": "It is an unintuitive fact that the softmax function is a linear map Rd \u2192 \u2206d. The key here is that addition and scalar multiplication are defined on \u2206d in a non-standard way. The elements of \u2206d are tuples of length d whose entries sum to one. Vector addition \u2295 in \u2206d is defined as elementwise multiplication followed by normalization\np\u2295 q = p\u2299 q\u2211d i=1 piqi , (4)\nand multiplication \u2297 by a constant \u03bb \u2208 R is elementwise exponentiation followed by normalization\n\u03bb\u2297 p = p \u03bb\u2211d\ni=1 p \u03bb i\n. (5)\nOne can check that these operations satisfy the axioms of a vector space, and that the softmax function satisfies additivity and homogeneity under these operations, i.e.,\nsoftmax(u+ v) = softmax(u)\u2295 softmax(v) (6)\nand softmax(\u03bbu) = \u03bb\u2297 softmax(u). (7)"
        },
        {
            "heading": "D PROOFS",
            "text": "Proof of Theorem 1. By the precondition of the theorem, we have log p\u2217i \u2212 log p\u0302i \u2264 \u03b4 for all i. It follows that: p\u0302i \u2265 p\u2217i exp(\u2212\u03b4). (8) Intuitively, since p\u0302 is a valid probability distribution summing to 1, if it cannot underestimate token probabilities beyond a factor of exp(\u2212\u03b4), then it also cannot overestimate other tokens\u2019 probabilities beyond a certain factor; we will show that this factor is 1\u2212 exp(\u2212\u03b4). To this end, we consider each token individually and calculate the maximum possible probability overestimation based on the maximum probability underestimation of the other tokens. Keeping in mind that any probability added to a token must be removed from other tokens to preserve a valid probability distribution, the maximum probability added to a token is the sum of the maximum probabilities subtracted from the other tokens. This gives us that for all i:\np\u0302i \u2212 p\u2217i = \u2211 k \u0338=i p\u2217k \u2212 \u2211 k \u0338=i p\u0302k (9)\n\u2264 \u2211 k \u0338=i ( p\u2217k \u2212 p\u2217k exp(\u2212\u03b4) ) From (8) (10)\n= (1\u2212 exp(\u2212\u03b4)) \u2211 k \u0338=i p\u2217k Factor out p \u2217 k (11) = (1\u2212 exp(\u2212\u03b4))(1\u2212 p\u2217i ) Probabilities sum to 1 (12) \u2264 1\u2212 exp(\u2212\u03b4) 0 \u2264 p\u2217i \u2264 1. (13)\nWe thus have our desired probability overestimation bound, starting with the assumption of a logprobability underestimation bound.\nProof of Theorem 2. We begin by assuming that our model has learned to minimize cross-entropy with the true distribution, implying that\n\u2202\n\u2202h crossentropy(softmax(Wh,p\u2217)) = 0. (14)\nExpanding and simplifying this equation, we can obtain\n\u2202\n\u2202h ( \u2212 \u2211 i p\u2217i log(softmax(Wh)i) ) = 0 cross entropy defn. (15)\n\u2202\n\u2202h \u2212\u2211 i p\u2217iWhi \u2212 p\u2217i log \u2211 j exp(Wh)j  = 0 Log of softmax (16) \u2202\n\u2202h \u2211 i p\u2217i log \u2211 j exp(Wh)j = \u2202 \u2202h \u2211 i p\u2217i (Wh)i Distribute \u2202 \u2202h (17)\n\u2202 \u2202h log \u2211 j exp(Wh)j = \u2202 \u2202h \u2211 i p\u2217i (Wh)i \u2211 i\np\u2217i = 1 (18)\u2211 j exp(Wh)j\n\u2202 \u2202h (Wh)j\u2211\nj exp(Wh)j =\n\u2202\n\u2202h \u2211 i p\u2217i (Wh)i Derivative (19)\n\u2202\n\u2202h (Wh)T exp(Wh)\u2211 j exp(Wh)j = \u2202 \u2202h (Wh)Tp\u2217 Factor (20)\n\u2202\n\u2202h (Wh)T softmax(Wh) =\n\u2202\n\u2202h (Wh)Tp\u2217 Softmax defn. (21)\nW T p\u0302 = W Tp\u2217 Derivative (22)\nwhere p\u0302 is the output distribution of the model. Thus, if there does not exist any valid probability distribution p such that pi = 0 and W Tp = W T p\u0302, then p\u2217i \u0338= 0."
        },
        {
            "heading": "E BASIS-AWARE THRESHOLD SAMPLING IN PRACTICE",
            "text": "Basis-aware sampling presents a number of practical challenges. Chief among them is the sheer size of the linear programs to be solved. These programs have v variables and d + 2v + 2 constraints. No open-source solver we tried was able to solve a single problem in a reasonable amount of time, avoid hitting a numerical errors, and solve within its default max-iteration limits. Proprietary solvers do better in some cases, but only the MOSEK solver (ApS, 2023) was able to solve the full problem in under 1 minute. Even this relatively faster solving rate makes text generation at scale impractical.\nTo address this, we reduce the size of the linear program dramatically by discarding many constraints. While doing so, however, we also aim to maintain as much of the original solution space as possible, so as to minimize the effect on the set of tokens discarded by basis-aware sampling.6\nIn order to reduce the number of constraints originating from the WTp = WT p\u0302 term from d to c, we can simply discard any d\u2212c columns of W to obtain W c. Clearly, if p satisfies WTp = WT p\u0302, it will continue to also satisfy W cTp = W cT p\u0302. Thus, if a token was originally rejected by bottleneckaware sampling, it would still be rejected, i.e., using W c instead of W does not add new candidate tokens. It may, however, remove some candidates, and we would like to minimize this effect.\nSuppose W has rank b \u2264 d. Then the set of probability distributions p satisfying WTp = WT p\u0302 forms a linear subspace S \u2286 Rv of dimension d \u2212 b. Further, W c has rank at most min{b, c}, implying the set of distributions p satisfying the relaxed condition W cTp = W cT p\u0302 forms a linear superspace Sc of S of dimension at least d \u2212 min{b, c}. Recall that the larger Sc is, the more candidate tokens will be removed by bottleneck sampling. Thus, to minimize candidate removal, we seek an Sc that is of dimension exactly d\u2212min{b, c}. This can be achieved easily by keeping in W c any set of min{b, c} linearly independent columns of W . Note that if b \u2264 c, the use of such a W c\n6Without any constraints, basis-aware threshold sampling reduces to basic threshold sampling.\nwill, in fact, not remove any candidate, as Sc will equal S. Otherwise Sc will be a d\u2212c dimensional superspace of S.\nWhen b > c, however, this solution is still not optimal, as which linearly independent columns of W we choose to keep in W c determines how \u201cclose\u201d Sc will be to the original solution space S. Intuitively, we would like to preserve S along dimensions that correspond to the c largest eigenvalues of W . To accomplish this, we turn to singular value decomposition: find three matrices U \u2208 Rv\u00d7d, \u03a3 \u2208 Rd\u00d7d, and V \u2208 Rd\u00d7d such that W = U\u03a3V T , then replace W with U c \u2208 Rv\u00d7c, where U c represents the first c columns of U . Since U is simply a linear transformation of W , the solutions (in terms of p) of UTp = UT p\u0302 are precisely the subspace S of dimension d\u2212 b as before. Again, as before, replacing W with U c does not add new tokens to the set of candidates, and may remove some candidates when b > c. Importantly, when b > c, U c will intuitively be the \u201cclosest\u201d possible approximation of W (capturing its c largest eigenvalues). Thus, Sc will form a desirable approximation of S.\nThe above SVD based approximation is what we use in practice. This reduces the number of constraints from d (\u2248 700-1200 for our models) to c (typically 20), and shortens the run time from over a minute on a proprietary solver to about a second."
        },
        {
            "heading": "F PARAMETER SELECTION",
            "text": "When comparing sampling methods, choice of parameters is very important, since each method has its own diversity-coherence trade-off characteristics. Without proper controls, it is impossible to tell whether the performance gap between two heuristics might be closed by simply adjusting the parameter of the worse-performing method. To remedy this, we control for parameter choice by matching parameters of compared methods based on how conservative they are with respect to human text. In particular, for each vanilla threshold sampling method x, we choose the BA-x parameter that rejects the same proportion of tokens from a human corpus. Table 4 illustrates how we measure this human-text rejection rate (HRR). In our experiments, measure HRR by sampling 10,000 tokens with their prefixes from Open Web Text and calculating the proportion of the tokens that are accepted by a sampling method with a given parameter.\nFigure 8 gives the sampling parameters as a function of HRR. As HRR approaches zero, parameters become more permissive, i.e., nucleus approaches one, \u03b7 and \u03f5 approach zero, in order to accept more tokens. We observe that as HRR increases, BAT parameters are consistently more conservative than their vanilla counterparts since BAT methods sample tokens beyond the threshold. In the case of BA-p, the parameter maxes out around 28% HRR, meaning that it cannot reject more than 28% of human tokens.\nF.1 HUMAN EVALUATION\nAnnotators are paid $1 USD per annotation, and each annotation takes on average less than 2 minutes. Figure 9 provides the exact instructions and layout given to the annotators."
        },
        {
            "heading": "G TRUNCATED LANGUAGE MODEL DISTRIBUTIONS ARE HIGH-RANK",
            "text": "We motivated truncation sampling as helping to correctly discard tokens that are not in the support of the true distribution p\u2217 when those errors are due to the low-rank nature of language models\u2019 distributions. In this additional experiment, we show that the post-truncation conditional distribution matrix A is high-rank relative to the pre-truncation distribution.\nWe run the GPT2-XL model on samples of OpenWebText, concatenate the conditional logdistributions log p\u0302 for each prefix, and compute the rank of the resulting matrix. This becomes a rather large matrix, since each log p\u0302 is in R50257, so we are limited in the number of prefixes we can consider. Since the number of prefixes upper-bounds the estimated rank, and we cannot run, e.g., 50257 prefixes, we plot the rank for various numbers of prefixes. We find that the GPT2-xl model, which has a hidden dimensionality of 1600, has rank that saturates at 1600, as expected. For truncation sampling strategies nucleus, \u03b7-sampling, and \u03f5-sampling, we find that the estimate of the rank continues to grow with the number of prefixes, far past 1600. See Table 10."
        },
        {
            "heading": "H MORE UNIT TESTS",
            "text": "We give the unit tests used in Figures 1 and 4 in tabular form (Tables 5-11)."
        }
    ],
    "year": 2023
}