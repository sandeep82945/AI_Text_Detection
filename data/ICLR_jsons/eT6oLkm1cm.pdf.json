{
    "abstractText": "In standard adversarial training, models are optimized to fit invariant one-hot labels for adversarial data when the perturbations are within allowable budgets. However, the overconfident target harms generalization and causes the problem of robust overfitting. To address this issue and enhance adversarial robustness, we analyze the characteristics of robust models and identify that robust models tend to produce smoother and well-calibrated outputs. Based on the observation, we propose a simple yet effective method, Annealing Self-Distillation Rectification (ADR), which generates soft labels as a better guidance mechanism that reflects the underlying distribution of data. By utilizing ADR, we can obtain rectified labels that improve model robustness without the need for pre-trained models or extensive extra computation. Moreover, our method facilitates seamless plug-andplay integration with other adversarial training techniques by replacing the hard labels in their objectives. We demonstrate the efficacy of ADR through extensive experiments and strong performances across datasets.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yu-Yu Wu"
        },
        {
            "affiliations": [],
            "name": "Hung-Jui Wang"
        },
        {
            "affiliations": [],
            "name": "Shang-Tse Chen"
        }
    ],
    "id": "SP:a1c94d627dbf670111c064a6217210dc6804693b",
    "references": [
        {
            "authors": [
                "Sravanti Addepalli",
                "Samyak Jain",
                "R. Venkatesh Babu"
            ],
            "title": "Efficient and effective augmentation strategy for adversarial training",
            "year": 2022
        },
        {
            "authors": [
                "Maksym Andriushchenko",
                "Francesco Croce",
                "Nicolas Flammarion",
                "Matthias Hein"
            ],
            "title": "Square attack: A query-efficient black-box adversarial attack via random search",
            "venue": "Computer Vision - ECCV 2020 - 16th European Conference,",
            "year": 2020
        },
        {
            "authors": [
                "Anish Athalye",
                "Nicholas Carlini",
                "David A. Wagner"
            ],
            "title": "Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples",
            "venue": "Proceedings of the 35th International Conference on Machine Learning,",
            "year": 2018
        },
        {
            "authors": [
                "Nicholas Carlini",
                "Anish Athalye",
                "Nicolas Papernot",
                "Wieland Brendel",
                "Jonas Rauber",
                "Dimitris Tsipras",
                "Ian Goodfellow",
                "Aleksander Madry",
                "Alexey Kurakin"
            ],
            "title": "On evaluating adversarial robustness",
            "year": 1902
        },
        {
            "authors": [
                "Yair Carmon",
                "Aditi Raghunathan",
                "Ludwig Schmidt",
                "John C. Duchi",
                "Percy Liang"
            ],
            "title": "Unlabeled data improves adversarial robustness",
            "venue": "Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems",
            "year": 2019
        },
        {
            "authors": [
                "Mathilde Caron",
                "Hugo Touvron",
                "Ishan Misra",
                "Herv\u00e9 J\u00e9gou",
                "Julien Mairal",
                "Piotr Bojanowski",
                "Armand Joulin"
            ],
            "title": "Emerging properties in self-supervised vision transformers",
            "venue": "IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Erh-Chung Chen",
                "Che-Rung Lee"
            ],
            "title": "LTD: low temperature distillation for robust adversarial training",
            "venue": "CoRR, abs/2111.02331,",
            "year": 2021
        },
        {
            "authors": [
                "Tianlong Chen",
                "Zhenyu Zhang",
                "Sijia Liu",
                "Shiyu Chang",
                "Zhangyang Wang"
            ],
            "title": "Robust overfitting may be mitigated by properly learned smoothening",
            "venue": "In 9th International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Francesco Croce",
                "Matthias Hein"
            ],
            "title": "Minimally distorted adversarial examples with a fast adaptive boundary attack",
            "venue": "In Proceedings of the 37th International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Francesco Croce",
                "Matthias Hein"
            ],
            "title": "Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks",
            "venue": "In Proceedings of the 37th International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Francesco Croce",
                "Maksym Andriushchenko",
                "Vikash Sehwag",
                "Edoardo Debenedetti",
                "Nicolas Flammarion",
                "Mung Chiang",
                "Prateek Mittal",
                "Matthias Hein"
            ],
            "title": "Robustbench: a standardized adversarial robustness benchmark",
            "venue": "Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks",
            "year": 2021
        },
        {
            "authors": [
                "Jiequan Cui",
                "Shu Liu",
                "Liwei Wang",
                "Jiaya Jia"
            ],
            "title": "Learnable boundary guided adversarial training",
            "venue": "IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Jia Deng",
                "Wei Dong",
                "Richard Socher",
                "Li-Jia Li",
                "Kai Li",
                "Li Fei-Fei"
            ],
            "title": "Imagenet: A large-scale hierarchical image database",
            "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition",
            "year": 2009
        },
        {
            "authors": [
                "Chengyu Dong",
                "Liyuan Liu",
                "Jingbo Shang"
            ],
            "title": "Label noise in adversarial training: A novel perspective to study robust overfitting",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Yinpeng Dong",
                "Ke Xu",
                "Xiao Yang",
                "Tianyu Pang",
                "Zhijie Deng",
                "Hang Su",
                "Jun Zhu"
            ],
            "title": "Exploring memorization in adversarial training",
            "venue": "In The Tenth International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Logan Engstrom",
                "Andrew Ilyas",
                "Anish Athalye"
            ],
            "title": "Evaluating and understanding the robustness of adversarial logit pairing",
            "year": 2018
        },
        {
            "authors": [
                "Timur Garipov",
                "Pavel Izmailov",
                "Dmitrii Podoprikhin",
                "Dmitry P. Vetrov",
                "Andrew Gordon Wilson"
            ],
            "title": "Loss surfaces, mode connectivity, and fast ensembling of dnns",
            "venue": "Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems",
            "year": 2018
        },
        {
            "authors": [
                "Micah Goldblum",
                "Liam Fowl",
                "Soheil Feizi",
                "Tom Goldstein"
            ],
            "title": "Adversarially robust distillation",
            "venue": "In The Thirty-Fourth AAAI Conference on Artificial Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Micah Goldblum",
                "Liam Fowl",
                "Soheil Feizi",
                "Tom Goldstein"
            ],
            "title": "Adversarially robust distillation",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Sven Gowal",
                "Chongli Qin",
                "Jonathan Uesato",
                "Timothy A. Mann",
                "Pushmeet Kohli"
            ],
            "title": "Uncovering the limits of adversarial training against norm-bounded adversarial examples",
            "year": 2010
        },
        {
            "authors": [
                "Julia Grabinski",
                "Paul Gavrikov",
                "Janis Keuper",
                "Margret Keuper"
            ],
            "title": "Robust models are less overconfident",
            "year": 2022
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Identity mappings in deep residual networks",
            "venue": "Computer Vision - ECCV 2016 - 14th European Conference,",
            "year": 2016
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Kevin Gimpel"
            ],
            "title": "Bridging nonlinearities and stochastic regularizers with gaussian error linear units",
            "year": 2016
        },
        {
            "authors": [
                "Geoffrey E. Hinton",
                "Oriol Vinyals",
                "Jeffrey Dean"
            ],
            "title": "Distilling the knowledge in a neural network",
            "venue": "CoRR, abs/1503.02531,",
            "year": 2015
        },
        {
            "authors": [
                "Jonathan Ho",
                "Ajay Jain",
                "Pieter Abbeel"
            ],
            "title": "Denoising diffusion probabilistic models",
            "venue": "Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems",
            "year": 2020
        },
        {
            "authors": [
                "Pavel Izmailov",
                "Dmitrii Podoprikhin",
                "Timur Garipov",
                "Dmitry P. Vetrov",
                "Andrew Gordon Wilson"
            ],
            "title": "Averaging weights leads to wider optima and better generalization",
            "venue": "Proceedings of the Thirty-Fourth Conference on Uncertainty in Artificial Intelligence,",
            "year": 2018
        },
        {
            "authors": [
                "Xiaojun Jia",
                "Yong Zhang",
                "Baoyuan Wu",
                "Ke Ma",
                "Jue Wang",
                "Xiaochun Cao"
            ],
            "title": "LAS-AT: adversarial training with learnable attack strategy",
            "venue": "In IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Alex Krizhevsky",
                "Geoffrey Hinton"
            ],
            "title": "Learning multiple layers of features from tiny images",
            "year": 2009
        },
        {
            "authors": [
                "Alexey Kurakin",
                "Ian J. Goodfellow",
                "Samy Bengio"
            ],
            "title": "Adversarial machine learning at scale",
            "venue": "In 5th International Conference on Learning Representations,",
            "year": 2017
        },
        {
            "authors": [
                "Ya Le",
                "Xuan Yang"
            ],
            "title": "Tiny imagenet visual recognition challenge",
            "venue": "CS 231N,",
            "year": 2015
        },
        {
            "authors": [
                "Hao Li",
                "Zheng Xu",
                "Gavin Taylor",
                "Christoph Studer",
                "Tom Goldstein"
            ],
            "title": "Visualizing the loss landscape of neural nets",
            "venue": "Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems",
            "year": 2018
        },
        {
            "authors": [
                "Lin Li",
                "Michael W. Spratling"
            ],
            "title": "Data augmentation alone can improve adversarial training",
            "venue": "CoRR, abs/2301.09879,",
            "year": 2023
        },
        {
            "authors": [
                "Fangzhou Liao",
                "Ming Liang",
                "Yinpeng Dong",
                "Tianyu Pang",
                "Xiaolin Hu",
                "Jun Zhu"
            ],
            "title": "Defense against adversarial attacks using high-level representation guided denoiser",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter"
            ],
            "title": "SGDR: stochastic gradient descent with warm restarts",
            "venue": "In 5th International Conference on Learning Representations,",
            "year": 2017
        },
        {
            "authors": [
                "Aleksander Madry",
                "Aleksandar Makelov",
                "Ludwig Schmidt",
                "Dimitris Tsipras",
                "Adrian Vladu"
            ],
            "title": "Towards deep learning models resistant to adversarial attacks",
            "venue": "In 6th International Conference on Learning Representations,",
            "year": 2018
        },
        {
            "authors": [
                "Rafael M\u00fcller",
                "Simon Kornblith",
                "Geoffrey E. Hinton"
            ],
            "title": "When does label smoothing help",
            "venue": "Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems",
            "year": 2019
        },
        {
            "authors": [
                "Daniel Paleka",
                "Amartya Sanyal"
            ],
            "title": "A law of adversarial risk, interpolation, and label",
            "venue": "noise. CoRR,",
            "year": 2022
        },
        {
            "authors": [
                "Tianyu Pang",
                "Xiao Yang",
                "Yinpeng Dong",
                "Hang Su",
                "Jun Zhu"
            ],
            "title": "Bag of tricks for adversarial training",
            "venue": "In 9th International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Yao Qin",
                "Xuezhi Wang",
                "Alex Beutel",
                "Ed H. Chi"
            ],
            "title": "Improving calibration through the relationship with adversarial robustness",
            "venue": "Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems",
            "year": 2021
        },
        {
            "authors": [
                "Rahul Rade",
                "Seyed-Mohsen Moosavi-Dezfooli"
            ],
            "title": "Reducing excessive margin to achieve a better accuracy vs. robustness trade-off",
            "venue": "In The Tenth International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Sylvestre-Alvise Rebuffi",
                "Sven Gowal",
                "Dan A. Calian",
                "Florian Stimberg",
                "Olivia Wiles",
                "Timothy A. Mann"
            ],
            "title": "Fixing data augmentation to improve adversarial robustness",
            "venue": "CoRR, abs/2103.01946,",
            "year": 2021
        },
        {
            "authors": [
                "Sylvestre-Alvise Rebuffi",
                "Sven Gowal",
                "Dan Andrei Calian",
                "Florian Stimberg",
                "Olivia Wiles",
                "Timothy Mann"
            ],
            "title": "Data augmentation can improve robustness",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Leslie Rice",
                "Eric Wong",
                "J. Zico Kolter"
            ],
            "title": "Overfitting in adversarially robust deep learning",
            "venue": "In Proceedings of the 37th International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Ludwig Schmidt",
                "Shibani Santurkar",
                "Dimitris Tsipras",
                "Kunal Talwar",
                "Aleksander Madry"
            ],
            "title": "Adversarially robust generalization requires more data",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Vikash Sehwag",
                "Saeed Mahloujifar",
                "Tinashe Handina",
                "Sihui Dai",
                "Chong Xiang",
                "Mung Chiang",
                "Prateek Mittal"
            ],
            "title": "Robust learning meets generative models: Can proxy distributions improve adversarial robustness",
            "venue": "In The Tenth International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Leslie N. Smith",
                "Nicholay Topin"
            ],
            "title": "Super-convergence: Very fast training of residual networks using large learning",
            "venue": "rates. CoRR,",
            "year": 2017
        },
        {
            "authors": [
                "Jasper Snoek",
                "Yaniv Ovadia",
                "Emily Fertig",
                "Balaji Lakshminarayanan",
                "Sebastian Nowozin",
                "D. Sculley",
                "Joshua V. Dillon",
                "Jie Ren",
                "Zachary Nado"
            ],
            "title": "Can you trust your model\u2019s uncertainty? evaluating predictive uncertainty under dataset shift",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "David Stutz",
                "Matthias Hein",
                "Bernt Schiele"
            ],
            "title": "Confidence-calibrated adversarial training: Generalizing to unseen attacks",
            "venue": "In Proceedings of the 37th International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "David Stutz",
                "Matthias Hein",
                "Bernt Schiele"
            ],
            "title": "Relating adversarially robust generalization to flat minima",
            "venue": "IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Christian Szegedy",
                "Wojciech Zaremba",
                "Ilya Sutskever",
                "Joan Bruna",
                "Dumitru Erhan",
                "Ian J. Goodfellow",
                "Rob Fergus"
            ],
            "title": "Intriguing properties of neural networks",
            "venue": "In Yoshua Bengio and Yann LeCun (eds.), 2nd International Conference on Learning Representations,",
            "year": 2014
        },
        {
            "authors": [
                "Christian Szegedy",
                "Vincent Vanhoucke",
                "Sergey Ioffe",
                "Jonathon Shlens",
                "Zbigniew Wojna"
            ],
            "title": "Rethinking the inception architecture for computer vision",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Antti Tarvainen",
                "Harri Valpola"
            ],
            "title": "Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results",
            "venue": "In 5th International Conference on Learning Representations,",
            "year": 2017
        },
        {
            "authors": [
                "Antonio Torralba",
                "Robert Fergus",
                "William T. Freeman"
            ],
            "title": "80 million tiny images: A large data set for nonparametric object and scene recognition",
            "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,",
            "year": 1958
        },
        {
            "authors": [
                "Jonathan Uesato",
                "Brendan O\u2019Donoghue",
                "Pushmeet Kohli",
                "A\u00e4ron van den Oord"
            ],
            "title": "Adversarial risk and the dangers of evaluating against weak attacks",
            "venue": "Proceedings of the 35th International Conference on Machine Learning,",
            "year": 2018
        },
        {
            "authors": [
                "Eric Wong",
                "J. Zico Kolter"
            ],
            "title": "Provable defenses against adversarial examples via the convex outer adversarial polytope",
            "venue": "Proceedings of the 35th International Conference on Machine Learning,",
            "year": 2018
        },
        {
            "authors": [
                "Dongxian Wu",
                "Shu-Tao Xia",
                "Yisen Wang"
            ],
            "title": "Adversarial weight perturbation helps robust generalization",
            "venue": "Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems",
            "year": 2020
        },
        {
            "authors": [
                "Sangdoo Yun",
                "Dongyoon Han",
                "Sanghyuk Chun",
                "Seong Joon Oh",
                "Youngjoon Yoo",
                "Junsuk Choe"
            ],
            "title": "Cutmix: Regularization strategy to train strong classifiers with localizable features",
            "venue": "IEEE/CVF International Conference on Computer Vision,",
            "year": 2019
        },
        {
            "authors": [
                "Sergey Zagoruyko",
                "Nikos Komodakis"
            ],
            "title": "Wide residual networks",
            "venue": "Proceedings of the British Machine Vision Conference 2016,",
            "year": 2016
        },
        {
            "authors": [
                "Hongyang Zhang",
                "Yaodong Yu",
                "Jiantao Jiao",
                "Eric P. Xing",
                "Laurent El Ghaoui",
                "Michael I. Jordan"
            ],
            "title": "Theoretically principled trade-off between robustness and accuracy",
            "venue": "In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), International conference on machine learning,",
            "year": 2019
        },
        {
            "authors": [
                "Shudong Zhang",
                "Haichang Gao",
                "Tianwei Zhang",
                "Yunyi Zhou",
                "Zihui Wu"
            ],
            "title": "Alleviating robust overfitting of adversarial training with consistency regularization",
            "year": 2022
        },
        {
            "authors": [
                "Shiji Zhao",
                "Jie Yu",
                "Zhenlong Sun",
                "Bo Zhang",
                "Xingxing Wei"
            ],
            "title": "Enhanced accuracy and robustness via multi-teacher adversarial distillation",
            "venue": "Computer Vision - ECCV 2022 - 17th European Conference,",
            "year": 2022
        },
        {
            "authors": [
                "Jianing Zhu",
                "Jiangchao Yao",
                "Bo Han",
                "Jingfeng Zhang",
                "Tongliang Liu",
                "Gang Niu",
                "Jingren Zhou",
                "Jianliang Xu",
                "Hongxia Yang"
            ],
            "title": "Reliable adversarial distillation with unreliable teachers",
            "venue": "In The Tenth International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Bojia Zi",
                "Shihao Zhao",
                "Xingjun Ma",
                "Yu-Gang Jiang"
            ],
            "title": "Revisiting adversarial robustness distillation: Robust soft labels make student better",
            "venue": "IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Yun"
            ],
            "title": "A ADDITIONAL RELATED WORK: MITIGATE ROBUST OVERFITTING The phenomenon of robust overfitting Rice et al. (2020) represents a significant challenge in AT, motivating researchers to explore various avenues for mitigation. One such approach is to use heuristic-driven augmentations",
            "venue": "Rebuffi et al",
            "year": 2024
        },
        {
            "authors": [
                "Sehwag"
            ],
            "title": "2022), the introduction of an extra set of 500K pseudo-labeled images from 80M-TI Torralba et al. (2008) eliminates the occurrence of robust overfitting Rebuffi et al. (2021a). Despite the demonstrated effectiveness of extra data, increasing the size of the training set is computationally expensive, rendering AT infeasible for larger datasets. Early stopping is a straightforward method for producing robust models Rice et al. (2020)",
            "year": 2020
        },
        {
            "authors": [
                "Chen"
            ],
            "title": "ever, due to the fact that the checkpoint of optimal robust accuracy and that of standard accuracy frequently do not align Chen et al. (2021), utilizing either of these measures can result in a compromise of overall performance",
            "venue": "Weight Average (WA) Izmailov et al",
            "year": 2017
        },
        {
            "authors": [
                "Athalye"
            ],
            "title": "2018) argued that some defenses improving the robustness by obfuscated gradient",
            "year": 2018
        },
        {
            "authors": [
                "Chen"
            ],
            "title": "Comparison of loss landscapes for the non-robust model (the first row), PGD-AT model (the second row), and ADR model (the third row). Loss plots in each column are generated from the same image chosen from the CIFAR-10. Following the same setting as Engstrom et al",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Deep Neural Network (DNN) has been shown to exhibit susceptibility to adversarial attacks (Szegedy et al., 2014), wherein intentionally crafted imperceptible perturbations introduced into the original input cause the model\u2019s predictions to be altered. Among various defense methods (Kurakin et al., 2017; Liao et al., 2018; Wong & Kolter, 2018), Adversarial Training (AT) (Madry et al., 2018) stands out as one of the most effective techniques (Athalye et al., 2018; Uesato et al., 2018) to enhance DNN\u2019s adversarial robustness. However, while AT has proven effective in countering adversarial attacks, it is not immune to the problem of robust overfitting (Rice et al., 2020). AT constrains the model to generate consistent output when subjected to perturbations within an \u03f5 attack budget. However, manually assigned hard labels are noisy (Dong et al., 2022b;a) for AT since they fail to reflect shifts in the data distribution. Minimizing the adversarial training loss results in worse generalization ability on the test data. To address this issue, several approaches have been proposed, including label smoothing (Pang et al., 2021), consistency regularization (Dong et al., 2022b; Tarvainen & Valpola, 2017), and knowledge distillation (Chen et al., 2021; Cui et al., 2021; Goldblum et al., 2020a; Zhu et al., 2022; Zi et al., 2021; Zhao et al., 2022), which create a smoother objective function to alleviate the problem of robust overfitting. However, these approaches often assume uniform label noise (Pang et al., 2021), require consistency in the model\u2019s output over time (Dong et al., 2022b; Tarvainen & Valpola, 2017), or depend on an additional robust model trained in advance (Chen et al., 2021; Cui et al., 2021; Goldblum et al., 2020a). None of these methods directly focus on designing a well-rectified target without the need for pre-trained models.\nTo enhance AT, we investigate the characteristics that distinguish robust models from non-robust ones by analyzing the disparity in output distributions. Our findings indicate that robust models should possess good calibration ability, which is manifested by a lower average confidence level when it is likely to make errors. In addition, robust models\u2019 output distribution should remain consistent for the clean data and its adversarial counterpart. Based on this observation, we propose a novel approach called Annealing Self-Distillation Rectification (ADR), which interpolates the outputs from model weight\u2019s momentum encoder with one-hot targets to generate noise-aware labels that reflect the underlying distribution of data. The intuition behind this method is that if an image\nis similar to the other classes at once, we should assign a higher probability to those classes but still maintain the dominant probability for the actual class to ensure the final prediction is not altered.\nThe weight momentum encoding scheme, also known as Mean Teacher, is a widely used technique in semi-supervised (Tarvainen & Valpola, 2017) and self-supervised (Grill et al., 2020; Caron et al., 2021) learning that involves maintaining exponential moving average (EMA) of weights on the trained model. The self-distillation EMA also serves as a Weight Average (WA) (Garipov et al., 2018) method, which smoothes the loss landscape to enhance robustness (Izmailov et al., 2018; Chen et al., 2021; Gowal et al., 2020). To ensure our model generates well-calibrated results that reflect inter-class relations within examples, we introduce softmax temperature (Hinton et al., 2015) to scale outputs from the EMA model. Initially, the temperature is set to a larger value producing uniform distribution but gradually decreases following a cosine annealing schedule. This dynamic adjustment of temperature is undertaken in recognition of the EMA model\u2019s capacity to represent inter-class relationships improves over the course of training. We introduce an interpolation strategy to ensure the true class label consistently maintains the highest probability in targets. Notably, the interpolation factor experiences progressive increments as a reflection of our growing confidence in the precision and robustness of the momentum encoder. In summary, our contributions are:\n\u2022 We conduct a comprehensive analysis of the robust models\u2019 output properties. Our investigation confirms the calibration ability of robust models. Additionally, we observe that robust models maintain output consistency on benign data and its adversarial counterpart, which motivates us to design a unified rectified label to enhance the efficacy of adversarial defense. For samples within the lp norm neighborhood of a given input, they should be associated with a single smooth and rectified target in adversarial training.\n\u2022 We propose Annealing Self-Distillation Rectification (ADR), a simple yet effective technique that leverages noise-aware label reformulation to refine the original one-hot target. Through this method, we obtain well-calibrated results without requiring pre-trained models or extensive extra computational resources. Additionally, ADR can be incorporated into other adversarial training algorithms at ease by substituting the hard label in their objectives, thus enabling a seamless plug-and-play integration.\n\u2022 Our experimental results across multiple datasets demonstrate the efficacy of ADR in improving adversarial robustness. Substitute the hard labels with well-calibrated ones generated by ADR alone can achieve remarkable gains in robustness. When combined with other AT tricks (WA, AWP), ADR further outperforms the state-of-the-art results on CIFAR-100 and TinyImageNet-200 datasets with various architectures."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Adversarial training (AT) has been demonstrated to be effective in enhancing the white box robustness of DNN (Croce et al., 2021). PGD-AT (Madry et al., 2018), which introduces worst-case inputs during training, has been the most popular approach for improving robustness. An alternative AT method, TRADES (Zhang et al., 2019), provides a systematic approach to regulating the trade-off between natural accuracy and robustness and has yielded competitive results across multiple datasets. Despite the efficacy, AT often suffers from robust overfitting (Rice et al., 2020). Below, we summarize works that address the issue of robust overfitting by reforming the label. We also provide an extra survey on works aiming to mitigate the issue with other methods in Appendix A.\nRectify labels in AT. AT can smooth the predictive distributions by increasing the likelihood of the target around the \u03f5-neighborhood of the observed training examples (Lakshminarayanan et al., 2017). A recent study by Grabinski et al. (2022) has shown that robust models produced by AT tend to exhibit lower confidence levels than non-robust models, even when evaluated on clean data. Due to the substantial differences in output distributions between robust and standard models, using one-hot labels, which encourage high-confidence predictions on adversarial examples, may not be optimal. Dong et al. (2022b) and Dong et al. (2022a) have demonstrated that one-hot labels are noisy in AT, as they are inherited from clean examples while the data had been distorted by attacks. The mismatch between the assigned labels and the true distributions can exacerbate overfitting compared to standard training. Rectifying labels is shown to be effective in addressing the issue of robust overfitting in AT (Dong et al., 2022a). Label Smoothing (LS) (Szegedy et al., 2016) is a technique\nthat softens labels by combining one-hot targets and a uniform distribution. By appropriately choosing the mixing factor, mild LS can enhance model robustness while calibrating the confidence of the trained model (Pang et al., 2021; Stutz et al., 2020). However, overly smoothing labels in a data-blind manner can diminish the discriminative power of the model (Mu\u0308ller et al., 2019; Paleka & Sanyal, 2022) and make it susceptible to gradient masking (Athalye et al., 2018). Prior works (Chen et al., 2021; Cui et al., 2021; Goldblum et al., 2020a; Zhu et al., 2022; Zi et al., 2021; Zhao et al., 2022) have utilized Knowledge Distillation (KD) to generate data-driven soft labels, outperforming baseline approaches. Temporal Ensembling (TE) (Dong et al., 2022b) and Mean Teacher (MT) (Zhang et al., 2022) have applied consistency loss into training objectives, thus preventing overconfident predictions through consistency regularization. More recently, Dong et al. (2022a) have employed a pre-trained robust model to reform training labels, addressing label noise in AT."
        },
        {
            "heading": "3 PRELIMINARIES",
            "text": ""
        },
        {
            "heading": "3.1 ADVERSARIAL TRAINING (AT)",
            "text": "Given a dataset D = {(xi, yi)}ni=1, where xi \u2208 Rd is a benign example, yi \u2208 {1, . . . , C} is ground truth label often encoded as a one-hot vector yi \u2208 {0,1}C , and C is the total number of classes, PGD-AT (Madry et al., 2018) can be formulated as the following min-max optimization problem:\nmin \u03b8 n\u2211 i=1 max x\u2032i\u2208S(xi) \u2113(f\u03b8(x \u2032 i),yi) (1)\nwhere f\u03b8 is a model with parameter \u03b8. \u2113 is the cross-entropy loss function, and S(x) = {x\u2032 : ||x\u2032\u2212x||p \u2264 \u03f5} is an adversarial region centered at x with radius \u03f5 > 0 under lp-norm threat model. The adversarial example x\u2032i can be obtained by projected gradient descent (PGD) to approximate the inner maximization in adversarial training, which randomly initializes a point within S(xi) and iteratively updates the point for K steps with:\nxt+1i = \u03a0S(xi)(x t i + \u03b1 \u00b7 sign(\u2207x\u2113(f\u03b8(xti),yi))) (2)\nwhere \u03a0(.) is the projection, \u03b1 is the attack step size, t denotes iteration count, and x\u2032i = x K i ."
        },
        {
            "heading": "3.2 DISTRIBUTIONAL DIFFERENCE IN THE OUTPUTS OF ROBUST AND NON-ROBUST MODEL",
            "text": "The standard training approach encourages models to generate confident predictions regardless of the scenario, leading to overconfident outcomes when the testing distribution changes. In contrast, robust models, when compared to their standardly trained counterparts, possess superior calibration properties that exhibit low confidence in incorrect classified examples (Grabinski et al., 2022). In addition, a study conducted by Qin et al. (2021) has revealed that poorly calibrated examples are more vulnerable to attacks. The interplay between robustness and confidence calibration motivates us to enhance the information inherent in labels. Therefore, we initiate our analysis by examining the differences in output distribution between robust and normal models.\nRobust model generates a random output on OOD data When there is a significant distribution shift in the testing data, a well-calibrated model is expected to display uncertainty in its predictions by assigning uniformly random probabilities to unseen examples. To analyze the difference in output distributions when predicting out-of-distribution (OOD) data, we follow the approach by Snoek et al. (2019); Qin et al. (2021). Specifically, we compare the histogram of the output entropy of the models. We use ResNet-18 to train the models on the CIFAR-10 (in-distribution) training set and evaluate the CIFAR-100 (OOD) testing set. Given that most categories in CIFAR-100 were not present during training, we expect the models to reveal suspicion. As shown in Figure 1, the non-robust model has low entropy (high confidence) on OOD data, while the robust model exhibits high uncertainty on average.\nRobust models are uncertain on incorrectly classified examples To investigate potential distributional differences in the behavior of standard and robust models when encountering correctly or incorrectly classified examples, we consider the model\u2019s confidence level in its predictions. Lower confidence levels are typically associated with higher error rates if the model is well-calibrated. To this end, we experiment by using a ResNet-18 model trained and evaluated on the CIFAR-10 dataset to demonstrate the distributional differences between correctly and incorrectly classified examples for both standard and robust models. Specifically, Figure 2a illustrates that the standard model exhibits low entropy levels for correctly classified examples, but relatively uniform entropy levels for incorrectly classified ones. Higher confidence in the prediction does not guarantee better performance. On the other hand, Figure 2b shows that the robust model tends to exhibit relatively high entropy levels (low confidence) for misclassified examples. We can infer that when the robust model is confident in its prediction, the classification accuracy is likely to be high.\nOutput distribution of models on clean or adversarial examples are consistent Several prior studies (Zhao et al., 2022; Cui et al., 2021) have suggested learning clean images\u2019 representation from the standard model and adversarial example\u2019s representation from the robust model to improve robustness while maintaining accuracy. However, the underlying assumption that the robust model exhibits comparable representations of clean images to those generated by the standard model has not been thoroughly examined. Therefore, we investigate whether these models show comparable behavior when presented with clean and adversarial (PGD-10) examples on the CIFAR-10 dataset.\nWe demonstrate that the standard model exhibits low entropy in both scenarios (Figure 2c), whereas the robust model yields high entropy on average (Figure 2d). Additionally, Figure 2e reveals two models\u2019 histograms of JS divergence, representing the extent of output distribution shift when input is attacked. We can observe that even if robust models are attacked successfully, the change of output distribution measured in JS divergence is still small compared to standard models. The robust models show higher consistency (low JS divergence), while the standard models make drastic output changes. Therefore, promoting learning from standard models on normal examples may not be ideal for robust models, as robust models do not generate high confidence output on clean data."
        },
        {
            "heading": "4 METHODOLOGY",
            "text": ""
        },
        {
            "heading": "4.1 MOTIVATION: RECTIFY LABELS IN A NOISE-AWARE MANNER",
            "text": "Based on previous analysis, robust models should satisfy three key properties: first, they generate nearly random probability on OOD data; second, they demonstrate high uncertainty when it is likely to make a mistake; and third, they exhibit output consistency for both clean examples and their adversarial counterparts. However, the one-hot label used in AT does not provide sufficient guidance to reflect real-world distribution. Restricting the output to fix hard labels can be toxic to adversarial training (Dong et al., 2022a), as it causes the model to memorize the labels (Dong et al., 2022b) to minimize training loss, but at the expense of losing generalization ability on the testing set. In a recent study conducted by Paleka & Sanyal (2022), it was found that uniform label noise has a similar degree of adverse impact as worst-case data poisoning. They also provide empirical evidence that real-world noise is less harmful than uniform-label noise. Specifically, the noise introduced by human annotators poses a lower adversarial risk than uniform label noise. Therefore, designing a label-softening mechanism that gradually approaches the true underlying distribution instead of assigning uniform noise as label smoothing is essential to improve robustness. Building on these in-\nsights, we propose a data-driven scheme, Annealing Self-Distillation Rectification (ADR), to rectify labels in a noise-aware manner that mimics the behavior of human annotators."
        },
        {
            "heading": "4.2 ANNEALING SELF-DISTILLATION RECTIFICATION",
            "text": "To be specific, let \u03b8s represent the trained model\u2019s parameter to optimize, and \u03b8t be the EMA of \u03b8s, which is updated by \u03b8t = \u03b3 \u00b7 \u03b8t + (1\u2212 \u03b3) \u00b7 \u03b8s where \u03b3 is the decay factor. Pt(xi) is EMA\u2019s output distribution on input xi, and Pt(xi)(c) serves as EMA\u2019s predicted probability for the class c. To obtain a rectified label, we first calculate the EMA\u2019s softened distribution Pt(xi) with temperature \u03c4 , which follows a cosine annealing from high to low. Since f\u03b8t cannot provide sufficient knowledge at the beginning of training, the high temperature encourages the EMA\u2019s output to approach uniform noise. As f\u03b8t becomes more accurate and robust, we anneal the temperature to make the distribution more descriptive of the inter-class relations. The smoothed distribution Pt(xi) for f\u03b8t is as follows,\nPt(xi) = softmax(f\u03b8t(xi)/\u03c4) (3)\nHowever, f\u03b8t does not always classify correctly, especially when training is insufficient. To ensure the correct class has the highest probability so the target is unchanged, we interpolate the predicted distribution of f\u03b8t , Pt(xi), with ground-truth one-hot yi, which is built from yi, by an interpolation ratio \u03bb. \u03bb follows an increasing cosine schedule, allowing us to trust EMA more over time. For each xi, we also adjust \u03bb to \u03bbi to ensure the true class has the highest probability across the distribution.\n\u03bbi = clip[0,1](\u03bb\u2212 (Pt(xi)(\u03c8i) \u2212 Pt(xi)(yi))) (4)\nwhere \u03c8i exhibits the EMA\u2019s predicted class and yi represents the ground truth class. When EMA makes a correct prediction, that is Pt(xi)(\u03c8i) = Pt(xi)(yi), there is no need to adjust the interpolation rate, otherwise, we decrease \u03bb by the amount that the EMA model makes mistake on xi and then clip to the [0, 1] range. Finally, the rectified distribution P (xi) used for adversarial attack and training is carried out as P (xi) = \u03bbi \u00b7 Pt(xi) + (1\u2212 \u03bbi) \u00b7 yi (5) We use the rectified label P (xi) to replace the ground truth label yi in Equation 1 and Equation 2 to conduct adversarial training. Similarly, the softened P (xi) can be applied in other adversarial training algorithms, e.g. TRADES (Zhang et al., 2019), by replacing the hard labels. We illustrate the overview of ADR in Figure 3 and summarize the pseudo-code in Appendix B."
        },
        {
            "heading": "5 EXPERIMENTS",
            "text": "In this section, we compare the proposed ADR to PGD-AT and TRADES in Table 1. We further investigate the efficacy of ADR in conjunction with model-weight-space smoothing techniques Weight Average (WA) (Izmailov et al., 2018; Gowal et al., 2020) and Adversarial Weight Perturbation (AWP) (Wu et al., 2020) with ResNet18 (He et al., 2016a) in Table 2. Experiments are conducted on well-established benchmark datasets, including CIFAR-10, CIFAR-100 (Krizhevsky et al., 2009), and TinyImageNet-200 (Le & Yang, 2015; Deng et al., 2009). We highlight that\ndefending against attacks on datasets with enormous classes is more difficult as the model\u2019s decision boundary becomes complex. We also provide experiments on Wide ResNet (WRN-34-10) (Zagoruyko & Komodakis, 2016) with additional data compared to other state-of-the-art methods reported on RobustBench (Croce et al., 2021) in Table 3. Our findings reveal that ADR outperforms methods across different architectures, irrespective of the availability of additional data."
        },
        {
            "heading": "5.1 TRAINING AND EVALUATION SETUP",
            "text": "We perform adversarial training with perturbation budget \u03f5 = 8/255 under l\u221e-norm in all experiments. In training, we use the 10-step PGD adversary with step size \u03b1 = 2/255. We adopt \u03b2 = 6 for TRADES as outlined in the original paper. The models are trained using the SGD optimizer with Nesterov momentum of 0.9, weight decay 0.0005, and a batch size of 128. The initial learning rate is set to 0.1 and divided by 10 at 50% and 75% of the total training epochs. Simple data augmentations include 32\u00d7 32 random crop with 4-pixel padding and random horizontal flip (Rice et al., 2020; Gowal et al., 2020; Pang et al., 2021) are applied in all experiments. Following Wu et al. (2020); Gowal et al. (2020), we choose radius 0.005 for AWP and decay rate \u03b3 = 0.995 for WA. For CIFAR-10/100, we use 200 total training epochs, \u03bb follows cosine scheduling from 0.7 to 0.95, and \u03c4 is annealed with cosine decreasing from 2.5 to 2 on CIFAR-10 and 1.5 to 1 on CIFAR100, respectively. As for TinyImageNet-200, we crop the image size to 64\u00d7 64 and use 80 training epochs. We adjust \u03bb from 0.5 to 0.9 and \u03c4 from 2 to 1.5 on this dataset.\nDuring training, we evaluate the model with PGD-10 and select the model that has the highest robust accuracy on the validation set with early stopping (Rice et al., 2020). For testing, we use AutoAttack (Croce & Hein, 2020b) which comprises an ensemble of 4 attacks including APGD-CE (Croce & Hein, 2020b), APGD-DLR (Croce & Hein, 2020b), FAB (Croce & Hein, 2020a) and Square attack (Andriushchenko et al., 2020) for rigorous evaluation. To eliminate the possibility of gradient obfuscations (Athalye et al., 2018), we provide sanity checks in Appendix E. Unless otherwise specified, the robust accuracy (RA) is computed under AutoAttack to demonstrate the model\u2019s generalization ability on unseen attacks. The computation cost analysis is attached in Appendix G."
        },
        {
            "heading": "5.2 SUPERIOR PERFORMANCE ACROSS ROBUSTIFIED METHODS AND DATASETS",
            "text": "Table 1 demonstrates the results of ADR combined with PGD-AT and TRADES on CIFAR-10, CIFAR-100, and TinyImageNet-200. Our initial observations reveal that robust overfitting exists in all baselines, with differences between final and best early-stopping robust accuracies as large as 5.62% on CIFAR-10 while the standard accuracy (SA) remains stable with more training epochs.\nWhen combined with our approach, we observe consistent improvements across datasets, with reduced robust overfitting gaps from 5.62% to 3.2% on CIFAR-10, 5.29% to 2.64% on CIFAR-100, and 2.2% to 0.63% on TinyImageNet-200. Furthermore, the robust accuracy improves by 0.92% to 1.92% across experiments. By alleviating robust overfitting, the best checkpoints are closer to the end of the training, thereby improving the SA in most settings. Our findings indicate that ADR can effectively enhance the RA-SA trade-off by improving robustness while achieving higher standard accuracy. We also present performance variation across multiple reruns in Appendix H."
        },
        {
            "heading": "5.3 COMBING WITH WEIGHT SPACE SMOOTHING TECHNIQUES AND LARGER ARCHITECTURE",
            "text": "The proposed ADR can be integrated with other AT techniques to boost robustness further (Table 2). Additional experiments with TRADES are presented in Appendix C. WA (Gowal et al., 2020) and AWP (Wu et al., 2020) are the model-weight-space smoothing techniques that improve the stability and performance of AT. In our case, we can acquire WA result by evaluating \u03b8t as it maintains the EMA of the trained model\u2019s weight. Combining ADR with AWP and WA, we obtain large gains in RA ranging from 1.12% to 3.55% and 0.37% to 3.23% in SA compared to the ResNet-18 baselines.\nFollowing prior works (Chen et al., 2021; Addepalli et al., 2022), we additionally use Wide ResNet (WRN-34-10) to demonstrate that ADR scales to larger architectures and improves RA and SA. The result shows that our method effectively enhances robust accuracy up to 3.14% on CIFAR-10, 3.15% on CIFAR-100, and 2.57% on TinyImageNet-200 compared to each of its baselines. Notably, we use the same \u03bb, \u03c4 as ResNet-18 for WRN-34-10, which might not be optimal, to reduce the cost of hyperparameter searching. Therefore, we observe a slight drop in standard accuracy in some WRN cases. Nevertheless, ADR still outperforms baselines in robustness without tuning hyper-parameters."
        },
        {
            "heading": "5.4 COMPARISON WITH RELATED WORKS AND USE ADDITIONAL DATA ON CIFAR-100",
            "text": "Table 3 compares our proposed ADR defense against related works on a more challenging CIFAR100 dataset. We select leading methods (Rade & Moosavi-Dezfooli, 2022; Gowal et al., 2020; Rebuffi et al., 2021a; Addepalli et al., 2022; Cui et al., 2021; Sehwag et al., 2022; Jia et al., 2022; Chen & Lee, 2021) on RobustBench and methods similar to ours which introduce smoothing in training labels (Dong et al., 2022b;a; Zhang et al., 2022) to make a fair comparison. Since knowledge distillation also promotes learning from the soft target, we discuss the benefits of ADR over those methods in Appendix I. The reported numbers are listed in their original papers or on RobustBench. We also provide a similar comparison on TinyImageNet-200 in Appendix D. Given the observed benefits of incorporating additional training data to promote robust generalization (Schmidt et al., 2018), we employ a DDPM (Ho et al., 2020) synthetic dataset (Gowal et al., 2020; Rebuffi et al., 2021a) composed of 1 million samples. Detailed experiment setup can be found in Appendix F.\nOur experimentation with AT-WA-AWP-ADR on ResNet-18 yields a robust accuracy of 28.5% and a standard accuracy of 57.36%, comparable to Rebuffi et al. (2021a) that utilizes an additional 1M DDPM data on Preact-ResNet18, which yields an RA of 28.5% and SA of 56.87%. Remarkably, our model attains equal robustness and superior standard accuracy without using additional data when employing a similar-sized model. Similarly, we achieve an RA of 31.6% on WRN-34-10, while Sehwag et al. (2022) scores only 31.15% with additional data. Additionally, adding DDPM data in the training set leads to further improvement in robust accuracy for ADR, by 1.09% and 0.59% for ResNet-18 and WRN-34-10, respectively. In both cases, ADR achieves new state-ofthe-art performance, both with and without additional data, on the CIFAR-100 benchmark. It is worth noting that some methods introduce extra auxiliary examples when training (Dong et al., 2022b; Rade & Moosavi-Dezfooli, 2022), and some bring complex augmentation into AT (Rade & Moosavi-Dezfooli, 2022; Rebuffi et al., 2021a), and so might obtain superior SA compared to ADR. However, regarding the optimal robustness to achieve, our empirical findings provide compelling evidence that rectifying training labels with a realistic distribution is a valuable approach.\n5.5 ACHIEVING FLATTER WEIGHT LOSS LANDSCAPE\nSeveral studies (Wu et al., 2020; Stutz et al., 2021) have found that a flatter weight loss landscape leads to a smaller robust generalization gap when the training process is sufficient. Many methods (Wu et al., 2020; Chen et al., 2021; Gowal et al., 2020; Zhang et al., 2019) addressing robust overfitting issues predominantly find flatter minima. We visualize the weight loss landscape by plotting the loss change when moving the weight w along a random direction d with magnitude \u03b1. The direction d is sampled from Gaussian distribution with filter normalization (Li et al., 2018). For each perturbed model, we generate adversarial examples on the fly with PGD-10 and calculate the mean loss across the testing set.\nFigure 4 compares the weight loss landscape between AT and ADR on CIFAR-10. ADR achieves a flatter landscape, implying better robust generalization ability. While we smooth the label for ADR in training, we use the one-hot label as ground truth to calculate the cross-entropy loss in this experiment, so the model trained by ADR has a higher loss value than AT on average. Additionally, we visualize the loss landscape around the data point in Appendix E and observe a similar phenomenon that ADR produces a flatter loss landscape.\n5.6 ACCURACY VS. ROBUSTNESS TRADE-OFF\nMitigating the trade-off between accuracy and robustness has been challenging in the realm of adversarial training. To investigate if ADR is capable of reducing such trade-off, we combine ADR with TRADES and adjust the trade-off parameter \u03b2 to demonstrate the performance difference in terms of different values of \u03b2. The result is represented in Figure 6. We decrease \u03b2 from left to right, where a higher value of \u03b2 gives better robustness. We can clearly observe that TRADES+ADR achieves a better trade-off compared to that of using the hard label alone."
        },
        {
            "heading": "5.7 ABLATION STUDY ON EFFECTIVENESS OF TEMPERATURE AND INTERPOLATION FACTOR",
            "text": "To disclose the impact of temperature \u03c4 and interpolation factor \u03bb on the proposed ADR, we conduct ablation studies to exhibit grid search outcomes of SA and RA in Figure 5. Throughout experiments, \u03c4 and \u03bb are held constant unless explicitly specified as \u201cAnnealing.\u201d to scrutinize the effects of varying parameter values. Furthermore, we include Label Smoothing (LS) in this study, which can be viewed as a special case where \u03c4 approaches infinity, to evaluate how data-driven smoothness improves performance. Our analysis reveals that in terms of clean accuracy, choosing a smaller \u03bb and suitable temperature \u03c4 = 3 can achieve the best performance. As for the robustness, using moderately large \u03bb with appropriate \u03c4 can ensure that the training labels inherent enough inter-class relationship. Therefore, selecting a proper value of parameters is vital to maintaining clean accuracy while enhancing robustness. Our experiment also reveals that annealing in both temperature and interpolation factors is beneficial to improve robustness, which shows the efficacy of gradually increasing reliance on the EMA model."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "In this paper, we characterize the key properties that distinguish robust and non-robust model output. We find that a robust model should exhibit good calibration and maintain output consistency on clean data and its adversarial counterpart. Based on this observation, we propose a data-driven label softening scheme ADR without the need for pre-trained resources or extensive computation overhead. To achieve this, we utilize the self-distillation EMA model to provide labeling guidance for the trained model, with increasing trust placed in the EMA as training progresses. Comprehensive experiments demonstrate that ADR effectively improves robustness, alleviates robust overfitting, and obtains a better trade-off in terms of accuracy and robustness. However, we note that the algorithm\u2019s optimal temperature and interpolation ratio depend on the dataset, and improper selection of these parameters can limit performance improvements. The automatic determination of optimal parameters in training will be an important future research direction that can further boost the robustness.\nETHICS STATEMENT\nAdversarial training has the potential to improve the security, and reliability of machine learning systems. In practical settings, adversarial attacks can be employed by malevolent actors in an attempt to deceive machine learning systems. This phenomenon can engender grave consequences for domains such as autonomous driving vehicles and facial recognition. To enhance the security and reliability of machine learning systems, adversarial training can be employed to produce more dependable models. However, it is worth noting that robust models can also be exploited by ill-intentioned users. In the context of the CAPTCHA, adversarial perturbations can be added to images to distinguish between humans and robots since the robots are expected to be fooled by the adversarial data. If robots can attain robust models, they would not be susceptible to adversarial examples and could supply accurate answers. The advancement of model robustness may inspire people to formulate a better strategy to differentiate between humans and robots.\nREPRODUCIBILITY\nWe describe the detailed experiment settings and hyperparameters in section 5.1 and Appendix F. Furthermore, the source code can be found in the supplementary materials to ensure the reproducibility of this project.\nACKKNOWLEDGEMENT\nThis work was supported in part by the National Science and Technology Council under Grants MOST 110-2634-F002-051, MOST 110-2222-E-002-014-MY3, NSTC 113-2923-E-002010-MY2, NSTC-112-2634-F-002-002-MBK, by National Taiwan University under Grant NTUCC-112L891006, and by Center of Data Intelligence: Technologies, Applications, and Systems under Grant NTU-112L900903."
        },
        {
            "heading": "A ADDITIONAL RELATED WORK: MITIGATE ROBUST OVERFITTING",
            "text": "The phenomenon of robust overfitting Rice et al. (2020) represents a significant challenge in AT, motivating researchers to explore various avenues for mitigation. One such approach is to use heuristic-driven augmentations Rebuffi et al. (2021b), such as CutMix Yun et al. (2019); Rebuffi et al. (2021a), DAJAT Addepalli et al. (2022), and CropShift Li & Spratling (2023), which employ sets of augmentations carefully designed to increase data diversity and alleviate robust overfitting. Another strategy involves the expansion of the training set, which offers a direct means to address overfitting. By incorporating additional unlabeled data Carmon et al. (2019) or high-quality generated images via deep diffusion probabilistic models (DDPM) Rebuffi et al. (2021a); Gowal et al. (2020); Sehwag et al. (2022), the introduction of an extra set of 500K pseudo-labeled images from 80M-TI Torralba et al. (2008) eliminates the occurrence of robust overfitting Rebuffi et al. (2021a). Despite the demonstrated effectiveness of extra data, increasing the size of the training set is computationally expensive, rendering AT infeasible for larger datasets.\nEarly stopping is a straightforward method for producing robust models Rice et al. (2020). However, due to the fact that the checkpoint of optimal robust accuracy and that of standard accuracy frequently do not align Chen et al. (2021), utilizing either of these measures can result in a compromise of overall performance. Weight Average (WA) Izmailov et al. (2018); Chen et al. (2021); Gowal et al. (2020) tracks the exponential moving average of model weights, thereby promoting flatter minima and increased robustness Hein & Andriushchenko (2017). Another effective regularization technique is Adversarial Weight Perturbation (AWP) Wu et al. (2020), which serves to promote flatness within the weight loss landscape and yields enhanced generalization capabilities Stutz et al. (2021)."
        },
        {
            "heading": "B ALGORITHM FOR ADR",
            "text": "Algorithm 1 Annealing Self-Distillation Rectification (ADR) Input: Training set D = {(xi, yi)}ni=1 Parameter: A classifier f(.) with learnable parameters \u03b8s; \u03b8t is exponential moving average of \u03b8s with decay rate \u03b3; Batch size m; Learning rate \u03b7; Total training iterations E; Attack radius \u03f5, attack step size \u03b1, number of attack iteration K; Temperature \u03c4 ; Interpolation ratio \u03bb.\n1: Randomly initialize the network parameters \u03b8s, \u03b8t \u2190 \u03b8s 2: for e = 1 to E do 3: Calculate \u03c4e according to the current iterations. 4: Calculate \u03bbe according to the current iterations. 5: Sample a mini-batch {(xj , yj)}mj=1 from D 6: for j = 1 to m (in parallel) do 7: Pt(xj)\u2190 softmax(f\u03b8t(xj)/\u03c4e) \u25b7 Calculate rectified label 8: \u03bbj = clip[0,1](\u03bbe \u2212 (Pt(xj)(\u03c8j) \u2212 Pt(xj)(yj))) 9: P (xj)\u2190 \u03bbj \u00b7 Pt(xj) + (1\u2212 \u03bbj) \u00b7 yj 10: x\u2032j \u2190 x\u2032j + \u03f5 \u00b7 \u03b4, where \u03b4 \u223c Uniform(\u22121, 1) \u25b7 Construct adversarial example 11: for k = 1 to K do 12: x\u2032j = \u03a0S(x\u2032j)(x \u2032 j + \u03b1 \u00b7 sign(\u2207x\u2113(f\u03b8s(x\u2032j), P (xj)))) 13: end for 14: end for 15: \u03b8s \u2190 \u03b8s \u2212 \u03b7m \u00b7 \u2211m j=1\u2207\u03b8s(\u2113(f\u03b8s(x\u2032j), P (xj))) \u25b7 Update model parameters 16: \u03b8t \u2190 \u03b3 \u00b7 \u03b8t + (1\u2212 \u03b3) \u00b7 \u03b8s 17: end for"
        },
        {
            "heading": "C TEST ACCURACY OF TRADES + ADR COMBING WITH WA AND AWP",
            "text": "In this study, we investigate the impact of combining TRADES and ADR with other adversarial training techniques, namely WA and AWP, on ResNet-18, as outlined in Table-4. Our experimental results demonstrate that leveraging a soft target generated by ADR yields exceptional robustness and\nstandard accuracy improvement, thereby achieving a superior trade-off. Specifically, ADR results in 1.18%, 2.92%, and 2.13% RA improvement and 0.45%, 2.21%, and 3.5% SA improvement on the baseline performance of CIFAR-10, CIFAR-100, and TinyImageNet-200, respectively.\nHowever, we observe that the robustness improvement saturates or even slightly deteriorates when combining TRADES+ADR with weight smoothing techniques on CIFAR-10. This is attributed to the fact that TRADES already promotes learning and attacking adversarial data on a softened target, making the additional soft objective by ADR less effective. Nevertheless, the TRADES+ADR approach remains beneficial when dealing with more challenging datasets that contain a greater number of target classes combined with WA and AWP."
        },
        {
            "heading": "D TEST ACCURACY (%) COMPARED WITH RELATED WORKS ON TINYIMAGENET-200.",
            "text": "We present ADR evaluated against related works Madry et al. (2018); Rade & Moosavi-Dezfooli (2022); Dong et al. (2022a); Rebuffi et al. (2021b) in Table-5 on the TinyImageNet-200 dataset, which is a more challenging robustness benchmark than CIFAR-10 or CIFAR-100 due to its larger class size and higher-resolution images, using the original numbers from their respective papers. A model trained on a larger class dataset often results in more complex decision boundaries, which increases the likelihood of an attacker identifying vulnerabilities in the model. Our experimental results demonstrate that ADR achieves state-of-the-art performance, improving RA by 1.94% to 2.17% when using ResNet-18, and achieving a remarkable 2.57% improvement over the baseline on WRN-34-10. In summary, we observe that ADR stands out in the challenging multi-class scenario."
        },
        {
            "heading": "E SANITY CHECK FOR GRADIENT OBFUSCATION",
            "text": "Athalye et al. (2018) argued that some defenses improving the robustness by obfuscated gradient, which is introduced intentionally through non-differentiable operations or unintentionally through\nnumerical instability, can be circumvented. Though we already use AutoAttack Croce & Hein (2020b) which has been shown to provide a reliable estimation of robustness as an adversary throughout the experiments, we conduct additional evaluations with ADR to eliminate the possibility of gradient obfuscation. Following the guidelines by Carlini et al. (2019), we examine the impact of changes on the robustness with ResNet-18 trained by AT+ADR on CIFAR-10 against l\u221e perturbation.\nUnbounded PGD attack The unbounded PGD adversary should reduces model robustness to 0%. Figure 7a shows the changes of PGD-100 robust accuracy of AT+ADR with different attack radius \u03f5. The robust accuracy for AT+ADR monotonically drops to 0% as the attack radius \u03f5 increases.\nIncreasing PGD attack steps Increasing the number of attack iterations should only marginally lower the robust accuracy. Figure 7b shows the changes in robust accuracy of AT+ADR with different steps of PGD attack. We can observe that the robust accuracy almost converges after K = 16, more steps of attack do not lead to lower robust accuracy.\nInspect loss landscape around inputs Figure 8 shows the loss landscape of Non-Robust, AT, and AT+ADR model around randomly selected examples from CIFAR-10. Compared to the NonRobust model, the adversarially trained models (both PGD-AT and AT+ADR) have flattened the rugged landscape, which does not exhibit the typical patterns of gradient obfuscation Engstrom et al. (2018). It is notable that despite both PGD-AT and ADR having smooth landscapes, ADR has a lower gradient magnitude (dark blue color in the figure), which implies the loss changes for AT+ADR is smaller than AT when small perturbations is added to the input."
        },
        {
            "heading": "F EXPERIMENT SETUP WHEN UTILIZING ADDITIONAL DATA",
            "text": "Following Gowal et al. (2020); Rebuffi et al. (2021a), we train Preact-ResNet18 (He et al., 2016b) and WRN-34-10 with SiLU (Hendrycks & Gimpel, 2016) as the activation function when utilizing synthetic data. We adopt a rigorous experimental design following (Gowal et al., 2020; Rebuffi et al., 2021a), training the Preact-ResNet18 (He et al., 2016b) and WRN-34-10 architectures with SiLU (Hendrycks & Gimpel, 2016) as the activation function when utilizing synthetic data. We leverage cyclic learning rates (Smith & Topin, 2017) with cosine annealing (Loshchilov & Hutter, 2017) by setting the maximum learning rate to 0.4 and warmup period of 10 epochs, ultimately training for a total of 400 CIFAR-100 equivalent epochs. Our training batch size is set to 1024, with 75% of the batch composed of the synthetic data. We maintain consistency with other experiment details outlined in section 5.1."
        },
        {
            "heading": "G COMPUTATION COST ANALYSIS",
            "text": "A standard 10 step AT includes 10 forwards and backward to find the worst-case perturbation when given a normal data point x. After we generate the adversarial data x\u2032, it requires an additional 1 forward and backward pass to optimize the model. We will need 11 forward and backward pass per iteration to conduct PGD-10 adversarial training. When introducing ADR to rectify the targets, an additional forward is needed for the EMA model. We need 12 forward and 11 backward pass in total.\nWe provide time per epoch for adversarial training in Table-6. The experiment is reported by running each algorithm on a single NVIDIA RTX A6000 GPU with batch size 128. From the table, we can infer that the computation cost introduced by ADR is relatively small (1.83% on ResNet-18 4.45% on WRN-34-10 on average) while the overhead brought by AWP is a lot higher (12.6% on ResNet18, 12.8% on WRN-34-10 on average). We can achieve similar robustness improvement to AWP with ADR with less computation cost required.\nH VARIANCE ACROSS RERUNS\nTable-7 presents the results of five repeated runs for AT and the proposed defenses AT+ADR on CIFAR-10 with ResNet-18. Our findings indicate that the proposed ADR approach consistently\noutperforms AT as evidenced by a higher mean (54.91% for AT+ADR compared to 52.6 for AT) and lower standard deviation in robust accuracy (0.128% for AT+ADR compared to 0.182% for AT). This suggests that ADR provides superior performance and greater stability in terms of robustness. While there is a larger standard deviation observed in standard accuracy when ADR is combined with AT, we consider the variance to be within an acceptable range (0.18%). Our results demonstrate that ADR yields stable performance that is independent of random seeds or other initialization states."
        },
        {
            "heading": "I DISCUSSION ABOUT DIFFERENCE WITH KNOWLEDGE DISTILLATION BASED METHODS",
            "text": "Knowledge distillation in adversarial training generally requires pre-trained resources as teacher models, on the other hand, we do not acquire anything in advance when using ADR. Therefore, it is a great advantage that ADR can achieve satisfactory results without additional resources. Furthermore, in the phase of training a teacher model for knowledge distillation, the hard label still encourages the teacher to exhibit nearly one-hot output and may lead to overconfident results. It is challenging to control the softness of the target when distillate to the student model because the teacher might output nearly one-hot distribution for some examples that have high confidence and rather smooth distribution for others. The bias and confidence from the teacher model might be incorrectly inherited by the student model. Instead of forcing the model to learn from one-hot ground truth, the noise-aware ADR label promotes the model to adapt label noise from the early training stage. We can control the smoothness of the learning target by manipulating the \u03bb and \u03c4 , as \u03bb controls the fraction of the ground truth class and \u03c4 decides the smoothness of the noise composed in the rectified label. We provide additional results in Table 8 to validate that ADR is superior to the knowledge distillation based methods."
        },
        {
            "heading": "J LIMITATIONS",
            "text": "In this work, we proposed ADR that employs a self-distillate EMA model to generate a finely calibrated soft label to enhance the robustness of models against adversarial attacks. However, we observe that the optimal parameters for each dataset vary. Thus, selecting appropriate parameters that suit the current training state is crucial to ensure optimal performance. It is also noteworthy that while ADR demonstrates its efficacy in improving the performance of TRADES, the extent of improvement saturates when combined with other adversarial training techniques (WA, AWP) on fewer class datasets e.g. CIFAR-10. This outcome could be attributed to the fact that TRADES already promotes attacking and learning the data with soft targets generated by the trained model itself, and these additional techniques further smooth the model weight space. Thus, when the target class is fewer, the improvement provided by ADR, which also emphasizes a smooth objective, becomes indistinguishable when all techniques are employed simultaneously."
        },
        {
            "heading": "K A CLOSER LOOK AT ADR MODELS\u2019 OUTPUT DISTRIBUTION",
            "text": "From Figure 9a, we can see that the ADR model trained on CIFAR-10 is more uncertain than the model trained with PGD-AT when encountering out-of-distribution data (CIFAR-100). It supports our claim that the robust model should not be over-confident when seeing something it has never seen before. From Figure 9b and Figure 9c, we can observe that the ADR-trained model exhibits similar output distribution as the model trained with PGD-AT (Figure 2b and Figure 2d), except that the ADR-trained model exhibits higher entropy levels in general. The reason behind this phenomenon is that ADR builds a sofer target as training labels, so the output of the ADR model will also be smoother, resulting in higher entropy levels on average."
        },
        {
            "heading": "L ADR TRAINING TRANSFERRING FROM PRE-TRAINED INITIALIZATION",
            "text": "In this experiment, we leverage pre-trained models for the initialization of the ADR process. Specifically, we employ ResNet-18 on the CIFAR-10 dataset, adhering to all other parameter settings outlined in Section 5.1. The key variation lies in the initialization of the model with either a standard or PGD-AT pre-trained model, followed by subsequent ADR training. Our reporting in Table 9 is based on the optimal checkpoint obtained during our comprehensive evaluation. Moreover, to offer a more comprehensive view of the observed trends, we present line charts depicting the performance trajectories with both clean and robust accuracy in Figure 10.\nThe experimental findings reveal a consistent trend wherein ADR consistently outperforms the baseline in terms of robust accuracy, regardless of whether it is initialized with a pre-trained model or not. The utilization of either a standard model or a PGD-AT pre-trained weight as an initialization fails to further augment the maximum robustness achievable through ADR. Nevertheless, it is noteworthy that employing an AT pre-trained checkpoint results in a notable enhancement of standard accuracy by 1.25% when compared to training with random initialization. This outcome underscores the potential for mitigating the accuracy-robustness tradeoff, indicating the feasibility of achieving improved performance by utilizing an AT pre-trained model during the initialization phase."
        }
    ],
    "title": "PROVES ADVERSARIAL TRAINING",
    "year": 2024
}