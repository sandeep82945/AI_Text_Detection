{
    "abstractText": "We consider the general class of time-homogeneous stochastic dynamical systems, both discrete and continuous, and study the problem of learning a representation of the state that faithfully captures its dynamics. This is instrumental to learn the transfer operator of the system, that in turn can be used for numerous tasks, such as forecasting and interpreting the system dynamics. We show that the search for a good representation can be cast as an optimization problem over neural networks. Our approach is supported by recent results in statistical learning theory, highlighting the role of approximation error and metric distortion in the context of transfer operator regression. The objective function we propose is associated with projection operators from the representation space to the data space, overcomes metric distortion, and can be empirically estimated from data. In the discrete time setting, we further derive a relaxed objective function that is differentiable and numerically well-conditioned. We compare our method against state-of-the-art approaches on different datasets, showing better performance across the board.",
    "authors": [],
    "id": "SP:7ea859708d591d895ef4735503a972ef7f8a3888",
    "references": [
        {
            "authors": [
                "R. Alexander",
                "D. Giannakis"
            ],
            "title": "Operator-theoretic framework for forecasting nonlinear time series with kernel analog techniques",
            "venue": "Physica D: Nonlinear Phenomena, 409:132520.",
            "year": 2020
        },
        {
            "authors": [
                "E. Allen"
            ],
            "title": "Modeling with It\u00f4 stochastic differential equations, volume 22",
            "venue": "Springer Science & Business Media.",
            "year": 2007
        },
        {
            "authors": [
                "A.L. Andrew",
                "R.C.E. Tan"
            ],
            "title": "Computation of derivatives of repeated eigenvalues and the corresponding eigenvectors of symmetric matrix pencils",
            "venue": "SIAM Journal on Matrix Analysis and Applications, 20(1):78\u2013100.",
            "year": 1998
        },
        {
            "authors": [
                "G. Andrew",
                "R. Arora",
                "J. Bilmes",
                "K. Livescu"
            ],
            "title": "Deep canonical correlation analysis",
            "venue": "International Conference on Machine Learning, pages 1247\u20131255. PMLR.",
            "year": 2013
        },
        {
            "authors": [
                "L. Arnold"
            ],
            "title": "Stochastic Differential Equations: Theory and Applications, volume 2",
            "venue": "John Wiley & Sons.",
            "year": 1974
        },
        {
            "authors": [
                "N. Aronszajn"
            ],
            "title": "Theory of reproducing kernels",
            "venue": "Transactions of the American Mathematical Society, 68(3):337\u2013404.",
            "year": 1950
        },
        {
            "authors": [
                "O. Azencot",
                "N.B. Erichson",
                "V. Lin",
                "M. Mahoney"
            ],
            "title": "Forecasting sequential data using consistent koopman autoencoders",
            "venue": "International Conference on Machine Learning, pages 475\u2013485. PMLR.",
            "year": 2020
        },
        {
            "authors": [
                "P. Bevanda",
                "M. Beier",
                "S. Kerz",
                "A. Lederer",
                "S. Sosnowski",
                "S. Hirche"
            ],
            "title": "KoopmanizingFlows: Diffeomorphically Learning Stable Koopman Operators",
            "venue": "arXiv preprint arXiv.2112.04085.",
            "year": 2021
        },
        {
            "authors": [
                "L. Bonati",
                "G. Piccini",
                "M. Parrinello"
            ],
            "title": "Deep learning the slow modes for rare events sampling",
            "venue": "Proceedings of the National Academy of Sciences, 118(44).",
            "year": 2021
        },
        {
            "authors": [
                "J. Bouvrie",
                "B. Hamzi"
            ],
            "title": "Kernel Methods for the Approximation of Nonlinear Systems",
            "venue": "SIAM Journal on Control and Optimization, 55(4):2460\u20132492.",
            "year": 2017
        },
        {
            "authors": [
                "S.L. Brunton",
                "M. Budi\u0161i\u0107",
                "E. Kaiser",
                "J.N. Kutz"
            ],
            "title": "Modern Koopman Theory for Dynamical Systems",
            "venue": "SIAM Review, 64(2):229\u2013340.",
            "year": 2022
        },
        {
            "authors": [
                "P. Cannarsa",
                "D. Mansutti",
                "A. Provenzale"
            ],
            "title": "Mathematical Approach to Climate Change and Its Impacts: MAC2I, volume 38",
            "venue": "Springer.",
            "year": 2020
        },
        {
            "authors": [
                "L. Chanussot",
                "A. Das",
                "S. Goyal",
                "T. Lavril",
                "M. Shuaibi",
                "M. Riviere",
                "K. Tran",
                "J. Heras-Domingo",
                "C. Ho",
                "W. Hu",
                "A. Palizhati",
                "A. Sriram",
                "B. Wood",
                "J. Yoon",
                "D. Parikh",
                "C.L. Zitnick",
                "Z. Ulissi"
            ],
            "title": "Open catalyst 2020 (OC20) dataset and community challenges",
            "venue": "ACS Catalysis, 11(10):6059\u20136072.",
            "year": 2021
        },
        {
            "authors": [
                "G. Cybenko"
            ],
            "title": "Approximation by superpositions of a sigmoidal function",
            "venue": "Mathematics of control, signals and systems, 2(4):303\u2013314.",
            "year": 1989
        },
        {
            "authors": [
                "S. Das",
                "D. Giannakis"
            ],
            "title": "Koopman spectra in reproducing kernel Hilbert spaces",
            "venue": "Applied and Computational Harmonic Analysis, 49(2):573\u2013607.",
            "year": 2020
        },
        {
            "authors": [
                "F. Fan",
                "B. Yi",
                "D. Rye",
                "G. Shi",
                "I.R. Manchester"
            ],
            "title": "Learning Stable Koopman Embeddings",
            "venue": "arXiv preprint arXiv.2110.06509.",
            "year": 2021
        },
        {
            "authors": [
                "M. Fisher",
                "J. Nocedal",
                "Y. Tr\u00e9molet",
                "S.J. Wright"
            ],
            "title": "Data assimilation in weather forecasting: a case study in pde-constrained optimization",
            "venue": "Optimization and Engineering, 10(3):409\u2013426.",
            "year": 2009
        },
        {
            "authors": [
                "K. Fukumizu",
                "F.R. Bach",
                "A. Gretton"
            ],
            "title": "Statistical consistency of kernel canonical correlation analysis",
            "venue": "Journal of Machine Learning Research, 8(14):361\u2013383.",
            "year": 2007
        },
        {
            "authors": [
                "M. Ghorbani",
                "S. Prasad",
                "J.B. Klauda",
                "B.R. Brooks"
            ],
            "title": "GraphVAMPNet, using graph neural networks and variational approach to markov processes for dynamical modeling of biomolecules",
            "venue": "The Journal of Chemical Physics, 156(18):184103.",
            "year": 2022
        },
        {
            "authors": [
                "G.H. Golub",
                "V. Pereyra"
            ],
            "title": "The differentiation of pseudo-inverses and nonlinear least squares problems whose variables separate",
            "venue": "SIAM Journal on Numerical Analysis,",
            "year": 1973
        },
        {
            "authors": [
                "G.H. Golub",
                "C.F. Van Loan"
            ],
            "title": "Matrix Computations",
            "venue": "JHU press.",
            "year": 2013
        },
        {
            "authors": [
                "I. Goodfellow",
                "Y. Bengio",
                "A. Courville"
            ],
            "title": "Deep Learning",
            "venue": "MIT press.",
            "year": 2016
        },
        {
            "authors": [
                "V. Gorini",
                "A. Kossakowski"
            ],
            "title": "Completely positive dynamical semigroups of N-level systems",
            "venue": "Journal of Mathematical Physics, 17(5):821.",
            "year": 1976
        },
        {
            "authors": [
                "A. Gretton",
                "O. Bousquet",
                "A. Smola",
                "B. Sch\u00f6lkopf"
            ],
            "title": "Measuring statistical dependence with hilbert-schmidt norms",
            "venue": "Algorithmic Learning Theory: 16th International Conference, ALT 2005, Singapore, October 8-11, 2005. Proceedings 16, pages 63\u201377. Springer.",
            "year": 2005
        },
        {
            "authors": [
                "D.R. Hardoon",
                "S. Szedmak",
                "J. Shawe-Taylor"
            ],
            "title": "Canonical correlation analysis: An overview with application to learning methods",
            "venue": "Neural Computation, 16(12):2639\u20132664.",
            "year": 2004
        },
        {
            "authors": [
                "H. Harold"
            ],
            "title": "Relations between two sets of variates",
            "venue": "Biometrika, 28(3/4):321.",
            "year": 1936
        },
        {
            "authors": [
                "B. Hou",
                "S. Sanjari",
                "N. Dahlin",
                "S. Bose",
                "U. Vaidya"
            ],
            "title": "Sparse learning of dynamical systems in rkhs: An operator-theoretic approach",
            "venue": "ICML2023.",
            "year": 2023
        },
        {
            "authors": [
                "T. Kato"
            ],
            "title": "Perturbation theory for linear operators; 2nd ed",
            "venue": "Grundlehren der mathematischen Wissenschaften : a series of comprehensive studies in mathematics. Springer, Berlin.",
            "year": 1976
        },
        {
            "authors": [
                "Y. Kawahara"
            ],
            "title": "Dynamic Mode Decomposition with Reproducing Kernels for Koopman Spectral Analysis",
            "venue": "Advances in Neural Information Processing Systems, volume 29.",
            "year": 2016
        },
        {
            "authors": [
                "S. Klus",
                "F. N\u00fcske",
                "B. Hamzi"
            ],
            "title": "Kernel-based approximation of the koopman generator and schr\u00f6dinger operator",
            "venue": "Entropy, 22(7):722.",
            "year": 2020
        },
        {
            "authors": [
                "S. Klus",
                "I. Schuster",
                "K. Muandet"
            ],
            "title": "Eigendecompositions of transfer operators in reproducing kernel Hilbert spaces",
            "venue": "Journal of Nonlinear Science, 30(1):283\u2013315.",
            "year": 2019
        },
        {
            "authors": [
                "M. Korda",
                "I. Mezi\u0107"
            ],
            "title": "On Convergence of Extended Dynamic Mode Decomposition to the Koopman operator",
            "venue": "Journal of Nonlinear Science, 28(2):687\u2013710.",
            "year": 2017
        },
        {
            "authors": [
                "V. Kostic",
                "K. Lounici",
                "P. Novelli",
                "M. Pontil"
            ],
            "title": "Koopman operator learning: Sharp spectral rates and spurious eigenvalues",
            "year": 2023
        },
        {
            "authors": [
                "V. Kostic",
                "P. Novelli",
                "A. Maurer",
                "C. Ciliberto",
                "L. Rosasco",
                "M. Pontil"
            ],
            "title": "Learning dynamical systems via Koopman operator regression in reproducing kernel hilbert spaces",
            "venue": "Advances in Neural Information Processing Systems.",
            "year": 2022
        },
        {
            "authors": [
                "H. Kramers"
            ],
            "title": "Brownian motion in a field of force and the diffusion model of chemical reactions",
            "venue": "Physica, 7(4):284\u2013304.",
            "year": 1940
        },
        {
            "authors": [
                "J.N. Kutz",
                "S.L. Brunton",
                "B.W. Brunton",
                "J.L. Proctor"
            ],
            "title": "Dynamic Mode Decomposition",
            "venue": "Society for Industrial and Applied Mathematics.",
            "year": 2016
        },
        {
            "authors": [
                "A. Lasota",
                "M.C. Mackey"
            ],
            "title": "Chaos, Fractals, and Noise, volume 97 of Applied Mathematical Sciences",
            "venue": "Springer New York.",
            "year": 1994
        },
        {
            "authors": [
                "Q. Li",
                "F. Dietrich",
                "E.M. Bollt",
                "I.G. Kevrekidis"
            ],
            "title": "Extended dynamic mode decomposition with dictionary learning: A data-driven adaptive spectral decomposition of the koopman operator",
            "venue": "Chaos: An Interdisciplinary Journal of Nonlinear Science, 27(10):103111.",
            "year": 2017
        },
        {
            "authors": [
                "Z. Li",
                "D. Meunier",
                "M. Mollenhauer",
                "A. Gretton"
            ],
            "title": "Optimal rates for regularized conditional mean embedding learning",
            "venue": "Advances in Neural Information Processing Systems.",
            "year": 2022
        },
        {
            "authors": [
                "G. Lindblad"
            ],
            "title": "On the generators of quantum dynamical semigroups",
            "venue": "Communications in Mathematical Physics, 48(2):119\u2013130.",
            "year": 1976
        },
        {
            "authors": [
                "K. Lindorff-Larsen",
                "S. Piana",
                "R.O. Dror",
                "D.E. Shaw"
            ],
            "title": "How fast-folding proteins fold",
            "venue": "Science, 334(6055):517\u2013520.",
            "year": 2011
        },
        {
            "authors": [
                "B. Lusch",
                "J.N. Kutz",
                "S.L. Brunton"
            ],
            "title": "Deep learning for universal linear embeddings of nonlinear dynamics",
            "venue": "Nature Communications,",
            "year": 2018
        },
        {
            "authors": [
                "A. Mardt",
                "L. Pasquali",
                "F. No\u2019e",
                "H. Wu"
            ],
            "title": "Deep learning markov and koopman models with physical constraints",
            "venue": "In Mathematical and Scientific Machine Learning",
            "year": 2019
        },
        {
            "authors": [
                "A. Mardt",
                "L. Pasquali",
                "H. Wu",
                "F. No\u00e9"
            ],
            "title": "Vampnets for deep learning of molecular kinetics",
            "venue": "Nature Communications, 9(1):5.",
            "year": 2018
        },
        {
            "authors": [
                "J. McCarty",
                "M. Parrinello"
            ],
            "title": "A variational conformational dynamics approach to the selection of collective variables in metadynamics",
            "venue": "The Journal of Chemical Physics, 147(20):204109.",
            "year": 2017
        },
        {
            "authors": [
                "G. Meanti",
                "A. Chatalic",
                "V.R. Kostic",
                "P. Novelli",
                "M. Pontil",
                "L. Rosasco"
            ],
            "title": "Estimating koopman operators with sketching to provably learn large scale dynamical systems",
            "year": 2023
        },
        {
            "authors": [
                "S. Minsker"
            ],
            "title": "On some extensions of Bernstein\u2019s inequality for self-adjoint operators",
            "venue": "Statistics & Probability Letters, 127:111\u2013119.",
            "year": 2017
        },
        {
            "authors": [
                "J. Morton",
                "A. Jameson",
                "M.J. Kochenderfer",
                "F. Witherden"
            ],
            "title": "Deep dynamical modeling and control of unsteady fluid flows",
            "venue": "Advances in Neural Information Processing Systems, 31.",
            "year": 2018
        },
        {
            "authors": [
                "P. Novelli",
                "L. Bonati",
                "M. Pontil",
                "M. Parrinello"
            ],
            "title": "Characterizing metastable states with the help of machine learning",
            "venue": "Journal of Chemical Theory and Computation, 18(9):5195\u20135202.",
            "year": 2022
        },
        {
            "authors": [
                "F. N\u00fcske",
                "S. Peitz",
                "F. Philipp",
                "M. Schaller",
                "K. Worthmann"
            ],
            "title": "Finite-data error bounds for koopman-based prediction and control",
            "venue": "Journal of Nonlinear Science, 33(1):14.",
            "year": 2023
        },
        {
            "authors": [
                "A. Ostruszka",
                "P. Pako\u0144ski",
                "W. S\u0142omczy\u0144ski",
                "K. \u017byczkowski"
            ],
            "title": "Dynamical entropy for systems with stochastic perturbation",
            "venue": "Physical Review E, 62(2):2018\u20132029.",
            "year": 2000
        },
        {
            "authors": [
                "S.E. Otto",
                "C.W. Rowley"
            ],
            "title": "Linearly recurrent autoencoder networks for learning dynamics",
            "venue": "SIAM Journal on Applied Dynamical Systems, 18(1):558\u2013593.",
            "year": 2019
        },
        {
            "authors": [
                "A. Pascucci"
            ],
            "title": "PDE and Martingale Methods in Option Pricing",
            "venue": "Springer Milan.",
            "year": 2011
        },
        {
            "authors": [
                "J.L. Proctor",
                "S.L. Brunton",
                "J.N. Kutz"
            ],
            "title": "Dynamic mode decomposition with control",
            "venue": "SIAM Journal on Applied Dynamical Systems, 15(1):142\u2013161.",
            "year": 2016
        },
        {
            "authors": [
                "M. Raissi",
                "A. Yazdani",
                "G.E. Karniadakis"
            ],
            "title": "Hidden fluid mechanics: Learning velocity and pressure fields from flow visualizations",
            "venue": "Science, 367(6481):1026\u20131030.",
            "year": 2020
        },
        {
            "authors": [
                "S.M. Ross"
            ],
            "title": "Stochastic Processes",
            "venue": "John Wiley & Sons.",
            "year": 1995
        },
        {
            "authors": [
                "P.J. Schmid"
            ],
            "title": "Dynamic mode decomposition of numerical and experimental data",
            "venue": "Journal of Fluid Mechanics, 656:5\u201328.",
            "year": 2010
        },
        {
            "authors": [
                "K.T. Sch\u00fctt",
                "S.S.P. Hessmann",
                "N.W.A. Gebauer",
                "J. Lederer",
                "M. Gastegger"
            ],
            "title": "SchNetPack 2.0: A neural network toolbox for atomistic machine learning",
            "venue": "The Journal of Chemical Physics,",
            "year": 2023
        },
        {
            "authors": [
                "K.T. Sch\u00fctt",
                "P. Kessel",
                "M. Gastegger",
                "K.A. Nicoli",
                "A. Tkatchenko",
                "M\u00fcller",
                "K.-R."
            ],
            "title": "SchNetPack: A Deep Learning Toolbox For Atomistic Systems",
            "venue": "Journal of Chemical Theory and Computation, 15(1):448\u2013455.",
            "year": 2019
        },
        {
            "authors": [
                "C. Sch\u00fctte",
                "W. Huisinga",
                "P. Deuflhard"
            ],
            "title": "Transfer Operator Approach to Conformational Dynamics in Biomolecular Systems",
            "venue": "Ergodic Theory, Analysis, and Efficient Simulation of Dynamical Systems, pages 191\u2013223. Springer Berlin Heidelberg.",
            "year": 2001
        },
        {
            "authors": [
                "C.R. Schwantes",
                "V.S. Pande"
            ],
            "title": "Modeling Molecular Kinetics with tICA and the Kernel Trick",
            "venue": "Journal of Chemical Theory and Computation, 11(2):600\u2013608.",
            "year": 2015
        },
        {
            "authors": [
                "I. Steinwart",
                "A. Christmann"
            ],
            "title": "Support Vector Machines",
            "venue": "Springer New York.",
            "year": 2008
        },
        {
            "authors": [
                "W. Tian",
                "H. Wu"
            ],
            "title": "Kernel embedding based variational approach for low-dimensional approximation of dynamical systems",
            "venue": "Computational Methods in Applied Mathematics, 21(3):635\u2013 659.",
            "year": 2021
        },
        {
            "authors": [
                "L.N. Trefethen",
                "M. Embree"
            ],
            "title": "Spectra and Pseudospectra: The Behavior of Nonnormal Matrices and Operators",
            "year": 2020
        },
        {
            "authors": [
                "J.A. Tropp"
            ],
            "title": "User-friendly tail bounds for sums of random matrices",
            "venue": "Technical report.",
            "year": 2012
        },
        {
            "authors": [
                "M. Tuckerman"
            ],
            "title": "Statistical Mechanics: Theory and Molecular Simulation",
            "venue": "Oxford university press.",
            "year": 2010
        },
        {
            "authors": [
                "E. Ullah",
                "R. Arora"
            ],
            "title": "Generalization bounds for kernel canonical correlation analysis",
            "venue": "Transactions on Machine Learning Research.",
            "year": 2023
        },
        {
            "authors": [
                "M.O. Williams",
                "I.G. Kevrekidis",
                "C.W. Rowley"
            ],
            "title": "A data\u2013driven approximation of the koopman operator: Extending dynamic mode decomposition",
            "venue": "Journal of Nonlinear Science, 25(6):1307\u20131346.",
            "year": 2015
        },
        {
            "authors": [
                "M.O. Williams",
                "C.W. Rowley",
                "I.G. Kevrekidis"
            ],
            "title": "A kernel-based method for data-driven Koopman spectral analysis",
            "venue": "Journal of Computational Dynamics, 2(2):247\u2013265.",
            "year": 2015
        },
        {
            "authors": [
                "H. Wu",
                "F. No\u00e9"
            ],
            "title": "Variational Approach for Learning Markov Processes from Time Series Data",
            "venue": "Journal of Nonlinear Science, 30(1):23\u201366.",
            "year": 2019
        },
        {
            "authors": [
                "E. Yeung",
                "S. Kundu",
                "N. Hodas"
            ],
            "title": "Learning deep neural network representations for koopman operators of nonlinear dynamical systems",
            "venue": "2019 American Control Conference (ACC). IEEE.",
            "year": 2019
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Dynamical systems are a mathematical framework describing the evolution of state variables over time. These models, often represented by nonlinear differential equations (ordinary or partial) and possibly stochastic, have broad applications in science and engineering, ranging from climate sciences (Cannarsa et al., 2020; Fisher et al., 2009), to finance (Pascucci, 2011), to atomistic simulations (Mardt et al., 2018; McCarty and Parrinello, 2017; Sch\u00fctte et al., 2001), and to open quantum system dynamics (Gorini and Kossakowski, 1976; Lindblad, 1976), among others. However, in general no analytical models of the dynamics are available and one must resort to data-driven techniques to characterize a dynamical system. Two powerful paradigms have emerged: deep neural networks (DNN) and kernel methods. The latter are backed up by solid statistical guarantees (Steinwart and Christmann, 2008) determining when linearly parameterized models can be learned efficiently. Yet, selecting an appropriate kernel function may be a hard task, requiring a significant amount of experimentation and expertise. In comparison, the former are very effective in learning complex data representations (Goodfellow et al., 2016), and benefit from a solid ecosystem of software tools, making the learning process feasible on large scale systems. However, their statistical analysis is still in its infancy, with only few proven results on their generalization properties. Kernel-based methods hinge on the powerful idea of characterizing dynamical systems by lifting their definition over a Hilbert space of functions and then studying the associated transfer operators. They describe the average evolution of functions of the state (observables) over time and for deterministic systems are also known as Koopman operators. Furthermore, transfer operators are linear, and under additional assumptions admit a spectral decomposition, which provides a valuable tool to interpret and analyse the behaviour of non-linear systems (see e.g. Brunton et al., 2022; Kutz et al., 2016, and references therein). The usefulness of this approach critically relies on an appropriate choice of the observable space. In particular, in order to fully benefit from the spectral decomposition, it is crucial to find an observable space F which linearizes the dynamics and is invariant under the action of the transfer operator. In kernel-based methods such a space is implicitly linked to the kernel function, and gives a clear mathematical meaning to the problem of choosing a \u201cgood\u201d kernel. Unfortunately, the analytical form of transfer operators is often intractable or unavailable, especially in complex or poorly understood systems, posing challenges in constructing invariant representations.\nIn this paper we build synergistically upon both kernel and DNN paradigms: we first employ DNNs to learn an invariant representation which fully captures the system dynamics, and then forward this representation to kernel-based algorithms for the actual transfer operator regression task. This general framework is illustrated in Fig. 1. Our method, named deep projection networks (DPNets), addressing the challenge of providing good representations to the operator regression algorithms, can be cast as an optimization problem over neural networks and benefits from a differentiable and numerically well-conditioned score functional, enhancing the stability of the training process. Previous work Extensive research has been conducted on learning dynamical systems from data. The monographs (Brunton et al., 2022; Kutz et al., 2016) are standard references in this field. To learn transfer operators we mention the works (Alexander and Giannakis, 2020; Bouvrie and Hamzi, 2017; Das and Giannakis, 2020; Kawahara, 2016; Klus et al., 2019; Kostic et al., 2022; Williams et al., 2015b) presenting kernel-based algorithms, and (Bevanda et al., 2021; Fan et al., 2021; Lusch et al., 2018) based on deep learning schemes. Finding meaningful representations of the state of the system to be used in conjunction with Koopman operator learning is a critical challenge, tackled by many authors. We mention the works (Azencot et al., 2020; Lusch et al., 2018; Morton et al., 2018; Otto and Rowley, 2019) where a representation is learned via encoding-decoding schemes, as well as (Kawahara, 2016; Li et al., 2017; Mardt et al., 2019; 2018; Tian and Wu, 2021; Yeung et al., 2019; Wu and No\u00e9, 2019) addressing the problem of learning invariant subspaces of the transfer operator. Mostly related to our methodology is (Andrew et al., 2013), which introduced deep canonical correlation analysis and, especially, VAMPnets (Mardt et al., 2018; Wu and No\u00e9, 2019), which repurposed this approach to learn transfer operators. Within our theoretical framework, indeed, we will show that VAMPNets can be recovered as a special case in the setting of discrete dynamics. Contributions Our contributions are: 1) Leveraging recent results in statistical learning theory, we formalize the problem of learning an invariant representation for both discrete and continuous dynamical systems, using as our main tools the orthogonal projections of the transfer operator in data-spaces (Sec. 2); 2) We show how to reliably solve the corresponding optimization problems using neural networks. In the discrete case we prove that our score leads to optimal representation learning under the mild assumption of compactness of the transfer operator, relying on universality of DNNs (Thm. 1). In the continuous case, our method applies to time reversal invariant dynamics (Thm. 2) including the important case of Langevin dynamics; 3) We discuss how the learned representation can be used within the framework of operator regression in a variety of settings. Numerical experiments illustrate the versatility and competitive performance of our approach against several baselines. Notation We let N the set of natural numbers and let N0 = {0} [ N. For m2N we denote [m] := {1, . . . ,m}. If T is a compact linear operator on two Hilbert spaces we let i(T ) be its i-th largest singular value, we let (T )\u2020 be the Moore-Penrose pseudo-inverse, and T \u21e4 the adjoint operator. We denote by k \u00b7 k and k \u00b7 kHS the operator and Hilbert-Schmidt norm, respectively."
        },
        {
            "heading": "2 REPRESENTATION LEARNING FOR DYNAMICAL SYSTEMS",
            "text": "In this section, we give some background on transfer operators for dynamical systems, we discuss key challenges in learning them from data, and we propose our representation learning approach. Background Let (Xt)t2T \u2713 X be a stochastic process in some state space X , where the time index t can be either discrete (T = N0) or continuous (T = [0,+1)), and denote by \u00b5t the law of Xt. Let F be a prescribed space of real-valued functions on X (space of observables). For every pair of time steps t, t0 2 T, with t0 t, the forward transfer operator Tt,t0 : F ! F evolves an observable f : X ! R from time t 2 T to time t0, by the conditional expectation\n[Tt,t0(f)](x) := E[f(Xt0) |Xt = x], x 2 X . (1) For a large class of stochastic dynamical systems, these linear operators are time-homogeneous, that is they only depend on the time difference t0 t. In this case, without loss of generality, we can define\nTs,s+t = T0,t =: Tt for all s 0, and use the shorthand notation T := T1. Transfer operators at different times are related through the Chapman-Kolmogorov equation (Allen, 2007), Tt+t0 = Tt Tt0 , implying that the family (Tt)t2T forms a semigroup.\nThe utility of transfer operators is that, when the space F is suitably chosen, they linearize the process. Two key requirements are that F is invariant under the action of Tt, that is Tt[F ] \u2713 F for all t (a property that we tacitly assumed above), and rich enough to represent the flow of the process. Specifically, for deterministic systems, given a starting state, T should allow to generate future states, while for stochastic ones, it should allow to generate the laws of future states. Two common choices for F that fulfill both requirements are the space of bounded continuous functions and the space of square integrable functions L2\u21e1(X ) with respect to the invariant (stationary) distribution \u21e1, when one exists. To ease our presentation, we first focus on latter case, and then extend our results to non-stationary time-homogeneous processes. Formally, the process (Xt)t2T admits an invariant distribution \u21e1 when \u00b5t = \u21e1 implies that \u00b5t0 = \u21e1 for every t0 t (see Ross, 1995). This in turn allows one to define the transfer operator on F = L2\u21e1(X ).\nWhile in the discrete time setting the whole process can be studied only through T = T1, when time is continuous the process is characterized by the infinitesimal generator L : W 1,2\u21e1 (X )!W 1,2\u21e1 (X ) of the semigroup (Tt)t 0, given by\nL := lim t!0+\nTt I\nt (2)\nwhich is properly defined on the Sobolev space W 1,2\u21e1 (X ) formed by functions in L2\u21e1(X ) having square integrable gradients (see Lasota and Mackey, 1994; Ross, 1995).\nLearning transfer operators In practice, dynamical systems are only observed, and neither T nor its domain F =L2\u21e1(X ) are known, providing a key challenge to learn them from data. The most popular algorithms (Brunton et al., 2022; Kutz et al., 2016) aim to learn the action of T : F!F on a predefined reproducing kernel Hilbert space (RKHS) H, forming a subset of functions in F . This allows one, via the kernel trick, to formulate the problem of learning T|H : H! F , the restriction of T to H, via empirical risk minimization, leading to an estimator bT : H ! H (Kostic et al., 2022). However, recent theoretical advances (Korda and Mezic\u0301, 2017; Klus et al., 2019; N\u00fcske et al., 2023), proved that such algorithms are statistically consistent only to PHT|H , where PH is the orthogonal projection into the closure of H in F . The projection PH constrains the evolved observables back inside H, thereby, in general, altering the dynamics of the system. Therefore, to assure that one properly learns dynamics, two major requirements on H are needed: i) T|H needs to approximate well T , i.e. the space H needs to be big enough relative to the domain of T ; ii) the difference between the projected restriction and the true one, i.e. the approximation error\n[I PH]T|H , needs to be small.\nWhen H is an infinite-dimensional universal RKHS, both of the above requirements are satisfied (Kostic et al., 2022), i.e. H is dense in F and the approximation error is zero, leading to an arbitrarily good approximation of dynamics with enough data. Still, another key issue arises: the norms on the a-priori chosen H and the unknown F do not coincide, since the latter depends on the process itself. This metric distortion phenomenon has been recently identified as the source of spurious estimation of the spectra of T (Kostic et al., 2023), limiting the utility of the learned transfer operators. Indeed, even if T is self-adjoint, that is the eigenfunctions are orthogonal in F , the estimated ones will not be orthogonal in H, giving rise to spectral pollution (Kato, 1976). This motivates one to additionally require that iii) H is a subspace of F , i.e. both spaces have the same norm.\nTo summarize, the desired optimal H is the leading invariant subspace of T , that is the subspace corresponding to the largest (in magnitude) eigenvalues of T . This subspace H achieves zero approximation error, eliminates metric distortion and best approximates (in the dynamical system sense) the operator T . Since any RKHS H is entirely described (Steinwart and Christmann, 2008) by a feature map, we argue that learning a leading invariant subspace H from data is, fundamentally, a representation learning problem.\nApproach We start by formalizing the problem of learning a good finite dimensional representation space for T , and then address the same for the generator L. We keep the discussion less formal here, and state our main results more precisely in the next section. Our approach is inspired by the following upper and lower bounds on the approximation error, a direct consequence of the norm change from H to F ,\n[I PH]T PH 2 + min (CH)  [I PH]T|H 2  [I PH]T PH 2 max(CH), (3)\nwhere CH is the covariance operator on H w.r.t. measure \u21e1, while +min and max stand for the smallest positive and the largest eigenvalue, respectively. Note that the norms on the hypothetical domain H and true domain L2\u21e1(X ) coincide if and only if CH = I , in which case equalities hold in (3) and the approximation error becomes [I PH]T PH .\nWhen the operator T is known, the latter quantity can be directly minimized by standard numerical algorithms for spectral computation to find invariant subspaces (see e.g. Golub and Van Loan, 2013). Unfortunately, in our stochastic setting T is unknown since we cannot compute the conditional expectation in (1). To overcome this issue we propose a learning approach to recover the invariant space H, which is rooted in the singular value decomposition, holding under the mild assumption that T is a compact operator1. The main idea is that the subspace made of the leading r left singular functions of T serves as a good approximation of the desired leading invariant subspace of T . Namely, due to the orthonormality of the singular functions, we have that CH = I and PHT becomes the r-truncated SVD of T , that is, its best rank-r approximation. Therefore, according to (3), the approximation error is at most r+1(T ), which can be made arbitrarily small by rising r. Moreover, the distance of the subspace of left singular functions to the desired leading invariant subspace is determined by the \"normality\" of T (Trefethen and Embree, 2020). If the operator T is normal, that is T T \u21e4 = T \u21e4T , then both its left and right singular spaces are invariant subspaces of T , resulting in zero approximation error irrespectively of r. This leads us to the following optimization problem\nmax H,H0\u21e2L2\u21e1(X )\nn PHT PH0 2 HS | CH = CH0 = I, dim(H)  r, dim(H 0)  r o . (4)\nRelying on the application of Eckart-Young-Mirsky\u2019s Theorem (Kato, 1976), we can show that the desired representation space H can be computed by solving (4). Note that, in general, the auxiliary space H0 is needed to capture right singular functions, while if we have prior knowledge that T is normal without loss of generality one can set H = H0 in (4).\nThe same reasoning cannot be applied straight away to the generator L of a continuous dynamical system, which in general is not even bounded (Lasota and Mackey, 1994), let alone compact. For time-reversal invariant processes, however, T and L are self-adjoint, that is T = T \u21e4 and L = L\u21e4. This includes the important case of Langevin dynamics, which are of paramount importance for molecular dynamics (see, e.g., Tuckerman, 2010). For such systems we show (see Theorem 2) that the leading r-dimensional subspace of the generator L can be found by solving the following optimization problem featuring the partial trace objective\nmax H\u21e2W 1,2\u21e1 (X ) {tr (PH LPH) | CH = I, dim(H)  r} , (5)\nwhere now the projector is PH : W 1,2\u21e1 (X ) ! W 1,2\u21e1 (X ). We stress that, since we assume L = L\u21e4, we can relax the above assumption on the compactness of the transfer operators (Tt)t 0, and still show that the leading invariant subspace of L is the optimal H in (5). So, on such H we can estimate well L via generator operator regression, see e.g. (Hou et al., 2023; Klus et al., 2020)."
        },
        {
            "heading": "3 DEEP PROJECTION SCORE FUNCTIONALS",
            "text": "In this section we show how to solve the representation learning problems (4) and (5) using DNNs.\nIn the discrete case, we consider a generalized version of problem (4), which encompasses nonstationary processes, for which the probability distributions change along the trajectory. Namely, let X and X 0 be two X -valued random variables with probability measures \u00b5 and \u00b50, respectively, where X models the state at time t and X 0 its evolution after time t0. Then the transfer operator can be defined on the data-dependent domains Tt0 : L2\u00b50(X )! L 2\n\u00b5(X ). So, replacing in (4) PH0 by the projector P 0\nH0 : L2\u00b50(X )! L 2 \u00b50(X ) and taking the covariances CH and CH0 w.r.t. measures \u00b5 and \u00b5 0,\nrespectively, we obtain the appropriate representation learning problem for non-stationary process; see App. B for more details. Within this general setting we choose two families of feature maps\nw(x) := ( w,1(x), . . . , w,r(x)) 2 Rr, and 0w(x) := ( 0w,1(x), . . . , 0w,r(x)) 2 Rr, (6) 1This property is fulfilled by a large class of Markov processes (see e.g. Kostic et al., 2022) and is much\nweaker than requiring the operator be Hilbert-Schmidt as in (Mardt et al., 2018).\nparameterized by a parameter w taking values in some set W . Next, using that the corresponding RKHSs are Hw := span( w,j)j2[r] and H0w := span( 0w,j)j2[r] (equipped with the standard inner product), we aim to solve (4) by optimizing over the weights w. To avoid solving the constrained optimization problem, we further propose to relax the hard constraints in (4) through a metric distortion loss R : Rr\u21e5r ! R+ that is zero if and only if it is applied to the identity matrix. Then, using the (uncentered) covariance and cross-covariance matrices in the feature spaces\nCwX := E w(X) w(X)>, CwX0 := E 0w(X 0) 0w(X 0)> and C wXX0 := E w(X) 0w(X 0)>, (7) we show in Lem. 3 in App. C that, with > 0, we can express the objective as\nP (w) = (CwX)\u2020/2C wXX0(CwX0)\u2020/2 2 HS (R(CwX) +R(C w X0)) . (8)\nNotice that if = 0 and the covariances CwX and C w X0 are nonsingular, then the score reduces to (CwX) 1 2C wXX0(C w X0) 1 2 2 HS\nwhich is called VAMP-2 score in the VAMPNets approach of (Mardt et al., 2018). Moreover, if in (8) the HS norm is replaced by nuclear norm, the score becomes the objective of the canonical correlation analysis (CCA) (Harold, 1936; Hardoon et al., 2004) in feature space. When DNNs are used to parametrize the feature maps (6), such score is the objective of Deep-CCA (Andrew et al., 2013), also employed in (Mardt et al., 2018) as VAMP-1 score in the context of molecular kinetics. Therefore, by maximizing P we look for the strongest linear correlation between Hw and H0w. We stress that differently from Deep-CCA and VAMPNets, in order to learn a proper representation space, the role of R is to decorrelate the features within each space guiding the maximization towards the optimum in which the norms of H and H0 coincide with those of L2\u00b5(X ) and L2\u00b50(X ), respectively, thereby overcoming metric distortion.\nWhile in the optimal representation the covariances CwX and C w X0 are the identity, in general they are non-invertible, making the score P non-differentiable during optimization. Indeed, unless the rank of both covariances is stable for every w 2W , we might have exploding gradients (Golub and Pereyra, 1973). Even ignoring differentiability issues, the use of the pseudo-inverse, as well as the use of the inverse in the non-singular case, can introduce severe numerical instabilities when estimating P\nand its gradients during the training process. More precisely, the numerical conditioning of evaluating P using (8) is determined by the covariance condition numbers 1(CwX)/ r(C w X) and\n1(CwX0)/ r(C w X0) that can be very large in practical situations (see e.g. the fluid dynamics example\nin Sec. 5) To overcome this issue, we introduce the relaxed score\nS (w) := C wXX0 2 HS / ( CwX CwX0 ) (R(CwX) +R(CwX0)) , (9)\nwhich, as implied by Theorem 1 below, is a lower bound for the score in (8). A key advantage of this approach is that the score S is both differentiable (apart from the trivial case CwX , C w X0 = 0) and has stable gradients, since we avoid matrix inversion. Indeed, computing S is always well-conditioned, as the numerical operator norm has conditioning equals to one.\nThe following theorem provides the theoretical backbone of our representation learning approach. Theorem 1. If T : L2\u00b50(X )!L2\u00b5(X ) is compact, Hw \u2713L2\u00b5(X ) and H0w \u2713L2\u00b50(X ), then for all 0\nS (w)  P (w)  2 1 (T ) + \u00b7 \u00b7 \u00b7+ 2r(T ). (10)\nMoreover, if ( w,j)j2[r] and ( 0w,j)j2[r] are the leading r left and right singular functions of T , respectively, then both equalities in (10) hold. Finally, if the operator T is Hilbert-Schmidt, r(T ) > r+1(T ) and > 0, then the \u201conly if\u201d relation is satisfied up to unitary equivalence.\nThis result establishes that the relaxed score is a tight lower bound, in the optimization sense, for (8). More precisely, in general |S (w) P (w)| is arbitrarily small for small enough R(w), while on the feasible set in (4) both scores coincide. In particular, both scores are maximized at the same (Hw,H0w) when there exists w 2W such that the leading r left (resp. right) singular functions of T belong to Hw (resp. H0w). For instance, for a deterministic linear system this happens as soon as the classes of features w and 0w include linear functions. In general, we can appeal to the universal approximation properties of DNN (Cybenko, 1989) to fulfill the hypotheses of Theorem 1.\nGenerator learning We now consider how to learn the representation of continuous timehomogeneous processes arising form stochastic differential equation (SDE)\ndXt = A(Xt) dt+B(Xt) dWt, (11)\nAlgorithm 1 DPNets Training Input: Data Dn; metric loss parameter ; DNNs w, 0w : X ! Rr; optimizer U ; # of steps k.\n1: Initialize DNN weights to w1 2: for j = 1 to k do; 3: C\u0302wjX , C\u0302 wj X0 , C\u0302 wj XX0 covariances for wj and 0\nwj from a mini-batch of m  n samples 4: F (wj) bS m(wj) estimator of Eq. (9) and the covariances from Step 3 5: wj+1 U(wj ,rF (wj)) where rF (wj) is computed using backpropagation 6: end for 7: return representations wK , 0wK\nwhere function A is the drift, function B is the diffusion, and Wt is a Wiener process, whose flow is a stochastic process that is time-reversal invariant w.r.t. its invariant distribution \u21e1, (see Arnold, 1974). Now, having in mind (2), the action of L is related to time derivatives of the process, we introduce the continuous version of the cross-covariance as C wX@ = E[ w(X) d w(X)>], in which vector d w(\u00b7) is defined via Ito\u0304 formula (see e.g. (Arnold, 1974; Klus et al., 2020; Hou et al., 2023)) as\nd w,i(X) = r w,i(X) >dX/dt+ 1 2 (dX/dt)>r2 w,i(X)(dX/dt) (12)\n= r w,i(X) >A(X) + 1 2 tr(B(X)>r2 w,i(X)B(X)), (13)\nwhere i 2 [r] and (13) follows from (11). As announced, to show that solving (5) leads to learning the leading invariant subspace of L, we restrict to the case L = L\u21e4 and consider the partial trace of L w.r.t. to the subspace Hw := span( w,j)j2[r] \u2713W 1,2\u21e1 (X ) as the objective. In such a way, exploiting the structure of L, we can relax the compactness assumption of Thm. 1. Indeed, we require the much weaker assumption that L has the largest eigenvalues separated from the essential spectrum, i.e. part of the spectrum consisting of points z 2 C such that the operator L zI is not a Fredholm one (Kato, 1976). Since we can show that tr (PHw LPHw) = tr (CwX) \u2020C wX@ , we obtain the following result, the proof of which is given in App. E. Theorem 2. If Hw \u2713 W 1,2\u21e1 (X ), and 1(L) \u00b7 \u00b7 \u00b7 r+1(L) are eigenvalues of L above its essential spectrum, then for every 0 it holds\nP @ (w) := tr (CwX) \u2020C wX@ R(CwX)  1(L) + \u00b7 \u00b7 \u00b7+ r(L), (14)\nand the equality is achieved when w,j is the eigenfunction of L corresponding to j(L), for j 2 [r].\nNote that the tr (CwX) \u2020C wX@ in Eq. (14) is the sum of the (finite) eigenvalues of the symmetric eigenvalue problem C wX@ C w X . Thus, in non-pathological cases, its value and its derivatives can be computed efficiently in a numerically stable way, see e.g. (Andrew and Tan, 1998)."
        },
        {
            "heading": "4 METHODS",
            "text": "Learning the representation To optimize a DPNet representation one needs to replace the population covariances and cross-covariance in Eqs (8), (9) and (14) with their empirical counterparts. In practice, given NN architectures w, 0w : X ! Rr and a dataset D = (xi, x0i)i2[n] consisting of samples from the joint distribution of (X,X 0) one estimates the needed covariances CwX , C w X0 and C w XX0 as\nbCwX = 1n X\ni2[n]\nw(xi) w(xi) >, bCwX0 = 1n\nX\ni2[n]\n0w(x 0 i) 0 w(x 0 i) > and bC wXX0 = 1n\nX\ni2[n]\nw(xi) 0 w(x 0 i) >.\nIn App. D.1 we derive theoretical guarantees for the concentration of the empirical scores bP n : W ! R and bS n : W ! R, that is the scores computed using empirical covariances, to the true ones for any fixed w. Obtaining uniform guarantees over w requires advanced tools from empirical processes and regularity assumptions on the representations which could be subject of future work. The difficulties arising from estimation of P during optimisation are also addressed.\nIn Alg. 1 we report the training procedure for DPNets-relaxed, which can be also applied to the other scores (8) and (14) in step 4, respectively. When using mini-batch SGD methods to compute r bP m(w) or r bS m(w) via backpropagation at each step, m  n should be taken sufficiently large to\nmitigate the impact of biased stochastic estimators. We include an extended version of the algorithm and address the time complexity of training in App. D.4. Operator regression Once a representation w is learned, it can be used within the framework of operator regression, c.f. Fig. 1. Following the reasoning in (Kostic et al., 2022) one sees that any estimator bTw : Hw ! Hw of the transfer operator T acts on the functions in Hw as\nhz := w(\u00b7) >z 7! bTwhz := w(\u00b7)> bTz, z 2 Rr, (15)\nwhere bT 2 Rr\u21e5r is a matrix in the representation space computed from data (xi, x0i)i2[n]. For example, denoting the data matrices in the representation space by b w = [ w(x1) | \u00b7 \u00b7 \u00b7 | w(xn)], b 0w = [ w(x01) | \u00b7 \u00b7 \u00b7 | w(x 0\nn)] 2 Rr\u21e5n, the ordinary least square estimator (OLS) minimizes the empirical risk\nb 0w bT> b w 2 HS , yielding bT := (b >w)\u2020b 0>w .\nDownstream tasks Once bT is obtained, it can be used predict the next expected state of the dynamics given the initial one, compute the modal decomposition of the given observable (Kutz et al., 2016), estimate the spectrum of the transfer operator (Kostic et al., 2023) and even controlling the dynamics (Proctor et al., 2016). Indeed, recalling that X 0 is the one step ahead evolution of X , we can use (15) to approximate E[hz(X 0) |X = x] as w(x)> bTz. Moreover, relying on the reproducing property of Hw, the predictions can be extended to functions f : X ! R` as E[f(X 0) |X = x] \u21e1 w(x)>(b >w)\u2020 bF 0, where bF 0 = [f(x01) | . . . | f(x0n)]> 2 Rn\u21e5` is the matrix of the observations of the evolved data. Clearly, when the observable is the state itself (i.e. f(x) = x), we obtain one step ahead predictions.\nNext, observe that eigenvalue decomposition of the matrix bT leads to the eigenvalue decomposition of the operator bT . Namely, let (b i, bui, bvi) 2 C\u21e5Cd\u21e5Cd be an eigen-triplet made of eigenvalue, left eigenvector and right eigenvector of bT , that is bTbvi = b ibvi, bu\u21e4i bT = b ibu\u21e4i and bu\u21e4i bvk = i,k, i, k 2 [r], we directly obtain, using (15), the spectral decomposition\nbTw = P i2[r] b i bfi \u2326 bgi, where bfi(x) := w(x)>bvi and bgi(x) := (bui)\u21e4 w(x). (16)\nFinally, we can use the spectral decomposition of bT to efficiently forecast observables for several time-steps in the future via E[hz(Xt) |X0 = x] \u21e1 P i2[r]\nb ti hbgi, hziHw bfi(x), which is known as the extended dynamic mode decomposition (EDMD), see e.g. (Brunton et al., 2022). Noticing that hbgi, hziHw = (bui)\n\u21e4z = (bui)\u21e4 bTz/b i, when b i 6= 0, and, again, using the reproducing property, we can extend this approximation to vector-valued observables f : X ! R` as\nE[f(Xt) |X0 = x] \u21e1 P i2[r] b t 1i w(x)>bvi bu\u21e4i (b >w)\u2020 bF 0 2 R`. (17)"
        },
        {
            "heading": "5 EXPERIMENTS",
            "text": "In this experimental section we show that DPNets (i) learns reliable representations of non-normal dynamical systems, (ii) can be used in its continuous-dynamics form (14) to approximate the eigenvalues of a Langevin generator, and (iii) outperforms kernel methods and auto-encoders in large-scale and/or structured (e.g. graphs and images) data settings. To have a fair comparison, every neural-network model in these experiments has been trained on the same data splits, batch sizes, number of epochs, architectures and seeds. The learning rate, however has been optimized for each one separately. We defer every technical detail, as well as provide additional results to App. F. Baselines We compared our methods DPNets (P ) and DPNets-relaxed (S ), where appropriate, with the following baselines: Dynamic AE (DAE) of (Lusch et al., 2018), Consistent AE of (Azencot et al., 2020), DMD of (Schmid, 2010), KernelDMD and ExtendedDMD of (Williams et al., 2015a;b), and VAMPNets of (Mardt et al., 2018). Logistic Map We study the dynamical system Xt+1 = (4Xt(1 Xt) + \u21e0t) mod 1, for X = [0, 1) and \u21e0t being i.i.d. trigonometric noise (Ostruszka et al., 2000) . The associated transfer operator T is non-normal, making the learning of its spectral decomposition particularly challenging (Kostic et al., 2023). Since T can be computed exactly, we can sidestep the problem of operator regression (see Fig. 1), and focus directly on evaluating the quality of the representation encoded by Hw. We evaluate DPNets on two different metrics: (i) the optimality gap Pr i=1 2 i (T ) P 0(w) for r = 3 and (ii) the\nspectral error, given by maxi minj | i(PHT|H) j(T )|. While (i) informs on how close one is to solve (4), (ii) measures how well the true eigenvalues of T can be recovered within the representation space Hw. In Tab. 1 we compare the DPNets representation against VAMPNets, ExtendedDMD with a feature map of Chebyshev polynomials and the feature defining the trigonometric noise Ostruszka et al. (2000) of the process itself. Notice that in this challenging setting recovering eigenvalues via singular vectors is generally problematic, so a moderate optimality gap may lead to larger spectral error. In Fig. 2 we report the evolution of the spectral error during training for DPNets and VAMPNets, while we defer to App. F.1 an in-depth analysis of the role of the feature dimension. Notice that DPNets and DPNets-relaxed excel in both metrics. Continuous dynamics We investigate a one-dimensional continuous SDE describing the stochastic motion of a particle into the Schwantes potential (Schwantes and Pande, 2015). The invariant distribution for this process is the Boltzmann distribution \u21e1(dx) / e V (x)dx, where V (x) is the potential at state x. The non-null eigenvalues of L hold physical significance, as their (negative) inverse represents the average time for the particle to cross one of the system\u2019s potential barriers (Kramers, 1940); our objective is to accurately estimate them. Here Xt is sampled non-uniformly according to a geometric law. In the lower panel of Fig. 2 we report the estimated transition rates, i.e. the absolute-value of the non-null eigenvalues of L along the DPNets training loop. Notice that the embedding w progressively improves eigenvalue estimation, indicating that the invariant subspaces of the generator L are well captured. Ordered MNIST Following Kostic et al. (2022), we create a stochastic dynamical system by randomly sampling images from the MNIST dataset according to the rule that Xt should be an image of the digit t (mod 5) for all t 2 N0. Given an image from the dataset with label c, a model for the transfer operator T of this system should then be able to produce an MNIST-alike image of the next digit in the cycle. In the upper panel of Fig. 3 we thus evaluate DPNets and a number of baselines by how accurate is an \u201coracle\u201d supervised MNIST classifier (test accuracy for in-distribution 99%) in predicting the correct label c+ t (mod 5) after t steps of evolution. DPNets consistently retain an accuracy above 95%, while for every other method it degrades. The \u201creference\u201d line corresponds to random guessing, while the \u201cOracle-Feature\u201d baseline is an operator regression model (EDMD) using, as the dictionary of functions, the output logits of the oracle, and despite having been trained with the true labels, its performance degrades drastically after t 5. Fluid dynamics We study the classical problem of the transport of a passive scalar field by a 2D fluid flow past a cylinder (Raissi et al., 2020). Each data point comprises a regular 2D grid that encompasses fluid variables, including velocity, pressure, and scalar field concentration at each grid point. This system is non-stationary and is also known to exhibit non-normal dynamics (see Trefethen and Embree, 2020). We evaluate each trained baseline by feeding to it the last snapshot of the train trajectory and evaluating the relative RMSE (that is, the RMSE normalized by the data variance) between the forecasts and the subsequent test snapshots. In this experiment we also have a physics-informed (PINN) baseline not related to transfer operator learning, which is however the model for which this dataset was created in the first place. Remarkably, the forecasting error of DPNets does not grow sensibly with time as it does for every other method. The Metastable states of Chignolin In our last experiment, we study the dynamics of Chignolin, a folding protein, from a 106\u00b5s long molecular dynamics simulation sampled every 200 ps, totalling over 500, 000 data points (Lindorff-Larsen et al., 2011). We focus on the leading eigenfunctions of the transfer operator, which are known (Sch\u00fctte et al., 2001) to provide a simple characterization of the slowest (and usually most important) physical processes occurring along the dynamics. From\nFigure 2: Eigenvalue error decays during training. Upper: spectral error for the logistic map. Lower: DPNetsestimated eigenvalues of L for the Langevin dynamics.\nFigure 3: Forecasting with DPNets. Upper: classification accuracy over time for ordered MNIST. Lower: forecasting RMSE over time for fluid dynamic example.\nFigure 4: Free energy surface of the 2 slowest modes of Chignolin,estimated by DPNets and Nystr\u0308om PCR. To be compared with Bonati et al. (2021).\nthe leading left eigenfunctions of bT , one can indeed construct the free energy surface (see Fig. 4), whose local minima identify the metastable states of the system. The free energy surface, indeed, is proportional to the negative log-pdf of finding the system in a given state while at thermodynamic equilibrium, meaning that a state of low free energy is highly probable, hence metastable. For Chignolin, the metastable states have been thoroughly studied (see e.g. Novelli et al., 2022). The leading left eigenfunction of bT encodes the folding-unfolding transition and the typical transition time is the implied timescale (Mardt et al., 2019) associated to its eigenvalue. The difference between local minimas of the free energy surface encodes the enthalpy H of the transition. In Tab. 2 we compare these quantities to the reference values reported in Lindorff-Larsen et al. (2011). We trained a GNN-based DPNet-relaxed, as both DPNets unrelaxed and VAMPNets failed to converge, possibly due to the large scale of the data. We compared it to a KernelDMD estimator trained with the recent Nystr\u00f6m sketching technique (Meanti et al., 2023) as classical kernel methods are completely intractable at this scale2. Notice that DPNets succeed in finding additional meta-stable states of the system, which match the analysis of (Bonati et al., 2021); see App. F for more discussion."
        },
        {
            "heading": "6 CONCLUSIONS",
            "text": "We propose a general framework for learning representation of dynamical systems, based on orthogonal projections in data-spaces. This representation captures a leading invariant subspace of the transfer operator and can be applied to both discrete and continuous dynamical systems. In the discrete setting, the representations are learned through the optimization of a smooth objective function that is numerically well conditioned. Extensive numerical experiments demonstrate the effectiveness and generality of DPNets in various settings, suggesting that they are a promising tool for data-driven dynamical systems. A limitation of this work is that the score functional for the continuous systems might be unstable since it leverages covariance matrix inversion. Moreover, a future direction would be to study the statistical learning properties of the algorithm presented here.\n2A back-of-the-envelope calculation shows that 450 GBs would be needed just to store kernel matrices in single precision."
        }
    ],
    "year": 2023
}