{
    "abstractText": "The information bottleneck principle provides an information-theoretic method for learning a good representation as a tradeoff between conciseness and predictive ability, which can reduce information redundancy, eliminate irrelevant and superfluous features, and thus enhance the in-domain generalizability. However, in low-resource or out-of-domain scenarios where the assumption of i.i.d does not necessarily hold true, superfluous (or redundant) relevant features may be supplemental to the mainline features of the model, and be beneficial in making prediction for test dataset with distribution shifts. Therefore, instead of squeezing the input information by information bottleneck, we propose to keep as much relevant information as possible in use for making predictions. A three-stage supervised learning framework is designed and implemented to jointly learn the mainline and supplemental features, relieving supplemental features from the suppression of mainline features. Extensive experiments have shown that the learned representations of our method have good in-domain and out-of-domain generalization abilities, especially in low-resource cases.",
    "authors": [],
    "id": "SP:b8511f16228733a49317f1523d7197cf361ce5ed",
    "references": [
        {
            "authors": [
                "Kartik Ahuja",
                "Ethan Caballero",
                "Dinghuai Zhang",
                "Jean-Christophe Gagnon-Audet",
                "Yoshua Bengio",
                "Ioannis Mitliagkas",
                "Irina Rish"
            ],
            "title": "Invariance principle meets information bottleneck for out-ofdistribution generalization",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Mahbubul Alam",
                "Manar D Samad",
                "Lasitha Vidyaratne",
                "Alexander Glandon",
                "Khan M Iftekharuddin"
            ],
            "title": "Survey on deep neural networks in speech and vision systems",
            "year": 2020
        },
        {
            "authors": [
                "Alexander A Alemi",
                "Ian Fischer",
                "Joshua V Dillon",
                "Kevin Murphy"
            ],
            "title": "Deep variational information bottleneck",
            "venue": "In ICLR,",
            "year": 2017
        },
        {
            "authors": [
                "Philip Bachman",
                "R Devon Hjelm",
                "William Buchwalter"
            ],
            "title": "Learning representations by maximizing mutual information across views",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Anthony J Bell",
                "Terrence J Sejnowski"
            ],
            "title": "An information-maximization approach to blind separation and blind deconvolution",
            "venue": "Neural computation,",
            "year": 1995
        },
        {
            "authors": [
                "Luis Candanedo"
            ],
            "title": "Appliances energy prediction",
            "venue": "UCI Machine Learning Repository,",
            "year": 2017
        },
        {
            "authors": [
                "Daniel Cer",
                "Mona Diab",
                "Eneko Agirre",
                "Inigo Lopez-Gazpio",
                "Lucia Specia"
            ],
            "title": "Semeval-2017 task 1: Semantic textual similarity-multilingual and cross-lingual focused evaluation",
            "venue": "arXiv preprint arXiv:1708.00055,",
            "year": 2017
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova. Bert"
            ],
            "title": "Pre-training of deep bidirectional transformers for language understanding",
            "venue": "arXiv preprint arXiv:1810.04805,",
            "year": 2018
        },
        {
            "authors": [
                "Yingjun Du",
                "Jun Xu",
                "Huan Xiong",
                "Qiang Qiu",
                "Xiantong Zhen",
                "Cees GM Snoek",
                "Ling Shao"
            ],
            "title": "Learning to learn with variational information bottleneck for domain generalization",
            "venue": "In Computer Vision\u2013ECCV 2020: 16th European Conference,",
            "year": 2020
        },
        {
            "authors": [
                "Marco Federici",
                "Anjan Dutta",
                "Patrick Forr\u00e9",
                "Nate Kushman",
                "Zeynep Akata"
            ],
            "title": "Learning robust representations via multi-view information bottleneck",
            "venue": "arXiv preprint arXiv:2002.07017,",
            "year": 2020
        },
        {
            "authors": [
                "Ian J Goodfellow",
                "Jonathon Shlens",
                "Christian Szegedy"
            ],
            "title": "Explaining and harnessing adversarial examples",
            "venue": "arXiv preprint arXiv:1412.6572,",
            "year": 2014
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Olivier Henaff"
            ],
            "title": "Data-efficient image recognition with contrastive predictive coding",
            "venue": "In International conference on machine learning,",
            "year": 2020
        },
        {
            "authors": [
                "R. Devon Hjelm",
                "Alex Fedorov",
                "Samuel Lavoie-Marchildon",
                "Karan Grewal",
                "Philip Bachman",
                "Adam Trischler",
                "Yoshua Bengio"
            ],
            "title": "Learning deep representations by mutual information estimation and maximization",
            "venue": "In 7th International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "arXiv preprint arXiv:1412.6980,",
            "year": 2014
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Max Welling"
            ],
            "title": "Auto-encoding variational bayes",
            "venue": "arXiv preprint arXiv:1312.6114,",
            "year": 2013
        },
        {
            "authors": [
                "Alex Krizhevsky",
                "Geoffrey Hinton"
            ],
            "title": "Learning multiple layers of features from tiny images",
            "year": 2009
        },
        {
            "authors": [
                "Xiang Lisa Li",
                "Jason Eisner"
            ],
            "title": "Specializing word embeddings (for parsing) by information bottleneck",
            "venue": "arXiv preprint arXiv:1910.00163,",
            "year": 2019
        },
        {
            "authors": [
                "Ralph Linsker"
            ],
            "title": "Self-organization in a perceptual network",
            "year": 1988
        },
        {
            "authors": [
                "Andrew Maas",
                "Raymond E Daly",
                "Peter T Pham",
                "Dan Huang",
                "Andrew Y Ng",
                "Christopher Potts"
            ],
            "title": "Learning word vectors for sentiment analysis. In Proceedings of the 49th annual meeting of the association for computational linguistics",
            "venue": "Human language technologies,",
            "year": 2011
        },
        {
            "authors": [
                "Rabeeh Karimi Mahabadi",
                "Yonatan Belinkov",
                "James Henderson"
            ],
            "title": "Variational information bottleneck for effective low-resource fine-tuning",
            "venue": "arXiv preprint arXiv:2106.05469,",
            "year": 2021
        },
        {
            "authors": [
                "Behnam Neyshabur"
            ],
            "title": "Implicit regularization in deep learning",
            "venue": "arXiv preprint arXiv:1709.01953,",
            "year": 2017
        },
        {
            "authors": [
                "Aaron van den Oord",
                "Yazhe Li",
                "Oriol Vinyals"
            ],
            "title": "Representation learning with contrastive predictive coding",
            "venue": "arXiv preprint arXiv:1807.03748,",
            "year": 2018
        },
        {
            "authors": [
                "Daniel W Otter",
                "Julian R Medina",
                "Jugal K Kalita"
            ],
            "title": "A survey of the usages of deep learning for natural language processing",
            "venue": "IEEE transactions on neural networks and learning systems,",
            "year": 2020
        },
        {
            "authors": [
                "Bo Pang",
                "Lillian Lee"
            ],
            "title": "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales",
            "venue": "arXiv preprint cs/0506075,",
            "year": 2005
        },
        {
            "authors": [
                "Joshua Robinson",
                "Li Sun",
                "Ke Yu",
                "Kayhan Batmanghelich",
                "Stefanie Jegelka",
                "Suvrit Sra"
            ],
            "title": "Can contrastive learning avoid shortcut solutions",
            "venue": "Advances in neural information processing systems,",
            "year": 2021
        },
        {
            "authors": [
                "Wojciech Samek",
                "Thomas Wiegand",
                "Klaus-Robert M\u00fcller"
            ],
            "title": "Explainable artificial intelligence: Understanding, visualizing and interpreting deep learning models",
            "venue": "arXiv preprint arXiv:1708.08296,",
            "year": 2017
        },
        {
            "authors": [
                "Ravid Shwartz-Ziv",
                "Naftali Tishby"
            ],
            "title": "Opening the black box of deep neural networks via information",
            "venue": "arXiv preprint arXiv:1703.00810,",
            "year": 2017
        },
        {
            "authors": [
                "Karen Simonyan",
                "Andrea Vedaldi",
                "Andrew Zisserman"
            ],
            "title": "Deep inside convolutional networks: Visualising image classification models and saliency maps",
            "venue": "arXiv preprint arXiv:1312.6034,",
            "year": 2013
        },
        {
            "authors": [
                "Richard Socher",
                "Alex Perelygin",
                "Jean Wu",
                "Jason Chuang",
                "Christopher D Manning",
                "Andrew Y Ng",
                "Christopher Potts"
            ],
            "title": "Recursive deep models for semantic compositionality over a sentiment treebank",
            "venue": "In Proceedings of the 2013 conference on empirical methods in natural language processing,",
            "year": 2013
        },
        {
            "authors": [
                "Xiaofei Sun",
                "Diyi Yang",
                "Xiaoya Li",
                "Tianwei Zhang",
                "Yuxian Meng",
                "Han Qiu",
                "Guoyin Wang",
                "Eduard Hovy",
                "Jiwei Li"
            ],
            "title": "Interpreting deep learning models in natural language processing: A review",
            "venue": "arXiv preprint arXiv:2110.10470,",
            "year": 2021
        },
        {
            "authors": [
                "Yonglong Tian",
                "Dilip Krishnan",
                "Phillip Isola"
            ],
            "title": "Contrastive multiview coding",
            "venue": "In Computer Vision\u2013ECCV 2020: 16th European Conference,",
            "year": 2020
        },
        {
            "authors": [
                "Naftali Tishby",
                "Noga Zaslavsky"
            ],
            "title": "Deep learning and the information bottleneck principle",
            "venue": "In 2015 ieee information theory workshop (itw),",
            "year": 2015
        },
        {
            "authors": [
                "Naftali Tishby",
                "Fernando C.N. Pereira",
                "William Bialek"
            ],
            "title": "The information bottleneck method",
            "venue": "In The 37th annual Allerton Conference on Communication, Control, and Computing,",
            "year": 1999
        },
        {
            "authors": [
                "Peter West",
                "Ari Holtzman",
                "Jan Buys",
                "Yejin Choi"
            ],
            "title": "Bottlesum: Unsupervised and selfsupervised sentence summarization using the information bottleneck principle",
            "year": 1909
        },
        {
            "authors": [
                "Matthew D Zeiler",
                "Rob Fergus"
            ],
            "title": "Visualizing and understanding convolutional networks",
            "venue": "In Computer Vision\u2013ECCV 2014: 13th European Conference,",
            "year": 2014
        },
        {
            "authors": [
                "Xiang Zhang",
                "Junbo Zhao",
                "Yann LeCun"
            ],
            "title": "Character-level convolutional networks for text classification",
            "venue": "Advances in neural information processing systems,",
            "year": 2015
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Deep neural network (DNN) has revolutionized a variety of fields (inclusive of but not limited to computer vision, natural language processing and speech recognition) in supervised learning tasks (Alam et al., 2020; Otter et al., 2020). Although usually overparameterized, DNN has shown good generalization ability and performs well on test data. Dating back to the Occam\u2019s Razor philosophical principle, it has long been believed that a good model should be as simple as possible. Some research works (Shwartz-Ziv & Tishby, 2017; Neyshabur, 2017) have been done to uncover the implicit regularization phenomenon or mechanism in the training process of DNNs.\nTo reveal the dynamics of DNN training process, by visualizing information plane, it was observed (Shwartz-Ziv & Tishby, 2017) that most of the DNN training efforts are spent on compressing the input to concise representation. This observation has justified the application of the information bottleneck principle (Tishby et al., 1999; Tishby & Zaslavsky, 2015) in the supervised learning settings, seeking to capture and efficiently represent the relevant information in the input variable about the label variable and building up a good presentation in terms of a fundamental tradeoff between conciseness and good predictive ability. By explicitly minimizing the mutual information between the input data and its representation and simultaneously maximizing the mutual information between the representation and the label, it has been shown the information bottleneck principle has led to the robustness of fitted model (Alemi et al., 2017).\nOn the other hand, in the setting of unsupervised representation learning, there is no supervised label to identify relevant or irrelevant information, and thus the InfoMax principle (Linsker, 1988) is usually used to maximize the mutual information between the input and its representation (Oord et al., 2018; Hjelm et al., 2019), with the expectation that all the (potentially) predictive information is preserved for various downstream supervised tasks.\nIn contrast to the information bottleneck principle that ignores as many details of the input, our work goes to another end, with the so-called Information Retention principle as our main thrust: when making predictions (or decisions), it is preferable to keep as much relevant information as possible in use. In other words, the principle of information retention differs from the information bottleneck: information retention explicitly preserves the relevant redundant information, but\ninformation bottleneck implicitly suppresses the redundant relevant information. Bearing the idea of information retention in mind, we design and implement a three-stage process for supervised learning. It firstly learns mainline features via vanilla DNN model, then erases salient input features (with respect to the mainline features) from the original inputs to produce modified inputs, and finally uses a conditional mutual information as regularization term forcing the supplemental features to be complementary to the mainline features and alleviating feature suppression.\nIn addition, our approach is also different from the InfoMax principle. It is infeasible to directly maximize the mutual information between the representation and the input in supervised learning, because it will learn irrelevant information and produce irrelevant features without any restriction.\nOur contribution are three folds:\n\u2022 We propose the information retention principle that favors using as much relevant information as possible in supervised learning.\n\u2022 To alleviate the problem of feature suppression, we develop a three-stage process for information retention via learning supplemental features.\n\u2022 Experimental results indicate that the learned representation of our method has better indomain and out-of-domain generalization ability than several competitors.\nTo better motivate the idea of information retention, we also provide our thought from the perspective of causal diagram (Appendix D.1) and investigate a simple toy example (Appendix D.2)."
        },
        {
            "heading": "2 METHOD",
            "text": "This section focuses on the details of the proposed method that is called \u201cInformation Retention by Learning Supplemental Features\u201d (or InfoR-LSF in short). The basic idea is to build up a more abundant supervised representation for classification task. To facilitate discussion, this paper assumes that a DNN-based classification model consists of two parts: the first is an encoder that maps an input vector x \u2208 RM to a representation vector z \u2208 RD, and the second is a classification head that maps the representation vector z to the final prediction y\u0303. Finally, the L(y\u0303,y) denote the supervised loss used to train the model, where y is the true label of x. In our method, the representation vector z = [zM ; zS ] \u2208 RD is divided into two groups: the mainline representation zM \u2208 RD/2, and the supplemental representation zS \u2208 RD/2, which are learned in a three-stage process. The first stage can be thought of as a \u201dburn-in\u201d phase, initially building up the mainline representation from the original train data. The second stage erases the salient input features from each original input and produces a modified input, where the salient input features denote the features that are important with respect to the mainline representation zM . The third stage aims to jointly learn mainline representation zM and supplemental representation zS by forcing zS to forget the salient input features that have already been captured by mainline representation and alleviate feature suppression, with the help of the modified inputs from the second stage. In the following sections, we will specifically discuss each stage below and also provide an algorithm table in Appendix C."
        },
        {
            "heading": "2.1 THE FIRST STAGE: INITIAL TRAINING OF MAINLINE FEATURES",
            "text": "At the first stage, the task is to train an initial encoder and obtain the mainline features zM by maximizing the mutual information between zM and the label y and (optionally) simultaneously minimizing the mutual information between zM and input x. Therefore, the first-stage objective is:\nmaximize I(zM ;y)\u2212 \u03b2 \u00b7 I(zM ;x) (1)\nwhere \u03b2 is the coefficient used to control information compression. Setting \u03b2 = 0 will disable the information bottleneck mechanism.\nFigure 1 shows the architecture of the first stage for initially training of our method. The encoding network consists of a backbone f\u03b8(\u00b7) and a variational encoder g\u03d5(\u00b7) with few parameters. For input x, we use an encoding network to obtain the distribution parameters \u00b5 and \u03a3, and generate the mainline representation zM through reparameterization trick (Kingma & Welling, 2013). It should be highlighted that in the first stage, we only calculate the supervised loss with information bottleneck restriction on the mainline representation."
        },
        {
            "heading": "2.2 THE SECOND STAGE: SALIENCY ERASING FROM INPUTS",
            "text": "The objective of the second stage is to find and erase the salient input features with respect to mainline features zM from the input x and produce a modified input which will be used in the third stage for auxiliary loss calculation. Formally, we denote xsf as the most salient features that zM has learned in x, and utilize x\u2032 = MASK(x) = x/xsf to represent the modified input after removing xsf from x, where MASK(\u00b7) refers to a feature erasure operation on the input x varying with different data types.\nSalient Input Feature Selection. Given an input data x, its mainline features can be obtained as zM = g\u03d5(f\u03b8(x) via the well-trained mainline representation extractor g\u03d5(f\u03b8(\u00b7)) of the first stage. The task here is to find a feature subset xsf \u2282 x that the current model relies on most to make prediction. To measure the importance of input features, we use the norm of the loss gradient with respect to input features.\nxsf = topK x\u2208x ||\u2207xL(g\u03d5(f\u03b8(x)),y)|| (2)\nThe underlying premise is that the larger the loss gradient, the higher the dependence level. Previous studies also supports the rationality of gradient-based feature significance analysis. (Samek et al., 2017; Sun et al., 2021)\nSalient Input Feature Erasing. After selecting the salient input features xsf that the current mainline representation zM most heavily depends on, the next step is to perform MASK(\u00b7) operation on the raw input x. For image data, the masking operation works at the level of pixels and xsf designates a certain proportion of pixels based on the Equation 2. We then fill these pixels with random values that conform to a Gaussian distribution N (\u00b5, \u03c3), where the \u00b5 and \u03c3 are calculated based on empirical distribution of pixels of the whole train set. In addition, we will ensure that the random pixel values filled in are within the range of valid image through clip operations.\nAs for text data, each raw input x is represented as a sequence of tokens, and thus the masking operation is done at token level. As we utilize pretrained language models with special tokens, we erase text feature by replacing a certain proportion of tokens selected by gradient norm of token embeddings with [MASK] token.\nAdditionally, it should be emphasized that the modification on x ought to be kept within a restricted range, as we expect that the remaining part still contains relevant information about y. The model could be negatively affected by excessive erasure since the obtained x\u2032 may lose excessive supervisory information."
        },
        {
            "heading": "2.3 THE THIRD STAGE: JOINT TRAINING OF MAINLINE AND SUPPLEMENTAL FEATURES",
            "text": "In the third stage, the task is to jointly train the whole model and simultaneously learn the mainline features zM and the supplemental features zS . The mainline features zM still uses the same supervised objective as the one used in the first stage (Equation 1), while the zS needs to suppress learning salient features xsf of zM while learning supervised objective.\nThe intuitive idea here is to force the model to continuously accquire new potential features distinct from the features already learnt by current representation zM , and expect these new features to be helpful for classification. We refer to these features as supplementary features, and we think the key characteristic of these features is that they are partially non-overlapping features with respect to xsf and are easy to be suppressed or overlooked by the mainline features zM , possibly weaker or harder, as long as they will contribute to the supervisory task.\nTo accomplish this, we leverage another supplementary representation zS to learn these supplementary features. However, it is not guaranteed that zS won\u2019t extract the same salient feature xsf that zM has already learnt if there are no constraints on zS . As a result, zS should be restricted not to learn features that exists in x but not in x\u2032. According to the chain rule of mutual information, the information zS contains about x could be divided into two parts(the proof can be found at Appendix B.1):\nI(zS ;x) = I(zS ;x \u2032) + I(zS ;x|x\u2032) (3)\nThe first term means the information zS contains about x\u2032, and the second term intuitively represents the information zS contains which is unique to x and is not predictable by observing x\u2032. This term is exactly what we tend to suppress as we only expect zS to learn information relevant to y from x\u2032. In this way, we can derive the learning objective of zS as follow:\nmaximize I(zS ;y)\u2212 \u03b2 \u00b7 I(zS ;x)\u2212 \u03b1 \u00b7 I(zS ;x|x\u2032) (4)\nwhere the the first two terms represent supervised loss with information bottleneck, which is consistent with zM \u2019s learning objective. The third item is a regularization restriction on zS representation aiming to force it to learn supplementary features.\nFigure 2 illustrates the architecture and information flow of the third stage. We use two different variational encoder g\u03d5 and g\u03c8 to generate zM and zS , respectively. However, the backbone and the final linear classifier are shared, and the variational encoder only has very few parameters, so our method only uses a small number of additional parameters, which is different from model integration. Additionally, although the joint training stage makes use of the modified inputs generated by the second stage, their use in our method are essentially different from the other data-augmentationbased methods. It should be noted that the supervision loss for zS is still calculated on the representation corresponding to the original inputs x but not the modified inputs x\u2032. The modified inputs x\u2032 are used only for a regularization term I(zS ;x|x\u2032). Therefore, in contrast to data-augmentationbased methods, we need not worry about the incorrect supervision signal (or wrong label) that are possibly introduced to the modified inputs x\u2032."
        },
        {
            "heading": "2.4 MODEL ARCHITECTURE AND LOSS FUNCTION",
            "text": "In this section, we specifically discuss how to achieve optimization goals mentioned above in pratice. From the perspective of architecture, we design a network with two different variational encoder head on a shared backbone and a shared linear classifier. It should be noted that in inference stage, we will use both zM and zS by averaging logits.\nAs for the optimizing objectives of each stage, firstly, for the Equation 1, the loss could be derived as follow based on the variational estimate proposed by Alemi et al.:\nLVIB(x, zM , \u03b8, \u03d5) = Ex[EzM\u223cp\u03b8,\u03d5(zM |x)[\u2212 log q(y|zM )] + \u03b2 \u00b7DKL[p\u03b8,\u03d5(zM |x)||r\u03d5(zM )]] (5)\nwhere r\u03d5(zM ) is prior distribution of zM . We use a parameterized Gaussian distribution N (\u00b5\u03d5,\u03a3\u03d5) to represent r\u03d5(zM ). As for I(zS ;x|x\u2032) used in Equation 4, we restrict the term by minimizing its\nupper bound (the proof can be found at Appendix B.2): LIS = Ex,x\u2032 [DKL[p\u03b8,\u03c8(zS |x)||p\u03b8,\u03c8(z\u2032S |x\u2032)]] (6)\nIn this way, the total loss of the third stage can be derived as L = LVIB(x, zM , \u03b8, \u03d5) + LVIB(x, zS , \u03b8, \u03c8) + \u03b1 \u00b7 LIS (7)\nwhere \u03b1 is a coefficient that controls the weight of information suppression. Besides, since the distribution of both zM and zS follows a Gaussian distribution, there exists a closed form solution for calculating KL-divergence in the above losses."
        },
        {
            "heading": "3 EXPERIMENTS",
            "text": "This section is devoted to a thorough empirical study of the proposed InfoR-LSF method. We first evaluate it in-domain generalization ability on classification and regression tasks (Section 3.1) and then investigate the out-of-domain generalization of learned representation on sentiment analysis task (Section 3.2. In addition, we also conduct experiments to analyze the effect of information retention (Section 3.3) and study the hyperparameter sensitivity (Section 3.4). Finally, results of ablation experiments (Section 3.5) are presented.\nArchitectures. We choose ResNet-18 (He et al., 2016) as the backbone for image classification, the pretrained BERTBASE (Devlin et al., 2018) as the backbone for text-related tasks, and MLP network as the backbone for tabular regression. More implementation details about training hyperparameters and optimizer are provided in the Appendix A.2.\nBaselines. We compare against several influential works: IFM (Robinson et al., 2021), a method which avoids shortcut solutions by implicit feature modification; FGSM (Goodfellow et al., 2014), a classic adversarial training method in computer vision; VIB (Alemi et al., 2017), a variational approximation to the information bottleneck by leveraging the reparameterization trick; VIBERT (Mahabadi et al., 2021), a method implementing the variational information bottleneck on the pre-trained BERT to suppress irrelevant features and enhance generalization ability when fine-tuning. In addition, the basic ResNet-18 (or BERTBASE) model serves as a baseline."
        },
        {
            "heading": "3.1 IN-DOMAIN GENERALIZATION ON SUPERVISED LEARNING TASK",
            "text": "Image classification. We use two image classification datasets1, CIFAR10 (Krizhevsky et al., 2009) and CIFAR100(Krizhevsky et al., 2009). Different sizes of training data (subsampled from the original training set), ranging from 50 to 3000, are used to evaluate the test accuracies in lowresource settings. Each experiment is conducted five times, each time with a different random seed for data sampling. The average test accuracies for CIFAR10 are shown in Table 1. The performance on CIFAR100 is provided in Appendix A.3.\nText classification. We use two sentiment analysis datasets, namely IMDB (Maas et al., 2011) and YELP (Zhang et al., 2015). The sizes of training data range from 50 to 1000, with training examples randomly sampled with five seeds (13, 21, 42, 87 and 100). We report the average and standard deviation of test accuracies in Table 2. Here, under low resource settings, we do not use the original validation set, but instead sample a validation subset of the same size as the train data.\n1See Appendix A for dataset statistics information\nTextual Similarity Score Regression. The task STS-B(Cer et al., 2017) is to regress textual similarity score. The results of STS-B are shown in Table 3.\nTraditional Tabular Regression. We further conduct experiments on a tabular regression task Appliance Energy Prediction(Candanedo, 2017) from UCI Machine Learning Repository (Asuncion & Newman, 2007). We first process the data as same as public preprocessing 2 and then apply a 5-layer MLP for prediction. We adjust the hyper-parameters to make MLP reach the public SOTA performance, and then verify the effect of InfoR-LSF on this basis. The results of energy prediction are shown in Table 4.\nObservation and Analysis. Firstly, from Table 1 and Table 2, it is obvious that our InfoRLSF method surpasses all competitors under all settings of training data sizes, for both image and text classification tasks. Table 3 and Table 4 further prove that InfoR-LSF also performs well on regression tasks. These observations indicate the universality of information retention principle. We attribute this substantial performance gain to its ability of learning diverse features, inclusive of not only the mainline features but also the supplemental features (Please note that the supplemental features may be redundant with respect to the mainline ones from the perspective of training data). Secondly, VIB achieves little performance gain on CIFAR, while VIBERT exhibits more performance gain on text-related tasks. We attribute this to the essential difference between images and texts: the tokens, as the basic units of texts, are meaningful, and only a few of them can indicate the label of a given text; however, the pixels, as the basic units of images, are meaningless alone, oversimple features that comprise fewer pixels may not necessarily imply better generalization ability. That also explains why VIB is more often applied to NLP tasks. Last but not the least, InfoR-LSF\n2https://www.kaggle.com/code/rrakzz/r2-68-accuracy-95\nexhibits much notable improvements in low resource conditions, and its performance gain gradually declines as the number of training examples rises. It suggests that the feature redundancy in the training data may diminish with the increased availability of labeled data resource, and the train and test sets tend to be more equally-distributed as the data size grows."
        },
        {
            "heading": "3.2 OUT-OF-DOMAIN PERFORMANCE",
            "text": "Since InfoR-LSF is wedded to learn more versatile relevant features, it is naturally hypothesized that the learned representation is beneficial to similar out-of-domain tasks. To verify this hypothesis, we choose the sentiment classification task, and use the full-size YELP data (Zhang et al., 2015) as the source domain to train models. By freezing the backbone and retraining a linear task-specific classification head, we evaluate the linear readout of each model on a series of out-of-domain target tasks, including IMDB (Maas et al., 2011), YELP-2 (Zhang et al., 2015), SST-2 (Socher et al., 2013), SST-5 (Socher et al., 2013), MR (Pang & Lee, 2005), Amazon-2 (Zhang et al., 2015) and Amazon5 (Zhang et al., 2015). Each linear head is trained with 1000 labeled data from the target task. As shown in Table 5, on the all target tasks, InfoR-LSF consistently achieves the highest improvement. We conjecture the reason to be our method\u2019s ability of extracting more versatile features and thus the learned representation is more likely to cover the useful features in target domains, leading to better out-of-domain generalization."
        },
        {
            "heading": "3.3 EFFECT VERIFICATION OF INFORMATION RETENTION",
            "text": "The main motivation of our method is to extract as much information about y from x as possible. Previous experiments have demonstrated its superiority in various downstream tasks, but is the improvement in effect due to the idea of information retention? To explore this question, we investigate whether the model exploits more input features by observing the model\u2019s attention distribution on input images. If the learned representation at the final hidden layer utilizes more original input features, the attention of the model should be distributed more evenly on more pixels. Otherwise, the attention will be distributed intensively on fewer pixels. Therefore, we use the gradient norm of loss as the model\u2019s attention score for each pixel, which has been proven to be an effective method for evaluating the contribution of features (Simonyan et al., 2013; Zeiler & Fergus, 2014). Specifically, we first obtain the gradient \u2207xL(z,y) for each pixel x \u2208 x, normalize the absolute values of the gradients over the entire image, and then examine the distribution of gradient amplitudes across the entire test set. From the visualized results in Figure 3, it can be observed that our method has the most dispersed attention distribution on the original input x under different training data sizes, and its distribution peak is significantly lower than other methods. On the contrary, VIB has the highest peak of gradient distribution, and thus attends intensively on the fewest pixels. The phenomenon meets our expectations, as the VIB restricts the mutual information between the representation z and input x through an explicit regularization term, so the utilization of information by the representation z on x is insufficient. Surprisingly, The gradient distribution of FGSM, which is based on adversarial training, is actually more concentrated than baseline, indicating that adversarial attacks can improve the robustness of the model, but may not necessarily force the model to leverage more original input information. In summary, compared to other baselines, our method exhibits a more uniform and dispersed trend in pixel level attention distribution, demonstrating the mechanism of information retention is in work effectively.\n3.4 SENSITIVITY ANALYSIS OF \u03b1 AND MASKING RATIO\nTo examine the effect of loss term LIS in Equation 7, We perform sensitivity analysis experiments by varying coefficient \u03b1 and masking ratio on the IMDB dataset under low resource setting (50 train data). As shown in Figure 4a (or 4b), test and validation accuracies first increase and then decrease by increasing \u03b1 (or by increasing masking ratio), and they both achieve the best performance at \u03b1 = 1e\u22123 (or with the masking ratio of 0.05). It means that over-high values of \u03b1 and making ratio put excessive restriction on zS representation, and thus do harm to the performance.\nFigure 4c shows the difference in the attention distribution of zS and zM on salient input features xsf . We calculate the attention proportion by first calculating the gradient norm ||\u2207xsfL(zM ,y)|| and ||\u2207xsfL(zS ,y)||, and then normalizing them across the whole sentence. Obviously, the gap between zM and zS gradually increases, indicating that the restrictions on regularization terms are tightening. Additionally, we have noticed that an excessively large \u03b1 will also constrain zM meanwhile because zM and zS share the encoder parameter. This can also explain why excessive \u03b1 is harmful."
        },
        {
            "heading": "3.5 ABLATION STUDY",
            "text": "To explore the effect of the regularization restriction on zS and the IB restriction, we conduct ablation experiments on three datasets. We remove the regularization restriction on zS by setting \u03b1 = 0 and remove IB restriction by setting \u03b2 = 0. The results are shown in Table 6. Firstly, regardless of whether there are IB restrictions, the performance does reduce on all considered datasets if \u03b1 = 0, demonstrating the solid gain of the regularization term of our method. Secondly, we find that in some cases(especially on CIFAR10), only removing the IB restriction(\u03b2 = 0) results in performance improvement, indicating that the IB restriction can lead to negative effects in some cases. The phenomenon is also consistent with our previous analysis in Section 3.1 that the gains brought by IB on text are more significant than that on images. Furthermore, it should be noted that the ablation results still differ from the base model due to the use of two variational encoders. When \u03b1 = 0, InfoR-LSF degenerates to a VIB model with two heads (the modified input x\u2032 will not participate in any loss calculation if \u03b1 = 0). In this case, the results should be similar to VIB or VIBERT."
        },
        {
            "heading": "4 RELATED WORK",
            "text": "Information Bottleneck. The information bottleneck(IB) (Tishby et al., 1999) was first proposed in traditional machine learning and then used to analyze and interpret the behavior of deep neural networks (Tishby & Zaslavsky, 2015; Shwartz-Ziv & Tishby, 2017). Later, Alemi et al. (2017) presented variational information bottleneck (VIB) to improve the learning of DNNs by optimizing a variational bound of IB objective. (Federici et al., 2020) extended the IB method to learn robust representations in unsupervised multi-view setting by minimizing superfluous information not shared by the two views. Additionally, IB were introduced to NLP tasks such as dependency parsing (Li & Eisner, 2019) and unsupervised sentence summarization (West et al., 2019). Mahabadi et al. (2021) applied VIB in the finetuning of pretrained BERT (Devlin et al., 2018) , showing good low-resource performance. Besides, IB also helped domain generalization(Ahuja et al., 2021; Du et al., 2020).\nInformation Maximization. The InfoMax principle (Linsker, 1988; Bell & Sejnowski, 1995) were proposed to advocate maximizing mutual information between the input and output. Hjelm et al. (2019) applied the InfoMax principle on unsupervised representations learning for deep neural networks and Bachman et al. (2019) further developed self-supervised representation learning based on maximizing mutual information between features extracted from multiple views. Besides, the InfoMax principle has also been leveraged in contrastive representation learning, such as contrastive predictive coding (Oord et al., 2018; Henaff, 2020) and contrastive multi-view coding (Tian et al., 2020).\nAlthough it has been observed that IB methods can learn parsimonius features with better generalization in resource-limited scenarios, they implicitly suppress other redundant relevant features. The main motive of this work advocate the learned representations should incorporate these suppressed relevant features, which makes it distinct from IB methods."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "In this work, we introduce the principle of information retention, which aims to keep as much relevant information as possible in use for making predictions. We further design a three-stage supervised learning framework named InfoR-LSF for information retention by jointly learning the mainline and supplemental features. In experiments, we compare InfoR-LSF against other methods, and its strong performance in different fields under both low-resource and out-of-domain scenario shows that InfoR-LSF can be practically applied to various type of tasks. Furthermore, analysis experiments indicate that our framework indeed achieves information retention and extracts more relevant features than other competitors."
        },
        {
            "heading": "A EXPERIMENTAL SUPPLEMENT",
            "text": "A.1 DATASET STATISTICS\nTable 7 shows statistic information of datasets used in our experiments.\nA.2 IMPLEMENTATION DETAILS\nFor CIFAR10, refering to the open source implementation3, we train the model for 200 epochs with batch size 256, initial learning rate 0.1 and weight decay 5e-4. We apply SGD optimizer with momentum 0.9 and a step scheduler that decays the learning rate by 0.1 every 160 epochs. For CIFAR100, we train the model for 240 epochs with batch size 64, initial learning rate 0.05 and weight decay 5e-4. We use SGD optimizer with momentum 0.9 and multi-step scheduler that decays the learning rate by 0.1 at milestones 150, 180, 210. For text classification tasks, we finetune all the models for 25 epochs with batch size 8 to allow them to converge. We use AdamW optimizer (Kingma & Ba, 2014) with initial learning rate 2e-5 and warmup ratio 0.05 for all models for fine-tuning. Besides, we use early stopping for in all text experiments based on the performance of the validation set.\nAs for hyper-parameters of variational information bottleneck, we use bottleneck size as half of the hidden layer representation dimension, i.e., 384 for BERT and 256 for ResNet-18. And the \u03b2 of information bottleneck is set to 1e-5 for all experiments. We use sample size 5 for reparameterization.\nAs for the coefficient \u03b1 of our method, we use 1e-3 for all the text experiments, 1e-2 of CIFAR10 and CIFAR100 and 0.1 for Appliance Energy Prediction. And for the modification of input features, in all our experiments, the mask proportion is 5%, i.e., 5% of the pixels in the image or 5% of the tokens for the sentence.\nA.3 IMAGE CLASSIFICATION RERFORMANCE ON CIFAR100\nOn CIFAR100, we construct low resource subset of various sizes from original train set, with scales ranging from 1000 to 50000. Results are provided in Table 8. Consistent with the conclusion on CIFAR10, our method outperforms all baseline methods at all data sizes.\n3https://github.com/kuangliu/pytorch-cifar\nA.4 COMPUTATIONAL COMPLEXITY ANALYSIS\nTheoretically, InfoR-LSF conducts forward propagation and backward propagation twice during the training phase, and there is double data in the second time. As a result, the time consumption should be approximately 2-3 times that of the base model. During the inference phase, InfoR-LSF doesn\u2019t require additional backward propagation.The increase in calculation amount only comes from the additional variational encoder. Therefore, the time cost should be comparable to the base model. We measure the running time of each method on CIFAR10 and IMDB. The results are shown in Table 9, in which the base model is used as a benchmark. It can be observed that the training time of InfoR-LSF is about 2.43 times that of the base model, and the inference time is about 1.03 times that of the base model, which is consistent with the conclusion of the theoretical analysis."
        },
        {
            "heading": "B PROOFS",
            "text": "In the framework of InfoR-LSF, we first train zM on original input x, and then mask the salient feature of x according to zM to obtain x\u2032. In the third stage, we jointly train both zM and zS , where zS is restricted by a regularization term. The relationship between x, x\u2032, zM , zS , and z\u2032S is shown as Figure 5. It can be derived that x\u2032 and zS are conditionally independent for any given x.\nB.1 PROOF OF EQUATION 3\nHypothesis:\nGiven x, x\u2032 and zS are conditionally independent: I(zS ;x\u2032|x) = 0 Proof.\nI(zS ;x) = I(zS ;xx \u2032)\u2212 I(zS ;x\u2032|x)\n= I(zS ;xx \u2032) = I(zS ;x \u2032) + I(zS ;x|x\u2032)\n(8)\nB.2 PROOF OF EQUATION 6\nHypothesis:\nGiven x, x\u2032 and zS are conditionally independent: p(zSx\u2032|x) = p(zS |x)p(x\u2032|x) Proof.\nI(zS ;x|x\u2032) = Ex\u2032\u223cp(x\u2032)EzS ,x\u223cp(zS ,x|x\u2032) log p(zS ,x|x\u2032)\np(zS |x\u2032)p(x|x\u2032)\n= Ex,x\u2032\u223cp(x,x\u2032)EzS\u223cp(zS |xx\u2032) log p(zS ,x|x\u2032)\np(zS |x\u2032)p(x|x\u2032)\n= Ex,x\u2032\u223cp(x,x\u2032)EzS\u223cp(zS |xx\u2032)) log p(zS |xx\u2032) p(zS |x\u2032)\n= Ex,x\u2032\u223cp(x,x\u2032)EzS\u223cp(zS |x) log p(zS |x) p(zS |x\u2032) = Ex,x\u2032\u223cp(x,x\u2032)EzS\u223cp(zS |x) log p(zS |x)p(z\u2032S |x\u2032) p(z\u2032S |x\u2032)p(zS |x\u2032) = Ex,x\u2032\u223cp(x,x\u2032)[DKL[p(zS |x)||p(z\u2032S |x\u2032)]\u2212DKL[p(zS |x\u2032)p(z\u2032S |x\u2032)]] \u2264 Ex,x\u2032\u223cp(x,x\u2032)[DKL[p(zS |x)||p(z\u2032S |x\u2032)]]\n(9)"
        },
        {
            "heading": "C ALGORITHM TABLE OF INFOR-LSF",
            "text": "The framework of InfoR-LSF is shown in Algorithm 1.\nAlgorithm 1 InfoR-LSF Input: Input data x \u2208 RM and label y. A model M consisting with a backbone network f\u03b8(\u00b7) and two variational encoders g\u03d5(\u00b7) and g\u03c8(\u00b7). Training epochs T . Feature suppression coefficient \u03b1 and VIB coefficient \u03b2. Output: Final model M.\n1: Train mainline feature zM = g\u03d5(f\u03b8(x)) with VIB loss LVIB(x, zM , \u03b8, \u03d5) at the first epoch. 2: for t = 1, 2, ...., T \u2212 1 do 3: Find salient input feature of mainline feature zM as xsf = topKx\u2208x ||\u2207xL(g\u03d5(f\u03b8(x)),y)|| 4: Erase salient feature xsf from x to obtain erased input x\u2032. 5: Jointly train mainline feature zM = g\u03d5(f\u03b8(x)) and supplemental feature zS = g\u03c8(f\u03b8(x)) by L = LVIB(x, zM , \u03b8, \u03d5) + LVIB(x, zS , \u03b8, \u03c8) + \u03b1 \u00b7 LIS 6: end for 7: return Final model M"
        },
        {
            "heading": "D MOTIVATIONS FOR INFORMATION RETENTION PRINCIPLE",
            "text": "D.1 FROM THE PERSPECTIVE OF CAUSAL DIAGRAM\nTo motivate the information retention principle, let us investigate the causal diagram in Figure 6. Since the common cause h are latent and unobserved, correlation exists between the input x and the label y. Traditional machine learning or deep learning methods usually exploits the correlation to make prediction are based on the correlation, with y\u0302 = f(x) as the predicted label for the input data x. At the level of causal mechanism, the task of inferring y from x consists of two steps: to infer the posterior of latent cause h of the observed input x,and then to pre-\ndict y based on p(h|x). During this process, taking more input features of x into consideration is expected to yield better estimation of h, although some components of h may have no effect on y.\nD.2 A TOY EXAMPLE TO ILLUSTRATE THE IDEA OF INFORMATION RETENTION\nx1 x2 x3 x4 y 1 1 1 2 True 1 1 1 2 True 0 2 2 2 True 0 3 2 2 False 1 2 2 1 False 0 3 2 1 False\nthe label and have medium predictive ability. However, since f1 has captured all the information of the label y, taking f2 or f3 into consideration will not bring any lifting in predictive ability but will lead to increased mutual information with the input. Thus, both f2 and f3 are suppressed and discarded by the information bottleneck principle.\nFinally, it comes to the test time and a new data [x1 = 1, x2 = 3, x3 = 1, x4 = 2] arrives, which has the feature f1 = 4. The classification model based on f1 has not seen this feature value in the training phase and cannot make a reliable prediction of label. However, feature f2 and feature f3 can deal with this situation, and both of them supports the prediction of y = True.\nTherefore, it is beneficial and desirable to endow with the ability of relieve redundant relevant features from the suppression of existing mainline features. Such redundant relevant features is supplemental to the mainline features.\nFurthermore, given the model that heavily relies on f1 (called the mainline feature) and thus attends to the input features x1 and x2, one possible solution to build up some new supplemental features by requiring them to capture the least information that has already been captured by mainline features, and simultaneously maximize their mutual information with the label."
        },
        {
            "heading": "E AN EXAMPLE FOR VISUALIZATION OF LEARNED FEATURES",
            "text": "Figure 7 shows an example from IMDB test set. We compute the gradient norms on input token embedding sequences and normalize them over the whole sentence. The three lines respectively represent the gradient distribution of BERT, zM head and zS head of InfoR-LSF. The depth of the color represents the magnitude of the gradient. It can be observed that with the regularization term LIS, zS partly erases salient features of zM (\u201dmovie a 1 .\u201d) and learns new features(\u201dmade me sick .\u201d) which are also helpful for classification. In addition, from the perspective of out-of-domain generalization, \u201dmovie a 1 .\u201d is a task-specific feature while \u201dmade me sick\u201d is a more general feature, this can also explain the better out-of-domain performance of InfoR-LSF."
        }
    ],
    "year": 2023
}