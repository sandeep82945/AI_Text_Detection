{
    "abstractText": "Recent empirical evidence indicates that transformer based in-context learning performs better when using a prefix language model (prefixLM), in which incontext samples can all attend to each other, compared to causal language models (causalLM), which use auto-regressive attention that prohibits in-context samples to attend to future samples. While this result is intuitive, it is not understood from a theoretical perspective. In this paper we take a theoretical approach and analyze the convergence behavior of prefixLM and causalLM under a certain parameter construction. Our analysis shows that both LM types converge to their stationary points at a linear rate, but that while prefixLM converges to the optimal solution of linear regression, causalLM convergence dynamics follows that of an online gradient descent algorithm, which is not guaranteed to be optimal even as the number of samples grows infinitely. We supplement our theoretical claims with empirical experiments over synthetic and real tasks and using various types of transformers. Our experiments verify that causalLM consistently underperforms prefixLM in all settings.",
    "authors": [
        {
            "affiliations": [],
            "name": "Nan Ding"
        },
        {
            "affiliations": [],
            "name": "Tomer Levinboim"
        },
        {
            "affiliations": [],
            "name": "Jialin Wu"
        },
        {
            "affiliations": [],
            "name": "Sebastian Goodman"
        },
        {
            "affiliations": [],
            "name": "Radu Soricut"
        }
    ],
    "id": "SP:7d235f9173b9901c4dcb91b6c77ac3e016301fe2",
    "references": [
        {
            "authors": [
                "Ekin Aky\u00fcrek",
                "Dale Schuurmans",
                "Jacob Andreas",
                "Tengyu Ma",
                "Denny Zhou"
            ],
            "title": "What learning algorithm is in-context learning? investigations with linear models",
            "venue": "arXiv preprint arXiv:2211.15661,",
            "year": 2022
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Amelot",
                "Kenton Lee",
                "Andreas Steiner",
                "Yang Li",
                "Daniel Keysers",
                "Anurag Arnab",
                "Yuanzhong Xu",
                "Keran Rong",
                "Alexander Kolesnikov",
                "Mojtaba Seyedhosseini",
                "Anelia Angelova",
                "Xiaohua Zhai",
                "Neil Houlsby",
                "Radu Soricut"
            ],
            "title": "Pali-x: On scaling up a multilingual vision and language model",
            "venue": "ArXiv, abs/2305.18565,",
            "year": 2023
        },
        {
            "authors": [
                "Aakanksha Chowdhery",
                "Sharan Narang",
                "Jacob Devlin",
                "Maarten Bosma",
                "Gaurav Mishra",
                "Adam Roberts",
                "Paul Barham",
                "Hyung Won Chung",
                "Charles Sutton",
                "Sebastian Gehrmann"
            ],
            "title": "Palm: Scaling language modeling with pathways",
            "venue": "arXiv preprint arXiv:2204.02311,",
            "year": 2022
        },
        {
            "authors": [
                "Hyung Won Chung",
                "Le Hou",
                "Shayne Longpre",
                "Barret Zoph",
                "Yi Tay",
                "William Fedus",
                "Eric Li",
                "Xuezhi Wang",
                "Mostafa Dehghani",
                "Siddhartha Brahma"
            ],
            "title": "Scaling instruction-finetuned language models",
            "venue": "arXiv preprint arXiv:2210.11416,",
            "year": 2022
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova. Bert"
            ],
            "title": "Pre-training of deep bidirectional transformers for language understanding",
            "venue": "arXiv preprint arXiv:1810.04805,",
            "year": 2018
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "Lucas Beyer",
                "Alexander Kolesnikov",
                "Dirk Weissenborn",
                "Xiaohua Zhai",
                "Thomas Unterthiner",
                "Mostafa Dehghani",
                "Matthias Minderer",
                "Georg Heigold",
                "Sylvain Gelly"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
            "venue": "arXiv preprint arXiv:2010.11929,",
            "year": 2020
        },
        {
            "authors": [
                "Shivam Garg",
                "Dimitris Tsipras",
                "Percy S Liang",
                "Gregory Valiant"
            ],
            "title": "What can transformers learn in-context? a case study of simple function classes",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Daniel Sohn",
                "Simon Tokumine",
                "Dasha Valter",
                "Vijay Vasudevan",
                "Kiran Vodrahalli",
                "Xuezhi Wang",
                "Pidong Wang",
                "Zirui Wang",
                "Tao Wang",
                "John Wieting",
                "Yuhuai Wu",
                "Kelvin Xu",
                "Yunhan Xu",
                "Linting Xue",
                "Pengcheng Yin",
                "Jiahui Yu",
                "Qiao Zhang",
                "Steven Zheng",
                "Ce Zheng",
                "Weikang Zhou",
                "Denny Zhou",
                "Slav Petrov",
                "Yonghui Wu"
            ],
            "title": "Palm 2 technical report, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Collin Burns",
                "Steven Basart",
                "Andy Zou",
                "Mantas Mazeika",
                "Dawn Song",
                "Jacob Steinhardt"
            ],
            "title": "Measuring massive multitask language understanding",
            "venue": "arXiv preprint arXiv:2009.03300,",
            "year": 2020
        },
        {
            "authors": [
                "Arnulf Jentzen",
                "Philippe Von Wurstemberger"
            ],
            "title": "Lower error bounds for the stochastic gradient descent optimization algorithm: Sharp convergence rates for slowly and fast decaying learning rates",
            "venue": "Journal of Complexity,",
            "year": 2020
        },
        {
            "authors": [
                "Andrej Karpathy",
                "Li Fei-Fei"
            ],
            "title": "Deep visual-semantic alignments for generating image descriptions",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2015
        },
        {
            "authors": [
                "Sewon Min",
                "Xinxi Lyu",
                "Ari Holtzman",
                "Mikel Artetxe",
                "Mike Lewis",
                "Hannaneh Hajishirzi",
                "Luke Zettlemoyer"
            ],
            "title": "Rethinking the role of demonstrations: What makes in-context learning work",
            "venue": "arXiv preprint arXiv:2202.12837,",
            "year": 2022
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J. Liu"
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "Journal of Machine Learning Research,",
            "year": 2020
        },
        {
            "authors": [
                "Colin Raffel",
                "Noam Shazeer",
                "Adam Roberts",
                "Katherine Lee",
                "Sharan Narang",
                "Michael Matena",
                "Yanqi Zhou",
                "Wei Li",
                "Peter J Liu"
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2020
        },
        {
            "authors": [
                "Lee",
                "Dan Garrette",
                "James Lee-Thorp",
                "Colin Raffel",
                "Noam Shazeer",
                "Marvin Ritter",
                "Maarten Bosma",
                "Alexandre Passos",
                "Jeremy Maitin-Shepard",
                "Noah Fiedel",
                "Mark Omernick",
                "Brennan Saeta",
                "Ryan Sepassi",
                "Alexander Spiridonov",
                "Joshua Newlan",
                "Andrea Gesmundo"
            ],
            "title": "Scaling up models and data with t5x and seqio",
            "venue": "arXiv preprint arXiv:2203.17189,",
            "year": 2022
        },
        {
            "authors": [
                "Piyush Sharma",
                "Nan Ding",
                "Sebastian Goodman",
                "Radu Soricut"
            ],
            "title": "Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning",
            "venue": "In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics",
            "year": 2018
        },
        {
            "authors": [
                "Mirac Suzgun",
                "Nathan Scales",
                "Nathanael Sch\u00e4rli",
                "Sebastian Gehrmann",
                "Yi Tay",
                "Hyung Won Chung",
                "Aakanksha Chowdhery",
                "Quoc V Le",
                "Ed H Chi",
                "Denny Zhou"
            ],
            "title": "Challenging big-bench tasks and whether chain-of-thought can solve them",
            "venue": "arXiv preprint arXiv:2210.09261,",
            "year": 2022
        },
        {
            "authors": [
                "Yi Tay",
                "Mostafa Dehghani",
                "Vinh Q Tran",
                "Xavier Garcia",
                "Dara Bahri",
                "Tal Schuster",
                "Huaixiu Steven Zheng",
                "Neil Houlsby",
                "Donald Metzler"
            ],
            "title": "Unifying language learning paradigms",
            "venue": "arXiv preprint arXiv:2205.05131,",
            "year": 2022
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141 ukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Johannes Von Oswald",
                "Eyvind Niklasson",
                "Ettore Randazzo",
                "Jo\u00e3o Sacramento",
                "Alexander Mordvintsev",
                "Andrey Zhmoginov",
                "Max Vladymyrov"
            ],
            "title": "Transformers learn in-context by gradient descent",
            "venue": "In International Conference on Machine Learning,",
            "year": 2023
        },
        {
            "authors": [
                "Jerry Wei",
                "Jason Wei",
                "Yi Tay",
                "Dustin Tran",
                "Albert Webson",
                "Yifeng Lu",
                "Xinyun Chen",
                "Hanxiao Liu",
                "Da Huang",
                "Denny Zhou"
            ],
            "title": "Larger language models do in-context learning differently",
            "venue": "arXiv preprint arXiv:2303.03846,",
            "year": 2023
        },
        {
            "authors": [
                "Sang Michael Xie",
                "Aditi Raghunathan",
                "Percy Liang",
                "Tengyu Ma"
            ],
            "title": "An explanation of in-context learning as implicit bayesian inference",
            "venue": "arXiv preprint arXiv:2111.02080,",
            "year": 2021
        },
        {
            "authors": [
                "Ruiqi Zhang",
                "Spencer Frei",
                "Peter L Bartlett"
            ],
            "title": "Trained transformers learn linear models in-context",
            "venue": "arXiv preprint arXiv:2306.09927,",
            "year": 2023
        }
    ],
    "sections": [
        {
            "text": "Recent empirical evidence indicates that transformer based in-context learning performs better when using a prefix language model (prefixLM), in which incontext samples can all attend to each other, compared to causal language models (causalLM), which use auto-regressive attention that prohibits in-context samples to attend to future samples. While this result is intuitive, it is not understood from a theoretical perspective. In this paper we take a theoretical approach and analyze the convergence behavior of prefixLM and causalLM under a certain parameter construction. Our analysis shows that both LM types converge to their stationary points at a linear rate, but that while prefixLM converges to the optimal solution of linear regression, causalLM convergence dynamics follows that of an online gradient descent algorithm, which is not guaranteed to be optimal even as the number of samples grows infinitely. We supplement our theoretical claims with empirical experiments over synthetic and real tasks and using various types of transformers. Our experiments verify that causalLM consistently underperforms prefixLM in all settings."
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "Transformer-based models (Vaswani et al., 2017) have become the default foundational model for various machine learning applications such as natural language processing (Devlin et al., 2018; Brown et al., 2020; Chowdhery et al., 2022) and computer vision (Dosovitskiy et al., 2020). Beyond their traditional usage in machine learning applications, it has recently been discovered that pretraining large transformers on a vast amounts of data leads them to develop a striking ability referred to as in-context learning (ICL) (Brown et al., 2020). Specifically, once such pretraining is complete, these models are able to solve new tasks at inference time (without changing their parameters) by simply ingesting a short sequence (prefix) of labeled examples from a task and then computing a prediction for a query example.\nThe ICL capability was first demonstrated by GPT-3 (Brown et al., 2020), where a causalLM (a Transformer decoder with auto-regressive attention masks) was used as the main model architecture. However, follow up work empirically found that restricting the auto-regressive masks on the entire sequence is too prohibitive and therefore proposed the so-called prefixLM (Raffel et al., 2020b; Tay et al., 2022) which allows full attention within the prefix tokens. Moreover, the latest models (such as PaLM2 (Google et al., 2023)) adopt a mixture of different LM objectives during pretraining to achieved state-of-art performance across a diverse set of tasks and capabilities.\nHowever, beyond the few empirical results in those and related papers, there is yet no theoretical explanation that accounts for the different ICL behavior of prefixLM and causalLM. Indeed, theoretical studies of ICL are difficult due to the complicated non-linearity of the (ordinary) transformer architecture. However, recent work (Von Oswald et al., 2023) focusing on ICL of linear regression was able to show that a specifically designed parameter construction of a one-layer Linear Self-Attention (LSA) transformer can simulate a single step of gradient descent by using the in-context examples as training data. Moreover, a different recent study (Zhang et al., 2023) used gradient flow to prove that a randomly initialized LSA-transformer indeed converges to such a construction during training.\nIn this paper, we continue the theoretical line of work above by investigating the convergence properties of ICL for both prefixLM and causalLM multi-layer LSA-transformers in a linear regression setting. We summarizes our contributions as follows:\n\u2022 We first present a clear, formal proof that establishes the relationship between a multi-layer LSA and multi-step gradient descents in linear regression.\n\u2022 We then show that both causalLM and prefixLM based multi-layer LSA-transformers converge to their respective stationary points with linear rates of convergence. We prove that the stationary point of prefixLM corresponds to the optimal least square solution of the linear regression problem, while the stationary points of causalLM correspond to the weights obtained along the iterations of online gradient descent with non-decaying step sizes. Importantly, the stationary points obtained by causalLM may not become optimal even as the number of in-context examples increases, which indicates that causalLM is not optimal for in-context learning.\n\u2022 Finally, we verify the above theoretical insights by conducting experiments with LSAtransformers as well as ordinary softmax attention based transformers on various synthetic tasks including linear and non-linear regression, and multiclass classifications. We also compare causalLM and prefixLM ICL based on LLMs including T5 (Roberts et al., 2022) and PaLM2 (Google et al., 2023), as well as the multimodal model PaLI-X (Chen et al., 2023). Our experimental results support our theoretical findings and consistently show the superiority of prefixLM over causalLM on ICL for such settings."
        },
        {
            "heading": "2 BACKGROUND",
            "text": "We begin by reviewing a few types of transformer attention and in-context learning (ICL), as well as a specific transformer construction for linear regression ICL by (Von Oswald et al., 2023) which our theories will be based on. The discussions of other related work are deferred to Appendix A."
        },
        {
            "heading": "2.1 TRANSFORMERS: SSA, LSA, CAUSALLM, AND PREFIXLM",
            "text": "Given a sequence of input vectors Z = (z1, . . . , zn), the output of standard Softmax Self-Attention (SSA) layer is\nzj \u2190 zj +PVZ softmax(Z\u22a4 K\u22a4 Qzj),\nwhere P,V,K,Q respectively corresponds to the output projection, value transformation, key transformation and query transformation.\nSince the softmax attention of standard transformers is non-linear, its theoretical analysis becomes complicated even for a single layer. For this reason, theoretical approaches to analyze transformers have often resorted to the Linear Self-Attention (LSA) layer (Von Oswald et al., 2023; Zhang et al., 2023), which simply drops the softmax function from the attention,\nzj \u2190 zj +PVZ(Z\u22a4 K\u22a4 Qzj) = zj +PV n\u2211\ni=1\nzi ( z\u22a4i K \u22a4 Qzj ) . (1)\nFurthermore, since each input zj can attend to all positions j \u2208 {1 . . . n}, this form of attention is categorized as full (or bidirectional) attention, and is typically used in the transformer encoder.\nOn the other hand, a (linear) transformer decoder uses the auto-regressive attention\nzj \u2190 zj +PV j\u2211\ni=1\nzi ( z\u22a4i K \u22a4 Qzj ) . (2)\nwhich restricts each token zj to attend only to previous positions (and itself) from {1 . . . j}. This restriction is due to the role of the decoder as a causal language model (causalLM) which predicts the next token in the context of the previously generated ones.\nThe original transformer involves both a full attention based encoder and an auto-regressive attention based decoder. However, prominent NLP research has often chosen either encoder-only (e.g.\nBERT (Devlin et al., 2018)) or decoder-only (e.g. GPT (Brown et al., 2020), PaLM (Chowdhery et al., 2022)) models according to the task at hand. This is partially for the purpose of halving the parameter sizes.\nAnother version of attention, between full and auto-regressive, followed from the observation that some tasks can benefit from a prefix sequence such as context or prompt. That is, the input sequence Z is composed of n\u2032 prefix tokens (z1, . . . , zn\u2032) configured for the task, while the tokens (zn\u2032+1, . . . , zn) represent the sample. Specifically, prefixLM (Raffel et al., 2020b) suggests the following attention (in its LSA version):\nzj \u2190 zj +PV max(j,n\u2032)\u2211\ni=1\nzi ( z\u22a4i K \u22a4 Qzj ) ,\nwhere max(j, n\u2032) ensures each prefix token zj with j < n\u2032 can attend to all prefix tokens."
        },
        {
            "heading": "2.2 IN-CONTEXT LEARNING",
            "text": "A formal framework of in-context learning has been described in various existing literature such as (Garg et al., 2022; Zhang et al., 2023). Here, we briefly review the problem setting and introduce notation that will be used across the paper.\nIn-context learning refers to the ability of models to produce context-driven predictions at inference time. That is, at inference time, a model is fed with a sequence consisting of input-label pairs and a query input (x1, y1, . . . ,xn, yn,xquery) and its goal is to predict the label yquery of xquery using the context examples (x1, y1, . . . ,xn, yn) (specifically, without changing the model parameters)."
        },
        {
            "heading": "2.3 LINEAR REGRESSION IN-CONTEXT LEARNERS",
            "text": "Linear regression is a classical machine learning problem. Given a set of input-label pairs (xi, yi), the goal is to find an optimal weight vector w that minimizes the l2-loss:\nL(w) = 1\n2n n\u2211 i=1 \u2225wxi\u2212yi\u222522.\nThe gradient of the loss is \u2207wL = 1n \u2211n\ni=1(wxi\u2212yi)x\u22a4i , and a gradient descent algorithm with step size \u03b7 follows the update rule:\nw(l) =w(l\u22121) + \u03b7\nn n\u2211 i=1 (yi \u2212w(l\u22121) xi)x\u22a4i . (3)\nUsing linear regression as a lens to study in-context learning was first proposed in (Garg et al., 2022), where the authors laid out an approach for training transformers to in-context learn a class of simple predictors, including linear regression. However, no theoretical study was provided. More recently, and most relevant to our work, (Von Oswald et al., 2023) proposed a succinct construction that demonstrates how a single LSA layer can effectively implement a single gradient descent step. According to their setup the input is formulated as\nZ = (z (0) 1 , . . . , z (0) n ), where z (0) j = ( xj yj ) (4)\nand the parameter matrices of (1) are set as:\nK = Q = ( Id\u00d7d 0 0 0 ) ,V = ( 0d\u00d7d 0 w(0) \u22121 ) ,P = \u03b7 n I, (5)\nwhere w(0) is an initial weight vector. (Von Oswald et al., 2023) then showed that this configuration results in an update of their so-called transformed target yj \u2190 yj + \u03b7 (\u2207w(0)L)xj , and that this target update is equivalent to the one performed by a single-step gradient descent of linear regression.\nAlthough the construction of (Von Oswald et al., 2023) connected LSA-based ICL to the gradient descent of linear regression, the \"transformed target\" view seems unnatural* to work with. Moreover, their extension from single-layer to multi-layer LSA is unfortunately unclear.\n*The traditional ML formulation updates the weight vector or the model prediction, while the groundtruth target remains fixed."
        },
        {
            "heading": "3 MULTI-LAYER IN-CONTEXT LEARNER",
            "text": "In this section, we provide a formal proof that a multi-layer LSA under the construction of (Von Oswald et al., 2023) progresses identically to multi-step gradient descent.\nInstead of the \"transformed target\" view, the following proposition explicitly connects the GD weights of (3) to the outputs of the multi-layer LSA under the constructions of K, Q, P and V in (5). Note that we keep w(0) = 0 in the proposition because it simplifies the equations and makes the outputs more meaningful. However, such specification is not mandatory, and we provide general propositions, for arbitrary w(0), in Appendix C.\nProposition 1 For a multi-layer LSA satisfying the construction (5) and with w(0) = 0, if its input Z is formatted as (4), then its l-th layer output is z(l)j = (x \u22a4 j , \u03b4 (l) j )\n\u22a4, where \u03b4(l)j = yj \u2212w(l) xj and w(l) is the l-th updated weight from the gradient descents update rule in (3).\nProof Sketch: Plugging in K, Q, P and V of (5) with w(0) = 0 and z(l)j = (x \u22a4 j , \u03b4 (l) j ) \u22a4 into (1), we obtain that for all l > 0, (\nxj \u03b4 (l) j\n) = ( xj\n\u03b4 (l\u22121) j\n) \u2212 \u03b7\nn n\u2211 i=1 ( 0 \u03b4 (l\u22121) i ) x\u22a4i xj .\nSince zj never changes its first d-dimension corresponding to xj , we can simplify it and focus only on \u03b4(l)j , which is the last output coordinate of the j-th LSA-layer,\n\u03b4 (l) j = \u03b4 (l\u22121) j \u2212\n\u03b7\nn n\u2211 i=1 \u03b4 (l\u22121) i x \u22a4 i xj , (6)\nwith \u03b4(0)j = yj . Defining y\u0303 (l) j = yj \u2212 \u03b4 (l) j and rearranging (6), we obtain y\u0303 (0) j = 0 and \u2200l > 0:\ny\u0303 (l) j = y\u0303 (l\u22121) j +\n\u03b7\nn n\u2211 i=1 (yi \u2212 y\u0303(l\u22121)i )x \u22a4 i xj . (7)\nFinally, using (7) and the fact that y\u0303(0)j = 0 = w (0) xj , it can be proved by induction that \u2200l : y\u0303(l)j = w(l) xj . A complete proof is provided in Appendix B.\nTo summarize, the newly introduced variable y\u0303(l)j is exactly the prediction of the l-th gradient descent weights w(l) for xj , and \u03b4 (l) j is the difference between the true label yj and the predicted y\u0303 (l) j . Therefore, y\u0303(l)j serves as a bridge to connect the LSA output \u03b4 (l) j and the GD weight w (l).\nSo far, we have dealt with the behavior of LSA layers with full attention. In what follows, we move on to the practical setting of in-context learning, where the input contains not only n in-context (training) examples in the format of (4), but also an additional (test) query z(0)query = (x\u22a4query, 0)\n\u22a4. In particular, we will focus on the two common ICL variants: prefixLM and causalLM, each with a different type of attention."
        },
        {
            "heading": "3.1 PREFIXLM ICL",
            "text": "A prefixLM ICL treats the in-context examples Z as the prefix and uses full attention on the first n positions, so that they can each freely attend to each other. The last query vector zquery can also attend to any example in Z, but cannot attend to itself\u2020. As a result, the updates of the prefixLM-ICL under the same construction follow (6), with the outputs of the l-th layer being,\n\u03b4 (l) j = yj \u2212 y\u0303 (l) j = yj \u2212w (l) xj ,\nand \u03b4(l)query = \u2212y\u0303(l)query = \u2212w(l) xquery, \u2020This is because the query does not contain a meaningful label. Attending to itself would cause it to include its last-dim input as a label, which would contaminate the resulting multi-layer prediction. This observation was not considered in (Von Oswald et al., 2023).\nwhere the initial y\u0303(0)j = y\u0303 (0) query = 0.\nIntuitively, the dynamics of the prefixLM ICL is as follows: all y\u0303(l)j starts as 0 at l = 0, and gradually approach to the true label yj as l increases, so that the difference (also as the output) \u03b4 (l) j gradually approaches to 0. At the same time, \u03b4(l)query starts at 0, and gradually approaches to \u2212yquery, the negation of the query label. Figure 1 provides an illustration of these dynamics."
        },
        {
            "heading": "3.2 CAUSALLM ICL",
            "text": "A causalLM applies auto-regressive attention throughout the entire sequence. Therefore, plugging the same K, Q, P, V into (2), the update rules of (6) and (7) become:\n\u03b4 (l) j = \u03b4 (l\u22121) j \u2212\n\u03b7\nn j\u2211 i=1 \u03b4 (l\u22121) i x \u22a4 i xj , (8)\ny\u0303 (l) j = y\u0303 (l\u22121) j +\n\u03b7\nn j\u2211 i=1 (yi \u2212 y\u0303(l\u22121)i )x \u22a4 i xj (9)\n\u2021with \u03b4(l)j = yj \u2212 y\u0303 (l) j . Moreover, since different \u03b4j , y\u0303j are exposed to different ranges of inputs, there is no uniform w as in (3) that is associated with all y\u0303j . Instead, if we define wj for each different position j with w(0)j = 0 and\nw (l) j = w (l\u22121) j +\n\u03b7\nn j\u2211 i=1 (yi \u2212w(l\u22121)i xi)x \u22a4 i (10)\nthen we have the following proposition:\nProposition 2 For a multi-layer causalLM-LSA satisfying (5) with w(0) = 0, if its input Z is formatted as (4), then its l-th layer output is z(l)j = (x \u22a4 j , \u03b4 (l) j ) \u22a4, where \u03b4(l)j = yj \u2212w (l) j xj and w (l) j follow (10).\nThe proof of Proposition 2 is provided in Appendix B. Similar to prefixLM-ICL, causalLM-ICL also has y\u0303(l)j = w (l) j xj , and\n\u03b4(l)query = \u2212y\u0303(l)query = \u2212w(l)n xquery .\nIn summary, causalLM-ICL and prefixLM-ICL are associated with different update rules: w(l)j follows (10) while w(l) follows (3). Specifically, in causalLM, it can be seen that the w(l\u22121)i\n\u2021There is another way of update which changes \u03b7/n to \u03b7/j for the j-th example. We provide more details in Appendix D and show it performs worse than the main version in (8).\ncorresponding to the first positions are biased due to restricted access to only a few data points and furthermore, that these biases are propagated to later positions by (10). In prefixLM on the other hand, each position has access to all the data and a single w(l) can be used across the entire sequence as in (3). Although Eq. (3) and Eq. (10) only hold for the structured LSA case, the profound difference between causalLM and prefixLM stems from their architectural difference and therefore we believe extends to general transformers, as indicated by our experimental results in Section 5."
        },
        {
            "heading": "4 CONVERGENCE OF THE MULTI-LAYER IN-CONTEXT LEARNERS",
            "text": "In this section, we prove that both multi-layer prefixLM and causalLM converge to their respective stationary points with increasing layers (and with linear rates). In addition, we show that the stationary point of prefixLM corresponds to the optimal least-square solution of the linear regression problem, while the ones corresponding to causalLM are equivalent to the iterative weights of online gradient descent of linear regression, which are known to be sub-optimal for a limited number of examples."
        },
        {
            "heading": "4.1 CONVERGENCE OF THE PREFIXLM ICL",
            "text": "The fact that a multi-layer prefixLM computation exactly follows the update rule of w(l) as in (3), implies that the layer outputs of prefixLM have the same dynamics of multi-step gradient descent on a linear regression problem. The convergence properties of such dynamics are well-known, and are stated in the following proposition:\nProposition 3 If w(l) follows the iterative updates of (3), then there exists a stationary point w\u2217 with coefficients satisfying:\nyX\u22a4 = w\u2217 XX\u22a4,\nwhere y = (y1, . . . , yn) and X = (x1, . . . ,xn). Furthermore, the iterative weights w(l) converge to w\u2217 with a linear rate of convergence:\nw(l)\u2212w\u2217 = (w(l\u22121)\u2212w\u2217)(I\u2212 \u03b7 n XX\u22a4).\nThat is, Proposition 3 holds for the multi-layer prefixLM, so that the same exact w\u2217 is also the stationary point of prefixLM, to which it converges in a linear rate. Furthermore this stationary point is exactly the (optimal) least square solution of the linear regression problem."
        },
        {
            "heading": "4.2 CONVERGENCE OF THE CAUSALLM ICL",
            "text": "Following the update rule of (10), we can view a multi-layer causalLM as implicitly maintaining different weight vectors wj for each position j. In what follows, we show that: (a) Each such position j has its own stationary point w\u2217j , which appears to be different from the global optimal point w\n\u2217 of linear regression; (b) even when the number of in-context samples n grows to infinity, convergence to w\u2217 is not guaranteed.\nSpecifically, in Appendix B we provide a proof for the following proposition: Proposition 4 If w(l)j = \u2211j i=1 a (l) i,j x \u22a4 i follows the iterative updates of (10), then\na (l) i,j = a (l) i,i \u2261 a (l) i \u2200j \u2265 i, and there exist stationary points w\u2217j = \u2211j i=1 a \u2217 i x \u22a4 i (for j \u2208 1, . . . , n) with coefficients from a\u2217 = (a\u22171, . . . , a \u2217 n) that satisfy y = a \u2217 T, where\nT =  x\u22a41 x1 x \u22a4 1 x2 \u00b7 \u00b7 \u00b7 x\u22a41 xn 0 x\u22a42 x2 \u00b7 \u00b7 \u00b7 x\u22a42 xn ... ... . . .\n... 0 0 \u00b7 \u00b7 \u00b7 x\u22a4n xn  . Furthermore, the coefficients a(l) converges to the stationary point a\u2217 with linear rate of convergence:\na(l)\u2212a\u2217 = (a(l\u22121)\u2212a\u2217)(I\u2212 \u03b7 n T).\nThis proposition implies that the stationary points w\u2217j of causalLM-ICL are different from w \u2217, the least square solution of linear regression. However, a natural question is: if j increases, would w\u2217j ultimately converge to the optimal solution?\nTo answer this question, the next proposition shows that the stationary points w\u2217j follow an online gradient descent algorithm, whose loss and gradient at the j-th step is,\nLj(wj) = 1\n2 (wj xj+1\u2212yj+1)2,\n\u2207wjLj(wj) = (wj xj+1\u2212yj+1)x\u22a4j+1 .\nProposition 5 Assuming that w\u2217j is the stationary points obtained in Proposition 4, then\nw\u2217j+1 = w \u2217 j \u2212\n1\n\u2225xj+1 \u222522 \u2207w\u2217jLj(w \u2217 j ). (11)\nThe proof of Proposition 5 is provided in Appendix B. Note that online gradient descent is known to converge to an optimal solution only with a decaying step size j\u2212\u03bd for \u03bd > 0 (Jentzen & Von Wurstemberger, 2020). Since the step size of (11) does not decay, we conclude that causalLM may not converge to w\u2217 even with increasing layers and increasing number of in-context examples. More concretely, as for the case of in-context learning, where the number of in-context examples n is limited, convergence to the optimal solution w\u2217 cannot be achieved by causalLM-ICL."
        },
        {
            "heading": "5 NUMERICAL EXPERIMENTS",
            "text": "Our experiments contain three parts.\n\u2022 We first use LSA-transformers on linear regression problems to directly verify our theorems. In Section 5.1, we show that despite that the in-context example (training) error of causalLM and prefixLM both decays in linear rates, the query (test) error of causalLM is significantly larger, which indicates its stationary solution is not optimal.\n\u2022 Secondly, we use ordinary softmax transformers on a few synthetic tasks including linear regression, nonlinear regression and multiclass classification. In Section 5.2, we show that our theoretical insights generalize to other tasks types (i.e., that ICL prefixLM still outperforms causalLM in all these cases).\n\u2022 Lastly, in Section 5.3, we conduct LLM based ICL experiments using T5 (Roberts et al., 2022). We also provide additional experimental results on PaLM2 (Google et al., 2023) as well as large multimodal models (PaLI-X, Chen et al. (2023)) in Appendix E.6 and E.7."
        },
        {
            "heading": "5.1 LSA-TRANSFORMERS ON LINEAR REGRESSION",
            "text": "In order to directly verify our theorems from Section 4, we first study in-context learning on linear regression problem with the LSA transformer of (5). Each of the input sequence contains 40 incontext examples and 200 queries, and each query attends to all the in-context examples but does not attend to each other. See Appendix E for an illustration. The data input xi of the sequence is sampled from U(\u22121, 1)16. Each sequence is associated with a single weight vector w that is sampled from N (0, I), and the labels are computed as yi = wxi. Assuming the prediction of each layer is y\u0303(l)i , we evaluate the MSE \u2225yi \u2212 y\u0303(l)i \u222522 on both in-context and query examples across different layers l. The results are plotted in Figure 2 left (for prefixLM) and middle (for causalLM). Our results are averaged over 64 randomly generated sequences. As we can see, although both prefixLM and causalLM has a linear rate of convergence (with respect to the number of layers) on the in-context examples, the query errors of causalLM are stuck above the 10\u22121 level, while the query error of prefixLM decays in the same linear rate as its training error.\nFurthermore, in Figure 2 right, we plot the query errors of the stationary points (following Proposition 4, corresponding to the outputs of infinite layers) of causalLM-ICL with increasing number of in-context examples up to 300. Although causalLM-ICL is able to eventually converge to optimal solution when \u00b5x = 0, it takes more than 100 examples to reach below 10\u22122. The convergence is\neven worse as we vary the input distribution x \u223c U(\u22121, 1)d + \u00b5x with increasing \u00b5x \u2208 {0, 1, 2, 3}, which demonstrates that causalLM-ICL is not optimal for few-shot ICL."
        },
        {
            "heading": "5.2 STANDARD TRANSFORMERS ON SYNTHETIC TASKS",
            "text": "Previous experiments provided a proof of concept verification of the propositions from Section 4. Next we verify if a standard softmax transformer-based prefixLM and causalLM ICL exhibit similar differences on various types of synthetic tasks including linear regression, non-linear regression and multiclass classification.\nAll three tasks used 16-dim inputs with x \u223c U(\u22121, 1)16 and w \u223c N (0, I). For non-linear regression, we apply a sigmoid activation on the logit such that y = sigmoid(wx); and for multiclass classification, we randomly generate three wc \u223c N (0, I), and assign labels based on y = argmaxc {wc x}. We trained a few 24-layer transformers containing 128 hidden units and 2 heads. Besides of the comparisons of prefixLM and causalLM, we also compare the transformers with or without sharing layers (SL vs UL). In particular, the sharing-layer transformer can be considered a recurrent system (Dehghani et al., 2018) where the dynamic is continual along the layers and a stationary point may exist given infinite number of layers, which makes it closer to our constructed LSA.\nThe ICL training dataset contains 64,000 training sequences. Each sequence contains 40 in-context examples and 20 queries, where queries are independent of each other similar to Section 5.1. The transformers are trained with batch size 64 for 100 epochs. More details of the hyper-parameters of the experiments are provided in Appendix E.\nWe evaluate the ICL performance using 64 holdout test sequences and report the test errors on the query examples. The results are summarized in Table 1. We find that both prefixLM-SL and prefixLM-UL significantly outperform their counterparts of causalLM in all cases. As a side note, transformer-SL appears to outperform transformer-UL in the classification tasks, which indicates the overfitting problem of the latter due to over-parameterization. In addition, we also add probes at the output of each SL-transformer layer to visualize the test errors of intermediate layers in Figure 3. Comparing Figure 3 and Figure 2 (left/middle) reveals some similarities. Although the test query errors of causalLM decay in roughly the same rate as the ones of prefixLM in earlier layers, the decays become much slower in later layers possibly due to the nature of its non-optimal stationary points. These results suggest that the title argument of the paper also holds beyond LSA-based transformers and linear regression."
        },
        {
            "heading": "5.3 ICL ON LARGE LANGUAGE MODELS",
            "text": "In order to compare the ICL performance of causalLM and prefixLM in a large language model setting, we conduct experiments using the publicly available T5 family of models (Roberts et al., 2022). Note that the existing public T5X \u00a7 checkpoints are all based on EncDec models, which are similar to prefixLM. Thus, it would be unfair and unnatural to compare with causalLM by simply replacing the bidirectional attention of the encoder to the causal attention during the finetuning stage. To make a more reasonable comparison, we reran the pretraining stages of T5 on the C4 corpus (Raffel et al., 2020a) from a random initialization point using a span corruption objective, but in the DecoderOnly setting. Moreover, for each size (from Base, Large and XL) of the models, we pretrained two checkpoints, one with prefixLM and the other with causalLM, each for 1M steps using the same T5 pretraining recipe. After pretraining, we use the FLAN recipe (Chung et al., 2022) to finetune each checkpoint (40k steps for Base, 20k steps for Large and XL) with its pretrained attention mask and evaluate the ICL capability of the finetuned models on two benchmarks: MMLU (Hendrycks et al., 2020) and BBH (Suzgun et al., 2022).\nTable 2 shows that for all three sizes of T5X DecoderOnly models, the MMLU and BBH accuracies of prefixLM surpasses that of causalLM consistently and such gap widens as the size of the model becomes larger. This result empirically verifies that our conjecture generalizes to the practical case. We supply additional empirical evidence on state-of-the-art models in Appendix E.6 and E.7."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "In this paper, we analyzed the convergence properties of two types of widely-used transformer-based language models (causalLM and prefixLM), during in-context learning. Using a simplified LSA attention in a linear regression setting, we proved that both LM types converge to their stationary points in linear rates, but that their stationary points have significantly different properties. In particular, the stationary points of prefixLM coincides with the optimal least square solution; while the ones of causalLM is equivalent to the weights of an online learning system, that is not guaranteed to converge to the optimal solution. Our experiments verified the above theoretical results, and also empirically extend the findings to general transformer on non-linear regression as well as classification tasks. Finally, we compare causalLM and prefixLM on a few large language models and find that prefixLM also consistently wins over causalLM in practical few-shot tasks.\n\u00a7https://github.com/google-research/t5x"
        },
        {
            "heading": "ACKNOWLEDGEMENTS",
            "text": "We want to specially thank Xinhua Zhang for helpful discussions and references about online learning. We also thank Yi Tay for comments regarding the PaLM2 checkpoints."
        }
    ],
    "title": "CAUSALLM IS NOT OPTIMAL FOR IN-CONTEXT LEARN- ING"
}