{
    "abstractText": "Offline Goal-Conditioned Reinforcement Learning (GCRL) is tasked with learning to achieve multiple goals in an environment purely from offline datasets using sparse reward functions. Offline GCRL is pivotal for developing generalist agents capable of leveraging pre-existing datasets to learn diverse and reusable skills without hand-engineering reward functions. However, contemporary approaches to GCRL based on supervised learning and contrastive learning are often suboptimal in the offline setting. An alternative perspective on GCRL optimizes for occupancy matching, but necessitates learning a discriminator, which subsequently serves as a pseudo-reward for downstream RL. Inaccuracies in the learned discriminator can cascade, negatively influencing the resulting policy. We present a novel approach to GCRL under a new lens of mixture-distribution matching, leading to our discriminator-free method: SMORe. The key insight is combining the occupancy matching perspective of GCRL with a convex dual formulation to derive a learning objective that can better leverage suboptimal offline data. SMORe learns scores or unnormalized densities representing the importance of taking an action at a state for reaching a particular goal. SMORe is principled and our extensive experiments on the fully offline GCRL benchmark composed of robot manipulation and locomotion tasks, including high-dimensional observations, show that SMORe can outperform state-of-the-art baselines by a significant margin.",
    "authors": [
        {
            "affiliations": [],
            "name": "Harshit Sikchi \u030a\u03b8"
        },
        {
            "affiliations": [],
            "name": "Rohan Chitnis"
        },
        {
            "affiliations": [],
            "name": "Ahmed Touati"
        },
        {
            "affiliations": [],
            "name": "Alborz Geramifard"
        },
        {
            "affiliations": [],
            "name": "Amy Zhang"
        },
        {
            "affiliations": [],
            "name": "Scott Niekum"
        }
    ],
    "id": "SP:a410490cdb02d706eebe405db65a41e98614b6b5",
    "references": [
        {
            "authors": [
                "A. Agarwal",
                "N. Jiang",
                "S.M. Kakade",
                "W. Sun"
            ],
            "title": "Reinforcement learning: Theory and algorithms",
            "venue": "CS Dept., UW Seattle,",
            "year": 2019
        },
        {
            "authors": [
                "S. Agarwal",
                "I. Durugkar",
                "P. Stone",
                "A. Zhang"
            ],
            "title": "f-policy gradients: A general framework for goal-conditioned rl using f-divergences",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2024
        },
        {
            "authors": [
                "F. Al-Hafez",
                "D. Tateo",
                "O. Arenz",
                "G. Zhao",
                "J. Peters"
            ],
            "title": "Ls-iq: Implicit reward regularization for inverse reinforcement learning",
            "venue": "arXiv preprint arXiv:2303.00599,",
            "year": 2023
        },
        {
            "authors": [
                "M. Andrychowicz",
                "F. Wolski",
                "A. Ray",
                "J. Schneider",
                "R. Fong",
                "P. Welinder",
                "B. McGrew",
                "J. Tobin",
                "O. Pieter Abbeel",
                "W. Zaremba"
            ],
            "title": "Hindsight experience replay",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Y. Chebotar",
                "K. Hausman",
                "Y. Lu",
                "T. Xiao",
                "D. Kalashnikov",
                "J. Varley",
                "A. Irpan",
                "B. Eysenbach",
                "R. Julian",
                "C. Finn"
            ],
            "title": "Actionable models: Unsupervised offline reinforcement learning of robotic skills",
            "venue": "arXiv preprint arXiv:2104.07749,",
            "year": 2021
        },
        {
            "authors": [
                "L. Chen",
                "R. Paleja",
                "M. Gombolay"
            ],
            "title": "Learning from suboptimal demonstration via self-supervised reward regression",
            "venue": "arXiv preprint arXiv:2010.11723,",
            "year": 2020
        },
        {
            "authors": [
                "Y. Ding",
                "C. Florensa",
                "P. Abbeel",
                "M. Phielipp"
            ],
            "title": "Goal-conditioned imitation learning",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "F. Ebert",
                "Y. Yang",
                "K. Schmeckpeper",
                "B. Bucher",
                "G. Georgakis",
                "K. Daniilidis",
                "C. Finn",
                "S. Levine"
            ],
            "title": "Bridge data: Boosting generalization of robotic skills with cross-domain datasets",
            "venue": "arXiv preprint arXiv:2109.13396,",
            "year": 2021
        },
        {
            "authors": [
                "B. Eysenbach",
                "R. Salakhutdinov",
                "S. Levine"
            ],
            "title": "C-learning: Learning to achieve goals via recursive classification",
            "venue": "arXiv preprint arXiv:2011.08909,",
            "year": 2020
        },
        {
            "authors": [
                "B. Eysenbach",
                "S. Udatha",
                "R.R. Salakhutdinov",
                "S. Levine"
            ],
            "title": "Imitating past successes can be very suboptimal",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "B. Eysenbach",
                "T. Zhang",
                "S. Levine",
                "R.R. Salakhutdinov"
            ],
            "title": "Contrastive learning as goal-conditioned reinforcement learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "M. Fang",
                "T. Zhou",
                "Y. Du",
                "L. Han",
                "Z. Zhang"
            ],
            "title": "Curriculum-guided hindsight experience replay",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "J. Fu",
                "A. Kumar",
                "O. Nachum",
                "G. Tucker",
                "S. Levine"
            ],
            "title": "D4rl: Datasets for deep data-driven reinforcement learning",
            "venue": "arXiv preprint arXiv:2004.07219,",
            "year": 2020
        },
        {
            "authors": [
                "D. Garg",
                "S. Chakraborty",
                "C. Cundy",
                "J. Song",
                "S. Ermon"
            ],
            "title": "Iq-learn: Inverse soft-q learning for imitation",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "D. Garg",
                "J. Hejna",
                "M. Geist",
                "S. Ermon"
            ],
            "title": "Extreme q-learning: Maxent rl without entropy",
            "venue": "arXiv preprint arXiv:2301.02328,",
            "year": 2023
        },
        {
            "authors": [
                "S.K.S. Ghasemipour",
                "R. Zemel",
                "S. Gu"
            ],
            "title": "A divergence minimization perspective on imitation learning methods",
            "venue": "In Conference on Robot Learning,",
            "year": 2020
        },
        {
            "authors": [
                "D. Ghosh",
                "A. Gupta",
                "A. Reddy",
                "J. Fu",
                "C. Devin",
                "B. Eysenbach",
                "S. Levine"
            ],
            "title": "Learning to reach goals via iterated supervised learning",
            "year": 1912
        },
        {
            "authors": [
                "J. Hejna",
                "J. Gao",
                "D. Sadigh"
            ],
            "title": "Distance weighted supervised learning for offline interaction data",
            "venue": "arXiv preprint arXiv:2304.13774,",
            "year": 2023
        },
        {
            "authors": [
                "L.P. Kaelbling"
            ],
            "title": "Learning to achieve goals",
            "venue": "In IJCAI,",
            "year": 1993
        },
        {
            "authors": [
                "I. Kostrikov",
                "O. Nachum",
                "J. Tompson"
            ],
            "title": "Imitation learning via off-policy distribution matching",
            "venue": "arXiv preprint arXiv:1912.05032,",
            "year": 2019
        },
        {
            "authors": [
                "I. Kostrikov",
                "A. Nair",
                "S. Levine"
            ],
            "title": "Offline reinforcement learning with implicit q-learning",
            "venue": "arXiv preprint arXiv:2110.06169,",
            "year": 2021
        },
        {
            "authors": [
                "A. Kumar",
                "A. Zhou",
                "G. Tucker",
                "S. Levine"
            ],
            "title": "Conservative q-learning for offline reinforcement learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "C. Lynch",
                "M. Khansari",
                "T. Xiao",
                "V. Kumar",
                "J. Tompson",
                "S. Levine",
                "P. Sermanet"
            ],
            "title": "Learning latent plans from play",
            "venue": "In Conference on robot learning,",
            "year": 2020
        },
        {
            "authors": [
                "Y.J. Ma",
                "J. Yan",
                "D. Jayaraman",
                "O. Bastani"
            ],
            "title": "How far i\u2019ll go: Offline goal-conditioned reinforcement learning via f -advantage regression",
            "venue": "arXiv preprint arXiv:2206.03023,",
            "year": 2022
        },
        {
            "authors": [
                "A.S. Manne"
            ],
            "title": "Linear programming and sequential decisions",
            "venue": "Management Science,",
            "year": 1960
        },
        {
            "authors": [
                "G. Molinaro",
                "A.G. Collins"
            ],
            "title": "A goal-centric outlook on learning",
            "venue": "Trends in Cognitive Sciences,",
            "year": 2023
        },
        {
            "authors": [
                "O. Nachum",
                "B. Dai"
            ],
            "title": "Reinforcement learning via fenchel-rockafellar duality",
            "venue": "arXiv preprint arXiv:2001.01866,",
            "year": 2020
        },
        {
            "authors": [
                "T. Ni",
                "H. Sikchi",
                "Y. Wang",
                "T. Gupta",
                "L. Lee",
                "B. Eysenbach"
            ],
            "title": "f-irl: Inverse reinforcement learning via state marginal matching",
            "venue": "In Conference on Robot Learning,",
            "year": 2021
        },
        {
            "authors": [
                "A. v. d. Oord",
                "Y. Li",
                "O. Vinyals"
            ],
            "title": "Representation learning with contrastive predictive coding",
            "venue": "arXiv preprint arXiv:1807.03748,",
            "year": 2018
        },
        {
            "authors": [
                "A. Padalkar",
                "A. Pooley",
                "A. Jain",
                "A. Bewley",
                "A. Herzog",
                "A. Irpan",
                "A. Khazatsky",
                "A. Rai",
                "A. Singh",
                "A. Brohan"
            ],
            "title": "Open x-embodiment: Robotic learning datasets and rt-x models",
            "venue": "arXiv preprint arXiv:2310.08864,",
            "year": 2023
        },
        {
            "authors": [
                "K. Paster",
                "S.A. McIlraith",
                "J. Ba"
            ],
            "title": "Planning from pixels using inverse dynamics models",
            "venue": "arXiv preprint arXiv:2012.02419,",
            "year": 2020
        },
        {
            "authors": [
                "M. Plappert",
                "M. Andrychowicz",
                "A. Ray",
                "B. McGrew",
                "B. Baker",
                "G. Powell",
                "J. Schneider",
                "J. Tobin",
                "M. Chociej",
                "P. Welinder"
            ],
            "title": "Multi-goal reinforcement learning: Challenging robotics environments and request for research",
            "venue": "arXiv preprint arXiv:1802.09464,",
            "year": 2018
        },
        {
            "authors": [
                "A. R\u00e9nyi"
            ],
            "title": "On measures of entropy and information. In Proceedings of the Fourth Berkeley Symposium on Mathematical Statistics and Probability, Volume 1: Contributions to the Theory of Statistics, pages 547\u2013561",
            "year": 1961
        },
        {
            "authors": [
                "H. Sikchi",
                "A. Saran",
                "W. Goo",
                "S. Niekum"
            ],
            "title": "A ranking game for imitation learning",
            "venue": "arXiv preprint arXiv:2202.03481,",
            "year": 2022
        },
        {
            "authors": [
                "H. Sikchi",
                "Q. Zheng",
                "A. Zhang",
                "S. Niekum"
            ],
            "title": "Dual rl: Unification and new methods for reinforcement and imitation learning",
            "venue": "In Sixteenth European Workshop on Reinforcement Learning,",
            "year": 2023
        },
        {
            "authors": [
                "D. Silver",
                "G. Lever",
                "N. Heess",
                "T. Degris",
                "D. Wierstra",
                "M. Riedmiller"
            ],
            "title": "Deterministic policy gradient algorithms",
            "venue": "In International conference on machine learning,",
            "year": 2014
        },
        {
            "authors": [
                "R.K. Srivastava",
                "P. Shyam",
                "F. Mutz",
                "W. Ja\u015bkowski",
                "J. Schmidhuber"
            ],
            "title": "Training agents using upside-down reinforcement learning",
            "year": 1912
        },
        {
            "authors": [
                "G. Swamy",
                "S. Choudhury",
                "J.A. Bagnell",
                "S. Wu"
            ],
            "title": "Of moments and matching: A game-theoretic framework for closing the imitation gap",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "H. Walke",
                "K. Black",
                "A. Lee",
                "M.J. Kim",
                "M. Du",
                "C. Zheng",
                "T. Zhao",
                "P. Hansen-Estruch",
                "Q. Vuong",
                "A. He"
            ],
            "title": "Bridgedata v2: A dataset for robot learning at scale",
            "venue": "arXiv preprint arXiv:2308.12952,",
            "year": 2023
        },
        {
            "authors": [
                "Y. Wu",
                "G. Tucker",
                "O. Nachum"
            ],
            "title": "Behavior regularized offline reinforcement learning",
            "venue": "arXiv preprint arXiv:1911.11361,",
            "year": 2019
        },
        {
            "authors": [
                "H. Xu",
                "L. Jiang",
                "J. Li",
                "Z. Yang",
                "Z. Wang",
                "V.W.K. Chan",
                "X. Zhan"
            ],
            "title": "Offline rl with no ood actions: In-sample learning via implicit value regularization",
            "venue": "arXiv preprint arXiv:2303.15810,",
            "year": 2023
        },
        {
            "authors": [
                "C. Yang",
                "X. Ma",
                "W. Huang",
                "F. Sun",
                "H. Liu",
                "J. Huang",
                "C. Gan"
            ],
            "title": "Imitation learning from observations by minimizing inverse dynamics disagreement",
            "year": 1910
        },
        {
            "authors": [
                "R. Yang",
                "Y. Lu",
                "W. Li",
                "H. Sun",
                "M. Fang",
                "Y. Du",
                "X. Li",
                "L. Han",
                "C. Zhang"
            ],
            "title": "Rethinking goalconditioned supervised learning and its connection to offline rl",
            "venue": "arXiv preprint arXiv:2202.04478,",
            "year": 2022
        },
        {
            "authors": [
                "C. Zheng",
                "B. Eysenbach",
                "H. Walke",
                "P. Yin",
                "K. Fang",
                "R. Salakhutdinov",
                "S. Levine"
            ],
            "title": "Stabilizing contrastive rl: Techniques for offline goal reaching",
            "venue": "arXiv preprint arXiv:2306.03346,",
            "year": 2023
        },
        {
            "authors": [
                "L. Zheng",
                "T. Fiez",
                "Z. Alumbaugh",
                "B. Chasnov",
                "L.J. Ratliff"
            ],
            "title": "Stackelberg actor-critic: Gametheoretic reinforcement learning algorithms",
            "venue": "arXiv preprint arXiv:2109.12286,",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "A generalist agent will require a vast repertoire of skills, and large amounts of offline pre-collected data offer a way to learn useful skills without any environmental interaction. Many subfields of machine learning like vision and NLP have enjoyed great success by designing objectives to learn a general model from large and diverse datasets. In robot learning, offline interaction data has become more prominent in the recent past (Ebert et al., 2021), with the scale of the datasets growing consistently (Walke et al., 2023; Padalkar et al., 2023). Goal-conditioned reinforcement learning (GCRL) offers a principled way to acquire a variety of useful skills without the prohibitively difficult process of hand-engineering reward functions. In GCRL, the agent learns a policy to accomplish a variety of goals in the environment. The rewards are sparse and goal-conditioned: 1 when the agent\u2019s state is proximal to the goal and 0 otherwise. However, the benefit of not requiring the designer to hand-engineer dense reward functions can also be a curse, because learning from sparse rewards is difficult. Driving progress in fundamental offline GCRL algorithms thus becomes an important aspect of moving towards performant generalist agents whose skills scale with data.\nDespite recent progress in developing methods for goal-reaching in the online setting (where environment interactions are allowed), a number of these methods are either suboptimal in the offline setting or suffer from learning difficulties. Prior GCRL algorithms can largely be classified into one of three categories: iterated behavior cloning, RL with sparse rewards, and contrastive learning. Iterated behavior cloning or goal-conditioned supervised learning approaches (Ghosh et al., 2019; Yang et al., 2019) have been shown to be provably suboptimal (Eysenbach et al., 2022a) for GCRL. Modifying single-task RL methods (Silver et al., 2014; Kostrikov et al., 2021) for GCRL with 0-1 reward implies learning a Q-function that predicts the discounted probability of goal reaching, which\n\u02daWork done partially during an internship at Meta AI. Correspondence to hsikchi@utexas.edu :Equal Contribution. Project page (Code and Videos): hari-sikchi.github.io/smore/\nmakes it essentially a density model. Modeling density directly is a hard problem, an insight which has prompted the development of methods (Eysenbach et al., 2020) that learn density-ratio instead of densities, as classification is an easier problem than density estimation. Contrastive RL approaches to GCRL (Eysenbach et al., 2020; 2022b; Zheng et al., 2023) aim to do precisely this and are the main methods to enjoy success for applying GCRL in high-dimensional observation spaces. However, when dealing with offline datasets, contrastive RL approaches (Eysenbach et al., 2022b; Zheng et al., 2023) are suboptimal, as they only learn a policy that is a greedy improvement over the Q-function of the data generation policy. This begs the question: How can we derive a performant GCRL method that learns near-optimal policies from offline datasets of suboptimal quality?\nIn this work, we leverage the underexplored insight of formulating GCRL as an occupancy matching problem. Occupancy matching between the joint state-action-goal visitation distribution induced by the current policy and the distribution over state-actions that transition to goals can be shown to be equivalent to optimizing a max-entropy GCRL objective. Occupancy matching has been studied extensively in imitation learning (Ghasemipour et al., 2020) and often requires learning a discriminator and using the learned discriminator for downstream policy learning through RL. Indeed, a prior GCRL work (Ma et al., 2022) explores a similar insight. Unfortunately, errors in learned discriminators can compound and adversely affect the learned policy\u2019s performance, especially in the offline setting where these errors cannot be corrected with further interaction with the environment.\nGoing beyond the shortcomings of the previous methods, our proposed method combines the insight of formulating GCRL as an occupancy matching problem along with an efficient, discriminator-free dual formulation that learns from offline data. The resulting algorithm SMORe forgoes learning density functions or classifiers, but instead learns unnormalized densities or scores that allow it to produce near-optimal goal-reaching policies. The scores are learned via a Bellman-regularized contrastive procedure that makes our method a desirable candidate for GCRL with high-dimensional observations, avoiding the need for density modeling. Our experiments represent a wide variety of goal-reaching environments \u2013 consisting of robotic arms, anthropomorphic hands, and locomotion environments. We lay out the following contributions: 1) on the extended offline GCRL benchmark, our results demonstrate that SMORe significantly outperforms prior methods in the offline GCRL setting. 2) In line with our hypothesis, discriminator-free training makes SMORe particularly robust to decreasing goal-coverage in the offline dataset, a property we demonstrate in the experiments. 3) We test SMORe for zero-shot GCRL on a prior benchmark (Zheng et al., 2023) for high dimensional vision-based GCRL where contrastive RL approaches are the only class of GCRL methods that have been successful, and show improved performance over other state-of-the-art baselines."
        },
        {
            "heading": "2 PROBLEM FORMULATION",
            "text": "We consider an infinite horizon discounted Markov Decision Process denoted by the tuple M \u201c pS,A, p, r, \u03b3, d0q, where S is the state space, A is the action space, p is the transition probability function, r : S \u02c6 A \u00d1 R is the reward function, \u03b3 P p0, 1q is the discount factor, and d0 is the initial state distribution. We constrain ourselves to the goal-conditioned RL setting, where we additionally assume a goal space G where states in S are mapped to the goal space using a known mapping: \u03d5 : S \u00d1 G. The reward function rps, a, gq in GCRL is sparse and also depends on the goal. A goal conditioned policy \u03c0 : S \u02c6 G \u00d1 \u2206pAq outputs a distribution over actions in a given state conditioned on a goal. Given a distribution over desired evaluation goals qtestpgq, the objective of goal-conditioned RL is to find a policy \u03c0g1 that maximizes the expected discounted return:\nJp\u03c0gq :\u201c Eg\u201eqtestppgqq,s0\u201ed0,at\u201e\u03c0g\n\u00ab\n8 \u00ff t\u201c0 \u03b3trpst, at, gq\nff\n. (1)\nWe denote by P\u03c0g the transition operator induced by the policy \u03c0g defined as P\u03c0gSps, a, gq :\u201c Es1\u201epp\u00a8|s,aq,a1\u201e\u03c0gp\u00a8|s1,gqrSps1, a1, gqs , for any score function S : S\u02c6A\u02c6G \u00d1 R. We use d\u03c0ps, a | gq to denote the discounted goal-conditioned state-action occupancy distribution of \u03c0g, i.e d\u03c0g ps, a | gq \u201c p1 \u00b4 \u03b3q\u03c0pa|s, gq\n\u01598 t\u201c1r\u03b3tPrpst \u201c s|\u03c0g, d0qs. which represents the expected discounted time\nspent in each state-action pair by the policy \u03c0g conditioned on the goal g. For complete generality, in GCRL, the distribution of goals the policy is trained on often differs from the test goal distribution. To make this distinction clear we define the training distribution qtrainpgq, a uniform measure over goals\n1We use the subscript g to make the policy\u2019s conditioning on g explicit.\nwe desire to learn to optimally reach during training. We write d\u03c0g ps, a, gq \u201c qtrainpgqd\u03c0g ps, a | gq as the joint state-action-goal visitation distribution of the policy \u03c0g under the training goal distribution. A state-action-goal occupancy distribution must satisfy the Bellman flow constraint in order for it to be a valid occupancy2 distribution for some stationary policy \u03c0g , @s P S, a P A, g P G:\ndps, a, gq \u201c p1 \u00b4 \u03b3qd0ps, gq\u03c0gpa | s, gq ` \u03b3 \u00ff\ns1,a1\npps | s1, a1qdps1, a1, gq\u03c0gpa | s, gq, (2)\nwhere d0ps, gq \u201c d0psqqtrainpgq. Finally, given d\u03c0g , we can express the learning objective for the GCRL agent under the training goal distribution as Jtrainp\u03c0gq \u201c 11\u00b4\u03b3Eps,a,gq\u201ed\u03c0g rrps, a, gqs.\nIn this work, we focus on the offline setup where the agent cannot interact with the environment M and instead has access to a offline dataset of D :\u201c t\u03c4iuNi\u201c1, where each trajectory \u03c4 piq \u201c pspiq0 , a piq 0 , r piq 0 , s piq 1 , ...; g\npiqq with spiq0 \u201e d0. The trajectories are usually relabelled with the qtrainpgq during learning. We denote the joint state-action-goal distribution of the offline dataset D as \u03c1ps, a, gq."
        },
        {
            "heading": "3 SCORE-MODELS FOR OFFLINE GOAL CONDITIONED REINFORCEMENT",
            "text": "LEARNING\nIn this section, we introduce our method in two parts: First, we build up the equivalence of the GCRL objective to the occupancy matching problem in Section 3.1, and then we derive a discriminator-free dual objective for solving the occupancy matching problem using off-policy data in Section 3.2. Finally, we present the algorithm for SMORe under practical considerations in Section 3.3."
        },
        {
            "heading": "3.1 GCRL AS AN OCCUPANCY MATCHING PROBLEM",
            "text": "Define a goal-transition distribution qps, a, gq in a stochastic MDP as qps, a, gq 9 qtrainpgqEs1\u201epp\u00a8|s,aq \u201c I\u03d5ps1q\u201cg \u2030\n. Intuitively, the distribution has probability mass on each transition that leads to a goal. We formulate the GCRL problem as an occupancy matching problem by searching for the policy \u03c0g that minimizes the discrepancy between its state-action-goal occupancy distribution and the goal-transition distribution qps, a, gq: Occupancy matching problem: Df pd\u03c0g ps, a, gq}qps, a, gqq, (3) whereDf denotes an f -divergence with generator function f . Note that the q distribution is potentially unachievable by any goal-conditioned policy \u03c0g. Firstly, it does not account for the initial transient phase that the policy must navigate to reach the desired goal. Secondly, even if we consider only the stationary regime (when \u03b3 \u00d1 1), it may not be dynamically possible for the policy to continuously remain at the goal and rather necessitate cycling around the goal. However, in Proposition 1, we show that the occupancy matching in Eq. 3 offers a principled objective since it forms a lower bound to the max-entropy GCRL problem. Proposition 1. Consider a stochastic MDP, a stochastic policy \u03c0, and a sparse reward function rps, a, gq \u201c Es1\u201epp\u00a8|s,aq \u201c Ip\u03d5ps1q \u201c g, qtrainpgq \u0105 0q \u2030\nwhere I is an indicator function. Define a soft goal transition distribution to be qps, a, gq 9 expp\u03b1 rps, a, gqq. The following bounds hold for any f -divergence that upper bounds KL-divergence (eg. \u03c72, Jensen-Shannon):\nJ trainp\u03c0gq ` 1 \u03b1 Hpd\u03c0g q \u011b \u00b4 1 \u03b1 Df pd\u03c0g ps, a, gq}qps, a, gqq ` C, (4)\nwhere H denotes the entropy, \u03b1 is a temperature parameter and C is the partition function for eRps,a,gq. Furthermore, the bound is tight when f is the KL-divergence.\nMa et al. (2022) (in Proposition 4.1) presented a similar result connecting state-goal distribution matching ( i.e DKLpd\u03c0ps, gq||qps, gqq) to GCRL objective and Proposition 1 extends their results to goal-transition distribution matching. Matching action-free distributions necessitates constructing a loose lower bound that is tractable to optimize. By considering goal-transition distributions we sidestep constructing a loose lower bound and instead directly obtain a tractable distribution matching objective (Ghasemipour et al., 2020; Kostrikov et al., 2019) that is tight under KL-divergence.\nHow does converting a GCRL objective to an imitation learning objective make learning easier? Estimating the f -divergence still requires estimating the joint policy visitation probabilities d\u03c0g ps, a, gq, which itself presents a challenging problem. We show in the following section that we can leverage\n2We will use \u201coccupancy\u201d and \u201cvisitation\u201d interchangeably.\n1\nconvex duality to transform the imitation learning problem into an off-policy optimization problem, removing the need to sample from d\u03c0g ps, a, gq whilst being able to leverage offline data collected from arbitrary sources."
        },
        {
            "heading": "3.2 SMORE: A DUAL FORMULATION FOR OCCUPANCY MATCHING",
            "text": "The previous section establishes GCRL as an occupancy matching problem (Eq. 3) but provides no way to use offline data whose joint visitation distribution is given by \u03c1ps, a, gq. To leverage offline data to learn performant goal-reaching policies, we consider a surrogate objective to the occupancy matching learning problem by matching mixture distributions:\nmin \u03c0g\nDf pMix\u03b2pd\u03c0g , \u03c1q}Mix\u03b2pq, \u03c1qq, (5)\nwhere for any two distributions \u00b51 and \u00b52, Mix\u03b2p\u00b51, \u00b52q denotes the mixture distribution with coefficient \u03b2 P p0, 1s defined as Mix\u03b2p\u00b51, \u00b52q \u201c \u03b2\u00b51 ` p1 \u00b4 \u03b2q\u00b52. Proposition 2 (in appendix) shows the matching mixture distribution3 provably maximizes a lower bound to the Lagrangian relaxation of the max-entropy GCRL objective subject to a dataset regularization constraint. We can rewrite the mixture occupancy matching objective as a convex program with linear constraints (Manne, 1960; Nachum and Dai, 2020):\nmax \u03c0g,d\n\u00b4Df pMix\u03b2pd, \u03c1q}Mix\u03b2pq, \u03c1qq\ns.t dps, a, gq \u201c p1 \u00b4 \u03b3qd0ps, gq\u03c0pa|sq ` \u03b3 \u0159 s1PS dps1, a1, gqpps|s1, a1q\u03c0pa1|s1, gq, @s P S. (6) An illustration of this objective can be found in Figure 1. Effectively, we have simply rewritten Eq. 5 into an equivalent problem by considering an arbitrary probability distribution dps, a, gq in the optimization objective, only to later constrain it to be a valid probability distribution induced by some policy \u03c0g using the Bellman-flow constraints. The motivation behind this construction of the primal form is that we have made computing the Lagrangian-dual easier as this objective is convex with linear constraints. Theorem 1 shows that we can leverage tools from convex duality to obtain an unconstrained dual problem that does not require computing d\u03c0g ps, a, gq or sampling from it, while effectively leveraging offline data. Theorem 1. The dual problem to the primal occupancy matching objective (Equation 6) is given by:\nmax \u03c0g min S \u03b2p1 \u00b4 \u03b3qEd0,\u03c0g rSps, a, gqs ` EMix\u03b2pq,\u03c1qrf\n\u02dap\u03b3P\u03c0gSps, a, gq \u00b4 Sps, a, gqqs (7)\n\u00b4 p1 \u00b4 \u03b2qE\u03c1r\u03b3P\u03c0gSps, a, gq \u00b4 Sps, a, gqs, where f\u02da is conjugate function of f and S is the Lagrange dual variable defined as S : S \u02c6A\u02c6G \u00d1 R. Moreover, as strong duality holds from Slater\u2019s conditions the primal and dual share the same optimal solution \u03c0\u02dag for any offline transition distribution \u03c1.\nTo our knowledge, the closest prior works to our proposed method are GoFAR (Ma et al., 2022) and Dual-RL (Sikchi et al., 2023). GoFAR considers the special case of KL-divergence for the imitation formulation and derives a dual objective that requires learning the density ratio \u03c1ps,gqqps,gq in the\n3Note that Eq. 5 shares the same global optima as the previous occupancy matching objective at d\u03c0g ps, a, gq \u201c qps, a, gq when q is an achievable visitation under some policy and recovers the original objective in Eq. 3 when \u03b2 \u201c 1.\nform of a discriminator and using this as a pseudo-reward. This leads to compounding errors in the downstream RL optimization when learning the density ratio is challenging, e.g. in the case of low coverage between \u03c1ps, a, gq and qps, a, gq. We show this phenomenon experimentally in Section 4.3. Dual-RL (Sikchi et al., 2023) uses convex duality for matching visitation distribution of realizable expert demonstrations and does not deal with the GCRL setting. Our contribution is a novel method for GCRL that is discriminator-free, applicable for a number of f -divergences, and robust to low coverage of goals in the offline dataset.\nSampling from the goal-transition distribution: Goal relabelling is an effective technique to address reward sparsity by widening the training goal distribution qtrainpgq. It utilizes knowledge about reaching other goals, possibly unrelated to test goals, to help in reaching the test distribution of goals qtestpgq. In the most general case, qtrainpgq can be set to a uniform distribution over goals corresponding to all the states in the offline data. A common method, Hindsight Experience Replay (HER) (Andrychowicz et al., 2017) chooses a training goal distribution that depends on the current sampled state from the offline dataset as well as the data-collecting policies. In this setting, the sampling distribution used for training Eq 7, \u03c1ps, a, gq, can no longer be factorized into \u03c1ps, aq and qtrainpgq, as goals are conditionally dependent on state-actions. However, our formulation can naturally account for learning from such relabelled data as the SMORe objective in Eq 7 is derived considering the joint distribution \u03c1ps, a, gq. In this setting, we construct our goal transition distribution qps, a, gq as the uniform distribution over all transitions that lead to the goals selected by the HER procedure \u2014 in practice, this amounts to first selecting g through HER and then selecting ts, au that transitions to the selected goal from the offline dataset to get a sample ts, a, gu from goal transition distribution. We emphasize that relabelling does not change the test distribution of goals, which is an immutable property of the environment."
        },
        {
            "heading": "3.3 PRACTICAL ALGORITHM",
            "text": "To devise a stable learning algorithm we consider the Pearson \u03c72 divergence. Pearson \u03c72 divergence has been found to lead to distribution matching objectives that are stable to train as a result of a smooth quadratic generator function f (Garg et al., 2021; Al-Hafez et al., 2023; Sikchi et al., 2023). Our dual formulation SMORe simplifies to the following objective:\nmax\u03c0g minS\nDecrease score at transitions under current policy \u03c0g hkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkikkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkj \u03b2p1 \u00b4 \u03b3qEps,gq\u201ed0,a\u201e\u03c0gp\u00a8|s,gqrSps, a, gqs ` \u03b2\u03b3Eps,a,gq\u201eq,s1\u201epp\u00a8|s,aq,a1\u201e\u03c0gp\u00a8|s1,gq \u201c Sps1, a1, gq \u2030\n\u00b4 \u03b2Eps,a,gq\u201eqrSps, a, gqs loooooooooooomoooooooooooon\nIncrease score at the proposed goal transition distribution\n`0.25Eps,a,gq\u201eMix\u03b2pq,\u03c1q \u201c p\u03b3Sps1, \u03c0gps1q, gq \u00b4 Sps, a, gqq2 \u2030\nloooooooooooooooooooooooooooooooomoooooooooooooooooooooooooooooooon\nSmoothness/Bellman regularization\n. (8)\nEquation 8 suggests a contrastive procedure, maximizing the score at the goal-transition distribution and minimizing the score at the offline data distribution under the current policy with Bellman regularization. The Bellman regularization has the interpretation of discouraging neighboring S values from deviating far and smoothing the score landscape. Instantiating with KL divergence results in an objective with similar intuition while resembling an InfoNCE (Oord et al., 2018) objective. Although Propositions 1 and 2 suggest that KL divergence gives an objective that is a tighter bound to the GCRL objective, prior work has found KL divergence to be unstable in practice (Sikchi et al., 2023; Garg et al., 2023) for dual optimization.\nAlgorithm 1: SMORe 1: Init S\u03d5, M\u03c8 , and \u03c0\u03b8 2: Params: expectile \u03c4 , mixture ratio \u03b2,\ntemperature \u03b1 3: Let D \u201c p\u03c1 \u201c tps, a, s1, gqu be an offline\ndataset and q be goal-transition distribution 4: for t \u201c 1..T iterations do 5: Train S\u03d5 via Eq. 10 6: Train M\u03c8 via Eq. 9 7: Update \u03c0\u03b8 via Eq. 11 8: end for\nIt is important to note that S-function is not grounded to any rewards and does not serve as a probability density of reaching goals, but is rather a score function learned via a Bellman-regularized contrastive learning procedure.\nWe now derive a practical approach for SMORe in the offline GCRL setting. We use parameterized functions: S\u03d5ps, a, gq, M\u03c8ps, gq, \u03c0\u03b8pa|s, gq. The offline learning regime necessitates measures to constrain the learning policy to the offline data support in order to prevent overestimation due to maximizing \u03c0g in Eq. 8 over potentially out-of-distribution actions. Inspired by prior work (Kostrikov et al., 2021), we use implicit maximization to constrain the learning algorithm to learn expectiles using the observed empirical samples. More concretely, we use expectile\nregression: min \u03c8 Lp\u03c8q :\u201c Eps,a,gq\u201e\u03c1rL\u03c42pM\u03c8ps, gq \u00b4 S\u03d5ps, a, gqqs, (9) where L\u03c42puq \u201c |\u03c4 \u00b4 1pu \u0103 0q|u2. Intuitively, this step implements the maximization w.r.t \u03c0 by using expectile regression. With the above practical considerations, our objective for learning S\u03d5 reduces to:\nmin \u03d5\nLp\u03d5q :\u201c \u03b2p1 \u00b4 \u03b3qEps,gq\u201eD,a\u201e\u03c0gp\u00a8|a,gqrS\u03d5ps, \u03c0gpsq, gqs ` \u03b2\u03b3Eps,a,gq\u201eq,s1\u201epp\u00a8|s,aq \u201c S\u03d5ps1, \u03c0gps1q, gq \u2030\n\u00b4 \u03b2Eps,a,gq\u201eqrS\u03d5ps, a, gqs ` Eps,a,gq\u201eMix\u03b2pq,\u03c1q \u201c p\u03b3M\u03c8ps1, gq \u00b4 S\u03d5ps, a, gqq2 \u2030\n, (10)\nwhere we have set the offline data distribution as our initial state distribution. Finally, the policy is extracted via advantage-weighted regression that learns in-distribution actions maximizing the score Sps, a, gq:\nmin \u03b8 Lp\u03b8q :\u201c Eps,a,gq\u201e\u03c1rexpp\u03b1pS\u03d5ps, a, gq \u00b4M\u03c8ps, gqqq logp\u03c0\u03b8pa|s, gqqs, (11) where \u03b1 is the temperature parameter. Algorithm 1 details the practical implementation."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": "Our experiments study the effectiveness of proposed GCRL algorithm SMORe on a set of simulated benchmarks against other GCRL methods that employ behavior cloning, RL with sparse reward, and contrastive learning. We also analyze if SMORe is robust to environment stochasticity \u2014 a number of prior methods are based on an assumption of deterministic dynamics. Then, we study if the discriminator-free nature of SMORe is indeed able to prevent performance degradation in the face of low expert coverage in offline data. Finally, we analyze if SMORe\u2019s score-modeling approach helps SMORe scale to a vision-based manipulation offline GCRL benchmark, as density modeling and discriminator learning become increasingly difficult with high-dimensional observations. Hyperparameter ablations can be found in Appendix D."
        },
        {
            "heading": "4.1 EXPERIMENTAL SETUP",
            "text": "Our experiments will use a suite of simulated goal-conditioned tasks extending the tasks from previous work (Ma et al., 2022; Plappert et al., 2018). In particular we consider the following environments: Reacher, Robotic arm environments - [SawyerReach, SawyerDoor, FetchReach, FetchPick, FetchPush, FetchSlide], Anthropomorphic hand environment - HandReach and Locomotion environments - [CheetahTgtVel-me,CheetahTgtVel-re,AntTgtVel-me,AntTgtVel-re]. Tasks in all environments are specified by a sparse reward function. Depending on whether the task involves object manipulation, the goal distribution is defined over valid configurations in robot or object space. The offline dataset for manipulation tasks consists of transitions collected by a random policy or mixture of 90% random policy and 10% expert policy. For locomotion tasks, we generate our dataset using the D4RL benchmark (Fu et al., 2020), combining a random or medium dataset with 30 episodes of expert data. Note that the policies used to collect the expert locomotion datasets have a different objective than the tasks here, which are to achieve and maintain a particular desired velocity."
        },
        {
            "heading": "4.2 OFFLINE GOAL-CONDITIONED RL BENCHMARK",
            "text": "Baselines. We compare to state-of-art offline GCRL algorithms, consisting of both regression-based and actor-critic methods. The occupancy-matching based methods are: (1) GoFar (Ma et al., 2022), which derives a dual objective for GCRL based on a coverage assumption. The behavior cloning based methods are: (1) GCSL (Ghosh et al., 2019), which incorporates hindsight relabeling in conjunction with behavior cloning to clone actions that lead to a specified goal, and (2) WGCSL (Yang et al., 2022), which improves upon GCSL by incorporating discount factor and advantage weighting into the supervised policy learning update. Contrastive RL (Eysenbach et al., 2022b) generalizes Clearning (Eysenbach et al., 2020) and represents contrastive GCRL approaches. The RL with sparse reward methods are (1) IQL (Kostrikov et al., 2021) where we use a state-of-the-art offline RL method repurposed for GCRL along with HER (Andrychowicz et al., 2017) goal sampling, and (2)\nActionableModel (AM) (Chebotar et al., 2021), which incorporates conservative Q-Learning (Kumar et al., 2020) as well as goal-chaining on top of an actor-critic method.\nThe results for all baselines are tuned individually, particularly the best HER ratio was searched among t0.2, 0.5, 0.8, 1.0u for each task. SMORe shares the same network architecture for baselines and uses a mixture ratio of \u03b2 \u201c 0.5. Each method is trained for 10 seeds. Complete architecture and hyperparameter table as well as additional training details are provided in Appendix C.\nTable 1 reports the discounted return obtained by the learned policy with a sparse binary task reward. (\u2039) denotes statistically significant improvement over the second best method under a Mann-Whitney U test with a significance level of 0.05. This metric allows us to compare the algorithms on a finer scale to understand which methods reach the goal as fast as possible and stay in the goal region thereafter for the longest time. Additional results on metrics like success rate and final distance to goal can be found in the appendix. These additional metrics do not take into consideration how precisely and consistently a goal is being reached. In Table 1, we see that SMORe enjoys a high-performance gain consistently across all tasks in the extended offline GCRL benchmark.\nRobustness to environment stochasticity: We consider a noisy version of the FetchReach environment in this experiment. Gaussian zero-mean noise is added before executing an action to generate different variants of the environment with standard deviations of t0.5, 1.0, 1.5u. Datasets for these environments are obtained from prior work (Ma et al., 2022). As we see in Figure 2, SMORe is robust to stochasticity in the environment, outperforming baselines in terms of discounted return. Behavior cloning based approaches assume deterministic dynamics and are therefore over-optimistic in stochastic environments."
        },
        {
            "heading": "4.3 ROBUSTNESS OF OCCUPANCY-MATCHING METHODS TO DECREASING EXPERT COVERAGE",
            "text": "We posit that the discriminator-free nature of SMORe makes it more robust to decreasing goal coverage, as it does not suffer from cascading errors stemming from a learned discriminator. In this section, we set out to test this hypothesis by decreasing the amount of expert data in the offline goal-reaching dataset. We compare with GoFAR in Table 2 due to the similarity between methods and GoFAR\u2019s restrictive assumption on coverage of expert data in the suboptimal dataset. Comparison against all the baselines can be found in Appendix D.\nOur hypothesis holds true as we see in Table 2, the performance of the discriminator-based method GoFar rapidly decays as expert data is decreased in the offline dataset \u2013 28.4% with 2.5% and 36.15% with 1% expert data(i.e. optimal policy\u2019s coverage) respectively. SMORe shows a much slower decay\nin performance, 7.6% with 2.5% and 16% with 1% expert data, attesting to the method\u2019s robustness under decreasing expert coverage in the offline dataset."
        },
        {
            "heading": "4.4 OFFLINE GCRL WITH IMAGE OBSERVATIONS",
            "text": "SMORe provides an effective algorithm for offline GCRL in high-dimensional observation spaces by learning unnormalized scores using a contrastive procedure as opposed to prior works that learn normalized densities (Eysenbach et al., 2020) which are difficult to learn or density ratios (Eysenbach et al., 2022b; Zheng et al., 2023) which do not optimize for the optimal goal-conditioned policy in the offline GCRL setting. Similar to prior work (Eysenbach et al., 2022b), we consider the following structure in S-function parameterization to learn performant and generalizable policies: Sps, a, gq \u201c \u03d5ps, aqT\u03c8pgq. The S-function can be interpreted as the similarity between the two representations given by \u03d5 and \u03c8. Our network architecture for both representations is similar to Zheng et al. (2023) and is kept the same across all baselines to ensure a fair comparison of the underlying GCRL method.\nWe use the offline GCRL benchmark from (Zheng et al., 2021) which learns goal-reaching policies from an image-observation dataset of 250K transitions with the horizon ranging from 50-100. The benchmark adds another layer of complexity by testing on goals absent from the dataset \u2014 the dataset contains primitive behaviors like picking up objects and pushing drawers but no behavior that completes the compound task we consider from the initial state. The observations and goals are 48x48x3 RGB images.\nBaselines We compare to the best performing GCRL algorithms from Section 1 as well as a recent state-of-the-art work, stable contrastive RL Zheng et al. (2023). Stable contrastive RL features a number of improvements over contrastive RL by changing design decisions in neural network architecture, layer normalization, and data augmentation. Since our objective is to compare the quality of the underlying GCRL algorithm, we keep these design decisions consistent across the board.\nResults Figure 3 shows the success rate on a variety of unseen tasks for all the methods. SMORe achieves highest success rates across all the tasks, even for the most challenging task of pick, place\nand closing the drawer. We note that our results differ from Zheng et al. (2023) for the baselines as we apply the same design decisions for all methods whereas Zheng et al. (2023) focuses on ablating design decisions."
        },
        {
            "heading": "5 RELATED WORKS",
            "text": "Offline Goal Conditioned Reinforcement Learning. Learning to achieve goals in the environment optimally forms the basis of goal-condition RL problems. Studies in cognitive science (Molinaro and Collins, 2023) underscore the importance goal-achieving plays in human development. Offline GCRL approaches are typically catered to designing learning algorithms for addressing the sparsity of reward function in the offline setting. One of the most successful techniques in this setting has been hindsight relabelling. Hindsight-experience relabelling (HER) (Kaelbling, 1993; Andrychowicz et al., 2017) suggests relabelling any experience with some commanded goal to the goal that was actually achieved in order to leverage generalization. HER has been investigated in the setting of learning from demonstrations (Ding et al., 2019) and exploration (Fang et al., 2019) to validate its effectiveness. A number of prior works (Ghosh et al., 2019; Yang et al., 2019; Chen et al., 2020; Ding et al., 2019; Lynch et al., 2020; Paster et al., 2020; Srivastava et al., 2019; Hejna et al., 2023) have investigated using goal-conditioned behavior cloning, a strategy that uses relabelling to learn goal-conditioned policies, as a way to learn performant policies. Eysenbach et al. (2022a) shows that this line of work has a limitation of learning suboptimal policies that do not consistently improve over the policy that collected the dataset. The simplest strategy of applying single-task RL to the problem of multi-task goal reaching requires learning a Q-function which represents normalized densities over the state-action space. Contrastive RL (Eysenbach et al., 2022b; 2020; Zheng et al., 2023) emerged as another alternative for GCRL which relabels trajectories and, rather than use that relabelling to learn policies, learns a Q-function using a contrastive procedure. While these approaches learn optimal policies in the online setting, they fall behind in the offline setting where they only learn a policy that greedily improves over the Q-function of the data collecting policy. Our work learns optimal policies by presenting an off-policy objective that solves GCRL and furthermore learns scores (or unnormalized densities) that alleviate the learning challenges of normalized density estimation.\nDistribution matching. Our approach is inspired by the distribution matching approach (Ghasemipour et al., 2020; Ni et al., 2021; Sikchi et al., 2022; Swamy et al., 2021; Sikchi et al., 2023) prominent in imitation learning. Ghasemipour et al. (2020); Ni et al. (2021) takes the problem of imitating an expert demonstrator in the environment and converts it into a problem of distribution matching between the current policy\u2019s state-action visitation distribution and the expert policy\u2019s visitation distribution. Indeed, a prior work f -PG (Agarwal et al., 2024) proposes a distribution matching approach to GCRL but is restricted to the on-policy setting. Another, prior work (Ma et al., 2022) creates one such distribution matching problem and presents a new optimization problem for GCRL in the form of an off-policy dual (Nachum and Dai, 2020; Sikchi et al., 2023). Such an off-policy dual is very appealing for the offline RL setup, as optimizing for this dual only requires sampling from the offline data distribution. A limitation of their dual construction is the fact that they require learning a discriminator and use that discriminator as the pseudo-reward for solving the GCRL objective. Our approach presents a new construction for GCRL as a distribution matching problem along with a dual construction that leads to a more performant discriminator-free off-policy approach for GCRL."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "Prior work in performant online goal-conditioned RL often relies on iterated behavior cloning or contrastive RL. However, these approaches are suboptimal for the offline setting. Existing methods specifically derived for offline GCRL require learning a discriminator and using it as a pseudo-reward, enabling compounding errors that make the resulting policy ineffective. We present an occupancymatching approach to offline GCRL that provably optimizes a lower bound to the regularized GCRL objective. Our method is discriminator-free, applicable to a number of f -divergences, and learns unnormalized scores over actions at a state to reach the goal. We show that these positive aspects of our algorithm allow us to empirically outperform prior methods, stay robust under decreasing goal coverage, and scale to high-dimensional observation space for GCRL."
        },
        {
            "heading": "ACKNOWLEDGEMENTS",
            "text": "We thank the MIDI lab members and ICLR reviewers for valuable feedback on this work. This work has taken place in the Safe, Correct, and Aligned Learning and Robotics Lab (SCALAR) at The University of Massachusetts Amherst and Machine Intelligence through Decision-making and Interaction (MIDI) Lab at The University of Texas at Austin. SCALAR research is supported in part by the NSF (IIS-2323384), AFOSR (FA9550-20-1-0077), and ARO (78372-CS, W911NF19-2-0333), and the Center for AI Safety (CAIS). This research was also sponsored by the Army Research Office under Cooperative Agreement Number W911NF-19-2-0333. HS and AZ are funded in part by a sponsored research agreement with Cisco Systems Inc. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the Army Research Office or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation herein."
        },
        {
            "heading": "A APPENDIX",
            "text": ""
        },
        {
            "heading": "A.1 THEORY",
            "text": "In this section, we first show the equivalence of the GCRL problem and the distribution-matching objective of imitation learning. Then, we show how the mixture distribution objective relates to offline GCRL objective. Finally, we derive the dual objective for mixture distribution matching that leads to our method SMORe."
        },
        {
            "heading": "A.1.1 REDUCTION OF GCRL TO DISTRIBUTION MATCHING",
            "text": "Proposition 1. Consider a stochastic MDP, a stochastic policy \u03c0, and a sparse reward function rps, a, gq \u201c Es1\u201epp\u00a8|s,aq \u201c Ip\u03d5ps1q \u201c g, qtrainpgq \u0105 0q \u2030\nwhere I is an indicator function. Define a soft goal transition distribution to be qps, a, gq 9 expp\u03b1 rps, a, gqq. The following bounds hold for any f -divergence that upper bounds KL-divergence (eg. \u03c72, Jensen-Shannon):\nJ trainp\u03c0gq ` 1 \u03b1 Hpd\u03c0g q \u011b \u00b4 1 \u03b1 Df pd\u03c0g ps, a, gq}qps, a, gqq ` C, (4)\nwhere H denotes the entropy, \u03b1 is a temperature parameter and C is the partition function for eRps,a,gq. Furthermore, the bound is tight when f is the KL-divergence.\nProof. This proof is adapted from Ma et al. (2022) for goal transition distributions and state-action distributions. Let Z \u201c \u015f\neRps,a,gq ds da dg and \u03b1 \u0105 0 be the temperatue parameter. Note that qps, a, gq \u201c erps,a,gq where r is defined in the proposition, strictly generalizes the original definition qps, a, gq \u201c qtrainpgqEs1\u201epp\u00a8|s,aqrIp\u03d5ps1q \u201c gqs and recovers it when \u03b1 \u00d1 8. Starting with the true GCRL objective:\n\u03b1Jp\u03c0gq \u201c Ed\u03c0g r\u03b1Rps, a, gqs (12)\n\u201c Ed\u03c0g \u201d log e\u03b1Rps,a,gq \u0131\n(13)\n\u201c Ed\u03c0g \u201e logpe \u03b1Rps,a,gq\nZ\nd\u03c0g ps, a, gq d\u03c0g ps, a, gqZq\n\u0237\n(14)\n\u201c Ed\u03c0g \u201e logp qps, a, gq d\u03c0g ps, a, gqZq \u0237 ` Ed\u03c0g rlog d\u03c0g s (15)\n\u201c \u00b4DKLpd\u03c0g ps, a, gq}qps, a, gqq \u00b4 Hpd\u03c0g q ` logpZq (16) Rearranging terms we get:\nJp\u03c0gq ` 1 \u03b1 Hpd\u03c0g q \u201c \u00b4 1 \u03b1 DKLpd\u03c0g ps, a, gq}qps, a, gqq ` C (17)\nFor any f -divergence that upper bounds the KL divergence we have:\nJp\u03c0gq ` 1 \u03b1 Hpd\u03c0g q \u201c \u00b4 1 \u03b1 DKLpd\u03c0g ps, a, gq}qps, a, gqq `C \u011b \u00b4 1 \u03b1 Df pd\u03c0g ps, a, gq}qps, a, gqq `C\n(18)\nA (dataset) regularized GCRL objective: Define a regularized objective for GCRL as follows: Jofflinep\u03c0q \u201c \u03b11Ed\u03c0 \u201d erps,a,gq \u0131 ` \u03b12Ed\u03c0ps,a,gqr\u03c1ps, a, gqs. (19)\nThe second term in the objective Ed\u03c0ps,a,gqr\u03c1ps, a, gqs above is maximized when the policy visitation places more probability mass on the most visited transitions in the dataset. To see why this is, consider two probability distributions represented as vectors d\u03c0 and \u03c1 with individuals elements of the vector indexed by i:\nxd\u03c0, \u03c1y \u010f max i \u03c1i (20)\nThe equality holds only when d\u03c0 places probability mass on all state-action-goal tuples which are most visited in the offline dataset \u03c1. The first term maximizes the true GCRL objective while the second term prefers staying close to transitions that are most frequently observed in the offline dataset. A constraint of xd\u03c0, \u03c1y \u011b 1 \u00b4 \u03b4 implies that the agent visitation places atleast half of the probability mass on state-action-goal tuples whose average visitation in offline dataset is greater than or equal\nto 1 \u00b4 \u03b4. With weights \u03b11 and \u03b12, the objective above reflects an lagrangian relaxation to this constraint. Thus the above offline objective presents an alternative offline objective when compared to the classical offline RL objectives Wu et al. (2019); Nachum and Dai (2020).\nProposition 2 derives the connection between the dataset regularized GCRL objective and SMORe: Proposition 2. Consider a stochastic MDP, a stochastic policy \u03c0, and a sparse reward function rps, a, gq \u201c Es1\u201epp\u00a8|s,aq \u201c Ip\u03d5ps1q \u201c g, qtrainpgq \u0105 0q \u2030\nwhere I is an indicator function, define a soft goal transition distribution to be qps, a, gq 9 expp\u03b1 rps, a, gqq the following bounds hold for any f -divergence that upper bounds KL-divergence (eg. \u03c72, Jensen-Shannon): log Jofflinep\u03c0gq ` HpMix\u03b2pd, \u03c1qps, a, gqq ` C \u011b \u00b4Df pMix\u03b2pd, \u03c1qps, a, gq}Mix\u03b2pq, \u03c1qps, a, gqq, (21) where H denotes the entropy, \u03b1 is a temperature parameter, \u03b11 \u201c \u03b22, \u03b12 \u201c \u03b2p1 \u00b4 \u03b2qZ and C is a positive constant. Furthermore, the bound is tight when f is the KL-divergence.\nProof. We first consider the following two objectives for GCRL and show that they are equivalent. This reduction will later help in proving a connection to mixture occupancy matching. We consider \u03b1 \u201c 1 w.l.o.g. Here are two objectives we consider:\nJp\u03c0q \u201c Ed\u03c0 rrps, a, gqs (22) J 1p\u03c0q \u201c Ed\u03c0 \u201d erps,a,gq \u0131 (23)\nIn GCRL reward functions are sparse and binary. We show the equivalence of first two objectives in find the optimal goal conditioned policy via two arguments. First, notice that the rewards for goal transition states for objective J 1p\u03c0q is e and 1 for all other transitions. This is in contrast to Jp\u03c0q which considers a reward function 1 at goal transitions states and 0 otherwise. Under our assumption of infinite horizon discounted MDP, we can translate the rewards while keeping the optimal policy same in MDP considered by J 1p\u03c0q to e\u00b4 1 at goal transitions states and 0 otherwise. Further we can scale the rewards by 1{pe\u00b4 1q and recover and MDP with same optimal policy that has reward of 1 at goal-transition states and 0 otherwise. This concludes the equivalence of maximizing J 1p\u03c0q as an alternative to Jp\u03c0q while recovering the same optimal policy. We now consider a regularized (pessimistic/offline) GCRL problem with the shifted reward functions erps,a,gq that maximizes the reward while ensuring the policy visitation stays close to offline data visitation in cosine similarity.\nJofflinep\u03c0q \u201c \u03b11Ed\u03c0 \u201d erps,a,gq \u0131 ` \u03b12Ed\u03c0ps,a,gqr\u03c1ps, a, gqs. (24)\nWith a particular instantiation of hyperparameters we show that the Jofflinep\u03c0q objective can be simplified to an equivalent objective J 1offlinep\u03c0q by setting \u03b11 \u201c \u03b22 and \u03b12 \u201c \u03b2p1 \u00b4 \u03b2qZ where Z is the partition function for erps,a,gq over entire S \u02c6 A \u02c6 G.\nJ 1offlinep\u03c0q \u201c EMix\u03b2pd,\u03c1qps,a,gq \u201d \u03b2erps,a,gq ` p1 \u00b4 \u03b2q\u03c1ps, a, gq.Z \u0131\n(25)\nJ 1offlinep\u03c0q \u201c EMix\u03b2pd,\u03c1qps,a,gq \u201d \u03b2erps,a,gq ` p1 \u00b4 \u03b2q\u03c1ps, a, gq.Z \u0131\n(26)\n\u201c \u03b22Ed\u03c0 \u201d erps,a,gq \u0131 ` \u03b2p1 \u00b4 \u03b2qZEd\u03c0 r\u03c1ps, a, gqs (27)\n` p1 \u00b4 \u03b2qEdO \u201d \u03b2erps,a,gq ` p1 \u00b4 \u03b2q\u03c1ps, a, gq.Z \u0131 \u03b2 (28)\n(29)\n\u201c \u03b22Ed\u03c0 \u201d erps,a,gq \u0131 ` \u03b2p1 \u00b4 \u03b2qZEd\u03c0 r\u03c1ps, a, gqs ` C 1 (30)\n\u201c Jofflinep\u03c0q ` C 1 (31)\nNow that we have shown J 1offlinep\u03c0q \u201d Jofflinep\u03c0q and hence solving the same optimization problem, we proceed to derive connections with mixture occupancy matching which follows through an application of Jensen\u2019s inequality:\nlog J 1offlinep\u03c0q \u201c logEMix\u03b2pd,\u03c1qps,a,gq \u201d \u03b2erps,a,gq ` p1 \u00b4 \u03b2q\u03c1ps, a, gq.Z \u0131\n(32)\n\u011b EMix\u03b2pd,\u03c1qps,a,gq \u201d logp\u03b2erps,a,gq ` p1 \u00b4 \u03b2q\u03c1ps, a, gq.Zq \u0131\n(33)\n(34) \u201c EMix\u03b2pd,\u03c1qps,a,gqrlogp\u03b2qps, a, gq ` p1 \u00b4 \u03b2q\u03c1ps, a, gqqs ` logZ (35) \u201c \u00b4DKLrMix\u03b2pd, \u03c1qps, a, gq}Mix\u03b2pq, \u03c1qps, a, gqs \u00b4 HpMix\u03b2pd, \u03c1qps, a, gqq ` logZ (36)\nFor any f -divergence that upperbounds the KL divergence since Z \u011b 1 we have:\nlog J 1offlinep\u03c0q ` 1\n\u03b1 HpMix\u03b2pd, \u03c1qps, a, gqq \u011b \u00b4\n1 \u03b1 Df pMix\u03b2pd, \u03c1qps, a, gq}Mix\u03b2pq, \u03c1qps, a, gqq\n(37)\nFurther simplifying using Eq 31:\nlog Jofflinep\u03c0q` 1\n\u03b1 HpMix\u03b2pd, \u03c1qps, a, gq`C \u011b \u00b4\n1 \u03b1 Df pMix\u03b2pd, \u03c1qps, a, gq}Mix\u03b2pq, \u03c1qps, a, gqq\n(38)\nOptimizing the mixture distribution matching objective of SMORe maximizes a variant of offline/dataset regularized GCRL objective where the entropy for distribution Mix\u03b2pd, \u03c1qps, a, gq is jointly maximized. Therefore we have shown that the minimizing discrepancy of mixture distribution occupancy maximizes a lower bounds to an offline variant of maxent GCRL objective.\nA.2 CONVEX CONJUGATES AND f -DIVERGENCES\nWe first review the basics of duality in reinforcement learning. Let f : R` \u00d1 R be a convex function. The convex conjugate f\u02da : R` \u00d1 R of f is defined by: f\u02dapyq \u201c supxPR` rxy \u00b4 fpxqs. (39) The convex conjugates have the important property that f\u02da is also convex and the convex conjugate of f\u02da retrieves back the original function f . We also note an important relation regarding f and f\u02da: pf\u02daq1 \u201c pf 1q\u00b41, where the 1 notation denotes first derivative. Going forward, we would be dealing extensively with f -divergences. Informally, f - divergences (Re\u0301nyi, 1961) are a measure of distance between two probability distributions. Here\u2019s a more formal definition:\nLet P and Q be two probability distributions over a space Z such that P is absolutely continuous with respect to Q 4. For a function f : R` \u00d1 R that is a convex lower semi-continuous and fp1q \u201c 0, the f -divergence of P from Q is\nDf pP || Qq \u201c Ez\u201eQ \u201e f \u02c6 P pzq Qpzq \u02d9\u0237 . (40)\nTable 3 lists some common f -divergences with their generator functions f and the conjugate functions f\u02da."
        },
        {
            "heading": "A.3 SMORE: DUAL OBJECTIVE FOR OFFLINE GOAL CONDITIONED REINFORCEMENT LEARNING",
            "text": "In this section, we derive the dual objective for solving the multi-task occupancy problem formulation for GCRL. First, we derive the original variant of SMORe for the GCRL problem and later derive the action-free SMORe variant for the interested readers.\n4Let z denote the random variable. For any measurable set Z \u010e Z , Qpz P Zq \u201c 0 implies P pz P Zq \u201c 0.\nTheorem 1. The dual problem to the primal occupancy matching objective (Equation 6) is given by: max \u03c0g min S \u03b2p1 \u00b4 \u03b3qEd0,\u03c0g rSps, a, gqs ` EMix\u03b2pq,\u03c1qrf \u02dap\u03b3P\u03c0gSps, a, gq \u00b4 Sps, a, gqqs (7)\n\u00b4 p1 \u00b4 \u03b2qE\u03c1r\u03b3P\u03c0gSps, a, gq \u00b4 Sps, a, gqs, where f\u02da is conjugate function of f and S is the Lagrange dual variable defined as S : S \u02c6A\u02c6G \u00d1 R. Moreover, as strong duality holds from Slater\u2019s conditions the primal and dual share the same optimal solution \u03c0\u02dag for any offline transition distribution \u03c1.\nProof. Recall that: Mix\u03b2pd, \u03c1qps, a, gq :\u201c \u03b2dps, a, gq`p1\u00b4\u03b2q\u03c1ps, a, gq and Mix\u03b2pq, \u03c1qps, a, gq :\u201c \u03b2qps, a, gq ` p1 \u00b4 \u03b2q\u03c1ps, a, gq. Mix\u03b2pd, \u03c1qps, a, gq denotes the mixture between the current agent\u2019s joint-goal visitation distribution with an offline transition dataset potentially suboptimal and Mix\u03b2pq, \u03c1qps, a, gq is the mixture between the expert\u2019s visitation distribution with arbitrary experience from the offline transition dataset. Minimizing the divergence between these visitation distributions still solves the occupancy problem, i.e d\u03c0g \u201c q when q is achievable. We start with the primal formulation from Eq 6 for mixture divergence regularization:\nmax dps,a,gq\u011b0,\u03c0pa|sq\n\u00b4Df pMix\u03b2pd, \u03c1qps, a, gq || Mix\u03b2pq, \u03c1qps, a, gqq\ns.t dps, a, gq \u201c p1 \u00b4 \u03b3q\u03c10ps, gq.\u03c0pa|s, gq ` \u03b3\u03c0pa|s, gq \u00ff\ns1,a1\ndps1, a1, gqpps|s1, a1q.\nApplying Lagrangian duality and convex conjugate (39) to this problem, we can convert it to an unconstrained problem with dual variables Sps, a, gq defined for all s, a P S \u02c6 A \u02c6 G: max \u03c0,d\u011b0 min Sps,a,gq \u00b4Df pMix\u03b2pd, \u03c1qps, a, gq || Mix\u03b2pq, \u03c1qps, a, gqq\n` \u00ff\ns,a,g\nSps, a, gq \u02dc p1 \u00b4 \u03b3qd0ps, gq.\u03c0pa|s, gq ` \u03b3 \u00ff\ns1,a1\ndps1, a1, gqpps|s1, a1q\u03c0pa|s, gq \u00b4 dps, a, gq \u00b8\n(41) \u201c max \u03c0,d\u011b0 min Sps,a,gq p1 \u00b4 \u03b3qEd0ps,gq,\u03c0pa|s,gqrSps, a, gqs\n` Es,a,g\u201ed\n\u00ab\n\u03b3 \u00ff\ns1,a1\npps1|s, aq\u03c0pa1|s1qSps1, a1, gq \u00b4 Sps, a, gq ff\n(42)\n\u00b4Df pMix\u03b2pd, \u03c1qps, a, gq || Mix\u03b2pq, \u03c1qps, a, gqq (43)\n\u201c max \u03c0,d\u011b0 min Sps,a,gq \u03b2p1 \u00b4 \u03b3qEd0ps,gq,\u03c0pa|s,gqrSps, a, gqs\n` \u03b2Es,a,g\u201ed\n\u00ab\n\u03b3 \u00ff\ns1,a1\npps1|s, aq\u03c0pa1|s1qSps1, a1, gq \u00b4 Sps, a, gq ff\n(44)\n` p1 \u00b4 \u03b2qEs,a,g\u201e\u03c1\n\u00ab\n\u03b3 \u00ff\ns1,a1\npps1|s, aq\u03c0pa1|s1qSps1, a1, gq \u00b4 Sps, a, gq ff\n\u00b4 p1 \u00b4 \u03b2qEs,a,g\u201e\u03c1\n\u00ab\n\u03b3 \u00ff\ns1,a1\npps1|s, aq\u03c0pa1|s1, gqSps1, a1, gq \u00b4 Sps, a, gq ff\n(45)\n\u00b4Df pMix\u03b2pd, \u03c1qps, a, gq || Mix\u03b2pq, \u03c1qps, a, gqq (46)\nNow using the fact that strong duality holds in this problem we can swap the inner max and min resulting in:\n\u201c max \u03c0 min Sps,a,gq max Mix\u03b2pd,\u03c1qps,a,gq\u011b0 \u03b2p1 \u00b4 \u03b3qEd0ps,gq,\u03c0pa|s,gqrSps, a, gqs\n` \u03b2Es,a,g\u201ed\n\u00ab\n\u03b3 \u00ff\ns1,a1\npps1|s, aq\u03c0pa1|s1qSps1, a1, gq \u00b4 Sps, a, gq ff\n` p1 \u00b4 \u03b2qEs,a,g\u201e\u03c1\n\u00ab\n\u03b3 \u00ff\ns1,a1\npps1|s, aq\u03c0pa1|s1qSps1, a1, gq \u00b4 Sps, a, gq ff\n\u00b4 p1 \u00b4 \u03b2qEs,a,g\u201e\u03c1\n\u00ab\n\u03b3 \u00ff\ns1,a1\npps1|s, aq\u03c0pa1|s1, gqSps1, a1, gq \u00b4 Sps, a, gq ff\n(47)\n\u00b4Df pMix\u03b2pd, \u03c1qps, a, gq || Mix\u03b2pq, \u03c1qps, a, gqq (48) (49)\nWe can now apply the convex conjugate (Eq. (39)) definition to obtain a closed form for the inner maximization problem simplifying to:\nmax \u03c0pa|s,gq min Sps,a,gq \u03b2p1 \u00b4 \u03b3qEd0ps,gq,\u03c0pa|s,gqrSps, a, gqs\n` Es,a,g\u201eMix\u03b2pq,\u03c1qps,a,gq\n\u00ab\nf\u02dap\u03b3 \u00ff\ns1,a1\npps1|s, a, gq\u03c0pa1|s1qSps1, a1, gq \u00b4 Sps, a, gqq ff\n\u00b4 p1 \u00b4 \u03b2qEs,a,g\u201e\u03c1\n\u00ab\n\u03b3 \u00ff\ns1,a1\npps1|s, a, gq\u03c0pa1|s1qSps1, a1, gq \u00b4 Sps, a, gq ff\n(50)\nThis completes our derivation of the SMORe objective. Since strong duality holds (objective convex, constraints linear and feasible), SMORe and the primal mixture occupancy matching share the same global optima \u03c0\u02dag ."
        },
        {
            "heading": "A.4 ACTION-FREE SMORE: DUAL-V OBJECTIVE FOR OFFLINE GOAL CONDITIONED REINFORCEMENT LEARNING",
            "text": "The primal problem in Equation 6 is over-constrained. The objective determines the visitation distribution d uniquely under a fixed policy. It turns out we can further relax this constraint to get an objective that results in the same optimal solution (Agarwal et al., 2019) \u03c0\u02dag by rewriting our primal formulation as:\nmax dps,a,gq\u011b0\n\u00b4Df pMix\u03b2pd, \u03c1qps, a, gq || Mix\u03b2pq, \u03c1qps, a, gqq\ns.t \u00ff\na\ndps, a, gq \u201c p1 \u00b4 \u03b3q\u03c10ps, gq ` \u03b3 \u00ff\ns1,a1\ndps1, a1, gqpps|s1, a1q. (51)\nTheorem 2. Let yps, a, gq \u201c \u03b3Es1\u201epp\u00a8|s,aqrSps1, gqs \u00b4 Sps, gq. The action-free dual problem to the multi-task mixture occupancy matching objective (Equation 51) is given by:\nmin Sps,gq \u03b2p1 \u00b4 \u03b3qEd0ps,gqrSps, gqs\n`Es,a,g\u201eMix\u03b2pq,\u03c1qps,a,gq \u201c max ` 0, pf 1q\u00b41 pyps, a, gqq \u02d8 yps, a, gq \u00b4 f ` max ` 0, pf 1q\u00b41 pyps, a, gqq \u02d8\u02d8\u2030\n\u00b4 p1 \u00b4 \u03b2qEs,a,g\u201e\u03c1\n\u00ab\n\u03b3 \u00ff\ns1\npps1|s, aqSps1, gq \u00b4 Sps, gq ff\nwhere S is the lagrange dual variable defined as S : S \u02c6 G \u00d1 R . Moreover, strong duality holds from Slater\u2019s conditions the primal and dual share the same optimal solution \u03c0\u02dag for any offline transition distribution dO.\nProof. Proceeding as before and applying Lagrangian duality and convex conjugate (39) to this problem, we can convert it to an unconstrained problem with dual variables Sps, gq defined for all s, g P S \u02c6 G:\nmax d\u011b0 min Sps,gq\n\u00b4Df pMix\u03b2pd, \u03c1qps, a, gq || Mix\u03b2pq, \u03c1qps, a, gqq\n` \u00ff\ns,g\nSps, gq \u02dc p1 \u00b4 \u03b3qd0ps, gq ` \u03b3 \u00ff\ns1,a1,g\ndps1, a1, gqpps|s1, a1, gq \u00b4 \u00ff\na\ndps, a, gq \u00b8\n(52)\n\u201c max d\u011b0 min Sps,gq p1 \u00b4 \u03b3qEd0ps,gqrSps, gqs\n` Es,a,g\u201ed\n\u00ab\n\u03b3 \u00ff\ns1\npps1|s, aq\u03c0pa1|s1qSps1, gq \u00b4 Sps, gq ff\n(53)\n\u00b4Df pMix\u03b2pd, \u03c1qps, a, gq || Mix\u03b2pq, \u03c1qps, a, gqq (54)\n\u201c max d\u011b0 min Sps,gq \u03b2p1 \u00b4 \u03b3qEd0ps,gqrSps, gqs\n` \u03b2Es,a,g\u201ed\n\u00ab\n\u03b3 \u00ff\ns1\npps1|s, aqSps1, gq \u00b4 Sps, gq ff\n` p1 \u00b4 \u03b2qEs,a,g\u201edO \u00ab \u03b3 \u00ff\ns1\npps1|s, aqSps1, gq \u00b4 Sps, gq ff\n\u00b4 p1 \u00b4 \u03b2qEs,a,g\u201edO \u00ab \u03b3 \u00ff\ns1\npps1|s, aqSps1, gq \u00b4 Sps, gq ff\n(55)\n\u00b4Df pMix\u03b2pd, \u03c1qps, a, gq || Mix\u03b2pq, \u03c1qps, a, gqq (56)\nNow using the fact that strong duality holds in this problem we can swap the inner max and min resulting in:\n\u201c min Sps,gq max Mix\u03b2pd,\u03c1qps,a,gq\u011b0 \u03b2p1 \u00b4 \u03b3qEd0ps,gqrSps, gqs\n` \u03b2Es,a,g\u201ed\n\u00ab\n\u03b3 \u00ff\ns1\npps1|s, aqSps1, gq \u00b4 Sps, gq ff\n` p1 \u00b4 \u03b2qEs,a,g\u201edO \u00ab \u03b3 \u00ff\ns1\npps1|s, aqSps1, gq \u00b4 Sps, gq ff\n\u00b4 p1 \u00b4 \u03b2qEs,a,g\u201edO \u00ab \u03b3 \u00ff\ns1\npps1|s, aqSps1, gq \u00b4 Sps, gq ff\n(57)\n\u00b4Df pMix\u03b2pd, \u03c1qps, a, gq || Mix\u03b2pq, \u03c1qps, a, gqq (58)\nUnlike previous case where constraints uniquely define a valid d for any given \u03c0, in this case we need to take into account the hidden constraint d \u011b 0 or equivalently Mix\u03b2pd, \u03c1qps, a, gq \u011b 0. To incorporate the non-negativity constraints we consider the inner maximization separately and derive a closed-form solution that adheres to the non-negativity constraints. Recall yps, a, gq \u201c Es1\u201epps,aqrSps1, gqs \u00b4 Sps, gq.\nmax Mix\u03b2pd,\u03c1qps,a,gq\u011b0 Es,a,g\u201eMix\u03b2pd,\u03c1qps,a,gq\n\u00ab\n\u03b3 \u00ff\ns1\npps1|s, aqSps1, gq \u00b4 Sps, gq ff\n\u00b4Df pMix\u03b2pd, \u03c1qps, a, gq || Mix\u03b2pq, \u03c1qps, a, gqq We can now construct the Lagrangian dual to incorporate the constraint Mix\u03b2pd, \u03c1qps, a, gq \u011b 0 in its equivalent form wps, a, gq \u011b 0 and obtain the following where w \u2206\u201c Mix\u03b2pd,\u03c1qps,a,gqMix\u03b2pq,\u03c1qps,a,gq :\nmax wps,a,gq max \u03bb\u011b0\nEs,a\u201eMix\u03b2pq,\u03c1qps,a,gqrwps, a, gqyps, a, gqs \u00b4 EMix\u03b2pq,\u03c1qps,a,gqrfpwps, a, gqqs ` \u00ff\ns,a,g\n\u03bbpwps, a, gq \u00b4 0q\n(59)\nSince strong duality holds, we can use the KKT constraints to find the solutions w\u02daps, a, gq and \u03bb\u02daps, a, gq.\n1. Primal feasibility: w\u02daps, a, gq \u011b 0 @ s, a\n2. Dual feasibility: \u03bb\u02da \u011b 0 @ s, a\n3. Stationarity: Mix\u03b2pq, \u03c1qps, a, gqp\u00b4f 1pw\u02daps, a, gqq ` yps, a, gq ` \u03bb\u02daps, a, gqq \u201c 0 @ s, a\n4. Complementary Slackness: pw\u02daps, a, gq \u00b4 0q\u03bb\u02daps, a, gq \u201c 0 @ s, a\nUsing stationarity we have the following: f 1pw\u02daps, a, gqq \u201c yps, a, gq ` \u03bb\u02daps, a, gq @ s, a, g (60) Now using complementary slackness, only two cases are possible w\u02daps, a, gq \u011b 0 or \u03bb\u02daps, a, gq \u011b 0. Combining both cases we arrive at the following solution for this constrained optimization:\nw\u02daps, aq \u201c max \u00b4 0, f 1 \u00b41pyps, a, gqq \u00af\n(61)\nUsing the optimal closed-form solution (w\u02da) for Mix\u03b2pd, \u03c1qps, a, gq of the inner optimization in Eq. (57) we obtain\nmin Sps,aq \u03b2p1 \u00b4 \u03b3qEd0psqrSps, gqs\n` Es,a,g\u201eMix\u03b2pq,\u03c1qps,a,gq \u201c max ` 0, pf 1q\u00b41 pyps, a, gqq \u02d8 yps, a, gq \u00b4 \u03b1f ` max ` 0, pf 1q\u00b41 pyps, a, gqq \u02d8\u02d8\u2030\n\u00b4 p1 \u00b4 \u03b1qEs,a\u201e\u03c1\n\u00ab\n\u03b3 \u00ff\ns1\npps1|s, aq\u03c0pa1|s1qSps1, gq \u00b4 Sps, gq ff\n(62)\nFor deterministic dynamics, this reduces to the action-free SMORe objective: min Sps,aq \u03b2p1 \u00b4 \u03b3qEd0psqrSps, gqs\n` Es,a\u201eMix\u03b2pq,\u03c1qps,a,gq \u201c max ` 0, pf 1q\u00b41 pyps, a, gqq \u02d8 yps, a, gq \u00b4 f ` max ` 0, pf 1q\u00b41 pyps, a, gqq \u02d8\u02d8\u2030 \u00b4 p1 \u00b4 \u03b2qEs,a\u201e\u03c1 \u201c \u03b3Sps1, gq \u00b4 Sps, gq \u2030 (63)\nwhere yps, a, gq \u201c \u03b3Sps1, gq \u00b4 Sps, gq. Note that we no longer need actions in the offline dataset to learn an optimal goal conditioned score function. This score function can be used to learn presentation in action-free datasets as well as for transfer of value function across differing action-modalities where agents share the same observation space (eg. images as observations).\nB SMORE ALGORITHMIC DETAILS\nB.1 SMORE WITH COMMON f -DIVERGENCES\na. KL divergence\nWe consider the reverse KL divergence and start with the general SMORe objective: max \u03c0g min S \u03b2p1 \u00b4 \u03b3qEd0,\u03c0g rSps, a, gqs ` Es,a,g\u201eMix\u03b2pq,\u03c1qps,a,gqrf \u02dap\u03b3P\u03c0gSps, a, gq \u00b4 Sps, a, gqqs\n\u00b4 p1 \u00b4 \u03b2qEs,a,g\u201e\u03c1r\u03b3P\u03c0gSps, a, gq \u00b4 Sps, a, gqs (64)\nPlugging in the conjugate f\u02da for reverse KL divergence we get:\nmax \u03c0g min S \u03b2p1 \u00b4 \u03b3qEd0,\u03c0g rSps, a, gqs ` Es,a,g\u201eMix\u03b2pq,\u03c1qps,a,gq\n\u201d\nep\u03b3P \u03c0gSps,a,gq\u00b4Sps,a,gqq\n\u0131\n\u00b4 p1 \u00b4 \u03b2qEs,a,g\u201e\u03c1r\u03b3P\u03c0gSps, a, gq \u00b4 Sps, a, gqs (65)\nUsing the telescoping sum for the last term in the objective above, we can simplify it as follows:\nmax \u03c0g min S \u03b2p1 \u00b4 \u03b3qEd0,\u03c0g rSps, a, gqs ` Es,a,g\u201eMix\u03b2pq,\u03c1qps,a,gq\n\u201d\nep\u03b3P \u03c0gSps,a,gq\u00b4Sps,a,gqq\n\u0131\n` p1 \u00b4 \u03b2qEs,g\u201ed0,a\u201e\u03c1p\u00a8|s,gqrSps, a, gqs (66)\nWith the initial state distribution d0 set to the offline dataset distribution \u03c1, and Since our initial state distribution is the same as offline data distribution, we get:\nmax \u03c0g min S \u03b2p1 \u00b4 \u03b3qE\u03c1,\u03c0g rSps, a, gqs ` Es,a,g\u201eMix\u03b2pq,\u03c1qps,a,gq\n\u201d\nep\u03b3P \u03c0gSps,a,gq\u00b4Sps,a,gqq\n\u0131\n` p1 \u00b4 \u03b2qE\u03c1rSps, a, gqs (67)\nCollecting terms together we get:\nmax \u03c0g min Q\nE\u03c1rEa\u201e\u03c0r\u03b2p1 \u00b4 \u03b3qSps, a, gqs ` Ea\u201e\u03c1rp1 \u00b4 \u03b2qSps, a, gqss\n` Es,a,g\u201eMix\u03b2pq,\u03c1qps,a,gq \u201d ep\u03b3P \u03c0gSps,a,gq\u00b4Sps,a,gqq \u0131 (68)\nThe objective for SMORe with reverse KL divergence pushes down the \u201dscore\u201d of offline dataset transitions selectively (without pushing down score of the goal-transition distribution) while minimizing the term resembling bellman regularization that also encourages increasing score at the mixture dataset jointly over the offline dataset as well as the goal transition distribution.\nb. Pearson chi-squared divergence\nWe consider the Pearson \u03c72 and start with the general SMORe objective: max \u03c0g min S \u03b2p1 \u00b4 \u03b3qEd0,\u03c0g rSps, a, gqs ` Es,a,g\u201eMix\u03b2pq,\u03c1qps,a,gqrf \u02dap\u03b3P\u03c0gSps, a, gq \u00b4 Sps, a, gqqs\n\u00b4 p1 \u00b4 \u03b2qEs,a,g\u201e\u03c1r\u03b3P\u03c0gSps, a, gq \u00b4 Sps, a, gqs (69)\nWith the initial state distribution d0 set to the offline dataset distribution \u03c1, and plugging in the conjugate f\u02da for Pearson \u03c72 divergence we get:\nmax \u03c0g min S \u03b2p1\u00b4\u03b3qEd0,\u03c0g rSps, a, gqs`0.25Es,a,g\u201eMix\u03b2pq,\u03c1qps,a,gq\n\u201c p\u03b3P\u03c0gSps, a, gq \u00b4 Sps, a, gqq2 \u2030\n`Es,a,g\u201eMix\u03b2pq,\u03c1qps,a,gqrp\u03b3P \u03c0gSps, a, gq \u00b4 Sps, a, gqqs\u00b4p1\u00b4\u03b2qEs,a,g\u201e\u03c1r\u03b3P\u03c0gSps, a, gq \u00b4 Sps, a, gqs\n(70)\nUsing the fact that Mix\u03b2pq, \u03c1qps, a, gq \u201c \u03b2qps, a, gq ` p1\u00b4 \u03b2q\u03c1ps, a, gq, we can further simplify the above equation to:\nmax \u03c0g min S \u03b2p1\u00b4\u03b3qEd0,\u03c0g rSps, a, gqs`0.25Es,a,g\u201eMix\u03b2pq,\u03c1qps,a,gq\n\u201c p\u03b3P\u03c0gSps, a, gq \u00b4 Sps, a, gqq2 \u2030\n` \u03b2Es,a,g\u201eqrp\u03b3P\u03c0gSps, a, gq \u00b4 Sps, a, gqqs (71)\nCollecting terms together we get:\nmax \u03c0g min S \u03b2p1 \u00b4 \u03b3qE\u03c1,\u03c0g rSps, a, gqs ` \u03b2Es,g\u201eq,a\u201e\u03c0g r\u03b3P\u03c0gSps, a, gqs\n\u00b4 \u03b2Es,a,g\u201eqrSps, a, gqs ` 0.25Es,a,g\u201eMix\u03b2pq,\u03c1qps,a,gq \u201c p\u03b3P\u03c0gSps, a, gq \u00b4 Sps, a, gqq2 \u2030 (72)\nObserving the equation above, we note that the first two terms decrease score at offline data distribution as well as the goal transition distribution when actions are sampled according to the policy \u03c0g. Simultaneously the third term pushes score up for the ts, a, gu tuples that are sampled from goal transition distribution. Finally the last term encouraged enforces a bellman regularization enforcing smoothness is the scores of neighbouring states.\nC SMORE EXPERIMENTAL DETAILS"
        },
        {
            "heading": "C.1 TASKS WITH OBSERVATIONS AS STATES",
            "text": "Environments: For the offline GCRL experiments we consider the benchmark used in prior work GoFar and extend it with locomotion tasks. For the manipulations tasks we consider the Fetch environment and a dextrous shadow hand environment. Fetch environments (Plappert et al., 2018) consists of a manipulator with seven degrees of freedom along with a parallel gripper. The set of environments get a sparse reward of 1 when the goal is within 5 cm and 0 otherwise. The action space is 4 dimensional (3 dimension cartesian control + 1 dimension gripper control). The shadow hand is 24 DOF manipulator with 20-dimensional action space. The goal is 15-dimension specifying the position for each of the five fingers. The tolerance for goal reaching is 1 cm. For the locomotion environments, the task is to achieve a particular velocity in the x direction and stay at the velocity. For HalfCheetah, the target velocity is set to 11.0 and for Ant the target velocity is 5.0. For locomotion environments, the tolerance for goal reaching if 0.5. The MuJoCo environments used in this work are licensed under CC BY 4.0.\nOffline Datasets: We use existing datasets from the offline GCRL benchmark used in (Ma et al., 2022) for all manipulation tasks except Reacher, SawyerReach, and SawyerDoor. For Reacher, SawyerReach, and SawyerDoor we use existing datasets from (Yang et al., 2022). These datasets are comprised on x% random data and (100-x)% expert data depending on the coverage over goals reached in individual datasets. We create our own datasets for locomotion by using \u2019random/medium/mediumreplay\u2019 data as our offline (suboptimal) data combined with 30 trajectories from corresponding \u2019expert\u2019 datasets. The datasets used from D4RL are licensed under Apache 2.0.\nBaselines: To benchmark and analyze the performance of our proposed methods for offline imitation learning with suboptimal data, we consider the following representative baselines in this work: GoFAR (Ma et al., 2022), WGCSL (Yang et al., 2022), GCSL (Ghosh et al., 2019), and Actionable Models (Chebotar et al., 2021), Contrastive RL (Eysenbach et al., 2020) and GC-IQL Kostrikov et al. (2021). GoFAR is a dual occupancy matching approach to GCRL that formulates it as a weighted regression problem. WGCSL and GSCL use goal-conditioned behavior cloning with goal relabelling as the base algorithms and WGCL uses weights to learn improved policy over GCSL. Actionable models uses conservative learning with goal chaining to learn goal-reaching behaviours using offline datasets. Contrastive RL treats GCRL as a classification problem - contrastive goals that are achieved in trajectory from random goals. Finally, GC-IQL extends the single task offline RL algorithm IQL to GCRL.\nThe open-source implementations of the baselines GoFAR, WGCSL, GCSL, Actionable models, Contrastive RL and IQL are provided by the authors (Ma et al., 2022) and employed in our experiments. We use the hyperparameters provided by the authors, which are consistent with those used in the original GoFAR paper, for all the MuJoCo locomotion and manipulation environments. We implement\ncontrastive learning using the code from Contrastive RL repository. GC-IQL is implemented using code from author\u2019s implementation found here.\nArchitecture and Hyperparameters For the baselines, we use tuned hyperparameters from previous works that were tuned on the same set of tasks and datasets. Implementation for SMORe shares the same network architecture as baselines. GoFAR additionally requires training a discriminator. For all experiments, all methods are trained for 10 seeds with each training run. Fetch manipulation (except Push) tasks are trained for a maximum of 400k minibatch updates of size 512 whereas all other environments training is done for 1M minibatch updates. The expectile parameter \u03c4 was searched over [0.65 ,0.7,0.8,0.85]. For the results shown in table 1, Fetch and Sawyer environments use \u03c4 \u201c 0.8, Locomotion and Adroit hand environments use \u03c4 \u201c 0.7. In general, the HER ratio is searched over [0.2,0.5,0.8,1.0] for all methods and the best one was selected. HER ratio of 0.8 gave best performance across all tasks for SMORe."
        },
        {
            "heading": "C.2 TASKS WITH OBSERVATIONS AS IMAGES",
            "text": "Tasks and dataset Our experiments use a suite of simulated goal-conditioned control tasks based on prior work Zheng et al. (2023). The observations and goals are 48 \u02c6 48 \u02c6 3 RGB images. The evaluation tasks require multi-trajectory stitching whereas the dataset contains trajectories solving only parts of the evaluation tasks.\nIn the simulation, we employed an offline manipulation dataset comprising near-optimal examples of basic action sequences, including the initiation of drawer movement, the displacement of blocks, and the grasping of items. The demonstrations exhibit variations in length, ranging between 50 to 100 horizon, while the offline dataset contains a total of 250,000 state transitions in its entirety. It is important to note that the offline trajectories do not depict a complete progression from the initial condition to the final objective. For the purposes of evaluation, we consider 4 tasks similar to Zheng et al. (2023), against which we compare the success rates in realizing these specified objectives.\nBaseline and SMORe implementations. We use the open-source implementation of Stablecontrastive RL to use the same design decisions and implement SMORe, GC-IQL on that codebase. We use the same hyperparameters as the stable-contrastive RL implementation for the shared hyperparameters. The hyperparameters for SMORe were kept the same as in Table 4."
        },
        {
            "heading": "D ADDITIONAL EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "D.1 RESULTS ON OFFLINE GCRL BENCHMARK WITH VARYING EXPERT COVERAGE IN OFFLINE DATASET",
            "text": "We ablate the effect of dataset quality on the performance of an offline GCRL method in this sections. Table 6, 7, 8 show performance of all methods with 5%, 2.5% and 1% expert data in the offline dataset respectively."
        },
        {
            "heading": "D.2 SUCCESS RATE AND FINAL DISTANCE TO GOAL ON MANIPULATION TASKS",
            "text": "Table 10 and Table 11 reports the success rate and final distance to goal metrics on manipulation tasks.\nD.3 ROBUSTNESS OF MIXTURE DISTRIBUTION PARAMETER \u03b2\nWe find that SMORe is quite robust to the mixture distribution parameter \u03b2 except in the environment FetchPush where \u03b2 \u201c 0.5 is the most performant. Table 9 shows this result empirically."
        },
        {
            "heading": "D.4 HOW MUCH DOES HER CONTRIBUTE TO THE PERFORMANCE IMPROVEMENTS OF",
            "text": "SMORE?\nGoFAR demonstrated improved performance without relying on HER. The authors also demonstrated that HER is detrimental to GoFAR\u2019s performance. In this section, we aim to conduct a similar stufy and see how much HER contributed to SMORe\u2019s performance. Table 12 shows that HER gives SMORe a small performance boost and show that SMORe is still able to outperform GoFAR without HER."
        },
        {
            "heading": "D.5 COMPARISON TO VARIANTS OF GOFAR",
            "text": "GoFAR formulates GCRL as an occupancy matching problem, but it is also suggested that using a discriminator is optional. Without a discriminator, GoFAR reduces to a sparse reward RL problem. Table 13 shows that GoFAR achieves poor performance when a reward function is substituted in place on an discriminator. We also study if the performance benefits we obtain are due to the offline learning strategy we used from IQL. We modify GoFAR with discriminator reward to use expectile loss for value learning and AWR for policy learning. Results in Table 13 shows that no performance gains were observed."
        },
        {
            "heading": "D.6 OFFLINE GCRL WITH PURELY SUBOPTIMAL DATA",
            "text": "In this experiment, we study offline GCRL from purely suboptimal datasets. Except FetchReach, these datasets provide very sparse coverage of goals expected to reach in evaluation. Table 14 shows the robustness of SMORe even in the setting of poor quality offline data."
        },
        {
            "heading": "D.7 COMPARISON WITH IN-SAMPLE LEARNING METHODS",
            "text": "In-sample learning methods perform value improvement using bellman backups without OOD action sampling. This makes them a particularly suitable candidate for offline setting. We compare against a number of recent in-sample learning methods, IQL (Kostrikov et al., 2021), SQL/EQL Xu et al. (2023) and XQL (Garg et al., 2023). Table 15 compares SMORe to in-sample learning methods adapted to GCRL.\nD.8 ABLATING COMPONENTS OF SMORE FOR OFFLINE SETTING\nIn offline setting, it is well known that bellman backups suffer from overestimation and results in poor policy performance. We validate the utility of the components used in this work in Table 16 : expectile loss function and constrained policy optimization with AWR."
        }
    ],
    "title": "GOAL-CONDITIONED REINFORCEMENT LEARNING",
    "year": 2024
}