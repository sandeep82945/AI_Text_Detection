{
    "abstractText": "We present a new accelerated stochastic second-order method that is robust to both gradient and Hessian inexactness, which occurs typically in machine learning. We establish theoretical lower bounds and prove that our algorithm achieves optimal convergence in both gradient and Hessian inexactness in this key setting. We further introduce a tensor generalization for stochastic higher-order derivatives. When the oracles are non-stochastic, the proposed tensor algorithm matches the global convergence of Nesterov Accelerated Tensor method. Both algorithms allow for approximate solutions of their auxiliary subproblems with verifiable conditions on the accuracy of the solution.",
    "authors": [],
    "id": "SP:3f2af94da5ee690baba1ddbe62310c26a8fb20f2",
    "references": [
        {
            "authors": [
                "Artem Agafonov",
                "Pavel Dvurechensky",
                "Gesualdo Scutari",
                "Alexander Gasnikov",
                "Dmitry Kamzolov",
                "Aleksandr Lukashevich",
                "Amir Daneshmand"
            ],
            "title": "An accelerated second-order method for distributed stochastic optimization",
            "venue": "In 2021 60th IEEE Conference on Decision and Control (CDC),",
            "year": 2021
        },
        {
            "authors": [
                "Artem Agafonov",
                "Dmitry Kamzolov",
                "Pavel Dvurechensky",
                "Alexander Gasnikov",
                "Martin Tak\u00e1\u010d"
            ],
            "title": "Inexact tensor methods and their application to stochastic convex optimization",
            "venue": "Optimization Methods and Software,",
            "year": 2023
        },
        {
            "authors": [
                "Naman Agarwal",
                "Elad Hazan"
            ],
            "title": "Lower bounds for higher-order convex optimization",
            "venue": "Proceedings of the 31st Conference On Learning Theory,",
            "year": 2018
        },
        {
            "authors": [
                "Kimon Antonakopoulos",
                "Ali Kavis",
                "Volkan Cevher"
            ],
            "title": "Extra-newton: A first approach to noise-adaptive accelerated second-order methods",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Yossi Arjevani",
                "Ohad Shamir",
                "Ron Shiff"
            ],
            "title": "Oracle complexity of second-order methods for smooth convex optimization",
            "venue": "Mathematical Programming,",
            "year": 2019
        },
        {
            "authors": [
                "Michel Baes"
            ],
            "title": "Estimate sequence methods: extensions and approximations",
            "venue": "Institute for Operations Research, ETH, Zu\u0308rich, Switzerland,",
            "year": 2009
        },
        {
            "authors": [
                "Stefania Bellavia",
                "Gianmarco Gurioli"
            ],
            "title": "Stochastic analysis of an adaptive cubic regularization method under inexact gradient evaluations and dynamic hessian accuracy",
            "year": 2022
        },
        {
            "authors": [
                "Stefania Bellavia",
                "Gianmarco Gurioli",
                "Benedetta Morini",
                "Ph L Toint"
            ],
            "title": "Adaptive regularization for nonconvex optimization using inexact function values and randomly perturbed derivatives",
            "venue": "Journal of Complexity,",
            "year": 2022
        },
        {
            "authors": [
                "S\u00e9bastien Bubeck",
                "Qijia Jiang",
                "Yin Tat Lee",
                "Yuanzhi Li",
                "Aaron Sidford"
            ],
            "title": "Near-optimal method for highly smooth convex optimization",
            "venue": "In Conference on Learning Theory,",
            "year": 2019
        },
        {
            "authors": [
                "Yair Carmon",
                "Danielle Hausler",
                "Arun Jambulapati",
                "Yujia Jin",
                "Aaron Sidford"
            ],
            "title": "Optimal and adaptive monteiro-svaiter acceleration",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Coralia Cartis",
                "Katya Scheinberg"
            ],
            "title": "Global convergence rate analysis of unconstrained optimization methods based on probabilistic models",
            "venue": "Mathematical Programming,",
            "year": 2018
        },
        {
            "authors": [
                "Coralia Cartis",
                "Nicholas IM Gould",
                "Philippe L Toint"
            ],
            "title": "Adaptive cubic regularisation methods for unconstrained optimization. part i: motivation, convergence and numerical results",
            "venue": "Mathematical Programming,",
            "year": 2011
        },
        {
            "authors": [
                "Coralia Cartis",
                "Nicholas IM Gould",
                "Philippe L Toint"
            ],
            "title": "Adaptive cubic regularisation methods for unconstrained optimization. part ii: worst-case function-and derivative-evaluation complexity",
            "venue": "Mathematical programming,",
            "year": 2011
        },
        {
            "authors": [
                "Coralia Cartis",
                "Nicholas IM Gould",
                "Philippe L Toint"
            ],
            "title": "Improved second-order evaluation complexity for unconstrained nonlinear optimization using high-order regularized models",
            "venue": "arXiv preprint arXiv:1708.04044,",
            "year": 2017
        },
        {
            "authors": [
                "Chih-Chung Chang",
                "Chih-Jen Lin"
            ],
            "title": "Libsvm: A library for support vector machines",
            "venue": "ACM transactions on intelligent systems and technology (TIST),",
            "year": 2011
        },
        {
            "authors": [
                "Amir Daneshmand",
                "Gesualdo Scutari",
                "Pavel Dvurechensky",
                "Alexander Gasnikov"
            ],
            "title": "Newton method over networks is fast up to the statistical precision",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Olivier Devolder",
                "Fran\u00e7ois Glineur",
                "Yurii Nesterov"
            ],
            "title": "First-order methods of smooth convex optimization with inexact oracle",
            "venue": "Mathematical Programming,",
            "year": 2014
        },
        {
            "authors": [
                "Nikita Doikov",
                "Yurii Nesterov"
            ],
            "title": "Gradient regularization of newton method with bregman distances",
            "venue": "Mathematical Programming,",
            "year": 2023
        },
        {
            "authors": [
                "Nikita Doikov",
                "Yurii E Nesterov"
            ],
            "title": "Inexact tensor methods with dynamic accuracies",
            "venue": "In ICML,",
            "year": 2020
        },
        {
            "authors": [
                "Nikita Doikov",
                "Konstantin Mishchenko",
                "Yurii Nesterov"
            ],
            "title": "Super-universal regularized newton method",
            "venue": "arXiv preprint arXiv:2208.05888,",
            "year": 2022
        },
        {
            "authors": [
                "Nikita Doikov",
                "El Mahdi Chayti",
                "Martin Jaggi"
            ],
            "title": "Second-order optimization with lazy hessians",
            "venue": "Proceedings of the 40th International Conference on Machine Learning,",
            "year": 2023
        },
        {
            "authors": [
                "Pavel Dvurechensky",
                "Dmitry Kamzolov",
                "Aleksandr Lukashevich",
                "Soomin Lee",
                "Erik Ordentlich",
                "C\u00e9sar A Uribe",
                "Alexander Gasnikov"
            ],
            "title": "Hyperfast second-order local solvers for efficient statistically preconditioned distributed optimization",
            "venue": "EURO Journal on Computational Optimization,",
            "year": 2022
        },
        {
            "authors": [
                "Alexander Gasnikov",
                "Pavel Dvurechensky",
                "Eduard Gorbunov",
                "Evgeniya Vorontsova",
                "Daniil Selikhanovych",
                "C\u00e9sar A Uribe"
            ],
            "title": "Optimal tensor methods in smooth convex and uniformly convexoptimization",
            "venue": "In Conference on Learning Theory,",
            "year": 2019
        },
        {
            "authors": [
                "Alexander Gasnikov",
                "Pavel Dvurechensky",
                "Eduard Gorbunov",
                "Evgeniya Vorontsova",
                "Daniil Selikhanovych",
                "C\u00e9sar A Uribe",
                "Bo Jiang",
                "Haoyue Wang",
                "Shuzhong Zhang",
                "S\u00e9bastien Bubeck",
                "Qijia Jiang",
                "Yin Tat Lee",
                "Yuanzhi Li",
                "Aaron Sidford"
            ],
            "title": "Near optimal methods for minimizing convex functions with lipschitz p-th derivatives",
            "venue": "Proceedings of the Thirty-Second Conference on Learning Theory,",
            "year": 2019
        },
        {
            "authors": [
                "Saeed Ghadimi",
                "Guanghui Lan"
            ],
            "title": "Optimal stochastic approximation algorithms for strongly convex stochastic composite optimization, ii: shrinking procedures and optimal algorithms",
            "venue": "SIAM Journal on Optimization,",
            "year": 2013
        },
        {
            "authors": [
                "Saeed Ghadimi",
                "Han Liu",
                "Tong Zhang"
            ],
            "title": "Second-order methods with cubic regularization under inexact information",
            "venue": "arXiv preprint arXiv:1710.05782,",
            "year": 2017
        },
        {
            "authors": [
                "Hiva Ghanbari",
                "Katya Scheinberg"
            ],
            "title": "Proximal quasi-newton methods for regularized convex optimization with linear and accelerated sublinear convergence rates",
            "venue": "Computational Optimization and Applications,",
            "year": 2018
        },
        {
            "authors": [
                "Geovani Nunes Grapiglia",
                "Yu Nesterov"
            ],
            "title": "Tensor methods for minimizing convex functions with h\u00f6lder continuous higher-order derivatives",
            "venue": "SIAM Journal on Optimization,",
            "year": 2020
        },
        {
            "authors": [
                "G.N. Grapiglia",
                "Yu. Nesterov"
            ],
            "title": "On inexact solution of auxiliary problems in tensor methods for convex optimization",
            "venue": "Optimization Methods and Software,",
            "year": 2021
        },
        {
            "authors": [
                "Andreas Griewank"
            ],
            "title": "The modification of newton\u2019s method for unconstrained optimization by bounding cubic terms",
            "venue": "Technical report, Technical report NA/12,",
            "year": 1981
        },
        {
            "authors": [
                "K H Hoffmann",
                "H J Kornstaedt"
            ],
            "title": "Higher-order necessary conditions in abstract mathematical programming",
            "venue": "Journal of Optimization Theory and Applications,",
            "year": 1978
        },
        {
            "authors": [
                "Bo Jiang",
                "Haoyue Wang",
                "Shuzhong Zhang"
            ],
            "title": "An optimal high-order tensor method for convex optimization",
            "venue": "In Conference on Learning Theory,",
            "year": 2019
        },
        {
            "authors": [
                "Ruichen Jiang",
                "Aryan Mokhtari"
            ],
            "title": "Accelerated quasi-newton proximal extragradient: Faster rate for smooth convex optimization",
            "venue": "arXiv preprint arXiv:2306.02212,",
            "year": 2023
        },
        {
            "authors": [
                "Ruichen Jiang",
                "Qiujiang Jin",
                "Aryan Mokhtari"
            ],
            "title": "Online learning guided curvature approximation: A quasi-newton method with global non-asymptotic superlinear convergence",
            "venue": "CoRR, abs/2302.08580,",
            "year": 2023
        },
        {
            "authors": [
                "Dmitry Kamzolov"
            ],
            "title": "Near-optimal hyperfast second-order method for convex optimization",
            "venue": "Mathematical Optimization Theory and Operations Research,",
            "year": 2020
        },
        {
            "authors": [
                "Dmitry Kamzolov",
                "Alexander Gasnikov",
                "Pavel Dvurechensky",
                "Artem Agafonov",
                "Martin Tak\u00e1\u010d"
            ],
            "title": "Exploiting higher-order derivatives in convex optimization methods",
            "venue": "arXiv preprint arXiv:2208.13190,",
            "year": 2022
        },
        {
            "authors": [
                "Dmitry Kamzolov",
                "Klea Ziu",
                "Artem Agafonov",
                "Martin Tak\u00e1\u010d"
            ],
            "title": "Accelerated adaptive cubic regularized quasi-newton methods",
            "venue": "arXiv preprint arXiv:2302.04987,",
            "year": 2023
        },
        {
            "authors": [
                "Leonid Vitalyevich Kantorovich"
            ],
            "title": "On newton\u2019s method",
            "venue": "Trudy Matematicheskogo Instituta imeni VA Steklova,",
            "year": 1949
        },
        {
            "authors": [
                "Jonas Moritz Kohler",
                "Aurelien Lucchi"
            ],
            "title": "Sub-sampled cubic regularization for non-convex optimization",
            "venue": "In International Conference on Machine Learning,",
            "year": 2017
        },
        {
            "authors": [
                "Dmitry Kovalev",
                "Alexander Gasnikov"
            ],
            "title": "The first optimal acceleration of high-order methods in smooth convex optimization",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Guanghui Lan"
            ],
            "title": "An optimal method for stochastic composite optimization",
            "venue": "Mathematical Programming,",
            "year": 2012
        },
        {
            "authors": [
                "Aurelien Lucchi",
                "Jonas Kohler"
            ],
            "title": "A sub-sampled tensor method for nonconvex optimization",
            "venue": "IMA Journal of Numerical Analysis, 43:2856\u20132891,",
            "year": 2023
        },
        {
            "authors": [
                "Renato D C Monteiro",
                "B F Svaiter"
            ],
            "title": "An accelerated hybrid proximal extragradient method for convex optimization and its implications to second-order methods",
            "venue": "SIAM Journal on Optimization,",
            "year": 2013
        },
        {
            "authors": [
                "Jorge J. Mor\u00e9"
            ],
            "title": "The levenberg\u2013marquardt algorithm: implementation and theory",
            "venue": "In Conference on Numerical Analysis, University of Dundee, Scotland,",
            "year": 1977
        },
        {
            "authors": [
                "A Nemirovski",
                "A Juditsky",
                "G Lan",
                "A Shapiro"
            ],
            "title": "Robust stochastic approximation approach to stochastic programming",
            "venue": "SIAM Journal on Optimization,",
            "year": 2009
        },
        {
            "authors": [
                "Arkadi Semenovich Nemirovski",
                "David Borisovich Yudin"
            ],
            "title": "Problem Complexity and Method Efficiency in Optimization",
            "venue": "A Wiley-Interscience publication. Wiley,",
            "year": 1983
        },
        {
            "authors": [
                "Yurii Nesterov"
            ],
            "title": "Accelerating the cubic regularization of newton\u2019s method on convex problems",
            "venue": "Mathematical Programming,",
            "year": 2008
        },
        {
            "authors": [
                "Yurii Nesterov"
            ],
            "title": "Lectures on Convex Optimization. Springer Cham, 2 edition, 2018",
            "venue": "ISBN 978-3-31991577-7",
            "year": 1991
        },
        {
            "authors": [
                "Yurii Nesterov"
            ],
            "title": "Inexact high-order proximal-point methods with auxiliary search procedure",
            "venue": "SIAM Journal on Optimization,",
            "year": 2021
        },
        {
            "authors": [
                "Yurii Nesterov"
            ],
            "title": "Implementable tensor methods in unconstrained convex optimization",
            "venue": "Mathematical Programming,",
            "year": 2021
        },
        {
            "authors": [
                "Yurii Nesterov"
            ],
            "title": "Superfast second-order methods for unconstrained convex optimization",
            "venue": "Journal of Optimization Theory and Applications,",
            "year": 2021
        },
        {
            "authors": [
                "Yurii Nesterov",
                "Boris T Polyak"
            ],
            "title": "Cubic regularization of newton method and its global performance",
            "venue": "Mathematical Programming,",
            "year": 2006
        },
        {
            "authors": [
                "Boris T Polyak",
                "Anatoli B Juditsky"
            ],
            "title": "Acceleration of stochastic approximation by averaging",
            "venue": "SIAM Journal on Control and Optimization,",
            "year": 1992
        },
        {
            "authors": [
                "Boris Teodorovich Polyak"
            ],
            "title": "A new method of stochastic approximation type",
            "venue": "Avtomatika i Telemekhanika,",
            "year": 1990
        },
        {
            "authors": [
                "Roman Polyak"
            ],
            "title": "Complexity of the regularized newton method",
            "venue": "arXiv preprint arXiv:1706.08483,",
            "year": 2017
        },
        {
            "authors": [
                "Roman A Polyak"
            ],
            "title": "Regularized newton method for unconstrained convex optimization",
            "venue": "Mathematical Programming,",
            "year": 2009
        },
        {
            "authors": [
                "Herbert Robbins",
                "Sutton Monro"
            ],
            "title": "A stochastic approximation method",
            "venue": "The Annals of Mathematical Statistics,",
            "year": 1951
        },
        {
            "authors": [
                "Katya Scheinberg",
                "Xiaocheng Tang"
            ],
            "title": "Practical inexact proximal quasi-newton method with global complexity analysis",
            "venue": "Mathematical Programming,",
            "year": 2016
        },
        {
            "authors": [
                "Damien Scieur"
            ],
            "title": "Adaptive quasi-newton and anderson acceleration framework with explicit global (accelerated) convergence rates",
            "venue": "arXiv preprint arXiv:2305.19179,",
            "year": 2023
        },
        {
            "authors": [
                "Nilesh Tripuraneni",
                "Mitchell Stern",
                "Chi Jin",
                "Jeffrey Regier",
                "Michael I Jordan"
            ],
            "title": "Stochastic cubic regularization for fast nonconvex optimization",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Blake Woodworth",
                "Nathan Srebro"
            ],
            "title": "Lower bound for randomized first order convex optimization",
            "venue": "arXiv preprint arXiv:1709.03594,",
            "year": 2017
        },
        {
            "authors": [
                "Peng Xu",
                "Fred Roosta",
                "Michael W Mahoney"
            ],
            "title": "Newton-type methods for non-convex optimization under inexact hessian information",
            "venue": "Mathematical Programming,",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "In this paper, we consider the following general convex optimization problem: min x2Rd f(x), (1) where f(x) is a convex and sufficiently smooth function. We assume that a solution x\u21e4 2 Rd exists and we denote f\u21e4 := f(x\u21e4). We define R = kx0 x\u21e4k as a distance to the solution. Assumption 1.1. The function f(x) 2 C2 has L2-Lipschitz-continuous Hessian if for any x, y 2 Rd\nkr 2f(x) r2f(y)k  L2kx yk.\nSince the calculation of an exact gradient is very expensive or impossible in many applications in several domains, including machine learning, statistics, and signal processing, efficient methods that can work with inexact stochastic gradients are of great interest. Assumption 1.2. For all x 2 Rd, we assume that stochastic gradients g(x, \u21e0) 2 Rd satisfy\nE[g(x, \u21e0) | x] = rf(x), E \u21e5 kg(x, \u21e0) rf(x)k2 | x \u21e4  21 . (2)\nExtensive research has been conducted on first-order methods, both from a theoretical and practical perspective. For L1-smooth functions with stochastic gradients characterized by a variance of 21 , lower bound \u2326 \u21e3 1Rp T + L1R 2 T 2 \u2318 has been established by Nemirovski & Yudin (1983). For f(x) = E[F (x, \u21e0)], the stochastic approximation (SA) was developed, starting from the pioneering paper by Robbins & Monro (1951). Important improvements of the SA were developed by Polyak (1990); Polyak & Juditsky (1992); Nemirovski et al. (2009), where longer stepsizes with iterate averaging and proper step-size modifications were proposed, obtaining the rate O \u21e3 1Rp\nT + L1R\n2\nT\n\u2318 .\nThe optimal method matching the lower bounds has been developed by Lan (2012) with a convergence rate O \u21e3 1Rp\nT + L1R\n2\nT 2\n\u2318 . However, the literature on second-order methods is significantly limited for\nthe study of provable, globally convergent stochastic second-order methods for convex minimization. Second-order methods. Although second-order methods have been studied for centuries (Newton, 1687; Raphson, 1697; Simpson, 1740; Kantorovich, 1949; Mor\u00e9, 1977; Griewank, 1981), most of the results are connected with local quadratic convergence. The significant breakthroughs regarding global convergence have been achieved only recently, starting from the paper on Cubic Regularized Newton (CRN) method by Nesterov & Polyak (2006), the first second-order method with a global convergence rate O \u21e3 L2R 3\nT 2\n\u2318 . Following this work, Nesterov (2008) proposes an acceleration mechanism on top\nof CRN and achieves the convergence rate of O \u21e3 L2R 3\nT 3\n\u2318 , going beyond the \u2326(1/T 2) lower bound\nfor first-order methods. Another cornerstone in the field is the work by Monteiro & Svaiter (2013), which achieves lower complexity bound \u2326 \u21e3 L2R 3\nT 7/2\n\u2318 (Agarwal & Hazan, 2018; Arjevani et al., 2019)\nup to a logarithmic factor, for the first time in the literature. The gap between upper and lower bounds was closed only in 2022 in subsequent works of Kovalev & Gasnikov (2022); Carmon et al. (2022). One of the main limitations of the second-order methods is a high per-iteration cost as they require computation of exact Hessian. Therefore, it is natural to use approximations of derivatives instead of their exact values. In (Ghadimi et al., 2017), CRN method with 2-inexact Hessian information and its accelerated version were proposed, achieving convergence rate O \u21e3 2R 2\nT 2 + L2R\n3\nT 3\n\u2318 . This\nalgorithm was later extended by Agafonov et al. (2023) to handle 1-inexact gradients (and highorder derivatives) with a resulting convergence rate of O \u21e3 1R+ 2R 2 T 2 + L2R 3 T 3 \u2318 . A recent paper by Antonakopoulos et al. (2022) proposes a stochastic adaptive second-order method based on the extragradient method without line-search and with the convergence rate O \u21e3 1Rp\nT + 2R\n2\nT 3/2 + L2R\n3\nT 3\n\u2318\nwhen gradients and Hessians are noisy with variances 21 and 22 . In the light of these results, we identify several shortcomings and open questions:\nWhat are the lower bounds for inexact second-order methods? What is the optimal trade-off between inexactness in the gradients and the Hessian?\nIn this work, we attempt to answer these questions in a systematic manner. Detailed descriptions of other relevant studies can be found in Appendix A.\nContributions. We summarize our contributions as follows: 1. We propose an accelerated second-order algorithm that achieves the convergence rate of\nO \u21e3\n1Rp T\n+ 2R 2\nT 2 + L2R\n3\nT 3\n\u2318 for stochastic Hessian with variance 22 and O \u21e3 1Rp T + 2R 2 T 2 + L2R 3 T 3 \u2318\nfor 2-inexact Hessian, improving the existing results (Agafonov et al., 2023; Antonakopoulos et al., 2022) (see Table 1). 2. We prove that the above bounds are tight with respect to the variance of the gradient and the Hessian by developing a matching theoretical complexity lower bound (see Table 1).\n1 2-inexact Hessian: 22 I Hx r 2f(x) 2I\n2It is worth noting that the Accelerated Inexact Tensor Method can also be applied to the case of stochastic derivatives. Specifically, when p = 2, the total number of stochastic gradient computations is on the order of O(\" 7/3), while the total number of stochastic Hessian computations is on the order of O(\" 2/3) Agafonov et al. (2023). In our work, we propose an algorithm that achieves the same number of stochastic Hessian computations but significantly improves the number of stochastic gradient computations to O(\" 2).\n3 1-inexact gradient: kgx rf(x)k  1 4 2-inexact Hessian: k Hx r2f(x) (y x)k  2ky xk 5Unbiased stochasic Hessian: E[H(x, \u21e0) | x] = r2f(x), E h H(x, \u21e0) r2f(x) 2 | x i  22 6Under assumption of 2-inexact Hessian the convergence is O \u21e3 2R 2\nT2\n\u2318\n3. Our algorithm involves solving a cubic subproblem that arises in several globally convergent second-order methods (Nesterov & Polyak, 2006; Nesterov, 2008). To address this, we propose a criterion based on the accuracy of the subproblem\u2019s gradient, along with a dynamic strategy for selecting the appropriate level of inexactness. This ensures an efficient solution of the subproblems without sacrificing the fast convergence of the initial method. 4. We extend our method for higher-order minimization with stochastic/inexact oracles. We achieve\nthe O \u2713\n1Rp T\n+ pP\ni=2\niR i T i + LpR\np+1\nTp+1\n\u25c6 rate with i-inexact i-th derivative.\n5. We propose a restarted version of our algorithm for strongly convex minimization, which exhibits a linear rate. Via a mini-batch strategy, we demonstrate that the total number of Hessian computations scales linearly with the desired accuracy \"."
        },
        {
            "heading": "2 PROBLEM STATEMENT AND PRELIMINARIES",
            "text": "Taylor approximation and oracle feedback. Our starting point for constructing second-order method is based primarily on the second-order Taylor approximation of the function f(x)\nx(y) def = f(x) + hrf(x), y xi+ 12\n\u2326 y x,r2f(x)(y x) \u21b5 , y 2 Rd.\nIn particular, since the exact computation of the Hessians can be a quite tiresome task, we attempt to employ more tractable inexact estimators g(x) and H(x) for the gradient and Hessian. These estimators are going to be the main building blocks for the construction of the \"inexact\" second-order Taylor approximation. Formally, this is given by:\nx(y) = f (x) + hg(x), y xi+ 1 2 hy x,H(x)(y x)i , y 2 R d. (3)\nTherefore, by combining Assumption 1.1 with the aforementioned estimators, we readily get the following estimation: Lemma 2.1 ((Agafonov et al., 2023, Lemma 2)). Let Assumption 1.1 hold. Then, for any x, y 2 Rd, we have |f(y) x(y)|  kg(x) rf(x)k+ 12k H(x) r2f(x) (y x)k ky xk+ L26 ky xk 3.\nkrf(y) r x(y)k  kg(x) rf(x)k+ k H(x) r2f(x) (y x)k+ L23 ky xk 2.\nNow, having established the main toolkit concerning the approximation of f in the rest of this section, we introduce the blanket assumptions regarding the inexact gradients and Hessians (for a complete overview, we refer to Table 1). In particular, we assume that our estimators satisfy the following statistical conditions. Assumption 2.2 (Unbiased stochastic gradient with bounded variance and stochastic Hessian with bounded variance). For all x 2 Rd, stochastic gradient g(x, \u21e0) satisfies (2) and stochastic Hessian H(x, \u21e0) satisfies\nE \u21e5 kH(x, \u21e0) r2f(x)k22 | x \u21e4  22 . (4)\nAssumption 2.3 (Unbiased stochastic gradient with bounded variance and inexact Hessian). For all x 2 Rd stochastic gradient g(x, \u21e0) satisfies (2). For given x, y 2 Rd inexact Hessian H(x) satisfies\nk(H(x) r2f(x))[y x]k  x,y2 ky xk. (5)\nAssumptions 2.2 and 2.3 differ from Condition 1 in (Agafonov et al., 2023) by the unbiasedness of the gradient. An unbiased gradient allows us to attain optimal convergence in the corresponding term O(1/ p T ), while an inexact gradient slows down the convergence to O(1) since a constant error can misalign the gradient. Note, that we do not assume the unbiasedness of the Hessian in all assumptions. Finally, note that Assumption 2.3 does not require (5) to be met for all x, y 2 Rd. Instead, we only consider inexactness along the direction y x, which may be significantly less than the norm of the difference between Hessian and its approximation H(x). Auxiliary problem. Most second-order methods with global convergence require solving an auxiliary subproblem at each iteration. However, to the best of our knowledge, existing works that consider convex second-order methods under inexact derivatives do not account for inexactness in the solution of the subproblem. To address this gap, we propose incorporating a gradient criteria for the subproblem solution, given by miny2Rd!x(y) such that kr!x(y)k  \u2327, (6) where !x(y) is the objective of subproblem and \u2327 0 is a tolerance parameter. We highlight that this criterion is verifiable at each step of the algorithm, which facilitates determining when to stop.\nBy setting a constant tolerance parameter \u2327 , we get the following relationship between the absolute accuracy \u270f required for the initial problem and \u2327 : \u2327 = O \u21e3 \u270f 5 6 \u2318 . In practice, it may not be necessary\nto use a very small accuracy in the beginning. Later, we will discuss strategies for choosing the sequence of \u2327t based on the number of iterations t."
        },
        {
            "heading": "3 THE METHOD",
            "text": "In this section, we present our proposed method, dubbed as Accelerated Stochastic Cubic Regularized Newton\u2019s method. In particular, extending on recent accelerated second-order algorithms (Nesterov, 2021b; Ghadimi et al., 2017; Agafonov et al., 2023), we propose a new variant of the accelerated cubic regularization method with stochastic gradients that achieves optimal convergence in terms corresponding to gradient and Hessian inexactness. Moreover, the proposed scheme allows for the approximate solution of the auxiliary subproblem, enabling a precise determination of the required level of subproblem accuracy. We begin the algorithm description by introducing the main step. Given constants \u0304 > 0 and M L2, we define a model of the objective\n!M,\u0304x (y) := x(y) + \u0304 2kx yk 2 + M6 kx yk 3.\nAt each step of the algorithm, we aim to find u 2 argminy2Rd !M,\u0304x (y). However, finding the respective minimizer is a separate challenge. Instead of computing the exact minimum, we aim to find a point s 2 Rd with a small norm of the gradient. Definition 3.1. Denote by sM,\u0304,\u2327 (x) a \u2327 -inexact solution of subproblem, i.e. a point s := sM,\u0304,\u2327 (x) such that kr!M,\u0304x (s)k  \u2327.\nNext, we employ the technique of estimating sequences to propose the Accelerated Stochastic Cubic Newton method. Such acceleration is based on aggregating stochastic linear models given by\nl(x, y) = f(y) + hg(y, \u21e0), x yi in function t(x) (8), (9). The method is presented in detail in Algorithm 1.\nAlgorithm 1 Accelerated Stochastic Cubic Newton 1: Input: y0 = x0 is starting point; constants M 2L2; non-negative non-decreasing sequences\n{\u0304t}t 0, { t}t 0, {\u0304t2}t 0, {\u0304t3}t 0, and \u21b5t = 3 t+3 , At =\ntQ j=1\n(1 \u21b5j), A0 = 1, (7) 0(x) := \u030402+ 0 2 kx x0k 2 + \u0304 0 3 3 kx x0k 3. (8)\n2: for t 0 do 3:\nvt = (1 \u21b5t)xt + \u21b5tyt, xt+1 = s M,\u0304t,\u2327 (vt)\n4: Compute yt+1 = arg min\nx2Rn\nn t+1(x) := t(x) + t+1 t 2 kx x0k 2\n+ 3P\ni=2\n\u0304t+1i \u0304 t i\ni kx x0k i + \u21b5tAt l(x, xt+1) .\n(9)\nTheorem 3.2. Let Assumption 1.1 hold and M 2L2. \u2022 Let Assumption 2.2 hold. After T 1 with parameters\n\u0304t+12 = 2\u0304t\u21b5\n2 t\nAt , \u0304t+13 = 8M 3 \u21b53t+1 At+1 , t = 1 R (t+ 3) 5 2 , \u0304t = 2 2 + 1+\u2327 R (t+ 3) 3 2 , (10)\nwe get the following bound E [f(xT ) f(x\u21e4)]  O \u21e3 \u2327Rp T + 1Rp T + 2R 2 T 2 + MR3 T 3 \u2318 . (11)\n\u2022 Let Assumption 2.3 hold. After T 1 with parameters defined in (10) and 2 = 2 = max\nt=1,...,T vt 1,xtt , we get the following bound\nE[f(xT ) f(x\u21e4)]  O \u21e3\n\u2327Rp T + 1Rp T + 2R\n2 T 2 + MR3 T 3\n\u2318 . (12)\nThis result provides an upper bound for the objective residual after T iterations of Algorithm 1. The last term in the RHS of (11) and (12) corresponds to the case of exact Accelerated Cubic Newton method (Nesterov, 2008). The remaining terms reveal how the convergence rate is affected by the imprecise calculation of each derivative and by inexact solution of subproblem. We provide sufficient conditions for the inexactness in the derivatives to ensure that the method can still obtain an objective residual smaller than \". Specifically, this result addresses the following question: given that the errors are controllable and can be made arbitrarily small, how small should each derivative\u2019s error be to achieve an \"-solution? Corollary 3.3. Let assumptions of Theorem 3.2 hold and let \" > 0 be the desired solution accuracy. \u2022 Let the levels of inexactness in Assumption 2.2 be:\n\u2327 = O \u21e3 \"\n5 6 M R3\n1 6 \u2318 , 1 = O \u21e3 \" 5 6 M R3 1 6 \u2318 , 2 = O \u21e3 \" 1 3M 2 3 \u2318\n\u2022 Let the levels of inexactness in Assumption 2.3 be: \u2327 = O \u21e3 \"\n5 6 M R3\n1 6 \u2318 , 1 = O \u21e3 \" 5 6 M R3 1 6 \u2318 , 2 = O \u21e3 \" 1 3M 2 3 \u2318\nAnd let the number of iterations of Algorithm 1 satisfy T = O \u21e3 MR3\n\"\n\u2318 1 3\n. Then xT is an \"-solution of problem (1), i.e. f(xT ) f(x\u21e4)  \".\nIn practice, achieving an excessively accurate solution for the subproblem on the initial iterations is not essential. Instead, a dynamic strategy can be employed to determine the level of accuracy required for the subproblem. Specifically, we can choose a dynamic precision level according to \u2327t =\nc t5/2 , where c > 0. As a result, the convergence rate term associated with the inexactness of the subproblem becomes O\nc T 3 , which matches the convergence rate of the Accelerated Cubic Newton\nmethod."
        },
        {
            "heading": "4 THEORETICAL COMPLEXITY LOWER BOUND",
            "text": "In this section, we present a novel theoretical complexity lower bound for inexact second-order methods with stochastic gradient and inexact (stochastic) Hessian. The proof technique draws inspiration from the works (Devolder et al., 2014; Nesterov, 2021b; 2018). For this section, we assume that the function f(x) is convex and has L1-Lipschitz-continuous gradient and L2-Lipschitzcontinuous Hessian. To begin, we describe the information and structure of stochastic second-order methods. At each point xt, the oracle provides us with an unbiased stochastic gradient gt = g(xt, \u21e0) and an inexact (stochastic) Hessian Ht = H(xt, \u21e0). The method can compute the minimum of the following models:\nht+1 = argminh xt(h) = a1 hgt, hi+ a2 hHth, hi+ b1khk 2 + b2khk3 .\nNow, we formulate the main assumption regarding the method\u2019s ability to generate new points. Assumption 4.1. The method generates a recursive sequence of test points xt that satisfies the following condition\nxt+1 2 x0 + Span {h1, . . . , ht+1}\nMost first-order and second-order methods, including accelerated versions, typically satisfy this assumption. However, we highlight that randomized methods are not covered by this lower bound. Randomized lower bound even for exact high-order methods is still an open problem. More details on randomized lower bounds for first-order methods are presented in (Woodworth & Srebro, 2017; Nemirovski & Yudin, 1983). Finally, we present the main theoretical complexity lower bound theorem for stochastic second-order methods. Theorem 4.2. Let some second-order method M with exactly solved subproblem satisfy Assumption 4.1 and have access only to unbiased stochastic gradient and inexact Hessian satisfying Assumption 2.2 or Assumption 2.3 with 2 = 2 = max\nt=1,...,T xt 1,xtt . Assume the method M ensures for any\nfunction f with L1-Lipschitz-continuous gradient and L2-Lipschitz-continuous Hessian the following convergence rate\nmin 0tT\nE [f(xt) f(x\u21e4)]  O(1)max n\n1R \u23051(T )\n; 2R 2\n\u23052(T ) ; L2R\n3\n\u23053(T )\no . (13)\nThen for all T 1 we haven \u23051(T )  p T , \u23052(T )  T 2, \u23053(T )  T 7/2. (14)\nProof. We prove this Theorem from contradiction. Let assume that there exist the method M that satisfies conditions of the Theorem 4.2 and it is faster in one of the bounds from (14).\nThe first case, \u23051(T ) > p T or \u23052(T ) > T 2. Let us apply this method for the first-order lower bound\nfunction. It is well-known, that for the first-order methods, the lower bound is \u2326 \u21e3\n1Rp T\n+ L1R 2\nT 2\n\u2318\n(Nemirovski & Yudin, 1983). Also, the first-order lower bound function has 0-Lipschitz-continuous Hessian. It means, that the method M can be applied for the first-order lower-bound function. We fix stochastic Hessian oracle as H(x, \u21e0) = 2L1I . It means that 2 = 2L for such inexact Hessian. With such matrix H(x, \u21e0) = 2L1I , the method M has only the first-order information and lies in the class of first-order methods. Hence, we apply the method M to the first-order lower bound function and get the rate min\n0tT E [f(xt) f(x\u21e4)]  O(1)max n 1R \u23051(T ) ; 2R 2 \u23052(T ) o , where \u23051(T ) > p T or\n\u23052(T ) > T 2. It means that we\u2019ve got a faster method than a lower bound. It is a contradiction, hence the rates for the method M are bounded as \u23051(T )  p T ,\u23052(T )  T 2. The second case,\n\u23053(T ) > T 7/2. It is well-known, that the deterministic second-order lower bound is \u2326 \u21e3 L2R 3\nT 7/2\n\u2318 .\nLet us apply the method M for the second-order lower bound function, where the oracle give us exact gradients and exact Hessians, then 1 = 0, 2 = 0 and the method M is in class of exact second-order methods but converges faster than the lower bound. It is a contradiction, hence the rate for the method M is bounded as \u23053(T )  T 7/2."
        },
        {
            "heading": "5 TENSOR GENERALIZATION",
            "text": "In this section we propose a tensor generalization of Algorithm 1. We start with introducing the standard assumption on the objective f for tensor methods. Assumption 5.1. Function f is convex, p times differentiable on Rd, and its p-th derivative is Lipschitz continuous, i.e. for all x, y 2 Rd\nkr pf(x) rpf(y)k  L2kx yk.\nWe denote the i-th directional derivative of function f at x along directions s1, . . . , si 2 Rn as r\nif(x)[s1, . . . , si]. If all directions are the same we write rif(x)[s]i. For a p-th order tensor U , we denote by kUk its tensor norm recursively induced (Cartis et al., 2017) by the Euclidean norm on the space of p-th order tensors:\nkUk = max ks1k=...=kspk=1 {|U [s1, . . . , sp]|},\nwhere k \u00b7 k is the standard Euclidean norm. We construct tensor methods based on the p-th order Taylor approximation of the function f(x), which can be written as follows:\nx,p(y) def = f(x) + Pp i=1 1 i!r\nif(x)[y x]i, y 2 Rd. Using approximations Gi(x) for the derivatives rif (x) we create an inexact p-th order Taylor series expansion of the objective\nx,p(y) = f (x) + Pp i=1 1 i!Gi(x)[y x]\ni. Next, we introduce a counterpart of Lemma 2.1 for high-order methods. Lemma 5.2 ((Agafonov et al., 2023, Lemma 2)). Let Assumption 5.1 hold. Then, for any x, y 2 Rd, we have\n|f(y) x,p(y)|  pP\ni=1\n1 i!k(Gi(x) r if(x))[y x]i 1kky xk+ Lp(p+1)!ky xk p+1,\nkrf(y) r x,p(y)k  pP\ni=1\n1 (i 1)!k(Gi(x) r if(x))[y x]i 1k+ Lpp! ky xk p,\nwhere we use the standard convention 0! = 1. Following the assumptions for the second-order method we introduce analogical assumptions for high-order method. Assumption 5.3 (Unbiased stochastic gradient with bounded variance and stochastic high-order derivatives with bounded variance). For any x 2 Rd stochastic gradient G1(x, \u21e0) and stochastic high-order derivatives Gi(x, \u21e0), i = 2, . . . , p satisfy\nE[G1(x, \u21e0) | x] = rf(x), E \u21e5 kG1(x, \u21e0) rf(x)k 2 | x \u21e4  21 , (15)\nE \u21e5 kGi(x, \u21e0) r if(x)k2 | x \u21e4  2i , i = 2, . . . , p.\nAssumption 5.4 (Unbiased stochastic gradient with bounded variance and inexact high-order derivatives). For any x 2 Rd stochastic gradient G1(x, \u21e0) satisfy (15). For given x, y 2 Rd inexact high-order derivatives Gi(x), i = 2, . . . , p satisfy\nk(Gi(x) r if(x))[y x]i 1k  x,yi ky xk i 1. To extend Algorithm 1 to tensor methods, we introduce a p-th order model of the function:\n!M,\u0304x,p (y) := x,p(y) + \u0304 2kx yk\n2 + pP\ni=3\n\u2318i i i! kx yk i + pM(p+1)!kx yk p+1,\nwhere \u2318i > 0, 3  i  p. Next, we modify Definition 3.1 for the high order derivatives case\nDefinition 5.5. Denote by SM,\u0304,\u2327p (x) a point S := SM,\u0304,\u2327p (x) such that kr!M,\u0304x,p (S)k  \u2327. Now, we are prepared to introduce the method and state the convergence theorem.\nAlgorithm 2 Accelerated Stochastic Tensor Method 1: Input: y0 = x0 is starting point; constants M 2pLp; \u2318i 4, 3  i  p; starting inexactness \u03040 0; nonnegative nondecreasing sequences {\u0304ti}t 0 for i = 2, . . . , p+ 1, and\n\u21b5t = p+1 t+p+1 , At = tQ\nj=1 (1 \u21b5j), A0 = 1. (16)\n0(x) := \u030402+ 0 2 kx x0k 2 + pP i=3 \u03040i i! kx x0k i.\n2: for t 0 do 3:\nvt = (1 \u21b5t)xt + \u21b5tyt, xt+1 = S M,\u0304t,\u2327 p (vt)\n4: Compute yt+1 = arg min\nx2Rn\nn t+1(x) := t(x) + t+1 t 2 kx x0k 2\n+ pP\ni=2\n\u0304t+1i \u0304 t i\ni! kx x0k i + \u21b5tAt l(x, xt+1) .\nTheorem 5.6. Let Assumption 5.1 hold and M 2pLp.\n\u2022 Let Assumption 5.3 hold. After T 1 with parameters\n\u0304t2 = O \u21e3 \u0304t\u21b5 2 t\nAt\n\u2318 , \u0304t+1i = O \u21e3 \u21b5it+1 i At+1 \u2318 , \u0304t+1p+1 = O \u2713 \u21b5p+1t+1M At+1 \u25c6 ,\nt = O \u21e3\n1 R t\np+1/2 \u2318 , t = O \u21e3 2 + 1+\u2327 R t 3 2\n\u2318 (17)\nwe get the following bound E [f(xT ) f(x\u21e4)]  O \u21e3 \u2327Rp T + 1Rp T + Pp i=2 iR i T i + MRp+1 Tp+1 \u2318 .\n\u2022 Let Assumption 5.4 hold. After T 1 with parameters defined in (10) and i = i = max\nt=1,...,T vt 1,xti,t we get the following bound\nE [f(xT ) f(x\u21e4)]  O \u21e3\n\u2327Rp T + 1Rp T + Pp i=2 iR i T i + MRp+1 Tp+1\n\u2318 ."
        },
        {
            "heading": "6 STRONGLY CONVEX CASE",
            "text": "Assumption 6.1. Function f is \u00b5-strongly convex, p times differentiable on Rd, and its p-th derivative is Lipschitz continuous, i.e. for all x, y 2 Rd\nkr pf(x) rpf(y)k  Lpkx yk.\nTo exploit the strong convexity of the objective function and attain a linear convergence rate, we introduce a restarted version of Restarted Accelerated Stochastic Tensor Method (Algorithm 2). In each iteration of Restarted Accelerated Stochastic Tensor Method (Algorithm 3), we execute Algorithm 2 for a predetermined number of iterations as specified in equation (18). The output of this run is then used as the initial point for the subsequent iteration of Algorithm 1, which resets the parameters, and this process repeats iteratively. Theorem 6.2. Let Assumption 6.1 hold and let parameters of Algorithm 1 be chosen as in (17). Let {zs}s 0 be generated by Algorithm 3 and R > 0 be such that kz0 x\u21e4k  R. Then for any s 0\nAlgorithm 3 Restarted Accelerated Stochastic Tensor Method Input: z0 2 Rd, strong convexity parameter \u00b5 > 0, M Lp, and R0 > 0 such that kz0 x\u21e4k  R0. For s = 1, 2, . . .:\n1. Set x0 = zs 1, rs 1 = R02s 1 , and Rs 1 = kzs 1 x \u21e4 k.\n2. Run Algorithm 2 for ts iterations, where\nts = O(1)max\n( 1, \u21e3\n\u2327 \u00b5rs 1\n\u23182 , \u21e3\n1 \u00b5rs 1 \u23182 , max i=2,...,p \u2713 iR i 2 s 1 \u00b5 \u25c6 1 i , \u2713 LpR p 1 s 1 \u00b5 \u25c6 1 p+1 ) . (18)\n3. Set zs = xts .\nwe have Ekzs x\u21e4k2  4 sR2, Ef(zs) f(x\u21e4)  2 2s 1\u00b5R2. (19) Moreover, the total number of iterations to reach desired accuracy \" : f(zs) f(x\u21e4)  \" in expectation is\nO\n\u2713 (\u2327+ 1) 2 \u00b5\" + \u21e3q 2 \u00b5 + 1 \u2318 log f(z0) f(x \u21e4) \" + Pp i=3 \u21e3 iR i 2 \u00b5 \u2318 1 i + \u21e3 LpR p 1 \u00b5 \u2318 1 p+1 \u25c6 .\nNow, let us make a few observations regarding the results obtained in Theorem 6.2. For simplicity let solution of the subproblem be exact and p = 2, i.e. we do the restarts of the Accelerated Stochastic Cubic Newton, so the total number of iterations is\nO \u2713 21 \u00b5\" + \u21e3q 2 \u00b5 + 1 \u2318 log f(z0) f(x \u21e4) \" + \u21e3 L2R \u00b5 \u2318 1 3 \u25c6 . (20)\nNext, let\u2019s consider solving the stochastic optimization problem min x2Rd F (x) = E[f(x, \u21e0)] using the mini-batch Restarted Accelerated Stochastic Cubic Newton method (Algorithm 3) with p = 2. In this approach, the mini-batched stochastic gradient is computed as 1r1 Pr1 i=1 rf(x, \u21e0i) and the\nmini-batched stochastic Hessian is computed as 1r2 Pr2 i=1 r 2f(x, \u21e0i), where r1 and r2 represent the batch sizes for gradients and Hessians, respectively. From the convergence estimates in (19) and (20), we can determine the required sample sizes for computing the batched gradients and batched Hessians. Specifically, we have r1 = O\u0303 \u21e3 21\n\"\u00b52/3\n\u2318 and\nr2 = O \u21e3\n2 \u00b51/3\n\u2318 . Consequently, the overall number of stochastic gradient computations is O \u21e3 21\n\"\u00b52/3\n\u2318 ,\nwhich is similar to the accelerated SGD method (Ghadimi & Lan, 2013). Interestingly, the number of stochastic Hessian computations scales linearly with the desired accuracy \", i.e., O \u21e3 2\n\u00b51/3 log 1\"\n\u2318 .\nThis result highlights the practical importance of second-order methods. Since the batch size of the Hessian is constant, there is no need to adjust it as the desired solution as accuracy increases. This is particularly useful in distributed optimization problems under the assumption of beta similarity (Zhang & Lin, 2015). In methods with such assumption (Zhang & Lin, 2015; Daneshmand et al., 2021; Agafonov et al., 2023), the server stores a Hessian sample that provides a \"good\" approximation of the exact Hessian of the objective function. Algorithms utilize this approximation instead of exchanging curvature information with the workers. The constant batch size allows for accurately determining the necessary sample size to achieve fast convergence to any desired accuracy."
        },
        {
            "heading": "7 EXPERIMENTS",
            "text": "In this section, we present numerical experiments conducted to demonstrate the efficiency of our proposed methods. We consider logistic regression problems of the form:\nf(x) = E \u21e5 log(1 + exp( b\u21e0 \u00b7 a > \u21e0 x)) \u21e4 ,\nwhere (a\u21e0, b\u21e0) are the training samples described by features a\u21e0 2 Rd and class labels bi 2 { 1, 1}. Setup. We present results on the a9a dataset (d = 123) from LibSVM by Chang & Lin (2011). We demonstrate the performance of Accelerated Stochastic Cubic Newton in three regimes: deterministic oracles (Figure 1), stochastic oracles with the same batch size for gradient and Hessians (Figures 2a, 2b), and stochastic oracles with smaller batch size for Hessians (Figures 2c, 2d). The final mode is especially intriguing because the convergence component of Algorithm 1 as-\nsociated with gradient noise decreases as 1/ p t, while the component related to Hessian noise decreases as 1/t2. This enables the use of smaller Hessian batch sizes (see Corollary 3.3).\nFor stochastic experiments, we randomly split the dataset into training (30000 data samples) and test (2561 data samples) sets. The methods randomly sample data from the training set and do not have access to the test data. In this case, the training loss represents finite sum minimization properties, and the test loss represents expectation minimization. We compare the performance of the SGD, Extra-Newton (EN), and Accelerated Stochastic Cubic Newton (ASCN). We present experiments for fine-tuned hyperparameters in Figures 1, 2. For SGD, we\u2019ve fine-tuned 1 parameter lr. For EN, we\u2019ve fine-tuned 2 parameters: and 0. For ASCN, we\u2019ve finetuned 2 parameters: M and 1R (only for stochastic case) as the entity, also \u2327 = 0 and 2 = 0 as they are dominated by 1R . To demonstrate the globalization properties of the methods, we consider the starting\npoint x0 far from the solution, specifically x0 = 3 \u00b7 e, where e is the all-one vector. All methods are implemented as PyTorch 2.0 optimizers. Additional details and experiments are provided in the Appendix C.\n(a) Train loss. Gradient and Hessian batch sizes are 1500 (b) Test loss. Gradient and Hessian batch sizes are 1500 (c) Train loss. Gradient batch size is 10000, Hessian batch size is 150 (d) Test loss. Gradient batch size is 10000, Hessian batch size is 150\nFigure 2: Logistic regression on a9a with stochastic oracles\nResults. The ASCN method proposed in this study consistently outperforms Extra-Newton and SGD across all experimental scenarios. In deterministic settings, ASCN exhibits a slight superiority over Extra-Newton. In stochastic experiments, we observe a notable improvement as well. However, it\u2019s worth noting that in stochastic regime as we approach convergence, all methods tend to converge to the same point. This convergence pattern is primarily influenced by the stochastic gradient noise 1Rp T term, which dominates in rates as we converge to solution. Furthermore, experiment with different batch sizes for gradients and Hessians support the theory, confirming that the Hessian inexactness term in ASCN 2R 2\nT 2 has faster rate than the corresponding term in Extra-Newton 2R\n2\nT 3/2 . To conclude,\nthe experiments show that second-order information could significantly accelerate the convergence. Moreover, the methods need significantly less stochastic Hessians than stochastic gradients."
        },
        {
            "heading": "8 CONCLUSION",
            "text": "In summary, our contribution includes a novel stochastic accelerated second-order algorithm for convex and strongly convex optimization. We establish a lower bound for stochastic second-order optimization and prove our algorithm\u2019s achievement of optimal convergence in both gradient and Hessian inexactness. Additionally, we introduce a tensor generalization of second-order methods for stochastic high-order derivatives. Nevertheless, it\u2019s essential to acknowledge certain limitations. Like other globally convergent second-order methods, our algorithm involves a subproblem that necessitates an additional subroutine to find its solution. To mitigate this challenge, we offer theoretical insights into the required accuracy of the subproblem\u2019s solution. Future research could involve enhancing the adaptiveness of the algorithm. Additionally, there is a potential for constructing optimal stochastic second-order and tensor methods by incorporating stochastic elements into existing exact methods. These efforts could further improve both practical and theoretical aspects of stochastic second-order and high-order optimization.\nEthics Statement. The authors acknowledge that they have read and adhere to the ICLR Code of Ethics.\nReproducibility Statement. The experimental details are provided in Section 7 and Appendix C."
        }
    ],
    "title": "METHOD WITH OPTIMAL ADAPTATION TO INEXACTNESS",
    "year": 2023
}