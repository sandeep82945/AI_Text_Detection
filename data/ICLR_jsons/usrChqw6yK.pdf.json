{
    "abstractText": "Inspired by the outstanding zero-shot capability of vision language models (VLMs) in image classification tasks, open-vocabulary object detection has attracted increasing interest by distilling the broad VLM knowledge into detector training. However, most existing open-vocabulary detectors learn by aligning region embeddings with categorical labels (e.g., bicycle) only, disregarding the capability of VLMs on aligning visual embeddings with fine-grained text description of object parts (e.g., pedals and bells). This paper presents DVDet, a Descriptor-Enhanced Open Vocabulary Detector that introduces conditional context prompts and hierarchical textual descriptors that enable precise region-text alignment as well as open-vocabulary detection training in general. Specifically, the conditional context prompt transforms regional embeddings into image-like representations that can be directly integrated into general open vocabulary detection training. In addition, we introduce large language models as an interactive and implicit knowledge repository which enables iterative mining and refining visually oriented textual descriptors for precise region-text alignment. Extensive experiments over multiple large-scale benchmarks show that DVDet outperforms the state-of-the-art consistently by large margins.",
    "authors": [
        {
            "affiliations": [],
            "name": "FINE-GRAINED DESCRIPTORS"
        },
        {
            "affiliations": [],
            "name": "Sheng Jin"
        },
        {
            "affiliations": [],
            "name": "Xueying Jiang"
        },
        {
            "affiliations": [],
            "name": "Jiaxing Huang"
        },
        {
            "affiliations": [],
            "name": "Lewei Lu"
        },
        {
            "affiliations": [],
            "name": "Shijian Lu"
        }
    ],
    "id": "SP:35ee2a18e5efc6f39931d0b4ca3e7b8113996870",
    "references": [
        {
            "authors": [
                "Hyojin Bahng",
                "Ali Jahanian",
                "Swami Sankaranarayanan",
                "Phillip Isola"
            ],
            "title": "Exploring visual prompts for adapting large-scale models",
            "venue": "arXiv preprint arXiv:2203.17274,",
            "year": 2022
        },
        {
            "authors": [
                "Hyojin Bahng",
                "Ali Jahanian",
                "Swami Sankaranarayanan",
                "Phillip Isola"
            ],
            "title": "Visual prompting: Modifying pixel space to adapt pre-trained models",
            "venue": "arXiv preprint arXiv:2203.17274,",
            "year": 2022
        },
        {
            "authors": [
                "Hanoona Bangalath",
                "Muhammad Maaz",
                "Muhammad Uzair Khattak",
                "Salman H Khan",
                "Fahad Shahbaz Khan"
            ],
            "title": "Bridging the gap between object and image-level representations for openvocabulary detection",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Zhaowei Cai",
                "Gukyeong Kwon",
                "Avinash Ravichandran",
                "Erhan Bas",
                "Zhuowen Tu",
                "Rahul Bhotika",
                "Stefano Soatto"
            ],
            "title": "X-detr: A versatile architecture for instance-wise vision-language tasks",
            "venue": "arXiv preprint arXiv:2204.05626,",
            "year": 2022
        },
        {
            "authors": [
                "Yu Du",
                "Fangyun Wei",
                "Zihe Zhang",
                "Miaojing Shi",
                "Yue Gao",
                "Guoqi Li"
            ],
            "title": "Learning to prompt for open-vocabulary object detection with vision-language model",
            "venue": "arXiv preprint arXiv:2203.14940,",
            "year": 2022
        },
        {
            "authors": [
                "Mark Everingham",
                "Luc Van Gool",
                "Christopher KI Williams",
                "John Winn",
                "Andrew Zisserman"
            ],
            "title": "The pascal visual object classes (voc) challenge",
            "venue": "International journal of computer vision,",
            "year": 2010
        },
        {
            "authors": [
                "Chengjian Feng",
                "Yujie Zhong",
                "Zequn Jie",
                "Xiangxiang Chu",
                "Haibing Ren",
                "Xiaolin Wei",
                "Weidi Xie",
                "Lin Ma"
            ],
            "title": "Promptdet: Towards open-vocabulary detection using uncurated images",
            "venue": "In European Conference on Computer Vision,",
            "year": 2022
        },
        {
            "authors": [
                "Golnaz Ghiasi",
                "Yin Cui",
                "Aravind Srinivas",
                "Rui Qian",
                "Tsung-Yi Lin",
                "Ekin D Cubuk",
                "Quoc V Le",
                "Barret Zoph"
            ],
            "title": "Simple copy-paste is a strong data augmentation method for instance segmentation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Xiuye Gu",
                "Tsung-Yi Lin",
                "Weicheng Kuo",
                "Yin Cui"
            ],
            "title": "Open-vocabulary object detection via vision and language knowledge distillation",
            "venue": "arXiv preprint arXiv:2104.13921,",
            "year": 2021
        },
        {
            "authors": [
                "Agrim Gupta",
                "Piotr Dollar",
                "Ross Girshick"
            ],
            "title": "Lvis: A dataset for large vocabulary instance segmentation",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Dat Huynh",
                "Jason Kuen",
                "Zhe Lin",
                "Jiuxiang Gu",
                "Ehsan Elhamifar"
            ],
            "title": "Open-vocabulary instance segmentation via robust cross-modal pseudo-labeling",
            "venue": "arXiv preprint arXiv:2111.12698,",
            "year": 2021
        },
        {
            "authors": [
                "Chao Jia",
                "Yinfei Yang",
                "Ye Xia",
                "Yi-Ting Chen",
                "Zarana Parekh",
                "Hieu Pham",
                "Quoc Le",
                "Yun-Hsuan Sung",
                "Zhen Li",
                "Tom Duerig"
            ],
            "title": "Scaling up visual and vision-language representation learning with noisy text supervision",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Menglin Jia",
                "Luming Tang",
                "Bor-Chun Chen",
                "Claire Cardie",
                "Serge Belongie",
                "Bharath Hariharan",
                "Ser-Nam Lim"
            ],
            "title": "Visual prompt tuning",
            "venue": "In European Conference on Computer Vision,",
            "year": 2022
        },
        {
            "authors": [
                "Chen Ju",
                "Tengda Han",
                "Kunhao Zheng",
                "Ya Zhang",
                "Weidi Xie"
            ],
            "title": "Prompting visual-language models for efficient video understanding",
            "venue": "In European Conference on Computer Vision,",
            "year": 2022
        },
        {
            "authors": [
                "Aishwarya Kamath",
                "Mannat Singh",
                "Yann LeCun",
                "Gabriel Synnaeve",
                "Ishan Misra",
                "Nicolas Carion"
            ],
            "title": "Mdetr-modulated detection for end-to-end multi-modal understanding",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Liunian Harold Li",
                "Pengchuan Zhang",
                "Haotian Zhang",
                "Jianwei Yang",
                "Chunyuan Li",
                "Yiwu Zhong",
                "Lijuan Wang",
                "Lu Yuan",
                "Lei Zhang",
                "Jenq-Neng Hwang"
            ],
            "title": "Grounded language-image pretraining",
            "venue": "arXiv preprint arXiv:2112.03857,",
            "year": 2021
        },
        {
            "authors": [
                "Xiang Lisa Li",
                "Percy Liang"
            ],
            "title": "Prefix-tuning: Optimizing continuous prompts for generation",
            "venue": "arXiv preprint arXiv:2101.00190,",
            "year": 2021
        },
        {
            "authors": [
                "Chuang Lin",
                "Peize Sun",
                "Yi Jiang",
                "Ping Luo",
                "Lizhen Qu",
                "Gholamreza Haffari",
                "Zehuan Yuan",
                "Jianfei Cai"
            ],
            "title": "Learning object-language alignments for open-vocabulary object detection",
            "venue": "arXiv preprint arXiv:2211.14843,",
            "year": 2022
        },
        {
            "authors": [
                "Chuang Lin",
                "Peize Sun",
                "Yi Jiang",
                "Ping Luo",
                "Lizhen Qu",
                "Gholamreza Haffari",
                "Zehuan Yuan",
                "Jianfei Cai"
            ],
            "title": "Learning object-language alignments for open-vocabulary object detection",
            "year": 2023
        },
        {
            "authors": [
                "Tsung-Yi Lin",
                "Michael Maire",
                "Serge Belongie",
                "James Hays",
                "Pietro Perona",
                "Deva Ramanan",
                "Piotr Doll\u00e1r",
                "C Lawrence Zitnick"
            ],
            "title": "Microsoft coco: Common objects",
            "year": 2014
        },
        {
            "authors": [
                "Ziyi Lin",
                "Shijie Geng",
                "Renrui Zhang",
                "Peng Gao",
                "Gerard de Melo",
                "Xiaogang Wang",
                "Jifeng Dai",
                "Yu Qiao",
                "Hongsheng Li"
            ],
            "title": "Frozen clip models are efficient video learners",
            "venue": "In European Conference on Computer Vision,",
            "year": 2022
        },
        {
            "authors": [
                "Xiao Liu",
                "Kaixuan Ji",
                "Yicheng Fu",
                "Weng Lam Tam",
                "Zhengxiao Du",
                "Zhilin Yang",
                "Jie Tang"
            ],
            "title": "Ptuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks",
            "venue": "arXiv preprint arXiv:2110.07602,",
            "year": 2021
        },
        {
            "authors": [
                "Ze Liu",
                "Yutong Lin",
                "Yue Cao",
                "Han Hu",
                "Yixuan Wei",
                "Zheng Zhang",
                "Stephen Lin",
                "Baining Guo"
            ],
            "title": "Swin transformer: Hierarchical vision transformer using shifted windows",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 10012\u201310022,",
            "year": 2021
        },
        {
            "authors": [
                "Zongyang Ma",
                "Guan Luo",
                "Jin Gao",
                "Liang Li",
                "Yuxin Chen",
                "Shaoru Wang",
                "Congxuan Zhang",
                "Weiming Hu"
            ],
            "title": "Open-vocabulary one-stage detection with hierarchical visual-language knowledge distillation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Sachit Menon",
                "Carl Vondrick"
            ],
            "title": "Visual classification via description from large language models",
            "venue": "ICLR,",
            "year": 2023
        },
        {
            "authors": [
                "Matthias Minderer",
                "Alexey Gritsenko",
                "Austin Stone",
                "Maxim Neumann",
                "Dirk Weissenborn",
                "Alexey Dosovitskiy",
                "Aravindh Mahendran",
                "Anurag Arnab",
                "Mostafa Dehghani",
                "Zhuoran Shen"
            ],
            "title": "Simple open-vocabulary object detection with vision transformers",
            "venue": "arXiv preprint arXiv:2205.06230,",
            "year": 2022
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Yongming Rao",
                "Wenliang Zhao",
                "Guangyi Chen",
                "Yansong Tang",
                "Zheng Zhu",
                "Guan Huang",
                "Jie Zhou",
                "Jiwen Lu"
            ],
            "title": "Denseclip: Language-guided dense prediction with context-aware prompting",
            "venue": "arXiv preprint arXiv:2112.01518,",
            "year": 2021
        },
        {
            "authors": [
                "Shaoqing Ren",
                "Kaiming He",
                "Ross Girshick",
                "Jian Sun"
            ],
            "title": "Faster r-cnn: Towards real-time object detection with region proposal networks",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2015
        },
        {
            "authors": [
                "Size Wu",
                "Wenwei Zhang",
                "Sheng Jin",
                "Wentao Liu",
                "Chen Change Loy"
            ],
            "title": "Aligning bag of regions for open-vocabulary object detection",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Jiahui Yu",
                "Zirui Wang",
                "Vijay Vasudevan",
                "Legg Yeung",
                "Mojtaba Seyedhosseini",
                "Yonghui Wu"
            ],
            "title": "Coca: Contrastive captioners are image-text foundation models",
            "year": 1917
        },
        {
            "authors": [
                "Lu Yuan",
                "Dongdong Chen",
                "Yi-Ling Chen",
                "Noel Codella",
                "Xiyang Dai",
                "Jianfeng Gao",
                "Houdong Hu",
                "Xuedong Huang",
                "Boxin Li",
                "Chunyuan Li"
            ],
            "title": "Florence: A new foundation model for computer vision",
            "venue": "arXiv preprint arXiv:2111.11432,",
            "year": 2021
        },
        {
            "authors": [
                "Alireza Zareian",
                "Kevin Dela Rosa",
                "Derek Hao Hu",
                "Shih-Fu Chang"
            ],
            "title": "Open-vocabulary object detection using captions",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Xiaohua Zhai",
                "Xiao Wang",
                "Basil Mustafa",
                "Andreas Steiner",
                "Daniel Keysers",
                "Alexander Kolesnikov",
                "Lucas Beyer"
            ],
            "title": "Lit: Zero-shot transfer with locked-image text tuning",
            "venue": "arXiv preprint arXiv:2111.07991,",
            "year": 2021
        },
        {
            "authors": [
                "Renrui Zhang",
                "Xiangfei Hu",
                "Bohao Li",
                "Siyuan Huang",
                "Hanqiu Deng",
                "Yu Qiao",
                "Peng Gao",
                "Hongsheng Li"
            ],
            "title": "Prompt, generate, then cache: Cascade of foundation models makes strong fewshot learners",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Yiwu Zhong",
                "Jianwei Yang",
                "Pengchuan Zhang",
                "Chunyuan Li",
                "Noel Codella",
                "Liunian Harold Li",
                "Luowei Zhou",
                "Xiyang Dai",
                "Lu Yuan",
                "Yin Li"
            ],
            "title": "Regionclip: Region-based language-image pretraining",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Chong Zhou",
                "Chen Change Loy",
                "Bo Dai"
            ],
            "title": "Denseclip: Extract free dense labels from clip",
            "venue": "arXiv preprint arXiv:2112.01071,",
            "year": 2021
        },
        {
            "authors": [
                "Xingyi Zhou",
                "Vladlen Koltun",
                "Philipp Kr\u00e4henb\u00fchl"
            ],
            "title": "Probabilistic two-stage detection",
            "venue": "arXiv preprint arXiv:2103.07461,",
            "year": 2021
        },
        {
            "authors": [
                "Xingyi Zhou",
                "Rohit Girdhar",
                "Armand Joulin",
                "Phillip Kr\u00e4henb\u00fchl",
                "Ishan Misra"
            ],
            "title": "Detecting twenty-thousand classes using image-level supervision",
            "venue": "arXiv preprint arXiv:2201.02605,",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Vision Language Models (VLMs) (Yu et al., 2022; Yuan et al., 2021; Zhai et al., 2021; Jia et al., 2021; Radford et al., 2021; Zhou et al., 2021a; Rao et al., 2021; Huynh et al., 2021) have demonstrated unparalleled zero-shot capabilities in various image classification tasks, largely attributed to the web-scale image-text data they were trained with (Radford et al., 2021; Jia et al., 2021). As researchers naturally move to tackle the challenge of open vocabulary object detection (OVOD) (Li et al., 2021; Kamath et al., 2021; Cai et al., 2022), they are facing a grand data challenge as there does not exist similar web-scale data with box-level annotations. The much less training data in OVOD inevitably leads to clear degradation in text-image alignment, manifesting in much weaker zero-shot capabilities in most open-vocabulary object detectors. An intriguing question arises: Can we leverage the superior image-text alignment abilities of VLMs to enhance OVOD performance?\nRecent studies (Du et al., 2022; Feng et al., 2022) indeed resonate with this idea, attempting to distill the knowledge from VLMs to extend the vocabulary of object detectors. For example, ViLD (Gu et al., 2021) and its subsequent work (Ma et al., 2022; Zhou et al., 2022; Lin et al., 2023) enforce detectors\u2019 embeddings to be aligned with the embeddings from CLIP image encoder or text encoder. However, VLMs still clearly outperform open-vocabulary detectors while aligning visual embeddings with text embeddings of categorical labels (e.g., \u2018bicycle\u2019) as illustrated in Fig 1. Upon deep examination, we found that VLMs are particularly good at aligning fine-grained descriptors of object attributes or parts (e.g., \u2018bell\u2019 and \u2018pedal\u2019) with their visual counterparts, an expertise yet harnessed in existing OVOD models. Specifically, most existing OVOD methods focus on distilling coarse and category-level alignment knowledge on visual and textual embedding. They largely neglect the fine-grained and descriptor-level alignment knowledge that VLMs possess, leading to the under-utilization of VLM knowledge in the trained OVOD models.\n\u2217Corresponding author.\nWe design Descriptor-Enhanced Open Vocabulary Detection (DVDet) that exploits VLMs\u2019 prowess in descriptor-level region-text alignment for open vocabulary object detection. The essential idea is to exploit VLMs\u2019 alignment ability via customized visual prompt, which mines regional fine-grained descriptors from large language models (LLMs) iteratively and enables prompt training without resource-intensive grounding annotations. The key design in DVDet is Conditional Context visual Prompt (CCP) that transforms region embeddings into image-like counterparts by fusing contextual background information around region proposals. This allows CCP to be seamlessly integrated into various open-vocabulary detection training with little extra designs.\nTo train CCP effectively, we introduce LLMs as communicable and implicit knowledge repositories for iterative generation of fine-grained descriptors for precise region-text alignment. Specifically, we design a hierarchical update mechanism that interacts with LLMs by retaining most-related descriptors while actively soliciting new descriptors from LLMs, enabling CCP training without additional annotation costs. This mechanism enhances descriptor diversity and descriptor availability, leading to regional fine-grained descriptors that are tailored to the most relevant object categories. In addition, we design a simple yet effective descriptor merging and selection strategy to tackle two challenges in CCP training: 1) distinct object categories could share similar fine-grained descriptors which leads to potential confusion in CCP training; 2) object images may not have all fine-grained descriptors present due to occlusions, etc., more detail to be described in the ensuing Method.\nThe contributions of this work can be summarized in three major aspects. First, we introduce a feature-level visual prompt that transforms object embeddings into image-like representations that can be seamlessly plugged into existing open vocabulary detectors in general. Second, we design a novel hierarchical update mechanism that enables effective descriptor merging and selection and dynamical refinement of region-text alignment via iterative interaction with LLMs. Third, extensive experiments demonstrate that the proposed technique improves open-vocabulary detection substantially for both base and novel categories."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Open-Vocabulary Object Detection (OVOD): OVOD utilizing the knowledge of pretrained VLMs (Radford et al., 2021) has attracted increasing attention (Zhong et al., 2022; Minderer et al., 2022) with the advance in open vocabulary image classification (Yu et al., 2022; Yuan et al., 2021; Zhai et al., 2021; Jia et al., 2021; Radford et al., 2021). For example, ViLD (Gu et al., 2021) distills knowledge from VLMs into a two-stage detector, harmonizing the detector\u2019s visual embeddings with those from the CLIP image encoder. HierKD (Ma et al., 2022) focuses on hierarchical globallocal distillation and RKD (Bangalath et al., 2022) explores region-based knowledge distillation to improve the alignment between region-level and image-level embeddings. In addition, VLDet (Lin et al., 2023) and Detic (Zhou et al., 2022) align their detector embeddings with those from CLIP text encoder. Nevertheless, all these prior studies share similar misalignment between their trained detectors and pretrained VLMs: VLMs capture more comprehensive knowledge including fine-grained knowledge about object parts, object attributes, and contextual background while open vocabulary\ndetectors focus on learning precise localization of interested objects. Such misalignment tends to restrict the efficacy of knowledge distillation from VLMs to OVOD.\nRecently, prompt-based methods such as DetPro (Du et al., 2022) and PromptDet (Feng et al., 2022) have emerged as an alternative for alleviating the misalignment between upstream classification knowledge in VLMs and downstream knowledge in detection tasks. These methods adjust the textual embedding space of VLMs to align with regional visual object features by incorporating continuous prompts within VLMs. Despite their success, the adjustment focuses on cross-modal alignment between categorical labels and ROI embedding only, which tends to disrupt the inherent visual-textual alignment properties of VLMs. We design a feature-level prompt learning technique that formulates the ROI embeddings of detectors to be highly similar to image-level embeddings, preserving the image-text alignment capabilities of pretrained VLMs effectively.\nVisual Prompt Learning The concept of \u2018prompting\u2019 originates from the field of Natural Language Processing (NLP) and has gradually gained increasing attention as a means of guiding language models via task instructions (Brown et al., 2020). The evolution of continuous prompt vectors in few-shot scenarios (Li & Liang, 2021; Liu et al., 2021a) demonstrates its cross-domain applicability. Pertinently, VPT (Jia et al., 2022) and its successors (Bahng et al., 2022a;b) have extended this idea to the visual domain, achieving precise pixel-level predictions. In addition, the prompting idea has also been explored in pre-trained video recognition models as well (Ju et al., 2022; Lin et al., 2022b). However, existing visual prompt algorithms generate a single prompt for each downstream task, which cannot handle detection tasks well that often involve multiple objects in a single image. We address this issue by designing CCP which generates a conditional prompt for each object.\nLeveraging Language Language Models Linguistic data has been increasingly exploited in openvocabulary related research, and the recent LLMs have demonstrated their comprehensive knowledge that can be beneficial in various NLP tasks. This trend has extended to computer vision research, and several studies have been reported to investigate how LLMs can assist in downstream computer vision tasks. For example, (Menon & Vondrick, 2023; Zhang et al., 2023) have harnessed linguistic knowledge in pretrained LLMs to generate descriptors for each visual category. Such augmentation enriches VLMs without additional training or labeling efforts. Inspired by CuPL (Menon & Vondrick, 2023), CaF employs GPT-3 (Brown et al., 2020) to craft semantically enriched texts, thereby enhancing the alignment between CLIP\u2019s text and images. However, most existing research treats LLMs as a static database, acquiring useful information through a one-time interaction. We introduce a simple yet effective hierarchical mechanism that continuously interacts with LLMs during the model\u2019s training process, obtaining more diverse and visual-oriented textual data."
        },
        {
            "heading": "3 METHOD",
            "text": ""
        },
        {
            "heading": "3.1 OVERVIEW",
            "text": "Problem setup. Open Vocabulary Object Detection (OVOD) (Zareian et al., 2021) leverages a dataset of image-text pairs to broaden its detection vocabulary from pre-defined categories to novel categories. Formally, the task is to construct an object detector using a detection dataset defined as T = ({(Ii, gi, Di)}Ni=1), where Ii represents an image, gi = (bi, ci) denotes the ground truth annotations consisting of bounding box coordinates bi and associated base categories ci \u2208 Cbase, and Di symbolizes fine-grained descriptors that are generated through LLMs. The primary goal is to facilitate the detection of new classes Cnovel in the inference stage.\nThe predominant OVOD framework typically utilizes a two-stage detection architecture as its backbone, incorporating text embeddings to reformulate the classification layer. Generally, a popular two-stage object detector, such as Mask-RCNN, comprises a visual backbone encoder denoted as \u03a6ENC, a class-agnostic region proposal network (RPN) represented by \u03a6RPN, and an open vocabulary classification module labeled as \u03a6CLS. The overall detection can be formulated by:\n{y\u03021, . . . , y\u0302n} = \u03a6CLS \u25e6 \u03a6RPN \u25e6 \u03a6ENC(Ii) (1) where Ii denotes the i-th input image and {y\u03021, . . . , y\u0302n} represents the set of predicted outputs. Fig. 2 shows an overview of the proposed DVDet framework including a VLM-guided conditional context prompting flow and a LLMs-assisted descriptor generation flow. In the prompting flow, the network takes an input image I0 and extracts the features ri0 for the i-th proposal. It then enlarges\nthe proposal to integrate the contextual background and further extracts features to formulate r \u2032i 0 . We design a learnable meta-net that takes r \u2032i 0 to create a region prompt \u03c0 i o and combines the prompt with ri0 to obtain a prompted features v i 0. Finally, r i 0 is fed to a Box Predictor, and v i 0 to a Class Predictor to assimilate the text embeddings of category labels and their fine-grained descriptors Dc. In the descriptor flow, we design a hierarchy mechanism to generate and update Dc for each category via iterative interactions with LLMs in training. Specifically, Dc records the frequency of fine-grained descriptors as well as their probability of being misclassified to other categories. It also records confusing categories that statistically have high misclassification probability. During the training process, Dc retains high-frequency fine-grained descriptors while discarding low-frequency ones. It employs both high-frequency descriptors and confusing categories to prompt LLMs to generate more diverse and visually relevant descriptors, which are further incorporated into Dc with a semantic merging process."
        },
        {
            "heading": "3.2 CONDITIONAL CONTEXT REGIONAL PROMPTS",
            "text": "In this section, we introduce the Conditional Context Regional Prompts method (CCP), a strategy designed to bridge the gap between pretrained foundational classification models and downstream detection tasks. This technique uses the surrounding contextual background information to transform ROI features into image-like features. Importantly, since current detectors excel at finding unfamiliar objects but have difficulty classifying them accurately, the CCP is integrated only into the classification branch of existing detectors. This improves their accuracy without affecting the localization branch\u2019s ability to identify as numerous unknown targets as possible.\nGiven a pre-trained backbone E and a dataset for downstream tasks, we extract features for an image I0 as R0 = [r10, r 2 0, \u00b7 \u00b7 \u00b7 , rM0 ] where ri0 = E(I0(bi0)). Notably, M represents the number of region proposals and bi0 signifies the i-th proposal. Our objective is to develop a region-conditional visual prompt \u03c0i0 for the i-th detected proposal, more details to be elaborated in the ensuing subsection.\nPrompt Design. In classification tasks, the visual prompt mechanism (Jia et al., 2022) learns a dataset-specific prompt for each task. But for detection tasks, we require a mechanism that can create a contextual conditional prompt \u03c0i0 for i-th detected proposal. Considering the varying scales and quantities of proposals across different samples, we adopt convolutional layers to build a lightweight\nmeta-network, that is adept at processing a variety of object proposals. For each proposal bi0 = (x1, x2, y1, y2), we merge the surrounding background information where the background region is defined by b \u2032i 0 = (x \u2032 1, x \u2032 2, y \u2032 1, y \u2032 2), calculated as follows:\nx\u20321 = x1 \u2212m, y\u20321 = y1 \u2212 n x\u20322 = x2 +m, y \u2032 2 = y2 + n\n(2)\nwhere m and n are constants. Next, we extract the features r \u2032i 0 from the expanded region, and the meta-network learns the regional visual prompt \u03c0i0 using the formula \u03c0 \u2032i 0 = h\u03b8(r \u2032i 0 ), and h\u03b8(.) represents the Meta-Net parameterized by \u03b8. Finally, the learned prompt is combined with the feature ri0 to create a more detailed prompted feature v i 0 = r i 0 + \u03c0 \u2032i 0 ."
        },
        {
            "heading": "3.3 LLMS MEETS VLMS",
            "text": "In this section, we treat the LLMs as interactive implicit knowledge repositories to generate finegrained descriptors for CCP training. Specifically, we design a hierarchical generation mechanism that interacts with LLMs iteratively to generate more diverse and visually relevant category descriptions throughout the training process, with more detail to be elaborated in the ensuing subsections.\nDescriptors Initialization. We adopt a similar input protocol as (Menon & Vondrick, 2023) to prompt LLMs. For each category denoted as c, we extract its fine-grained descriptors Dc, represented as Dc = [dc1, dc2, \u00b7 \u00b7 \u00b7 , dcK ], accompanied by the corresponding text embeddings Tc = [tc1, tc2, \u00b7 \u00b7 \u00b7 , tcK ], where K signifies the quantity of fine-grained descriptors. For all categories, we obtain a fine-grained descriptor dictionary D = [D1, D2, ......, DM ].\nDescriptors Record. The fine-grained descriptors are used for the category prediction. Since we cannot guarantee the presence of each descriptor in every sample, we introduce a semantic selection strategy for each proposal. The selection function s(c, Iio) is defined by:\ns(c, Iio) = 1\nN \u2211 d\u2208RankN (Dc) \u03d5(d, Iio) (3)\nwhere, \u03d5(d, Iio) represents the probability of how the descriptor d is relevant to the i-th proposal of image I0, and RankN selects the top N descriptors based on the value of \u03d5(d, Iio). For the i-th proposal, we predict its category label via argmaxc\u2208C s(c, I i o). For each category c, we record the usage frequency of each descriptor and its probability of being misclassified to other categories (i.e., the confusing categories with high misclassification probability).\nDescriptors Hierarchy Generation and Update. During the training stage, we generate finegrained descriptors via a hierarchy mechanism at intervals of every N iterations. The updating of fine-grained descriptors consists of two core operations. First, we record the usage frequency of the descriptor according to Eq. 3. The high-frequency descriptors are preserved and the low-frequency descriptors are discarded. Second, we prompt LLMs with an input template that consists of highfrequency descriptors to gather descriptors designed as follows:\nQ: There are several useful visual features to tell there is a {category name} in a photo, including {the first frequency descriptors, the second frequency descriptors, ...}.\nwhere {category name} is substituted for a given category label c. The generated list then constitutes the descriptor dictionary D. Subsequently, we further generate fine-grained descriptors for this category to differentiate it from confusing classes. We prompt LLMs with an input template that consists of the confusing categories:\nQ: Which visual features could be used to distinguish {category name} from some confusing categories including {confusing category 1, confusing category 2, confusing category 3, ...} in a photo?\nThe newly generated descriptors further expand the descriptor dictionary D. However, some newly generated descriptors di may already exist in D already. Further, including it into D may lead to the presence of the same descriptor in multiple categories, leading to potential semantic confusion during training. We address this issue by measuring the cosine similarity sij between di and D. If sij > \u03b3, we merge the descriptor\u2019s text embedding via tj = \u03b1ti + (1\u2212 \u03b1)tj , where \u03b3 is a constant and \u03b1 is the momentum coefficient."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "4.1 DATASETS",
            "text": "We evaluated DVDet over two widely adopted benchmarks, ie, COCO (Lin et al., 2014) and LVIS (Gupta et al., 2019). For the COCO dataset, we follow OV-RCNN (Zareian et al., 2021) to split the object categories into 48 base categories and 17 novel categories. As in (Zareian et al., 2021), we keep 107,761 images with base class annotations as the training set and 4,836 images with base and novel class annotations as the validation set. Following (Gu et al., 2021; Zareian et al., 2021), we report mean Average Precision (mAP) at an IoU of 0.5. For the LVIS dataset, we follow ViLD (Gu et al., 2021) to split the 337 rare categories into novel categories and the rest common and frequent categories into base categories (866 categories). Following (Lin et al., 2023), we report the mask AP for all categories. For brevity, we denote the open-vocabulary benchmarks based on COCO and LVIS as OV-COCO and OV-LVIS."
        },
        {
            "heading": "4.2 IMPLEMENTATION DETAILS",
            "text": "In our experiments, we employ pre-trained models in prior studies as the base and include our prompt learning techniques on top of them for evaluations. Specifically, we employ the CLIP text encoder to encode both categorical labels and their fine-grained descriptors. In the open-vocabulary COCO experiments, we follow the OVR-CNN setting (Zareian et al., 2021) without any data augmentation and adopt Faster R-CNN (Ren et al., 2015) with ResNet50-C4 (He et al., 2016) as the backbone. For the warmup, we increase the learning rate from 0 to 0.002 for the first 1000 iterations. The model is trained for 5,000 iterations using SGD optimizer with batch size 8 and the learning rate is scaled down by a factor of 10 at 6000 and 8000 iterations. In open-vocabulary LVIS experiments, we follow Detic (Zhou et al., 2022) to adopt CenterNet2 (Zhou et al., 2021b) with ResNet50 (He et al., 2016) as backbone. We use large-scale jittering (Ghiasi et al., 2021) and repeat factor sampling as data augmentation. For the warmup, we increase the learning rate from 0 to 2e-4 for the first 1000 iterations. The model is trained for 10,000 iterations using Adam optimizer with batch size 8. All expriments are conducted on 4 NVIDIA V100 GPUs. More details can be found in Appendix."
        },
        {
            "heading": "4.3 OPEN-VOCABULARY DETECTION ON COCO",
            "text": "Table 1 shows the performance of different methods on the open-vocabulary COCO datasets. It can be seen that our method, when incorporated into multiple existing open-vocabulary detectors, achieves stable performance improvements consistently. This indicates that introducing alignment with fine-grained descriptors can effectively enhance the performance of existing open-vocabulary detectors. The baseline method utilizes the pretrained RPN (Zhong et al., 2022) to extract proposals and directly feed them into CLIP for classification. We can observe that CLIP achieves good accuracy on novel classes, reaffirming its powerful zero-shot capabilities. As a comparison, stateof-the-art OVOD methods experience sharp accuracy drops while handling objects of novel classes,\nand this applies to various existing OVOD approaches that seek broader cross-modal alignment with dataset caption annotations (Lin et al., 2023), introduction of classification datasets (Zhou et al., 2022) and construction of a concept pool (Zhong et al., 2022). The accuracy drop clearly highlights the necessity of optimizing the classifier in open vocabulary object detectors. By introducing the alignment with fine-grained textual descriptions of categories, our method serves as a general plugin that can complement existing open-vocabulary object detectors consistently and effectively."
        },
        {
            "heading": "4.4 OPEN-VOCABULARY DETECTION ON LVIS",
            "text": "Table 2 shows open-vocabulary detection on LVIS dataset. Similar to the experiments on COCO dataset, the \u201cBase-only\u201d still performs better on the novel classes. However, its performance generally falls below state-of-the-art OVOD methods due to the higher complexity of object detection tasks. Nevertheless, our proposed method complements existing methods consistently, for both ViLD-type methods and prompt-based methods such as DetPro (Du et al., 2022). This shows that compared to the current strategies involving prompts in text encoders, our approach serves as an effective complement. We further examine the generalization of our method by adopting Swin-B as the backbone. Experiments show that incorporating our method into Detic and VLDet improves the accuracy by around 1.3% on novel categories consistently.\nVisualization. We show how introducing fine-grained descriptors improves the open-vocabulary detection qualitatively. As Fig. 4 shows, including our design improves the detection significantly while facing challenging scenarios with distant or occluded objects, small inter-class variations, etc. With fine-grained descriptors such as hair, zippers, and large glass front windshield, our model can better align with the text space and enable more accurate recognition and understanding while handling novel classes. In Fig. 5, we further show how our model progressively aligns targets with relevant descriptors (e.g., the word \u2018school bus\u2019 on the vehicle) of new categories (along the training process), thereby reducing ambiguity and misclassifications (airplane \u2192 car \u2192 bus) effectively. More visualization results can be found in Appendix."
        },
        {
            "heading": "4.5 ABALTION STUDIES",
            "text": "In this section, we conduct ablation studies on OV-COCO benchmark using VLDet (Lin et al., 2022a) and RegionCLIP (Zhong et al., 2022) as two base networks, respectively.\nComponent Analysis. We examine the effectiveness of different components in DVDet on the OVCOCO benchmark. As Table 3 shows, the contribution of fine-grained categorical descriptors is compromised clearly at the absence of prompt learning (i.e., Prompt), and this is well aligned with the statistical data in Fig. 1. In addition, incorporating the prompting with fine-grained descriptors improves the detection performance significantly, substantiating the benefits of fine-grained descriptors to visual-textual alignment. Besides, we adopt two templates to interact with LLM to generate fine-grained descriptors. The template using the confusing categories outperforms that using high-frequency descriptors slightly, largely due to the synergy of the semantic selection with the frequency-based filtering mechanism that plays a critical role in filtering out irrelevant descriptors and partially ensures the reliability of the overall descriptor pool in training. Further, DVDet outperforms other variants consistently, demonstrating the synergy of merging fine-grained descriptors generated from different templates with prompt learning which enriches the training with a more comprehensive understanding of object categories.\nTransfer To Other Datasets. To ascertain the generalization of the proposed DVDet, we apply our COCO-trained model to the test set of PASCAL VOC (Everingham et al., 2010) and the validation set of LVIS with little additional training. This is achieved by utilizing the context conditional prompts from the OVCOCO and modifying the class embeddings of the classifier head accordingly. PASCAL VOC contains 20 object categories including 9 absent in COCO, thereby presenting a notable chal-\nlenge while transferring models without aids from any supplementary training images, not to mention the inherent domain gap. LVIS dataset boasts a substantial catalogue of 1203 object categories, vastly exceeding the label space in COCO. Despite these challenges, DVDet demonstrates remarkable effectiveness across diverse image domains and language vocabularies, as evidenced by 2.3% and 2.1% improvements on the two new datasets as shown in Table 4. It should be highlighted that though many LVIS category names are absent in COCO, DVDet succeeds in learning close descriptors and thereby facilitating a smoother transition while adapting to the LVIS benchmark.\nEffectiveness of Interactive Knowledge Base. The successful creation of fine-grained descriptors depends on iterative interaction with LLMs in training. We validate this by comparing it with a onetime interaction, where LLMs act like a static knowledge base. Specifically, we employ confident samples from CAF (Menon & Vondrick, 2023) to obtain visual-related fine-grained descriptors. As Table 5 shows, the iterative interaction (34.6 AP and 28.4 AP for novel classes) outperforms the static method (33.2 AP and 27.6 AP) clearly. This reconfirms that the dynamic interaction allows LLMs to better understand the detector\u2019s requirements, offering more trustworthy descriptors."
        },
        {
            "heading": "5 CONCLUSIONS",
            "text": "This paper presents DVDet, an innovative open vocabulary detection approach that introduces finegrained descriptor for better region-text alignment and open-vocabulary detection. DVDet consists of two key designs. The first is Conditional Context regional Prompt (CCP), which ingeniously transforms region embeddings into image-like representations by merging contextual background information, enabling CCP to be seamlessly integrated into open vocabulary detection with little extra designs. The second is a hierarchical descriptor generation that iteratively interacts with LLMs to mine and refine fine-grained descriptors according to their performance in prompt training. Without any resource-intensive grounding annotations, DVDet coordinates LLMs-assisted descriptor generation and VLM-guided prompt training effectively. Extensive experiments show that DVDet improves the performance of existing open vocabulary detectors consistently. Moving forwards, we plan to investigate the synergy between powerful foundational models including LLMs and VLMs, for various open vocabulary dense prediction tasks."
        },
        {
            "heading": "6 ACKNOWLEDGEMENT",
            "text": "This study is supported under the RIE2020 Industry Alignment Fund \u2013 Industry Collaboration Projects (IAF-ICP) Funding Initiative, as well as cash and in-kind contributions from the industry partner(s)."
        }
    ],
    "title": "LLMS MEET VLMS: BOOST OPEN VOCABULARY OB-",
    "year": 2024
}