{
    "abstractText": "Large-scale graph machine learning is challenging as the complexity of learning models scales with the graph size. Subsampling the graph is a viable alternative, but sampling on graphs is nontrivial as graphs are non-Euclidean. Existing graph sampling techniques require not only computing the spectra of large matrices but also repeating these computations when the graph changes, e.g., grows. In this paper, we introduce a signal sampling theory for a type of graph limit\u2014the graphon. We prove a Poincar\u00e9 inequality for graphon signals and show that complements of node subsets satisfying this inequality are unique sampling sets for Paley-Wiener spaces of graphon signals. Exploiting connections with spectral clustering and Gaussian elimination, we prove that such sampling sets are consistent in the sense that unique sampling sets on a convergent graph sequence converge to unique sampling sets on the graphon. We then propose a related graphon signal sampling algorithm for large graphs, and demonstrate its good empirical performance on graph machine learning tasks.",
    "authors": [],
    "id": "SP:2afdee701cb30514ca1a5f9770f09928741a32f6",
    "references": [
        {
            "authors": [
                "David J. Aldous"
            ],
            "title": "Representations for partially exchangeable arrays of random variables",
            "venue": "Journal of Multivariate Analysis,",
            "year": 1981
        },
        {
            "authors": [
                "A. Anis",
                "A. Gadde",
                "A. Ortega"
            ],
            "title": "Efficient sampling set selection for bandlimited graph signals using graph spectral proxies",
            "venue": "IEEE Trans. Signal Process.,",
            "year": 2016
        },
        {
            "authors": [
                "A.L. Barab\u00e1si",
                "R. Albert",
                "H. Jeong"
            ],
            "title": "Scale-free characteristics of random networks: the topology of the world-wide web",
            "venue": "Physica A: Statistical Mechanics and its Applications,",
            "year": 2000
        },
        {
            "authors": [
                "C. Borgs",
                "J. Chayes"
            ],
            "title": "Graphons: A nonparametric method to model, estimate, and design algorithms for massive networks",
            "venue": "In Proceedings of the 2017 ACM Conference on Economics and Computation,",
            "year": 2017
        },
        {
            "authors": [
                "C. Borgs",
                "J.T. Chayes",
                "L. Lov\u00e1sz",
                "V.T. S\u00f3s",
                "K. Vesztergombi"
            ],
            "title": "Convergent sequences of dense graphs I: Subgraph frequencies, metric properties and testing",
            "venue": "Adv. Math.,",
            "year": 2008
        },
        {
            "authors": [
                "C. Borgs",
                "J. Chayes",
                "A. Smith"
            ],
            "title": "Private graphon estimation for sparse graphs",
            "venue": "Neural Inform. Process. Syst.,",
            "year": 2015
        },
        {
            "authors": [
                "J. Cervino",
                "L. Ruiz",
                "A. Ribeiro"
            ],
            "title": "Learning by transference: Training graph neural networks on growing graphs",
            "venue": "IEEE Trans. Signal Process.,",
            "year": 2023
        },
        {
            "authors": [
                "L.F.O. Chamon",
                "A. Ribeiro"
            ],
            "title": "Greedy sampling of graph signals",
            "venue": "IEEE Trans. Signal Process.,",
            "year": 2017
        },
        {
            "authors": [
                "S. Chen",
                "R. Varma",
                "A. Sandryhaila",
                "J. Kovacevic"
            ],
            "title": "Discrete signal processing on graphs: Sampling theory",
            "venue": "IEEE Trans. Signal Process.,",
            "year": 2015
        },
        {
            "authors": [
                "F. Chung",
                "O. Simpson"
            ],
            "title": "Computing heat kernel pagerank and a local clustering algorithm",
            "venue": "European Journal of Combinatorics,",
            "year": 2018
        },
        {
            "authors": [
                "V.P. Dwivedi",
                "A.T. Luu",
                "T. Laurent",
                "Y. Bengio",
                "X. Bresson"
            ],
            "title": "Graph neural networks with learnable structural and positional representations",
            "venue": "[cs.LG],",
            "year": 2021
        },
        {
            "authors": [
                "J. Eldridge",
                "M. Belkin",
                "Y. Wang"
            ],
            "title": "Graphons, mergeons, and so on",
            "venue": "Neural Inform. Process. Syst.,",
            "year": 2016
        },
        {
            "authors": [
                "S. Freitas",
                "R. Duggal",
                "D.H. Chau"
            ],
            "title": "MalNet: A large-scale image database of malicious software",
            "venue": "[cs.LG],",
            "year": 2021
        },
        {
            "authors": [
                "Douglas N Hoover"
            ],
            "title": "Relations on probability spaces and arrays",
            "venue": "of. t, Institute for Advanced Study,",
            "year": 1979
        },
        {
            "authors": [
                "D. Kempe",
                "J. Kleinberg",
                "\u00c9. Tardos"
            ],
            "title": "Maximizing the spread of influence through a social network",
            "venue": "In ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD),",
            "year": 2003
        },
        {
            "authors": [
                "J.M. Kleinberg"
            ],
            "title": "The small-world phenomenon: An algorithmic perspective",
            "venue": "In Symposium on Theory of Computing (STOC),",
            "year": 2000
        },
        {
            "authors": [
                "S. Krishnagopal",
                "L. Ruiz"
            ],
            "title": "Graph neural tangent kernel: Convergence on large graphs",
            "venue": "Int. Conf. Mach. Learning,",
            "year": 2023
        },
        {
            "authors": [
                "T. Le",
                "S. Jegelka"
            ],
            "title": "Limits, approximation and size transferability for gnns on sparse graphs via graphops",
            "venue": "[cs.LG],",
            "year": 2023
        },
        {
            "authors": [
                "C. Li",
                "S. Jegelka",
                "S. Sra"
            ],
            "title": "Polynomial time algorithms for dual volume sampling",
            "venue": "Neural Inform. Process. Syst.,",
            "year": 2017
        },
        {
            "authors": [
                "D. Lim",
                "J. Robinson",
                "L. Zhao",
                "T. Smidt",
                "S. Sra",
                "H. Maron",
                "S. Jegelka"
            ],
            "title": "Sign and basis invariant networks for spectral graph representation learning",
            "venue": "[cs.LG],",
            "year": 2022
        },
        {
            "authors": [
                "L. Lov\u00e1sz"
            ],
            "title": "Large Networks and Graph Limits, volume 60",
            "venue": "American Mathematical Society,",
            "year": 2012
        },
        {
            "authors": [
                "P. Ma",
                "M. Mahoney",
                "B. Yu"
            ],
            "title": "A statistical perspective on algorithmic leveraging",
            "venue": "In Int. Conference on Machine Learning (ICML),",
            "year": 2014
        },
        {
            "authors": [
                "A.G. Marques",
                "S. Segarra",
                "G. Leus",
                "A. Ribeiro"
            ],
            "title": "Sampling of graph signals with successive local aggregations",
            "venue": "IEEE Trans. Signal Process.,",
            "year": 2015
        },
        {
            "authors": [
                "S. Maskey",
                "R. Levie",
                "Y. Lee",
                "G. Kutyniok"
            ],
            "title": "Generalization analysis of message passing neural networks on large random graphs",
            "venue": "Neural Inform. Process. Syst.,",
            "year": 2022
        },
        {
            "authors": [
                "A. Ortega",
                "P. Frossard",
                "J. Kova\u010devi\u0107",
                "J.M.F. Moura",
                "P. Vandergheynst"
            ],
            "title": "Graph signal processing: Overview, challenges, and applications",
            "venue": "Proc. IEEE,",
            "year": 2018
        },
        {
            "authors": [
                "I. Pesenson"
            ],
            "title": "Sampling in Paley-Wiener spaces on combinatorial graphs",
            "venue": "Transactions of the American Mathematical Society,",
            "year": 2008
        },
        {
            "authors": [
                "F. Pukelsheim"
            ],
            "title": "Optimal design of experiments",
            "year": 2006
        },
        {
            "authors": [
                "A. Rudi",
                "D. Calandriello",
                "L. Carratino",
                "L. Rosasco"
            ],
            "title": "On fast leverage score sampling and optimal learning",
            "venue": "Neural Inform. Process. Syst.,",
            "year": 2018
        },
        {
            "authors": [
                "L. Ruiz",
                "L.F.O. Chamon",
                "A. Ribeiro"
            ],
            "title": "Graphon neural networks and the transferability of graph neural networks",
            "venue": "In 34th Neural Inform. Process. Syst.,",
            "year": 2020
        },
        {
            "authors": [
                "L. Ruiz",
                "L.F.O. Chamon",
                "A. Ribeiro"
            ],
            "title": "The Graphon Fourier Transform",
            "venue": "IEEE Int. Conf. Acoust., Speech and Signal Process.,",
            "year": 2020
        },
        {
            "authors": [
                "L. Ruiz",
                "L.F.O. Chamon",
                "A. Ribeiro"
            ],
            "title": "Graphon signal processing",
            "venue": "IEEE Trans. Signal Process.,",
            "year": 2021
        },
        {
            "authors": [
                "A. Sandryhaila",
                "J.M.F. Moura"
            ],
            "title": "Discrete signal processing on graphs: Frequency analysis",
            "venue": "IEEE Trans. Signal Process.,",
            "year": 2014
        },
        {
            "authors": [
                "G. Schiebinger",
                "M.J. Wainwright",
                "B. Yu"
            ],
            "title": "The geometry of kernelized spectral clustering",
            "venue": "The Annals of Statistics,",
            "year": 2015
        },
        {
            "authors": [
                "D.I. Shuman",
                "S.K. Narang",
                "P. Frossard",
                "A. Ortega",
                "P. Vandergheynst"
            ],
            "title": "The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains",
            "venue": "IEEE Signal Process. Mag.,",
            "year": 2013
        },
        {
            "authors": [
                "D. Spielman",
                "N. Srivastava"
            ],
            "title": "Graph sparsification by effective resistances",
            "venue": "In Proceedings of the 40th Annual ACM Symposium on Theory of Computing,",
            "year": 2008
        },
        {
            "authors": [
                "M.L. Takac"
            ],
            "title": "Z\u00e1bovsk\u00fd. Data analysis in public social networks",
            "venue": "International Scientific Conference and International Workshop Present Day Trends of Innovations,",
            "year": 2012
        },
        {
            "authors": [
                "Z. Yang",
                "W. Cohen",
                "R. Salakhudinov"
            ],
            "title": "Revisiting semi-supervised learning with graph embeddings",
            "venue": "In Int. Conf. Mach. Learning,",
            "year": 2016
        },
        {
            "authors": [
                "Ruiz"
            ],
            "title": "Under review as a conference",
            "year": 2021
        }
    ],
    "sections": [
        {
            "text": "Large-scale graph machine learning is challenging as the complexity of learning models scales with the graph size. Subsampling the graph is a viable alternative, but sampling on graphs is nontrivial as graphs are non-Euclidean. Existing graph sampling techniques require not only computing the spectra of large matrices but also repeating these computations when the graph changes, e.g., grows. In this paper, we introduce a signal sampling theory for a type of graph limit\u2014the graphon. We prove a Poincare\u0301 inequality for graphon signals and show that complements of node subsets satisfying this inequality are unique sampling sets for Paley-Wiener spaces of graphon signals. Exploiting connections with spectral clustering and Gaussian elimination, we prove that such sampling sets are consistent in the sense that unique sampling sets on a convergent graph sequence converge to unique sampling sets on the graphon. We then propose a related graphon signal sampling algorithm for large graphs, and demonstrate its good empirical performance on graph machine learning tasks."
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "Graphs are ubiquitous data structures in modern data science and machine learning. Examples range from social networks (Kempe et al., 2003; Baraba\u0301si et al., 2000) and recommender systems (Ying et al., 2018) to drug interactions (Zitnik et al., 2018) and protein folding (Jumper et al., 2021), in which the graph can have tens of thousands to millions of nodes and edges (Takac & Za\u0301bovsky\u0301, 2012). The ability to sense systems at this scale presents unprecedented opportunities for scientific and technological advancement. However, it also poses challenges, as traditional algorithms and models may need to scale more efficiently to large graphs, including neural graph learning methods.\nHowever, the large size of modern graphs does not necessarily indicate the degree of complexity of the underlying problem. In fact, many graph-based problems have low intrinsic dimensions. For instance, the \u2018small-world phenomenon\u2019 (Kleinberg, 2000) observes that any two entities in a network are likely to be connected by a short sequence of intermediate nodes. In another example, Baraba\u0301si et al. (2000) shows that in power-law graphs, there is a small population of highly connected influencers and a large population of scattered nodes.\nAt a high level, this paper studies how to exploit these simplicities in large graphs to design scalable algorithms with theoretical guarantees. In particular, we combine two ideas: graph limits, which are used to approximate large, random graphs; and sampling theory, which studies the problem of representing (graph) signals using the smallest possible subset of data points (nodes), with the least possible loss of information. We then illustrate how to use the resulting sampling techniques to compress graphs for GNN training and to compute faster, subsampled positional encodings.\nGraphons and graph limits. Leveraging continuous limits to analyze large discrete data is helpful because limits often reveal the intrinsic dimension of the data. E.g., in Euclidean domain, the Fourier transform (FT) of a continuous signal is easier to analyze than the FT of its discrete counterpart, which is periodic and may exhibit aliasing. We propose to study the graph signal sampling problem on a graph limit called graphon. Graphons can be thought of as undirected graphs with an uncountable number of nodes, and are both random graph models and limits of large dense graphs (Borgs et al., 2008; Lova\u0301sz, 2012).\n(Graph) signal sampling. Sampling theory is a long-standing line of work with deep roots in signal processing. Traditionally, sampling seeks to answer the fundamental question: Given an analog (continuous) signal, if one can only observe discrete samples, under what conditions can the analog signal be perfectly reconstructed? On a graph on n nodes, signals are vectors x \u2208 Rn that map each node to some value. The graph signal sampling problem is then defined as follows.\nProblem 1. For some signal space X of interest, find subsets S of nodes such that if x,x\u2032 \u2208 X and xi = x \u2032 i for all i \u2208 S then xj = x\u2032j for all other nodes. Thus, such a set can uniquely represent any signals in X and is called a uniqueness set for X .\nProblem 1 was first studied by Pesenson (2008), who introduced Paley-Wiener spaces for graph signals, defined graph uniqueness sets, and derived a Poincare\u0301 inequality for discrete graph signals that allows recovering such uniqueness sets. These definitions are reviewed in Section 2. Graph signal sampling theory subsequently found applications in the field of graph signal processing (GSP) (Shuman et al., 2013; Ortega et al., 2018), with Chen et al. (2015) describing how sampling sets can be obtained via column-wise Gaussian elimination of the eigenvector matrix.\nCurrent limitations. Though widely used, Chen et al. (2015)\u2019s approach requires expensive spectral computations. Several methods, briefly discussed in Section 1.1, have been proposed to circumvent these computations; however, these approaches still present stringent tradeoffs between complexity and quality of approximation on very large graphs. Perhaps more limiting, the discrete sampling sets yielded by these methods are no longer applicable if the graph changes, as often happens in large real-world network problems, e.g., an influx of new users in a social network.\nContributions. To address the abovementioned issues, we propose sampling uniqueness sets on the limit graphon. By solving a single sampling problem at the graph limit (graphon), we obtain a uniqueness set that generalizes to any large finite graphs in a sequence converging to the limit graphon. We provide both theoretical guarantees and experiments to verify this generalization in downstream graph-based tasks. In summary, our contributions are:\n1. Motivated by Pesenson (2008), we formulate signal sampling over a graphon and study traditional sampling theory notions, such as Paley-Weiner spaces and uniqueness set, in a Euclidean setting of L2([0, 1]) while still incorporating graph structure into the sampling procedure.\n2. We prove a Poincare\u0301 inequality for graphons and relate bandlimitedness in graphon signal space to optimal sampling sets. This generalizes previous results on finite graphs and rigorously answers a reconstruction question. Unlike other results for graphon signal processing in the literature, we do not require any continuity or smoothness assumption on the graphon.\n3. We uncover a connection between graphon sampling and kernel spectral clustering and design a Gaussian-elimination-based algorithm to sample from the graphon uniqueness set with provable consistency, using an argument from (Schiebinger et al., 2015).\n4. We empirically evaluate our sampling method on two tasks: (1) transferability: training a GNN on subsampled graphs and testing on the full graph; (2) accelerating the computation of positional encodings for GNNs by restricting them to a sampled subset of nodes."
        },
        {
            "heading": "1.1 RELATED WORK",
            "text": "Graphons in machine learning. In machine learning, graphons have been used for network model estimation (Borgs et al., 2015), hierarchical clustering (Eldridge et al., 2016) and to study the theoretical properties of graph neural networks (GNNs) on large graphs. Specifically, Ruiz et al. (2020b) have shown that graph convolutions converge to graphon convolutions, further proving a non-asymptotic result that implies that GNNs are transferable across graphon-sampled graphs (Ruiz et al., 2020a). Similar studies have been done using graphops (Le & Jegelka, 2023), which are very general graph limits that range from graphons to very sparse graphs. Graphons have also been used to show convergence of GNN training on increasing graph sequences (Cervino et al., 2023), to prove PAC-Bayes bounds for GNN learning (Maskey et al., 2022), and to study the learning dynamics of wide large-graph NNs (Krishnagopal & Ruiz, 2023).\nGraph signal sampling. Graph signal sampling has been studied at length in GSP. Chen et al. (2015) describe how sampling sets can be obtained via column-wise Gaussian elimination of the\neigenvector matrix and derive conditions for perfect reconstruction. Noting that this approach requires expensive spectral computations, several methods were proposed to avoid them. E.g., Anis et al. (2016) calculate eigenvalue and eigenvector approximations using power iteration; Marques et al. (2015) compute n signal aggregations at a single node i to construct an n-dimensional local signal from which K elements are sampled; and Chamon & Ribeiro (2017) do greedy sampling and provide near optimal guarantees when the interpolation error is approximately supermodular.\nConnections with other sampling techniques. The sampling algorithm we propose is based on a greedy iterative procedure that attempts to find the signal with the lowest total variation on the complement of the current sampling set S, and adds the node corresponding to the largest component in this signal to S. This heuristic is derived by trying to maximize the largest eigenvalue of the normalized Laplacian restricted to S (see (Anis et al., 2016, Section IV.C) for a detailed discussion). Thus, our algorithm has close connections with E-optimal design, which minimizes the largest eigenvalue of the pseudo-inverse of the sampled matrix (Pukelsheim, 2006), and with dual volume sampling (Li et al., 2017), which provides approximation guarantees for E-optimal sampling. This type of objective also appears in effective resistance/leverage scores sampling (Ma et al., 2014; Rudi et al., 2018), which is used for graph sparsification (Spielman & Srivastava, 2008)."
        },
        {
            "heading": "2 PRELIMINARIES",
            "text": ""
        },
        {
            "heading": "2.1 GRAPH SIGNAL PROCESSING",
            "text": "Setup. We consider graphs G = (V, E) with n nodes and edges E \u2286 V \u00d7 V . We write a graph\u2019s adjacency matrix as A \u2208 Rn\u00d7n; its degree matrix as D = diag(A1); and its Laplacian matrix as D\u2212A. We also consider the normalized adjacency and Laplacian matrices A\u0304 = (D\u2020)1/2A(D\u2020)1/2 and L\u0304 = I\u2212A\u0304 where \u00b7\u2020 is the pseudoinverse, with eigendecomposition L\u0304 = V\u039bVT and eigenvalues \u03bb1 \u2264 . . . \u2264 \u03bbn. We further consider node signals x \u2208 Rn, which assign data value xi to node i; e.g., in a social network, xi may represent the political affiliation of person i.\nTotal variation and graph frequencies. The total variation of a graph signal is defined as TV(x) = xT L\u0304x (Anis et al., 2016; Sandryhaila & Moura, 2014). This allows interpreting the eigenvalues \u03bbi as the graph\u2019s essential frequencies, with oscillation modes given by the eigenvectors vi = [V]:i.\nGraph FT and Paley-Wiener spaces. We may analyze signals on the graph frequency domain via the graph Fourier transform (GFT). The GFT x\u0302 of x is its projection onto the Laplacian eigenbasis x\u0302 = VTx (Sandryhaila & Moura, 2014). The GFT further allows defining bandlimited graph signals, or, more formally, Paley-Wiener (PW) spaces. On G, the PW space with cutoff frequency \u03bb is defined as PW\u03bb(G) = {x s.t. [x\u0302]i = 0 for all \u03bbi > \u03bb} (Anis et al., 2016; Pesenson, 2008). Uniqueness sets. When X is a PW space PW\u03bb(G) with \u03bb \u2264 \u03bbK for some K < n, there exists a subset of at most K nodes that perfectly determine any signal in X called uniqueness set. The following theorem from (Anis et al., 2016) gives conditions under which a proposed subset S is a uniqueness set for PW\u03bb(G). Theorem 1 (Uniqueness sets for PW\u03bb(G)). Let S \u2286 V . Let VK \u2208 Rn\u00d7K denote the first K columns of the eigenvector matrix V and \u03a8S \u2208 RK\u00d7K be the submatrix of V with rows indexed by S. If rank\u03a8S = K, then S is a uniqueness set for PW\u03bb(G) for all \u03bb \u2264 \u03bbK(G). If \u03bbK \u2264 \u03bb < \u03bbK+1 then rank\u03a8S = K is also necessary.\nIn addition to providing a sufficient condition to verify if a set is a uniqueness set for some PW space, this theorem suggests a two-step strategy for obtaining such sets: first compute VK , and then design a sampling method that outputs S such that rank\u03a8S = K. However, these sampling strategies, e.g., the one suggested by Thm. 1, can be limiting on large graphs as they require computing the eigendecomposition of a large matrix."
        },
        {
            "heading": "2.2 GRAPHON SIGNAL PROCESSING",
            "text": "Graphons and graphon signals. A graphon is a symmetric, bounded, measurable function W : \u2126 \u00d7 \u2126 \u2192 [0, 1], where \u2126 is a general measurable space (Borgs & Chayes, 2017). We assume that there exists an invertible map \u03b2 : \u2126 \u2192 [0, 1] and w.l.o.g.,we can also write W : [0, 1]2 \u2192 [0, 1]. Graphons are only defined up to a bijective measure-preserving map, similar to how finite graphs\nare defined up to node permutations. Graphons are limits of graph sequences {Gn} in the so-called homomorphism density sense (Borgs et al., 2008), and can also be seen as random graph models where nodes ui, uj are sampled from \u2126 and edges (ui, uj) \u223c Bernoulli(W(ui,uj)).Graphons can also be motivated via infinite exchangeable graphs (Hoover, 1979; Aldous, 1981).\nGraphon signals are functions X : [0, 1] \u2192 R. They represent data on the \u201cnodes\u201d of a graphon, i.e., X(u) is the value of the signal at node u \u2208 [0, 1] (Ruiz et al., 2021). Since two graphons that differ on a set of Lebesgue measure 0 are identified, so are graphon signals. We restrict attention to finite-energy signals X \u2208 L2([0, 1]). Graphon Laplacian and FT. Given a graphon W, its degree function is d(v) = \u222b 1 0 W(u, v)du.\nDefine the normalized graphon W\u0304(u, v) = W(u, v)/ \u221a\nd(u)d(v) if d(u),d(v) \u0338= 0 and 0 otherwise. Given a graphon signal X , we define the normalized graphon Laplacian:\nL\u0304X = X \u2212 \u222b 1 0 W\u0304(u, \u00b7)X(u)du. (1)\nThe spectrum of L\u0304 consists of at most countably many nonnegative eigenvalues with finite multiplicity in [0, 2]. Its essential spectrum consists of at most one point {1}, and this is also the only possible accumulation point. We enumerate the eigenvalues as 0 \u2264 \u03bb1 \u2264 \u03bb2 \u2264 . . . \u2264 2. The corresponding set of eigenfunctions {\u03c6i}i\u2208Z\\{0} forms an orthonormal basis of L2([0, 1]); see App. B. We define the graphon Fourier transform (WFT) of signal X as the projection\nX\u0302(\u03bbi) = \u222b 1 0 X(u)\u03c6i(u)du (2)\nfor all i. Note that this is different from the WFTs defined in (Ruiz et al., 2020b), which correspond to projections onto the eigenbasis of a different but related linear operator."
        },
        {
            "heading": "3 SAMPLING THEORY FOR GRAPHONS",
            "text": "As our first contribution, we generalize the graph sampling problem studied in Pesenson (2008) to a graphon sampling problem. The sampling procedure returns a (Lebesgue) measurable subset U \u2286 [0, 1]. Intuitively, we would like to choose a set U such that sampling from U gives us the most information about the whole signal over [0, 1]. These are called uniqueness sets. Similar to finite graphs, when the graphon signals have limited bandwidth, there exist nontrivial (other than U = [0, 1]) uniqueness sets. Finding these sets under theoretical guarantees is the main focus of the sampling theory for graphons that we develop here.\nFor an arbitrary bandwidth cutoff \u03bb > 0, we use the normalized graphon Laplacian (1) with eigenvalues 0 \u2264 \u03bb1 \u2264 \u03bb2 \u2264 . . . \u2264 \u03bb\u22122 \u2264 \u03bb\u22121 \u2264 2. First, we define the Paley-Wiener space: Definition 1 (Graphon signal PW\u03bb(W) space). The Paley-Wiener space associated with \u03bb \u2208 [0, 1] and graphon W, denoted PW\u03bb(W), is the space of graphon signals X : [0, 1] \u2192 R such that X\u0302(\u03bbi) = 0 for all \u03bbi > \u03bb, where X\u0302 is the projection operator defined in Eq. (2).\nThe definition of PW\u03bb(W) depends on the underlying limit graphon through the projection operator (2), in particular the positions of its Laplacian eigenvalues. When \u03bb \u2265 \u03bb\u22121, PW\u03bb is all of L2([0, 1]) as the definition above is vacuously satisfied. Decreasing \u03bb induces some constraints on what functions are allowed in PW\u03bb; until \u03bb = 0 then PW0 = {0} contains only the trivial function. On the other hand, for any signals space H \u2286 L2([0, 1]), we generalize finite-graph uniqueness set: Definition 2 (Graphon uniqueness set). A measurable U \u2286 [0, 1] is a uniqueness set for the signal space H \u2286 L2([0, 1]) if, for anyX,Y \u2208 H, \u222b U |X(u)\u2212Y (u)|2du = 0 implies \u2225X\u2212Y \u22252L2([0,1]) = 0.\nSince U = [0, 1] is a trivial uniqueness set for any H \u2286 L2([0, 1]), we are mainly interested in the interplay between the bandwidth cutoff \u03bb in PW\u03bb(W), and its corresponding non-trivial uniqueness sets. More precisely, we study the question:\nProblem 2. Assume that a graphon signal comes from PW\u03bb(W) for some \u03bb and W, is there an algorithm that outputs a uniqueness set U(\u03bb,W)?\nWe answer this question in the positive and provide two approaches. First, by generalizing results by Pesenson (2008) for finite graphs, we give a graphon Poincare\u0301 inequality (Thm. 2) for nontrivial measurable subsets of [0, 1]. Then, in Thm. 3, we show that if a set S satisfies the Poincare\u0301 inequality with constant \u039b > 0 then the complement U = [0, 1]\\S is a uniqueness set for PW1/\u039b(W) (Thm. 3). Thus, we can find uniqueness set U by first finding an S that satisfies the Poincare\u0301 inequality with constant 1/\u03bb.\nThe second approach is more direct: the analogous question for finite graphs admits a straightforward answer using Gaussian elimination (see the discussion underneath Thm. 1). However, in the limit of infinitely many nodes, it does not make sense to perform Gaussian elimination as is. Instead, we form a sequence of graphs {Gn} that converges to the prescribed graphon W. We then prove, using techniques from (Schiebinger et al., 2015), that performing Gaussian elimination with proper pivoting for Gn recovers sets that converge to a uniqueness set for PW\u03bb(W) (Prop. 5). Finally, we implement and analyze this approach empirically in Section 6."
        },
        {
            "heading": "4 MAIN RESULTS",
            "text": ""
        },
        {
            "heading": "4.1 POINCARE\u0301 INEQUALITY AND BANDWIDTH OF UNIQUENESS SET",
            "text": "We start with the first approach to Problem 2, by giving a Poincare\u0301 inequality for subsets of [0, 1] and showing that Poincare\u0301 inequality for a set S implies uniqueness of [0, 1]\\S at some bandwidth. First, we need some definitions. These definitions generalize Pesenson (2008)\u2019s observation that for finite graphs, any strict subset T of the vertex set satisfies a Poincare\u0301 inequality with constant determined by spectral properties of another graph \u0393(T ). Intuitively, \u0393(T ) is designed to capture the non-Euclidean geometry induced by nodes in T and their neighbors. We now want to construct an analogous \u0393(S) in the graphon case. Fix an arbitrary graphon W and measurable subset S \u2282 [0, 1]. Define the neighborhood N (S) of S as the measurable set N (S) :={ v \u2208 [0, 1]\\S : \u222b S W(u, v)du > 0 } .\nTo define \u0393(S), make a copy of S by letting S\u2032 be a set disjoint from [0, 1] such that there is a measure-preserving bijection \u03b8 : S\u2032 \u2192 S. Let S\u0303 := S \u222a N (S) and S\u0303\u2032 := S\u2032 \u222a N (S). Observe that one can extend \u03b8 : S\u0303\u2032 \u2192 S\u0303 by mapping elements of N (S) to itself. We will define a graphon on the extended domain D = S\u0303 \u222a S\u2032:\n\u0393(S) : D2 \u2192 [0, 1] : (u, v) 7\u2192  W(u, v) if u \u2208 S\u0303 and v \u2208 S\u0303 W(\u03b8(u), \u03b8(v)) if u \u2208 S\u0303\u2032 and v \u2208 S\u0303\u2032 0 otherwise.\n(3)\nSpectral properties of \u0393(S) determines the constant in our Poincare\u0301 inequality: a class of important results in functional analysis that control the action of the functional (normalized Laplacian) by the (non-Euclidean) geometry of the underlying space (here, a graph). Theorem 2 (Graphon Poincare\u0301 inequality ). Let S \u228a [0, 1] such that N (S) has positive Lebesgue measure. Denote by \u03bb1 the smallest nonzero eigenvalue of the scaled normalized Laplacian operator applied to \u0393(S). Then for every X \u2208 L2([0, 1]) supported only on S, \u2225X\u2225L2 \u2264 1\u03bb1 \u2225LX\u2225L2 .\nThe proof of this theorem is in App. C and generalizes that in (Pesenson, 2008). Next, we prove that if we can find a set S that satisfies a Poincare\u0301 inequality with constant \u039b, then its compliment is a uniqueness set for any PW\u03bb(W) with \u03bb < 1/\u039b. Theorem 3. Let S be a proper subset of [0, 1] satisfying the Poincare\u0301 inequality\n\u2225X\u2225L2 \u2264 \u039b\u2225LX\u2225L2 (4)\nfor all X \u2208 L2([0, 1]) supported only on S. Then, U = [0, 1]\\S is a uniqueness set for any PW\u03bb(W) with \u03bb < 1/\u039b.\nThe proof of this result is in App. C, providing an answer to Problem 2: given a bandwidth limit \u03bb, one can find a uniqueness set U by searching through measurable sets S and compute the smallest nonzero eigenvalue \u03bb1 of \u0393(S). If \u03bb < \u03bb1 then U = [0, 1]\\S is a uniqueness set. This approach is inefficient as we may need to check every S. Next, we investigate a more efficient approach."
        },
        {
            "heading": "4.2 GAUSSIAN ELIMINATION AND CONVERGENCE OF UNIQUENESS SETS",
            "text": "Our second approach to Problem 2 relies on approximating the underlying graphon with a sequence of finite graphs {Gn}n\u2208N which has the graphon as its limit, and solving Problem 2 in one of these graphs. While attempting to solve the graphon sampling problem on a finite graph may appear tautological, our goal is to exploit the countable (and usually finite) rank structure of the graphon to make the problem tractable.\nTo establish the connection between the continuous sampling sets in a graphon and its finite rank K, we partition the graphon sampling set into K elements and view each element as representing a mixture component or \u201ccluster\u201d. This leads to a connection to mixture models and spectral clustering, which we exploit in two ways. First, to quantify the quality of the graphon sampling sets via a \u201cdifficulty\u201d function borrowed from (Schiebinger et al., 2015) relating to the separability of the mixture components. Second, similar to consistency of kernelized spectral clustering, to prove that in convergent graph sequences, graph sampling sets converge to graphon sampling sets.\nGraphons are equivalent to mixture models of random graphs. To make the above connection rigorous, the first step is to show we can view the graphon as a mixture model of random graphs.\nDefinition 3 (Mixture model for random graphs). Let \u2126 \u2282 Rd be a compact space and P(\u2126) the space of probability measures on \u2126. For some number of components K, components {Pi \u2208 P(\u2126)}Ki=1, weights {wi \u2265 0}Ki=1 that sum to 1, and a bounded, symmetric, measurable kernel k : \u2126 \u00d7 \u2126 \u2192 [0, 1], a mixture model for random graphs K(\u2126,P,k) samples nodes from some mixture distribution; then sample edges using B - the Bernoulli distribution over the kernel k:\n\u03c9w \u223c P := \u2211K\ni=1 wiPi, for 1 \u2264 w \u2264 n, (u, v) \u223c B(k(\u03c9u, \u03c9v)), for 1 \u2264 u, v \u2264 n. (5)\nHistorically, some authors (Borgs & Chayes, 2017) have defined graphons as in Def. 3, where P is not necessarily a mixture. Under mild conditions on P, we assert that our simpler definition of a graphon is still equivalent to a random graph model. We leave the proof to App. D.\nProposition 1. Assume that the CDF associated with P is strictly monotone. Then, the mixture model K(\u2126,P,k) (Def. 3) is equivalent to the random graph model W([0, 1],U,W), where W : [0, 1]2 \u2192 [0, 1] is a graphon given by W = k \u25e6 \u03b2, and \u03b2 : [0, 1] \u2192 \u2126 is the inverse of the CDF associated with P.\nRecall that Problem 2 prescribes a bandwidth \u03bb, and requires finding a uniqueness set for graphon signals with the prescribed bandwidth. Let K be the number of eigenvalues of W which are smaller than \u03bb (i.e., K = sup{k | \u03bbk < \u03bb}). The following result shows that K is precisely the number of elements or samples that we need to add to the graphon uniqueness set.\nProposition 2. There exists a set of functions {fi}Ki=1, called frames, such that for any graphon signal X \u2208 PW\u03bb(W) there is a unique reconstruction of X from samples {\u27e8fi, X\u27e9}Ki=1.\nTo see why this result is possible, recall that if X \u2208 PW\u03bb(W) for some \u03bb < \u03bbK+1 then X is a linear combination of K eigenfunctions {\u03c6i}Ki=1 corresponding to {\u03bbi}Ki=1. Therefore, it suffices to learn the K coefficients c = (ci)Ki=1 by forming a full rank system (if one exists), which can then be solved via Gaussian elimination: \u27e8f1,\u03c61\u27e9 \u27e8f1,\u03c62\u27e9 ... \u27e8f1,\u03c6K\u27e9\u27e8f2,\u03c61\u27e9 \u27e8f2,\u03c62\u27e9 ... \u27e8f2,\u03c6K\u27e9... ... ... ...\n\u27e8fK ,\u03c61\u27e9 \u27e8fK ,\u03c62\u27e9 ... \u27e8fK ,\u03c6K\u27e9\n c =  \u27e8f1,X\u27e9\u27e8f2,X\u27e9...\n\u27e8fK ,X\u27e9  The next result tells us that different choices of mixture components and k result in frames with different approximation quality. Specifically, the approximation quality is a function of how wellseparated the components in P are with respect to k and is measured quantitatively by a difficulty function \u03d5(P,K) (Schiebinger et al., 2015). E.g., if there are repeated components in the mixture, or a bimodal component, we expect \u03d5 to be high.\nProposition 3. When W is viewed as a mixture of model for random graph K(\u2126,P,k), with K components {Pi}Ki=1, the square-root kernelized density {qi := \u221a\u222b \u2126 k(\u2126, \u00b7)dPi(\u2126)}Ki=1 is a good\nframe approximation. Quantitatively, let \u03a6 be the subspace spanned by the eigenfunctions of W corresponding to {\u03bbi}Ki=1, and Q the subspace spanned by the {qi}Ki=1. Then:\n\u2225\u03a0\u03a6 \u2212\u03a0Q\u2225HS \u2264 16 \u221a 12 + b\u03d5(P,k), (6)\nwhere \u2225.\u2225HS is the Hilbert-Schmidt norm, \u03a0 is the projection operator, and the difficulty function \u03d5 and the boundedness parameter b are as in (Schiebinger et al., 2015) and App. F1.\nNext, we connect the square-root kernelized density of mixture components qi back to graphon uniqueness sets. The following result shows that when qi\u2019s aligns with eigenfunctions of W, there is a clear correspondence between uniqueness set and mixture components. The proof is in App. D. Theorem 4. Fix a small \u03f5 > 0. Assuming that \u2225qi \u2212 \u03c6i\u2225L2(Pi) < \u03f5 for all i \u2208 [K]; and that there exists a set of disjoint measurable subsets {Ai \u2282 [0, 1]}Ki=1 such that EITHER:\n\u2022 the kernelized density pi := \u222b X k(\u03c9, \u00b7)dPi(\u03c9) is concentrated around an interval Ai \u2282 [0, 1] in\nthe sense that pi(Ai)\u2212K2\u03f52 > \u2211 i\u2032 \u0338=i pi(Ai\u2032)/(K \u2212 1)2 for each i \u2208 [K], OR\n\u2022 for each i \u2208 [K], the likelihood ratio statistic is large: pi(Ai)\u2212K 2\u03f52\u2211\nk \u0338=i pk(Ai) > 1/(K \u2212 1)2,\nthen the set U = \u22c3K\ni=1Ai is a uniqueness set for PW\u03bb(W) for any \u03bb \u2208 (\u03bbK , \u03bbK+1).\nPut together, the above results culminate in a method to find uniqueness sets by recovering the mixture components. However, this is still cumbersome to implement due to the continuous nature of graphons. Next we explore an efficient approach to find approximate uniqueness sets for a graphon by finding uniqueness sets for a finite graph sampled from (and thus converging to2) the graphon.\nGaussian elimination (GE) on (approximations) of graphon eigenfunctions returns uniqueness sets for finite sampled graphs. We now derive a scheme to sample points \u03c9 from a uniqueness set U with high probability. Assume that from W = K, we sample n points to collect a dataset {\u03c9i}ni=1. From a graphon perspective, these points are nodes in a finite graph Gn of size n where the edges are sampled with probability given by W. From a mixture model perspective, the points \u03c9i \u2208 \u2126 are associated with a latent variable {zi \u2208 [K]}ni=1 that indicates the component the sample came from. By building on a result by Schiebinger et al. (2015) on the geometry of spectral clustering, we can unify these two perspectives: running a variant of GE over the Laplacian eigenvectors of a large enough Gn returns a sample from each mixture component with high probability. Theorem 5. For any t > c0 \u221a \u03d5n(\u03b4)w \u22123 min, GE over the Laplacian eigenvectors of Gn recovers K samples distributed according to each of the mixture components Pi, 1 \u2264 i \u2264 K, with probability at least(\n1\u2212 8K2 exp\u2212 c2n\u03b4 4\n\u03b42 + Smax + C\n) (1\u2212 \u03b1)K(N \u2212 nmin)K\n(N \u2212 (1 + \u03b1)nmin)K , with nmin = min m\u2208[K] |{i : zi = m}|,\n(7) where \u03b1 is upper bounded as \u03b1 \u2264 c1\u03d5n(\u03b4)/w3/2min + \u03c8(2t). The constants c1, c2, wmin and \u03b4, and the functions C, S, \u03d5n and \u03c8 are as in (Schiebinger et al., 2015) and App. F.\nProp. 5 in App. D works out a small example, corresponding to a case where Pi\u2019s are uniformly distributed on disjoint domains. There, we show that by using GE, we end up solving an eigenvector problem of order K - number of components, instead of the naive order n\u226b K. Intuitively, for well-separated mixture models, embedding the dataset via the top Laplacian eigenvectors returns an embedded dataset that exhibits an almost orthogonal structure: points that share the same latent variable (i.e., which came from the same mixture component) have a high probability of lying along the same axis in the orthogonal system; while points sampled from different distributions tend to be positioned orthogonally. GE with proper pivoting on Gn is thus a good heuristic for sampling uniqueness sets, as it selects points that are almost orthogonal to each other, which is equivalent to picking a sample from each component. The significance of this result is twofold: it bridge graphon sampling and kernelized spectral clustering; and the almost orthogonal structure ensures that the set sampled via GE is a uniqueness set for large graphs sampled from W with high probability. This is stated in the following proposition, which we prove in App. D.\n1For completeness, we have define and discuss the parameters of the difficulty function in App. F. 2Sequences of graphs sampled from a graphon are always convergent (Borgs et al., 2008).\nProposition 4. Consider a graph sequence Gn n\u2192\u221e\u2212\u2212\u2212\u2212\u2192 W. If there is a \u03b4 \u2208 (0, \u2225k\u2225P/(b \u221a 2\u03c0)) such that the difficulty function3 is small, i.e., \u03d5n(\u03b4) < ( w3mint (3/\u03c0+1)c0 )2 , then with probability at least that in Thm. 5, there exists an minimum number of nodes N such that, for all n > N , the sampled nodes form a uniqueness set for the finite graph Gn. All quantities in the bound and additional assumptions are the same as in (Schiebinger et al., 2015) and App. F."
        },
        {
            "heading": "5 ALGORITHM",
            "text": "Motivated by Theorems 3\u20135, we propose a novel algorithm for efficient sampling of signals on large graphs via graphon signal sampling. When regularity assumptions of our theorems are satisfied, this algorithm will generate a consistent sampling set.\nConsider a graph Gn and signal xn from which we want to sample a subgraph Gm and signal xm with minimal loss of information (i.e., we would like the signal xn to be uniquely represented on the sampled graph Gm). The proposed algorithm consists of three steps:\n(1) Represent Gn as its induced graphon Wn(\u03c9, \u03b8) = \u2211n\ni=1 \u2211n j=1[An]ijI(\u03c9 \u2208 Ii)I(\u03b8 \u2208 Ij) where\nI1 \u222a . . . \u222a In is the n-equipartition of [0, 1]. (2) Define a coarser equipartition I \u20321 \u222a . . . \u222a I \u2032q , q < n, of [0, 1]. Given the bandwith \u03bb of the signal\nxn, sample a graphon uniqueness interval \u222apj=1I \u2032ij (Def. 2), p < q, from I \u2032 1 \u222a . . . \u222a I \u2032q .\n(3) Sample the graph Gm by sampling r = \u230am/(p \u2212 1)\u230b points from each of the I \u2032i1 , . . . , I \u2032 ip\u22121 in the graphon uniqueness set (and the remaining m \u2212 (p \u2212 1)r nodes from Iip). By Prop. 4, this procedure yields a uniqueness set for Gn with high probability.\nTo realize (2), we develop a heuristic based on representing the graphon Wn on the partition I \u20321 \u222a . . . \u222a I \u2032q as a graph G\u0303q with adjacency matrix given by [A\u0303q]ij = \u222b I\u2032i \u222b I\u2032j Wn(x, y)dxdy. We then sample p nodes from G\u0303q\u2014each corresponding to an interval I \u2032ij \u2282 I \u2032 1 \u222a . . . \u222a I \u2032q\u2014using the graph signal sampling algorithm from (Anis et al., 2016). This algorithm is a greedy heuristic closely connected to GE and E-optimal sampling but without spectral computations.\nThe sampling of m nodes from I \u2032i1 \u222a . . . \u222a I \u2032 ip in step (3) is flexible in the way nodes in each interval are sampled. Random sampling is the obvious choice, but one could also design more elaborate sampling schemes based on local node information. To increase node diversity, we employ a scheme utilizing a local clustering algorithm based on the localized heat kernel PageRank (Chung & Simpson, 2018) to cluster the graph nodes into a fixed number of communities, and then sample an equal number of nodes from each community.\nRuntime analysis. The advantages of algorithm (1)\u2013(3) with respect to conventional graph signal sampling algorithms (e.g., (Anis et al., 2016; Marques et al., 2015)) are twofold. First, if q \u226a n, (2) is much cheaper. E.g., the greedy heuristic from (Anis et al., 2016) now costs O(pq2) as opposed to O(p|E|). Overall, if step (3) uses simple uniform sampling then our method runs inO(|E|+pq2+m) where E is the number of edges in Gn; whereas obtaining the a uniqueness set of size m from Anis et al. (2016) requires O(m|E|) time. Second, given the limit graphon W, we only need to calculate the sampled intervals once, and reuse them to find approximate uniqueness sets for any graph Gn generated from W as described in Section 2.2, provided that their node labels \u03c91, . . . , \u03c9n (or at least their order) are known. Thus we save time on future sampling computations."
        },
        {
            "heading": "6 NUMERICAL EXPERIMENTS",
            "text": "Transferability for node classification. We use our sampling algorithm to subsample smaller graphs for training GNNs that are later transferred for inference on the full-sized graph. We consider node classification on three citation networks (Yang et al., 2016), and compare the accuracy achieved by GNNs trained on the full graph, on graphs subsampled following the proposed algorithm, and on graphs sampled at random. To ablate the effect of different parameters, we consider a base scenario and three variations. For Cora and CiteSeer, the base scenario fixes cutoff frequency equal to the 5th smallest Laplacian eigenvalue, \u03bb5, of the full graph. It partitions [0, 1] into q = 20 intervals and\n3notice a slight reparameterization\nsamples p = 10 intervals from this partition in step (2). In step (3), it clusters the nodes in each sampled interval into 2 communities, and samples r = 20 nodes from each sampled interval, 10 from each community. For PubMed, the parameters are the same except q = 30 and p = 15. The three variations are doubling (i) the number of communities, (ii) r, and (iii) the eigenvalue index. In each variation, the other parameters remain unchanged. Further details are deferred to Appendix G.\nTable 1 reports results for 5 realizations. Graphon sampling performs better than random sampling in the base case, where the subsampled graphs have less than 10% of the full graph size. Increasing the number of communities improves performance for Cora and widens the gap between graphon and random sampling for both Cora and CiteSeer. For PubMed, it tips the scale in favor of random sampling, which is not very surprising since PubMed has less classes. When we double r, the difference between graphon and random sampling shrinks as expected. Finally, when we increase \u03bb, graphon sampling performs worse than random sampling. This could be caused by the sample size being too small to preserve the bandwith, thus worsening the quality of the sampling sets.\nPositional encodings for graph classification. Many graph positional encodings (PE) for GNNs and graph transformers use the firstK normalized Laplacian eigenvectors (or their learned representations) as input signals (Dwivedi et al., 2021; Lim et al., 2022); they provide additional localization information for each node. While they can greatly improve performance, they are expensive to compute for large graphs. In this experiment, we show how our algorithm can mitigate this issue. We sample subgraphs for which the Laplacian eigenvectors are computed, and then use these eigenvectors as PEs for the full-sized graph by zero-padding them at the non-sampled nodes.\nWe consider the MalNet-Tiny dataset (Freitas et al., 2021), modified to anonymize the node features and pruned to only keep large graphs (with at least 4500 nodes). After balancing the classes, we obtain a dataset with 216 graphs and 4 classes on which we compare four models: (i) without PEs, and qith PEs calculated from (ii) the full-sized graph, (iii) a graphon-sampled subgraph, and (iv) a randomly sampled subgraph. For (iii) and (iv), we also consider the case where isolated nodes are removed from the sampled graphs to obtain more meaningful PEs.\nWe report results for 10 random realizations in Table 2. The PEs from the graphon-subsampled graphs were not as effective as the PEs from the full-sized graph, but still improved performance with respect to the model without PEs, especially without isolated nodes. In contrast, on average, PEs from subgraphs with randomly sampled nodes did not yield as significant an improvement, and displayed only slightly better accuracy than random guessing when isolated nodes were removed."
        },
        {
            "heading": "A EXTRA NOTATIONS",
            "text": "For some probability measure Q, and some functions in the sameL2(Q) spaces, denote by \u27e8\u00b7, \u00b7\u27e9L2(Q) the L2(Q) inner product and \u2225 \u00b7 \u2225L2(Q) the induced L2 norm. We will also abuse notation and write L2(D) for some set D that is a closed subset of the real line to mean the L2 space supported on D under the usual Lebesgue measure. When the measure space is clear, we will also drop it and simply write L2.\nFor some set of functions {f1, . . . fK}, {g1, . . . , gK} where fi and gj are in the same L2 space, denote by ((fi, gj))Ki,j=1 the K \u00d7K matrix:\n((fi, gj)) K i,j=1 =  \u27e8f1, g1\u27e9L2 \u27e8f1, g2\u27e9L2 . . . \u27e8f1, gK\u27e9L2 \u27e8f2, g1\u27e9L2 \u27e8f2, g2\u27e9L2 . . . \u27e8f2, gK\u27e9L2\n... ... . . . ... \u27e8fK , g1\u27e9L2 \u27e8fK , g2\u27e9L2 . . . \u27e8fK , gK\u27e9L2\n (8)"
        },
        {
            "heading": "B ADDITIONAL BACKGROUND",
            "text": "In this section, we revisit operator theory arguments in our construction of various graphon objects (degree function, normalized graphon, graphon shift operators and normalized graphon Laplacian) from Section 2.2.\nRecall that a graphon W is a bounded, symmetric and L2-measurable function from [0, 1]2 \u2192 [0, 1] and thus induces a Hilbert-Schmidt kernel with open connected domain W : (0, 1)2 \u2192 [0, 1]. We will abuse notation and refer to both of these objects as graphons. The associated Hilbert-Schmidt integral operator for W is:\nH : L2([0, 1]) \u2192 L2([0, 1]) : X 7\u2192 ( v 7\u2192 \u222b 1 0 W(u, v)X(u)du ) , (9)\nwhere the resulting function is understood to be in L2. When W is viewed as the adjacency matrix of a graph with infinitely many vertices, if X is taken to assign each nodes with a feature in [0, 1], then H is understood as a message-passing operator that aggregates neighboring features into each node. Note that measurable functions are only defined up to a set of measure 0.\nIn the paper, we consider a normalized version of W:\nW(u, v) =\n{ W(u, v)/ \u221a d(u)d(v) if d(u) \u0338= 0 and d(v) \u0338= 0\n0 otherwise. (10)\nwhere d \u2208 L2([0, 1]) is the degree function:\nd(u) = \u222b 1 0 W(u, v)dv. (11)\nIt is clear that W is also bounded, symmetric and L2-measurable. The corresponding HS operator is denotes H . When the kernel is symmetric and has bounded L2([0, 1]2)-norm, then Hilbert-Scmidt operator theory tells us that H is continuous, compact and self-adjoint.\nSpectral theory of HS operators then tell us that H and H has countable discrete spectrum {\u03bb1 \u2265 \u03bb2 \u2265 . . .}, {\u03bb1 \u2265 \u03bb2 \u2265 . . .} and the essential spectrum of a single accumulation point 0 (Lova\u0301sz,\n2012). Furthermore, each nonzero eigenvalues have finite multiplicity (Lova\u0301sz, 2012). As compact self-adjoint operator, H and H admits a spectral theorem:\nW(u, v) \u223c \u2211 k\u2208N \u03bbk\u03c6k(u)\u03c6k(v), (12)\nfor some eigenfunctions {\u03c6k}k\u2208N, \u2225\u03c6k\u2225L2 = 1 (Lova\u0301sz, 2012). Recall that Mercer\u2019s theorem asserts that continuous positive semi-definite kernel k admits a spectral theorem: there exists a set of orthonormal functions {pi}i\u2208N and a countable set of eigenvalues {\u03bbi}i\u2208N such that \u2211\u221e i=1 \u03bbipi(u)pj(v) = k(u, v) where the convergence is absolute and uniform. For measurable kernels (graphons), Eq. (12) only converges in L2 norm. However, the sequence of eigenvalues admits a stronger \u21132 convergence:\n\u221e\u2211 i=1 \u03bb2i = \u2225W\u222522. (13)\nNote that by our normalization, \u2225W\u222522 \u2264 1 and thus |\u03bbi| \u2264 1 for all i \u2208 N. Finally, we defined the normalized Laplacian operator L = Id\u2212H . It is then straightforward to see that the spectrum of L is just 1\u2212 \u03c3(H) set-wise."
        },
        {
            "heading": "C POINCARE\u0301 INEQUALITY",
            "text": "Proof of Thm. 2. The proof mirrors Pesenson (2008). Fix an X in L2(U). Define X \u2032 \u2208 L2(D) as:\nX \u2032(u) =  X(u) if u \u2208 U \u2212X(u) if u \u2208 U \u2032 0 otherwise.\n(14)\nIt is clear that X \u2032 is measureable (with respect to Lebesgue measure on D). Consider: \u2225X \u2032(u)\u22252L2(D) = \u222b U (X \u2032(u))2du+ \u222b U \u2032 (X \u2032(u))2du = 2\u2225X(u)\u22252L2(U), (15) and at the same time, for all u \u2208 U :\u222b 1 0 W(u, v)dv = \u222b U\u222aN (U) W(u, v)dv = \u222b D (\u0393(U))(u, v)dv. (16)\nThis in particular means that normalizing \u0393(U) as \u0393(U)\u2032 means scaling by the same scalar as normalizing W into W\u2032.\nNow we investigate the image of X \u2032 under Laplacian operator:\nL\u2032\u0393(U)X \u2032(u) := X \u2032(u)\u2212 \u222b D (\u0393(U))\u2032(u, v)X \u2032(v)dv (17)\n=  X(u)\u2212 \u222b 1 0 W\u2032(u, v)X(v)dv if u \u2208 U \u2212X(u)\u2212 \u222b 1 0 \u2212W\u2032(u, v)X(v)dv if u \u2208 U \u2032\n0 otherwise, (18)\n=  L\u2032X(u) if u \u2208 U \u2212L\u2032X(u) if u \u2208 U \u2032 0 otherwise.\n(19)\nAnd therefore: \u2225L\u2032\u0393(U)X \u2032\u2225L2(D) = \u221a 2\u2225L\u2032X\u2225L2(U) \u2264 \u221a 2\u2225L\u2032X\u2225L2([0,1]). The point of constructing \u0393(U)\u2032 is that it has a nice eigenfunction that corresponds to eigenvalue 0. Let \u03c60 be such a function, then\n0 = L\u2032\u0393(U)\u03c60(u) = \u03c60(u)\u2212 \u222b D (\u0393(U))(u, v)\u221a\u222b D (\u0393(U))(z, v)dz \u222b D (\u0393(U))(u, z)dz \u03c60(v)dv. (20)\nBy inspection, setting \u03c60(u) := \u221a\u222b\nD (\u0393(U))(u, v)dv satisfies the above equation and this is the\neigenfunction of L\u2032\u0393(U) corresponding to eigenvalue 0. Expand X \u2032 in the eigenfunction basis of L\u2032\u0393(U) to get:\n\u2225X \u2032\u2225L2(D) = \u2211\ni\u2208N\u222a{0}\n|\u27e8X \u2032, \u03c6i\u27e9|2. (21)\nHowever, the first coefficient vanishes:\n\u27e8X \u2032, \u03c60\u27e9 = \u222b D X \u2032(u) \u221a\u222b D (\u0393(U))(u, v)dvdu (22)\n= \u222b U X(u) \u221a\u222b D W(u, v)dvdu\u2212 \u222b U \u2032 X(u) \u221a\u222b D W(u, v)dvdu = 0, (23)\nand we have: \u221a 2\u2225L\u2032X\u2225L2([0,1]) \u2265 \u2225L\u2032\u0393(U)X \u2032\u22252L2(D) (24)\n= \u2211 i\u2208N \u03bb2i |\u27e8f \u2032, \u03c6i\u27e9|2 (25) \u2265 \u03bb21\u2225X \u2032\u22252L2(D) (26) = \u221a 2\u2225X\u22252L2(U), (27)\nwhich finishes the proof.\nProof of Thm. 3. If X,Y \u2208 PW\u03bb(W), then X \u2212 Y \u2208 PW\u03bb(W) and we have:\n\u2225L\u0304(X \u2212 Y )\u2225L2 \u2264 \u03bb\u2225X \u2212 Y \u2225L2 . (28)\nIf X and Y coincide on U , then X \u2212 Y \u2208 L2(S) and we can write the Poincare\u0301 inequality:\n\u2225X \u2212 Y \u2225L2 \u2264 \u039b\u2225L\u0304(X \u2212 Y )\u2225L2 . (29)\nCombining the two inequalities, we have:\n\u2225X \u2212 Y \u2225L2 \u2264 \u039b\u2225L\u0304(X \u2212 Y )\u2225L2 \u2264 \u039b\u03bb\u2225X \u2212 Y \u2225L2 (30)\nwhich can only be true if \u2225X \u2212 Y \u2225L2 = 0 since \u03bb\u039b < 1."
        },
        {
            "heading": "D PROOF FROM SECTION 4.2",
            "text": "D.1 GRAPHON IS EQUIVALENT TO MIXTURE MODEL FOR RANDOM GRAPHS\nProof of Prop. 1. Let \u03c9 \u223c P(\u2126). We want to find a strictly monotone function \u03b2 : [0, 1] \u2192 \u2126 such that U = \u03b2\u22121(\u03c9) is uniformly distributed over [0, 1]. Let F\u03c9(\u03c9) = P(\u03c9 \u2264 \u03c9), and assume the function \u03b2 exists. Then, for all \u03c9 we can write\nF\u03c9(\u03c9) = P(\u03c9 \u2264 \u03c9) = P(\u03b2(U) \u2264 \u03c9) = P(U \u2264 \u03b2\u22121(\u03c9)) = \u03b2\u22121(\u03c9) (31)\nwhere the second equality follows from the fact that, since \u03b2 is strictly monotone, it has an inverse. This proves that \u03b2 exists and is equal to the inverse of the CDF of \u03c9.\nBefore continuing, let us introduce a few useful definitions. The Laplacian associated with the model K(\u2126,P,K) is defined as LKf = f \u2212 \u222b \u2126 K\u0304(\u03c9, \u00b7)f(\u03c9)dP(\u03c9) (32)\nwhere K\u0304(\u03c9, \u03b8) = K(\u03c9, \u03b8)/(q(\u03c9)q(\u03b8)) and q(\u03c9) = \u221a\u222b\n\u2126 K\u0304(\u03c9, \u03b8)dP(\u03b8). The operator L is self-\nadjoint and positive semidefinite, therefore it has a non-negative real spectrum {\u03bbi, \u03c6i}\u221ei=1.\nTo simplify matters, we will consider the problem of finding frames {fi}Ki=1 allowing to uniquely represent signals in any PW\u2126(\u03bb) with \u03bb \u2264 \u03bbK . Note that the graphon W (and therefore its associated Laplacian) are themselves rank K. Recall that, in order to uniquely represent a signal, the frame {fi} must satisfy\nrank  \u27e8f1, \u03c61\u27e9 \u27e8f1, \u03c62\u27e9 . . . \u27e8f1, \u03c6K\u27e9 \u27e8f2, \u03c61\u27e9 \u27e8f2, \u03c62\u27e9 . . . \u27e8f2, \u03c6K\u27e9\n... ... . . . ... \u27e8fK , \u03c61\u27e9 \u27e8fK , \u03c62\u27e9 . . . \u27e8fK , \u03c6K\u27e9  = K (33) where {\u03c6i}Ki=1 are the eigenfunctions associated with strictly positive eigenvalues of LK , sorted according to their magnitude.\nBy (Schiebinger et al., 2015, Thm.1), the functions qi(\u03b8) = \u222b \u2126 K(\u03c9, \u03b8)dPi(\u03c9), 1 \u2264 i \u2264 K, form such a frame.\nD.2 MIXTURE COMPONENT GIVES RISE TO UNIQUENESS SETS\nProof of Thm. 4. Define the Heaviside frame {hi : X \u2192 R}Ki=1 as hi(\u03c9) = \u03b4\u2208Ai(\u03c9) \u221a pi(\u03c9)/pi(Ai) where \u03b4E is the Dirac delta function for a measurable set E, for each i \u2208 [K] and Leb is the Lebesgue measure on R. It is straightforward to check that hi is also in L2(pi) for each i \u2208 [K]. Define the subspace H := span{h1, . . . , hK} and the Heaviside embedding \u03a6H : X \u2192 RK as \u03a6H(\u03c9) = (h1(\u03c9), . . . , hK(\u03c9)).\nStep 1: Show that ((hi, qj))Ki,j=1 is full-rank. To show that ((hi, qj))Ki,j=1 is full-rank, we compute entries of ((hi, qj))Ki,j=1: for any i, j \u2208 [K],\n\u27e8hi, qj\u27e9 = 1\u221a pi(Ai) \u222b Ai qj(\u03c9) \u221a pi(\u03c9)dLeb(\u03c9) = 1\u221a pi(Ai) \u222b Ai \u221a pj(\u03c9)pi(\u03c9)dLeb(\u03c9). (34)\nFor diagonal entries, note that:\n\u27e8hj , qj\u27e9 = 1\u221a\npj(Aj) \u222b Aj pj(\u03c9)dLeb(\u03c9) = \u221a pj(Aj). (35)\nFix an j \u2208 [K] and consider:\u2211 i\u0338=j |\u27e8hi, qj\u27e9| = \u2211 i \u0338=j 1\u221a pi(Ai) \u222b Ai \u221a pj(\u03c9)pi(\u03c9)dLeb(\u03c9) (36)\n\u2264 \u2211 i \u0338=j 1\u221a pi(Ai) \u221a\u222b Ai pj(\u03c9)dLeb(\u03c9) \u221a\u222b Ai pi(\u03c9)dLeb(\u03c9) (37)\n= \u2211 i \u0338=j 1\u221a pi(Ai) \u221a pj(Ai) \u221a pi(Ai) (38)\n= \u2211 i \u0338=j \u221a pj(Ai), (39)\nwhere the inequality is from Cauchy Schwarz. In the first choice of assumption, we have pj(Aj)\u2212K2\u03f52 > \u2211 i \u0338=j pj(Ai)/(K \u2212 1)2 and thus \u221a pj(Aj)\u2212K\u03f5 > \u221a\u2211 i \u0338=j pi(Ai)/(K \u2212 1) >\u2211\ni \u0338=j \u221a pi(Ai), due to monotonicity of square root and Cauchy Schwarz. Thus, we have shown that for every j \u2208 [K], the j-th column of ((hi, qj))Ki,j=1 has j-th entry larger (in absolute value) than the sum of absolute values of all other entries. Gershgorin circle theorem then tells us that eigenvalues of ((hi, qj))Ki,j=1 lie in at least one disk center at some diagonal value with radius sum of absolute value of remaining column entries. None of the Gershgorin disks contain the origin, and we can conclude that ((hi, qj))Ki,j=1 has no 0 eigenvalue. Therefore, it is full rank.\nNow, fix an i \u2208 [K] and consider:\u2211 j \u0338=i |\u27e8hi, qj\u27e9| = \u2211 j \u0338=i 1\u221a pi(Ai) \u222b Ai \u221a pj(\u03c9)pi(\u03c9)dLeb(\u03c9) (40)\n\u2264 \u2211 j \u0338=i 1\u221a pi(Ai) \u221a\u222b Ai pj(\u03c9)dLeb(\u03c9) \u221a\u222b Ai pi(\u03c9)dLeb(\u03c9) (41)\n= \u2211 j \u0338=i 1\u221a pi(Ai) \u221a pj(Ai) \u221a pi(Ai) (42)\n= \u2211 j \u0338=i \u221a pj(Ai) (43)\nIn the second choice of assumption, the same thing happens: pi(Ai)\u2212K2\u03f52 > \u2211\nj \u0338=i pj(Ai)/(K\u2212 1)2 implies that \u221a pi(Ai) \u2212 K\u03f5 > \u2211 j \u0338=i \u221a pi(Ai) and once again, the center of any Gershgorin disk (but this time in the rows) are further away from zero than the sum of absolute value of other non-diagonal entries. Therefore, none of the disks contain the origin and ((hi, qj))Ki,j=1 cannot have 0 eigenvalue, thus full-rank. Therefore, either choices of assumption leads to full-rank-ness of the system ((hi, qj))Ki,j=1.\nStep 2. Full-rank implies uniqueness. By the premise of this result, we have for each i,\n\u2225qi \u2212 \u03c6i\u2225L2 < \u03f5. (44)\nThus,\n\u27e8hi, \u03c6j\u27e9 = \u27e8hi, qj\u27e9 \u2212 \u27e8hj , qj \u2212 \u03c6j\u27e9 \u2208 (\u27e8hi, qj\u27e9 \u2212 \u03f5, \u27e8hi, qj\u27e9+ \u03f5), (45)\nby Cauchy-Schwarz.\nRecall that ((hi, qj))i,j is full rank, and that Gershgorin circle theorem applied in the previous step still has a slack of at leastK\u03f5. Therefore, perturbation element-wise of additive size \u03f5 of ((hi, qj))i,j will still be full rank by Gershgorin circle theorem and we conclude that ((hi, \u03c6j))i,j is full-rank.\nLet X \u2208 PW\u03bb(W) for some \u03bb \u2208 (\u03bbK , \u03bbK+1), then by definition, there exists a vector c \u2208 RK such that X = \u2211K j=1 cj\u03c6j . Take inner product (in L\n2(P) = L2(W)), we have: \u27e8h1, \u03c61\u27e9 \u27e8h1, \u03c62\u27e9 . . . \u27e8h1, \u03c6K\u27e9 \u27e8h2, \u03c61\u27e9 \u27e8h2, \u03c62\u27e9 . . . \u27e8h2, \u03c6K\u27e9\n... ... . . . ... \u27e8hK , \u03c61\u27e9 \u27e8hK , \u03c62\u27e9 . . . \u27e8hK , \u03c6K\u27e9\n c =  \u27e8h1, X\u27e9 \u27e8h2, X\u27e9\n... \u27e8hK , X\u27e9  (46) .\nTo test if U = \u22c3K\ni=1Ai is a uniqueness set, we assume that \u2225X\u03b4U\u2225L2(W) = 0. But |\u27e8hi, X\u27e9| = |\u27e8hi, \u03b4AiX\u27e9| \u2264 \u2225hi\u2225\u2225X\u03b4U\u2225 = 0 for each i in [K]. Thus: \u27e8h1, \u03c61\u27e9 \u27e8h1, \u03c62\u27e9 . . . \u27e8h1, \u03c6K\u27e9 \u27e8h2, \u03c61\u27e9 \u27e8h2, \u03c62\u27e9 . . . \u27e8h2, \u03c6K\u27e9\n... ... . . . ... \u27e8hK , \u03c61\u27e9 \u27e8hK , \u03c62\u27e9 . . . \u27e8hK , \u03c6K\u27e9  c =  0 0 ... 0  (47) .\nFinally, since ((hi, \u03c6j))Ki,j=1 is full rank, its null space is trivial, implying c = 0 and thus X = 0, which proves uniqueness of U .\nD.3 CONSISTENCY THEOREM\nThis result is an adaptation of (Schiebinger et al., 2015, Thm. 2), which is reproduced below.\nTheorem 6 (Thm.2, Schiebinger et al. (2015)). There are numbers c, c0, c1, c2 depending only on b and r such that for any \u03b4 \u2208 (0, \u2225K\u2225P\nb \u221a 2\u03c0 ) satisfying condition (Schiebinger et al., 2015, 3.17) and any t > c0w \u22121 min \u221a \u03d5n(\u03b4), the embedded dataset {\u03a6V(\u03c9i), Zi}ni=1 has (\u03b1, \u03b8) orthogonal cone structure with\n| cos \u03b8| \u2264 c0 \u221a \u03d5n(\u03b4)\nw3mint\u2212 c0 \u221a \u03d5n(\u03b4)\n(48)\n\u03b1 \u2264 c1 w\n3/2 min\n\u03d5n(\u03b4) + \u03c8(2t) (49)\nand this event holds with probability at least 1\u2212 8K2 exp\u2212 c2n\u03b4 4\n\u03b42+Smax+C .\nThm. 6 elucidates the conditions under which the spectral embeddings of the nodes \u03c9 form an orthogonal cone structure (see (Schiebinger et al., 2015, Def. 1) for a precise definition). This is helpful for Gaussian elimination, as provided that we pick a pivot inside a cone, the other rows to be picked\u2014which are orthogonal to the pivot\u2014are themselves inside other cones, and therefore likely to belong to a different cluster (i.e., to be distributed according to a different mixture component).\nWe first recall connections between graphons and mixture models and explain how each objects in the context of Thm. 6 can be understood in graphons terms. In mixture model, we sample dataset {\u03c9i}ni=1 from the mixture distribution. This is equivalent to sampling nodes under a pushforward in when we sample finite graphs from a graphon. Thus, each data point \u03c9i is a \u2018node\u2019 of a finite graph sampled from the graphon. Next, the spectral embedding of datapoints in spectral clustering is equivalent to computing the eigenfunction of graphon Laplacian at that datapoint - embedding it in frequency domain. Therefore, from a graphon perspective, the theorem is asserting that given some underlying structure controlled by the difficulty function, embedding of nodes in finite graph from a fix graphon into frequency domain under GFT has a peculiar structure: an orthogonal cone. While we do not have easy access to graphon eigenfunction, computing an approximation once with a large graph suffices. This is because we can reuse the embedding when new points are sampled into the graph!\nProof of Thm. 5. Let us consider what happens when performing Gaussian elimination on the columns of \u03a6V(\u03c9). When picking the pivot, the probability of picking a \u201cgood\u201d point inside a cone, i.e., a point that is both inside a cone and that is distributed according to the mixture component associated with that cone, is 1 \u2212 \u03b1. Conditioned on this event, the probability of picking a second \u201cgood\u201d point from another cone is (1\u2212\u03b1)(n\u2212n1)n\u2212(1\u2212\u03b1)n1 , where n1 is the number of points distributed according to the pivot\u2019s distribution, denoted P1. More generally, the probability of picking a \u201cgood\u201d point at the ith step, conditioned on having picked i\u2212 1 \u201cgood\u201d points, is\nP(ith point is \u201cgood\u201d | 1, . . . , i\u2212 1 are \u201cgood\u201d) = (1\u2212 \u03b1)n\u2212i n\u2212 (1\u2212 \u03b1)n+i\n(50)\nwhere n\u2212i = \u2211i\u22121 j=1 n\u2212 nj and n+i = n\u2212 n\u2212i.\nSince Eq. (50) is a decreasing function of n\u2212i, the probability of picking K good points is lower bounded by\nP(1, . . . ,K are \u201cgood\u201d) \u2265 (1\u2212 \u03b1) K(n\u2212 nmin)K\n(n\u2212 (1\u2212 \u03b1)nmin)K (51)\nwhere nmin = min1\u2264j\u2264K nj . Combining Eq. (51) with Theorem Thm. 6 gives the proposition\u2019s result.\nProof of Prop. 4. The conditions on the difficulty function in the hypothesis of Prop. 4 means that the angle \u03b8 in the cone structure is at least \u03c0/3.\nNote that every finite graph Gn induces a graphon via stochastic block model:\nWGn := n\u2211 i=1 n\u2211 j=1 [An]i,jI(x \u2208 Ii)I(y \u2208 Ij) (52)\nFrom Ruiz et al. (2021), we know that the eigenvalues of the adjacency HS operator of WGn converges to that of W. As the graphon Laplacian is a scaled and translated operator from the adjacency operator, eigenvalues of the Laplacian also converges. Let the eigenvalues of the finite graph be \u03bb\u0302n,1 \u2264 . . . \u2264 \u03bb\u0302n,\u22121 Pick an n0 large enough such that there is a spectral gap \u03bb\u0302n,K < \u03bb\u0302n,K+1 for all n > n0. Then pick an even larger n1 such that \u03bb \u2208 (\u03bb\u0302n,K , \u03bb\u0302n,K+1) for all n > n1. Such a choice of n0, n1 is guaranteed by convergence of eigenvalue.\nNot only do eigenvalue converges, when there is an eigengap, the subspace spanned by the first K eigenfunctions also converges. The convergence is in term of convergence in operator norm of the projection operator (Ruiz et al., 2021). Let the projection operator be \u03a6Gn and \u03a6W, corresponding to that for the finite graph Gn and for the graphon W respectively. However, since both of these operators are Hilbert-Schmidt, convergence in operator norm is equivalent to convergence in HilbertSchmidt norm. Therefore, we select yet a larger n2 such that \u2225\u03a6Gn \u2212 \u03a6W\u2225HS < \u03f5 for all n > n2 and for some \u03f5 to be chosen later.\nRecall that we picked some sample via Thm. 5 and with high probability, our sample attains an orthogonal cone structure. In other words, there is a permutation of samples such that for each i \u2208 [K], | cos \u03c4(i)| > 1/2 with high probability, where \u03c4(i) is the angle between \u03c6(xi) and the unit vector with all zero entries but the i-th one. This means that for any i, |\u03c6i(xi)|/\u2225\u03c6i(xi)\u22252 > 1/2. Therefore, the matrix:  \u03c61(x1) \u03c62(x1) . . . \u03c6K(x1) \u03c61(x2) \u03c62(x2) . . . \u03c6K(x2)\n... ... . . . ... \u03c61(xK) \u03c62(xK) . . . \u03c6K(xK)  (53) is full rank, since the off-diagonal absolute value sum does not exceed the absolute value of the diagonal entry for every row, via Gershgorin circle theorem. As a corollary from Thm. 1 of Anis et al. (2016), the system being full rank means that the samples drawn form a uniqueness set and the proof is complete.\nTo select \u03f5, notice that there are still slack in Gershgorin circle theorem and one can select such an \u03f5 that the two projection has eigenfunctions differs by at most that slack amount in L2. This is possible since full-ranked-ness is a robust property: if a matrix is full-rank then other matrices within a small ball from it is also full-rank. Thus, if there is a converging sequence of eigenfunction/eigenspace to \u03c6(x) then the perturbed matrix analogous to Eq. (53) would eventually enter the small ball of full-rank matrices. We leave more precise nonasymptotic analysis to future work."
        },
        {
            "heading": "E SMALL EXAMPLE: BLOCK MODEL AND MIXTURE OF DISJOINT UNIFORM DISTRIBUTIONS",
            "text": "Let us consider a simplified setting, consisting of a blockmodel kernel and uniform mixture components, to show an example where Gaussian elimination recovers intervals distributed according to the {qi}Ki=1.\nProposition 5. Let I = \u21261\u222a . . .\u222a\u2126N be anN -partition of \u2126. Let the kernel k be aK-block model over a coarser partition I \u2032 = \u2126\u20321 \u222a . . . \u222a \u2126\u2032K of \u2126 containing I (each block has value given by the integral of K over the centroid). Let the Pi be uniform over the \u2126\u2032i. Then, column-wise Gaussian elimination over the positive eigenfunctions(vectors) finds subsets \u2126j1 , . . . ,\u2126jK distributed with probability density functions equal to the corresponding qi, up to a normalization.\nProof of Prop. 5. The kernel k can be written as\nk(\u03c9, \u03b8) = K\u2211 i,j=1 aijI(\u03c9 \u2208 \u2126\u2032i)I(\u03b8 \u2208 \u2126\u2032j). (54)\nTherefore, the model K can be represented as a SBM graphon,\nA =\n i11 . . . i 1 k1\n. . . iK1 . . . i K kK\ni11 a11 . . . a11 . . . a1K . . . a1K ... ... . . . ... ... . . . ... i1k1 a11 . . . a11 . . . a1K . . . a1K\n... ...\n. . . ...\niK1 a1K . . . a1K . . . aKK . . . aKK ... ... . . . ... ... . . . ... iKkK a1K . . . a1K . . . aKK . . . aKK\n (55)\nwhere iljl , 1 \u2264 jl \u2264 kl, indexes elements of I contained in \u2126 \u2032 l (i.e., in the support of Pl), and\u2211K\nj=1 kj = N . For a more concise representation, let us write\nA = A11 . . . A1K... . . . ... A1K . . . AKK  (56) where Aij = aij11T .\nConsider the normalized adjacency A\u0303 = (D\u2020)1/2A(D\u2020)1/2, which has the same block structure as A but with blocks A\u0303ij . Note that technically, we would find eigenvectors of the normalized Laplacian I\u2212A\u0303 but the identity shift only shifts the spectrum by 1 (after inversion about the origin). Therefore it is equivalent to finding the eigenvectors of A\u0303:\n A\u030311 . . . A\u03031K... . . . ... A\u03031K . . . A\u0303KK u = \u03bbu. (57) Note, however, that for each 1 \u2264 i \u2264 K, the rows corresponding to [A\u03031i . . . A\u0303Ki]u are repeated, so plugging A\u0303 into an eigensolver without simplifying A\u0303 first is going to incur huge computational cost for little gain. We can exploit the repeated structure of A\u0303 to do some preprocessing first, via a variant of Gaussian elimination. Permuting the rows and columns of this matrix to ensure the sequence ai1, . . . , aiK appears in the first K columns, and subtracting the repeated rows, we can rewrite this as  a\u030311 . . . a\u03031K b1 ... . . . ...\n... a\u03031K . . . a\u0303KK bK 0 . . . 0 0\nu = \u03bbu (58)\nwhere the bi \u2208 RN\u2212K are row vectors collecting the remaining entries of row i after permutation, and 0 denotes the all-zeros vector of dimension N \u2212K. For the linear system in Eq. (58), it is easy to see that the solutions u must have form u = [u1 . . . uk 0 . . . 0]\nT . Hence, the eigenvectors of the modified matrix in Eq. (58) are the eigenvectors of its K \u00d7K principal submatrix padded with zeros. To obtain the eigenvectors of the original matrix Eq. (57), we simply have to \u201crevert\u201d the operations performed to get from there to Eq. (58), with the appropriate normalizations to ensure orthonormality. By doing so, we get eigenvectors of\nthe following form\nu =\n u1 k1 times ... u1 ... uK\nkK times ... uK\n (59)\ni.e., in every eigenvector of A\u0303, entries corresponding to sets \u2126i contained in the same set \u2126\u2032k are the same.\nNow, assume that we have found all K eigenvectors of A\u0303 and collect them in the matrix UK \u2208 RN\u00d7K . To find a uniqueness set for the associated graphon, we perform columnwise Gaussian elimination on UK , and add the indices of the zeros in the Kth row of the echelon form to the sampling set.\nIn the current example, this heuristic is always guaranteed to find a uniqueness set. Any combination of indices corresponding to K different rows from UK forms such a set. Since through Gaussian elimination we are guaranteed to pickK linearly independent rows, when picking a row from cluster \u2126i for arbitrary i, all ki rows are equally likely to be picked, as they are equal and thus have the same \u201cpivoting\u201d effect. In an independent trial, the probability of picking a row from \u2126\u2032i is thus (ki/N)\u00d7Pi. Up to a normalization, this probability is equal to qi = APi. The entries of this vector determine the level sets of qi as qi(x) = qiI(x \u2208 \u2126\u2032i) (60) completing the proof."
        },
        {
            "heading": "F ELEMENTS FROM (SCHIEBINGER ET AL., 2015)",
            "text": "For completeness, we reproduce elements from (Schiebinger et al., 2015) that were used in our paper.\nF.1 DIFFICULTY FUNCTION FOR MIXTURE MODELS\nRecall \u2126 is a measurable space and P(\u2126) is a set of all probability measures on \u2126. Let Pi \u2208 P(\u2126) mixture components for i = 1..K. A mixture model is a convex combination:\nP := K\u2211 i=1 wiPi, (61)\nfor a set of weights wi \u2265 0 for i = 1 . . .K and \u2211\ni wi = 1. Recall that there is also a kernel k associated with the mixture model.\nThe statistics of how well-separated the mixture components are can be quantified through five defined quantities:\nSimilarity index. For any distinct pair of mixtures l \u0338= k, the kernel-dependent similarity index between Pl and Pk is: S(Pl,Pk) := \u222b \u2126 \u222b \u2126 k(\u03c9, \u03b8)dPl(\u03c9)dPl(\u03b8)\u222b\n\u2126 \u222b \u2126 k(\u03c9, \u03b8)dP(\u03c9)dPl(\u03b8) , (62)\nand the maximum over all ordered pairs of similarity index is: Smax(P) := max\nl \u0338=k S(Pl,Pk) (63)\nIn general, Smax measures the worst overlap between any two components with respect to the kernel k.\nCoupling parameter. The coupling parameter is defined as:\nC(P) := max m \u2225\u2225\u2225\u2225 k(\u03c9, \u03b8)qm(\u03c9)qm(\u03b8) \u2212 wm k(\u03c9, \u03b8)q(\u03c9)q(\u03b8) \u2225\u2225\u2225\u22252 Pm\u2297Pm , (64)\nwhere q(\u03b8) = \u221a\u222b k(\u03c9, \u03b8)dP(\u03c9) and qm(\u03b8) = \u221a\u222b\nk(\u03c9, \u03b8)dPm(\u03c9). It measures the coupling of function spaces over P2 with respect to the Laplacian operator. When it is 0, for instance, the Laplacian over P is the weighted sum of Laplacians over Pm with weights wm.\nIndivisibility parameter. The indivisibility of a probability measure is defined as:\n\u0393(Q) := inf S\u2282\u2126\np(\u2126) \u222b S \u222b Sc\nk(\u03c9, \u03b8)dQ(\u03c9)dQ(\u03b8) p(S)p(Sc) , (65)\nwhere p(S) := \u222b S \u222b \u2126 k(\u03c9, \u03b8)dQ(\u03c9)dQ(\u03b8).\nAnd \u0393min(P) := minm \u0393(Pm) measures how easy it is to split a single component into two which is suggestive of ill-fittedness of the current model.\nBoundedness parameter. Finally, we define:\nbmax := max m \u2225\u2225\u2225\u2225 k(\u00b7, \u03b8)qm(\u00b7)qm(\u03b8)dPm(\u03b8) \u2225\u2225\u2225\u22252 \u221e . (66)\nThis is just a constant when the kernel is bounded.\nThe difficulty function. With these parameters set up, we can now define the difficulty function used in Prop. 3: \u03d5(P,k) := \u221a K(Smax(P) + C(P)) minm wm\u03932min(P) . (67)\nF.2 FINITE-SAMPLE CONE STRUCTURE ELEMENTS\nTo get Theorem 2 from (Schiebinger et al., 2015), we require additional concepts and notations. For two vectors u, v in RK , we define the angle between them angle(u, v) := arccos \u27e8u,v\u27e9\u2225u\u2225\u2225v\u2225 . An orthogonal cone structure OSC with parameter \u03b1, \u03b8 is an embedding of n points {(Xi \u2208 Rn, Zi \u2208 [K])}i\u2208[n] into RK such that for each m \u2208 [K], we can find a subset Sm with at least a (1 \u2212 \u03b1) proportion of all points with Zi = m where any K points taken from each one of these subsets have pairwise angle at least \u03b8.\nIn the derivation of Thm. 6, Schiebinger et al. (2015) also let b be such that k \u2208 (0, b), and r be such that qm(Xm) \u2265 r > 0 with probability 1. c0, c1, . . . are then other constant that depends only on b and r.\nIn conjunction with other works on the topic, they also defined a tail decay parameter:\n\u03c8(t) := K\u2211 m=1 Pm [ q2m(X) \u2225qm\u22252P < t ] (68)\nand an extra requirement and the difficulty function: that there exists a \u03b4 > 0 such that:\n\u03d5(P;K) + 1\n\u03932min(P)\n( 1\u221a n + \u03b4 ) \u2264 c\u03932min(P). (69)\nIn words, it means that the indivisibility parameter of the mixture model is not too small relative to the clustering function. Finally, in the statement of Thm. 6, the difficulty parameter is reparameterized as the left hand side of Eq. (69):\n\u03d5n(t) := \u03d5(P; k) + 1\n\u03932min(P)\n( 1\u221a n + \u03b4 ) , (70)\nwhere n is the number of points in the dataset."
        },
        {
            "heading": "G ADDITIONAL EXPERIMENT DETAILS",
            "text": "All the code for the numerical experiments was written using the PyTorch and PyTorch Geometric libraries. The first set of experiments was run on an Intel i7 CPU, and the second set on an NVIDIA A6000 GPU.\nTransferability for node classification. The details of the citation network datasets used in this experiment are displayed in Table 3. To perform graphon sampling, the nodes in these networks were sorted by degree. We considered a 60-20-20 training-validation-test random split of the data for each realization. In all scenarios, we trained a GNN consisting of a 2-layer GCN with embedding dimension 32 and ReLU nonlinearity, and 1 readout layer followed by softmax. We minimized the negative log-likelihood using ADAM with learning rate 0.001 and default forgetting factors over 100 training epochs.\nPositional encodings for graph classification. We anonymized the MalNet-Tiny dataset by removing the node features and replacing them with the all-ones signal. Since we focus on large graphs, we further removed any graphs with less than 4500 nodes. This brought the number of classes down to 4, and we additionally removed samples from certain classes at random to balance the class sizes, yielding a dataset with 216 graphs in total (54 per class). We considered a 60-20-20 random split of the data for each realization. In all scenarios, we trained a GNN consisting of a 4-layer GCN with embedding dimension 64 and ReLU nonlinearity, and 1 readout layer with mean aggregation followed by softmax. We minimized the negative log-likelihood using ADAM with batch size 8, learning rate 0.001 and default forgetting factors over 150 training epochs. The PEs are the 10 first normalized Laplacian eigenvectors, and to obtain the graphon-sampled subgraph, we fix \u03bb = \u03bb10, q = 20, p = 10, 2 communities, and r = 10."
        }
    ],
    "year": 2023
}