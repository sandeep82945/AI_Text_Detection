{
    "abstractText": "One-shot Federated Learning (OFL) has become a promising learning paradigm, enabling the training of a global server model via a single communication round. In OFL, the server model is aggregated by distilling knowledge from all client models (the ensemble), which are also responsible for synthesizing samples for distillation. In this regard, advanced works show that the performance of the server model is intrinsically related to the quality of the synthesized data and the ensemble model. To promote OFL, we introduce a novel framework, Co-Boosting, in which synthesized data and the ensemble model mutually enhance each other progressively. Specifically, Co-Boosting leverages the current ensemble model to synthesize higher-quality samples in an adversarial manner. These hard samples are then employed to promote the quality of the ensemble model by adjusting the ensembling weights for each client model. Consequently, Co-Boosting periodically achieves high-quality data and ensemble models. Extensive experiments demonstrate that Co-Boosting can substantially outperform existing baselines under various settings. Moreover, Co-Boosting eliminates the need for adjustments to the client\u2019s local training, requires no additional data or model transmission, and allows client models to have heterogeneous architectures.",
    "authors": [
        {
            "affiliations": [],
            "name": "ENSEMBLE CO-BOOSTING"
        },
        {
            "affiliations": [],
            "name": "Rong Dai"
        },
        {
            "affiliations": [],
            "name": "Yonggang Zhang"
        },
        {
            "affiliations": [],
            "name": "Ang Li"
        },
        {
            "affiliations": [],
            "name": "Tongliang Liu"
        },
        {
            "affiliations": [],
            "name": "Xun Yang"
        },
        {
            "affiliations": [],
            "name": "Bo Han"
        }
    ],
    "id": "SP:780b97b35b686269c319f63065e62cb729b65c82",
    "references": [
        {
            "authors": [
                "Durmus Alp Emre Acar",
                "Yue Zhao",
                "Ramon Matas",
                "Matthew Mattina",
                "Paul Whatmough",
                "Venkatesh Saligrama"
            ],
            "title": "Federated learning based on dynamic regularization",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Debora Caldarola",
                "Barbara Caputo",
                "Marco Ciccone"
            ],
            "title": "Improving generalization in federated learning by seeking flat minima",
            "venue": "In European Conference on Computer Vision,",
            "year": 2022
        },
        {
            "authors": [
                "Tianyu Chang",
                "Xun Yang",
                "Xin Luo",
                "Wei Ji",
                "Meng Wang"
            ],
            "title": "Learning style-invariant robust representation for generalizable visual instance retrieval",
            "venue": "In Proceedings of the 31st ACM International Conference on Multimedia,",
            "year": 2023
        },
        {
            "authors": [
                "Tianyu Chang",
                "Xun Yang",
                "Tianzhu Zhang",
                "Meng Wang"
            ],
            "title": "Domain generalized stereo matching via hierarchical visual transformation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2023
        },
        {
            "authors": [
                "Hanting Chen",
                "Yunhe Wang",
                "Chang Xu",
                "Zhaohui Yang",
                "Chuanjian Liu",
                "Boxin Shi",
                "Chunjing Xu",
                "Chao Xu",
                "Qi Tian"
            ],
            "title": "Data-free learning of student networks",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2019
        },
        {
            "authors": [
                "Hong-You Chen",
                "Cheng-Hao Tu",
                "Ziwei Li",
                "Han Wei Shen",
                "Wei-Lun Chao"
            ],
            "title": "On the importance and applicability of pre-training for federated learning",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Rong Dai",
                "Li Shen",
                "Fengxiang He",
                "Xinmei Tian",
                "Dacheng Tao"
            ],
            "title": "Dispfl: Towards communicationefficient personalized federated learning via decentralized sparse training",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Rong Dai",
                "Xun Yang",
                "Yan Sun",
                "Li Shen",
                "Xinmei Tian",
                "Meng Wang",
                "Yongdong Zhang"
            ],
            "title": "Fedgamma: Federated learning with global sharpness-aware minimization",
            "venue": "IEEE Transactions on Neural Networks and Learning Systems,",
            "year": 2023
        },
        {
            "authors": [
                "Don Kurian Dennis",
                "Tian Li",
                "Virginia Smith"
            ],
            "title": "Heterogeneity for the win: One-shot federated clustering",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Enmao Diao",
                "Jie Ding",
                "Vahid Tarokh"
            ],
            "title": "Heterofl: Computation and communication efficient federated learning for heterogeneous clients",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Yiqun Diao",
                "Qinbin Li",
                "Bingsheng He"
            ],
            "title": "Towards addressing label skews in one-shot federated learning",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Jiahua Dong",
                "Yang Cong",
                "Gan Sun",
                "Bineng Zhong",
                "Xiaowei Xu"
            ],
            "title": "What can be transferred: Unsupervised domain adaptation for endoscopic lesions segmentation",
            "venue": "In Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Yaroslav Ganin",
                "Evgeniya Ustinova",
                "Hana Ajakan",
                "Pascal Germain",
                "Hugo Larochelle",
                "Fran\u00e7ois Laviolette",
                "Mario Marchand",
                "Victor Lempitsky"
            ],
            "title": "Domain-adversarial training of neural networks. The journal of machine learning",
            "year": 2030
        },
        {
            "authors": [
                "Ian J Goodfellow",
                "Jonathon Shlens",
                "Christian Szegedy"
            ],
            "title": "Explaining and harnessing adversarial examples",
            "venue": "arXiv preprint arXiv:1412.6572,",
            "year": 2014
        },
        {
            "authors": [
                "Neel Guha",
                "Ameet Talwalkar",
                "Virginia Smith"
            ],
            "title": "One-shot federated learning",
            "venue": "arXiv preprint arXiv:1902.11175,",
            "year": 2019
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Clare Elizabeth Heinbaugh",
                "Emilio Luz-Ricca",
                "Huajie Shao"
            ],
            "title": "Data-free one-shot federated learning under very high statistical heterogeneity",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Geoffrey Hinton",
                "Oriol Vinyals",
                "Jeff Dean"
            ],
            "title": "Distilling the knowledge in a neural network",
            "venue": "arXiv preprint arXiv:1503.02531,",
            "year": 2015
        },
        {
            "authors": [
                "Andrew Howard",
                "Mark Sandler",
                "Grace Chu",
                "Liang-Chieh Chen",
                "Bo Chen",
                "Mingxing Tan",
                "Weijun Wang",
                "Yukun Zhu",
                "Ruoming Pang",
                "Vijay Vasudevan"
            ],
            "title": "Searching for mobilenetv3",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2019
        },
        {
            "authors": [
                "Peter Kairouz",
                "H Brendan McMahan",
                "Brendan Avent",
                "Aur\u00e9lien Bellet",
                "Mehdi Bennis",
                "Arjun Nitin Bhagoji",
                "Kallista Bonawitz",
                "Zachary Charles",
                "Graham Cormode",
                "Rachel Cummings"
            ],
            "title": "Advances and open problems in federated learning",
            "venue": "Foundations and Trends\u00ae in Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Sai Praneeth Karimireddy",
                "Satyen Kale",
                "Mehryar Mohri",
                "Sashank Reddi",
                "Sebastian Stich",
                "Ananda Theertha Suresh"
            ],
            "title": "Scaffold: Stochastic controlled averaging for federated learning",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Alex Krizhevsky",
                "Geoffrey Hinton"
            ],
            "title": "Learning multiple layers of features from tiny images",
            "venue": "Master thesis,",
            "year": 2009
        },
        {
            "authors": [
                "Ya Le",
                "Xuan Yang"
            ],
            "title": "Tiny imagenet visual recognition challenge",
            "venue": "CS 231N,",
            "year": 2015
        },
        {
            "authors": [
                "Yann LeCun",
                "L\u00e9on Bottou",
                "Yoshua Bengio",
                "Patrick Haffner"
            ],
            "title": "Gradient-based learning applied to document recognition",
            "venue": "Proceedings of the IEEE,",
            "year": 1998
        },
        {
            "authors": [
                "Buyu Li",
                "Yu Liu",
                "Xiaogang Wang"
            ],
            "title": "Gradient harmonized single-stage detector",
            "venue": "In Proceedings of the AAAI conference on artificial intelligence,",
            "year": 2019
        },
        {
            "authors": [
                "Da Li",
                "Yongxin Yang",
                "Yi-Zhe Song",
                "Timothy M Hospedales"
            ],
            "title": "Deeper, broader and artier domain generalization",
            "venue": "In Proceedings of the IEEE international conference on computer vision,",
            "year": 2017
        },
        {
            "authors": [
                "Huantong Li",
                "Xiangmiao Wu",
                "Fanbing Lv",
                "Daihai Liao",
                "Thomas H Li",
                "Yonggang Zhang",
                "Bo Han",
                "Mingkui Tan"
            ],
            "title": "Hard sample matters a lot in zero-shot quantization",
            "venue": "In Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Qinbin Li",
                "Bingsheng He",
                "Dawn Song"
            ],
            "title": "Practical one-shot federated learning for cross-silo setting",
            "venue": "Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Tian Li",
                "Anit Kumar Sahu",
                "Ameet Talwalkar",
                "Virginia Smith"
            ],
            "title": "Federated learning: Challenges, methods, and future directions",
            "venue": "IEEE signal processing magazine,",
            "year": 2020
        },
        {
            "authors": [
                "Tian Li",
                "Anit Kumar Sahu",
                "Manzil Zaheer",
                "Maziar Sanjabi",
                "Ameet Talwalkar",
                "Virginia Smith"
            ],
            "title": "Federated optimization in heterogeneous networks",
            "venue": "Proceedings of Machine Learning and Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Ying Li",
                "Xingwei Wang",
                "Rongfei Zeng",
                "Praveen Kumar Donta",
                "Ilir Murturi",
                "Min Huang",
                "Schahram Dustdar"
            ],
            "title": "Federated domain generalization: A survey",
            "venue": "arXiv preprint arXiv:2306.01334,",
            "year": 2023
        },
        {
            "authors": [
                "Tao Lin",
                "Lingjing Kong",
                "Sebastian U Stich",
                "Martin Jaggi"
            ],
            "title": "Ensemble distillation for robust model fusion in federated learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Yi Liu",
                "Jialiang Peng",
                "JQ James",
                "Yi Wu"
            ],
            "title": "Ppgan: Privacy-preserving generative adversarial network",
            "year": 2019
        },
        {
            "authors": [
                "Ningning Ma",
                "Xiangyu Zhang",
                "Hai-Tao Zheng",
                "Jian Sun"
            ],
            "title": "Shufflenet v2: Practical guidelines for efficient cnn architecture design",
            "venue": "In Proceedings of the European conference on computer vision (ECCV),",
            "year": 2018
        },
        {
            "authors": [
                "Brendan McMahan",
                "Eider Moore",
                "Daniel Ramage",
                "Seth Hampson",
                "Blaise Aguera y Arcas"
            ],
            "title": "Communication-efficient learning of deep networks from decentralized data",
            "venue": "In Artificial Intelligence and statistics,",
            "year": 2017
        },
        {
            "authors": [
                "Viraaji Mothukuri",
                "Reza M Parizi",
                "Seyedamin Pouriyeh",
                "Yan Huang",
                "Ali Dehghantanha",
                "Gautam Srivastava"
            ],
            "title": "A survey on security and privacy of federated learning",
            "venue": "Future Generation Computer Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Yuval Netzer",
                "Tao Wang",
                "Adam Coates",
                "Alessandro Bissacco",
                "Bo Wu",
                "Andrew Y Ng"
            ],
            "title": "Reading digits in natural images with unsupervised feature learning",
            "year": 2011
        },
        {
            "authors": [
                "Adam Paszke",
                "Sam Gross",
                "Francisco Massa",
                "Adam Lerer",
                "James Bradbury",
                "Gregory Chanan",
                "Trevor Killeen",
                "Zeming Lin",
                "Natalia Gimelshein",
                "Luca Antiga"
            ],
            "title": "Pytorch: An imperative style, highperformance deep learning library",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Yixing Peng",
                "Quan Wang",
                "Zhendong Mao",
                "Yongdong Zhang"
            ],
            "title": "Sade: A self-adaptive expert for multi-dataset question answering",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2023
        },
        {
            "authors": [
                "Siyuan Qiao",
                "Wei Shen",
                "Zhishuai Zhang",
                "Bo Wang",
                "Alan Yuille"
            ],
            "title": "Deep co-training for semisupervised image recognition",
            "venue": "In Proceedings of the european conference on computer vision (eccv),",
            "year": 2018
        },
        {
            "authors": [
                "Zhe Qu",
                "Xingyu Li",
                "Rui Duan",
                "Yao Liu",
                "Bo Tang",
                "Zhuo Lu"
            ],
            "title": "Generalized federated learning via sharpness aware minimization",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Karen Simonyan",
                "Andrew Zisserman"
            ],
            "title": "Very deep convolutional networks for large-scale image recognition",
            "venue": "arXiv preprint arXiv:1409.1556,",
            "year": 2014
        },
        {
            "authors": [
                "Peipei Song",
                "Dan Guo",
                "Xun Yang",
                "Shengeng Tang",
                "Erkun Yang",
                "Meng Wang"
            ],
            "title": "Emotion-prior awareness network for emotional video captioning",
            "venue": "In Proceedings of the 31st ACM International Conference on Multimedia,",
            "year": 2023
        },
        {
            "authors": [
                "Zhenheng Tang",
                "Yonggang Zhang",
                "Shaohuai Shi",
                "Xin He",
                "Bo Han",
                "Xiaowen Chu"
            ],
            "title": "Virtual homogeneity learning: Defending against data heterogeneity in federated learning",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Yusuke Tashiro",
                "Yang Song",
                "Stefano Ermon"
            ],
            "title": "Diversity can be transferred: Output diversification for white-and black-box attacks",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Manasi Vartak",
                "Harihar Subramanyam",
                "Wei-En Lee",
                "Srinidhi Viswanathan",
                "Saadiyah Husnoo",
                "Samuel Madden",
                "Matei Zaharia"
            ],
            "title": "Modeldb: a system for machine learning model management",
            "venue": "In Proceedings of the Workshop on Human-In-the-Loop Data Analytics,",
            "year": 2016
        },
        {
            "authors": [
                "Derui Wang",
                "Chaoran Li",
                "Sheng Wen",
                "Surya Nepal",
                "Yang Xiang"
            ],
            "title": "Man-in-the-middle attacks against machine learning classifiers via malicious generative models",
            "venue": "IEEE Transactions on Dependable and Secure Computing,",
            "year": 2021
        },
        {
            "authors": [
                "Haozhao Wang",
                "Yichen Li",
                "Wenchao Xu",
                "Ruixuan Li",
                "Yufeng Zhan",
                "Zhigang Zeng"
            ],
            "title": "Dafkd: Domain-aware federated knowledge distillation",
            "venue": "In Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Jianyu Wang",
                "Qinghua Liu",
                "Hao Liang",
                "Gauri Joshi",
                "H Vincent Poor"
            ],
            "title": "Tackling the objective inconsistency problem in heterogeneous federated optimization",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Han Xiao",
                "Kashif Rasul",
                "Roland Vollgraf"
            ],
            "title": "Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms",
            "venue": "arXiv preprint arXiv:1708.07747,",
            "year": 2017
        },
        {
            "authors": [
                "Mingzhao Yang",
                "Shangchao Su",
                "Bin Li",
                "Xiangyang Xue"
            ],
            "title": "Exploring one-shot semi-supervised federated learning with a pre-trained diffusion model",
            "venue": "arXiv preprint arXiv:2305.04063,",
            "year": 2023
        },
        {
            "authors": [
                "Xun Yang",
                "Fuli Feng",
                "Wei Ji",
                "Meng Wang",
                "Tat-Seng Chua"
            ],
            "title": "Deconfounded video moment retrieval with causal intervention",
            "venue": "In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval,",
            "year": 2021
        },
        {
            "authors": [
                "Xun Yang",
                "Shanshan Wang",
                "Jian Dong",
                "Jianfeng Dong",
                "Meng Wang",
                "Tat-Seng Chua"
            ],
            "title": "Video moment retrieval with cross-modal neural architecture search",
            "venue": "IEEE Transactions on Image Processing,",
            "year": 2022
        },
        {
            "authors": [
                "Hongxu Yin",
                "Pavlo Molchanov",
                "Jose M Alvarez",
                "Zhizhong Li",
                "Arun Mallya",
                "Derek Hoiem",
                "Niraj K Jha",
                "Jan Kautz"
            ],
            "title": "Dreaming to distill: Data-free knowledge transfer via deepinversion",
            "venue": "In Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Hongxu Yin",
                "Arun Mallya",
                "Arash Vahdat",
                "Jose M Alvarez",
                "Jan Kautz",
                "Pavlo Molchanov"
            ],
            "title": "See through gradients: Image batch recovery via gradinversion",
            "venue": "In Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Jie Zhang",
                "Chen Chen",
                "Bo Li",
                "Lingjuan Lyu",
                "Shuang Wu",
                "Shouhong Ding",
                "Chunhua Shen",
                "Chao Wu"
            ],
            "title": "Dense: Data-free one-shot federated learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Lin Zhang",
                "Li Shen",
                "Liang Ding",
                "Dacheng Tao",
                "Ling-Yu Duan"
            ],
            "title": "Fine-tuning global model via data-free knowledge distillation for non-iid federated learning",
            "venue": "In Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Ruipeng Zhang",
                "Qinwei Xu",
                "Jiangchao Yao",
                "Ya Zhang",
                "Qi Tian",
                "Yanfeng Wang"
            ],
            "title": "Federated domain generalization with generalization adjustment",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Yonggang Zhang",
                "Mingming Gong",
                "Tongliang Liu",
                "Gang Niu",
                "Xinmei Tian",
                "Bo Han",
                "Bernhard Sch\u00f6lkopf",
                "Kun Zhang"
            ],
            "title": "Adversarial robustness through the lens of causality",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Sheng Zhou",
                "Dan Guo",
                "Jia Li",
                "Xun Yang",
                "Meng Wang"
            ],
            "title": "Exploring sparse spatial relation in graph inference for text-based vqa",
            "venue": "IEEE Transactions on Image Processing,",
            "year": 2023
        },
        {
            "authors": [
                "Yanlin Zhou",
                "George Pu",
                "Xiyao Ma",
                "Xiaolin Li",
                "Dapeng Wu"
            ],
            "title": "Distilled one-shot federated learning",
            "venue": "arXiv preprint arXiv:2009.07999,",
            "year": 2020
        },
        {
            "authors": [
                "Zhuangdi Zhu",
                "Junyuan Hong",
                "Jiayu Zhou"
            ],
            "title": "Data-free knowledge distillation for heterogeneous federated learning",
            "venue": "In International conference on machine learning,",
            "year": 2021
        },
        {
            "authors": [
                "Weiming Zhuang",
                "Chen Chen",
                "Lingjuan Lyu"
            ],
            "title": "When foundation model meets federated learning: Motivations, challenges, and future directions",
            "venue": "arXiv preprint arXiv:2306.15546,",
            "year": 2023
        },
        {
            "authors": [
                "Zhou et al",
                "Song"
            ],
            "title": "2023) among many others who can improve AI-based products or user experience via deep learning techniques with large data requirements. With the growing concern of data privacy, federated learning (FL) (McMahan et al., 2017) allows multiple decentralized clients to collectively train a machine learning model without the need to gather all clients\u2019 data. Most FL algorithms follow the communication prototype of FedAvg (McMahan et al., 2017)",
            "year": 2017
        },
        {
            "authors": [
                "Li"
            ],
            "title": "trained models as the ensemble teacher. Subsequently, an auxiliary dataset is employed to distill knowledge from this ensemble and consolidate it into a unified server model. This paradigm, which is the mainstream of OFL, inherently relates the performance of the server to the data and ensemble used in the knowledge distillation stage",
            "year": 2021
        },
        {
            "authors": [
                "Zhou"
            ],
            "title": "2020) proposes to distill the local dataset on the client side and then send it to the server, which may help the aggregation. Yang et al. (2023) proposes to transmit class prototype and utilize auxiliary pre-trained diffusion models. To avoid transmitting additional information, Zhang et al. (2022a) proposes to generate fake data scouring from the direct ensemble and then use these fake data to distill the server model",
            "year": 2022
        },
        {
            "authors": [
                "ensemble",
                "Dennis"
            ],
            "title": "Regarding the improvement",
            "venue": "Heinbaugh et al",
            "year": 2023
        },
        {
            "authors": [
                "Chen"
            ],
            "title": "In the centralized setting, a generator or noise is optimized to mimic the behavior of the teacher model. These synthetic data are then used for knowledge distillation. Inspired by these centralized data-free KD works, generators are updated locally and aggregated globally to synthesize distillation",
            "year": 2020
        },
        {
            "authors": [
                "Zhuang"
            ],
            "title": "understanding the unique challenges and approaches in OFL. Additionally, viewing OFL through the lens of today\u2019s pre-trained foundation model era, as highlighted in works like Chen et al",
            "year": 2023
        },
        {
            "authors": [
                "Diao"
            ],
            "title": "Both of these approaches may not be applicable in real-world scenarios. Moreover, since there is only one communication round, standard FL algorithms based on regularization terms including FedProx (Li et al., 2020b), SCAFFOLD (Karimireddy et al., 2020), and FedDyn",
            "venue": "Heinbaugh et al",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Federated learning (FL) (McMahan et al., 2017) has emerged as a prominent distributed machine learning framework to train a global server model via collaboration among users without sharing their dataset. Though the multi-round parameter-server communication paradigm offers the benefit of effectively exchanging information among clients and the central server, it might not be feasible in the real world. This paradigm brings forth significant challenges: 1) heavy communication burden and the risk of connection drop errors between clients and the server (Li et al., 2020a; Kairouz et al., 2021; Dai et al., 2022), and 2) potential risk for man-in-the-middle attacks (Wang et al., 2021) and various other privacy or security concerns (Mothukuri et al., 2021; Yin et al., 2021).\nOne-shot FL (OFL) (Guha et al., 2019) has emerged as a solution to these issues by restricting communication rounds to a single iteration, thereby mitigating errors arising from multi-round communication and concurrently diminishing the vulnerability to malicious interception. Furthermore, OFL is more practical, particularly within contemporary model market scenarios (Vartak et al., 2016) where clients predominantly offer pre-trained models. In OFL, the server model is aggregated by distilling knowledge from all client models, commonly using the ensemble, while the ensemble is also responsible for synthesizing data samples for knowledge distillation. Consequently, as illustrated in Guha et al. (2019) and Zhang et al. (2022a), the server model\u2019s performance is intricately linked to both the quality of synthesized data and the ensemble. Thus, the primary challenge in improving performance lies in the process of improving the data and the ensemble.\nExisting approaches tend to tackle this challenge by exclusively concentrating on either enhancing the quality of the ensemble or improving the quality of synthetic data. For instance, to bolster ensemble, prior works including Dennis et al. (2021), Heinbaugh et al. (2023), and Diao et al. (2023)\n\u2217Corresponding author. Work done during Rong\u2019s visit to TMLR Group at HKBU.\nmodify the local training phase and require additional transmissions. In terms of improving synthetic data, Li et al. (2021) utilizes auxiliary public datasets, Zhou et al. (2020) proposes transmitting distilled datasets to the server, Yang et al. (2023) proposes to use auxiliary diffusion model and Zhang et al. (2022a) employs data-free data generation methods to synthesize data directly from averaged ensemble models. While the distilled server may improve through the above methods, it is noteworthy that these approaches typically follow a sequential process, which means the enhancement of data or the ensemble is a prerequisite step before the server model can benefit, omitting the crucial relationship between them. What\u2019s more, in contemporary model market scenarios where only well-pre-trained models with diverse architectural possibilities are accessible, any modifications to local training or additional data or model transmissions are discouraged and often disallowed.\nTo address these challenges, we propose Co-Boosting, a novel one-shot federated learning algorithm as in Fig. 1(a), in which the synthesized data and the ensemble model mutually boost each other progressively. More specifically, in each training epoch, higher-quality hard samples are generated based on the previous epoch\u2019s ensemble and server model. Based on these hard samples, the aggregation weight for each client model is adjusted, forming a better ensemble. Subsequently, the server model is updated by distilling knowledge from both the enriched data and the refined ensemble. As a result, with the continuous enhancement of both data and the ensemble, the final server model naturally improves. As depicted in Fig. 1(b), (c), and (d), with a better weighted ensemble model and better-quality hard samples, Co-boosting naturally achieves state-of-art performance.\nThorough experiments on multiple benchmark datasets demonstrate the superiority of the proposed Co-Boosting. What\u2019s more, due to its inherent nature, our proposed Co-Boosting is more practical to today\u2019s model market scenarios. In summary, our main contributions can be summarized as follows: 1) We demonstrate that it is possible to simultaneously improve the quality of the synthesized data and the ensemble, which are two key elements in OFL. This discovery could spur progress in OFL methods, highlighting the need to optimize their interaction. 2) Within an adversarial paradigm, we introduce Co-Boosting, a novel one-shot federated learning method. Periodically, in Co-Boosting, hard samples are generated from the current ensemble, which, in turn, are used to reweight clients, forming an improved ensemble. This mutual enhancement of synthetic data quality and the ensemble collectively contributes to the natural emergence of a high-performing distilled server model. 3) Our proposed method Co-Boosting, is highly practical to the contemporary model market scenarios as it eliminates the necessity for client-side training adjustments, entails no extra data or model transmissions, and accommodates diverse client model architectures. 4) Extensive experiments confirm the effectiveness of Co-Boosting, consistently outperforming other baselines thanks to the improved quality of both the synthetic data and ensemble."
        },
        {
            "heading": "2 RELATED WORKS",
            "text": ""
        },
        {
            "heading": "2.1 ONE-SHOT FEDERATED LEARNING",
            "text": "Guha et al. (2019) originally proposes OFL which collects local models as an ensemble for the final prediction and further proposes to use knowledge distillation (KD) on such ensemble with public data. This paradigm, which is followed by most works, inherently relates the performance of the server model to the data and ensemble used in the KD stage. Li et al. (2021) proposes to improve the ensemble on the public data. Instead of using public data, Zhou et al. (2020) proposes to transmit the distilled local dataset for the server, Yang et al. (2023) proposes to use auxiliary pre-trained diffusion model, while Zhang et al. (2022a) generates fake data scouring from the direct ensemble. Regarding the improvement of the ensemble, Dennis et al. (2021) utilizes a cluster-based method and requires uploading the cluster means. Diao et al. (2023) and Heinbaugh et al. (2023) modify the local training phase of each client by introducing placeholders, or conditional variation auto-encoders. However, none of the aforementioned methods simultaneously address improvements in both data and the ensemble. Moreover, few works can be practically applied, especially in contemporary modelmarket scenarios (Vartak et al., 2016) where only well-pretrained models are provided to the server. This situation implies constraints such as no alterations to the client\u2019s local training, no additional transmissions, and the possibility of client model heterogeneity."
        },
        {
            "heading": "2.2 KNOWLEDGE DISTILLATION",
            "text": "Knowledge distillation (KD) (Hinton et al., 2015) is proposed to transfer knowledge from one or more networks (teacher) to another (student). Taking the same spirit, KD in federated learning focuses on transferring knowledge from multiple local clients to the global server model. Lin et al. (2020) initially introduced to utilize KD at the server side based on an unlabeled auxiliary dataset. In an effort to reduce reliance on proxy datasets, generators that are locally updated and globally aggregated are used in Zhu et al. (2021) and Zhang et al. (2022b) to synthesize distillation samples. Wang et al. (2023) further enhances the basic ensemble distillation by using weighted averaging based on locally trained discriminators. However, in the context of OFL, conducting multiple rounds of training or transmitting generators and discriminators is not practical. Additionally, the need for an additional local client component violates the constraints in modern model-market OFL settings. More seriously, the generator trained locally has direct access to the training samples, potentially leading to privacy leakage through its ability to remember all the training data (Liu et al., 2019). On the other hand, the generator in OFL is trained without access to even one single raw data."
        },
        {
            "heading": "3 METHODOLOGY",
            "text": "In this section, we first introduce the general process of one-shot federated learning (OFL). Then we detail the proposed method, Co-Boosting, in how we generate high-quality data, high-quality ensemble, and how to link and make them boost each other as illustrated in Fig. 1(a)."
        },
        {
            "heading": "3.1 ONE-SHOT FEDERATED LEARNING",
            "text": "Suppose we have a set of clients C, with n = |C| clients in total. Each client ck \u2208 C has a local private dataset Dk = {(xi, yi)}nki=1, where nk = |Dk| is the number of local data samples xi with the corresponding label yi. OFL\u2019s goal is to train a good machine learning model with parameter \u03b8S over D \u225c \u222ank=1Dk with the help of a server in only one communication, as in\nmin \u03b8S L(\u03b8S) \u225c\n1 |D| \u2211\n{xi,yi}\u2208D\n\u2113CE(fS(xi;\u03b8S), yi), (1)\nwhere \u2113CE(\u00b7, \u00b7) is the cross-entropy function, fS(xi;\u03b8S) is the prediction function of the server that outputs the logits (i.e., outputs of the last fully connected layer) of xi given parameter \u03b8S .\nNoticeably, in one-shot federated learning, the original training set Dk cannot be accessed, and only well-pretrained models parameterized by \u03b8k, are provided. Here, we define the ensemble as:\nAw(x; {\u03b8k}nk=1) \u225c n\u2211\nk=1\nwkfk(x;\u03b8k), (2)\nwhere fk(x;\u03b8k) denotes the prediction function that output the logits of x given \u03b8k, while w = [w1, w2, .., wn] adjusts the weights of each local client logits. When wk = 1/n, the ensemble is the same as the averaged ensemble, while when wk = nk/ \u2211n k=1 nk, the ensemble becomes weighted according to the data amount. For simplicity, in the rest paper, we use Aw to denote the ensemble and Aw(x) to denote Aw(x; {\u03b8k}nk=1), which means the output logits of the ensemble given x. When aggregating pre-trained models {\u03b8k}nk=1 into one server model \u03b8S , existing works mostly follow a two-stage framework. The first is to synthesize data DS based on the ensemble output. In particular, giving a random noise z sampled from a standard Gaussian distribution and a random uniformly sampled label ys, the generator G(\u00b7) with \u03b8G is responsible for generating the data xS = G(z), forming the synthetic dataset DS . Typically, to make sure the synthetic data can be classified correctly with a high probability by the ensemble Aw, the following loss is adopted:\nL(\u03b8G) \u225c 1 |DS | \u2211\n{xs,ys}\u2208DS\n\u2113CE(Aw(xs), ys). (3)\nAfter getting the synthetic dataset DS based on the generator in Eq.(3), OFL intends to distill the ensemble Aw into the final server model \u03b8S with the help of these synthetic data, as in:\nmin \u03b8S L(\u03b8S) \u225c\n1 |DS | \u2211\n{xs,ys}\u2208DS\n\u2113KL(Aw(xs), fS(xs;\u03b8S)), (4)\nwhere \u2113KL(\u00b7, \u00b7) denotes the Kullback-Leibler (KL) divergence. Existing works illustrate that the performance of the server model is intrinsically related to the synthetic data DS and the ensemble Aw, which can also be concluded according to Eq.(4)."
        },
        {
            "heading": "3.2 BOOSTING THE DATA QUALITY",
            "text": "Synthesizing data DS is used to distill the ensemble model into the final server model as in Eq.(4). The quality of these synthesized data has been demonstrated vital to the distillation stage Lin et al. (2020). Moreover, since these data are also generated sourcing from the ensemble as in Eq.(3), it is of great importance to make these data embed as much the knowledge of the ensemble as possible and make them transferable to the final server model.\nHowever, as hinted in Wang et al. (2020) and Zhang et al. (2022a), by utilizing only the CE loss, the synthesized data can be easily fitted by the server model, resulting in poor performance in the knowledge distillation stage. Therefore, to improve the quality of the generated data and make them focus more on transferable components, taking inspiration from Dong et al. (2020) and Li et al. (2023a), we increase the importance of hard samples while suppressing the importance of easy-tofit samples in the generation stage. More specifically, given a prediction function f which output logits, we employ the GHM introduced in Li et al. (2019) to measure the sample difficulty d of x:\nd(x, f) = 1\u2212 \u03c3(f(x;\u03b8))y, (5)\nwhere \u03c3(f(x;\u03b8))y is the probability on label y predicted by the function f(\u00b7) with \u03b8. Built upon the sample difficulty, we propose a hard-sample-enhanced loss LH to synthesize data:\nLH(xs, ys;\u03b8G) \u225c 1 |DS | \u2211\n{xs,ys}\u2208DS\nd(xs, Aw)\u2113CE(Aw(xs), ys), (6)\nMoreover, to make synthesized samples hard for the server model to fit, an adversarial loss (Zhang et al., 2022c) is also introduced to generate hard samples. We try to maximize the differences in predictions between the ensemble model and the server model when generating data as follows:\nLA(xs,\u03b8S ;\u03b8G) \u225c 1 |DS | \u2211\n{xs,ys}\u2208DS\n\u2212\u2113KL(Aw(xs), fS(xs;\u03b8S)). (7)\nBy combining the above losses, we can obtain the loss used to train the generator as follows:\nL(\u03b8G) \u225c LH(xs, ys;\u03b8G) + \u03b2LA(xs,\u03b8S ;\u03b8G), (8)\nwhere \u03b2 is the scaling factor for the losses, which is set as 1 in the implementations.\nThough samples synthesized using Eq.(8) are hard to fit for the current ensemble model, their difficulties for the server model are still lacking. This stems from the fact that the server model can easily fit these limited unchanged data during multiple distillation steps. To further promote the sample difficulty and diversity on the fly, we draw inspiration from adversarial learning (Goodfellow et al., 2014; Tashiro et al., 2020) to generate hard and diverse samples for the server model to learn.\nMore specifically, we diverse and increase the sample difficulty d(xs, Aw) on the fly to make the synthetic samples hard to fit by introducing a perturbation \u03b4i for each xs:\n\u03b4i = arg max \u2225\u03b4\u2032\u2225\u221e\u2264\u03f5\nd(xs + \u03b4 \u2032, Aw), (9)\nwhere || \u00b7 ||\u221e represents the L\u221e-norm and \u03f5 controls the strength of perturbation. To simplify the computation and enhance the diversity, instead of the iterative adversarial attacks, we take only one step of loss backward to seek the direction in the input space that maximizes the similarity between the model output and a randomly sampled vector. The hard samples are constructed as follows:\nx\u0303s \u225c xs + \u03f5 \u25bdxs(u\u22a4Aw(xs))\n|| \u25bdxs (u\u22a4Aw(xs))||2 , (10)\nwhere u \u223c Unif([\u22121, 1])d is a randomly sample vector with dimension d. Following these, the originally generated hard samples are further harder and more diverse due to the randomness in u.\nBy replacing each sample xs in DS into x\u0303s, we can achieve a more hard and diverse synthetic dataset DS . Utilizing these hard samples, the knowledge of the ensemble model is transferred to the server model with parameter \u03b8S by knowledge distillation the same as in Eq.(4).\nOverall, with the hard sample technique embedding in the data synthesizing stage (replacing the generator loss in Eq.(3) with Eq.(8) and making them diverse in the distillation stage (reconstruct the synthetic dataset DS according to Eq.(10)), the quality of data generated and used for distillation becomes better, naturally boosting the performance of the server model."
        },
        {
            "heading": "3.3 BOOSTING THE ENSEMBLE QUALITY",
            "text": "The ensemble model takes the role of aggregating knowledge from all pre-trained models {\u03b8k}nk=1 and forms a virtually best-performance teacher. A straightforward method is to obtain the global model by averaging the parameters of all client models (e.g. FedAvg (McMahan et al., 2017)). However, FedAvg may fail to deliver a good performance when data among clients are non-IID (Karimireddy et al., 2020; Acar et al., 2021) and cannot handle the challenge of client model heterogeneity. Recent works (Heinbaugh et al., 2023; Diao et al., 2023) intend to construct a better ensemble model by altering the local training phase of each client, which may be unreliable, especially in today\u2019s model market scenarios. To tackle the client model heterogeneity and make the ensemble more practical, Guha et al. (2019); Zhang et al. (2022a) utilizes the direct ensemble Aw with wk = 1/n as the teacher, which means averaging the logits or weighted averaging them according to the number of the client data (wk = nk/ \u2211n k=1 nk). However, as suggested by Zhang et al. (2023) and Wang et al. (2023), the simple averaging or weighted averaging based on the client data amount may not be effective, especially in non-IID settings. There exists a better weighted combination of each client\u2019s contribution. Yet, their methods either need to alter the local training or transmit additional information, therefore their methods cannot be applied to one-shot FL.\nTo this end, we propose to boost the ensemble quality by searching for a more effective weighted ensemble of logits. As demonstrated by our experimental results in Fig. 1(b), given high-quality data (validation data), we can achieve a better ensemble with weights different from simple averaging or data amount based averaging. Fortunately, instead of using auxiliary data, we actually can acquire high-quality generated data from the hard synthesized samples set DS . Therefore, to get the best weights w = [w1, w2, \u00b7 \u00b7 \u00b7 , wN ] on DS , we need to solve the following optimization problem:\nmin w Lw(w) \u225c\n1 |DS | \u2211\n{xs,ys}\u2208DS\n\u2113CE( N\u2211 k=1 wkfk(xs;\u03b8k), ys), (11)\nwhere ys is the corresponding label to each synthesized hard samples xs. Exploring the optimal w requires multiple inner steps, leading to the training time to increase exponentially. Also inspired by\nAlgorithm 1 Co-Boosting 1: Input: Clients\u2019 local models {\u03b81, \u00b7 \u00b7 \u00b7 ,\u03b8n}, server model \u03b8S , synthetic dataset DS = \u2205, ensem-\nble Aw, generator \u03b8G, perturbation strength \u03f5, step size \u00b5, learning rate of generator and server \u03b7G and \u03b7S , generation iterations TG, global model training epochs T , and batch size b\n2: Output: Global server model \u03b8S 3: for epoch = 0 to T \u2212 1 do 4: // Generate hard synthetic samples 5: Sample a batch of noises and labels {zi, yi}bi=1 6: for tg = 0 to TG \u2212 1 do 7: Generate {xs}bi=1 with {zi}bi=1 and \u03b8G 8: Update the generator: \u03b8G \u2190 \u03b8G \u2212 \u03b7G \u25bd\u03b8G L(\u03b8G), where L(\u03b8G) is defined in Eq.(8) 9: end for\n10: DS \u2190 DS \u222a {xs}bi=1 11: Diverse each sample {xs} in DS to {x\u0303s} according to Eq.(10) 12: // Obtain a better ensemble 13: Update the mixing weights with DS according to Eq.(12) 14: Construct an updated ensemble Aw with updated w according to Eq.(2) 15: // Obtain the final server model 16: for sampling batch {xs} in DS do 17: Update the server model: \u03b8S \u2190 \u03b8S \u2212 \u03b7S \u25bd\u03b8S L(\u03b8S), where L(\u03b8S) is defined in Eq.(4) 18: end for 19: end for\nmethods of adversarial attacks (Goodfellow et al., 2014), we use the gradient\u2019s direction and fixed step size \u00b5 to update w every time after getting each batch of the synthesized data DS :\nwt = Normalize(wt\u22121 \u2212 \u00b5sign(\u25bdwLw(w))), (12)\nwhere Normalize denotes bounding each wk into [0, 1] and sign(\u00b7) means the sign function. The reweighting of each client\u2019s logit results in a superior ensemble model, which will naturally benefit the server model. Moreover, since the operations are done on the logit layer, this reweighting technique can be easily applied to both heterogeneous and homogeneous client model settings."
        },
        {
            "heading": "3.4 CO-BOOSTING THE DATA AND THE ENSEMBLE",
            "text": "As aforementioned, we introduce how to boost the data quality by utilizing hard sample techniques with a fixed ensemble and how to boost the ensemble with a fixed synthetic dataset. Actually, these two stages are inherently entangled and can boost each other at the same time. To get high-quality ensemble Aw and synthesized data DS , we are in fact trying to solve the following problem:\nmin w\n1 |DS | \u2211\n{xs,ys}\u2208DS\nmax \u03b4\u2208S \u2113CE( n\u2211 k=1 wkfk(xs + \u03b4;\u03b8k), ys), (13)\nwhere ys is the label of sample xs, \u03b4 is the perturbation constrained in S. This problem can be addressed adversarially, which means the improvement of the data and the ensemble can be done simultaneously. With better quality data, the weighted ensemble can reach higher performance, while with this better ensemble, the data synthesized sourcing from this ensemble can further embed more knowledge. Therefore, by mutually boosting the quality of the synthesized data and the ensemble, we can naturally get a better-performance server model through the distillation in Eq.(4).\nThe overall algorithm is summarized in Algorithm 1. In each epoch, we first generate hard samples based on the current ensemble model and the last epoch server model. With these generated data, an enhanced ensemble model is cultivated by searching for the optimal ensembling weights of each client\u2019s logits. Utilizing the generated data and the upgraded ensemble, the final server model is trained by distilling the ensemble on these data. As illustrated in Sec. 3.2 and Sec. 3.3, with either one fixed, one can benefit from the other, thus by mutually boosting each other in the proposed Co-Boosting, we can achieve both better quality data and the ensemble periodically. Therefore, the global server model trained on them will inherently become better than other methods."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "4.1 EXPERIMENTAL DETAILS",
            "text": "Datasets and partitions. We conduct experiments on five real-world image datasets that are standard in the FL literature: MNIST (LeCun et al., 1998), FMNIST (Xiao et al., 2017), SVHN (Netzer et al., 2011), CIFAR10, and CIFAR100 (Krizhevsky et al., 2009). To simulate statistical heterogeneity, we use Dirichlet distribution to generate disjoint non-IID client training datasets as in Zhang et al. (2022a) and Heinbaugh et al. (2023). In particular, we sample pk \u223c Dir(\u03b1) and allocate a pik proportion of the data of class i to client k. The parameter \u03b1 controls the level of statistical imbalance, with a smaller \u03b1 inducing more skewed label distributions among clients.\nBaselines. Within the contemporary model-market scenarios, we compare the performance of CoBoosting1 against two existing methods: FedAvg (McMahan et al., 2017) and DENSE (Zhang et al., 2022a). Similar in (Zhang et al., 2022a), we also introduce two prevailing data-free KD methods DAFL (Chen et al., 2019) and ADI (Yin et al., 2020) and apply these to one-shot FL, giving F-DAFL and F-ADI. We also include FedDF (Lin et al., 2020) using the real validation dataset as the baseline.\nConfigurations. Following McMahan et al. (2017), we use CNN with 5 layers for SVHN, CIFAR10, and CIFAR100, LeNet-5 (LeCun et al., 1998) for MNIST and FMNIST. All available test data is used to evaluate the final server model (or ensemble). Unless otherwise stated, experiments are done with 10 clients and Dir(0.1)-parted. Results are reported averaged across at least 3 random seeds."
        },
        {
            "heading": "4.2 GENERAL RESULTS",
            "text": "Overall Comparison. To evaluate the effectiveness of our method, we conduct experiments under various non-IID settings by varying \u03b1 = {0.05, 0.1, 0.3} and report the performance across different datasets and methods in Table 1. Notice, that we use the validation set in FedDF, which is not practical in the real world. From the table, we can conclude that Co-Boosting consistently outperforms all other baselines in all settings. Notably, in many settings, Co-Boosting achieves over a 5% accuracy improvement compared to the best baseline, DENSE. In cases of extreme statistical heterogeneity, such as when \u03b1 = 0.05, Co-Boosting surpasses the best baseline by substantial margins with 12.87%, 5.85%, 5.16%, 8.83%, and 3.07% on MNIST, FMNIST, SVHN, CIFAR-10, and CIFAR-100, respectively. We also compare the performance of the ensemble used in different methods on SVHN and CIFAR-10 in Table 2, others please refer to the Appendix. FedENS denotes averaged ensemble. The superior performance of the server model can be attributed to the enhanced\n1Code is available at https://github.com/rong-dai/Co-Boosting\nquality of synthesized data and the ensemble. While all compared methods, except FedAvg, utilize the direct logits averaging ensemble as the ensemble and aim to distill knowledge from it in a datafree manner, Co-Boosting, with its co-enhancing technique, results in a superior ensemble teacher, surpassing FedENS significantly. In a word, the superiority of our proposed method can be owed to the enhanced data and ensemble quality, which naturally translates into a better server model.\nAdaptation to Client Model Heterogeneity. To evaluate our proposed method in a potential client heterogeneity setting, we apply five different models under a CIFAR-10, Dir(0.1)-parted setting. The heterogeneous models include CNN1 in McMahan et al. (2017), CNN2 in the pytorch tutorial (Paszke et al., 2019), ResNet (He et al., 2016), MobileNet (Howard et al., 2019), and ShuffleNet (Ma et al., 2018). Table 3 demonstrates the results of comparable methods, where Local denotes directly taking the pre-trained model for testing. We take ResNet as the server architecture and omit FedAvg as it does not support this setting. We use the same optimization hyperparameter for all methods and across all model architectures. We remark that as suggested in Zhang et al. (2022a) and Diao et al. (2021), FL under both non-IID data and different model architecture is a quite challenging task. Even under this setting, our proposed Co-Boosting still consistently outperforms other baselines by a large margin thanks to the benefits of making the ensemble and data improve together."
        },
        {
            "heading": "4.3 IN-DEPTH STUDY",
            "text": "Different Local Data Amount. To further assess the effectiveness of Co-Boosting, which involves a betterweighted ensemble, we conduct experiments in an unbalanced local data setting. Similar to Acar et al. (2021), we sample data amounts for each client from a lognormal distribution. Higher values of \u03c3 result in more unequal data distribution. Table 4 and Fig. 2 display the performance of the ensemble and the server, where the prefix \u2019DW-\u2019 signifies weighted averaging based on the local data amount. As observed, reweighting clients based on their data amount yields some benefits, but it falls short of achieving the best ensemble. Furthermore, when using the averaged ensemble FedENS as the teacher, all baseline methods perform poorly due to the suboptimal teacher. In contrast, benefiting from simultaneously boosting data and ensemble, the ensemble we get consistently outperforms FedENS. This leads to a substantial performance gain of the server model achieved in Co-Boosting over all baselines, with a margin of at least 10%.\nTable 4: Test accuracy of ensemble.\nMethod \u03c3=0.4 \u03c3=0.8 \u03c3=1.2\nFedENS 46.87\u00b11.02 41.86\u00b11.20 37.88\u00b11.38 DW-FedENS 47.80\u00b11.21 53.25\u00b10.52 47.52\u00b10.40 Co-Boosting 58.94\u00b10.50 57.41\u00b11.12 55.27\u00b11.72\n0.4 0.8 1.2 0\n10\n20\n30\n40\n50\n60\n70 FedAvg DW-FedAvg FedDF\nF-ADI F-DAFL\nDENSE Co-Boosting\nFigure 2: Test accuracy of server\nDifferent Data Distribution Shift. Following Diao et al. (2023), we also conduct experiments on a C cls partition setting, which means each client only possesses data of C out of all classes. Results in Table 5 further demonstrate the superiority of our proposed method. With better data quality and better ensemble, our method consistently achieves the best server model.\nDifferent Number of Clients. We evaluate the performance of these methods by varying the number of clients participating in OFL in Table 6. From the table, the final sever model still achieves the best accuracy when increasing the number of clients. This again validates that the increment quality of the ensemble model and data naturally brings a better server model.\nEffects of the proposed components. We further study the effectiveness of our proposed hard sample generation loss in Sec.3.2, on-the-fly sample difficulty promotion in Sec.3.2, and ensemble enhancing in Sec.3.3. Table 7 shows the experimental results on SVHN and CIFAR-10 in a 10-client Dir(0.05) parted setting. The results in the table illustrate that individually improving either data or ensemble leads to noticeable enhancements in the final server model performance. However, the most remarkable results are achieved when both data quality and ensemble capability are improved simultaneously. This finding strongly aligns with the underlying motivation of our study.\nMore facets. For a thorough and comprehensive understanding, we operate sensitivity analyses of hyperparameters, compare with multi-round federated learning, and conduct experiments with heavier local models. Please refer to the Appendix for the results. Moreover, since our Co-Boosting needs no alternation of the local training, it can be combined with advanced local training. The results attached in the Appendix further demonstrate the superiority of our proposed Co-Boosting.\nLimitation. The mixing weights are determined using synthetic samples. Though promising, there is still some disparity when compared to a mixing-weighted ensemble trained on real training data, as in Fig. 1(b). One possible way is to introduce virtual data (Tang et al., 2022). The exploration of methods to generate data capable of bridging this gap remains an avenue for further research."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "In this paper, we seek to tackle the inherent bottleneck of one-shot federated learning, where the performance of the server model is inextricably linked with the quality of the generated data and the ensemble. We propose Co-Boosting, a novel method that facilitates a mutually beneficial relationship between data generation and ensemble improvement. By iteratively generating hard samples from the ensemble and enhancing the ensemble based on these data, Co-Boosting adversarially improves the quality of both the data and the ensemble, leading to the natural refinement of the server model. Extensive experiments across various settings validate the efficacy of our method and demonstrate that our method can be practically applied to contemporary model-market scenarios."
        },
        {
            "heading": "ACKNOWLEDGEMENTS",
            "text": "RD, YGZ and BH were supported by the NSFC General Program No. 62376235, Guangdong Basic and Applied Basic Research Foundation No. 2022A1515011652, CCF-Baidu Open Fund, HKBU Faculty Niche Research Areas No. RC-FNRA-IG/22-23/SCI/04, and HKBU CSD Departmental Incentive Scheme. RD and XY were supported by National Natural Science Foundation of China (NSFC) under Grant U22A2094. TL is partially supported by the following Australian Research Council projects: FT220100318, DP220102121, LP220100527, LP220200949, IC190100031."
        },
        {
            "heading": "A MORE DISCUSSIONS ABOUT RELATED WORKS",
            "text": ""
        },
        {
            "heading": "A.1 ONE-SHOT FEDERATED LEARNING",
            "text": "Large-scale data is beneficial to service providers in areas such as computer vision (Chang et al., 2023b;a), natural language processing (Peng et al., 2023), or video processing (Yang et al., 2021; 2022; Zhou et al., 2023; Song et al., 2023) among many others who can improve AI-based products or user experience via deep learning techniques with large data requirements. With the growing concern of data privacy, federated learning (FL) (McMahan et al., 2017) allows multiple decentralized clients to collectively train a machine learning model without the need to gather all clients\u2019 data. Most FL algorithms follow the communication prototype of FedAvg (McMahan et al., 2017), which requires many communication rounds to train an effective global. To minimize communication costs and mitigate potential security risks associated with multi-round communications, one-shot federated learning (OFL) has emerged as a promising research direction.\nCompared to the straightforward baseline of parameter averaging, as in FedAvg (McMahan et al., 2017), Guha et al. (2019) is the first to propose OFL and utilizes the ensemble of each client\u2019s pretrained models as the ensemble teacher. Subsequently, an auxiliary dataset is employed to distill knowledge from this ensemble and consolidate it into a unified server model. This paradigm, which is the mainstream of OFL, inherently relates the performance of the server to the data and ensemble used in the knowledge distillation stage.\nWith the purpose of using high-quality data, Li et al. (2021) proposes a two-tier knowledge distillation stage with the public dataset. Instead of using public data, Zhou et al. (2020) proposes to distill the local dataset on the client side and then send it to the server, which may help the aggregation. Yang et al. (2023) proposes to transmit class prototype and utilize auxiliary pre-trained diffusion models. To avoid transmitting additional information, Zhang et al. (2022a) proposes to generate fake data scouring from the direct ensemble and then use these fake data to distill the server model.\nRegarding the improvement of the ensemble, Dennis et al. (2021) reforms the OFL task by utilizing a cluster-based method and requires uploading the cluster means. Diao et al. (2023) modify the local training phase of each client by introducing the placeholders in each client model. Heinbaugh et al. (2023) alters the local model into conditional variation auto-encoders and uses these to generate samples for ensemble and knowledge distillation. Nevertheless, it\u2019s worth noting that none of the previously mentioned approaches tackle enhancements in both data and ensemble simultaneously. Furthermore, only a few methods are applicable in real-world scenarios, particularly in the context of modern model-market scenarios (Vartak et al., 2016), where the server is supplied with exclusively well-pretrained models. In this context, there are constraints that include maintaining the integrity of the client\u2019s local training process, avoiding additional data transmissions, and accommodating potential variations in client model heterogeneity.\nWe compare these OFL algorithms in Table 8. Noticeably, we are the first to simultaneously enhance both data and ensemble, and our approach is adaptable to contemporary model-market scenarios."
        },
        {
            "heading": "A.2 KNOWLEDGE DISTILLATION",
            "text": "Knowledge distillation (KD) (Hinton et al., 2015) is proposed to transfer knowledge from one or more networks (teacher) to another (student). Typically, the student model is trained by minimizing the discrepancy between student and teacher logits generated using a suitable auxiliary dataset; KLdivergence is often chosen as the measure of the discrepancy.\nIn the same vein, Knowledge Distillation (KD) in federated learning aims to transfer knowledge from multiple local clients to a global server model. This concept was initially introduced by Lin et al. (2020) which proposed using KD at the server side based on an unlabeled auxiliary dataset. To reduce dependency on proxy datasets, researchers have turned to data-free knowledge distillation methods used in centralized settings, such as those proposed in Chen et al. (2019) and Yin et al. (2020). In the centralized setting, a generator or noise is optimized to mimic the behavior of the teacher model. These synthetic data are then used for knowledge distillation. Inspired by these centralized data-free KD works, generators are updated locally and aggregated globally to synthesize distillation samples, as demonstrated in the works Zhu et al. (2021) and Zhang et al. (2022b). Wang et al. (2023) further improved upon basic ensemble distillation by implementing weighted averaging based on locally trained discriminators.\nHowever, in one-shot federated learning (OFL), conducting multiple rounds of training or transmitting generators and discriminators is not practical. Moreover, the requirement for an additional local client component contradicts the constraints in modern model-market OFL settings. A more serious concern is that a locally trained generator has direct access to the training samples, which could potentially lead to privacy leakage as it has the ability to remember all the training data Liu et al. (2019). In contrast, the generator in OFL is trained without access to even a single raw data point."
        },
        {
            "heading": "A.3 MORE DISCUSSIONS",
            "text": "The parallels and contrasts with centralized co-learning techniques, such as those discussed in Qiao et al. (2018), are indeed intriguing to explore. It\u2019s important to note that centralized co-learning typically operates under the premise of learning from data that presents different views. However, in the one-shot federated learning context, the divergence in data distributions among clients is significantly more pronounced. Moreover, in OFL, models provided to the server are already pretrained, which precludes the possibility of retraining local models. This distinction is crucial for understanding the unique challenges and approaches in OFL.\nAdditionally, viewing OFL through the lens of today\u2019s pre-trained foundation model era, as highlighted in works like Chen et al. (2022); Zhuang et al. (2023) opens up new avenues of exploration. Our work positions itself as a pioneering effort in adapting OFL to the era of foundation models. The key strength of our method is its independence from alterations in local training and its architectureagnostic nature. This flexibility is particularly relevant in the context of foundation models, which are becoming increasingly central in various domains."
        },
        {
            "heading": "B MORE EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "B.1 MORE DETAILS ABOUT THE EXPERIMENT CONFIGURATION",
            "text": "Dataset. We use five datasets in our experiments. MNIST (LeCun et al., 1998) is a large database of 10-class handwritten digits. It consists of 60,000 training images and 10,000 testing images, each of which is a 28x28 grayscale image. FMNIST (Xiao et al., 2017) is a dataset of Zalando\u2019s 10-class article images. It consists of a training set of 60,000 examples and a test set of 10,000 examples, each of which is a 28x28 grayscale image. SVHN (Netzer et al., 2011) is a real-world image dataset in 10 classes with a size of 32x32 used for developing machine learning and object recognition algorithms, 72,357 for train, and 26,032 for test. CIFAR-10 (Krizhevsky et al., 2009) consists of 60,000 32x32 color images in 10 classes, 50,000 for train, and 10,000 for test, while CIFAR-100 dataset is similar to the CIFAR-10 dataset but it has 100 classes containing 600 images each. Each image is also a 32x32 color image. For the FedDF method, we use 20% of the training set as a validation set for distillation. While all of the test data is used for testing.\nBaselines. To accommodate the contemporary model-market scenarios as seen in Table 8, we compare the performance of Co-Boosting against two existing methods: FedAvg (McMahan et al., 2017) and DENSE (Zhang et al., 2022a). To ensure fair comparisons, we omit comparisons with methods that require the use of auxiliary public datasets, such as Li et al. (2021), or the modification of the local training phases of each client, as seen in Diao et al. (2023) and Heinbaugh et al. (2023). Both of these approaches may not be applicable in real-world scenarios. Moreover, since there is only one communication round, standard FL algorithms based on regularization terms including FedProx (Li et al., 2020b), SCAFFOLD (Karimireddy et al., 2020), and FedDyn (Acar et al., 2021) have no effect. Furthermore, since data-free one-shot FL can be addressed simply by operating data-free KD methods on the direct ensemble model of all models, similar in (Zhang et al., 2022a), we also introduce two prevalent data-free KD methods DAFL (Chen et al., 2019) and ADI (Yin et al., 2020) resulting in F-DAFL and F-ADI. More specifically, F-DAFL and F-ADI aim to either optimize a generator or a noise to mimic the performance of the ensemble teacher model. The data generated from this process is then utilized to distill knowledge into the student (server) model. Additionally, we include FedDF (Lin et al., 2020) using the validation dataset.\nHyper-parameters. Unless otherwise stated, we conduct experiments with 10 clients and Dir(0.1) (high heterogeneous) partition. Results are reported averaged across at least 3 random seeds. For each client\u2019s local training, we use the SGD optimizer with momentum=0.9 and learning rate=0.01. We set the batch size to 128 and the local epoch to 300. Following McMahan et al. (2017), we use a simple CNN with 5 layers for SVHN, CIFAR10, and CIFAR100, LeNet-5 (LeCun et al., 1998) for MNIST and FMNIST. All available test data is used to evaluate the final server model (or ensemble). The generator we use is the same as in Zhang et al. (2022a); Chen et al. (2019) and it is trained by Adam optimizer with a learning rate \u03b7g = 1e3 over TG = 30 rounds. The distillation temperature in the knowledge distillation stage used in the server model stage is set to 4, while the temperature used in the KL loss in the generator loss is set to 1. The perturbation strength is set to \u03f5 = 8/255 and the step size \u00b5 is set to 0.1/n. For the training of the server model fS(\u00b7), we use the SGD optimizer with learning rate \u03b7S = 0.01 and momentum=0.9. The number of total epochs T is set to 500. We maintain the same hyperparameters for all baseline methods."
        },
        {
            "heading": "B.2 COMPARISON OF THE ENSEMBLE MODEL",
            "text": "Our Co-Boosting method, in addition to improving the capabilities of the final server model, also yields an enhanced ensemble model through iterative enhancement of synthetic data and ensemble learning, as illustrated in Algorithm 1. The server model obtained through Co-Boosting can be considered as distilled from the progressively improved ensemble and the generated hard samples. Therefore, investigating the performance of this ensemble is also highly meaningful. In this section, we present the final ensembles obtained by Co-Boosting under various experimental settings corresponding to the main paper and compare them with the ensembles used by other methods, FedENS (average logit output). Results on Dir-parted setting, C cls-parted setting, and different numbers of client settings are demonstrated in Table 9, 10 and 11 respectively.\nAs can be easily concluded from the above tables, the final ensemble model we get in our proposed method consistently outperforms the baseline FedENS to a large extent. This demonstrates that by finding an optimal weight based on the hard samples we generate, we can form a much better ensemble, which will naturally in turn become a better teacher in the server distillation age. It is noteworthy that in cases where the data partition exhibits a higher degree of difficulty (resulting in greater distribution shift among clients), the weighted ensemble we obtain can outperform the baseline weighted average by a more substantial margin. This phenomenon may be attributed to the fact that when clients share a more similar distribution of data, their knowledge tends to converge, making simple averaging somewhat sufficient to encapsulate their collective knowledge. Nevertheless, this is still not optimal. Our weighted ensemble, learned based on the hard samples, excels in determining more effective weights for aggregating all clients across all settings."
        },
        {
            "heading": "B.3 MORE EXPERIMENTS ON MORE DATASETS",
            "text": "To comprehensively evaluate our method\u2019s performance, we conduct experiments using widely used Tiny-ImageNet dataset (Le & Yang, 2015) (200 classes, 500 training samples each) with ResNet18 serving as the client architecture backbone. Using the same settings as other experiments, we explore server models based on both CNN and ResNet18 architectures. The results, presented in Table 12, not only confirm the effectiveness of our method but also highlight its architecture-agnostic feature.\nIt is also of great importance to evaluate our proposed Co-Boosting under scenarios involving feature shifts. To this end, we extend our experiments to include domain generalization datasets, specifically MNIST-M (Ganin et al., 2016) and PACS Li et al. (2017). Experiments follow the settings outlined in Li et al. (2023b); Zhang et al. (2023), which focus on federated domain generalization. In these experiments, each domain is allocated to a different client, and we employ the leave-one-domain-out testing technique.We utilized CNN for MNIST-M and ResNet18 for PACS. To further evaluate our method\u2019s versatility, we tested with both CNN and ResNet18 as server architectures. Results in Table 13 and Table 14 demonstrate the superior performance of our proposed method, which we attribute to the mutual enhancement principle inherent in our approach. The results not only validates our method\u2019s model-agnostic feature but also its effectiveness in settings with feature distribution shifts."
        },
        {
            "heading": "B.4 MORE EXPERIMENTS IN LOCAL MODEL HETEROGENEITY SETTING",
            "text": "Similar to the main paper, we evaluated the performance of the proposed Co-Boosting against other baselines under the same client model heterogeneity setting, while varying the server model architecture. Results are shown in Table 15, all experiments are done in a 5-client Dir(0.1) parted setting. While it is natural to observe variations in performance among different architectures, this can be attributed to the use of the same hyperparameters across all architectures and the inherent differences in representation capacity among them. However, it is noteworthy that Co-Boosting consistently outperforms other baselines regardless of the server architecture. This further underscores the effectiveness of the proposed Co-Boosting technique for enhancing both data and ensemble."
        },
        {
            "heading": "B.5 COMBINATION WITH ADVANCED LOCAL TRAINING",
            "text": "To alleviate the non-IID data problem in the Federated Learning (FL) setting, recent works (Qu et al., 2022; Caldarola et al., 2022; Dai et al., 2023) have shown that the use of the Sharpness Aware Minimization (SAM) technique can be beneficial. It is suggested that incorporating this technique into local training can lead to improved server aggregation. In this section, to investigate the performance of methods utilizing SAM for local training in the one-shot federated learning setup, we conduct experiments on SVHN and CIFAR-10 datasets. The experiments are done with 10 clients under a Dir(0.05) partition scenario. From Table 16, it is evident that introducing SAM-based techniques during the local training phase, as compared to the baseline in Table 1, indeed enhances the performance of various methods. This improvement is attributed to the potential alignment achieved during local training, which ultimately leads to better server performance upon aggregation. While, our proposed method, Co-Boosting, continues to outperform all the baselines, highlighting the su-\nperiority of our approach. Moreover, this shows that our method is not dependent on the specifics of the local training phase. With a better local training method, our approach can further improve."
        },
        {
            "heading": "B.6 MORE EXPERIMENTS WITH HEAVIER MODELS",
            "text": "In this section, we conduct further experiments using larger models, VGG11 (Simonyan & Zisserman, 2014) and ResNet18 (He et al., 2016), on CIFAR-10. The experiments were carried out with 10 clients in a federated setting partitioned according to a Dir(0.05) distribution, which represents a significantly large difference in data distribution. Results in Table 17 show that our method continues to perform well with heavier models. Our proposed Co-Boosting outperforms the existing baselines by a significant margin, approximately 10% in both cases of VGG and ResNet architectures."
        },
        {
            "heading": "B.7 SENSITIVITY ANALYSIS OF HYPERPARAMETERS",
            "text": "Step size \u00b5. \u00b5 is the step size used when adjusting the ensemble weights of each client. It controls the range of weight changes based on the synthetic data. As can be seen from Table 18, when \u00b5 is very small, the performance will decline because the range of weight exploration is not large. When \u00b5 is of moderate size, it does not have a significant impact on the final ensemble and the final server model. This is because after each weight adjustment, a normalization operation is performed to limit it within a certain range. Overall, we recommend using 0.1/n as the step size.\nPerturbation strength \u03f5. \u03f5 is the parameter that adjusts the difficulty of synthesizing data on the fly during data distillation. It controls the range of pixel-level changes on the image plane. As can be seen from Table 19, when \u03f5 is small, it cannot generate sufficiently diverse hard samples, thus not maximizing the extraction of information from the samples. Conversely, when the \u03f5 is large, it causes significant damage at the image level, potentially destroying semantic information. Therefore, we recommend using a moderate epsilon value. In this paper, we use 8/255 as the default."
        },
        {
            "heading": "B.8 COMPARISON WITH MULTI-ROUND FEDERATED LEARNING",
            "text": "In this section, we run FedAvg (McMahan et al., 2017), and Scaffold (Karimireddy et al., 2020), two popular and efficient algorithms for 200 communication rounds. There are 10 clients with 10 local epochs in each round. Fig. 3 shows that our approach can outperform the multi-round FedAvg algorithm across various settings. Furthermore, our method remains competitive with the results of the Scaffold algorithm, which undergoes 200 rounds of communication. However, it\u2019s crucial to note that the performance improvement in multi-round algorithms is a result of multiple rounds of information exchange and model alignment. The multi-round paradigm entails the possibility of client dropouts and vulnerability to potential attacks. In contrast, our algorithm necessitates only a single communication round, making it exceptionally suitable for real-world applications.\nB.9 VISUALIZATION OF SYNTHETIC DATA\nWe conduct data visualization for the synthetic data of our proposed Co-Boosting on the MNIST and CIFAR-10 datasets in Fig. 4 and Fig. 5, where digits 0 to 9 represent 10 different classes. There are a total of five rows corresponding to images generated at the 100th, 200th, 300th, 400th, and 500th global model training epochs. For each epoch, we randomly select 3 images for display. From the figures, it can be observed that the generated data does not hold significant practical value to the human eye. However, it is sufficient for machine learning models to learn a robust classifier."
        }
    ],
    "year": 2024
}