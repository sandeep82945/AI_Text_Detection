{
    "abstractText": "This work aims to improve the efficiency of vision transformers (ViTs). While ViTs use computationally expensive self-attention operations in every layer, we identify that these operations are highly correlated across layers \u2013 a key redundancy that causes unnecessary computations. Based on this observation, we propose SKIPAT, a method to reuse self-attention computation from preceding layers to approximate attention at one or more subsequent layers. To ensure that reusing self-attention blocks across layers does not degrade the performance, we introduce a simple parametric function, which outperforms the baseline transformer\u2019s performance while running computationally faster. We show that SKIPAT is agnostic to transformer architecture and is effective in image classification, semantic segmentation, image denoising, and video denoising. We achieve improved throughput at the same-or-higher accuracy levels in all these tasks.",
    "authors": [],
    "id": "SP:bc54ada3747160204f85aa9f64beb23cb1be2008",
    "references": [
        {
            "authors": [
                "Abdelrahman Abdelhamed",
                "Stephen Lin",
                "Michael S. Brown"
            ],
            "title": "A high-quality denoising dataset for smartphone cameras",
            "venue": "In CVPR,",
            "year": 2018
        },
        {
            "authors": [
                "Abdelrahman Abdelhamed",
                "Stephen Lin",
                "Michael S Brown"
            ],
            "title": "A high-quality denoising dataset for smartphone cameras",
            "venue": "In CVPR,",
            "year": 2018
        },
        {
            "authors": [
                "Nicolas Carion",
                "Francisco Massa",
                "Gabriel Synnaeve",
                "Nicolas Usunier",
                "Alexander Kirillov",
                "Sergey Zagoruyko"
            ],
            "title": "End-to-end object detection with transformers",
            "venue": "In ECCV,",
            "year": 2020
        },
        {
            "authors": [
                "Mathilde Caron",
                "Hugo Touvron",
                "Ishan Misra",
                "Herv\u00e9 J\u00e9gou",
                "Julien Mairal",
                "Piotr Bojanowski",
                "Armand Joulin"
            ],
            "title": "Emerging properties in self-supervised vision transformers",
            "venue": "In ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "Pierre Charbonnier",
                "Laure Blanc-Feraud",
                "Gilles Aubert",
                "Michel Barlaud"
            ],
            "title": "Two deterministic half-quadratic regularization algorithms for computed imaging",
            "venue": "In ICIP,",
            "year": 1994
        },
        {
            "authors": [
                "Tianlong Chen",
                "Yu Cheng",
                "Zhe Gan",
                "Lu Yuan",
                "Lei Zhang",
                "Zhangyang Wang"
            ],
            "title": "Chasing sparsity in vision transformers: An end-to-end exploration",
            "venue": "NeurIPS, 2021",
            "year": 2021
        },
        {
            "authors": [
                "Shen Cheng",
                "Yuzhi Wang",
                "Haibin Huang",
                "Donghao Liu",
                "Haoqiang Fan",
                "Shuaicheng Liu"
            ],
            "title": "Nbnet: Noise basis learning for image denoising with subspace projection",
            "venue": "In CVPR,",
            "year": 2021
        },
        {
            "authors": [
                "Fran\u00e7ois Chollet"
            ],
            "title": "Xception: Deep learning with depthwise separable convolutions",
            "venue": "In CVPR, 2017",
            "year": 2017
        },
        {
            "authors": [
                "Xiangxiang Chu",
                "Zhi Tian",
                "Yuqing Wang",
                "Bo Zhang",
                "Haibing Ren",
                "Xiaolin Wei",
                "Huaxia Xia",
                "Chunhua Shen"
            ],
            "title": "Twins: Revisiting the design of spatial attention in vision transformers",
            "venue": "In NeurIPS,",
            "year": 2021
        },
        {
            "authors": [
                "Corinna Cortes",
                "Mehryar Mohri",
                "Afshin Rostamizadeh"
            ],
            "title": "Algorithms for learning kernels based on centered alignment",
            "year": 2012
        },
        {
            "authors": [
                "Zihang Dai",
                "Hanxiao Liu",
                "Quoc V Le",
                "Mingxing Tan"
            ],
            "title": "Coatnet: Marrying convolution and attention for all data",
            "venue": "sizes. NeurIPS,",
            "year": 2021
        },
        {
            "authors": [
                "Mostafa Dehghani",
                "Anurag Arnab",
                "Lucas Beyer",
                "Ashish Vaswani",
                "Yi Tay"
            ],
            "title": "The efficiency misnomer",
            "venue": "In ICLR,",
            "year": 2022
        },
        {
            "authors": [
                "Jia Deng",
                "Wei Dong",
                "Richard Socher",
                "Li-Jia Li",
                "Kai Li",
                "Li Fei-Fei"
            ],
            "title": "Imagenet: A large-scale hierarchical image database",
            "venue": "In CVPR,",
            "year": 2009
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "Lucas Beyer",
                "Alexander Kolesnikov",
                "Dirk Weissenborn",
                "Xiaohua Zhai",
                "Thomas Unterthiner",
                "Mostafa Dehghani",
                "Matthias Minderer",
                "Georg Heigold",
                "Sylvain Gelly"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
            "venue": "In ICLR,",
            "year": 2020
        },
        {
            "authors": [
                "M. Everingham",
                "L. Van Gool",
                "C.K.I. Williams",
                "J. Winn",
                "A. Zisserman"
            ],
            "title": "The PASCAL Visual Object Classes",
            "venue": "Challenge 2012 (VOC2012) Results. http://www.pascalnetwork.org/challenges/VOC/voc2012/workshop/index.html",
            "year": 2012
        },
        {
            "authors": [
                "Mohsen Fayyaz",
                "Soroush Abbasi Koohpayegani",
                "Farnoush Rezaei",
                "Sunando Sengupta",
                "Hamid Reza Vaezi Joze",
                "Hamed Pirsiavash",
                "Juergen Gall"
            ],
            "title": "Adaptive token sampling for efficient vision transformers",
            "venue": "In ECCV, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Rohit Girdhar",
                "Joao Carreira",
                "Carl Doersch",
                "Andrew Zisserman"
            ],
            "title": "Video action transformer network",
            "venue": "In CVPR,",
            "year": 2019
        },
        {
            "authors": [
                "Benjamin Graham",
                "Alaaeldin El-Nouby",
                "Hugo Touvron",
                "Pierre Stock",
                "Armand Joulin",
                "Herv\u00e9 J\u00e9gou",
                "Matthijs Douze"
            ],
            "title": "Levit: a vision transformer in convnet\u2019s clothing for faster inference",
            "venue": "In ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "Qi Han",
                "Zejia Fan",
                "Qi Dai",
                "Lei Sun",
                "Ming-Ming Cheng",
                "Jiaying Liu",
                "Jingdong Wang"
            ],
            "title": "On the connection between local attention and dynamic depth-wise convolution",
            "venue": "In ICLR,",
            "year": 2022
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Delving deep into rectifiers: Surpassing humanlevel performance on imagenet classification",
            "venue": "In ICCV,",
            "year": 2015
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In CVPR,",
            "year": 2016
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Kevin Gimpel"
            ],
            "title": "Gaussian error linear units (gelus)",
            "venue": "arXiv preprint arXiv:1606.08415,",
            "year": 2016
        },
        {
            "authors": [
                "Alexander Kirillov",
                "Ross Girshick",
                "Kaiming He",
                "Piotr Doll\u00e1r"
            ],
            "title": "Panoptic feature pyramid networks",
            "venue": "In CVPR,",
            "year": 2019
        },
        {
            "authors": [
                "Zhenglun Kong",
                "Peiyan Dong",
                "Xiaolong Ma",
                "Xin Meng",
                "Wei Niu",
                "Mengshu Sun",
                "Bin Ren",
                "Minghai Qin",
                "Hao Tang",
                "Yanzhi Wang"
            ],
            "title": "Spvit: Enabling faster vision transformers via soft token pruning",
            "venue": "In ECCV,",
            "year": 2022
        },
        {
            "authors": [
                "Simon Kornblith",
                "Mohammad Norouzi",
                "Honglak Lee",
                "Geoffrey Hinton"
            ],
            "title": "Similarity of neural network representations revisited",
            "venue": "In ICML,",
            "year": 2019
        },
        {
            "authors": [
                "Kunchang Li",
                "Yali Wang",
                "Gao Peng",
                "Guanglu Song",
                "Yu Liu",
                "Hongsheng Li",
                "Yu Qiao"
            ],
            "title": "Uniformer: Unified transformer for efficient spatiotemporal representation learning",
            "venue": "In ICLR,",
            "year": 2022
        },
        {
            "authors": [
                "Jingyun Liang",
                "Jiezhang Cao",
                "Yuchen Fan",
                "Kai Zhang",
                "Rakesh Ranjan",
                "Yawei Li",
                "Radu Timofte",
                "Luc Van Gool"
            ],
            "title": "Vrt: A video restoration transformer",
            "venue": "arXiv preprint arXiv:2201.12288,",
            "year": 2022
        },
        {
            "authors": [
                "Ze Liu",
                "Yutong Lin",
                "Yue Cao",
                "Han Hu",
                "Yixuan Wei",
                "Zheng Zhang",
                "Stephen Lin",
                "Baining Guo"
            ],
            "title": "Swin transformer: Hierarchical vision transformer using shifted windows",
            "venue": "In ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "Zhuang Liu",
                "Hanzi Mao",
                "Chao-Yuan Wu",
                "Christoph Feichtenhofer",
                "Trevor Darrell",
                "Saining Xie. A convnet for the"
            ],
            "title": "In CVPR, 2022",
            "venue": "3, 8",
            "year": 2020
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter"
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "arXiv preprint arXiv:1711.05101,",
            "year": 2017
        },
        {
            "authors": [
                "Karttikeya Mangalam",
                "Haoqi Fan",
                "Yanghao Li",
                "Chao-Yuan Wu",
                "Bo Xiong",
                "Christoph Feichtenhofer",
                "Jitendra Malik"
            ],
            "title": "Reversible vision transformers",
            "venue": "In CVPR,",
            "year": 2022
        },
        {
            "authors": [
                "Sachin Mehta",
                "Mohammad Rastegari"
            ],
            "title": "Mobilevit: Light-weight, general-purpose, and mobile-friendly vision transformer",
            "venue": "In ICLR,",
            "year": 2021
        },
        {
            "authors": [
                "Sachin Mehta",
                "Mohammad Rastegari"
            ],
            "title": "Separable self-attention for mobile vision transformers",
            "venue": "arXiv preprint arXiv:2206.02680,",
            "year": 2022
        },
        {
            "authors": [
                "Luke Melas-Kyriazi",
                "Christian Rupprecht",
                "Iro Laina",
                "Andrea Vedaldi"
            ],
            "title": "Deep spectral methods: A surprisingly strong baseline for unsupervised semantic segmentation and localization",
            "venue": "In CVPR,",
            "year": 2022
        },
        {
            "authors": [
                "Chong Mou",
                "Jian Zhang",
                "Zhuoyuan Wu"
            ],
            "title": "Dynamic attentive graph learning for image restoration",
            "venue": "In ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "Junting Pan",
                "Adrian Bulat",
                "Fuwen Tan",
                "Xiatian Zhu",
                "Lukasz Dudziak",
                "Hongsheng Li",
                "Georgios Tzimiropoulos",
                "Brais Martinez"
            ],
            "title": "Edgevits: Competing light-weight cnns on mobile devices with vision transformers",
            "venue": "In ECCV,",
            "year": 2022
        },
        {
            "authors": [
                "Zizheng Pan",
                "Bohan Zhuang",
                "Jing Liu",
                "Haoyu He",
                "Jianfei Cai"
            ],
            "title": "Scalable vision transformers with hierarchical pooling",
            "venue": "In ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "Zizheng Pan",
                "Jianfei Cai",
                "Bohan Zhuang"
            ],
            "title": "Fast vision transformers with hilo attention. NeurIPS, 2022b",
            "year": 2022
        },
        {
            "authors": [
                "Zizheng Pan",
                "Bohan Zhuang",
                "Haoyu He",
                "Jing Liu",
                "Jianfei Cai"
            ],
            "title": "Less is more: Pay less attention in vision transformers",
            "venue": "In AAAI, 2022c",
            "year": 2022
        },
        {
            "authors": [
                "Maithra Raghu",
                "Thomas Unterthiner",
                "Simon Kornblith",
                "Chiyuan Zhang",
                "Alexey Dosovitskiy"
            ],
            "title": "Do vision transformers see like convolutional neural networks",
            "venue": "In NeurIPS,",
            "year": 2022
        },
        {
            "authors": [
                "Yongming Rao",
                "Wenliang Zhao",
                "Benlin Liu",
                "Jiwen Lu",
                "Jie Zhou",
                "Cho-Jui Hsieh"
            ],
            "title": "Dynamicvit: Efficient vision transformers with dynamic token sparsification",
            "venue": "NeurIPS, 2021",
            "year": 2021
        },
        {
            "authors": [
                "Chao Ren",
                "Xiaohai He",
                "Chuncheng Wang",
                "Zhibo Zhao"
            ],
            "title": "Adaptive consistency prior based deep network for image denoising",
            "venue": "In CVPR,",
            "year": 2021
        },
        {
            "authors": [
                "Olaf Ronneberger",
                "Philipp Fischer",
                "Thomas Brox"
            ],
            "title": "U-net: Convolutional networks for biomedical image segmentation",
            "venue": "In MICCAI,",
            "year": 2015
        },
        {
            "authors": [
                "Yehui Tang",
                "Kai Han",
                "Yunhe Wang",
                "Chang Xu",
                "Jianyuan Guo",
                "Chao Xu",
                "Dacheng Tao"
            ],
            "title": "Patch slimming for efficient vision transformers",
            "venue": "In CVPR, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Matias Tassano",
                "Julie Delon",
                "Thomas Veit"
            ],
            "title": "Fastdvdnet: Towards real-time deep video denoising without flow estimation",
            "venue": "In CVPR,",
            "year": 2020
        },
        {
            "authors": [
                "Hugo Touvron",
                "Matthieu Cord",
                "Matthijs Douze",
                "Francisco Massa",
                "Alexandre Sablayrolles",
                "Herv\u00e9 J\u00e9gou"
            ],
            "title": "Training data-efficient image transformers & distillation through attention",
            "venue": "In ICML,",
            "year": 2021
        },
        {
            "authors": [
                "Gregory Vaksman",
                "Michael Elad",
                "Peyman Milanfar"
            ],
            "title": "Patch craft: Video denoising by deep modeling and patch matching",
            "venue": "In ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "In NeurIPS,",
            "year": 2017
        },
        {
            "authors": [
                "Jesse Vig"
            ],
            "title": "A multiscale visualization of attention in the transformer model",
            "venue": "arXiv preprint arXiv:1906.05714,",
            "year": 1906
        },
        {
            "authors": [
                "Jesse Vig",
                "Yonatan Belinkov"
            ],
            "title": "Analyzing the structure of attention in a transformer language model",
            "venue": "arXiv preprint arXiv:1906.04284,",
            "year": 1906
        },
        {
            "authors": [
                "Qilong Wang",
                "Banggu Wu",
                "Pengfei Zhu",
                "Peihua Li",
                "Wangmeng Zuo",
                "Qinghua Hu"
            ],
            "title": "Eca-net: Efficient channel attention for deep convolutional neural networks",
            "venue": "In CVPR,",
            "year": 2020
        },
        {
            "authors": [
                "Wenhai Wang",
                "Enze Xie",
                "Xiang Li",
                "Deng-Ping Fan",
                "Kaitao Song",
                "Ding Liang",
                "Tong Lu",
                "Ping Luo",
                "Ling Shao"
            ],
            "title": "Pyramid vision transformer: A versatile backbone for dense prediction without convolutions",
            "venue": "In CVPR,",
            "year": 2021
        },
        {
            "authors": [
                "Wenhai Wang",
                "Enze Xie",
                "Xiang Li",
                "Deng-Ping Fan",
                "Kaitao Song",
                "Ding Liang",
                "Tong Lu",
                "Ping Luo",
                "Ling Shao"
            ],
            "title": "Pvt v2: Improved baselines with pyramid vision transformer",
            "venue": "Computational Visual Media,",
            "year": 2022
        },
        {
            "authors": [
                "Yujing Wang",
                "Yaming Yang",
                "Jiangang Bai",
                "Mingliang Zhang",
                "Jing Bai",
                "Jing Yu",
                "Ce Zhang",
                "Gao Huang",
                "Yunhai Tong"
            ],
            "title": "Evolving attention with residual convolutions",
            "venue": "In ICML,",
            "year": 2021
        },
        {
            "authors": [
                "Zhendong Wang",
                "Xiaodong Cun",
                "Jianmin Bao",
                "Wengang Zhou",
                "Jianzhuang Liu",
                "Houqiang Li"
            ],
            "title": "Uformer: A general u-shaped transformer for image restoration",
            "venue": "In CVPR, 2022b",
            "year": 2022
        },
        {
            "authors": [
                "Zhou Wang",
                "Alan C Bovik",
                "Hamid R Sheikh",
                "Eero P Simoncelli"
            ],
            "title": "Image quality assessment: from error visibility to structural similarity",
            "venue": "IEEE Transactions on Image Processing,",
            "year": 2004
        },
        {
            "authors": [
                "Haiping Wu",
                "Bin Xiao",
                "Noel Codella",
                "Mengchen Liu",
                "Xiyang Dai",
                "Lu Yuan",
                "Lei Zhang"
            ],
            "title": "Cvt: Introducing convolutions to vision transformers",
            "venue": "In ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "Tete Xiao",
                "Yingcheng Liu",
                "Bolei Zhou",
                "Yuning Jiang",
                "Jian Sun"
            ],
            "title": "Unified perceptual parsing for scene understanding",
            "venue": "In ECCV,",
            "year": 2018
        },
        {
            "authors": [
                "Tong Xiao",
                "Yinqiao Li",
                "Jingbo Zhu",
                "Zhengtao Yu",
                "Tongran Liu"
            ],
            "title": "Sharing attention weights for fast transformer",
            "venue": "In IJCAI,",
            "year": 2019
        },
        {
            "authors": [
                "Saining Xie",
                "Ross Girshick",
                "Piotr Doll\u00e1r",
                "Zhuowen Tu",
                "Kaiming He"
            ],
            "title": "Aggregated residual transformations for deep neural networks",
            "venue": "In CVPR,",
            "year": 2017
        },
        {
            "authors": [
                "Weijian Xu",
                "Yifan Xu",
                "Tyler Chang",
                "Zhuowen Tu"
            ],
            "title": "Co-scale conv-attentional image transformers",
            "venue": "In ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "Hongxu Yin",
                "Arash Vahdat",
                "Jose M Alvarez",
                "Arun Mallya",
                "Jan Kautz",
                "Pavlo Molchanov"
            ],
            "title": "A-vit: Adaptive tokens for efficient vision transformer",
            "venue": "In CVPR, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Chengxuan Ying",
                "Guolin Ke",
                "Di He",
                "Tie-Yan Liu"
            ],
            "title": "Lazyformer: Self attention with lazy update",
            "venue": "arXiv preprint arXiv:2102.12702,",
            "year": 2021
        },
        {
            "authors": [
                "Weihao Yu",
                "Mi Luo",
                "Pan Zhou",
                "Chenyang Si",
                "Yichen Zhou",
                "Xinchao Wang",
                "Jiashi Feng",
                "Shuicheng Yan"
            ],
            "title": "Metaformer is actually what you need for vision",
            "venue": "In CVPR,",
            "year": 2022
        },
        {
            "authors": [
                "Li Yuan",
                "Yunpeng Chen",
                "Tao Wang",
                "Weihao Yu",
                "Yujun Shi",
                "Zi-Hang Jiang",
                "Francis EH Tay",
                "Jiashi Feng",
                "Shuicheng Yan"
            ],
            "title": "Tokens-to-token vit: Training vision transformers from scratch on imagenet",
            "venue": "In ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "Syed Waqas Zamir",
                "Aditya Arora",
                "Salman Khan",
                "Munawar Hayat",
                "Fahad Shahbaz Khan",
                "Ming-Hsuan Yang",
                "Ling Shao"
            ],
            "title": "Multi-stage progressive image restoration",
            "venue": "In CVPR,",
            "year": 2021
        },
        {
            "authors": [
                "Syed Waqas Zamir",
                "Aditya Arora",
                "Salman Khan",
                "Munawar Hayat",
                "Fahad Shahbaz Khan",
                "Ming-Hsuan Yang"
            ],
            "title": "Restormer: Efficient transformer for high-resolution image restoration",
            "venue": "In CVPR,",
            "year": 2022
        },
        {
            "authors": [
                "Pengchuan Zhang",
                "Xiyang Dai",
                "Jianwei Yang",
                "Bin Xiao",
                "Lu Yuan",
                "Lei Zhang",
                "Jianfeng Gao"
            ],
            "title": "Multi-scale vision longformer: A new vision transformer for high-resolution image encoding",
            "venue": "In ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "Bolei Zhou",
                "Hang Zhao",
                "Xavier Puig",
                "Sanja Fidler",
                "Adela Barriuso",
                "Antonio Torralba"
            ],
            "title": "Scene parsing through ade20k dataset",
            "venue": "In CVPR,",
            "year": 2017
        },
        {
            "authors": [
                "Daquan Zhou",
                "Bingyi Kang",
                "Xiaojie Jin",
                "Linjie Yang",
                "Xiaochen Lian",
                "Zihang Jiang",
                "Qibin Hou",
                "Jiashi Feng"
            ],
            "title": "Deepvit: Towards deeper vision transformer",
            "venue": "arXiv preprint arXiv:2103.11886,",
            "year": 2021
        },
        {
            "authors": [
                "Daquan Zhou",
                "Yujun Shi",
                "Bingyi Kang",
                "Weihao Yu",
                "Zihang Jiang",
                "Yuan Li",
                "Xiaojie Jin",
                "Qibin Hou",
                "Jiashi Feng"
            ],
            "title": "Refiner: Refining self-attention for vision transformers",
            "venue": "arXiv preprint arXiv:2106.03714,",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "The transformer architecture (Vaswani et al., 2017) has become an important and highly influential model family, due to its simplicity, scalability, and its wide range of applications. While originally stemming from the domain of natural language processing (NLP), with the advent of the Vision transformer (ViT) (Dosovitskiy et al., 2020), this has become a standard architecture in computer vision, setting various state-of-the-art (SoTA) performances on tasks ranging from representation learning, semantic segmentation, object detection and video understanding (Caron et al., 2021; Liu et al., 2021; Carion et al., 2020; Liang et al., 2022; Girdhar et al., 2019).\nHowever, the original formulation of the transformer includes a quadratic computational complexity with respect to the number of input tokens. Given that this number typically ranges from 142 for image classification all the way to 1282 = 16K for image denoising, this constraint on memory and compute severely limits its applicability. To tackle this problem, there have been three sets of approaches. The first leverages redundancies across input tokens and simply reduces computation by efficient sampling, e.g., dropping or merging redundant tokens (Tang et al., 2022; Fayyaz et al., 2022; Yin et al., 2022). This, however, means that the final output of the ViT is not spatially continuous and can thus not be used beyond image-level applications such as semantic segmentation or object localization. The second set of approaches aims to cheaply estimate the attention computation, but generally at the cost of reduced performances (Yu et al., 2022; Chu et al., 2021).\nIn this work, we propose a novel, so far unexplored approach to solving this problem: simply approximating the computationally expensive blocks of the transformer with a much faster, simpler parametric function. To arrive at this solution, we first thoroughly analyse the crucial multi-head self-attention (MSA) block of the ViT. Through this analysis, we find that the attention of the CLS tokens to the spatial patches has a very high correlation across the transformer\u2019s blocks, thus leading to unnecessary computations. This motivates our approach to leverage attention from an early part of the model and simply reuse it for deeper blocks \u2013 basically \u201cskipping\u201d subsequent SA calculations instead of recomputing them at every layer.\nBased on this, we go one step further and explore if the entire MSA block of a layer can be skipped by reusing the representation from previous layers. We find that a simple parametric function inspired from ResneXt\u2019s depth-wise convolutions (Xie et al., 2017) can outperform the baseline performance \u2013 while being computationally faster in terms of throughput and FLOPs. Previous works that use convolutions for improving efficiency in transformers have proposed merging convolution layers\nImage classification (ImageNet-1k)\nSelf-supervised learning (ImageNet-1K)\nSemantic segmentation (ADE20K)\nUnsupervised segmentation (Pascal VOC2012)\nImage Denoising (SIDD)\nwith transformer blocks (Graham et al., 2021), refining MSA representations by introducing convolutions inside MSA blocks (Zhou et al., 2021a;b), or replacing MSA blocks with MLP layers (Pan et al., 2022c). In contrast, we propose to leverage redundancies across MSA blocks and approximate them wholly using parametric functions. SKIPAT is general-purpose and can be applied to a ViT in any context: Figure 1 shows that our novel parametric function achieves superior accuracy vs. efficiency trade-off compared to the baseline transformer on a wide variety of tasks, datasets, and model sizes. SKIPAT is architecture agnostic and can be applied to isotropic, hierarchical, and hybrid transformer architectures resulting in superior performances than the baseline.\nIn summary, our main contributions are as follows:\n1. We propose a novel plug-in module that can be placed in any ViT architecture for reducing the costly O(n2) Self-Attention computations\n2. We show that SKIPAT is agnostic to transformer architecture and achieves state-of-the-art performances in throughput at same-or-better accuracies for ImageNet, Pascal-VOC2012, SIDD, DAVIS and ADE20K (in the latter of which we obtain 40% speedup)\n3. We further demonstrate the generality of our method by obtaining a 26% reduction in selfsupervised pretraining time (at no downstream accuracy loss) and by demonstrating superior on-device latency\n4. Finally, we analyse the sources of performance gains and extensively ablate our method to provide a model family which can be used for trading off accuracy and throughput\nl\u22121 ."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Great effort has been made to improve the efficiency of vision transformers (ViT) (Dosovitskiy et al., 2020) from multiple aspects:\nToken sampling improves the efficiency either by restructuring images during the tokenization step (Yuan et al., 2021; Han et al., 2021), pruning the redundant tokens over training (Kong et al., 2022; Tang et al., 2022) or dynamically at inference (Yin et al., 2022; Rao et al., 2021; Fayyaz et al., 2022; Chen et al., 2021). Despite their effectiveness in reducing the computational cost in image classification, token sampling methods are hardly applicable to dense prediction tasks, e.g. semantic segmentation and image denoising, where the output image should be spatially continuous. Our approach is complementary to these methods and achieves on-par or higher performance on both classification and dense prediction tasks.\nHow do they improve efficiency? Token Hybrid Efficient SKIPAT sampling network attention\nDoes the method satisfy each property?\nImprove throughput during inference? \u2713 \u2713 \u2717 \u2713 Generalize to dense prediction tasks? \u2717 \u2713 \u2713 \u2713 Tackle quadratic complexity of self-attention? \u2717 \u2717 \u2713 \u2713 Generalize to different transformer backbones? \u2717 \u2717 \u2717 \u2713\nTable 1: SKIPAT vs. vision transformers. Comparison between SKIPAT and methods that improve the efficiency of vision transformers. Among the listed methods, only SKIPAT satisfies all the listed properties.\nHybrid architectures such as Uniformer (Li et al., 2022), MobileViT (Mehta & Rastegari, 2021), and others (Liu et al., 2022; Pan et al., 2022a; Mehta & Rastegari, 2022), incorporate efficient convolutional modules into vision transformers. These architectures achieve this by employing MobileNet blocks in Uniformer, MobileNetV2 blocks in MobileViT, or using stacks of convolutions in the image tokenization step (Graham et al., 2021;\nWu et al., 2021). Other approaches disentangle high and low-frequency representations in MSA blocks (Pan et al., 2022b) or replace MSA blocks in the early layers of the network with MLP layers (Pan et al., 2022c). In our work, we use convolutions to accelerate the computation of vision transformers. However, instead of crafting custom blocks, as done in (Mehta & Rastegari, 2021; Pan et al., 2022a; Mehta & Rastegari, 2022; Li et al., 2022; Pan et al., 2022c), we approximate entire MSA block using convolutions. This enables us to apply our method across isotropic, hierarchical, and hybrid transformer architectures. We compare SKIPAT with the existing methods relevant to improving the efficiency of vision transformers in Table 1 and show that among the listed methods, only SKIPAT shows all the listed properties.\nEfficient attention methods aim to reduce the quadratic complexity of the self-attention operation in vision transformers. Several approaches have been proposed, such as global downsampling of key and value embeddings (Wang et al., 2021a; 2022a; Wu et al., 2021), performing self-attention in local windows (Liu et al., 2021), alternating between local and global self-attentions (Chu et al., 2021; Mehta & Rastegari, 2021; Pan et al., 2022a), or replacing self-attention with a simple pooling (Yu et al., 2022). However, reducing self-attention to a local neighborhood limits their ability to model long-range dependencies, leading to significant performance degradation with only moderate speedups (Zhang et al., 2021). In addition, some methods, such as cyclic shift in Swin (Liu et al., 2021), lack efficient support, thus reducing actual efficiency gains in terms of latency. In contrast, our method relies on a few blocks with strong, yet inefficient self-attention operators and lighter, accurate attention estimators in other blocks. As the estimators only use standard convolutional operations, our method translates to actual latency gains. The approach of using convolution layers is similar to (Zhou et al., 2021b;a), that introduce convolutions inside MSA to refine attention maps. However, adding a convolution operation at every layer increases computation overhead. Additionally, (Xiao et al., 2019; Wang et al., 2021b; Ying et al., 2021) observed redundancies in attention maps for NLP tasks. Instead of copying attention maps, we propose an efficient parametric function that achieves high throughput while maintaining high model performance in vision tasks."
        },
        {
            "heading": "3 SKIP-ATTENTION",
            "text": ""
        },
        {
            "heading": "3.1 PRELIMINARIES",
            "text": "Vision Transformer. Let x \u2208 Rh\u00d7w\u00d7c be an input image, where h \u00d7 w is the spatial resolution and c is the number of channels. The image is first tokenized into n = hw/p2 non-overlapping patches, where p\u00d7p is patch size. Each patch is projected into an embedding zi \u2208 Rd using a linear layer to obtain the tokenized image:\nZ0 = (z1; . . . ; zn) \u2208 Rn\u00d7d (1)\nHere, \u201c; \u201d denotes row-wise stacking. Positional embeddings are added to Z0 to retain positional information. The token embeddings are then input to a L = {1, . . . , L} layer transformer whose output is denoted as ZL. In the supervised setting, a learnable token z[CLS] \u2208 Rd is prepended to the tokenized image in (1) as Z0 := (z[CLS];Z0) \u2208 R(n+1)\u00d7d. Transformer Layer. Every layer of the transformer consists of a multi-head self attention (MSA) block followed by a multi-layer perceptron (MLP) block. In the MSA block, the input, Zl\u22121 \u2208 Rn\u00d7d, for l \u2208 L, is first projected into three learnable embeddings {Q,K, V } \u2208 Rn\u00d7d. The\nattention matrix A, is calculated as\nA := \u03c3 ( QKT\u221a\nd\n) \u2208 Rn\u00d7n (2)\nwhere \u03c3(.) denotes the row-wise softmax operation. The \u201cmulti-head\u201d in MSA is defined by considering h attention heads where each head is a sequence of n \u00d7 dh matrix. The attention heads are reprojected back to n\u00d7 d using a linear layer which is combined with the value matrix as\nZMSA := AV \u2208 Rn\u00d7d (3) The output representations from the MSA block is then input to the MLP block which comprises two linear layers separated by a GeLU activation (Hendrycks & Gimpel, 2016). At a given layer l, the computational flow of representations through a transformer block is denoted as\nZl \u2190 ZMSAl + Zl\u22121, (4) Zl \u2190 MLP(Zl) + Zl. (5)\nBoth the MSA and MLP blocks have residual connections with layer normalization (LN) (Ba et al., 2016). While MSA blocks in each layer of the transformer learn representations independently, in the next subsection, we show that empirically there exist high correlation across these layers."
        },
        {
            "heading": "3.2 MOTIVATION: LAYER CORRELATION ANALYSIS",
            "text": "Attention-map correlation. The MSA block in ViT encodes the similarity of each patch to every other patch as an n\u00d7n attention matrix. This operator is computationally expensive with O(n2) complexity (2). As ViTs scale up, i.e., as n increases, the complexity grows quadratically and this operation becomes a bottleneck. Recent NLP works (Vig & Belinkov, 2019; Vig, 2019) have shown that self-attention across adjacent layers in SoTA language models exhibit very high correlation. This raises the question \u2013 is it worth to compute self-attention at every layer of a vision transformer?\nTo address this question, we analyze the correlation of the self-attention maps across different layers of ViT. As shown in Figure 2, the self-attention maps from the class token, A[CLS], exhibit high correlation especially in the intermediate layers. The cosine similarity between A[CLS]l\u22121 and A [CLS] l can be as high as 0.97, as indicated in the bottom of each attention map in Figure 2. We observe similar behavior from other token embeddings, which we analyze in the supplementary material. To quantify this correlation, we compute the Centered Kernel Alignment (CKA) (Kornblith et al., 2019; Cortes et al., 2012) between A[CLS]i and A [CLS] j for every i, j \u2208 L across all validation samples of ImageNet-1K. CKA measures the similarity between representations obtained from intermediate layers of the network, where a high value of CKA indicates high correlation between the representations. From Figure 3 (a), we observe that ViT-T has a high correlation across A[CLS], especially from layers 3 through 10.\nFeature correlation. In ViTs, the high correlation is not just limited to A[CLS], but the representation from MSA blocks, ZMSA, also show high correlation throughout the model (Raghu et al., 2022). To analyze the similarity across these representations, we compute the CKA between ZMSAi and ZMSAj for every i, j \u2208 L. We observe from Figure 3 (b), that ZMSA also have high similarity across adjacent layers of the model especially in the earlier layers, i.e., from layer 2 through 8."
        },
        {
            "heading": "3.3 IMPROVING EFFICIENCY BY SKIPPING ATTENTION",
            "text": "Based on our observation of high representation similarity across MSA blocks of a transformer (subsection 3.2), we propose to leverage the correlation across both the attention matrix and the\nrepresentations from the MSA block to improve the efficiency of vision transformers. Instead of computing the MSA operation (3) independently at every layer, we explore a simple and effective strategy to utilize dependencies across the features from these layers.\nIn particular, we propose to skip MSA computation in one or more layers of a transformer by reusing representations from its adjacent layers. We term this operation as Skip Attention or SKIPAT. As the compute and memory benefit from skipping the entire MSA block is greater than skipping just the self-attention operation (O(n2d+nd2) vs.O(n2d)), in this paper we focus on former. However, instead of directly reusing features, i.e., copying the features from the source MSA block to one or more adjacent MSA blocks, we introduce a parametric function. The parametric function ensures that directly reusing features does not affect the translation invariance and equivariance in these MSA blocks and acts as a strong regularizer to improve model generalization.\nSKIPAT parametric function Let \u03a6 : Rn\u00d7d \u2192 Rn\u00d7d denote the parametric function that maps output of the MSA block from l \u2212 1 to l as Z\u0302MSAl := \u03a6(ZMSAl\u22121 ). Here, Z\u0302MSAl is the approximation of ZMSAl . The parametric function can be as simple as an identity function, where Z MSA l\u22121 is directly reused. Instead of computing MSA operation at l, we use ZMSAl\u22121 as the input to the MLP block at l. When using an identity function, due to the absence of MSA operation at l, the relation across tokens is no longer encoded in the attention matrix, which affects representation learning. To mitigate this, we introduce the SKIPAT parametric function inspired from ResNeXt (Xie et al., 2017) as shown in Figure 4, to encode local relations among tokens. The SKIPAT parametric function consists of two linear layers and a depth-wise convolution (DwC) (Chollet, 2017) in between, as follows:\nZ\u0302MSAl := ECA ( FC2 ( DwC ( FC1(ZMSAl\u22121 ) ))) (6)\nIn the case of supervised learning, we first separate the CLS embeddings from ZMSA \u2208 R(n+1)\u00d7d into class embeddings ZMSAC \u2208 Rd and the patch embeddings to ZMSAP \u2208 Rn\u00d7d. The patch embeddings are then input to the first linear layer FC1 : Rn\u00d7d \u2192 Rn\u00d72d, which expands the channel dimension. This is followed by DwC : R \u221a n\u00d7 \u221a n\u00d72d \u2192 R \u221a n\u00d7 \u221a n\u00d72d with kernel r \u00d7 r. Note that before the DwC operation, we spatially reshape the input matrix to a feature tensor. Han et al. (Han et al., 2022) shows that the behavior of depth-wise convolution operation resembles local attention, which helps learn translation equivalent representations and also reduces the complexity of the parametric function. The output of the DwC is then flattened back to a vector and fed to the last FC layer FC2 : Rn\u00d72d \u2192 Rn\u00d7d which reduces the channel dimension back to its initial dimension d. We use GeLU activations after FC1 and DwC. Following (Wang et al., 2020), we use efficient channel attention module (ECA) after FC2 to enhance the cross-channel dependencies. The ECA module first aggregates the features along the channel dimension using global average pooling (GAP). A 1 \u00d7 1 convolution with adaptive kernel size proportional to channel dimension is applied followed by sigmoid activation. This operation of the ECA module enhances cross-channel dependencies. We then concatenate the embedding of the class-token with the output of the ECA to obtain Z\u0302MSAl .\nSKIPAT framework. The overall framework of SKIPAT is illustrated in Figure 4. SKIPAT can be incorporated into any transformer architecture which we empirically show in section 4. Depending on the architecture, one can skip the MSA operation in one or more layers of the transformer. In\nViT, as we empirically observe that representations from the MSA block, ZMSA, have high correlations from layer 2 through 7 (subsection 3.2), we employ the SKIPAT parametric function in these layers. This means that we use the ZMSA2 as input to the SKIPAT parametric function and skip MSA operations in layers 3-8. Instead, the features from the output of the SKIPAT parametric function is used as input to the MLP block. The computation flow of representations is now:\nZl \u2190 \u03a6(ZMSAl\u22121 ) + Zl\u22121 (7) Zl \u2190 MLP(Zl) + Zl (8)\nDue to the presence of residual connections in the MSA and MLP blocks, which is standard in ViT (Dosovitskiy et al., 2020), the MLP blocks at layer 3 through 8 learn representations independently and cannot be discarded from the computational graph. It is important to note that, with SKIPAT the total number of layers in ViT remain unchanged, but there are fewer MSA blocks.\nComplexity: MSA vs. SKIPAT The self-attention operation involves three operations. Firstly, the token embeddings are projected into query, key and value embeddings, secondly, attention matrix A is computed as dot product between Q and K and finally, the output representations are computed as dot product between A and V . This results in a complexity of O(4nd2 + n2d). Since d\u226a n, the complexity of MSA block can be reduced to O(n2d). The SKIPAT parametric function consists of two linear layers and one depth-wise convolution, which results in a O(2nd2 + r2nd) complexity, where r \u00d7 r is the kernel size of the DwC operation. The overall complexity of SKIPAT can be reduced to O(nd2) since r2 \u226a d. Thus, SKIPAT has fewer FLOPs than MSA block as O(nd2) < O(n2d) when n increases as transformers scale up."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "4.1 COMPARISON WITH STATE-OF-THE-ART",
            "text": "Image Classification We use isotropic transformer architectures like ViT-T/16 (Dosovitskiy et al., 2020), ViT-S/16 (Dosovitskiy et al., 2020), ViT-B/16 (Dosovitskiy et al., 2020), hierarchical architectures like PvT-T (Wang et al., 2021a), PvT-S (Wang et al., 2021a) and hybrid architectures like LIT-T (Pan et al., 2022c) and LIT-S (Pan et al., 2022c) as our backbone on ImageNet-1K. For fair comparisons, we follow the experimental settings in (Touvron et al., 2021), (Wang et al., 2021a) and (Pan et al., 2022c) to train ViT, PvT and LIT respectively. For ViT, we evaluate SKIPAT against SoTA methods: A-ViT (Yin et al., 2022), ATS (Fayyaz et al., 2022), PS-ViT (Tang et al., 2022), and Rev-Vit (Mangalam et al., 2022). To the best of our knowledge, these are all the works that improve the efficiency of ViT without modifying its underlying architecture.\nFrom Table 2a, we observe that SKIPAT achieves the best accuracy vs. efficiency trade-off compared to all SoTA methods on different transformer backbones. Notably, we outperform different variants of ViT by 0.1% to 0.4% and improve throughput by 19%, to 25% . Interestingly, SoTA methods achieve lower accuracy or are on-par with the baseline. Since SKIPAT uses a parametric function to skip computing MSA blocks, our reduction in number of parameters and in FLOPs is comparable to the SoTA. Dehghani et al. (Dehghani et al., 2022) highlight the significance of using throughput as a metric to measure model efficiency: as the reduction in FLOPs does not necessarily correspond to improvements in latency, as it does not take into account the degree of parallelism or other hardware details. In line with this argument, we observe that while SoTA methods such as ATS (Fayyaz et al., 2022) achieve large reduction in FLOPs, they have lower throughput when compared to SKIPAT.\nWe also observe from Table 2a that SKIPAT improves the performance of pyramid architectures PvTT by 1.0% and improves throughput by 19%. On average, SKIPAT outperforms variants of PvT with 20% gain in throughput. We also observe that SKIPAT enhances the performance of hybrid architectures LIT with an average gain of 12% in throughput. Additionally, LIT-S + SKIPAT achieves the same accuracy as baseline LIT-v2-S but with fewer parameters, FLOPs, and 7% gain in throughput. Thus, we show the ability of SKIPAT to generalize to different transformer backbones.\nVisualizing attention maps and ZMSA correlation. We analyze the effect of the SKIPAT parametric function by visualizing the mean of attention heads of the CLS token from the last four layers of ViT-T/16. From Figure 5a, we observe that while attention maps from vanilla ViT (last two layers) do not solely attend to the object, the attention maps from SKIPAT accurately focuses on the object.\nIt is interesting to note that, the attention maps from SKIPAT are also capable of attending to multiple objects in the image (Figure 5a: second example). The CKA of the representations from the MSA block in Figure 5b, shows that ZMSA has lower correlation across layers except between the layers where the MSA operation is skipped (layer 3 to 8). However, unlike vanilla ViT (Figure 3 (b)) the correlation from each layer to every other layer is quite low. This shows that our SKIPAT parametric function acts as a strong regularizer and thus improves the representations of the model.\nUnsupervised object discovery. We further analyze whether pretrained ViTs can attend to semantically meaningful regions of the image when evaluated on a different dataset without fine-tuning it. We follow (Caron et al., 2021), and visualize the segmentation masks produced from the final layer of the pretrained SKIPAT on the Pascal-VOC12 (Everingham et al.). From Table 2(c),we observe that while vanilla ViT-S/16 does not accurately attend to the object, SKIPAT is able to localize objects quite accurately without any fine-tuning. To quantify this observation, using Jaccard similarity and CorLoc (Melas-Kyriazi et al., 2022). As shown in Table 2(c), SKIPAT outperforms different variants of vanilla ViT with a significant gap in terms of Jaccard similarity and CorLoc.\nPerformance on mobile device. To verify the efficiency of SKIPAT on low-power devices, we measure its inference time (averaged over 20 iterations) on a Samsung Galaxy S22 device powered by Qualcomm \u201cSnapdragon\u00ae 8 Gen. 1 Mobile Platform\u201d with a Qualcomm\u00ae HexagonTM processor for image resolutions of 224 \u00d7 224 and 384 \u00d7 384 using ViT-T/16. The inference is performed on Neural Processing Unit in 8-bit precision. As shown in Table 2b, SKIPAT improves the runtime by 19% for image size of 224 \u00d7 224. The gain is even larger at 34% for image resolution 384 \u00d7 384, since the number of token increases. Thus, skipping computationally-heavy MSA blocks increases throughput by large margins and is confirmed even on mobile hardware.\nSemantic Segmentation on ADE20K We show the performance of SKIPAT to dense prediction tasks such as semantic segmentation on ADE20K (Zhou et al., 2017). We follow (Liu et al., 2022; 2021) and use MMSegmentation (Contributors, 2020). We observe from Table 3, that SKIPAT outperforms all variants of ViT with 15% fewer FLOPs and 25% improved throughput. Interestingly, SKIPAT-S (ViT-S + SKIPAT) achieves 8% higher mIoU while being faster than ViT-T. Furthermore, SKIPAT-S has comparable mIoU with Swin-T (Liu et al., 2021) whilst having 3\u00d7 fewer FLOPs and being 1.7\u00d7 faster. Comparing to fully convolution-based architectures, SKIPAT-T (ViT-T + SKIPAT) is on par with ResNet-18 in mIoU while having 4.7\u00d7 fewer FLOPs and being 1.8\u00d7 faster.\nImage Denoising SKIPAT can also generalize to low-level tasks such as image denoising on SIDD (Abdelhamed et al., 2018b), which consists of images with real-world noise. We apply SKIPAT to Uformer (Wang et al., 2022b), a SoTA image denoising model, which is a U-shaped hierarchical network with Swin transformer blocks as the encoder and decoder. Detailed implementation of SKIPAT on Uformer is in the Appendix. Following the settings in (Wang et al., 2022b), we observe in Table 4 that SKIPAT outperforms the baseline Uformer variants with the 25% higher throughput on average. Furthermore, we observe that SKIPAT-B (Uformer-B + SKIPAT) achieves comparable performance with Restormer (Zamir et al., 2022), in terms of PSNR and SSIM, while having 2\u00d7 fewer FLOPs. Thus, we show the ability of SKIPAT to generalize to different tasks and also across architectures. Experiments on video denoising are provided in the Appendix.\n4.2 ABLATIONS\nAll ablations are performed using ViTT/16 on ImageNet-1K for 100 epochs to reduce the training time. Unless specified, following SKIPAT we skip the MSA blocks from layer 3 through 8 for all ablations. Additional ablations are provided in the supplementary material.\nParametric function \u03a6. We study the effect of different parametric functions. As discussed in subsection 3.3, \u03a6 can be as simple as an identity function, where we directly reuse representations from a previous MSA block into one of more subsequent MSA blocks. From Table 5, using an identity function results in a 4.7% drop in top-1 accuracy while being 47% faster than baseline ViT. Using\na convolution or DwC (Chollet, 2017) with kernel size 5 \u00d7 5 as a parametric function leads to the same performance as the baseline. However, DwC is 0.2% better and 50% faster than convolution, and 34% faster than the baseline. SKIPAT parametric function outperforms all.\nKernel size. By default SKIPAT uses a DwC with kernel size of 5\u00d7 5. As shown in Table 5, using a 3\u00d7 3 kernel is faster than default SKIPAT by 6%, but it is 0.6% worse in accuracy. A larger kernel size has poor accuracy and lower throughout. Irrespective of the kernel size, SKIPAT outperforms the baseline ViT-T by at least 1.4%, showing its ability to encode cross-token interactions.\nChannel expansion. In the SKIPAT , the first linear layer FC1, expands the channel dimension from d \u2192 2d. Table 5 shows the impact of channel dimension, i.e., when the channel expansion ratio of FC1 is 1.0 (d \u2192 d) and 0.5 (d \u2192 d/2). We observe that while the lower channel expansion ratio improves the throughput, it performs worse than default SKIPAT. This could be due to sub-optimal representations encoded by the DwC due to fewer filters."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "We proposed SKIPAT, a plug-in module that can be used in any ViT architecture to reduce selfattention computations. SKIPAT leverages the dependency across MSA blocks and bypasses attention computation by re-using attention from previous MSA blocks. We introduced a simple and light parametric function that does not affect the inductive bias encoded in MSA. The SKIPAT function captures cross-token relations and outperforms the baseline while being computationally faster in terms of throughput and FLOPs. We plugged SKIPAT in different transformer architectures and showed its effectiveness on 7 different tasks."
        },
        {
            "heading": "6 IMPLEMENTATION DETAILS",
            "text": ""
        },
        {
            "heading": "6.1 HYPER-PARAMETERS",
            "text": "ImageNet-1K: Image classification. We train SKIPAT on the ILSVRC-2012 dataset (Deng et al., 2009) with 1000 classes (referred as ImageNet-1K). We follow the experimental settings of DeIT (Touvron et al., 2021) and use the codebase from the timm library (Wightman, 2019) to train ViT-T, ViT-S and ViT-B. We use the default 16 \u00d7 16 patch size, using an image resolution of 224\u00d7 224 with total number of tokens n = 196. We train baseline ViT and SKIPAT for 300 epochs from scratch on 4 NVIDIA A100 GPUs using batch sizes of 2048 for ViT-T and 1024 for ViT-S and ViT-B.\nImageNet-1K: Self-supervised learning. We follow the experimental settings of DINO (Caron et al., 2021) and pre-train DINO and SKIPAT on ImageNet-1K using ViT-S/16 as the backbone. While likely the hyperparameters could be tuned further for our proposed SKIPAT method, we use same hyper-parameters for both the baseline and ours, yielding a conservative estimate of our model\u2019s performance. We pre-train both methods from scratch for 100 epochs using 4 NVIDIA A100 GPUs. For linear-probing, we freeze the backbone from the pre-training stage and fine-tune the classifier for 100 epochs, exactly as done in (Caron et al., 2021).\nPascal-VOC2012: Unsupervised object segmentation. We use the Pascal VOC 2012 (Everingham et al.) validation set for this experiment, containing 1449 images. We follow DINO and obtain unsupervised segmentation masks by thresholding the averaged self-attention map (extracted from the last layer of a pretrained ViT/SKIPAT model) to keep 80% of the mass. The Jaccard similarity J between a predicted mask, P , and ground-truth mask, G, is defined as:\nJ(P,G) = G \u2229 P G \u222a P\nWe report Jaccard similarity, averaged over all the samples.\nADE20K: Semantic segmentation. We evaluate SKIPAT on ADE20K (Zhou et al., 2017), a widely-used semantic segmentation dataset, covering 150 semantic categories. The dataset includes 20K and 2K images in the training and validation set, respectively. Different variants of SKIPAT are evaluated using UperNet (Xiao et al., 2018) as the backbone. We use our ImageNet-1K pretrained model to initialize the backbone and Kaiming (He et al., 2015) initialization for other layers. We use AdamW (Loshchilov & Hutter, 2017), with an initial learning rate of 6e \u2212 5, weight decay of 1e\u22122, and linear warmup of 1500 iterations. All models are trained for 160K iterations with a batch size of 16 using MMSegmentation repo (Contributors, 2020). We keep the same hyper-parameters for SKIPAT and ViT.\nSIDD: Image denoising. We follow the experimental settings in Uformer (Wang et al., 2022b) and train SKIPAT on the Smartphone Image Denoising Dataset (SIDD) (Abdelhamed et al., 2018a) which consists of real-world noise. The training samples are first randomly cropped to 128 \u00d7 128 patches and input to the model, which is trained for 250 epochs using batch size 32. The model is then evaluated on images of size 256\u00d7 256.\nDAVIS: Video denoising. We further apply our model to the temporal task of video denoising. We adopt the same U-shape encoder-decoder based architecture of UFormer. As the encoder and decoder backbone, we use UniFormer (Li et al., 2022). We train the model on noise level \u03c3 = 30 using Charbonnier loss (Charbonnier et al., 1994) on patches of 7 \u00d7 128 \u00d7 128 using a multipleinput, multiple-output (MIMO) paradigm (Liang et al., 2022) (i.e., the model outputs 7 reconstructed frames from 7 input frames). During inference, a video is divided into 3D patches of 7\u00d7 128\u00d7 128 with an overlap of 10 pixels. Each patch is fed to the model and the outputs are merged to obtain the final denoised video. Following (Tassano et al., 2020), PSNR is calculated as averaged over videos. We use the same training hyper-parameters as image denoising."
        },
        {
            "heading": "6.2 ARCHITECTURE",
            "text": "Image Classification. All baseline ViT variants have 12 layers in total, which remains unchanged with SKIPAT. Following the CKA analysis of ZMSA in Figure 3(b) of our main paper, we skip computing the MSA blocks in layer 3 through 8 for all ViT variants and retrain it from scratch.\nImage Denoising. We apply SKIPAT to Uformer (Li et al., 2022) a SoTA image denoising model. Uformer is a U-shaped hierarchical network with Swin transformer blocks as the encoder and decoder, and skip connections between them. In SKIPAT, we skip window self-attention (WSA) block in each decoder block by reusing attention of the corresponding encoder block via SKIPAT parametric function. Let ZWSAel \u2208 Rn\u00d7c denote the output of the WSA block at layer l from the encoder and Zdl\u22121 \u2208 Rn\u00d7c denote the output of the layer l\u2212 1 from the decoder of Uformer. The input to the WSA block (which is skipped) at layer l of the decoder is given by\nZ\u0302WSAdl = \u03a6(Z WSAe l ;Z d l\u22121) \u2208 Rn\u00d72c (9)\nHere, \u201c;\u201d denotes concatenation along the channel dimension. We show the framework of SKIPAT on Uformer in Figure 6"
        },
        {
            "heading": "6.3 VIDEO DENOISING",
            "text": "We apply our model to the temporal task of video denoising. As encoder and decoder backbone, we use UniFormer (Li et al., 2022), a U-shaped hybrid encoder-decoder architecture with 3D convolutions and spatio-temporal global self-attention blocks. The encoder of UniFormer comprises two 3D convolution layers followed by two spatio-temporal transformer layers with global self-attention (MSA) blocks. A downsampling operation is used after every layer in the encoder. The decoder is symmetric to the encoder with two transformer layers followed by two 3D convolution layers with an upsampling operation between each layer. Similar to Uformer, skip connections are used between encoder and decoder. Similar to image denoising, we skip MSA blocks in the decoder, however, simply adopt a naive SKIPAT, where we reuse window self-attention matrix, A, of the corresponding encoder block using an Identity function. Let Ael \u2208 Rn\u00d7n denote the self-attention matrix at layer l from the encoder. The self-attention in the decoder stage at layer l is given by Adl = I(A e l ) \u2208 Rn\u00d7n, where I(.) is the identity function. We empirically observe that reusing attention works better in this task, and shows the ability of our method to be applied for different scenarios. We follow the experimental settings in (Tassano et al., 2020) and train SKIPAT on DAVIS (Pont-Tuset et al., 2017) dataset. We train using Charbonnier loss (Charbonnier et al., 1994) on patches of 7\u00d7128\u00d7128 using a multiple-input, multiple-output (MIMO) paradigm (i.e. the model outputs 7 reconstructed frames from 7 input frames) for noise level \u03c3 = 30. From Table 6, we observe that SKIPAT performs on par\nBACKBONE METHOD TOP-1 PARAM GFLOPS THROUGHPUT (%) (\u00d7106) (img/sec \u00d7103)\nT2T-ViT (Yuan et al., 2021) 71.7 5.8 1.1 \u2013 ConvNeXt (iso) (Liu et al., 2022) 72.7 5.7 1.1 5.8\nViT (Dosovitskiy et al., 2020) 72.8 5.7 1.2 5.8 A-ViT (Yin et al., 2022) 71.0 5.7 0.8 6.3 Dynamic ViT (Rao et al., 2021) 70.9 \u2013 0.9 6.1\nViT-T/16 SViTE (Chen et al., 2021) 71.7 4.0 0.9 6.2 SPViT (Kong et al., 2022) 72.7 5.7 0.9 6.7 ATS (Fayyaz et al., 2022) 72.7 5.7 0.9 6.1 PS-ViT (Tang et al., 2022) 72.6 \u2013 0.7 6.6 HVT (Pan et al., 2021) 70.2 5.7 0.7 7.2\nSKIPAT 72.9 5.8 1.1 6.9\nConvNext-T (Liu et al., 2022) 82.1 29.0 4.5 2.6 ConvNeXt (iso) (Liu et al., 2022) 79.7 22.4 4.3 3.3 Swin-T (Liu et al., 2021) 81.3 28.3 4.5 2.5 T2T-ViT (Yuan et al., 2021) 80.7 21.5 5.2 \u2013 CoaT-Lite-S (Xu et al., 2021) 81.9 20 4.0 \u2013 CoAtNet-0 (Dai et al., 2021) 81.6 25 4.2 \u2013 Poolformer-S24 (Yu et al., 2022) 80.3 21.0 3.4 \u2013 Twins-SVT-S (Chu et al., 2021) 81.7 24.0 2.8 \u2013 MobileViT-S (Mehta & Rastegari, 2021) 78.4 5.6 2.0 \u2013 PVT (Wang et al., 2021a) 79.8 24.5 3.8 \u2013\nwith baseline Uniformer, while having 17% fewer FLOPs. This shows that SKIPAT can generalize to temporal tasks.\n7 ADDITIONAL EXPERIMENTS\nImage classification. Here we extend our SoTA comparison with methods that go beyond vanilla ViT architectures. These methods include hierarchical (Swin, PVT, Poolformer, MobileViT, TwinsSVT) and Hybrid (ConvNext, CoAT) architectures. We provide the complete set of SoTA methods that improve the efficiency of ViT either by token sampling (extending Table 1 in our main paper), using hybrid architectures or window self-attention blocks in Table 7. Apart from methods that perform efficient token sampling, none of the other methods are directly comparable because they modify the underlying architecture of ViT, either by using window self-attention blocks or reducing the overall number of transformer layers.\nSelf-Supervised Learning with DINO Next, we show the generality of SKIPAT as its use in the backbone for self-supervised representation learning (SSL), using DINO (Caron et al., 2021). Since, SSL methods are quite expensive in the pretraining stage in terms of compute and training time, we illustrate that SKIPAT achieves comparable performance to using a ViT but with shorter training time. Following the experimental settings of DINO (Caron et al., 2021), we use ViT-S/16 (Dosovitskiy et al., 2020) as our student and teacher networks with SKIPAT parametric function. We pretrain both baseline and ours using DINO for 100 epochs. We observe that SKIPAT achieves almost the same performance as fully trained DINO with around 26% less training time (73.3% in 96 GPUhours vs. 73.6% in 131 GPU-hours). When trained on 100 epochs, we observe that SKIPAT outperforms DINO by 0.5% (74.1% vs. 73.6%).\nUnsupervised segmentation of DINO. We follow DINO (Caron et al., 2021) and evaluate the performance of baseline DINO vs. SKIPAT on unsupervised object segmentation on PascalVOC2012 (Everingham et al.) dataset. We follow the experimental setting as discussed in section 6 and observe that baseline DINO has a Jaccard similarity of 45.3 while SKIPAT achieves 44.7. While SKIPAT outperforms DINO on image classification by 0.5%, we achieve comparable performance in terms of unsupervised object segmentation."
        },
        {
            "heading": "8 ADDITIONAL ABLATIONS",
            "text": "Reusing self-attention. As mentioned in Subsection 3.3, we skip the ZMSA in SKIPAT as the compute and memory benefit from skipping the entire MSA block is greater than skipping just the self-attention operation. Here we study the effect of skipping just the self-attention operation. Let Al\u22121 denote the self-attention matrix at layer l \u2212 1, then the self-attention matrix at layer l is given by A\u0302l = I(Al\u22121). Similar to SKIPAT we skip computing the self-attention matrix from layers 3 through 8. As parametric function \u03a6, we use an identity mapping and train ViT-T/16 from scratch for 100 epochs on ImageNet-1K. We observe from Table 9, that skipping the self-attention matrix results in a top-1 accuracy of 63.2% which is 2.1% higher than the skipping ZMSA with an identity function (61.1% - Table 7 of main paper). However, skipping self-attention matrix results in 20% decrease in throughput (8500\u2192 6800 images/sec) as compared to using an identity function to skip MSA block. It is interesting to note that skipping self-attention matrix results in a lower drop in performance as compared to skipping MSA block. However, applying a parametric function to skip self-attention can be challenging due to the properties of the self-attention matrix, and we leave this to future work.\nSKIPAT in pretrained model. As mentioned in subsection 6.2, we train SKIPAT with all variants of ViT from scratch. For completeness, we also study the effect of skipping the self-attention matrix and the MSA block on a pretrained ViT-T using an Identity function, without retraining. We observe from Table 9 that skipping the self-attention computation in layers 3 through 8, results in a top-1 accuracy of 53.9%, while skipping MSA blocks results in top-1 accuracy of 47.8%. It is interesting to note that the drop in top-1 accuracy from skipping self-attention is merely 19% (72.8\u2192 53.9) on average and does not result in an extremely large drop as one might expect. This shows that there indeed exists high correlation across self-attention and ZMSA, which SKIPAT utilizes to improve the efficiency of the ViTs.\nSkipping MSA in alternate configuration. Instead of skipping the MSA operation in the layers 3 \u2212 8, we study the effect of skipping MSA operation at l \u2208 {3, 5, 7, 9}, {3, 4, 5, 6}, {3, 4, 5, 6, 7, 8, 9, 10} instead of default {3, 4, 5, 6, 7, 8} in Table 8. We observe the default configuration outperforms the baseline ViT by 1.9% (65.8 vs. 67.7%) while being computationally faster in terms of throughput.\n9 CKA ANALYSIS OF ATTENTION FROM VIT-T\nAs discussed in Section 3.2 of our main paper, we analyze the CKA of the selfattention matrix for all tokens between different layers of ViT-T/16 pretrained on ImageNet-1K. Since in the supervised setting A \u2208 R(n+1)\u00d7(n+1), we first remove the CLS token to obtain AP \u2208 Rn\u00d7n. We then compute the CKA of APl for l \u2208 L. We visualization the attention maps for two random patches in Figure 8. We observe similar correlation patterns as observed for CLS token. We akso observe from Figure 7, that there exists a high correlation across all the tokens from the selfattention matrix. Thus, reusing selfattention from different layers of the ViT can improve the overall throughput while yielding comparable accuracy as the baseline ViT."
        }
    ],
    "title": "SKIP-ATTENTION: IMPROVING VISION TRANSFORM- ERS BY PAYING LESS ATTENTION",
    "year": 2023
}