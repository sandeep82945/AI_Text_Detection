{
    "abstractText": "In Causal Bayesian Optimization (CBO), an agent intervenes on a structural causal model with known graph but unknown mechanisms to maximize a downstream reward variable. In this paper, we consider the generalization where other agents or external events also intervene on the system, which is key for enabling adaptiveness to non-stationarities such as weather changes, market forces, or adversaries. We formalize this generalization of CBO as Adversarial Causal Bayesian Optimization (ACBO) and introduce the first algorithm for ACBO with bounded regret: Causal Bayesian Optimization with Multiplicative Weights (CBO-MW). Our approach combines a classical online learning strategy with causal modeling of the rewards. To achieve this, it computes optimistic counterfactual reward estimates by propagating uncertainty through the causal graph. We derive regret bounds for CBO-MW that naturally depend on graph-related quantities. We further propose a scalable implementation for the case of combinatorial interventions and submodular rewards. Empirically, CBO-MW outperforms non-causal and non-adversarial Bayesian optimization methods on synthetic environments and environments based on real-word data. Our experiments include a realistic demonstration of how CBO-MW can be used to learn users\u2019 demand patterns in a shared mobility system and reposition vehicles in strategic areas.",
    "authors": [
        {
            "affiliations": [],
            "name": "Scott Sussex"
        },
        {
            "affiliations": [],
            "name": "Pier Giuseppe Sessa"
        },
        {
            "affiliations": [],
            "name": "Anastasiia Makarova"
        }
    ],
    "id": "SP:e65376af493f8d444b616d1f5259188babef34f9",
    "references": [
        {
            "authors": [
                "Jonas Mo\u010dkus"
            ],
            "title": "On Bayesian methods for seeking the extremum",
            "venue": "In Optimization Techniques,",
            "year": 1975
        },
        {
            "authors": [
                "Virginia Aglietti",
                "Xiaoyu Lu",
                "Andrei Paleyes",
                "Javier Gonz\u00e1lez"
            ],
            "title": "Causal Bayesian Optimization",
            "venue": "In International Conference on Artificial Intelligence and Statistics (AISTATS),",
            "year": 2020
        },
        {
            "authors": [
                "Scott Sussex",
                "Anastasia Makarova",
                "Andreas Krause"
            ],
            "title": "Model-based causal bayesian optimization",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2022
        },
        {
            "authors": [
                "Frederick Eberhardt",
                "Richard Scheines"
            ],
            "title": "Interventions and causal inference",
            "venue": "Philosophy of science,",
            "year": 2007
        },
        {
            "authors": [
                "Jiaqi Zhang",
                "Chandler Squires",
                "Caroline Uhler"
            ],
            "title": "Matching a desired causal state via shift interventions",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Niranjan Srinivas",
                "Andreas Krause",
                "Sham Kakade",
                "Matthias Seeger"
            ],
            "title": "Gaussian process optimization in the bandit setting: No regret and experimental design",
            "venue": "In Proceedings of International Conference on Machine Learning (ICML),",
            "year": 2010
        },
        {
            "authors": [
                "Raul Astudillo",
                "Peter I. Frazier"
            ],
            "title": "Bayesian Optimization of Function Networks",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2021
        },
        {
            "authors": [
                "Finnian Lattimore",
                "Tor Lattimore",
                "Mark D Reid"
            ],
            "title": "Causal bandits: Learning good interventions via causal inference",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2016
        },
        {
            "authors": [
                "Burak Varici",
                "Karthikeyan Shanmugam",
                "Prasanna Sattigeri",
                "Ali Tajer"
            ],
            "title": "Causal bandits for linear structural equation models",
            "venue": "arXiv preprint arXiv:2208.12764,",
            "year": 2022
        },
        {
            "authors": [
                "Ilija Bogunovic",
                "Jonathan Scarlett",
                "Stefanie Jegelka",
                "Volkan Cevher"
            ],
            "title": "Adversarially robust optimization with gaussian processes",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2018
        },
        {
            "authors": [
                "Johannes Kirschner",
                "Ilija Bogunovic",
                "Stefanie Jegelka",
                "Andreas Krause"
            ],
            "title": "Distributionally robust Bayesian optimization",
            "venue": "In International Conference on Artificial Intelligence and Statistics (AISTATS),",
            "year": 2020
        },
        {
            "authors": [
                "Thanh Nguyen",
                "Sunil Gupta",
                "Huong Ha",
                "Santu Rana",
                "Svetha Venkatesh"
            ],
            "title": "Distributionally robust Bayesian quadrature optimization",
            "venue": "In International Conference on Artificial Intelligence and Statistics (AISTATS),",
            "year": 2020
        },
        {
            "authors": [
                "Pier Giuseppe Sessa",
                "Ilija Bogunovic",
                "Maryam Kamgarpour",
                "Andreas Krause"
            ],
            "title": "Mixed strategies for robust optimization of unknown objectives",
            "venue": "In International Conference on Artificial Intelligence and Statistics (AISTATS),",
            "year": 2020
        },
        {
            "authors": [
                "Anastasia Makarova",
                "Ilnura Usmanova",
                "Ilija Bogunovic",
                "Andreas Krause"
            ],
            "title": "Risk-averse heteroscedastic bayesian optimization",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2021
        },
        {
            "authors": [
                "Shogo Iwazaki",
                "Yu Inatsu",
                "Ichiro Takeuchi"
            ],
            "title": "Mean-variance analysis in Bayesian optimization under uncertainty",
            "venue": "In International Conference on Artificial Intelligence and Statistics (AISTATS),",
            "year": 2021
        },
        {
            "authors": [
                "Virginia Aglietti",
                "Neil Dhir",
                "Javier Gonz\u00e1lez",
                "Theodoros Damoulas"
            ],
            "title": "Dynamic causal bayesian optimization",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2021
        },
        {
            "authors": [
                "Pier Giuseppe Sessa",
                "Ilija Bogunovic",
                "Maryam Kamgarpour",
                "Andreas Krause"
            ],
            "title": "No-regret learning in unknown games with correlated payoffs",
            "venue": "Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2019
        },
        {
            "authors": [
                "Pier Giuseppe Sessa",
                "Ilija Bogunovic",
                "Andreas Krause",
                "Maryam Kamgarpour"
            ],
            "title": "Online submodular resource allocation with applications to rebalancing shared mobility systems",
            "venue": "In Proceedings of International Conference on Machine Learning (ICML),",
            "year": 2021
        },
        {
            "authors": [
                "Nick Littlestone",
                "Manfred K Warmuth"
            ],
            "title": "The weighted majority algorithm",
            "venue": "Information and computation,",
            "year": 1994
        },
        {
            "authors": [
                "Yoav Freund",
                "Robert E Schapire"
            ],
            "title": "A decision-theoretic generalization of on-line learning and an application to boosting",
            "venue": "Journal of computer and system sciences,",
            "year": 1997
        },
        {
            "authors": [
                "Nicolo Cesa-Bianchi",
                "G\u00e1bor Lugosi"
            ],
            "title": "Prediction, learning, and games",
            "venue": "Cambridge university press,",
            "year": 2006
        },
        {
            "authors": [
                "Sebastian Curi",
                "Felix Berkenkamp",
                "Andreas Krause"
            ],
            "title": "Efficient Model-Based Reinforcement Learning through Optimistic Policy Search and Planning",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2020
        },
        {
            "authors": [
                "Jason R Marden",
                "Adam Wierman"
            ],
            "title": "Distributed welfare games",
            "venue": "Operations Research,",
            "year": 2013
        },
        {
            "authors": [
                "D Paccagnan",
                "JR Marden"
            ],
            "title": "Utility design for distributed resource allocation-par ii: Applications to submodular, covering, and supermodular problems",
            "venue": "IEEE Transactions on Automatic Control,",
            "year": 2022
        },
        {
            "authors": [
                "Pier Giuseppe Sessa",
                "Ilija Bogunovic",
                "Maryam Kamgarpour",
                "Andreas Krause"
            ],
            "title": "Learning to play sequential games versus unknown opponents",
            "venue": "Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2020
        },
        {
            "authors": [
                "Pier Giuseppe Sessa",
                "Ilija Bogunovic",
                "Andreas Krause",
                "Maryam Kamgarpour"
            ],
            "title": "Contextual games: Multi-agent learning with side information",
            "venue": "Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2020
        },
        {
            "authors": [
                "An Bian",
                "Kfir Levy",
                "Andreas Krause",
                "Joachim M Buhmann"
            ],
            "title": "Continuous dr-submodular maximization: Structure and algorithms",
            "venue": "Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2017
        },
        {
            "authors": [
                "Francis Bach"
            ],
            "title": "Submodular functions: from discrete to continuous domains",
            "venue": "Mathematical Programming,",
            "year": 2019
        },
        {
            "authors": [
                "Astudillo",
                "Frazier"
            ],
            "title": "We modify each environment in two ways (Perturb and Penny) in order to incorporate an adversary, resulting in 8 environments total. For all environments we tried to make the fewest modifications possible when incorporating the adversary in a way that made the game nontrivial while maintaining the spirit of the original function network",
            "venue": "CBO algorithms Sussex et al",
            "year": 2021
        },
        {
            "authors": [
                "Sessa"
            ],
            "title": "2021) and thus we largely refer to this regarding simulator details, unless otherwise specified. The simulator uses real demand data amalgamated from several SMS sources in Louisville Kentucky Lou (2021). We treat the system as a single SMS where all transport units are identical. A single trip taken in the Louisville data at a specific time is treated as a single unit of demand in the simulator. The demand is fulfilled if the location of the demand",
            "year": 2021
        },
        {
            "authors": [
                "Sessa"
            ],
            "title": "xi give the number of bikes at day start in depot number i within region r. xr is the total fulfilled trips that started in region r. Reward Y is then the total trips in a day. All observations are normalized to ensure they are fixed",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "How can a scientist efficiently optimize an unknown function that is expensive to evaluate? This problem arises in automated machine learning, drug discovery and agriculture. Bayesian optimization (BO) encompasses an array of algorithms for sequentially optimizing unknown functions (Moc\u030ckus, 1975). Classical BO algorithms treat the unknown function mostly as a black box and make minimal structural assumptions. By incorporating more domain knowledge about the unknown function into the algorithm, one can hope to optimize the function using fewer evaluations.\nA recent line of work on causal Bayesian optimization (CBO) (Aglietti et al., 2020) attempts to integrate use of a structural causal model (SCM) into BO methods. It is assumed that actions are interventions on some set of observed variables, which are causally related to each other and a reward variable through a known causal graph (Fig. 1b), but unknown mechanisms. Many important BO problems might take such a shape. For example, managing supply in a Shared Mobility System (SMS) involves intervening on the distribution of bikes and scooters across the city. Importantly, Sussex et al. (2022) show that a BO approach leveraging the additional structure of CBO can achieve exponentially lower regret in terms of the action space size.\nMost CBO methods to date assume that the system is completely stationary across interactions and that only one agent interacts with the system. However, often it would be desirable to incorporate the influence of external events. For example, in a SMS the day\u2019s demand is highly non-stationary, and can only be fully observed at the day\u2019s end. We would like an algorithm that adapts to the variability in these external events.\nIn this work, we incorporate external events into CBO by introducing a novel adversarial CBO (ACBO) setup, illustrated in Fig. 1c. Crucially, in ACBO the downstream reward is explicitly influenced by potentially adversarial interventions on certain nodes in the causal graph (identified using dashed nodes in Fig. 1c) that can only be observed a-posteriori. For this general setting, we propose a novel algorithm \u2013 CBO with multiplicative weights (CBO-MW) \u2013 and prove a regret\nBayesian Optimization (BO)\na Y\n(a)\nCausal Bayesian Optimization (CBO)\naY\na0 X0\nX1\nY\n(b)\nAdversarial Causal Bayesian Optimization (ACBO)\naY\na0 X0\nX1\nY\na\u2032Ya \u2032 1\na\u20320\n(c)\nFigure 1: Visualizations of Bayesian Optimization (BO), Causal BO (CBO), and the Adversarial CBO (ACBO) introduced in this work. Agents must select actions a that maximize rewards Y . (a) Standard BO assumes the simplest DAG from actions to reward, regardless of the problem structure. (b) CBO incorporates side observations, e.g., X0, X1 and causal domain knowledge. Each observation could be modeled with a separate model. (c) In ACBO (this work), we additionally model the impact of external events (weather, perturbations, other players\u2019 actions, etc.) that cannot be controlled, but can only be observed a-posteriori. These are depicted as dashed blue nodes and could directly affect the reward node, like a\u2032Y , or indirectly affect it by perturbing upstream variables, like a \u2032 0, a \u2032 1.\nguarantee using a stronger (but natural) notion of regret than the one used in existing CBO works. For settings where the number of intervention targets is large, we propose a distributed version of CBO-MW which is computationally efficient and can achieve approximate regret guarantees under some additional submodularity assumptions on the reward. Finally, we find empirical evidence that CBO-MW outperforms relevant prior work in adversarial versions of previously studied CBO benchmarks and in learning to re-balance units in an SMS simulation based upon real data."
        },
        {
            "heading": "2 BACKGROUND AND PROBLEM STATEMENT",
            "text": "We consider the problem of an agent interacting with an SCM for T rounds in order to maximize the value of a reward variable. We start by introducing SCMs, the soft intervention model used in this work, and then define the adversarial sequential decision-making problem we study. In the following, we denote with [m] the set of integers {0, . . . ,m}.\nStructural Causal Models Our SCM is described by a tuple \u27e8G, Y,X,F ,\u2126\u27e9 of the following elements: G is a known DAG; Y is the reward variable; X = {Xi}m\u22121i=0 is a set of observed scalar random variables; the set F = {fi}mi=0 defines the unknown functional relations between these variables; and \u2126 = {\u2126i}mi=0 is a set of independent noise variables with zero-mean and known distribution. We use the notation Y and Xm interchangeably and assume the elements of X are topologically ordered, i.e., X0 is a root and Xm is a leaf. We denote with pai \u2282 {0, . . . ,m} the indices of the parents of the ith node, and use the notation Zi = {Xj}j\u2208pai for the parents this node. We sometimes use Xi to refer to both the ith node and the ith random variable.\nEach Xi is generated according to the function fi : Zi \u2192 Xi, taking the parent nodes Zi of Xi as input: xi = fi(zi) + \u03c9i, where lowercase denotes a realization of the corresponding random variable. The reward is a scalar xm \u2208 [0, 1] while observation Xi is defined over a compact set xi \u2208 Xi \u2282 R, and its parents are defined over Zi = \u220f j\u2208pai Xj for i \u2208 [m\u2212 1]. 1\nInterventions In our setup, an agent and an adversary both perform interventions on the SCM 2. We consider a soft intervention model (Eberhardt and Scheines, 2007) where interventions are parameterized by controllable action variables. A simple example of a soft intervention is a shift intervention, where actions affect their outputs additively (Zhang et al., 2021).\n1Here we consider scalar observations for ease of presentation, but we note that the methodology and analysis can be easily extended to vector observations as in Sussex et al. (2022)\n2Our framework allows for there to be potentially multiple adversaries, but since we consider everything from a single player\u2019s perspective, it is sufficient to combine all the other agents into a single adversary.\nFirst, consider the agent and its action variables a = {ai}mi=0. Each action ai is a real number chosen from some finite set. That is, the space Ai of action ai is Ai \u2282 R[0,1] where |Ai| = Ki for some Ki \u2208 N. Let A be the space of all actions a = {ai}mi=0. We represent the actions as additional nodes in G (see Fig. 1): ai is a parent of only Xi, and hence an additional input to fi. Since fi is unknown, the agent does not know apriori the functional effect of ai on Xi. Not intervening on a node Xi can be considered equivalent to selecting ai = 0. For nodes that cannot be intervened on by our agent, we set Ki = 1 and do not include the action in diagrams, meaning that without loss of generality we consider the number of action variables to be equal to the number of nodes m. 3\nFor the adversary we consider the same intervention model but denote their actions by a\u2032 with each a\u2032i defined over A\u2032i \u2282 R[0,1] where |A\u2032i| = K \u2032i and K \u2032i is not necessarily equal to Ki.\nAccording to the causal graph, actions a,a\u2032 induce a realization of the graph nodes:\nxi = fi(zi, ai, a \u2032 i) + \u03c9i, \u2200i \u2208 [m]. (1)\nIf an index i corresponds to a root node, the parent vector zi denotes an empty vector, and the output of fi only depends on the actions.\nProblem statement Over multiple rounds, the agent and adversary intervene simultaneously on the SCM, with known DAG G and fixed but unknown functions F = {fi}mi=1 with fi : Zi\u00d7Ai\u00d7A\u2032i \u2192 Xi. At round t the agent selects actions a:,t = {ai,t}mi=0 and obtains observations x:,t = {xi,t}mi=0, where we add an additional subscript to denote the round of interaction. When obtaining observations, the agent also observes what actions the adversary chose a\u2032:,t = {a\u2032i,t}mi=0. We assume the adversary does not have the power to know a:,t when selecting a\u2032:,t, but only has access to the history of interactions until round t and knowledge of the agent\u2019s algorithm. The agent obtains a reward given by\nyt = fm(zm,t, am,t, a \u2032 m,t) + \u03c9m,t, (2)\nwhich implicitly depends on the whole action vector a:,t and adversary actions a\u2032:,t. The agent\u2019s goal is to select a sequence of actions that maximizes their cumulative expected reward\u2211T t=1 r(a:,t,a \u2032 :,t) where r(a:,t,a \u2032 :,t) = E [ yt |a:,t,a\u2032:,t ] and expectations are taken over \u03c9 unless otherwise stated. The challenge for the agent lies in not knowing a-priori neither the causal model (i.e., the functions F = {fi}mi=1), nor the sequence of adversarial actions {a\u2032:,t}\u00b7\u00b7\u00b7t=1.\nPerformance metric After T timesteps, we can measure the performance of the agent via the notion of regret:\nR(T ) = max a\u2208A T\u2211 t=1 r(a,a\u2032:,t)\u2212 T\u2211 t=1 r(a:,t,a \u2032 :,t), (3)\ni.e., the difference between the best cumulative expected reward obtainable by playing a single fixed action if the adversary\u2019s action sequence and F were known in hindsight, and the agent\u2019s cumulative expected reward. We seek to design algorithms for the agent that are no-regret, meaning that R(T )/T \u2192 0 as T \u2192 \u221e, for any sequence a\u2032:,t. We emphasize that while we use the term \u2018adversary\u2019, our regret notion encompasses all strategies that the adversary could use to select actions. This might include cooperative agents or mechanism non-stationarities.\nFor simplicity, we consider only adversary actions observed after the agent chooses actions. Our methods can be extended to also consider adversary actions observed before the agent chooses actions, i.e., a context. This results in learning a policy that returns actions depending on the context, rather than just learning a fixed action. This extension is straightforward and we briefly discuss it in Appendix A.2.\nRegularity assumptions We consider standard smoothness assumptions for the unknown functions fi : S \u2192 Xi defined over a compact domain S (Srinivas et al., 2010). In particular, for each node i \u2208 [m], we assume that fi(\u00b7) belongs to a reproducing kernel Hilbert space (RKHS)Hki , a space of smooth functions defined on S = Zi\u00d7Ai\u00d7A\u2032i. This means that fi \u2208 Hki is induced by a kernel function ki : S \u00d7 S \u2192 R. We also assume that ki(s, s\u2032) \u2264 1 for every s, s\u2032 \u2208 S4. Moreover, the RKHS\n3There may be constraints on the actions our agent can take. We refer the reader to Sussex et al. (2022) for how our setup can be extended to handle constraints.\n4This is known as the bounded variance property, and it holds for many common kernels.\nnorm of fi(\u00b7) is assumed to be bounded \u2225fi\u2225ki \u2264 Bi for some fixed constant Bi > 0. Finally, to ensure the compactness of the domainsZi, we assume that the noise \u03c9 is bounded, i.e., \u03c9i \u2208 [\u22121, 1]d."
        },
        {
            "heading": "3 RELATED WORK",
            "text": "Causal Bayesian optimization Several recent works study how to perform Bayesian optimization on systems with an underlying causal graph. Aglietti et al. (2020) proposes the first CBO setting with hard interventions and an algorithm that uses the do-calculus to generalise from observational to interventional data, even in settings with unobserved confounding. Astudillo and Frazier (2021) consider a noiseless setting with soft interventions (known as a function network) where a full system model is learnt, and an expected improvement objective used to select interventions. Sussex et al. (2022) propose MCBO, an algorithm with theoretical guarantees that can be used with both hard and soft interventions. MCBO propagates epistemic uncertainty about causal mechanisms through the graph, balancing exploration and exploitation using the optimism principle (Srinivas et al., 2010). Causal bandits, which similarly incorporate causal graph knowledge into the bandit setting, usually consider discrete actions with categorical observations (Lattimore et al., 2016) or linear mechanisms with continuous observations (Varici et al., 2022). All of these methods consider only stationary environments and do not account for possible adversaries.\nBayesian optimization in non-i.i.d. settings Multiple works study how to develop robust strategies against shifts in uncontrollable covariates. They study notions of worst-case adversarial robustness (Bogunovic et al., 2018), distributional robustness (Kirschner et al., 2020; Nguyen et al., 2020), robust mixed strategies (Sessa et al., 2020a) and risk-aversion to uncontrollable environmental parameters (Makarova et al., 2021; Iwazaki et al., 2021). Nonstationarity is studied in the canonical BO setup in Kirschner et al. (2020); Nguyen et al. (2020) and in the CBO setup in Aglietti et al. (2021). However, these methods do not accommodate adversaries in the system, e.g., multiple agents that we cannot control. A foundation for our work is the GP-MW algorithm (Sessa et al., 2019) which studies learning in unknown multi-agents games and is a special case of our setting. We compare further with GP-MW in Section 5. Another special case of CBO-MW with a specific graph is STACKELUCB (Sessa et al., 2021), designed for playing unknown Stackelberg games with multiple types of opponent (see Appendix A.1.1)."
        },
        {
            "heading": "4 METHOD",
            "text": "In this section, we introduce the methodology for the proposed CBO-MW algorithm."
        },
        {
            "heading": "4.1 CALIBRATED MODELS",
            "text": "An important component of our approach is the use of calibrated uncertainty models to learn functions F , as done in Sussex et al. (2022). At the end of each round t, we use the dataset Dt = {z:,1:t,a:,1:t,a\u2032:,1:t,x:,1:t} of past interventions to fit a separate model for every node in the system. CBO-MW can be applied with any set of models that have calibrated uncertainty. That is, for every node i at time t the model has a mean function \u00b5i,t and variance function \u03c3i,t (learnt from Dt) that accurately capture any epistemic uncertainty in the true model. Assumption 1 (Calibrated model). All statistical models are calibrated w.r.t. F , so that \u2200i, t there exists a sequence \u03b2t \u2208 R>0 such that, with probability at least (1\u2212\u03b4), for all zi, ai, a\u2032i \u2208 Zi\u00d7Ai\u00d7A\u2032i we have |fi(zi, ai, a\u2032i)\u2212 \u00b5i,t\u22121(zi, ai, a\u2032i)| \u2264 \u03b2t\u03c3i,t\u22121(zi, ai, a\u2032i), element-wise.\nIf the models are calibrated, we can form confidence bounds that contain the true system model with high probability. This is done by combining separate confidence bounds for the mechanism at each node. At time t, the known setMt of statistically plausible functions F\u0303 = {f\u0303i}mi=0 is defined as:\nMt = { F\u0303 = {f\u0303i}mi=0, s.t. \u2200i : f\u0303i \u2208 Hki , \u2225f\u0303i\u2225ki \u2264 Bi, and\u2223\u2223\u2223f\u0303i(zi, ai, a\u2032i)\u2212 \u00b5i,t\u22121(zi, ai, a\u2032i)\u2223\u2223\u2223 \u2264 \u03b2t\u03c3i,t\u22121(zi, ai, a\u2032i), \u2200zi \u2208 Zi, ai \u2208 Ai, a\u2032i \u2208 A\u2032i}. (4)\nGP models Gaussian Process (GP) models can be used to model epistemic uncertainty. These are the model class we study in our analysis (Section 5), where we also give explicit forms for \u03b2t that satisfy Assumption 1. For all i \u2208 [m], let \u00b5i,0 and \u03c32i,0 denote the prior mean and variance functions for each fi, respectively. Since \u03c9i is bounded, it is also subgaussian and we denote the variance by b2i . The corresponding posterior GP mean and variance, denoted by \u00b5i, t and \u03c3 2 i,t respectively, are computed based on the previous evaluations Dt:\n\u00b5i,t(si,1:t) = kt(si,1:t) \u22a4(Kt + b 2 i It) \u22121xi,1:t (5) \u03c32i,t(si,1:t) = k(si,1:t; si,1:t)\u2212 kt(si,1:t)\u22a4(Kt + b2i It)\u22121kt(si,1:t) , (6)\nwhere si,1:t = (zi,1:t,ai,1:t,a\u2032i,1:t), kt(si,1:t) = [k(si,j , si,1:t)] t j=1, and Kt = [k(si,j , si,j\u2032)]j,j\u2032 is the kernel matrix.\n4.2 THE CBO-MW ALGORITHM Algorithm 1 Causal Bayesian Optimization Multiplicative Weights (CBO-MW)\nRequire: parameters \u03c4, {\u03b2t}t\u22651,G,\u2126, kernel functions ki and prior means \u00b5i,0 = 0 \u2200i \u2208 [m]\n1: Initialize w1 = 1|A| (1, . . . , 1) \u2208 R |A| 2: for t = 1, 2, . . . do 3: Sample at \u223c wt 4: Observe samples {zi,t, xi,t, a\u2032i,t}mi=0 5: Update posteriors {\u00b5i,t(\u00b7), \u03c32i,t(\u00b7)}mi=0 6: for a \u2208 A do 7: Compute UCBGt (a,a \u2032 t) using Algorithm 2 8: y\u0302ta = min(1,UCB G t (a,a \u2032 t))\n9: wt+1a \u221d wta exp(\u03c4 \u00b7 y\u0302ta) 10: end for 11: end for\nWe can now present the proposed CBOMW algorithm. Our approach is based upon the classic multiplicative weights method (Littlestone and Warmuth, 1994; Freund and Schapire, 1997), widely used in adversarial online learning. Indeed, ACBO can be seen as a specific structured online learning problem. At time t, CBO-MW maintains a weight wta for every possible intervention a \u2208 A such that \u2211 a w t a = 1 and uses these to sample the chosen intervention, i.e., at \u223c wta. Contrary to standard CBO (where algorithms can choose actions deterministically), in adversarial environments such as ACBO randomization is necessary to achieve no-regret, see, e.g., Cesa-Bianchi and Lugosi (2006).\nThen, CBO-MW updates the weights at the end of each round based upon what action the adversary took a\u2032t and the observations xt. If the mechanism between actions, adversary actions, and rewards were to be completely known (i.e., the function r(\u00b7) in our setup), a standard MW strategy suggests updating the weight for every a according to\nwt+1a \u221d wta exp (\u03c4 \u00b7 r(a,a\u2032t)) ,\nwhere \u03c4 is a tunable learning rate. In particular, r(a,a\u2032t) is the counterfactual of what would have happened, in expectation over noise, had a\u2032t remained fixed but the algorithm selected a instead of at.\nHowever, in ACBO the system is unknown and thus such counterfactual information is not readily available. On the other hand, as outlined in the previous section, we can build and update calibrated models around the unknown mechanisms and then estimate counterfactual quantities from them. Specifically, CBO-MW utilizes the confidence setMt to compute an optimistic estimate of r(a,a\u2032t):\nUCBGt (a,a \u2032) = max\nF\u0303\u2208Mt E\u03c9\n[ y | F\u0303 ,a,a\u2032t ] . (7)\nGiven action a, opponent action a\u2032t and confidence setMt, UCBGt (a,a\u2032) represents the highest expected return among all system models in this confidence set. CBO-MW uses such estimates to update the weights in place of the true but unknown counterfactuals r(a,a\u2032t). Computing UCB G t (a,a\n\u2032) is challenging since our confidence setMt consists of a set of m different models and one must propagate epistemic uncertainty through all models in the system, from actions to rewards. Because mechanisms can be non-monotonic and nonlinear, one cannot simply independently maximize the output of every mechanism. We thus defer this task to an algorithmic subroutine (denoted causal UCB oracle) which we describe in the next subsection. CBO-MW is summarized in Algorithm 1.\nWe note that CBO-MW strictly generalizes the GP-MW algorithm of Sessa et al. (2019), which was first to propose combining MW with optimistic counterfactual reward estimates. However, they consider a trivial causal graph with only a target node, thus a single GP model. For this simpler\nmodel one can compute UCBGt in closed-form but must ignore any causal structure in the reward model. In Section 5 and in our experiments we show CBO-MW can significantly outperform GP-MW both theoretically and experimentally."
        },
        {
            "heading": "4.3 CAUSAL UCB ORACLE",
            "text": "The problem in Eq. (7) is not amenable to commonly used optimization techniques, due to the maximization over a set of functions with bounded RKHS norm. Therefore, similar to Sussex et al. (2022) we make use of the reparameterization trick to write any f\u0303i \u2208 F\u0303 \u2208 Mt using a function \u03b7i : Zi \u00d7Ai \u00d7A\u2032i \u2192 [\u22121, 1] as\nf\u0303i,t(z\u0303i, a\u0303i, a\u0303 \u2032 i) = \u00b5i,t\u22121(z\u0303i, a\u0303i, a\u0303 \u2032 i) + \u03b2t\u03c3i,t\u22121(z\u0303i, a\u0303i, a\u0303 \u2032 i)\u03b7i(z\u0303i, a\u0303i, a\u0303 \u2032 i), (8)\nwhere x\u0303i = f\u0303i(z\u0303i, a\u0303i, a\u0303\u2032i) + \u03c9\u0303i denotes observations from simulating actions in one of the plausible models, and not necessarily the true model. The validity of this reparameterization comes directly from the definition ofMt in Eq. (4) and the range of \u03b7i.\nThe reparametrization allows for rewriting UCBGt in terms of \u03b7 : Z \u00d7A\u00d7A\u2032 \u2192 [\u22121, 1]|X |:\nUCBGt (a,a \u2032) = max\n\u03b7(\u00b7) E\u03c9\n[ y | F\u0303 ,a,a\u2032t ] , s.t. F\u0303 = {f\u0303i,t} in Eq. (8). (9)\nAlgorithm 2 Causal UCB Oracle Require: neural networks \u03b7, actions a,a\u2032, model\nposteriors \u00b5,\u03c3, parameter \u03b2t, repeats Nrep. 1: Initialize SOLUTIONS = \u2205 2: for j = 1, . . . , Nrep do 3: Randomly initialize weights of each \u03b7i \u2208 \u03b7 4: UCBGt,j = max\u03b7(\u00b7) E[y | F\u0303 ,a,a\u2032] computed\nvia stochastic gradient ascent on \u03b7 5: SOLUTIONS = SOLUTIONS \u222a {UCBGt,j}. 6: end for 7: return max(SOLUTIONS) In practice, we can parameterize \u03b7, for example with neural networks, and maximize this objective using stochastic gradient ascent, as described in Algorithm 2. The use of the reparameterization trick simplifies the optimization problem because we go from optimizing over functions with a tricky constraint (F\u0303 \u2208 Mt) to a much simpler constraint (\u03b7 just needing output in [0, 1]). Since the optimization problem is still non-convex, we deploy multiple random re-initializations of the \u03b7 parameters."
        },
        {
            "heading": "5 ANALYSIS",
            "text": "Here we analyse the theoretical performance of CBO-MW and provide a bound on its regret as a function of the underlying causal graph. For our analysis, we make some additional technical assumptions. First, we assume all fi \u2208 F are Lf -Lipschitz continuous. This follows directly from the regularity assumptions of Section 2. Second, we assume that \u2200i, t, the functions \u00b5i, \u03c3i,t are L\u00b5, L\u03c3 Lipschitz continuous. This holds if the RKHS of each fi has a Lipschitz continuous kernel (see Curi et al. (2020), Appendix G). Finally, we assume the causal UCB oracle can always compute UCBGt (a, a \u2032) (Eq. (9)) exactly.\nIn addition, to show how the regret guarantee depends on the specific GP hyperparameters used, we use a notion of model complexity for each node i:\n\u03b3i,T := max Ai\u2282 {Zi\u00d7Ai\u00d7A\u2032i} T I(xi,1:T , fi) (10)\nwhere I is the mutual information and the observations xi,1:T implicitly depend on the GP inputs Ai. This is analogous to the maximum information gain used in the analysis of standard BO algorithms Srinivas et al. (2010). We also define\n\u03b3T = max i \u03b3i,T (11)\nas the worst-case maximum information gain across all nodes in the system.\nFinally, we define two properties of the causal graph structure that the regret guarantee will depend on. In the DAG G over nodes {Xi}mi=0, let \u2206 denote the maximum number of parents of any variable in G: \u2206 = maxi |pa(i)|. Then let N denote the maximum distance from a root node to Xm: N = maxi dist(Xi, Xm) where dist(\u00b7, \u00b7) is the number of edges in the longest path from a node Xi to Xm. We can then prove the following theorem on the regret of CBO-MW.\nTheorem 1. Fix \u03b4 \u2208 (0, 1), if actions are played according to CBO-MW with \u03b2t = O ( B + \u221a \u03b3t\u22121 + log(m/\u03b4) ) and \u03c4 = \u221a (8 log |A|)/T , then with probability at least 1\u2212 \u03b4,\nR(T ) = O (\u221a T log |A|+ \u221a T log(2/\u03b4) + ( B + \u221a \u03b3T + log(m/\u03b4) )N+1 \u2206NLN\u03c3 L N f m \u221a T\u03b3T ) ,\nwhere B = maxi Bi, \u03b3T = max \u03b3i,t. That is, \u03b3T is the worst-case maximium information gain of any of the GP models.\nTheorem 1, whose proof is deferred to the appendix, shows that CBO-MW is no-regret for a variety of common kernel functions, for example linear and squared exponential kernels. This is because even when accounting for the dependence of \u03b3T in T , the bound is still sublinear in T . We discuss the dependence of \u03b3T on T for specific kernels in Appendix A.1.2.\nComparison with GP-MW We can use Theorem 1 to demonstrate that the use of graph structure in CBO-MW results in a potentially exponential improvement in the rate of regret compared to GP-MW (Sessa et al., 2019) with respect to the number of action variables m. Consider the graph in Fig. 4b (see Appendix) for illustration. When all Xi in CBO-MW are modeled with squared exponential kernels, we have \u03b3T = O((\u2206 + 2)(log T )\u2206+3). This results in a cumulative regret that is exponential in \u2206 and N . Instead, GP-MW uses a single high-dimensional GP (Fig. 4a), implying \u03b3T = O((log T )m) for a squared exponential kernel. Note that m \u2265 \u2206+N and thus, for several common graphs, the exponential scaling in N and \u2206 could be significantly more favorable than the exponential scaling in m. Specifically for the binary tree of Fig. 4b, where N = log(m) and \u2206 = 2, the cumulative regret of CBO-MW will have only polynomial dependence on m.\nFurthermore, in addition to favorable scaling of the regret in m, the model class considered by CBOMW is considerably larger than that of GP-MW, because CBO-MW can model systems where reward depends on actions according to a composition of GPs based on G, rather than a single GP."
        },
        {
            "heading": "6 COMPUTATIONAL CONSIDERATIONS IN LARGER ACTION SPACES",
            "text": "The computational complexity of CBO-MW is dominated by the computation of the counterfactual expected reward estimate for each possible intervention a \u2208 A (Line 7 in Algorithm 1). In many situations, even with a large number of action variables m, |A| may still be small and thus CBO-MW feasible to implement. A notable example is when there exist constraints on the possible interventions, such as only being able to intervene on at most a few nodes simultaneously. In the worst case, though, |A| grows exponentially with m and thus CBO-MW may not be computationally feasible. In this section we will show how prior knowledge of the problem structure can be used to modify CBO-MW to be computationally efficient even in settings with huge action spaces. We note that the computational efficiency of CBO-MW is not affected by the number of possible adversary interventions |A\u2032| because these are only observed a-posteriori (in fact, A\u2032 need not be finite). The general idea consists of decomposing CBO-MW into a decentralized algorithm, which we call D-CBO-MW, that orchestrates multiple sub-agents. First recall that A can be decomposed into m smaller action spaces so thatA = A1\u00d7 ...\u00d7Am. We then consider m independent agents where each agent i performs CBO-MW but only over the action spaceAi. Importantly, the observations of the actions of the other agents are considered part ofA\u2032 for that agent. Moreover, all agents utilize the same calibrated modelMt at each round. We provide full pseudo-code and theory in the appendix. Our approach is inspired by Sessa et al. (2021) who propose a distributed analog of the GP-MW algorithm.\nIn Appendix B we show that under specific assumptions on the reward function, D-CBO-MW provably enjoys an approximate version of the guarantees in Theorem 1. Namely, we study a setting where r is a submodular and monotone increasing function of a for any given set of adversary actions. Submodularity is a diminishing returns property (see formal definition in the appendix) widely exploited in a variety of domains to derive efficient methods with approximation guarantees, see e.g., Marden and Wierman (2013); Sessa et al. (2021); Paccagnan and Marden (2022). Similarly, we exploit submodularity in the overall reward\u2019s structure to parallelize the computation over the possible interventions in our causal graph. In our experiments, we study rebalancing an SMS where CBO-MW is not applicable due to a combinatorial action space but D-CBO-MW is efficient and achieves good performance."
        },
        {
            "heading": "7 EXPERIMENTS",
            "text": "We evaluate CBO-MW and D-CBO-MW on various synthetic problems and a simulator of rebalancing an SMS based on real data. The goal of the experiments is to understand how the use of causal modelling and adaptability to external factors in CBO-MW affects performance compared to other BO methods that are missing one or both of these components. All methods are evaluated over 10 repeats, with mean and standard error reported for different time horizons."
        },
        {
            "heading": "7.1 FUNCTION NETWORKS",
            "text": "Networks We evaluate CBO-MW on 8 diverse environments. As a base, we take 4 examples of function networks from Astudillo and Frazier (2021). Function networks is a noiseless CBO setup with no adversaries. To study an adversarial setup, we modify each environment by adding adversaries\u2019 inputs in 2 ways: Penny and Perturb. In Penny, an adversary can affect a key node with a dynamic similar to the classic matching pennies game. In Perturb, the adversary can perturb some of the agent\u2019s interventions. The exact way in which the adversary\u2019s actions affect the environment is unknown and the actions themselves can only be observed a-posteriori. In each round, the adversary has a 20% chance to play randomly, and an 80% chance to try and minimize the agent\u2019s reward, using full knowledge of the agent\u2019s strategy and the environment. The causal graph and structural equation model for each environment in given in Appendix C.\nBaselines We compare the performance of CBO-MW (Algorithm 1) with GP-MW Sessa et al. (2019) which does not exploit the causal structure. We additionally compare against non-adversarial baselines GP-UCB (Srinivas et al., 2010), and MCBO (Sussex et al., 2022) which uses a causal model but cannot account for adversaries.\nResults We give results from 3 of the 8 environments in Fig. 2. Others can be found in Appendix C. In general across the 8 environments, MCBO and GP-UCB obtain linear regret, while the regret of GP-MW and CBO-MW grows sublinearly, consistent with Theorem 1. CBO-MW has the strongest or joint-strongest performance on 7 out of the 8 environments. The setting where CBO-MW is not strongest involves a dense graph where worst-case GP sparsity is the same as in GP-MW, which is consistent with our theory. In settings such as Alpine where the graph is highly sparse, we observe the greatest improvements from using CBO-MW."
        },
        {
            "heading": "7.2 LEARNING TO REBALANCE SHARED MOBILITY SYSTEMS (SMSS)",
            "text": "We evaluate D-CBO-MW on the task of rebalancing an SMS, a setting introduced in Sessa et al. (2021). The goal is to allocate bikes to city locations (using relocation trucks, overnight) to maximize the number of bike trips in the subsequent day. The action space is combinatorial because each of the 5 trucks can independently reallocate units to a new depot. This makes the approaches studied in Section 7.1 computationally infeasible, so we deploy D-CBO-MW. We expect the reward Yt (total trips given unit allocation) to be monotone submodular because adding an extra unit to a depot should increase total trips but with decreasing marginal benefit, as discussed in Sessa et al. (2021). The goal is to compare D-CBO-MW to a distributed version of GP-MW and understand whether the use of a causal graph can improve sample efficiency.\nSetup and trips simulator A simulator is constructed using historical data from an SMS in Louisville, KY (Lou, 2021). Before each day t, all 40 bikes in the system are redistributed across 116 depots. This is done by 5 trucks, each of which can redistribute 8 bikes at one depot, meaning a truck i\u2019s action is ai \u2208 [116]. The demand for trips corresponds to real trip data from Lou (2021). After each day, we observe weather and demand data from the previous day which are highly non-stationary and thus correspond to adversarial interventions a\u2032 according to our model. Our simulator is identical to the one of Sessa et al. (2021), except we exclude weekends for simplicity. We give more details in Appendix C.\nWe compare three methods on the SMS simulator. First, in RANDOM each truck places its bikes at a depot chosen uniformly at random. Second, D-GP-MW modifies GP-MW using the same ideas as those presented in Section 6. It is a special case of D-CBO-MW but using the graph in Fig. 1(a). That is, a single GP is used to predict Yt given at,a\u2032t. Finally, we evaluate D-CBO-MW which utilizes a more structured causal graph exploiting the reward structure. Based on historical data, we cluster the depots into regions such that trips don\u2019t frequently occur across two different regions (e.g., when such regions are too far away). Then, our graph models the trips starting in each region only using bike allocations to depots in that region. Yt is computed by summing the trips across all regions (see Fig. 8 (b) in the appendix for an illustration). This system model uses many low-dimensional GPs instead of a single high-dimensional GP as used in D-GP-MW.\nResults Fig. 3 (a) displays the allocation strategy of D-CBO-MW. We observe that D-CBO-MW learns the underlying demand patterns and allocates bikes in strategic areas where the demand (green dots) is higher. This plot is zoomed-in for readability. The allocation strategy over all depots is shown in Appendix Fig. 9. Moreover, in Fig. 3 (b) we see that D-CBO-MW is significantly more sample efficient than D-GP-MW. This improvement is largely attributed to early rounds, where D-CBO-MW learns the demand patterns much faster than D-GP-MW due to learning smaller-dimensional GPs."
        },
        {
            "heading": "8 CONCLUSION",
            "text": "We introduce CBO-MW, the first principled approach to causal Bayesian optimization in non-stationary and potentially multi-agent environments. We prove a sublinear regret guarantee for CBO-MW and demonstrate a potentially exponential improvement in regret, in terms of the number of possible intervention targets, compared to state-of-the-art methods. We further propose a distributed version of our approach, D-CBO-MW, that can scale to large action spaces and achieves approximate regret guarantees when rewards are monotone submodular. Empirically, our algorithms outperform existing non-causal and non-adversarial methods on synthetic function network tasks and on an SMS rebalancing simulator based on real data."
        },
        {
            "heading": "REPRODUCIBILITY STATEMENT",
            "text": "Attached to this submission we include code (https://github.com/ssethz/acbo) that implements CBO-MW, D-CBO-MW and all baselines seen in the experiments. It includes code for all function network environments and the SMS setting, which are also detailed in the appendix. All datasets used in the SMS setting are also included at this url. The appendix includes information on our experimental protocol, for example how we selected the hyperparameters for the experiments shown in the paper.\nAll technical assumptions are given in the main paper, and complete proofs of all theoretical results are given in the appendix. Pseudocode for CBO-MW is given in the main paper and pseudocode for D-CBO-MW is given in the appendix."
        },
        {
            "heading": "ACKNOWLEDGEMENTS",
            "text": "We thank Lars Lorch for his feedback on the draft of this paper.\nThis research was supported by the Swiss National Science Foundation under NCCR Automation, grant agreement 51NF40 180545, by the European Research Council (ERC) under the European Union\u2019s Horizon grant 815943, and by ELSA (European Lighthouse on Secure and Safe AI) funded by the European Union under grant agreement No. 101070617."
        },
        {
            "heading": "ADVERSARIAL CAUSAL BAYESIAN OPTIMIZATION",
            "text": ""
        },
        {
            "heading": "A APPENDIX",
            "text": ""
        },
        {
            "heading": "A.1 PROOF OF THEOREM 1",
            "text": "We start by proving that there exists a sequence of \u03b2t that allow Assumption 1 to hold.\nLemma 1 (Node Confidence Lemma ). Let Hki be a RKHS with underlying kernel function ki. Consider an unknown function fi : Zi \u00d7 A\u2032i \u00d7 A\u2032i \u2192 Xi in Hki such that \u2225f\u2225ki \u2264 Bi, and the sampling model xi,t = f(zi,t, ai,t, a\u2032i,t)+\u03c9t where \u03c9t is b-sub-Gaussian (with independence between times). By setting\n\u03b2t = Bi + b \u221a 2 (\u03b3t\u22121 + log(m/\u03b4)) ,\nthe following holds with probability at least 1\u2212 \u03b4:\n|\u00b5t\u22121(zi, ai, ai\u2032)\u2212 fi(zi, ai, a\u2032i)| \u2264 \u03b2t\u03c3i,t\u22121(zi, ai, a\u2032i) ,\n\u2200zi, ai, a\u2032i \u2208 Z \u2032i\u00d7Ai\u00d7A\u2032i, \u2200t \u2265 1, \u2200i \u2208 [m], where \u00b5t\u22121(\u00b7) and \u03c3t\u22121(\u00b7) are given in Eq. (5), Eq. (6) and \u03b3t\u22121 is the maximum information gain defined in (11).\nProof. Lemma 1 follows directly from Sessa et al. (2019, Lemma 1) after applying a union bound so that the statement holds for all m GP models in our system.\nNow we give an upper and lower confidence bound for r at each t. Note that since in our setup all \u03c9i are assumed bounded in [0, 1], we can use b = 14 .\nLemma 2 (Reward Confidence Lemma). Choose \u03b4 \u2208 [0, 1] and then choose the sequence {\u03b2t}Tt=1 according to Lemma 1.\n\u2200a, ai \u2208 A\u00d7A\u2032 , \u2200t we have with probability 1\u2212 \u03b4\nUCBGt (a, a \u2032)\u2212 Ct(\u03b4) \u2264 r(a, a\u2032) \u2264 UCBGt (a, a\u2032t)\nwhere we define Ct(\u03b4) as\nCt(\u03b4) = LY,t\u2206 N \u221a\u221a\u221a\u221amE\u03c9[ m\u2211 i=0 \u2225\u2225\u03c3i,t\u22121(zi,t, ai,t, a\u2032i,t)\u2225\u22252 ] .\nWe define LY,t = 2\u03b2t(1 + Lf + 2\u03b2tL\u03c3)N .\nProof. The RHS follows firstly from Assumption 1 meaning that with probability at least 1 \u2212 \u03b4, f \u2208 {f\u0303t}. The RHS then follows directly from the definition of UCBG in Eq. (7). The LHS follows directly from Sussex et al. (2022, Lemma 4). The addition of the adversary actions does not change the analysis in this lemma because for a given t, a\u2032t is fixed for both the true model f and the model in {f\u0303} that leads to the upper confidence bound.\nNow we can prove the main theorem. Choose \u03b4 \u2208 [0, 1] and then choose the sequence {\u03b2t}Tt=1 according to Lemma 1 so that Assumption 1 holds with probability 1\u2212 \u03b42 . First recall that regret is defined as\nR(T ) = T\u2211 t=1 r(a\u0304,a\u2032t)\u2212 T\u2211 t=1 r(at,a \u2032 t)\nwhere a\u0304 = argmax \u2211T\nt=1 r(a\u0304,a \u2032 t). Now we can say that with probability at least 1\u2212 \u03b42\nR(T ) \u2264 T\u2211\nt=1\nmin{1,UCBGt (a\u0304,a\u2032t)} \u2212 T\u2211\nt=1\n[ UCBGt (at,a \u2032 t)\u2212 Ct (\u03b4/2) ] (12)\n\u2264 T\u2211\nt=1\nmin{1,UCBGt (a\u0304,a\u2032t)} \u2212 T\u2211\nt=1\nUCBGt (at,a \u2032 t) + T\u2211 t=1 Ct (\u03b4/2) (13)\nwhere the first line follows from Lemma 2.\nEvaluating the last term we see\nT\u2211 t=1 Ct (\u03b4/2) 1 = \u221a T \u221a\u221a\u221a\u221a T\u2211 t=1 Ct (\u03b4/2) 2 (14)\n2 \u2264 \u221a T \u221a\u221a\u221a\u221a T\u2211 t=1 L2Y,t\u2206 2NE\u03c9 [ m\u2211 i=0 \u2225\u2225\u03c3i,t\u22121(zi,t, ai,t, a\u2032i,t)\u2225\u22252 ]\n(15)\n3 = O ( LY,T\u2206 N \u221a Tm\u03b3T ) . (16)\n1 comes from AM-QM inequality and 2 comes from plugging in for Ct( \u03b42 ). Finally, 3 comes from LT,t being weakly increasing in t and from using the same analysis as (Sussex et al., 2022, Theorem 1) to upper bound the sum of \u03c3s with \u03b3T .\nMoreover, we can upper bound the first two terms of Eq. (13) using a standard regret bound for the MW update rule (Line 9 in CBO-MW), e.g. from (Cesa-Bianchi and Lugosi, 2006, Corollary 4.2). Indeed, with probability 1\u2212 \u03b42 it holds:\nT\u2211 t=1 min{1,UCBG(a\u0304,a\u2032t)} \u2212 T\u2211 t=1 UCBG(at,a\u2032t) = O (\u221a T log |A|+ \u221a T log(2/\u03b4) ) . (17)\nUsing the union bound on the two different probabilistic events discussed so far (Assumption 1 and Eq. (17)) we can say that with probability at least 1\u2212 \u03b4\nR(T ) = O (\u221a T log |A|+ \u221a T log(2/\u03b4) + LY,T\u2206 N \u221a Tm\u03b3T ) .\nSubstituting in for LY,t and \u03b2t gives the result of Theorem 1."
        },
        {
            "heading": "A.1.1 STACKELUCB AS A SPECIAL CASE",
            "text": "In Fig. 4 and Section 5 we give a comparison between GP-MW and CBO-MW, and see that GP-MW can be seen as a special case of CBO-MW with a one-node causal graph. STACKELUCB Sessa et al. (2020b) is another online learning algorithm that can be seen as a special case of CBO-MW. We show the graph in Fig. 5. In STACKELUCB an agent plays an unknown Stackelberg game against a changing opponent which at time t has representation a\u20320,t. After the agent selects an action a0,t, the opponent sees this action and responds based upon a fixed but unknown response mechanism for that opponent: their response is X0,t = f0(a\u20320,t, a0,t). Then, the response, game outcome Yt, and opponent identity are revealed to the agent. The sequence of opponent identities, i.e. {a\u20320,t}Tt=1, can be selected potentially adversarially based on knowledge of the agent\u2019s strategy.\nA.1.2 DEPENDENCE OF \u03b2T ON T FOR PARTICULAR KERNELS In Theorem 1, there is a dependence of the bound on \u03b3T . If \u03b3T scales with at worst O (\u221a T )\n, then the overall bound will not be in T , resulting in CBO-MW being no-regret. \u03b3T will depend on the\nkernel of the GP used to model each system component. For simplicity, in this section we\u2019ll assume that the same kernel is used for modelling all nodes, though if different kernels are used one just needs to consider the most complex (worst scaling of \u03b3T with T )\nFor \u03b3T corresponding to the linear kernel and squared exponential kernel, we have sublinear regret regardless of N because \u03b3T will be only logarithmic in T . In particular, a linear kernel leads to \u03b3T = O ((\u2206 + 1) log T ) and a squared exponential kernel leads to \u03b3T = O ( (\u2206 + 1)(log T )\u2206+2 ) .\nHowever, for a Mat\u00e9rn kernel, where the best known bound is \u03b3T = O ((T )clog(T )) with hyperparameter 0 < c < 1, the cumulative regret bound will not be sublinear if N and c are sufficiently large. A similar phenomena with the Mat\u00e9rn kernel appears in the guarantees of Curi et al. (2020) which use GP models in model-based reinforcement learning and in Sussex et al. (2022).\nA.2 INCORPORATING CONTEXTS INTO CBO-MW\nOur approach can be easily extended to a setting where a\u2032t, the adversary actions at time t, are observed before the agent chooses at. In this setting the adversary actions could be thought of as contexts. Our method for computing UCBG can be plugged into the algorithm of Sessa et al. (2020c). Their algorithm maintains a separate set of weights for every context unless two contexts are \u2018close\u2019 \u2013 correspond to a similar expected reward for all possible at.\nAlgorithm 3 Multiplicative Weights Update (MWU) Require: set Ai where |Ai| = Ki, learning rate \u03b7\n1: Initialize w1 = 1Ki (1, . . . , 1) \u2208 R Ki 2: function PLAY_ACTION 3: p = w \u00b7 1/ (\u2211Ki j=1 w[j] ) 4: a \u223c p 5: return a // sample action 6: end function 7: 8: function UPDATE(f(\u00b7)) 9: f = min(1, [f(a)]a\u2208Ai) \u2208 RK // rewards vector 10: w = w \u00b7 exp (\u03b7f) // MW update 11: return 12: end function\nAlgorithm 4 Distributed Causal Bayesian Optimization Multiplicative Weights (D-CBO-MW) Require: parameters \u03c4, {\u03b2t}t\u22651,G,\u2126, kernel functions ki and prior means \u00b5i,0 = 0, \u2200i \u2208 [m]\n1: Algoi \u2190 MWU(Ai), i = 1, . . . ,m, Algorithm 3 // initialize agents 2: for t = 1, 2, . . . do 3: Algoi.PLAY_ACTION(), i = 1, . . . ,m // sample actions and perform interventions 4: Observe samples {zi,t, xi,t, a\u2032i,t}mi=0 5: Update posteriors {\u00b5i,t(\u00b7), \u03c32i,t(\u00b7)}mi=0 6: for i \u2208 1, . . . ,m do 7: for ai \u2208 Ai do 8: Compute ucbi,t(ai) = UCBGt (ai,a\u2212i,t,a \u2032 t) for ai \u2208 Ai using Algorithm 2\n9: end for 10: Algoi.UPDATE(ucbi,t(\u00b7)), i = 1, . . . ,m 11: end for 12: end for"
        },
        {
            "heading": "B AN EFFICIENT VARIANT OF CBO-MW FOR LARGE ACTION SPACES",
            "text": ""
        },
        {
            "heading": "B.1 THE DISTRIBUTED CBO-MW (D-CBO-MW) ALGORITHM",
            "text": "For ease of readability in the pseudocode we pull-out some key functionality of the MW algorithm, which we put into a class called MWU described in Algorithm 3. Each agent is an instance of this class, i.e., each agent maintains its own set of weights over its actions, and updates to these instances are coordinated by D-CBO-MW as described in Algorithm 4. Conceptually it can be thought of as m instances of CBO-MW, where the action spaces of other agents are absorbed into A\u2032. While for presenting the algorithm we always decompose A as A1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 Am and have each agent control the intervention on one node, one could in practice choose to decompose the action space in a different way. A simple example would be having agent one control A1 \u00d7A2 and thus designing the intervention for both X1, X2\nWhen computing the UCBG for a single agent i \u2208 [m], we will find the following notation convenient. Let ai,t \u2208 Ai be the action chosen by agent i at time t. a\u2212i,t \u2208 A\u2212i \u2286 A are the actions chosen at time t by all agents except agent i. Note that since the subspaces of the action space each agent controls are non-overlapping, A = Ai \u222aA\u2212i. When agent i chooses actions in Ai, for convenience we will from now on represent it as a vector in A with 0 at all indexes the agent cannot intervene. We do similarly for a\u2212i,t. Then we use the notation UCBGt (ai,t,a\u2212i,t,a \u2032 t) = UCB G t (ai,t + a\u2212j,t,a \u2032 t)."
        },
        {
            "heading": "B.2 APPROXIMATION GUARANTEES FOR MONOTONE SUBMODULAR REWARDS",
            "text": "In this section we show that D-CBO-MW enjoys provable approximation guarantees in case of monotone and DR-submodular rewards. Both such notions are defined below.\nDefinition 1 (Monotone Function). A function f : A \u2286 Rm \u2192 R is monotone if for x \u2264 y, f(x) \u2264 f(y).\nDefinition 2 (DR-Submodularity, (Bian et al., 2017)). A function f : A \u2286 Rm \u2192 R is DRsubmodular if, for all x \u2264 y \u2208 A, \u2200i \u2208 [m],\u2200k \u2265 0 such that (x+ kei) and (y + kei) \u2208 A,\nf(x+ kei)\u2212 f(x) \u2265 f(y + kei)\u2212 f(y).\nDR-submodularity is a generalization of the more common notion of a submodular set function to continuous domains Bach (2019). For our analysis, in particular, we assume that for every a\u2032:,t \u2208 A\u2032, the reward function r(a:,t,a\u2032:,t) is monotone DR-submodular in a:,t.\nWe consider ACBO as a game played among all the distributed agents and the adversary. Our guarantees are then based on the results of Sessa et al. (2021) and depend on the following notions of average and worst-case game curvature. Definition 3 (Game Curvature). Consider a sequence of adversary actions a\u20321, . . . ,a\u2032T . We define the average and worst-case game curvature as\ncavg({a\u2032:,t}Tt=1) = 1\u2212 inf i \u2211T t=1[\u2207r(2amax,a\u2032:,t)]i\u2211T\nt=1[\u2207r(0,a\u2032:,t)]i \u2208 [0, 1],\ncwc({a\u2032:,t}Tt=1) = 1\u2212 inf t,i [\u2207r(2amax,a\u2032:,t)]i [\u2207r(0,a\u2032:,t)]i \u2208 [0, 1],\nwhere amax = amax1 and amax is the largest intervention value a single agent can assign at any index. If 2amax is outside the domain of r, then the definition can be applied to a monotone extension of r over Rm. A definition of game curvature for non-differentiable r is given in the appendix of Sessa et al. (2021).\nCurvature measures how close r(\u00b7,a\u2032:,t) is to being linear in the agents\u2019 actions, with cavg = cwc = 0 coinciding with a completely linear function. The closer r(\u00b7,a\u2032:,t) is to linear, generally the easier the function is to optimize in a distributed way, because a linear function is separable in its inputs. cwc is the worst-case curvature of r(\u00b7,a\u2032:,t) across all rounds t, while cavg is the average curvature across rounds. The curvature of r(\u00b7,a\u2032:,t) will change with t because a\u2032:,t will change across rounds.\nWe will without loss of generality assume that r(0,a\u2032) = 0 for all a\u2032. If this did not hold then r(0,a\u2032) could simply be subtracted from all observations to make it true. Then, we can show the following. Theorem 2. Consider the setting of Theorem 1 but with the additional monotone submodularity and curvature assumptions made in this section. Assume |Ai| = K for all i. Then, if actions are played according to D-CBO-MW with \u03b2t = O ( B + \u221a \u03b3t\u22121 + log (m/\u03b4) ) and \u03b7 = \u221a 8 log(K)/T then\nwith probability at least 1\u2212 \u03b4, T\u2211\nt=1\nr(at,a \u2032 t) \u2265 \u03b1 \u00b7 OPT \u2212m \u00b7 O\n(( B + \u221a \u03b3T + log(m/\u03b4) )N+1 \u2206NLN\u03c3 L N f m \u221a T\u03b3T ) (18)\n\u2212m \u00b7 O (\u221a T logK + \u221a T log(2m/\u03b4) ) ,\nwith \u03b1 = max { 1\u2212 cavg({a\u2032:,t}Tt=1), ( 1 + cwc({a\u2032:,t}Tt=1) )\u22121} and OPT = maxa\u2208A \u2211T t=1 r(a:,t,a \u2032 :,t) is the expected reward achieved by playing the best fixed interventions in hindsight. B and \u03b3T are defined the same as in Theorem 1.\nProof. We will find useful the notation of the regret of an individual agent i \u2208 [m]. We will consider the regret of each agent to be not in terms of the reward r but in terms of the feedback that agent receives: the UCB. We therefore define\nRi(T ) = max a T\u2211 t=1 UCBGt (a,a\u2212i,t,a \u2032 t)\u2212 T\u2211 t=1 UCBGt (ai,t,a\u2212i,t,a \u2032 t).\nIt can be thought of as the regret of agent i compared to the best fixed action in hindsight, given that the actions of all other agents are fixed, and the agent is trying to maximize the sum of UCBs.\nUsing the above definitions, and provided that UCBGt (\u00b7) are a valid upper confidence bound functions on the reward (according to Lemma 2), we can directly apply (Sessa et al., 2021, Theorem 1). This shows that with probability 1\u2212 \u03b42 , the overall reward obtained by D-CBO-MW is lower bounded by\nT\u2211 t=1 r(at,a \u2032 t) \u2265 \u03b1 \u00b7 OPT \u2212m T\u2211 t=1 Ct ( \u03b4 2 ) \u2212 m\u2211 i=1 Ri(T ),\nwhere \u03b1 is defined in Theorem 2 and Ct(\u03b4/2) is as defined in Lemma 2. We note that Sessa et al. (2021) stated their theorem for the case where the UCB was computed using a single GP model (our setting with only a reward node and parent actions), however the proof is identical when any method to compute the UCB is used with an accompanying confidence bound in the form of Lemma 2.\nThen, we can obtain the bound in the theorem statement by further bounding the agents\u2019 regrets Ri(T ). Indeed, because each agent updates its strategy (Line 10 in Algorithm 4) using the MW rule (Algorithm 3), we can bound each Ri(T ) with probability 1\u2212 \u03b42m using Eq. (17). We can also substitute in for Ct(\u03b4/2) using Lemma 2. Via a union bound we get our final result with probability 1\u2212 \u03b4."
        },
        {
            "heading": "C EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "C.1 FUNCTION NETWORKS",
            "text": "We evaluate CBO-MW on 8 environments that have been modified from existing function networks Astudillo and Frazier (2021). These have been previously used to study CBO algorithms Sussex et al. (2022). We modify each environment in two ways (Perturb and Penny) in order to incorporate an adversary, resulting in 8 environments total. For all environments we tried to make the fewest modifications possible when incorporating the adversary in a way that made the game nontrivial while maintaining the spirit of the original function network.\nPerturb environments allow the adversary to modify some of the input actions of the agents. Penny environments incorporate some element of the classic matching pennies game into the environment. If node Xi has an adversary action parent, part of the mechanism for generating Xi will involve multiplying another parent by the adversary action. Because the adversary can select negative actions (see below), this results in a dynamic similar to matching pennies.\nThroughout the setup and theory we assume that there is one action per node for simplicity. For many of the function networks experiments there may be 0 or more than one action per node. Similar theoretical results for this case can also be shown using our analysis.\nIn all environments we map the discrete action space [0,K \u2212 1] to the continuous domain of the original function network. Using a as the continuous action at a single node and a\u0304 as the discrete action input at that node, we always use mapping a = ( a\u0304 K\u22121 \u2212 0.5 ) C1 + C2, where C1, C2 defines some environment specific linear map that usually maps to the input space of the original function network in Astudillo and Frazier (2021). We use the same mapping for adversary actions in Perturb environments\nFor the adversary\u2019s actions in Penny environments, we use a more complicated mapping from a\u0304\u2032 to a\u2032 to ensure that the adversary normally cannot select 0, which would result in a trivial game that the adversary can solve by always playing a\u2032 = 0. If K is even we use\na\u2032 =\n( a\u0304\u2032\nK \u2212 1 (1\u2212 2\u03f5)\u03f5\u2212 0.5\n) C1 + C2,\nwhere \u03f5 = 0.05. If K is odd we use\na\u2032 =\n( a\u0304\u2032 + 0.5\nK \u2212 1 (1\u2212 2\u03f5)\u03f5\u2212 0.5\n) C1 + C2,\nwhere \u03f5 = 0.05.\nDropwave-Penny\na0\na1\nX0\na\u2032Y\nY\nDropwave-Perturb\na0\na1\nX0a \u2032 Y Y\nAlpine-Penny\na0 a1 a\u20322 a3\nX0 X1 X2 Y\nAlpine-Perturb\na0 a1 a2 a3\na\u20320 a \u2032 1 a \u2032 2\nX0 X1 X2 Y"
        },
        {
            "heading": "Rosenbrock-Penny",
            "text": "a0 a1 a2 a3\na\u20321 a \u2032 Y\nX0 X1 Y\nRosenbrock-Perturb\na0 a1 a2 a3\na\u20320 a \u2032 1\nX0 X1 Y\nAckley-Penny\na0 a1 a2 a3\nX0\nX1\nY\na\u2032Y\nAckley-Perturb\na0 a\u20320 a1 a \u2032 1\na2 a3\nX0\nX1\nY\nFigure 6: The DAGs corresponding to each task in the function networks experiments.\nDAGs illustrating the casual structure of each environment are shown in Fig. 6. For some environments, we made the number of nodes smaller than the environment\u2019s counterpart in Astudillo and Frazier (2021) for computational reasons. We give the full SCM for each environment at the end of this subsection.\nTo select hyperparamaters for each method we perform a small hyperparameter sweep to select the best hyperparameters, before fixing the best hyperparameters and running 10 repeats on fresh seeds. On all repeats the agent is initialized with 2m+ 1 samples of uniformly randomly sampled a and a\u2032, where m is the number of action nodes.\nFig. 7 shows the regret plots for environments not already shown in the main paper. As discussed, we find that standard non-adversarial BO algorithms often fail to achieve sublinear regret and that CBOMW is often the most sample efficient compared to GP-MW. Only on Ackley-Penny is GP-MW obtaining a lower regret. This can be understood by our theory. Ackley-Penny is not at all sparse. That is, \u2206 \u2248 m. Our Theorem 1 suggests that most improvement will come in high dimensional settings with sparse graphs. This is made clear by the superior performance of CBO-MW on the Alpine2 environments.\nHere we systematically list the SCM for each environment.\nDropwave-Penny a \u2208 [0, 2]2, a\u2032 \u2208 [\u22121, 1]1. We have\nX0 = \u2225a\u2225 ,\nY = cos(3X0)\n2 + 0.5X20 a\u20320.\nThis is the original Dropwave from Astudillo and Frazier (2021) but with a modified input space and a matching pennies dynamic on the final node.\nDropwave-Perturb a \u2208 [\u221210.24, 10.24]2, a\u2032 \u2208 [\u221210.24/5, 10.24/5]1. Like many Perturb systems, the adversary has a smaller domain than the agent to prevent it from being too strong. We have\nX0 = \u2225\u2225\u2225\u2225([a0 \u2212 a\u20320a1 ])\u2225\u2225\u2225\u2225 ,\nY = cos(3X0)\n2 + 0.5X20 .\nThis is the original Dropwave from Astudillo and Frazier (2021) but with one of the actions being perturbed by the adversary.\nAlpine-Penny a \u2208 [0, 10]4, a\u2032 \u2208 [1, 11]1. We have X0 = \u2212 \u221a a0 sin(a0)\nFor nodes Xi with an adversary parent we have Xi = \u2212 \u221a a\u2032i sin(a \u2032 i)Xi\u22121,\nand for nodes influenced by the agent we have\nXi = \u2212 \u221a ai sin(ai)Xi\u22121.\nThis is the original Alpine2 from Astudillo and Frazier (2021) but with an adversary controlling one of the nodes instead of the agent. We shift that adversary\u2019s action space so that they cannot 0 the output with a fixed action.\nAlpine-Perturb a \u2208 [0, 10]4, a\u2032 \u2208 [0, 2]3. Let a\u0304i = ai + a\u2032i for i when Xi has an adversarial action input, and a\u0304i = ai otherwise. We have\nX0 = \u2212 \u221a a\u03040 sin(a\u03040),\nXi = \u2212 \u221a a\u0304\u2032i sin(a\u0304i)Xi\u22121.\nRosenbrock-Penny a \u2208 [0, 1]4, a\u2032 \u2208 [0, 1]2. We have\nX0 = \u2212100(a1 \u2212 a20)2 \u2212 (1\u2212 a0)2 + 10, Xi = ( \u2212100(ai+1 \u2212 a2i )2 \u2212 (1\u2212 ai)2 + 10 +Xi\u22121 ) a\u0304\u2032i,\nwhere a\u0304\u2032i = 1 if there is no adversary over node i and otherwise a\u0304 \u2032 i = a \u2032 i.\nRosenbrock-Perturb a \u2208 [\u22122, 2]4, a\u2032 \u2208 [\u22121, 1]2. Let a\u0304i = ai + a\u2032i for i when Xi has an adversarial action input, and a\u0304i = ai otherwise. We have\nX0 = \u2212100(a\u03041 \u2212 a\u030420)2 \u2212 (1\u2212 a\u03040)2 + 10,\nXi = \u2212100(a\u0304i+1 \u2212 a\u03042i )2 \u2212 (1\u2212 a\u0304i)2 + 10 +Xi\u22121.\nAckley-Penny a \u2208 [\u22122, 2]4, a\u2032 \u2208 [\u22121, 1]1. Let a\u0304i = ai + a\u2032i for i when Xi has an adversarial action input, and a\u0304i = ai otherwise. We have\nX0 = 1\n4 \u2211 i a\u03042i ,\nX1 = 1\n4 \u2211 i cos(2\u03c0a\u0304i),\nY = 20 exp(\u22120.2 \u221a X0) + e X1 .\nAckley-Perturb a \u2208 [\u22122, 2]4, a\u2032 \u2208 [\u22121, 1]2. We have\nX0 = 1\n4 \u2211 i a2i ,\nX1 = 1\n4 \u2211 i cos(2\u03c0ai),\nY = 20a\u20320 exp(\u22120.2 \u221a X0) + e X1 .\nThis is the original Ackley from Astudillo and Frazier (2021) but with a matching pennies dynamic on the final node."
        },
        {
            "heading": "C.2 SHARED MOBILITY SYSTEM",
            "text": "We use the same SMS simulator as Sessa et al. (2021) and thus we largely refer to this regarding simulator details, unless otherwise specified. The simulator uses real demand data amalgamated from several SMS sources in Louisville Kentucky Lou (2021). We treat the system as a single SMS where all transport units are identical. A single trip taken in the Louisville data at a specific time is treated as a single unit of demand in the simulator. The demand is fulfilled if the location of the demand (where the trip started in the dataset) is within a certain distance (0.8km Euclidean distance) of a depot containing at least one bike. If the demand for a trip is satisfied, a single trip starting from the depot is counted and the bike is transported to the depot nearest to the end location of the trip in the dataset, after removing the bike from the system for 30 minutes to simulate the trip having non-zero duration. The simulator\u2019s use of real trip data means that the geographical and temporal distribution of demand, and its relation to the weather, is realistic.\nThe depots are not in the original trip data but constructed from the trip data using a similar procedure to Sessa et al. (2021). The start location for every trip taken over the year is clustered via k-means, and then clusters that are very close together are removed. This left 116 depots where bikes can be left. We consider a system with 40 bikes, which are distributed initially by 5 trucks that place all bikes in that truck at the same depot.\nWe further allocate depots to regions. These are constructed by using trip data across the whole year, and using a heuristic that clusters depots into regions so that there is a low chance that any given trip starts in one region and ends in another. As shown in Fig. 8 this leads to nearby depots often being in the same region, which is reasonable. We get 15 regions R1 to R15.\nAgent action ai is a one-hot 116-length vector for which depot truck i\u2019s bikes are placed at.\nWe obtain 3 measurements for a\u2032t at the end of each day t. This is the day\u2019s average temperature, rainfall, and total demand (including unmet demand). These are part of a\u2032 because they are out of our agent\u2019s control and not sampled i.i.d. across days. The agent must adaptively respond to these observations over time. Weather data is the real weather from that day obtained from Loc.\nObservations xri give the number of bikes at day start in depot number i within region r. xr is the total fulfilled trips that started in region r. Reward Y is then the total trips in a day. All observations are normalized to ensure they are fixed in [0, 1].\nIn Sessa et al. (2021), 2 separate GPs are used to model weekday and weekend demand. For simplicity we use a simulator that skips weekends, and therefore we don\u2019t need a separate model for the two types of day. No matter the algorithm used, the first 10 days of the year use the RANDOM strategy to gather exploratory data to initialize the GP models for the xr.\nThe graph used by D-CBO-MW is given in Fig. 8(b). The relationship between the bike allocations and bike distribution at day start {Xr: } R15 r=R1\nis a fixed known function. The mechanism from the starting bike distribution in a region r (Xr: ), adversary actions a\n\u2032 (weather and demand) and total trips in region r over the day (Xr) is an unknown function that must be learnt for each r. The relationship between total trips Y and its parents is known (sum over parents). For this kind of graph the output of the Causal UCB Oracle (Algorithm 2) will always set \u03b7 = 1, because more trips in any region results in more total trips. For computational efficiency we therefore implement the Causal UCB Oracle to set \u03b7 to 1 instead of optimising over \u03b7."
        }
    ],
    "title": "ADVERSARIAL CAUSAL BAYESIAN OPTIMIZATION",
    "year": 2024
}