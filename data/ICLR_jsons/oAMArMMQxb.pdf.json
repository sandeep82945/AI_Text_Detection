{
    "abstractText": "There is a long history, as well as a recent explosion of interest, in statistical and generative modeling approaches based on score functions \u2014 derivatives of the log-likelihood of a distribution. In seminal works, Hyv\u00e4rinen proposed vanilla score matching as a way to learn distributions from data by computing an estimate of the score function of the underlying ground truth, and established connections between this method and established techniques like Contrastive Divergence and Pseudolikelihood estimation. It is by now well-known that vanilla score matching has significant difficulties learning multimodal distributions. Although there are various ways to overcome this difficulty, the following question has remained unanswered \u2014 is there a natural way to sample multimodal distributions using just the vanilla score? Inspired by a long line of related experimental works, we prove that the Langevin diffusion with early stopping, initialized at the empirical distribution, and run on a score function estimated from data successfully generates natural multimodal distributions (mixtures of log-concave distributions).",
    "authors": [],
    "id": "SP:3c10d876048e9a3938f73ea19588d6b1e028142b",
    "references": [
        {
            "authors": [
                "Michael S Albergo",
                "Denis Boyda",
                "Daniel C Hackett",
                "Gurtej Kanwar",
                "Kyle Cranmer",
                "S\u00e9bastien Racaniere",
                "Danilo Jimenez Rezende",
                "Phiala E Shanahan"
            ],
            "title": "Introduction to normalizing flows for lattice field theory",
            "venue": "arXiv preprint arXiv:2101.08176,",
            "year": 2021
        },
        {
            "authors": [
                "Dominique Bakry",
                "Ivan Gentil",
                "Michel Ledoux"
            ],
            "title": "Analysis and geometry of Markov diffusion operators, volume",
            "year": 2014
        },
        {
            "authors": [
                "Alessandro Barp",
                "Francois-Xavier Briol",
                "Andrew Duncan",
                "Mark Girolami",
                "Lester Mackey"
            ],
            "title": "Minimum stein discrepancy estimators",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Julian Besag"
            ],
            "title": "Statistical analysis of non-lattice data",
            "venue": "Journal of the Royal Statistical Society: Series D (The Statistician),",
            "year": 1975
        },
        {
            "authors": [
                "Adam Block",
                "Youssef Mroueh",
                "Alexander Rakhlin"
            ],
            "title": "Generative modeling with denoising autoencoders and langevin sampling",
            "venue": "arXiv preprint arXiv:2002.00107,",
            "year": 2020
        },
        {
            "authors": [
                "Matthew Brennan",
                "Guy Bresler",
                "Wasim Huleihel"
            ],
            "title": "Reducibility and computational lower bounds for problems with planted sparse structure",
            "venue": "In Conference On Learning Theory,",
            "year": 2018
        },
        {
            "authors": [
                "Hong-Bin Chen",
                "Sinho Chewi",
                "Jonathan Niles-Weed"
            ],
            "title": "Dimension-free log-sobolev inequalities for mixture distributions",
            "venue": "Journal of Functional Analysis,",
            "year": 2021
        },
        {
            "authors": [
                "Sitan Chen",
                "Sinho Chewi",
                "Jerry Li",
                "Yuanzhi Li",
                "Adil Salim",
                "Anru R. Zhang"
            ],
            "title": "Sampling is as easy as learning the score: theory for diffusion models with minimal data assumptions, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Sinho Chewi",
                "Murat A. Erdogdu",
                "Mufan Bill Li",
                "Ruoqi Shen",
                "Matthew Zhang"
            ],
            "title": "Analysis of langevin monte carlo from poincar\u00e9 to log-sobolev, 2021",
            "year": 2021
        },
        {
            "authors": [
                "Arnak S Dalalyan"
            ],
            "title": "Theoretical guarantees for approximate sampling from smooth and log-concave densities",
            "venue": "Journal of the Royal Statistical Society. Series B (Statistical Methodology),",
            "year": 2017
        },
        {
            "authors": [
                "Persi Diaconis",
                "Laurent Saloff-Coste"
            ],
            "title": "Logarithmic sobolev inequalities for finite markov chains",
            "venue": "The Annals of Applied Probability,",
            "year": 1996
        },
        {
            "authors": [
                "Peter GM Forbes",
                "Steffen Lauritzen"
            ],
            "title": "Linear estimating equations for exponential families with application to gaussian linear concentration models",
            "venue": "Linear Algebra and its Applications,",
            "year": 2015
        },
        {
            "authors": [
                "Ruiqi Gao",
                "Yang Lu",
                "Junpei Zhou",
                "Song-Chun Zhu",
                "Ying Nian Wu"
            ],
            "title": "Learning generative convnets via multi-grid modeling and sampling",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Rong Ge",
                "Holden Lee",
                "Andrej Risteski"
            ],
            "title": "Simulated tempering langevin monte carlo ii: An improved proof using soft markov chain decomposition",
            "venue": "arXiv preprint arXiv:1812.00793,",
            "year": 2018
        },
        {
            "authors": [
                "Geoffrey E Hinton"
            ],
            "title": "Training products of experts by minimizing contrastive divergence",
            "venue": "Neural computation,",
            "year": 2002
        },
        {
            "authors": [
                "Geoffrey E Hinton"
            ],
            "title": "A practical guide to training restricted boltzmann machines",
            "venue": "Neural Networks: Tricks of the Trade: Second Edition,",
            "year": 2012
        },
        {
            "authors": [
                "Aapo Hyv\u00e4rinen"
            ],
            "title": "Estimation of non-normalized statistical models by score matching",
            "venue": "Journal of Machine Learning Research,",
            "year": 2005
        },
        {
            "authors": [
                "Aapo Hyv\u00e4rinen"
            ],
            "title": "Connections between score matching, contrastive divergence, and pseudolikelihood for continuous-valued variables",
            "venue": "IEEE Transactions on neural networks,",
            "year": 2007
        },
        {
            "authors": [
                "Aapo Hyv\u00e4rinen"
            ],
            "title": "Some extensions of score matching",
            "venue": "Computational statistics & data analysis,",
            "year": 2007
        },
        {
            "authors": [
                "Ioannis Karatzas",
                "Steven E Shreve"
            ],
            "title": "Brownian motion and stochastic calculus, volume 113",
            "venue": "Springer Science & Business Media,",
            "year": 1991
        },
        {
            "authors": [
                "Frederic Koehler",
                "Alexander Heckett",
                "Andrej Risteski"
            ],
            "title": "Statistical efficiency of score matching: The view from isoperimetry",
            "venue": "arXiv preprint arXiv:2210.00726,",
            "year": 2022
        },
        {
            "authors": [
                "Scott Lawrence",
                "Yukari Yamauchi"
            ],
            "title": "Normalizing flows and the real-time sign problem",
            "venue": "Physical Review D,",
            "year": 2021
        },
        {
            "authors": [
                "Holden Lee",
                "Andrej Risteski",
                "Rong Ge"
            ],
            "title": "Beyond log-concavity: Provable guarantees for sampling multi-modal distributions using simulated tempering langevin monte carlo",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Holden Lee",
                "Jianfeng Lu",
                "Yixin Tan"
            ],
            "title": "Convergence for score-based generative modeling with polynomial complexity",
            "venue": "arXiv preprint arXiv:2206.06227,",
            "year": 2022
        },
        {
            "authors": [
                "Holden Lee",
                "Jianfeng Lu",
                "Yixin Tan"
            ],
            "title": "Convergence of score-based generative modeling for general data distributions",
            "venue": "arXiv preprint arXiv:2209.12381,",
            "year": 2022
        },
        {
            "authors": [
                "David A Levin",
                "Yuval Peres"
            ],
            "title": "Markov chains and mixing times, volume 107",
            "year": 2017
        },
        {
            "authors": [
                "Neal Madras",
                "Dana Randall"
            ],
            "title": "Markov chain decomposition for convergence rate analysis",
            "venue": "Annals of Applied Probability,",
            "year": 2002
        },
        {
            "authors": [
                "Ilya Mironov"
            ],
            "title": "R\u00e9nyi differential privacy",
            "venue": "IEEE 30th computer security foundations symposium (CSF),",
            "year": 2017
        },
        {
            "authors": [
                "Erik Nijkamp",
                "Mitch Hill",
                "Song-Chun Zhu",
                "Ying Nian Wu"
            ],
            "title": "Learning non-convergent nonpersistent short-run mcmc toward energy-based model",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Erik Nijkamp",
                "Mitch Hill",
                "Tian Han",
                "Song-Chun Zhu",
                "Ying Nian Wu"
            ],
            "title": "On the anatomy of mcmc-based maximum likelihood learning of energy-based models",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Phillippe Rigollet",
                "Jan-Christian H\u00fctter"
            ],
            "title": "High dimensional statistics",
            "venue": "Lecture notes for course 18S997,",
            "year": 2017
        },
        {
            "authors": [
                "Robin Rombach",
                "Andreas Blattmann",
                "Dominik Lorenz",
                "Patrick Esser",
                "Bj\u00f6rn Ommer"
            ],
            "title": "Highresolution image synthesis with latent diffusion models",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Andr\u00e9 Schlichting"
            ],
            "title": "Poincar\u00e9 and log\u2013sobolev inequalities for mixtures",
            "year": 2019
        },
        {
            "authors": [
                "Yang Song",
                "Stefano Ermon"
            ],
            "title": "Generative modeling by estimating gradients of the data distribution",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Yang Song",
                "Sahaj Garg",
                "Jiaxin Shi",
                "Stefano Ermon"
            ],
            "title": "Sliced score matching: A scalable approach to density and score estimation",
            "venue": "In Uncertainty in Artificial Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Yang Song",
                "Jascha Sohl-Dickstein",
                "Diederik P Kingma",
                "Abhishek Kumar",
                "Stefano Ermon",
                "Ben Poole"
            ],
            "title": "Score-based generative modeling through stochastic differential equations",
            "venue": "arXiv preprint arXiv:2011.13456,",
            "year": 2020
        },
        {
            "authors": [
                "Bharath Sriperumbudur",
                "Kenji Fukumizu",
                "Arthur Gretton",
                "Aapo Hyv\u00e4rinen",
                "Revant Kumar"
            ],
            "title": "Density estimation in infinite dimensional exponential families",
            "venue": "Journal of Machine Learning Research,",
            "year": 2017
        },
        {
            "authors": [
                "Belinda Tzen",
                "Tengyuan Liang",
                "Maxim Raginsky"
            ],
            "title": "Local optimality and generalization guarantees for the langevin algorithm via empirical metastability",
            "venue": "In Conference On Learning Theory,",
            "year": 2018
        },
        {
            "authors": [
                "Ramon Van Handel"
            ],
            "title": "Probability in high dimension",
            "venue": "Technical report, PRINCETON UNIV NJ,",
            "year": 2014
        },
        {
            "authors": [
                "Santosh Vempala",
                "Andre Wibisono"
            ],
            "title": "Rapid convergence of the unadjusted langevin algorithm: Isoperimetry suffices",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Roman Vershynin"
            ],
            "title": "High-dimensional probability: An introduction with applications in data science, volume 47",
            "year": 2018
        },
        {
            "authors": [
                "Pascal Vincent"
            ],
            "title": "A connection between score matching and denoising autoencoders",
            "venue": "Neural computation,",
            "year": 2011
        },
        {
            "authors": [
                "Larry Wasserman"
            ],
            "title": "All of nonparametric statistics",
            "venue": "Springer Science & Business Media,",
            "year": 2006
        },
        {
            "authors": [
                "Li Wenliang",
                "Danica J Sutherland",
                "Heiko Strathmann",
                "Arthur Gretton"
            ],
            "title": "Learning deep kernels for exponential family densities",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Jianwen Xie",
                "Yang Lu",
                "Song-Chun Zhu",
                "Yingnian Wu"
            ],
            "title": "A theory of generative convnet",
            "venue": "In International Conference on Machine Learning,",
            "year": 2016
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Score matching is a fundamental approach to generative modeling which proceeds by attempting to learn the gradient of the log-likelihood of the ground truth distribution from samples (\u201cscore function\u201d) Hyv\u00e4rinen (2005). This is an elegant approach to learning energy-based models from data, since it circumvents the need to compute the (potentially intractable) partition function which arises in Maximum Likelihood Estimation (MLE). Besides the original version of the score matching method (often referred to as vanilla score matching), many variants have been proposed and have seen dramatic experimental success in generative modeling, especially in the visual domain (see e.g. Song & Ermon (2019); Song et al. (2020b); Rombach et al. (2022)).\nIn this work, we revisit the vanilla score matching approach. It is known that learning a distribution via vanilla score matching generally fails in the multimodal setting (Wenliang et al., 2019; Song & Ermon, 2019; Koehler et al., 2022). However, there are also many positive aspects of modeling a distribution with the vanilla score. To name a few:\n1. Simplicity to fit: computing the best estimate to the vanilla score is easy in many situations. For example, there is a simple closed form solution the class of models being fit is an exponential family (Hyv\u00e4rinen, 2007b), and this in turn lets us compute the best fit in a kernel exponential family (see e.g. Sriperumbudur et al. (2017); Wenliang et al. (2019)).\n2. Compatibility with energy-based models: for a distribution p(x) \u221d exp(E(x)), the vanilla score function is \u2207E(x) so it is straightforward to go between the energy and the score function. This is related to the previous point (why exponential families are simple to score match), and also why it is easy to implement the Langevin chain for sampling an energy-based model.\n3. Statistical inference: in cases where vanilla score matching does work well, it comes with attractive statistical features like \u221a n-consistency, asymptotic normality, relative efficiency\nguarantees compared to the MLE, etc. \u2014 see e.g. Barp et al. (2019); Forbes & Lauritzen (2015); Koehler et al. (2022); Song et al. (2020a).\nIn addition, score matching is also closely related to other celebrated methods for fitting distributions which have been successfully used for a long time in statistics and machine learning \u2014 pseudolikelihood estimation (Besag, 1975) and contrastive divergence training (Hinton, 2002). (See e.g. Hyv\u00e4rinen (2007a); Koehler et al. (2022).)\nFor these reasons, we would like to better understand the apparent failure of score matching in the multimodal setting. In this work, we study score matching in the context of the most canonical family of multimodal distributions \u2014 mixtures of log-concave distributions. (As a reminder, any distribution can be approximated by a sufficiently large mixture, see e.g. Wasserman (2006).) While vanilla score matching itself does not correctly estimate these distributions, we show that the trick of using \u201cdata-based initialization\u201d when sampling, which is well-known in the context of CD/MLE training of energy based models (see e.g. Hinton (2012); Xie et al. (2016) and further references below), provably corrects the bias of any model which accurately score matches the ground truth distribution."
        },
        {
            "heading": "1.1 OUR RESULTS",
            "text": "We now state our results in full detail. We are interested in the question of generative modeling using the vanilla score function. Generally speaking, there is some ground truth distribution \u00b5, which for us we will assume is a mixture of log-concave distributions, and we are interested in outputing a good estimate \u00b5\u0302 of it. We show that this is possible provided access to:\n1. A good estimate of the score function of \u2207 log\u00b5. (In many applications, this would be learned from data using a procedure like score matching.)\n2. A small number of additional samples from \u00b5, which are used for data-based initialization.\nTo make the above points precise, the following is our model assumption on \u00b5: Assumption 1. We assume probability distribution \u00b5 is a mixture of K log-concave components: explicitly, \u00b5 = \u2211K i=1 pi\u00b5i for some weights p1, . . . , pK s.t. pi > 0 and \u2211 i pi = 1. Furthermore, we suppose the density of each component \u00b5i is \u03b1 strongly-log-concave and \u03b2-smooth with \u03b2 \u2265 11 i.e. \u03b1I \u2aaf \u2212\u22072 log\u00b5i(x) \u2aaf \u03b2I for all x. We define the notation p\u2217 = mini pi and \u03ba = \u03b2/\u03b1 \u2265 1. Remark 1. The assumption that \u00b5i is \u03b1-strongly log-concave and \u03b2-smooth is the most standard setting where the Langevin dynamics are guaranteed to mix rapidly (see e.g. Dalalyan (2017)).\nand the following captures formally what we mean by a \u201cgood estimate\u201d of the score function: Definition 1. For \u00b5 a probability distribution with smooth density \u00b5(x), an \u03f5score-accurate estimate of the score in L2(\u00b5) is a function s such that\nEx\u223c\u00b5[||s(x)\u2212\u2207 log\u00b5(x)||2] \u2264 \u03f52score. (1)\nAs discussed in the below remark, this is the standard and appropriate assumption to make when score functions are learned from data. There are also other settings of interest where the ground truth score function is known exactly (e.g. \u00b5 is an explicit energy-based model which we have access to, and we want to generate more samples from it2) in which case we can simply take \u03f5score = 0. Remark 2. Assumption (1) says that on average over a fresh sample from the distribution, s(x) is a good estimate of the true score function \u2207 log\u00b5(x). This is the right assumption when score functions are estimated from data, because it is generally impossible to learn the score function far from the support of the true distribution. See the previous work e.g. Chen et al. (2023); Lee et al. (2022a;b); Block et al. (2020) where the same distinction is discussed in more detail.\nGiven a class of functions which contains a good model for the true score function and has a small Rademacher complexity compared to the number of samples, the function output by vanilla score matching will achieve small L2 error (see proof of Theorem 1 of Koehler et al. (2022)). In particular, this can be straightforwardly applied to parametric families of distributions like mixtures of Gaussians. We would also generally expect this assumption to be satisfied when the distribution is successfully learned via other learning procedures, such as MLE/contrastive divergence. (See related simulation in Appendix I.)\n1We can always re-scale the domain so that \u03b2 \u2265 1. 2For example, one use case of generative modeling is when we have the ground truth and want to accelerate\nan existing sampler which is expensive to run, see e.g. Albergo et al. (2021); Lawrence & Yamauchi (2021).\nWe show the distribution output by Langevin dynamics on an approximate score function will be close to the ground truth provided (1) we initialize the Langevin diffusion from the empirical distribution of samples, and (2) we perform early stopping of the diffusion, so that it does not reach its stationary distribution. Formally, let the Langevin Monte Carlo (LMC, a.k.a. discrete-time Langevin dynamics) chain with initial state X0, score function s, and step size h > 0 be defined by the recursion\nXh(i+1) = Xhi + h s(Xhi) + \u221a 2h\u2206hi\nwhere each noise variable \u2206hi \u223c N(0, I) is independent of the previous ones. Our main result gives a guarantee for samplling with LMC started from a small set of samples and run for time T : Theorem 1. Let \u03f5TV \u2208 (0, 1/2). Suppose \u00b5 is a mixture of strongly log-concave measures as in Assumption 1 and s is a function which estimates the score of \u00b5 within L2 error \u03f5score in the sense of Definition 1. Let\nT = \u0398\u0303\n(( exp(K)d\u03ba\np\u2217\u03f5TV\n)OK(1)) , h = \u0398\u0303 ( \u03f54TV\n(\u03b2\u03ba2K exp(K))4d3T\n) .\nLet Usample be a set of M i.i.d. samples from \u00b5 and \u03bdsample be the uniform distribution over Usample. Suppose that M = \u2126(p\u22122\u2217 \u03f5 \u22124 TV K 4 log(K/\u03f5TV ) log(K/\u03c4)), and that\n\u03f5score \u2264 p 1/2 \u2217\n\u221a h\u03f52TV 7T = \u0398\u0303\n( p 1/2 \u2217 \u03f5 4 TV\n(\u03b2\u03ba2K exp(K))2d3/2T 3/2\n) .\nLet (X\u03bdsamplenh )n\u2208N be the LMC chain with score s and step size h initialized at \u03bdsample. Then with probability at least 1\u2212\u03c4 over the randomness of Usample, the conditional law \u00b5\u0302 = L(X \u03bdsample T | Usample) satisfies dTV (\u00b5\u0302, \u00b5) \u2264 \u03f5TV . (2)\nWe now make a few comments to discuss the meaning of the result. Conclusion (2) says that we have successfully found an \u03f5TV -close approximation of the ground truth distribution \u00b5. Unpacking the definitions, it says that with high probability over the sample set: (1) picking a uniform sample from the training set, and (2) running the Langevin chain for time T will generate an \u03f5TV -approximate sample from the distribution \u00b5. Note in particular that we have can draw as many samples as we like from the distribution without needing new training data. The fact that this is conditional on the dataset is a key distinction: the marginal law of any element of the training set would be \u00b5, but its conditional law is a delta-distribution at that training sample, and the conditional law is what is relevant for generative modeling (being able to draw new samples from the right distribution). See also Figure 1 for a simulation which helps illustrate this distinction. Remark 3. Provided the number of components in the mixture is O(1), i.e. upper bounded by a constant, the dependence on all other parameters is polynomial or logarithmic. It is possible to remove the dependence on the minimum weight p\u2217 completely \u2014 see Corollary 2 in Appendix H. Remark 4. It turns out Theorem 1 is a new result even in the very special case that the ground truth is unimodal. The closest prior work is Theorem 2.1 of Lee et al. (2022a), where it was proved that the Langevin diffusion computed using an approximate score function succeeds to approximately sample from the correct distribution given a (polynomially-)warm start in the \u03c722-divergence. However, while the empirical distribution of samples is a natural candidate for a warm start, in high dimensions it will not be anywhere close to the ground truth distribution unless we have an exponentially large (in the dimension) number of samples, due to the \u201ccurse of dimensionality\u201d, see e.g. Wasserman (2006)."
        },
        {
            "heading": "1.2 FURTHER DISCUSSION",
            "text": "One motivation: computing score functions at substantial noise levels can be computationally difficult. In some cases, computing/learning the vanilla score may be a substantially easier task than alternatives; for example, compared to learning the score function for all noised versions of the ground truth (as used in diffusion models like Song & Ermon (2019)). As a reminder, denoising diffusion models are based on the observation that the score function of a noised distribution N(0, \u03c32I) \u22c6 p exactly corresponds to a Bayesian denoising problem: computing the posterior mean on X \u223c p given a noisy observation Y \u223c N(x, \u03c32I) Vincent (2011); Block et al. (2020), via the equation\ny + \u03c32\u2207 log(N(0, \u03c32I) \u22c6 p)(y) = E[X | Y = y].\nUnlike the vanilla score function this will not be closed form for most energy-based models; the optimal denoiser might be complex when the signal is immersed in a substantive amount of noise.\nFor example, results in the area of computational-statistical gaps tell us that for certain values of the noise level \u03c3 and relatively simple distributions p, approximate denoising can be average-case computationally hard under widely-believed conjectures. For example, let p be a distribution over matrices of the form N(rrT , \u03f52) with r a random sparse vector and \u03f5 > 0 small. Then the denoising problem for this distribution will be \u201cestimation in the sparse spiked Wigner model\u201d. In this model, for a certain range of noise levels \u03c3 performing optimal denoising is as hard as the (conjecturally intractible) \u201cPlanted Clique\u201d problem (Brennan et al., 2018); in fact, even distinguishing this model from a pure noise model with r = 0 is computationally hard despite the fact it is statistically possible \u2014 see the reference for details. So unless the Planted Clique conjecture is false, there is no hope of approximately computing the score function of p \u22c6 N(0, \u03c32) for these values of \u03c3. On the other hand, there is no computational obstacle to computing the score of p itself provided \u03f5 > 0 is small \u2014 denoising is only tricky once the noise level becomes sufficiently large.\nRelated Experimental Work. As mentioned before, many experimental works have found success generating samples, especially of images, by running the Langevin diffusion (or other Markov chain) for a small amount of time. One aspect which varies in these works is how the diffusion is initialized. To use the terminology of Nijkamp et al. (2020), the method we study uses an informative/databased initialization similar to contrastive divergence Hinton (2012); Gao et al. (2018); Xie et al. (2016). While in CD the early stopping of the dynamics is usually motivated as a way to save computational resources, the idea that stopping the sampler early can improve the quality of samples is consistent with experimental findings in the literature on energy-based models. As the authors of Nijkamp et al. (2020) say, \u201cit is much harder to train a ConvNet potential to learn a steady-state over realistic images. To our knowledge, long-run MCMC samples of all previous models lose the realism of short-run samples.\u201d One possible intuition for the benefit of early stopping, consistent with our analysis and simulations, is that it reduces the risk of stepping into low-probability regions where the score function may be poorly estimated. Some works have also found success using random/uninformative initializations with appropriate tweaks (Nijkamp et al., 2019; 2020), although they still found informative initialization to have some advantages \u2014 for example in terms of output quality after larger numbers of MCMC steps.\nRelated Theoretical Work. The works Block et al. (2020); Lee et al. (2022a) established results for learning unimodal distributions (in the sense of being strongly log-concave or satisfying a log-Sobolev inequality) via score matching, provided the score functions are estimated in an L2 sense. The work Koehler et al. (2022) showed that the sample complexity of vanilla score matching is related to the size of a restricted version of the log-Sobolev constant of the distribution, and in particular proved negative results for vanilla score matching in many multimodal settings. The works Lee et al. (2022b); Chen et al. (2023) proved that even for multimodal distributions, annealed score matching will successfully learn the distribution provided all of the annealed score functions can be successfully estimated in L2. In our work we only assume access to a good estimate of the vanilla score function, but still successfully learn the ground truth distribution in a multimodal setting.\nIn the sampling literature, our result can be thought of establishing a type of metastability statement, where the dynamics become trapped in local minima for moderate amounts of time \u2014 see e.g. Tzen et al. (2018) for further background. Also in the sampling context, the works Lee et al. (2018); Ge et al. (2018) studied a related problem, where the goal is to sample a mixture of isotropic Gaussians given black-box access to the score function (which they do via simulated tempering). This problem ends up to be different to the ones arising in score matching: they need exact knowledge of the true score function (far away from the support of the distribution), but they do not have access to training data from the true distribution. As a consequence of the differing setup, they prove an impossibility result (Ge et al., 2018, Theorem F.1) for a mixture of two Gaussians with covariances I and 2I (it will not be possible to find both components), but our result proves this is not an issue in our setting.\nQuestions for future work. In our result, we proved the first bound for sampling with the vanilla score, estimated from data, which succeeds in the multimodal setting, but it is an open question if the dependence on the number of components is optimal; it seems likely that the dependence can be improved, at least in many cases. Finally, it is interesting to ask what the largest class of\ndistributions our result can generalize to \u2014 with data-based initialization, multimodality itself is no longer an obstruction to sampling with Langevin from estimated gradients, but are there other possible obstructions?"
        },
        {
            "heading": "2 TECHNICAL OVERVIEW",
            "text": "We first review some background and notation which is helpful for discussing the proof sketch. We leave complete proofs of all results to the appendices.\nNotation. We use standard big-Oh notation and use tildes, e.g. O\u0303(\u00b7), to denote inequality up to log factors and OB(\u00b7) to denote an inequality with a constant allowed to depend on B. We let dTV (\u00b5, \u03bd) = supA |\u00b5(A)\u2212 \u03bd(A)| be the usual total variation distance between probability measures \u00b5 and \u03bd defined on the same space, where the supremum ranges over measurable sets. Given a random variable X , we write L(X) to denote its law.\nLog-Sobolev inequality. We say probability distribution \u03c0 satisfies a log-Sobolev inequality (LSI) with constant CLS if for all smooth functions f , E\u03c0[f2 log(f2/E\u03c0[f2])] \u2264 2CLSE\u03c0[||\u2207f ||2]. Due to the Bakry-Emery criterion, if \u03c0 is \u03b1-strongly log-concave then \u03c0 satisfies LSI with constant CLS = 1/\u03b1. LSI is equivalent to a statement about mixing of the Langevin dynamics \u2014 if we let \u03c0t denote the law of the diffusion at time t then an LSI is equivalent to the inequality\nDKL(\u03c0t||\u03c0) \u2264 exp(\u22122t/CLS)DKL(\u03c00||\u03c0)\nholding for an arbitrary initial distribution \u03c00. Here DKL(P,Q) = EP [log dPdQ ] is the KullbackLiebler divergence. See Bakry et al. (2014); Van Handel (2014) for more background.\nStochastic calculus. We will need to use stochastic calculus to compare the behavior of similar diffusion processes \u2014 see Karatzas & Shreve (1991) for formal background. Let (Xt)t\u22650 and (Yt)t\u22650 be two Ito processes defined by SDEs: dXt = s1(Xt)dt+ dBt and dYt = s2(Xt)dt+ dBt. Let PT , QT be the laws of the paths (Xt)t\u2208[0,T ] and (Yt)t\u2208[0,T ] respectively. The following follows by Girsanov\u2019s theorem (see (Chen et al., 2023, Eq. (5.5) and Theorem 9))\ndTV (YT , XT ) 2 \u2264 dTV (QT , PT )2 \u2264\n1 2 EQT [\u222b T 0 ||s2(Yt)\u2212 s1(Yt)||2dt ] In particular, this is useful to compare continuous and discrete time Langevin diffusions. If (Yt) be the continuous Langevin diffusion with score function s, and (Xt) is a linearly interpolated version of the discrete-time Langevin dynamics defined by dXt = s(X\u2308t/h\u2309h)dt+ dBt, then\ndTV (YT , XT ) 2 \u2264 1\n2 EQT [\u222b T 0 ||s(Yt)\u2212 s(Y\u2308t/h\u2309h)||2dt ] (3)"
        },
        {
            "heading": "2.1 PROOF SKETCH",
            "text": "High-level discussion. At a high level, our argument proceeds by (1) group the components of the mixture into larger \u201cwell-connected\u201d pieces, and (2) showing that the process mixes well within each of these pieces, while preserving the correct relative weight of each piece. One of the challenges in proving our result is that, contrary to the usual situation in the analysis of Markov chains (as in e.g. Bakry et al. (2014); Levin & Peres (2017)), we do not want to run the Langevin diffusion until it mixes to its stationary distributions. If we ran the process until mixing, then we would be performing the vanilla score matching procedure which provably fails in most multimodal settings because it incorrectly weights the different components (Koehler et al., 2022). So what we want to do is prove the process succeeds at some intermediate time T (See Figure 1 for a simulation illustrating this.)\nTo build intuition, consider the special case where all of the components in the mixture distributions are very far from each other. In this case, one might guess that taking T to be the maximum of the mixing times of each of the individual components will work. Provided there are enough samples in the dataset, the initialization distribution will accurately model the relative weights of the different\nclusters in the data, and running the process up to time T will approximately sample from the cluster that the initialization is drawn from. We could hope to prove the result by arguing that the dynamics on the mixture is close to the dynamics on one of the mixture components.\nSome challenges to overcome in the analysis. This is the right intuition, but for the general case the behavior of the dynamics is more complicated. When components are close, the score function of the mixture distribution may not be close to the score function of either component in the region of overlap; relatedly, particles may cross over between components. Also, the following remark shows that natural variants of our main theorem are actually false. Remark 5. We might think that initializing from the center of each mixture component would work just as well as initializing from samples. This is fine if the clusters are all very far from each other, but wrong in general. If the underlying mixture distribution is 12N(0, Id) + 1 2N(0, 2Id) and the dimension d is large, then the first component will have almost all of its mass within distance O(1) of a sphere of radius\n\u221a d and the second component will similarly concentrate about a sphere of radius\u221a\n2d. (See Theorem 3.1.1 of Vershynin (2018).) As a consequence, the dynamics initialized at the origin will mix within the shell of radius \u221a d but take exp(\u2126(d)) time to cross to the larger \u221a 2d shell. (This can be proved by observing that the gap between the two spheres forms a \u201cbottleneck\u201d for the dynamics, see Levin & Peres (2017).) In contrast, if we initialize from samples then approximately half of them will lie on the outer shell and, as we prove, the dynamics mix correctly.\nWe now proceed to explain in more detail how we prove our result. We start with the analysis of an idealized diffusion process, and then through several comparison arguments establish the result for the real LMC algorithm.\nAnalysis of idealized diffusion. To start out, we analyze an idealized process in which:\n1. The score function \u2207 log\u00b5 is known exactly. (Our result is still new in this case.) 2. The dynamics is the continous-time Langevin diffusion given by the Ito process\ndX\u0304t = \u2207 log\u00b5(X\u0304t) dt+ \u221a 2 dBt.\nThis is the scaling limit of the discrete-time LMC chain as we take the step size h \u2192 0, where dBt is the differential of a Brownian motion Bt.\n3. For purposes of exposition, we make the fictitious assumption that the ground truth distribution \u00b5 is supported in a ball of radius R. This will not be literally true, but for sufficiently large R \u00b5 will be almost entirely contained within a radius R ball. (In the supplement, we handle this rigorously using concentration, see e.g. proof of Lemma 11 of Appendix F).\nAdditionally, for the purpose of illustration, in this proof sketch we assume the target distance in TV is 0.01 and consider the case where there are two \u03b1-strongly log concave and \u03b2-smooth components \u00b51 and \u00b52, and \u00b5 = 12\u00b51 + 1 2\u00b52. After we complete the proof sketch for this setting, we will go back and explain how to generalize the analysis to arbitrary mixtures, handle the error induced by discretization, and finally make the analysis work with an L2 estimate of the true score function.\nOverlap parameter. We define \u03b412 := 1\u2212 dTV (\u00b51, \u00b52) = \u222b min{\u00b51(x), \u00b52(x)}dx\nas a quantitative measure of how much components 1 and 2 overlap; for example, \u03b412 = 1 iff \u00b51 and \u00b52 are identical. The analysis splits into cases depending on whether \u03b412 is large; we let \u03b4 > 0 be a parameter which determines this split and which will be optimized at the end.\nHigh overlap case (Appendix C). If \u00b51 and \u00b52 has high overlap, in the sense that \u03b412 \u2265 \u03b4, then we show that \u00b5 satisfies a log Sobolev inequality with constant at most O(1/(\u03b1\u03b4)), by applying our Theorem 2, an important technical ingredient which is discussed in more detail below. Thus for a typical sample x from \u00b5, the continuous Langevin diffusion (X\u03b4xt )t\u22650 with score function \u2207 log\u00b5 initialized at x converges to \u00b5 i.e. dTV (L(X\u0304\u03b4xt ), \u00b5) \u2264 \u03f5 for T \u2265 \u2126( 1\u03b1\u03b4 log(d\u03f5 \u22121)).3\n3This follows as LSI yields exponential convergence in KL-divergence. While the KL-divergence of the initialization \u03b4x with respect to \u00b5 is unbounded, we can bound the KL-divergence of X\u0304\u03b4xh for some small h.\nLow overlap case (Appendix F, Lemma 11). When \u00b51 and \u00b52 have small overlap i.e. \u03b412 \u2264 \u03b4, we will show that for x \u223c \u00b5, with high probability, the gradient of the log-likelihood of the mixture distribution \u00b5 at x is close to that of one of the components \u00b51, \u00b52 (Appendix F.1). This is because, supposing that ||x|| \u2264 R, for i \u2208 {1, 2} we can upper bound\n||\u2207 log\u00b5(x)\u2212\u2207 log\u00b5i(x)|| \u2264 2\u03b2R ( 1\u2212 \u00b5i(x)\n\u00b51(x) + \u00b52(x)\n) ,\nand low overlap implies that mini ( 1\u2212 \u00b5i(x)\u00b51(x)+\u00b52(x) ) is small for typical x \u223c \u00b5.\nConsider the continuous Langevin diffusion (X\u0304\u03b4xt ) initialized at \u03b4x i.e. X\u03040 = x. Observe that the marginal law of X\u0304\u03b4xt where x \u223c \u00b5 is exactly \u00b5, since \u00b5 is the stationary distribution of the Langevin diffusion. Let H > 0 be a parameter to be tuned later. The above discussion and Markov\u2019s inequality allows us to argue that for a typical sample x, the gradient of the log-likelihood of \u00b5 at X\u0304\u03b4xnH is close to that of either components \u00b51, \u00b52 with high probability.\nNext, we perform a union bound over n \u2208 {0, \u00b7 \u00b7 \u00b7 , N \u2212 1} and bound the drift ||\u2207 log\u00b5(x) \u2212 \u2207 log\u00b5i(x)|| in each small time interval [nH, (n+1)H]. By doing so, we can argue that for a typical sample x \u223c \u00b5, with probability at least 1\u2212\u03f5\u22121\u03b2RN\u03b412 over the randomness of the Brownian motion driving the Langevin diffusion, the gradient of the log-likelihood at X\u0304\u03b4xt for t \u2208 [0, NH] is close to that of the component distribution \u00b5i closest to the initial point x (see Proposition 26 of Appendix F).\nIn other words, assuming that the initial point x satisfies \u00b51(x) \u2265 \u00b52(x) and letting T = NH , we can show that with high probability,\nsup t\u2208[0,T ]\n||\u2207 log\u00b5(X\u0304\u03b4xt )\u2212\u2207 log\u00b51(X\u0304 \u03b4x t )|| \u2264 1.1\u03f5.\nThis allows us, using (3), to compare our Langevin diffusion with the one with score function \u2207 log\u00b51 and show the output at time T is approximately a sample from \u00b51.\nIn a typical set Usample of i.i.d. samples from \u00b5, roughly 50% of the samples x \u2208 Usample satisfy \u00b51(x) \u2265 \u00b52(x) and the other 50% samples satisfy \u00b52(x) \u2265 \u00b51(x), thus the Langevin dynamics (X\u0304\n\u03bdsample t )t\u22650 initialized at the uniform distribution \u03bdsample over Usample will be close to \u00b51+\u00b52 2 = \u00b5\nafter time T provided we set H,T, \u03f5, \u03b4 appropriately.\nConcluding the idealized analysis. Either \u03b412 \u2265 \u03b4 in which case the high-overlap analysis above based on the log-Sobolev constant succeeds, or \u03b412 < \u03b4 in which case the low-overlap analysis succeeds. Optimizing over \u03b4, we find that in either case, with high probability over the set Usample of samples from \u00b5, for t \u2265 \u2126\u0303( (\u03b2R) 3\n\u03b15/2 ) we have\ndTV (L(X\u0304 \u03bdsample t | Usample), \u00b5) \u2264 0.01\nas desired.\nGeneralizing idealized analysis to arbitrary mixtures. (Appendix F, Theorem 5) When there are more than two components, we can generalize this analysis \u2014 the key technical difficulty, alluded to earlier, is analyzing the overlap between different mixture components. We do this by defining, for each \u03b4 > 0, a graph G\u03b4 where there is an edge between i, j \u2208 [K] when \u03b4ij := 1\u2212dTV (\u00b5i, \u00b5j) \u2264 \u03b4. As long as the minimum of the weights p\u2217 := mini pi is not too small, each connected component C of G\u03b4 is associated with a probability distribution \u00b5C = \u2211 i\u2208C pi\u00b5i\u2211 i\u2208C pi\nthat has log Sobolev constant on the order of OK,p\u22121\u2217 (1/\u03b1\u03b4).\nSuppose for a moment that the connected components are well separated compared to the magnitude of \u03b4. More precisely, suppose that for i, j in different connected components and some \u03b4 > 0 we have\n\u03b4ij \u2264 f(\u03b4) := \u0398 ( (\u03b1\u03b4)3/2\n(\u03b2R)3\n) . (4)\nThen, a direct generalization of the argument for two components shows that for a typical set Usample of i.i.d. samples from \u00b5, the continuous Langevin diffusion (X\u0304\u03bdsamplet )t\u22650 initialized at the uniform distribution over Usample converges to \u00b5 after time T\u03b4 = (\u03b1\u03b4)\u22121.\nIt remains to discuss how we select \u03b4 so that (4) is satisfied. We consider a decreasing sequence 1 = \u03b40 > \u03b41 > \u00b7 \u00b7 \u00b7 > \u03b4K\u22121 where \u03b4r+1 = f(\u03b4r) as in Eq. (4). Let Gr := G\u03b4r . If any two vertices from different connected components of Gr have overlap at most \u03b4r+1, then the above argument applies. Otherwise, Gr+1 must have one less connected component than Gr, and since G0 has at most K connected components, GK\u22121 must have 1 connected component and the above argument applies to it. Thus, in all cases, the distribution of X\u0304\u03bdsampleT\u03b4K\u22121 is close to \u00b5 in total variation distance.\nDiscretization analysis. (Appendix G, Lemma 14) We now move from a continuous-time to discrete-time process. Let (Xnh)n\u2208N and (X\u0304t)t\u22650 be respectively the LMC with step size h and the continuous Langevin diffusion. Both are with score function \u2207 log\u00b5 and have the same initialization. By an explicit calculation, we can bound ||\u22072 log\u00b5(x)||OP along the trajectory of the continuous process. This combined with the consequence of Girsanov\u2019s theorem (3) allows us to bound the total variation distance between the continuous (X\u0304t) and discretized (Xnh) processes. For appropriate choices of step size h and time T = Nh, using triangle inequality and the bound dTV (X\u0304T , \u00b5), we conclude that the discretized process XNh is close to \u00b5.\nSampling with an L2-approximate score function. (Appendix G) In many cases, score functions are learned from data, so we only have access to an L2-estimate s of the score such that E\u00b5[||s(x)\u2212 \u2207 log\u00b5(x)||2] \u2264 \u03f52score. We now describe how to make the analysis work in this setting. Using Girsanov\u2019s theorem, we can bound the total variation distance between the LMC (Xs,\u00b5nh )n\u2208N initialized at \u00b5 with score estimate s and the continuous Langevin diffusion (Z\u0304\u00b5nh)n\u2208N with true score function \u2207 log\u00b5, thus we can bound the probability that the LMC (Xs,\u00b5nh )n={0,\u00b7\u00b7\u00b7 ,N\u22121} hits the bad set\nBscore := {x : ||s(x)\u2212 log\u00b5(x)|| \u2265 \u03f5score,1}.\n(The idea of defining a \u201cbad set\u201d is inspired by the analysis of Lee et al. (2022a).) Similar to the argument for the continuous process, let Xs,\u03bdsamplenh denote the LMC with score function s and step size h initialized at the empirical distribution \u03bdsample. Since we know that X s,\u00b5 nh avoids the bad set and that L(Xs,\u00b5nh ) = EUsample\u223c\u00b5\u2297M [L(X s,\u03bdsample nh ))], we have by Markov\u2019s inequality that for a typical Usample, with high probability over the randomness of the Brownian motion, Xs,\u03bdsamplenh also avoids the bad set Bscore for all 0 \u2264 n < N. Thus, we can compare X s,\u03bdsample nh with the LMC with true score function \u2207 log\u00b5, and conclude that L(Xs,\u03bdsampleNh ) is close to \u00b5 in total variation distance."
        },
        {
            "heading": "2.2 TECHNICAL INGREDIENT: LOG-SOBOLEV CONSTANT OF WELL-CONNECTED MIXTURES",
            "text": "The following theorem, which we prove in the appendix, is used in the above argument to bound the log-Sobolev constant of mixture distributions where the components have significant overlap.\nTheorem 2. Let I be a set, and consider probability measures {\u00b5i}i\u2208I , nonnegative weights (pi)i\u2208I summing to one, and mixture distribution \u00b5 = \u2211 i pi\u00b5i. Let G be the graph on vertex set I where there is an edge between i, j if \u00b5i, \u00b5j have high overlap i.e.\n\u03b4ij := \u222b min{\u00b5i(x), \u00b5j(x)}dx \u2265 \u03b4.\nSuppose G is connected and let p\u2217 = min pi. The mixture distribution \u00b5 = \u2211\ni\u2208I pi\u00b5i has logSobolev constant\nCLS(\u00b5) \u2264 C|I|,p\u2217\n\u03b4 max i CLS(\u00b5i)\nwhere C|I|,p\u2217 = 4|I|(1 + log(p\u22121\u2217 ))p\u22121\u2217 only depends on |I| and p\u2217.\nA version of Theorem which bounds the (weaker) Poincar\u00e9 constant instead appeared before as Theorem 1.2 of Madras & Randall (2002), but the result for the log-Sobolev constant is new to the best of our knowledge. Compared to Chen et al. (2021), our assumption is milder than their assumption that the chi-square divergence between any two components is bounded. (For example, two non-isotropic Gaussians might have infinite chi-square divergence (see e.g. (Schlichting, 2019, Section 4.3)), so in that case their result doesn\u2019t imply a finite bound on the LSI of their mixture.) Schlichting (2019) bounds LSI of \u00b5 = p\u00b51 + (1\u2212 p)\u00b52 when either \u03c72(\u00b51||\u00b52) or \u03c72(\u00b52||\u00b51) are bounded; our bound applies to mixtures of more than two components."
        },
        {
            "heading": "3 SIMULATIONS",
            "text": "In Figure 1, we simulated the behavior of the Langevin dynamics with step size 0.01 and an estimated score function initialized at the ground truth distribution on a simple 1-dimensional example, a mixture of two Gaussians. If the Langevin dynamics are run until mixing, this corresponds to exactly performing the standard vanilla score matching procedure and this will fail to estimate the ground truth distribution well, which we see in the rightmost subfigure. The empirical distribution (time zero for the dynamics) is also not a good fit to the ground truth, but as our theory predicts the early-stopped Langevin diffusion (subfigure (b)) is indeed a good estimate for the ground truth.\nIn Figure 2 we simulated the trajectories of Langevin dynamics with step size 0.001, again with initialization from samples and a learned score function, in a 32-dimensional mixture of Gaussians. Similar to the one-dimensional example, we can see that at moderate times the trajectories have mixed well within their component, and at large times the trajectories sometimes pass through the region in between the components where the true density is very small. Additional simulations (including an experiment with Contrastive Divergence training) and information is in Appendix I."
        },
        {
            "heading": "A ORGANIZATION OF APPENDIX",
            "text": "In Appendix B, we review some basic mathematical preliminaries and notation, such as the definition of log-Sobolev and Poincar\u00e9 inequalities. In Appendix C we prove Theorem 4 (Theorem 2 of the main text), which shows that when clusters have significant overlap that the Langevin dynamics for the mixture distribution will successfully mix. Appendix D and Appendix E contain intermediate results which are used in the following sections: Appendix D shows how to analyze the Langevin diffusion starting from a point, and Appendix E shows how to bound the drift of the continuous Langevin diffusion over a short period of time. In Appendix F we prove Theorem 5, which shows that the continuous Langevin diffusion with score function \u2207V converges to \u00b5 after a suitable time T. In Appendix G, we prove our main results Theorem 6 and Corollary 1, which show that the discrete LMC with score function s with appropriately chosen step size is close to \u00b5 in total variation distance at a suitable time. Corollary 1 corresponds to Theorem 1 of the main text. In Appendix H, we remove the dependency of the runtime and number of samples on the minimum weight of the components i.e. p\u2217 = mini\u2208I pi (see Theorem 8 and Corollary 2 for the analogy of Theorem 6 and Corollary 1 respectively that has no dependency on p\u2217). Appendix I contains some additional simulations."
        },
        {
            "heading": "B PRELIMINARIES",
            "text": "In the preliminaries, we review in more detail the needed background on divergences between probability measures, functional inequalities, log-concave distributions, etc. in order to prove our main results.\nNotation. We use standard big-Oh notation and use tildes, e.g. O\u0303(\u00b7), to denote inequality up to log factors. We similarly use the notation \u2272 to denote inequality up to a universal constant. We let dTV (\u00b5, \u03bd) = supA |\u00b5(A)\u2212 \u03bd(A)| be the usual total variation distance between probability measures \u00b5 and \u03bd defined on the same space, where the supremum ranges over measurable sets. Given a random variable X , we write L(X) to denote its law. In general, we use the same notation for a measure and its probability density function as long as there is no ambiguity. For random variables X,Z, we will write dTV (X,Z) to denote the total variation distance between their laws L(X) and L(Z)."
        },
        {
            "heading": "B.1 RENYI DIVERGENCE",
            "text": "The Renyi divergence, which generalizes the more well-known KL divergence, is a useful technical tool in the analysis of the Langevin diffusion \u2014 see e.g. Vempala & Wibisono (2019). The Renyi divergence of order q \u2208 (1,\u221e) of \u00b5 from \u03c0 is defined to be\nRq(\u00b5||\u03c0) = 1\nq \u2212 1 lnE\u03c0\n[( d\u00b5(x)\nd\u03c0(x)\n)q] = 1\nq \u2212 1 ln\n\u222b ( d\u00b5(x)\nd\u03c0(x)\n)q d\u03c0(x)\n= 1\nq \u2212 1 ln\n\u222b ( d\u00b5(x)\nd\u03c0(x)\n)q\u22121 d\u00b5(x) = 1\nq \u2212 1 lnE\u00b5\n[( d\u00b5(x)\nd\u03c0(x) )q\u22121] The limit Rq as q \u2192 1 is the Kullback-Leibler divergence DKL(\u00b5||\u03c0) = \u222b \u00b5(x) log \u00b5(x)\u03c0(x)dx, thus we write R1(\u00b7) = DKL(\u00b7). Renyi divergence increases as q increases i.e. Rq \u2264 Rq\u2032 for 1 \u2264 q \u2264 q\u2032. Lemma 1 (Weak triangle inequality, (Vempala & Wibisono, 2019, Lemma 7), Mironov (2017)). For q > 1 and any measure \u03bd absolutely continuous with respect to measure \u00b5,\nRq(\u03bd||\u00b5) \u2264 q \u2212 1/2 q \u2212 1 R2q(\u03bd||\u03bd\u2032) +R2q\u22121(\u03bd\u2032||\u00b5)\nLemma 2 (Weak convexity of Renyi entropy). For q > 1, if \u00b5 is a convex combination of \u00b5i i.e. \u00b5(x) = \u2211 pi\u00b5i(x) then\nE\u03bd\n[( d\u03bd(x)\nd\u00b5(x) )q\u22121] \u2264 \u2211 i piE\u03bd [( d\u03bd(x) d\u00b5i(x) )q\u22121] .\nConsequently, Rq(\u03bd||\u00b5) \u2264 maxi Rq(\u03bd||\u00b5i) and Rq(\u00b5||\u03bd) \u2264 maxi Rq(\u00b5i||\u03bd)\nProof. By Holder\u2019s inequality\n( \u2211 i pi\u00b5i(x)) q\u22121\n( d\u2211\ni=1\npi \u00b5i(x)q\u22121 ) \u2265 ( \u2211 i pi) q = 1\nthus ( \u03bd(x)\n\u00b5(x) )q\u22121 \u2264 \u2211 i pi ( \u03bd(x) \u00b5i(x) )q\u22121 Taking expectation in \u03bd gives the first statement. Similarly, since q > 1 > 0,\nE\u03bd [( \u03bd(x)\n\u00b5(x) )q] \u2264 \u2211 i piE\u03bd [( \u03bd(x) \u00b5i(x) )q] For the second statement\nRq(\u03bd||\u00b5) = lnE\u03bd [( d\u03bd(x)d\u00b5(x) ) q\u22121]\nq \u2212 1 \u2264\nln(maxi E\u03bd [( d\u03bd(x)d\u00b5i(x) ) q\u22121])\nq \u2212 1 = max i Rq(\u03bd||\u00b5i)\nand\nRq(\u00b5||\u03bd) = lnE\u03bd [( d\u03bd(x)d\u00b5(x) ) q]\nq \u2212 1 \u2264\nln(maxi E\u03bd [( d\u03bd(x)d\u00b5i(x) ) q])\nq \u2212 1 = max i Rq(\u00b5i||\u03bd)."
        },
        {
            "heading": "B.2 LOG-CONCAVE DISTRIBUTIONS",
            "text": "Consider a density function \u03c0 : Rd \u2192 R\u22650 where \u03c0(x) = exp(\u2212V (x)). Throughout the paper, we will assume V is a twice continuously differentiable function. We say \u03c0 is \u03b2-smooth if V has bounded Hessian for all x \u2208 Rd: \u2212\u03b2I \u2aaf \u22072V (x) \u2aaf \u03b2I. We say \u03c0 is \u03b1-strongly log-concave if\n0 \u227a \u03b1I \u2aaf \u22072V (x) for all x \u2208 Rd."
        },
        {
            "heading": "B.3 FUNCTIONAL INEQUALITIES",
            "text": "For nonnegative smooth f : Rd \u2192 R\u22650, let the entropy of f with respect to probability distribution \u03c0 be Ent\u03c0[f ] = E\u03c0[f ln(f/E\u03c0[f ])]. We say \u03c0 satisfies a log-Sobolev inequality (LSI) with constant CLS if for all smooth functions f ,\nEnt\u03c0[f 2] \u2264 2CLSE\u03c0[||\u2207f ||2]\nand \u03c0 satisfies a Poincare inequality (PI) with constant CPI if Var\u03c0[f ] \u2264 2CPIE\u03c0[||\u2207f ||2]. The log-Sobolev inequality implies Poincare inequality: CPI \u2264 CLS . Due to the Bakry-Emery criterion, if \u03c0 is \u03b1-strongly log-concave then \u03c0 satisfies LSI with constant CLS = 1/\u03b1.\nLSI and PI are equivalent to statements about exponential ergodicity of the continuous-time Langevin diffusion, which is defined by the Stochastic Differential Equation\ndX\u0304\u03c0t = \u2207 log \u03c0(X\u0304 \u00b5 t ) dt+ \u221a 2 dBt.\nSpecifically, let \u03c0t denote the law of the diffusion at time t initialized from \u03c00 then a LSI is equivalent to the inequality DKL(\u03c0t||\u03c0) \u2264 exp(\u22122t/CLS)DKL(\u03c00||\u03c0) holding for an arbitrary initial distribution \u03c00. Similarly, a PI is equivalent to \u03c72(\u03c0t||\u03c0) \u2264 exp(\u22122t/CPI)\u03c72(\u03c00||\u03c0). Here DKL(P,Q) = EP [log dPdQ ] is the Kullback-Liebler divergence and \u03c72(P,Q) = EQ[(dP/dQ \u2212 1)2] is the \u03c72-divergence. See Bakry et al. (2014); Van Handel (2014) for more background."
        },
        {
            "heading": "B.4 CONCENTRATION",
            "text": "Proposition 1 (Concentration of Brownian motion, (Chewi et al., 2021, Lemma 32)). Let (Bt)t\u22650 be a standard Brownian motion in Rd. Then, if \u03bb \u2265 0 and h \u2264 1/(4\u03bb),\nE [ exp ( \u03bb sup\nt\u2208[0,h] ||Bt||2\n)] \u2264 exp(6dh\u03bb)\nIn particular, for all \u03b7 \u2265 0\nP [ sup\nt\u2208[0,h] ||Bt||2 \u2265 \u03b7\n] \u2264 exp ( \u2212 \u03b7 2\n6dh ) Proposition 2. Suppose a random non-negative real variable Z satisfies\n\u2200t : P[Z \u2265 D + t] \u2264 2 exp(\u2212\u03b3t2)\nfor some D \u2265 0, \u03b3 > 0. Then there exists numerical constant C s.t.\nE[Zp] \u2264 Cpp/2(D + \u03b3\u22121/2)p\nProof. For some R \u2265 D to be chosen later E[Zp] = \u222b \u221e 0 P[Zp \u2265 x]dx\n= \u222b Rp 0 P[Zp \u2265 x]dx+ \u222b \u221e Rp P[Zp \u2265 x]dx\n\u2264 \u222b Rp 0 1dx+ \u222b \u221e R P[Z \u2265 y]d(yp)\n\u2264 Rp + 2p \u222b \u221e R yp\u22121 exp(\u2212\u03b3(y \u2212D)2)dy\n\u2264 Rp + p2p( \u222b \u221e R zp\u22121 exp(\u2212\u03b3z2)dz +Dp\u22121 \u222b \u221e R exp(\u2212\u03b3z2)dz) \u2264 Rp + 2p\u22121(\u03b3\u2212p/2(p/2)p/2 + pDp\u22121\u03b3\u22121/2 \u221a \u03c0)\nwhere in the last inequality, we make a change of variable u = \u03b3z2 and note that 2p \u222b zp\u22121 exp(\u2212\u03b3z2)dz = \u03b3\u2212pp \u222b up/2\u22121 exp(\u2212u)du = \u0393(p/2) \u2264 (p/2)p/2 and\u222b\u221e\n0 exp(\u2212\u03b3z2)dz = (2\u03b3)\u22121/2\n\u221a 2\u03c0/2. Take R = D gives the desired result.\nProposition 3 ((Bakry et al., 2014, 5.4.2), restated in (Lee et al., 2022a, Lemma E.2) ). Suppose \u03c0 : Rd \u2192 R\u22650 satisfies LSI with constant 1/\u03b1. Let f : Rd \u2192 R be a L-Lipschitz function then\nPx\u223c\u03c0[|f(x)\u2212 E\u03c0[f(x)]| \u2265 t] \u2264 exp ( \u2212 \u03b1t 2\n2L2 ) Proposition 4 (Sub-Gaussian concentration of norm for strongly log concave measures). Let V : Rd \u2192 R be a \u03b1-strongly convex and \u03b2-smooth function. Let \u03ba = \u03b2/\u03b1. Let \u03c0 be the probability measure with \u03c0(x) \u221d exp(\u2212V (x)). Let x\u2217 = argminx V (x) then for D = 5 \u221a d \u03b1 ln(10\u03ba) we have\nPx\u223c\u03c0[||x\u2212 x\u2217|| \u2265 D + t] \u2264 exp(\u2212\u03b1t2/4)\nthus by Proposition 2, for p \u2265 1.\nE\u03c0[||x\u2212 x\u2217||p]1/p \u2264 O(1) \u221a p\n\u221a d\n\u03b1 ln(10\u03ba)p\nProof. By (Lee et al., 2022a, Lemma E.3), let x\u0304 = E\u03c0[x] then ||x\u0304 \u2212 x\u2217|| \u2264 12 \u221a d \u03b1 ln(10\u03ba). By Proposition 3, for any unit vector v \u2208 Rd, the function \u27e8v, x \u2212 x\u0304\u27e9 is 1-Lipschitz, since |\u27e8v, x\u27e9 \u2212\n\u27e8v, y\u27e9| \u2264 \u221a ||v||2||x \u2212 y||2 = ||x \u2212 y||2. Thus, by Proposition 3, \u27e8v, x \u2212 x\u0304\u27e9 has mean 0 and subGaussian concentration for all unit vector v, thus x \u2212 x\u0304\u27e9 is a sub-Gaussian random vector. From sub-Gaussianity, a standard argument (see e.g. Theorem 1.19 of Rigollet & H\u00fctter (2017)) shows that\nP\u03c0 [ ||x\u2212 x\u0304|| \u2265 4 \u221a d\n\u03b1 + t\n] \u2264 exp(\u2212\u03b1t2/4)\nthus by triangle inequality, using that ||x\u0304\u2212 x\u2217|| \u2264 \u221a\nd \u03b1 1 2 ln(10\u03ba), we have\nP\u03c0 [ ||x\u2212 x\u2217|| \u2265 (4 + 1/2 ln(10\u03ba)) \u221a d\n\u03b1 + t\n] \u2264 P\u03c0 [ ||x\u2212 x\u0304|| \u2265 4 \u221a d\n\u03b1 + t\n] \u2264 exp(\u2212\u03b1t2/4)\nProposition 5 (Normalization factor bound). Let V : Rd \u2192 R be a \u03b1-strongly convex and \u03b2-smooth function. Let \u03c0 be the probability measure defined by \u03c0(x) \u221d exp(\u2212V (x)) and Z := Z\u03c0 =\u222b exp(\u2212V (x))dx be its normalization factor. For any y \u2208 Rd\nexp ( \u2212V (y) + ||\u2207V (y)|| 2\n2\u03b2\n) (2\u03c0\u03b2\u22121)d/2 \u2264 Z \u2264 exp ( \u2212V (y) + ||\u2207V (y)|| 2\n2\u03b1\n) (2\u03c0\u03b1\u22121)d/2\nLet y = x\u2217 = argminV (x) and assume w.l.o.g. V (y) = 0 gives\nd 2 ln 1 \u03b2 \u2264 lnZ\u03c0 \u2212 d 2 ln(2\u03c0) \u2264 d 2 ln 1 \u03b1\nProof. Since \u03b1I \u2aaf \u22072V (x) \u2aaf \u03b2I,\n\u27e8\u2207V (y), x\u2212 y\u27e9+ \u03b1||x\u2212 y||2/2 \u2264 V (x)\u2212 V (y) \u2264 \u27e8\u2207V (y), x\u2212 y\u27e9+ \u03b2||x\u2212 y||2/2\nZ \u2264 \u222b exp(\u2212V (y)\u2212 \u27e8\u2207V (y), x\u2212 y\u27e9 \u2212 \u03b1||x\u2212 y||2/2)dx\n= exp ( \u2212V (y) + ||\u2207V (y)|| 2\n2\u03b1\n)\u222b exp ( \u2212\u03b1||(x\u2212 y) + \u03b1 \u22121\u2207V (y)||2\n2\n) dx\n= exp ( \u2212V (y) + ||\u2207V (y)|| 2\n2\u03b1\n) (2\u03c0\u03b1\u22121)d/2\nThe lower bound follows similarly. The second statement follows from the first since \u2207V (x\u2217) = 0."
        },
        {
            "heading": "B.5 GIRSANOV\u2019S THEOREM",
            "text": "Theorem 3 (Girsanov\u2019s Theorem (Karatzas & Shreve, 1991, Chapter 3.5)). Let (Xt)t\u22650 be stochastic processes adapted to the same filtration. Let PT and QT be probability measure on the path space C([0, T ];Rd) s.t. Xt evolved according to\ndXt = b P t dt+ \u221a 2dBPt under PT\ndXt = b Q t dt+ \u221a 2dBQt under QT\nAssume that Novikov\u2019s condition\nEQT\n[ exp ( 1\n4 \u222b T 0 ||bPt \u2212 b Q t ||2dt )] < \u221e (5)\nholds. Then\ndPT dQT = exp (\u222b T 0 1\u221a 2 \u27e8bPt \u2212 b Q t , dB Q t \u27e9 \u2212 1 4 \u222b T 0 ||bPt \u2212 b Q t ||2dt ) (6)\nLemma 3 (Application of Girsanov with approximation argument (Chen et al., 2023, Equation 5.5, Proof of Theorem 9)). Let (Xt)t\u22650 be stochastic processes adapted to the same filtration. Let PT and QT be probability measure on the path space C([0, T ];Rd) s.t. Xt evolved according to\ndXt = b P t dt+ \u221a 2dBPt under PT\ndXt = b Q t dt+ \u221a 2dBQt under QT\nSuppose EQT [ \u222b T 0 ||bPt \u2212 b Q t ||2dt] < \u221e then\n2dTV (QT ||PT )2 \u2264 DKL(QT ||PT ) \u2264 EQT [\u222b T 0 ||bPt \u2212 b Q t ||2dt ] Lemma 4 (Corollary of Theorem 3, (Chewi et al., 2021, Corollary 20)). With the setup and preconditions in Theorem 3, For any event E ,\nEQT [(\ndPT dQT\n)q 1E ] \u2264 \u221a\u221a\u221a\u221aEQT [ exp ( q2 \u222b T 0 ||bPt \u2212 b Q t ||2dt ) 1E ]"
        },
        {
            "heading": "B.6 MIXTURE POTENTIAL",
            "text": "Notation for indexing components. Let I = [K] be the set of indices i for the components \u00b5i of the mixture distribution \u00b5. We will need to work with subsets S of I and the mixture distribution forms by components \u00b5i for i \u2208 S. Definition 2. For S \u2286 I, let pS = \u2211 i\u2208S pi, and \u00b5S = p \u22121 S \u2211 i\u2208S pi\u00b5i. Let VS = \u2212 log\u00b5S .\nIf S = I we omit the subscript S.\nDerivative computations. For future use, we compute the derivatives of V. Proposition 6 (Gradient of V ).\n\u2207V (x) = \u2211\npi\u00b5i(x)\u2207Vi(x) \u00b5(x)\n(7)\nConsequently, ||\u2207V (x)|| \u2264 max ||\u2207Vi(x)||.\nProof. The statement follows from\n\u2207V (x) = \u2207 log\u00b5(x) = \u2207\u00b5(x) \u00b5(x)\nand\n\u2207\u00b5(x) = \u2207( \u2211\npiZ \u22121 i exp(\u2212Vi(x)) = \u2212 \u2211 pi\u00b5i(x)\u2207Vi(x).\nProposition 7 (Hessian of V ). \u22072V (x) = \u2211\ni pi\u00b5i(x)\u22072Vi(x) \u00b5(x) \u2212 \u2211 i,j pipj\u00b5i(x)\u00b5j(x)(\u2207Vi(x)\u2212\u2207Vj(x))(\u2207Vi(x)\u2212\u2207Vj(x))\u22a4 4\u00b52(x)\n(8) hence if \u22072Vi \u2aaf \u03b2I for all i \u2208 I then \u22072V (x) \u2aaf \u03b2I.\nProof. Let Zi = \u222b exp(\u2212Vi(x))dx be the normalization factor of \u00b5i. Note that\n\u2207(\u00b5i(x)\u2207Vi(x)) = \u2207(Z\u22121i exp(\u2212Vi(X))\u2207Vi(x)) = Z \u22121 i exp(\u2212Vi(x))(\u2212\u2207Vi(x)\u2207Vi(x)\n\u22a4 +\u22072Vi(x)) = \u00b5i(x)(\u22072Vi(x)\u2212\u2207Vi(x)\u2207Vi(x)\u22a4)\nand \u2207\u00b5(x) = \u2212 \u2211 pi\u00b5i(x)\u2207Vi(x), thus\n\u22072V (x) = \u2207( \u2211\ni pi\u00b5i(x)\u2207Vi(x)) \u00b5(x)\n\u2212 ( \u2211\npi\u00b5i(x)\u2207Vi(x))\u2207\u00b5(x) \u00b52(x)\n=\n\u2211 pi\u00b5i(x)(\u22072Vi(x)\u2212\u2207Vi(x)\u2207Vi(x)\u22a4)\n\u00b5(x) +\n( \u2211 pi\u00b5i(x)\u2207Vi(x))( \u2211 pi\u00b5i(x)\u2207Vi(x))\u22a4\n\u00b52(x)\nNext, ( \u2211 pi\u00b5i(x)\u2207Vi(x))( \u2211 pi\u00b5i(x)\u2207Vi(x))\u22a4 \u2212 ( \u2211 pi\u00b5i\u2207Vi(x)\u2207Vi(x)\u22a4)( \u2211 pi\u00b5i)\n= \u2211 i,j pipj\u00b5i(x)\u00b5j(x)\u2207Vi(x)\u2207V \u22a4j \u2212 \u2211 i,j pipj\u00b5i(x)\u00b5j(x)\u2207Vi(x)\u2207Vi(x)\u22a4\n= 1\n2 \u2211 i \u0338=j pipj\u00b5i(x)\u00b5j(x)(\u2207Vi(x)\u2207V \u22a4j +\u2207Vj(x)\u2207V \u22a4i \u2212\u2207Vi(x)\u2207V \u22a4i \u2212\u2207Vj(x)\u2207V \u22a4j )\n= \u22121 2 \u2211 i\u0338=j pipj\u00b5i(x)\u00b5j(x)(\u2207Vi(x)\u2212\u2207Vj(x))(\u2207Vi(X)\u2212\u2207Vj(x))\u22a4\nthus the first statement follows. The second statement follows from noticing that (\u2207Vi(x) \u2212 \u2207Vj(x))(\u2207Vi(X)\u2212\u2207Vj(x))\u22a4 \u2ab0 0."
        },
        {
            "heading": "B.7 PROPERTIES OF SMOOTH AND STRONGLY LOG-CONCAVE DISTRIBUTION",
            "text": "We record the consequences of \u03b1-strongly log-concave and \u03b2-smooth that we will use. Lemma 5. Suppose \u00b5i is \u03b1-strongly log-concave and \u03b2-smooth then for \u03ba = \u03b2/\u03b1, ui = argminVi(x), D = 5 \u221a d \u03b1 ln(10\u03ba), and cz = d 2 ln\u03ba, we have\n1. For all x : ||\u22072Vi(x)||OP \u2264 \u03b2 and ||\u2207Vi(x)|| \u2264 \u03b2||x\u2212 ui||\n2. \u03b1||x\u2212 ui||2 \u2264 Vi(x) \u2264 \u03b2||x\u2212 ui||2. Consequently, for Zi = \u222b \u00b5i(x)dx, there exists z+ \u2264 z\u2212 with z+ = z\u2212 \u2212 cz s.t.\nexp(\u2212\u03b2||x\u2212 ui||2 \u2212 z\u2212) \u2264 \u00b5i(x) = Z\u22121i exp(\u2212Vi(x)) \u2264 exp(\u2212\u03b1||x\u2212 ui|| 2 \u2212 z+)\n3. Sub-gaussian concentration:\nP[||x\u2212 ui|| \u2265 D + t] \u2264 exp(\u2212\u03b1t2/4)\nBy Proposition 2, this implies that for all p\nE\u00b5i [||x\u2212 ui||p] \u2272p Dp.\n4. \u00b5i satisfies a LSI with constant CLS = 1\u03b1 .\nProof. This is due to Proposition 5 and Proposition 2, and the fact that \u2207Vi(ui) = 0 for ui = argminVi(x)."
        },
        {
            "heading": "B.8 BASIC MATHEMATICAL FACTS",
            "text": "Proposition 8. For any constant a > 0, b, p \u2208 N\u22650 f(x) = exp(\u2212ax \u2212 b)xp is decreasing on [p/a,+\u221e)\nProof. Let g(x) = log f(x) = \u2212ax\u2212 b+ p log x and observe that\ng\u2032(x) = \u2212a+ p/x \u2264 0\nwhen x \u2265 p/a, so the claim follows by integrating.\nProposition 9. Let P1, . . . , Pk, Q1, . . . , Qk be distributions s.t. dTV (Pi, Qi) \u2264 \u03f5i. Let \u03b11, \u00b7 \u00b7 \u00b7 , \u03b1k, \u03b21, \u00b7 \u00b7 \u00b7 , \u03b2k be s.t. \u03b1i, \u03b2i \u2265 0\u2200i and \u2211 i \u03b1i = \u2211 i \u03b2i = 1. Then\ndTV ( \u2211 i \u03b1iPi, \u2211 i \u03b1iQi) \u2264 \u2211 i \u03b1i\u03f5i\nand dTV ( \u2211 i \u03b1iQi, \u2211 i \u03b2iQi) \u2264 1 2 \u2211 i |\u03b1i \u2212 \u03b2i|\nProof. By triangle inequality\n2dTV ( \u2211 i \u03b1iPi, \u2211 i \u03b1iQi) = \u222b x\u2208\u2126 | \u2211 i \u03b1iPi(x)\u2212 \u2211 i \u03b1iQi(x)|dx\n\u2264 \u222b x\u2208\u2126 \u2211 i \u03b1i|Pi(x)\u2212Qi(x)|dx = 2 \u2211 i \u03b1idTV (Pi, Qi)\nSimilarly,\n2dTV ( \u2211 i \u03b1iQi, \u2211 i \u03b2iQi) = \u222b x\u2208\u2126 | \u2211 i \u03b1iQi(x)\u2212 \u2211 i \u03b2iQi(x)|dx\n\u2264 \u222b x\u2208\u2126 \u2211 i |\u03b1i \u2212 \u03b2i|Qi(x)dx = \u2211 i |\u03b1i \u2212 \u03b2i|"
        },
        {
            "heading": "C LOG-SOBOLEV INEQUALITY FOR WELL-CONNECTED MIXTURES",
            "text": "In this section, we show that the mixture \u2211\npi\u00b5i has a good log-Sobolev constant if its component distributions \u00b5i have high overlap. The below Theorem 4 corresponds to Theorem 2 of the main text. Definition 3. For distributions \u03bd, \u03c0, let \u03b4(\u03bd, \u03c0) = \u222b min{\u03bd(x), \u03c0(x)}dx be the overlap of \u03bd and \u03c0. Let \u03b4ij denote \u03b4(\u00b5i, \u00b5j). Note that\n1\u2212 \u03b4(\u03bd, \u03c0) = \u222b (\u03bd(x)\u2212min{\u03bd(x), \u03c0(x)})dx = \u222b x:\u03bd(x)\u2265\u03c0(x) (\u03bd(x)\u2212 \u03c0(x))dx = dTV (\u03bd, \u03c0).\nTheorem 4. Let G be the graph on I where {i, j} \u2208 E(G) iff \u00b5i, \u00b5j have high overlap i.e.\n\u03b4ij := \u222b min{\u00b5i(x), \u00b5j(x)}dx \u2265 \u03b4.\nSuppose G is connected. Let M \u2264 |I| be the diameter of G. The mixture distribution \u00b5 = \u2211\ni\u2208I pi\u00b5i has\n1. Poincare constant (Madras & Randall, 2002, Theorem 1.2)\nCPI(\u00b5) \u2264 4M\n\u03b4 max i\u2208I\nCPI(\u00b5i)\npi\n2. Log Sobolev constant\nCLS(\u00b5) \u2264 4MCLS(p)\n\u03b4 max i\nCLS(\u00b5i)\npi\nwhere for p\u2217 = mini pi, CLS(p) = 1 + log(p\u22121\u2217 ) is the log Sobolev constant of the instant mixing chain for p. Hence\nCLS(\u00b5) \u2264 C|I|,p\u2217\u03b4 \u22121 max\ni CLS(\u00b5i)\nwhere C|I|,p\u2217 = 4|I|2(1 + log(p\u22121\u2217 ))p\u22121\u2217 only depends on |I| and p\u2217\nBelow we fix a test function f s.t. E\u00b5[f2] \u2264 \u221e. Let\nCi,j = \u222b \u222b (f(x)\u2212 f(y))2\u00b5i(x)\u00b5j(x)dxdy. (9)\nLemma 6 (Triangle inequality).\nCi0,i\u2113 \u2264 \u2113 \u2113\u22121\u2211 j=0 Cij ,ij+1\nProof. Without loss of generality, assume ij = j for all j. Then\nCi0,i\u2113 = \u222b \u222b (f(x0)\u2212 f(x\u2113))2\u00b50(x0)\u00b5\u2113(x\u2113)dx0dx\u2113\n= \u222b x0 \u00b7 \u00b7 \u00b7 \u222b x\u2113 (f(x0)\u2212 f(x1) + \u00b7 \u00b7 \u00b7+ f(x\u2113\u22121)\u2212 f(x\u2113))2 \u2113\u220f\nj=0\n\u00b5j(xj)dx0dx1 . . . dx\u2113\n\u2264 \u222b x0 \u00b7 \u00b7 \u00b7 \u222b x\u2113 \u2113 \u2113\u22121\u2211 j=0 (f(xj)\u2212 f(xj+1))2  \u2113\u220f j=0 \u00b5j(xj)dx0dx1 . . . dx\u2113\n= \u2113 \u2113\u22121\u2211 j=0 \u222b xj \u222b xj+1 (f(xj)\u2212 f(xj+1))2\u00b5j(xj)\u00b5j+1xj+1dxjdxj+1 = \u2113 \u2113\u22121\u2211 j=0 Cj,j+1\nwhere the inequality is Holder\u2019s inequality.\nThe following comes from (Madras & Randall, 2002, Proof of Theorem 1.2) Lemma 7. If \u222b min{\u00b5i(x), \u00b5j(x)}dx \u2265 \u03b4 then\nCi,j \u2264 2(2\u2212 \u03b4)\n\u03b4 (Var\u00b5i(f) + Var\u00b5j (f)).\nProposition 10 (Variance decomposition).\n2Var\u00b5(f) = \u222b x \u222b y (f(x)\u2212 f(y))2\u00b5(x)\u00b5(y)dxdy\n= \u2211 i,j pipjCij\n= 2 \u2211 i p2i Var\u00b5i(f) + 2 \u2211 i<j pipjCij\nProof.\nVar\u00b5(f) = \u222b x \u00b5(x)f2(x)dx\u2212 (\u222b x \u00b5(x)f(x)dx )2 =\n\u222b x \u222b y f2(x)\u00b5(x)\u00b5(y)dxdy \u2212 \u222b x \u222b y \u00b5(x)f(x)\u00b5(y)f(y)dxdy\n= 1\n2 \u222b x \u222b y \u00b5(x)\u00b5(y)(f2(x) + f2(y)\u2212 2f(x)f(y))dxdy\n= 1\n2 \u222b x \u222b y \u00b5(x)\u00b5(y)(f(x)\u2212 f(y))2dxdy\nSince \u00b5(x) = \u2211\ni pi\u00b5i(x), we can further rewrite\n2Var\u00b5(f) = \u222b x \u222b y (f(x)\u2212 f(y))2 (\u2211\ni\npi\u00b5i(x) )(\u2211 i pi\u00b5i(y) ) dxdy\n= \u222b x \u222b y (f(x)\u2212 f(y))2 \u2211\ni,j\npipj\u00b5i(x)\u00b5j(y)  dxdy = \u2211 i,j pipj \u222b x \u222b y (f(x)\u2212 f(y))2\u00b5i(x)\u00b5j(y)dxdy\n= \u2211 i,j pipjCij\n= \u2211 i p2iCii + \u2211 i<j (Cij + Cji)\n= 2 \u2211 i p2i Var\u00b5i [f ] + 2 \u2211 ij Cij\nwhere the last equality is because Cij = Cji.\nLemma 8. For i, j let \u03b3ij be the shortest path in G from i to j and let |\u03b3ij | be its length i.e. the number of edges in that path. For u, v, let uv denote the edge {u, v} of G if it is in E(G). Let D = maxij |\u03b3ij | be the diameter of G. Then\u2211\ni<j\npipjCij \u2264 \u2211\nuv\u2208E(G) Cuv \u2211 i<j:uv\u2208\u03b3ij pipj |\u03b3ij |  \u2264 M(2\u2212 \u03b4)\n\u03b4\n\u2211 u Var\u00b5u(f)\n\u2264 M(2\u2212 \u03b4) \u03b4 \u2211 u CPI(\u00b5u)E\u00b5u [||\u2207f ||2]\nProof. \u2211 i<j pipjCij \u2264 \u2211 uv\u2208E(G) (Cuv \u2211 i<j:uv\u2208\u03b3ij pipj |\u03b3ij |) \u2264 M \u2211 uv\u2208E(G) (Cuv \u2211 i<j:uv\u2208\u03b3ij pipj)\nBy Lemma 7 and the definition of G, Cuv \u2264 2(2\u2212\u03b4)\u03b4 (Var\u00b5u(f) + Var\u00b5v (f)), thus\u2211 i<j pipjCij \u2264 2M(2\u2212 \u03b4) \u03b4 \u2211 uv\u2208E(G) (Var\u00b5u(f) + Var\u00b5v (f)) \u2211 i<j:uv\u2208\u03b3ij pipj  \u2264 2M(2\u2212 \u03b4)\n\u03b4\n\u2211 u Var\u00b5u(f) \u2211 v,uv\u2208E(G),i<j:uv\u2208\u03b3ij pipj  =\n2M(2\u2212 \u03b4) \u03b4 \u2211 u Var\u00b5u(f) \u2211 i<j:u\u2208\u03b3ij pipj  \u2264 M(2\u2212 \u03b4)\n\u03b4\n\u2211 u Var\u00b5u(f)\n\u2264 M(2\u2212 \u03b4) \u03b4 \u2211 u CPI(\u00b5u)E\u00b5u [||\u2207f ||2]\nProposition 11. For Ci,j be as in Eq. (9)\nCi,j = 1\n2 (Var\u00b5i(f) + Var\u00b5j (f) + (E\u00b5i [f ]\u2212 E\u00b5j [f ])2)\nProof. Let \u03bd = 12\u00b5i + 1 2\u00b5j . We write Var(\u03bd) in two ways. First, E\u03bd [f ] = 1 2 (E\u00b5i [f ] + E\u00b5j [f ]) thus\nV ar\u03bd(f) = E\u03bd [f2]\u2212 (E\u03bd [f ])2 = 1\n2 (E\u00b5i [f2] + E\u00b5j [f2])\u2212\n1 4 (E\u00b5i [f ] + E\u00b5j [f ]) 2\n= 1\n2 \u2211 k\u2208{i,j} (E\u00b5k[f2]\u2212 (E\u00b5k [f ])2) + 1 4 (E2\u00b5i [f ] + E 2 \u00b5j\u00b5j [f ]\u2212 2E\u00b5i [f ]E\u00b5j [f ])\n= 1\n2 (Var\u00b5i [f ] + Var\u00b5j [f ]) +\n1 4 (E\u00b5i [f ]\u2212 E\u00b5j [f ])2\nOn the other hand, by Proposition 10,\nVar\u03bd(f) = 1\n4 (Var\u00b5i(f) + Var\u00b5j (f)) +\n1 2 Cij\nRearranging terms gives the desired equation.\nProposition 12. Let g \u2261 f2. Let the projection of g on I be defined by g\u0304(i) = E\u00b5i [g]. Then Ent[f2] = \u2211 i\u2208I pi Ent\u00b5i [f 2] + Entp[g\u0304]\nProof.\nEnt[f2] = \u222b \u00b5(x)g(x) log g(x)dx\u2212 E\u00b5[g(X)] log(E\u00b5[g(x)])\n= \u222b (\u2211 i pi\u00b5i(x))g(x) log g(x)dx\u2212 E\u00b5[g(x)] log(E\u00b5[g(x)] )\n= \u2211 i pi (\u222b \u00b5i(x)g(x) log g(x)dx\u2212 E\u00b5i [g(x)] log(E\u00b5i [g(x)]) ) + \u2211 i pig\u0304(i) log g\u0304(i)\u2212 E\u00b5[g(x)] log(E\u00b5[g(x)])\nwhere in the last equality, we use the definition of g\u0304(i). Note that\nEi\u223cp[g\u0304(i)] = \u2211 i pig\u0304(i) = \u2211 i ( pi \u222b \u00b5i(x)g(x)dx ) = \u222b (\u2211 i pi\u00b5i ) g(x) = E\u00b5[g(x)]\nthus Ent[f2] = \u2211 i pi Ent\u00b5i [f 2] + Enti\u223cp[g\u0304(i)]\nProposition 13. Let g\u0304 be defined as in Proposition 12, then\n( \u221a g\u0304(i)\u2212 \u221a g\u0304(j))2 \u2264 Var\u00b5i [f2] + Var\u00b5j [f2] + (E\u00b5i [f ]\u2212 E\u00b5j [f ])2 = 2Cij\nProof. The first inequality comes from (Schlichting, 2019, Proof of Lemma 3) and the second part from Proposition 11.\nProposition 14 (Log Sobolev inequality for the instant mixing chain, (Diaconis & Saloff-Coste, 1996, Theorem A.1)). Let p be the distribution over I where the probability of sampling i \u2208 I is pi. For a function h : I \u2192 R\u22650\nEntp[h] \u2264 Cp Varp[ \u221a h]\nwith Cp = ln(4p\u22121\u2217 ) with p\u2217 = mini pi.\nLemma 9. With g\u0304 defined as in Proposition 12,\nVarp[ \u221a g\u0304] = \u2211 i<j pipj( \u221a g\u0304(i)\u2212 \u221a g\u0304(j))2 \u2264 2 \u2211 i<j pipjCij\nProof of Theorem 4 part 2. We can rewrite\nEnt\u00b5[f 2] = \u2211 i\u2208I pi Ent\u00b5i [f 2] + Entp[g\u0304]\n\u2264(1) \u2211 i piCLS(\u00b5i)E\u00b5i [||\u2207f ||2] + CLS(p)Varp( \u221a g\u0304)\n\u2264(2) \u2211 i piCLS(\u00b5i)E\u00b5i [||\u2207f ||2] + 2CLS(p) \u2211 i<j pipjCij\n\u2264(3) \u2211 i piCLS(\u00b5i)E\u00b5i [||\u2207f ||2] + 2M(2\u2212 \u03b4)CLS(p) \u03b4 \u2211 u CPI(\u00b5u)E\u00b5u [||\u2207f ||2]\n\u2264(4) 4MCLS(p)\n\u03b4 max i {CLS(\u00b5i) pi } \u2211 i piE\u00b5i [||\u2207f ||2]\n= 4MCLS(p)\n\u03b4 max i {CLS(\u00b5i) pi }E\u00b5[||\u2207f ||2]\nwhere (1) is due to definition of CLS(\u00b5i) and Proposition 14, (2) is due to Lemma 9, (3) is due to Lemma 8, and (4) is due to CPI(\u00b5i) \u2264 CLS(\u00b5i) and CLS(p),M \u2265 1.\nD INITIALIZATION ANALYSIS\nFor the continuous Langevin diffusion (X\u0304t)t\u22650 initialized at a bounded support distribution \u03bd0, we bound Rq(L(X\u0304h)||\u00b5) for some small h. Consequently, for \u00b5 being the stationary distribution of the Langevin diffusion and satisfying a LSI with constant CLS , we can use the fact that DKL(L(X\u0304t)||\u00b5) \u2264 exp(\u2212 t\u2212hCLS )DKL(L(X\u0304h)||\u00b5) to show that X\u0304t converges to \u00b5. Lemma 10 (Initialization bound). Let \u00b5 = \u2211 i\u2208I pi\u00b5i be a mixture of distributions \u00b5i \u221d exp(\u2212Vi(x)) which are \u03b1-strongly log concave and \u03b2-smooth. Let V (x) = \u2212 ln\u00b5(x). Let (\u03bd\u0304t)t\u2208[0,h], (\u03bdt)t\u2208[0,h] be respectively the distribution of the continuous Langevin diffusion and the LMC with step size h and score function \u2207V initialized at \u03b4x. Let G(x) := maxi ||\u2207Vi(x)||. Suppose h \u2264 1/(30\u03b2) then for q \u2208 (2, 110\u03b2h ),\nRq(\u03bd\u0304h||\u03bdh) \u2264 O(q2h(G2(x) + \u03b22dh)),\nRq\u22121(\u03bdh||\u00b5) \u2264 d\n2 ln((2\u03b1h)\u22121) + \u03b1\u22121G(x)\nand Rq/2(\u03bd\u0304h||\u00b5) \u2264 O(q2h(G2(x) + \u03b22dh)) + d\n2 ln((2\u03b1h)\u22121) + \u03b1\u22121G2(x)\nIf we replace \u03b4x with any \u03bd0 then by weak convexity of Renyi divergence (Lemma 2), the claim holds when we replace G(x) with G\u03bd = supx\u2208supp(\u03bd0) G(x).\nProposition 15. Let \u03bd = N (y, \u03c32I). If \u03c0(x) \u221d exp(\u2212W (x)) is \u03b1-strongly log concave and \u03b2Lipschitz and \u03c32\u03b2 \u2264 1/2 then\nR\u221e(\u03bd||\u03c0) \u2264 \u2212 d\n2 ln(\u03b1\u03c32) + ||\u2207W (y)||2/\u03b1\nProof. Since \u03b1I \u2aaf \u22072W (x) \u2aaf \u03b2I,\n\u27e8\u2207W (y), x\u2212 y\u27e9+ \u03b1||x\u2212 y||2/2 \u2264 W (x)\u2212W (y) \u2264 \u27e8\u2207W (y), x\u2212 y\u27e9+ \u03b2||x\u2212 y||2/2 By Proposition 5, we can upper bound the normalization factor Z = \u222b exp(\u2212W (x))dx by\nexp ( \u2212W (y) + ||\u2207W (y)|| 2\n2\u03b1\n) (2\u03c0\u03b1\u22121)d/2.\nFor x \u2208 Rd, using the upper bound on Z\n\u03bd(x)/\u03c0(x) = (2\u03c0\u03c32)\u2212d/2Z exp ( \u2212||x\u2212 y|| 2\n2\u03c32 +W (x) ) \u2264 (\u03b1\u03c32)\u2212d/2 exp ( W (x)\u2212W (y) + ||\u2207W (y)|| 2\n2\u03b1 \u2212 ||x\u2212 y|| 2 2\u03c32 ) = (\u03b1\u03c32)\u2212d/2 exp ||\u2207W (y)||2( 12\u03b1+ \u03c32 2(1\u2212\u03b2\u03c32) ) exp \u2212( \u221a (1\u2212\u03b2\u03c32)||x\u2212y||2 2\u03c32 \u2212 \u221a \u03c32||\u2207W (y)||2 2(1\u2212\u03b2\u03c32) )2)\n\u2264 (\u03b1\u03c32)\u2212d/2 exp ( ||\u2207W (y)||2 1\u2212 (\u03b2 \u2212 \u03b1)\u03c3 2\n2\u03b1(1\u2212 \u03b2\u03c32) ) \u2264 (\u03b1\u03c32)\u2212d/2 exp(||\u2207W (y)||2/\u03b1)\nwhere the last inequality follows from 1/2 \u2264 1\u2212 \u03b2\u03c32 \u2264 1\u2212 (\u03b2 \u2212 \u03b1)\u03c32 \u2264 1.\nProof of Lemma 10. We apply Theorem 3 with T = h, PT = (\u03bd\u0304t)t\u2208[0,h] and QT = (\u03bdt)t\u2208[0,h]. Note that, bPt = \u2212\u2207V (Xt) and b Q t = \u2212\u2207V (x). We first check that Novikov\u2019s condition Eq. (5) holds.\nEQT\n[ exp ( 1\n4 \u222b T 0 ||bPt \u2212 b Q t ||2dt )] = E [ exp ( 1 4 \u222b h 0 ||\u2207V (Xt)\u2212\u2207V (x)||2dt )] with (Xt)t\u2208[0,h] be the solution of the interpolated Langevin process i.e.\nXt \u2212 x = \u2212t\u2207V (x) + \u221a 2Bt\nBy \u03b2-Lipschitzness of \u2207Vj\n||\u2207Vj(Xt)|| \u2212 ||\u2207Vj(x)|| \u2264 \u03b2j ||Xt \u2212 x|| \u2264 \u03b2t||\u2207V (x)||+ \u03b2 \u221a 2||Bt||\nthus\n||\u2207V (Xt)|| \u2264 G(Xt) = max j\u2208I ||\u2207Vj(Xt)|| \u2264 G(x) + \u03b2tG(x) + \u03b2 \u221a 2 sup t\u2208[0,h] ||Bt||\n\u2264 1.1G(x) + \u03b2 \u221a 2 sup t\u2208[0,h] ||Bt||\nand \u222b h 0 ||\u2207V (Xt)\u2212\u2207V (x)||2dt \u2264 2 \u222b h 0 (||\u2207V (Xt)||2 + ||\u2207V (x)||2)dt\n\u2264 h[2(1.1G(x))2 + 4\u03b22 sup t\u2208[0,h] ||Bt||+G(x)2]\n\u2264 4hG2(x) + 4\u03b22h sup t\u2208[0,h] ||Bt||2\n(10)\nWe first prove the following.\nProposition 16. For any \u03bb < 18\u03b22h2 ,\nEQT\n[ exp ( \u03bb \u222b T 0 ||bPt \u2212 b Q t ||2dt )] \u2264 exp(4\u03bbhG2(x)) ( 1 + 8\u03bb\u03b22h2 1\u2212 8\u03bb\u03b22h2 )d .\nProof. By Proposition 1, for \u03bb \u2264 116\u03b22h2\nE [ exp ( \u03bb \u222b h 0 ||\u2207V (Xt)\u2212\u2207V (x)||2dt )] \u2264 E [ exp ( 4h\u03bbG2(x) + 4\u03bb\u03b22h sup t\u2208[0,h] ||Bt||2 )] \u2264 exp(4\u03bbhG2(x)) exp(6\u03b22h2d\u03bb)\nApply Proposition 16 with \u03bb = 1/4 gives\nEQT\n[ exp ( 1\n4 \u222b T 0 ||bPt \u2212 b Q t ||2dt )] = E [ exp ( 1 4 \u222b h 0 ||\u2207V (Xt)\u2212\u2207V (x)||2dt )] \u2264 exp ( hG2(x)) exp(1.5\u03b22h2d\u03bb ) < \u221e\nNext, let\nHt = \u222b t 0 1\u221a 2 \u27e8bPs \u2212 bQs , dBQs \u27e9 \u2212 1 4 \u222b t 0 ||bPs \u2212 bQs ||2ds\nthen dPtdQt = exp(Ht) and\ndHt = \u2212 1\n4 ||\u2207V (Xt)\u2212\u2207V (x)||2dt+ 1\u221a 2 \u27e8\u2212\u2207V (Xt) +\u2207V (x), dBQt \u27e9\nBy Ito\u2019s formula,\nd exp(qHt)\n= q2 \u2212 q\n4 exp(qHt)||\u2207V (Xt)\u2212\u2207V (x)||2 + q exp(qHt) 1\u221a 2 \u27e8\u2207V (x)\u2212\u2207V (Xt), dBQt \u27e9\nThus\nEQT [exp(qHT )]\u2212 1 = q2 \u2212 q\n4 E [\u222b h 0 exp(qHt)||\u2207V (Xt)\u2212\u2207V (x)||2dt ]\n\u2264 q 2\n4 \u222b h 0 \u221a E[exp(2qHt)] \u00b7 \u221a E[||\u2207V (Xt)\u2212\u2207V (x)||4]dt\nWe bound each term under the square root.\nE[||\u2207V (Xt)\u2212\u2207V (x)||4] \u2264 E[(1.1G(x) + \u03b2 \u221a 2 sup t\u2208[0,h] ||Bt||+G(x))4]\n\u2264 40G4(x) + 32\u03b24E[ sup t\u2208[0,h] ||Bt||4]\n\u2264 40G4(x) +O(\u03b24d2h2)\nBy Lemma 4 and Proposition 16, if q2 < 1100\u03b22h2 then\n(E[exp(2qHt)])2 \u2264 E [ exp ( 4q2 \u222b h 0 ||\u2207V (Xt)\u2212\u2207V (x)||2dt )] \u2264 exp(16q2hG2(x)) exp(24q2\u03b22h2) \u2264 exp(16q2hG2(x) + 72q2\u03b22h2d)\nSubstitute back in gives\nEQT [exp(qHT )]\u2212 1 \u2264 q2h\n4 (7G2(x) +O(\u03b22dh)) exp(4q2hG2(x) + 18q2\u03b22h2d)\nBy the data processing inequality\nRq(\u03bd\u0304h||\u03bdh) \u2264 Rq(PT ||QT ) = lnEQT [exp(qHT )]\nq \u2212 1 \u2264 ln ( 1 + q2h\n4 (7G2(x) + 6C\u03b22dh) exp(4q2hG2(x) + 18q2\u03b22h2d) ) \u2264 ln [( 1 + q2h\n4 (7G2(x) + 6C\u03b22dh\n) exp(4q2hG2(x) + 18q2\u03b22h2d) ] \u2264 ln ( 1 + q2h\n4 (7G2(x) + 6C\u03b22dh)) + (4q2hG2(x) + 18q2\u03b22h2d ) \u2264 6q2h(G2(x) + (3 + C/2)\u03b22dh)\nNow, note that \u03bdh = N (y, \u03c32I) with y = x \u2212 h\u2207V (x) and \u03c32 = 2h. Note that ||\u2207Vi(y)|| \u2264 ||\u2207Vi(x)||+ \u03b2||y \u2212 x|| \u2264 ||\u2207Vi(x)||+ \u03b2h||\u2207V (x)|| \u2264 1.1G(x). By Lemma 2 and Proposition 15\nR2q\u22121(\u03bdh||\u00b5) \u2264 max i\nR2q\u22121(\u03bdh||\u00b5i) \u2264 d\n2 ln((2\u03b1h)\u22121) + \u03b1\u22121 max i ||\u2207Vi(y)||2\n\u2264 d 2 ln((2\u03b1h)\u22121) + 2\u03b1\u22121G2(x)\nThe final statement follows from the weak triangle inequality (Lemma 1)."
        },
        {
            "heading": "E PERTURBATION ANALYSIS",
            "text": "In this section, we bound the drift ||X\u0304t \u2212 X\u0304kh|| for t \u2208 [kh, (k + 1)h] of the continuous Langevin diffusion X\u0304t. These bounds will be used to bound the mixing time of the continuous Langevin diffusion and to compare the discrete LMC with the continuous process via Girsanov\u2019s theorem.\nWe will consider subset S of I such that the components \u00b5i for i \u2208 S have modes that are close together. We record the properties of the mixture distribution \u00b5S (see Definition 2 for definition) and and its log density function VS = \u2212 log\u00b5S in Assumption 2. To be clear, we are defining this assumption as it is shared between multiple lemmas (and will be satisfied when we apply the lemmas), it is not a new assumption for the final result. Assumption 2 (Cluster assumption). We say a subset S of I satisfies the cluster assumption if there exists uS \u2208 Rd, AHess,1, AHess,0, Agrad,1, Agrad,0 s.t.\n1. ||\u22072VS(x)||OP \u2264 mini\u2208S AHess,1||x\u2212 ui||2 +AHess,0 2. ||\u2207VS(x)|| \u2264 Agrad,1||x\u2212 uS ||+Agrad,0.\nProposition 17. Suppose for all i \u2208 S, \u00b5i satisfies item 1 of Lemma 5. Let ui and D be as in Lemma 5 and suppose ||ui \u2212 uj || \u2264 L for i, j \u2208 S with L \u2265 10D. Then \u00b5S satisfies Assumption 2 with uS = p\u22121S \u2211 i\u2208S piui, Agrad,1 = \u03b2, Agrad,0 = \u03b2L, AHess,1 = 2\u03b2 2, AHess,0 = 2\u03b22L2.\nIn addition, if \u00b5i satisfies item 3 of Lemma 5 then P\u00b5S [||x\u2212 uS || \u2265 1.1L+ t] \u2264 exp(\u2212\u03b1t2/4). Proof. First, \u2200i \u2208 S : ||ui \u2212 uS || = p\u22121S \u2211 j\u2208S pj ||ui \u2212 uj || \u2264 L. By Proposition 6\npS\u2207VS(x) = \u2211 i\u2208S pi\u2207Vi(x) \u2264 \u2211 i\u2208S pi\u03b2||x\u2212 ui||\n\u2264 \u2211 i\u2208S pi\u03b2(||x\u2212 uS ||+ ||ui \u2212 uS ||) \u2264 pS(\u03b2||x\u2212 uS ||+ L)\nWe replace I with S and use the formula from Proposition 7. By Holder\u2019s inequality ||\u2207Vi(x)\u2212\u2207Vj(x)||2 \u2264 4max\nk\u2208S ||\u2207Vk(x)||2 \u2264 4\u03b22 max k\u2208S ||x\u2212 uk||2 \u2264 8\u03b22 min k\u2208S (||x\u2212 uk||2 + L2)\nNext, for p\u0303i = pi/pS , we have\u2211 i,j\u2208S p\u0303ip\u0303j\u00b5i(x)\u00b5j(x) = (\u2211 i\u2208S p\u0303i\u00b5i(x) )2 = \u00b52C(x)\nthus \u03b2I \u2ab0 \u22072VC(x) \u2ab0 0\u2212 I max\ni,j\u2208S ||\u2207Vi(x)\u2212\u2207Vj(x)||2/4 \u2ab0 \u22122I\u03b22 min k\u2208S (||x\u2212 uk||2 + L2).\nFor D\u0303 = D + L \u2264 1.1L and \u03b3 = 2\u03b1 . P\u00b5S [||Z\u0304 \u2212 uS || \u2265 D\u0303 + \u221a \u03b3 ln(1/\u03b7)) = p\u22121S \u2211 i\u2208S pi\u00b5i(Z\u0304 : ||Z\u0304 \u2212 uS || \u2265 D\u0303 + \u221a \u03b3 ln(1/\u03b7))\n\u2264 p\u22121S \u2211 i\u2208S pi\u00b5i(Z\u0304 : ||Z\u0304 \u2212 ui|| \u2265 D + \u221a \u03b3 ln(1/\u03b7))\n\u2264 p\u22121S \u2211 i\u2208S pi\u03b7 = \u03b7\nwhere first inequality is due to ||ui \u2212 uS || \u2264 L for all i \u2208 S.\nProposition 18. Suppose S \u2286 I satisfies item 1 and item 2 of Assumption 2. Let (Z\u0304t)t\u22650 be the continuous Langevin diffusion with score \u2207VS initialized at Z\u03040 \u223c \u03bd0 then for t \u2208 [kh, (k + 1)h)\nE[||\u2207V (Z\u0304kh)\u2212\u2207V (Z\u0304t)||2] \u2272 \u221a\nE[A4Hess,1(||Z\u0304kh \u2212 uS ||8 + ||Z\u0304t \u2212 uS ||8) +A4Hess,0]\n\u00d7 \u221a (t\u2212 kh)3 \u222b t kh (A4grad,1E[||Z\u0304s \u2212 uS ||4] +A4grad,0)ds+ d2(t\u2212 kh)2\nProof. By the mean value inequality\n||\u2207VS(Z\u0304kh)\u2212\u2207VS(Z\u0304t)||2 \u2264 ||Z\u0304kh \u2212 Z\u0304t|| max y=\u03b7Z\u0304kh+(1\u2212\u03b7)Z\u0304t,\u03b7\u2208[0,1] ||\u22072VS(y)||\nBy item 1 of Assumption 2, the fact that y = \u03b7Z\u0304kh + (1\u2212 \u03b7)Z\u0304t and Holder\u2019s inequality\n||\u22072VS(y)||OP \u2264 AHess,1||y \u2212 uS ||2 +AHess,0 \u2264 AHess,1(||Z\u0304kh \u2212 uS ||2 + ||Z\u0304t \u2212 uS ||2) +AHess,0\nand so\nE[||\u2207VS(Z\u0304kh)\u2212\u2207VS(Z\u0304t)||2] \u2264E [( AHess,1(||Z\u0304kh \u2212 uS ||2 + ||Z\u0304t \u2212 uS ||2) +AHess,0 )2 \u00b7 || \u2212 \u222b t kh \u2207VS(Z\u0304s)ds+ \u221a 2Bt\u2212kh||2 ] \u2264 \u221a E ( AHess,1(||X\u0304kh \u2212 uS ||2 + ||X\u0304t \u2212 uS ||2) +AHess,0\n)4 \u00b7 \u221a E|| \u2212\n\u222b t kh \u2207VS(Z\u0304s)ds+ \u221a 2Bt\u2212kh||4.\nBy item 2 of Assumption 2 and Holder\u2019s inequality, for p = O(1)\nE[|| \u2212 \u222b t kh \u2207VS(Z\u0304s)ds+ \u221a 2Bt\u2212kh||2p]\n\u2272 E[(t\u2212 kh)2p\u22121 \u222b t kh ||\u2207VS(Z\u0304s)||2pds] + E[||Bt\u2212kh||2p]\n\u2272 (t\u2212 kh)2p\u22121 \u222b t kh (A2pgrad,1||Z\u0304s \u2212 uS || 2p +A2pgrad,0)ds+ (d(t\u2212 kh)) p\nThe desired result follows from p = 4.\nProposition 19. Suppose S \u2286 I satisfies item 2 of Assumption 2. Let (Z\u0304t)t\u22650 be the continuous Langevin diffusion wrt \u00b5S initialized at \u03bd0. Suppose h \u2264 12Agrad,1 and supk\u2208[0,N\u22121]\u2229N ||Z\u0304kh\u2212uS || \u2264 D then\nsup k\u2208[0,N\u22121]\u2229N,t\u2208[0,h]\n||Z\u0304kh+t \u2212 Z\u0304kh|| \u2264 2h(Agrad,0 +Agrad,1||Z\u0304kh \u2212 uS ||) + \u221a 48dh ln 6N\n\u03b7\nthus with probability \u2265 1\u2212 \u03b7\nsup k\u2208[0,N\u22121]\u2229N,t\u2208[0,h]\n||Z\u0304kh+t \u2212 uS || \u2264 2hAgrad,0 + 2D + \u221a 48dh ln 6N\n\u03b7\nProof. The proof is identical to (Chewi et al., 2021, Lemma 24). By triangle inequality, ||Z\u0304kh+t \u2212 Z\u0304kh||\n\u2264 \u222b t 0 ||\u2207VS(Z\u0304kh+r)||dr + \u221a 2||Bkh+t \u2212Bkh||\n\u2264 hAgrad,0 +Agrad,1 \u222b t 0 ||Z\u0304kh+r \u2212 uS ||dr + \u221a 2||Bkh+t \u2212Bkh||\n\u2264 hAgrad,0 +Agrad,1 ( h||Z\u0304kh \u2212 uS ||+ \u222b t 0 ||Z\u0304kh+r \u2212 Z\u0304kh||dr ) + \u221a 2||Bkh+t \u2212Bkh||\nwhere we use item 2 of Assumption 2 in the second inequality. Gronwall\u2019s inequality then implies ||Z\u0304kh+t \u2212 Z\u0304kh||\n\u2264 ( h(Agrad,0 +Agrad,1||Z\u0304kh \u2212 uS ||) +\n\u221a 2 sup t\u2208[0,h] ||Bkh+t \u2212Bkh||\n) exp(hAgrad,1)\n\u2264 2h(Agrad,0 +Agrad,1||Z\u0304kh \u2212 uS ||) + \u221a 8 sup t\u2208[0,h] ||Bkh+t \u2212Bkh||\nas long as h \u2264 12Agrad,1 .\nThus by triangle inequality, ||Z\u0304kh+t \u2212 uS || \u2264 ||Z\u0304kh \u2212 uS ||+ ||Z\u0304kh+t \u2212 Z\u0304kh||\n\u2264 2hAgrad,0 + ||Z\u0304kh \u2212 uS ||(2hAgrad,1 + 1) + \u221a 8 sup t\u2208[0,h] ||Bkh+t \u2212Bkh||\nBy union bounds and concentration for Brownian motion (see (Chewi et al., 2021, Lemma 32)), with probability 1\u2212 \u03b7,\nsup k\u2208[0,N\u22121]\u2229N,t\u2208[0,h]\n||Bkh+t \u2212Bkh|| \u2264 \u221a 6dh ln 6N\n\u03b7\nthus\nsup k\u2208[0,N\u22121]\u2229N,t\u2208[0,h]\n||Z\u0304kh+t \u2212 uS || \u2264 2hAgrad,0 + 2D + \u221a 48dh ln 6N\n\u03b7"
        },
        {
            "heading": "F ANALYSIS OF CONTINUOUS-TIME DIFFUSION",
            "text": "In this section, we analyze an idealized version of the final LMC chain: we assume knowledge of the exact score function and run the continuous time Langevin diffusion. First in Lemma 11 below, we prove that when the diffusion is initialized from a point, it converges in a certain amount of time to a sample from a mixture distribution corresponding to the clusters near the initialization. Then in Theorem 5 we deduce the analogue of our main result for the idealized process: the diffusion started from samples converges to the true distribution. Definition 4. For S \u2286 I and x \u2208 Rd, let imax,S(x) = argmaxi\u2208S \u00b5i(x). We break ties in lexicographic order of i i.e. we let imax,S(x) be the maximum index among all indices i s.t. \u00b5i(x) = maxj\u2208S \u00b5j(x). Lemma 11. Fix \u03f5TV , \u03c4 \u2208 (0, 1/2), \u03b4 \u2208 (0, 1]. Fix S \u2286 I. Let p\u0304i = pip\u22121S and recall that \u00b5S =\u2211 i\u2208S p\u0304i\u00b5i. Let p\u2217 = mini\u2208S p\u0304i. Note that p\u2217 \u2265 mini\u2208I pi. Recall that |I| = K.\nSuppose for i \u2208 S, \u00b5i are \u03b1-strongly log-concave and \u03b2-smooth with \u03b2 \u2265 1. Let ui = argminx Vi(x) and D \u2265 5 \u221a d \u03b1 be as defined in Lemma 5. Suppose there exists L \u2265 10D such that for any i, j \u2208 S, ||ui \u2212 uj || \u2264 L.\nLet G\u03b4 := G\u03b4(S,E) be the graph on S with an edge between i, j iff \u03b4ij \u2264 \u03b4. Let\nT = 2Cp\u2217,K \u03b4\u03b1\n( ln ( \u03b22L\n\u03b1\n) + ln ln \u03c4\u22121 + 2 ln \u03f5\u0303\u22121TV ) .\nSuppose for all i, j \u2208 S which are not in the same connected component of G\u03b4 , \u03b4ij \u2264 \u03b4\u2032 with\n\u03b4\u2032 = \u03b43/2\u03b13/2p 5/2 \u2217 \u03f5 2 TV \u03c4\n105K5d(\u03b2L)3 ln3/2(p\u22121\u2217 ) ln 3/2 \u03b22L\u03f5\n\u22121 TV ln \u03c4 \u22121\n\u03b1 ln 2.51 16d(\u03b2L)2\n\u03f5TV \u03c4\u03b4\u03b1\nFor x \u2208 Rd, let (X\u0304\u03b4xt )t\u22650 denote the continuous Langevin diffusion with score \u2207VS initialized at \u03b4x, and let Cmax(x) be the unique connected component of G\u03b4 containing imax,S(x) = argmaxi\u2208S \u00b5i(x) as defined in Definition 4. Then\nPx\u223c\u00b5S [dTV (L(X\u0304 \u03b4x t |x), \u00b5Cmax(x)) \u2264 \u03f5TV ] \u2265 1\u2212 \u03c4\nFrom the above lemma, we can deduce the following theorem. (The proof of the lemma is deferred until after the proof of the theorem.) In this result, the reader can consider simply the case S = I; the flexibility to pick a subset of indices is allowed for convenience later. Theorem 5. Fix \u03f5TV , \u03c4 \u2208 (0, 1/2). Fix S \u2286 I. Let p\u0304i = pip\u22121S and recall that \u00b5S = \u2211 i\u2208S p\u0304i\u00b5i. Let p\u2217 = mini\u2208S p\u0304i. Note that p\u2217 \u2265 mini\u2208I pi. Recall that |I| = K. Suppose for i \u2208 S, \u00b5i are \u03b1-strongly log-concave and \u03b2-smooth with \u03b2 \u2265 1. Let ui = argminx Vi(x) and D \u2265 5 \u221a d \u03b1 be as defined in Lemma 5. Suppose there exists L \u2265 10D such that for any i, j \u2208 S, ||ui\u2212uj || \u2264 L. Let Usample be a set of M i.i.d. samples from \u00b5S and \u03bdsample be the uniform distribution over Usample. Let (X\u0304 \u03bdsample t )t\u22650 be the continuous Langevin diffusion with score \u2207VS initialized at \u03bdsample. Let\n\u0393\u0303 = p 7/2 \u2217 \u03f5 3 TV \u03b1 3/2\n108d(\u03b2L)3exp(K) ln3/2(p\u22121\u2217 ) ln 5 16d(\u03b2L)2\n\u03f5TV \u03c4\u03b1\n,"
        },
        {
            "heading": "If M \u2265 600(\u03f52TV p\u2217)\u22121K2 log(K\u03c4\u22121) and",
            "text": "T \u2265 \u0398 ( \u03b1\u22121K2p\u22121\u2217 ln(10p \u22121 \u2217 )\u0393\u0303 \u22122((3/2)K\u22121\u22121) )\nthen PUsample [dTV (L(X\u0304 \u03bdsample T |Usample), \u00b5S) \u2264 \u03f5TV ] \u2265 1\u2212 \u03c4\nRemark 6. Note that after fixing Usample, \u00b5\u0302 Usample S := L(X\u0304 \u03bdsample T |Usample) is a function of Usample and Brownian motions (Bt)t\u2208[0,T ]. Each run of the Langevin diffusion produces a sample from \u00b5\u0302 Usample S by choosing/sampling a value for the Brownian motions, thus we can produce as many samples as desired from \u00b5\u0302UsampleS , while Theorem 5 guarantees that \u00b5\u0302 Usample S is approximately close to \u00b5S in total variation distance for a typical set of samples Usample. Proof of Theorem 5. Let p\u0304i = pip\u22121S then \u00b5S = \u2211 i\u2208S p\u0304i\u00b5i. For C \u2286 S, let p\u0304C = \u2211 i\u2208C p\u0304i.\nLet \u03f5\u0303TV = \u03f5TV9K and \u03c4\u0303 = p\u2217\u03f5TV 9K \u2264 min{ \u03f5TV 9K2 , p\u2217/3}. Define the sequence 1 = \u03b40 > \u03b41 > \u00b7 \u00b7 \u00b7 > \u03b4K inductively as follow:\n\u03b4s+1 = \u03b4 3/2 s \u03b13/2p 5/2 \u2217 \u03f5\u0303 2 TV \u03c4\u0303\n105K5d(\u03b2L)3 ln3/2(p\u22121\u2217 ) ln 3/2 \u03b22L\u03f5\u0303\n\u22121 TV ln \u03c4\u0303 \u22121\n\u03b1 ln 2.51 16d(\u03b2L)2\n\u03f5\u0303TV \u03c4\u0303\u03b4s\u03b1\n\u2265 \u03b4 3/2 s \u03b13/2p 7/2 \u2217 \u03f5 3 TV\n108K8d(\u03b2L)3 ln3/2(p\u22121\u2217 ) ln 3/2 \u03b22L\u03f5\n\u22121 TV K\n\u03b1 ln 2.51 16d(\u03b2L)2K\n\u03f5TV \u03b4s\u03b1\nLet Gs := G\u03b4(S,E) be the graph on S with an edge between i, j iff \u03b4ij \u2264 \u03b4s. Fix one such s s.t. s \u2264 K \u2212 2. Suppose \u03b4ij \u2264 \u03b4s+1 for all i, j not in the same connected component of Gs, y then Lemma 11 applies. Let the connected components of Gs be Cs1 , . . . , Csm. For x \u2208 Rd, let Csmax(x) be the unique connected component of Gs containing imax,S(x) and let (X\u0304\u03b4xt )t\u22650 denote the continuous Langevin diffusion with score \u2207V initialized at \u03b4x, then for Ts = 2Cp\u2217,K\u03b4s\u03b1 (ln \u03b22L \u03b1 + ln ln \u03c4\u0303 \u22121 + 2 ln \u03f5\u0303\u22121TV ),\nPx\u223c\u00b5S [dTV (X\u0304 \u03b4x Ts , \u00b5Csmax(x)) \u2264 \u03f5\u0303TV ] \u2265 1\u2212 \u03c4\u0303\nand by Proposition 21, Px\u223c\u00b5S [imax,S(x) \u2208 Csr ] \u2265 (1\u2212 \u03b4s+1)p\u0304Csr . It is easy to see that \u03b4s+1 \u2264 \u03f5\u0303TV /3. By Proposition 23, as long as M \u2265 600(\u03f52TV p\u2217)\u22121K2(logK + log \u03c4\u22121)\nPUsample [dTV (L(X\u0304 \u03bdsample Ts |Usample), \u00b5S) \u2264 \u03f5TV ] \u2265 1\u2212 \u03c4 Since \u00b5S is the stationary distribution of the continuous Langevin with score function \u2207VS , for any T \u2265 TK\u22121 \u2265 Ts, dTV (L(X\u0304 \u03bdsample T |Usample), \u00b5S) \u2264 dTV (L(X\u0304 \u03bdsample Ts |Usample), \u00b5S) thus\nPUsample [dTV (L(X\u0304 \u03bdsample Ts |Usample), \u00b5S) \u2264 \u03f5TV ] \u2265 1\u2212 \u03c4.\nOn the other hand, suppose for all s \u2208 [0,K \u2212 2] \u2229 N, there exists i, j not in the same connected component of Gs s.t. \u03b4ij > \u03b4s+1, then Gs+1 has one fewer connected components than Gs. Thus GK\u22121 is connected then \u00b5 has LSI constant \u221d \u03b4\u22121K\u22121, thus Lemma 11 apply with \u03b4 = \u03b4K\u22121 and Proposition 21 apply with \u03b4\u2032 = 0. For T \u2265 TK\u22121,\nPUsample [dTV (L(X\u0304 \u03bdsample T |Usample), \u00b5S) \u2264 \u03f5TV ] \u2265 1\u2212 \u03c4.\nLet \u0393 = p 7/2 \u2217 \u03f5 3 TV \u03b1 3/2 108K8d(\u03b2L)3 . If we ignore log terms, then \u03b4s+1 = \u03b4 3/2 s \u0393 thus \u03b4s \u2248 \u03931+3/2+\u00b7\u00b7\u00b7+(3/2) s\u22121 = \u03932((3/2) s\u22121).To get the correct bound for \u03b4s and Ts, we can let\n\u03931 = p 7/2 \u2217 \u03f5 3 TV \u03b1 3/2\n8000d(\u03b2L)3exp(K) ln3/2(p\u22121\u2217 ) ln 4.5 16d(\u03b2L)2\n\u03f5TV \u03c4\u03b1\n\u2264 \u0393\nthen we can inductively prove \u03b4s \u2265 \u03932((3/2) s\u22121) 1 and thus get the bound on Ts i.e.\nTs \u2264 \u0398(\u03b1\u22121K2p\u22121\u2217 ln(10p\u22121\u2217 ) ln( \u03b22L\u03f5TV \u03b1p\u2217K )\u0393 \u22122((3/2)s\u22121) 1 )\n= \u0398(\u03b1\u22121K2p\u22121\u2217 ln(10p \u22121 \u2217 )\u0393\u0303 \u22122((3/2)s\u22121))\nwith \u0393\u0303 = p 7/2 \u2217 \u03f5 3 TV \u03b1 3/2\n108d(\u03b2L)3exp(K) ln3/2(p\u22121\u2217 ) ln 5 16d(\u03b2L)\n2\n\u03f5TV \u03c4\u03b1\n.\nProof of Lemma 11. Let (X\u0304t) denote the continuous Langevin with score \u2207VS initialized at \u00b5S . Since \u00b5S is the stationary distribution of continuous the Langevin with score \u2207VS , the law L(X\u0304t) of X\u0304t is \u00b5S at all time t. Let \u03b7 = \u03c4\u03f5TV /2. Let h > 0, \u03b3 \u2208 (0, 1) to be chosen later. Let C be the partition of S consisting of connected components of the graph G\u03b4, and BS,C,\u03b3 be defined as in Definition 5. Suppose \u03b4\u2032, \u03b3 satisfies K2\u03b3\u22121\u03b4\u2032 \u00d7 T/h \u2264 \u03b7/2, then by Lemma 12, \u00b5S(BS,C,\u03b3)N \u2264 K2\u03b3\u22121\u03b4\u2032 \u00d7 T/h \u2264 \u03b7/2 Since the law of X\u0304kh is \u00b5S , we can bound ||X\u0304kh \u2212 uS || using sub-Gaussian concentration of \u00b5S (due to Proposition 17). By the union bound, with probability 1\u2212 \u03b7, the event Ediscrete happens where Ediscrete is defined by: \u2200k \u2208 [0, N \u2212 1] \u2229 N : ||X\u0304kh \u2212 uS || \u2264 2L+ \u221a 64 \u03b1 ln 16N \u03b7 and X\u0304kh \u0338\u2208 BS,C,\u03b3 .\nSince \u00b5S = Ex\u223c\u00b5S [\u03b4x] and \u00b5S and \u03b4x are the initial distribution of X\u0304t and X\u0304 \u03b4x t respectively, so L(X\u0304kh) = Ex\u223c\u00b5S [L(X\u0304 \u03b4x kh|x)] where L(X) denote the law of the random variable X. Thus, let\nL\u0303 := 2L+ \u221a\n64 \u03b1 ln 16N \u03b7 and Gx is the event\nPFt [\u2200k \u2208 [0, N \u2212 1] \u2229 N : ||X\u0304 \u03b4x kh \u2212 uS || \u2264 L\u0303 \u2227 X\u0304 \u03b4x kh \u0338\u2208 BS,C,\u03b3 ] \u2265 1\u2212 \u03f5TV/10\nwhere the probability is taken over the randomness of the Brownian motions, then Px\u223c\u00b5[Gx] \u2265 1\u2212\u03c4/2 Fix x, let C = Cmax(x) and suppose Gx holds. Suppose h satisfies the precondition of Proposition 26, then with probability \u2265 1\u2212 \u03f5TV/5,\nsup t\u2208[0,T ]\n||\u2207VS(X\u0304\u03b4xt )\u2212\u2207VCmax(x)(X\u0304 \u03b4x t )|| \u2264 \u03f5score,1 := 36p\u22121\u2217 \u03b3\u03b2L\u0303\nthus X\u0304\u03b4xt \u0338\u2208 B for all t \u2208 [0, T ], where B is the \"bad\" set defined by B = {z \u2208 Rd : ||\u2207VS(z) \u2212 \u2207VC(z)|| > \u03f5score,1}. Let \u03bd0 be the distribution of X\u0304\u03b4xh\u2032 for some h\u2032 \u2264 1/(2\u03b2). Let Ginit,x be the event\nthat ||x \u2212 uS || \u2264 L1 := 2L + log(10/\u03c4) then Px\u223c\u00b5[Ginit,x] \u2265 1 \u2212 \u03c4/10. Suppose Ginit,x happens. Then GS(x) = maxi\u2208S ||\u2207Vi(x)|| \u2272 \u03b2L1. Set h\u2032 = min{ 1\u03b2L\u0303 , 1 \u03b2d} then by Lemma 10,\nDKL(\u03bd0||\u00b5C) \u2272 d lnL1 + \u03b1\u22121\u03b22L21 Pick T = 2Cp\u2217,K\u03b4\u03b1 (ln \u03b22L \u03b1 + ln ln \u03c4 \u22121) then T \u2212 h\u2032 \u2265 Tprocess := Cp\u2217,K\u03b4\u03b1 (lnDKL(\u03bd0||\u00b5S) + 2 ln \u03f5 \u22121 TV )\nLet (Z\u0304\u03bd0t )t\u22650 be the continuous Langevin initialized at \u03bd0 with score s\u221e defined by\ns\u221e(z) = { \u2207VS(z) if x \u0338\u2208 B \u2207VC(z) if x \u2208 B\nthen supz\u2208Rd ||s\u221e(z)\u2212\u2207VC(z)||2 \u2264 \u03f52score,1. Note that if Gx holds then X\u0304 \u03b4x t+h\u2032 \u0338\u2208 B\u2200t \u2208 [0, T \u2212h\u2032] and Z\u0304\u03bd0t = X\u0304 \u03b4x t+h\u2032\u2200t \u2208 [0, T \u2212 h\u2032] thus\ndTV (X\u0304 \u03b4x t , Z\u0304 \u03bd0 T\u2212h\u2032) \u2264 \u03f5TV /5\nProposition 20 gives\ndTV (L(Z\u0304\u03bd0T\u2212h\u2032 |x), \u00b5C) \u2264 \u03f5score,1 \u221a T/2 + \u03f5TV /5\nSet \u03b3 = p\u2217\u03f5TV 18\u03b2L\u0303 \u221a T then \u03f5score,1 = 18p\u22121\u2217 \u03b3\u03b2L\u0303 \u2264 \u03f5TV\u221aT then by triangle inequality\ndTV (L(X\u0304\u03b4xt |x), \u00b5C) \u2264 \u03f5TV . This holds conditioned on Gx and Ginit,x both happen, thus by union bound\nPx\u223c\u00b5[dTV (L(X\u0304\u03b4xt |x), \u00b5Cmax(x)) \u2264 \u03f5TV ] \u2265 1\u2212 \u03c4 Plug in T, \u03b3 and set\nh = 1\n2000d(\u03b2L)2 ln2 16d(\u03b2L) 2T\n\u03f5TV \u03c4\nthen h ln(1/h) \u2264 12000d(\u03b2L)2 and h ln 2(1/h) = 11000(\u03b22/\u03b1) and h \u2264 1 100(\u03b22/\u03b1) ln2(16T/\u03b7) . Hence h satisfies the precondition of Proposition 26.\nFinally, since L\u0303 \u2264 L \u221a ln 16Th\u03b7 \u2264 2L \u221a ln(\u03b2L\u03f5\u22121TV \u03c4 \u22121T ), thus with\n\u03b4\u2032 \u2264 \u03b43/2\u03b13/2p 5/2 \u2217 \u03f5 2 TV \u03c4 ln\n3/2 \u03b2 2L\u03f5\u22121TV ln \u03c4 \u22121\n\u03b1\n105K5d(\u03b2L)3 ln(p\u22121\u2217 ) ln 2.51 16d(\u03b2L)2\n\u03f5TV \u03c4\u03b4\u03b1\n\u2264 p\u2217\u03f5 2 TV \u03c4\n105K2T 3/2d(\u03b2L)3 ln2.51 16d(\u03b2L) 2T\n\u03f5TV \u03c4\nthe precondition\nK2\u03b4\u2032\u03b3\u22121 \u00d7 T/h = K2\u03b4\u2032 \u00d7 18\u03b2L\u0303 \u221a T\np\u2217\u03f5TV \u00d7 T/h\n\u2264 \u03b4\u2032 \u00d7 36K2\u03b2LT 3/2\n\u221a ln(\u03b2L\u03f5\u22121TV \u03c4 \u22121T )\np\u2217\u03f5TV h\n\u2264 \u03b7/2 = \u03f5TV \u03c4/4\nholds, so we are done.\nProposition 20 (Continuous chain with score estimation with L\u221e error bound). Fix C \u2286 I. Let (Z\u0304t)t\u22650 and X\u0304t be the continuous Langevin diffusion with score functions \u2207VC and s respectively and both (Z\u0304t) and (X\u0304t) are initialized at \u03bd0. Suppose supx\u2208Rd ||s(x)\u2212\u2207VC(x)||2 \u2264 \u03f52score,1 then\n2dTV (X\u0304T , Z\u0304T ) 2 \u2264 DKL(X\u0304T ||Z\u0304T ) \u2264 E [\u222b T 0 ||s(Z\u0304t)\u2212\u2207VC(Z\u0304t)||2dt ] \u2264 \u03f52score,1T\nSuppose \u00b5S has log Sobolev constant CLS and T \u2265 CLS(log(2DKL(\u03bd0||\u00b5S)) + 2 log \u03f5\u22121TV ) dTV (L(X\u0304T ), \u00b5C) \u2264 dTV (X\u0304T , Z\u0304T ) + dTV (L(Z\u0304T ), \u00b5C) \u2264 \u03f5score,1 \u221a T/2 + \u03f5TV /2\nProof. Clearly, by the assumption on s, E[ \u222b T 0 ||s(Z\u0304t)\u2212\u2207VC(Z\u0304t)||2dt] \u2264 \u222b T 0 \u03f52score,1dt = \u03f5 2 score,1T. The first statement thus follows from Girsanov and the approximation argument in (Chen et al., 2023, Lemma 9) and Pinsker\u2019s inequality. Next, since \u00b5C has LSI constant CLS , with this choice of T,\nDKL(L(Z\u0304T )||\u00b5C) \u2264 DKL(\u03bd0||\u00b5S) exp(\u2212 T\nCLS ) \u2264 \u03f52TV /2\nand the second statement follows from Pinsker\u2019s inequality and triangle inequality for TV distance.\nWe need these propositions to go from Lemma 11 to Theorem 5 Proposition 21. Suppose \u00b5 = \u2211 i\u2208I pi\u00b5i. Fix a set C \u2286 I. If the overlap between \u00b5i, \u00b5j for i \u2208 C and j \u0338\u2208 C is \u2264 \u03b4\u2032 for all such i, j then \u00b5({x : imax(x) \u2208 C}) \u2265 pC(1\u2212 \u03b4\u2032|I|)\nTo remove dependency on p\u2217, we will use the following modified version of Proposition 21 Proposition 22. Fix C,C\u2217 \u2286 I s.t. C \u2229 C\u2217 = \u2205. Let I \u2032 = I \\ C\u2217. If for i \u2208 C, j \u2208 I \u2032 \\ C, the overlap between \u00b5i and \u00b5j is \u2264 \u03b4\u2032 then for imax,I\u2032(x) = argmaxi\u2208I\u2032 \u00b5i(x)\n\u00b5I({x : imax,I\u2032(x) \u2208 C}) \u2265 pC(1\u2212 \u03b4\u2032|I|)\nProof of Propositions 21 and 22. We first prove Proposition 21. For i \u2208 C, j \u0338\u2208 C \u00b5i({x : \u00b5i(x) \u2264 \u00b5j(x)}) = \u222b x:\u00b5i(x)\u2264\u00b5j(x) \u00b5i(x)dx\n= \u222b x:\u00b5i(x)\u2264\u00b5j(x) min{\u00b5i(x), \u00b5j(x)}dx\n\u2264 \u222b min{\u00b5i(x), \u00b5j(x)}dx \u2264 \u03b4\u2032\nBy union bound, for i \u2208 C \u00b5i({x | \u2203j \u0338\u2208 C : \u00b5i(x) \u2264 \u00b5j(x)}) \u2264 \u03b4\u2032|I|\nLet \u039b = {x : imax(x) \u2208 C}. If \u2200j \u0338\u2208 C : \u00b5i(x) > \u00b5j(x) then imax(x) \u2208 C. Thus \u039bi := {x : \u00b5i(x) > \u00b5j(x)\u2200j \u0338\u2208 C} \u2286 \u039b and \u00b5i(\u039bi) = 1\u2212 \u00b5i({x|\u2203j \u0338\u2208 C : \u00b5i(x) \u2264 \u00b5j(x)} \u2265 1\u2212 \u03b4\u2032|I|. Since \u00b5(x) \u2265 \u2211 i\u2208C pi\u00b5i(x)\n\u00b5({x : imax(x) \u2208 C}) = \u222b x\u2208\u039b \u00b5(x)dx \u2265 \u222b x\u2208\u039b \u2211 i\u2208C pi\u00b5i(x)dx = \u2211 i\u2208C pi\u00b5i(\u039b)\n\u2265 \u2211 i\u2208C pi\u00b5i(\u039bi) \u2265 \u2211 i\u2208C pi(1\u2212 \u03b4\u2032|I|) = pC(1\u2212 \u03b4\u2032|I|)\nThe proof of Proposition 22 is identical, except we will consider i \u2208 C, j \u2208 I \u2032 \\ C and argue that \u00b5i(x : \u00b5i(x) \u2264 \u00b5j(x)) \u2264 \u03b4\u2032. Then \u00b5i(x|\u2203j \u2208 I \u2032 \\ C : \u00b5i(x) \u2264 \u00b5j(x)) \u2264 \u03b4\u2032|I|. For i \u2208 C, \u039bi = {x|\u00b5i(x) > \u00b5j(x)\u2200j \u2208 I \u2032 \\ C} then \u00b5i(\u039bi) \u2265 1 \u2212 \u03b4\u2032|I| and \u039bi \u2286 {x : imax,I\u2032(x) \u2208 C}. Finally,\n\u00b5({x : imax,I\u2032(x) \u2208 C}) \u2265 \u2211 i\u2208C pi\u00b5i(\u039bi) \u2265 pC(1\u2212 \u03b4\u2032|I|).\nProposition 23. Consider distributions \u00b5i for i \u2208 I. Suppose \u00b5 = \u2211\ni\u2208I pi\u00b5i for pi > 0 and\u2211 i\u2208I pi = 1. Suppose we have a partition C of I into C1, . . . , Cm. For x \u2208 Rd, let C = Cmax(x) be the unique part of the partition C containing imax(x) = argmaxi\u2208I \u00b5i(x). Let p\u2217 = mini\u2208I pi. For x \u2208 Rd, let (X\u03b4xt )t be a process initialized at \u03b4x. Suppose for any \u03f5\u0303TV \u2208 (0, 1/10), \u03c4\u0303 \u2208 (0, p\u2217/3), there exists T\u03f5\u0303TV ,\u03c4\u0303 such that the following holds:\nPx\u223c\u00b5[dTV (L(XxT\u03f5\u0303TV ,\u03c4\u0303 |x), \u00b5Cmax(x)) \u2264 \u03f5\u0303TV ] \u2265 1\u2212 \u03c4\u0303 .\nIn addition, there exists \u03b4\u2032 \u2208 (0, \u03f5\u0303TV ) s.t. for C \u2208 {C1, . . . , Cm} Px\u223c\u00b5[Cmax(x) = C] \u2265 pC(1\u2212 \u03b4\u2032).\nLet Usample be a set of M i.i.d. samples from \u00b5 and \u03bdsample be the uniform distribution over Usample. Let (X \u03bdsample t )t\u22650 be the process with score estimate s initialized at \u03bdsample. If M \u2265 6\u00d7102|I|2\u03f5\u22122TV p\u22121\u2217 log(K\u03c4\u22121), with probability \u2265 1\u2212 \u03c4 over Usample, let T = T \u03f5TV9|I| ,min{ \u03f5TV9|I|2 ,p\u2217/3} and \u00b5\u0302 = L(X\u03bdsampleT |Usample), then\nPUsample [dTV (L(X \u03bdsample T |Usample), \u00b5) \u2264 \u03f5TV ] \u2265 1\u2212 \u03c4\nTo remove the dependency on p\u2217 = mini\u2208I pi, we will use this modified version of Proposition 23. Proposition 24. Consider distributions \u00b5i for i \u2208 I. Suppose \u00b5 = \u2211 i\u2208I pi\u00b5i for pi > 0 and\u2211\ni\u2208I pi = 1. For x \u2208 Rd, let (X \u03b4x t )t be a process initialized at \u03b4x. Suppose for any \u03f5\u0303TV \u2208 (0, 1/10), \u03c4\u0303 \u2208 (0, 1), there exists T\u03f5\u0303TV ,\u03c4\u0303 such that the following holds. Let I \u2032 = {i \u2208 I : pi \u2265 \u03f5\u0303TV|I| } and C\u2217 = C \\ I \u2032. Suppose we have a partition C of I \u2032 into C1, \u00b7 \u00b7 \u00b7 , Cr. For x \u2208 Rd, let Cmax(x) be the unique part of the partition C containing imax,I\u2032(x) = argmaxi\u2208I\u2032 \u00b5i(x).\nPx\u223c\u00b5[dTV (L(X\u03b4xT\u03f5\u0303TV ,\u03c4\u0303 |x), \u00b5Cmax(x)) \u2264 \u03f5\u0303TV ] \u2265 1\u2212 \u03c4\u0303 .\nIn addition, there exists \u03b4\u2032 \u2208 (0, \u03f5\u0303TV ) s.t. for C \u2208 {C1, . . . , Cm} Px\u223c\u00b5[Cmax,I\u2032(x) = C] \u2265 pC(1\u2212 \u03b4\u2032).\nLet Usample be a set of M i.i.d. samples from \u00b5 and \u03bdsample be the uniform distribution over Usample. Let (X \u03bdsample t )t\u22650 be the process with score estimate s initialized at \u03bdsample. If M \u2265 2\u00d7 104|I|3\u03f5\u22123TV log(|I|\u03c4\u22121), then PUsample [dTV (L(X \u03bdsample T |Usample), \u00b5) \u2264 \u03f5TV ] \u2265 1\u2212 \u03c4\nProof of Proposition 23 and Proposition 24. We will prove Proposition 24. The proof of Proposition 23 is similar. Set \u03c4\u0303 = \u03f5\u0303TV|I| . Let \u2126r = {x : Cmax(x) = Cr \u2227 dTV (X \u03b4x T , \u00b5Cmax(x)) \u2264 \u03f5TV }. Clearly, \u2126r are disjoint, and by union bound \u00b5(\u2126r) \u2265 p\u0303Cr := (1\u2212 \u03b4\u2032)pCr \u2212 \u03c4\u0303 \u2265 \u03f5\u0303TV10|I| .\nLet Ur = \u2126r \u2229 Usample then Chernoff bound gives\nP[|Ur| \u2265 Mp\u0303Cr (1\u2212 \u03f5\u0303TV )] \u2265 1\u2212 exp(\u2212\u03f5\u03032TV p\u0303CrM/2) \u2265 1\u2212 exp(\u2212 \u03f5\u03033TV M\n20|I| )\nLet E be the event \u2200r : |Ur| \u2265 Mp\u0303Cr (1\u2212 \u03f5\u0303TV ). By union bound, P[E ] \u2265 1\u2212 |I| exp(\u2212 \u03f5\u03033TV M 20|I| ).\nSuppose E happens. Let U\u2205 = Usample \\ \u22c3 r\u2208J Ur then\n|U\u2205| \u2264 M \u2212M(1\u2212 \u03f5\u0303TV ) \u2211 r\u2208J (pCr (1\u2212 \u03b4\u2032)\u2212 \u03f5\u0303TV |I| )\n\u2264 M [1\u2212 (1\u2212 \u03f5\u0303TV )((1\u2212 \u03b4\u2032)(1\u2212 \u03f5\u0303TV )\u2212 \u03f5\u0303TV )] \u2264 M(3\u03f5\u0303TV + \u03b4\u2032) \u2264 4M\u03f5\u0303TV\nwhere the second inequality is due to \u2211 r\u2208J pCr \u2265 1\u2212 \u2211 r \u0338\u2208J pCr \u2265 1\u2212 |I| \u00d7 \u03f5\u0303TV /|I|.\nNote that L(X\u03bdsampleT |Usample) = 1 M \u2211 x\u2208Usample L(X \u03b4x t |x). Thus, let \u00b5\u0302 = \u2211 r |Ur| |Usample\\U\u2205|\n\u00b5Cr and \u00b5\u0303 :=\u2211 r |Ur| M \u00b5Cr + |U\u2205| M \u00b5\u0302, we can apply part 1 of Proposition 9\ndTV (L(X \u03bdsample T |Usample), \u00b5\u0303) \u2264 M\u22121 \u2211\nr \u2211 x\u2208Ur dTV (L(X\u03b4xT |x), \u00b5Cr ) + \u2211 x\u2208U\u2205 dTV (L(X\u03b4xT |x), \u00b5\u0302)  \u2264 M\u22121(\u03f5\u0303TV (M \u2212 |U\u2205|) + |U\u2205|) \u2264 \u03f5\u0303TV + 4\u03f5\u0303TV \u2264 5\u03f5\u0303TV\nNext, note that \u00b5 = \u2211 r pCr\u00b5Cr +pC\u2217\u00b5C\u2217 and \u00b5\u0303 = \u2211 r p\u0304Cr\u00b5Cr with p\u0304Cr := |Ur| M (1+ |U\u2205| |Usample\\U\u2205|\n) = |Ur|\nM\u2212|U\u2205| . We bound |p\u0304Cr \u2212 pCr |.\n|Ur| M \u2212 |U\u2205| \u2265 |Ur| M\n\u2265 M(1\u2212 \u03f5\u0303TV )((1\u2212 \u03b4\u2032)pCr \u2212 \u03f5\u0303TV|I| )\nM\n\u2265 pCr (1\u2212 \u03f5\u0303TV \u2212 \u03b4\u2032)\u2212 \u03f5\u0303TV |I| \u2265 pCr \u2212 \u03f5\u0303TV (2 + 1 |I| )\nWe upper bound |Ur|. Since Ur\u2019s are disjoint,\n|Ur| \u2264 M \u2212 \u2211\ns\u2208J,s \u0338=r\n|Us| \u2264 M \u2212M \u2211\ns\u2208J,s \u0338=r\n[ pCs(1\u2212 \u03f5\u0303TV \u2212 \u03b4\u2032)\u2212\n\u03f5\u0303TV |I| ] \u2264 M(pCr + 3\u03f5\u0303TV + \u03b4\u2032) \u2264 M(pCr + 4\u03f5\u0303TV )\nwhere the first inequality is due to the lower bound of |Us| above and the second inequality is due to 1\u2212 \u2211 s:s\u0338=r pCs \u2264 pCr + \u03f5\u0303TV and \u2211 s:s \u0338=r pCs(\u03f5\u0303TV + \u03b4 \u2032) \u2264 (\u03f5\u0303TV + \u03b4\u2032). Thus\n|Ur| M \u2212 |U\u2205| \u2212 pCr \u2264 M(pCr + 4\u03f5\u0303TV ) M(1\u2212 4\u03f5\u0303TV ) \u2212 pCr \u2264 4\u03f5\u0303TV (pCr + 1) 1\u2212 4\u03f5\u0303TV \u2264 16\u03f5\u0303TV\nwhere in the last inequality, we use the bounds \u03f5\u0303TV \u2264 1/10 and pCr \u2264 1. thus\n|p\u0304Cr \u2212 pCr | \u2264 max{16\u03f5\u0303TV , \u03f5\u0303TV (2 + 1\n|I| )}\nPart 2 of Proposition 9 gives\n2dTV (\u00b5, \u00b5\u0303) \u2264 \u2211 r |p\u0304Cr \u2212 pCr |+ pC\u2217 \u2264 |I|max{16\u03f5\u0303TV , \u03f5\u0303TV (2 + 1 |I| )}+ |I| \u00d7 \u03f5\u0303TV /|I|\n\u2264 (16|I|+ 1)\u03f5\u0303TV\nThus by triangle inequality,\ndTV (L(X \u03bdsample T |Usample), \u00b5) \u2264 dTV (L(X \u03bdsample T |Usample), \u00b5\u0303) + dTV (\u00b5, \u00b5\u0303) \u2264 9|I|\u03f5\u0303TV\nLetting \u03f5\u0303TV = \u03f5TV9|I| and M \u2265 2\u00d710 4|I|3\u03f5\u22123TV log(|I|\u03c4\u22121) \u2265 20|I|\u03f5\u0303 \u22123 TV log(|I|\u03c4\u22121) gives the desired result.\nIn the proof of Proposition 23 we will set \u03c4\u0303 = min{ \u03f5\u0303TV|I| , p\u2217/3} which implies \u00b5(\u2126r) \u2265 p\u2217/3 and the event E happens with probability 1\u2212 |I| exp(\u2212p\u2217 \u03f5\u0303 2 TV M 6 ). The rest of the argument follows through, and we need to set M = 6 \u00d7 102p\u22121\u2217 |I|2\u03f5\u22122TV log(|I|\u03c4\u22121) to ensure E happens with probability \u2265 1\u2212 \u03c4."
        },
        {
            "heading": "F.1 GRADIENT ERROR BOUND FOR CONTINUOUS PROCESS",
            "text": "Definition 5 (Bad set for partition). Let C = {C1, . . . , Cm} be a partition of S i.e. \u22c3 Cr = S and Cr \u2229 Cr\u2032 = \u2205 if r \u0338= r\u2032. For x \u2208 Rd, let \u00b5max,S(x) = maxi\u2208S \u00b5i(x), imax,S(x) = argmaxi\u2208S \u00b5i(x)\n4 and Cmax(x) is the unique part of the partition containing imax,S(x). For \u03b3 \u2208 (0, 1) let BS,C,\u03b3 = {x | \u2203j \u2208 S \\ Cmax : \u00b5max,S(x) \u2264 \u03b3\u22121\u00b5j(x)} If these are clear from context, we omit S, C in the subscript. Lemma 12. Fix S \u2286 I, C is a partition of S, and define B\u03b3 = BS,C,\u03b3 as in Definition 5. If \u03b4ij \u2264 \u03b4 for i, j not being in the same part of the partition then \u00b5(B\u03b3) \u2264 \u03b3\u22121\u03b4|I|2/2.\n4If there are ties, we break ties according to the lexicographic order of I.\nProposition 25 (Absolute gradient difference bound). Fix S \u2286 I. For i \u2208 S, let p\u0304i = pip\u22121S and recall that \u00b5S(x) = \u2211 i\u2208S p\u0304i\u00b5i(S). Let i := imax,S(x) = argmaxi\u2032\u2208S\u2032 \u00b5i\u2032(x). Suppose i \u2208 C \u2286 S and for all j \u2208 S \\ C, \u00b5i(x) \u2265 \u03b3\u22121\u00b5j(x). Let GS(x) = maxi\u2208S ||\u2207Vi(x)|| then\n||\u2207VS(x)\u2212\u2207VC(x)|| \u2264 4\u03b3\np\u0304i GS(x)\nIn Appendix H, we will state generalized versions of Definition 5, Lemma 12, and Proposition 25. For proofs of Lemma 12 and Proposition 25, refers to proof of Lemma 16 and Proposition 32 respectively.\nThe following proposition shows that if the continuous Langevin process (Z\u0304\u03b4xt ) initialized at x doesn\u2019t hit the bad set BS,C,\u03b3 , then the gradient \u2207VS(Z\u0304t) will be close to the gradient \u2207VC(Z\u0304t) where C is the unique part of the partition C containing imax,S(x). Proposition 26. Fix a set S. Suppose we have a partition C of S as in Definition 5. Suppose for i \u2208 S, \u00b5i satisfies item 1 of Lemma 5 with \u03b2 \u2265 1 and ||ui \u2212 uj || \u2264 L\u2200i, j \u2208 S. Let p\u0304i = p\u22121S pi, and recall that \u00b5S = \u2211 i\u2208S p\u0304i\u00b5i. Let (Z\u0304 \u03b4x t )t\u22650 be the continuous Langevin diffusion with score function \u2207VS initialized at \u03b4x. Fix \u03b3 \u2208 (0, 1/2). Suppose for any \u03b7 \u2208 (0, 1), with probability 1 \u2212 \u03b7/2, the event Ediscrete,\u03b7 happens where Ediscrete,\u03b7 is defined by: for all k \u2208 [0, N \u2212 1] \u2229 N,\n||Z\u0304\u03b4xkh \u2212 uS || \u2264 L\u0303 := L+\n\u221a 64\n\u03b1 ln\n16N\n\u03b7\nand\nZ\u0304\u03b4xkh \u0338\u2208 BS,C,\u03b3 .\nLet T = Nh and C = Cmax(x) \u2208 C be the unique part of the partition C containing imax,S(x). Fix \u03b7 \u2208 (0, 1). Suppose T \u2265 1,\nh \u2264 min{ 1 (\u03b22/\u03b1) ln2(16T/\u03b7) , 1 40(\u03b2L)2 ,\n1\n2000d(\u03b2L)2 ln(16T/\u03b7) },\nh ln(1/h) \u2264 12000d(\u03b2L)2 and h ln 2(1/h) \u2264 11000(\u03b22/\u03b1) .\nThen with probability 1\u2212 \u03b7,\n\u2200t \u2208 [0, T ] : ||\u2207VS(Z\u0304\u03b4xt )\u2212\u2207VC(Z\u0304 \u03b4x t )|| \u2264\n18\u03b3\u03b2L\u0303\nmini\u2208C p\u0304i .\nProof. By Proposition 17, S satisfies item 2 of Assumption 2 with Agrad,0 = \u03b2L and Agrad,1 = \u03b2.\nFrom Proposition 19, with probability \u2265 1\u2212 \u03b7/2, the following event Edrift,\u03b7/2 happens\nsup k\u2208[0,N\u22121]\u2229N,t\u2208[0,h]\n||Z\u0304\u03b4xkh+t \u2212 Z\u0304 \u03b4x kh|| \u2264 4\u03b2hL+ 2\n\u221a( 64(\u03b2h)2\n\u03b1 + 48dh\n) ln 16N\n\u03b7 \u2264 1/(20\u03b2L\u0303)\nHere we use the fact that ln(16N/\u03b7) = ln(16T/(\u03b7h) = ln(16T/\u03b7) + ln(1/h) thus\nh(\u03b2L)(\u03b2L\u0303) \u2264 h(\u03b2L)2 + 2h(\u03b2L)\u03b2\n\u221a 64\n\u03b1 ln\n16T\n\u03b7 + 2h(\u03b2L)\u03b2\n\u221a 64\n\u03b1 ln(1/h)\n\u2264 h(\u03b2L)2 + 16 \u221a h \u03b22\n\u03b1 \u00b7\n\u221a h(\u03b2L)2 ln 16T\n\u03b7 + 16\n\u221a h \u03b22 \u03b1 \u00b7 \u221a h(\u03b2L)2 ln(1/h) \u2264 1 160\nand\u221a( 64(\u03b2h)2\n\u03b1 + 48dh\n) ln 16N\n\u03b7 \u00d7 (\u03b2L\u0303)\n\u2264 10 \u221a dh( \u221a ln 16T \u03b7 + \u221a ln(1/h)) ( \u03b2L+ 2\u03b2 \u221a 64 \u03b1 ln 16T \u03b7 + 2\u03b2 \u221a 64 \u03b1 ln(1/h) )\n\u2264 10 (\u221a hd(\u03b2L)2 ln 16T \u03b7 + \u221a hd(\u03b2L)2 ln(1/h) + 48 \u221a h\u03b22 \u03b1 (ln2 16T \u03b7 + ln2(1/h)) ) \u2264 1 80\nSuppose both events Edrift,\u03b7/2 and Edrift,\u03b7/2 happen. By union bound, this occurs with probability \u2265 1\u2212 \u03b7. We have, by triangle inequality\nsup k\u2208[0,N\u22121]\u2229N,t\u2208[0,h]\n||Z\u0304\u03b4xkh+t \u2212 uS || \u2264 L\u0303+ 1/(10\u03b2L\u0303) \u2264 1.1L\u0303\nand for i \u2208 S, by item 1 of Lemma 5 and ||ui \u2212 uS || \u2264 L\n||\u2207Vi(Z\u0304\u03b4xkh+t)|| \u2264 \u03b2(||Z\u0304 \u03b4x kh+t \u2212 uS ||+ L) \u2264 2.2\u03b2L\u0303. (11)\nFor any i, j \u2208 S and t \u2208 [0, h]\nlog \u00b5j(Z\u0304\n\u03b4x kh+t)\n\u00b5i(Z\u0304 \u03b4x kh+t)\n\u2212 log \u00b5j(Z\u0304\n\u03b4x kh)\n\u00b5i(Z\u0304 \u03b4x kh)\n= Vj(Z\u0304 \u03b4x kh)\u2212 Vj(Z\u0304 \u03b4x kh+t)\u2212 (Vi(Z\u0304 \u03b4x kh)\u2212 Vi(Z\u0304 \u03b4x kh+t))\n\u2264 (||\u2207Vi(Z\u0304\u03b4xkh)||+ ||\u2207Vj(Z\u0304 \u03b4x kh)||)||Z\u0304 \u03b4x kh+t \u2212 Z\u0304 \u03b4x kh||+ \u03b2||Z\u0304 \u03b4x kh+t \u2212 Z\u0304 \u03b4x kh|| 2 \u2264 5\u03b2L\u0303(20\u03b2L\u0303)\u22121 + \u03b2(20\u03b2L\u0303)\u22122 \u2264 1/2\n(12)\nwhere we use the assumption \u03b2 \u2265 1. Below we write imax instead of imax,S since S is clear from context. We first argue by induction on k that imax(Z\u0304\u03b4xkh) \u2208 C. The base case k = 0 holds trivially. Let y be a realization of Z\u0304 \u03b4x kh. Condition on Z\u0304\u03b4xkh = y, we argue that imax(Z\u0304 \u03b4x (k+1)h) \u2208 Cmax(y). Since Cmax(y) = C by the inductive hypothesis for k, the inductive hypothesis for k + 1 follows. Apply Eq. (12) for t = h, i := imax(y) and j \u0338\u2208 Cmax(y) gives\nlog \u00b5j(Z\u0304\n\u03b4x (k+1)h)\n\u00b5i(Z\u0304 \u03b4x (k+1)h)\n\u2264 log \u00b5j(Z\u0304\n\u03b4x kh)\n\u00b5i(Z\u0304 \u03b4x kh)\n+ 1/2 = log \u00b5j(y)\n\u00b5max(y) + 1/2 \u2264 log \u03b3 + 1/2 < 0\nwhere the penultimate inequality follows from Z\u0304kh \u0338\u2208 B\u03b3 and j \u0338\u2208 Cmax(y), and the final inequality from \u03b3 < 1/2. Thus, for all j \u0338\u2208 Cmax(y), \u00b5i(Z\u0304\u03b4x(k+1)h) > \u00b5j(Z\u0304 \u03b4x (k+1)h) thus imax(Z\u0304 \u03b4x (k+1)h) \u2208 Cmax(y). Finally, we argue for k \u2208 [0, N \u2212 1] \u2229 N and t \u2208 (0, h), imax(Z\u0304\u03b4xkh+t) \u2208 C and Z\u0304 \u03b4x kh+t \u0338\u2208 B2\u03b3 . Condition on Z\u0304\u03b4xkh = y, apply Eq. (12) for t = h, i := imax(y) and j \u0338\u2208 Cmax(y) = C gives\nlog \u00b5j(Z\u0304\n\u03b4x kh+t)\n\u00b5i(Z\u0304xkh+t) \u2264 log\n\u00b5j(Z\u0304 \u03b4x kh) \u00b5i(Z\u0304 \u03b4x kh) + 1/2 = log \u00b5j(y) \u00b5max(y) + 1/2 \u2264 log \u03b3 + 1/2 < log(2\u03b3)\nthus \u2200j \u0338\u2208 C : \u00b5max(Z\u0304\u03b4xkh+t) \u2265 \u00b5i(Z\u0304 \u03b4x kh+t) \u2265 (2\u03b3)\u22121\u00b5j(Z\u0304 \u03b4x kh+t). Combine this with the bound on \u2207Vi(Z\u0304\u03b4xkh+t) in Eq. (11) and using Proposition 25 gives the desired result. Indeed,\n||\u2207VS(Z\u0304\u03b4xt )\u2212\u2207VC(Z\u0304 \u03b4x t )|| \u2264 4\u00d7 (2\u03b3)GS(Z\u0304\u03b4xt ) p\u0304i \u2264 18\u03b2L\u0303 p\u0304i ."
        },
        {
            "heading": "G ANALYSIS OF LMC WITH APPROXIMATE SCORE",
            "text": "In this section, we prove the main result (Corollary 1). Definition 6. Let HL be the graph where there is an edge between i, j iff ||ui \u2212 uj || \u2264 L. Proposition 27. Suppose C is a connected component of HL then for any i, j \u2208 C, ||ui\u2212uj || \u2264 KL.\nProof. For any i, j \u2208 C, there exists a path i := p0, p1, \u00b7 \u00b7 \u00b7 , pm := j s.t. ||ups \u2212 ups+1 || \u2264 L. The statement then follows from triangle inequality."
        },
        {
            "heading": "G.1 EXPECTED SCORE ERROR BOUND",
            "text": "Lemma 13. Suppose \u00b5i satisfies the conditions stated in Lemma 5. Let ui be as defined in Lemma 5. Fix S,R \u2286 I, S\u2229R = \u2205. Let p\u2212 = maxj\u2208R pj . Suppose for j \u2208 I \\ (S\u222aR), ||ui\u2212uj || \u2265 L\u2200i \u2208 S, with L \u2265 30max{ \u221a d \u03b1 , \u03ba \u221a d} ln(10\u03ba). If score estimate s satisfies E\u00b5[||s(x)\u2212\u2207V (x)||2] \u2264 \u03f52score then\nE\u00b5S [||\u2207V (x)\u2212\u2207VS(x)||2] \u2264 3p\u22121S (\u03f5 2 score + 8\u03b2\n2K exp(\u2212 L 2\n80\u03ba ) + 10K2p\u2212\u03b2 2L2)\nProof. Since \u2207VS(x) = p\u22121S \u2211 i\u2208S \u2207Vi(x), we can write\n||\u2207V (x)\u2212\u2207VS(x)|| = (\u00b5(x)pS\u00b5S(x))\u22121 \u2211\ni\u2208S,j \u0338\u2208S\npipj\u00b5i(x)\u00b5j(x)||\u2207Vi(x)\u2212\u2207Vj(x)||\n\u2264 \u2211\ni\u2208S,j \u0338\u2208S:||ui\u2212uj ||<L\npipj\u00b5i(x)\u00b5j(x)||\u2207Vi(x)\u2212\u2207Vj(x)|| \u00b5(x)pS\u00b5S(x)\n+ \u2211\ni\u2208S,j \u0338\u2208S:||ui\u2212uj ||\u2265L\npipj\u00b5i(x)\u00b5j(x)||\u2207Vi(x)\u2212\u2207Vj(x)|| \u00b5(x)pS\u00b5S(x)\nIf ||ui \u2212uj || \u2264 L then ||\u2207Vi(x)\u2212\u2207Vj(x)|| \u2264 \u03b2(||x\u2212ui||+ ||x\u2212uj ||) \u2264 \u03b2(2||x\u2212uj ||+L) thus the first term can be bounded by\n(pS\u00b5S(x)) \u22121 \u2211 i\u2208S \u2211 j \u0338\u2208S:||ui\u2212uj ||\u2264L pipj\u00b5i\u00b5j(x) \u00b5(x) \u03b2(2||x\u2212 uj ||+ L)  \u2264 \u03b2\n\u2211 j \u0338\u2208S:pj\u2264p\u2212 pj\u00b5j(x)(2||x\u2212 uj ||+ L) \u00b5(x)\nwhere in the last inequality we use the fact that if ||ui \u2212 uj || \u2264 L for some i \u2208 S then j \u2208 R and pj \u2264 p\u2212. Hence, by Holder\u2019s inequality\nE\u00b5S [||\u2207V (x)\u2212\u2207VS(x)||2] \u2264 3(E\u00b5S [||s(x)\u2212\u2207V (x)||2] +A1 +A2) (13)\nwith A2 = E\u00b5S [( \u2211 i\u2208S,j\u2208T2 pipj\u00b5i(x)\u00b5j(x) pS\u00b5S(x)\u00b5(x) )2] and\nA1 = E\u00b5S \u03b22  \u2211\nj \u0338\u2208S:pj\u2264p\u2212\npj\u00b5j(x)(2||x\u2212 uj ||+ L) \u00b5(x)\n2 \n\u2264 5\u03b22K \u2211\nj \u0338\u2208S:pj\u2264p\u2212\n\u222b \u00b5S(x) ( pj\u00b5j(x)\n\u00b5(x)\n)2 (||x\u2212 uj ||2 + L2)dx\n\u2264 5\u03b22Kp\u22121S \u2211\nj \u0338\u2208S:pj\u2264p\u2212\npj \u222b \u00b5j(x)(||x\u2212 uj ||2 + L)dx\n\u2264 10\u03b22K2p\u22121S p\u2212L 2\nNow we bound the term A2. Let T2 = {j : j \u0338\u2208 S, pj \u2265 p\u2212}.\nE\u00b5S   \u2211\ni\u2208S,j\u2208T2\npipj\u00b5i(x)\u00b5j(x)\npS\u00b5S(x)\u00b5(x)\n2 \n\u2264 E\u00b5S\n (\u2211 i\u2208S,j\u2208T2 pipj\u00b5i(x)\u00b5j(x)||\u2207Vi(x)\u2212\u2207Vj(x)|| 2 )(\u2211 i\u2208S,j\u2208T2 pipj\u00b5i(x)\u00b5j(x) )\n(pS\u00b5S(x)\u00b5(x))2  = p\u22121S\n\u222b \u2211 i\u2208S,j\u2208T2 pipj\u00b5i(x)\u00b5j(x)||\u2207Vi(x)\u2212\u2207Vj(x)||2 \u00b5(x) dx\n= p\u22121S \u2211\ni\u2208S,j\u2208T2\npiE\u00b5i [ pj\u00b5j(x)\n\u00b5(x) ||\u2207Vi(x)\u2212\u2207Vj(x)||2\n]\n\u2264 8p\u22121S K\u03b2 2 exp(\u2212 L\n2\n40\u03ba )\nwhere in the last inequality we use Proposition 29. Plug these inequalities back into Eq. (13), and use Proposition 28 gives the desired results.\nProposition 28. Suppose s satisfies Definition 1 then\nE\u00b5S [||s(x)\u2212\u2207V (x)||2] \u2264 p\u22121S \u03f5 2 score\nProof. pSE\u00b5S [||s(x)\u2212\u2207V (x)||2] = pS \u222b \u00b5S(x)||s(x)\u2212\u2207V (x)||2dx\n\u2264 \u222b (pS\u00b5S(x) + pSc\u00b5Sc(x))||s(x)\u2212\u2207V (x)||2dx\n= E\u00b5[||s(x)\u2212\u2207V (x)||2] \u2264 \u03f52score\nProposition 29 (Pairwise gradient difference for large ||ui \u2212 uj ||). Suppose \u00b5i, \u00b5j satisfies items 2 and 3 in Lemma 5. Let ui, uj be as defined in Lemma 5 and r := ||ui \u2212 uj ||. If \u03b1r 2/2+cz 17/16\u03b1+\u03b2 \u2265 4D 2 then\nEx\u223c\u00b5i [ pj\u00b5j(x)\n\u00b5(x) ||\u2207Vi(x)\u2212\u2207Vj(x)||2\n] \u2264 8\u03b22p\u22121i r 2 exp ( \u2212 \u03b1r 2 + cz 17\u03b1+ 16\u03b2 ) Consequently, suppose \u00b5i, \u00b5j are \u03b1-strongly log concave and \u03b2-smooth with \u03b2 \u2265 1 and \u03ba = \u03b2/\u03b1, and ||ui \u2212 uj || \u2265 L with L \u2265 30max{ \u221a d \u03b1 , \u03ba \u221a d} ln(10\u03ba)\npiEx\u223c\u00b5i [ pj\u00b5j(x)\n\u00b5(x) ||\u2207Vi(x)\u2212\u2207Vj(x)||2\n] \u2264 8\u03b22 exp ( \u2212 L 2\n80\u03ba ) Proof. By Lemma 5, item 2\n\u00b5i(x) \u00b5j(x) \u2265 exp\n( \u2212\u03b2||x\u2212 ui||2 \u2212 z\u2212 + \u03b1||x\u2212 uj ||2 + z+ ) \u2265 exp (\u03b1 2 ||ui \u2212 uj ||2 \u2212 (\u03b1+ \u03b2)||x\u2212 ui||2 + cz\n) where the second inequality follows from ||ui \u2212 uj ||2/2 \u2264 (||x \u2212 ui|| + ||x \u2212 uj ||)2/2 \u2264 ||x \u2212 ui||2 + ||x\u2212 uj ||2 thus\npj\u00b5j(x) \u00b5(x) \u2264 pj\u00b5j(x) pj\u00b5j(x) + pi\u00b5i(x) =\n1\n1 + pi\u00b5i(x)pj\u00b5j(x) \u2264 H(||x\u2212 ui||2)\nwhere H(y) =\n1\n1 + pi exp( \u03b1 2 ||ui \u2212 uj ||2 \u2212 (\u03b1+ \u03b2)y + cz)\n.\nLet A := Ex\u223c\u00b5i [H(||x\u2212 ui||2)] and B := Ex\u223c\u00b5i [||x\u2212 ui||2H(||x\u2212 ui||2)]. Using the fact that ||\u2207Vi(x)\u2212\u2207Vj(x)||2 \u2264 \u03b22(||x\u2212 ui||+ ||x\u2212 uj ||)2 \u2264 2\u03b22(4||x\u2212 ui||2 + ||ui \u2212 uj ||2),\nwe can bound\nEx\u223c\u00b5i [ pj\u00b5j(x)\n\u00b5(x) ||\u2207Vi(x)\u2212\u2207Vj(x)||2\n] \u2264 2\u03b22(r2A+ 4B)\nFirst we bound A. We have E\u00b5i [H(||x\u2212 ui||2)] = \u222b ||x\u2212ui||\u2265R H(||x\u2212 ui||2)\u00b5i(x)dx+ \u222b ||x\u2212ui||<R H(||x\u2212 ui||2)\u00b5i(x)dx\n\u2264 Px\u223c\u00b5i [||x\u2212 ui|| \u2265 R] +H(R2) \u2264 exp(\u2212\u03b1(R\u2212D)2/4) + p\u22121i exp(\u2212 \u03b1\n2 r2 + (\u03b1+ \u03b2)R2 \u2212 cz)\nwhere the second inequality follows from H being an increasing function bounded above by 1, and the third inequality follows from H(y) \u2264 p\u22121i exp(\u2212\u03b1r2 + (\u03b1 + \u03b2)y \u2212 cz). Set R2 = \u03b1r2/2+cz \u03b1+\u03b2+\u03b1/16 then R \u2265 2D thus exp(\u2212\u03b1(R \u2212D)2/4) \u2264 exp(\u2212\u03b1R2/16) = exp(\u2212\u03b1r2/2 + (\u03b1 + \u03b2)R2 \u2212 cz). Hence, the rhs is bounded by 2p\u22121i exp(\u2212 \u03b1r2/2+cz 17\u03b1+16\u03b2 ).\nNow we bound B. By Holder\u2019s inequality E\u00b5i [||x\u2212 ui||2H(||x\u2212 ui||2)]\n\u2264 \u221a E\u00b5i [||x\u2212 ui||4] \u00b7 \u221a E\u00b5i [H2(||x\u2212 ui||2)]\n\u2264 D2 \u221a\nPx\u223c\u00b5i [||x\u2212 ui|| \u2265 R\u0303] +H2(R\u03032) \u2264 D2 \u221a\nexp(\u2212\u03b1(R\u0303\u2212D)2/4) + p\u22122i exp(\u22122\u03b1r2 + 2(\u03b1+ \u03b2)R\u03032 \u2212 2cz) where we use the sub-Gaussian moment assumption to bound E\u00b5i [||x \u2212 ui||4] and the same argument as in the bound for A to bound E\u00b5i [H2(||x\u2212 ui||2)], noting that H2(\u00b7) is also an increasing function bounded above by 1. Set R\u03032 = \u03b1r\n2/2+cz \u03b1+\u03b2+\u03b1/32 then R\u0303 \u2265 2D thus exp(\u2212\u03b1(R\u0303 \u2212 D)\n2/4) \u2264 exp(\u2212\u03b1R\u03032/16) = exp(\u22122(\u03b1r2 + (\u03b1+ \u03b2)R\u03032 \u2212 cz)). Hence,\nB \u2264 2D2p\u22121i exp(\u2212R\u0303 2/32) = 2D2p\u22121i exp\n( \u2212\u03b1r\n2/2 + cz 33\u03b1+ 32\u03b2 ) For the second statement, plug in D = 5 \u221a d \u03b1 ln(10\u03ba) and cz = \u2212 d 2 ln(\u03ba), and use the fact that \u03b2 \u2265 1, we have \u03b1r2/2 + cz 17\u03b1+ 16\u03b2 \u2265 0.45\u03b1r 2 33\u03b2 \u2265 80\u00d7 \u03b2d \u03b1 ln2(10\u03ba) = 4D2\nThus by Proposition 8 and the fact that L2 \u2265 2\u03ba, r2 exp(\u2212\u03b1r 2/2+cz\n17\u03b1+16\u03b2 ) \u2264 r 2 exp(\u2212 r\n2\n80\u03ba ) \u2264 L2 exp(\u2212 L 2\n80\u03ba )\nTheorem 6. Suppose each \u00b5i is \u03b1 strongly-log-concave and \u03b2-smooth for all i \u2208 I with \u03b2 \u2265 1. Recall that |I| = K. Let ui = argminVi(x), p\u2217 = mini\u2208I pi, \u03ba = \u03b2/\u03b1. Set\nL0 = \u0398 ( \u03ba2K \u221a d(ln(10\u03ba) + exp(K) ln(dp\u22121\u2217 \u03f5 \u22121 TV )) ) = \u0398\u0303(\u03ba2K exp(K) \u221a d).\nLet S be a connected component of HL, where there is an edge between i, j if ||ui \u2212 uj || \u2264 L := L0/(\u03baK). Let Usample be a set of M i.i.d. samples from \u00b5S and \u03bdsample be the uniform distribution over Usample. Let (X \u03bdsample nh )n\u2208N be the LMC with score s and step size h initialized at \u03bdsample. Set\nT = \u0398 \u03b1\u22121K2p\u22121\u2217 ln(10p\u22121\u2217 ) 108d(\u03b2L0)3exp(K) ln3/2(p\u22121\u2217 ) ln5 16d(\u03b2L0)2\u03f5TV \u03c4\u03b1\np 7/2 \u2217 \u03f53TV \u03b1 3/2\n2((3/2) K\u22121\u22121) \nLet the step size h = \u0398 (\n\u03f54TV (\u03b2L0)4dT\n) = \u0398\u0303 ( \u03f54TV\n(\u03b2\u03ba2K exp(K))4d3T\n) . Suppose s satisfies Definition 1 with\n\u03f5score\n\u2264p 1/2 \u2217 \u03f5 2 TV\n\u221a h\n7T\n=\u0398( p 1/2 \u2217 \u03f5 4 TV\n(\u03b2L0)2T 3/2 )\n=\u0398\u0303\n( p 1/2 \u2217 \u03f5 4 TV\n(\u03b2\u03ba2K exp(K))2d3/2T 3/2\n)\n=\u0398  p2\u2217\u03f54TV \u03b13/2 K3 ln3/2(10p\u22121\u2217 )(\u03b2L0)2 ( p 7/2 \u2217 \u03f5 3 TV \u03b1 3/2 108d(\u03b2L0)3exp(K) ln 3/2(p\u22121\u2217 ) ln 5 16d(\u03b2L0)2\n\u03f5TV \u03c4\u03b1 )3((3/2)K\u22121\u22121) Suppose the number of samples M satisfies M \u2265 4000p\u22121\u2217 \u03f5\u22124TV K2 log(K\u03f5 \u22121 TV ) log(\u03c4 \u22121), then\nPUsample [dTV (L(X \u03bdsample T | Usample), \u00b5S) \u2264 \u03f5TV ] \u2265 1\u2212 \u03c4\nCorollary 1. Suppose \u00b5i is \u03b1 strongly-log-concave and \u03b2-smooth for all i with \u03b2 \u2265 1. Let p\u2217 = mini\u2208I pi. Suppose s satisfies Definition 1. Let Usample be a set of M i.i.d. samples from \u00b5 and \u03bdsample be the uniform distribution over Usample. With T, h, \u03f52score as in Theorem 6 and M \u2265 20000p\u22122\u2217 \u03f5\u22124TV K2 log(K\u03f5 \u22121 TV ) log(K\u03c4\n\u22121). Let (X\u03bdsamplenh )n\u2208N be the LMC with score s and step size h initialized at \u03bdsample, then\nPUsample [dTV (L(X \u03bdsample T | Usample), \u00b5) \u2264 \u03f5TV ] \u2265 1\u2212 \u03c4\nProof. This is a consequence of Theorem 6 and Proposition 31. Here we apply Proposition 31 with\nM0 = 4000p \u22121 \u2217 \u03f5 \u22124 TV K 2 log(K\u03f5\u22121TV ) log(\u03c4 \u22121).\nProof of Theorem 6. Let ui = argminx Vi(x) then \u2207Vi(ui) = 0. W.l.o.g. we can assume Vi(ui) = 0. By Proposition 27, ||ui \u2212 uj || \u2264 L\u0302 := KL = L0/\u03ba for i, j \u2208 S. By Proposition 17, with uS = p \u22121 S \u2211 i\u2208S piui, S satisfies Assumption 2 with Agrad,1 = \u03b2, Agrad,0 = \u03b2L\u0302, AHess,1 = 2\u03b2\n2 and AHess,0 = 2\u03b2 2L\u03022.\nWe first show the statement for M = M0 := 600p\u22121\u2217 \u03f5 \u22122 TV K 2(log(K\u03c4\u22121)), where we set \u03c4 = \u03f5TV , then use Proposition 30 to obtain the result for M \u2265 4000p\u22121\u2217 \u03f5\u22124TV K4 log(K\u03f5 \u22121 TV )) log(\u03c4\n\u22121) \u2265 6M0\u03f5 \u22122 TV log(\u03f5 \u22121 sample ).\nFrom this point onward set \u03c4 = \u03f5TV and M = M0 as defined above. Let (X \u00b5S nh )n\u2208N be the LMC with score estimate s and step size h initialized at \u00b5S and (X\u0304 \u00b5S t )t\u22650 be the continuous Langevin diffusion with score \u2207VS initialized at \u00b5S . Let QT and Q\u0304T denote the distribution of the paths (X\u00b5Snh )n\u2208[0,T/h]\u2229N and (X\u0304 \u00b5S t )t\u2208[0,T ]. Note that L \u2265 50\u03ba \u221a d ln(10\u03ba) \u2265 10D, so Lemma 15 gives\n2dTV (QT , Q\u0304T ) 2 \u2264 2h2T\u03b26L\u03026 + 2hTd\u03b24L\u03024 + T\u03f52score,0\nwith \u03f52score,0 := 3p \u22121 S (\u03f5 2 score+8\u03b2 2K exp(\u2212 L 2 80\u03ba )). Let \u03f5 2 score,1 = \u03f52TV 8T and B = {z : ||s(z)\u2212VS(z)|| > \u03f5score,1} then by Markov\u2019s inequality \u00b5(B) \u2264 \u03f52score,0 \u03f52score,1 = 8\u03f52score,0T \u03f52TV .\nLet \u03b7 = \u03f5TV \u03c4. Suppose T\u03f52score,0 \u2264 \u03b72/100 and h \u2264 (100)\u22121 min{ \u03b7 (\u03b2L\u0302)3 \u221a T , \u03b7\n2\n(\u03b2L\u0302)4dT } then\ndTV (QT , Q\u0304T ) \u2264 \u03b7/4, thus P[\u2203n \u2208 [0, N \u2212 1] \u2229 N : X\u00b5Snh \u2208 B] \u2264 P[\u2203n \u2208 [0, N \u2212 1] \u2229 N : X\u0304 \u00b5S nh \u2208 B] + dTV (QT , Q\u0304T )\n\u2264 A1 := T\nh \u00d7 8T\u03f52score,0 \u03f52TV + \u03f5TV \u03c4/4\nSince EUsample [\u03bdsample] = \u00b5S , L(X \u00b5S nh ) = EUsample [L(X \u03bdsample nh |Usample)] and\nEUsample [PFn [\u2203n \u2208 [0, N \u2212 1] \u2229 N : X \u03bdsample nh \u2208 B]] = P[\u2203n \u2208 [0, N \u2212 1] \u2229 N : X \u00b5S nh \u2208 B] \u2264 A1\nBy Markov\u2019s inequality, let E0 be the event PFn [\u2203n \u2208 [0, N \u2212 1] \u2229 N : X \u03bdsample nh \u2208 B] \u2264 2A1/\u03c4 then\nPUsample [E0 occurs] \u2265 1\u2212 \u03c4/2\nSuppose E0 occurs. Let \u03bd := \u03bdsample. Let (Z \u03bdsample nh )n\u2208N be the LMC initialized at \u03bd with score estimate s\u221e defined by\ns\u221e(z) = { s(z) if z \u0338\u2208 B \u2207VS(z) if x \u2208 B\nthen supz\u2208Rd ||s\u221e(z)\u2212\u2207VS(z)||2 \u2264 \u03f52score,1.\nNote that if Xnh \u0338\u2208 B\u2200n \u2208 [0, N \u2212 1] \u2229 N then Z \u03bdsample nh = X \u03bdsample nh \u2200n \u2208 [0, N ] \u2229 N thus conditioned on E0 occurs, dTV (Z \u03bdsample Nh , X \u03bdsample Nh ) \u2264 2A1/\u03c4. Let (Z\u0304 \u03bdsample t )t be the continuous Langevin with score \u2207VS initialized at \u03bd. We want to bound dTV (Z \u03bdsample Nh , Z\u0304 \u03bdsample T ). By sub-Gaussian concentration of \u00b5i and union bound over M samples, we have with probability \u2265 1\u2212 \u03c4/3, the following event E1 happens:\nsup x\u223c\u03bd max i\u2208S\n||x\u2212 ui|| \u2264 L\u0303 := 2L\u0302+ \u221a 4\n\u03b1 ln(\n8M\n\u03c4 ) \u2264 3L\u0302\nsince for \u03b2 \u2265 1, \u221a\n4 \u03b13 ln( 8M \u03c4 ) \u2264 3\u03ba 3/2 \u221a ln(\u03f5\u22121TV ) \u2264 L\u0302.\nLet E2 be the event dTV (L(Z\u0304 \u03bdsample T |Usample), \u00b5S) \u2264 \u03f5TV /4 By Lemma 11, if M \u2265 605(p\u2217\u03f52TV )\u22121K2 log(K\u03c4\u22121) then PU sample [E2] \u2265 1\u2212 \u03c4/6. Suppose E0, E1, E2 all hold; by union bound, this happens with probability \u2265 1\u2212 \u03c4. By Lemma 14, for\nL0 = L\u0302+ \u03baL\u0303+\n\u221a d\n\u03b1 ln((2\u03b1h)\u22121) +\n\u221a (16/\u03b1+ 200dh) ln( 8T\nh )\nwe have\ndTV (Z \u03bdsample Nh , Z\u0304 \u03bdsample T ) 2 \u2272 h2T\u03b26L60 + hTd\u03b2 4L40 + \u03f5 2 score,1T/2 \u2264 \u03f52TV /64\nif 100h \u2264 \u03f5 2 TV\n(\u03b2L0)4dT \u2264 \u03f5TV (\u03b2L0)3 \u221a T .\nBy triangle inequality\ndTV (L(X \u03bdsample nh |Usample), \u00b5S)\n\u2264 dTV (X \u03bdsample nh , Z \u03bdsample Nh ) + dTV (Z \u03bdsample Nh , Z\u0304 \u03bdsample T ) + dTV (L(Z\u0304 \u03bdsample T |Usample), \u00b5S) \u2264 16T 2\u03f52score,0 h\u03f52TV \u03c4 + \u03f5TV /2 + \u03f5TV /8 + \u03f5TV /4 \u2264 \u03f5TV\nif 16T 2\u03f52score,0\nh\u03f52TV \u03c4 \u2264 \u03f5TV /8.\nOur choice of parameters satisfies all the conditions mentioned above. Since h \u2264 1/(100\u03b2d), \u03b2 \u2265 1 and L\u0302 \u2265 \u221a d \u03b1 ln(10\u03ba) \u2265 \u221a d \u03b1 ln(\u03b1 \u22121). we can bound L0 by\nL0 \u2264 2L\u0302\u03ba+ \u221a d\n\u03b1 ln((2\u03b1h)\u22121) +\n\u221a 17\n\u03b1 ln(\n8T\nh ) \u2264 3L\u0302\u03ba+ \u221a 17\n\u03b1 ln(8T ) + 2 ln(1/h)\n\u221a d+ 1\n\u03b1\n= 3L\u0302\u03ba+ exp(K) \u221a \u03bad ln(p\u22121\u2217 d\u03f5 \u22121 TV L0\u03baK)\nwhere we use the bound on T and h to bound ln(T ) and ln(1/h). Set L = 50\u03ba \u221a d ln(10\u03ba) + \u221a \u03bad exp(K) ln(d\u03bap\u22121\u2217 \u03f5 \u22121 TV ). Since 3\u03baL\u0302 \u2264 L0 \u2264 5\u03baL\u0302,\nL0 = \u0398(\u03ba 2 \u221a d(ln(10\u03ba) + exp(K) ln(dp\u22121\u2217 \u03f5 \u22121 TV ))).\nWe need to check that h \u2272 \u03f5 4 TV\n(\u03b2L0)4dT but this is true due to the choice of h. Next, we need to check\n16T 2\u03f52score,0 h\u03f52TV \u03c4\n\u2264 \u03f5TV /8 and T 1/2\u03f5score,0 \u2264 \u03b7/10 = \u03f52TV /10. We note that the former implies the latter, and the latter is true since\n\u03f5score \u2264 p 1/2 S \u03f5 2 TV\n\u221a h\n7T and\np \u22121/2 S \u03b2\n\u221a K exp(\u2212 L 2\n160\u03ba )T \u2264\n\u221a h\u03f52TV /20,\nwhich in turn is implied by\nL/ \u221a \u03ba \u2265 exp(K) ln(d\u03bap\u22121\u2217 \u03f5\u22121TV ) \u2265 5 ln(Th \u22121\u03b2Kp\u22121\u2217 \u03f5 \u22121 TV )\nwhich is true for our choice of L, h and T.\nLemma 14. Fix S \u2286 I. For ui and D as defined in Lemma 5, suppose ||ui \u2212 uj || \u2264 L\u2200i, j \u2208 S with L \u2265 10D. Let \u03bd0 be a distribution s.t. supx\u223c\u03bd0 maxi\u2208S ||x \u2212 ui|| \u2264 L\u0303. Let (Z\u0304 \u03bd0 t )t\u22650 the continuous Langevin with score \u2207VS initialized at \u03bd0. Let (Z\u03bd0nh)n\u2208N be the LMC with step size h and score s\u221e s.t. supx\u2208Rd ||s(x) \u2212 \u2207VS(x)|| \u2264 \u03f52score,1. Suppose h \u2264 1/(30\u03b2) then for D\u0303 :=\n6L+O ( \u03baL\u0303+ \u221a d \u03b1 ln((2\u03b1h) \u22121) ) + \u221a ( 16\u03b1 + 200dh) ln(8N), we have\ndTV (Z \u03bd0 T , Z\u0304 \u03bd0 T ) 2 \u2264 h2T\u03b26D\u03036 + hTd\u03b24D\u03034 + \u03f52score,1T/2\nProof. To simplify notations, we omit the superscript \u03bd0 and write Znh and Z\u0304t in the proof instead of Z\u03bd0nh and Z\u0304 \u03bd0 t . Let \u03bd\u0304h be the distribution of Z\u0304h. First, we bound R2(\u03bd\u0304h||\u00b5S). By Lemma 10,\nR2(\u03bd\u0304h||\u00b5S) \u2264 O(\u03b1\u22121(\u03b2L\u0303)2 + d ln((2\u03b1h)\u22121)\nBy Proposition 17, let uS = p\u22121S \u2211 i\u2208S piui then \u00b5S satisfies Assumption 2 so\nP\u00b5S [||x\u2212 uS || \u2265 1.1L+ t] \u2264 exp(\u2212\u03b1t2/4).\nLet N = T/h. By the change of measure argument in (Chewi et al., 2021, Lemma 24), with probability \u2265 1\u2212 \u03b7/2\nmax k\u2208[1,N\u22121]\u2229N\n||Z\u0304kh \u2212 uS || \u2264 1.1L+ \u221a 2\n\u03b1 R2(\u03bd\u0304h||\u00b5S) +\n\u221a 4\n\u03b1 ln\n8N\n\u03b7\n\u2264 1.1L+ \u03baL\u0303+ \u221a \u03b1\u22121d ln((2\u03b1h)\u22121) +\n\u221a 4\n\u03b1 ln\n8N\n\u03b7 .\nBy Proposition 19, this implies that with probability \u2265 1\u2212 \u03b7, for \u03b3 = 16\u03b1 + 200dh\nsup t\u2208[0,T ]\n||Z\u0304t \u2212 uS || \u2264 D\u0303 + \u221a \u03b3 ln(1/\u03b7)\nwith D\u0303 := 6L+O(\u03baL\u0303+ \u221a \u03b1\u22121d ln((2\u03b1h)\u22121)) + \u221a \u03b3 ln(8N). By Proposition 2, this implies, for\np = O(1)\nE[||Z\u0304t \u2212 uS ||p] \u2272 (D\u0303 + \u221a \u03b3)p \u2272 D\u0303p\nwhere we use the fact that \u221a \u03b3 \u2264 \u221a d+16 \u03b1 \u2264 D\u0303/50.\nBy Proposition 18, for t \u2208 [kh, (k + 1)h],\nE[||\u2207V (Z\u0304kh)\u2212\u2207V (Z\u0304t)||2] \u2272 \u221a\nE[A4Hess,1(||Z\u0304kh \u2212 uS ||8 + ||Z\u0304t \u2212 uS ||8) +A4Hess,0]\n\u00d7 \u221a (t\u2212 kh)3 \u222b t kh (A4grad,1E[||Z\u0304s \u2212 uS ||4] +A4grad,0)ds+ d2(t\u2212 kh)2\n\u2272 (A2Hess,1D\u0303 4 +A2Hess,0)(h 2(A2grad,1D\u0303 2 +A2grad,0) + dh)\n\u2272 \u03b24(D\u03034 + L4)(h2\u03b22(D\u03032 + L2) + dh)\n\u2272 \u03b24D\u03034(h2\u03b22D\u03032 + dh)\nwhere in the second inequality, we use the moment bounds for ||Z\u0304s \u2212 uS ||, in the third inequality, we use Proposition 17 to substitute in the parameters AHess,1, AHess,0, Agrad,1, Agrad,0, and in the final bound, we use D\u0303 \u2265 6L. Then by Girsanov\u2019s theorem (see Lemma 3)\n2dTV (Z \u03bd0 T , Z\u0304 \u03bd0 T ) 2 \u2264 E[ \u222b T 0 ||s(Z\u0304\u230at/h\u230bh)\u2212\u2207V (Z\u0304t)||2dt]\n\u2272 \u03f52score,1T + E[ \u222b T 0 ||\u2207V (Z\u0304\u230at/h\u230bh)\u2212\u2207V (Z\u0304t)||2dt]\n\u2272 \u03f52score,1T + h 2T\u03b26D\u03036 + hTd\u03b24D\u03034.\nLemma 15. Suppose the score estimate s satisfies Definition 1. Let ui and D be defined as in Lemma 5. Let S be a connected component of HL with L \u2265 10D. Let (X\u00b5Snh )n\u2208N be the LMC with score estimate s and step size h initialized at \u00b5S and (X\u0304 \u00b5S t )t\u22650 be the continuous Langevin diffusion with score \u2207VS initialized at \u00b5S . Let T = Nh, QT and Q\u0304T denote the distribution of the paths of (X\u00b5Snh )n\u2208[0,T/h]\u2229N and (X\u0304 \u00b5S t )t\u2208[0,T ]. Then for L\u0302 = LK,\n2dTV (Q\u0304T , QT ) 2 \u2264 E [\u222b T 0 ||s(X\u0304\u00b5S\u230at/h\u230bh)\u2212\u2207VS(X\u0304 \u00b5S t )||2dt ] \u2272 2h2T\u03b26L\u03026 + 2hTd\u03b24L\u03024 + T\u03f52score,0\nwith \u03f52score,0 := 3p \u22121 S (\u03f5 2 score + \u03b2 2L28K3 exp(\u2212 L 2 40\u03ba )).\nProof. By Proposition 27, ||ui \u2212 uj || \u2264 L\u0302 for i, j \u2208 S and ||ui \u2212 uj || > L for i \u2208 S, j \u0338\u2208 S. Note that since \u00b5S is the stationary distribution of the continuous Langevin diffusion with score \u2207VS , the law of X\u0304\u00b5St is \u00b5S at all time t. Thus, for t \u2208 [kh, (k + 1)h]\nE[||s(X\u0304\u00b5Skh )\u2212\u2207VS(X\u0304 \u00b5S t )||2]\n\u2264 2(E[||s(X\u0304\u00b5Skh )\u2212\u2207VS(X\u0304 \u00b5S kh )|| 2] + E[||\u2207VS(X\u0304\u00b5Skh )\u2212\u2207VS(X\u0304 \u00b5S t )||2]\n\u2264 2(\u03f52score,0 + \u03b24L\u03024(h2\u03b22L\u03022 + dh)) (14)\nwhere in the second inequality, we use Lemma 13 with R = \u2205 to bound the first term and Proposition 19 and Proposition 2 to bound the second term. The argument is similar to the one in the proof of Lemma 14. Let uS = p\u22121S \u2211 i\u2208S piui then ||ui \u2212 uS || \u2264 L\u2200i \u2208 S. For D\u0303 = D + L\u0302 \u2264 1.1L\u0302 and \u03b3 = 4\u03b1 , since the law of X\u0304 \u00b5S t is \u00b5S , by Proposition 17\nP[||X\u0304\u00b5St \u2212 uS || \u2265 D\u0303 + \u221a \u03b3 ln(1/\u03b7)] \u2264 \u03b7\nthus by Proposition 2 and D\u0303 \u2265 \u221a 100/\u03b1, for p = O(1), E[||X\u0304\u00b5St \u2212uS ||p] \u2272 D\u0303p. By Proposition 19,\nE[||\u2207VS(X\u0304\u00b5Skh )\u2212\u2207VS(X\u0304 \u00b5S t )||2] \u2264 \u03b24(D\u03034 + L\u03024)(h2\u03b22(D\u03032 + L\u03022) + dh)\n\u2272 \u03b24L\u03024(h2\u03b22L\u03022 + dh)\nThe statement follows from integrating Eq. (14) from 0 to T and Girsanov\u2019s theorem (see Lemma 3).\nThis proposition is used in Theorem 6 to go from a set of samples of fixed size M0 to a set of samples with size M that can be arbitrarily large.\nProposition 30. Fix distributions \u00b5sample, \u00b5. For a set Usample \u2286 Rd, let (X \u03bdsample t )t be a process initialized at \u03bdsample, the uniform distribution over Usample. Suppose there exists T > 0, \u03f5TV \u2208 (0, 1) s.t. with probability \u2265 1\u2212 \u03f5TV /10 over the choice of Usample consisting of M0 i.i.d. samples from \u00b5sample, dTV (L(X \u03bdsample T |Usample), \u00b5) \u2264 \u03f5TV /10. Then, for M \u2265 6\u03f5 \u22122 TV M0 log(\u03c4\n\u22121), with probability \u2265 1\u2212 \u03c4 over the choice of Usample consisting of M i.i.d. samples from \u00b5sample,\ndTV (L(X \u03bdsample T |Usample), \u00b5) \u2264 \u03f5TV /2.\nProof. Let Usample be a set of M i.i.d. samples x(1), \u00b7 \u00b7 \u00b7 , x(M) from \u00b5sample. For r \u2208 {1, \u00b7 \u00b7 \u00b7 , \u230aM/M0\u230b} Let Ur = {x(i) : (r \u2212 1)M0 + 1 \u2264 rM0} and U\u2205 = Usample \\ \u22c3 r Ur. Let \u03bdr be the uniform distribution over Ur and \u03bd\u2205 be the uniform distribution over U\u2205. For m = \u230aM/M0\u230b\n\u03bd = M0 M \u2211 r \u03bdr + M \u2212M0m M \u03bd\u2205\nLet \u2126 be the set of U \u2208 (Rd)M0 s.t. dTV (X\u03bdT , \u00b5) \u2264 \u03f5TV /2 with \u03bd being the uniform distribution over U.\nSimilar to the proof of Proposition 23, if we choose M/M0 \u2265 6\u03f5\u22122TV log(\u03c4\u22121), then with probability \u2265 1\u2212 \u03c4, |{r : Ur \u2208 \u2126}| \u2265 m(1\u2212 \u03f5TV /5). By Proposition 9,\ndTV (L(X \u03bdsample T |Usample), \u00b5) \u2264 \u2211 r:Ur\u2208\u2126 M0 M dTV (L(X\u03bdrT ), \u00b5) + M \u2212M0m(1\u2212 \u03f5TV /5) M\n\u2264 \u03f5TV /10 + \u03f52TV /6 + \u03f5TV /5 \u2264 \u03f5TV /2\nwhere in the penultimate inequality, we use the definition of \u2126, M0m \u2264 M and M \u2212m0M \u2264 M0 \u2264 \u03f52TV M/6.\nThe following proposition combined with Theorem 6 implies Corollary 1.\nProposition 31. For a set Usample \u2286 Rd, let (X \u03bdsample t )t be a process initialized at the uniform distribution over Usample. Consider distributions \u00b5C for C \u2208 C. Let \u00b5 = \u2211\npC\u00b5C with pC > 0 and \u2211 pC = 1. Let p\u2217 = min pC . Suppose there exists T > 0, \u03f5TV \u2208 (0, 1) s.t. with probability \u2265 1 \u2212 \u03c410|C| over the choice of UC,sample consisting of M \u2265 M0 i.i.d. samples from \u00b5C , dTV (L(X \u03bdC,sample T |UC,sample), \u00b5C) \u2264 \u03f5TV /10, where \u03bdC,sample is the uniform distribution over UC,sample. Then, for M \u2265 min p\u22121\u2217 {M0, 20\u03f5\u22122TV log(|C|\u03c4\u22121)}, with probability \u2265 1 \u2212 \u03c4 over the choice of Usample consisting of M i.i.d. samples from \u00b5,\ndTV (L(X \u03bdsample T |Usample), \u00b5) \u2264 \u03f5TV .\nProof of Proposition 31. Since \u00b5 = \u2211\nC pC\u00b5C , a sample x (i) from \u00b5 can be drawn by first sampling\nC(i) \u2208 C from the distribution defined by the weights {pC}C\u2208C , then sample from \u00b5C(i) . Consider M i.i.d. samples x(i) using this procedure, and let UC = {x(i) : C(i) = C}. Since M \u2265 20p\u22121\u2217 \u03f5\u22122TV , and E[|UC |] = pCM, by Chernoff\u2019s inequality and union bound, with probability 1\u2212 \u03c4/2 over the randomness of Usample, the following event E1 holds\n\u2200C : | |UC | M \u2212 pC | \u2264 pC\u03f5TV /2\nSuppose E1 holds. Then, |UC |M \u2265 pC(1\u2212 \u03f5TV /2)M \u2265 M0. Thus by union bound, with probability 1 \u2212 \u03f5TV /10 over the randomness of Usample, the following event E2 holds with \u03bdC be the uniform distribution over UC\n\u2200C : dTV (L(X\u03bdCT |UC), \u00b5C) \u2264 \u03f5TV /10 then let \u00b5\u0303 = \u2211\nC |UC | M \u00b5C , by part 1 of Proposition 9,\ndTV (L(X \u03bdsample T |Usample), \u00b5\u0303) = dTV (\u2211 C |UC | M L(X\u03bdCT |UC), \u00b5\u0303 ) \u2264 \u2211 C |UC | M \u03f5TV /10 = \u03f5TV /10\nand dTV (\u00b5\u0303, \u00b5) \u2264 \u2211 C | |UC | M \u2212 pC | \u2264 \u03f5TV /2. Condition on E1 and E2 both hold, which happens with probability 1\u2212 \u03c4, we have\ndTV (L(X \u03bdsample T |Usample), \u00b5) \u2264 dTV (L(X \u03bdsample T |Usample), \u00b5\u0303) + dTV (\u00b5\u0303, \u00b5)\n\u2264 \u03f5TV /10 + \u03f5TV /2 \u2264 \u03f5TV\nH REMOVING THE DEPENDENCY ON p\u2217 = mini\u2208I pi.\nIn this section, we remove the dependency on the minimum weight p\u2217 = mini\u2208I pi. The idea is to consider only the components \u00b5i with significant weight pi i.e. pi \u2265 pthreshold for some chosen threshold pthreshold. In Lemma 17, Theorems 7 and 8, and Corollary 2, we prove analogs of Lemma 11, Theorems 5 and 6, and Corollary 1 respectively with no dependency on p\u2217.\nWe will need modified versions of Lemma 12 and Proposition 25, which are Lemma 16 and Proposition 32 respectively.\nDefinition 7 (Bad set for partition (modified)). Fix S \u2282 I, C\u2217 \u2286 S, S\u2032 = S \\ C\u2217. Suppose we have a partition C = {C1, . . . , Cm} of S\u2032. For x \u2208 Rd, let imax,S\u2032(x) = argmaxi\u2208S\u2032 \u00b5i(x) and \u00b5max,S\u2032(x) = \u00b5imax,S\u2032 (x) = maxi\u2208S\u2032 \u00b5i(x) as in Definition 4. Let Cmax,S\u2032(x) is the unique part of the partition C containing imax,S\u2032(x). For \u03b3 \u2208 (0, 1), \u03b3\u2217 > 0 let\nB\u0303S,C\u2217,C,\u03b3,\u03b3\u2217\n= {x|\u2203j \u2208 S\u2032 \\ Cmax,S\u2032(x) : \u00b5max,S\u2032(x) \u2264 \u03b3\u22121\u00b5j(x) or \u2203j \u2208 C\u2217 : \u00b5max,S\u2032(x) \u2264 \u03b3\u22121\u2217 \u00b5j(x)}\nNote that if C\u2217 = \u2205 then B\u0303S,C\u2217,C,\u03b3,\u03b3\u2217 = BS,C,\u03b3 as defined in Definition 5. If they are clear from context, we omit S,C\u2217, C in the subscript. Lemma 16 (Bad set bound (generalized version of Lemma 12)). Fix S \u2286 I, C\u2217 \u2286 C, C be a partition of S\u2032 = S \\ C\u2217. Let pS = \u2211 i\u2208S pi and p\u0304i = pip \u22121 S . Recall that \u00b5S = \u2211 i\u2208S p\u0304i\u00b5i. For \u03b3, \u03b4 \u2208 (0, 1), define B\u0303\u03b3 = B\u0303S,C\u2217,C,\u03b3,\u03b3\u2217 as in Definition 7 with \u03b3 \u22121 \u2217 = \u03b3 \u22121\u03b4K/8 . Suppose\n1. If i \u2208 C\u2217 then p\u0304i \u2264 \u03b4/8\n2. \u03b4ij \u2264 \u03b4 for i, j which are in S\u2032 and are not in the same part of the partition C of S\u2032\nthen \u00b5S(B\u0303\u03b3) \u2264 \u03b3\u22121\u03b4K2.\nProof of Lemmas 12 and 16. We prove Lemma 16, then Lemma 12 follows immediately by setting C\u2217 = \u2205 in Definition 7. Consider x \u2208 B\u0303\u03b3 s.t. imax,S\u2032(x) = i. For j \u2208 S\u2032, let C(j) denote the unique part of the partition C containing j. Let k = imax 2,S\u2032(x) = argmaxj\u2208S\u2032\\C(i) \u00b5j(x). If j \u2208 C(i) then by definition of imax,S\u2032(x) = i, \u00b5j(x) \u2264 \u00b5i(x). If j \u2208 S\u2032 \\ C(i), then by definition of k, \u00b5j(x) \u2264 \u00b5k(x). Let\nB\u2032\u03b3 = {x | \u2203j \u2208 S\u2032 \\ Cmax,S\u2032(x) : \u00b5max,S\u2032(x) \u2264 \u03b3\u22121\u00b5j(x)}\nand B\u2217 = {x | \u2203j \u2208 C\u2217 : \u00b5max,S\u2032 \u2264 \u03b3\u22121\u2217 \u00b5j(x)}.\nLet p\u0304j = pjp\u22121S for j \u2208 S. If x \u2208 B\u2032\u03b3 , \u00b5i(x) \u2264 \u03b3\u22121\u00b5k(x), and for\n\u00b5S(x) = \u2211 p\u0304j\u00b5j(x) = \u2211\nj\u2208C(i)\npj\u00b5j(x) + \u2211\nj\u2208S\u2032\\C(i) p\u0304j\u00b5j(x) + \u2211 j\u2208C\u2217 p\u0304j\u00b5j(x)\n\u2264 \u2211\nj\u2208C(i)\np\u0304j\u00b5i(x) + \u2211\nj\u2208S\u2032\\C(i) p\u0304j\u00b5k(x) + \u2211 j\u2208C\u2217 p\u0304j\u00b5j(x)\n\u2264 \u2211 j\u2208S\u2032 p\u0304j\u03b3 \u22121\u00b5k(x) + \u2211 j\u2208C\u2217 p\u0304j\u00b5j(x)\n\u2264 \u03b3\u22121\u00b5k(x) + \u2211 j\u2208C\u2217 p\u0304j\u00b5j(x)\nLet p\u0304C\u2217 := \u2211 j\u2208C\u2217 p\u0304j then p\u0304C\u2217 \u2264 K \u00d7 \u03b4/8 \u2264 \u03b3 \u22121\u03b4K/8 since \u03b3\u22121 > 1. For i, k \u2208 S\u2032, let \u2126i,k be the set of x s.t. imax,S\u2032(x) = i and imax 2,S\u2032(x) = k. Since {\u2126i,k|i, k \u2208 S\u2032, C(i) \u0338= C(k)} forms a partition of Rd, we have\n\u00b5S(B \u2032 \u03b3) = \u2211 i,k\u2208S\u2032:C(i) \u0338=C(k) \u222b x\u2208B\u03b3\u2229\u2126i,k \u00b5S(x)dx\n\u2264 \u2211\ni,k:C(i) \u0338=C(k)\n\u222b x\u2208B\u03b3\u2229\u2126i,k (\u03b3\u22121\u00b5k(x) + \u2211 j\u2208C\u2217 p\u0304j\u00b5j(x))dx\n= \u03b3\u22121 \u2211\ni<k:C(i)\u0338=C(k)\n(\u222b x\u2208B\u03b3\u2229\u2126i,k \u00b5k(x)dx+ \u222b x\u2208B\u03b3\u2229\u2126k,i \u00b5i(x)dx )\n+ \u2211 j\u2208C\u2217 p\u0304j \u2211 i,k \u00b5j(B\u03b3 \u2229 \u2126i,k)  = \u03b3\u22121\n\u2211 i<k:C(i)\u0338=C(k) \u222b x\u2208B\u03b3\u2229(\u2126i,k\u222a\u2126k,i) min{\u00b5i(x), \u00b5k(x)}dx+ \u2211 j\u2208C\u2217 p\u0304j\n\u2264 \u03b3\u22121 \u2211\ni<k:C(i)\u0338=C(k)\n\u03b4 + \u03b3\u22121\u03b4K/8\n\u2264 \u03b3\u22121\u03b4K2/2 + \u03b3\u22121\u03b4K/8\nwhere in the penultimate inequality, we use the fact that \u03b4ik \u2264 \u03b4 for i, k which are not in C\u2217 and not in the same part of the partition, and pj \u2264 \u03b4K/2 \u2264 \u03b3\u22121\u03b4K/2 for j \u2208 C\u2217. For i \u2208 C\u2217, let \u2126\u2217i be the set of x s.t. imax,C\u2217 = i. If x \u2208 \u2126\u2217i \u2229B\u2217 then\n\u00b5S(x) = \u2211 j\u2208C\u2217 p\u0304j\u00b5j(x) + \u2211 j\u2208S\u2032 p\u0304j\u00b5j(x) \u2264 \u2211 j\u2208C\u2217 p\u0304j\u00b5i(x) + \u2211 j\u2208S\u2032 p\u0304j\u03b3 \u22121 \u2217 \u00b5i(x) = \u00b5i(x)(p\u0304C\u2217 + \u03b3 \u22121 \u2217 ).\nThus\n\u00b5S(B\u2217) = \u2211 i\u2208C\u2217 \u222b x\u2208B\u2217\u2229\u2126\u2217i \u00b5S(x)dx\n\u2264 \u2211 i\u2208C\u2217 \u222b x\u2208B\u2217\u2229\u2126\u2217i (p\u0304C\u2217 + \u03b3 \u22121 \u2217 )\u00b5i(x)dx\n\u2264 (p\u0304C\u2217 + \u03b3\u22121\u2217 ) \u2211 i\u2208C\u2217 \u00b5i(B\u2217 \u2229 \u2126\u2217i ) \u2264 (\u03b3\u22121\u03b4K/8 + \u03b3\u22121\u03b4K/8)K\nwhere in the last inequality we use the definition of \u03b3\u2217 and the fact that \u00b5i(B\u2217 \u2229 \u2126\u2217i ) \u2264 1. Thus by union bound\n\u00b5S(B\u0303S,C\u2217,C,\u03b3,\u03b3\u2217) \u2264 \u00b5S(B\u2032\u03b3) + \u00b5S(B\u2217) \u2264 \u03b3\u22121\u03b4K2.\nProposition 32 (Absolute gradient difference bound (generalized version of Proposition 25)). Fix S \u2286 I, C\u2217 \u2286 S. Let S\u2032 = S \\C\u2217. For i \u2208 S, let p\u0304i = pip\u22121S and recall that \u00b5S(x) = \u2211 i\u2208S p\u0304i\u00b5i(S). Suppose p\u0304j \u2264 \u03b48 for j \u2208 C\u2217. Let i := imax,S\u2032(x) = argmaxi\u2032\u2208S\u2032 \u00b5i\u2032(x). Suppose i \u2208 C \u2286 S \u2032 and\n1. \u00b5i(x) \u2265 \u03b3\u22121\u00b5j(x)\u2200j \u2208 S\u2032 \\ C\n2. \u00b5i(x) \u2265 \u03b3\u22121\u2217 \u00b5j(x)\u2200j \u2208 C\u2217 where \u03b3\u22121\u2217 = \u03b3\u22121\u03b4K/8.\nLet GS(x) = maxi\u2208S ||\u2207Vi(x)|| then\n||\u2207VS(x)\u2212\u2207VC(x)|| \u2264 4\u03b3\np\u0304i GS(x)\nProof of Proposition 32 and Proposition 25. We prove Proposition 32, then Proposition 25 follows immediately by setting C\u2217 = \u2205. For C \u2032 \u2286 S, let p\u0304C\u2032 = \u2211 i\u2208C\u2032 p\u0304i. By Proposition 6, we can write\n\u2207VS(x)\u2212\u2207VC(x) = p\u0304C\u00b5C(x)\u2207VC(x) +\n\u2211 j\u2208S\\C p\u0304j\u00b5j(x)\u2207Vj(x)\n\u00b5S(x) \u2212\u2207VC(x)\n= p\u0304C\u00b5C(x)\u2207VC(x) +\n\u2211 j\u2208S\\C p\u0304j\u00b5j(x)\u2207Vj(x)\np\u0304C\u00b5C(x) + \u2211 j\u2208S\\C p\u0304j\u00b5j(x) \u2212\u2207VC(x)\n= \u2211\nj\u2208S\\C\np\u0304j\u00b5j(x) p\u0304C\u00b5C(x) + \u2211 j\u2208S\\C p\u0304j\u00b5j(x) (\u2207Vj(x)\u2212\u2207VC(x))\nFor j \u2208 S\u2032 \\ C, p\u0304C\u00b5C(x) + \u2211 j\u2032\u2208S\\C p\u0304j\u00b5j(x)\np\u0304j\u00b5j(x) \u2265 p\u0304i\u00b5i(x) p\u0304j\u00b5j(x) \u2265 p\u0304i p\u0304j \u03b3\u22121\nand for j \u2208 C\u2217, using the upper bound on pj and the assumption \u00b5i(x) \u2265 \u03b3\u22121\u2217 \u00b5j(x) p\u0304C\u00b5C(x) + \u2211 j\u2032\u2208S\\C p\u0304j\u2032\u00b5j(x)\np\u0304j\u00b5j(x) \u2265 p\u0304i\u00b5i(x) p\u0304j\u00b5j(x) \u2265 p\u0304i\u03b3\n\u22121 \u2217\np\u0304j \u2265 p\u0304iK\u03b3\u22121\nNext, by Proposition 6, ||\u2207VC(x)|| \u2264 GS(x) thus,\n||\u2207VS(x)\u2212\u2207VC(x)|| \u2264 2GS(x)\u03b3  \u2211 j\u2208S\\(C\u222aC\u2217) p\u0304j p\u0304i + \u2211 j\u2208C\u2217 1 Kp\u0304i  \u2264 4\u03b3GS(x) p\u0304i\nThe following is a modified version of Lemma 11. Lemma 17. Fix \u03f5TV , \u03c4 \u2208 (0, 1/2), \u03b4 \u2208 (0, 1]. Fix S \u2286 I. Let p\u0304i = pip\u22121S and recall that \u00b5S =\u2211\ni\u2208S p\u0304i\u00b5i. Suppose for i \u2208 S, \u00b5i are \u03b1-strongly log-concave and \u03b2-smooth with \u03b2 \u2265 1.Let ui = argminx Vi(x) and D \u2265 5 \u221a d \u03b1 be as defined in Lemma 5. Suppose there exists L \u2265 10D such that for any i, j \u2208 S, ||ui \u2212 uj || \u2264 L. Fix p\u2217 > 0. Let S\u2032 = {i \u2208 S : p\u0304i \u2265 p\u2217} and C\u2217 = S \\ S\u2032. Let G\u03b4 := G\u03b4(S\u2032, E) be the graph on S\u2032 with an edge between i, j iff \u03b4ij \u2264 \u03b4. Let\nT = 2Cp\u2217,K \u03b4\u03b1\n( ln( \u03b22L\n\u03b1 ) + ln ln \u03c4\u22121 + 2 ln \u03f5\u0303\u22121TV\n) .\nand\n\u03b4\u2032 = \u03b43/2\u03b13/2p 5/2 \u2217 \u03f5 2 TV \u03c4\n105K5d(\u03b2L)3 ln3/2(p\u22121\u2217 ) ln 3/2 \u03b22L\u03f5\n\u22121 TV ln \u03c4 \u22121\n\u03b1 ln 2.51 16d(\u03b2L)2\n\u03f5TV \u03c4\u03b4\u03b1\n.\nSuppose maxi\u2208C\u2217 p\u0304i \u2264 \u03b4\u2032/8 and for all i, j in S\u2032 that are not in the same connected component of G\u03b4 , \u03b4ij \u2264 \u03b4\u2032.\nFor x \u2208 Rd, let (X\u0304\u03b4xt )t\u22650 denote the continuous Langevin diffusion with score \u2207VS initialized at \u03b4x. Let Cmax,S\u2032 be the unique connected component of G\u03b4 containing imax,S\u2032(x) = argmaxi\u2032\u2208S\u2032 \u00b5i\u2032(x).\nPx\u223c\u00b5S [dTV (L(X\u0304 \u03b4x T |x), \u00b5Cmax,S\u2032 (x)) \u2264 \u03f5TV ] \u2265 1\u2212 \u03c4\nProof. The proof is same as Lemma 11, but we replace Lemma 12 with Lemma 16, Proposition 25 with Proposition 32 and Proposition 21 with Proposition 22. Note that we use \u03b3 = p\u2217\u03f5TV\n100L\u0303 \u221a T\nand B\u0303\u03b3 as defined in Lemma 16 to ensure that for y \u0338\u2208 B\u0303\u03b3 , ||\u2207VCmax,S\u2032 (y)(y) \u2212 \u2207VS(y)|| \u2264 4\u03b3(\u03b2L \u221a ln(\u03b2L\u03f5\u22121TV \u03c4 \u22121T ))\np\u2217 \u2264 \u03f5TV 10 \u221a T so that we can bound the total variation distance between the continuous Langevin diffusions with scores \u2207VS and \u2207VCmax,S\u2032 (x) by \u03f5TV /10.\nTheorem 7. Fix \u03f5TV , \u03c4 \u2208 (0, 1/2). Fix S \u2286 I. Suppose for i \u2208 S, \u00b5i are \u03b1-strongly log-concave and \u03b2-smooth with \u03b2 \u2265 1. Let ui = argminx Vi(x) and D \u2265 5 \u221a d \u03b1 be as defined in Lemma 5. Suppose there exists L \u2265 10D such that for any i, j \u2208 S, ||ui \u2212 uj || \u2264 L. Let Usample be a set of M i.i.d. samples from \u00b5S and \u03bdsample be the uniform distribution over Usample. Let (X\u0304 \u03bdsample t )t\u22650 be the continuous Langevin diffusion with score \u00b5S initialized at \u03bdsample. For M \u2265 105(\u03f53TV )\u22121K3 log(K\u03c4\u22121) and\nT \u2265 \u0398 \u03b1\u22121 108d(\u03b2L)3exp(K) ln5 16d(\u03b2L)2\u03f5TV \u03b1\n\u03f53TV \u03b1 3/2\nexp(20K) \nthen PUsample [dTV (L(X\u0304 \u03bdsample t |Usample), \u00b5S) \u2264 \u03f5TV ] \u2265 1\u2212 \u03c4\nProof. For i \u2208 S, let p\u0304i = pip\u22121S . As in Lemma 17, fix p0,\u2217 = 1 K and let S \u2032 0 = {i \u2208 S : p\u0304i \u2265 p0,\u2217}, C0,\u2217 = S \\ S\u20320 then S\u20320 \u0338= \u2205, since there must be at least one i s.t. p\u0304i \u2265 1K . By the same argument as in proof of Theorem 5, we take the sequence 1 = \u03b40,0 > \u03b40,1 > \u00b7 \u00b7 \u00b7 > \u03b40,K\u22121 where we use the notation \u03b40,s to emphasizes its dependency on p0,\u2217. If maxi\u2208C0,\u2217 p\u0304i < \u03b40,K\u22121 8 then Lemma 17 applies. More precisely, we will use Proposition 24 and the inductive argument on \u03b40,s as in the proof of Theorem 5 to show that the continuous Langevin diffusion initialized at M samples will converge to \u00b5S after a suitable time T defined by \u03b40,K\u22121. If this is not the case, then we let p1,\u2217 = \u03b40,K\u22121 8 and S \u2032 1 = {i \u2208 S : pi \u2265 p1,\u2217} then |S\u20321| \u2265 |S\u20320| + 1. In general, we inductively set ps+1,\u2217 = \u03b4s,K\u22121\n8 . If maxi\u2208Cs,\u2217 pi \u2264 ps+1,\u2217 for some s \u2264 K \u2212 2 then we are done, else CK\u22121,\u2217 = \u2205 thus mini\u2208S p\u0304i \u2265 pK\u22121,\u2217 and we can use Theorem 5. In all cases, for p\u2217 = pK\u22121,\u2217, the continuous Langevin diffusion initialized at samples converges to \u00b5S after time\nT \u2265 \u0398 ( \u03b1\u22121K2p\u22121\u2217 ln(10p \u22121 \u2217 )\u03b4 \u22121 K\u22121,K\u22121 ) = \u0398(\u03b1\u22121\u039e\u2212 exp(20K))\nTo justify the above equation, we lower bound p\u2217 = pK\u22121,\u2217 and \u03b4K\u22121,K\u22121.\nLet \u0393\u0303s = p7/2s,\u2217 \u03f5 3 TV \u03b1 3/2\n8000d(\u03b2L)3exp(K) ln3/2(p\u22121s,\u2217) ln 5 16d(\u03b2L)\n2\n\u03f5TV \u03b1\n\u2265 p 3.51 s,\u2217 \u03f5 3 TV \u03b1 3/2\n105d(\u03b2L)3exp(K) ln5 16d(\u03b2L)2\n\u03f5TV \u03b1\nthen\n\u03b4s,K\u22121 \u2265 \u0393\u03032((3/2) K\u22121) s \u2265 p 7.02((3/2)K\u22121) s,\u2217 \u039e\nwith \u039e = ( \u03f5 3 TV \u03b1 3/2\n105d(\u03b2L)3exp(K) ln5 16d(\u03b2L)2\n\u03f5TV \u03b1\n)2((3/2) K\u22121) and we can prove by induction on s that\nps,\u2217 \u2265 K\u2212 exp(10(s+1))\u039eexp(2(s+1)) \u2265 \u039eexp(4.9(s+1)),\nthus\n\u03b4\u22121K\u22121,K\u22121 \u2264 p \u22127.02((3/2)K\u22121) K\u22121,\u2217 \u039e \u2264 (\u039e exp(4.9K))\u22127.02((3/2) K\u22121) \u00b7 \u039e \u2264 \u039e\u2212 exp(12K)\nTheorem 8. Suppose each \u00b5i is \u03b1 strongly-log-concave and \u03b2-smooth for all i with \u03b2 \u2265 1. Let ui = argminVi(x). Set\nL0 = \u0398 ( \u03ba2K \u221a d(ln(10\u03ba) + exp(60K) ln(d\u03f5\u22121TV )) ) .\nLet S be a connected component of HL, where there is an edge between i, j if ||ui \u2212 uj || \u2264 L := L0/(\u03baK). Let Usample be a set of M i.i.d. samples from \u00b5S and \u03bdsample be the uniform distribution over Usample. Let (X \u03bdsample nh )n\u2208N be the LMC with score s and step size h initialized at \u03bdsample. Set\nT = \u0398 \u03b1\u22121 108d(\u03b2L0)3exp(K) ln5 16d(\u03b2L0)2\u03f5TV \u03b1\n\u03f53TV \u03b1 3/2\nexp(20K) \nLet the step size h = \u0398( \u03f5 4 TV\n100(\u03b2L0)4dT ). Suppose pS \u2265 \u03f5TVK and s satisfies Definition 1\nwith \u03f5score \u2264 \u03f5 5/2 TV\n\u221a h\n7 \u221a KT\n\u2264 p 1/2 S \u03f5 2 TV\n\u221a h\n7T . Suppose the number of samples M satisfies M \u2265 107\u03f5\u22125TV K 3 log(K\u03f5\u22121TV ) log(\u03c4 \u22121), then\nPUsample [dTV (L(X \u03bdsample T | Usample), \u00b5S) \u2264 \u03f5TV ] \u2265 1\u2212 \u03c4\nProof. The proof is identical to proof of Theorem 6, but we plug in T from Theorem 7 instead of Theorem 5. With the same setup as in proof of Theorem 6, \u03f52score,0 = 3p \u22121 S (\u03f5 2 score+8\u03b2 2K exp(\u2212 L 2\n80\u03ba )), thus as long as we assume pS \u2265 \u03f5TVK , we can ensure that with our choice of L and \u03f5score, \u03f5score,0 \u2264 p 1/2 S \u03f5 2 TV \u221a h\n7T as required.\nCorollary 2. Suppose \u00b5i is \u03b1 strongly-log-concave and \u03b2-smooth for all i with \u03b2 \u2265 1. Suppose s satisfies Definition 1. Let Usample be a set of M i.i.d. samples from \u00b5 and \u03bdsample be the uniform distribution over Usample. With T, h, \u03f52score as in Theorem 8 and M \u2265 108\u03f5\u22126TV K4 log(K\u03f5 \u22121 TV ) log(K\u03c4\n\u22121). Let (X\u03bdsamplenh )n\u2208N be the LMC with score s and step size h initialized at \u03bdsample, then\nPUsample [dTV (L(X \u03bdsample T | Usample), \u00b5) \u2264 \u03f5TV ] \u2265 1\u2212 \u03c4\nProof. This is a consequence of Theorem 8 and Proposition 33. Here we apply Proposition 33 with M0 = 10 7\u03f5\u22125TV K 3 log(K\u03f5\u22121TV ) log(K\u03c4 \u22121).\nTo remove dependency on p\u2217, we will use the following variant of Proposition 31. Proposition 33. For a set Usample \u2286 Rd, let (X \u03bdsample t )t be a process initialized at the uniform\ndistribution \u03bdsample over Usample. Consider distributions \u00b5C for C \u2208 C. Let \u00b5 = \u2211\npC\u00b5C with pC > 0 and \u2211 pC = 1. Suppose if pC \u2265 \u03f5TV8|C| , there exists T > 0, \u03f5TV \u2208 (0, 1) s.t. with probability \u2265 1 \u2212 \u03c410|C| over the choice of UC,sample consisting of M \u2265 M0 i.i.d. samples from \u00b5C , dTV (L(X \u03bdC,sample T |UC,sample), \u00b5C) \u2264 \u03f5TV /10 where \u03bdC,sample is the uniform distribution over UC,sample. Then, for M \u2265 ( \u03f5TV8|C| ) \u22121 min{M0, 20\u03f5\u22122TV log(|C|\u03c4\u22121)}, with probability \u2265 1\u2212 \u03c4 over the choice of Usample consisting of M i.i.d. samples from \u00b5, dTV (L(X \u03bdsample T |Usample), \u00b5) \u2264 \u03f5TV\nProof of Proposition 33. The proof is analogous to Proposition 31. We use the same setup and will spell out the differences between the two proofs. Let C\u2032 = {C \u2208 C : pC \u2265 \u03f5TV8|C| }. We redefine the event E1 as\n\u2200C \u2208 C\u2032 : | |UC | M \u2212 pC | \u2264 pC\u03f5TV /8\nand E2 as, for \u03bdC be the uniform distribution over \u00b5C\n\u2200C \u2208 C\u2032 : dTV (L(X\u03bdCT |UC), \u00b5C) \u2264 \u03f5TV /10\nLet U\u2205 = Usample \\ \u22c3 C\u2208C\u2032 C then\n|U\u2205| M = \u2211 C \u0338\u2208C\u2032 |UC | M \u2264 1\u2212 \u2211 C\u2208C\u2032 pC(1\u2212 \u03f5TV /8) \u2264 1\u2212 (1\u2212 \u03f5TV /8)(1\u2212 \u03f5TV /8) \u2264 \u03f5TV /4.\nSuppose E1 and E2 both hold, which occur with probability 1 \u2212 \u03c4 by Chernoff\u2019s inequality. Let \u00b5\u0303 = \u2211 C\u2208C |UC | M \u00b5C . By part 1 of Proposition 9\ndTV (L(X \u03bdsample T |Usample), \u00b5\u0303) = dTV (\u2211 C |UC | M L(X\u03bdCT |UC), \u2211 C |UC | M \u00b5C )\n\u2264 \u2211 C\u2208C\u2032 |UC | M \u03f5TV /10 + \u2211 C \u0338\u2208C\u2032 |UC | M \u2264 \u03f5TV /10 + \u03f5TV /4 \u2264 \u03f5TV /2\nBy part 2 of Proposition 9\u2211 C | |UC | M \u2212 pC | \u2264 \u2211 C\u2208C\u2032 pC\u03f5TV /8 + \u2211 C \u0338\u2208C\u2032 max{ |UC | M ,pC} \u2264 \u03f5TV /8 + \u03f5TV /4\nBy triangle inequality\ndTV (L(X \u03bdsample T |Usample), \u00b5) \u2264 dTV (L(X \u03bdsample T |Usample), \u00b5\u0303) + dTV (\u00b5\u0303, \u00b5)\n\u2264 \u03f5TV /2 + 3\u03f5TV /8 \u2264 \u03f5TV"
        },
        {
            "heading": "I ADDITIONAL SIMULATIONS",
            "text": "In this section we give some additional details about the simulations in the main text as well as a few supplementary ones.\nFor the simulation in Figure 1 of the main text, the estimated score function was learned from data by running 3\u00d7 105 steps of stochastic gradient descent without batching, using a fresh sample at each step with learning rate 10\u22125. The loss function was the vanilla score matching loss from Hyv\u00e4rinen (2005). The neural network architecture used had a single hidden layer with tanh nonlinearity and 2048 units. The stationary distribution shown in the rightmost subfigure was computed by numerical integration of the estimated score.\nFor the 32-dimensional simulation in Figure 2 of the main text, to train the network we used ADAM with a batch size of 256 examples, again generated fresh each time; we used 200 batches per epoch and 300 epochs and we learned the vanilla score function using an equivalent denoising formulation as in Vincent (2011). Figure 3 is the same but the network was trained for only 30 epochs. In Figure 4, we performed the same experiment as Figure 2 but we used Contrastive Divergence (CD) training Hinton (2012), which has been used by numerous experimental papers in the literature, instead of score matching as the mechanism to learn the approximate gradient. More precisely, we used CD (again trained over 300 epochs) to learn a distribution of the form exp(f(x)) where the potential f was parameterized by a 8192 unit one-hidden-layer neural network with tanh activations. Once this network is learned, \u2207f was used as the approximate score function since this is the score function of the learned distribution. We also observed in Figure 5 that the score matching loss, which was explicitly trained in the other figures, is also monotonically decreasing over time under CD training. The fact that the behavior is somewhat similar under CD and score matching is morally in agreement with theoretical connections between the two observed by Hyv\u00e4rinen (2007b). Note that in all three of these figures, the same random seeds were used so that colored trajectories will correspond to each other."
        }
    ],
    "year": 2023
}