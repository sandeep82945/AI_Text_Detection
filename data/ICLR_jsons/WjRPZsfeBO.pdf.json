{
    "abstractText": "Variational Autoencoders (VAEs) have gained significant popularity among researchers as a powerful tool for understanding unknown distributions based on limited samples. This popularity stems partly from their impressive performance and partly from their ability to provide meaningful feature representations in the latent space. Wasserstein Autoencoders (WAEs), a variant of VAEs, aim to not only improve model efficiency but also interpretability. However, there has been limited focus on analyzing their statistical guarantees. The matter is further complicated by the fact that the data distributions to which WAEs are applied such as natural images are often presumed to possess an underlying low-dimensional structure within a high-dimensional feature space, which current theory does not adequately account for, rendering known bounds inefficient. To bridge the gap between the theory and practice of WAEs, in this paper, we show that WAEs can learn the data distributions when the network architectures are properly chosen. We show that the convergence rates of the expected excess risk in the number of samples for WAEs are independent of the high feature dimension, instead relying only on the intrinsic dimension of the data distribution.",
    "authors": [
        {
            "affiliations": [],
            "name": "WASSERSTEIN AUTOEN"
        },
        {
            "affiliations": [],
            "name": "Saptarshi Chakraborty"
        },
        {
            "affiliations": [],
            "name": "Peter L. Bartlett"
        }
    ],
    "id": "SP:7f56a6210142ebd3a1c79a15b260c090d03bb706",
    "references": [
        {
            "authors": [
                "Martin Anthony",
                "Peter Bartlett"
            ],
            "title": "Neural network learning: Theoretical foundations",
            "year": 2009
        },
        {
            "authors": [
                "Martin Arjovsky",
                "Soumith Chintala",
                "L\u00e9on Bottou"
            ],
            "title": "Wasserstein generative adversarial networks",
            "venue": "In International Conference on Machine Learning,",
            "year": 2017
        },
        {
            "authors": [
                "Peter L Bartlett",
                "Shahar Mendelson"
            ],
            "title": "Rademacher and Gaussian Complexities: Risk Bounds and Structural Results",
            "venue": "Journal of Machine Learning Research,",
            "year": 2002
        },
        {
            "authors": [
                "Peter L Bartlett",
                "Nick Harvey",
                "Christopher Liaw",
                "Abbas Mehrabian"
            ],
            "title": "Nearly-tight VC-dimension and pseudodimension bounds for piecewise linear neural networks",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2019
        },
        {
            "authors": [
                "St\u00e9phane Boucheron",
                "G\u00e1bor Lugosi",
                "Pascal Massart"
            ],
            "title": "Concentration inequalities: A nonasymptotic theory of independence",
            "year": 2013
        },
        {
            "authors": [
                "Anish Chakrabarty",
                "Swagatam Das"
            ],
            "title": "Statistical regeneration guarantees of the wasserstein autoencoder with latent space consistency",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Minshuo Chen",
                "Wenjing Liao",
                "Hongyuan Zha",
                "Tuo Zhao"
            ],
            "title": "Distribution approximation and statistical estimation guarantees of generative adversarial networks",
            "venue": "arXiv preprint arXiv:2002.03938,",
            "year": 2020
        },
        {
            "authors": [
                "Biraj Dahal",
                "Alexander Havrilla",
                "Minshuo Chen",
                "Tuo Zhao",
                "Wenjing Liao"
            ],
            "title": "On deep generative models for approximation and estimation of distributions on manifolds",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Jia Deng",
                "Wei Dong",
                "Richard Socher",
                "Li-Jia Li",
                "Kai Li",
                "Li Fei-Fei"
            ],
            "title": "Imagenet: A large-scale hierarchical image database",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2009
        },
        {
            "authors": [
                "Jeff Donahue",
                "Philipp Kr\u00e4henb\u00fchl",
                "Trevor Darrell"
            ],
            "title": "Adversarial feature learning",
            "venue": "In International Conference on Learning Representations,",
            "year": 2017
        },
        {
            "authors": [
                "Rafael G\u00f3mez-Bombarelli",
                "Jennifer N Wei",
                "David Duvenaud",
                "Jos\u00e9 Miguel Hern\u00e1ndez-Lobato",
                "Benjam\u0131\u0301n S\u00e1nchez-Lengeling",
                "Dennis Sheberla",
                "Jorge Aguilera-Iparraguirre",
                "Timothy D Hirzel",
                "Ryan P Adams",
                "Al\u00e1n Aspuru-Guzik"
            ],
            "title": "Automatic chemical design using a data-driven continuous representation of molecules",
            "venue": "ACS central science,",
            "year": 2018
        },
        {
            "authors": [
                "Ian Goodfellow",
                "Jean Pouget-Abadie",
                "Mehdi Mirza",
                "Bing Xu",
                "David Warde-Farley",
                "Sherjil Ozair",
                "Aaron Courville",
                "Yoshua Bengio"
            ],
            "title": "Generative Adversarial Nets",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2014
        },
        {
            "authors": [
                "Karol Gregor",
                "Ivo Danihelka",
                "Alex Graves",
                "Danilo Rezende",
                "Daan Wierstra"
            ],
            "title": "Draw: A recurrent neural network for image generation",
            "venue": "In International Conference on Machine Learning,",
            "year": 2015
        },
        {
            "authors": [
                "Arthur Gretton",
                "Karsten M. Borgwardt",
                "Malte J. Rasch",
                "Bernhard Sch\u00f6lkopf",
                "Alexander Smola"
            ],
            "title": "A kernel two-sample test",
            "venue": "Journal of Machine Learning Research,",
            "year": 2012
        },
        {
            "authors": [
                "Ishaan Gulrajani",
                "Faruk Ahmed",
                "Martin Arjovsky",
                "Vincent Dumoulin",
                "Aaron C Courville"
            ],
            "title": "Improved Training of Wasserstein GANs",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Martin Heusel",
                "Hubert Ramsauer",
                "Thomas Unterthiner",
                "Bernhard Nessler",
                "Sepp Hochreiter"
            ],
            "title": "Gans trained by a two time-scale update rule converge to a local nash equilibrium",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Irina Higgins",
                "Loic Matthey",
                "Arka Pal",
                "Christopher Burgess",
                "Xavier Glorot",
                "Matthew Botvinick",
                "Shakir Mohamed",
                "Alexander Lerchner"
            ],
            "title": "beta-vae: Learning basic visual concepts with a constrained variational framework",
            "venue": "In International Conference on Learning Representations,",
            "year": 2017
        },
        {
            "authors": [
                "Jian Huang",
                "Yuling Jiao",
                "Zhen Li",
                "Shiao Liu",
                "Yang Wang",
                "Yunfei Yang"
            ],
            "title": "An error analysis of generative adversarial networks for learning distributions",
            "venue": "Journal of Machine Learning Research,",
            "year": 2022
        },
        {
            "authors": [
                "Hisham Husain",
                "Richard Nock",
                "Robert C Williamson"
            ],
            "title": "A primal-dual link between gans and autoencoders",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Alan F. Karr"
            ],
            "title": "Probability. Springer Texts in Statistics. Springer New York, NY, 1 edition, 1993. doi: 10.1007/978-1-4612-0891-4",
            "venue": "URL https://doi.org/10.1007/",
            "year": 1993
        },
        {
            "authors": [
                "Jisu Kim",
                "Jaehyeok Shin",
                "Alessandro Rinaldo",
                "Larry Wasserman"
            ],
            "title": "Uniform convergence rate of the kernel density estimator adaptive to intrinsic volume dimension",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Diederik P. Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "International Conference on Learning Representations,",
            "year": 2015
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Max Welling"
            ],
            "title": "Auto-encoding variational bayes",
            "venue": "In International Conference on Learning Representations,",
            "year": 2014
        },
        {
            "authors": [
                "Frederic Koehler",
                "Viraj Mehta",
                "Chenghui Zhou",
                "Andrej Risteski"
            ],
            "title": "Variational autoencoders in the presence of low-dimensional data: landscape and implicit bias",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Andrey N Kolmogorov",
                "Vladimir Mikha\u0131lovich Tikhomirov"
            ],
            "title": "\u03b5-entropy and \u03b5-capacity of sets in function spaces",
            "venue": "Translations of the American Mathematical Society,",
            "year": 1961
        },
        {
            "authors": [
                "Tengyuan Liang"
            ],
            "title": "How well generative adversarial networks learn distributions",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2021
        },
        {
            "authors": [
                "Hao Liu",
                "Alex Havrilla",
                "Rongjie Lai",
                "Wenjing Liao"
            ],
            "title": "Deep nonparametric estimation of intrinsic data structures by chart autoencoders: Generalization error and robustness",
            "venue": "arXiv preprint arXiv:2303.09863,",
            "year": 2023
        },
        {
            "authors": [
                "Shiao Liu",
                "Yunfei Yang",
                "Jian Huang",
                "Yuling Jiao",
                "Yang Wang"
            ],
            "title": "Non-asymptotic error bounds for bidirectional gans",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Alireza Makhzani",
                "Jonathon Shlens",
                "Navdeep Jaitly",
                "Ian Goodfellow",
                "Brendan Frey"
            ],
            "title": "Adversarial autoencoders",
            "venue": "In International Conference on Learning Representations,",
            "year": 2016
        },
        {
            "authors": [
                "Lars Mescheder",
                "Sebastian Nowozin",
                "Andreas Geiger"
            ],
            "title": "Adversarial variational bayes: Unifying variational autoencoders and generative adversarial networks",
            "venue": "In Proceedings of the 34th International Conference on Machine Learning-Volume",
            "year": 2017
        },
        {
            "authors": [
                "Ryumei Nakada",
                "Masaaki Imaizumi"
            ],
            "title": "Adaptive approximation and generalization of deep neural network with intrinsic dimensionality",
            "venue": "Journal of Machine Learning Research,",
            "year": 2020
        },
        {
            "authors": [
                "Phil Pope",
                "Chen Zhu",
                "Ahmed Abdelkader",
                "Micah Goldblum",
                "Tom Goldstein"
            ],
            "title": "The intrinsic dimension of images and its impact on learning",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Aditya Ramesh",
                "Mikhail Pavlov",
                "Gabriel Goh",
                "Scott Gray",
                "Chelsea Voss",
                "Alec Radford",
                "Mark Chen",
                "Ilya Sutskever"
            ],
            "title": "Zero-shot text-to-image generation",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Michal Rolinek",
                "Dominik Zietlow",
                "Georg Martius"
            ],
            "title": "Variational autoencoders pursue pca directions (by accident)",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Mark Rudelson",
                "Roman Vershynin"
            ],
            "title": "Hanson-Wright inequality and sub-gaussian concentration",
            "venue": "Electronic Communications in Probability,",
            "year": 2013
        },
        {
            "authors": [
                "Nicolas Schreuder",
                "Victor-Emmanuel Brunel",
                "Arnak Dalalyan"
            ],
            "title": "Statistical guarantees for generative models without domination",
            "venue": "In Algorithmic Learning Theory,",
            "year": 2021
        },
        {
            "authors": [
                "Kihyuk Sohn",
                "Honglak Lee",
                "Xinchen Yan"
            ],
            "title": "Learning structured output representation using deep conditional generative models",
            "venue": "Advances in neural information processing systems,",
            "year": 2015
        },
        {
            "authors": [
                "Hideyuki Tachibana",
                "Katsuya Uenoyama",
                "Shunsuke Aihara"
            ],
            "title": "Efficiently trainable text-to-speech system based on deep convolutional networks with guided attention",
            "venue": "IEEE international conference on acoustics, speech and signal processing (ICASSP),",
            "year": 2018
        },
        {
            "authors": [
                "Rong Tang",
                "Yun Yang"
            ],
            "title": "On empirical bayes variational autoencoder: An excess risk bound",
            "venue": "In Conference on Learning Theory,",
            "year": 2021
        },
        {
            "authors": [
                "Aaron Van Den Oord",
                "Oriol Vinyals"
            ],
            "title": "Neural discrete representation learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Roman Vershynin"
            ],
            "title": "High-dimensional probability: An introduction with applications in data science, volume 47",
            "year": 2018
        },
        {
            "authors": [
                "Martin J Wainwright"
            ],
            "title": "High-dimensional statistics: A Non-asymptotic Viewpoint, volume 48",
            "year": 2019
        },
        {
            "authors": [
                "Zichao Yang",
                "Zhiting Hu",
                "Ruslan Salakhutdinov",
                "Taylor Berg-Kirkpatrick"
            ],
            "title": "Improved variational autoencoders for text modeling using dilated convolutions",
            "venue": "In International Conference on Machine Learning,",
            "year": 2017
        },
        {
            "authors": [
                "Dmitry Yarotsky"
            ],
            "title": "Error bounds for approximations with deep relu networks",
            "venue": "Neural Networks,",
            "year": 2017
        },
        {
            "authors": [
                "Shengjia Zhao",
                "Jiaming Song",
                "Stefano Ermon"
            ],
            "title": "Infovae: Information maximizing variational autoencoders",
            "venue": "arXiv preprint arXiv:1706.02262,",
            "year": 2017
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "The problem of understanding and possibly simulating samples from an unknown distribution only through some independent realization of the same is a key question for the machine learning community. Parallelly with the appearance of Generative Adversarial Networks (GANs) (Goodfellow et al., 2014), Variational Autoencoders (Kingma & Welling, 2014) have also gained much attention not only due to their useful feature representation properties in the latent space but also for data generation capabilities. It is important to note that in GANs, the generator network learns to create new samples that are similar to the training data by fooling the discriminator network. However, GANs and their popular variants do not directly provide a way to manipulate the generated data or explore the latent space of the generator. On the other hand, a VAE learns a latent space representation of the input data and allows for interpolation between the representations of different samples. Several variants of VAEs have been proposed to improve their generative performance. One popular variant is the conditional VAE (CVAE) (Sohn et al., 2015), which adds a conditioning variable to the generative model and has shown remarkable empirical success. Other variants include InfoVAE (Zhao et al., 2017), \u03b2-VAE (Higgins et al., 2017), and VQ-VAE (Van Den Oord et al., 2017), etc., which address issues such as disentanglement, interpretability, scalability, etc. Recent works have shown the effectiveness of VAEs and their variants in a variety of applications, including image (Gregor et al., 2015) and text generation (Yang et al., 2017), speech synthesis (Tachibana et al., 2018), and drug discovery (Go\u0301mez-Bombarelli et al., 2018). A notable example is the DALL-E model (Ramesh et al., 2021), which uses a VAE to generate images from textual descriptions.\nHowever, despite their effectiveness in unsupervised representation learning, VAEs have been heavily criticized for their poor performance in approximating multi-modal distributions. Influenced by the superior performance of GANs, researchers have attempted to leverage this advantage of adversarial losses by incorporating them into VAE objective (Makhzani et al., 2016; Mescheder et al., 2017). Wasserstein Autoencoder (WAEs) (Tolstikhin et al., 2018) tackles the problem from an opti-\nmal transport viewpoint. Incorporating such a GAN-like architecture, not only preserves the latent space representation that is unavailable in GANs but also enhances data generation capabilities. Both VAEs and WAEs attempt to minimize the sum of a reconstruction cost and a regularizer that penalizes the difference between the distribution induced by the encoder and the prior distribution on the latent space. While VAEs force the encoder to match the prior distribution for each input example, which can lead to overlapping latent codes and reconstruction issues, WAEs force the continuous mixture of the encoder distribution over all input examples to match the prior distribution, allowing different examples to have more distant latent codes and better reconstruction results. Furthermore, the use of the Wasserstein distance allows WAEs to incorporate domain-specific constraints into the learning process. For example, if the data is known to have a certain structure or topology, this information can be used to guide the learning process and improve the quality of generated samples. This results in a more robust model that can handle a wider range of distributions, including multimodal and heavy-tailed distributions.\nWhile VAE and its variants have demonstrated empirical success, little attention has been given to analyzing their statistical properties. Recent developments from an optimization viewpoint include Rolinek et al. (2019), who showed VAEs pursue Principal Component Analysis (PCA) embedding under certain situations, and Koehler et al. (2022), who analyzed the implicit bias of VAEs under linear activation with two layers. For explaining generalization, Tang & Yang (2021) proposed a framework for analyzing excess risk for vanilla VAEs through M-estimation. When having access to n i.i.d. samples from the target distribution, Chakrabarty & Das (2021) derived a bound based on the Vapnik-Chervonenkis (VC) dimension, providing a guarantee of O(n\u22121/2)-convergence with a non-zero margin of error, even under model specification. However, their analysis is limited to a parametric regime under restricted assumptions and only considers a theoretical variant of WAEs, known as f -WAEs (Husain et al., 2019), which is typically not implemented in practice.\nDespite recent advancements in the understanding of VAEs and their variants, existing analyses fail to account for the fundamental goal of these models, i.e. to understand the data generation mechanism where one can expect the data to have an intrinsically low-dimensional structure. For instance, a key application of WAEs is to understand natural image generation mechanisms and it is believed that natural images have a low-dimensional structure, despite their high-dimensional pixel-wise representation (Pope et al., 2020). Furthermore, the current state-of-the-art views the problem only through a classical learning theory approach to derive O(n\u22121/2) or faster rates (under additional assumptions) ignoring the model misspecification error. Thus, such rates do not align with the well-known rates for classical non-parametric density estimation approaches (Kim et al., 2019). Additionally, these approaches only consider the scenario where the network architecture is fixed, but in practice, larger models are often employed for big datasets.\nIn this paper, we aim to address the aforementioned shortcomings in the current literature and bridge the gap between the theory and practice of WAEs. Our contributions include:\n\u2022 We propose a framework to provide an error analysis of Wasserstein Autoencoders (WAEs) when the data lies in a low-dimensional structure in the high-dimensional representative feature space.\n\u2022 Informally, our results indicate that if one has n independent and identically distributed (i.i.d.) samples from the target distribution, then under the assumption of Lipschitz-smoothness of the true model, if the corresponding networks are properly chosen, the error rate for the problem scales as O\u0303 ( n \u2212 12+d\u00b5 ) , where, d\u00b5 is the upper Minkowski dimension of the support of the target distribution.\n\u2022 The networks can be chosen as having O(n\u03b3e) many weights for the encoder and O(n\u03b3g ) for the generator, where, \u03b3e, \u03b3g \u2264 1 and only depend on d\u00b5 and \u2113 (dimension of the latent space), respectively. Furthermore, the values of \u03b3e and \u03b3g decrease as the true model becomes smoother.\n\u2022 We show that one can ensure encoding and decoding guarantees, i.e. the encoded distribution is close enough to the target latent distribution, and the generator maps back the encoded points close to the original points. Under additional regularity assumptions, we show that the approximating push-forward measure, induced by the generator, is close to the target distribution, in the Wasserstein sense, almost surely."
        },
        {
            "heading": "2 A PROOF OF CONCEPT",
            "text": "Before we theoretically explore the problem, we discuss an experiment to demonstrate that the error rates for WAEs depend primarily only on the intrinsic dimension of the data. Since it is difficult to assess the intrinsic dimensionality of natural images, we follow the prescription of Pope et al. (2020) to generate low-dimensional synthetic images. We use a pre-trained Bi-directional GAN (Donahue et al., 2017) with 128 latent entries and outputs of size 128\u00d7128\u00d73, trained on the ImageNet dataset (Deng et al., 2009). Using the decoder of this pre-trained BiGAN, we generate 11, 000 images, from the class, soap-bubblewhere we fix most entries of the latent vectors to zero leaving only d int free entries. We take d int to be 2 and 16, respectively. We reduce the image sizes to 28\u00d7 28 for computational ease. We train a WAE model with the standard architecture as proposed by Tolstikhin et al. (2018) with the number of training samples varying in {2000, 4000, . . . , 10000} and keep the last 1000 images for testing. For the latent distribution, we use the standard Gaussian distribution on the latent space R8 and use the Adam optimizer (Kingma & Ba, 2015) with a learning rate of 0.0001. We also take \u03bb = 10 for the penalty on the dissimilarity in objective (4). After training for 10 epochs, we generate 1000 sample images from the distribution G\u0302\u266f\u03bd (see Section 3 for notations) and compute the Frechet Inception Distance (FID) (Heusel et al., 2017) to assess the quality of the generated samples with respect to the target distribution. We also compute the reconstruction error for these test images. We repeat the experiment 10 times and report the average. The experimental results for both variants of WAE, i.e. the GAN and MMD are shown in Fig. 1. It is clear from Fig. 1 that the error rates for d int = 2 is lower than for the case d int = 16. The codes for this experimental study can be found at https://github.com/SaptarshiC98/WAE."
        },
        {
            "heading": "3 BACKGROUND",
            "text": ""
        },
        {
            "heading": "3.1 NOTATIONS AND SOME PRELIMINARY CONCEPTS",
            "text": "This section introduces preliminary notation and concepts for theoretical analyses.\nNotation We use notations x \u2228 y := max{x, y} and x \u2227 y := min{x, y}. T\u266f\u00b5 denotes the pushforward of measure \u00b5 by the map T . For function f : S \u2192 R, and probability measure \u03b3 on S, let \u2225f\u2225Lp(\u03b3) := (\u222b S |f(x)| pd\u03b3(x) )1/p\n. Similarly, \u2225f\u2225L\u221e(A) := supx\u2208A |f(x)|. For any function class F , and distributions P and Q, \u2225P \u2212 Q\u2225F = supf\u2208F | \u222b fdP \u2212 \u222b fdQ| denotes the Integral Probability Metric (IPM) w.r.t. F . We say An \u2272 Bn (also written as An = O(Bn)) if there exists C > 0, independent of n, such that An \u2264 CBn. Similarly, we use the notation, An \u227e Bn (also written as An = O\u0303(Bn)) if An \u2264 CBn logC(en), for some C > 0. We say An \u224d Bn, if An \u2272 Bn and Bn \u2272 An. For a function f : Rd1 \u2192 Rd2 , we write, \u2225f\u2225Lip = supx \u0338=y \u2225f(x)\u2212f(y)\u22252 \u2225x\u2212y\u22252 .\nDefinition 1 (Neural networks). Let L \u2208 N and {Ni}i\u2208[L] \u2282 N. Then a L-layer neural network f : Rd \u2192 RNL is defined as,\nf(x) = AL \u25e6 \u03c3L\u22121 \u25e6AL\u22121 \u25e6 \u00b7 \u00b7 \u00b7 \u25e6 \u03c31 \u25e6A1(x) (1)\nHere, Ai(y) = Wiy + bi, with Wi \u2208 RNi\u00d7Ni\u22121 and bi \u2208 RNi\u22121 , with N0 = d. \u03c3j is applied component-wise. Here, {Wi}1\u2264i\u2264L are known as weights, and {bi}1\u2264i\u2264L are known as biases. {\u03c3i}1\u2264i\u2264L\u22121 are known as the activation functions. Without loss of generality, one can take \u03c3\u2113(0) =\n0, \u2200 \u2113 \u2208 [L \u2212 1]. We define the following quantities: (Depth) L(f) := L is known as the depth of the network; (Number of weights) The number of weights of the network f is denoted as W(f). NN{\u03c3i}i\u2208[L\u22121](L,W,R) = {f of the form (1) : L(f) \u2264 L, W(f) \u2264 W, sup\nx\u2208Rd \u2225f(x)\u2225\u221e \u2264 R}.\nIf \u03c3j(x) = x \u2228 0, for all j = 1, . . . , L\u2212 1, we denote NN{\u03c3i}1\u2264i\u2264L\u22121(L,W,R) as RN (L,W,R). We often omit R in cases where it is clear that R is bounded by a constant. Definition 2 (Ho\u0308lder functions). Let f : S \u2192 R be a function, where S \u2286 Rd. For a multi-index s = (s1, . . . , sd), let, \u2202sf = \u2202 |s|f\n\u2202x s1 1 ...\u2202x sd d\n, where, |s| = \u2211d\n\u2113=1 s\u2113. We say that a function f : S \u2192 R is \u03b2-Ho\u0308lder (for \u03b2 > 0) if\n\u2225f\u2225H\u03b2 := \u2211\ns:0\u2264|s|<\u230a\u03b2\u230b\n\u2225\u2202sf\u2225\u221e + \u2211\ns:|s|=\u230a\u03b2\u230b\nsup x\u0338=y \u2225\u2202sf(x)\u2212 \u2202sf(y)\u2225 \u2225x\u2212 y\u2225\u03b2\u2212\u230a\u03b2\u230b < \u221e.\nIf f : Rd1 \u2192 Rd2 , then we define \u2225f\u2225H\u03b2 = \u2211d2 j=1 \u2225fj\u2225H\u03b2 .\nFor notational simplicity, let, H\u03b2(S1,S2, C) = {f : S1 \u2192 S2 : \u2225f\u2225H\u03b2 \u2264 C}. Here, both S1 and S2 are both subsets of a real vector spaces. Definition 3 (Maximum Mean Discrepancy (MMD)). Let HK be the Reproducible Kernel Hilbert Space (RKHS) corresponding to the reproducing kernel K(\u00b7, \u00b7), defined on Rd. Let the corresponding norm in this RKHS be \u2225 \u00b7 \u2225HK . The Maximum Mean Discrepancy between two distributions P and Q is defined as: MMDK(P,Q) = supf :\u2225f\u2225HK\u22641 (\u222b fdP \u2212 \u222b fdQ ) ."
        },
        {
            "heading": "3.2 WASSERSTEIN AUTOENCODERS",
            "text": "Let \u00b5 be a distribution in the data-space X = [0, 1]d and Z = [0, 1]\u2113 be the latent space. In Wasserstein Autoencoders (Tolstikhin et al., 2018), one tries to learn a generator map, G : Z \u2192 X and an encoder map E : X \u2192 Z by minimizing the following objective,\nV (\u00b5, \u03bd,G,E) = \u222b c(x,G \u25e6 E(x))d\u00b5(x) + \u03bbdiss(E\u266f\u00b5, \u03bd). (2)\nHere, \u03bb > 0 is a hyper-parameter, often tuned based on the data. The first term in (2) aims to minimize a reconstruction error, i.e. the decoded value of the encoding should approximately result in the same value. The second term ensures that the encoded distribution is close to a known distribution \u03bd that is easy to sample from. The function c(\u00b7, \u00b7)-is a loss function on the data space. For example, Tolstikhin et al. (2018) took c(x, y) = \u2225x \u2212 y\u222522. diss(\u00b7, \u00b7) is a dissimilarity measure between probability distributions defined on the latent space. Tolstikhin et al. (2018) recommended either a GAN-based dissimilarity measure or a Maximum Mean Discrepancy (MMD)-based measure (Gretton et al., 2012). In this paper, we will consider the special cases, where this dissimilarity measure is taken to be the Wasserstein-1 metric, which is the dissimilarity measure for WGANs (Arjovsky et al., 2017; Gulrajani et al., 2017) or the squared MMD-metric.\nIn practice, however, one does not have access to \u00b5 but only a sample {Xi}i\u2208[n], assumed to be independently generated from \u00b5. Let \u00b5\u0302n be the empirical measure based on the data. One then minimizes the following empirical objective to estimate E and G.\nV (\u00b5\u0302n, \u03bd,G,E) = \u222b c(x,G \u25e6 E(x))d\u00b5\u0302n(x) + \u03bbd\u0302iss(E\u266f\u00b5\u0302n, \u03bd). (3)\nHere, d\u0302iss(\u00b7, \u00b7) is an estimate of diss(\u00b7, \u00b7), based only on the data, {Xi}i\u2208[n]. For example, if diss(\u00b7, \u00b7) is taken to be the Wasserstein-1 metric, then, d\u0302iss(E\u266f\u00b5\u0302n, \u03bd) = W1(E\u266f\u00b5\u0302n, \u03bd). On the other hand, if diss(\u00b7, \u00b7) is taken to be the MMD2K-measure, one can take,\nd\u0302iss(E\u266f\u00b5\u0302n, \u03bd) = 1 n(n\u2212 1) \u2211 i \u0338=j K(E(Xi), E(Xj))+EZ,Z\u2032\u223c\u03bdK(Z,Z \u2032)\u2212 2 n n\u2211 i=1 \u222b K(E(Xi), z)d\u03bd(z).\nOf course, in practice, one does a further estimation of the involved dissimilarity measure through taking an estimate \u03bd\u0302m, based on m i.i.d samples {Zj}j\u2208[m] from \u03bd, i.e. \u03bd\u0302m = 1m \u2211m j=1 \u03b4Zj . In this case the estimate of V in (2) is given by,\nV (\u00b5\u0302n, \u03bd\u0302m, G,E) = \u222b c(x,G \u25e6 E(x))d\u00b5\u0302n(x) + \u03bbd\u0302iss(E\u266f\u00b5\u0302n, \u03bdm). (4)\nIf diss(\u00b7, \u00b7) is taken to be the Wasserstein-1 metric, then, d\u0302iss(E\u266f\u00b5\u0302n, \u03bd\u0302m) = W1(E\u266f\u00b5\u0302n, \u03bd\u0302m). On the other hand, if diss(\u00b7, \u00b7) is taken to be the MMD2K-measure, one can take,\nd\u0302iss(E\u266f\u00b5\u0302n, \u03bd\u0302m) = 1 n(n\u2212 1) \u2211 i \u0338=j K(E(Xi), E(Xj))+ 1 m(m\u2212 1) \u2211 i \u0338=j K(Zi, Zj)\u2212 2 nm n\u2211 i=1 m\u2211 j=1 K(E(Xj), Zj).\nSuppose that \u2206opt > 0 is the optimization error. The empirical WAE estimates satisfy the following properties:\n(G\u0302n, E\u0302n) \u2208 { G \u2208 G, E \u2208 E : V (\u00b5\u0302n, \u03bd,G,E) \u2264 inf\nG\u2208G,E\u2208E V (\u00b5\u0302n, \u03bd,G,E) + \u2206opt\n} (5)\n(G\u0302n,m, E\u0302n,m) \u2208 { G \u2208 G, E \u2208 E : V (\u00b5\u0302n, \u03bd\u0302n, G,E) \u2264 inf\nG\u2208G,E\u2208E V (\u00b5\u0302n, \u03bd\u0302m, G,E) + \u2206opt\n} . (6)\nThe functions in G and E are implemented through neural networks with ReLU activation RN (Lg,Wg) and RN (Le,We), respectively."
        },
        {
            "heading": "4 INTRINSIC DIMENSION OF DATA DISTRIBUTION",
            "text": "Real data is often assumed to have a lower-dimensional structure within the high-dimensional feature space. Various approaches have been proposed to characterize this low dimensionality, with many using some form of covering number to measure the effective dimension of the underlying measure. Recall that the \u03f5-covering number of S w.r.t. the metic \u03f1 is defined as N (\u03f5;S, \u03f1) = inf{n \u2208 N : \u2203x1, . . . xn such that \u222ani=1 B\u03f1(xi, \u03f5) \u2287 S}, with B\u03f1(x, \u03f5) = {y : \u03f1(x, y) < \u03f5}. We characterize this low-dimensional nature of the data, through the (upper) Minkowski dimension of the support of \u00b5. We recall the definition of Minkowski dimensions, Definition 4 (Upper Minkowski dimension). For a bounded metric space (S, \u03f1), the upper Minkwoski dimension of S is defined as dimM (S, \u03f1) = lim sup\u03f5\u21920 logN (\u03f5;S, \u03f1) log(1/\u03f5) .\nThroughout this analysis, we will assume that \u03f1 is the \u2113\u221e-norm and simplify the notation to dimM (S). dimM (S, \u03f1) essentially measures how the covering number of S is affected by the radius of balls covering that set. As the concept of dimensionality relies solely on covering numbers and doesn\u2019t require a smooth mapping to a lower-dimensional Euclidean space, it encompasses both smooth manifolds and even highly irregular sets like fractals. In the literature, Kolmogorov & Tikhomirov (1961) provided a comprehensive study on the dependence of the covering number of different function classes on the underlying Minkowski dimension of the support. Nakada & Imaizumi (2020) showed how deep regression learners can incorporate this low-dimensionality of the data that is also reflected in their convergence rates. Recently, Huang et al. (2022) showed that WGANs can also adapt to this low-dimensionality of the data. For any measure \u00b5 on [0, 1]d, we use the notation d\u00b5 := dimM (supp(\u00b5)). When the data distribution is supported on a low-dimensional structure in the nominal high-dimensional feature space, one can expect d\u00b5 \u226a d. It can be observed that the image of a unit hypercube under a Ho\u0308lder map has a Minkowski dimension that is no more than the dimension of the pre-image divided by the exponent of the Ho\u0308lder map. Lemma 5. Let, f \u2208 H\u03b3 ( A, [0, 1]d2 , C ) , with A \u2286 [0, 1]d1 . Then, dimM (f (A)) \u2264 dimM (A)/(\u03b3 \u2227 1)."
        },
        {
            "heading": "5 THEORETICAL ANALYSES",
            "text": ""
        },
        {
            "heading": "5.1 ASSUMPTIONS AND ERROR DECOMPOSITION",
            "text": "To facilitate theoretical analysis, we assume that the data distributions are realizable, meaning that a \u201ctrue\u201d generator and a \u201ctrue\u201d encoder exist. Specifically, we make the assumption that there is a true smooth encoder that maps the \u00b5 to \u03bd, and the left inverse of this true encoder exists and is also smooth. Formally,\nA1. There exists G\u0303 \u2208 H\u03b1g ([0, 1]d, [0, 1]\u2113, C) and E\u0303 \u2208 H\u03b1e([0, 1]\u2113, [0, 1]d, C), such that, E\u0303\u266f\u00b5 = \u03bd and (G\u0303 \u25e6 E\u0303)(\u00b7) = id(\u00b7), a.e. [\u00b5].\nIt is also important to note that A1 entails that the manifold has a single chart, in a probabilistic sense, which is a strong assumption. Naturally, when it comes to GANs, one can work with a weaker assumption as the learning task becomes notably much simpler as one does not have to learn an inverse map to the latent space. A similar problem, while analyzing autoencoders, was faced by Liu et al. (2023) where they tackled the problem by considering chart-autoencoders, which have additional components in the network architecture, compared to regular autoencoders. A similar approach of employing chart-based WAEs could be proposed and subjected to rigorous analysis. This potential avenue could be an intriguing direction for future research.\nOne immediate consequence of assumption A1 ensures that the generator maps \u03bd to the target \u00b5. We can also ensure that the latent distribution remains unchanged if one passes it through the generator and maps it back through the encoder. Furthermore, the objective function (2) at this true encodergenerator pair takes the value, zero, as expected.\nProposition 6. Under assumption A1, the following holds: (a) G\u0303\u266f\u03bd = \u00b5, (b) (E\u0303 \u25e6 G\u0303)\u266f\u03bd = \u03bd, (c)V (\u00b5, \u03bd, G\u0303, E\u0303) = 0.\nFrom Lemma 5, It is clear that d\u00b5 = dimM (supp(\u00b5)) \u2264 dimM ( G\u0303 ( [0, 1]\u2113 )) \u2264\nmax {\u2113/(\u03b1g \u2227 1), d} . If \u2113 \u226a d and \u03b1g is not very small, then, d\u00b5 = (\u03b1g \u2227 1)\u22121\u2113 \u226a d. Thus, the usual conjecture of d\u00b5 \u226a d can be modeled through assumption A1 when the latent space has a much smaller dimension and the true generator is well-behaved, i.e. \u03b1g is not too small.\nA key step in the theoretical analysis is the following oracle inequality that bounds the excess risk in terms of the optimization error, misspecification error, and generalization error. Lemma 7 (Oracle Inequality). Suppose that, F = {f(x) = c(x,G \u25e6E(x)) : G \u2208 G, E \u2208 E}. Then the following hold:\nV (\u00b5, \u03bd, G\u0302n, E\u0302n) \u2264 \u2206miss +\u2206opt + 2\u2225\u00b5\u0302n \u2212 \u00b5\u2225F + 2\u03bb sup E\u2208E |d\u0302iss(E\u266f\u00b5\u0302n, \u03bd)\u2212 diss(E\u266f\u00b5, \u03bd)|. (7)\nV (\u00b5, \u03bd, G\u0302n,m, E\u0302n,m) \u2264 \u2206miss +\u2206opt + 2\u2225\u00b5\u0302n \u2212 \u00b5\u2225F + 2\u03bb sup E\u2208E |d\u0302iss(E\u266f\u00b5\u0302n, \u03bd\u0302m)\u2212 diss(E\u266f\u00b5, \u03bd)|. (8)\nHere, \u2206miss = infG\u2208G,E\u2208E V (\u00b5, \u03bd,G,E) denotes the misspecification error for the problem.\nFor our theoretical analysis, we need to ensure that the used kernel in the MMD and the loss function c(\u00b7, \u00b7) are regular enough. To impose such regularity, we assume the following: A2. We assume that, (a) for some B > 0, K(x, y) \u2264 B2, for all x, y \u2208 [0, 1]\u2113; (b) For some \u03c4k, |K(x, y)\u2212K(x\u2032, y\u2032)| \u2264 \u03c4k(\u2225x\u2212 x\u2032\u22252 + \u2225y \u2212 y\u2032\u22252). A3. The loss function c(\u00b7, \u00b7) is Lipschitz on [0, 1]d \u00d7 [0, 1]d, i.e. |c(x, y) \u2212 c(x\u2032, y\u2032)| \u2264 \u03c4c(\u2225x \u2212 x\u2032\u22252 + \u2225y \u2212 y\u2032\u22252) and c(x, y) \u2264 Bc, for all, x, y \u2208 [0, 1]d."
        },
        {
            "heading": "5.2 MAIN RESULT",
            "text": "Under assumptions A1\u20133, one can control the expected excess risk of the WAE problem for both the W1 and MMD dissimilarities. The main idea is to select appropriate sizes for the encoder and generator networks, that minimize both the misspecification errors and generalization errors to bound the expected excess risk using Lemma 7. Theorem 8 shows that one can appropriately select the network size in terms of the number of samples available, i.e n, to achieve a trade-off between the generalization and misspecification errors as selecting a larger network facilitates better approximation but makes the generalization gap wider and vice-versa. The main result of this paper is stated as follows. Theorem 8. Suppose that assumptions A1\u20133 hold and \u2206opt \u2264 \u2206 for some fixed non-negative threshold \u2206. Furthermore, suppose that s > d\u00b5. Then we can find n0 \u2208 N and \u03b2 > 0, that might depend on d, \u2113, \u03b1g, \u03b1e, G\u0303 and E\u0303, such that if n \u2265 n0, we can choose G = RN (Lg,Wg) and E = RN (Le,We), with, Le \u2264 \u03b2 log n, We \u2264 \u03b2n s 2\u03b1e+s log n, Lg \u2264 \u03b2 log n and Wg \u2264 \u03b2n \u2113 \u03b1e(\u03b1g\u22271)+\u2113 log n, then, for the estimation problem (5),\n(a) EV (\u00b5, \u03bd, G\u0302n, E\u0302n) \u2272 \u2206+ n \u2212 1 max\n{ 2+ \u2113\n\u03b1g ,2+ s \u03b1e(\u03b1g\u22271) ,\u2113 } log2 n, for diss(\u00b7, \u00b7) = W1(\u00b7, \u00b7),\n(b) EV (\u00b5, \u03bd, G\u0302n, E\u0302n) \u2272 \u2206+ n \u2212 1 2+max\n{ \u2113\n\u03b1g , s \u03b1e(\u03b1g\u22271) } log2 n, for diss(\u00b7, \u00b7) = MMD2K(\u00b7, \u00b7).\nFurthermore, for the estimation problem (6), if m \u2265 n \u2228 n ( max { 2+ \u2113\u03b1g ,2+ d\u00b5 \u03b1e(\u03b1g\u22271) ,\u2113 })\u22121 (\u2113\u22282)\n(c) EV (\u00b5, \u03bd, G\u0302n, E\u0302n) \u2272 \u2206+ n \u2212 1 max\n{ 2+ \u2113\n\u03b1g ,2+ s \u03b1e(\u03b1g\u22271) ,\u2113 } log2 n, for diss(\u00b7, \u00b7) = W1(\u00b7, \u00b7),\n(d) EV (\u00b5, \u03bd, G\u0302n,m, E\u0302n,m) \u2272 \u2206+ n \u2212 1 2+max\n{ \u2113\n\u03b1g , s \u03b1e(\u03b1g\u22271) } log2 n, for diss(\u00b7, \u00b7) = MMD2K(\u00b7, \u00b7).\nBefore we proceed, we observe some key consequences of Theorem 8.\nRemark 9 (Number of Weights). We note that Theorem 8 suggests that one can choose the networks to have number of weights to be an exponent of n, which is smaller than 1. Moreover, this exponent only depends on the dimensions of the latent space and the intrinsic dimension of the data. Furthermore, for smooth models i.e. \u03b1e and \u03b1g are large, one can choose smaller networks that require less many parameters as opposed to non-smooth models as also observed in practice since easier problems require less complicated networks.\nRemark 10 (Rates for Lipschitz models). For all practical purposes, one can assume that the dimension of the latent space is at least 2. If the true models are Lipschitz, i.e. if \u03b1e = \u03b1g = 1, then we can conclude that \u2113 = d\u00b5. Hence, for both models, we observe that the excess risk scales as O\u0303(n\u2212 1\n2+d\u00b5 ), barring the poly-log factors. This closely matches rates for the excess risks for GANs (Huang et al., 2022).\nRemark 11 (Inference for Data on Manifolds). We recall that we call a set M is d\u0303-regular w.r.t. the d\u0303-dimensional Hausdorff measure Hd\u0303 if H(B\u03f1(x, r)) \u224d rd\u0303, for all x \u2208 M (see Definition 6 of Weed and Bach (2019)). It is known (Mattila, 1999) that if M is d\u0303-regular, then the Minkowski dimension of M is d\u0303. Thus, when supp(\u00b5) is d\u0303-regular, d\u00b5 = d\u0303. Since compact d\u0303-dimensional differentiable manifolds are d\u0303-regular (Proposition 9 of Weed and Bach (2019)), this implies that for when supp(\u00b5) is a compact differentiable d\u0303-dimensional manifold, the error rates for the sample estimates scale as in Theorem 8, with d\u00b5 replaced with d\u0303. A similar result holds when supp(\u00b5) is a nonempty, compact convex set spanned by an affine space of dimension d\u0303; the relative boundary of a nonempty, compact convex set of dimension d\u0303+1; or self-similar set with similarity dimension d\u0303."
        },
        {
            "heading": "5.3 RELATED WORK ON GANS",
            "text": "To contextualize our contributions, we conduct a qualitative comparison with existing GAN literature. Notably, Chen et al. (2020) expressed the generalization rates for GAN when the data is restricted to an affine subspace or has a mixture representation with smooth push-forward measures; while Dahal et al. (2022) derived the convergence rates under the Wasserstein-1 distance in terms of the manifold dimension. Both Liang (2021) and Schreuder et al. (2021) study the expected excess risk of GANs for smooth generator and discriminator function classes. Liu et al. (2021) studied the properties of Bidirectional GANs, expressing the rates in terms of the number of data points, where the exponents depend on the full data and latent space dimensions. It is important to note that both Dahal et al. (2022) and Liang (2021) assume that the densities of the target distribution (either w.r.t Hausdorff or the Lebesgue measure) are bounded and smooth. In comparison, we do not make any assumption of the existence of density (or its smoothness) for the target distribution and consider the practical case where the generator is realized through neural networks as opposed to smooth functions as done by Liang (2021) and Schreuder et al. (2021). Diverging from the hypotheses of Chen et al. (2020), we do not presuppose that the support of the target measure forms an affine subspace. Furthermore, the analysis by Liu et al. (2021) derives rates that depend on the dimension of the entire space and not the manifold dimension of the support of the data as done in this analysis. It is important to emphasize that Huang et al. (2022) arrived at a rate comparable to ours concerning WGANs (Arjovsky et al., 2017). While both studies share a common overarching approach in addressing the problem by bounding the error using an oracle inequality and managing individual terms, our method necessitates extra assumptions to guarantee the generative capability of WAEs, which does not apply to WGANs due to their simpler structure. Interestingly, our derived rates closely resemble those found in GAN literature. This suggests limited room for substantial\nimprovement. However, demonstrating minimaxity remains a significant challenge and a promising avenue for future research."
        },
        {
            "heading": "5.4 PROOF OVERVIEW",
            "text": "From Lemma 7, it is clear that the expected excess risk can be bounded by the misspecification error \u2206miss and the generalization gap, \u2225\u00b5\u0302n \u2212 \u00b5\u2225F + \u03bb supE\u2208E |d\u0302iss(E\u266f\u00b5\u0302n, \u03bd) \u2212 diss(E\u266f\u00b5, \u03bd)|. To control \u2206miss, we first show that if the generator and encoders are chosen as G = RN (Wg, Lg) and E = RN (We, Le), with Le \u2264 \u03b10 log(1/\u03f5g), Lg \u2264 \u03b10 log(1/\u03b1g), We \u2264 \u03b10\u03f5\u2212s/\u03b1ee log(1/\u03f5e) and Wg \u2264 \u03b10\u03f5 \u2212\u2113/\u03b1g g log(1/\u03f5g) then, \u2206miss \u2272 \u03f5g+\u03f5 \u03b1g\u22271 e .\nOn the other hand, we show that the generalization error is roughly \u221a\nn\u22121WeLe logWe log n +\u221a n\u22121(We +Wg)(Le + Lg) log(We +Wg) log n, with additional terms depending on the estimator. Thus, the bounds in Lemma 7, leads to a bound roughly,\n\u2206+ \u03f5g + \u03f5 \u03b1g\u22271 e + \u221a n\u22121WeLe logWe logn+ \u221a n\u22121(We +Wg)(Le + Lg) log(We +Wg) logn. (9)\nBy the choice of the networks, we can upper bound the above as a function of \u03f5g and \u03f5e and then minimize the expression w.r.t. these two variables to arrive at the bounds of Theorem 8. Of course, the bound in (9) changes slightly based on the estimates and the dissimilarity measure. We refer the reader to the appendix, which contains the details of the proof."
        },
        {
            "heading": "5.5 IMPLICATIONS OF THE THEORETICAL RESULTS",
            "text": "Apart from finding the error rates for the excess risk for the WAE problem, in what follows, we also ensure a few desirable properties of the obtained estimates. For simplicity, we ignore the optimization error and set \u2206opt = 0.\nEncoding Guarantee Suppose we fix \u03bb > 0, then, it is clear from Theorem 8 that EW1(E\u0302\u266f\u00b5, \u03bd) \u2272 n \u2212 1 max { 2+ \u2113 \u03b1g ,2+ s \u03b1e(\u03b1g\u22271) ,\u2113 } log2 n and EMMD2K(E\u0302\u266f\u00b5, \u03bd) \u2272 n \u2212 1 2+max { \u2113 \u03b1g , s \u03b1e(\u03b1g\u22271) } log2 n. We can not only characterize the expected rate of convergence of E\u0302\u266f\u00b5 to \u03bd but also can say that E\u0302\u266f\u00b5 converges in distribution to \u03bd, almost surely. This is formally stated in the following proposition. Proposition 12. Suppose that assumptions A1\u20133 hold. Then, for both the dissimilarity measures W1(\u00b7, \u00b7) and MMD2K(\u00b7, \u00b7) and the estimates (5) and (6), E\u0302\u266f\u00b5 d\u2212\u2192 \u03bd, almost surely.\nTherefore, if the number of data points is large, i.e., n is large, then the estimated encoded distribution E\u0302\u266f\u00b5 will converge to the true target latent distribution \u03bd almost surely, indicating that the latent distribution can be approximated through encoding with a high degree of accuracy. Decoding Guarantee One can also show that E \u222b c(x, G\u0302 \u25e6 E\u0302(x))d\u00b5(x) \u2264 EV (\u00b5, \u03bd, G\u0302, E\u0302), for both the estimates in (5) and (6). For simplicity, if we let c(x, y) = \u2225x \u2212 y\u222522, then, it can easily seen that, E\u2225id(\u00b7) \u2212 G\u0302 \u25e6 E\u0302(\u00b7)\u22252L2(\u00b5) \u2192 0 as n \u2192 \u221e, where id(x) = x is the identity map from Rd \u2192 Rd. Furthermore, it can be shown that, \u2225id(\u00b7)\u2212 G\u0302 \u25e6 E\u0302(\u00b7)\u22252L2(\u00b5)\na.s.\u2212\u2212\u2192 0 as stated in Corollary 13 Proposition 13. Suppose that assumptions A1\u20133 hold. Then, for both the dissimilarity measures W1(\u00b7, \u00b7) and MMD2K(\u00b7, \u00b7) and the estimates (5) and (6), \u2225id(\u00b7)\u2212 G\u0302 \u25e6 E\u0302(\u00b7)\u22252L2(\u00b5) a.s.\u2212\u2212\u2192 0.\nProposition 13 guarantees that the generator is able to map back the encoded points to the original data if a sufficiently large amount of data is available. In other words, if one has access to a large number of samples from the data distribution, then the generator is able to learn a mapping, from the encoded points to the original data, that is accurate enough to be useful.\nData Generation Guarantees A key interest in this theoretical exploration is whether one can guarantee that one can generate samples from the unknown target distribution \u00b5, through the generator, i.e. whether G\u0302\u266f\u03bd is close enough to \u00b5 in some sense. However, one requires some additional assumptions (Chakrabarty & Das, 2021; Tang & Yang, 2021) on G\u0302 or the nature of convergence\nof E\u0302\u266f\u00b5 to \u03bd to ensure this. We present the corresponding results subsequently as follows. Before proceeding, we recall the definition of Total Variation (TV) distance between two measures \u03b31 and \u03b32, defined on \u2126, as, TV (\u03b31, \u03b32) = supB\u2208B(\u2126) |\u03b31(B) \u2212 \u03b32(B)|, where, B(\u2126) denotes the Borel \u03c3-algebra on \u2126. Theorem 14. Suppose that assumptions A1\u20133 hold and TV (E\u0302\u266f\u00b5, \u03bd) \u2192 0, almost surely. Then, G\u0302\u266f\u03bd d\u2212\u2192 \u00b5, almost surely.\nWe note that convergence in TV is a much stronger assumption than convergence in W1 or MMD in the sense that TV convergence implies weak convergence but not the other way around.\nAnother way to ensure that G\u0302\u266f\u03bd converges to \u00b5 is to put some sort of regularity on the generator estimates. Tang & Yang (2021) imposed a Lipschitz assumption to ensure this, but one can also work with something weaker, such as uniform equicontinuity of the generators. Recall that we say a family of functions, F is uniformly equicontinuous if, for anyf \u2208 F and for all \u03f5 > 0, there exists a \u03b4 > 0 such that, |f(x)\u2212 f(y)| \u2264 \u03f5, whenever, \u2225x\u2212 y\u2225 \u2264 \u03b4. Theorem 15. Suppose that assumptions A1\u20133 hold and let the family of estimated generators {G\u0302n}n\u2208N be uniformly equicontinuous, almost surely. Then, G\u0302n\u266f \u03bd d\u2212\u2192 \u00b5, almost surely.\nUniformly Lipschitz Generators Suppose that diss(\u00b7, \u00b7) = W1(\u00b7, \u00b7). If one assumes that the estimated generators are uniformly Lipschitz, then, one can say that W1(G\u0302\u266f\u03bd, \u00b5) is upper bounded by V (\u00b5, \u03bd, G\u0302, E\u0302), disregarding some constants.Thus, the same rate of convergence as in Theorem 8 holds for uniformly Lipschitz generator. We state this result formally as a corollary as follows. Corollary 16. Let diss(\u00b7, \u00b7) = W1(\u00b7, \u00b7) and suppose that the assumptions of Theorem 8 are satisfied and s > d\u00b5. Also let supn\u2208N \u2225G\u0302n\u2225Lip, supm,n\u2208N \u2225G\u0302n,m\u2225Lip \u2264 L, almost surely, for some L > 0. W1(G\u0302\u266f\u03bd, \u00b5) \u2272 V (\u00b5, \u03bd, G\u0302, E\u0302) for both estimators (5) and (6).\nIt is important to note that although assumptions A1\u20133 do not directly guarantee either of these two conditions, it is reasonable to expect the assumptions made in Theorems 14 and 15 to hold in practice. This is because regularization techniques are commonly used to ensure the learned networks E\u0302 and G\u0302 are sufficiently well-behaved. These techniques can impose various constraints, such as weight decay or dropout, that encourage the networks to have desirable properties, such as smoothness or sparsity. Therefore, while the assumptions made in the theorems cannot be directly ensured by A1\u20133, they are likely to hold in practice with appropriate regularization techniques applied to the network training. It would be a key step in furthering our understanding to develop a similar error analysis for such regularized networks and we leave this as a promising direction for future research."
        },
        {
            "heading": "6 DISCUSSIONS AND CONCLUSION",
            "text": "In this paper, we developed a framework to analyze error rates for learning unknown distributions using Wasserstein Autoencoders, especially when data points exhibit an intrinsically low-dimensional structure in the representative high-dimensional feature space. We characterized this low dimensionality with the so-called Minkowski dimension of the support of the target distribution. We developed an oracle inequality to characterize excess risk in terms of misspecification, generalization, and optimization errors for the problem. The excess risk bounds are obtained by balancing model-misspecification and stochastic errors to find proper network architectures in terms of the number of samples that achieve this tradeoff. Our framework allows us to analyze the accuracy of encoding and decoding guarantees, i.e., how well the encoded distribution approximates the target latent distribution, and how well the generator maps back the latent codes close to the original data points. Furthermore, with additional regularity assumptions, we establish that the approximating push-forward measure can effectively approximate the target distribution.\nWhile our findings provide valuable insights into the theoretical characteristics of Wasserstein Autoencoders (WAEs), it\u2019s crucial to acknowledge that achieving accurate estimates of the overall error in practical applications necessitates the consideration of an optimization error term. However, the precise estimation of this term poses a significant challenge due to the non-convex and intricate nature of the optimization process. Importantly, our error analysis remains independent of this optimization error and can seamlessly integrate with analyses involving such optimization complexities."
        },
        {
            "heading": "ACKNOWLEDGMENT",
            "text": "We gratefully acknowledge the support of the NSF and the Simons Foundation for the Collaboration on the Theoretical Foundations of Deep Learning through awards DMS-2031883 and #814639 and the NSF\u2019s support of FODSI through grant DMS-2023505."
        },
        {
            "heading": "CONTENTS",
            "text": ""
        },
        {
            "heading": "1 Introduction 1",
            "text": ""
        },
        {
            "heading": "2 A Proof of Concept 3",
            "text": ""
        },
        {
            "heading": "3 Background 3",
            "text": "3.1 Notations and some Preliminary Concepts . . . . . . . . . . . . . . . . . . . . . . 3\n3.2 Wasserstein Autoencoders . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4"
        },
        {
            "heading": "4 Intrinsic Dimension of Data Distribution 5",
            "text": ""
        },
        {
            "heading": "5 Theoretical Analyses 5",
            "text": "5.1 Assumptions and Error Decomposition . . . . . . . . . . . . . . . . . . . . . . . . 5\n5.2 Main Result . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\n5.3 Related work on GANs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n5.4 Proof Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\n5.5 Implications of the Theoretical Results . . . . . . . . . . . . . . . . . . . . . . . . 8"
        },
        {
            "heading": "6 Discussions and Conclusion 9",
            "text": ""
        },
        {
            "heading": "A Additional Notations 14",
            "text": ""
        },
        {
            "heading": "B Proof of the Main Result (Theorem 8) 14",
            "text": "B.1 Misspecification Error . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\nB.2 Bounding the Generalization Gap . . . . . . . . . . . . . . . . . . . . . . . . . . 15\nB.3 Proof of Theorem 8 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16"
        },
        {
            "heading": "C Detailed Proofs 17",
            "text": "C.1 Proofs from Section 4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\nC.2 Proofs from Section 5.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\nC.2.1 Proof of Proposition 6 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\nC.2.2 Proof of Lemma 7 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\nC.3 Proofs from Section B.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\nC.3.1 Proof of Theorem 18 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\nC.3.2 Proof of Lemma 19 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\nC.4 Proofs from Section B.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\nC.4.1 Proof of Lemma 20 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\nC.4.2 Proof of Corollary 21 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\nC.4.3 Proof of Lemma 22 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\nC.4.4 Proof of Lemma 23 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\nC.4.5 Proof of Lemma 24 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\nC.4.6 Proof of Lemma 25 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\nC.5 Proofs from Section 5.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\nC.6 Proofs from Section 5.5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\nC.6.1 Proof of Proposition 12 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\nC.6.2 Proof of Proposition 13 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\nC.6.3 Proof of Theorem 14 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\nC.6.4 Proof of Corollary 16 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30"
        },
        {
            "heading": "D Supporting Results for Approximation Guarantees 31",
            "text": ""
        },
        {
            "heading": "E Supporting Results from the Literature 32",
            "text": ""
        },
        {
            "heading": "A ADDITIONAL NOTATIONS",
            "text": "For function classes F1 and F2, F1 \u25e6 F2 = {f1 \u25e6 f2 : f1 \u2208 F1, f2 \u2208 F2}. Definition 17 (Covering and Packing Numbers). For a metric space (S, \u03f1), the \u03f5-covering number w.r.t. \u03f1 is defined as: N (\u03f5;S, \u03f1) = inf{n \u2208 N : \u2203x1, . . . xn such that \u222ani=1 B\u03f1(xi, \u03f5) \u2287 S}. A minimal \u03f5 cover of S is denoted as C(\u03f5;S, \u03f1). Similarly, the \u03f5-packing number is defined as: M(\u03f5;S, \u03f1) = sup{m \u2208 N : \u2203x1, . . . xm \u2208 S such that \u03f1(xi, xj) \u2265 \u03f5, for all i \u0338= j}."
        },
        {
            "heading": "B PROOF OF THE MAIN RESULT (THEOREM 8)",
            "text": ""
        },
        {
            "heading": "B.1 MISSPECIFICATION ERROR",
            "text": "We begin with a theoretical result to approximate any function on a low-dimensional structure using a ReLU network with sufficiently large depth and width. Let f belong to the space H\u03b2(Rd,R, C), with C > 0, and let \u03b3 be a measure on Rd. For notational simplicity, let M = supp(\u03b3). Then, for any \u03f5 > 0 and s > d\u03b3 , we prove that there exists a ReLU network, denoted by f\u0302 , with a depth of at most O(log(1/\u03f5)), and number of weights not exceeding O(\u03f5\u2212s/\u03b2 log(1/\u03f5)), and bounded weights. This network satisfies the condition \u2225f \u2212 f\u0302\u2225L\u221e(M) \u2264 \u03f5. A similar result with bounded depth but unbounded weights was derived by Nakada & Imaizumi (2020).\nTheorem 18. Let f be an element of H\u03b2(Rd,R, C), where C > 0. Then, for any s > d\u03b3 , there exists constants \u03f50 (which may depend on \u03b3) and \u03b1, (which may depend on \u03b2, d, and C), such that if \u03f5 \u2208 (0, \u03f50], a ReLU network f\u0302 can be constructed with L(f\u0302) \u2264 \u03b1 log(1/\u03f5) and W(f\u0302) \u2264 \u03b1 log(1/\u03f5)\u03f5\u2212s/\u03b2 , satisfying the condition, \u2225f \u2212 f\u0302\u2225L\u221e(M) \u2264 \u03f5.\nApplying the above theorem, one can control \u2206miss, when the network size is large enough. Under assumptions A1\u20133, we derive the following bound on the misspecification error. It is important to note that none of the network dimensions depend on the dimensionality of the entire data space, i.e. d.\nLemma 19. Suppose assumptions A1\u20133 hold and let, diss(\u00b7, \u00b7) \u2261 W1(\u00b7, \u00b7) or MMD2K(\u00b7, \u00b7). Also, let s > d\u00b5. Then, we can find positive constants \u03f50, \u03b10 and R, that might depend on d, \u2113, G\u0303 and E\u0303, such that if 0 < \u03f5g, \u03f5e \u2264 \u03f50 and G = RN (Wg, Lg, R) and E = RN (We, Le, R), with\nLe \u2264 \u03b10 log(1/\u03f5g), Lg \u2264 \u03b10 log(1/\u03b1g), We \u2264 \u03b10\u03f5\u2212s/\u03b1ee log(1/\u03f5e) and Wg \u2264 \u03b10\u03f5\u2212\u2113/\u03b1gg log(1/\u03f5g)\nthen, \u2206miss \u2272 \u03f5g + \u03f5 \u03b1g\u22271 e ."
        },
        {
            "heading": "B.2 BOUNDING THE GENERALIZATION GAP",
            "text": "Let f : Rd \u2192 Rd\u2032 and {Xi}i\u2208[n] \u2282 Rd. We define f|X1:n as [f(X1) : \u00b7 \u00b7 \u00b7 : f(Xn)] \u2208 R d\u2032\u00d7n. For a function class F , we define\nF|X1:n = {f|X1:n : f \u2208 F} \u2286 R d\u2032\u00d7n.\nThe covering number of F|X1:n with respect to the \u2113\u221e-norm is denoted by N (\u03f5;F|X1:n , \u2113\u221e). The result extends the seminal works of Bartlett et al. (2019) to determine the metric entropy of deep learners with multivariate outputs. Lemma 20. Suppose that n \u2265 6 and F are a class neural network with depth at most L and number of weights at most W . Furthermore, the activation functions are piece-wise polynomial activation with the number of pieces and degree at most k \u2208 N. Then, there is a constant \u03b8 (that might depend on d and d\u2032), such that, if n \u2265 \u03b8(W + 6d\u2032 + 2d\u2032L)(L+ 3) (log(W + 6d\u2032 + 2d\u2032L) + L+ 3),\nlogN (\u03f5;F|X1:n , \u2113\u221e) \u2272 (W + 6d \u2032 + 2d\u2032L)(L+ 3) (log(W + 6d\u2032 + 2d\u2032L) + L+ 3) log\n( nd\u2032\n\u03f5\n) ,\nwhere d\u2032 is the output dimension of the networks in F .\nWe can use the result above to provide bounds on the metric entropies of the function classes described in Lemma 7. This bound is a function of the number of samples and the size of the network classes G and E. Corollary 21. Suppose that W(E) \u2264 We, L(E) \u2264 Le, W(G) \u2264 Wg and L(G) \u2264 Lg , with Le, Lg \u2265 3, We \u2265 6\u2113 + 2\u2113Le and Wg \u2265 6d + 2dLg . Then, there is a constant \u03be1, such that if n \u2265 \u03be1(We +Wg)(Le + Lg) (log(We +Wg) + Le + Lg),\nlogN ( \u03f5;E|X1:n , \u2113\u221e ) \u2272WeLe(logWe + Le) log ( n\u2113\n\u03f5\n) ,\nlogN ( \u03f5; (G \u25e6 E)|X1:n , \u2113\u221e ) \u2272(We +Wg)(Le + Lg) (log(We +Wg) + Le + Lg) log ( nd\n\u03f5\n) .\nUsing Corollary 21, the following lemma provides a bound on the distance between the empirical and target distributions w.r.t. the IPM based on F. Lemma 22. Suppose R(G) \u2272 1 and F = {f(x) = c(x,G \u25e6 E(x)) : G \u2208 G, E \u2208 E}. Furthermore, let, L(E) \u2264 Le, W(G) \u2264 Wg and L(G) \u2264 Lg , with Le, Lg \u2265 3, We \u2265 6\u2113 + 2\u2113Le and Wg \u2265 6d + 2dLg . Then, there is a constant \u03be2, such that if n \u2265 \u03be2(We + Wg)(Le + Lg) (log(We +Wg) + Le + Lg)\nE\u2225\u00b5\u0302n \u2212 \u00b5\u2225F \u2272 n\u22121/2 ( (We +Wg)(Le + Lg) ( log(We +Wg) + Le + Lg ) log(nd) )1/2 .\nTo control the fourth terms in (7) and (8), we first consider the case when diss(\u00b7, \u00b7) is the W1distance. Lemma 23 controls this uniform concentration via the size of the networks in E and the sample size n. Lemma 23. Let \u00b5\u0302n = 1n \u2211n i=1 \u03b4Xi and E = RN (Le,We). Then,\nsup E\u2208E\n|W1(E\u266f\u00b5n, \u03bd)\u2212W1(E\u266f\u00b5, \u03bd)| \u2272 ( n\u22121/\u2113 \u2228 n\u22121/2 log n ) + \u221a WeLe(logWe + Le) log(n\u2113)\nn .\nFurthermore,\nsup E\u2208E\n|W1(E\u266f\u00b5\u0302n, \u03bd\u0302m)\u2212W1(E\u266f\u00b5, \u03bd)| \u2272 ( n\u22121/\u2113 \u2228 n\u22121/2 log n ) + ( m\u22121/\u2113 \u2228m\u22121/2 logm ) + \u221a WeLe(logWe + Le) log(n\u2113)\nn .\nBefore deriving the corresponding uniform concentration bounds for |M\u0302MD2K(E\u266f\u00b5\u0302n, \u03bd) \u2212 MMD2K(E\u266f\u00b5\u0302n, \u03bd)| or |M\u0302MD2K(E\u266f\u00b5\u0302n, \u03bd\u0302m) \u2212 MMD2K(E\u266f\u00b5\u0302n, \u03bd)|, we recall the definition of\nRademacher complexity (Bartlett & Mendelson, 2002). For any real-valued function class F and data points X1:n = {X1, . . . , Xn}, the empirical Rademacher complexity is defined as:\nR(F , X1:n) = 1\nn E\u03c3 sup\nf\u2208F n\u2211 i=1 \u03c3if(Xi),\nwhere \u03c3i\u2019s are i.i.d. Rademacher random variables taking values in {\u22121,+1}, with equal probability. In the following lemma, we derive a bound on the Rademacher complexity of the class of functions in the unit ball w.r.t. the HK-norm composed with E. This lemma plays a key role in the proof of Lemma 24. The proof crucially uses the results by Rudelson & Vershynin (2013). Lemma 24. Suppose assumption A2 holds and let, L(E) \u2264 Le and L(G) \u2264 Lg , with Le \u2265 3, We \u2265 2\u2113(3 + Le). Also suppose that, \u03a6 = {\u03d5 \u2208 HK : \u2225\u03d5\u2225HK \u2264 1}, then,\nR((\u03a6 \u25e6 E), X1:n) \u2272 \u221a WeLe(logWe + Le) log(n\u2113)\nn .\nUsing Lemma 24, we bound the fourth term in (7) and (8) for diss(\u00b7, \u00b7) = MMD2K(\u00b7, \u00b7), in Lemma 25. Lemma 25. Under assumption A2, the following holds:\n(a) E sup E\u2208E \u2223\u2223\u2223M\u0302MD2K(E\u266f\u00b5\u0302n, \u03bd)\u2212 MMD2K(E\u266f\u00b5, \u03bd)\u2223\u2223\u2223 \u2272\u221aWeLe logWe log(n\u2113)n , (b) E sup\nE\u2208E \u2223\u2223\u2223M\u0302MD2K(E\u266f\u00b5\u0302n, \u03bd\u0302m)\u2212 MMD2K(E\u266f\u00b5, \u03bd)\u2223\u2223\u2223 \u2272\u221aWeLe(logWe+Le) log(n\u2113)n + 1\u221am ."
        },
        {
            "heading": "B.3 PROOF OF THEOREM 8",
            "text": "Theorem 8. Suppose that assumptions A1\u20133 hold and \u2206opt \u2264 \u2206 for some fixed non-negative threshold \u2206. Furthermore, suppose that s > d\u00b5. Then we can find n0 \u2208 N and \u03b2 > 0, that might depend on d, \u2113, \u03b1g, \u03b1e, G\u0303 and E\u0303, such that if n \u2265 n0, we can choose G = RN (Lg,Wg) and E = RN (Le,We), with, Le \u2264 \u03b2 log n, We \u2264 \u03b2n s 2\u03b1e+s log n, Lg \u2264 \u03b2 log n and Wg \u2264 \u03b2n \u2113 \u03b1e(\u03b1g\u22271)+\u2113 log n, then, for the estimation problem (5),\n(a) EV (\u00b5, \u03bd, G\u0302n, E\u0302n) \u2272 \u2206+ n \u2212 1 max\n{ 2+ \u2113\n\u03b1g ,2+ s \u03b1e(\u03b1g\u22271) ,\u2113 } log2 n, for diss(\u00b7, \u00b7) = W1(\u00b7, \u00b7),\n(b) EV (\u00b5, \u03bd, G\u0302n, E\u0302n) \u2272 \u2206+ n \u2212 1 2+max\n{ \u2113\n\u03b1g , s \u03b1e(\u03b1g\u22271) } log2 n, for diss(\u00b7, \u00b7) = MMD2K(\u00b7, \u00b7).\nFurthermore, for the estimation problem (6), if m \u2265 n \u2228 n ( max { 2+ \u2113\u03b1g ,2+ d\u00b5 \u03b1e(\u03b1g\u22271) ,\u2113 })\u22121 (\u2113\u22282)\n(c) EV (\u00b5, \u03bd, G\u0302n, E\u0302n) \u2272 \u2206+ n \u2212 1 max\n{ 2+ \u2113\n\u03b1g ,2+ s \u03b1e(\u03b1g\u22271) ,\u2113 } log2 n, for diss(\u00b7, \u00b7) = W1(\u00b7, \u00b7),\n(d) EV (\u00b5, \u03bd, G\u0302n,m, E\u0302n,m) \u2272 \u2206+ n \u2212 1 2+max\n{ \u2113\n\u03b1g , s \u03b1e(\u03b1g\u22271) } log2 n, for diss(\u00b7, \u00b7) = MMD2K(\u00b7, \u00b7).\nProof. Proof of part (a) From Lemmas 7, 22 and 23, we get,\nEV (\u00b5, \u03bd, G\u0302n, E\u0302n)\n\u2272\u2206+ \u03f5g + \u03f5 \u03b1g\u22271 e +\n\u221a WeLe logWe log n\nn +\n\u221a (We +Wg)(Le + Lg) log(We +Wg) log n n + ( n\u22121/\u2113 \u2228 n\u22121/2 ) \u2272\u2206+ \u03f5g + \u03f5 \u03b1g\u22271 e + ( log ( 1\n\u03f5e \u2227 \u03f5g ))3/2\u221a\u03f5\u2212s/\u03b1ee log n n + \u221a (\u03f5 \u2212s/\u03b1e e + \u03f5 \u2212\u2113/\u03b1g g ) log n n + (n\u22121/\u2113 \u2228 n\u22121/2)\n\u2272\u2206+ \u03f5g + \u03f5 \u03b1g\u22271 e +\n( log ( 1\n\u03f5e \u2227 \u03f5g ))3/2\u221a\u03f5\u2212s/\u03b1ee log n n + \u221a \u03f5 \u2212\u2113/\u03b1g g log n n + (n\u22121/\u2113 \u2228 n\u22121/2)\nWe choose, \u03f5g \u224d n \u2212 1 2+ \u2113 \u03b1g and \u03f5e \u224d n \u2212 1 2(\u03b1g\u22271)+ s\u03b1e . This makes,\nEV (\u00b5, \u03bd, G\u0302n, E\u0302n) \u2272\u2206+ log2 n\u00d7 n \u2212 1 max\n{ 2+ \u2113\n\u03b1g ,2+ s \u03b1e(\u03b1g\u22271) } + n\u22121/\u2113.\nProof of part (b) From Lemmas 7, 22 and 25, we get, EV (\u00b5, \u03bd, G\u0302n, E\u0302n)\n\u2272\u2206+ \u03f5g + \u03f5 \u03b1g\u22271 e +\n\u221a WeLe logWe log n\nn +\n\u221a (We +Wg)(Le + Lg) log(We +Wg) log n\nn\n\u2272\u2206+ \u03f5g + \u03f5 \u03b1g\u22271 e +\n( log ( 1\n\u03f5e \u2227 \u03f5g ))3/2\u221a\u03f5\u2212s/\u03b1ee log n n + \u221a (\u03f5 \u2212s/\u03b1e e + \u03f5 \u2212\u2113/\u03b1g g ) log n n  \u2272\u2206+ \u03f5g + \u03f5 \u03b1g\u22271 e + ( log ( 1\n\u03f5e \u2227 \u03f5g ))3/2\u221a\u03f5\u2212s/\u03b1ee log n n + \u221a \u03f5 \u2212\u2113/\u03b1g g log n n  We choose, \u03f5g \u224d n \u2212 1 2+ \u2113 \u03b1g and \u03f5e \u224d n \u2212 1 2(\u03b1g\u22271)+ s\u03b1e . This makes,\nEV (\u00b5, \u03bd, G\u0302n, E\u0302n) \u2272\u2206+ log2 n\u00d7 n \u2212 1 max\n{ 2+ \u2113\n\u03b1g ,2+ s \u03b1e(\u03b1g\u22271) } ."
        },
        {
            "heading": "Proof of part (c)",
            "text": "Again from Lemmas 7, 22 and 23, we get, EV (\u00b5, \u03bd, G\u0302n, E\u0302n) \u2272\u2206+ \u03f5g + \u03f5\u03b1g\u22271e + ( log ( 1\n\u03f5e \u2227 \u03f5g ))3/2\u221a\u03f5\u2212s/\u03b1ee log n n + \u221a \u03f5 \u2212\u2113/\u03b1g g log n n  + ( n\u22121/\u2113 \u2228 n\u22121/2 ) + ( m\u22121/\u2113 \u2228m\u22121/2\n) Choosing \u03f5g \u224d n \u2212 1 2+ \u2113 \u03b1g , \u03f5e \u224d n \u2212 1\n2(\u03b1g\u22271)+ s\u03b1e and m as in the theorem statement gives us the desired result.\nProof of part (d) Similarly, from Lemmas 7, 22 and 25, we get,\nEV (\u00b5, \u03bd, G\u0302n, E\u0302n) \u2272\u2206+ \u03f5g + \u03f5\u03b1g\u22271e + ( log ( 1\n\u03f5e \u2227 \u03f5g ))3/2\u221a\u03f5\u2212s/\u03b1ee log n n + \u221a \u03f5 \u2212\u2113/\u03b1g g log n n + 1\u221a m\nChoosing \u03f5g \u224d n \u2212 1 2+ \u2113 \u03b1g , \u03f5e \u224d n \u2212 1 2(\u03b1g\u22271)+ s\u03b1e and m as in the theorem statement gives us the desired result."
        },
        {
            "heading": "C DETAILED PROOFS",
            "text": ""
        },
        {
            "heading": "C.1 PROOFS FROM SECTION 4",
            "text": "Lemma 5. Let, f \u2208 H\u03b3 ( A, [0, 1]d2 , C ) , with A \u2286 [0, 1]d1 . Then, dimM (f (A)) \u2264 dimM (A)/(\u03b3 \u2227 1).\nProof. Then, for any x, y \u2208 A, \u2225f(x) \u2212 f(y)\u2225\u221e \u2264 \u2225f(x) \u2212 f(y)\u22252 \u2264 L\u2225x \u2212 y\u2225\u03b3\u222712 \u2264 Ld\n(\u03b3\u22271)/2 1 \u2225x\u2212 y\u2225\u03b3\u22271\u221e . Thus, N (\u03f5; f (A) , \u2225 \u00b7 \u2225\u221e) \u2264 N ( 1\u221a d1 (\u03f5/L)(\u03b3\u22271) \u22121 ;A, \u2225 \u00b7 \u2225\u221e ) .\ndimM (f (A)) = lim \u03f5\u21920 logN (\u03f5; f (A) , \u2225 \u00b7 \u2225\u221e) log(1/\u03f5) \u2264 lim \u03f5\u21920\nlogN (\n1\u221a d1 (\u03f5/L)(\u03b3\u22271) \u22121 ;A, \u2225 \u00b7 \u2225\u221e ) log(1/\u03f5) \u2264 dimM (A) \u03b3 \u2227 1 ."
        },
        {
            "heading": "C.2 PROOFS FROM SECTION 5.1",
            "text": ""
        },
        {
            "heading": "C.2.1 PROOF OF PROPOSITION 6",
            "text": "Proposition 6. Under assumption A1, the following holds: (a) G\u0303\u266f\u03bd = \u00b5, (b) (E\u0303 \u25e6 G\u0303)\u266f\u03bd = \u03bd, (c)V (\u00b5, \u03bd, G\u0303, E\u0303) = 0.\nProof. (a) Let f : Z \u2192 R be any bounded continuous function. Then,\u222b f(x)d(G\u0303\u266f\u03bd)(x) = \u222b f(G\u0303(z))d\u03bd(z)\n= \u222b f(G\u0303(E\u0303(x)))d\u00b5(x) (10)\n= \u222b f(x)d\u00b5(x) (11)\nHence, G\u0303\u266f\u03bd = \u00b5. Here both (10) and (11) follows from A1.\n(b) Let f : X \u2192 R be any bounded continuous function. Then,\u222b f(x)d ( (E\u0303 \u25e6 G\u0303)\u266f\u03bd ) = \u222b f(E(G(z)))d\u03bd(z)\n= \u222b f(E(x))d\u00b5(x) (12)\n= \u222b f(z)d\u03bd(z). (13)\nHere, (12) follows from part (a) and (13) follows from A1.\n(c) To prove part (c), We note that, W1(E\u266f\u00b5, \u03bd),MMD2K(E\u266f\u00b5, \u03bd) = 0 and (G\u0303 \u25e6 E\u0303)(\u00b7) = id(\u00b7), a.e. [\u00b5]."
        },
        {
            "heading": "C.2.2 PROOF OF LEMMA 7",
            "text": "Lemma 7 (Oracle Inequality). Suppose that, F = {f(x) = c(x,G \u25e6E(x)) : G \u2208 G, E \u2208 E}. Then the following hold:\nV (\u00b5, \u03bd, G\u0302n, E\u0302n) \u2264 \u2206miss +\u2206opt + 2\u2225\u00b5\u0302n \u2212 \u00b5\u2225F + 2\u03bb sup E\u2208E |d\u0302iss(E\u266f\u00b5\u0302n, \u03bd)\u2212 diss(E\u266f\u00b5, \u03bd)|. (7)\nV (\u00b5, \u03bd, G\u0302n,m, E\u0302n,m) \u2264 \u2206miss +\u2206opt + 2\u2225\u00b5\u0302n \u2212 \u00b5\u2225F + 2\u03bb sup E\u2208E |d\u0302iss(E\u266f\u00b5\u0302n, \u03bd\u0302m)\u2212 diss(E\u266f\u00b5, \u03bd)|. (8)\nHere, \u2206miss = infG\u2208G,E\u2208E V (\u00b5, \u03bd,G,E) denotes the misspecification error for the problem.\nProof. To prove the first inequality, we observe that, V (\u00b5\u0302n, \u03bd, G\u0302n, E\u0302n) \u2264 V (\u00b5\u0302n, \u03bd,G,E) + \u2206opt, for any G \u2208 G and E \u2208 E. Thus,\nV (\u00b5, \u03bd, G\u0302n, E\u0302n) =V (\u00b5, \u03bd,G,E) + ( V (\u00b5, \u03bd, G\u0302n, E\u0302n)\u2212 V (\u00b5\u0302n, \u03bd, G\u0302n, E\u0302n) ) + ( V (\u00b5\u0302n, \u03bd, G\u0302 n, E\u0302n)\u2212 V (\u00b5, \u03bdG,E) )\n\u2264V (\u00b5, \u03bd,G,E) + ( V (\u00b5, \u03bd, G\u0302n, E\u0302n)\u2212 V (\u00b5\u0302n, \u03bd, G\u0302n, E\u0302n) ) + (V (\u00b5\u0302n, \u03bd,G,E)\u2212 V (\u00b5, \u03bd,G,E)) + \u2206opt\n\u2264\u2206opt + V (\u00b5, \u03bd,G,E) + 2 sup G\u2208G, E\u2208E |V (\u00b5\u0302n, \u03bd,G,E)\u2212 V (\u00b5, \u03bd,G,E)|\n=\u2206opt + V (\u00b5, \u03bd,G,E)\n+ 2 sup G\u2208G, E\u2208E\n\u2223\u2223\u2223\u2223\u222b c(x,G \u25e6 E(x))d\u00b5\u0302n(x) + \u03bbd\u0302iss(E\u266f\u00b5\u0302n, \u03bd)\u2212 \u222b c(x,G \u25e6 E(x))d\u00b5(x)\u2212 \u03bbdiss(E\u266f\u00b5, \u03bd)\u2223\u2223\u2223\u2223\n\u2264\u2206opt + V (\u00b5, \u03bd,G,E) + 2\u2225\u00b5\u0302n \u2212 \u00b5\u2225F + 2\u03bb sup E\u2208E |d\u0302iss(E\u266f\u00b5\u0302n, \u03bd)\u2212 diss(E\u266f\u00b5, \u03bd)|.\nTaking infimum on G and E, we get inequality (7). Inequality (8) follows from a similar derivation."
        },
        {
            "heading": "C.3 PROOFS FROM SECTION B.1",
            "text": ""
        },
        {
            "heading": "C.3.1 PROOF OF THEOREM 18",
            "text": "Fix \u03f5 > 0. For s > d\u00b5, we can say that N (\u03f5;M, \u2113\u221e) \u2264 C\u03f5\u2212s, for all \u03f5 > 0. Let K = \u2308 12\u03f5\u2309. For any i \u2208 [K]d, let \u03b8i = (\u03f5+2(i1 \u2212 1)\u03f5, . . . , \u03f5+2(id \u2212 1)\u03f5). We also let, P\u03f5 = {B\u2113\u221e(\u03b8\ni, \u03f5) : i \u2208 [K]d}. By construction, the sets in P\u03f5 are disjoint. We first claim the following: Lemma 26. |{A \u2208 P\u03f5 : A \u2229M \u0338= \u2205}| \u2264 C2d\u03f5\u2212s.\nProof. Let, r = N (\u03f5;M, \u2113\u221e) and suppose that {a1, . . . ,ar} be an \u03f5-net of M and P\u2217\u03f5 = {B\u2113\u221e(ai, \u03f5) : i \u2208 [r]} be an optimal \u03f5-cover of M. Note that each box in P\u2217\u03f5 can intersect at most 2d boxes in P\u03f5. This implies that,\n|P\u03f5 \u2229M| \u2264 |P\u03f5 \u2229 (\u222ari=1B\u2113\u221e(ai, \u03f5))| = |\u222ari=1 (P\u03f5 \u2229B\u2113\u221e(ai, \u03f5))| \u2264 2dr, which concludes the proof.\nWe are now ready to prove Theorem 18. For the ease of readability, we restate the theorem as follows: Theorem 18. Let f be an element of H\u03b2(Rd,R, C), where C > 0. Then, for any s > d\u03b3 , there exists constants \u03f50 (which may depend on \u03b3) and \u03b1, (which may depend on \u03b2, d, and C), such that if \u03f5 \u2208 (0, \u03f50], a ReLU network f\u0302 can be constructed with L(f\u0302) \u2264 \u03b1 log(1/\u03f5) and W(f\u0302) \u2264 \u03b1 log(1/\u03f5)\u03f5\u2212s/\u03b2 , satisfying the condition, \u2225f \u2212 f\u0302\u2225L\u221e(M) \u2264 \u03f5. Proof. We also let I = { i \u2208 [K]d : B\u2113\u221e(\u03b8 i, \u03f5) \u2229M \u0338= \u2205 }\n. We also let I\u2020 = {j \u2208 [K]d : mini\u2208I \u2225i\u2212 j\u22251 \u2264 1}. We know that |I\u2020| \u2264 3d|I| \u2264 6dN(\u03f5;M, \u2113\u221e). For 0 < b \u2264 a, let,\n\u03bea,b(x) = ReLU ( x+ a\na\u2212 b\n) \u2212 ReLU ( x+ b\na\u2212 b\n) \u2212 ReLU ( x\u2212 b a\u2212 b ) + ReLU ( x\u2212 a a\u2212 b ) .\nA pictorial view of this function is given in Fig. 2 and can be implemented by a ReLU network of depth two and width four. Thus, L(\u03bea,b) = 2 and W(\u03bea,b) = 12. Suppose that 0 < \u03b4 < \u03f5/3 and let, \u03b6(x) = \u220fd \u2113=1 \u03be\u03f5+\u03b4,\u03b4(x\u2113). It is easy to observe that {\u03b6(\u00b7 \u2212 \u03b8\ni) : i \u2208 I\u2020} forms a partition of unity on M, i.e. \u2211 i\u2208I\u2020 \u03b6(x\u2212 \u03b8\ni) = 1,\u2200x \u2208 M. We consider the Taylor approximation of f around \u03b8 as,\nP\u03b8(x) = \u2211\n|s|<\u230a\u03b2\u230b\n\u2202sf(\u03b8)\ns! (x\u2212 \u03b8)s .\nNote that for any x \u2208 [0, 1]d, f(x)\u2212 P\u03b8(x) = \u2211 s:|s|=\u230a\u03b2\u230b (x\u2212\u03b8)s s! (\u2202 sf(y)\u2212 \u2202sf(\u03b8)), for some y, which is a convex combination of x and \u03b8. Thus,\nf(x)\u2212 P\u03b8(x) = \u2211\ns:|s|=\u230a\u03b2\u230b\n(x\u2212 \u03b8)s\ns! (\u2202sf(y)\u2212 \u2202sf(\u03b8)) \u2264\u2225x\u2212 \u03b8\u2225\u230a\u03b2\u230b\u221e \u2211 s:|s|=\u230a\u03b2\u230b 1 s! |\u2202sf(y)\u2212 \u2202sf(\u03b8)|\n\u2264\u2225x\u2212 \u03b8\u2225\u230a\u03b2\u230b\u221e \u2225y \u2212 \u03b8\u2225\u03b2\u2212\u230a\u03b2\u230b\u221e \u2264\u2225x\u2212 \u03b8\u2225\u03b2\u221e. (14)\nNext we define f\u0303(x) = \u2211\ni\u2208I\u2020 \u03b6(x\u2212 \u03b8 i)P\u03b8i(x). Thus, if x \u2208 M,\n|f(x)\u2212 f\u0303(x)| = \u2223\u2223\u2223\u2223\u2223\u2223 \u2211 i\u2208I\u2020 \u03b6(x\u2212 \u03b8i)(f(x)\u2212 P\u03b8i(x)) \u2223\u2223\u2223\u2223\u2223\u2223 \u2264 \u2211\ni\u2208I\u2020:\u2225x\u2212\u03b8i\u2225\u221e\u22642\u03f5\n|f(x)\u2212 P\u03b8i(x)|\n\u22642d(2\u03f5)\u03b2\n=2d+\u03b2\u03f5\u03b2 . (15) We note that, f\u0303(x) = \u2211\ni\u2208I\u2020 \u03b6(x \u2212 \u03b8 i)P\u03b8i(x) = \u2211 i\u2208I\u2020 \u2211 |s|<\u230a\u03b2\u230b \u2202sf(\u03b8i) s! \u03b6(x \u2212 \u03b8 i) ( x\u2212 \u03b8i )s .\nLet ai,s = \u2202sf(\u03b8i)\ns! and\nf\u0302i,s(x) = prod(d+|s|)m (\u03be\u03f51,\u03b41(x1 \u2212 \u03b8i1), . . . , \u03be\u03f5d,\u03b4d(xd \u2212 \u03b8id), (x1 \u2212 \u03b8i1), . . . , (x1 \u2212 \u03b8i1)\ufe38 \ufe37\ufe37 \ufe38 s1 times ,\n. . . , (x1 \u2212 \u03b8id), . . . , (xd \u2212 \u03b8id)\ufe38 \ufe37\ufe37 \ufe38 sd times ),\nwhere, prod(\u00b7) is defined in Lemma 34. Here, prod(d+|s|)m has at most d + |s| \u2264 d + \u230a\u03b2\u230b many inputs. By Lemma 34, prod(d+|s|)m can be implemented by a ReLU network with L(prod(d+|s|)m ),W(prod (d+|s|) m ) \u2264 c3m. Thus, L(f\u0302i,s) \u2264 c3m+2 and W(f\u0302i,s) \u2264 c3m+8d+4|s| \u2264 c3m+ 8d+ 4k. With this f\u0302i,s, we observe that,\u2223\u2223\u2223f\u0302i,s(x)\u2212 \u03b6(x\u2212 \u03b8i)(x\u2212 \u03b8i)s\u2223\u2223\u2223 \u2264 (d+ \u230a\u03b2\u230b)3 22m+2 , \u2200x \u2208 M. (16)\nFinally, let, f\u0302(x) = \u2211 i\u2208I\u2020 \u2211\n|s|\u2264\u230a\u03b2\u230b ai,sf\u0302i,s(x). Clearly, L(f\u0302i,s) \u2264 c3m + 3 and W(f\u0302i,s) \u2264 kd (c3m+ 8d+ 4k). This implies that,\n|f\u0302(x)\u2212 f\u0303(x)| \u2264 \u2211\ni\u2208I\u2020:\u2225x\u2212\u03b8i\u2225\u221e\u22642\u03f5\n\u2211 |s|<\u230a\u03b2\u230b |ai,s|\u03b6(x\u2212 \u03b8i)|f\u0302is(x)\u2212 ( x\u2212 \u03b8i )s |\n\u22642d \u2211\n|s|<\u230a\u03b2\u230b\n|a\u03b8,s| \u2223\u2223\u2223f\u0302\u03b8i(x),s(x)\u2212 \u03b6\u03f5,\u03b4(x\u2212 \u03b8(i(x))(x\u2212 \u03b8i(x))s\u2223\u2223\u2223\n\u2264 (d+ \u230a\u03b2\u230b) 3C\n22m+2\u2212d .\nWe thus get that if x \u2208 M,\n|f(x)\u2212 f\u0302(x)| \u2264|f(x)\u2212 f\u0303(x)|+ |f\u0302(x)\u2212 f\u0303(x)| \u2264 2d+\u03b2\u03f5\u03b2 + (d+ \u230a\u03b2\u230b) 3C\n22m+2\u2212d . (17)\nWe choose \u03f5 = (\n\u03b7 2d+k+2\n)1/\u03b2 and m = \u2308 log2 ( (d+k)3C\n\u03b7\n)\u2309 + d\u2212 1. Then,\n\u2225f \u2212 f\u0302\u2225L\u221e(M) \u2264\u03b7.\nWe note that f\u0302 has |I\u2020| \u2264 6dN\u03f5(M) \u2272 6d\u03f5\u2212s many networks with depth c3m + 3 and number of weights \u230a\u03b2\u230bd (c3m+ 8d+ 4\u230a\u03b2\u230b). Thus, L(f\u0302) \u2264 c3m + 4 and W(f\u0302) \u2264 \u03f5\u2212s(6\u230a\u03b2\u230b)d (c3m+ 8d+ 4\u230a\u03b2\u230b). we thus get,\nL(f\u0302) \u2264 c3m+ 4 \u2264 c3 (\u2308 log2 ( (d+ \u230a\u03b2\u230b)3C\u03b4\n\u03b7\n)\u2309 + d\u2212 1 ) + 4 \u2264 c4 log ( 1\n\u03b7\n) ,\nwhere c4 is a function of \u03b4, \u230a\u03b2\u230b and d. Similarly,\nW(f\u0302) \u2264\u03f5\u2212s(6\u230a\u03b2\u230b)d (c3m+ 8d+ 4\u230a\u03b2\u230b) \u2264 ( \u03b7 2d+k+2 )\u2212s/\u03b2 (6\u230a\u03b2\u230b)d ( c3 ( log2 ( (d+ \u230a\u03b2\u230b)3C\u03b4 \u03b7 ) + d\u2212 1 ) + 8d+ 4\u230a\u03b2\u230b ) \u2264c6 log(1/\u03b7)\u03b7\u2212s/\u03b2 .\nTaking \u03b1 = c4 \u2228 c6 gives the result."
        },
        {
            "heading": "C.3.2 PROOF OF LEMMA 19",
            "text": "Lemma 19. Suppose assumptions A1\u20133 hold and let, diss(\u00b7, \u00b7) \u2261 W1(\u00b7, \u00b7) or MMD2K(\u00b7, \u00b7). Also, let s > d\u00b5. Then, we can find positive constants \u03f50, \u03b10 and R, that might depend on d, \u2113, G\u0303 and E\u0303, such that if 0 < \u03f5g, \u03f5e \u2264 \u03f50 and G = RN (Wg, Lg, R) and E = RN (We, Le, R), with\nLe \u2264 \u03b10 log(1/\u03f5g), Lg \u2264 \u03b10 log(1/\u03b1g), We \u2264 \u03b10\u03f5\u2212s/\u03b1ee log(1/\u03f5e) and Wg \u2264 \u03b10\u03f5\u2212\u2113/\u03b1gg log(1/\u03f5g)\nthen, \u2206miss \u2272 \u03f5g + \u03f5 \u03b1g\u22271 e .\nProof. We first prove the result for the Wasserstein-1 distance and then for the MMDK-metric.\nCase 1: diss(\u00b7, \u00b7) \u2261 W1(\u00b7, \u00b7) For any G \u2208 G and E \u2208 E, we observe that,\nV (\u00b5, \u03bd,G,E) \u2264V (\u00b5, \u03bd, G\u0303, E\u0303) + |V (\u00b5, \u03bd,G,E)\u2212 V (\u00b5, \u03bd, G\u0303, E\u0303)| \u2264\u2225c(\u00b7, G\u0303 \u25e6 E\u0303(\u00b7))\u2212 c(\u00b7, G \u25e6 E(\u00b7))\u2225L\u221e(M) + |W1(E\u266f\u00b5, \u03bd)\u2212W1(E\u0303\u266f\u00b5, \u03bd)| \u2272\u2225G \u25e6 E \u2212 G\u0303 \u25e6 E\u0303\u2225L\u221e(M) +W1(E\u0303\u266f\u00b5,E\u266f\u00b5) \u2272\u2225G \u25e6 E \u2212 G\u0303 \u25e6 E\u0303\u2225L\u221e(supp(\u00b5)) + \u2225E\u0303 \u2212 E\u2225L\u221e(supp(\u00b5)) \u2264\u2225G \u25e6 E \u2212 G\u0303 \u25e6 E\u2225L\u221e(supp(\u00b5)) + \u2225G\u0303 \u25e6 E \u2212 G\u0303 \u25e6 E\u0303\u2225L\u221e(supp(\u00b5)) + \u2225E\u0303 \u2212 E\u2225L\u221e(supp(\u00b5)) \u2272\u2225G\u2212 G\u0303\u2225L\u221e([0,1]\u2113) + \u2225E \u2212 E\u0303\u2225 \u03b1g\u22271 L\u221e(supp(\u00b5)) + \u2225E\u0303 \u2212 E\u2225L\u221e(supp(\u00b5))\nWe can take G = RN (log(1/\u03f5g), \u03f5 \u2212\u2113/\u03b1g g log(1/\u03f5g)) and E = RN (log(1/\u03f5e), \u03f5\u2212s/\u03b1ee log(1/\u03f5e)) by approximating in each of the individual coordinate-wise output of the vector-valued functions G\u0303 and E\u0303 and stacking them parallelly. This makes,\nV (\u00b5, \u03bd,G,E) \u2272 \u03f5g + \u03f5 \u03b1g\u22271 e + \u03f5e \u2272 \u03f5g + \u03f5 \u03b1g\u22271 e .\nCase 2: diss(\u00b7, \u00b7) \u2261 MMD2k(\u00b7, \u00b7) Before we begin, we note that,\n|MMD2K(E\u266f\u00b5, \u03bd)\u2212 MMD2K(E\u0303\u266f\u00b5, \u03bd)| \u2264|EX\u223c\u00b5,X\u2032\u223c\u00b5K(E(X), E(X \u2032))\u2212 EX\u223c\u00b5,X\u2032\u223c\u00b5K(E\u0303(X), E\u0303(X \u2032))| + 2|EX\u223c\u00b5,Z\u223c\u03bdK(E(X), Z)\u2212 EX\u223c\u00b5,Z\u223c\u03bdK(E\u0303(X), Z)|\n\u22642\u03c4k\u2225E \u2212 E\u0303\u2225L\u221e(supp(\u00b5)) + 2\u03c4k\u2225E \u2212 E\u0303\u2225L\u221e(supp(\u00b5)) =4\u03c4k\u2225E \u2212 E\u0303\u2225L\u221e(supp(\u00b5)). (18)\nFor any G \u2208 G and E \u2208 E, we observe that,\nV (\u00b5, \u03bd,G,E) =V (\u00b5, \u03bd, G\u0303, E\u0303) + |V (\u00b5, \u03bd,G,E)\u2212 V (\u00b5, \u03bd, G\u0303, E\u0303)| =\u2225c(\u00b7, G\u0303 \u25e6 E\u0303(\u00b7))\u2212 c(\u00b7, G \u25e6 E(\u00b7))\u2225L\u221e(supp(\u00b5)) + |MMD 2 K(E\u266f\u00b5, \u03bd)\u2212 MMD2K(E\u0303\u266f\u00b5, \u03bd)|\n\u2272\u2225G \u25e6 E \u2212 G\u0303 \u25e6 E\u0303\u2225L\u221e(\u00b5) + 4\u03c4k\u2225E \u2212 E\u0303\u2225L\u221e(supp(\u00b5)) (19) \u2272\u2225G \u25e6 E \u2212 G\u0303 \u25e6 E\u0303\u2225L\u221e(supp(\u00b5)) + \u2225E\u0303 \u2212 E\u2225L\u221e(supp(\u00b5)) \u2264\u2225G \u25e6 E \u2212 G\u0303 \u25e6 E\u2225L\u221e(supp(\u00b5)) + \u2225G\u0303 \u25e6 E \u2212 G\u0303 \u25e6 E\u0303\u2225L\u221e(supp(\u00b5)) + \u2225E\u0303 \u2212 E\u2225L\u221e(supp(\u00b5)) \u2272\u2225G\u2212 G\u0303\u2225L\u221e([0,1]\u2113) + \u2225E \u2212 E\u0303\u2225 \u03b1g\u22271 L\u221e(supp(\u00b5)) + \u2225E\u0303 \u2212 E\u2225L\u221e(supp(\u00b5)).\nIn the above calculations, we have used (18) to arrive at (19). As before, we take G = RN (log(1/\u03f5g), \u03f5 \u2212\u2113/\u03b1g g log(1/\u03f5g)) and E = RN (log(1/\u03f5e), \u03f5\u2212s/\u03b1ee log(1/\u03f5e)) by approximating in each of the individual coordinate-wise output of the vector-valued functions G\u0303 and E\u0303 and stacking them parallelly. This makes,\nV (\u00b5, \u03bd,G,E) \u2272 \u03f5g + \u03f5 \u03b1g\u22271 e + \u03f5e \u2272 \u03f5g + \u03f5 \u03b1g\u22271 e ."
        },
        {
            "heading": "C.4 PROOFS FROM SECTION B.2",
            "text": ""
        },
        {
            "heading": "C.4.1 PROOF OF LEMMA 20",
            "text": "Lemma 20. Suppose that n \u2265 6 and F are a class neural network with depth at most L and number of weights at most W . Furthermore, the activation functions are piece-wise polynomial activation with the number of pieces and degree at most k \u2208 N. Then, there is a constant \u03b8 (that might depend on d and d\u2032), such that, if n \u2265 \u03b8(W + 6d\u2032 + 2d\u2032L)(L+ 3) (log(W + 6d\u2032 + 2d\u2032L) + L+ 3),\nlogN (\u03f5;F|X1:n , \u2113\u221e) \u2272 (W + 6d \u2032 + 2d\u2032L)(L+ 3) (log(W + 6d\u2032 + 2d\u2032L) + L+ 3) log\n( nd\u2032\n\u03f5\n) ,\nwhere d\u2032 is the output dimension of the networks in F .\nProof. We let, h(x, y) = y\u22a4f(x) and let H = {h(x, y) = y\u22a4f(x) : f \u2208 F}. Also, let, T = {(h(Xi, e\u2113)|i\u2208[n],\u2113\u2208[d\u2032]) \u2208 Rnd \u2032 : h \u2208 H}. Here e\u2113 denotes the \u2113-th unit vector. By construction of T , it is clear that, N (\u03f5;F|X1:n , \u2113\u221e) = N (\u03f5; T , \u2113\u221e). We observe that,\nh(x, y) = 1\n4 (\u2225y + f(x)\u222522 \u2212 \u2225y \u2212 f(x)\u222522)\nClearly, h can be implemented by a network with L(h) = L(f) + 3 and W(h) = W(f) + 6d\u2032 + 2d\u2032L(f) (see Fig. 3 for such a construction). Thus, from Theorem 12.9 of Anthony & Bartlett (2009) (see Lemma 37), we note that, if n \u2265 Pdim(H),\nN (\u03f5; T , \u2113\u221e) \u2264 ( 2end\u2032\n\u03f5Pdim(H)\n)Pdim(H) ,\nwith, Pdim(H) \u2272 W(h)L(h) logW(h) +W(h)L2(h),\nfrom applying Theorem 6 of Bartlett et al. (2019) (see Lemma 38). This implies that, logN (\u03f5;H, \u2113\u221e) \u2264Pdim(H) log ( 2end\u2032\n\u03f5Pdim(H) ) \u2264Pdim(H) log ( nd\u2032\n\u03f5 ) \u2272 ( W(h)L(h) logW(h) +W(h)L2(h) ) log ( nd\u2032\n\u03f5\n) .\nPlugging in the values of W(h) and L(h) yields the result."
        },
        {
            "heading": "C.4.2 PROOF OF COROLLARY 21",
            "text": "Corollary 21. Suppose that W(E) \u2264 We, L(E) \u2264 Le, W(G) \u2264 Wg and L(G) \u2264 Lg , with Le, Lg \u2265 3, We \u2265 6\u2113 + 2\u2113Le and Wg \u2265 6d + 2dLg . Then, there is a constant \u03be1, such that if n \u2265 \u03be1(We +Wg)(Le + Lg) (log(We +Wg) + Le + Lg),\nlogN ( \u03f5;E|X1:n , \u2113\u221e ) \u2272WeLe(logWe + Le) log ( n\u2113\n\u03f5\n) ,\nlogN ( \u03f5; (G \u25e6 E)|X1:n , \u2113\u221e ) \u2272(We +Wg)(Le + Lg) (log(We +Wg) + Le + Lg) log ( nd\n\u03f5\n) .\nProof. The proof easily follows from applying Lemma 20 and noting the sizes of the networks in E and G \u25e6 E."
        },
        {
            "heading": "C.4.3 PROOF OF LEMMA 22",
            "text": "Lemma 22. Suppose R(G) \u2272 1 and F = {f(x) = c(x,G \u25e6 E(x)) : G \u2208 G, E \u2208 E}. Furthermore, let, L(E) \u2264 Le, W(G) \u2264 Wg and L(G) \u2264 Lg , with Le, Lg \u2265 3, We \u2265 6\u2113 + 2\u2113Le and Wg \u2265 6d + 2dLg . Then, there is a constant \u03be2, such that if n \u2265 \u03be2(We + Wg)(Le + Lg) (log(We +Wg) + Le + Lg)\nE\u2225\u00b5\u0302n \u2212 \u00b5\u2225F \u2272 n\u22121/2 ( (We +Wg)(Le + Lg) ( log(We +Wg) + Le + Lg ) log(nd) )1/2 .\nProof. Let R(G) \u2264 B, for some B > 0 and let Bc = sup0\u2264x\u2264B |c(x)| From Dudley\u2019s chaining (Wainwright, 2019, Theorem 5.22),\nE\u2225\u00b5\u0302n \u2212 \u00b5\u2225F \u2272EX1:n inf 0\u2264\u03b4\u2264Bc/2\n( \u03b4 +\n1\u221a n \u222b Bc/2 \u03b4 \u221a logN (\u03f5,F|X1:n , \u2113\u221e)d\u03f5 ) . (20)\nLet For any G \u2208 G and E \u2208 E, we can find v \u2208 C(\u03f5; (G \u25e6 E)|X1:n , \u2113\u221e), such that, \u2225(G \u25e6 E)|X1:n \u2212 v\u2225\u221e \u2264 \u03f5. This implies that \u2225(G \u25e6 E)(Xi) \u2212 vi\u2225\u221e \u2264 \u03f5, for all i \u2208 [n]. Let, A = {(c(X1, v1), . . . , c(Xn, vn)) : v \u2208 (G \u25e6 E)|X1:n }. Thus, For any G \u2208 G, E \u2208 E,\nmax 1\u2264i\u2264n |c(Xi, G \u25e6 E(Xi))\u2212 c(Xi, vi)| \u2264\u03c4c max 1\u2264i\u2264n \u2225(G \u25e6 E)(Xi)\u2212 vi\u2225\u221e \u2264 \u03c4c\u03f5.\nThus, A constitutes a \u03c4c\u03f5-cover of F|X1:n . Hence, N (\u03f5,F|X1:n , \u2113\u221e) \u2264 N (\u03f5/\u03c4c, (G\u25e6E)|X1:n , \u2113\u221e) \u2264 (We+Wg)(Le+Lg) (log(We +Wg) + Le + Lg) log ( \u03c4cnd\n\u03f5\n) .\nHere, the last inequality follows from Lemma 20. Plugging in the above bound in equation (20), we get,\nE\u2225\u00b5\u0302n \u2212 \u00b5\u2225F \u2272EX1:n inf 0\u2264\u03b4\u2264Bc/2\n( \u03b4 +\n1\u221a n \u222b Bc/2 \u03b4 \u221a logN (\u03f5,F|X1:n , \u2113\u221e)d\u03f5 )\n\u2264 \u221a (We +Wg)(Le + Lg) log(We +Wg)\nn\n\u222b Bc 0 \u221a log ( \u03c4cnd \u03f5 ) d\u03f5\n\u2272\n\u221a (We +Wg)(Le + Lg) (log(We +Wg) + Le + Lg) log(nd)\nn ."
        },
        {
            "heading": "C.4.4 PROOF OF LEMMA 23",
            "text": "Lemma 23. Let \u00b5\u0302n = 1n \u2211n i=1 \u03b4Xi and E = RN (Le,We). Then,\nsup E\u2208E\n|W1(E\u266f\u00b5n, \u03bd)\u2212W1(E\u266f\u00b5, \u03bd)| \u2272 ( n\u22121/\u2113 \u2228 n\u22121/2 log n ) + \u221a WeLe(logWe + Le) log(n\u2113)\nn .\nFurthermore,\nsup E\u2208E\n|W1(E\u266f\u00b5\u0302n, \u03bd\u0302m)\u2212W1(E\u266f\u00b5, \u03bd)| \u2272 ( n\u22121/\u2113 \u2228 n\u22121/2 log n ) + ( m\u22121/\u2113 \u2228m\u22121/2 logm ) + \u221a WeLe(logWe + Le) log(n\u2113)\nn .\nProof. Note that if diss(\u00b7, \u00b7) = W1(\u00b7, \u00b7), then\nsup E\u2208E |d\u0302iss(E\u266f\u00b5, \u03bd)\u2212 diss(E\u266f\u00b5, \u03bd)| = sup E\u2208E |W1(E\u266f\u00b5\u0302n, \u03bd)\u2212W1(E\u266f\u00b5, \u03bd)| \u2264 sup E\u2208E W1(E\u266f\u00b5\u0302n, E\u266f\u00b5)\nWe note that,\nsup E\u2208E W (E\u266f\u00b5\u0302, E\u266f\u00b5) = sup E\u2208E sup f :\u2225f\u2225Lip\u22641 EX\u223c\u00b5, X\u0302\u223c\u00b5\u0302f(E(X))\u2212 f(E(X\u0302))\nWe take F1 = {f : [0, 1]\u2113 \u2192 R : \u2225f\u2225Lip \u2264 1} = H1( \u221a \u2113). By the result of Kolmogorov & Tikhomirov (1961) (Lemma 36), we note that logN (\u03f5;F1, \u2113\u221e) \u2272 \u03f5\u2212\u2113. Furthermore, if we take F2 = E, we observe that, logN (\u03f5;E|X1:n , \u2113\u221e) \u2272 WeLe(logWe + Le) log ( n\u2113 \u03f5 ) from Lemma 21. From Dudley\u2019s chaining, we observe the following:\nE sup E\u2208E W (E\u266f\u00b5\u0302, E\u266f\u00b5)\n=E sup E\u2208E sup f :\u2225f\u2225Lip\u22641 EX\u223c\u00b5, X\u0302\u223c\u00b5\u0302f(E(X))\u2212 f(E(X\u0302))\n=E\u2225\u00b5\u0302\u2212 \u00b5\u2225F1\u25e6E\n\u2272E inf 0\u2264\u03b4\u2264Re\n( \u03b4 +\n1\u221a n \u222b Re \u03b4 \u221a logN ( \u03f5; (F1 \u25e6 E)|X1:n , \u2113\u221e ) d\u03f5 )\n\u2264E inf 0\u2264\u03b4\u2264Re\n( \u03b4 +\n1\u221a n \u222b Re \u03b4 \u221a logN (\u03f5/2;F1, \u2113\u221e) + logN ( \u03f5/2;E|X1:n , \u2113\u221e ) d\u03f5 )\n\u2272E inf 0\u2264\u03b4\u2264Re\n( \u03b4 +\n1\u221a n \u222b Re \u03b4 (\u221a logN (\u03f5/2;F1, \u2113\u221e) + \u221a logN ( \u03f5/2;E|X1:n , \u2113\u221e )) d\u03f5 )\n\u2272E inf 0\u2264\u03b4\u2264Re\n( \u03b4 +\n1\u221a n \u222b Re \u03b4 \u221a logN (\u03f5/2;F1, \u2113\u221e)d\u03f5+ 1\u221a n \u222b Re \u03b4 \u221a logN ( \u03f5/2;E|X1:n , \u2113\u221e ) d\u03f5 )\n\u2272 inf 0\u2264\u03b4\u2264Re\n( \u03b4 +\n1\u221a n \u222b Re \u03b4 \u03f5\u2212\u2113/2d\u03f5+ 1\u221a n \u222b 1 0 \u221a WeLe(logWe + Le) log ( 2en\u2113 \u03f5 ) d\u03f5 )\n\u2272 inf 0\u2264\u03b4\u2264Re\n( \u03b4 +\n1\u221a n \u222b Re \u03b4 \u03f5\u2212\u2113/2d\u03f5 ) + \u221a WeLe(logWe + Le) log(n\u2113) n\n\u2272 inf 0\u2264\u03b4\u2264Re\n( \u03b4 +\n1\u221a n \u222b Re \u03b4 \u03f5\u2212\u2113/2d\u03f5 ) + \u221a \u2113WeLe logWe log n n\n\u2272 ( n\u22121/\u2113 \u2228 n\u22121/2 log n ) + \u221a WeLe(logWe + Le) log(n\u2113)\nn ."
        },
        {
            "heading": "C.4.5 PROOF OF LEMMA 24",
            "text": "Lemma 24. Suppose assumption A2 holds and let, L(E) \u2264 Le and L(G) \u2264 Lg , with Le \u2265 3, We \u2265 2\u2113(3 + Le). Also suppose that, \u03a6 = {\u03d5 \u2208 HK : \u2225\u03d5\u2225HK \u2264 1}, then,\nR((\u03a6 \u25e6 E), X1:n) \u2272 \u221a WeLe(logWe + Le) log(n\u2113)\nn .\nProof.\nR((\u03a6 \u25e6 E), X1:n)\n= 1\nn E sup\n\u03d5\u2208\u03a6, f\u2208E\n\u2223\u2223\u2223\u2223\u2223 n\u2211\ni=1\n\u03c3i\u03d5(f(Xi)) \u2223\u2223\u2223\u2223\u2223 = 1\nn E sup\n\u03d5\u2208\u03a6, f\u2208E\n\u2223\u2223\u2223\u2223\u2223 n\u2211\ni=1\n\u03c3i\u27e8K(f(Xi), \u00b7), \u03d5\u27e9 \u2223\u2223\u2223\u2223\u2223 = 1\nn E sup\n\u03d5\u2208\u03a6, f\u2208E\n\u2223\u2223\u2223\u2223\u2223 \u2329 n\u2211 i=1 \u03c3iK(f(Xi), \u00b7), \u03d5 \u232a\u2223\u2223\u2223\u2223\u2223 \u2264 1 n E sup\nf\u2208E\n\u2225\u2225\u2225\u2225\u2225 n\u2211\ni=1\n\u03c3iK(f(Xi), \u00b7) \u2225\u2225\u2225\u2225\u2225 HK\n= 1\nn E sup v\u2208C ( \u03f5,E|X1:n ,\u2113\u221e ) sup f\u2208E\n\u2225\u2225\u2225\u2225\u2225 n\u2211\ni=1\n\u03c3i (K(vi, \u00b7) +K(f(Xi), \u00b7)\u2212K(vi, \u00b7)) \u2225\u2225\u2225\u2225\u2225 HK\n\u2264 1 n E sup v\u2208C ( \u03f5,E|X1:n ,\u2113\u221e ) sup f\u2208E\n\u2225\u2225\u2225\u2225\u2225 n\u2211\ni=1\n\u03c3iK(vi, \u00b7) \u2225\u2225\u2225\u2225\u2225 HK + 1 n n\u2211 i=1 \u2225K(f(Xi), \u00b7)\u2212K(vi, \u00b7)\u2225HK  \u2264 1 n E max\nv\u2208C ( \u03f5,E|X1:n ,\u2113\u221e ) \u2225\u2225\u2225\u2225\u2225 n\u2211 i=1 \u03c3iK(vi, \u00b7) \u2225\u2225\u2225\u2225\u2225 HK + \u221a 2\u03c4k\u03f5 (21)\nFor any v \u2208 C ( \u03f5,E|X1:n , \u2113\u221e ) , let, Yv = \u2225 \u2211n i=1 \u03c3iK(vi, \u00b7)\u2225HK and Kv = ((K(vi, vj)) \u2208 R n\u00d7n. It is easy to observe that, Y 2v = \u03c3 \u22a4Kv\u03c3 and Yv = \u2225K1/2v \u03c3\u2225. By Theorem 2.1 of Rudelson & Vershynin (2013), we note that,\nP (\u2223\u2223\u2223\u2225K1/2v \u03c3\u2225 \u2212 \u2225K1/2v \u2225HS\u2223\u2223\u2223 > t) \u2264 2 exp { \u2212 ct 2\n\u2225K1/2v \u22252\n} = 2 exp { \u2212 ct 2\n\u2225Kv\u2225\n} ,\nfor some universal constant c > 0. From Perron\u2013Frobenius theorem, we note that,\n\u2225Kv\u2225 \u2264 max 1\u2264i\u2264n n\u2211 j=1 K(vi, vj) \u2264 B2n.\nHence,\nP (\u2223\u2223\u2223\u2225K1/2v \u03c3\u2225 \u2212 \u2225K1/2v \u2225HS\u2223\u2223\u2223 > t) \u2264 2 exp{\u2212 ct2nB2 } .\nThis implies that,\nexp(\u03bb(\u2225K1/2v \u03c3\u2225 \u2212 \u2225K1/2v \u2225HS)) \u2264 exp { \u2212c \u2032\u03bb2\nn\n} ,\nfor some absolute constant c\u2032, by applying Proposition 2.5.2 of Vershynin (2018). From Theorem 2.5 of Boucheron et al. (2013), we observe that,\nE max v\u2208C ( \u03f5,E|X1:n ,\u2113\u221e )(\u2225K1/2v \u03c3\u2225 \u2212 \u2225K1/2v \u2225HS) \u2272\n\u221a n logN ( \u03f5,E|X1:n , \u2113\u221e ) .\nFrom equation (21), we observe that,\nR(\u03a6 \u25e6 E, X1:n) \u2264 1\nn E max v\u2208C ( \u03f5,E|X1:n ,\u2113\u221e ) \u2225\u2225\u2225\u2225\u2225 n\u2211 i=1 \u03c3iK(vi, \u00b7) \u2225\u2225\u2225\u2225\u2225 HK + \u221a 2\u03c4k\u03f5\n= 1\nn E max v\u2208C ( \u03f5,E|X1:n ,\u2113\u221e )(\u2225K1/2v \u03c3\u2225 \u2212 \u2225K1/2v \u2225HS + \u2225K1/2v \u2225HS) +\u221a2\u03c4k\u03f5\n\u2264 1 n E max v\u2208C ( \u03f5,E|X1:n ,\u2113\u221e )(\u2225K1/2v \u03c3\u2225 \u2212 \u2225K1/2v \u2225HS)\n+ 1\nn max v\u2208C ( \u03f5,E|X1:n ,\u2113\u221e ) \u2225K1/2v \u2225HS +\u221a2\u03c4k\u03f5\n\u2272 \u221a\u221a\u221a\u221a logN (\u03f5,E|X1:n , \u2113\u221e) n + B\u221a n + \u221a \u03f5\n\u2272\n\u221a WeLe logWe log ( n\u2113 \u03f5 ) n + \u221a \u03f5 (22)\nWe take \u03f5 = \u221a\nWeLe(logWe+Le) log(n\u2113) n makes R((\u03a6 \u25e6 E), X1:n) \u2272\n\u221a WeLe(logWe+Le) log(n\u2113)\nn ."
        },
        {
            "heading": "C.4.6 PROOF OF LEMMA 25",
            "text": "To prove Lemma 25, we need some supporting results, which we sequentially state and prove as follows. The first such result, i.e. Lemma 27 ensures that the kernel function is Lipschitz when it is considered as a map from a real vector space to the corresponding Hilbert space.\nLemma 27. Suppose assumption A2 holds. Then, \u2225K(x, \u00b7)\u2212K(y, \u00b7)\u22252HK \u2264 2\u03c4k\u2225x\u2212 y\u22252.\nProof. We observe the following:\n\u2225K(x, \u00b7)\u2212K(y, \u00b7)\u22252HK =K(x, x) +K(y, y)\u2212 2K(x, y) =(K(x, x)\u2212K(x, y)) + (K(y, y)\u2212K(x, y)) \u22642\u03c4k\u2225x\u2212 y\u22252.\nLemma 28 states that the difference between the estimated and actual squared MMD-dissimilarity scales as O(1/n) for estimates (5) and O(1/n+ 1/m) for estimates (6). Lemma 28. Suppose assumption A2 holds. Then, for any E \u2208 E,\n(a) \u2223\u2223\u2223M\u0302MD2K(E\u266f\u00b5\u0302n, \u03bd)\u2212 MMD2K(E\u266f\u00b5\u0302n, \u03bd)\u2223\u2223\u2223 \u2264 2B2n .\n(b) \u2223\u2223\u2223M\u0302MD2K(E\u266f\u00b5\u0302n, \u03bd\u0302m)\u2212 MMD2K(E\u266f\u00b5\u0302n, \u03bd)\u2223\u2223\u2223 \u2264 2B2 ( 1n + 1m).\nProof. We note that,\nM\u0302MD 2\nK(E\u266f\u00b5\u0302n, \u03bd)\u2212 MMD2(E\u266f\u00b5\u0302n, \u03bd)\n= 1 n(n\u2212 1) \u2211 i \u0338=j K(E(Xi), E(Xj))\u2212 1 n2 n\u2211 i,j=1 K(E(Xi), E(Xj))\n= 1 n2(n\u2212 1) \u2211 i\u0338=j K(E(Xi), E(Xj))\u2212 1 n2 n\u2211 i=1 K(E(Xi), E(Xi))\nThus,\u2223\u2223\u2223M\u0302MD2K(E\u266f\u00b5\u0302n, \u03bd)\u2212 MMD2(E\u266f\u00b5\u0302n, \u03bd)\u2223\u2223\u2223 \u2264 1n2(n\u2212 1) \u00d7 n(n\u2212 1)B2 + 1n2 \u00d7 nB2 = 2B2n . Part (b) follows similarly.\nWe also note that the MMDK-metric is bounded under A2 as seen in Lemma 29. Lemma 29. Under assumption A2, MMDK(P,Q) \u2264 2B, for any two distributions P and Q.\nProof. |f(x)| = \u27e8K(x, \u00b7), f\u27e9 \u2264 \u2225K(x, \u00b7)\u2225HK = B. This implies that MMDK(P,Q) = sup\u03d5\u2208\u03a6( \u222b \u03d5dP \u2212 \u222b \u03d5dQ) \u2264 2B\nLemma 30. Suppose assumption A2 holds. Then, (a) E supE\u2208E \u2223\u2223\u2223M\u0302MDK(E\u266f\u00b5\u0302n, \u03bd)\u2212 MMDK(E\u266f\u00b5, \u03bd)\u2223\u2223\u2223 \u2272\u221aWeLe(logWe+Le) log(n\u2113)n .\n(b) E supE\u2208E \u2223\u2223\u2223M\u0302MDK(E\u266f\u00b5\u0302n, \u03bd\u0302m)\u2212 MMDK(E\u266f\u00b5, \u03bd)\u2223\u2223\u2223 \u2272\u221aWeLe(logWe+Le) log(n\u2113)n + 1\u221am .\nProof. Proof of Part (a)\nWe begin by noting that,\nE sup E\u2208E \u2223\u2223\u2223M\u0302MDK(E\u266f\u00b5\u0302n, \u03bd)\u2212 MMDK(E\u266f\u00b5, \u03bd)\u2223\u2223\u2223 \u2264E sup\nE\u2208E |MMDK(E\u266f\u00b5\u0302n, \u03bd)\u2212 MMDK(E\u266f\u00b5, \u03bd)|+ E sup E\u2208E \u2223\u2223\u2223M\u0302MDK(E\u266f\u00b5\u0302n, \u03bd)\u2212 MMDK(E\u266f\u00b5\u0302n, \u03bd)\u2223\u2223\u2223 \u2264E sup\nE\u2208E MMDK(E\u266f\u00b5\u0302n, E\u266f\u00b5) + 2B\n\u221a 1\nn (23)\n=E sup E\u2208E sup \u03d5\u2208\u03a6\n(\u222b \u03d5(E(x))d\u00b5\u0302n(x)\u2212 \u222b \u03d5(E(x))d\u00b5(x) ) + 2B \u221a 1\nn \u22642R(\u03a6 \u25e6 E, \u00b5) + 2B \u221a 1\nn (24)\n\u2272\n\u221a WeLe(logWe + Le) log(n\u2113)\nn . (25)\nIn the above calculations, (23) follows from Lemma 28. Inequality (24) follows from symmetrization, whereas, (25) follows from Lemma 24.\nProof of Part (b) Similar to the calculations in part (a), we note the following:\nE sup E\u2208E \u2223\u2223\u2223M\u0302MDK(E\u266f\u00b5\u0302n, \u03bd\u0302m)\u2212 MMDK(E\u266f\u00b5, \u03bd)\u2223\u2223\u2223 \u2264E sup\nE\u2208E |MMDK(E\u266f\u00b5\u0302n, \u03bd\u0302m)\u2212 MMDK(E\u266f\u00b5, \u03bd)|+ E sup E\u2208E \u2223\u2223\u2223M\u0302MDK(E\u266f\u00b5\u0302n, \u03bd\u0302m)\u2212 MMDK(E\u266f\u00b5\u0302n, \u03bd)\u2223\u2223\u2223 \u2264E sup\nE\u2208E MMDK(E\u266f\u00b5\u0302n, E\u266f\u00b5) + EMMDK(\u03bd\u0302m, \u03bd) + 2B\n\u221a 1\nn +\n1\nm\n\u22642R(\u03a6 \u25e6 E, \u00b5) + R(\u03a6, \u03bd) + 2B \u221a 1\nn +\n1\nm\n\u2272\n\u221a WeLe(logWe + Le) log(n\u2113)\nn + 1\u221a m .\nWe are now ready to prove Lemma 25. For ease of readability, we restate the Lemma as follows.\nLemma 25. Under assumption A2, the following holds:\n(a) E sup E\u2208E \u2223\u2223\u2223M\u0302MD2K(E\u266f\u00b5\u0302n, \u03bd)\u2212 MMD2K(E\u266f\u00b5, \u03bd)\u2223\u2223\u2223 \u2272\u221aWeLe logWe log(n\u2113)n , (b) E sup\nE\u2208E \u2223\u2223\u2223M\u0302MD2K(E\u266f\u00b5\u0302n, \u03bd\u0302m)\u2212 MMD2K(E\u266f\u00b5, \u03bd)\u2223\u2223\u2223 \u2272\u221aWeLe(logWe+Le) log(n\u2113)n + 1\u221am . Proof. Proof of part (a) We begin by noting the following:\nE sup E\u2208E \u2223\u2223\u2223M\u0302MD2K(E\u266f\u00b5\u0302n, \u03bd)\u2212 MMD2K(E\u266f\u00b5, \u03bd)\u2223\u2223\u2223 =E sup\nE\u2208E \u2223\u2223\u2223\u22232MMDK(E\u266f\u00b5, \u03bd)(M\u0302MDK(E\u266f\u00b5\u0302n, \u03bd)\u2212 MMDK(E\u266f\u00b5, \u03bd))+ (M\u0302MDK(E\u266f\u00b5\u0302n, \u03bd)\u2212 MMDK(E\u266f\u00b5, \u03bd))2\u2223\u2223\u2223\u2223 \u22642BE sup\nE\u2208E \u2223\u2223\u2223M\u0302MDK(E\u266f\u00b5\u0302n, \u03bd)\u2212 MMDK(E\u266f\u00b5, \u03bd)\u2223\u2223\u2223+ E sup E\u2208E \u2223\u2223\u2223M\u0302MDK(E\u266f\u00b5\u0302n, \u03bd)\u2212 MMDK(E\u266f\u00b5, \u03bd)\u2223\u2223\u22232 (26)\n\u2272\n\u221a \u2113WeLe(logWe + Le) log n\nn . (27)\nInequality (26) follows from applying Lemma 29, whereas, (27) is a consequence of Lemma (30).\nProof of part (b) Similarly,\nE sup E\u2208E \u2223\u2223\u2223M\u0302MD2K(E\u266f\u00b5\u0302n, \u03bd\u0302m)\u2212 MMD2K(E\u266f\u00b5, \u03bd)\u2223\u2223\u2223 =E sup\nE\u2208E \u2223\u2223\u2223\u22232MMDK(E\u266f\u00b5, \u03bd)(M\u0302MDK(E\u266f\u00b5\u0302n, \u03bd\u0302m)\u2212 MMDK(E\u266f\u00b5, \u03bd))+ (M\u0302MDK(E\u266f\u00b5\u0302n, \u03bd\u0302m)\u2212 MMDK(E\u266f\u00b5, \u03bd))2\u2223\u2223\u2223\u2223 \u22642BE sup\nE\u2208E \u2223\u2223\u2223M\u0302MDK(E\u266f\u00b5\u0302n, \u03bd\u0302m)\u2212 MMDK(E\u266f\u00b5, \u03bd)\u2223\u2223\u2223+ E sup E\u2208E \u2223\u2223\u2223M\u0302MDK(E\u266f\u00b5\u0302n, \u03bd\u0302m)\u2212 MMDK(E\u266f\u00b5, \u03bd)\u2223\u2223\u22232 \u2272 \u221a \u2113WeLe(logWe + Le) log n\nn + 1\u221a m ."
        },
        {
            "heading": "C.5 PROOFS FROM SECTION 5.2",
            "text": "In this section, we prove the main result of this paper, i.e. Theorem 8."
        },
        {
            "heading": "C.6 PROOFS FROM SECTION 5.5",
            "text": "To begin our analysis, we first show the following:\nTheorem 31. Under assumptions, A1\u20133, V (\u00b5, \u03bd, G\u0302n, E\u0302) \u2192 0, almost surely.\nProof. For simplicity, we consider the estimator (5). A similar proof holds for estimator (6). Consider the oracle inequality (7). We only consider the case, when, diss = W1, the case when, diss = MMD2K can be proved similarly.\nWe note that F is a bounded function class, with bound Bc. Thus, a simple application of the bounded difference inequality yields that with probability at least 1\u2212 \u03b4/2,\n\u2225\u00b5\u0302n \u2212 \u00b5\u2225F \u2264 E\u2225\u00b5\u0302n \u2212 \u00b5\u2225F + \u03b81\n\u221a log(1/\u03b4)\nn ,\nfor some positive constant \u03b81. The fourth term in (5) can be written as:\nsup E\u2208E\n|W1(E\u266f\u00b5\u0302n, \u03bd)\u2212W1(E\u266f\u00b5, \u03bd)|.\nSuppose that \u00b5\u0302\u2032n denotes the empirical distribution on (X1, . . . , Xi\u22121, X \u2032 i, . . . , Xn). Then replacing \u00b5\u0302n with \u00b5\u0302\u2032n, yields an error at most,\nsup E\u2208E |W1(E\u266f\u00b5\u0302n, \u03bd)\u2212W1(E\u266f\u00b5\u0302\u2032n, \u03bd)| \u2264 sup E\u2208E W1(E\u266f\u00b5\u0302n, E\u266f\u00b5\u0302 \u2032 n) \u2264 sup E\u2208E sup\nf :\u2225f\u2225Lip\u22641\n1 n |f(E(Xi))\u2212 f(E(X \u2032i)| \u2272 1 n ,\nsince by construction, E\u2019s are chosen from bounded ReLU functions. Again by a simple application of bounded difference inequality, we get,"
        },
        {
            "heading": "2\u03bb sup",
            "text": "E\u2208E |W1(E\u266f\u00b5\u0302n, \u03bd)\u2212W1(E\u266f\u00b5, \u03bd)| \u2264 2\u03bbE sup E\u2208E |W1(E\u266f\u00b5\u0302n, \u03bd)\u2212W1(E\u266f\u00b5, \u03bd)|+ \u03b82\n\u221a log(1/\u03b4)\nn ,\nwith probability at least 1\u2212 \u03b4/2. Hence, by union bound, with probability at least 1\u2212 \u03b4 2\u2225\u00b5\u0302n \u2212 \u00b5\u2225F + 2\u03bb sup\nE\u2208E |W1(E\u266f\u00b5\u0302n, \u03bd)\u2212W1(E\u266f\u00b5, \u03bd)|\n\u22642E\u2225\u00b5\u0302n \u2212 \u00b5\u2225F + 2\u03bbE sup E\u2208E |W1(E\u266f\u00b5\u0302n, \u03bd)\u2212W1(E\u266f\u00b5, \u03bd)|+ \u03b83\n\u221a log(1/\u03b4)\nn ,\nfor some absolute constant \u03b83. Since, all other terms in (7) are bounded independent of the random sample, with probability at least 1\u2212 \u03b4,\nV (\u00b5, \u03bd, G\u0302, E\u0302) \u2264 EV (\u00b5, \u03bd, G\u0302, E\u0302) + \u03b83\n\u221a log(1/\u03b4)\nn .\nFrom the above, P(|V (\u00b5, \u03bd, G\u0302, E\u0302) \u2212 EV (\u00b5, \u03bd, G\u0302, E\u0302)| > \u03f5) \u2264 e\u2212 n\u03f52 \u03b83 . this implies that\u2211 n\u22651 P(|V (\u00b5, \u03bd, G\u0302, E\u0302) \u2212 EV (\u00b5, \u03bd, G\u0302, E\u0302)| > \u03f5) < \u221e. A simple application of the first BorelCantelli Lemma yields (see Proposition 5.7 of Karr (1993)) that this implies that |V (\u00b5, \u03bd, G\u0302, E\u0302) \u2212 EV (\u00b5, \u03bd, G\u0302, E\u0302)| \u2192 0, almost surely. Since, limn\u2192\u221e EV (\u00b5, \u03bd, G\u0302, E\u0302) = 0, the result follows."
        },
        {
            "heading": "C.6.1 PROOF OF PROPOSITION 12",
            "text": "Proposition 12. Suppose that assumptions A1\u20133 hold. Then, for both the dissimilarity measures W1(\u00b7, \u00b7) and MMD2K(\u00b7, \u00b7) and the estimates (5) and (6), E\u0302\u266f\u00b5 d\u2212\u2192 \u03bd, almost surely.\nProof. Let diss = W1. From Theorem 31, it is clear that, W1(E\u0302\u266f\u00b5, \u03bd) \u2192 0, almost surely. Since convergence in Wasserstein distance characterizes convergence in distribution, E\u0302\u266f\u00b5\nd\u2212\u2192 \u03bd, almost surely.\nWhen, diss = MMD2K, we can similarly say that MMD 2 K(E\u0302\u266f\u00b5, \u03bd) \u2192 0, almost surely. From Theorem 3.2 (b) of Schreuder et al. (2021), we conclude that E\u0302\u266f\u00b5 d\u2212\u2192 \u03bd, almost surely."
        },
        {
            "heading": "C.6.2 PROOF OF PROPOSITION 13",
            "text": "Proposition 13. Suppose that assumptions A1\u20133 hold. Then, for both the dissimilarity measures W1(\u00b7, \u00b7) and MMD2K(\u00b7, \u00b7) and the estimates (5) and (6), \u2225id(\u00b7)\u2212 G\u0302 \u25e6 E\u0302(\u00b7)\u22252L2(\u00b5) a.s.\u2212\u2212\u2192 0.\nProof. The proof follows from observing that 0 \u2264 \u2225id(\u00b7) \u2212 G\u0302 \u25e6 E\u0302(\u00b7)\u22252L2(\u00b5) \u2264 V (\u00b5, \u03bd, G\u0302, E\u0302) and applying Theorem 31."
        },
        {
            "heading": "C.6.3 PROOF OF THEOREM 14",
            "text": "Theorem 14. Suppose that assumptions A1\u20133 hold and TV (E\u0302\u266f\u00b5, \u03bd) \u2192 0, almost surely. Then, G\u0302\u266f\u03bd d\u2212\u2192 \u00b5, almost surely.\nProof. We begin by observing that,\nW1(G\u0302\u266f\u03bd, \u00b5) \u2264W1(G\u0302\u266f\u03bd, (G\u0302 \u25e6 E\u0302)\u266f\u00b5) +W1(G\u0302 \u25e6 E\u0302)\u266f\u00b5, \u00b5) (28)\nWe first note that\nTV (G\u0302\u266f\u03bd, (G\u0302 \u25e6 E\u0302)\u266f\u00b5) = sup B\u2208B(Rd) |(G\u0302\u266f\u03bd)(B)\u2212 ((G\u0302 \u25e6 E\u0302)\u266f\u00b5)(B)|\n= sup B\u2208B(Rd)\n|\u03bd(G\u0302\u22121(B))\u2212 (E\u0302\u266f\u00b5)(G\u0302\u22121(B))|\n\u2264 sup B\u2208B(R\u2113) |\u03bd(B)\u2212 (E\u0302\u266f\u00b5)(B)| (29)\n=TV (\u03bd, E\u0302\u266f\u00b5) \u2192 0, almost surely. Here (29) follows from the fact that { G\u0302\u22121(B) : B \u2208 Rd } \u2286 R\u2113, since G\u0302\u2019s are measurable. Thus,\nTV (G\u0302\u266f\u03bd, (G\u0302 \u25e6 E\u0302)\u266f\u00b5) \u2192 0, almost surely. Since convergence in TV implies convergence in distribution, this implies that W1(G\u0302\u266f\u03bd, (G\u0302 \u25e6 E\u0302)\u266f\u00b5) \u2192 0, almost surely.\nWe also note that, from Proposition 13, EX\u223c\u00b5\u2225X \u2212 G\u0302 \u25e6 E\u0302(X)\u22252 \u2192 0, almost surely. This implies that \u2225X \u2212 G\u0302 \u25e6 E\u0302(X)\u2225 P\u2212\u2192 0, almost surely, which further implies that G\u0302 \u25e6 E\u0302(X) d\u2212\u2192 X , almost surely. Hence, W1(G\u0302 \u25e6 E\u0302)\u266f\u00b5, \u00b5) \u2192 0, almost surely. Plugging these in (28) gives us the desired result.\nTheorem 15. Suppose that assumptions A1\u20133 hold and let the family of estimated generators {G\u0302n}n\u2208N be uniformly equicontinuous, almost surely. Then, G\u0302n\u266f \u03bd d\u2212\u2192 \u00b5, almost surely.\nProof. We note that from the proof of Theorem 14, equation (28) holds and W1(G\u0302n \u25e6 E\u0302n)\u266f\u00b5, \u00b5) \u2192 0, almost surely. We fix an \u03c9 in the sample space, for which, W1(G\u0302n\u03c9 \u25e6 E\u0302n\u03c9)\u266f\u00b5, \u00b5) \u2192 0 and (E\u0302n\u03c9)\u266f\u00b5\nd\u2212\u2192 \u03bd. Here we use the subscript \u03c9 to show that G\u0302n and E\u0302n might depend on \u03c9. Clearly, the set of all \u03c9\u2019s, for which this convergence holds, has probability 1.\nBy Skorohod\u2019s theorem, we note that we can find a sequence of random variables {Yn}n\u2208N and Z, such that Yn follows the distribution E\u0302n\u266f \u00b5 and Z \u223c \u03bd, such that Yn\na.s.\u2212\u2212\u2192 Z. Since {G\u0302n\u03c9}n\u2208N are uniformly equicontinuous, for any \u03f5 > 0, we can find \u03b4 > 0, such that if |yn \u2212 z| < \u03b4, |G\u0302n\u03c9(yn) \u2212 G\u0302n\u03c9(z)| < \u03f5. Thus, G\u0302n\u03c9(Yn) \u2212 G\u0302n\u03c9(Z)\na.s.\u2212\u2212\u2192 0. Since this implies that G\u0302n\u03c9(Yn) \u2212 G\u0302n\u03c9(Z)\nd\u2212\u2192 0, it is easy to see that, W1(G\u0302n\u03c9(Yn), G\u0302n\u03c9(Z)) \u2192 0. Now, since, W1(G\u0302n\u03c9(Yn), G\u0302n\u03c9(Z)) = W1((G\u0302 n \u03c9)\u266f\u03bd, (G\u0302 n \u03c9 \u25e6 E\u0302n\u03c9)\u266f\u00b5), we conclude that W1((G\u0302n\u03c9)\u266f\u03bd, (G\u0302n\u03c9 \u25e6 E\u0302n\u03c9)\u266f\u00b5) \u2192 0, as n \u2192 \u221e. Thus, with probability one, the RHS of (28) goes to 0 as n \u2192 \u221e. Hence, W1(G\u0302n\u266f \u03bd, \u00b5) \u2192 0, almost surely."
        },
        {
            "heading": "C.6.4 PROOF OF COROLLARY 16",
            "text": "Corollary 16. Let diss(\u00b7, \u00b7) = W1(\u00b7, \u00b7) and suppose that the assumptions of Theorem 8 are satisfied and s > d\u00b5. Also let supn\u2208N \u2225G\u0302n\u2225Lip, supm,n\u2208N \u2225G\u0302n,m\u2225Lip \u2264 L, almost surely, for some L > 0. W1(G\u0302\u266f\u03bd, \u00b5) \u2272 V (\u00b5, \u03bd, G\u0302, E\u0302) for both estimators (5) and (6).\nProof. Denoting G\u0302 as either of the estimators (5) and (6), it is easy to see that,\nW1(G\u0302\u266f\u03bd, \u00b5) \u2264W1(G\u0302\u266f\u03bd, (G\u0302 \u25e6 E\u0302)\u266f\u00b5) +W1(G\u0302 \u25e6 E\u0302)\u266f\u00b5, \u00b5) \u2264LW1(\u03bd, E\u0302\u266f\u00b5) +W1(G\u0302 \u25e6 E\u0302)\u266f\u00b5, \u00b5)\n\u2272W1(\u03bd, E\u0302\u266f\u00b5) + \u222b \u2225G\u0302 \u25e6 E\u0302(x)\u2212 x\u222522d\u00b5(x)"
        },
        {
            "heading": "D SUPPORTING RESULTS FOR APPROXIMATION GUARANTEES",
            "text": "Lemma 32. (Proposition 2 of Yarotsky (2017)) The function f(x) = x2 on the segment [0, 1] can be approximated with any error by a ReLU network, sqm(\u00b7), such that,\n1. L(sqm), W(sqm) \u2264 c1m.\n2. sqm ( k 2m ) = ( k 2m )2 , for all k = 0, 1, . . . , 2m.\n3. \u2225sqm \u2212 x2\u2225L\u221e([0,1]) \u2264 122m+2 . Lemma 33. Let sqm(\u00b7) be taken as in Lemma 32, then, \u2225sqm \u2212 x2\u2225H\u03b2 \u2264 12m\u22121 .\nProof. We begin by noting that, sqm(x) = ( (k+1)2 2m \u2212 k2 2m ) ( x\u2212 k2m ) + ( k 2m )2 , whenever, x \u2208[\nk 2m , k+1 2m ) . Thus, on ( k2m , k+1 2m )),\n\u2225sqm \u2212 x2\u2225H\u03b2 =\u2225sqm \u2212 x2\u2225L\u221e(( k2m , k+12m )) + \u2225\u2225\u2225\u2225 (k + 1)22m \u2212 k22m \u2212 2x \u2225\u2225\u2225\u2225 L\u221e(( k2m , k+1 2m )) = 1 22m+2 + 1 2m \u2264 1 2m\u22121 .\nThis implies that, \u2225sqm \u2212 x2\u2225H\u03b2 \u2264 12m\u22121 .\nLemma 34. Let M > 0, then we can find a ReLU network prod(2)m , such that,\n1. L(prod(2)m ),W(prod (2) m ) \u2264 c2m, for some absolute constant c2.\n2. \u2225prod(2)m \u2212 xy\u2225L\u221e([\u2212M,M ]\u00d7[\u2212M,M ]) \u2264 M 2 22m+1 .\nProof. Let prod(2)m (x, y) = M 2 ( sqm ( |x+y| 2M ) \u2212 sqm ( |x\u2212y| 2M )) . Clearly, prod(2)m (x, y) = 0, if\nxy = 0. We note that, L(prod(2)m ) \u2264 c1m + 1 \u2264 c2m and W(prod (2) m ) \u2264 2c1m + 2 \u2264 c2m, for some absolute constant c2. Clearly,\n\u2225prod(2)m \u2212 xy\u2225L\u221e([\u2212M,M ]\u00d7[\u2212M,M ]) \u2264 2M 2\u2225sq \u2212 x2\u2225L([0,1]) \u2264\nM2\n22m+1 .\nLemma 35. For any m \u2265 3, we can construct a ReLU network prod(d)m : Rd \u2192 R, such that for any x1, . . . , xd \u2208 [\u22121, 1], \u2225prod(d)m (x1, . . . , xd)\u2212 x1 . . . xd\u2225L\u221e([\u22121,1]d) \u2264 d3 22m+2 .\nProof. Let M = 1 and d \u2265 2. We define prod(k)m (x1, . . . , xk) = prod(2)m (prod (k\u22121) m (x1, . . . , xk\u22121), xd), k \u2265 3. Clearly W(prod (d) m ),L(prod (d) m ) \u2264 c3dm, for some absolute constant c3. We also note that, |prod(d)m (x1, . . . , xd)| \u2264 M 2 22m+1 + xd|prod(d\u22121)m (x1, . . . , xd\u22121)| \u2264 M 2 22m+1 + M |prod (d\u22121) m (x1, . . . , xd\u22121)| \u2264 M 2 22m+1 + M3 22m+1 + \u00b7 \u00b7 \u00b7+ M d\u22121\n22m+1 +M d \u2264 M\n2\n22m+1 + (d\u2212 2)M d = d\u2212 2 + 122m+1 \u2264 d\u2212 1. From induction, it is easy to\nsee that, prod(k)m \u2264 d\u2212 1. Taking M = d\u2212 1, we get that,\n\u2225prod(d)m (x1, . . . , xd)\u2212 x1 . . . xd\u2225L\u221e([\u22121,1]d) =\u2225prod(2)m (prod (d\u22121) m (x1, . . . , xd\u22121), xd)\u2212 x1 . . . xd\u2225L\u221e([\u22121,1]d)\n\u2264\u2225prod(d\u22121)m (x1, . . . , xd\u22121)\u2212 x1 . . . xd\u22121\u2225L\u221e([\u22121,1]d) + M2\n22m+2\n\u2264 dM 2\n22m+2\n= d3\n22m+2 ."
        },
        {
            "heading": "E SUPPORTING RESULTS FROM THE LITERATURE",
            "text": "This section lists some of the supporting results from the literature, used in the paper. Lemma 36. (Kolmogorov & Tikhomirov, 1961) The \u03f5-covering number of H\u03b2([0, 1]d,R, 1) can be bounded as,\nlogN ( (\u03f5;H\u03b2([0, 1]d), \u2225 \u00b7 \u2225\u221e ) \u2272 \u03f5\u2212d/\u03b2 .\nLemma 37. (Theorem 12.2 of Anthony & Bartlett (2009)) Assume for all f \u2208 F , \u2225f\u2225\u221e \u2264 M . Denote the pseudo-dimension of F as Pdim(F), then for n \u2265 Pdim(F), we have for any \u03f5 and any X1, . . . , Xn,\nN \u03f5,F|X1:n , \u2113\u221e) \u2264 ( 2eMn\n\u03f5Pdim(F)\n)Pdim(F) .\nLemma 38. (Theorem 6 of Bartlett et al. (2019)) Consider the function class computed by a feedforward neural network architecture with W many weight parameters and U many computation units arranged in L layers. Suppose that all non-output units have piecewise-polynomial activation functions with p+1 pieces and degrees no more than d, and the output unit has the identity function as its activation function. Then the VC-dimension and pseudo-dimension are upper-bounded as\nVCdim(F),Pdim(F) \u2264 C \u00b7 LW log(pU) + L2W log d."
        }
    ],
    "title": "CODERS FOR INTRINSICALLY LOW-DIMENSIONAL DATA",
    "year": 2024
}