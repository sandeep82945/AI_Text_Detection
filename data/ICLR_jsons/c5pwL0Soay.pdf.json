{
    "abstractText": "Unsupervised pre-training strategies have proven to be highly effective in natural language processing and computer vision. Likewise, unsupervised reinforcement learning (RL) holds the promise of discovering a variety of potentially useful behaviors that can accelerate the learning of a wide array of downstream tasks. Previous unsupervised RL approaches have mainly focused on pure exploration and mutual information skill learning. However, despite the previous attempts, making unsupervised RL truly scalable still remains a major open challenge: pure exploration approaches might struggle in complex environments with large state spaces, where covering every possible transition is infeasible, and mutual information skill learning approaches might completely fail to explore the environment due to the lack of incentives. To make unsupervised RL scalable to complex, high-dimensional environments, we propose a novel unsupervised RL objective, which we call Metric-Aware Abstraction (METRA). Our main idea is, instead of directly covering the entire state space, to only cover a compact latent space Z that is metrically connected to the state space S by temporal distances. By learning to move in every direction in the latent space, METRA obtains a tractable set of diverse behaviors that approximately cover the state space, being scalable to high-dimensional environments. Through our experiments in five locomotion and manipulation environments, we demonstrate that METRA can discover a variety of useful behaviors even in complex, pixel-based environments, being the first unsupervised RL method that discovers diverse locomotion behaviors in pixel-based Quadruped and Humanoid. Our code and videos are available at https://seohong.me/projects/metra/",
    "authors": [
        {
            "affiliations": [],
            "name": "Seohong Park"
        },
        {
            "affiliations": [],
            "name": "Oleh Rybkin"
        },
        {
            "affiliations": [],
            "name": "Sergey Levine"
        }
    ],
    "id": "SP:bc6417c43fbcb4b8eae3c98f920c44f463c26606",
    "references": [
        {
            "authors": [
                "Joshua Achiam",
                "Harrison Edwards",
                "Dario Amodei",
                "Pieter Abbeel"
            ],
            "title": "Variational option discovery",
            "venue": "algorithms. ArXiv,",
            "year": 2018
        },
        {
            "authors": [
                "Adri\u00e0 Puigdom\u00e8nech Badia",
                "Pablo Sprechmann",
                "Alex Vitvitskyi",
                "Daniel Guo",
                "Bilal Piot",
                "Steven Kapturowski",
                "Olivier Tieleman",
                "Mart\u0131\u0301n Arjovsky",
                "Alexander Pritzel",
                "Andew Bolt",
                "Charles Blundell"
            ],
            "title": "Never give up: Learning directed exploration strategies",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2020
        },
        {
            "authors": [
                "Richard F Bass"
            ],
            "title": "Real analysis for graduate students",
            "venue": "Createspace Ind Pub,",
            "year": 2013
        },
        {
            "authors": [
                "Kate Baumli",
                "David Warde-Farley",
                "Steven Stenberg Hansen",
                "Volodymyr Mnih"
            ],
            "title": "Relative variational intrinsic control",
            "venue": "In AAAI Conference on Artificial Intelligence (AAAI),",
            "year": 2021
        },
        {
            "authors": [
                "Marc G. Bellemare",
                "Sriram Srinivasan",
                "Georg Ostrovski",
                "Tom Schaul",
                "David Saxton",
                "R\u00e9mi Munos"
            ],
            "title": "Unifying count-based exploration and intrinsic motivation",
            "venue": "In Neural Information Processing Systems (NeurIPS),",
            "year": 2016
        },
        {
            "authors": [
                "Glen Berseth",
                "Daniel Geng",
                "Coline Devin",
                "Nicholas Rhinehart",
                "Chelsea Finn",
                "Dinesh Jayaraman",
                "Sergey Levine"
            ],
            "title": "Smirl: Surprise minimizing reinforcement learning in unstable environments",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2021
        },
        {
            "authors": [
                "Rajendra Bhatia"
            ],
            "title": "Matrix analysis",
            "venue": "Springer Science & Business Media,",
            "year": 2013
        },
        {
            "authors": [
                "Ilya Sutskever",
                "Dario Amodei"
            ],
            "title": "Language models are few-shot learners",
            "venue": "In Neural Information Processing Systems (NeurIPS),",
            "year": 2020
        },
        {
            "authors": [
                "Yuri Burda",
                "Harrison Edwards",
                "Amos J. Storkey",
                "Oleg Klimov"
            ],
            "title": "Exploration by random network distillation",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2019
        },
        {
            "authors": [
                "V\u0131\u0301ctor Campos Cam\u00fa\u00f1ez",
                "Alex Trott",
                "Caiming Xiong",
                "Richard Socher",
                "Xavier Gir\u00f3 Nieto",
                "Jordi Torres Vi\u00f1als"
            ],
            "title": "Explore, discover and learn: unsupervised discovery of state-covering skills",
            "venue": "In International Conference on Machine Learning (ICML),",
            "year": 2020
        },
        {
            "authors": [
                "Ting Chen",
                "Simon Kornblith",
                "Mohammad Norouzi",
                "Geoffrey E. Hinton"
            ],
            "title": "A simple framework for contrastive learning of visual representations",
            "venue": "In International Conference on Machine Learning (ICML),",
            "year": 2020
        },
        {
            "authors": [
                "Wenze Chen",
                "Shiyu Huang",
                "Yuan Chiang",
                "Tingling Chen",
                "Jun Zhu"
            ],
            "title": "Dgpo: Discovering multiple strategies with diversity-guided policy optimization",
            "venue": "In AAAI Conference on Artificial Intelligence (AAAI),",
            "year": 2024
        },
        {
            "authors": [
                "Xinyue Chen",
                "Che Wang",
                "Zijian Zhou",
                "Keith W. Ross"
            ],
            "title": "Randomized ensembled double qlearning: Learning fast without a model",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2021
        },
        {
            "authors": [
                "Jongwook Choi",
                "Archit Sharma",
                "Honglak Lee",
                "Sergey Levine",
                "Shixiang Gu"
            ],
            "title": "Variational empowerment as representation learning for goal-conditioned reinforcement learning",
            "venue": "In International Conference on Machine Learning (ICML),",
            "year": 2021
        },
        {
            "authors": [
                "John D. Co-Reyes",
                "Yuxuan Liu",
                "Abhishek Gupta",
                "Benjamin Eysenbach",
                "P. Abbeel",
                "Sergey Levine"
            ],
            "title": "Self-consistent trajectory autoencoder: Hierarchical reinforcement learning with trajectory embeddings",
            "venue": "In International Conference on Machine Learning (ICML),",
            "year": 2018
        },
        {
            "authors": [
                "Ishan Durugkar",
                "Mauricio Tec",
                "Scott Niekum",
                "Peter Stone"
            ],
            "title": "Adversarial intrinsic motivation for reinforcement learning",
            "venue": "In Neural Information Processing Systems (NeurIPS),",
            "year": 2021
        },
        {
            "authors": [
                "Benjamin Eysenbach",
                "Abhishek Gupta",
                "Julian Ibarz",
                "Sergey Levine"
            ],
            "title": "Diversity is all you need: Learning skills without a reward function",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2019
        },
        {
            "authors": [
                "Benjamin Eysenbach",
                "Ruslan Salakhutdinov",
                "Sergey Levine"
            ],
            "title": "Search on the replay buffer: Bridging planning and reinforcement learning",
            "venue": "In Neural Information Processing Systems (NeurIPS),",
            "year": 2019
        },
        {
            "authors": [
                "Carlos Florensa",
                "Yan Duan",
                "P. Abbeel"
            ],
            "title": "Stochastic neural networks for hierarchical reinforcement learning",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2017
        },
        {
            "authors": [
                "Carlos Florensa",
                "Jonas Degrave",
                "Nicolas Manfred Otto Heess",
                "Jost Tobias Springenberg",
                "Martin A. Riedmiller"
            ],
            "title": "Self-supervised learning of image embedding for continuous control",
            "year": 1901
        },
        {
            "authors": [
                "Justin Fu",
                "John D. Co-Reyes",
                "Sergey Levine"
            ],
            "title": "Ex2: Exploration with exemplar models for deep reinforcement learning",
            "venue": "In Neural Information Processing Systems (NeurIPS),",
            "year": 2017
        },
        {
            "authors": [
                "Abhishek Gupta",
                "Vikash Kumar",
                "Corey Lynch",
                "Sergey Levine",
                "Karol Hausman"
            ],
            "title": "Relay policy learning: Solving long-horizon tasks via imitation and reinforcement learning",
            "venue": "In Conference on Robot Learning (CoRL),",
            "year": 2019
        },
        {
            "authors": [
                "Michael U Gutmann",
                "Aapo Hyv\u00e4rinen"
            ],
            "title": "Noise-contrastive estimation: A new estimation principle for unnormalized statistical models",
            "venue": "In International Conference on Artificial Intelligence and Statistics (AISTATS),",
            "year": 2010
        },
        {
            "authors": [
                "Tuomas Haarnoja",
                "Aurick Zhou",
                "Pieter Abbeel",
                "Sergey Levine"
            ],
            "title": "Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor",
            "venue": "In International Conference on Machine Learning (ICML),",
            "year": 2018
        },
        {
            "authors": [
                "Tuomas Haarnoja",
                "Aurick Zhou",
                "Kristian Hartikainen",
                "G. Tucker",
                "Sehoon Ha",
                "Jie Tan",
                "Vikash Kumar",
                "Henry Zhu",
                "Abhishek Gupta",
                "Pieter Abbeel",
                "Sergey Levine"
            ],
            "title": "Soft actor-critic algorithms and applications",
            "venue": "ArXiv, abs/1812.05905,",
            "year": 2018
        },
        {
            "authors": [
                "Danijar Hafner",
                "Timothy P. Lillicrap",
                "Jimmy Ba",
                "Mohammad Norouzi"
            ],
            "title": "Dream to control: Learning behaviors by latent imagination",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2020
        },
        {
            "authors": [
                "Danijar Hafner",
                "Kuang-Huei Lee",
                "Ian S. Fischer",
                "P. Abbeel"
            ],
            "title": "Deep hierarchical planning from pixels",
            "venue": "In Neural Information Processing Systems (NeurIPS),",
            "year": 2022
        },
        {
            "authors": [
                "Nicklas Hansen",
                "Xiaolong Wang",
                "Hao Su"
            ],
            "title": "Temporal difference learning for model predictive control",
            "venue": "In International Conference on Machine Learning (ICML),",
            "year": 2022
        },
        {
            "authors": [
                "S. Hansen",
                "Will Dabney",
                "Andr\u00e9 Barreto",
                "T. Wiele",
                "David Warde-Farley",
                "V. Mnih"
            ],
            "title": "Fast task inference with variational intrinsic successor features",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2020
        },
        {
            "authors": [
                "Kristian Hartikainen",
                "Xinyang Geng",
                "Tuomas Haarnoja",
                "Sergey Levine"
            ],
            "title": "Dynamical distance learning for semi-supervised and unsupervised skill discovery",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2020
        },
        {
            "authors": [
                "Elad Hazan",
                "Sham M. Kakade",
                "Karan Singh",
                "Abby Van Soest"
            ],
            "title": "Provably efficient maximum entropy exploration",
            "venue": "In International Conference on Machine Learning (ICML),",
            "year": 2019
        },
        {
            "authors": [
                "Shuncheng He",
                "Yuhang Jiang",
                "Hongchang Zhang",
                "Jianzhun Shao",
                "Xiangyang Ji"
            ],
            "title": "Wasserstein unsupervised reinforcement learning",
            "venue": "In AAAI Conference on Artificial Intelligence (AAAI),",
            "year": 2022
        },
        {
            "authors": [
                "Takuya Hiraoka",
                "Takahisa Imagawa",
                "Taisei Hashimoto",
                "Takashi Onishi",
                "Yoshimasa Tsuruoka"
            ],
            "title": "Dropout q-functions for doubly efficient reinforcement learning",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2022
        },
        {
            "authors": [
                "Rein Houthooft",
                "Xi Chen",
                "Yan Duan",
                "John Schulman",
                "Filip De Turck",
                "P. Abbeel"
            ],
            "title": "Vime: Variational information maximizing exploration",
            "venue": "In Neural Information Processing Systems (NeurIPS),",
            "year": 2016
        },
        {
            "authors": [
                "Edward S. Hu",
                "Richard Chang",
                "Oleh Rybkin",
                "Dinesh Jayaraman"
            ],
            "title": "Planning goals for exploration",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2023
        },
        {
            "authors": [
                "Zheyuan Jiang",
                "Jingyue Gao",
                "Jianyu Chen"
            ],
            "title": "Unsupervised skill discovery via recurrent skill training",
            "venue": "In Neural Information Processing Systems (NeurIPS),",
            "year": 2022
        },
        {
            "authors": [
                "Leslie Pack Kaelbling"
            ],
            "title": "Learning to achieve goals",
            "venue": "In International Joint Conference on Artificial Intelligence (IJCAI),",
            "year": 1993
        },
        {
            "authors": [
                "Pierre-Alexandre Kamienny",
                "Jean Tarbouriech",
                "Alessandro Lazaric",
                "Ludovic Denoyer"
            ],
            "title": "Direct then diffuse: Incremental unsupervised skill discovery for state covering and goal reaching",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2022
        },
        {
            "authors": [
                "Jaekyeom Kim",
                "Seohong Park",
                "Gunhee Kim"
            ],
            "title": "Unsupervised skill discovery with bottleneck option learning",
            "venue": "In International Conference on Machine Learning (ICML),",
            "year": 2021
        },
        {
            "authors": [
                "Seongun Kim",
                "Kyowoon Lee",
                "Jaesik Choi"
            ],
            "title": "Variational curriculum reinforcement learning for unsupervised discovery of skills",
            "venue": "In International Conference on Machine Learning (ICML),",
            "year": 2023
        },
        {
            "authors": [
                "Diederik P. Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2015
        },
        {
            "authors": [
                "Martin Klissarov",
                "Marlos C. Machado"
            ],
            "title": "Deep laplacian-based options for temporally-extended exploration",
            "venue": "In International Conference on Machine Learning (ICML),",
            "year": 2023
        },
        {
            "authors": [
                "Ilya Kostrikov",
                "Denis Yarats",
                "Rob Fergus"
            ],
            "title": "Image augmentation is all you need: Regularizing deep reinforcement learning from pixels",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2021
        },
        {
            "authors": [
                "Saurabh Kumar",
                "Aviral Kumar",
                "Sergey Levine",
                "Chelsea Finn"
            ],
            "title": "One solution is not all you need: Few-shot extrapolation via structured maxent rl",
            "venue": "In Neural Information Processing Systems (NeurIPS),",
            "year": 2020
        },
        {
            "authors": [
                "Michael Laskin",
                "Denis Yarats",
                "Hao Liu",
                "Kimin Lee",
                "Albert Zhan",
                "Kevin Lu",
                "Catherine Cang",
                "Lerrel Pinto",
                "P. Abbeel"
            ],
            "title": "Urlb: Unsupervised reinforcement learning benchmark",
            "venue": "In Neural Information Processing Systems (NeurIPS) Datasets and Benchmarks Track,",
            "year": 2021
        },
        {
            "authors": [
                "Michael Laskin",
                "Hao Liu",
                "Xue Bin Peng",
                "Denis Yarats",
                "Aravind Rajeswaran",
                "P. Abbeel"
            ],
            "title": "Unsupervised reinforcement learning with contrastive intrinsic control",
            "venue": "In Neural Information Processing Systems (NeurIPS),",
            "year": 2022
        },
        {
            "authors": [
                "Yann LeCun",
                "Bernhard E. Boser",
                "John S. Denker",
                "Donnie Henderson",
                "Richard E. Howard",
                "Wayne E. Hubbard",
                "Lawrence D. Jackel"
            ],
            "title": "Backpropagation applied to handwritten zip code recognition",
            "venue": "Neural Computation,",
            "year": 1989
        },
        {
            "authors": [
                "Lisa Lee",
                "Benjamin Eysenbach",
                "Emilio Parisotto",
                "Eric P. Xing",
                "Sergey Levine",
                "Ruslan Salakhutdinov"
            ],
            "title": "Efficient exploration via state marginal matching",
            "year": 1906
        },
        {
            "authors": [
                "Mengdi Li",
                "Xufeng Zhao",
                "Jae Hee Lee",
                "Cornelius Weber",
                "Stefan Wermter"
            ],
            "title": "Internally rewarded reinforcement learning",
            "venue": "In International Conference on Machine Learning (ICML),",
            "year": 2023
        },
        {
            "authors": [
                "Hao Liu",
                "Pieter Abbeel"
            ],
            "title": "APS: Active pretraining with successor features",
            "venue": "In International Conference on Machine Learning (ICML),",
            "year": 2021
        },
        {
            "authors": [
                "Hao Liu",
                "Pieter Abbeel"
            ],
            "title": "Behavior from the void: Unsupervised active pre-training",
            "venue": "In Neural Information Processing Systems (NeurIPS),",
            "year": 2021
        },
        {
            "authors": [
                "Marlos C. Machado",
                "Marc G. Bellemare",
                "Michael Bowling"
            ],
            "title": "A laplacian framework for option discovery in reinforcement learning",
            "venue": "In International Conference on Machine Learning (ICML),",
            "year": 2017
        },
        {
            "authors": [
                "Marlos C. Machado",
                "Clemens Rosenbaum",
                "Xiaoxiao Guo",
                "Miao Liu",
                "Gerald Tesauro",
                "Murray Campbell"
            ],
            "title": "Eigenoption discovery through the deep successor representation",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2018
        },
        {
            "authors": [
                "Pietro Mazzaglia",
                "Ozan \u00c7atal",
                "Tim Verbelen",
                "B. Dhoedt"
            ],
            "title": "Curiosity-driven exploration via latent bayesian surprise",
            "venue": "In AAAI Conference on Artificial Intelligence (AAAI),",
            "year": 2022
        },
        {
            "authors": [
                "Pietro Mazzaglia",
                "Tim Verbelen",
                "B. Dhoedt",
                "Alexandre Lacoste",
                "Sai Rajeswar"
            ],
            "title": "Choreographer: Learning and adapting skills in imagination",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2023
        },
        {
            "authors": [
                "Russell Mendonca",
                "Oleh Rybkin",
                "Kostas Daniilidis",
                "Danijar Hafner",
                "Deepak Pathak"
            ],
            "title": "Discovering and achieving goals via world models",
            "venue": "In Neural Information Processing Systems (NeurIPS),",
            "year": 2021
        },
        {
            "authors": [
                "Shakir Mohamed",
                "Danilo J. Rezende"
            ],
            "title": "Variational information maximisation for intrinsically motivated reinforcement learning",
            "venue": "In Neural Information Processing Systems (NeurIPS),",
            "year": 2015
        },
        {
            "authors": [
                "Mirco Mutti",
                "Lorenzo Pratissoli",
                "Marcello Restelli"
            ],
            "title": "Task-agnostic exploration via policy gradient of a non-parametric state entropy estimate",
            "venue": "In AAAI Conference on Artificial Intelligence (AAAI),",
            "year": 2021
        },
        {
            "authors": [
                "OpenAI OpenAI",
                "Matthias Plappert",
                "Raul Sampedro",
                "Tao Xu",
                "Ilge Akkaya",
                "Vineet Kosaraju",
                "Peter Welinder",
                "Ruben D\u2019Sa",
                "Arthur Petron",
                "Henrique Pond\u00e9 de Oliveira Pinto",
                "Alex Paino",
                "Hyeonwoo Noh",
                "Lilian Weng",
                "Qiming Yuan",
                "Casey Chu",
                "Wojciech Zaremba"
            ],
            "title": "Asymmetric self-play for automatic goal discovery",
            "venue": "in robotic manipulation. ArXiv,",
            "year": 2021
        },
        {
            "authors": [
                "Georg Ostrovski",
                "Marc G. Bellemare",
                "A\u00e4ron van den Oord",
                "R\u00e9mi Munos"
            ],
            "title": "Count-based exploration with neural density models",
            "venue": "In International Conference on Machine Learning (ICML),",
            "year": 2017
        },
        {
            "authors": [
                "Sherjil Ozair",
                "Corey Lynch",
                "Yoshua Bengio",
                "A\u00e4ron van den Oord",
                "Sergey Levine",
                "Pierre Sermanet"
            ],
            "title": "Wasserstein dependency measure for representation learning",
            "venue": "In Neural Information Processing Systems (NeurIPS),",
            "year": 2019
        },
        {
            "authors": [
                "Seohong Park",
                "Sergey Levine"
            ],
            "title": "Predictable mdp abstraction for unsupervised model-based rl",
            "venue": "In International Conference on Machine Learning (ICML),",
            "year": 2023
        },
        {
            "authors": [
                "Seohong Park",
                "Jongwook Choi",
                "Jaekyeom Kim",
                "Honglak Lee",
                "Gunhee Kim"
            ],
            "title": "Lipschitzconstrained unsupervised skill discovery",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2022
        },
        {
            "authors": [
                "Seohong Park",
                "Dibya Ghosh",
                "Benjamin Eysenbach",
                "Sergey Levine"
            ],
            "title": "Hiql: Offline goalconditioned rl with latent states as actions",
            "venue": "In Neural Information Processing Systems (NeurIPS),",
            "year": 2023
        },
        {
            "authors": [
                "Seohong Park",
                "Kimin Lee",
                "Youngwoon Lee",
                "P. Abbeel"
            ],
            "title": "Controllability-aware unsupervised skill discovery",
            "venue": "In International Conference on Machine Learning (ICML),",
            "year": 2023
        },
        {
            "authors": [
                "Deepak Pathak",
                "Pulkit Agrawal",
                "Alexei A. Efros",
                "Trevor Darrell"
            ],
            "title": "Curiosity-driven exploration by self-supervised prediction",
            "venue": "In International Conference on Machine Learning (ICML),",
            "year": 2017
        },
        {
            "authors": [
                "Deepak Pathak",
                "Dhiraj Gandhi",
                "Abhinav Kumar Gupta"
            ],
            "title": "Self-supervised exploration via disagreement",
            "venue": "In International Conference on Machine Learning (ICML),",
            "year": 2019
        },
        {
            "authors": [
                "Silviu Pitis",
                "Harris Chan",
                "S. Zhao",
                "Bradly C. Stadie",
                "Jimmy Ba"
            ],
            "title": "Maximum entropy gain exploration for long horizon multi-goal reinforcement learning",
            "venue": "In International Conference on Machine Learning (ICML),",
            "year": 2020
        },
        {
            "authors": [
                "Vitchyr H. Pong",
                "Murtaza Dalal",
                "S. Lin",
                "Ashvin Nair",
                "Shikhar Bahl",
                "Sergey Levine"
            ],
            "title": "Skew-Fit: State-covering self-supervised reinforcement learning",
            "venue": "In International Conference on Machine Learning (ICML),",
            "year": 2020
        },
        {
            "authors": [
                "Ben Poole",
                "Sherjil Ozair",
                "A\u00e4ron van den Oord",
                "Alexander A. Alemi",
                "G. Tucker"
            ],
            "title": "On variational bounds of mutual information",
            "venue": "In International Conference on Machine Learning (ICML),",
            "year": 2019
        },
        {
            "authors": [
                "A.H. Qureshi",
                "Jacob J. Johnson",
                "Yuzhe Qin",
                "Taylor Henderson",
                "Byron Boots",
                "Michael C. Yip"
            ],
            "title": "Composing task-agnostic policies with deep reinforcement learning",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2020
        },
        {
            "authors": [
                "Sai Rajeswar",
                "Pietro Mazzaglia",
                "Tim Verbelen",
                "Alexandre Pich\u2019e",
                "B. Dhoedt",
                "Aaron C. Courville",
                "Alexandre Lacoste"
            ],
            "title": "Mastering the unsupervised reinforcement learning benchmark from pixels",
            "venue": "In International Conference on Machine Learning (ICML),",
            "year": 2023
        },
        {
            "authors": [
                "Nick Rhinehart",
                "Jenny Wang",
                "Glen Berseth",
                "John D. Co-Reyes",
                "Danijar Hafner",
                "Chelsea Finn",
                "Sergey Levine"
            ],
            "title": "Information is power: Intrinsic control via information capture",
            "venue": "In Neural Information Processing Systems (NeurIPS),",
            "year": 2021
        },
        {
            "authors": [
                "Nikolay Savinov",
                "Alexey Dosovitskiy",
                "Vladlen Koltun"
            ],
            "title": "Semi-parametric topological memory for navigation",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2018
        },
        {
            "authors": [
                "Tom Schaul",
                "Dan Horgan",
                "Karol Gregor",
                "David Silver"
            ],
            "title": "Universal value function approximators",
            "venue": "In International Conference on Machine Learning (ICML),",
            "year": 2015
        },
        {
            "authors": [
                "John Schulman",
                "Philipp Moritz",
                "Sergey Levine",
                "Michael I. Jordan",
                "P. Abbeel"
            ],
            "title": "High-dimensional continuous control using generalized advantage estimation",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2016
        },
        {
            "authors": [
                "John Schulman",
                "Filip Wolski",
                "Prafulla Dhariwal",
                "Alec Radford",
                "Oleg Klimov"
            ],
            "title": "Proximal policy optimization",
            "venue": "algorithms. ArXiv,",
            "year": 2017
        },
        {
            "authors": [
                "Ramanan Sekar",
                "Oleh Rybkin",
                "Kostas Daniilidis",
                "P. Abbeel",
                "Danijar Hafner",
                "Deepak Pathak"
            ],
            "title": "Planning to explore via self-supervised world models",
            "venue": "In International Conference on Machine Learning (ICML),",
            "year": 2020
        },
        {
            "authors": [
                "Younggyo Seo",
                "Lili Chen",
                "Jinwoo Shin",
                "Honglak Lee",
                "P. Abbeel",
                "Kimin Lee"
            ],
            "title": "State entropy maximization with random encoders for efficient exploration",
            "venue": "In International Conference on Machine Learning (ICML),",
            "year": 2021
        },
        {
            "authors": [
                "Nur Muhammad (Mahi) Shafiullah",
                "Lerrel Pinto"
            ],
            "title": "One after another: Learning incremental skills for a changing world",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2022
        },
        {
            "authors": [
                "Archit Sharma",
                "Shixiang Gu",
                "Sergey Levine",
                "Vikash Kumar",
                "Karol Hausman"
            ],
            "title": "Dynamicsaware unsupervised discovery of skills",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2020
        },
        {
            "authors": [
                "Pranav Shyam",
                "Wojciech Ja\u015bkowski",
                "Faustino J. Gomez"
            ],
            "title": "Model-based active exploration",
            "venue": "In International Conference on Machine Learning (ICML),",
            "year": 2019
        },
        {
            "authors": [
                "DJ Strouse",
                "Kate Baumli",
                "David Warde-Farley",
                "Vlad Mnih",
                "Steven Stenberg Hansen"
            ],
            "title": "Learning more skills through optimistic exploration",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2022
        },
        {
            "authors": [
                "Sainbayar Sukhbaatar",
                "Ilya Kostrikov",
                "Arthur D. Szlam",
                "Rob Fergus"
            ],
            "title": "Intrinsic motivation and automatic curricula via asymmetric self-play",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2018
        },
        {
            "authors": [
                "Haoran Tang",
                "Rein Houthooft",
                "Davis Foote",
                "Adam Stooke",
                "Xi Chen",
                "Yan Duan",
                "John Schulman",
                "Filip De Turck",
                "P. Abbeel"
            ],
            "title": "exploration: A study of count-based exploration for deep reinforcement learning",
            "venue": "In Neural Information Processing Systems (NeurIPS),",
            "year": 2017
        },
        {
            "authors": [
                "Emanuel Todorov",
                "Tom Erez",
                "Yuval Tassa"
            ],
            "title": "Mujoco: A physics engine for model-based control",
            "venue": "In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),",
            "year": 2012
        },
        {
            "authors": [
                "Ahmed Touati",
                "Yann Ollivier"
            ],
            "title": "Learning one representation to optimize all rewards",
            "venue": "In Neural Information Processing Systems (NeurIPS),",
            "year": 2021
        },
        {
            "authors": [
                "Ahmed Touati",
                "J\u00e9r\u00e9my Rapin",
                "Yann Ollivier"
            ],
            "title": "Does zero-shot reinforcement learning exist",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2023
        },
        {
            "authors": [
                "A\u00e4ron van den Oord",
                "Yazhe Li",
                "Oriol Vinyals"
            ],
            "title": "Representation learning with contrastive predictive",
            "venue": "coding. ArXiv,",
            "year": 2018
        },
        {
            "authors": [
                "C\u00e9dric Villani"
            ],
            "title": "Optimal transport: old and new",
            "year": 2009
        },
        {
            "authors": [
                "Tongzhou Wang",
                "Antonio Torralba",
                "Phillip Isola",
                "Amy Zhang"
            ],
            "title": "Optimal goal-reaching reinforcement learning via quasimetric learning",
            "venue": "In International Conference on Machine Learning (ICML),",
            "year": 2023
        },
        {
            "authors": [
                "David Warde-Farley",
                "Tom Van de Wiele",
                "Tejas Kulkarni",
                "Catalin Ionescu",
                "Steven Hansen",
                "Volodymyr Mnih"
            ],
            "title": "Unsupervised control through non-parametric discriminative rewards",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2019
        },
        {
            "authors": [
                "Rushuai Yang",
                "Chenjia Bai",
                "Hongyi Guo",
                "Siyuan Li",
                "Bin Zhao",
                "Zhen Wang",
                "Peng Liu",
                "Xuelong Li"
            ],
            "title": "Behavior contrastive learning for unsupervised skill discovery",
            "venue": "In International Conference on Machine Learning (ICML),",
            "year": 2023
        },
        {
            "authors": [
                "Denis Yarats",
                "Rob Fergus",
                "Alessandro Lazaric",
                "Lerrel Pinto"
            ],
            "title": "Reinforcement learning with prototypical representations",
            "venue": "In International Conference on Machine Learning (ICML),",
            "year": 2021
        },
        {
            "authors": [
                "Tom Zahavy",
                "Yannick Schroecker",
                "Feryal M.P. Behbahani",
                "Kate Baumli",
                "Sebastian Flennerhag",
                "Shaobo Hou",
                "Satinder Singh"
            ],
            "title": "Discovering policies with domino: Diversity optimization maintaining near optimality",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2023
        },
        {
            "authors": [
                "Tom Zahavy",
                "Vivek Veeriah",
                "Shaobo Hou",
                "Kevin Waugh",
                "Matthew Lai",
                "Edouard Leurent",
                "Nenad Tomasev",
                "Lisa Schut",
                "Demis Hassabis",
                "Satinder Singh"
            ],
            "title": "Diversifying ai: Towards creative chess with alphazero",
            "venue": "ArXiv, abs/2308.09175,",
            "year": 2023
        },
        {
            "authors": [
                "Jesse Zhang",
                "Haonan Yu",
                "Wei Xu"
            ],
            "title": "Hierarchical reinforcement learning by discovering intrinsic options",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2021
        },
        {
            "authors": [
                "Andrew Zhao",
                "Matthieu Lin",
                "Yangguang Li",
                "Y. Liu",
                "Gao Huang"
            ],
            "title": "A mixture of surprises for unsupervised reinforcement learning",
            "venue": "In Neural Information Processing Systems (NeurIPS),",
            "year": 2022
        },
        {
            "authors": [
                "Zihan Zhou",
                "Wei Fu",
                "Bingliang Zhang",
                "Yi Wu"
            ],
            "title": "Continuously discovering novel strategies via reward-switching policy optimization",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2022
        },
        {
            "authors": [
                "Park"
            ],
            "title": "Observation Global view (b) Humanoid Figure 10: Visualization of pixel-based DMC Quadruped and Humanoid. We use gradient-colored floors to allow the agent to infer its location from pixel observations, similarly to Hafner et al",
            "year": 2023
        },
        {
            "authors": [
                "2021 RL (Kostrikov et al",
                "2021 Chen et al",
                "Hiraoka"
            ],
            "title": "2022) or model-based RL (Hafner et al., 2020; Hansen et al., 2022) is an interesting direction for future work. Another limitation of this work is that, while we evaluate METRA on various locomotion and manipulation environments, following prior work in unsupervised RL and unsuperivsed skill discov",
            "year": 2022
        },
        {
            "authors": [
                "Park",
                "2023 Levine",
                "2023b Park et al",
                "2023 Li et al",
                "Kim"
            ],
            "title": "2023) and pure exploration (or unsupervised goal-conditioned RL) methods (Houthooft et al., 2016",
            "venue": "Bellemare et al.,",
            "year": 2019
        },
        {
            "authors": [
                "Hu et al",
                "Rajeswar"
            ],
            "title": "2023), there have also been proposed other types of unsupervised RL approaches, such as ones based on asymmetric self-play (Sukhbaatar et al., 2018; OpenAI et al., 2021), surprise minimization (Berseth et al., 2021; Rhinehart et al., 2021), and forward-backward representations (Touati",
            "venue": "Touati et al.,",
            "year": 2022
        },
        {
            "authors": [
                "Wang"
            ],
            "title": "2023) use similar temporal distance constraints to ours for goal-conditioned RL",
            "venue": "(Kaelbling,",
            "year": 1993
        },
        {
            "authors": [
                "Mendonca"
            ],
            "title": "We run our experiments on an internal cluster consisting of A5000 GPUs. Each run in Section 5.3 takes no more than 24 hours. G.1 ENVIRONMENTS Benchmark environments. For state-based environments, we use the same MuJoCo HalfCheetah and Ant environments",
            "venue": "(Todorov et al.,",
            "year": 2021
        },
        {
            "authors": [
                "Park"
            ],
            "title": "HalfCheetah has an 18-dimensional state space and Ant has a 29-dimensional state space. For pixel-based environments, we use pixel-based Quadruped and Humanoid from the DeepMind Control Suite",
            "venue": "(Tassa et al.,",
            "year": 2023
        },
        {
            "authors": [
                "Kitchen by Gupta"
            ],
            "title": "In DMC locomotion environments, we use gradient-colored floors to allow the agent to infer its location from pixels, similarly to Hafner et al",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Unsupervised pre-training has proven transformative in domains from natural language processing to computer vision: contrastive representation learning (Chen et al., 2020) can acquire effective features from unlabeled images, and generative autoregressive pre-training (Brown et al., 2020) can enable language models that can be adapted to a plethora of downstream applications. If we could derive an equally scalable framework for unsupervised reinforcement learning (RL) that autonomously explores the space of possible behaviors, then we could enable general-purpose unsupervised pre-trained agents to serve as an effective foundation for efficiently learning a broad range of downstream tasks. Hence, our goal in this work is to propose a scalable unsupervised RL objective that encourages an agent to explore its environment and learn a breadth of potentially useful behaviors without any supervision.\nWhile this formulation of unsupervised RL has been explored in a number of prior works, making fully unsupervised RL truly scalable still remains a major open challenge. Prior approaches to unsupervised RL can be categorized into two main groups: pure exploration methods (Burda et al., 2019; Pathak et al., 2019; Liu & Abbeel, 2021b; Mendonca et al., 2021; Rajeswar et al., 2023) and unsupervised skill discovery methods (Eysenbach et al., 2019a; Sharma et al., 2020; Laskin et al., 2022; Park et al., 2022). While these approaches have been shown to be effective in several unsupervised RL benchmarks (Mendonca et al., 2021; Laskin et al., 2021), it is not entirely clear whether such methods can indeed be scalable to complex environments with high intrinsic dimensionality. Pure exploration-based unsupervised RL approaches aim to either completely cover the entire state space (Burda et al., 2019; Liu & Abbeel, 2021b) or fully capture the transition dynamics of the Markov decision process (MDP) (Pathak et al., 2019; Sekar et al., 2020; Mendonca et al., 2021; Rajeswar et al., 2023). However, in complex environments with a large state space, it may be\ninfeasible to attain either of these aims. In fact, we will show that these methods fail to cover the state space even in the state-based 29-dimensional MuJoCo Ant environment. On the other hand, unsupervised skill discovery methods aim to discover diverse, distinguishable behaviors, e.g., by maximizing the mutual information between skills and states (Gregor et al., 2016; Eysenbach et al., 2019a). While these methods do learn behaviors that are mutually different, they either do not necessarily encourage exploration and thus often have limited state coverage in the complete absence of supervision (Eysenbach et al., 2019a; Sharma et al., 2020), or are not directly scalable to pixel-based control environments (Park et al., 2022; 2023b).\nIn this work, we aim to address these challenges and develop an unsupervised RL objective, which we call Metric-Aware Abstraction (METRA), that scales to complex, image-based environments with high intrinsic dimensionality. Our first main idea is to learn diverse behaviors that maximally cover not the original state space but a compact latent metric space defined by a mapping function \u03d5 : S \u2192 Z with a metric d. Here, the latent state is connected by the state space by the metric d, which ensures that covering latent space leads to coverage of the state space. Now, the question becomes which metric to use. Previous metric-based skill learning methods mostly used the Euclidean distance (or its scaled variant) between two states (He et al., 2022; Park et al., 2022; 2023b). However, such state-based metrics are not directly applicable to complex, high-dimensional state space (e.g., images). Our second main idea is therefore to use temporal distances (i.e., the number of minimum environment steps between two states) as a metric for the latent space. Temporal distances are invariant to state representations and thus applicable to pixel-based environments as well. As a result, by maximizing coverage in the compact latent space, we can acquire diverse behaviors that approximately cover the entire state space, being scalable to high-dimensional, complex environments (Figure 1).\nThrough our experiments on five state-based and pixel-based continuous control environments, we demonstrate that our method learns diverse, useful behaviors, as well as a compact latent space that can be used to solve various downstream tasks in a zero-shot manner, outperforming previous unsupervised RL methods. To the best of our knowledge, METRA is the first unsupervised RL method that demonstrates the discovery of diverse locomotion behaviors in pixel-based Quadruped and Humanoid environments."
        },
        {
            "heading": "2 WHY MIGHT PREVIOUS UNSUPERVISED RL METHODS FAIL TO SCALE?",
            "text": "The goal of unsupervised RL is to acquire useful knowledge, such as policies, world models, or exploratory data, by interacting with the environment in an unsupervised manner (i.e., without tasks or reward functions). Typically, this knowledge is then leveraged to solve downstream tasks more efficiently. Prior work in unsupervised RL can be categorized into two main groups: pure exploration methods and unsupervised skill discovery methods. Pure exploration methods aim to cover the entire state space or fully capture the environment dynamics. They encourage exploration by maximizing uncertainty (Pathak et al., 2017; Shyam et al., 2019; Burda et al., 2019; Pathak et al., 2019; Sekar et al., 2020; Mazzaglia et al., 2022) or state entropy (Lee et al., 2019; Pong et al., 2020; Liu & Abbeel, 2021b; Yarats et al., 2021). Based on the data collected by the exploration\npolicy, these methods learn a world model (Rajeswar et al., 2023), train a goal-conditioned policy (Pong et al., 2020; Pitis et al., 2020; Mendonca et al., 2021; Hu et al., 2023), learn skills via trajectory autoencoders (Campos Camu\u0301n\u0303ez et al., 2020; Mazzaglia et al., 2023), or directly finetune the learned exploration policy (Laskin et al., 2021) to accelerate downstream task learning. While these pure exploration-based approaches are currently the leading methods in unsupervised RL benchmarks (Mendonca et al., 2021; Laskin et al., 2021; Mazzaglia et al., 2023; Rajeswar et al., 2023), their scalability may be limited in complex environments with large state spaces because it is often computationally infeasible to completely cover every possible state or fully capture the dynamics. In Section 5, we empirically demonstrate that these approaches even fail to cover the state space of the state-based 29-dimensional Ant environment.\nAnother line of research in unsupervised RL aims to learn diverse behaviors (or skills) that are distinguishable from one another, and our method also falls into this category. The most common approach to unsupervised skill discovery is to maximize the mutual information (MI) between states and skills (Gregor et al., 2016; Eysenbach et al., 2019a; Sharma et al., 2020; Hansen et al., 2020):\nI(S;Z) = DKL(p(s, z)\u2225p(s)p(z)). (1) By associating different skill latent vectors z with different states s, these methods learn diverse skills that are mutually distinct. However, they share the limitation that they often end up discovering simple, static behaviors with limited state coverage (Campos Camu\u0301n\u0303ez et al., 2020; Park et al., 2022). This is because MI is defined by a KL divergence (Equation (1)), which is a metric-agnostic quantity (e.g., MI is invariant to scaling; see Figure 2). As a result, the MI objective only focuses on the distinguishability of behaviors, regardless of \u201chow different\u201d they are, resulting in limited state coverage (Campos Camu\u0301n\u0303ez et al., 2020; Park et al., 2022). To address this limitation, prior works combine the MI objective with exploration bonuses (Campos Camu\u0301n\u0303ez et al., 2020; Strouse et al., 2022; Park & Levine, 2023) or propose different objectives that encourage maximizing distances in the state space (He et al., 2022; Park et al., 2022; 2023b). Yet, it remains unclear whether these methods can scale to complex, high-dimensional environments, because they either attempt to completely capture the entire MDP (Campos Camu\u0301n\u0303ez et al., 2020; Strouse et al., 2022; Park & Levine, 2023) or assume a compact, structured state space (He et al., 2022; Park et al., 2022; 2023b). Indeed, to the best of our knowledge, no previous unsupervised skill discovery methods have succeeded in discovering locomotion behaviors on pixel-based locomotion environments. Unlike these approaches, our method learns a compact set of diverse behaviors that are maximally different in terms of the temporal distance. As a result, they can approximately cover the state space, even in a complex, high-dimensional environment. We discuss further related work in Appendix B."
        },
        {
            "heading": "3 PRELIMINARIES AND PROBLEM SETTING",
            "text": "We consider a controlled Markov process, an MDP without a reward function, defined as M = (S,A, \u00b5, p). S denotes the state space, A denotes the action space, \u00b5 : \u2206(S) denotes the initial state distribution, and p : S \u00d7A \u2192 \u2206(A) denotes the transition dynamics kernel. We consider a set of latent vectors z \u2208 Z , which can be either discrete or continuous, and a latent-conditioned policy \u03c0(a|s, z). Following the terminology in unsupervised skill discovery, we refer to latent vectors z (and their corresponding policies \u03c0(a|s, z)) as skills. When sampling a trajectory, we first sample a skill from the prior distribution, z \u223c p(z), and then roll out a trajectory with \u03c0(a|s, z), where z is fixed for the entire episode. Hence, the joint skill-trajectory distribution is given as p(\u03c4, z) =\np(z)p(s0) \u220fT\u22121 t=0 \u03c0(at|st, z)p(st+1|st, at), where \u03c4 denotes (s0, a0, s1, a1, . . . , sT ). Our goal in this work is to learn a set of diverse, useful behaviors \u03c0(a|s, z), without using any supervision, data, or prior knowledge."
        },
        {
            "heading": "4 A SCALABLE OBJECTIVE FOR UNSUPERVISED RL",
            "text": "Desiderata. We first state our two desiderata for a scalable unsupervised RL objective. First, instead of covering every possible state in a given MDP, which is infeasible in complex environments, we want to have a compact latent space Z of a tractable size and a latent-conditioned policy \u03c0(a|s, z) that translates latent vectors into actual behaviors. Second, we want the behaviors from different latent vectors to be different, collectively covering as much of the state space as possible. In other words, we want to maximize state coverage under the given capacity of Z . An algorithm that satisfies these two desiderata would be scalable to complex environments, because we only need to learn a compact set of behaviors that approximately cover the MDP.\nObjective. Based on the above, we propose the following novel objective for unsupervised RL: IW(S;Z) = W(p(s, z), p(s)p(z)), (2)\nwhere IW(S;Z) is the Wasserstein dependency measure (WDM) (Ozair et al., 2019) between states and skills, and W is the 1-Wasserstein distance on the metric space (S \u00d7 Z, d) with a distance metric d. Intuitively, the WDM objective in Equation (2) can be viewed as a \u201cWasserstein variant\u201d of the previous MI objective (Equation (1)), where the KL divergence in MI is replaced with the Wasserstein distance. However, despite the apparent similarity, there exists a significant difference between the two objectives: MI is completely agnostic to the underlying distance metric, while WDM is a metric-aware quantity. As a result, the WDM objective (Equation (2)) not only discovers diverse skills that are different from one another, as in the MI objective, but also actively maximizes distances d between different skill trajectories (Figure 2). This makes them collectively cover the state space as much as possible (in terms of the given metric d). The choice of metric for d is critical for effective skill discovery, and simple choices like Euclidean metrics on the state space would generally not be effective for non-metric state representations, such as images. Therefore, instantiating this approach with the right metric is an important part of our contribution, as we will discuss in Section 4.2. Until then, we assume that we have a given metric d."
        },
        {
            "heading": "4.1 TRACTABLE OPTIMIZATION",
            "text": "While our objective IW(S;Z) has several desirable properties, it is not immediately straightforward to maximize this quantity in practice. In this section, we describe a simple, tractable objective that can be used to maximize IW(S;Z) in practice. We begin with the Kantorovich-Rubenstein duality (Villani et al., 2009; Ozair et al., 2019), which provides a tractable way to maximize the Wasserstein dependency measure:\nIW(S;Z) = sup \u2225f\u2225L\u22641\nEp(s,z)[f(s, z)]\u2212 Ep(s)p(z)[f(s, z)], (3)\nwhere \u2225f\u2225L denotes the Lipschitz constant for the function f : S\u00d7Z \u2192 R under the given distance metric d, i.e., \u2225f\u2225L = sup(s1,z1 )\u0338=(s2,z2) |f(s1, z1) \u2212 f(s2, z2)|/d((s1, z1), (s2, z2)). Intuitively, f is a score function that assigns larger values to (s, z) tuples sampled from the joint distribution and smaller values to (s, z) tuples sampled independently from their marginal distributions. We note that Equation (3) is already a tractable objective, as we can jointly train a 1-Lipschitz-constrained score function f(s, z) using gradient descent and a skill policy \u03c0(a|s, z) using RL, with the reward function being an empirical estimate of Equation (3), r(s, z) = f(s, z)\u2212N\u22121\u2211Ni=1 f(s, zi), where z1, z2, . . . , zN are N independent random samples from the prior distribution p(z).\nHowever, since samplingN additional zs for each data point is computationally demanding, we will further simplify the objective to enable more efficient learning. First, we consider the parameterization f(s, z) = \u03d5(s)\u22a4\u03c8(z) with \u03d5 : S \u2192 RD and \u03c8 : Z \u2192 RD with independent 1-Lipschitz constraints1, which yields the following objective:\nIW(S;Z) \u2248 sup \u2225\u03d5\u2225L\u22641,\u2225\u03c8\u2225L\u22641 Ep(s,z)[\u03d5(s)\u22a4\u03c8(z)]\u2212 Ep(s)[\u03d5(s)]\u22a4Ep(z)[\u03c8(z)]. (4)\n1While \u2225\u03d5\u2225L \u2264 1, \u2225\u03c8\u2225L \u2264 1 is not technically equivalent to \u2225f\u2225L \u2264 1, we use the former as it is more tractable. Also, we note that \u2225f\u2225L can be upper-bounded in terms of \u2225\u03d5\u2225L, \u2225\u03c8\u2225L, sups \u2225\u03d5(s)\u22252, and supz \u2225\u03c8(z)\u22252 under d((s1, z1), (s2, z2)) = (sups \u2225\u03d5(s)\u22252)\u2225\u03c8\u2225Ld(z1, z2) + (supz \u2225\u03c8(z)\u22252)\u2225\u03d5\u2225Ld(s1, s2).\nHere, we note that the decomposition f(s, z) = \u03d5(s)\u22a4\u03c8(z) is universal; i.e., the expressiveness of f(s, z) is equivalent to that of \u03d5(s)\u22a4\u03c8(z) when D \u2192 \u221e. The proof can be found in Appendix C. Next, we consider a variant of the Wasserstein dependency measure that only depends on the last state: IW(ST ;Z), similarly to VIC (Gregor et al., 2016). This allows us to further decompose the objective with a telescoping sum as follows:\nIW(ST ;Z) \u2248 sup \u2225\u03d5\u2225L\u22641,\u2225\u03c8\u2225L\u22641 Ep(\u03c4,z)[\u03d5(sT )\u22a4\u03c8(z)]\u2212 Ep(\u03c4)[\u03d5(sT )]\u22a4Ep(z)[\u03c8(z)] (5)\n= sup \u03d5,\u03c8 T\u22121\u2211 t=0 ( Ep(\u03c4,z)[(\u03d5(st+1)\u2212 \u03d5(st))\u22a4\u03c8(z)]\u2212 Ep(\u03c4)[\u03d5(st+1)\u2212 \u03d5(st)]\u22a4Ep(z)[\u03c8(z)] ) , (6)\nwhere we also use the fact that p(s0) and p(z) are independent. Finally, we set \u03c8(z) to z. While this makes \u03c8 less expressive, it allows us to derive the following concise objective:\nIW(ST ;Z) \u2248 sup \u2225\u03d5\u2225L\u22641 Ep(\u03c4,z) [ T\u22121\u2211 t=0 (\u03d5(st+1)\u2212 \u03d5(st))\u22a4(z \u2212 z\u0304) ] , (7)\nwhere z\u0304 = Ep(z)[z]. Here, since we can always shift the prior distribution p(z) to have a zero mean, we can assume z\u0304 = 0 without loss of generality. This objective can now be easily maximized by jointly training \u03d5(s) and \u03c0(a|s, z) with r(s, z, s\u2032) = (\u03d5(s\u2032) \u2212 \u03d5(s))\u22a4z under the constraint \u2225\u03d5\u2225L \u2264 1. Note that we do not need any additional random samples of z, unlike Equation (3)."
        },
        {
            "heading": "4.2 FULL OBJECTIVE: METRIC-AWARE ABSTRACTION (METRA)",
            "text": "So far, we have not specified the distance metric d for the Wasserstein distance in WDM (or equivalently for the Lipschitz constraint \u2225\u03d5\u2225L \u2264 1). Choosing an appropriate distance metric is crucial for learning a compact set of useful behaviors, because it determines the priority by which the behaviors are learned within the capacity of Z . Previous metric-based skill discovery methods mostly employed the Euclidean distance (or its scaled variant) as a metric (He et al., 2022; Park et al., 2022; 2023b). However, they are not directly scalable to high-dimensional environments with pixel-based observations, in which the Euclidean distance is not necessarily meaningful.\nIn this work, we propose to use the temporal distance (Kaelbling, 1993; Hartikainen et al., 2020; Durugkar et al., 2021) between two states as a distance metric dtemp(s1, s2), the minimum number of environment steps to reach s2 from s1. This provides a natural way to measure the distance between two states, as it only depends on the inherent transition dynamics of the MDP, being invariant to the state representation and thus scalable to pixel-based environments. Using the temporal distance metric, we can rewrite Equation (7) as follows:\nsup \u03c0,\u03d5 Ep(\u03c4,z) [ T\u22121\u2211 t=0 (\u03d5(st+1)\u2212 \u03d5(st))\u22a4z ] s.t. \u2225\u03d5(s)\u2212 \u03d5(s\u2032)\u22252 \u2264 1, \u2200(s, s\u2032) \u2208 Sadj, (8)\nwhere Sadj denotes the set of adjacent state pairs in the MDP. Note that \u2225\u03d5\u2225L \u2264 1 is equivalently converted into \u2225\u03d5(s)\u2212 \u03d5(s\u2032)\u22252 \u2264 1 under the temporal distance metric (see Theorem C.3). Intuition and interpretation. We next describe how the constrained objective in Equation (8) may be interpreted. Intuitively, a policy \u03c0(a|s, z) that maximizes our objective should learn to move as far as possible along various directions in the latent space (specified by z). Since distances in the latent space, \u2225\u03d5(s1)\u2212 \u03d5(s2)\u22252, are always upper-bounded by the corresponding temporal distances in the MDP, given by dtemp(s1, s2), the learned latent space should assign its (limited) dimensions to the manifolds in the original state space that are maximally \u201cspread out\u201d, in the sense that shortest paths within the set of represented states should be as long as possible. This conceptually resembles \u201cprincipal components\u201d of the state space, but with respect to shortest paths rather than Euclidean distances, and with non-linear \u03d5 rather than linear \u03d5. Thus, we would expect \u03d5 to learn to abstract the state space in a lossy manner, preserving temporal distances (Figure 9), and emphasizing those degrees of freedom of the state that span the largest possible \u201ctemporal\u201d (non-linear) manifolds (Figure 1). Based on this intuition, we call our method Metric-Aware Abstraction (METRA). In Appendix D, we derive a formal connection between METRA and principal component analysis (PCA) under the temporal distance metric under several simplifying assumptions. Theorem 4.1 (Informal statement of Theorem D.2). Under some simplifying assumptions, linear squared METRA is equivalent to PCA under the temporal distance metric.\nAlgorithm 1 Metric-Aware Abstraction (METRA) 1: Initialize skill policy \u03c0(a|s, z), representation function \u03d5(s), Lagrange multiplier \u03bb, replay buffer D 2: for i\u2190 1 to (# epochs) do 3: for j \u2190 1 to (# episodes per epoch) do 4: Sample skill z \u223c p(z) 5: Sample trajectory \u03c4 with \u03c0(a|s, z) and add to replay buffer D 6: end for 7: Update \u03d5(s) to maximize E(s,z,s\u2032)\u223cD[(\u03d5(s\u2032)\u2212 \u03d5(s))\u22a4z + \u03bb \u00b7min(\u03b5, 1\u2212 \u2225\u03d5(s)\u2212 \u03d5(s\u2032)\u222522)] 8: Update \u03bb to minimize E(s,z,s\u2032)\u223cD[\u03bb \u00b7min(\u03b5, 1\u2212 \u2225\u03d5(s)\u2212 \u03d5(s\u2032)\u222522)] 9: Update \u03c0(a|s, z) using SAC (Haarnoja et al., 2018a) with reward r(s, z, s\u2032) = (\u03d5(s\u2032)\u2212 \u03d5(s))\u22a4z 10: end for\nConnections to previous skill discovery methods. There exist several intriguing connections between our WDM objective (Equation (2)) and previous skill discovery methods, including DIAYN (Eysenbach et al., 2019a), DADS (Sharma et al., 2020), CIC (Laskin et al., 2022), LSD (Park et al., 2022), and CSD (Park et al., 2023b). Perhaps the most apparent connections are with LSD and CSD, which also use similar constrained objectives to Equation (7). In fact, although not shown by the original authors, the constrained inner product objectives of LSD and CSD are also equivalent to IW(ST ;Z), but with the Euclidean distance (or its normalized variant), instead of the temporal distance. Also, the connection between WDM and Equation (7) provides further theoretical insight into the rather \u201cad-hoc\u201d choice of zero-centered one-hot vectors used in discrete LSD (Park et al., 2022); we must use a zero-mean prior distribution due to the z \u2212 z\u0304 term in Equation (7). There exist several connections between our WDM objective and previous MI-based skill discovery methods as well. Specifically, by simplifying WDM (Equation (2)) in three different ways, we can obtain \u201cWasserstein variants\u201d of DIAYN, DADS, and CIC. We refer to Appendix E for detailed derivations.\nZero-shot goal-reaching with METRA. Thanks to the state abstraction function \u03d5(s), METRA provides a simple way to command the skill policy to reach a goal state in a zero-shot manner, as in LSD (Park et al., 2022). Since \u03d5 abstracts the state space preserving temporal distances, the difference vector \u03d5(g) \u2212 \u03d5(s) tells us the skill we need to select to reach the goal state g from the current state s. As such, by simply setting z = (\u03d5(g) \u2212 \u03d5(s))/\u2225\u03d5(g) \u2212 \u03d5(s)\u22252 (for continuous skills) or z = argmaxdim (\u03d5(g) \u2212 \u03d5(s)) (for discrete skills), we can find the skill that leads to the goal. With this technique, METRA can solve goal-conditioned tasks without learning a separate goal-conditioned policy, as we will show in Section 5.3.\nImplementation. We optimize the constrained objective in Equation (8) using dual gradient descent with a Lagrange multiplier \u03bb and a small relaxation constant \u03b5 > 0, similarly to Park et al. (2023b); Wang et al. (2023). We provide a pseudocode for METRA in Algorithm 1.\nLimitations. One potential issue with Equation (8) is that we embed the temporal distance into the symmetric Euclidean distance in the latent space, where the temporal distance can be asymmetric. This makes our temporal distance abstraction more \u201cconservative\u201d in the sense that it considers the minimum of both temporal distances, i.e., \u2225\u03d5(s1) \u2212 \u03d5(s2)\u22252 \u2264 min(dtemp(s1, s2), dtemp(s2, s1)). While this conservatism is less problematic in our benchmark environments, in which transitions are mostly \u201csymmetric\u201d, it might be overly restrictive in highly asymmetric environments. To resolve this, we can replace the Euclidean distance \u2225\u03d5(s1) \u2212 \u03d5(s2)\u22252 in Equation (8) with an asymmetric quasimetric, as in Wang et al. (2023). We leave this extension for future work. Another limitation is that the simplified WDM objective in Equation (7) only considers behaviors that move linearly in the latent space. While this does not necessarily imply that the behaviors are also linear in the original state space (because \u03d5 : S \u2192 Z is a nonlinear mapping), this simplification, which stems from the fact that we set \u03c8(z) = z, might restrict the diversity of behaviors to some degree. We believe this can be addressed by using the full WDM objective in Equation (4). Notably, the full objective (Equation (4)) resembles contrastive learning, and we believe combining it with scalable contrastive learning techniques is an exciting future research direction (see Appendix E.3). We refer to Appendix A for practical limitations regarding our implementation of METRA."
        },
        {
            "heading": "5 EXPERIMENTS",
            "text": "Through our experiments in benchmark environments, we aim to answer the following questions: (1) Can METRA scale to complex, high-dimensional environments, including domains with image observations? (2) Does METRA discover meaningful behaviors in complex environments with no supervision? (3) Are the behaviors discovered by METRA useful for downstream tasks?\n5.1 EXPERIMENTAL SETUP Quadruped (Pixels) Humanoid (Pixels) Kitchen (Pixels) HalfCheetah (States) Ant (States)\nWe evaluate our method on five robotic locomotion and manipulation environments (Figure 4): state-based Ant and HalfCheetah from Gym (Todorov et al., 2012; Brockman et al., 2016), pixel-based Quadruped and Humanoid from the DeepMind Control (DMC) Suite (Tassa et al., 2018), and a pixel-based version of Kitchen\nfrom Gupta et al. (2019); Mendonca et al. (2021). For pixel-based DMC locomotion environments, we use colored floors to allow the agent to infer its location from pixels, similarly to Hafner et al. (2022); Park et al. (2023a) (Figure 10). Throughout the experiments, we do not use any prior knowledge, data, or supervision (e.g., observation restriction, early termination, etc.). As such, in pixelbased environments, the agent must learn diverse behaviors solely from 64\u00d7 64\u00d7 3 camera images. We compare METRA against 11 previous methods in three groups: (1) unsupervised skill discovery, (2) unsupervised exploration, and (3) unsupervised goal-reaching methods. For unsupervised skill discovery methods, we compare against two MI-based approaches, DIAYN (Eysenbach et al., 2019a) and DADS (Sharma et al., 2020), one hybrid method that combines MI and an exploration bonus, CIC (Laskin et al., 2022), and one metric-based approach that maximizes Euclidean distances, LSD (Park et al., 2022). For unsupervised exploration methods, we consider five pure exploration approaches, ICM (Pathak et al., 2017), RND (Burda et al., 2019), Plan2Explore (Sekar et al., 2020) (or Disagreement (Pathak et al., 2019)), APT (Liu & Abbeel, 2021b), and LBS (Mazzaglia et al., 2022), and one hybrid approach that combines exploration and successor features, APS (Liu & Abbeel, 2021a). We note that the Dreamer (Hafner et al., 2020) variants of these methods (especially LBS (Mazzaglia et al., 2022)) are currently the state-of-the-art methods in the pixel-based unsupervised RL benchmark (Laskin et al., 2021; Rajeswar et al., 2023). For unsupervised goal-reaching methods, we mainly compare with a state-of-the-art unsupervised RL approach, LEXA (Mendonca et al., 2021), as well as two previous skill discovery methods that enable zero-shot goal-reaching, DIAYN and LSD. We use 2-D skills for Ant and Humanoid, 4-D skills for Quadruped, 16 discrete skills for HalfCheetah, and 24 discrete skills for Kitchen. For CIC, we use 64-D skill latent vectors for all environments, following the original suggestion (Laskin et al., 2022)."
        },
        {
            "heading": "5.2 QUALITATIVE COMPARISON",
            "text": "We first demonstrate examples of behaviors (or skills) learned by our method and the 10 prior unsupervised RL methods on each of the five benchmark environments in Figure 3. The figure illustrates that METRA discovers diverse behaviors in both state-based and pixel-based domains. Notably, METRA is the only method that successfully discovers locomotion skills in pixel-based Quadruped and Humanoid, and shows qualitatively very different behaviors from previous unsupervised RL methods across the environments. Pure exploration methods mostly exhibit chaotic, random behaviors (videos), and fail to fully explore the state space (in terms of x-y coordinates) even in state-based\nPublished as a conference paper at ICLR 2024\nx\n0.04\nx\n0.00\n0.02\ny\nAnt and HalfCheetah. This is because it is practically infeasible to completely cover the infinitely many combinations of joint angles and positions in these domains. MI-based skill discovery methods also fail to explore large portions of the state space due to the metric-agnosticity of the KL divergence (Section 2), even when combined with an exploration bonus (i.e., CIC). LSD, a previous metric-based skill discovery method that maximizes Euclidean distances, does discover locomotion skills in state-based environments, but fails to scale to the pixel-based environments, where the Euclidean distance on image pixels does not necessarily provide a meaningful metric. In contrast to these methods, METRA learns various task-related behaviors by maximizing temporal distances in diverse ways. On our project page, we show additional qualitative results of METRA with different skill spaces. We note that, when combined with a discrete latent space, METRA discovers even more diverse behaviors, such as doing a backflip and taking a static posture, in addition to locomotion skills. We refer to Appendix F for visualization of learned latent spaces of METRA."
        },
        {
            "heading": "5.3 QUANTITATIVE COMPARISON",
            "text": "Next, we quantitatively compare METRA against three groups of 11 previous unsupervised RL approaches, using different metrics that are tailored to each group\u2019s primary focus. For quantitative results, we use 8 seeds and report 95% confidence intervals, unless otherwise stated.\nComparison with unsupervised skill discovery methods. We first compare METRA with other methods that also aim to solve the skill discovery problem (i.e., learning a latent-conditioned policy \u03c0(a|s, z) that performs different skills for different values of z). These include LSD, CIC, DIAYN, and DADS2. We implement these methods on the same codebase as METRA. For comparison, we employ two metrics: policy coverage and downstream task performance. Figure 5 presents the policy coverage results, where we evaluate the skill policy\u2019s x coverage (HalfCheetah), x-y coverage (Ant, Quadruped, and Humanoid), or task (Kitchen) coverage at each evaluation epoch. The results show that METRA achieves the best performance in most of the domains, and is the only method that successfully learns meaningful skills in the pixel-based settings, where previous skill discovery methods generally fail. In Figure 6, we evaluate the applicability of the skills discovered by each method to downstream tasks, where the downstream task is learned by a hierarchical controller \u03c0h(z|s) that selects (frozen) learned skills to maximize the task reward (see Appendix G for details). METRA again achieves the best performance on most of these tasks, suggesting that the behaviors learned by METRA not only provide greater coverage, but also are more suitable for downstream tasks in these domains.\nComparison with pure exploration methods. Next, we quantitatively compare METRA to five unsupervised exploration methods, which do not aim to learn skills but only attempt to cover the state space, ICM, LBS3, RND, APT, and Plan2Explore (or Disagreement), and one hybrid method\n2We do not compare against DADS in pixel-based environments due to the computational cost of its skill dynamics model p(s\u2032|s, z), which requires predicting the full next image.\n3Since LBS requires a world model, we only evaluate it on pixel-based environments, where we use the Dreamer variants of pure exploration methods (Rajeswar et al., 2023).\nPublished as a conference paper at ICLR 2024\nx\n0.02\n0.04\nx\n0.00\n0.02\ny\nthat combines exploration and successor features, APS. We use the original implementations by Laskin et al. (2021) for state-based environments and the Dreamer versions by Rajeswar et al. (2023) for pixel-based environments. As the underlying RL backbones are very different (e.g., Dreamer is model-based, while METRA uses model-free SAC), we compare the methods based on wall clock time. For the metric, instead of policy coverage (as in Figure 5), we measure total state coverage (i.e., the number of bins covered by any training trajectories up to each evaluation epoch). This metric is more generous toward the exploration methods, since such methods might not cover the entire space on any single iteration, but rather visit different parts of the space on different iterations (in contrast to our method, which aims to produce diverse skills). In Kitchen, we found that most methods max out the total task coverage metric, and we instead use both the queue coverage and policy coverage metrics (see Appendix G for details). Figure 7 presents the results, showing that METRA achieves the best coverage in most of the environments. While pure exploration methods also work decently in the pixel-based Kitchen, they fail to fully explore the state spaces of statebased Ant and pixel-based Humanoid, which have complex dynamics with nearly infinite possible combinations of positions, joint angles, and velocities.\nComparison with unsupervised goal-reaching methods. Finally, we compare METRA with LEXA, a state-of-the-art unsupervised goal-reaching method. LEXA trains an exploration policy with Plan2Explore (Sekar et al., 2020), which maximizes epistemic uncertainty in the transition dynamics model, in parallel with a goal-conditioned policy \u03c0(a|s, g) on the data collected by the exploration policy. We compare the performances of METRA, LEXA, and two previous skill discovery methods (DIAYN and LSD) on five goal-reaching downstream tasks. We use the procedure described in Section 4.2 to solve goal-conditioned tasks in a zero-shot manner with METRA. Figure 8 presents the comparison results, where METRA achieves the best performance on all of the five downstream tasks. While LEXA also achieves non-trivial performances in three tasks, it struggles with state-based Ant and pixel-based Humanoid, likely because it is practically challenging to completely capture the transition dynamics of these complex environments."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "In this work, we presented METRA, a scalable unsupervised RL method based on the idea of covering a compact latent skill space that is connected to the state space by a temporal distance metric. We showed that METRA learns diverse useful behaviors in various locomotion and manipulation environments, being the first unsupervised RL method that learns locomotion behaviors in pixel-based Quadruped and Humanoid.\nFinal remarks. In unsupervised RL, many excellent prior works have explored pure exploration or mutual information skill learning. However, given that these methods are not necessarily readily scalable to complex environments with high intrinsic state dimensionality, as discussed in Section 2, we may need a completely different approach to enable truly scalable unsupervised RL. We hope that this work serves as a step toward broadly applicable unsupervised RL that enables large-scale pre-training with minimal supervision."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "We would like to thank Youngwoon Lee for an informative discussion, and RAIL members and anonymous reviewers for their helpful comments. This work was partly supported by the Korea Foundation for Advanced Studies (KFAS), ARO W911NF-21-1-0097, and the Office of Naval Research. This research used the Savio computational cluster resource provided by the Berkeley Research Computing program at UC Berkeley."
        },
        {
            "heading": "REPRODUCIBILITY STATEMENT",
            "text": "We provide our code at the following repository: https://github.com/seohongpark/ METRA. We provide the full experimental details in Appendix G."
        },
        {
            "heading": "A LIMITATIONS",
            "text": "Despite its state-of-the-art performance in several benchmark environments, METRA, in its current form, has limitations. We refer to Section 4.2 for the limitations and future research directions regarding the METRA objective. In terms of practical implementation, METRA, like other similar unsupervised skill discovery methods (Sharma et al., 2020; Park et al., 2022; 2023b), uses a relatively small update-to-data (UTD) ratio (i.e., the average number of gradient steps per environment step); e.g., we use 1/4 for Kitchen and 1/16 for Quadruped and Humanoid. Although we demonstrate that METRA learns efficiently in terms of wall clock time, we believe there is room for improvement in terms of sample efficiency. This is mainly because we use vanilla SAC (Haarnoja et al., 2018a) as its RL backbone for simplicity, and we believe increasing the sample efficiency of METRA by combining it with recent techniques in model-free RL (Kostrikov et al., 2021; Chen et al., 2021; Hiraoka et al., 2022) or model-based RL (Hafner et al., 2020; Hansen et al., 2022) is an interesting direction for future work.\nAnother limitation of this work is that, while we evaluate METRA on various locomotion and manipulation environments, following prior work in unsupervised RL and unsuperivsed skill discovery (Eysenbach et al., 2019a; Sharma et al., 2020; Mendonca et al., 2021; Laskin et al., 2021; He et al., 2022; Park et al., 2022; Zhao et al., 2022; Shafiullah & Pinto, 2022; Laskin et al., 2022; Park et al., 2023b; Yang et al., 2023), we have not evaluated METRA on other different types of environments, such as Atari games. Also, since we assume a fixed MDP (i.e., stationary, fully observable dynamics, Section 3), METRA in its current form does not particularly deal with non-stationary or\nnon-Markovian dynamics. We leave applying METRA to more diverse environments or extending the idea behind METRA to non-stationary or non-Markovian environments for future work."
        },
        {
            "heading": "B EXTENDED RELATED WORK",
            "text": "In addition to unsupervised skill discovery (Mohamed & Rezende, 2015; Gregor et al., 2016; Florensa et al., 2017; Co-Reyes et al., 2018; Achiam et al., 2018; Eysenbach et al., 2019a; Warde-Farley et al., 2019; Shyam et al., 2019; Lee et al., 2019; Sharma et al., 2020; Campos Camu\u0301n\u0303ez et al., 2020; Hansen et al., 2020; Pong et al., 2020; Baumli et al., 2021; Choi et al., 2021; Yarats et al., 2021; Kim et al., 2021; Zhang et al., 2021; He et al., 2022; Strouse et al., 2022; Laskin et al., 2022; Park et al., 2022; Shafiullah & Pinto, 2022; Jiang et al., 2022; Zhao et al., 2022; Kamienny et al., 2022; Park & Levine, 2023; Park et al., 2023b; Li et al., 2023; Kim et al., 2023) and pure exploration (or unsupervised goal-conditioned RL) methods (Houthooft et al., 2016; Bellemare et al., 2016; Tang et al., 2017; Ostrovski et al., 2017; Fu et al., 2017; Pathak et al., 2017; Hazan et al., 2019; Shyam et al., 2019; Burda et al., 2019; Pathak et al., 2019; Lee et al., 2019; Ecoffet et al., 2020; Pitis et al., 2020; Badia et al., 2020; Mutti et al., 2021; Liu & Abbeel, 2021b; Mendonca et al., 2021; Yarats et al., 2021; Seo et al., 2021; Mazzaglia et al., 2022; 2023; Hu et al., 2023; Rajeswar et al., 2023), there have also been proposed other types of unsupervised RL approaches, such as ones based on asymmetric self-play (Sukhbaatar et al., 2018; OpenAI et al., 2021), surprise minimization (Berseth et al., 2021; Rhinehart et al., 2021), and forward-backward representations (Touati & Ollivier, 2021; Touati et al., 2023). One potentially closely related line of work is graph Laplacian-based option discovery methods (Machado et al., 2017; 2018; Klissarov & Machado, 2023). These methods learn a set of diverse behaviors based on the eigenvectors of the graph Laplacian of the MDP\u2019s adjacency matrix. Although we have not found a formal connection to these methods, we suspect there might exist a deep, intriguing connection between METRA and graph Laplacian-based methods, given that they both discover behaviors based on the temporal dynamics of the MDP. METRA is also related to several works in goal-conditioned RL that consider temporal distances (Kaelbling, 1993; Schaul et al., 2015; Savinov et al., 2018; Eysenbach et al., 2019b; Florensa et al., 2019; Hartikainen et al., 2020; Durugkar et al., 2021; Wang et al., 2023). In particular, Durugkar et al. (2021); Wang et al. (2023) use similar temporal distance constraints to ours for goal-conditioned RL."
        },
        {
            "heading": "C THEORETICAL RESULTS",
            "text": ""
        },
        {
            "heading": "C.1 UNIVERSALITY OF INNER PRODUCT DECOMPOSITION",
            "text": "Lemma C.1. Let X and Y be compact Hausdorff spaces (e.g., compact subsets in RN ) and C(A) be the set of real-valued continuous functions on A. For any function f(x, y) \u2208 C(X \u00d7 Y) and \u03f5 > 0, there exist continuous functions \u03d5(x) : X \u2192 RD and \u03d5(y) : Y \u2192 RD with D \u2265 1 such that supx\u2208X ,y\u2208Y |f(x, y)\u2212 \u03d5(x)\u22a4\u03c8(y)| < \u03b5.\nProof. We invoke the Stone-Weierstrass theorem (Bass (2013), Theorem 20.44), which implies that the set of functions T := {\u2211Di=1 \u03d5i(x)\u03c8i(y) : D \u2208 N,\u22001 \u2264 i \u2264 D,\u03d5i \u2208 C(X ), \u03c8i \u2208 C(Y)} is dense in C(X \u00d7 Y) if T is an algebra that separates points and vanishes at no point. The only non-trivial part is to show that T is closed under multiplication. Consider g(1)(x, y) = \u2211D1 i=1 \u03d5 (1) i (x)\u03c8 (1) i (y) \u2208 T and g(2)(x, y) = \u2211D2 i=1 \u03d5 (2) i (x)\u03c8 (2) i (y) \u2208 T . We have\ng(1)(x, y)g(2)(x, y) = \u2211D1 i=1 \u2211D2 j=1 \u03d5 (1) i (x)\u03d5 (2) j (x)\u03c8 (1) i (y)\u03c8 (2) j (y), where \u03d5 (1) i (x)\u03d5 (2) j (x) \u2208 C(X ) and \u03c8(1)i (y)\u03c8 (2) j (y) \u2208 C(Y) for all i, j. Hence, g(1)(x, y)g(2)(x, y) \u2208 T .\nTheorem C.2 (\u03d5(x)\u22a4\u03c8(y) is a universal approximator of f(x, y)). Let X and Y be compact Hausdorff spaces and \u03a6 \u2282 C(X ) and \u03a8 \u2282 C(Y) be dense sets in C(X ) and C(Y), respectively. Then, T := {\u2211Di=1 \u03d5i(x)\u03c8i(y) : D \u2208 N,\u22001 \u2264 i \u2264 D,\u03d5i \u2208 \u03a6, \u03c8i \u2208 \u03a8} is also dense in C(X \u00d7 Y). In other words, \u03d5(x)\u22a4\u03c8(y) can approximate f(x, y) to arbitrary accuracy if \u03d5 and \u03c8 are modeled with universal approximators (e.g., neural networks) and D \u2192 \u221e.\nProof. By Lemma C.1, for any f \u2208 C(X \u00d7 Y) and \u03b5 > 0, there exist D \u2208 N, \u03d5i \u2208 C(X ), and \u03c8i \u2208 C(Y) for 1 \u2264 i \u2264 D such that supx\u2208X ,y\u2208Y |f(x, y) \u2212 \u2211D i=1 \u03d5i(x)\u03c8i(y)| < \u03b5/3. Define\nMy := sup1\u2264i\u2264D,y\u2208Y |\u03c8i(y)|. Since \u03a6 is dense, for each 1 \u2264 i \u2264 D, there exists \u03d5\u0303i \u2208 \u03a6 such that supx\u2208X |\u03d5i(x) \u2212 \u03d5\u0303i(x)| < \u03b5/(3DMy). Define Mx := sup1\u2264i\u2264D,x\u2208X |\u03d5\u0303i(x)|. Similarly, for each 1 \u2264 i \u2264 D, there exists \u03c8\u0303i \u2208 \u03a8 such that supy\u2208Y |\u03c8i(y)\u2212 \u03c8\u0303i(y)| < \u03b5/(3DMx). Now, we have\u2223\u2223\u2223\u2223\u2223f(x, y)\u2212 D\u2211 i=1 \u03d5\u0303i(x)\u03c8\u0303i(y) \u2223\u2223\u2223\u2223\u2223 \u2264 \u2223\u2223\u2223\u2223\u2223f(x, y)\u2212 D\u2211 i=1 \u03d5i(x)\u03c8i(y) \u2223\u2223\u2223\u2223\u2223+ D\u2211 i=1\n\u2223\u2223\u2223\u03d5\u0303i(x)\u03c8\u0303i(y)\u2212 \u03d5i(x)\u03c8i(y)\u2223\u2223\u2223 (9) < \u03b5\n3 + D\u2211 i=1 |\u03d5\u0303i(x)(\u03c8\u0303i(y)\u2212 \u03c8i(y))|+ D\u2211 i=1 |(\u03d5\u0303i(x)\u2212 \u03d5i(x))\u03c8i(y)|\n(10)\n< \u03b5 3 + \u03b5 3 + \u03b5 3 (11)\n= \u03b5, (12)\nfor any x \u2208 X and y \u2208 Y . Hence, T is dense in C(X \u00d7 Y)."
        },
        {
            "heading": "C.2 LIPSCHITZ CONSTRAINT UNDER THE TEMPORAL DISTANCE METRIC",
            "text": "Theorem C.3. The following two conditions are equivalent:\n(a) \u2225\u03d5(u)\u2212 \u03d5(v)\u22252 \u2264 dtemp(u, v) for all u, v \u2208 S. (b) \u2225\u03d5(s)\u2212 \u03d5(s\u2032)\u22252 \u2264 1 for all (s, s\u2032) \u2208 Sadj.\nProof. We first show (a) implies (b). Assume (a) holds. Consider (s, s\u2032) \u2208 Sadj. If s \u0338= s\u2032, by (a), we have \u2225\u03d5(s) \u2212 \u03d5(s\u2032)\u22252 \u2264 dtemp(s, s\u2032) = 1. Otherwise, i.e., s = s\u2032, \u2225\u03d5(s) \u2212 \u03d5(s\u2032)\u22252 = 0 \u2264 1. Hence, (a) implies (b).\nNext, we show (b) implies (a). Assume (b) holds. Consider u, v \u2208 S. If dtemp(u, v) = \u221e (i.e., v is not reachable from u), (a) holds trivially. Otherwise, let k be dtemp(u, v). By definition, there exists (s0 = u, s1, . . . , sk\u22121, sk = v) such that (si, si+1) \u2208 Sadj for all 0 \u2264 i \u2264 k\u2212 1. Due to the triangle inequality and (b), we have \u2225\u03d5(u) \u2212 \u03d5(v)\u22252 \u2264 \u2211k\u22121 i=0 \u2225\u03d5(si) \u2212 \u03d5(si+1)\u22252 \u2264 k = dtemp(u, v). Hence, (b) implies (a)."
        },
        {
            "heading": "D A CONNECTION BETWEEN METRA AND PCA",
            "text": "In this section, we derive a theoretical connection between METRA and principal component analysis (PCA). Recall that the METRA objective can be written as follows:\nsup \u03c0,\u03d5 Ep(\u03c4,z) [ T\u22121\u2211 t=0 (\u03d5(st+1)\u2212 \u03d5(st))\u22a4z ] = Ep(\u03c4,z) [ \u03d5(sT ) \u22a4z ]\n(13)\ns.t. \u2225\u03d5(u)\u2212 \u03d5(v)\u22252 \u2264 dtemp(u, v), \u2200u, v \u2208 S, (14) where dtemp denotes the temporal distance between two states. To make a formal connection between METRA and PCA, we consider the following squared variant of the METRA objective in this section.\nsup \u03c0,\u03d5\nEp(\u03c4,z) [ (\u03d5(sT ) \u22a4z)2 ] s.t. \u2225\u03d5(u)\u2212 \u03d5(v)\u22252 \u2264 dtemp(u, v), \u2200u, v \u2208 S, (15)\nwhich is almost the same as Equation (13) but the objective is now squared. The reason we consider this variant is simply for mathematical convenience.\nNext, we introduce the notion of a temporally consistent embedding.\nDefinition D.1 (Temporally consistent embedding). We call that an MDP M admits a temporally consistent embedding if there exists \u03c8(s) : S \u2192 Rm such that\ndtemp(u, v) = \u2225\u03c8(u)\u2212 \u03c8(v)\u22252, \u2200u, v \u2208 S. (16)\nIntuitively, this states that the temporal distance metric can be embedded into a (potentially very high-dimensional) Euclidean space. We note that \u03c8 is different from \u03d5 in Equation (13), and Rm can be much higher-dimensional than Z . An example of an MDP that admits a temporally consistent embedding is the PointMass environment: if an agent in Rn can move in any direction up to a unit speed, \u03c8(x) = x satisfies dtemp(u, v) = \u2225u \u2212 v\u22252 for all u, v \u2208 Rn (with a slightly generalized notion of temporal distances in continuous time) and thus the MDP admits the temporally consistent embedding of \u03c8. A pixel-based PointMass environment is another example of such an MDP.\nNow, we formally derive a connection between squared METRA and PCA. For simplicity, we assume Z = Rd and p(z) = N (0, Id), where N (0, Id) denotes the d-dimensional isotropic Gaussian distribution. We also assume that M has a deterministic initial distribution and transition dynamics function, and every state is reachable from the initial state within T steps. We denote the set of n\u00d7n positive definite matrices as Sn++, the operator norm of a matrixA as \u2225A\u2225op, and them-dimensional unit \u21132 ball as Bm. Theorem D.2 (Linear squared METRA is PCA in the temporal embedding space). Let M be an MDP that admits a temporally consistent embedding \u03c8 : S \u2192 Rm. If \u03d5 : S \u2192 Z is a linear mapping from the embedding space, i.e., \u03d5(s) = W\u22a4\u03c8(s) with W \u2208 Rm\u00d7d, and the embedding space \u03a8 = {\u03c8(s) : s \u2208 S} forms an ellipse, i.e., \u2203A \u2208 Sm++ s.t. \u03a8 = {x \u2208 Rm : x\u22a4A\u22121x \u2264 1}, then W = [a1 a2 \u00b7 \u00b7 \u00b7 ad] maximizes the squared METRA objective in Equation (15), where a1, . . . , ad are the top-d eigenvectors of A.\nProof. Since M admits a temporally consistent embedding, we have \u2225\u03d5(u)\u2212 \u03d5(v)\u22252 \u2264 dtemp(u, v) \u2200u, v \u2208 S (17)\n\u21d0\u21d2 \u2225W\u22a4(\u03c8(u)\u2212 \u03c8(v))\u22252 \u2264 \u2225\u03c8(u)\u2212 \u03c8(v)\u22252 \u2200u, v \u2208 S (18) \u21d0\u21d2 \u2225W\u22a4(u\u2212 v)\u22252 \u2264 \u2225u\u2212 v\u22252 \u2200u, v \u2208 \u03a8 (19) \u21d0\u21d2 \u2225W\u2225op \u2264 1, (20)\nwhere we use the fact that \u03c8 is a surjection from S to \u03a8 and that A is positive definite. Now, we have\n= sup \u03c0,\u2225W\u2225op\u22641\nEp(\u03c4,z)[(\u03d5(sT )\u22a4z)2] (21)\n= sup \u03c0,\u2225W\u2225op\u22641\nEp(\u03c4,z)[(\u03c8(sT )\u22a4Wz)2] (22)\n= sup f :Rd\u2192\u03a8,\u2225W\u2225op\u22641\nEp(z)[(f(z)\u22a4Wz)2] (\u2235 Every state is reachable within T steps) (23)\n= sup g:Rd\u2192Bm,\u2225W\u2225op\u22641\nEp(z)[(g(z)\u22a4 \u221a AWz)2] (g(z) = \u221a A\u22121f(z)) (24)\n= sup \u2225W\u2225op\u22641 Ep(z)[ sup g:Rd\u2192Bm\n(g(z)\u22a4 \u221a AWz)2] (25)\n= sup \u2225W\u2225op\u22641 Ep(z)[ sup \u2225u\u22252\u22641\n(u\u22a4 \u221a AWz)2] (26)\n= sup \u2225W\u2225op\u22641\nEp(z)[\u2225 \u221a AWz\u222522] (\u2235 Dual norm) (27)\n= sup \u2225W\u2225op\u22641\nEp(z)[z\u22a4W\u22a4AWz] (28)\n= sup \u2225W\u2225op\u22641\nEp(z)[tr(zz\u22a4W\u22a4AW )] (29)\n= sup \u2225W\u2225op\u22641\ntr(Ep(z)[zz\u22a4]W\u22a4AW ) (30)\n= sup \u2225W\u2225op\u22641\ntr(WW\u22a4A). (31)\nSince WW\u22a4 is a positive semidefinite matrix with rank at most d and \u2225W\u2225op \u2264 1, there exists d eigenvalues 0 \u2264 \u03bb1, . . . , \u03bbd \u2264 1 and the corresponding orthonormal eigenvectors v1, . . . , vd such that WW\u22a4 = \u2211d k=1 \u03bbkvkv \u22a4 k . Hence, tr(WW \u22a4A) = \u2211d k=1 \u03bbkv \u22a4 k Avk, and to maximize this, we\nmust set \u03bb1 = \u00b7 \u00b7 \u00b7 = \u03bbd = 1 as A is positive definite. The remaining problem is to find d orthonormal vectors v1, . . . , vd that maximize \u2211d k=1 v \u22a4 k Avk. By the Ky Fan\u2019s maximum principle (Bhatia, 2013), its solution is given as the d eigenvectors corresponding to the d largest eigenvalues of A. Therefore, W = [a1 a2 \u00b7 \u00b7 \u00b7 ad], where a1, . . . , ad are the top-d principal components of A, maximizes the squared METRA objective in Equation (15).\nTheorem D.2 states that linear squared METRA is equivalent to PCA in the temporal embedding space. In practice, however, \u03d5 can be nonlinear, the shape of \u03a8 can be arbitrary, and the MDP may not admit any temporally consistent embeddings. Nonetheless, this theoretical connection hints at the intuition that the METRA objective encourages the agent to span the largest \u201ctemporal\u201d manifolds in the state space, given the limited capacity of Z ."
        },
        {
            "heading": "E CONNECTIONS BETWEEN WDM AND DIAYN, DADS, AND CIC",
            "text": "In this section, we describe connections between our WDM objectives (either IW(S;Z) or IW(ST ;Z)) and previous mutual information skill learning methods, DIAYN (Eysenbach et al., 2019a), DADS (Sharma et al., 2020), and CIC (Laskin et al., 2022). Recall that the IW(S;Z) objective (Equation (4)) maximizes\nT\u22121\u2211 t=0 ( Ep(\u03c4,z)[\u03d5L(st)\u22a4\u03c8L(z)]\u2212 Ep(\u03c4)[\u03d5L(st)]\u22a4Ep(z)[\u03c8L(z)] ) , (32)\nand the IW(ST ;Z) objective (Equation (6)) maximizes T\u22121\u2211 t=0 ( Ep(\u03c4,z)[(\u03d5L(st+1)\u2212 \u03d5L(st))\u22a4\u03c8L(z)]\u2212 Ep(\u03c4)[\u03d5L(st+1)\u2212 \u03d5L(st)]\u22a4Ep(z)[\u03c8L(z)] ) , (33) where we use the notations \u03d5L and \u03c8L to denote that they are Lipschitz constrained. By simplifying Equation (32) or Equation (33) in three different ways, we will show that we can obtain \u201cWasserstein counterparts\u201d of DIAYN, DADS, and CIC. For simplicity, we assume p(z) = N (0, I), where N (0, I) denotes the standard Gaussian distribution."
        },
        {
            "heading": "E.1 DIAYN",
            "text": "If we set \u03c8L(z) = z in Equation (32), we get rt = \u03d5L(st)\n\u22a4z. (34) This is analogous to DIAYN (Eysenbach et al., 2019a), which maximizes\nI(S;Z) = \u2212H(Z|S) +H(Z) (35)\n\u2273 Ep(\u03c4,z) [ T\u22121\u2211 t=0 log q(z|st) ]\n(36)\n\u2243 Ep(\u03c4,z) [ T\u22121\u2211 t=0 \u2212\u2225z \u2212 \u03d5(st)\u222522 ] , (37)\nrDIAYNt = \u2212\u2225\u03d5(st)\u2212 z\u222522, (38) where \u2018\u2273\u2019 and \u2018\u2243\u2019 respectively denote \u2018>\u2019 and \u2018=\u2019 up to constant scaling or shifting, and we assume that the variational distribution q(z|s) is modeled as N (\u03d5(s), I). By comparing Equation (34) and Equation (38), we can see that Equation (34) can be viewed as a Lipschitz, inner-product variant of DIAYN. This analogy will become clearer later."
        },
        {
            "heading": "E.2 DADS",
            "text": "If we set \u03d5L(s) = s in Equation (33), we get rt = (st+1 \u2212 st)\u22a4\u03c8L(z)\u2212 (st+1 \u2212 st)\u22a4Ep(z)[\u03c8L(z)] (39)\n\u2248 (st+1 \u2212 st)\u22a4\u03c8L(z)\u2212 1\nL L\u2211 i=1 (st+1 \u2212 st)\u22a4\u03c8L(zi), (40)\nwhere we use L independent samples from N (0, I), z1, z2, . . . , zL, to approximate the expectation. This is analogous to DADS (Sharma et al., 2020), which maximizes:\nI(S\u2032;Z|S) = \u2212H(S\u2032|S,Z) +H(S\u2032|S) (41)\n\u2273 Ep(\u03c4,z) [ T\u22121\u2211 t=0 log q(st+1|st, z)\u2212 log p(st+1|st) ]\n(42)\n\u2248 Ep(\u03c4,z) [ T\u22121\u2211 t=0 ( log q(st+1|st, z)\u2212 1 L L\u2211 i=1 log q(st+1|st, zi) )] , (43)\nrDADSt = \u2212\u2225(st+1 \u2212 st)\u2212 \u03c8(st, z)\u222522 + 1\nL L\u2211 i=1 \u2225(st+1 \u2212 st)\u2212 \u03c8(st, z)\u222522, (44)\nwhere we assume that the variational distribution q(s\u2032|s, z) is modeled as q(s\u2032 \u2212 s|s, z) = N (\u03c8(s, z), I), as in the original implementation (Sharma et al., 2020). We also use the same samplebased approximation as Equation (40). Note that the same analogy also holds between Equation (40) and Equation (44) (i.e., Equation (40) is a Lipschitz, inner-product variant of DADS)."
        },
        {
            "heading": "E.3 CIC",
            "text": "If we do not simplify \u03d5L or \u03c8L in Equation (32), we get\nrt = \u03d5L(st) \u22a4\u03c8L(z)\u2212 \u03d5L(st)\u22a4Ep(z)[\u03c8L(z)] (45)\n\u2248 \u03d5L(st)\u22a4\u03c8L(z)\u2212 1\nL L\u2211 i=1 \u03d5L(st) \u22a4\u03c8L(zi), (46)\nwhere we use the same sample-based approximation as Equation (40). By Jensen\u2019s inequality, Equation (46) can be lower-bounded by\n\u03d5L(st) \u22a4\u03c8L(z)\u2212 log\n1\nL L\u2211 i=1 exp ( \u03d5L(st) \u22a4\u03c8L(zi) ) , (47)\nas in WPC (Ozair et al., 2019). This is analogous to CIC (Laskin et al., 2022), which estimates the MI via noise contrastive estimation (Gutmann & Hyva\u0308rinen, 2010; van den Oord et al., 2018; Poole et al., 2019):\nI(S;Z) \u2273 Ep(\u03c4,z) [ T\u22121\u2211 t=0 ( \u03d5(st) \u22a4\u03c8(z)\u2212 log 1 L L\u2211 i=1 exp ( \u03d5(st) \u22a4\u03c8(zi) ))] , (48)\nrCICt = \u03d5(st) \u22a4\u03c8(z)\u2212 log 1\nL L\u2211 i=1 exp ( \u03d5(st) \u22a4\u03c8(zi) ) . (49)\nNote that Equation (47) can be viewed as a Lipschitz variant of CIC (Equation (49)).\nIn this work, we use the \u03c8L(z) = z simplification with Equation (33) (i.e., Equation (7)), as we found this variant to work well while being simple, but we believe exploring these other variants is an interesting future research direction. In particular, given that Equation (47) resembles the standard contrastive learning formulation, combining this (more general) objective with existing contrastive learning techniques may lead to another highly scalable unsupervised RL method, which we leave for future work."
        },
        {
            "heading": "F ADDITIONAL RESULTS",
            "text": ""
        },
        {
            "heading": "F.1 FULL QUALITATIVE RESULTS",
            "text": "Figure 11 shows the complete qualitative results of behaviors discovered by METRA on state-based Ant and HalfCheetah, and pixel-based Quadruped and Humanoid (8 seeds for each environment). We use 2-D skills for Ant and Humanoid, 4-D skills for Quadruped, and 16 discrete skills for HalfCheetah. The full qualitative results suggest that METRA discovers diverse locomotion behaviors regardless of the random seed."
        },
        {
            "heading": "F.2 LATENT SPACE VISUALIZATION",
            "text": "METRA simultaneously learns both the skill policy \u03c0(a|s, z) and the representation function \u03d5(s), to find the most \u201ctemporally spread-out\u201d manifold in the state space. We train METRA on statebased Ant and pixel-based Humanoid with 2-D continuous latent spaces Z , and visualize the learned latent space by plotting \u03d5(s) trajectories in Figure 12. Since the x-y plane corresponds to the most temporally \u201cimportant\u201d manifold in both environments, METRA learns to capture the x-y coordinates in two-dimensional \u03d5, regardless of the input representations (note that Humanoid is pixelbased). We also note that, with a higher-dimensional latent space (especially when Z is discrete), METRA not only learns locomotion skills but also captures more diverse, non-linear behaviors, as shown in the Cheetah and Kitchen videos on our project page."
        },
        {
            "heading": "F.3 ABLATION STUDY OF LATENT SPACE SIZES",
            "text": "To demonstrate how the size of the latent space Z affects skill learning, we train METRA with 1- D, 2-D, and 4-D continuous skills and 2, 4, 8, 16, and 24 discrete skills on Ant and HalfCheetah. Figure 13 compares skills learned with different latent space sizes, which suggests that the diversity of skill generally increases as the capacity of Z grows."
        },
        {
            "heading": "F.4 ADDITIONAL BASELINES",
            "text": "In the main paper, we compare METRA with 11 previous unsupervised exploration and unsupervised skill discovery methods. In this section, we additionally compare METRA with DGPO (Chen et al., 2024), a method that aims to find diverse behaviors that maximize a task reward (Kumar et al., 2020; Zhou et al., 2022; Zahavy et al., 2023a;b; Chen et al., 2024). Since we consider a controlled Markov process without external rewards, we use only the intrinsic reward part of DGPO for\ncomparison:\nrDGPOt = min z\u2032\u2208Z,z\u2032 \u0338=z\nlog q(z|st+1)\nq(z|st+1) + q(z\u2032|st+1) , (50)\nwhere q is a skill discriminator (Eysenbach et al., 2019a) and DGPO assumes that Z is a discrete space. Intuitively, this objective encourages each behavior to be maximally different from the most similar other behavior.\nTable 1 presents the comparison results on HalfCheetah and Ant, where we train DIAYN, DGPO, and METRA with 16 discrete skills for 10000 epochs (16M steps). Even though DGPO maximizes \u201cworst-case\u201d diversity (Equation (50)), it still maximizes a metric-agnostic KL divergence between different skills (Chen et al., 2024), which leads to limited state coverage, as in DIAYN. In contrast, METRA maximizes a metric-aware Wasserstein distance and thus shows significantly better state coverage."
        },
        {
            "heading": "G EXPERIMENTAL DETAILS",
            "text": "We implement METRA on top of the publicly available LSD codebase (Park et al., 2022). Our implementation is available at https://github.com/seohongpark/METRA. For unsupervised skill discovery methods, we implement LSD (Park et al., 2022), CIC (Laskin et al., 2022), DIAYN (Eysenbach et al., 2019a), and DADS (Sharma et al., 2020) on the same codebase as METRA. For six exploration methods, ICM (Pathak et al., 2017), LBS (Mazzaglia et al., 2022), RND (Burda et al., 2019), APT (Liu & Abbeel, 2021b), APS (Liu & Abbeel, 2021a), and Plan2Explore (Sekar et al., 2020) (or Disagremeent (Pathak et al., 2019)), we use the original implementations by Laskin et al. (2021) for state-based environments and the Dreamer (Hafner et al., 2020) variants by Rajeswar et al. (2023) for pixel-based environments. For LEXA (Mendonca et al., 2021) in Section 5.3, we use the original implementation by Mendonca et al. (2021). We run our experiments on an internal cluster consisting of A5000 GPUs. Each run in Section 5.3 takes no more than 24 hours."
        },
        {
            "heading": "G.1 ENVIRONMENTS",
            "text": "Benchmark environments. For state-based environments, we use the same MuJoCo HalfCheetah and Ant environments (Todorov et al., 2012; Brockman et al., 2016) as previous work (Sharma et al., 2020; Park et al., 2022; 2023b). HalfCheetah has an 18-dimensional state space and Ant has a 29-dimensional state space. For pixel-based environments, we use pixel-based Quadruped and Humanoid from the DeepMind Control Suite (Tassa et al., 2018) and a pixel-based version of\nKitchen by Gupta et al. (2019); Mendonca et al. (2021). In DMC locomotion environments, we use gradient-colored floors to allow the agent to infer its location from pixels, similarly to Hafner et al. (2022); Park et al. (2023a). In Kitchen, we use the same camera setting as LEXA (Mendonca et al., 2021). Pixel-based environments have an observation space of 64 \u00d7 64 \u00d7 3, and we do not use any proprioceptive state information. The episode length is 200 for Ant and HalfCheetah, 400 for Quadruped and Humanoid, and 50 for Kitchen. We use an action repeat of 2 for pixel-based Quadruped and Humanoid, following Mendonca et al. (2021). In our experiments, we do not use any prior knowledge or supervision, such as the x-y prior (Eysenbach et al., 2019a; Sharma et al., 2020), or early termination (Park et al., 2022).\nMetrics. For the state coverage metric in locomotion environments, we count the number of 1\u00d7 1- sized x-y bins (Ant, Quadruped, and Humanoid) or 1-sized x bins (HalfCheetah) that are occupied by any of the target trajectories. In Kitchen, we count the number of pre-defined tasks achieved by any of the target trajectories, where we use the same six pre-defined tasks as Mendonca et al. (2021): Kettle (K), Microwave (M), Light Switch (LS), Hinge Cabinet (HC), Slide Cabinet (SC), and Bottom Burner (BB). Each of the three types of coverage metrics, policy state coverage (Figures 5 and 7), queue state coverage (Figure 7), and total state coverage (Figure 7), uses different target trajectories. Policy state coverage, which is mainly for skill discovery methods, is computed by 48 deterministic trajectories with 48 randomly sample skills at the current epoch. Queue state coverage is computed by the most recent 100000 training trajectories up to the current epoch. Total state coverage is computed by the entire training trajectories up to the current epoch.\nDownstream tasks. For quantitative comparison of skill discovery methods (Figure 6), we use five downstream tasks, AntMultiGoals, HalfCheetahGoal, HalfCheetahHurdle, QuadrupedGoal, and HumanoidGoal, mostly following the prior work (Park et al., 2022). In HalfCheetahGoal, QuadrupedGoal, and HumanoidGoal, the task is to reach a target goal (within a radius of 3) randomly sampled from [\u2212100, 100], [\u22127.5, 7.5]2, and [\u22125, 5]2, respectively. The agent receives a reward of 10 when it reaches the goal. In AntMultiGoals, the task is to reach four target goals (within a radius of 3), where each goal is randomly sampled from [sx\u2212 7.5, sx+7.5]\u00d7 [sy\u2212 7.5, sy+7.5], where (sx, sy) is the agent\u2019s current x-y position. The agent receives a reward of 2.5 whenever it reaches the goal. A new goal is sampled when the agent either reaches the previous goal or fails to reach it within 50 steps. In HalfCheetahHurdle (Qureshi et al., 2020), the task is to jump over multiple hurdles. The agent receives a reward of 1 whenever it jumps over a hurdle. The episode length is 200 for state-based environments and 400 for pixel-based environments.\nFor quantitative comparison with LEXA (Figure 8), we use five goal-conditioned tasks. In locomotion environments, goals are randomly sampled from [\u2212100, 100] (HalfCheetah), [\u221250, 50]2 (Ant), [\u221215, 15]2 (Quadruped), or [\u221210, 10]2 (Humanoid). We provide the full state as a goal g, whose dimensionality is 18 for HalfCheetah, 29 for Ant, and 64 \u00d7 64 \u00d7 3 for pixel-based Quadruped and Humanoid. In Kitchen, we use the same six (single-task) goal images and tasks as Mendonca et al. (2021). We measure the distance between the goal and the final state in locomotion environments and the number of successful tasks in Kitchen.\nG.2 IMPLEMENTATION DETAILS\nUnsupervised skill discovery methods. For skill discovery methods, we use 2-D continuous skills for Ant and Humanoid, 4-D continuous skills for Quadruped, 16 discrete skills for HalfCheetah, and 24 discrete skills for Kitchen, where continuous skills are sampled from the standard Gaussian distribution, and discrete skills are uniformly sampled from the set of zero-centered one-hot vectors (Park et al., 2022). METRA and LSD use normalized vectors (i.e., z/\u2225z\u22252) for continuous skills, as their objectives are invariant to the magnitude of z. For CIC, we use 64-D continuous skills for all environments, following the original suggestion (Laskin et al., 2022), and we found that using 64-D skills for CIC leads to better state coverage than using 2-D or 4-D skills. We present the full list of hyperparameters used for skill discovery methods in Table 2.\nUnsupervised exploration methods. For unsupervised exploration methods and LEXA, we use the original implementations and hyperparameters (Laskin et al., 2021; Mendonca et al., 2021; Rajeswar et al., 2023). For LEXA\u2019s goal-conditioned policy (achiever), we test both the temporal distance and cosine distance variants and use the former as it leads to better performance.\nHigh-level controllers for downstream tasks. In Figure 6, we evaluate learned skills on downstream tasks by training a high-level controller \u03c0h(z|s, stask) that selects a skill every K = 25 (Ant and HalfCheetah) or K = 50 (Quadruped and Humanoid) environment steps, where stask denotes the task-specific information: the goal position (\u2018-Goal\u2019 or \u2018-MultiGoals\u2019 tasks) or the next hurdle position and distance (HalfCheetahHurdle). At every K steps, the high-level policy selects a skill z, and then the pre-trained low-level skill policy \u03c0(a|s, z) executes the same z for K steps. We train high-level controllers with PPO (Schulman et al., 2017) for discrete skills and SAC (Haarnoja et al., 2018a) for continuous skills. For SAC, we use the same hyperparameters as unsupervised skill discovery methods (Table 2), and we present the full list of PPO-specific hyperparameters in Table 3.\nZero-shot goal-conditioned RL. In Figure 8, we evaluate the zero-shot performances of METRA, LSD, DIAYN, and LEXA on goal-conditioned downstream tasks. METRA and LSD use the procedure described in Section 4.2 to select skills. We re-compute z every step for locomotion environments, but in Kitchen, we use the same z selected at the first step throughout the episode, as we find that this leads to better performance. DIAYN chooses z based on the output of the skill discriminator at the goal state (i.e., q(z|g), where q denotes the skill discriminator of DIAYN). LEXA uses the goal-conditioned policy (achiever), \u03c0(a|s, g)."
        }
    ],
    "title": "METRA: SCALABLE UNSUPERVISED RL WITH METRIC-AWARE ABSTRACTION",
    "year": 2024
}