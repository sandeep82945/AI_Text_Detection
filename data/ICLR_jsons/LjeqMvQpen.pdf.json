{
    "abstractText": "Fusion is a technique for merging multiple independently-trained neural networks in order to combine their capabilities. Past attempts have been restricted to the case of fully-connected, convolutional, and residual networks. In this paper, we present a systematic approach for fusing two or more transformer-based networks exploiting Optimal Transport to (soft-)align the various architectural components. We flesh out an abstraction for layer alignment, that can generalize to arbitrary architectures \u2013 in principle \u2013 and we apply this to the key ingredients of Transformers such as multi-head self-attention, layer-normalization, and residual connections, and we discuss how to handle them via various ablation studies. Furthermore, our method allows the fusion of models of different sizes (heterogeneous fusion), providing a new and efficient way for compression of Transformers. The proposed approach is evaluated on both image classification tasks via Vision Transformer and natural language modeling tasks using BERT. Our approach consistently outperforms vanilla fusion, and, after a surprisingly short finetuning, also outperforms the individual converged parent models. In our analysis, we uncover intriguing insights about the significant role of soft alignment in the case of Transformers. Our results showcase the potential of fusing multiple Transformers, thus compounding their expertise, in the budding paradigm of model fusion and recombination.",
    "authors": [],
    "id": "SP:ac27efd74ef6563691bb639f4b913767f029ce46",
    "references": [
        {
            "authors": [
                "Samuel K Ainsworth",
                "Jonathan Hayase",
                "Siddhartha Srinivasa"
            ],
            "title": "Git re-basin: Merging models modulo permutation symmetries",
            "venue": "arXiv preprint arXiv:2209.04836,",
            "year": 2022
        },
        {
            "authors": [
                "Aditya Kumar Akash",
                "Sixu Li",
                "Nicol\u00e1s Garc\u0131\u0301a Trillos"
            ],
            "title": "Wasserstein barycenter-based model fusion and linear mode connectivity of neural networks",
            "venue": "arXiv preprint arXiv:2210.06671,",
            "year": 2022
        },
        {
            "authors": [
                "Gregor Bachmann",
                "Sotiris Anagnostidis",
                "Thomas Hofmann"
            ],
            "title": "Scaling mlps: A tale of inductive bias",
            "venue": "arXiv preprint arXiv:2306.13575,",
            "year": 2023
        },
        {
            "authors": [
                "Rishi Bommasani",
                "Drew A Hudson",
                "Ehsan Adeli",
                "Russ Altman",
                "Simran Arora",
                "Sydney von Arx",
                "Michael S Bernstein",
                "Jeannette Bohg",
                "Antoine Bosselut",
                "Emma Brunskill"
            ],
            "title": "On the opportunities and risks of foundation models",
            "venue": "arXiv preprint arXiv:2108.07258,",
            "year": 2021
        },
        {
            "authors": [
                "Marco Cuturi"
            ],
            "title": "Sinkhorn distances: Lightspeed computation of optimal transport",
            "venue": "Advances in neural information processing systems,",
            "year": 2013
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova"
            ],
            "title": "BERT: pre-training of deep bidirectional transformers for language understanding",
            "year": 2018
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "Lucas Beyer",
                "Alexander Kolesnikov",
                "Dirk Weissenborn",
                "Xiaohua Zhai",
                "Thomas Unterthiner",
                "Mostafa Dehghani",
                "Matthias Minderer",
                "Georg Heigold",
                "Sylvain Gelly",
                "Jakob Uszkoreit",
                "Neil Houlsby"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
            "venue": "URL https://arxiv.org/abs/2010.11929",
            "year": 2010
        },
        {
            "authors": [
                "Timur Garipov",
                "Pavel Izmailov",
                "Dmitrii Podoprikhin",
                "Dmitry P Vetrov",
                "Andrew G Wilson"
            ],
            "title": "Loss surfaces, mode connectivity, and fast ensembling of dnns",
            "venue": "Advances in neural information processing systems,",
            "year": 2018
        },
        {
            "authors": [
                "Stuart Geman",
                "Elie Bienenstock",
                "Ren\u00e9 Doursat"
            ],
            "title": "Neural networks and the bias/variance dilemma",
            "venue": "Neural computation,",
            "year": 1992
        },
        {
            "authors": [
                "Charles Godfrey",
                "Davis Brown",
                "Tegan Emerson",
                "Henry Kvinge"
            ],
            "title": "On the symmetries of deep learning models and their internal representations",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Jordan Hoffmann",
                "Sebastian Borgeaud",
                "Arthur Mensch",
                "Elena Buchatskaya",
                "Trevor Cai",
                "Eliza Rutherford",
                "Diego de Las Casas",
                "Lisa Anne Hendricks",
                "Johannes Welbl",
                "Aidan Clark"
            ],
            "title": "Training compute-optimal large language models",
            "venue": "arXiv preprint arXiv:2203.15556,",
            "year": 2022
        },
        {
            "authors": [
                "Alexia Jolicoeur-Martineau",
                "Emy Gervais",
                "Kilian Fatras",
                "Yan Zhang",
                "Simon Lacoste-Julien"
            ],
            "title": "Population parameter averaging (papa)",
            "venue": "arXiv preprint arXiv:2304.03094,",
            "year": 2023
        },
        {
            "authors": [
                "Jared Kaplan",
                "Sam McCandlish",
                "Tom Henighan",
                "Tom B Brown",
                "Benjamin Chess",
                "Rewon Child",
                "Scott Gray",
                "Alec Radford",
                "Jeffrey Wu",
                "Dario Amodei"
            ],
            "title": "Scaling laws for neural language models",
            "year": 2001
        },
        {
            "authors": [
                "Philip A Knight"
            ],
            "title": "The sinkhorn\u2013knopp algorithm: convergence and applications",
            "venue": "SIAM Journal on Matrix Analysis and Applications,",
            "year": 2008
        },
        {
            "authors": [
                "Xiaobin Li",
                "Lianlei Shan",
                "Weiqiang Wang"
            ],
            "title": "Fusing multitask models by recursive least squares",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2021
        },
        {
            "authors": [
                "Tianyang Lin",
                "Yuxin Wang",
                "Xiangyang Liu",
                "Xipeng Qiu"
            ],
            "title": "A survey of transformers - sciencedirect",
            "venue": "https://www.sciencedirect.com/science/article/pii/ S2666651022000146,",
            "year": 2022
        },
        {
            "authors": [
                "Chang Liu",
                "Chenfei Lou",
                "Runzhong Wang",
                "Alan Yuhan Xi",
                "Li Shen",
                "Junchi Yan"
            ],
            "title": "Deep neural network fusion via graph matching with applications to model ensemble and federated learning",
            "venue": "Proceedings of the 39th International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Michael S Matena",
                "Colin A Raffel"
            ],
            "title": "Merging models with fisher-weighted averaging",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Ibomoiye Domor Mienye",
                "Yanxia Sun"
            ],
            "title": "A survey of ensemble learning: Concepts, algorithms, applications, and prospects",
            "venue": "IEEE Access,",
            "year": 2022
        },
        {
            "authors": [
                "Dang Nguyen",
                "Khai Nguyen",
                "Dinh Phung",
                "Hung Bui",
                "Nhat Ho"
            ],
            "title": "Model fusion of heterogeneous neural networks via cross-layer alignment",
            "venue": "arXiv preprint arXiv:2110.15538,",
            "year": 2021
        },
        {
            "authors": [
                "Dang Nguyen",
                "Trang Nguyen",
                "Khai Nguyen",
                "Dinh Phung",
                "Hung Bui",
                "Nhat Ho"
            ],
            "title": "On cross-layer alignment for model fusion of heterogeneous neural networks",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2023
        },
        {
            "authors": [
                "Yossi Rubner",
                "Carlo Tomasi",
                "Leonidas J Guibas"
            ],
            "title": "The earth mover\u2019s distance as a metric for image retrieval",
            "venue": "International journal of computer vision,",
            "year": 2000
        },
        {
            "authors": [
                "Sidak Pal Singh",
                "Martin Jaggi"
            ],
            "title": "Model fusion via optimal transport",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "George Stoica",
                "Daniel Bolya",
                "Jakob Bjorner",
                "Taylor Hearn",
                "Judy Hoffman"
            ],
            "title": "Zipit! merging models from different tasks without training, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "C\u00e9dric Villani"
            ],
            "title": "Optimal transport: old and new, volume 338",
            "year": 2009
        },
        {
            "authors": [
                "Alex Wang",
                "Amanpreet Singh",
                "Julian Michael",
                "Felix Hill",
                "Omer Levy",
                "Samuel R. Bowman"
            ],
            "title": "GLUE: A multi-task benchmark and analysis platform for natural language understanding",
            "venue": "CoRR, abs/1804.07461,",
            "year": 2018
        },
        {
            "authors": [
                "Hongyi Wang",
                "Mikhail Yurochkin",
                "Yuekai Sun",
                "Dimitris Papailiopoulos",
                "Yasaman Khazaeni"
            ],
            "title": "Federated learning with matched averaging",
            "venue": "arXiv preprint arXiv:2002.06440,",
            "year": 2020
        },
        {
            "authors": [
                "Mitchell Wortsman",
                "Gabriel Ilharco",
                "Samir Ya Gadre",
                "Rebecca Roelofs",
                "Raphael Gontijo-Lopes",
                "Ari S Morcos",
                "Hongseok Namkoong",
                "Ali Farhadi",
                "Yair Carmon",
                "Simon Kornblith"
            ],
            "title": "Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Xiaohua Zhai",
                "Alexander Kolesnikov",
                "Neil Houlsby",
                "Lucas Beyer"
            ],
            "title": "Scaling vision transformers",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Da-Wei Zhou",
                "Han-Jia Ye",
                "De-Chuan Zhan"
            ],
            "title": "Co-transport for class-incremental learning",
            "venue": "In Proceedings of the 29th ACM International Conference on Multimedia, MM",
            "year": 2021
        },
        {
            "authors": [
                "Rubner"
            ],
            "title": "Besides allowing for a soft assignment, it also allows for an efficient solution via the Sinkhorn-Knapp algorithm (Knight, 2008) that results in a speed-up by an order of magnitude in the dimension d1 (or d2) and can be parallelized on GPUs",
            "venue": "In contrast,",
            "year": 2000
        },
        {
            "authors": [
                "OTFUSION OTFusion (Singh",
                "Jaggi"
            ],
            "title": "2020) first aligns several models: B,C",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Transformers, as introduced by Vaswani et al. (Vaswani et al., 2017), have profoundly impacted machine learning, establishing a prevailing neural network architecture across various domains. Transformers consistently excel in different fields, including natural language processing (Lin et al., 2022), time series forecasting (Wen et al., 2022), and computer vision (Dosovitskiy et al., 2020). Their success can be attributed to their scaling properties (Kaplan et al., 2020) and efficient utilization of contemporary hardware architectures designed for extensive parallel computing. The unification of a single architecture across tasks facilitates immediate, far-reaching applicability of any analysis that handles general properties of the Transformer architecture.\nAs large Transformer foundation models (Bommasani et al., 2021) continue to grow in size and complexity, the challenges associated with training, i.e., exponential increase in parameters, and compute for a fixed incremental improvement in performance (Hoffmann et al., 2022; Zhai et al., 2022; Bachmann et al., 2023), become increasingly more perilous. Consequently, achieving state-ofthe-art results is often confined to researchers with access to ample GPU resources. To address these issues and strive for more efficient and sustainable performance improvements, we embark on the following more compelling and alternative inquiry:\nCan we combine the capabilities of pre-trained Transformer models? Merging multiple Transformer models into a single entity while preserving their unique capabilities can yield several advantages; (a) Enhanced performance by harnessing the collective capabilities of individual models. (b) Reduced inference complexity, as querying a single model replaces the need to query n models in an ensemble, reducing computational (FLOPs) and storage requirements by a factor of n. (c) The necessity to train from scratch can be readily eliminated, leveraging existing public models, already available, and numerous in quantity 1.\nA straightforward way of fusing, i.e., merging, models of the same architecture, is to average their weight matrices one-to-one, referred to as \u2018Vanilla Fusion\u2019 (VF). However, this method overlooks\n1On huggingface there are more than 339,000 models available as of the 22nd of September 2023.\npotential misalignments between the parameter matrices, arising due to neurons at the same positions, in different models, encoding different information (Godfrey et al., 2022). Instead, we propose to use Optimal Transport fusion (OTFusion) (Singh & Jaggi, 2020), which at its core, aligns the weight or parameter matrices before fusing them.\nThus, by virtue of such an alignment, OTFusion ensures that the fused model effectively integrates the knowledge and capabilities of the individual models to be merged, rather than simply averaging the weight matrices without guaranteeing meaningful information preservation. Additionally, OTFusion accommodates the fusion of models with different widths, and in turn, different sizes, which is fundamentally not possible with VF. This is a crucial feature, as such heterogeneous models are available in plenty, to better unleash the potential of existing pre-trained models. Consequently, OTFusion has been shown to be an effective method for fusing fully connected (Singh & Jaggi, 2020), convolutional (Nguyen et al., 2021) and recurrent neural networks (Akash et al., 2022) on a variety of tasks, heavily outperforming VF.\nYet, despite its wide adoption (Nguyen et al., 2021; Liu et al., 2022; Ainsworth et al., 2022), the layerwise procedure proposed by OTFusion does not fit well with contemporary architectural design, that comprises of constant residual streams, normalization layers, and attention operations. It is not equipped in any way to align and fuse models with complex information streams and to fuse transformer-specific components. Hence, the primary aim of our work is to develop techniques that help bridge these gaps and successfully generalize fusion to Transformer-based architectures.\nOur contributions are: (a) We analyze each of the idiosyncratic architectural components in Transformers in thorough detail, with an ultimate aim to best fuse them across different models. Throughout our discussion, we exposit our approach based on the perspective of flow of the transportation maps2, that makes for intuitive visualizations and interpretation. (b) We uncover that, surprisingly, OTFusion based on a hard-alignment underperforms in this context, contrary to the case of fully-connected or convolutional architectures; and that, soft-alignment plays a key role in successful one-shot fusion. (c) We showcase the efficacy of our approach by extensive experimentation involving the fusion and finetuning of Vision Transformers (ViTs) across multiple datasets, including CIFAR10, CIFAR100, TINY IMAGENET and IMAGENET-1K, as well as BERT (Devlin et al., 2018) models for natural language tasks. Here, we consistently outperform the original converged models across tasks and datasets, by about \u223c 1.0%, while significantly reducing computational and storage costs by a factor of n.\nOverall, our research marks an important stride in advancing model fusion techniques, that help deliver enhanced performance and efficiency for modern Transformer based architectures."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Model combination and ensembling. The combination of multiple models has been a timeless idea in machine learning, from classical works on bagging and boosting (Breiman, 1996) to more contemporary approaches (Mienye & Sun, 2022; Garipov et al., 2018; Jolicoeur-Martineau et al., 2023). The key idea behind these works is to boost model performance, by capitalizing on the unique strengths of each model while mitigating their individual limitations. Or, more technically, one can think of model combination as a way of reducing the variance of the predictors (Geman et al., 1992). However, the main limitation is that such methods require the execution of each (parent) model for the final prediction, with a cost that scales linearly with the number of models.\nModel Fusion. Model fusion (Wang et al., 2020; Singh & Jaggi, 2020; Wortsman et al., 2022; Matena & Raffel, 2022; Ainsworth et al., 2022; Nguyen et al., 2023) has emerged as a particularly notable direction in recent years, gaining significant traction in the machine-learning community. This line of work focuses on building better model combination approaches that account for the network structure and its inherent symmetries. We elaborate on some of these works, which are more relevant to the focus of our paper, below.\nSingh & Jaggi (2020) proposes a novel approach based on the OT theory exploiting the Wasserstein distance, where the neuron association allows to fuse pre-existing models with the same depth in a\n2This should be reminiscent of the flow of tensors in the computation graph of neural networks, and thus allows one to see a general strategy that can be potentially be adapted for any architecture type.\none-shot fashion, thus without requiring retraining. OTFusion outperforms VF and was successfully used for model compression and fusion of CNNs, residual networks (ResNets), and multilayer perceptrons (MLPs). Since its publication, OTFusion has been extended in various way. Nguyen et al. (2021) address the same-depth requirement of OTFusion. Liu et al. (2022) generalized the work as a graph-matching task, and taking into account the second-order similarity of model weights instead of linear alignment. Recent efforts on the topic have shown theoretical insights on fusion, extensions of previous algorithms to new network topologies, in particular, Akash et al. (2022) adapted OTFusion for recurrent networks, such as RNNs and LSTMs. Further, Stoica et al. (2023) propose an algorithm, for convolutional and residual architecures, that aims at finding redundant features within the same model and across the different models to be fused, so as to keep only meaningful and unique features in the fused model.\nHowever, the fully layerwise interpretation of OTFusion (Singh & Jaggi, 2020) lacks generalization capabilities, and as such it is only applicable to simple architectures such as MLPs, CNNs, and instances of ResNet. It is not equipped in any way to align and fuse models with complex information streams and to fuse transformer-specific components such as multi-head attention layers, layernormalization, embeddings, or the sequential nature of the data.\nFusion with a focus on Transformers. Wortsman et al. (2022) consider fusing Transformer models that have a common backbone network that is pre-trained on the same dataset, but that are fine-tuned, say, with different hyperparameters. Owing to this the models remain sufficiently close in the parameter space, which precludes the need to align them, and lets them employ just vanilla fusion (one-to-one averaging of the parameters) while still obtaining a gain in performance.\nHowever, arguably, the more empowering capability is to fuse transformer networks that are potentially much more distant in their parameter spaces and are diverse in nature. For instance, this arises when the networks have different initializations, or see examples in different batch orderings, or when they have different sizes, and more. This specific problem is tackled in this work, which is, to the best of our knowledge, the first aiming at fusing transformer architectures by aligning their weights."
        },
        {
            "heading": "3 BACKGROUND",
            "text": "Optimal Transport (OT). OT (Villani et al., 2009) has gained prominence in machine learning for its ability to compare probability distributions effectively, with applications in generative modelling (Arjovsky et al., 2017), class incremental learning (Zhou et al., 2021) and model compression (Li et al., 2021). At its heart, OT aims to find a transport map (TM) T signifying how much of a discrete source distribution should be moved towards a discrete destination distribution to align the two. This alignment can be hard (T is a permutation matrix and the solution to the Earth-Mover\u2019s Distance, EMD, (Rubner et al., 2000) problem) or can be relaxed yielding a soft alignment (solved with the Sinkhorn-Knapp algorithm (Knight, 2008)). The softness of the alignment is controlled by a regularization parameter \u03bbsinkhorn, where lower values result in harder alignment. More details about OT can be found in the Appendix A.1.\nOTFusion. Singh & Jaggi (2020) applies this theory to align networks in a layerwise fashion, using either weights or activations as underlying distributions. After the alignment of one or more models to an anchor model, these are then averaged. Formally, for a layer \u2113 of the model, the transpose of the TM of the previous layer is pre-multiplied with the weight matrix of the current layer: W\u0302(\u2113,\u2113\u22121) \u2190 T(\u2113\u22121)\u22a4W(\u2113,\u2113\u22121). The current layer can then be aligned by post-multiplying with the TM of the current layer: W\u0303(\u2113,\u2113\u22121) \u2190 W\u0302(\u2113,\u2113\u22121)T(\u2113). Ainsworth et al. (2022) propose a highly similar approach which, in certain cases, effectively boils down to the same linear programming problem that uncovers (provably and practically) same alignments as OTFusion; thus we continue to base our approach on OTFusion henceforth."
        },
        {
            "heading": "4 METHODOLOGY AND IMPLEMENTATION",
            "text": "With a modular architecture like the transformer, it is intuitive to use a divide-and-conquer approach to develop a fusion algorithm. Therefore, we first divide the architecture into its simplest building block \u2014 fully connected layers \u2014 that can be fused by the prevalent OTFusion strategy. The question\nremains; how to effectively connect these building blocks, especially if heterogeneous? How to hierarchically reconstruct a fully fused transformer ensuring consistency of the single fused blocks?\nAs we provide solutions to such open questions, we will guide our discussion in this section with a transport flow perspective, which allows for an intuitive and effective concatenation of blocks of any sort, and that, therefore, in principle can be applied to every architecture. Henceforth, we will use the notation from Vaswani et al. (2017) for Transformers. We showcase our methodology in the non-masked self-attention case, but our method can generalize to the cross-attention or causal masked attention."
        },
        {
            "heading": "4.1 TRANSPORTATION MAP FLOW GRAPH",
            "text": "In the typical OTFusion application, the TM of the previous layer is simply passed to the next layer. However, in more complex architectures, the incoming TM of a layer can depend on multiple TMs. To formalize and visualize this flow of TMs, we present the Transportation Map Flow Graph.\nTo introduce the concept, we use the flow graph of a residual connection (Fig. 1) as an example. Rectangles represent the neural network layers; red nodes represent any non-learnable computations or permutations inside the network; edges represent the propagation of the TMs. Layers have exactly one incoming and one outgoing edge. Computation nodes always have multiple incoming edges and one outgoing edge, where the outgoing TM must depend on the incoming TMs. A major contribution of this work is to handle the various complex transportation map flows throughout the transformer architecture."
        },
        {
            "heading": "4.2 TRANSFORMER FUSION",
            "text": ""
        },
        {
            "heading": "4.2.1 RESIDUAL CONNECTIONS",
            "text": "In residual connections, the outputs of a current layer and a residual layer are summed up. The TMs coming from these two layers will be different,\ntherefore the ideal TM flow strategy has to be determined. We explored three heuristics to calculate a weighting vector \u03b3(\u2113), where each entry \u03b3(\u2113)i scales the corresponding rows of the TMs. After obtaining \u03b3(\u2113) we compute the weighted average as shown in Eq. 1. Find the results in Sec. 5.1.\nT (\u2113) out = T (\u2113) current diag(1\u2212 \u03b3(\u2113)) +T (\u2113) residual diag(\u03b3 (\u2113)) (1)\nAveraging For plain averaging, as proposed by Singh & Jaggi (2020), we set \u2200 i, \u03b3i = 0.5. This heuristic does not depend on activations and can therefore be used even in the case of weight-based alignment. However, it introduces the strict assumption that the residual and the current layer TM are of equal importance when aligning the subsequent layer. We therefore extend Singh & Jaggi (2020) with two novel residual policies.\nWeighted Scalar To alleviate the equal contribution constraint from the averaging method, we compute a weighting factor \u2200 i, \u03b3(\u2113)i = \u03b3 (\u2113) scalar (Eq. 2). We use the activations of the anchor model, over a batch of samples S, because only those carry information about the importance of the current and the residual branch in the anchor model to which we try to align the other models. f (\u2113)residual(x) are the activations coming from the residual branch while f (\u2113)current(x) are the activation from the current layer \u2113.\n\u03b3 (\u2113) scalar =\n\u2211 x\u2208S ||f\n(\u2113) residual(x)||1\u2211\nx\u2208S ||f (\u2113) current(x)||1 + \u2211 x\u2208S ||f (\u2113) residual(x)||1\n(2)\nWeighted Matrix As opposed to the Weighted Scalar method, here, we calculate a weight vector \u03b3(\u2113) where each entry \u03b3(\u2113)i weighs one strand of a residual connection.The computation of each \u03b3 (l) i is similar to Eq. 2 but here we do not compute the \u21131-Norm over the whole activation vectors instead we take the absolute value of the corresponding ith values of the activation vectors.\nWe note that Ainsworth et al. (2022) propose to propagate either the identity (Tout = I) or the residual transportation map itself (\u2200 i, \u03b3(l)i = 1). In the case of hard alignment, these methods perform worse than averaging."
        },
        {
            "heading": "4.2.2 MULTI-HEAD ATTENTION",
            "text": "The attention mechanism (Eq. 3) poses multiple challenges when it comes to TM flow(Fig. 2): what are the incoming TMs for WQ, WK and WV ? Which TM is propagated to WO? How to handle attention with multiple heads?\nSelf-Attention(x) = softmax( QKT \u221a dk )V, with {Q,K,V} = W{Q,K,V}x (3)\nThe first challenge is conveniently solved by the TM flow graph. We can simply use the TM from the previous layer for each WQ, WK and WV . This even holds true for multiple heads. The incoming TM of WO is more complex to obtain because it depends on the outgoing TMs of WQ, WK , and WV . However, if we constrain both TMs of WK and WQ to be equal permutation matrices (i.e., hard alignment with TQ = TK = TQK), we show that the permutation matrices cancel (see Eq. 4) leaving the softmax undisturbed. Therefore, we only propagate the outgoing TM of WV to WO.\nFor soft-alignment Eq. 4 no longer holds, in that case we investigated alleviating the constraint of equal TMs for WK and WQ. Removing this constraint slightly increased one-shot accuracy.\nQ\u0303 = QTQK and K\u0303 = KTQK and Q\u0303K\u0303\u22a4 = QTQKT\u22a4QKK \u22a4 = QK\u22a4 (4)\nFor multi-head attention fusion there is an additional layer of complexity because one must not only align the weights but also the heads. On top of that, there is no guarantee that a hard one-to-one alignment between heads exists. For that reason, we propose cross-head alignment. During crosshead alignment, WQi , W K i and W V i (where i is the head index) are con-\ncatenated across the output dimension to form three combined weight matrices (WQ, WK and WV ). OTFusion is then applied to each of the concatenated weight matrices. Finally, TV is propagated to WO. Find a visualization of our cross-head alignment method in App. B."
        },
        {
            "heading": "4.2.3 LAYER NORMALIZATION, EMBEDDINGS AND BIAS",
            "text": "The layer normalization is a learnable neural network parameter and consequently must be fused. It contains only two parameters (\u03b1 and \u03b2) per input and there are no interconnections between different inputs and outputs. Therefore, no TM has to be computed for this layer. The parameters are only aligned w.r.t. to the incoming TM. The incoming TM is then propagated to the subsequent layer.\nThe ViT embeddings fusion approach is most effectively conveyed by its TM flow graph, as depicted in Fig. 3. For the concatenation, we notice that the class token is only a small fraction of the full sequence, in other words, for the integrity of the sequence, it is far more important to propagate the TM of the patch embeddings than the one for the class\ntoken. After concatenation, the positional embeddings are added. We notice that the addition is the same operation as for residual connections, so we can use one of the three TM flow strategies from Sec. 4.2.1.\nThe bias is only connected to the output of a neural network layer, so we align it using the outgoing TM of the corresponding layer."
        },
        {
            "heading": "4.3 ALIGNMENT STRATEGIES",
            "text": "Soft vs Hard Alignment OTFusion technically allows soft alignment for MLPs, CNNs and ResNets, but Singh & Jaggi (2020) discovered that for these simpler architectures, hard alignment outperforms soft alignment. However, we do not want to limit the search space for optimal alignment to only permutation matrices (possibly too constraining for a complex architecture such Transformers). We, therefore, broaden the perspective on alignment introduced by OTFusion using the Sinkhorn algorithm and tuning the softness of the TM by optimizing over the Sinkhorn regularizer, discovering that soft alignment outperforms hard alignment for Transformers.\nWeights vs. activations alignment The methodology introduced so far, combined and the novel perspective on the TM flow, allow us to apply OTFusion to the single fully connected layers without further adaptations in the case of weight-based approach, while the activation-based strategy needs a bit more thought. Transformers operate on sequences of tokens as opposed to simpler architectures that only operate one token at a time. In our activations-based algorithm, we treat every token of the sequence as a possible activation.\nSequence Filtering For ViTs it is obvious that not every token contributes equally to the final image classification. We hypothesize that activations-based alignment performs best if only the most important tokens of a sequence are considered. Therefore, we explored filtering out unimportant tokens. For datasets where images are centered, we propose window filtering, where only the n by n center patches are considered as activations for activation-based alignment (window n). Additionally, we explored the case where only the class token is used for activations-based alignment (only cls)."
        },
        {
            "heading": "5 EXPERIMENTS AND RESULTS",
            "text": "We evaluate the quality of our approach with two prominent transformer-based architectures: the ViT (Dosovitskiy et al., 2020) and BERT (Devlin et al., 2018). Our focus is to assess the performance and robustness of our proposed fusion techniques in both image and NLP domains. These models offer a direct comparison as they share the same encoder-only architecture.\nWe conducted our experiments on multiple well-known image classification datasets: CIFAR10, CIFAR100, TINY IMAGENET, and IMAGENET-1K. We used Hugging Face both for the implementation of the ViT and for retrieving the datasets. Besides the image classification tasks, we showcase our fusion strategy on the BERT model for an NLP task. We train from scratch multiple BERT models on the masked language modeling (MLM) task presented in Devlin et al. (2018) over a subset of the Wikipedia dataset, publicly available on the Hugging Face Hub.\nModel Training First, we train individual models from scratch on each dataset until convergence. We ensure model diversity by initializing each model with different seed values and different batch randomization. This results in unique models with similar performance but located in diverse parts of the landscape, and whose suitable fusion can improve performance. These diverse models, which are rather distant in the parameter space, need a non-trivial alignment strategy to be successfully fused, and therefore exhibit a dramatic drop in performance when fused with a naive approach such as VF. This approximates a plethora of other scenarios (e.g. models trained on different (sub)datasets). Details and training parameters of all models can be found in Appendix C.\nModel Fusion We assessed the proposed fusion strategies, and their combination thereof, on the CIFAR10 dataset (refer to the ablation studies in Section 5.1). We measure the performance through the so-called one-shot capability, namely the performance of the fused model, without any retraining, on the same task and metric of the parents. This capability is the first important proxy of the capacity of the fusion algorithm to align and then fuse the parent models. The optimal fusion strategy identified on the CIFAR10 task is then applied to the other tasks and architectures. For each task and alignment strategy (i.e. weights-based and activations-based) we optimize the Sinkhorn regularizer separately (see Fig. 11). The fusion step runs in just seconds on a general-purpose CPU.\nFinetuning Besides the one-shot performance, similarly to Singh & Jaggi (2020); Nguyen et al. (2021), we evaluate the effect of finetuning the fused model. The resulting performance is compared against the single parent models at convergence (and thus do not benefit from finetuning), their ensembling, and the VF model that also went through a round of finetuning. Both our fused model and the VF model are optimized separately over a common set of reasonable hyperparameters.\nNote In every result table or caption we encode the model dimension as (hidden-layer dimension/intermediate-layer dimension/number of encoders). Additionally, we report the relative computational burden (latency and FLOPs) below each result table entry.\n5.1 ONE-SHOT EXPERIMENTS Anchor OT\nVF\n7.02\n9.69\n13.38\n18.47\n25.50\n35.21\n48.60\n67.10\n92.63\nFigure 4: Two-dimensional slice of the accuracy landscapes of the anchor and oneshot OT and VF fused models.\nWe optimize the fusion strategy on CIFAR10, searching the configurations previously introduced. In contrast with the observations of Singh & Jaggi (2020) with non-transformer architectures, we observe that a softalignment (Sinkhorn) strategy consistently outperforms hard-alignment (EMD). The value of the Sinkhorn regularizer is chosen to maximize the one-shot accuracy (separately for activations- and weights-based alignment). The optimal strategy for handling the residual connections has proven to be the averaging policy. Activationsbased alignment with the 6x6 window filtering (window 6) approach performs best among other filtering strategies and weights-based alignment.\nIn Tab. 1, we present the one-shot performance for the best configuration of fusion with the weightsbased alignment and the activations-based alignment, both in the scenario with two models and with five models together. VF dramatically drops at random accuracy, while our fusion methodologies are able to preserve most of the capabilities of the individual models. In particular, we achieve the best accuracy with our soft, activations-based fusion.\nFig. 4 visualizes a two-dimensional slice of the accuracy landscapes of the anchor model and the two fused models, OT and VF. The visualization is based on the procedure outlined in (Garipov et al., 2018). The plot shows the OT model being in the same basin as the anchor one, while the VF model is separated by a barrier from such basin. This representation effectively underscores the superior performance of our algorithm in comparison to VF, emphasizing its ability to facilitate more dependable knowledge transfer.\nAblation Studies In this paragraph, we study the effect of the different OTFusion hyperparameter choices on the one-shot performance on the CIFAR10 dataset for two-models fusion. From Fig. 5a, it is evident that alleviating the constraint of hard alignment (EMD) allows for better performance retention. We attribute this observation to the flexibility of soft alignment which better accommodates the highly complex nature of the transformer, as multi-head self-attention. We observe a bell-shaped curve with a maximum for a non-zero regularization, thus demonstrating that the optimal alignment is neither hard nor merely soft. We can therefore optimize this parameter with an inexpensive sweep. Furthermore, as shown in Fig. 5b, the soft alignment for the activations-based fusion is much more stable than hard alignment (EMD) for different seeds of data, suggesting that hard alignment is much more impacted by the activations.\nFig. 5c shows the impact of various filters on the one-shot accuracy of the fusion, thereby strengthening our hypothesis that discarding irrelevant activations helps our fusion algorithm converge to\na better optimum. Finally, in Fig. 5d we present the impact of the various transport map policies for residuals, as presented in Section 4.2.1. Both weighted policies perform very similarly, slightly falling behind the best accuracy given by the averaged policy."
        },
        {
            "heading": "5.2 FINETUNED PERFORMANCE",
            "text": "As a last stage of the experimental setup, we finetune the fused models. The performance, as well as the retraining curves, offer an important insight into the quality of the fusion algorithm. While the one-shot performance can be heavily impacted by even only a single problematic layer, the capacity of the fused model to effectively, rapidly, and easily recover the performance of the parents allows for a deeper insight into the quality of the fusion across the whole architecture.\nWe show the finetuning results on the widely adopted datasets CIFAR100, and IMAGENET-1K (results on TINY IMAGENET in the Appendix). We first employ our fusion approach on the ViTs trained on the CIFAR100 dataset. As mentioned, we separately optimize the fused model on a common set of hyperparameters, in this case a learning rate (LR) in {10\u22123, 10\u22124, 10\u22125} and the number of epochs in {10, 20, 100, 200}. In Tab. 2 we observe that both our soft-alignment strategies (i.e. with weights- and activations-based alignment) are capable of outperforming the converged parents, with the gain that increases with the number of parent models. This suggests a successful knowledge transfer of the parents into the fused model. While the obtained accuracy lacks behind the ensembling performance, in our scenario there is no computational overhead, while the cost of the ensembling model grows linearly with the number of models.\nIn Tab. 3 we present further results on the challenging and widely-adopted IMAGENET-1K dataset. The results are consistent with those found in the CIFAR100 case, strengthening the general applicability of our methods, and its scalability to larger models and more challenging datasets. We also stress the fact that, especially with this difficult dataset, even after finetuning, VF fails to recover a comparable accuracy, converging to suboptimal performance.\nIn this work, we focused on the vision application of the Transformer architecture, but our method is agile to architectural changes, and we demonstrate its wide applicability to the BERT model. Although preliminary explorations of our fusion strategy on the BERT model show some differences with respect to the ViT case (more details on this are provided in Appendix D), the results are on par with those presented above. In particular, the fused and finetuned model, outperforms both parents and VF on the widely adopted GLUE benchmark (Wang et al., 2018). The results are presented in Tab. 17 of the App. E.\n(192/1536/7) (384/1536/7) (192/768/7)\n64.07 64.79 67.94 64.88 (+0.81) \u00d71 \u00d72.3 \u00d73.3 \u00d71\n(384/1536/7) (576/2304/7) (384/1536/7)\nWe want to highlight an insight into the finetuning process. In particular, we have observed that the best accuracy of our fused models is achieved extremely quickly, as much as two orders of magnitude fewer steps needed to train the parents from scratch, and, as a comparison, VF requires far higher computation to reach a comparable (but worse) performance. For further exemplification refer to Fig. 12 in Appendix E.2.\nOur methodology, as opposed to VF, works out of the box with models having different widths (heterogeneous fusion). We find a consistent absolute increase in test accuracy over the performance of the smaller anchor network, thus implying successful knowledge transfer (Tab. 4). These results showcase that our method is an effective and efficient alternative to knowledge distillation."
        },
        {
            "heading": "6 DISCUSSION",
            "text": "The fusion methodology for transformer models proposed in this paper is easily adapted to different architectural variants and is readily applicable to models of different widths. However, heterogeneous fusion of networks of different depths is a common limitation of the predominant fusion methods Ainsworth et al. (2022); Singh & Jaggi (2020) which are inherently based on a sequential layerwise alignment. Consequently, we too inherit a similar limitation when expanding fusion to the case of Transformers. Overall, this is undoubtedly a fascinating research challenge to extend Transformer fusion (or, broadly speaking, fusion at large) to heterogeneous depth settings which, however, is outside the scope of the current work.\nIn summary we showcased how distinct independently trained transformer networks can be combined through the lens of Optimal Transport. Utilizing a novel graph interpretation of the transportation map flow, we developed an algorithm for fusing multiple transformer networks that extends the existing fusion techniques and that specifically caters to the idiosyncrasies of the transformer architecture. We also uncovered an intriguing benefit of using soft alignment when fusing Transformers, which had been under-utilized in the past. Overall, we showed that our technique can retain most of the performance of the converged parent models in one-shot, and even outperforms them after finetuning, across multiple vision and NLP tasks proving the scalability and wide applicability of our methods thereby providing a highly efficient and promising alternative to ensembling. Finally, our algorithm successfully applies to the fusion of models of different sizes, too, efficiently transferring knowledge from larger to smaller Transformers, and thus offering an effective alternative to distillation."
        },
        {
            "heading": "A BACKGROUND ON OPTIMAL TRANSPORT AND OTFUSION",
            "text": ""
        },
        {
            "heading": "A.1 OPTIMAL TRANSPORT THEORY",
            "text": "At its core, Optimal transport (OT) provides a way to compare two (or more) probability distributions \u00b5 := (a,X) = \u2211n i=1 ai \u00b7 \u03b4(xi) and \u03bd := (b,Y) = \u2211m j=1 bj \u00b7 \u03b4(yj), where \u03b4(\u00b7) is the Dirac-delta. These distributions are typically supported in a high-dimensional space, i.e., xi \u2208 X = Rd1 , and yj \u2208 Y = Rd2 , \u2200 i, j, and also where, being distributions, \u2211n i=1 ai = \u2211m j=1 bj = 1. These given distributions, in our case, may correspond to neurons or weights in a particular layer of the two networks. OT aims to find a transport plan T (or map) that signifies how much of these weights of the source model, should be moved towards the destination model, while adhering to the geometry of the underlying \u2018ground\u2019 space, usually available in the form of a \u2018ground metric\u2019, e.g., CG(x,y) = \u2225x \u2212 y\u222522 in the Euclidean case. Mathematically, one can formulate OT through an equivalent linear program:\nOT(\u00b5, \u03bd;C) := min \u27e8T,C\u27e9F s.t., T1m = a, T\u22a41n = b and T \u2208 R(n\u00d7m)+ .\nwhere appropriate mass conservation and positivity constraints are met. Here, \u27e8\u00b7, \u00b7\u27e9F is the Frobenius inner product and 1n \u2208 Rn denotes a vector containing all ones of size n. While the above problem will find a solution at the vertex of the polytope, one can relax the search to smooth solutions by regularizing the entropy h of the transport plan (Cuturi, 2013), i.e., h(T) = \u2211 i,j \u2212Tij log(Tij)\nOT\u03bb(\u00b5, \u03bd;C) := min \u27e8T,C\u27e9F \u2212 \u03bbh(T) s.t., T1m = a, T\u22a41n = b and T \u2208 R(n\u00d7m)+ .\nBesides allowing for a soft assignment, it also allows for an efficient solution via the Sinkhorn-Knapp algorithm (Knight, 2008) that results in a speed-up by an order of magnitude in the dimension d1 (or d2) and can be parallelized on GPUs. In contrast, the unregularized problem, which is also commonly referred to as the Earth-Mover\u2019s Distance (EMD; Rubner et al. (2000)), scales cubically in the dimension."
        },
        {
            "heading": "A.2 OTFUSION",
            "text": "OTFusion (Singh & Jaggi, 2020) first aligns several models: B,C, . . . , to an anchor model A. Then, the aligned models are averaged. Alignment is implemented through transportation maps, obtained by calculating the minimal transport cost between activations or weights of the neurons that should be aligned, giving rise to two different approaches, namely activations- and weights-based respectively. The OTFusion process works in a sequential fashion; assuming models with a specific depth L, each of the models\u2019 layers, at layer \u2113, are aligned before moving to the next layer \u2113+1. First, the transpose of the transportation map of the previous layer is pre-multiplied with the weight matrix of the current layer: W\u0302(l,l-1)B \u2190 T(l-1) \u22a4 W(l,l-1)B . The current layer can then be aligned by post-multiplying with the transportation map of the current layer: W\u0303(l,l-1)B \u2190 W\u0302 (l,l-1) B T (l)."
        },
        {
            "heading": "B CROSS-HEAD ALIGNMENT VISUALISATION",
            "text": "Fig. 6 visualizes the cross-head alignment algorithm for a tiny multi-head self-attention block. The aligned weights can then be averaged with the corresponding weights of the anchor model to get the weights for the OTFused model."
        },
        {
            "heading": "C EXPERIMENTAL SETUP",
            "text": "C.1 VISION TRANSFORMER - CIFAR10, CIFAR100, Tiny ImageNet AND ImageNet-1k\nModel Details We use the ViT implementation available on Hugging Face3 and we train it from scratch, without using any pre-trained weights. The architectural details of the model can be seen in Table 5.\n3https://huggingface.co/docs/transformers/model_doc/vit\nImage Augmentation We applied two different image augmentation policies on the CIFAR 10/100 and Tiny ImageNet datasets to achieve satisfactory training performance. For the CIFAR datasets, the augmentations have been adapted from an open-source implementation4, while for Tiny ImageNet the Autoaugment5 class from Pytorch has been used.\nTraining Details Training details are reported in Table 6. Figures 7, 8, 9 show the training curves for the CIFAR10, CIFAR100, and Tiny ImageNet respectively.\nC.2 VISION TRANSFORMER - IMAGENET\nModel Details We use the SimpleViT class from vit-pytorch6 and we train it from scratch, without using any pre-trained weights. The architectural details of the model can be seen in Table 7.\n4https://github.com/DeepVoltaire/AutoAugment 5https://pytorch.org/vision/main/generated/torchvision.transforms."
        },
        {
            "heading": "AutoAugment.html",
            "text": "6https://github.com/lucidrains/vit-pytorch\nImage Augmentation We first applied RandomResizedCrop() and RandomHorizontalFlip() to the input image form Pytorch transforms sub-package 7. Then we applied the Autoaugment class from the same Pytorch sub-package. Images are then normalized with \u00b5 = [0.485, 0.456, 0.406] and \u03c3 = [0.229, 0.224, 0.225].\nTraining Details Training details are reported in Table 8."
        },
        {
            "heading": "C.3 PROFILING INFORMATION",
            "text": "In Tab. 9 we provide profiling information for our most used ViT configuration."
        },
        {
            "heading": "C.4 BERT",
            "text": "Model Details We use the BERT implementation available on Hugging Face8 together with the pre-trained bert-base-uncased tokenizer 9. Our BERT model has the architectural details presented in Tab. 10.\nTraining Details We train the BERT models, from scratch, over five different seeds. Training details are shown in Tab. 11.\nWe use a MLM task on a subset of the Wikipedia dataset, available on Hugging Face 10, with an MLM probability of 0.15.\nThe training curve of the loss, for one seed, is presented in Fig. 10. 7https://pytorch.org/vision/stable/transforms.html 8https://huggingface.co/docs/transformers/model_doc/bert 9https://huggingface.co/docs/transformers/main_classes/tokenizer\n10https://huggingface.co/datasets/wikipedia/viewer/20220301.simple"
        },
        {
            "heading": "D SINKHORN REGULARIZER ABLATIONS",
            "text": "The Sinkhorn algorithm, and in general the soft alignment paradigm, has been heavily underused in literature and therefore there is little information about its impact on OTFusion. As presented above, we uncover intriguing behaviors, that require reconsidering its use. In the following Sections, we extend our findings related to soft alignment, in particular with the role of the regularization parameter."
        },
        {
            "heading": "D.1 ABLATION ON RESNET",
            "text": "To compare the findings for the transformer architecture, we also investigate the effect of the Sinkhorn regularizer on the ResNet architecture (Fig. 11a). In agreement with the findings of Singh & Jaggi (2020), the best result is achieved with EMD, and a small regularizer is preferred as it approaches the hard alignment. This result is thus suggesting an opposite behavior when it comes to soft alignment since the transformer benefits from a soft alignment.\nD.2 ABLATIONS ON CIFAR100, Tiny ImageNet, BERT MLM TASK\nIn Fig. 11 we present the effect of the Sinkhorn regularizer on the other considered datasets, namely CIFAR100 (Fig. 11b) and Tiny ImageNet (Fig. 11c) for the ViT, and the MLM task on the Wikipedia subset, for BERT (Fig. 11d).\nThe outcomes for CIFAR100 and Tiny ImageNet are in line with the results of the CIFAR10 case, namely a non-zero regularizer achieves the optimal performance.\nAs hinted in Sec. 5.2, we have observed some differences in the regularization effect on the BERT model. This difference can be observed in Fig. 11d, where we plot the effect of the regularization parameter on the validation loss. We observe that, in contrast to the observations for the ViT, the loss curve shows no inverted bell curve, suggesting that there is no finite optimal regularizer, i.e. that a completely soft alignment is best suited for this model."
        },
        {
            "heading": "D.3 WHAT HAPPENS AT THE EXTREME EDGE OF SINKHORN REGULARIZATION?",
            "text": "As presented above, the softness of the alignment is impacted by the Sinkhorn regularizer. If the regularizer is close to zero, the algorithm converges to a permutation matrix (i.e. hard alignment); in contrast, if the regularizer is very large, the algorithm converges to a unit-matrix divided by the dimension of itself."
        },
        {
            "heading": "D.3.1 SINKHORN REGULARIZER TO ZERO",
            "text": "In general, we have observed that the smaller the regularizer becomes, the harder the alignment gets. However, for very small Sinkhorn regularizer values the algorithm breaks down. This is especially visible in Fig. 11b and 11c where for the smallest regularizer the one-shot accuracy falls below the one-shot accuracy of EMD. We found that normalizing the cost matrix and the activations/weights to calculate the cost matrix, pushes the breakdown closer to zero and thus improving stability."
        },
        {
            "heading": "D.3.2 SINKHORN REGULARIZER TO INFINITY",
            "text": "We conducted an experiment to show that even in the case of extreme regularization (i.e. completely soft alignment) information is transferred from model B to the anchor model. In this experiment, we fuse a randomly initialized model (10% accuracy on CIFAR10) with a model at convergence (92% accuracy on CIFAR10). The one-shot accuracy for this experiment is 10%. On the other hand, if we fuse two converged models, we get a one-shot accuracy of 47% for a completely soft alignment. This suggests that, even in the highly regularized case, our algorithm allows knowledge transfer."
        },
        {
            "heading": "E FURTHER RESULTS",
            "text": "In this section, we provide more results from our experiments. We report both one-shot and finetuned accuracies over the datasets of choice.\nE.1 One-shot\nTab. 12 and Tab. 13 report the one-shot accuracies for Tiny ImageNet and CIFAR100 datasets, respectively."
        },
        {
            "heading": "E.2 FINETUNING",
            "text": "After fusing the models, we finetune them. Finetuning parameters and results are reported in the subsections below."
        },
        {
            "heading": "E.2.1 FINETUNING DETAILS - VIT",
            "text": "As mentioned in Sec. 5, we finetune VF and our fused models separately on a common set of hyperparameters. In the following paragraph the subset used over the different datasets and models:\n\u2022 ViT - CIFAR100: LR in {10\u22123, 10\u22124, 10\u22125}, number of epochs in {10, 20, 100, 200} \u2022 ViT - Tiny ImageNet: LR in {10\u22123, 10\u22124, 10\u22125}, number of epochs in {1, 2, 10, 20}\nFinetuning on the ImageNet-1k dataset is inherently expensive. We have thus finetuned for just 8 to 10 epochs the fused models, with an LR of 10\u22124. The boost in performance presented in Tab. 2 is thus even more noteworthy given the limited capacity to exhaustively find suitable hyper-parameters for finetuning."
        },
        {
            "heading": "E.2.2 RESULTS",
            "text": "Vision Transformer In Tab. 14 we report the finetuning results for the fusion and ensemble of two and six models on the CIFAR100 dataset. The results show how weight-based soft alignment outperforms both weight-based hard alignment and activation-based soft alignment. Furthermore, in Tab. 15 we present further results on the Tiny ImageNet dataset.\nBERT The results after finetuning for the BERT model are presented in Tab. 16 and Tab 17."
        }
    ],
    "year": 2023
}