{
    "abstractText": "In reinforcement learning (RL), rewards of states are typically considered additive, and following the Markov assumption, they are independent of states visited previously. In many important applications, such as coverage control, experiment design and informative path planning, rewards naturally have diminishing returns, i.e., their value decreases in light of similar states visited previously. To tackle this, we propose submodular RL (SUBRL), a paradigm which seeks to optimize more general, non-additive (and history-dependent) rewards modelled via submodular set functions which capture diminishing returns. Unfortunately, in general, even in tabular settings, we show that the resulting optimization problem is hard to approximate. On the other hand, motivated by the success of greedy algorithms in classical submodular optimization, we propose SUBPO, a simple policy gradient-based algorithm for SUBRL that handles non-additive rewards by greedily maximizing marginal gains. Indeed, under some assumptions on the underlying Markov Decision Process (MDP), SUBPO recovers optimal constant factor approximations of submodular bandits. Moreover, we derive a natural policy gradient approach for locally optimizing SUBRL instances even in large stateand actionspaces. We showcase the versatility of our approach by applying SUBPO to several applications such as biodiversity monitoring, Bayesian experiment design, informative path planning, and coverage maximization. Our results demonstrate sample efficiency, as well as scalability to high-dimensional state-action spaces.",
    "authors": [],
    "id": "SP:d477d2bec76f02cdcbfbedfc2df42197af227e3e",
    "references": [
        {
            "authors": [
                "David Abel",
                "Will Dabney",
                "Anna Harutyunyan",
                "Mark K Ho",
                "Michael Littman",
                "Doina Precup",
                "Satinder Singh"
            ],
            "title": "On the expressivity of markov reward",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Leemon C Baird",
                "III"
            ],
            "title": "Advantage updating",
            "venue": "WRIGHT LAB WRIGHT-PATTERSON AFB OH,",
            "year": 1993
        },
        {
            "authors": [
                "Egbert Bakker",
                "Lars Nyborg",
                "Hans B"
            ],
            "title": "Pacejka. Tyre modelling for use in vehicle dynamics studies",
            "venue": "In SAE Technical Paper. SAE International,",
            "year": 1987
        },
        {
            "authors": [
                "Maria-Florina Balcan",
                "Nicholas JA Harvey"
            ],
            "title": "Learning submodular functions",
            "venue": "In Proceedings of the forty-third annual ACM symposium on Theory of computing,",
            "year": 2011
        },
        {
            "authors": [
                "Soumya Basu",
                "Rajat Sen",
                "Sujay Sanghavi",
                "Sanjay Shakkottai"
            ],
            "title": "Blocking bandits",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Jonathan Baxter",
                "Peter L Bartlett"
            ],
            "title": "Infinite-horizon policy-gradient estimation",
            "venue": "journal of artificial intelligence research,",
            "year": 2001
        },
        {
            "authors": [
                "Stav Belogolovsky",
                "Philip Korsunsky",
                "Shie Mannor",
                "Chen Tessler",
                "Tom Zahavy"
            ],
            "title": "Inverse reinforcement learning in contextual mdps",
            "venue": "Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Jalaj Bhandari",
                "Daniel Russo"
            ],
            "title": "Global optimality guarantees for policy gradient methods",
            "venue": "arXiv preprint arXiv:1906.01786,",
            "year": 2019
        },
        {
            "authors": [
                "An Bian",
                "Kfir Levy",
                "Andreas Krause",
                "Joachim M Buhmann"
            ],
            "title": "Continuous dr-submodular maximization: Structure and algorithms",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Andrew An Bian",
                "Joachim M Buhmann",
                "Andreas Krause",
                "Sebastian Tschiatschek"
            ],
            "title": "Guarantees for greedy maximization of non-submodular functions with applications",
            "venue": "In International conference on machine learning,",
            "year": 2017
        },
        {
            "authors": [
                "Andrew An Bian",
                "Baharan Mirzasoleiman",
                "Joachim Buhmann",
                "Andreas Krause"
            ],
            "title": "Guaranteed nonconvex optimization: Submodular maximization over continuous domains",
            "venue": "In Artificial Intelligence and Statistics,",
            "year": 2017
        },
        {
            "authors": [
                "Jeff Bilmes"
            ],
            "title": "Submodularity in machine learning and artificial intelligence",
            "venue": "arXiv preprint arXiv:2202.00132,",
            "year": 2022
        },
        {
            "authors": [
                "Niladri Chatterji",
                "Aldo Pacchiano",
                "Peter Bartlett",
                "Michael Jordan"
            ],
            "title": "On the theory of reinforcement learning with once-per-episode feedback",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Chandra Chekuri",
                "M. Pal"
            ],
            "title": "A recursive greedy algorithm for walks in directed graphs",
            "venue": "In 46th Annual IEEE Symposium on Foundations of Computer Science",
            "year": 2005
        },
        {
            "authors": [
                "Lin Chen",
                "Andreas Krause",
                "Amin Karbasi"
            ],
            "title": "Interactive submodular bandit",
            "venue": "In NIPS,",
            "year": 2017
        },
        {
            "authors": [
                "Michele Conforti",
                "G\u00e9rard Cornu\u00e9jols"
            ],
            "title": "Submodular set functions, matroids and the greedy algorithm: Tight worst-case bounds and some generalizations of the rado-edmonds theorem",
            "venue": "Discrete Applied Mathematics,",
            "year": 1984
        },
        {
            "authors": [
                "Brian W Dolhansky",
                "Jeff A Bilmes"
            ],
            "title": "Deep submodular functions: Definitions and learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2016
        },
        {
            "authors": [
                "Michael O\u2019Gordon Duff"
            ],
            "title": "Optimal learning: Computational procedures for Bayes -adaptive Markov decision processes",
            "venue": "PhD thesis, University of Massachusetts Amherst,",
            "year": 2002
        },
        {
            "authors": [
                "Uriel Feige"
            ],
            "title": "A threshold of ln n for approximating set cover",
            "venue": "J. ACM,",
            "year": 1998
        },
        {
            "authors": [
                "Michael C. Fu"
            ],
            "title": "Chapter 19 gradient estimation",
            "year": 2006
        },
        {
            "authors": [
                "Neba Funwi-gabga",
                "Jorge Mateu"
            ],
            "title": "Understanding the nesting spatial behaviour of gorillas in the kagwene sanctuary, cameroon",
            "venue": "Stochastic Environmental Research and Risk Assessment,",
            "year": 2011
        },
        {
            "authors": [
                "Daniel Golovin",
                "Andreas Krause"
            ],
            "title": "Adaptive submodularity: Theory and applications in active learning and stochastic optimization",
            "venue": "Journal of Artificial Intelligence Research,",
            "year": 2011
        },
        {
            "authors": [
                "Evan Greensmith",
                "Peter L Bartlett",
                "Jonathan Baxter"
            ],
            "title": "Variance reduction techniques for gradient estimates in reinforcement learning",
            "venue": "Journal of Machine Learning Research,",
            "year": 2004
        },
        {
            "authors": [
                "Aldy Gunawan",
                "Hoong Chuin Lau",
                "Pieter Vansteenwegen"
            ],
            "title": "Orienteering problem: A survey of recent variants, solution approaches and applications",
            "venue": "European Journal of Operational Research,",
            "year": 2016
        },
        {
            "authors": [
                "Eran Halperin",
                "Robert Krauthgamer"
            ],
            "title": "Polylogarithmic inapproximability",
            "venue": "Proceedings of the Thirty-Fifth Annual ACM Symposium on Theory of Computing,",
            "year": 2003
        },
        {
            "authors": [
                "Hamed Hassani",
                "Mahdi Soltanolkotabi",
                "Amin Karbasi"
            ],
            "title": "Gradient methods for submodular maximization",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Elad Hazan",
                "Sham Kakade",
                "Karan Singh",
                "Abby Van Soest"
            ],
            "title": "Provably efficient maximum entropy exploration",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Sham M Kakade"
            ],
            "title": "A natural policy gradient",
            "venue": "Advances in neural information processing systems,",
            "year": 2001
        },
        {
            "authors": [
                "Mohammad Karimi"
            ],
            "title": "Stochastic submodular maximization: The case of coverage functions",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Miquel Kegeleirs",
                "Giorgio Grisetti",
                "Mauro Birattari"
            ],
            "title": "Swarm slam: Challenges and perspectives",
            "venue": "Frontiers in Robotics and AI,",
            "year": 2021
        },
        {
            "authors": [
                "Andreas Krause",
                "Daniel Golovin"
            ],
            "title": "Submodular function maximization",
            "venue": "Tractability,",
            "year": 2014
        },
        {
            "authors": [
                "Andreas Krause",
                "Ajit Singh",
                "Carlos Guestrin"
            ],
            "title": "Near-optimal sensor placements in gaussian processes: Theory, efficient algorithms and empirical studies",
            "venue": "J. Mach. Learn. Res., 9:235\u2013284,",
            "year": 2008
        },
        {
            "authors": [
                "David Lindner",
                "Matteo Turchetta",
                "Sebastian Tschiatschek",
                "Kamil Ciosek",
                "Andreas Krause"
            ],
            "title": "Information directed reward learning for reinforcement learning",
            "venue": "In Proc. Neural Information Processing Systems (NeurIPS),",
            "year": 2021
        },
        {
            "authors": [
                "Alexander Liniger",
                "Alexander Domahidi",
                "Manfred Morari"
            ],
            "title": "Optimization-based autonomous racing of 1: 43 scale rc cars",
            "venue": "Optimal Control Applications and Methods,",
            "year": 2015
        },
        {
            "authors": [
                "Mojm\u00edr Mutn\u00fd",
                "Andreas Krause"
            ],
            "title": "Sensing cox processes via posterior sampling and positive bases",
            "venue": "CoRR, abs/2110.11181,",
            "year": 2021
        },
        {
            "authors": [
                "Mojmir Mutny",
                "Tadeusz Janik",
                "Andreas Krause"
            ],
            "title": "Active exploration via experiment design in markov chains",
            "venue": "In International Conference on Artificial Intelligence and Statistics,",
            "year": 2023
        },
        {
            "authors": [
                "Mirco Mutti",
                "Riccardo De Santi",
                "Piersilvio De Bartolomeis",
                "Marcello Restelli"
            ],
            "title": "Challenging common assumptions in convex reinforcement learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "George L Nemhauser",
                "Laurence A Wolsey",
                "Marshall L Fisher"
            ],
            "title": "An analysis of approximations for maximizing submodular set functions\u2014i",
            "venue": "Mathematical programming,",
            "year": 1978
        },
        {
            "authors": [
                "Manish Prajapat",
                "Kamyar Azizzadenesheli",
                "Alexander Liniger",
                "Yisong Yue",
                "Anima Anandkumar"
            ],
            "title": "Competitive policy optimization",
            "venue": "In Uncertainty in Artificial Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Manish Prajapat",
                "Matteo Turchetta",
                "Melanie Zeilinger",
                "Andreas Krause"
            ],
            "title": "Near-optimal multiagent learning for safe coverage control",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Martin L. Puterman"
            ],
            "title": "Markov Decision Processes: Discrete Stochastic Dynamic Programming",
            "year": 1994
        },
        {
            "authors": [
                "John Schulman",
                "Sergey Levine",
                "Pieter Abbeel",
                "Michael Jordan",
                "Philipp Moritz"
            ],
            "title": "Trust region policy optimization",
            "venue": "In ICML,",
            "year": 2015
        },
        {
            "authors": [
                "John Schulman",
                "Philipp Moritz",
                "Sergey Levine",
                "Michael I. Jordan",
                "Pieter Abbeel"
            ],
            "title": "Highdimensional continuous control using generalized advantage estimation",
            "venue": "In 4th International Conference on Learning Representations,",
            "year": 2016
        },
        {
            "authors": [
                "John Schulman",
                "Filip Wolski",
                "Prafulla Dhariwal",
                "Alec Radford",
                "Oleg Klimov"
            ],
            "title": "Proximal policy optimization algorithms",
            "venue": "arXiv preprint arXiv:1707.06347,",
            "year": 2017
        },
        {
            "authors": [
                "Amarjeet Singh",
                "Andreas Krause",
                "William J. Kaiser"
            ],
            "title": "Nonmyopic adaptive informative path planning for multiple robots",
            "venue": "In Proceedings of the 21st International Joint Conference on Artificial Intelligence,",
            "year": 2009
        },
        {
            "authors": [
                "Matthew Streeter",
                "Daniel Golovin"
            ],
            "title": "An online algorithm for maximizing submodular functions",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2008
        },
        {
            "authors": [
                "Richard S Sutton",
                "Andrew G Barto"
            ],
            "title": "Reinforcement learning: An introduction",
            "venue": "MIT press,",
            "year": 2018
        },
        {
            "authors": [
                "Richard S Sutton",
                "David McAllester",
                "Satinder Singh",
                "Yishay Mansour"
            ],
            "title": "Policy gradient methods for reinforcement learning with function approximation",
            "venue": "Advances in neural information processing systems,",
            "year": 1999
        },
        {
            "authors": [
                "Jean Tarbouriech",
                "Shubhanshu Shekhar",
                "Matteo Pirotta",
                "Mohammad Ghavamzadeh",
                "Alessandro Lazaric"
            ],
            "title": "Active model estimation in markov decision processes",
            "venue": "In Conference on Uncertainty in Artificial Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Ehsan Tohidi",
                "Rouhollah Amiri",
                "Mario Coutino",
                "David Gesbert",
                "Geert Leus",
                "Amin Karbasi"
            ],
            "title": "Submodularity in action: From machine learning to signal processing applications",
            "venue": "IEEE Signal Processing Magazine,",
            "year": 2020
        },
        {
            "authors": [
                "Jan Vondrak"
            ],
            "title": "Submodularity and curvature: the optimal algorithm",
            "venue": "RIMS Ko\u0302kyu\u0302roku Bessatsu,",
            "year": 2010
        },
        {
            "authors": [
                "Ruosong Wang",
                "Hanrui Zhang",
                "Devendra Singh Chaplot",
                "Denis Garagi\u0107",
                "Ruslan Salakhutdinov"
            ],
            "title": "Planning with submodular objective functions",
            "venue": "arXiv preprint arXiv:2010.11863,",
            "year": 2020
        },
        {
            "authors": [
                "Yisong Yue",
                "Carlos Guestrin"
            ],
            "title": "Linear submodular bandits and their application to diversified retrieval",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2011
        },
        {
            "authors": [
                "Tom Zahavy",
                "Brendan O\u2019Donoghue",
                "Guillaume Desjardins",
                "Satinder Singh"
            ],
            "title": "Reward is enough for convex mdps",
            "venue": "In 35th Conference on Neural Information Processing Systems (NeurIPS",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "In reinforcement learning (RL), the agent aims to learn a policy by interacting with its environment in order to maximize rewards obtained. Typically, in RL, the environments are modelled as a (controllable) Markov chain, and the rewards are considered additive and independent of the trajectory. In this well-understood setting, referred to as Markov Decision Processes (MDP), the Bellman optimality principle allows to find an optimal policy in polynomial time for finite Markov chains (Puterman, 1994; Sutton & Barto, 2018). However, many interesting problems cannot straightforwardly be modelled via additive rewards. In this paper, we consider a rich and fundamental class of non-additive rewards, in particular submodular reward functions. Applications for planning under submodular rewards abound, from coverage control (Prajapat et al., 2022), entropy maximization, experiment design (Krause et al., 2008), informative path planning, orienteering problem Gunawan et al. (2016), resource allocation to Mapping (Kegeleirs et al., 2021).\nSubmodular functions capture an intuitive diminishing returns property that naturally arises in these applications: the reward obtained by visiting a state decreases in light of similar states visited previously. E.g., in a biodiversity monitoring application (see Fig. 1), if the agent has covered a particular region, the information gathered from neighbouring regions becomes redundant and tends to diminish. To tackle such history-dependent, non-Markovian rewards, one could naively augment the state to include all the past states visited so far. This approach, however, exponentially increases the state-space size, leading to intractability. In this paper, we make the following contributions:\nFirst, we introduce SUBRL, a paradigm for reinforcement learning with submodular reward functions. While this is the first work to consider submodular objectives in RL, we connect it to related areas such as submodular optimization, convex RL in Section 6. To establish limits of the SUBRL framework, we derive a lower bound that establishes hardness of approximation up to log factors (i.e., ruling out any constant factor approximation) in general (Section 3). Second, despite the hardness, we show that, in many important cases, SUBRL instances can often be effectively solved in practice. In particular, we propose an algorithm, SUBPO, motivated by the greedy algorithm in classic submodular\noptimization. It is a simple policy-gradient-based algorithm for SUBRL that handles non-additive rewards by greedily maximizing marginal gains (Section 4). Third, we show that in some restricted settings, SUBPO performs provably well. In particular, under specific assumptions on the underlying MDP, SUBRL reduces to an instance of constrained continuous DR-submodular optimization over the policy space. Even though the reduced problem is still NP-hard, we guarantee convergence to the information-theoretically optimal constant factor approximation of 1\u2212 1/e, generalizing previous results on submodular bandits (Section 5). Lastly, we demonstrate the practical utility of SUBPO in simulation as well as real-world applications. Namely, we showcase its use in biodiversity monitoring, Bayesian experiment design, informative path planning, building exploration, car racing and Mujoco robotics tasks. Our algorithm is sample efficient, discovers effective strategies, and scales well to high dimensional spaces (Section 7)."
        },
        {
            "heading": "2 SUBMODULAR RL: PRELIMINARIES AND PROBLEM STATEMENT",
            "text": "Submodularity. Let V be a ground set. A set function F : 2V \u2192 R is submodular if \u2200A \u2286 B \u2286 V , v \u2208 V\\B, we have, F (A \u222a {v})\u2212 F (A) \u2265 F (B \u222a {v})\u2212 F (B). The property captures a notion of diminishing returns, i.e., adding an element v to A will help at least as much as adding it to the superset B. We denote the marginal gain of element v as \u2206(v|A) := F (A\u222a{v})\u2212F (A). Functions for which \u2206(v|A) is independent of A are called modular. F is said to be monotone if \u2200A \u2286 B \u2286 V , we have, F (A) \u2264 F (B) (or, equiv., \u2206(v | A) \u2265 0 for all v, A). Controlled Markov Process (CMP). A CMP is a tuple formed by \u27e8V,A,P, \u03c1,H\u27e9, where V is the ground set, v \u2208 V is a state, A is the action space with a \u2208 A. \u03c1 denotes the initial state distribution and P = {Ph}H\u22121h=0 , where Ph(v\u2032|v, a) is the distribution of successive state v\u2032 after taking action a at state v at horizon h. We consider an episodic setting with finite horizon H .\nSubmodular MDP. We define a submodular MDP, or SMDP, as a CMP with a monotone submodular reward function, i.e., a tuple formed by \u27e8S,A,P, \u03c1,H, F \u27e9. Hereby, using S = H \u00d7 V to designate time-augmented states (i.e., each s = (h, v) \u2208 S is a state augmented with the current horizon step), we assume F : 2S \u2192 R1 is a monotone submodular reward function. The non-stationary transition distribution Ph(v\u2032|v, a) of the CMP can be equivalently converted to P ((h + 1, v\u2032)|(h, v), a) = P (s\u2032|s, a) since s accounts for time. An episode starts at s0 \u223c \u03c1, and at each time step h \u2265 0 at state sh, the agent draws its action ah according to a policy \u03c0 (see below). The environment evolves to a new state sh+1 following the CMP. A realization of this stochastic process is a trajectory \u03c4 = ( (sh, ah) H\u22121 h=0 , sH ) , an ordered sequence with a fixed horizon H . \u03c4l:l\u2032 = ( (sh, ah) l\u2032\u22121 h=l , sl\u2032\n) denotes the part from time step l to l\u2032. Note that \u03c4 = \u03c40:H . For each (partial) trajectory \u03c4l:l\u2032 , we use the notation F (\u03c4l:l\u2032) to refer to the objective F evaluated on the set of (state,time)-pairs visited by \u03c4l:l\u2032 .\nPolicies. The agent acts in the SMDP according to a policy, which in general maps histories \u03c40:h to (distributions over) actions ah. A policy \u03c0(ah|\u03c40:h) is called Markovian if its actions only depend on\n1Without loss of generality, this can be extended to state-action based rewards\nthe current state s, i.e., \u03c0(ah|\u03c40:h) = \u03c0(ah|sh). The set of all Markovian policies is denoted by \u03a0M. Similarly, we can consider Non-Markovian policies \u03c0(ah|\u03c4h\u2212k:h) that only depend on the previous past k steps \u03c4h\u2212k:h. The set of all Non-Markovian policies conditioned on history up to past k steps is denoted by \u03a0kNM, and we use \u03a0NM := \u03a0 H NM to allow arbitrary history-dependence.\nProblem Statement. For a given submodular MDP, we want to find a policy to maximize its expected reward. For a given policy \u03c0, let f(\u03c4 ;\u03c0) denote the probability distribution over the random trajectory \u03c4 following agent\u2019s policy \u03c0,\nf(\u03c4 ;\u03c0) = \u03c1(s0) H\u22121\u220f h=0 \u03c0(ah|\u03c40:h)P (sh+1|sh, ah). (1)\nThe performance measure J(\u03c0) is defined as the expectation of the submodular set function over the trajectory distribution induced by the policy \u03c0 and the goal is to find a policy that maximizes the performance measure within a given family of policies \u03a0 (e.g., Markovian ones). Precisely,\n\u03c0\u22c6 = argmax \u03c0\u2208\u03a0 J(\u03c0), where J(\u03c0) = \u2211 \u03c4 f(\u03c4 ;\u03c0)F (\u03c4). (2)\nGiven the non-additive reward function F , the optimal policy on Markovian and non-Markovian policy classes can differ (since the reward depends on the history, the policy may need to take the history into account for taking optimal actions). In general, even representing arbitrary non-Markovian policies is intractable, as its description size would grow exponentially with the horizon. It is easy to see that for deterministic MDPs, the optimal policy is indeed attained by a deterministic Markovian policy: Proposition 1. For any deterministic MDP with a fixed initial state, the optimal Markovian policy achieves the same value as the optimal non-Markovian policy.\nThe following proposition guarantees that, even for stochastic transitions, there always exists an optimal deterministic policy among the family of Markovian policies \u03a0M . Thus, we do not incur a loss compared to stochastic policies. Proposition 2. For any set function F , among the Markovian policies \u03a0M, there exists an optimal policy that is deterministic.\nThe proof is in Appendix A. The result extends to any non-Markovian policy class \u03a0kNM for as well, since one can group together the past k states and treat it as high-dimensional Markovian state space.\nExamples of submodular rewards. We first observe that classical MDPs are a strict special case of submodular MDPs. Indeed, for some classical reward function r : V \u2192 R+, by setting F ((h1, v1), . . . , (hk, vk)) := \u2211k l=1 \u03b3\nhlr(vl), F (\u03c4) simply recovers the (discounted) sum of rewards of the states visited by trajectory \u03c4 (hereby, \u03b3 \u2208 [0, 1], i.e., use \u03b3 = 1 for the undiscounted setting). A generic way to construct submodular rewards is to take a submodular set function F \u2032 : 2V \u2192 R defined on the ground set V , and define F (\u03c4) := F \u2032(T\u03c4), using an operator T : 2H\u00d7V \u2192 2V that drops the time indices. Thus, F (\u03c4) measures the value of the set of states T\u03c4 \u2286 V visited by \u03c4 . Note that each state is counted only once, i.e., even if F \u2032 is a modular function, F exhibits diminishing returns. There are many practical examples of F \u2032, such as coverage functions, experimental design criteria such as mutual information and others that have found applications in machine learning tasks (cf. Krause & Golovin, 2014; Bilmes, 2022). Moreover, many operations preserve submodularity, which can be exploited to build complex submodular objectives from simpler ones (Krause & Golovin, 2014, section 1.2) and can potentially be used in reward shaping. While submodularity is often considered for discrete V , the concept naturally generalizes to continuous domains."
        },
        {
            "heading": "3 SUBMODULAR RL: THEORETICAL LIMITS",
            "text": "We first show that the SUBRL problem is hard to approximate in general. In particular, we establish a lower bound that implies SUBRL cannot be approximated up to any constant factor in polynomial time, even for deterministic submodular MDPs. We prove this by reducing our problem to a known hard-to-approximate problem \u2013 the submodular orienteering problem (SOP) (Chekuri & Pal, 2005). Since we focus on deterministic SMDP\u2019s, according to Proposition 1 and Proposition 2, it suffices to consider deterministic, Markovian policies. We now formally state the inapproximability result, Theorem 1. Let OPT be the optimal value and \u03b3 > 0. Even for deterministic SMDP\u2019s, the SUBRL problem is hard to approximate within a factor of \u2126(log1\u2212\u03b3 OPT) unless NP \u2286 ZTIME(npolylog(n)).\nThus, under common assumptions in complexity theory (Chekuri & Pal, 2005; Halperin & Krauthgamer, 2003), the SUBRL problem cannot be approximated in general to better than logarithmic factors, i.e., no algorithm can guarantee J(\u03c0) \u2265 OPT\nlog1\u2212\u03b3 OPT for all input instances of SUBRL. The proof is in Appendix B. The significance of this result extends beyond submodular RL. As SUBRL falls within the broader category of general non-Markovian reward functions, Theorem 1 implies that problems involving general set functions are similarly inapproximable, limited to logarithmic factors.\nSince our inapproximability result is worst-case in nature, it does not rule out that interesting SUBRL problems remain practically solvable. In the next section, we introduce a general algorithm that is efficiently implementable, recovers constant factor approximation under assumptions (Section 5) and is empirically effective as shown in an extensive experimental study (Section 7)."
        },
        {
            "heading": "4 GENERAL ALGORITHM: SUBMODULAR POLICY OPTIMIZATION SUBPO",
            "text": "We now propose a practical algorithm for SUBRL that can efficiently handle submodular rewards. The core of our approach follows a greedy gradient update on the policy \u03c0. As common in the modern RL literature, we make use of approximation techniques for the policies to derive a method applicable to large state-action spaces. This means that the policy \u03c0\u03b8(a|s)is parameterized by \u03b8 \u2208 \u0398 where \u0398 \u2282 Rl is compact. In the case of tabular \u03c0, \u03b8 specifies an independent distribution over actions for each state.\nApproach. The objective from Eq. (2) can be equivalently formulated as \u03b8\u22c6 \u2208 argmax\u03b8\u2208\u0398 J(\u03c0\u03b8) as \u03b8 indexes our policy class. Due to the nonlinearity of the parameterization, it is often not feasible to find a global optimum for the above problem. In practice, with appropriate initialization and hyperparameters, variants of gradient descent are known to perform well empirically for MDPs. Precisely,\n\u03b8 \u2190 \u03b8 + argmax \u03b4\u03b8:\u03b4\u03b8+\u03b8\u2208\u0398\n\u03b4\u03b8\u22a4\u2207\u03b8J(\u03c0\u03b8)\u2212 1\n2\u03b1 \u2225\u03b4\u03b8\u22252. (3)\nVarious PG methods arise with different methods for gradient estimation and applying regularization (Kakade, 2001; Schulman et al., 2015; 2017). The key challenge to all of them is computation of the gradient\u2207J(\u03c0\u03b8). Below, we devise an unbiased gradient estimator for general non-additive functions. Gradient Estimator. As common in the policy gradient (PG) literature, we can use the score function g(\u03c4, \u03c0\u03b8) := \u2207\u03b8(log \u220fH\u22121 i=0 \u03c0\u03b8(ai|si)) to calculate the gradient\u2207\u03b8J . Namely, Given an MDP and the policy parameters \u03b8, \u2207\u03b8J(\u03c0\u03b8) =\n\u2211 \u03c4 f(\u03c4 ;\u03c0\u03b8)g(\u03c4, \u03c0\u03b8)F (\u03c4). (4)\nAs Eq. (4) shows, we do not require knowledge of the environment if sampled trajectories are available. It also does not require full observability of the states nor any structural assumption on the MDP. On the other hand, the score gradients suffer from high variance due to sparsity induced by trajectory rewards (Fu, 2006; Prajapat et al., 2021; Sutton & Barto, 2018). Hence, we take the SMDP structure into account to develop efficient algorithms.\nMarginal gain: We define the marginal gain for a state s in the trajectory \u03c40:j up to horizon j as F (s|\u03c40:j) = F (\u03c40:j \u222a {s})\u2212 F (\u03c40:j).\nOur approach aims to maximize the marginal gain associated with each action instead of maximizing state rewards. This approach shares similarities with the greedy algorithm commonly used in submodular maximization, which maximizes marginal gains and is known for its effectiveness. Moreover, decomposing the trajectory return into marginal gains and incorporating it in the policy gradient with suitable baselines Greensmith et al. (2004) removes sparsity and thus helps to reduce variance. Inspired by the policy gradient method for additive rewards (Sutton et al., 1999; Baxter & Bartlett, 2001), we propose the following for SMDP: Theorem 2. Given an SMDP and the policy parameters \u03b8, with any set function F ,\n\u2207\u03b8J(\u03c0\u03b8) = E \u03c4\u223cf(\u03c4 ;\u03c0\u03b8) H\u22121\u2211 i=0 \u2207\u03b8 log \u03c0\u03b8(ai|si) H\u22121\u2211 j=i F (sj+1|\u03c40:j)\u2212 b(\u03c40:i)  (5) We use an importance sampling estimator (log trick) to obtain Eq. (4). To reduce variance, we subtract a baseline b(\u03c40:i) from the score gradient, which can be a function of the past trajectory \u03c40:j . This incorporates the causality property in the estimator, ensuring that the action at timestep j cannot affect previously observed states. After simplifying and considering marginals, we obtain Theorem 2 (proof\n8: Update policy parameters (\u03b8) using Eq. (3)\nis in Appendix C). This estimator assigns a higher weight to policies with high marginal gains and a lower weight to policies with low marginal gains. Empirically this performs very well (Section 7).\nWe can optimize this approach by using an appropriate baseline as a function of the history \u03c40:j , which leads to an actor-critic type method. The versatility of the approach is demonstrated by the fact that Theorem 2 holds for any choice of baseline critic. We explain later in the experiments how to choose a baseline. One can perform a Monte Carlo estimate (Baird, 1993) or generalized advantage function (GAE) (Schulman et al., 2016) to estimate returns based on the marginal gain. To encourage exploration, similar to standard PG, we can employ a soft policy update based on entropy penalization, resulting in diverse trajectory samples. Entropy penalization in SUBRL can be thought of as the sum of modular and submodular rewards, which is a submodular function.\nAlgorithm. The outline of the steps is given in Algorithm 1. We represent the agent by a stochastic policy parameterized by a neural network. The algorithm operates in epochs and assumes a way to generate samples from the environment, e.g., via a simulator. In each epoch, the agent recursively samples actions from its stochastic policy and applies them in the environment leading to a roll out of the trajectory where it collects samples (Line 6). We execute multiple (B) batches in each epoch for accurate gradient estimation. To update the policy, we compute the estimator of the policy gradient as per Theorem 2, where we utilize marginal gains of the trajectory instead of immediate rewards as in standard RL (Line 7). Finally, we use stochastic gradient ascent to update the policy parameters."
        },
        {
            "heading": "5 PROVABLE GUARANTEES IN SIMPLIFIED SETTINGS",
            "text": "In general, Section 3 shows SUBRL is NP-hard to approximate. A natural question is: Is it possible to do better under additional structural assumptions? In this section, we present an interesting case under which SUBPO can be approximated to a constant factor via DR-submodular optimization. Definition 1 (DR submodularity and DR-property, Bian et al. (2017a)). For X \u2286 Rd, a function f : X \u2192 R is DR-submodular (has the DR property) if \u2200a \u2264 b \u2208 X , \u2200i \u2208 [d], \u2200k \u2208 R+ s.t. (kei + a) and (kei +b) are still in X , it holds, f(kei + a)\u2212 f(a) \u2265 f(kei +b)\u2212 f(b). (Notation: ei denotes ith basis vector and for any two vectors a,b,a \u2264 b means ai \u2264 bi\u2200i \u2208 [d]) Monotone DR-submodular functions form a family of generally non-convex functions, which can be approximately optimized. As discussed below, gradient-based algorithms find constant factor approximations over general convex, downward-closed polytopes.\nUnder the following condition on the Markov chain, we can show that as long as the policy is parametrized in a particular way, the objective is indeed monotone DR-submodular. Definition 2 (\u03f5-Bandit SMDP). An SMDP s.t. for any vj , vk \u2208 V , j \u0338= k, \u2200h \u2208 [H] and \u2200v\u2032 \u2208 V , Ph(vj |v\u2032, aj) = 1\u2212 \u03f5h, and Ph(vk|v\u2032, aj) = \u03f5h|V|\u22121 for \u03f5h \u2208 [ 0, |V||V|+1 ] is an \u03f5-Bandit SMDP.\nThis represents a \"nearly deterministic\" MDP where there is a unique action for each state in the MDP, which takes us to it with 1\u2212 \u03f5 probability and with the rest, we end up in any other state Fig. 2. While limiting, it generalizes the bandit scenario, which would occur when \u03f5 = 0. In the following, we consider a class of state-independent policies that can change in each horizon, denoting the horizon dependence with \u03c0h(a). We now formally establish the connection between SUBRL and DR-submodularity, Theorem 3. For horizon dependent policy \u03c0 parameterized as \u03c0h(a)\u2200h \u2208 [H] in an \u03f5-Bandit SMDP, and F (\u03c4) is a monotone submodular function, then J(\u03c0) is monotone DR-submodular.\nThe proof is in Appendix D. It builds on two steps; firstly, we use a reparameterization trick to handle policy simplex constraints. We relax the equality constraints on \u03c0 to lie on a convex polytope P = {\u03c0h(a) | 0 \u2264 \u03c0h(a) \u2264 1, 0 \u2264 \u2211 j,j \u0338=k \u03c0 h(aj) \u2264 1,\u2200k \u2208 [|A|],\u2200h \u2208 [H]} and enforce the equality\nconstraints directly in the objective Eq. (2). Secondly, under the assumptions of Theorem 3, we show that the Hessian of J(\u03c0) only has non-positive entries, which is an equivalent characterization of twice differentiable DR-submodular functions. Furthermore, the result can be generalized to accommodate state and action spaces that vary with horizons, although, for simplicity, we assumed fixed spaces.\nThe convex polytope P belongs to a class of down-closed convex constraints. Bian et al. (2017c) proposes a modified FRANK-WOLFE algorithm for DR-submodular maximization with down-closed constraints. This variant can achieve an (1 \u2212 1/e) approximation guarantee and has a sub-linear convergence rate. The algorithm proceeds as follows: the gradient oracle is the same as Theorem 2, while employing a tabular policy parameterization. The polytopic constraints P are ensured through a FRANK-WOLFE step, which involves solving a linear program over the policy domain. Finally, the policy is updated with a specific step size defined in (Bian et al., 2017c). Furthermore, Hassani et al. (2017) shows that any stationary point in the optimization landscape of DR-submodular maximization under general convex constraints is guaranteed to be 1/2 optimal. Therefore, any gradient-based optimizer can be used for the \u03f5-Bandit SMDP, and will result in an 1/2-optimal policy. In Section 6, we elaborate on how this setting generalizes previous works on submodular bandits.\nGeneral SMDP. While we cannot obtain a provable result for general SMDP\u2019s (Theorem 1), we can, interestingly, quantify the deviation of submodular function from a modular function using the notion of curvature (Conforti & Cornu\u00e9jols, 1984) and hope to get guarantees under the bounded deviation.\nCurvature: The notion of curvature reflects how much the marginal values \u2206(v|A) can decrease as a function A. The total curvature of F is defined as, c = 1 \u2212 minA,j /\u2208A \u2206(j|A)/F (j). Note that c \u2208 [0, 1], and if c = 0 then the marginal gain is independent of A (i.e., F is modular). Proposition 3. Consider a tabular SMDP, s.t. the reward function F is monotone submodular with bounded curvature c \u2208 (0, 1). Then, for the policy \u03c0 (with tabular parametrization) obtained via SUBPO, it holds that J(\u03c0) \u2265 (1\u2212 c)J(\u03c0\u22c6), where \u03c0\u22c6 is an optimal non-Markovian policy. Thus, under assumptions of bounded curvature, c \u2208 (0, 1), we can guarantee constant factor optimality for SUBPO (proof in Appendix D.2). Vondrak (2010) establishes a curvature-based hardness result for the simpler problem of submodular set function optimization under cardinality constraints, implying that 1\u2212 c is a near-optimal approximation ratio. Moreover, if c = 0, i.e., F denotes modular rewards, the SUBPO algorithm reduces to standard PG, and hence recovers the guarantees and benefits of the modular PG. In particular, with tabular policy parameterization, under mild regularity assumptions, any stationary point of the modular PG cost function is a global optimum (Bhandari & Russo, 2019)."
        },
        {
            "heading": "6 RELATED WORK",
            "text": "Beyond Markovian RL. Several prior works in RL identify the deficiency in the modelling ability of classical Markovian rewards. This manifests itself especially when exploration is desired, e.g., when the transition dynamics are not completely known (Tarbouriech et al., 2020; Hazan et al., 2019) or when the reward is not completely known Lindner et al. (2021); Belogolovsky et al. (2021). Chatterji et al. (2021) considers binary feedback drawn from the logistic model at the end episode and explores state representation, such as additivity, to propose an algorithm capable of learning non-Markovian policies. While all these address in some aspect the shortcomings of Markovian rewards, they tend to focus on a specific aspect instead of postulating a new class of reward functions as we do in this work.\nConvex RL. Convex RL also seeks to optimize a family of non-additive rewards. The goal is to find a policy that optimizes a convex function over the state visitation distribution (which averages over the randomness in the MDP and the policies actions). This framework has applications, e.g., in exploration and experimental design (Hazan et al., 2019; Zahavy et al., 2021; Duff, 2002; Tarbouriech et al., 2020; Mutny et al., 2023). While sharing some motivating applications, convex and submodular RL are rather different in nature. Beyond the high-level distinction that convex and submodular function classes are complimentary, our non-additive (submodular) rewards are defined over the actual sequence of states visited by the policy, not its average behaviour. Mutti et al. (2022) points out that this results in substantial differences, noting the deficiency of convex RL in modelling expected utilities of the form as in Eq. (2), which we address in our work.\nSubmodular Maximization. Submodular functions are widely studied in combinatorial optimization and operations research and have found many applications in machine learning (Krause & Golovin, 2014; Bilmes, 2022; Tohidi et al., 2020). The seminal work of Nemhauser et al. (1978) shows that greedy algorithms enjoy a constant factor 1 \u2212 1/e approximation for maximizing monotone\nsubmodular functions under cardinality constraints, which is information- and complexity- theoretically optimal (Feige, 1998). Beyond simpler cardinality (and matroid) constraints, more complex constraints have been considered: most relevant is the s-t-submodular orienteering problem, which aims to find an s-t-path in a graph of bounded length maximizing a submodular function of the visited nodes (Chekuri & Pal, 2005), and can be viewed as a special case of SUBRL on deterministic SMDP\u2019s with deterministic starting state and hard constraint on the goal state. It has been used as an abstraction for informative path planning (Singh et al., 2009). We generalize the setup and connect it with modern policy gradient techniques. Wang et al. (2020) considers planning under the surrogate multi-linear extension of submodular objectives. Certain problems considered in our work satisfy a notion called adaptive submodularity, which generalizes the greedy approximation guarantee over a set of policies (Golovin & Krause, 2011). While adaptive submodularity allows capturing history-dependence, it fails to address complex constraints (such as those imposed by CMP\u2019s).\nWhile submodularity is typically considered for discrete domains (i.e., for functions defined on {0, 1}|V|, the concept can be generalized to continuous domains, e.g., [0, 1]|V| using notions such as DR-submodularity (Bian et al., 2017b). This notion forms a class of non-convex problems admitting provable approximation guarantees in polynomial time, which we exploit in Section 5. The problem of learning submodular functions has also been considered (Balcan & Harvey, 2011). Dolhansky & Bilmes (2016) introduce the class of deep submodular functions, neural network models guaranteed to yield functions that are submodular in their input. These may be relevant when learning unknown rewards using function approximation, which is an interesting direction for future work.\nSince submodularity is a natural characterization of diminishing returns, numerous tasks involving exploration or discouraging repeated actions (Basu et al., 2019) can be captured via submodular functions. In addition to our experiments discussing experiment design, item collection and coverage objectives, Table 1 provides a summary of problems that can be addressed with SUBRL.\nThe submodular bandit problem is at the interface of learning and optimizing submodular functions (Streeter & Golovin, 2008; Chen et al., 2017; Yue & Guestrin, 2011). Algorithms with no-regret (relative to the 1-1/e approximation) exist, whose performance can be improved by exploiting linearity (Yue & Guestrin, 2011) or smoothness (Chen et al., 2017) in the objective. Our results in Section 5 can be viewed as addressing (a generalization of) the submodular stochastic bandit problem. Exploiting further linearity or smoothness to improve sample complexity is interesting direction for future work."
        },
        {
            "heading": "7 EXPERIMENTS",
            "text": "We empirically study the performance of SUBRL on multiple environments. They are i) Informative path planning, ii) Item collection, iii) Bayesian D-experimental design, iv) Building exploration, v) Car racing and vi) Mujoco-Ant. The environments involve discrete (i-iv) and continuous (v-vi) stateaction spaces and capture a range of submodular rewards, illustrating the versatility of the framework.\nThe problem is challenging in two aspects: firstly, how to maximize submodular rewards, and secondly, how to maintain an effective state representation to enable history-dependent policies. Our experiments mainly focus on the first aspect and demonstrate that even with a simple Markovian state representation, by greedily maximizing marginal gains, one can achieve good performance similar to the ideal case of non-Markovian representation in many environments. However, we do not claim that a Markovian state representation is sufficient in general. For instance, in the building exploration and item collection environments, Markovian policies are insufficient, and a history-dependent approach is necessary for further optimization. Natural avenues are to augment\nthe state representation to incorporate additional information, e.g., based on domain knowledge, or to use a non-Markovian parametric policy class such as RNNs. Exploring such representations is application specific, and beyond the scope of our work.\nWe consider two variants of SUBPO: SUBPO-M and SUBPO-NM, corresponding to Markovian and non-Markovian policies, respectively. SUBPO-NM uses a stochastic policy that conditions an action on the history. We model the policy using a neural network that maps the history to a distribution over actions, whereas SUBPO-M maps the state to a distribution over actions. Disregarding sample complexity, we expect SUBPO-NM perform the best, since it can track the complete history. In our experiments, we always compare with modular RL (MODPO), a baseline that represents standard RL, where the additive reward for any state s is F ({s}). In MODPO, we maximize the undiscounted sum of additive rewards, whereas, in contrast, SUBPO maximizes marginal gains. The rest of the process remains the same, i.e., we use the same policy gradient method. We implemented all algorithms in Pytorch and will make the code and the videos public. We deploy Pytorch\u2019s automatic differentiation package to compute an unbiased gradient estimator. Experiment details and extended empirical analysis are in Appendix E. Below we explain our observations for each environment:\nInformative path planning. We simulate a bio-diversity monitoring task, where we aim to cover areas with a high density of gorilla nests with a quadrotor in the Kagwene Gorilla Sanctuary (Fig. 1a). The quadrotor at location s covers a limited sensing region around it, Ds. The quadrotor starts in a random initial state and follows deterministic dynamics. It is equipped with five discrete actions representing directions. Let \u03c1 : V \u2192 R be the nest density obtained by fitting a smooth rate function (Mutn\u00fd & Krause, 2021) over Gorilla nest counts (Funwi-gabga & Mateu, 2011). The objective function is given by F (\u03c4) = g( \u22c3 s\u2208\u03c4 D s), where g(V ) = \u2211\nv\u2208V \u03c1(v). As shown in Fig. 3a, we observe that MODPO repeatedly maximizes its modular reward and gets stuck at a high-density region, whereas SUBPO achieves performance as good as SUBPO-NM while being more sample efficient. To generalize the analysis, we replace the nest density with randomly generated synthetic multimodal functions and observe a similar trend (Appendix E).\nItem collection. Fig. 1b, the environment consists of a grid with a group of items G = {banana, apple, strawberries, watermelon} located at gi \u2286 V , i \u2208 G. We consider stochastic dynamics such that with probability 0.9, the action we take is executed, and with probability 0.1, a random action is executed (up, down, left, right, stay). The agent has to find a policy that generates trajectories \u03c4 , which picks di items from group gi, for each i. Formally, the submodular reward function can be defined as F (\u03c4) = \u2211 i\u2208G min(|\u03c4 \u2229 gi|, di). We performed the experiment with 10 different randomly generated environments and 20 runs in each. In this environment, the agent must keep track of items collected so far to optimize for future items. Hence as shown in Fig. 3b, SUBPO-NM based on non-Markovian policy achieves good performance, and SUBPO-M achieves a slightly lower but yet comparable performance just by maximizing marginal gains.\nBayesian D-experimental design. In this experiment, we seek to estimate an a-priori unknown function f . The function f is assumed to be regular enough to be modelled using Gaussian Processes. Where should we sample f to estimate it as well as possible? Formally, our goal is to optimize over trajectories \u03c4 that provide maximum mutual information between f and the observations y\u03c4 = f\u03c4+\u03f5\u03c4 at the points \u03c4 . The mutual information is given by I(y\u03c4 ; f) = H(y\u03c4 )\u2212H(y\u03c4 |f), representing the reduction in uncertainty of f after knowing y\u03c4 , where H(y\u03c4 ) is entropy. We define the monotonic submodular function F (\u03c4) = I(y\u03c4 ; f). The gorilla nest density f is an a-priori unknown function. We generate 10 different environments by assuming random initialization and perform 20 runs on each to compute statistical confidence. In Fig. 3c, we observe a similar trend that MODPO gets stuck\nat a high uncertainty region and cannot effectively optimize the information gained by the entire trajectory, whereas SUBPO-M achieves performance as good as SUBPO-NM while being very sample efficient due to the smaller search space of the Markovian policy class.\nBuilding exploration. The environment consists of two rooms connected by a corridor. The agent at s covers a nearby region Ds around itself, marked as a green patch in Fig. 4a. The task is to find a trajectory \u03c4 that maximizes the submodular function F (\u03c4) = | \u222as\u2208\u03c4 Ds|. The agent starts in the corridor\u2019s middle and has deterministic dynamics. The horizon H is just enough to cover both rooms. Based on the time-augmented state space, there exists a deterministic Markovian policy that can solve the task. However, it is a challenging environment for exploration with Monte Carlo samples using Markovian policies. As shown in Fig. 5a, SUBPO achieves a sub-optimal solution of exploring primarily one side, whereas SUBPO-NM tracks the history and learns to explore the other room.\nCar Racing is an interesting high-dimensional environment, with continuous state-action space, where a race car tries to finish the racing lap as fast as possible (Prajapat et al., 2021). The environment is accompanied by an important challenge of learning a policy to maneuver the car at the limit of handling. The track is challenging, consisting of 13 turns with different curvature (Fig. 4b). The car has a six-dimensional state space representing position and velocities. The control commands are two-dimensional, representing throttle and steering. Detailed information is in the Appendix E. The car is equipped with a camera and observes a patch around its state s as Ds. The objective function is F (\u03c4) = | \u222as\u2208\u03c4 Ds|. We set a finite horizon of 700. The SUBPO-NM will have a large state space of 700\u00d7 6, which makes it difficult to train. For variance reduction, we use a baseline b(s) in Eq. (5) that estimates the cumulative sum of marginal gains. As shown in Fig. 5b, under the coverage-based reward formulation, the agent trained with MODPO tries to explore a little bit but gets stuck with a stationary action at the beginning of the episode to get a maximum modular reward. However, the SUBPO agent tries to maximise the marginal again at each timestep and hence learns to drive on the race track (https://youtu.be/jXp0QxIQ\u2013E). Although it is possible to use alternative reward functions to train the car using standard RL, the main objective of this study is to demonstrate SUBPO on the continuous domains and how submodular functions can provide versatility to achieve surrogate goals.\nMuJoCo Ant. The task is a high-dimensional locomotion task, as depicted in Fig. 4c. The state space dimension is 30, containing information about the robot\u2019s pose and the internal actuator\u2019s orientation. The control input dimension is 8, consisting of torque commands for each actuator. The Ant at any location s covers locations in 2D space,Ds and receives a reward based on it. The goal is to maximize F (\u03c4) = |\u222as\u2208\u03c4 Ds|. The results depicted in Fig. 5c demonstrate that SUBPO maximizes marginal gain and learns to explore the environment, while MODPO learns to stay stationary, maximizing modular rewards. The environment carries the core challenge of continuous control and high-dimensional observation spaces. This experiment shows that SUBPO can effectively scale to high-dimensional domains."
        },
        {
            "heading": "8 CONCLUSION",
            "text": "We introduced a novel framework, submodular RL for decision-making under submodular rewards. We prove the first-of-its-kind inapproximability result for SUBRL i.e., the problem is not just NP-Hard but intractable even to approximate up to any constant factor. We propose an algorithm, SUBPO for this problem and show that under simplified assumptions, it achieves constant-factor approximation guarantees. We show that the algorithm exhibits strong empirical performance and scales very well to high-dimensional spaces. We hope this work will expand the reach of the RL community to embrace the broad class of submodular objectives which are relevant to many practical real-world problems.\nREPRODUCIBILITY STATEMENT\nWe have included all of the code and environments used in this study in the supplementary materials. These resources will be made open-source later on. The attached code contains a README.md file that provides comprehensive instructions for running the experiments. Furthermore, Appendix E contains additional emperical results and the parameters to reproduce the results. Regarding the theoretical results, all the proofs of the propositions and the theorems can be found in the appendix."
        }
    ]
}