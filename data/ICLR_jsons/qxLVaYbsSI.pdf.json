{
    "abstractText": "Federated semi-supervised learning (FSSL) has emerged as a powerful paradigm for collaboratively training machine learning models using distributed data with label deficiency. Advanced FSSL methods predominantly focus on training a single model on each client. However, this approach could lead to a discrepancy between the objective functions of labeled and unlabeled data, resulting in gradient conflicts. To alleviate gradient conflict, we propose a novel twin-model paradigm, called Twin-sight, designed to enhance mutual guidance by providing insights from different perspectives of labeled and unlabeled data. In particular, Twin-sight concurrently trains a supervised model with a supervised objective function while training an unsupervised model using an unsupervised objective function. To enhance the synergy between these two models, Twin-sight introduces a neighbourhood-preserving constraint, which encourages the preservation of the neighbourhood relationship among data features extracted by both models. Our comprehensive experiments on four benchmark datasets provide substantial evidence that Twin-sight can significantly outperform state-of-the-art methods across various experimental settings, demonstrating the efficacy of the proposed Twin-sight. The code is publicly available at: github.com/tmlr-group/Twin-sight.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yonggang Zhang"
        },
        {
            "affiliations": [],
            "name": "Zhiqin Yang"
        },
        {
            "affiliations": [],
            "name": "Xinmei Tian"
        },
        {
            "affiliations": [],
            "name": "Nannan Wang"
        },
        {
            "affiliations": [],
            "name": "Tongliang Liu"
        },
        {
            "affiliations": [],
            "name": "Bo Han"
        }
    ],
    "id": "SP:50a8c9fe156ab97ca39ead339c963ff407ab6bae",
    "references": [
        {
            "authors": [
                "Abdullatif Albaseer",
                "Bekir Sait Ciftler",
                "Mohamed Abdallah",
                "Ala Al-Fuqaha"
            ],
            "title": "Exploiting unlabeled data in smart cities using federated learning",
            "venue": "arXiv preprint arXiv:2001.04030,",
            "year": 2020
        },
        {
            "authors": [
                "Philip Bachman",
                "R Devon Hjelm",
                "William Buchwalter"
            ],
            "title": "Learning representations by maximizing mutual information across views",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Mikhail Belkin",
                "Partha Niyogi"
            ],
            "title": "Semi-supervised learning on riemannian manifolds",
            "venue": "Machine learning,",
            "year": 2004
        },
        {
            "authors": [
                "David Berthelot",
                "Nicholas Carlini",
                "Ian Goodfellow",
                "Nicolas Papernot",
                "Avital Oliver",
                "Colin A Raffel"
            ],
            "title": "Mixmatch: A holistic approach to semi-supervised learning",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "David Berthelot",
                "Nicholas Carlini",
                "Ekin D Cubuk",
                "Alex Kurakin",
                "Kihyuk Sohn",
                "Han Zhang",
                "Colin Raffel"
            ],
            "title": "Remixmatch: Semi-supervised learning with distribution alignment and augmentation anchoring",
            "venue": "In ICLR,",
            "year": 2020
        },
        {
            "authors": [
                "Krzysztof Chalupka",
                "Pietro Perona",
                "Frederick Eberhardt"
            ],
            "title": "Visual causal feature learning",
            "venue": "arXiv preprint arXiv:1412.2309,",
            "year": 2014
        },
        {
            "authors": [
                "Ting Chen",
                "Simon Kornblith",
                "Mohammad Norouzi",
                "Geoffrey Hinton"
            ],
            "title": "A simple framework for contrastive learning of visual representations",
            "venue": "In International conference on machine learning,",
            "year": 2020
        },
        {
            "authors": [
                "Xinlei Chen",
                "Kaiming He"
            ],
            "title": "Exploring simple siamese representation learning",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Enmao Diao",
                "Jie Ding",
                "Vahid Tarokh"
            ],
            "title": "Semifl: Semi-supervised federated learning for unlabeled clients with alternate training",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Yiqun Diao",
                "Qinbin Li",
                "Bingsheng He"
            ],
            "title": "Towards addressing label skews in one-shot federated learning",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Jin Gao",
                "Jialing Zhang",
                "Xihui Liu",
                "Trevor Darrell",
                "Evan Shelhamer",
                "Dequan Wang"
            ],
            "title": "Back to the source: Diffusion-driven test-time adaptation",
            "year": 2023
        },
        {
            "authors": [
                "Jean-Bastien Grill",
                "Florian Strub",
                "Florent Altch\u00e9",
                "Corentin Tallec",
                "Pierre Richemond",
                "Elena Buchatskaya",
                "Carl Doersch",
                "Bernardo Avila Pires",
                "Zhaohan Guo",
                "Mohammad Gheshlaghi Azar"
            ],
            "title": "Bootstrap your own latent-a new approach to self-supervised learning",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Kaiming He",
                "Haoqi Fan",
                "Yuxin Wu",
                "Saining Xie",
                "Ross Girshick"
            ],
            "title": "Momentum contrast for unsupervised visual representation learning",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Tzu-Ming Harry Hsu",
                "Hang Qi",
                "Matthew Brown"
            ],
            "title": "Measuring the effects of non-identical data distribution for federated visual classification",
            "year": 1909
        },
        {
            "authors": [
                "Weidong Huang",
                "Jiaming Ji",
                "Borong Zhang",
                "Chunhe Xia",
                "Yaodong Yang"
            ],
            "title": "Safedreamer: Safe reinforcement learning with world models",
            "venue": "In The Twelfth International Conference on Learning Representations,",
            "year": 2024
        },
        {
            "authors": [
                "Wonyong Jeong",
                "Jaehong Yoon",
                "Eunho Yang",
                "Sung Ju Hwang"
            ],
            "title": "Federated semi-supervised learning with inter-client consistency & disjoint learning",
            "venue": "In 9th International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Shaoxiong Ji",
                "Yue Tan",
                "Teemu Saravirta",
                "Zhiqin Yang",
                "Lauri Vasankari",
                "Shirui Pan",
                "Guodong Long",
                "Anwar Walid"
            ],
            "title": "Emerging trends in federated learning: From model fusion to federated x learning",
            "venue": "arXiv preprint arXiv:2102.12920,",
            "year": 2023
        },
        {
            "authors": [
                "Yilun Jin",
                "Yang Liu",
                "Kai Chen",
                "Qiang Yang"
            ],
            "title": "Federated learning without full labels: A survey",
            "venue": "arXiv preprint arXiv:2303.14453,",
            "year": 2023
        },
        {
            "authors": [
                "Peter Kairouz",
                "H Brendan McMahan",
                "Brendan Avent",
                "Aur\u00e9lien Bellet",
                "Mehdi Bennis",
                "Arjun Nitin Bhagoji",
                "Kallista Bonawitz",
                "Zachary Charles",
                "Graham Cormode",
                "Rachel Cummings"
            ],
            "title": "Advances and open problems in federated learning",
            "venue": "Foundations and Trends\u00ae in Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Sai Praneeth Karimireddy",
                "Satyen Kale",
                "Mehryar Mohri",
                "Sashank Reddi",
                "Sebastian Stich",
                "Ananda Theertha Suresh"
            ],
            "title": "Scaffold: Stochastic controlled averaging for federated learning",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Alex Krizhevsky",
                "Geoffrey Hinton"
            ],
            "title": "Learning multiple layers of features from tiny images",
            "venue": "Technical report, University of Toronto,",
            "year": 2009
        },
        {
            "authors": [
                "Dong-Hyun Lee"
            ],
            "title": "Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks",
            "venue": "In Workshop on challenges in representation learning,",
            "year": 2013
        },
        {
            "authors": [
                "Qinbin Li",
                "Zeyi Wen",
                "Zhaomin Wu",
                "Sixu Hu",
                "Naibo Wang",
                "Yuan Li",
                "Xu Liu",
                "Bingsheng He"
            ],
            "title": "A survey on federated learning systems: vision, hype and reality for data privacy and protection",
            "venue": "IEEE Transactions on Knowledge and Data Engineering,",
            "year": 2021
        },
        {
            "authors": [
                "Qinbin Li",
                "Yiqun Diao",
                "Quan Chen",
                "Bingsheng He"
            ],
            "title": "Federated learning on non-iid data silos: An experimental study",
            "venue": "IEEE 38th International Conference on Data Engineering (ICDE),",
            "year": 2022
        },
        {
            "authors": [
                "Tian Li",
                "Anit Kumar Sahu",
                "Manzil Zaheer",
                "Maziar Sanjabi",
                "Ameet Talwalkar",
                "Virginia Smith"
            ],
            "title": "Federated optimization in heterogeneous networks",
            "venue": "Proceedings of Machine Learning and Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Wenqi Li",
                "Fausto Milletar\u00ec",
                "Daguang Xu",
                "Nicola Rieke",
                "Jonny Hancox",
                "Wentao Zhu",
                "Maximilian Baust",
                "Yan Cheng",
                "S\u00e9bastien Ourselin",
                "M Jorge Cardoso"
            ],
            "title": "Privacy-preserving federated brain tumour segmentation",
            "venue": "In Machine Learning in Medical Imaging: 10th International Workshop, MLMI 2019,Proceedings",
            "year": 2019
        },
        {
            "authors": [
                "Xiaoxiao Liang",
                "Yiqun Lin",
                "Huazhu Fu",
                "Lei Zhu",
                "Xiaomeng Li"
            ],
            "title": "Rscfed: random sampling consensus federated semi-supervised learning",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Haowen Lin",
                "Jian Lou",
                "Li Xiong",
                "Cyrus Shahabi"
            ],
            "title": "Semifed: Semi-supervised federated learning with consistency and pseudo-labeling",
            "venue": "arXiv preprint arXiv:2108.09412,",
            "year": 2021
        },
        {
            "authors": [
                "Tao Lin",
                "Lingjing Kong",
                "Sebastian U Stich",
                "Martin Jaggi"
            ],
            "title": "Ensemble distillation for robust model fusion in federated learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Bo Liu",
                "Xingchao Liu",
                "Xiaojie Jin",
                "Peter Stone",
                "Qiang Liu"
            ],
            "title": "Conflict-averse gradient descent for multi-task learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Hong Liu",
                "Jianmin Wang",
                "Mingsheng Long"
            ],
            "title": "Cycle self-training for domain adaptation",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Quande Liu",
                "Hongzheng Yang",
                "Qi Dou",
                "Pheng-Ann Heng"
            ],
            "title": "Federated semi-supervised medical image classification via inter-client relation matching",
            "venue": "In Medical Image Computing and Computer Assisted Intervention (MICCAI),",
            "year": 2021
        },
        {
            "authors": [
                "Guodong Long",
                "Yue Tan",
                "Jing Jiang",
                "Chengqi Zhang"
            ],
            "title": "Federated learning for open banking",
            "venue": "In Federated Learning: Privacy and Incentive,",
            "year": 2020
        },
        {
            "authors": [
                "Zewei Long",
                "Liwei Che",
                "Yaqing Wang",
                "Muchao Ye",
                "Junyu Luo",
                "Jinze Wu",
                "Houping Xiao",
                "Fenglong Ma"
            ],
            "title": "Fedsiam: Towards adaptive federated semi-supervised learning",
            "venue": "arXiv preprint arXiv:2012.03292,",
            "year": 2020
        },
        {
            "authors": [
                "Mi Luo",
                "Fei Chen",
                "Dapeng Hu",
                "Yifan Zhang",
                "Jian Liang",
                "Jiashi Feng"
            ],
            "title": "No fear of heterogeneity: Classifier calibration for federated learning with non-iid data",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Brendan McMahan",
                "Eider Moore",
                "Daniel Ramage",
                "Seth Hampson",
                "Blaise Aguera y Arcas"
            ],
            "title": "Communication-efficient learning of deep networks from decentralized data",
            "venue": "In Artificial intelligence and statistics,",
            "year": 2017
        },
        {
            "authors": [
                "Ishan Misra",
                "Laurens van der Maaten"
            ],
            "title": "Self-supervised learning of pretext-invariant representations",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Jovana Mitrovic",
                "Brian McWilliams",
                "Jacob Walker",
                "Lars Buesing",
                "Charles Blundell"
            ],
            "title": "Representation learning via invariant causal mechanisms",
            "venue": "arXiv preprint arXiv:2010.07922,",
            "year": 2020
        },
        {
            "authors": [
                "Yuval Netzer",
                "Tao Wang",
                "Adam Coates",
                "Alessandro Bissacco",
                "Bo Wu",
                "Andrew Y Ng"
            ],
            "title": "Reading digits in natural images with unsupervised feature learning",
            "venue": "In NIPS Workshop,",
            "year": 2011
        },
        {
            "authors": [
                "Aaron van den Oord",
                "Yazhe Li",
                "Oriol Vinyals"
            ],
            "title": "Representation learning with contrastive predictive coding",
            "venue": "arXiv preprint arXiv:1807.03748,",
            "year": 2018
        },
        {
            "authors": [
                "Prashant Pandey",
                "Mrigank Raman",
                "Sumanth Varambally",
                "Prathosh Ap"
            ],
            "title": "Generalization on unseen domains via inference-time label-preserving target projections",
            "venue": "In CVPR,",
            "year": 2021
        },
        {
            "authors": [
                "Sam T Roweis",
                "Lawrence K Saul"
            ],
            "title": "Nonlinear dimensionality reduction by locally linear embedding",
            "year": 2000
        },
        {
            "authors": [
                "Anindya Sarkar",
                "Anirban Sarkar",
                "Vineeth N Balasubramanian"
            ],
            "title": "Leveraging test-time consensus prediction for robustness against unseen noise",
            "venue": "In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision,",
            "year": 2022
        },
        {
            "authors": [
                "Kihyuk Sohn",
                "David Berthelot",
                "Nicholas Carlini",
                "Zizhao Zhang",
                "Han Zhang",
                "Colin A Raffel",
                "Ekin Dogus Cubuk",
                "Alexey Kurakin",
                "Chun-Liang Li"
            ],
            "title": "Fixmatch: Simplifying semi-supervised learning with consistency and confidence",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Zhenheng Tang",
                "Yonggang Zhang",
                "Shaohuai Shi",
                "Xin He",
                "Bo Han",
                "Xiaowen Chu"
            ],
            "title": "Virtual homogeneity learning: Defending against data heterogeneity in federated learning",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Antti Tarvainen",
                "Harri Valpola"
            ],
            "title": "Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Haobo Wang",
                "Ruixuan Xiao",
                "Yiwen Dong",
                "Lei Feng",
                "Junbo Zhao"
            ],
            "title": "Promix: Combating label noise via maximizing clean sample utility",
            "venue": "arXiv preprint arXiv:2207.10276,",
            "year": 2022
        },
        {
            "authors": [
                "Jianyu Wang",
                "Qinghua Liu",
                "Hao Liang",
                "Gauri Joshi",
                "H Vincent Poor"
            ],
            "title": "Tackling the objective inconsistency problem in heterogeneous federated optimization",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Yidong Wang",
                "Hao Chen",
                "Qiang Heng",
                "Wenxin Hou",
                "Yue Fan",
                "Zhen Wu",
                "Jindong Wang",
                "Marios Savvides",
                "Takahiro Shinozaki",
                "Bhiksha Raj"
            ],
            "title": "Freematch: Self-adaptive thresholding for semi-supervised learning",
            "venue": "In Eleventh International Conference on Learning Representations. OpenReview. net,",
            "year": 2023
        },
        {
            "authors": [
                "Zirui Wang",
                "Yulia Tsvetkov"
            ],
            "title": "Gradient vaccine: Investigating and improving multi-task optimization in massively multilingual models",
            "venue": "In Proceedings of the International Conference on Learning Representations (ICLR),",
            "year": 2021
        },
        {
            "authors": [
                "Xiao-Xiang Wei",
                "Hua Huang"
            ],
            "title": "Balanced federated semi-supervised learning with fairness-aware pseudo-labeling",
            "venue": "IEEE Transactions on Neural Networks and Learning Systems,",
            "year": 2023
        },
        {
            "authors": [
                "Zhirong Wu",
                "Yuanjun Xiong",
                "Stella X Yu",
                "Dahua Lin"
            ],
            "title": "Unsupervised feature learning via nonparametric instance discrimination",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Han Xiao",
                "Kashif Rasul",
                "Roland Vollgraf"
            ],
            "title": "Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms",
            "venue": "arXiv preprint arXiv:1708.07747,",
            "year": 2017
        },
        {
            "authors": [
                "Qizhe Xie",
                "Zihang Dai",
                "Eduard Hovy",
                "Thang Luong",
                "Quoc Le"
            ],
            "title": "Unsupervised data augmentation for consistency training",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Qizhe Xie",
                "Minh-Thang Luong",
                "Eduard Hovy",
                "Quoc V Le"
            ],
            "title": "Self-training with noisy student improves imagenet classification",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Jie Xu",
                "Benjamin S Glicksberg",
                "Chang Su",
                "Peter Walker",
                "Jiang Bian",
                "Fei Wang"
            ],
            "title": "Federated learning for healthcare informatics",
            "venue": "Journal of Healthcare Informatics Research,",
            "year": 2021
        },
        {
            "authors": [
                "Dong Yang",
                "Ziyue Xu",
                "Wenqi Li",
                "Andriy Myronenko",
                "Holger R Roth",
                "Stephanie Harmon",
                "Sheng Xu",
                "Baris Turkbey",
                "Evrim Turkbey",
                "Xiaosong Wang"
            ],
            "title": "Federated semi-supervised learning for covid region segmentation in chest ct using multi-national data from china, italy, japan",
            "venue": "Medical image analysis,",
            "year": 2021
        },
        {
            "authors": [
                "Qiang Yang",
                "Yang Liu",
                "Tianjian Chen",
                "Yongxin Tong"
            ],
            "title": "Federated machine learning: Concept and applications",
            "venue": "ACM Transactions on Intelligent Systems and Technology (TIST),",
            "year": 2019
        },
        {
            "authors": [
                "Zhiqin Yang",
                "Yonggang Zhang",
                "Yu Zheng",
                "Xinmei Tian",
                "Hao Peng",
                "Tongliang Liu",
                "Bo Han"
            ],
            "title": "Fedfed: Feature distillation against data heterogeneity in federated learning",
            "venue": "In Thirty-seventh Conference on Neural Information Processing Systems,",
            "year": 2023
        },
        {
            "authors": [
                "Tianhe Yu",
                "Saurabh Kumar",
                "Abhishek Gupta",
                "Sergey Levine",
                "Karol Hausman",
                "Chelsea Finn"
            ],
            "title": "Gradient surgery for multi-task learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Yonggang Zhang",
                "Mingming Gong",
                "Tongliang Liu",
                "Gang Niu",
                "Xinmei Tian",
                "Bo Han",
                "Bernhard Sch\u00f6lkopf",
                "Kun Zhang"
            ],
            "title": "Adversarial robustness through the lens of causality",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Jianing Zhu",
                "Jiangchao Yao",
                "Tongliang Liu",
                "Jianliang Xu",
                "Bo Han"
            ],
            "title": "Combating exacerbated heterogeneity for robust models in federated learning",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Xiaojin Zhu",
                "Andrew B Goldberg"
            ],
            "title": "Introduction to semi-supervised learning",
            "venue": "Synthesis lectures on artificial intelligence and machine learning,",
            "year": 2009
        },
        {
            "authors": [
                "Xiaojin Zhu",
                "Zoubin Ghahramani",
                "John D Lafferty"
            ],
            "title": "Semi-supervised learning using gaussian fields and harmonic functions",
            "venue": "In Proceedings of the 20th International conference on Machine learning",
            "year": 2003
        },
        {
            "authors": [
                "Weiming Zhuang",
                "Yonggang Wen",
                "Shuai Zhang"
            ],
            "title": "Divergence-aware federated self-supervised learning",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Barret Zoph",
                "Golnaz Ghiasi",
                "Tsung-Yi Lin",
                "Yin Cui",
                "Hanxiao Liu",
                "Ekin Dogus Cubuk",
                "Quoc Le"
            ],
            "title": "Rethinking pre-training and self-training",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Federated learning (FL) (Yang et al., 2019; Kairouz et al., 2021; Li et al., 2021; McMahan et al., 2017; Wang et al., 2020) has gained widespread popularity in machine learning, enabling models to learn from decentralized devices under diverse domains (Li et al., 2019; Xu et al., 2021; Long et al., 2020a). Despite the benefits of FL, obtaining high-quality annotations remains challenging in resource-constrained scenarios, often leading to label deficiency and degraded performance (Jin et al., 2023). In this regard, federated semi-supervised learning (FSSL) (Diao et al., 2022; Liu et al., 2021c; Jeong et al., 2021) has achieved significant improvements in tackling label scarcity by jointly training a global model using labeled and/or unlabeled data.\nAdvanced FSSL methods propose to combine off-the-rack semi-supervised methods (Sohn et al., 2020; Xie et al., 2020a; Berthelot et al., 2020) with FL (McMahan et al., 2017; Li et al., 2020), leveraging the strengths of both approaches like pseudo-labeling (Lee et al., 2013) and teacherstudent models (Tarvainen & Valpola, 2017). These methods typically train a single model on each client using labeled or unlabeled data, following the inspirits of traditional semi-supervised learning. However, the decentralized nature of FL scenarios distinguishes FSSL from traditional semi-supervised learning, where labeled and unlabeled data are on the same device. Namely, clients in FL may have diverse capabilities to label data, leading to label deficiency on many clients (Liu et al., 2021c; Yang et al., 2021; Liang et al., 2022). Training a single model using different objective functions could make gradients on different distributions collide, as depicted in Figure 2(a). Thus, it is urgent to develop an FL-friendly semi-supervised learning framework to tackle label deficiency.\nTo combat label deficiency, we propose a twin-model paradigm, called Twin-sight, to enhance mutual guidance by providing insights from different perspectives of labeled and unlabeled data, adapting\n\u2217Equal contributions. \u2020Correspondence to Bo Han (bhanml@comp.hkbu.edu.hk).\ntraditional semi-supervised learning to FL. In particular, Twin-sight trains a supervised model using a supervised objective function, while training an unsupervised model using an unsupervised objective function. The twin-model paradigm naturally avoids the issue of gradient conflict. Consequently, the interaction between the supervised and unsupervised models plays a crucial role in Twin-sight. Drawing inspiration from traditional semi-supervised learning (Belkin & Niyogi, 2004) from a manifold perspective (Roweis & Saul, 2000), we introduce a neighborhood-preserving constraint to encourage preserving the neighborhood relation among data features extracted by these two models. Consequently, the supervised and unsupervised models can co-guide each other by providing insights from different perspectives of labeled and unlabeled data without gradient conflict.\nThe overview of the proposed Twin-sight can be found in Figure 1. In Twin-sight, the unsupervised objective function, e.g., instance discrimination (Wu et al., 2018)1, does not vary with the presence or absence of labels for the unsupervised model. In contrast, the supervised objective function varies with the presence or absence of labels. For clients with label information, it can be a vanilla objective, e.g., cross-entropy loss. For clients without labels, Twin-sight regards predictions with high confidence as reliable labels to perform supervised learning. In Twin-sight, the constraint remains the same whether labels exist, encouraging the preservation of neighborhood relation (Sarkar et al., 2022; Gao et al., 2023; Pandey et al., 2021). Comprehensive experiments conducted on four standard datasets demonstrate the efficacy of the proposed Twin-sight.\nOverall, our contributions can be summarized as follows:\n\u2022 We point out that the discrepancy between the objective functions of labeled and unlabeled data could cause gradient conflict, posing specific challenges for semi-supervised learning approaches in FL scenarios.\n\u2022 To tackle label deficiency, we propose a twin-model framework, Twin-sight, to tackle gradient conflict in federated learning. Twin-sight trains a supervised model paired with an unsupervised model. Meanwhile, Twin-sight introduces a constraint to make the two models co-guide each other with insights from different perspectives of labeled and unlabeled data by preserving the neighborhood relation of data features.\n1The objective function can refine class-level identification into fine-grained challenges causally (Chalupka et al., 2014; Mitrovic et al., 2020).\n\u2022 We conduct comprehensive experiments under various settings using widely used benchmark datasets. Our experimental results show that Twin-sight outperforms previous methods, achieving state-of-the-art performance."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Federated Learning. Federated learning (FL) enables distributed clients to collaboratively train a global model with privacy-preserving (Kairouz et al., 2021; Ji et al., 2023). However, the performance of federated learning typically suffers from heterogeneity in data distributions, processing capabilities, and network conditions among clients (Lin et al., 2020; Li et al., 2022; Diao et al., 2023; Zhu et al., 2022; Tang et al., 2022). One of the most popular algorithms in FL is FedAvg (McMahan et al., 2017), which aggregates parameters from randomly selected clients to create a global model and achieves convergence after several rounds of communication. A series of works, e.g., FedProx (Li et al., 2020), SCAFFOLD (Karimireddy et al., 2020), is proposed to calibrate the local updating direction. These methods implicitly assume that all clients can label data, which could be violated in many practical scenarios. Some approaches include the sharing of privacy-free information (Tang et al., 2022) or the use of protected features (Yang et al., 2023). These strategies have shown promise in achieving better performance.\nSemi-Supervised Federated Learning (SemiFL). To relax the assumption, SemiFL (Diao et al., 2022) assumes that the server can annotate data, while clients collect data without labels. In SemiFL, selected clients generate pseudo-labels using the global model and then fine-tune the aggregated model using labeled data on the server side. Semi-supervised learning is a well-established approach that has proven to be effective in improving the performance of machine learning models by making use of both labeled and unlabeled data (Zhu et al., 2003; Zhu & Goldberg, 2009). Self-training methods (Xie et al., 2020b; Zoph et al., 2020; Liu et al., 2021b) have emerged as a popular approach for semi-supervised learning, in which a teacher model is trained on labeled data and used to generate pseudo-labels for the remaining unlabeled data. Another significant line of work is based on consistency training (Tarvainen & Valpola, 2017; Xie et al., 2020a). Apart from the above, combining these two methods is effective in achieving improved performance on various benchmark datasets, e.g., MixMatch (Berthelot et al., 2019), FixMatch (Sohn et al., 2020), and RemixMatch (Berthelot et al., 2020). However, the server may fail to collect data due to privacy concerns.\nFederated Semi-Supervised Learning (FSSL). Advanced works assume that the some clients have labeled data (Jin et al., 2023; Liu et al., 2021c), which has garnered significant attention. One stream of research focuses on the fully-labeled clients versus fully-unlabeled clients (Liu et al., 2021c; Yang et al., 2021; Liang et al., 2022), while another body of literature studies the use of partially labeled data at each client (Long et al., 2020b; Lin et al., 2021; Wei & Huang, 2023). For instance, RSCFed (Liang et al., 2022) leverages mean-teacher on fully-unlabeled clients and sub-sample clients for sub-consensus by distance-reweighted model aggregation. This approach comes at the cost of increased communication burden. FedIRM (Liu et al., 2021c) learns the inter-client relationships between different clients using a relation-matching module. However, these methods merely train a single model on labeled and unlabeled data, causing the gradient conflict issue.\nSelf-Supervised Learning. Self-supervised learning is an increasingly popular approach to acquiring meaningful representations without needing explicit labels (He et al., 2020; Chen & He, 2021). Contrastive methods (Wu et al., 2018; Bachman et al., 2019; Misra & Maaten, 2020) have demonstrated state-of-the-art performance, which enforces the similarity of representations between two augmented views of input. One of the predominant methods, SimCLR (Chen et al., 2020), applies InfoNCE (Oord et al., 2018) loss to discriminate positive pairs from numerous negative samples. There is a work (Zhuang et al., 2021) also investigates the federated version of these unsupervised methods. Previous work shows that the instance discrimination task can be regarded as a (more challenging) fine-grained version of the downstream task (Mitrovic et al., 2020). These insightful works inspire us to introduce self-supervised learning into FSSL for processing unlabeled data."
        },
        {
            "heading": "3 METHODOLOGY",
            "text": "In this section, we present our \u201cTwin-sight\u201d framework in detail. Before that, we provide a formal definition of the studied problem (Sec 3.1) and the motivation (Sec 3.2). We then elaborate on the\ntwin-model paradigm (Sec 3.3), outlining the roles and training procedures of the supervised and unsupervised models. Finally, we explore the interaction between these two sights (Sec 3.4)."
        },
        {
            "heading": "3.1 PROBLEM DEFINITION",
            "text": "In general, FL tends to train a global model parameterized by w with K participants collaboratively. In other words, the objective function J (w) of the global model is composed of the local function over all participants\u2019 data distribution:\nmin w J (w) = K\u2211 k=1 \u03b2kJk(wk), (1)\nwhere \u03b2k determines the weight of the k-th client\u2019s objective function. The k-th client possesses a local private dataset denoted by Dk, drawn from the distribution P (Xk, Yk). In FSSL, a typical scenario involves M clients with fully-labeled data, while the remaining T clients have unlabeled data. The set of all clients C = {ck}Kk=1 can be divided into two subsets, CL = {cm}Mm=1 and CU = {ct}Tt=1, corresponding to the clients with labeled and unlabeled data, respectively. The dataset of the m-th client in CL is DLm = {(xim, yim)} Nm i=1 \u223c P (Xm, Ym) and DUt = {(xit)} Nt i=1 \u223c P (Xt) denotes the dataset containing data witout annotation for ct."
        },
        {
            "heading": "3.2 MOTIVATION",
            "text": "In the existing FSSL framework, the local objective function on labeled data can be formulated as:\nJm(wm) := E(x,y)\u223cP (Xm.Ym)\u2113(wm;x, y), (2)\nwhere (Xm, Ym) is the random variable denoting the image Xm and its label Ym and \u2113(\u00b7) is the cross-entropy loss. To leverage unlabeled data, advanced methods (Liang et al., 2022; Liu et al., 2021c; Yang et al., 2021) propose to employ traditional semi-supervised earning techniques such as pseudo-labeling (Lee et al., 2013) and mean-teacher (Tarvainen & Valpola, 2017) in conjunction with a transformation function T(\u00b7). These methods utilize a global model parameterized to utilize unlabeled data on the t-th client:\nJt(wt) := Ex\u223cP (Xt)f(wt;x,T(x)), (3)\nwhere f(\u00b7; \u00b7) denotes a consistency constraint. Therefore, the global objective can be rewritten as:\nmin w J (w) = M\u2211 m=1 \u03b2mJm(w) + T\u2211 t=1 \u03b2tJt(w). (4)\nIn centralized training, this approach can achieve state-of-the-art performance. However, the objective function may cause \u201cclient drift\u201d due to the different objective functions of clients. Specifically, all parameters will be aggregated to construct a global model, even if these models are trained with different objective functions. In practice, aggregating models with different objective functions will cause \u201cclient drift\u201d (Wang et al., 2020). To verify the client drifts, we calculate the similarity between gradients calculated under different objective functions, i.e., Eq. 2 and Eq. 3. The results are shown in Figure 2(a), demonstrating that gradients from these two do not align well, i.e., gradient conflict.\nThe gradient conflict issue is inherently attributed to the decentralized nature of data. Specifically, FL models are trained on labeled or unlabeled data, leading to aggregation with models trained using different objective functions and data distributions."
        },
        {
            "heading": "3.3 TWIN-MODEL PARADIGM",
            "text": "Built upon the aforementioned analysis, we propose to introduce a twin-model paradigm to tackle gradient conflict. Intuitively, we can train a supervised model using labeled data while training an unsupervised model using unlabeled data. Consequently, the main challenge is designing an effective interaction mechanism between these two models, making these two models promote each other by providing insights from different perspectives of labeled and unlabeled data.\nAlgorithm 1 pseudo-code of Twin-sight Server input: communication round R Client k\u2019s input: local epochs E, k-th local dataset Dk\nInitialization: all clients initialize the model w0s,k,w0u,k. Server Executes: for each round r = 1, 2, \u00b7 \u00b7 \u00b7 , R do\nserver random samples a subset of clients Cr \u2286 {1, ...,K}, server communicates wrs,wru to selected clients for each client ck \u2208 Cr in parallel do\nwr+1u,k ,w r+1 s,k \u2190 Local_Training (k,wrs,wru)\nend for wr+1s ,w r+1 \u2190 AGG (wr+1s,k ,w r+1 u,k , ck \u2208 Cr)\nend for\nLocal_Training((k,wrs,wru)): if ck \u2208 CL then wr+1u,k ,w r+1 s,k \u2190SGD update by Eq 10 in E epochs. else if ck \u2208 CU then wr+1u,k ,w r+1 s,k \u2190SGD update by Eq 11 in E epochs. end if Return wr+1u,k ,w r+1 s,k to server\nThe twin-model paradigm has two models: an unsupervised model parameterized with wu and a supervised model parameterized with ws. The unsupervised model is trained with a fine-grained task of a downstream classification task, i.e., instance discrimination:\nmin wu J u(wu) = M\u2211 m=1\n\u03b2mJ um(wu)\ufe38 \ufe37\ufe37 \ufe38 Unsupervised model on fully-labeled clients +\nT\u2211 t=1\n\u03b2tJ ut (wu),\ufe38 \ufe37\ufe37 \ufe38 Unsupervised model\non fully-unlabeled clients\n(5)\nwhere the objective function J u\u00b7 (\u00b7) is the same for all client2:\nJ u\u00b7 (wu) = \u2212 log exp (sim (f(wu;xi), f(wu;xj)) /\u03c4)\u22112N\nk=1 I[k \u0338=i] exp (sim (f(wu;xi), f(wu;xk)) /\u03c4) , (6)\nwhere f(wu; \u00b7) is the unsupervised model and \u03c4 is the temperature hyper-parameter. Thus, the unsupervised model can be trained in a vanilla FL manner.\nSupervised models on clients with labeled data can be trained with a cross-entropy loss Jm(\u00b7). Notably, the label information is invalid on clients sampled from the unlabeled subset CU = {ct}Tt=1. Thus, we introduce a surrogate loss J st (\u00b7) to train the supervised model with unlabeled data on client ct. This can be formulated as:\nmin ws J s(ws) = M\u2211 m=1 \u03b2mJm(ws) + T\u2211 t=1 \u03b2tJ st (ws), (7)\nwhere the surrogate loss replaces the label used in cross-entropy loss with a pseudo label y\u0303 predicted by the supervised model f(ws; \u00b7):\nJ st (ws) := \u2212I [\u03c3(y\u0303) > r]\u03c3(y\u0303) log f(ws;xi), (8) where I(\u00b7) is an indicator function, \u03c3(\u00b7) can select the maximum value for a given vector, and r is a threshold working as a hyper-parameter to select predictions with high confidence. This is because training models using data with low-confidence predictions cause performance degradation, which is consistent with previous work (Wang et al., 2022). Consequently, we can train a supervised model in a vanilla FL manner.\n2Here, we omit the difference induced by distribution discrepancy between clients."
        },
        {
            "heading": "3.4 TWIN-SIGHT INTERACTION",
            "text": "Training two models separately cannot make these two models benefit each other. Therefore, we introduce a Twin-sight loss to complete the Twin-sight framework. The inspiration is drawn from local linear embedding (Roweis & Saul, 2000) and distribution alignment (Zhang et al., 2022), where the features (or embeddings) of the same data should keep the same neighborhood relations under different feature spaces.\nSpecifically, we introduce a constraint to encourage preserving the neighborhood relation among data features extracted by supervised and unsupervised models. The intuition is straightforward that features extracted by the supervised model and the unsupervised model can be drastically different, making it hard to align the feature distributions. Thus, we propose to Twin-sight loss Ja(\u00b7) to align the neighborhood relation among features:\nmin ws,wu\nJa(ws,wu) := d(N(f(ws;x)),N(f(wu;x)), (9)\nwhere d is a certain metric to measure the difference between two matrices, e.g., \u2113F -norm and N(\u00b7) stands for the function used to construct a neighborhood relation. The Twin-sight loss can be used to train both the supervised and unsupervised models in a vanilla FL manner. Consequently, the objective function on labeled data J l(\u00b7)is formulated as:\nJ l(ws,wu) = Jm(ws)\ufe38 \ufe37\ufe37 \ufe38 Supervised model\non fully-labeled clients + \u03bbuJ u(wu)\ufe38 \ufe37\ufe37 \ufe38 Unsupervised model on fully-labeled clients + \u03bbdJa(ws,wu),\ufe38 \ufe37\ufe37 \ufe38 Twin\u2212sight interaction on both models\n(10)\nSimilarly, we can leverage unlabeled data by loss function J u(\u00b7):\nJ u(ws,wu) = J st (ws)\ufe38 \ufe37\ufe37 \ufe38 Supervised model\non fully-unlabeled clients\n+ \u03bbuJ u(wu)\ufe38 \ufe37\ufe37 \ufe38 Unsupervised model\non fully-unlabeled clients\n+ \u03bbdJa(ws,wu),\ufe38 \ufe37\ufe37 \ufe38 Twin\u2212sight interaction\non both models\n(11)\nwhere \u03bbu and \u03bbd is the hypter-parameters to adjust.\nThe overview of the Twin-sight framework is illustrated in Figure 1 and Algorithm 1. According to the framework of Twin-sight, it is also possible to apply Twin-sight to a similar label deficiency scenario where all clients hold data with a portion of it labeled. This superiority is supported by our experiments, as shown in Table 3."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": "To evaluate our method, we have structured this section into four parts: 1) Detailed description of the datasets and baseline methods used in this paper within FSSL (Sec 4.1). 2) The main results that demonstrate the efficacy of our proposed method (Sec 4.2). 3) Extensive evaluations of Twin-sight to another scenario in FSSL, where all clients possess partially labeled data (Sec 4.3)."
        },
        {
            "heading": "4.1 EXPERIMENTAL SETUP",
            "text": "Datasets. In our experiments, we use four popular datasets that have been extensively utilized in FSSL research (Liang et al., 2022; Wei & Huang, 2023) including CIFAR-10 (Krizhevsky et al., 2009), SVHN (Netzer et al., 2011), Fashion-MNIST (FMNIST) (Xiao et al., 2017), and CIFAR-100 (Krizhevsky et al., 2009). The training sets of these four datasets are 50, 000, 73, 257, 60, 000, and 50, 000 respectively. They are partitioned into K clients in federated learning, and we resize all images to 32\u00d7 32 size. Federated Semi-supervised Learning Setting. 1) Data heterogeneity: To simulate data heterogeneity, we partition the dataset across clients using the Latent Dirichlet Sampling (LDA) strategy (Hsu et al., 2019), with \u03b3 in Dir(\u03b3) controlling the label and quantity skewness of the data distribution among clients. In our experiments, we mainly use a severe non-IID setting with \u03b3 = 0.1 (Fig. 2(b) (a) shows the data distribution across 10 clients ), which closely resembles real-world scenarios and is important for evaluating the effectiveness of federated learning algorithms. 2) FSSL: We follow the setting of existing FSSL works (Liang et al., 2022). Specifically, our federated learning (FL) system\ncomprises K clients, among which M have access to fully-labeled training data, and T have only unlabeled data. The proportion of fully-unlabeled clients, represented by the ratio \u03b1 = TK , constitutes a key factor determining the extent of annotation scarcity in Twin-sight, while (1\u2212 \u03b1) = K\u2212TK = M K highlights the degree of label richness across the participating clients.\nBaselines. To verify the performance and robustness of Twin-sight, we compare it against several methods, including the combination of semi-supervised and FL methods, as well as other state-ofthe-art baseline methods in FSSL. 1) FedAvg (McMahan et al., 2017), trained only with labeled data as a lower bound for comparison. 2) FedProx (Li et al., 2020), proposed to mitigate heterogeneous scenarios in FL. 3)FedAvg+FixMatch (McMahan et al., 2017; Sohn et al., 2020), the combination of two excellent methods in the respective fields of federated learning (FL) and semi-supervised learning (SSL. 4) FedProx+FixMatch (Li et al., 2020; Sohn et al., 2020), revise federated learning strategy to fit into heterogeneity. 5) FedAvg+Freematch (McMahan et al., 2017; Wang et al., 2023), vanilla FL method deployed with SOTA semi-supervised framework. 6)FedProx+Freematch (Li et al., 2020; Wang et al., 2023), a combination of two methods too. 7) Fed-Consist (Yang et al., 2021), use consistency loss computed by augmented data. 8) FedIRM (Liu et al., 2021c), a relation matching scheme between fully-labeled clients and fully-unlabeled clients. 9)RSCFed (Liang et al., 2022), randomly sub-sample for sub-consensus.\nImplementation Details. Similar to many works (Tang et al., 2022; Wei & Huang, 2023; Huang et al., 2024), we use Resnet-18 (He et al., 2016) as a backbone feature extractor on all datasets and baselines to ensure a fair comparison. In federated learning, we aggregate weights in a FedAvg (McMahan et al., 2017) manner. In accordance with previous works (Liang et al., 2022), all of our experimental results report on the performance of the global model after R = 500 rounds of training. The server randomly samples a subset of all clients which means |Cr| = 5 when clients number K = 10, namely the sampling rate S = 50%. The random seed in our experiments is 0. We use the SGD optimizer with a learning rate of 0.01, weight decay of 0.0001, and momentum of 0.9 in all of our experiments. The batch size is set to 64 for all datasets."
        },
        {
            "heading": "4.2 MAIN RESULTS",
            "text": "The experimental results for Twin-sight on CIFAR-10, SVHN, FMNIST, and CIFAR-100 are presented in Table 1 and Table 2. The experiments were conducted using the same random seed, with 6 out of 10 clients randomly selected to be fully-unlabeled clients while the remainder were fullylabeled clients, namely \u03b1 = 60%, and we select 5 clients per communication round (S = 50%) in FL system. Overall, the performance of both the baseline methods and Twin-sight is lower than the upper bound of FedAvg. However, our proposed method, \u201cTwin-sight,\u201d demonstrates a significant improvement in performance, outperforming all baselines, indicating successful mitigation of the\nThe performance of FedAvg-Lower Bound is the target accuracy. \u201cRound\u201d refers to the communication round required to reach the target accuracy. \u201cNone\u201d indicates that this method did not attain the target accuracy throughout the entire training period. The bold indicates the best result, while the underlined represents the runner-up.\ngradient conflict. Specifically, our method achieves excellent results on all datasets, with a particularly notable improvement on CIFAR-10.\nDespite its potential advantages, RSCFed did not exhibit superior performance compared to our methods due to the presence of gradient conflict (see Figure 2(a)).However, the combination of FedAvg (McMahan et al., 2017) and Fixmatch (Sohn et al., 2020) or Freematch (Wang et al., 2023) achieved comparable performance in certain scenarios, leveraging two fundamental methods from different fields despite its simplicity. Moreover, FedIRM results in a NaN loss when used in severely skewed label distributions."
        },
        {
            "heading": "4.3 PARTIALLY LABELED DATA SCENARIO",
            "text": "Furthermore, we explore the scenario where all clients have partially labeled data. To quantify the availability of labeled data for each client, we introduce \u03c4 , which represents the labeled data ratio, indicating the proportion of labeled data available. In addition to the vanilla FL method and the combination of FL and Semi-supervised learning (SSL) methods used in the previous setting, we\nincorporate three additional methods specifically designed for this partially labeled data scenario, FedSem (Albaseer et al., 2020), FedSiam (Long et al., 2020b), and FedMatch (Jeong et al., 2021). The results presented in Table 3 highlight the remarkable improvements achieved by Twin-sight in the new scenario, with the exception of the FMNIST dataset. The observed performance difference in the FMNIST dataset could be attributed to the fact that the algorithm\u2019s performance has reached a bottleneck when trained on only 5% of the available data. However, Twin-sight still demonstrates comparable results with other methods on the FMNIST dataset."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "In this work, we present Twin-sight, a novel twin-model paradigm designed to address the challenge of label deficiency in federated learning (FL). There are three key factors contributing to the improvement of Twin-sight. First of all, we decouple the learning objective into two models which avoids gradient conflicts. In the most important part, twin-sight interaction, our unsupervised model conducts an instance classification task which is a fine-grained classification problem. Namely, this task would contribute to the downstream classification tasks (Mitrovic et al., 2020). Moreover, the data, model, and the objective function are consistent among all clients. Lastly, our supervised model conducts a classification task. Furthermore, the data, model, and objective functions are consistent among all clients, except for some unlabelled data paired with pseudo labels.\nLimitation The twin-model paradigm introduces an additional model, which can potentially increase memory and communication overhead in federated learning (FL). As part of our future work, we aim to explore a memory-friendly dual-model paradigm that addresses these concerns.\nFuture works Currently, few existing methods can effectively address multiple FSSL scenarios. Therefore, future research should focus on proposing multi-scenario generalization and robust methods capable of handling FSSL problems in various situations. Furthermore, it is essential to consider communication overhead, computation overhead, and performance in the experimental evaluations to provide diverse solutions that cater to the different requirements of cross-silo and cross-device scenarios."
        },
        {
            "heading": "ETHIC STATEMENT",
            "text": "This paper does not raise any ethical concerns. This study does not involve any human subjects, practices to data set releases, potentially harmful insights, methodologies and applications, potential conflicts of interest and sponsorship, discrimination/bias/fairness concerns, privacy and security issues, legal compliance, and research integrity issues."
        },
        {
            "heading": "REPRODUCIBILITY STATEMENT",
            "text": "To make all experiments reproducible, we have listed all detailed hyper-parameters of each FL algorithm. Due to privacy concerns, we will upload the anonymous link of source codes and instructions during the discussion phase to make it only visible to reviewers."
        },
        {
            "heading": "ACKNOWLEDGMENTS AND DISCLOSURE OF FUNDING",
            "text": "Yonggang Zhang, Zhiqin Yang and Bo Han were supported by the NSFC General Program No. 62376235, Guangdong Basic and Applied Basic Research Foundation No. 2022A1515011652, HKBU Faculty Niche Research Areas No. RC-FNRA-IG/22-23/SCI/04, CCF-Baidu Open Fund, and HKBU CSD Departmental Incentive Scheme. Xinmei Tian was supported in part by NSFC No. 62222117, the Fundamental Research Funds for the Central Universities under contract WK3490000005, and KY2100000117. Nannan Wang was supported in part by the National Natural Science Foundation of China under Grants U22A2096. Tongliang Liu is partially supported by the following Australian Research Council projects: FT220100318, DP220102121, LP220100527, LP220200949, and IC190100031."
        },
        {
            "heading": "A EXPERIMENTAL DETAILS",
            "text": ""
        },
        {
            "heading": "A.1 THE SYMBOLIC REPRESENTATION",
            "text": "In this section, we give a description of the symbol in our paper in Table 4."
        },
        {
            "heading": "A.2 DATA VISUALIZATION",
            "text": "In this section, we present the data distribution of different clients on the SVHN dataset after performing LDA partition with a parameter value of \u03b3 = 0.1. The visualization clearly illustrates that each client predominantly contains samples from a specific class, indicating a significant concentration of data within individual client distributions. We also visualize another dataset CIFAR-10 in Figure 4, which contain both the distribution of clients and samples of fully-labeled client and fully-unlabeled client."
        },
        {
            "heading": "B MORE EXPERIMENTAL RESULTS",
            "text": ""
        },
        {
            "heading": "B.1 HOW DIFFERENT DATA HETEROGENEITY AFFECT TWIN-SIGHT?",
            "text": "In this section, we aim to assess the robustness and generalization capability of FSSL under varying levels of data heterogeneity. In addition to considering the degree of \u03b3 = 0.1 in the main paper, we also explore a more severe heterogeneous scenario \u03b3 = 0.05, similar to what conventional methods have investigated in prior works (Luo et al., 2021; Li et al., 2022). In this experiment, we maintained a consistent sampling rate and communication round as in the previous experiments. Specifically, we applied a sampling rate (S) of 50%, which involved randomly selecting 5 clients out of the total 10 clients. Additionally, we conducted a total of 500 communication rounds (R) during the experiment.\nThe results of the experiments conducted under different data heterogeneity degrees are presented in Table 5 and Table 6. When analyzed in conjunction with the results from Table 1 and Table 2, these findings highlight the impact of data heterogeneity on the performance of the evaluated methods. As the non-iid degree (\u03b3) decreased, indicating a more severe level of data heterogeneity among clients, the performance of all methods showed a decline. However, it is worth noting that our approach demonstrated a slower rate of performance decline compared to the other methods. These results\nsuggest that our method exhibits greater robustness and resilience in the face of increasing data heterogeneity."
        },
        {
            "heading": "B.2 HOW TO DETERMINE THE SELF-SUPERVISED METHOD?",
            "text": "To verify the performance of Twin-sight with different self-supervised methods, namely BYOL and SimSiam in addition to SimCLR, we conducted experiments and analyzed the results shown in Figure 5. The results indicate that Twin-sight exhibits competitive performance regardless of the specific self-supervised method employed. While SimCLR achieved the best performance, Twin-sight still demonstrated strong performance when using BYOL or SimSiam.\nThis suggests that Twin-sight is adaptable to different self-supervised methods and can effectively leverage their benefits within the twin-sight framework. The ability of Twin-sight to achieve favourable results across multiple self-supervised approaches highlights its versatility and robustness in incorporating different self-supervised learning techniques.\nB.3 WHAT WILL HAPPEN WHEN THE SAMPLING RATE (S) IS SMALL?\nIn a real-world federated learning setting, it is common for not all clients to be continuously online due to various factors such as network connectivity issues, power constraints, or intermittent availability. To simulate this on-and-off situation, we randomly select a subset of clients denoted as Cr from the complete set of clients C = {ck}Kk=1. In each communication round, we determine the fraction of clients to be sampled using a parameter called the sampling rate (S). This sampling rate represents the proportion of clients that are randomly selected from the entire client pool C for participation in that specific round.\nIf the available clients are small, what will happen? To investigate the scenario where only a small number of clients are available, we conducted exploratory experiments on the CIFAR-10 dataset using different sampling rates (S). In this case, we reduced the sampling rate from the commonly used 50% to 20% among the 10 available clients with R = 500 communication rounds. By decreasing the sampling rate to 20%, we simulated a scenario where a smaller fraction of clients actively participated in each communication round. This situation reflects scenarios where federated learning is carried out with limited client availability.\nThe results are presented in Table 7. Upon analysis, it is evident that as the sampling rate decreases, which corresponds to a smaller number of clients being sampled in each round, there are noticeable impacts on both the overall performance and the convergence speed of the global model. However, despite the reduced client participation, our method consistently maintains a significant performance gain when compared to other baseline methods. For instance, when transitioning from a 50% to a 40% sampling rate, the performance degradation of our method remains below 1%. This indicates that even with a reduction in client availability, our method effectively mitigates the performance drop, ensuring robust results. Moreover, when the sampling rate decreases to 20%, our method demonstrates a sustained performance level above 60%, further highlighting its robustness and effectiveness.\nB.4 DOES TWIN-SIGHT REMAIN EFFECTIVE ACROSS DIFFERENT NUMBERS OF CLIENTS (K)?\nThe total number of clients in a federated learning (FL) system poses a potential challenge for Twin-sight. As the number of clients increases, each client possesses a smaller amount of data, reflecting the cross-device scenario in the simulated federation. In this setting, different devices have limited data availability, and some devices lack the capability to label data, resulting in unlabeled data for those devices.\nTo assess its effectiveness in scenarios with varying client numbers, we conducted experiments and evaluated the results. The outcomes are presented in Table 8 and Table 9 under CIFAR-10. To simulate a scenario with a large number of clients, we selected 50 clients, of which 60% had no labeled data while the remaining clients had labeled data available. In this setup, we sampled 5 out\nof 50 clients to participate in the FL process. Similarly, we also sampled 10 out of 100 clients to simulate a scenario with a larger client pool (S = 10%).\nIn the scenario with 50 clients from Table 8, our method manages to achieve a performance of 62.2. Despite the reduced amount of data per client, our method demonstrates its effectiveness in leveraging the available labeled data and effectively utilizing the unlabeled data from devices that lack labeling capabilities. However, we can observe from Table 9, the reduction in client data further exacerbates the impact on performance. The limited amount of data available for each client poses a significant challenge, potentially affecting the overall performance of the system."
        },
        {
            "heading": "B.5 HOW DOES THE PERFORMANCE OF TWIN-SIGHT VARY WITH DIFFERENT RATIOS OF UNLABELED CLIENTS?",
            "text": "To evaluate the robustness of our approach, we conducted experiments with varying ratios of unlabeled clients, ranging from \u03b1 = 90% to \u03b1 = 40%. The results, illustrated in Figure 6, demonstrate that the ratios of unlabeled clients have an impact on the performance of Twin-sight. However, our method\nconsistently performs well and surpasses baseline methods in these scenarios. We observed that as the number of unlabeled clients increases, the overall performance tends to degrade. Despite this, Twin-sight continues to outperform other methods even in the presence of a higher ratio of unlabeled clients. Furthermore, we noticed that Twin-sight achieves larger performance gains compared to other methods when the number of unlabeled clients is relatively small. This highlights the importance of labeled data in guiding the twin-sight interaction and improving overall performance. More experimental and ablation results can be found in Appendix B."
        },
        {
            "heading": "B.6 DOES THE RANDOM SEED AFFECT ROBUSTNESS?",
            "text": "In this section, we aim to investigate the impact of different random seeds on the overall performance of the federated system. Random seed is essential for model initialization as well as client selection in each round, both of which can influence the final results. To assess the robustness of Twin-sight in the face of such variability, we select three different random seeds for each experiment and conduct tests accordingly. These experiments are conducted under \u03b3 = 0.1,K = 10 with sampling rate S = 50%. The results are reported in Table 10. Notably, even when considering three random seeds, Twin-sight consistently outperforms other baseline methods across all four datasets.\nB.7 CAN INCREASING THE NUMBER OF COMMUNICATION ROUNDS (R) IMPROVE PERFORMANCE?\nThe performance of a federated system is jointly influenced by the number of communication rounds R and the sampling rate S, as they determine the necessary communication bandwidth. To evaluate the effects of different communication round settings on performance improvement, we conducted experiments in two distinct scenarios. In one scenario, a subset of clients possessed fully-labeled data, while in the other scenario, all clients had partial labels for their data. The primary objective was to achieve a balance between enhancing performance and managing communication volume effectively.\nAs depicted in the Figure 7 and Figure 8, it is self-evident that when the number of rounds is relatively small, there is a substantial improvement in performance with each increment of 100 rounds. However, beyond 500 rounds, the performance improvement for most methods becomes less pronounced. Consequently, all the experiments presented in our paper were conducted within a maximum of 500 rounds. Our method exhibits rapid convergence and achieves excellent performance within the first 300 rounds. This observation demonstrates that Twin-sight not only enhances performance but also accelerates model convergence and reduces the number of required communication rounds. After 500 rounds, both FedAvg-Lower Bound and FedProx-Lower Bound exhibit substantial performance improvements (in Figure 8). This improvement can be ascribed to the slower convergence speed of the two lower-bound methods in this dataset, primarily due to the limited availability of labeled data. As the iterations progress, the performance gradually improves, and significant enhancements become evident after several hundred rounds of iterations."
        },
        {
            "heading": "C DETAILED EXPLANATIONS OF TWIN-SIGHT",
            "text": "In this section, we give more details and explanations of our method. Specifically, we introduce the gradient conflict problem in our paper, the self-supervised method we investigate and the twin-sight interaction.\nGradient conflict v.s. client drift: Gradient conflict is defined as the inconsistency between gradients. The inconsistency may result from multi-objectives, e.g., multi-task learning (MTL) (Yu et al., 2020; Liu et al., 2021a; Wang & Tsvetkov, 2021). The objective of MTL is the average loss of all kinds of tasks which typically leads to gradient conflict. According to the definition of gradient conflict, two gradients are considered conflicting if they point away from one another, i.e., have a negative cosine similarity. This phenomenon also can be observed in Figure 2(a) in our paper. The gradient conflict phenomenon has some drawbacks: i) this different loss may have various scales of gradient, and the largest one dominates the update direction; ii) The averaged objective can be quite unfavourable to a specific task\u2019s performance (Liu et al., 2021a). Client drift is caused by data heterogeneity while gradient conflict is typically induced by multiple objectives. In existing FSSL methods, the \"client drift\" and \"gradient conflicts\" often exist at the same time. In the context of label deficiency, gradient conflict may be attributed to both the samples and objectives. Specifically, partial samples have annotations and the objective on lableled and unlableled data are typically different. Advanced methods in FSSL typically use two different objective functions to train the local classifier, one for fully-labeled clients with cross-entropy loss and another for fully-unlabeled clients by unsupervised method. In this context, these methods suffer from gradient conflicts naturally.\nThe self-supversied method: In twin-sight, the J u(wu) in Eq.(6) refers to the unsupervised model parameterized by wu. And the sim (f(wu;xi), f(wu;xj)) denote the dot product between \u21132 normalized embedding of data i and j. To investigate different unsupervised methods in twin-sight, we perform some prevailing unsupervised methods, such as SimCLR (Chen et al., 2020), BYOL (Grill et al., 2020) and SimSiam (Chen & He, 2021), and SimCLR is chosen to be the backbone of J u(wu). The results of different self-supervised methods are reported in Appendix B.2.\nThe function N(\u00b7) and metric d(\u00b7): The function N(\u00b7) is leveraged to quantify the relationship between different samples in a mini-batch. Specifically, N(\u00b7) is the matrix calculating the distance among samples in the feature space, i.e., outputs of supervised model Zs \u2208 Rn\u00d7d and those of unsupervised model Zu \u2208 Rn\u00d7d with d being the dimension and n the batch size. Using these outputs, we can employ the matrix M \u2208 Rn\u00d7nto represents the relationships among samples, i.e., N(f(ws,x)) := Ms = Zs \u00b7 ZTs , N(f(wu,x)) := Mu = Zu \u00b7 ZTu . Meanwhile, the metric d(\u00b7) is realized as the mean square error, which is formulated as: d(N(f(ws,x)),N(f(wu,x))) = \u2225Ms \u2212Mu\u22252."
        }
    ],
    "title": "EXTREMELY LABEL DEFICIENCY",
    "year": 2024
}