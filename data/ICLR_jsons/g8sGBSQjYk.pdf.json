{
    "abstractText": "Second-order optimization has been developed to accelerate the training of deep neural networks and it is being applied to increasingly larger-scale models. In this study, towards training on further larger scales, we identify a specific parameterization for second-order optimization that promotes feature learning in a stable manner even if the network width increases significantly. Inspired by a maximal update parametrization, we consider a one-step update of the gradient and reveal the appropriate scales of hyperparameters including random initialization, learning rates, and damping terms. Our approach covers two major second-order optimization algorithms, K-FAC and Shampoo, and we demonstrate that our parametrization achieves higher generalization performance in feature learning. In particular, it enables us to transfer the hyperparameters across models with different widths.",
    "authors": [],
    "id": "SP:d7e1d48f9cd7ce0358500e8adac4a33c720b46ce",
    "references": [
        {
            "authors": [
                "S. Amari"
            ],
            "title": "Natural Gradient Works Efficiently in Learning",
            "venue": "Neural Computation,",
            "year": 1998
        },
        {
            "authors": [
                "Ehsan Amid",
                "Rohan Anil",
                "Manfred Warmuth"
            ],
            "title": "Locoprop: Enhancing backprop via local loss optimization",
            "venue": "In International Conference on Artificial Intelligence and Statistics,",
            "year": 2022
        },
        {
            "authors": [
                "Rohan Anil",
                "Vineet Gupta",
                "Tomer Koren",
                "Kevin Regan",
                "Yoram Singer"
            ],
            "title": "Scalable Second Order Optimization for Deep Learning",
            "year": 2002
        },
        {
            "authors": [
                "Frederik Benzing"
            ],
            "title": "Gradient descent on neurons and its link to approximate second-order optimization",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Lucas Beyer",
                "Xiaohua Zhai",
                "Am\u00e9lie Royer",
                "Larisa Markeeva",
                "Rohan Anil",
                "Alexander Kolesnikov"
            ],
            "title": "Knowledge distillation: A good teacher is patient and consistent",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Blake Bordelon",
                "Cengiz Pehlevan"
            ],
            "title": "Self-consistent dynamical field theory of kernel evolution in wide neural networks",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Aleksandar Botev",
                "James Martens"
            ],
            "title": "KFAC-JAX, 2022. URL https://github.com/ google-deepmind/kfac-jax",
            "year": 2022
        },
        {
            "authors": [
                "Tianle Cai",
                "Ruiqi Gao",
                "Jikai Hou",
                "Siyu Chen",
                "Dong Wang",
                "Di He",
                "Zhihua Zhang",
                "Liwei Wang"
            ],
            "title": "Gram-gauss-newton method: Learning overparameterized neural networks for regression problems",
            "year": 1905
        },
        {
            "authors": [
                "Lenaic Chizat",
                "Edouard Oyallon",
                "Francis Bach"
            ],
            "title": "On lazy training in differentiable programming",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Emily Dinan",
                "Sho Yaida",
                "Susan Zhang"
            ],
            "title": "Effective theory of transformers at initialization",
            "year": 2023
        },
        {
            "authors": [
                "Ning Ding",
                "Qiaosen Wang",
                "Yulin Chen",
                "Pengjun Xie",
                "Zhiyuan Liu",
                "Hai-Tao Zheng",
                "Maosong Sun"
            ],
            "title": "Applying second order optimization to deep transformers with parameter-efficient tuning",
            "venue": "OpenReview,",
            "year": 2023
        },
        {
            "authors": [
                "Kai-Xin Gao",
                "Xiaolei Liu",
                "Zheng-Hai Huang",
                "Min Wang",
                "Zidong Wang",
                "Dachuan Xu",
                "F. Yu"
            ],
            "title": "A trace-restricted Kronecker-factored approximation to natural gradient",
            "venue": "In AAAI Conference on Artificial Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Mario Geiger",
                "Leonardo Petrini",
                "Matthieu Wyart"
            ],
            "title": "Perspective: A phase diagram for deep learning unifying jamming, feature learning and lazy training",
            "year": 2012
        },
        {
            "authors": [
                "Mario Geiger",
                "Leonardo Petrini",
                "Matthieu Wyart"
            ],
            "title": "Landscape and training regimes in deep learning",
            "venue": "Physics Reports,",
            "year": 2021
        },
        {
            "authors": [
                "Roger Grosse",
                "James Martens"
            ],
            "title": "A Kronecker-factored approximate fisher matrix for convolution layers",
            "venue": "In International Conference on Machine Learning,",
            "year": 2016
        },
        {
            "authors": [
                "Vineet Gupta",
                "Tomer Koren",
                "Yoram Singer"
            ],
            "title": "Shampoo: Preconditioned stochastic tensor optimization",
            "venue": "In International Conference on Machine Learning,",
            "year": 2018
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Shun ichi Amari",
                "Jimmy Ba",
                "Roger Baker Grosse",
                "Xuechen Li",
                "Atsushi Nitanda",
                "Taiji Suzuki",
                "Denny Wu",
                "Ji Xu"
            ],
            "title": "When does preconditioning help or hurt generalization",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Gaurav Iyer",
                "Boris Hanin",
                "David Rolnick"
            ],
            "title": "Maximal initial learning rates in deep relu networks",
            "venue": "In International Conference on Machine Learning,",
            "year": 2023
        },
        {
            "authors": [
                "Arthur Jacot",
                "Franck Gabriel",
                "Cl\u00e9ment Hongler"
            ],
            "title": "Neural tangent kernel: Convergence and generalization in neural networks",
            "venue": "Advances in neural information processing systems,",
            "year": 2018
        },
        {
            "authors": [
                "Ryo Karakida",
                "Kazuki Osawa"
            ],
            "title": "Understanding approximate fisher information for fast convergence of natural gradient descent in wide neural networks",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Yann LeCun",
                "L\u00e9on Bottou",
                "Genevieve B Orr",
                "Klaus-Robert M\u00fcller"
            ],
            "title": "Efficient backprop",
            "venue": "In Neural networks: Tricks of the trade,",
            "year": 1998
        },
        {
            "authors": [
                "Jaehoon Lee",
                "Yasaman Bahri",
                "Roman Novak",
                "Samuel S Schoenholz",
                "Jeffrey Pennington",
                "Jascha Sohl-Dickstein"
            ],
            "title": "Deep neural networks as Gaussian processes",
            "venue": "ICLR 2018,",
            "year": 2018
        },
        {
            "authors": [
                "Xi-Lin Li"
            ],
            "title": "Preconditioned stochastic gradient descent",
            "venue": "IEEE transactions on neural networks and learning systems,",
            "year": 2017
        },
        {
            "authors": [
                "Etai Littwin",
                "Greg Yang"
            ],
            "title": "Adaptive optimization in the $\\infty$-width limit",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Tao Luo",
                "Zhi-Qin John Xu",
                "Zheng Ma",
                "Yaoyu Zhang"
            ],
            "title": "Phase diagram for two-layer ReLU neural networks at infinite-width limit",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2021
        },
        {
            "authors": [
                "James Martens"
            ],
            "title": "New insights and perspectives on the natural gradient method",
            "venue": "Journal of Machine Learning Research,",
            "year": 2020
        },
        {
            "authors": [
                "James Martens",
                "Roger Grosse"
            ],
            "title": "Optimizing neural networks with Kronecker-factored approximate curvature",
            "venue": "In International conference on machine learning,",
            "year": 2015
        },
        {
            "authors": [
                "James Martens",
                "Jimmy Ba",
                "Matt Johnson"
            ],
            "title": "Kronecker-factored curvature approximations for recurrent neural networks",
            "venue": "In International Conference on Learning Representations,",
            "year": 2018
        },
        {
            "authors": [
                "Tomas Mikolov",
                "Kai Chen",
                "Greg Corrado",
                "Jeffrey Dean"
            ],
            "title": "Efficient estimation of word representations in vector space",
            "year": 2013
        },
        {
            "authors": [
                "Kazuki Osawa",
                "Satoki Ishikawa",
                "Rio Yokota",
                "Shigang Li",
                "Torsten Hoefler"
            ],
            "title": "ASDL: A unified interface for gradient preconditioning in PyTorch",
            "year": 2023
        },
        {
            "authors": [
                "Kazuki Osawa",
                "Shigang Li",
                "Torsten Hoefler"
            ],
            "title": "Pipefisher: Efficient training of large language models using pipelining and fisher information matrices",
            "venue": "Proceedings of Machine Learning and Systems,",
            "year": 2023
        },
        {
            "authors": [
                "Daniel Park",
                "Jascha Sohl-Dickstein",
                "Quoc Le",
                "Samuel Smith"
            ],
            "title": "The effect of network width on stochastic gradient descent and generalization: an empirical study",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Razvan Pascanu",
                "Yoshua Bengio"
            ],
            "title": "Revisiting natural gradient for deep networks",
            "venue": "ICLR 2014,",
            "year": 2014
        },
        {
            "authors": [
                "J Gregory Pauloski",
                "Zhao Zhang",
                "Lei Huang",
                "Weijia Xu",
                "Ian T Foster"
            ],
            "title": "Convolutional neural network training with distributed K-FAC",
            "venue": "In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis,",
            "year": 2020
        },
        {
            "authors": [
                "J Gregory Pauloski",
                "Qi Huang",
                "Lei Huang",
                "Shivaram Venkataraman",
                "Kyle Chard",
                "Ian Foster",
                "Zhao Zhang"
            ],
            "title": "Kaisa: an adaptive second-order optimizer framework for deep neural networks",
            "venue": "In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis,",
            "year": 2021
        },
        {
            "authors": [
                "J Gregory Pauloski",
                "Lei Huang",
                "Weijia Xu",
                "Kyle Chard",
                "Ian T Foster",
                "Zhao Zhang"
            ],
            "title": "Deep neural network training with distributed K-FAC",
            "venue": "IEEE Transactions on Parallel and Distributed Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Kaare Brandt Petersen",
                "Michael Syskind Pedersen"
            ],
            "title": "The matrix cookbook",
            "venue": "Technical University of Denmark,",
            "year": 2008
        },
        {
            "authors": [
                "Alec Radford",
                "Jeffrey Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "venue": "OpenAI blog,",
            "year": 2019
        },
        {
            "authors": [
                "Vaishaal Shankar",
                "Alex Fang",
                "Wenshuo Guo",
                "Sara Fridovich-Keil",
                "Jonathan Ragan-Kelley",
                "Ludwig Schmidt",
                "Benjamin Recht"
            ],
            "title": "Neural kernels without tangents",
            "venue": "In International conference on machine learning,",
            "year": 2020
        },
        {
            "authors": [
                "Shaohuai Shi",
                "Lin Zhang",
                "Bo Li"
            ],
            "title": "Accelerating distributed K-FAC with smart parallelism of computing and communication tasks",
            "venue": "IEEE 41st International Conference on Distributed Computing Systems (ICDCS),",
            "year": 2021
        },
        {
            "authors": [
                "Mohammad Shoeybi",
                "Mostofa Patwary",
                "Raul Puri",
                "Patrick LeGresley",
                "Jared Casper",
                "Bryan Catanzaro"
            ],
            "title": "Megatron-lm: Training multi-billion parameter language models using model parallelism",
            "year": 1909
        },
        {
            "authors": [
                "Nikhil Vyas",
                "Alexander Atanasov",
                "Blake Bordelon",
                "Depen Morwani",
                "Sabarish Sainathan",
                "Cengiz Pehlevan"
            ],
            "title": "Feature-learning networks are consistent across widths at realistic scales",
            "year": 2023
        },
        {
            "authors": [
                "Ross Wightman"
            ],
            "title": "Pytorch image models. https://github.com/rwightman/ pytorch-image-models, 2019",
            "year": 2019
        },
        {
            "authors": [
                "Blake Woodworth",
                "Suriya Gunasekar",
                "Jason D Lee",
                "Edward Moroshko",
                "Pedro Savarese",
                "Itay Golan",
                "Daniel Soudry",
                "Nathan Srebro"
            ],
            "title": "Kernel and rich regimes in overparametrized models",
            "venue": "In Conference on Learning Theory,",
            "year": 2020
        },
        {
            "authors": [
                "Mitchell Wortsman",
                "Peter J. Liu",
                "Lechao Xiao",
                "Katie Everett",
                "Alex Alemi",
                "Ben Adlam",
                "John D. CoReyes",
                "Izzeddin Gur",
                "Abhishek Kumar",
                "Roman Novak",
                "Jeffrey Pennington",
                "Jascha Sohl-dickstein",
                "Kelvin Xu",
                "Jaehoon Lee",
                "Justin Gilmer",
                "Simon Kornblith"
            ],
            "title": "Small-scale proxies for large-scale transformer training instabilities",
            "year": 2023
        },
        {
            "authors": [
                "Lechao Xiao",
                "Yasaman Bahri",
                "Jascha Sohl-Dickstein",
                "Samuel S Schoenholz",
                "Jeffrey Pennington"
            ],
            "title": "Dynamical isometry and a mean field theory of CNNs: How to train 10,000-layer vanilla convolutional neural networks",
            "venue": "In Proceedings of International Conference on Machine Learning (ICML),",
            "year": 2018
        },
        {
            "authors": [
                "Greg Yang"
            ],
            "title": "Tensor programs II: Neural tangent kernel for any architecture",
            "year": 2006
        },
        {
            "authors": [
                "Greg Yang",
                "Edward J. Hu"
            ],
            "title": "Tensor programs iv: Feature learning in infinite-width neural networks",
            "venue": "In Proceedings of the 38th International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Greg Yang",
                "Etai Littwin"
            ],
            "title": "Tensor programs IVb: Adaptive optimization in the infinite-width limit",
            "year": 2023
        },
        {
            "authors": [
                "Greg Yang",
                "Edward J Hu",
                "Igor Babuschkin",
                "Szymon Sidor",
                "Xiaodong Liu",
                "David Farhi",
                "Nick Ryder",
                "Jakub Pachocki",
                "Weizhu Chen",
                "Jianfeng Gao"
            ],
            "title": "Tuning large neural networks via zero-shot hyperparameter transfer",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Guodong Zhang",
                "James Martens",
                "Roger B Grosse"
            ],
            "title": "Fast convergence of natural gradient descent for over-parameterized neural networks",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2019
        },
        {
            "authors": [
                "Guodong Zhang",
                "Chaoqi Wang",
                "Bowen Xu",
                "Roger Grosse"
            ],
            "title": "Three mechanisms of weight decay regularization",
            "venue": "In International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "Guodong Zhang",
                "Aleksandar Botev",
                "James Martens"
            ],
            "title": "Deep learning without shortcuts: Shaping the kernel with tailored rectifiers",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Lin Zhang",
                "Shaohuai Shi",
                "Wei Wang",
                "Bo Li"
            ],
            "title": "Scalable K-FAC training for deep neural networks with distributed preconditioning",
            "venue": "IEEE Transactions on Cloud Computing,",
            "year": 2022
        },
        {
            "authors": [
                "Lin Zhang",
                "Shaohuai Shi",
                "Bo Li"
            ],
            "title": "Accelerating distributed K-FAC with efficient collective communication and scheduling",
            "venue": "In IEEE INFOCOM 2023-IEEE Conference on Computer Communications,",
            "year": 2023
        },
        {
            "authors": [
                "Jacot"
            ],
            "title": "For the random initialization, these two matrices are well known as intermediate components of the neural tangent kernel studies",
            "venue": "Karakida",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Second-order optimization has been attracting considerable interest in improving the training efficiency of neural networks (Amari, 1998; Pascanu & Bengio, 2014). It accelerates the convergence of gradient dynamics (Martens & Grosse, 2015; Gupta et al., 2018; Li, 2017) and can optimize neural networks that are especially hard to train due to highly distorted parameter landscape (Martens et al., 2018; Zhang et al., 2022a; Beyer et al., 2022). Because of these successes in improving training efficiency, it is increasingly being applied to even larger-scale models year by year (Osawa et al., 2023b; Pauloski et al., 2021; Shi et al., 2021; Zhang et al., 2023; 2022b; Anil et al., 2021).\nIn general, gradient methods depend on some hyper-parameters (HPs), including a learning rate, and we need careful HP tuning to achieve better performance. The most straightforward approach is to train the model multiple times and search for better HP, but for large-scale models, the computational cost is high even for a single training, making it very challenging to find it. In particular, second-order optimization methods possess not only a learning rate but also a damping term which requires careful tuning (Pascanu & Bengio, 2014; Martens, 2020).\nHow can we obtain quantitative insight into appropriate settings of HP that hold on large scales? Usually, an appropriate scale of HPs depends on the model size and we need to scale them up or down depending on the network width or depth (Park et al., 2019; Iyer et al., 2023; Dinan et al., 2023). One approach to obtaining a robust scale of HPs that universally works in large models is to consider a large limit of the model size (Schoenholz et al., 2017; Lee et al., 2018; Xiao et al., 2018; Luo et al., 2021). In particular, to obtain a preferable HP that works in first-order gradient descent, Yang & Hu (2021) proposed Maximum Update Parameterization (MUP; \u00b5P). They analyzed an infinite width limit of neural networks and successfully identified HPs that prevent the parameter update from vanishing or exploding and ensured stable progress of training, which is independent of the width. The \u00b5P also enables us to avoid the lazy regime, where the model parameters remain sufficiently close to the initialization and allow feature learning at the maximum scale of the update in all layers. Moreover, Yang et al. (2021) have demonstrated that since the \u00b5P works effectively towards the infinite width, we can re-use the HP including a learning rate obtained in a smaller model to the large-scale one. To date, however, these studies have been limited to first-order and entry-wise optimization (Yang & Littwin, 2023).\nIn this work, we consider an infinite width limit and propose a \u00b5P for second-order optimization including two major methods; K-FAC (Martens & Grosse, 2015) and Shampoo (Gupta et al., 2018). Our main contributions can be summarized as follows:\n\u2022 We consider a one-step update of the second-order optimization methods in the infinite-width limit and reveal the HPs (i.e., scales of random initialization, learning rates, damping terms) that allow the \u00b5P (in Section 4.1). We especially clarify that the stable feature learning requires specific scales of learning rates; K-FAC works for constant learning rates whereas Shampoo requires scaling depending on the width. Regarding the damping terms, we find that a classical heuristic scaling of Shampoo satisfies the \u00b5P condition while that of K-FAC requires re-scaling (in Section 4.2).\n\u2022 In practice, the last layer\u2019s weight is sometimes initialized not by random initialization but by zero. By carefully considering the one-step update, we find that while the zero initialization allows feature learning in the usual first-order gradient, it can cause an approach to the network parameter corresponding to the neural network Gaussian process (NNGP) in the case of K-FAC (in Section 4.3). This can be regarded as a novel implicit bias specific to K-FAC that appears in the infinite-width limit.\n\u2022 We empirically verify the effectiveness of our proposed parameterization in the training of various neural networks. In particular, it enables us to transfer optimal learning rates and damping terms from narrow models to wider ones (in Section 5.2, 5.3).\nThus, this work provides quantitative insights that will serve as a foundation for scaling second-order optimization towards the learning of even larger models in the future."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Feature learning: In general, it is highly non-trivial that the large limit of the model allows stable learning under the widely-used standard parametrization (SP). In the infinite width limit, we may have unstable dynamics (i.e., vanishing/exploding gradient depending on the width) or, more non-trivially, the lazy regime (a.k.a. neural tangent kernel regime) (Chizat et al., 2019; Jacot et al., 2018). While the learning dynamics in the lazy regime progress in a stable manner, the parameters remain sufficiently close to the initialization, and the network is essentially approximated by a linear model. This means that no feature learning appears, thus there is growing interest in under what conditions feature learning progresses outside of the lazy regime (Woodworth et al., 2020; Geiger et al., 2021; Luo et al., 2021; Bordelon & Pehlevan, 2022). Based on an order evaluation of parameter updates in the middle of training, Yang & Hu (2021) proposed \u00b5P for stochastic gradient descent (SGD) that realizes feature learning in the infinite-width limit. Some experimental studies demonstrated the utility of \u00b5P Yang et al. (2021); Vyas et al. (2023). The theory is also generalized to entry-wise adaptive gradient methods including Adam (Littwin & Yang, 2023). The second-order optimization does not belong to the class analyzed in these existing studies, and thus the current work is the first to challenge this problem. Note that the second-order optimization in the lazy regime has been investigated by some work (Zhang et al., 2019a; Cai et al., 2019; Karakida & Osawa, 2020).\nSecond-order optimization: The preconditioned gradient can speed up the convergence of training dynamics. Natural gradient descent (NGD) (Amari, 1998) is a classical example of second-order optimization which preconditions the gradient by the inverse of the Fisher information matrix. However, since the computation of the inverse is computationally demanding, some approximation is required. K-FAC (Martens & Grosse, 2015; Grosse & Martens, 2016) is such an approximation for deep neural networks and it has been frequently employed for training large-scale models where acceleration is particularly important (Osawa et al., 2023b; Pauloski et al., 2020; 2021; Shi et al., 2021; Zhang et al., 2023; 2022b). Shampoo is another commonly used second-order optimization method (Gupta et al., 2018) and achieves fast convergence (Anil et al., 2021). Note that second-order optimization methods contain a damping term. Careful selection of such HPs is known to be important for the success of training (Martens, 2020; Zhang et al., 2019b; Gao et al., 2020)."
        },
        {
            "heading": "3 PRELIMINARIES",
            "text": "This section explains the second-order optimization in L-layered fully-connected neural networks:\nul = Wlhl\u22121 + bl, hl = \u03d5 (ul) (l = 1, ..., L), (1)\nwhere we define weight matrices Wl \u2208 RMl\u00d7Ml\u22121 , bias terms bl, and activations hl \u2208 RMl . We set the width of the hidden layer to Ml = M (l = 1, ..., L\u2212 1) for clarity, but this does not lose the\ngenerality of the following analysis. \u03d5(\u00b7) is a differentiable and polynomially-bounded activation function and theoretical works in \u00b5P usually assume either Tanh function or \u03c3-GELU function if necessary (Yang & Hu, 2021). Let (xi,yi) be a pair of input and target training sample. For simplicity, we consider the mean squared loss function for a one-dimensional target:\nL(\u03b8) = 1 n n\u2211 i=1 \u2225yi \u2212 f\u03b8(xi)\u22252 \u2208 R, (2)\nwhere \u03b8 denotes a vector of all parameters and f\u03b8 = uL is the output of the deep neural network. It is straightforward to generalize the following results to the cases of multi-classes and the cross-entropy loss as mentioned in Section A.5."
        },
        {
            "heading": "3.1 OVERVIEW OF SECOND-ORDER OPTIMIZATION",
            "text": "Second-order optimization is an algorithm that updates parameters by a preconditioned gradient: \u03b8t+1 = \u03b8t \u2212 \u03b7 (C(\u03b8t) + \u03c1I)\u22121\u2207\u03b8tL(\u03b8t), where \u03b7 is a learning rate, C(\u03b8) is the curvature matrix and \u03c1 is the damping term. Usually, this inverse is computationally demanding and hard to use. Therefore, the following seminal works have introduced smaller preconditioners for each layer and updated rules in a matrix form.\nK-FAC: Natural gradient descent (NGD) is the case where C is given by the Fisher information matrix. K-FAC approximates this C by the Kronecker product of two matrices (Martens & Grosse, 2015; Grosse & Martens, 2016). Its update rule in the matrix form is given by\nWl,t+1 = Wl,t \u2212 \u03b7(Bl + \u03c1BlI)\u2212eB\u2207WlL(\u03b8t)(Al\u22121 + \u03c1Al\u22121I)\u2212eA , (3)\nwhere eA = eB = 1. The preconditioning matrices are given by Bl = E[\u03b4l\u03b4l\u22a4] and Al = E[hlhl\u22a4] where \u03b4l = \u2207ulf\u03b8 and E[\u00b7] is an average over training samples. General exponents eA and eB are introduced here because some work only considers to use a part of preconditioners like (eA, eB) = (1, 0) (Benzing, 2022; Amid et al., 2022). The size of damping terms is usually determined in a heuristic manner as mentioned in Section 6.3.\nShampoo: Gupta et al. (2018) proposed the following update rule as a second-order optimization algorithm;\nWl,t+1 = Wl,t \u2212 \u03b7(Ll + \u03c1LlI)\u2212e/2\u2207WlL(\u03b8t)(Rl\u22121 + \u03c1Rl\u22121I)\u2212e/2, (4)\nwhere Ll = E[\u03b4lhl\u22121\u22a4hl\u22121\u03b4l\u22a4] and Rl = E[hl\u22121\u03b4l\u22a4\u03b4lhl\u22121\u22a4] with \u03b4l = \u2207ulL. In Shampoo, e = 1/2 is applied. If we neglect the non-diagonal entries of the preconditioners, this is similar to Adam and AdaGrad."
        },
        {
            "heading": "3.2 ABC-PARAMETERIZATION",
            "text": "ABC-parameterization scales parameters by the width as follows (Yang & Hu, 2021):\nWl = wl/M al , wl \u223c N (0, \u03c3\u20322/M2bl), \u03b7l = \u03b7\u2032l/M cl , (5)\nThe \u00b5P is an abc-parametrization that induces feature learning in every layer in a model with infinite widths. In short, the previous work characterizes feature learning by\n\u2206hl := hl,t \u2212 hl,0 = \u0398(1)1, (6)\nwhere \u0398(\u00b7) denotes the Big Theta notation for the order evaluation with respect to the width. Note that for the lazy regime, we have \u2206hl = o(1) in hidden layers. The previous work found that the feature learning (6) appears for some specific conditions. In particular, the following condition of Wl updated maximally plays a fundamental role:\n\u2206Wlhl\u22121 = \u0398(1). (7)\n1Precisely speaking, this is feature learning (Definition H.9) especially satisfying a stability condition (Theorem H.6) in Yang & Hu (2021). Note that as in the previous work, the last equality represents the coordinate size where v = \u0398(Ma) means \u221a \u2225v\u22252/M = \u0398(Ma) for v \u2208 RM\nYang & Hu (2021) analyzed feature learning in the one-step gradient under this condition. Note that they also obtain a mathematical expression of forward and backward propagated signals for the first-order gradient at general time steps by the Tensor Program. For the derivation of the \u00b5P, we focus only on the first one-step gradient as is explained in Section A.2 of Appendices.\nIn the following sections, we will set al = 0 for all layers in the same way as in the first-order case (Yang et al., 2021). The previous work has demonstrated that the \u00b5P of the first-order gradient is scale-invariant to the constant shift (a, b, c)\u2190 (a, b, c) + (k,\u2212k,\u22122k). As we will show later, the \u00b5P of second-order optimization also has this indeterminacy and we can eliminate it by al = 0."
        },
        {
            "heading": "4 PREFERABLE SCALING OF HPS IN SECOND-ORDER OPTIMIZATION",
            "text": "4.1 \u00b5P FOR SECOND-ORDER OPTIMIZATION\nIn this section, we derive the \u00b5P by considering the first one-step update of second-order optimization. We suppose that the damping term \u03c1X (X = {A,B,L,R}) satisfies\n\u03c1X = \u03c1 \u2032 X/M dX , (8) with a positive constant \u03c1\u2032X . For simplicity, suppose a one-step update from the initialization where the gradient and all preconditioners are computed on the same samples (Assumption A.5). We also suppose common assumptions used in \u00b5P for the first-order gradient (Assumptions A.3,A.4). When the eigenvalues of the preconditioning matrices have an equal width-dependent scale to the damping term (e.g., \u03c1A = \u0398(\u2225A\u22252)), we say that the second-order optimization is valid (Definition A.6). Then, we obtain the following. Proposition 4.1 (\u00b5P of second-order parameterization). Consider the first one-step update of K-FAC and Shampoo in the infinite width limit. The second-order optimization becomes valid for\ndAl = { \u22121 1 < l \u2264 L 0 l = 1 , dBl = { 0 l = L 1 1 \u2264 l < L , dLl , dRl =  1 l = 1 0 1 < l < L\n\u22121 l = L . (9)\nIt admits the \u00b5P for feature learning at\nbl =  0 l = 1 1/2 1 < l < L\n1 l = L\n, cl =  eB \u2212 1 l = 1 eB \u2212 eA 1 < l < L 1\u2212 eA l = L , (10)\nwhere we set al = 0 and setting eA = eB = e corresponds to Shampoo. Rough sketch of derivation. The detailed and comprehensive derivation is presented in Section A. Here, let us briefly explain the derivation of \u00b5P for K-FAC. The \u00b5P has two conditions to be satisfied (A.1,A.2). For an infinitesimal one-step update, these conditions have explicit and tractable expressions described in Section A.2. To check these conditions, we use the push-through identity:\n\u2206Wlhl\u22121 = 1/M 2a+c(B + \u03c1BI) \u2212eB\u03b4diag(\u03c7)h\u22a4(A+ \u03c1AI)\u2212eAh\n= 1/M2a+c\u03b4(\u03b4\u22a4\u03b4 + \u03c1BI) \u2212eBdiag(\u03c7)(h\u22a4h+ \u03c1AI)\u2212eAh\u22a4h (11)\nwhere we omitted the layer index and \u03c7 means an error vector. In the second line, the size of the inverse matrix is independent of the width and this expression enables us to carry out the order evaluation of \u2206Wlhl\u22121. Then, \u00b5P\u2019s conditions become\n2a1 + c1 + eB \u2212 (2eB \u2212 1)(aL + bL) = 0, (12) 2al + cl + eA + eB \u2212 1\u2212 (2eB \u2212 1)(aL + bL) = 0, (13)\n2aL + cL + eA \u2212 1 = 0, aL + bL \u2212 1 = 0. (14)\nFixing a constant shift by setting al = 0, we obtain the result.\nHere, let us explain some rather technical details of the result. First, note that we derived the \u00b5P from the one-step update. This is the same as in the original work on \u00b5P where the one-step update determines the \u00b5P for the whole (inductive) t-step updates. In fact, we empirically confirm the effectiveness of \u00b5P for t > 1. Second, we focus on the case where the preconditioning matrices become valid. Even if \u03c1 takes a much larger value than these matrices, the gradient may realize the feature learning under an appropriate scaling because it reduces to the first-order gradient. However, such unusual switching between the first and second-order optimization is outside the scope of the current work. Thus, we refer to the setting of this proposition as \u00b5P of second-order parameterization.\nTable 1 summarizes the \u00b5P for K-FAC and Shampoo. bL = 1 is a common characteristic of \u00b5P for all methods whereas c1 and cL depend on e. One interesting point is that just by setting bL = 1, K-FAC with \u00b5P works for cl = 0. That is, K-FAC does not require scaling of the learning rate in contrast to the first-order gradient (SGD), which requires cl depending on the width. Moreover, Eq.(10) indicates that this is unique to the K-FAC (e = 1). In other words, K-FAC\u2019s preconditioning effectively achieves the \u00b5P\u2019s scaling of learning rates of the first-order gradient. Note that we can also extend \u00b5P to the Gauss-Newton method without using the K-FAC approximation (Appendix A.4.3).\nFigure 1 empirically confirms that \u00b5P realizes the feature learning (6) in the training of MLP and Myrtle-5. The order of the features was kept during the training, as is shown in Figure 2. Figure 1 also justifies the \u00b5P in CNN. In the CNN model, width represents the number of channels (Xiao et al., 2018; Yang et al., 2021). Although our \u00b5P is derived on the MLP and K-FAC for CNN includes an additional approximation in addition to K-FAC for MLP, we empirically observe that the same parametrization as MLP is also true for CNN."
        },
        {
            "heading": "4.2 JUSTIFICATION OF DAMPING HEURISTICS",
            "text": "Implementations in practice often adjust the damping term of K-FAC using the following heuristics:\n\u03c1Al\u22121(= 1/\u03c1Bl) :=\n\u221a tr(Al\u22121)\nM\nM\ntr(Bl) \u03c1\u2032 = O(\n\u221a M \u00b7M2(aL+bL)\u22121), (15)\nthat is O(M) for the {a, b, c} given in Proposition 4.1. This heuristics is consistent with the valid damping scales (9) in hidden layers but not in the input and output layers. This causes \u2206hl to decay when using damping heuristics even if a, b, c are set to \u00b5P settings. See Section A.6 for more details. To overcome this problem, we propose to use the following rescaled damping satisfying the valid damping scale in \u00b5P:\n\u03c1ReA := \u03c1 \u2032 tr(h\u22a4l\u22121hl\u22121), \u03c1 Re B := \u03c1 \u2032 tr(\u03b4\u22a4l \u03b4l). (16)\nThis rescaling is useful because it enables explicitly expressing the dampings of all layers in a unified manner. Furthermore, it provides the heuristic scales of the preconditioners (i.e., trace) as proportional coefficients which are expected to ensure stable learning dynamics. If damping is set consistent with \u00b5P, \u2206hl neither decays nor grows with respect to width as shown in Figure 1 (3rd column).\nIn contrast to the K-FAC, we found that a standard heuristics of the damping, i.e., a constant multiple of the largest eigenvalue of Ll,Rl, is consistent with the \u00b5P in Proposition 4.1 and requires no modification towards the infinite width. See Section A.6 for more detail."
        },
        {
            "heading": "4.3 IMPLICIT BIAS OF K-FAC TOWARDS NNGP AT ZERO INITIALIZATION",
            "text": "Up to here, we supposed the most common setting where the output weight is given by random (Gaussian) initialization. However, some recent implementations utilized zero initialization on the head (Yang et al., 2021; Wightman, 2019; Botev & Martens, 2022). This corresponds to bL =\u221e, which also induces feature learning after the second step in SGD. The reason why feature learning occurs even at b =\u221e can be explained as follows. When the last layer is initialized with zero, the weights after a 1-step SGD update are\nWl,t=1 = \u03b7lhl,0y (l = L), Wl,0 (l < L). (17)\nSince WL,t=1 = \u0398(1/M), the weights after a 1-step update can be regarded as weights initialized by \u00b5P. Thus feature learning begins from the second step. This is also true for bL > 1. However, in K-FAC, when the last layer is initialized with zero, a single update results in the weight:\nWl,t=1 = \u03b7l(hl\u22121,0h \u22a4 l\u22121,0 + \u03c1AI) \u22121hl,0y (l = L), Wl,0 (l < L). (18)\nInterestingly, this represents an NNGP solution (Lee et al., 2018). When the model can deviate from the NNGP solution, feature learning begins in the training process, similar to SGD. However, when the batch size is close to the full batch size or the learning rate is set considerably low, moving away\nOptimizer bL Batch Size\n4 16 64 256 1024\nSGD 0.5 82.60 80.91 78.86 74.50 66.83 1.0 83.62 83.61 82.10 77.99 73.40 64.0 83.98 83.82 82.60 79.53 74.63 K-FAC 0.5 83.18 84.17 83.75 84.07 80.25 1.0 83.84 84.16 84.29 84.33 83.21 64.0 81.56 82.72 82.63 79.51 75.37\nTable 2: Test accuracy for different batch sizes shows the consistent results (3-layer CNN on FashionMNIST. The whole dataset size is set to 1024). The case of bL = 1 consistently performs better than bL = 0.5 and bL = 64 in K-FAC but not in SGD. Learning rates are tuned for each batch size.\n0 5000\nIteration\n0\n10\n20\n30\n40\nA n a lo\ng y\nA c c\n(W o rd\n2 V\ne c ) SP\n0 5000\nIteration\nmuP\nwidth 27\n29 211 213\n103 104 105\nIteration\n1\n2\n3\n4\nT e st\nL o ss\nSP\n103 104 105\nIteration\nmuP\nwidth 1\n2 4 8 16\nFigure 4: Wider models learn well under \u00b5P throughout training. Using \u00b5P, training proceeds equally across widths. In \u00b5P, the loss is lower for wider widths throughout training. (Left) We trained CBOW on WikiText2 by Shampoo with various widths. (Right) We trained ResNet18 on CIFAR100 by K-FAC while increasing the number of channels from 1 to 16.\nfrom the NNGP solution becomes challenging and does not incur feature learning. Since K-FAC is often used for large batch training, we must be careful about its behavior on bL \u226b 1. Figure 3 confirms this phenomenon. In this experiment, we trained a 3-layer CNN on FashionMNIST with MSE loss. For simplicity, the dataset size is reduced to 1024 and we are training in full batch. After sufficient learning, the training accuracy is highest only for bL = 1. Since the parameters are fixed at the initial kernel solution when bL \u226b 1, we can observe that accuracy at bL \u226b 1 is lower than bL = 1. SGD and Shampoo, on the other hand, achieve almost the same accuracy for bL = 1 and bL \u226b 1. Table 2 shows that \u00b5P when bL \u226b 1(bL = 64), its accuracy on K-FAC is consistently lower than bL = 1 while there is no decrease in accuracy by setting bL = 64 in SGD. In addition, bL = 1 (\u00b5P) achieves higher accuracy than bL = 0.5 (SP) across different batch sizes. We empirically observed that the difference in accuracy between bL = 1 and bL = 64 decreases as the batch size decreases. The same behavior can also be observed when using cross-entropy loss (Appendix.E.2).\nNote that our purpose is not to fully deny the current usage of zero initialization in relatively finite neural networks. Our finding claims that the current default settings do not necessarily work well with large models, and careful attention is required."
        },
        {
            "heading": "5 EXPERIMENTS",
            "text": "5.1 \u00b5P IN WIDE NEURAL NETWORKS\n\u00b5P can invoke feature learning equally across width. This enables a wider model performs better when trained in \u00b5P and given the same HPs (Yang et al., 2021). This section demonstrates that the above statement is also true for second-order optimization.\nWord2Vec: This is a toy model used in the previous work of \u00b5P (Yang & Hu, 2021) to check the advantage of feature learning. We train context as a bag-of-word (CBOW) on Wikitext2 dataset and evaluate its embedding vectors by a Word Analogy task. We optimize CBOW with Shampoo 2. Figure 4 shows that in SP the accuracy decreases as the width is increased. In the infinite width limit of SP, the embedding layer is fixed at initialization, and the accuracy does not increase from the churn\n2K-FAC is very sensitive to bL, and we found it very difficult to optimize word2vec in K-FAC. This could be explained by Section 4.3. Its details are described in Appendix.E.5.\nrate. However, in \u00b5P, increasing the width does not decrease the accuracy. These results highlighted that the \u00b5P for second-order optimization enables feature learning even in infinite-width models.\nResNet: We evaluate the test accuracy for SP and \u00b5P with VGG19 on CIFAR100, ResNet18 on CIFAR100 and ResNet50 on ImageNet. The number of channels in the middle layers is scaled from the original models. Original models correspond to width = 4. HPs are fixed regardless of width. Table 3 indicates that the accuracy can be consistently improved by \u00b5P. Figure 4 shows the learning curves for SP and \u00b5P with different widths. In SP, the test loss for width = 16 (wider model) is not necessarily higher than that for width = 1 (narrower model). However, in \u00b5P, wider models consistently match or outperform narrower models. This is more evident in the early stages of training.\nThe advantage of \u00b5P generally holds regardless of a fixed learning rate. Figure 5 shows that regardless of the fixed learning rate, \u00b5P achieves higher test accuracy compared with SP. As a side note, the difference between SP and \u00b5P is particularly large when the learning rate is small."
        },
        {
            "heading": "5.2 LEARNING RATE TRANSFER",
            "text": "\u00b5P can transfer a learning rate for models with different widths. Learning rate transfer, as defined in a previous study, means that we can set the optimal learning rate to remain constant relative to the order of width, and that \"wider is better\" holds at this optimal learning rate. We confirm this across three different architectures, MLP, CNN, and ResNet in Figure 6. In the SP for SGD, the optimal learning rate decreases as the width increases. Similarly, when the damping heuristics of Equation (1) are used for SP in K-FAC, there is a shift in the optimal learning rate. These shifts in the optimal learning rate are caused by a lack of stability in these parametrizations. In \u00b5P, which is a stable parametrization, the optimal learning rate is fixed with respect to width. One can also confirm that the wider is better for a fixed HP. For instance, if MLP is trained with SP in K-FAC, test accuracy at width = 16384 is lower than width = 512. In contrast, if one uses \u00b5P for training, accuracy consistently increases with the width; thus, \u00b5P works as a better parameterization to promote feature learning."
        },
        {
            "heading": "5.3 DAMPING TRANSFER",
            "text": "The damping term is another HP that requires careful tuning in second-order optimization. In \u00b5P, we can re-use the damping obtained in a small model to the large-scale one. As shown in Figure 7, if one optimizes CNN with K-FAC using the damping heuristics (Eq. 15) in SP (i.e., the default setting), the optimal damping value increases as the width increases. In contrast, when damping is consistent with \u00b5P, it can be transferred across the widths."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "We proposed a desirable parametrization inspired by \u00b5P for second-order optimization to achieve stable feature learning in infinite-width neural networks. As the computational resources are en-\nriched more and the model becomes larger, there is no guarantee that the current default settings of initialization, learning rate, and damping terms work well in a similar way to smaller models. In pursuit of such training of wider models, we empirically confirmed the effectiveness of the \u00b5P in commonly used second-order methods and the necessary modification of the HPs. It is also one of the advantages of the proposed parameterization that when using \u00b5P, HPs tuned in a smaller model can be transferred to a larger one.\nLimitation and future direction. The current work and the framework of \u00b5P focus on the wide neural networks as large-scale models. It would be interesting to investigate the parameterization effective for the depth. Some studies have scaled weight initialization with respect to depth (Shoeybi et al., 2019; Radford et al., 2019). Transformer is a widely used architecture in modern deep learning; however second-order optimization has not yet been established well, and we have still limited empirical observation in both pertaining (Pauloski et al., 2022; Osawa et al., 2023b) and fine-tuning (Ding et al., 2023). Moreover, the finite width effect may be non-negligible in the Transformer; thus, we will need more careful study focused on it (Dinan et al., 2023; Wortsman et al., 2023). From a theoretical perspective, mathematically rigorous evaluation of feature learning in general steps is curious as well. Tensor Program (Yang & Littwin, 2023) is a strong candidate for such evaluation, although it is currently not applicable to the second-order parameterization and analytical evaluation is still limited even in the first-order gradient. We expect that our proposed parameterization and empirical observation will serve as a foundation for providing deeper insight into second-order optimization both in theory and practice."
        },
        {
            "heading": "A DEVIATION OF MUP",
            "text": "In this section, we first explain basic conditions and assumptions for \u00b5P in Section A.1. Next, we provide a derivation of \u00b5P for the first-order gradient in Section A.2. This deviation is based on a one-step update and its perturbation, which is easily generalized to the cases of K-FAC (in Section A.4.1) and Shampoo (in Section A.4.2).\nA.1 SETTING OF \u00b5P\nHere, let us overview the conditions of \u00b5P proposed in Yang & Hu (2021). As we described in Section 3.2, the previous work defined the feature learning by\n\u2206hl := hl,t \u2212 hl,0 = \u0398(1). (S.1)\nNote that in the appendices, we avoid using bold fonts of vectors and matrices in order to prevent a messy appearance. Intuitively speaking, the activation varies with the constant order to generate features in each layer. This is crucial for distinguishing the learning from the lazy regime (Jacot et al., 2018; Chizat et al., 2019). In the lazy regime, the model can converge to a global minimum but the trained parameters remain sufficiently close to their initial values and the model is equivalent to a linearized model. In other words, changes in the hidden layer are infinitesimal, i.e., \u2206hl \u2192 0 while \u2206f = \u0398(1) in the infinite width limit. Thus, the condition S.1 is required to avoid the lazy regime. For clarity, we set the width of the hidden layers to the same and take the infinite width limit 3:\nMl = M, M \u2192\u221e (l = 1, ..., L\u2212 1). (S.2)\nTo realize the feature learning, they proposed to use the abc-parameterization satisfying the following two conditions:\nCondition A.1 (Wl updated maximally).\n\u2206Wl,thl\u22121,t = \u0398(1), (S.3)\nwhere \u2206Wl,t := Wl,t \u2212Wl,0.\nThis first condition plays a fundamental role in feature learning. As is described in Section H.7 of Yang & Hu (2021) and we will explain more detail in the next subsection, this condition naturally appears when one considers the infinitesimal change of the parameter (i.e., \u2202\u03b7). The previous work also requires the following condition for \u00b5P:\nCondition A.2 (WL initialized maximally).\nWL,0\u2206uL\u22121,t = \u0398(1). (S.4)\nThis condition plays a fundamental role for determining the variance of the weights in the last layer. Combining this condition with Condition A.1 makes the output change of order 1, that is, ft \u2212 f0 = \u2206WL,thL\u22121,t +WL,0\u2206hL\u22121,t = \u0398(1). The \u00b5P is derived from these two conditions. Subsequent studies have empirically verified that the successfully facilitates feature learning in various settings of actual training (Yang et al., 2021; Vyas et al., 2023).\nYang & Hu (2021) also assume\nAssumption A.3. ul,0, hl,0 = \u0398(1) (l < L), f0 = uL,0 = O(1).\nThe input and output of the activation function are assumed to be of the order of 1. This assumption has been commonly employed in the training of neural networks (LeCun et al., 1998). Additionally,\n3Our analysis remains applicable even if the widths of different layers vary, as long as all widths approach infinity in the same order. More precisely, even if the network width is given by Ml = \u03b1lM , with each \u03b1l > 0 being a constant, the \u00b5P remains the same in the infinite width limit of M \u2192 \u221e.\nthe \u00b5P setting allows that the network output f can be close to zero at initialization. This assumption immediately leads to\na1 + b1 = 0, (S.5) al + bl = 1/2 (1 < l < L), (S.6)\naL + bL \u2265 1/2, (S.7)\nas is described in Theorem H.6 of Yang & Hu (2021). We also have Assumption A.4. For feature learning, we suppose aL + bL > 1/2.\nThis is the Assumption H.23 of Yang & Hu (2021). This assumption is used to avoid technical subtleties potentially caused by aL+ bL = 1/2: for random initialization, f0 goes to 0 for aL+ bL > 1/2 while it becomes Gaussian process with a non-zero variance for aL + bL = 1/2. Roughly speaking, this assumption ensures that a backpropagated error converges to a deterministic constant (y \u2212 f0)\u2192 y in the infinite width limit and makes the derivation more clear by avoiding potential correlation between the error and trained features.\nA.2 \u00b5P\u2019S CONDITIONS OF ONE-STEP UPDATE AND PERTURBATION\nAs a preparation for the second-order optimization, let us explain the \u00b5P\u2019s conditions written in an infinitesimal one-step update. The essentially same perturbation is argued in Section H.7 of Yang & Hu (2021), but their explicit evaluation focuses on the development of feature kernels, i.e., ||hl,1||2, and the precise evaluation of this value requires much-complicated argument. Since the current work is interested only in a simpler problem on the order evaluation of hl,1, it will be more informative and clearer to show the perturbation argument specific to hl,1 directly. As you can see below, this clarifies an explicit connection between feature learning and Conditions A.1 & A.2.\nExpress the first one-step update of the weight by\n\u2206Wl,1 = \u03b7\u2032\nM2al+cl Gl, Gl := PA\u2207wlL0PB , (S.8)\nwhere PA and PB are preconditioners. We put\n\u03b7\u0304 := \u03b7\u2032\nM2al+cl . (S.9)\nNote that\u2207wlL0 depends only on the initialization (t = 0). We have\nWl,1hl\u22121,1 = (Wl,0 + \u03b7\u0304Gl)\u03d5(ul\u22121,1(\u03b7)), (S.10)\nwhere we remarked the dependence of {W1,1, ...,Wl\u22121,1} on \u03b7 by ul\u22121,1(\u03b7). If the derivative of the one-step update with respect to the learning rate is \u0398(1), this implies that feature learning appears. While Yang & Hu (2021) explicitly evaluated \u2202\u03b7\u2032 ||hl,1||2 \u2223\u2223 \u03b7\u2032=0\n, let us evaluate here the following quantities which have an explicit expansion by the chain rule with respect to \u03b7\u2032:\n\u2202\u03b7\u2032hl,1 \u2223\u2223 \u03b7\u2032=0 = \u03d5\u2032(ul) \u25e6 1\nM2al+cl Glhl\u22121 + l\u22121\u2211 k=1 \u2202hl,0 \u2202ul\u2212k \u25e6 v(l, k) (l < L), (S.11)\n\u2202\u03b7ft=1 \u2223\u2223 \u03b7\u2032=0 = 1\nM2aL+cL GLhL\u22121 +WL\u2202\u03b7\u2032hL\u22121,1\n\u2223\u2223 \u03b7\u2032=0 , (S.12)\nwhere v(l, k) :=\n1\nM2al\u2212k+cl\u2212k Gl\u2212khl\u2212k\u22121,0. (S.13)\nWe can find connections between (S.11, S.12) and Conditions (A.1, A.2) as follows:\n\u2202\u03b7\u2032(\u2206Wl,1hl\u22121,1) \u2223\u2223 \u03b7\u2032=0 = 1\nM2al+cl Glhl\u22121,0 (l = 1, ..., L), (S.14)\nand \u2202\u03b7\u2032(WL,0\u2206hL\u22121,1) \u2223\u2223 \u03b7\u2032=0 = WL,0\u2202\u03b7\u2032hL\u22121,1 \u2223\u2223 \u03b7\u2032=0 . (S.15)\nThat is, under the perturbation with a small \u03b7\u2032, the conditions reduce to\nCondition A.1\u2019 1\nM2al+cl Glhl\u22121,0 = \u0398(1). (S.16)\nCondition A.2\u2019 WL,0\u2202\u03b7\u2032hL\u22121,1 \u2223\u2223 \u03b7\u2032=0 = \u0398(1). (S.17)\nAfter all, one can see that the Conditions of \u00b5P (A.1,A.2) reduces to explicit representations (S.16,S.17) under the perturbation. If Conditions A.1\u2019 and A.2\u2019 hold, it implies (S.11, S.12) of \u0398(1) and then the feature learning (S.1) appears. One can say that the \u00b5P is the \"maximal\" update because it induces all layers trained as much as possible."
        },
        {
            "heading": "A.3 CASE OF FIRST-ORDER OPTIMIZATION",
            "text": "As an exercise for preparing second-order optimization, we first evaluate Conditions A.1\u2019 and A.2\u2019 for the first-order gradient with the preconditioners PA = PB = I . The second-order cases are shown later.\nOn Condition A.1\u2019. To avoid cumbersome notation, we omit the initialization index t = 0 as long as it doesn\u2019t lead to confusion. We can express the gradient Gl by\nGl = \u03b4ldiag(\u03c7)h\u22a4l\u22121, (S.18)\nwhere forward and backward signals are given by matrix forms hl, \u03b4l \u2208 RMl\u00d7n. Note that the backward signals are given by the chain rule:\n\u03b4l = \u03d5 \u2032(ul) \u25e6 (W\u22a4l+1\u03b4l+1) (l < L\u2212 1), (S.19)\n\u03b4L = en, (S.20)\nwhere we defined en := (1, ..., 1) \u2208 Rn. We also introduced an error vector \u03c7 := y \u2212 f \u2208 Rn. (S.21)\nAs is pointed out in the previous works, the important point of Glhl\u22121 (in other words, \u2206Wlhl\u22121) is that Gl and hl\u22121 are not independent. We have\nGlhl\u22121,0 = \u03b4ldiag(\u03c7)(h\u22a4l\u22121hl\u22121). (S.22)\nHere, let us introduce KAl := h \u22a4 l hl/M, K B l := \u03b4 \u22a4 l \u03b4l. (S.23) For the random initialization, these two matrices are well known as intermediate components of the neural tangent kernel studies in Jacot et al. (2018); Yang (2020); Karakida & Osawa (2020). Assume that the activation function and its derivatives are polynomially-bounded. Then, in the infinite width limit, these matrices become deterministic constants independent of the width. In more detail, the tensor program ensures almost sure convergence of the moments composed of the forward and backpropagated signals (hl and \u03b4l) (Yang, 2020). The l-layer component of the neural tangent kernel is given by KlA \u25e6KlB where \u25e6 denotes the Hadamard product. The kernel (KAl )nn\u2032 is \u0398(1). Since we suppose the abc-parameterization, \u03b4L\u22121 = \u0398(WL) = \u0398(1/MaL+bL). This leads to\n(KBl )nn\u2032 = \u0398(1/M 2(aL+bL)\u22121), (S.24)\nfor 1 \u2264 l < L, that is, the coordinate size is given by\n\u03b4l = \u0398(1/M (aL+bL)). (S.25)\nAfter all, we have \u2202\u03b7\u2032(\u2206Wl,1hl\u22121,1) \u2223\u2223 \u03b7\u2032=0 = 1 M2al+cl\u22121 \u03b4ldiag(\u03c7)Al\u22121 = \u0398(1/Mrl), (S.26)\nwith\nrl =  2a1 + c1 + (aL + bL) (l = 1),\n2al + cl \u2212 1 + (aL + bL) (1 < l < L), 2aL + cL \u2212 1 (l = L),\n(S.27)\nwhere we used the fact that \u03c7 is a constant vector of \u0398(1) in the infinite width limit under Assumption A.4. Compared to r1<l<L, r1 does not include \u22121 because M0 = \u0398(1) and A0 = \u0398(1/M). The case of rL does not include aL + bL because \u03b4L = \u0398(1).\nOn Condition A.2\u2019. We can represent Eq. (S.17) by\n\u2202\u03b7\u2032(WL,0\u2206hL\u22121,1)\n= WL,0 (\u03d5 \u2032(uL\u22121) \u25e6 (\u2202\u03b7\u2032(WL\u22121,0\u2206hL\u22122,1) + \u2202\u03b7\u2032(\u2206WL\u22121,1hL\u22122,1))) . (S.28)\nNote that\nWL,0(\u03d5 \u2032(uL\u22121) \u25e6 \u2202\u03b7\u2032(\u2206WL\u22121,1hL\u22122,1) \u2223\u2223 \u03b7\u2032=0 )\n= eM (\u03b4L\u22121 \u25e6 1\nM2aL+cL GL\u22121hL\u22122) (S.29)\n= 1\nM2aL+cL\u22121 eM (\u03b4L\u22121 \u25e6 (\u03b4L\u22121diag(\u03c7)AL\u22122)). (S.30)\nBecause \u03b4L\u22121 has the coordinate size of Eq. (S.25) and the product with eM means the summation over M , we obtain\n\u2202\u03b7\u2032(WL,0\u2206hL\u22121,1) \u2223\u2223 \u03b7\u2032=0 = \u0398(1/MaL+bL\u22121+rL\u22121). (S.31)\nFinally, from Eqs. (S.27) and (S.31), the \u00b5P is given by 2a1 + c1 + (aL + bL) = 0,\n2al + cl \u2212 1 + (aL + bL) = 0, 2aL + cL \u2212 1 = 0, aL + bL \u2212 1 = 0.\n(S.32)"
        },
        {
            "heading": "A.4 CASE OF SECOND-ORDER OPTIMIZATION",
            "text": "What we need to do here is to find the abc-parameterization satisfying Conditions A.1\u2019 and A.2\u2019 for the preconditioners PA and PB (S.8) of the second-order optimization methods. We assume the following:\nAssumption A.5. The gradient of the loss \u2207\u03b8L, preconditioners PA and PB are calculated on the same batch of training samples at the first one step.\nFurthermore, we consider the situation where the second-order optimization is valid, defined as follows.\nDefinition A.6 (Valid second-order optimization). Suppose that each preconditioning matrix X has a damping term \u03c1X as X + \u03c1XI . We define the second-order optimization as valid if the eigenvalues of X have an equal width-dependent scale to the damping term, that is, \u03c1X = \u0398(\u2225X\u22252).\nThis valid situation is rational because it prevents the effect of preconditioner from vanishing in the infinite width limit. It also avoids the damping term\u2019s faster convergence to zero, a scenario that can potentially lead to numerical instability when computing inverses for rank-deficient preconditioners.\nA.4.1 \u00b5P FOR K-FAC\nOn Condition A.1\u2019: For K-FAC, we have\n1\nM2al+cl Glhl\u22121 =\n1\nM2al+cl (Bl + \u03c1BlI)\n\u2212eB\u03b4ldiag(\u03c7)h\u22a4l\u22121(Al\u22121 + \u03c1Al\u22121I) \u2212eAhl\u22121, (S.33)\nwhere Al := hlh \u22a4 l , Bl := \u03b4l\u03b4 \u22a4 l . (S.34)\nUnder Assumption A.5, for eA, eB \u2208 {0, 1}, we can apply the push-through identity, i.e., (I + XY )\u22121X = X(I + Y X)\u22121, as follows.\n(i) For 1 < l < L, We have 1\nM2al+cl Glhl\u22121 (S.35) = MeB(2(aL+bL)\u22121)\u2212eA\nM2al+cl\u22121 \u03b4l (K\nB l + \u03c1BlM 1\u22122(aL+bL)I)\u2212eBdiag(\u03c7)(KAl\u22121 + \u03c1Al\u22121MI) \u2212eAAl\u22121\ufe38 \ufe37\ufe37 \ufe38\n=:Kl\n.\n(S.36)\nIt is noteworthy that in a similar way to Eq. (S.26), Kl converges to a deterministic value in the infinite width limit. Since KAl is a deterministic constant of \u0398(1), its eigenvalues are also \u0398(1). This means that the damping where the second-order optimization becomes valid is dAl = \u22121. As a side note, for dAl < \u22121, the damping term in (KAl + \u03c1Al\u22121MI)\u2212eA becomes dominant and the contribution of the preconditioner vanishes in the infinite width limit. In contrast, we may take dAl > \u22121 if KAl is positive-definite. Similarly, we have dBl = 2(aL + bL)\u2212 1.\nwe have \u2202\u03b7\u2032(\u2206Wl,1hl\u22121,1) \u2223\u2223 \u03b7\u2032 = \u0398(1/Mrl) with\nrl = 2al + cl + eA + eB \u2212 1\u2212 (2eB \u2212 1)(aL + bL) = 0. (S.37)\n(ii) For l = L, \u03b4L has the coordinate size \u0398(1) and this means \u03b4L(\u03b4\u22a4L \u03b4L + \u03c1BI)\u2212eB = \u0398(1). Therefore, we easily obtain dAl = \u22121, dBL = 0 and\nrL = 2aL + cL + eA \u2212 1 = 0. (S.38)\n(iii) For l = 1, note that the input h0 has the coordinate size \u0398(1) and (h\u22a40 h0+\u03c1AI)\u2212eAh\u22a40 = \u0398(1). Therefore, we have dAl = \u22121, dBL = 2(aL + bL)\u2212 1 and\nr1 = 2a1 + c1 + eB \u2212 (2eB \u2212 1)(aL + bL) = 0. (S.39)\nOn Condition 2\u2019. We have\nWL,0(\u03d5 \u2032(uL\u22121) \u25e6 \u2202\u03b7\u2032(\u2206WL\u22121,1hL\u22122,1)) \u2223\u2223 \u03b7\u2032=0\n= eM (\u03b4L\u22121 \u25e6 1\nM2aL+cL GL\u22121hL\u22122) (S.40)\n= MeB(2(aL+bL)\u22121)\u2212eA\nM2al+cl\u22121 eM (\u03b4L\u22121 \u25e6 (\u03b4L\u22121KL\u22121)), (S.41)\nwith Kl given by Eq. (S.36). To satisfy Condition 2\u2019, we need\naL + bL \u2212 1 + rL\u22121 = 0. (S.42)\nFinally, by combining Conditions A.1\u2019 and A.2\u2019, we obtain the abc-parameterization of \u00b5P as 2a1 + c1 + eB \u2212 (2eB \u2212 1)(aL + bL) = 0, 2al + cl + eA + eB \u2212 1\u2212 (2eB \u2212 1)(aL + bL) = 0, 2aL + cL + eA \u2212 1 = 0, aL + bL \u2212 1 = 0. (S.43)\nBy setting al = 0, we can eliminate the indeterminacy of the parameterization and obtain Proposition 4.1. By setting eA = eB = 0, we recover the case of the first-order gradient method.\nA.4.2 \u00b5P FOR SHAMPOO\nHere, let us consider a general e > 0.\nOn Condition A.1\u2019: For Shampoo, we have 1\nM2al+cl Glhl\u22121 =\n1\nM2al+cl (Ll + \u03c1LlI)\n\u2212e/2\u03b4ldiag(\u03c7)h\u22a4l\u22121(Rl\u22121 + \u03c1Rl\u22121I) \u2212e/2hl\u22121, (S.44)\nwhere we use \u03b4l defined in Eq. (S.19).\n(i) For 1 < l < L, In a similar way to the case of K-FAC, we use the push-through identity. The update is given by\n1\nM2al+cl Glhl\u22121 =\n1\nM2al+cl (\u03b4ldiag(\u03c7)h\u22a4l\u22121hl\u22121diag(\u03c7)\u03b4 \u22a4 l + \u03c1LlI) \u2212e/2\u03b4ldiag(\u03c7)\n\u00b7 h\u22a4l\u22121(hl\u22121diag(\u03c7)\u03b4\u22a4l \u03b4ldiag(\u03c7)h\u22a4l\u22121 + \u03c1Rl\u22121I)\u2212e/2hl\u22121. (S.45)\nUnder Assumption A.5, we use the following push-through identity here:\nLemma A.7 (e.g., Petersen et al. (2008)). For any analytic function g and real matrices X and Y ,\ng(XY )X = Xg(Y X). (S.46)\nFirst, let us consider\nQ := (hl\u22121diag(\u03c7)\u03b4\u22a4l \u03b4ldiag(\u03c7)h \u22a4 l\u22121 + \u03c1Rl\u22121I) \u2212e/2hl\u22121. (S.47)\nLet us express the largest eigenvalue of X by \u2225X\u22252. we have\n\u2225hl\u22121diag(\u03c7)\u03b4\u22a4l \u03b4ldiag(\u03c7)h\u22a4l\u22121\u22252 = \u2225diag(\u03c7)\u03b4\u22a4l \u03b4ldiag(\u03c7)h\u22a4l\u22121hl\u22121\u22252 (S.48) = \u0398(1/M2(aL+bL)\u22122). (S.49)\nBy setting \u03c1Rl\u22121 = (aL + bL) \u2212 1 and taking a certain constant \u03c1\u2032Rl\u22121 satisfying \u2225hl\u22121diag(\u03c7)\u03b4\u22a4l \u03b4ldiag(\u03c7)h\u22a4l\u22121\u22252/\u03c1Rl\u22121 < 1, the inverse matrix in Q has a matrix series expansion that converges. Thus, we can use Lemma A.7 and obtain\nQ = hl\u22121(diag(\u03c7)\u03b4\u22a4l \u03b4ldiag(\u03c7)h \u22a4 l\u22121hl\u22121 + \u03c1Rl\u22121I) \u2212e/2. (S.50)\nApplying the same argument to the Ll side of (S.45), We obtain\n1\nM2al+cl Glhl\u22121 =\n1\nM2al+cl \u03b4l(diag(\u03c7)h\u22a4l\u22121hl\u22121diag(\u03c7)\u03b4 \u22a4 l \u03b4l + \u03c1LlI) \u2212e/2diag(\u03c7)\n\u00b7 h\u22a4l\u22121hl\u22121(diag(\u03c7)\u03b4\u22a4l \u03b4ldiag(\u03c7)h\u22a4l\u22121hl\u22121 + \u03c1Rl\u22121I)\u2212e/2. (S.51)\nIn a similar way to Eq. (S.36), we obtain\nrl = 2al + cl + 2e\u2212 1\u2212 (2e\u2212 1)(aL + bL). (S.52)\n(ii) For l = 1, L, Similar to the case with K-FAC, by carefully handling RL for l = L and L0 for l = 0, we immediately find\nr1 = 2a1 + c1 + e\u2212 (2e\u2212 1)(aL + bL), (S.53) rL = 2aL + cL + e\u2212 1. (S.54)\nOn Condition 2\u2019. In the same way as in the case of K-FAC, we have\naL + bL \u2212 1 + rL\u22121 = 0. (S.55)\nIn summary, we obtain the abc-parameterization of \u00b5P as 2a1 + c1 + e\u2212 (2e\u2212 1)(aL + bL) = 0, 2al + cl + 2e\u2212 1\u2212 (2e\u2212 1)(aL + bL) = 0, 2aL + cL + e\u2212 1 = 0, aL + bL \u2212 1 = 0.\n(S.56)\nThis is equivalent to the case of K-FAC with eA = eB = e."
        },
        {
            "heading": "A.4.3 GAUSS-NEWTON METHOD",
            "text": "From the standpoint of computational cost, practical applications use approximate second-order optimization methods. For K-FAC and Shampoo two smaller pre-conditioners are applied to the first-order gradient in matrix form. Although other forms of second-order optimization are not so often utilized in deep learning, mentioning methods beyond K-FAC and Shampoo could be valuable, particularly from the perspective of demonstrating the theoretical applicability of these techniques.\nLet us consider Gauss-Newton method in a layer-wise manner. We choose this setting because the K-FAC and Shampoo are layer-wise and it makes the comparison easier. It is also straightforward to apply similar order evaluation to the case without layer-wise approximation. The vector form of the Gauss-Newton gradient is given by\n(\u2207w\u20d7lL\u22a4\u2207w\u20d7lL+ \u03c1lI)\u22121\u2207w\u20d7lL = (J\u22a4l diag(\u03c72)Jl + \u03c1lI)\u22121J\u22a4l \u03c7 (S.57) = J\u22a4l (diag(\u03c7 2)JlJ \u22a4 l + \u03c1lI) \u22121\u03c7, (S.58)\nwhere Jl = \u2207w\u20d7lf is a n\u00d7MlMl\u22121 Jacobian matrix. Put \u03c7\u0303l := (diag(\u03c72)JlJ\u22a4l + \u03c1lI)\u22121\u03c7. Then, the matrix form of the gradient (S.58) is given by\n\u2207wlL = \u03b4ldiag(\u03c7\u0303l)h\u22a4l\u22121, (S.59)\nunder al = 0. This corresponds to Gl in Eq.(S.18) and we can easily apply the same order evaluation shown in Section . The point is that JlJ\u22a4l is the NTK matrix for the l-th layer and we have\nJlJ \u22a4 l = (\u03b4l\u03b4 \u22a4 l ) \u25e6 (hl\u22121h\u22a4l\u22121). (S.60)\nFrom Eq. (S.25) and aL + bL = 1, we can see that JlJ\u22a4l is \u0398(1) for 1 < l < L, \u0398(1/M) for l = 1 and \u0398(M) for l = L and determines the order of \u03c7\u0303. Thus, the \u00b5P for the layer-wise Gauss-Newton method is as follows:\nbl =  0 l = 1 1/2 1 < l < L\n1 l = L\n, cl = 0, dl =  1 l = 1 0 1 < l < L\n\u22121 l = L . (S.61)\nwhere \u03c1l = \u03c1\u2032l/M dl . Thus, bl and cl are the same as in K-FAC and \u03c1l is the same as in Shampoo."
        },
        {
            "heading": "A.5 EXTENSION TO OTHER LOSSES",
            "text": "Until now, we have considered the MSE loss with a one-dimensional target. Extending these results to multi-dimensional target cases is straightforward. Suppose that the target sample yi is a Cdimensional vector, with C = \u0398(1). For a multi-dimensional target, its gradient (S.18) is given by\nGl = \u03b4\u0303l diag(\u03c7)(1c \u2297 h\u22a4l ), (S.62)\nwhere \u03b4\u0303 = [\u03b41, ..., \u03b4C ] and \u03c7 = y \u2212 f \u2208 RnC . (S.63)\nSince (1c \u2297 h\u22a4l )hl = (1c \u2297 h\u22a4l hl), we can apply the same arguments as those shown in Section A.2. Consequently, we can easily obtain the same \u00b5P for multi-dimensional targets.\nSimilarly, we can obtain the same \u00b5P for the case of a C-class cross-entropy loss. For SGD and Shampoo, we just need to replace the error vector in the gradient (S.18) with\n\u03c7 = y \u2212 \u03c3(f) \u2208 RnC , (S.64)\nwhere y is the one-hot encoded true label vector and \u03c3(f) denotes a softmax function with the input logits f . For each input sample xi, the softmax is defined by \u03c3(fk(xi)) := exp(fk(xi))/ \u2211C k\u2032 exp(fk\u2032(xi)) for i = 1, ..., n and k = 1, ..., C. Thus, from the same argument as for MSE loss with a multi-dimensional target, we can obtain the same \u00b5P.\nFor K-FAC, note that the Fisher information matrix is given with the following backward part:\nBl = \u03b4\u0303l\u039b\u03b4\u0303 \u22a4 l , (S.65)\nwhere \u039b(\u03c3) is an nC \u00d7 nC block diagonal matrix which is composed of C \u00d7 C block matrices; diag(\u03c3(f(xi))) \u2212 \u03c3(f(xi))\u03c3(f(xi))\u22a4 (i = 1, ..., n). Because \u039b depends only on f and its size is independent of the width, its contribution amounts to merely a constant multiplication. In addition, the only difference from the MSE case lies in the backward part. We can easily employ the push-through identity as\n(\u03b4\u0303l\u039b\u03b4\u0303 \u22a4 l + \u03c1Bl) \u22121\u03b4\u0303l = \u03b4\u0303l(\u039b\u03b4\u0303 \u22a4 l \u03b4\u0303l + \u03c1Bl) \u22121. (S.66) Therefore, we can obtain the same \u00b5P as those presented in Proposition 4.1."
        },
        {
            "heading": "A.6 DAMPING HEURISTICS",
            "text": "When using damping heuristics in K-FAC, second-order optimization does not become valid. In addition, \u2206hl decay with width if a, b, c are set to \u00b5P settings. This mechanism can be explained as follows. Consider the damping of the input layer determined by Eq. (15).\n\u03c1Al\u22121(= 1/\u03c1Bl) :=\n\u221a M\ntr(Bl) \u03c1 = \u0398(M (aL+bL)\u22121/2). (S.67)\nAccording to Eq. (9), the appropriate damping scales are \u03c1A = \u0398(1) and \u03c1B = \u0398(1/M2(aL+bL)\u22121). Thus, if we use damping heuristics, second-order optimization is not valid because \u03c1A and \u03c1B are both larger than the appropriate damping scale. Since the order of the damping term is larger than the appropriate damping scaling and \u03c1A \u00b7 \u03c1B = 1, \u2206W1h0 is of the same order as SGD. Thus the scale of \u2206W1h0 will be as follows:\n\u2202\u03b7\u2032(\u2206W1,1h0,1) \u2223\u2223 \u03b7\u2032=0 = \u0398(1/M2a1+c1+aL+bL). (S.68)\nThus, \u2206h1 will decay when c1 = 0 and bL \u2265 0. On the other hand, in the case of Shampoo, even if we use the damping of heuristics (determined by a constant multiple of the maximum eigenvalue), second-order optimization becomes valid. This mechanism can be explained as follows. The following three matrices all share the largest eigenvalue.\nLl + \u03c1I = \u03b4lhl\u22121 \u22a4hl\u22121\u03b4l \u22a4 + \u03c1I, (S.69)\nRl + \u03c1I = hl\u22121\u03b4l \u22a4\u03b4lhl\u22121 \u22a4 + \u03c1I, (S.70)\nKl + \u03c1I = h \u22a4 l\u22121hl\u22121\u03b4 \u22a4 l \u03b4l + \u03c1I. (S.71)\nSince the matrix size of Kl is independent of width, the maximum and mean eigenvalues of Kl are of the same order with respect to width. Thus, the maximum eigenvalues of Rl and Ll are of the same order as the mean eigenvalue of Kl."
        },
        {
            "heading": "B EXPERIMENTAL DETAILS",
            "text": ""
        },
        {
            "heading": "B.1 MODELS",
            "text": "We considered the following models for vision tasks.\n\u2022 MLP: We considered a 3-layer multilayer perceptron (MLP) with ReLU activation. The MLP models do not include bias.\n\u2022 CNN: We considered 3-layer CNN with ReLU activation. The models consist of a two-layer convolution and a linear layer. We trained with different hidden widths where the width represents the output dimension of the first layer. Max pooling is applied after the activation function.\n\u2022 Myrtle-5: We considered Myrtle family (Shankar et al., 2020) as an example for CNN. We trained with different hidden widths where the width represents the output dimension of the first layer.\n\u2022 ResNet: We considered ResNet18 and ResNet50 (He et al., 2016) with different hidden widths where widths represent the number of channels. We used the existing implementation4 for training CIFAR10. In addition, we used models from Pytorch Image models (Wightman, 2019) for ImageNet training.\n4https://github.com/uoguelph-mlrg/Cutout\nIn addition to these four models, we also considered CBOW as a language model.\n\u2022 CBOW: We used CBOW model for the word embedding task (Mikolov et al., 2013). The CBOW model is a network consisting of an embedding layer that embeds words and a linear layer that predicts context words. The models do not include an activation function."
        },
        {
            "heading": "B.2 DETAILS OF FIGURES",
            "text": "This section explains the experimental setting for each figure. In all experiments, we implemented second-order optimization based on the ASDL library (Osawa et al., 2023a).\nFigure 1 In the upper graph, we trained a 3-layer MLP on the MNIST dataset with cross-entropy loss. In the second graph, we trained a Myrtle-5 on the CIFAR10 with cross-entropy loss. Both graphs represent \u2206hl at the 10th iteration. The training sets have been reduced to 256 samples, and we trained models by full-batch training. We apply no data augmentation.\nFigure 2 We trained a 3-layer MLP on FashionMNIST with \u03b7 = 0.001, \u03c1 = 1. Its parametrization follows \u00b5P settings. We apply no data augmentation.\nFigure 3 We trained a 3-layer CNN on FashionMNIST with different bL for 50 epochs. This graph shows the ratio between the gradient and the preconditioned gradient. Layer-wise learning rate follows the scaling of c in muP in Eq. 10. The learning rate is tuned by grid search. Its settings are as follows.\n\u2022 Learning rate : 2z where z \u2208 {1, 0,\u22121, ...,\u221220} \u2022 Damping term \u03c1 for K-FAC: 1 \u2022 Damping term \u03c1 for Shampoo: 1e-3\nIn Table.2, we experimented with exactly the same settings as above, but with different batch sizes.\nFigure 4 (Left) We trained Context as a Bag-of-Words (CBOW) on WikiText2 by Shampoo with \u03b7 = 0.1, \u03c1 = 10\u22126 and \u03c4 = 0.1. We test the embeddings by the word analogy task on WordSim353. We used words that appeared more than 128 times in the dataset.\nFigure 4 (Right) We trained ResNet18 on CIFAR100 by K-FAC with \u03b7 = 0.003, \u03c1 = 10 and \u03c4 = 0.5. We use a cross-entropy loss with label smoothing. We apply RandomCrop, RandomHorizontalFlip, AutoAugment, and Cutout as data augmentation. In Table 3, we experimented with exactly the same settings when training ResNet18 on CIFAR100.\nFigure 5 We trained ResNet50 on ImageNet by K-FAC with \u03c1 = 0.001 and \u03c4 = 0.05. We use a cross-entropy loss with label smoothing. We apply RandomCrop, RandomHorizontalFlip, and Cutout as data augmentation. In addition, to prevent instability in the training, we used gradient clipping in Grosse & Martens (2016). In Table 3, we experimented with exactly the same settings when training ResNet50 on ImageNet.\nFigure 6 In the first row, we trained 3-layer MLP on MNIST for 20 epochs. The number of samples is reduced to 1024 and trained by MSE loss. In training with K-FAC, we used \u03c1 = 0.001 for heuristics damping and \u03c1 = 1 for rescaled damping.\nIn the second row, we trained a 3-layer CNN on FashionMNIST for 50 epochs. The number of samples is reduced to 1024 and trained by MSE loss. In training with K-FAC, we used \u03c1 = 1 for heuristics damping and \u03c1 = 100 for rescaled damping.\nIn the last row, we trained ResNet18 on FashionMNIST for 20 epochs. The number of samples is reduced to 1024 and trained by MSE loss. In training with K-FAC, we used \u03c1 = 10 for heuristics damping and \u03c1 = 10 for rescaled damping.\nFigure 7 We trained a 3-layer CNN on FashionMNIST for 50 epochs with \u03b7 = 0.003. The number of samples is reduced to 1024 and trained by MSE loss.\nTable 3 We trained VGG19 on CIFAR100 for 300 epochs, ResNet18 on CIFAR100 for 300 epochs and ResNet50 on ImageNet for 55 epochs. For VGG19 training with K-FAC, we set the learning rate \u03b7 = 0.01, for ResNet18 training with shampoo, we set \u03b7 = 0.001, and for ResNet50 training with K-FAC, we set \u03b7 = 0.2. In all other settings, \u03b7 = 0.003."
        },
        {
            "heading": "C ADDITIONAL DESCRIPTION OF SECOND-ORDER OPTIMIZATION",
            "text": ""
        },
        {
            "heading": "C.1 K-FAC",
            "text": "Natural gradient descent (Amari, 1998) is an optimization algorithm that preconditions the gradient by the inverse of the Fisher information matrix. Its update rule is given by\n\u03b8t+1 = \u03b8t \u2212 \u03b7t (F (\u03b8t) + \u03c1I)\u2212e\u2207L(\u03b8t), (S.72)\nwhere F (\u03b8) = Ex\u223cq(x),t\u2032\u223cp\u03b8(t\u2032|x) [ \u2207 log p\u03b8 (t\u2032 | x)\u2207 log p\u03b8 (t\u2032 | x) \u22a4 ] , (S.73)\nis the Fisher information matrix. K-FAC (Martens & Grosse, 2015; Grosse & Martens, 2016) is an approximation algorithm for NGD. It approximates Fl(\u03b8t) by Kronecker product of two matrices\nFl(\u03b8t) + \u03c1I \u2248 (Bl + \u03c1BI)\u2297 (Al\u22121 + \u03c1AI), (S.74) where Bl = E [ \u03b4l\u03b4l \u22a4 ] and Al = E [ hlhl \u22a4 ] . Exponential moving average is often utilized to stabilize\nthe estimation of the K-FAC curvature matrix.\nA (t+1) l = \u03beA (t) l + (1\u2212 \u03be)E [ hlhl \u22a4 ] , (S.75)\nB (t+1) l = \u03beB (t) l + (1\u2212 \u03be)E [ \u03b4l\u03b4l \u22a4 ] . (S.76)"
        },
        {
            "heading": "C.2 SHAMPOO",
            "text": "Adaptive gradient descent utilizes the square root of batched empirical Fisher F batchemp (\u03b8) = EB\u223cpdata [ \u2207LB(\u03b8)\u2207LB(\u03b8)T ] , (S.77)\ninstead of F (\u03b8t) where\u2207LB(\u03b8) = 1|B| \u2211\n(x,t)\u2208B\u2207 log p\u03b8 (t | x) represents the mini-batch gradient. Shampoo approximates this batched empirical Fisher by Kronecker product of two matrices (Gupta et al., 2018).\nF batchemp (\u03b8) + \u03c1I \u2248 (Rl + \u03c1RI)\u2297 (Ll\u22121 + \u03c1LI), (S.78)\nwhere Ll = E[\u03b4lhl\u22121\u22a4hl\u22121\u03b4l\u22a4] and Rl = E[hl\u22121\u03b4l\u22a4\u03b4lhl\u22121\u22a4]. Summation is often utilized to stabilize the estimation of the Shampoo curvature matrix while exponential moving average is also well utilized.\nL (t+1) l = L (t) l + E[\u03b4lhl\u22121 \u22a4hl\u22121\u03b4l \u22a4], (S.79)\nR (t+1) l = R (t) l + E[hl\u22121\u03b4l \u22a4\u03b4lhl\u22121 \u22a4]. (S.80)\nThis summation prevents L(t)l and R (t) l from becoming a zero matrix, which ensures stable learning even at an infinite width limit.\nD ADDITIONAL EXPERIMENTS ON EMPIRICAL VERIFICATION OF \u00b5P\nD.1 ADDITIONAL EXPERIMENTS ON \u2206h\nExponential moving average If we take an exponential moving average over the curvature matrix, we cannot use the push-through identity in Eq. 11. However, even in this case, the behavior of \u2206h is consistent with the order evaluation. Figure S.1 shows that the exponential moving average has almost no effect on the order of \u2206h. This shows that even if we use an exponential moving average, \u00b5P is still reasonable.\n2\u22122\n20\n\u2206 h\nS P\n(K -F\nA C\n)\nema=0.01 ema=0.03 ema=0.1 ema=0.3 ema=1\n29 212\nwidth\n2\u22122\n20\n\u2206 h\nm u P\n(K -F\nA C\n)\n29 212\nwidth\n29 212\nwidth\n29 212\nwidth\n29 212\nwidth\n2\u22122\n20\n\u2206 h\nS P\n(S h a m\np o o )\nema=0.01 ema=0.03 ema=0.1 ema=0.3 ema=1\n29 212\nwidth\n2\u22122\n20\n\u2206 h\nm u P\n(S h a m\np o o )\n29 212\nwidth\n29 212\nwidth\n29 212\nwidth\n29 212\nwidth\nFigure S.1 : Exponential moving average does not affect order evaluation. Exponential moving average has almost no effect on the behavior of \u2206h. More precisely, regardless of the exponential moving average, \u2206hl decays in the input and hidden layers with increasing width when trained with SP, but it does not decay when trained with \u00b5P. We trained a 3-layer MLP on CIFAR10. In K-FAC we used rescaled damping.\n29 212\n2\u22122\n20\n\u2206 h\n(M L\nP )\nKFAC SP Heuristics\n29 212\nKFAC muP Heuristics\n29 212\nKFAC muP Rescaled\n29 212\nShampoo SP\n29 212\nShampoo muP\nFigure S.2 : \u00b5P achieves feature learning in MLP with tanh activation We trained a 3-layer MLP on MNIST using cross-entropy loss. Even if the activation function for the MLP is tanh, we observed the same behavior as in Figure 1.\nActivation Function The behavior of \u2206h is not affected by the activation function. We trained 3-layer MLP with tanh activation in Figure S.2 . It shows the same behavior as when we trained 3-layer MLP with relu activation (Figure 1)."
        },
        {
            "heading": "E ADDITIONAL EXPERIMENTS ON THE VARIANCE OF THE LAST LAYER",
            "text": ""
        },
        {
            "heading": "E.1 TRAIN CURVE",
            "text": "Figure S.3 shows the learning curves for the experiment in Figure 3. K-FAC can reach the NNGP solution in one step. Thus, we can observe that K-FAC reaches high test accuracy from the first step for b = 64. However, the accuracy at b = 1 is higher than that at b = 64 since the training process for b = 64 stays at the initial value.\nNote that the optimal learning rate of b = 64 is smaller than that of b = 0.5 or b = 1. This is because the NNGP solution is a sharp local solution, and a large learning rate makes the subsequent behavior unstable.\n0 20 40 epochs\n20\n40\n60\nTe st\nA cc\n(A) SGD\n0 20 40 epochs\n20\n40\n60\n80 (B) K-FAC\n0 20 40 epochs\n40\n60\n80\n(C) Shampoo\nb(output layer) 0.5 1 64\nFigure S.3 : Train Curve Comparison for different bL K-FAC achieves high accuracy in one step when bL = 64 since K-FAC can achieve NNGP solution in one step. However, K-FAC can not escape from the NNGP solution. In this figure, we trained 3-layer CNNs on Fashion-MNIST with batch size = 1024."
        },
        {
            "heading": "E.2 CHOICE OF LOSS FUNCTION",
            "text": "The experiment in Figure 3 is performed with MSE loss to clarify the connection with NNGP solution. This result is easily extended from MSE loss to cross-entropy loss. Figure S.4 shows that even with a cross-entropy loss, K-FAC shows higher accuracy only when bL is at or near 1 in the last layer.\n1 1 6 3 1 4 6 6 1 7 6 9 1 1 0 6 1 2 1 1 3 6\nIteration\n-2.0 -1.5 -1.0 -0.5 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 5.5 6.0 lo g 2 b (o u tp u t la y e r)\nSGD (Train Acc)\n1 1 6 3 1 4 6 6 1 7 6 9 1 1 0 6 1 2 1 1 3 6\nIteration\n-2.0 -1.5 -1.0 -0.5 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 5.5 6.0\nKFAC (Train Acc)\n1 1 6 3 1 4 6 6 1 7 6 9 1 1 0 6 1 2 1 1 3 6\nIteration\n-2.0 -1.5 -1.0 -0.5 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 5.5 6.0\nShampoo (Train Acc)\n65\n70\n75\n76\n78\n80\n82\n84\n76\n78\n80\n82\nFigure S.4 : K-FAC converges to NNGP solution even in the cross-entropy loss. We experimented with a 3-layer CNN, moving only the last layer bL from \u00b5P. The loss function is cross-entropy. K-FAC shows that the regions of high accuracy are concentrated only around b = 1."
        },
        {
            "heading": "E.3 CHOICE OF MODEL ARCHITECTURE",
            "text": "The experiment in Figure 3 was performed with a 3-layer CNN model. This result can be generalized to other architectures. Figure S.5 indicates that accuracy at bL = 1 is higher than bL = 0.5. In this example, train accuracy for bL \u226b 1 is consistently higher than the train accuracy for bL = 0.5. In ResNet-18, the maximum value appears to be obtained at bL = 2, but this may be due to the constant factor rather than order, which occurs when experiments are conducted with finite widths.\n2\u22121 21 23 25\nbL (output layer)\n70\n75\n80\n85\n90\n95\nA c c u ra\nc y\nTrain Acc(K-FAC)\n2\u22121 21 23 25\nbL (output layer)\n70\n75\n80\n85\n90\n95\nTest Acc(K-FAC)\nmodel MLP\nCNN Myrtle ResNet18\nFigure S.5 : \u00b5P is effective in various architecture. According to the K-FAC \u00b5P, bL = 1 has a higher accuracy than bL = 0.5. This is consistent across various models. MLPs are trained with MNIST, while other models are trained with FashionMNIST. The number of samples is reduced to 1024. MLP and CNN are trained in full batches, while Myrtle and ResNet18 are trained in mini-batches with a batch size of 128.\nE.4 ON THE EFFECT OF MOMENTUM\n1 1 6 3 1 4 6 6 1 7 6 9 1 1 0 6 1 2 1 1 3 6\nIteration\n-2.0 -1.5 -1.0 -0.5 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 5.5 6.0 lo g 2 b (o u tp u t la y e r)\nSGD (Train Acc)\n1 1 6 3 1 4 6 6 1 7 6 9 1 1 0 6 1 2 1 1 3 6\nIteration\n-2.0 -1.5 -1.0 -0.5 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 5.5 6.0\nKFAC (Train Acc)\n1 1 6 3 1 4 6 6 1 7 6 9 1 1 0 6 1 2 1 1 3 6\nIteration\n-2.0 -1.5 -1.0 -0.5 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 5.5 6.0\nShampoo (Train Acc)\n62\n64\n66\n68\n70\n76\n78\n80\n76\n78\n80\n82\nFigure S.6 : K-FAC converges to NNGP solution even without momentum. We experimented with a 3-layer CNN, moving only the last layer bL from \u00b5P. The loss function is cross-entropy. K-FAC shows that the regions of high accuracy are concentrated only around b = 1.\nFigure S.7 : \u00b5P is effective across batch-size without momentum We trained FashionMNIST with a 3-layer CNN with different batch sizes. We did not use momentum in this figure. The values represent the accuracy of the training dataset.\nOptimizer b Batch Size4 16 64 256 1024 SGD 0.5 80.52 78.69 75.05 67.94 49.70 1.0 82.85 80.41 77.58 73.12 64.30 64.0 83.59 81.23 77.25 73.63 70.53 K-FAC 0.5 77.60 79.66 83.94 82.14 78.63 1.0 79.10 81.69 83.92 83.16 80.27 64.0 76.06 76.95 77.28 76.04 75.94\nFigure S.6 and Figure S.7 represents the results for training 3-layer CNN without momentum. Since momentum is known to have the effect of escaping from the saddle point, it is expected that it is more difficult to escape from the NNGP solution without momentum. However, there was almost no effect of removing momentum on the relationship between accuracy and bL while the overall accuracy is lowered by removing momentum.\n1 2 8 .0\n2 5 6 .0\n5 1 2 .0\n1 0 2 4 .0\n2 0 4 8 .0\n4 0 9 6 .0\n8 1 9 2 .0\nwidth\n0 .2 5 0 .7\n5 1 .5\n4 .0\n1 6 .0b\n(o u\ntp u\nt la\ny er\n)\nKFAC (Analogy Acc)\n1 2 8 .0\n2 5 6 .0\n5 1 2 .0\n1 0 2 4 .0\n2 0 4 8 .0\n4 0 9 6 .0\n8 1 9 2 .0\nwidth\n0 .2 5 0 .7\n5 1 .5\n4 .0 1 6 .0\nShampoo (Analogy Acc)\n25\n30\n35\n40\n25\n30\n35\n40\nFigure S.8 : Effect of bL on Accuracies of Word Analogy In K-FAC, only a limited range of bL has high accuracy, whereas Shampoo has nearly 40% accuracy over a wide range of bL. cl is correctly set to match the \u00b5P setting."
        },
        {
            "heading": "E.5 WORD2VEC",
            "text": "Feature learning is especially important in word2vec, which requires careful tuning of bL in K-FAC. Figure S.8 examines the effect of bL on accuracy in K-FAC and Shampoo. In this figure, HPs are tuned with the following settings. Note that we tuned the variance of weights at width=128 since K-FAC is sensitive to the variance of the last layer.\n\u2022 learning rate = {1e-1, 1e-2, 1e-3 }\n\u2022 embedding weight multiplier = {1, 1e-1, 1e-2}\n\u2022 output weight multiplier = {1, 2, 4, 8, 16}\nAs shown in Figure S.8 , K-FAC achieves high accuracy only around bL = 1. On the other hand, Shampoo achieves high accuracy in a wide range of bL. This example implies that while K-FAC learns features in Word2Vec, K-FAC requires more careful tuning of bL than Shampoo."
        },
        {
            "heading": "F ADDITIONAL EXPERIMENTS ON LEARNING RATE TRANSFER",
            "text": ""
        },
        {
            "heading": "F.1 SHAMPOO",
            "text": "Figure S.9 shows that \u00b5P for Shampoo allows the learning rate tuned in a small model to be re-used in a larger model. In Shampoo, the learning rate is stable even in SP while in \u00b5P, the wider model achieves higher accuracy over a wide range of learning rates."
        },
        {
            "heading": "F.2 FOOF",
            "text": "Since we introduced general exponents eA and eB , we can consider \u00b5P for FOOF. The \u00b5P for FOOF is immediately derived from Proposition 10.\nbl =  0 l = 1 1/2 1 < l < L\n1 l = L\n, cl =  \u22121 l = 1 \u22121 1 < l < L 0 l = L , dAl = { \u22121 1 < l \u2264 L 0 l = 1 .\n(S.81)\nFigure S.10 shows that \u00b5P for FOOF enables the learning rate to transfer across the width as well as in SGD or K-FAC. The learning rate may be transferred across widths since the SP for FOOF is a stable parametrization. In addition, we can observe that accuracy obtained by training with \u00b5P is higher than SP, especially in wide models.\n\u221210 0 log2 learning rate\n60\n70\n80\n90\nT es\nt A\ncc\nSP MLP\n\u221210 0 log2 learning rate\nmuP MLP\nwidth 64\n128 256 512 1024 2048 4096 8192 \u221210 0 log2 learning rate 40 60\n80\nT es\nt A\ncc\nSP CNN\n\u221210 0 log2 learning rate\nmuP CNN\nwidth 8\n16 32 64 128 256\n\u221220 \u221210 0 log2 learning rate\n40\n60\n80\nT es\nt A\ncc\nSP Myrtle\n\u221220 \u221210 0 log2 learning rate\nmuP Myrtle\nwidth 32\n64 128 256 512 1024\n\u221210 0 log2 learning rate\n20\n40\n60\n80\nT es\nt A\ncc\nSP ResNet18\n\u221210 0 log2 learning rate\nmuP ResNet18\nwidth 1\n2 4 8\nFigure S.9 : \u00b5P for Shampoo allows the learning rate (\u03b7\u2032) to transfer across widths. We trained various models with SP and \u00b5P by Shampoo. We trained 3-layer MLP on MNIST, 3-layer CNN on FashionMNIST, Myrtle-5 on FashionMNIST and ResNet18 on FashionMNIST. The number of samples is reduced to 1024.\n\u221210 0 log2 learning rate\n60\n70\n80\n90\nT es\nt A\ncc\nSP MLP\n\u221210 0 log2 learning rate\nmuP MLP\nwidth 64\n128 256 512 1024 2048 4096 8192 \u221210 \u22125 0 log2 learning rate 40 60\n80\nT es\nt A\ncc\nSP CNN\n\u221210 \u22125 0 log2 learning rate\nmuP CNN\nwidth 8\n16 32 64 128 256\nFigure S.10 : \u00b5P for FOOF allows the learning rate (\u03b7\u2032) to transfer across widths. We trained a 3-layer MLP on MNIST and a 3-layer CNN on FashionMNIST. The number of samples is reduced to 1024."
        },
        {
            "heading": "F.3 LEARNING RATE TRANSFER WHEN THE SAMPLE SIZE IS FULL",
            "text": "In Figure 6, we reduced the training samples. By reducing the dataset size, finite-width models are known to behave more closely to infinite-width models, as has often been seen in papers examining the theoretical aspects of second-order optimization and feature learning(Geiger et al., 2020; Karakida & Osawa, 2020; ichi Amari et al., 2021). However, even when training on the full dataset, its learning rate can be transferred using \u00b5P as shown in Figure S.11 . In addition, we can observe that the learning rate is correctly transferred when we optimize ResNet50 on ImageNet. As shown in Figure S.1 , the optimal learning rate is fixed at 0.2 regardless of the width.\nLearning Rate width 0.025 0.050 0.100 0.200 0.400\n0.5 69.26 70.07 70.19 70.56 70.15 1.0 74.35 74.67 75.54 75.92 75.32 2.0 77.45 78.07 78.47 78.79 47.64\nTable S.1 : optimal learning rate does not shift in training ResNet50 on ImageNet. The optimal learning rate is fixed at 0.2 and does not shift when scaling the width."
        },
        {
            "heading": "F.4 ACTIVATION FUNCTION AND LOSS FUNCTION",
            "text": "In Figure 6, we trained 3-layer MLP with ReLU activation by MSE Loss. Similar observations can be made when using Tanh activation or cross-entropy loss as shown in Figure S.12 . Note that squashing\n\u221210 0 log2 LR\n70\n80\n90\nT e st\nA c c\n(M L\nP )\nSGD SP\n\u221210 0 log2 LR\nSGD muP\n\u221210 0 log2 LR\nK-FAC SP(Heuristics)\n\u221210 0 log2 LR\nK-FAC SP\n\u221210 0 log2 LR\nK-FAC muP width\n128 256 512 1024 2048 4096 8192 16384\n\u221210 0 log2 LR\n40\n60\n80\nT e st\nA c c\n(C N\nN )\nSGD SP\n\u221210 0 log2 LR\nSGD muP\n\u221210 0 log2 LR\nK-FAC SP(Heuristics)\n\u221210 0 log2 LR\nK-FAC SP\n\u221210 0 log2 LR\nK-FAC muP width\n8 16 32 64 128 256\nFigure S.11 : \u00b5P allows the learning rate (\u03b7\u2032) to transfer across widths(Full Dataset) Using \u00b5P, one can transfer the learning rate with respect to width. We trained a 3-layer MLP on\nFashionMNIST and a 3-layer CNN on FashionMNIST. In contrast to Figure 6, the sample size is not been reduced. In KFAC with SP, we use damping heuristics which prevent the transfer of learning\nrate.\nactivation functions such as tanh and softmax are not recommended in some experiments in Yang et al. (2021) D.3. However, the properties of the hyperparameter landscape for each parametrization in Figure S.12 are almost the same as in Figure 6."
        },
        {
            "heading": "G ADDITIONAL EXPERIMENTS ON TRAINING WIDE MODELS",
            "text": ""
        },
        {
            "heading": "G.1 ANOTHER DATASET AND MODELS",
            "text": "In Figure 3, we compare the test accuracy with SP and \u00b5P and find \u00b5P takes a higher accuracy than SP in wide models. This tendency is also true for other settings. As shown in Figure S.13 and Figure S.14 , \u00b5P achieves higher valuation accuracy than SP in many cases. Specifically, the difference between SP and \u00b5P is larger for wider models, and the difference is often larger in the early stages of learning."
        },
        {
            "heading": "G.2 ERROR BAR",
            "text": "Table.S.2 shows the results of the comparison of the accuracy of SP and \u00b5P with standard deviation. There is a significant difference in accuracy between SP and \u00b5P regardless of the effect of seed.\nVGG19 (C100) ResNet18 (C100) Shampoo K-FAC Shampoo\nwidth SP \u00b5P SP \u00b5P SP \u00b5P 1 63.30(0.26) 63.01(0.44) 67.02(0.33) 67.00(0.30) 67.01(0.33) 66.99(0.35) 2 70.10(0.23) 70.21(0.29) 71.88(0.36) 72.31(0.39) 70.83(0.18) 71.68(0.21) 4 74.65(0.24) 75.04(0.18) 74.09(0.33) 75.06(0.22) 73.80(0.26) 75.28(0.34) 8 76.51(0.38) 77.30(0.21) 74.90(0.20) 76.92(0.12) 76.23(0.21) 78.39(0.17) 16 77.89(0.18) 78.35(0.19) 74.29(0.41) 78.12(0.25) 78.05(0.21) 80.24(0.23)\nTable S.2 : Test accuracies with different widths (with error bar). The results are averaged over 5 random seeds, with standard deviation shown in the brackets. The settings are the same as in Table.3.\n\u221210 0 log2 LR\n40\n60 80 T e st A c c\n(T a n h\nM S E\n)\nSGD SP\n\u221210 0 log2 LR\nSGD muP\n\u221210 \u22125 0 log2 LR\nK-FAC SP(Heuristics)\n\u221210 \u22125 0 log2 LR\nK-FAC SP\n\u221210 \u22125 0 log2 LR\nK-FAC muP width\n64 128 256 512 1024 2048 4096 8192 16384\n\u221210 0 log2 LR\n60\n70\n80\n90\nT e st\nA c c\n(T a n h\nx e n t)\nSGD SP\n\u221210 0 log2 LR\nSGD muP\n\u221210 0 log2 LR\nK-FAC SP(Heuristics)\n\u221210 0 log2 LR\nK-FAC SP\n\u221210 0 log2 LR\nK-FAC muP width\n64 128 256 512 1024 2048 4096 8192 16384\n\u221210 \u22125 0 log2 LR\n60\n70\n80\n90\nT e st\nA c c\n(R e L\nU x e n t)\nSGD SP\n\u221210 \u22125 0 log2 LR\nSGD muP\n\u221210 \u22125 0 log2 LR\nK-FAC SP(Heuristics)\n\u221210 \u22125 0 log2 LR\nK-FAC SP\n\u221210 \u22125 0 log2 LR\nK-FAC muP width\n64 128 256 512 1024 2048 4096 8192 16384\nFigure S.12 : Learning rate transfer across different activation functions and loss functions. We trained a 3-layer MLP on MNIST. We used tanh activation with MSE loss in the first row, tanh activation with cross-entropy loss in the second row, and ReLU activation with cross-entropy loss in the last row. We trained on MNIST dataset and the sample size was reduced to 1024.\nEpochs=100 Epochs=200 Epochs=300 K-FAC Shampoo K-FAC Shampoo K-FAC Shampoo\nwid SP / \u00b5P SP / \u00b5P SP / \u00b5P SP / \u00b5P SP / \u00b5P SP / \u00b5P 1 89.54/+0.00 88.27/+0.43 91.35/+0.00 91.01/-0.25 91.97/+0.00 91.57/-0.14 2 92.10/+0.26 91.31/+0.40 93.92/+0.30 93.62/+0.14 94.32/+0.03 93.95/+0.05 4 93.64/+0.57 93.32/+0.22 95.11/+0.45 95.19/-0.15 95.57/+0.14 95.56/-0.10 8 93.56/+0.74 94.02/+0.33 95.44/+0.20 95.81/+0.19 95.73/+0.44 96.09/-0.04 16 93.00/+1.36 94.81/+0.96 95.04/+0.66 96.28/+0.14 95.31/+0.72 96.49/+0.24\nFigure S.13 : Test Accuracies of ResNet18 on CIFAR10. We trained ResNet18 on CIFAR100 with \u03b7 = 0.003, \u03c1\u2032 = 10. \u00b5P has a higher accuracy than SP in models with wide widths."
        },
        {
            "heading": "G.3 DISTANCE FROM INITIAL WEIGHT",
            "text": "The relative distance from their initial weights value \u2206W/|W | is often used in the analysis of infinitewidth models. If \u2206W/|W | is always zero throughout training, the model will fall into a lazy regime. As shown in Figure S.15 , in the early stages of training with SP, \u2206W/|W | of wide-width models is almost zero. As the training proceeds, the weights begin to deviate from their initial values, which can be considered as finite-width effects. The wider the width, the longer the time that \u2206W/|W | is almost zero. This suggests that the weights do not move away from their initial values in the infinite-width limit. In \u00b5P, however, the point at which weights start to move away from their initial values remains the same, even as the width increases, which implies that training can proceed even at an infinite width limit.\nEpochs=100 Epochs=200 Epochs=300 K-FAC Shampoo K-FAC Shampoo K-FAC Shampoo\nwid SP / \u00b5P SP / \u00b5P SP / \u00b5P SP / \u00b5P SP / \u00b5P SP / \u00b5P 1 68.53/+1.87 65.36/-0.31 71.55/+1.88 68.49/+0.30 72.28/+1.93 69.20/-0.02 2 71.28/+3.66 69.28/+0.54 73.85/+2.99 72.24/+0.11 74.30/+3.23 73.05/+0.20 4 69.45/+7.87 72.81/+0.78 73.39/+5.83 75.59/+0.38 73.79/+5.91 76.57/+0.31 8 61.15/+17.01 74.87/+0.28 68.13/+11.63 77.10/+1.64 68.56/+12.09 78.08/+1.74\nFigure S.14 : Test Accuracies of ResNet50 on CIFAR100. We trained ResNet50 on CIFAR100 with \u03b7 = 0.003, \u03c1\u2032 = 10. \u00b5P has a higher accuracy than SP in models with wide widths. Note that in the training of K-FAC, the standard deviation of the last layer at width=1 is tuned carefully and it is 1/16 of the default SP value.\n0 2000 4000 6000\nIteration\n0\n200\n400\n\u2206 W\nSP\n0 2000 4000 6000\nIteration\nmuP\nwidth 1\n2 4 8\nVGG19 CIFAR100 (LR=0.03)\n0 10000\nIteration\n0\n100\n200\n300\n\u2206 W\nSP\n0 10000\nIteration\nmuP\nwidth 1\n2 4 8\nResNet50 CIFAR100 (LR=0.01)\nFigure S.15 : For SP, \u2206W/|W | in the early stages of training depends on the model width In training with SP, \u2206W/|W | depends on the width, especially in the early stages of training. Specifically, as the width increases, the period during which \u2206Wl/|Wl| \u2248 0 gets longer. This suggests that in the infinite width limit with SP, \u2206W/|W | remains 0 throughout training. On the other hand, in \u00b5P, the point at which weights start to move away from their initial values remains the same, even as the width increases (Left) We trained VGG19 on CIFAR100 using K-FAC. (Right) We trained ResNet50 on CIFAR100 using K-FAC."
        },
        {
            "heading": "H ADDITIONAL EXPERIMENTS ON DAMPING TRANSFER",
            "text": "H.1 ON FIXED DAMPING SCALING\n28 211 214\nlog2 width\n80\n85\n90\nT es\nt A\ncc\nDamping Scaling (MLP)\n(dA, dB)\n(dA, dB)=(1, 1) (dA, dB)=(1, -1) (dA, dB)=(-1, 1) (dA, dB)=(-1, -1) (dA, dB)=(0, 0) Figure S.16 : Damping term needs to be scaled as dA = \u22121 and dB = 1 (Eq. 8). We trained a 3-layer MLP on MNIST with K-FAC. The number of samples is reduced to 1024 and trained by MSE loss. Validation accuracy is highest at dA = \u22121 and dB = 1\nIn this paper, we determine damping mainly by Eq. 16 as this is consistent with \u00b5P. However, even if damping is determined by Eq. 8, damping is still consistent with \u00b5P. In Figure S.16 , we trained 3-layer MLP with damping determined by Eq. 8. Its learning rate is fixed at 0.01. \u03c1\u2032A and \u03c1 \u2032 B are tuned using the following grid:\n\u2022 \u03c1\u2032A = 2 z , where z \u2208 {0, 1, 2, ..., 10} \u2022 \u03c1\u2032B = 2 z , where z \u2208 {0,\u22121,\u22122, ...,\u221210}\nFigure S.16 shows that the optimal damping value for wide width models is given at (dA, dB) = (-1, 1) in Eq. 8. This is consistent with the scaling in Eq. 10.\nThere are two reasons why we determine damping using Eq. 16 instead of Eq. 8. The first reason is that the optimal values for \u03c1\u2032A and \u03c1 \u2032 B are generally different, resulting in one extra hyperparameter.\n0 20 40\nEpoch\n10\u22122\n10\u22121\n100\nC IF\nA R\n1 0\nMeanEig A\n0 20 40\nEpoch\n10\u22124\n10\u22122\nMeanEig B\n0 20 40\nEpoch\n100\n101\n102\ndamping A\n0 20 40\nEpoch\n10\u22122\n10\u22121\n100\ndamping B\n0 20 40\nEpoch\n10\u22122\n10\u22121\n100\nC IF\nA R\n1 0 0\nMeanEig A\n0 20 40\nEpoch\n10\u22123\n10\u22121\nMeanEig B\n0 20 40\nEpoch\n100\n101\n102\ndamping A\n0 20 40\nEpoch\n10\u22122\n10\u22121\n100\ndamping B\nFigure S.17 : Mean eigenvalues and the heuristics damping terms vary during training. We trained ResNet18 on CIFAR10 and CIFAR100 by K-FAC. (Left) Transition of the mean eigenvalues of Al and Bl in each layer during the training process. We can observe that the mean eigenvalues change significantly during the training. This implies that \u03c1A and \u03c1B need to be modified during the training to retain their second-order properties for numerical stability. Note that the transition of damping determined by Eq. 16 is consistent with the transition of mean eigenvalues. (Right) Transition of the damping determined by Eq. 15. The trend of damping determined by Eq. 15 is generally consistent with that of mean eigenvalues.\nThe second reason is that the eigenvalues of the curvature matrix may change over training and the optimal values for \u03c1\u2032A and \u03c1 \u2032 B may change during training(Zhang et al., 2019b). Figure S.16 shows that the mean eigenvalue of Al and Bl change during the training and the damping determined by Eq. 16 and Eq. 15 is adjusted according to the change of mean eigenvalue.\nH.2 DAMPING TRANSFER ON MLP\n\u221210 0 10 log2 damping\n60\n70\n80\n90\nT es\nt A\ncc\nHeuristics (SP)\n0 10\nlog2 damping\nRescaled (SP)\n0 10\nlog2 damping\nRescaled (muP)\nwidth 128\n256\n512\n1024\n2048\n4096\n8192\n\u221220 0 log2 damping\n60\n70\n80\n90\nT es\nt A\ncc\nHeuristics (SP)\n0 10\nlog2 damping\nRescaled (SP)\n0 10\nlog2 damping\nRescaled (muP)\nwidth 128\n256\n512\n1024\n2048\n4096\n8192\nFigure S.18 : We can transfer damping in MLP We trained a 3- layer MLP on MNIST and a 3-layer MLP on FashionMNIST with KFAC. When training MNIST, the number of samples is reduced to 1024 while in FashionMNIST we trained MLP by full-dataset. When using heuristics damping, the maximum damping value that does not diverge increases as the width increases.\nIn Section 5.3, we have observed that when using heuristics damping, the optimal damping value shifts as the width increases. We have also confirmed that rescaling damping prevents this shift. This\nphenomenon is observed not only in CNN but also in MLP as shown in Figure S.18 . In the second column of Figure S.18 , the dataset size is not reduced, but again the damping is transferred by using rescaling damping."
        }
    ],
    "year": 2023
}