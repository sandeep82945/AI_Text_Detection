{
    "abstractText": "Training and deploying machine learning models that meet fairness criteria for protected groups are fundamental in modern artificial intelligence. While numerous constraints and regularization terms have been proposed in the literature to promote fairness in machine learning tasks, most of these approaches are not amenable to stochastic optimization due to the complex and nonlinear structure of constraints and regularizers. Here, the term \u201cstochastic\u201d refers to the ability of the algorithm to work with small mini-batches of data. Motivated by the limitation of existing literature, this paper presents a unified stochastic optimization framework for fair empirical risk minimization based on f -divergence measures (f -FERM). The proposed stochastic algorithm enjoys theoretical convergence guarantees. In addition, our experiments demonstrate the superiority of fairness-accuracy tradeoffs offered by f -FERM for almost all batch sizes (ranging from full-batch to batch size of one). Moreover, we show that our framework can be extended to the case where there is a distribution shift from training to the test data. Our extension is based on a distributionally robust optimization reformulation of f -FERM objective under lp norms as uncertainty sets. Again, in this distributionally robust setting, f -FERM not only enjoys theoretical convergence guarantees but also outperforms other baselines in the literature in the tasks involving distribution shifts. An efficient stochastic implementation of f -FERM is publicly available 1.",
    "authors": [
        {
            "affiliations": [],
            "name": "Sina Baharlouei"
        },
        {
            "affiliations": [],
            "name": "Shivam Patel"
        },
        {
            "affiliations": [],
            "name": "Meisam Razaviyayn"
        }
    ],
    "id": "SP:0a3fc6ae7607b5087d2464e0298b8de5f2221e7d",
    "references": [
        {
            "authors": [
                "Sina Aghaei",
                "Mohammad Javad Azizi",
                "Phebe Vayanos"
            ],
            "title": "Learning optimal and fair decision trees for non-discriminative decision-making",
            "venue": "In Proceedings of the AAAI conference on artificial intelligence,",
            "year": 2019
        },
        {
            "authors": [
                "Muhammad Aurangzeb Ahmad",
                "Carly Eckert",
                "Ankur Teredesai"
            ],
            "title": "Interpretable machine learning in healthcare",
            "venue": "In Proceedings of the 2018 ACM international conference on bioinformatics, computational biology, and health informatics,",
            "year": 2018
        },
        {
            "authors": [
                "Ahmad Ajalloeian",
                "Sebastian U. Stich"
            ],
            "title": "Analysis of SGD with biased gradient estimators",
            "venue": "CoRR, abs/2008.00051,",
            "year": 2020
        },
        {
            "authors": [
                "Wael Alghamdi",
                "Hsiang Hsu",
                "Haewon Jeong",
                "Hao Wang",
                "Peter Michalak",
                "Shahab Asoodeh",
                "Flavio Calmon"
            ],
            "title": "Beyond adult and compas: Fair multi-class prediction via information projection",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Martin Arjovsky",
                "L\u00e9on Bottou",
                "Ishaan Gulrajani",
                "David Lopez-Paz"
            ],
            "title": "Invariant risk minimization",
            "venue": "arXiv preprint arXiv:1907.02893,",
            "year": 2019
        },
        {
            "authors": [
                "Sina Baharlouei",
                "Maher Nouiehed",
                "Ahmad Beirami",
                "Meisam Razaviyayn"
            ],
            "title": "R\u00e9nyi fair inference",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Sina Baharlouei",
                "Fatemeh Sheikholeslami",
                "Meisam Razaviyayn",
                "Zico Kolter"
            ],
            "title": "Improving adversarial robustness via joint classification and multiple explicit detection classes",
            "venue": "Proceedings of The 26th International Conference on Artificial Intelligence and Statistics,",
            "year": 2023
        },
        {
            "authors": [
                "Yogesh Balaji",
                "Swami Sankaranarayanan",
                "Rama Chellappa"
            ],
            "title": "Metareg: Towards domain generalization using meta-regularization",
            "venue": "Advances in neural information processing systems,",
            "year": 2018
        },
        {
            "authors": [
                "Alexandre Belloni",
                "Victor Chernozhukov",
                "Lie Wang"
            ],
            "title": "Square-root lasso: pivotal recovery of sparse signals via conic programming",
            "year": 2011
        },
        {
            "authors": [
                "Jose Blanchet",
                "Yang Kang",
                "Karthyek Murthy"
            ],
            "title": "Robust wasserstein profile inference and applications to machine learning",
            "venue": "Journal of Applied Probability,",
            "year": 2019
        },
        {
            "authors": [
                "Roberto Boselli",
                "Mirko Cesarini",
                "Fabio Mercorio",
                "Mario Mezzanzanica"
            ],
            "title": "Classifying online job advertisements through machine learning",
            "venue": "Future Generation Computer Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Joy Buolamwini",
                "Timnit Gebru"
            ],
            "title": "Gender shades: Intersectional accuracy disparities in commercial gender classification",
            "venue": "In Conference on fairness, accountability and transparency,",
            "year": 2018
        },
        {
            "authors": [
                "Nicholas Carlini",
                "David Wagner"
            ],
            "title": "Towards evaluating the robustness of neural networks",
            "venue": "IEEE Symposium on Security and Privacy (SP),",
            "year": 2017
        },
        {
            "authors": [
                "Alessandro Castelnovo",
                "Riccardo Crupi",
                "Greta Greco",
                "Daniele Regoli",
                "Ilaria Giuseppina Penco",
                "Andrea Claudio Cosentini"
            ],
            "title": "A clarification of the nuances in the fairness metrics landscape",
            "venue": "Scientific Reports,",
            "year": 2022
        },
        {
            "authors": [
                "Jie Chen",
                "Ronny Luss"
            ],
            "title": "Stochastic gradient descent with biased but consistent gradient estimators",
            "venue": "CoRR, abs/1807.11880,",
            "year": 2018
        },
        {
            "authors": [
                "Jaewoong Cho",
                "Gyeongjo Hwang",
                "Changho Suh"
            ],
            "title": "A fair classifier using kernel density estimation",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Imre Csisz\u00e1r"
            ],
            "title": "Information-type measures of difference of probability distributions and indirect observation. studia scientiarum",
            "venue": "Mathematicarum Hungarica,",
            "year": 1967
        },
        {
            "authors": [
                "Jessica Dai",
                "Sarah M Brown"
            ],
            "title": "Label bias, label shift: Fair machine learning with unreliable labels",
            "venue": "In NeurIPS 2020 Workshop on Consequential Decision Making in Dynamic Environments,",
            "year": 2020
        },
        {
            "authors": [
                "Constantinos Daskalakis",
                "Stratis Skoulakis",
                "Manolis Zampetakis"
            ],
            "title": "The complexity of constrained min-max optimization",
            "venue": "In Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing,",
            "year": 2021
        },
        {
            "authors": [
                "Yuyang Deng",
                "Mohammad Mahdi Kamani",
                "Pouria Mahdavinia",
                "Mehrdad Mahdavi"
            ],
            "title": "Distributed personalized empirical risk minimization",
            "venue": "In International Workshop on Federated Learning for Distributed Data Mining,",
            "year": 2023
        },
        {
            "authors": [
                "Frances Ding",
                "Moritz Hardt",
                "John Miller",
                "Ludwig Schmidt"
            ],
            "title": "Retiring adult: New datasets for fair machine learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Michele Donini",
                "Luca Oneto",
                "Shai Ben-David",
                "John S Shawe-Taylor",
                "Massimiliano Pontil"
            ],
            "title": "Empirical risk minimization under fairness constraints",
            "venue": "Advances in neural information processing systems,",
            "year": 2018
        },
        {
            "authors": [
                "Wei Du",
                "Xintao Wu"
            ],
            "title": "Fair and robust classification under sample selection bias",
            "venue": "In Proceedings of the 30th ACM International Conference on Information & Knowledge Management,",
            "year": 2021
        },
        {
            "authors": [
                "Cynthia Dwork",
                "Moritz Hardt",
                "Toniann Pitassi",
                "Omer Reingold",
                "Richard Zemel"
            ],
            "title": "Fairness through awareness",
            "venue": "In Proceedings of the 3rd innovations in theoretical computer science conference,",
            "year": 2012
        },
        {
            "authors": [
                "Tongtong Fang",
                "Nan Lu",
                "Gang Niu",
                "Masashi Sugiyama"
            ],
            "title": "Rethinking importance weighting for deep learning under distribution shift",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Pierre Foret",
                "Ariel Kleiner",
                "Hossein Mobahi",
                "Behnam Neyshabur"
            ],
            "title": "Sharpness-aware minimization for efficiently improving generalization",
            "venue": "arXiv preprint arXiv:2010.01412,",
            "year": 2020
        },
        {
            "authors": [
                "Stephen Giguere",
                "Blossom Metevier",
                "Bruno Castro da Silva",
                "Yuriy Brun",
                "Philip S Thomas",
                "Scott Niekum"
            ],
            "title": "Fairness guarantees under demographic shift",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Vincent Grari",
                "Sylvain Lamprier",
                "Marcin Detyniecki"
            ],
            "title": "Fairness-aware neural r\u00e9nyi minimization for continuous features",
            "venue": "In Twenty-Ninth International Joint Conference on Artificial Intelligence and Seventeenth Pacific Rim International Conference on Artificial Intelligence",
            "year": 2020
        },
        {
            "authors": [
                "Moritz Hardt",
                "Eric Price",
                "Nati Srebro"
            ],
            "title": "Equality of opportunity in supervised learning",
            "venue": "Advances in neural information processing systems,",
            "year": 2016
        },
        {
            "authors": [
                "Hisham Husain"
            ],
            "title": "Distributional robustness with ipms and links to regularization and gans",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Aleksandr Davidovich Ioffe",
                "Vladimir"
            ],
            "title": "Mihajlovi\u010d Tihomirov",
            "venue": "Theory of extremal problems. Elsevier,",
            "year": 2009
        },
        {
            "authors": [
                "Ray Jiang",
                "Aldo Pacchiano",
                "Tom Stepleton",
                "Heinrich Jiang",
                "Silvia Chiappa"
            ],
            "title": "Wasserstein fair classification",
            "venue": "In Uncertainty in artificial intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Faisal Kamiran",
                "Toon Calders"
            ],
            "title": "Data preprocessing techniques for classification without discrimination",
            "venue": "Knowledge and information systems,",
            "year": 2012
        },
        {
            "authors": [
                "Weiwei Kong",
                "Renato DC Monteiro"
            ],
            "title": "An accelerated inexact proximal point method for solving nonconvex-concave min-max problems",
            "venue": "SIAM Journal on Optimization,",
            "year": 2021
        },
        {
            "authors": [
                "Alex Krizhevsky",
                "Ilya Sutskever",
                "Geoffrey E Hinton"
            ],
            "title": "Imagenet classification with deep convolutional neural networks",
            "venue": "Communications of the ACM,",
            "year": 2017
        },
        {
            "authors": [
                "Daniel Kuhn",
                "Peyman Mohajerin Esfahani",
                "Viet Anh Nguyen",
                "Soroosh Shafieezadeh-Abadeh"
            ],
            "title": "Wasserstein distributionally robust optimization: Theory and applications in machine learning",
            "venue": "In Operations research & management science in the age of analytics,",
            "year": 2019
        },
        {
            "authors": [
                "Tosca Lechner",
                "Shai Ben-David",
                "Sushant Agarwal",
                "Nivasini Ananthakrishnan"
            ],
            "title": "Impossibility results for fair representations",
            "venue": "arXiv preprint arXiv:2107.03483,",
            "year": 2021
        },
        {
            "authors": [
                "Daniel Levy",
                "Yair Carmon",
                "John C Duchi",
                "Aaron Sidford"
            ],
            "title": "Large-scale methods for distributionally robust optimization",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Jiajin Li",
                "Linglingzhi Zhu",
                "Anthony Man-Cho So"
            ],
            "title": "Nonsmooth nonconvex-nonconcave minimax optimization: Primal-dual balancing and iteration complexity analysis, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Tian Li",
                "Ahmad Beirami",
                "Maziar Sanjabi",
                "Virginia Smith"
            ],
            "title": "Tilted empirical risk minimization",
            "venue": "arXiv preprint arXiv:2007.01162,",
            "year": 2020
        },
        {
            "authors": [
                "Tianyi Lin",
                "Chi Jin",
                "Michael Jordan"
            ],
            "title": "On gradient descent ascent for nonconvex-concave minimax problems",
            "venue": "Proceedings of the 37th International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Andrew Lowy",
                "Sina Baharlouei",
                "Rakesh Pavan",
                "Meisam Razaviyayn",
                "Ahmad Beirami"
            ],
            "title": "A stochastic optimization framework for fair risk minimization",
            "venue": "tmlr,",
            "year": 2022
        },
        {
            "authors": [
                "Yiwei Lu",
                "Guojun Zhang",
                "Sun Sun",
                "Hongyu Guo",
                "Yaoliang Yu"
            ],
            "title": "$f$-MICL: Understanding and generalizing infoNCE-based contrastive learning",
            "venue": "Transactions on Machine Learning Research,",
            "year": 2023
        },
        {
            "authors": [
                "Luo Luo",
                "Haishan Ye",
                "Zhichao Huang",
                "Tong Zhang"
            ],
            "title": "Stochastic recursive gradient descent ascent for stochastic nonconvex-strongly-concave minimax problems",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Aleksander Madry",
                "Aleksandar Makelov",
                "Ludwig Schmidt",
                "Dimitris Tsipras",
                "Adrian Vladu"
            ],
            "title": "Towards deep learning models resistant to adversarial attacks",
            "venue": "arXiv preprint arXiv:1706.06083,",
            "year": 2017
        },
        {
            "authors": [
                "Subha Maity",
                "Debarghya Mukherjee",
                "Mikhail Yurochkin",
                "Yuekai Sun"
            ],
            "title": "Does enforcing fairness mitigate biases caused by subpopulation shift",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Sadhika Malladi",
                "Tianyu Gao",
                "Eshaan Nichani",
                "Alex Damian",
                "Jason D Lee",
                "Danqi Chen",
                "Sanjeev Arora"
            ],
            "title": "Fine-tuning language models with just forward passes",
            "venue": "arXiv preprint arXiv:2305.17333,",
            "year": 2023
        },
        {
            "authors": [
                "Jeremie Mary",
                "Cl\u00e9ment Calauz\u00e8nes",
                "Noureddine El Karoui"
            ],
            "title": "Fairness-aware learning for continuous attributes and treatments",
            "venue": "Proceedings of the 36th International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Alan Mishler",
                "Niccol\u00f2 Dalmasso"
            ],
            "title": "Fair when trained, unfair when deployed: Observable fairness measures are unstable in performative prediction settings",
            "venue": "arXiv preprint arXiv:2202.05049,",
            "year": 2022
        },
        {
            "authors": [
                "Maher Nouiehed",
                "Maziar Sanjabi",
                "Tianjian Huang",
                "Jason D Lee",
                "Meisam Razaviyayn"
            ],
            "title": "Solving a class of non-convex min-max games using iterative first order methods",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Dmitrii M Ostrovskii",
                "Babak Barazandeh",
                "Meisam Razaviyayn"
            ],
            "title": "Nonconvex-nonconcave min-max optimization with a small maximization domain",
            "venue": "arXiv preprint arXiv:2110.03950,",
            "year": 2021
        },
        {
            "authors": [
                "Dmitrii M Ostrovskii",
                "Andrew Lowy",
                "Meisam Razaviyayn"
            ],
            "title": "Efficient search of first-order nash equilibria in nonconvex-concave smooth min-max problems",
            "venue": "SIAM Journal on Optimization,",
            "year": 2021
        },
        {
            "authors": [
                "Yury Polyanskiy",
                "Yihong Wu"
            ],
            "title": "Information theory: From coding to learning",
            "venue": "Book draft,",
            "year": 2022
        },
        {
            "authors": [
                "Flavien Prost",
                "Hai Qian",
                "Qiuwen Chen",
                "Ed H Chi",
                "Jilin Chen",
                "Alex Beutel"
            ],
            "title": "Toward a better trade-off between performance and fairness with kernel-based distribution matching",
            "year": 1910
        },
        {
            "authors": [
                "H Rafique",
                "M Liu",
                "Q Lin",
                "T Yang"
            ],
            "title": "Non-convex min\u2013max optimization: provable algorithms and applications in machine learning (2018)",
            "venue": "arXiv preprint arXiv:1810.02060,",
            "year": 2060
        },
        {
            "authors": [
                "Hassan Rafique",
                "Mingrui Liu",
                "Qihang Lin",
                "Tianbao Yang"
            ],
            "title": "Weakly-convex\u2013concave min\u2013max optimization: provable algorithms and applications in machine learning",
            "venue": "Optimization Methods and Software,",
            "year": 2022
        },
        {
            "authors": [
                "Meisam Razaviyayn",
                "Tianjian Huang",
                "Songtao Lu",
                "Maher Nouiehed",
                "Maziar Sanjabi",
                "Mingyi Hong"
            ],
            "title": "Nonconvex min-max optimization: Applications, challenges, and recent theoretical advances",
            "venue": "IEEE Signal Processing Magazine,",
            "year": 2020
        },
        {
            "authors": [
                "Meisam Razaviyayn",
                "Tianjian Huang",
                "Songtao Lu",
                "Maher Nouiehed",
                "Maziar Sanjabi",
                "Mingyi Hong"
            ],
            "title": "Nonconvex min-max optimization: Applications, challenges, and recent theoretical advances",
            "venue": "IEEE Signal Processing Magazine,",
            "year": 2020
        },
        {
            "authors": [
                "Ashkan Rezaei",
                "Anqi Liu",
                "Omid Memarrast",
                "Brian D Ziebart"
            ],
            "title": "Robust fairness under covariate shift",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Jessica Schrouff",
                "Natalie Harris",
                "Oluwasanmi Koyejo",
                "Ibrahim Alabdulmohsin",
                "Eva Schnider",
                "Krista Opsahl-Ong",
                "Alex Brown",
                "Subhrajit Roy",
                "Diana Mincu",
                "Christina Chen"
            ],
            "title": "Maintaining fairness across distribution shift: do we have viable solutions for real-world applications",
            "venue": "arXiv preprint arXiv:2202.01034,",
            "year": 2022
        },
        {
            "authors": [
                "Changjian Shui",
                "Gezheng Xu",
                "Qi Chen",
                "Jiaqi Li",
                "Charles X Ling",
                "Tal Arbel",
                "Boyu Wang",
                "Christian Gagn\u00e9"
            ],
            "title": "On learning fairness and accuracy on multiple subgroups",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Harvineet Singh",
                "Rina Singh",
                "Vishwali Mhasawade",
                "Rumi Chunara"
            ],
            "title": "Fairness violations and mitigation under covariate shift",
            "venue": "In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency,",
            "year": 2021
        },
        {
            "authors": [
                "Aman Sinha",
                "Hongseok Namkoong",
                "John Duchi"
            ],
            "title": "Certifying some distributional robustness with principled adversarial training",
            "venue": "In International Conference on Learning Representations,",
            "year": 2018
        },
        {
            "authors": [
                "Matthew Staib",
                "Stefanie Jegelka"
            ],
            "title": "Distributionally robust optimization and generalization in kernel methods",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Bahar Taskesen",
                "Viet Anh Nguyen",
                "Daniel Kuhn",
                "Jose Blanchet"
            ],
            "title": "A distributionally robust approach to fair classification, 2020",
            "year": 2020
        },
        {
            "authors": [
                "Kiran K Thekumparampil",
                "Prateek Jain",
                "Praneeth Netrapalli",
                "Sewoong Oh"
            ],
            "title": "Efficient algorithms for smooth minimax optimization",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Berk Ustun",
                "Yang Liu",
                "David Parkes"
            ],
            "title": "Fairness without harm: Decoupled classifiers with preference guarantees",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Mingyang Wan",
                "Daochen Zha",
                "Ninghao Liu",
                "Na Zou"
            ],
            "title": "Modeling techniques for machine learning fairness: A survey",
            "venue": "CoRR, abs/2111.03015,",
            "year": 2021
        },
        {
            "authors": [
                "Haotao Wang",
                "Junyuan Hong",
                "Jiayu Zhou",
                "Zhangyang Wang"
            ],
            "title": "How robust is your fairness? evaluating and sustaining fairness under unseen distribution shifts",
            "venue": "Transactions on Machine Learning Research,",
            "year": 2023
        },
        {
            "authors": [
                "Serena Wang",
                "Wenshuo Guo",
                "Harikrishna Narasimhan",
                "Andrew Cotter",
                "Maya Gupta",
                "Michael Jordan"
            ],
            "title": "Robust optimization for fairness with noisy protected groups",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Yang Xin",
                "Lingshuang Kong",
                "Zhi Liu",
                "Yuling Chen",
                "Yanmiao Li",
                "Hongliang Zhu",
                "Mingcheng Gao",
                "Haixia Hou",
                "Chunhua Wang"
            ],
            "title": "Machine learning and deep learning methods for cybersecurity",
            "venue": "Ieee access,",
            "year": 2018
        },
        {
            "authors": [
                "Ke Yan",
                "Lu Kou",
                "David Zhang"
            ],
            "title": "Learning domain-invariant subspace using domain features and independence maximization",
            "venue": "IEEE transactions on cybernetics,",
            "year": 2017
        },
        {
            "authors": [
                "Muhammad Bilal Zafar",
                "Isabel Valera",
                "Manuel Gomez Rogriguez",
                "Krishna P Gummadi"
            ],
            "title": "Fairness constraints: Mechanisms for fair classification",
            "venue": "In Artificial intelligence and statistics,",
            "year": 2017
        },
        {
            "authors": [
                "Rich Zemel",
                "Yu Wu",
                "Kevin Swersky",
                "Toni Pitassi",
                "Cynthia Dwork"
            ],
            "title": "Learning fair representations",
            "venue": "In International conference on machine learning,",
            "year": 2013
        },
        {
            "authors": [
                "Xuan Zhang",
                "Necdet Serhat Aybat",
                "Mert Gurbuzbalaban"
            ],
            "title": "Sapd+: An accelerated stochastic method for nonconvex-concave minimax problems",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Meiyu Zhong",
                "Ravi Tandon"
            ],
            "title": "Learning fair classifiers via min-max f-divergence regularization, 2023",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Machine learning models are increasingly deployed in critical applications ranging from healthcare (Ahmad et al., 2018) to image processing (Krizhevsky et al., 2017), education to job recruitment (Boselli et al., 2018), and social networking to cybersecurity (Xin et al., 2018). Machine learning practitioners have adopted learning algorithms to fathom inherently difficult and crucial problems. However, na\u00efve deployment of these models may lead to serious shortcomings such as biased predictions against minority groups (Angwin et al., 2016; Buolamwini & Gebru, 2018), vulnerability to adversarial attacks (Madry et al., 2017; Carlini & Wagner, 2017; Baharlouei et al., 2023), or lack of generalizability (Arjovsky et al., 2019). Consequently, it is of utmost importance to have reliable and trustworthy models that are, in particular, fair and comply with equality norms and provisions worldwide (Act, 1964; Elford, 2023).\nWith the increasing concern for the trustworthiness of unchecked machine learning algorithms, a broad class of paradigms has been proposed to counteract and mitigate both the cause and effects of model unreliability. Imposing statistical independence between model output and particular input features is of interest in various domains, especially when the generalization of a trained model is based on a collection of spurious features present in the training dataset (Dwork et al., 2012; Hardt et al., 2016; Yan et al., 2017). These could be sensitive features like gender, race, age, and/or income in the context of fairness or confounding factors like environmental artifacts in the context of image classification (Arjovsky et al., 2019). Existing literature on imposing statistical independence between selected input features and model outputs is directed into three approaches: pre-processing, post-processing, and in-processing methods.\nPre-processing methods entail upstream changes made in datasets to mask sensitive features or reduce the dependency of output variables on sensitive features through transforming data in a stage before the training phase (Kamiran & Calders, 2012; Zemel et al., 2013; Ustun et al., 2019). Post-processing methods involve model-specific adjustments to the model\u2019s output to ensure the independence of\n\u2217University of Southern California (baharlou,razaviya@usc.edu) \u2020Department of Electrical Engineering, IIT Bombay (shivamapatel2002@gmail.com) 1https://github.com/optimization-for-data-driven-science/f-FERM\npredictions and sensitive attributes (Hardt et al., 2016; Alghamdi et al., 2022). While pre-processing and post-processing methods do not affect the training procedure, they fail to exploit underlying training mechanisms for the best achievable accuracy-fairness tradeoffs. Unsurprisingly enough, optimizing accuracy and fairness jointly (in-processing) leads to better tradeoffs than sequentially optimizing fairness and accuracy in a pre-processing or post-processing fashion.\nIn-processing methods alternatively add fairness constraints or regularizers, penalizing dependence between sensitive attributes and output variables. (Zafar et al., 2017) utilizes covariance as the measure of independence between the sensitive attributes and the predictions. While such a measure is amenable to stochastic updates, it fails to capture correlations beyond linear. Alternatively, several non-linear measures such as R\u00e9nyi correlation (Baharlouei et al., 2020), \u03c72 divergence (Lowy et al., 2022), L\u221e distance (Donini et al., 2018), and Maximum Mean Discrepancy (MMD) (Prost et al., 2019) are proposed in the literature to establish the independence of the predictors and sensitive attributes. In-processing techniques can be model-specific (Wan et al., 2021; Aghaei et al., 2019) or generalizable to different training algorithms (Baharlouei et al., 2020; Lowy et al., 2022).\nIn the spirit of in-processing methods, input data-driven constraints or regularization terms are used to modify training objectives of problems like learning generalizable models to new environments, invariant learning, and learning in the presence of distribution shifts (Arjovsky et al., 2019; Mary et al., 2019; Baharlouei et al., 2020). Such constrained/regularized reformulations are prevalent in learning robust classifiers against adversarial attacks (Sinha et al., 2018), meta-learning (Balaji et al., 2018), federated learning (Deng et al., 2023), and alternative learning paradigms such as learning distributionally robust optimization (DRO) models (Kuhn et al., 2019; Levy et al., 2020), tilted empirical risk minimization (TERM) (Li et al., 2020), and Squared-root Lasso (Belloni et al., 2011).\nWhile in-processing techniques outperform pre-processing and post-processing approaches, they are not scalable to large datasets because of a lack of adaptability to stochastic optimization (Mary et al., 2019; Lowy et al., 2022). All aforementioned examples consist of regularization terms in their objective functions where the gradient cannot be described as a linear combination of data point functions. As a result, applying stochastic gradient descent or other stochastic first-order methods on the objective functions of such problems might not converge, especially for small batch sizes.\nMotivated by this, (Lowy et al., 2022) proposes a stochastic optimization framework for Exponential R\u00e9nyi Mutual Information as the measure of independence. More recently Zhong & Tandon (2023) use f -divergences as regularization terms to establish the independence between sensitive attributes and predictions. They estimate the f -divergence regularizers offline through multi-layer neural networks to avoid the computational challenges of devising scalable stochastic methods for nonconvex min-max problems. Our approach, on the other hand, directly solves the variational formulation for both full-batch and stochastic settings with convergence guarantees to non-spurious solutions. In Section 2, using the variational representation of f -divergences, we present a convergent stochastic optimization framework for fair learning via f -divergences. (Lowy et al., 2022) is a special case of f -divergences where f(t) = t2 \u2212 1 (\u03c72 divergence). Aside from \u03c72, all other divergences listed in Table 1 are not introduced in the literature to the best of our knowledge.\nDesigning convergent stochastic algorithms for fair empirical risk minimization can be further explored in scenarios involving changes in the data distribution from the source to the target domain. Detection and mitigation of biases against protected groups in the presence of distribution shifts have been extensively studied in recent years. Lechner et al. (2021) theoretically shows that learning fair representations (pre-processing) is nearly impossible for the popular notions of fairness, such as demographic parity in the presence of the distribution shift. Ding et al. (2021), on the other hand, experimentally demonstrates that applying post-processing fairness techniques (Hardt et al., 2016) to learn fair predictors of income concerning race, gender, and age fails to transfer from one US state (training domain) to another state. Overlooking distribution shifts can lead to catastrophic decisions threatening the well-being of human subjects when deploying a trained model in certain hospitals to other hospitals (Schrouff et al., 2022). The current literature for handling distribution shifts with in-processing methods relies on certain assumptions on the type of distribution shift (demographic shift (Fang et al., 2020; Du & Wu, 2021; Maity et al., 2021; Giguere et al., 2021), label shift (Dai & Brown, 2020), and/or covariate shift (Rezaei et al., 2021; Singh et al., 2021)) or explicit access to the causal graph (Mishler & Dalmasso, 2022; Schrouff et al., 2022) of predictors, sensitive attributes, and target variables. As a result, they face practical limitations and cannot cope with most real-world problems involving complex shifts that cannot be categorized in the ones assumed in their works.\nAlternatively, Taskesen et al. (2020) provides convex objective functions for imposing fairness on logistic regression using constraint optimization. Staib & Jegelka (2019) use MMD for defining uncertainty sets around training distribution, whereas Husain (2020) use Integral Probability Measure (IPM) to mitigate the distribution shift. The main limitation of these approaches is their reliance on the convexity of the underlying learning model and lack of scalability due to incompatibility with stochastic optimization algorithms. Wang et al. (2023) uses the Maximum Mean Discrepancy (MMD) distance between the spectral norm of the Hessian matrix at advantaged and disadvantaged data points. However, they do not provide convergence guarantees for their proposed algorithm to any notion of optimality. In addition, the method is not necessarily amenable to stochastic updates. While we naturally define the uncertainty set directly on the joint distribution of sensitive attributes and predictions, they use the curvature of the obtained solution quantified by the norm of the Hessian matrix as a heuristic for promoting the robustness of the fair solution.\nContributions: This paper establishes a scalable (stochastic) fair empirical risk minimization framework through regularization via f -divergences (f -FERM) for both standard and distributed shift settings. f -FERM presents a unified methodology based on the Legendre-Fenchel transformation, enabling us to develop theoretically convergent first-order stochastic algorithms when only small batches of data are available at each iteration. Further, we have presented the first distributionally robust optimization framework under \u2113p norms uncertainty sets covering nonconvex losses such as neural networks. The presented framework for fair inference in the presence of distribution shifts does not rely on the causal graph describing the causal interaction of input features, sensitive attributes, and target variables, which is rarely available in practical problems.\nPaper Organization: We structure our response towards designing scalable, robust, and fair algorithms into two sections. Section 2 motivates the design of unbiased gradient estimators of objectives with information-theoretic f -divergence regularizers. In Section 3, we present our approach for fair inference in the presence of the distribution shift in detail. Our experiments provide an extensive examination of various f -divergences and their suitability as regularizers and also show the consistency of our method across all batch sizes in contrast to existing benchmarks. Similar experiments are carried out for robust training on varying amounts of distributional shifts in data.\n2 FAIR EMPIRICAL RISK MINIMIZATION VIA f -DIVERGENCES A widely studied problem in algorithmic fairness is promoting a notion of group fairness, such as demographic parity, equalized odds, equality of opportunity, or sufficiency through an in-processing method. For these notions, we aim to establish a [conditional] statistical independence between the predictions (e.g., the creditworthiness of the individual) and the sensitive attributes (e.g., gender, race). For simplicity of presentation, we formulate all problems under the demographic parity notion, which requires statistical independence between the prediction and the sensitive attribute. Without loss of generality, all formulations and methods are generalizable to other aforementioned notions of group fairness by considering conditional random variables (see Appendix A). A popular in-processing approach for training fair (classification) models under the demographic parity notion is to regularize the empirical risk minimization:\nmin \u03b8\n1\nn n\u2211 i=1 \u2113(y\u0302\u03b8(xi), yi) + \u03bbD ( P(y\u0302\u03b8(x), s),P(y\u0302\u03b8(x))\u2297 P(s) ) , (1)\nwhere \u03b8 is the learning parameters (e.g., weights of the neural network); xi \u2208 Rd is the i-th input feature vector; yi is the actual label/class for sample i; y\u0302\u03b8(xi) is the prediction of the model for sample i; and \u2113(y\u0302\u03b8(xi), yi) is the loss function measuring the \u201cgoodness-of-fit\" for sample i. Here, D is a divergence between the joint probability distribution of the predictions and sensitive attributes and the Kronecker product of their marginal distributions. Recall that y\u0302\u03b8 and s are statistically independent iff P(y\u0302\u03b8(x), s) follows P(y\u0302\u03b8(x))\u2297 P(s). Thus, the second term in (1) is zero iff y\u0302\u03b8 and s are statistically independent (complete fairness under the demographic parity notion).\nThis section studies the fair empirical risk minimization regularized by a broad class of f -divergence measures. Let P and Q be two discrete probability measures taking values in P = {1, . . . ,m}. The f -divergence between P and Q is defined as (Polyanskiy & Wu, 2022, Def 4.9)(see Appendix B for the general continuous case):\nDf (P,Q) = m\u2211 j=1 Qjf ( Pj Qj ) (2)\nThe above definition, which is also known as f -mutual information (Lu et al., 2023; Csisz\u00e1r, 1967), covers many known divergence measures used for imposing fairness, such as KL-divergence for the choice of f(t) = t log(t) (Shui et al., 2022), or \u03c72 divergence when f(t) = (t \u2212 1)2 (Lowy et al., 2022). As shown in Appendix C, Df in (1) is zero if and only if the probability distribution of s and y\u0302\u03b8 are statistically independent for the choices of f listed in Table 1. In addition, we prove that these f -divergences either cover or provide upper bounds for the popular notions of fairness violations in the literature, such as \u2113p distances, R\u00e9nyi correlation (Baharlouei et al., 2020), and demographic parity (equalized odds) violation. This means that by minimizing these regularizers, we are minimizing an upper bound of (other) popular fairness violation measures, and thus we are controlling them implicitly. Further, unlike R\u00e9nyi correlation (Baharlouei et al., 2020; Grari et al., 2020), we can utilize Legendre-Fenchel duality (and variational representation) to develop (provably) convergent algorithms with stochastic (mini-batch) updates. This formulation and the resulting stochastic optimization algorithm are described in the next subsection.\n2.1 A CONVERGENT STOCHASTIC ALGORITHM FOR FAIR ERM VIA f -DIVERGENCES Let us start by rewriting (1) using f -divergences as the divergence measure:\nmin \u03b8\n1\nn n\u2211 i=1 \u2113(y\u0302\u03b8(xi), yi) + \u03bb \u2211 j\u2208Y, k\u2208S Ps(s = k)Py\u0302\u03b8 (y\u0302\u03b8)f ( Py\u0302\u03b8 ,s(y\u0302\u03b8 = j, s = k) Py\u0302\u03b8 (y\u0302\u03b8 = j)Ps(s = k) ) (f -FERM)\nWhile the non-linearity of f -divergences in (f -FERM) empowers the underlying model to capture more complex dependencies between sensitive attributes and predictions compared to the linear measures (Zafar et al., 2017), the objective function can no longer be represented as a summation of functions over input data points. Consequently, one cannot directly apply the stochastic gradient descent method (or its variations, such as Adam) to the objective function in (f -FERM). In particular, directly evaluating the gradient of the objective function of (f -FERM) on a mini-batch of data leads to a statistically biased estimation of the entire objective\u2019s gradient. Such statistical biases prevent the convergence of algorithms such as SGD (even with a strongly convex minimization landscape) (Ajalloeian & Stich, 2020; Chen & Luss, 2018), let aside the more complex objectives arising in modern-day neural networks.\nTo derive stochastic algorithms, one can use the variational forms of f -divergences to delineate them as a pointwise supremum of affine transformation over probability densities. The most commonly used and well-behaved transform is the Legendre-Fenchel transform (often called the convex conjugates), which linearizes the dependence of the objective function to input data points using a variational reformulation. Particularly, we can rewrite (f -FERM) using the following result:\nProposition 2.1. Let f(\u00b7) be a convex function. Then, (f -FERM) can be reformulated as:\nmin \u03b8 max A n\u2211 i=1 \u2113(y\u0302\u03b8(xi), yi) + \u03bb \u2211 j\u2208Y, k\u2208S [ AjkPy\u0302,s(y\u0302\u03b8 = j, s = k)\u2212 f\u2217(Ajk)Py\u0302(y\u0302\u03b8 = j)Ps(s = k) ] (3) where f\u2217(z) = supw\u2208dom(f) w T z \u2212 f(w) is the Legendre-Fenchel transformation of the function f .\nProof. The proof is standard and appears in Appendix D. In order to solve (3), we will use (stochastic) first-order methods. Notice that Ps(s = k) is constant through the optimization procedure and is computed once by counting the number of data points whose sensitive attribute takes the value of k: \u03c0k := Ps(s = k) = 1n \u2211n i=1 1(si = k). Assume we use the softmax layer to compute the probabilities of different classes in our classification task (as it is standard in logistic regression or using neural networks for classification). Let Fj(xi;\u03b8) be the j-th entry of the softmax layer output for datapoint xi, predicting the probability of class j. Then it is easy to show that we can obtain unbiased estimators of Py\u0302\u03b8 (y\u0302\u03b8 = j) and Py\u0302\u03b8,s(y\u0302\u03b8 = j, s = k) using i.i.d. mini-batch B of data points. More precisely, we have\nPy\u0302\u03b8 (y\u0302\u03b8 = j) = 1\nn n\u2211 i=1 Fj(xi;\u03b8) = E [ 1 |B| |B|\u2211 i=1\nFj(xi;\u03b8)\ufe38 \ufe37\ufe37 \ufe38 P\u0302y\u0302\u03b8 (j; B)\n]\nPy\u0302\u03b8 ,s(y\u0302\u03b8 = j, s = k) = 1\nn n\u2211 i=1 Fj(xi;\u03b8)1(si = k) = E [ 1 |B| |B|\u2211 i=1\nFj(xi;\u03b8)1(si = k)\ufe38 \ufe37\ufe37 \ufe38 P\u0302y\u0302\u03b8 ,s(j,k; B)\n] .\n(4)\nTable 1: Unbiased Estimators for f -divergence Regularizers\nDivergence f(t) The term rjk inside regularizer \u03bb\n\u2211 j,k rjk in (5)\n\u03c72 (t\u2212 1)2 \u03c0k[AjkPy\u0302\u03b8|sk \u2212 (Ajk + A2jk 4 )Py\u0302\u03b8 ] Reverse KL \u2212 ln t \u03c0k[AjkPy\u0302\u03b8|sk + (1 + ln(\u2212Ajk))Py\u0302\u03b8 ] Total Variational 12 |t\u2212 1| \u03c0kAjk[Py\u0302\u03b8|sk \u2212 Py\u0302\u03b8 ]I{|Ajk|<1/2} KL t ln t \u03c0k[AjkPy\u0302\u03b8|sk \u2212 eAjk\u22121Py\u0302\u03b8 ] Jensen-Shannon \u2212(t+ 1) ln( t+12 ) + t ln t \u03c0k[AjkPy\u0302\u03b8|sk + ln(2\u2212 e\nAjk)Py\u0302\u03b8 ] Squared Hellinger ( \u221a t\u2212 1)2 \u03c0k[AjkPy\u0302\u03b8|sk + (A \u22121 jk + 2)Py\u0302\u03b8 ]\nAs a result, Problem (3) can be written as a linearly separable function of input data points (xi\u2019s):\nmin \u03b8 max A\n1\nn n\u2211 i=1 \u2113(y\u0302\u03b8(xi), yi) + \u03bb \u2211 j\u2208Y, k\u2208S [ AjkFj(xi;\u03b8)1(si = k)\u2212 f\u2217(Ajk)\u03c0kFj(xi;\u03b8) ] (5) Thus, evaluating the gradient of the objective function w.r.t. the variables \u03b8 and A over a random batch of data points leads to an unbiased estimator of the gradient of the objective function.\nIn addition to providing an unbiased estimator of gradients, the reformulation (5) has another crucial property: the objective function is concave in A. Therefore, optimization problem (5) falls under the category of nonconvex-concave min-max optimization problems. That is, the objective is (possibly) nonconvex in \u03b8 and is concave in A. Thus, we can borrow tools from the (stochastic) nonconvexconcave min-max optimization literature (Lin et al., 2020; Razaviyayn et al., 2020a; Li et al., 2023) to derive a convergent first-order stochastic algorithm as presented in Algorithm 1. We listed the closed-form of f(\u00b7), f\u2217(\u00b7), for several widely-used f -divergence measures including KL-divergence, Reverse KL-divergence, \u03c72-divergence, Squared Hellinger distance, Jensen-Shannon divergence, and total variation distance in Table 1. For the derivation, see Appendix E.\nAlgorithm 1 Stochastic Gradient Descent-Ascent (SGDA) for f -FERM 1: Input: \u03b80 \u2208 Rd\u03b8 , step-sizes \u03b7\u03b8, \u03b7\u03b1, fairness parameter \u03bb \u2265 0, iteration number T , Batchsize b 2: for t = 1, . . . , T do 3: Sample minibatch of data Bt = {(xt1,yt1), \u00b7 \u00b7 \u00b7 , (xtb,ytb)} 4: \u03b8t = \u03b8t\u22121\u2212 \u03b7\u03b8b \u2211 \u2207\u03b8\u2113(y\u0302\u03b8(x), y)\u2212\u03b7\u03b8\u03bb\u2207\u03b8 ( At\u22121jk P\u0302y\u0302\u03b8,s(j, k; Bt)\u2212\u03c0kf\u2217(A t\u22121 jk )P\u0302y\u0302\u03b8 (j; Bt)\n) 5: Atjk = A t\u22121 jk + \u03b7\u03b1 \u2207A ( At\u22121jk P\u0302y\u0302\u03b8,s(j, k; Bt)\u2212 \u03c0kf\u2217(A t\u22121 jk )P\u0302y\u0302\u03b8 (j; Bt)\n) 6: Return: \u03b8T\nTheorem 2.2. (Informal Statement) Assume that \u2113(\u00b7, \u00b7) and Fj(\u00b7,\u03b8) are Lipschitz continuous for any given j and \u03b8 and their gradients are L-Lipshitz. Further, assume that P(s = k) > 0 for all protected groups and P(y\u0302\u03b8 = j) > 0 at every iteration for all labels j. Then, for any given batch size 1 \u2264 |B| \u2264 n, Algorithm 1 finds an \u03f5-stationary solution of (f -FERM) in O( 1\u03f58 ) for any given \u03f5 > 0.\nProof. The formal statement and proof are relegated to Appendix F.\nTheorem 2.2 applies to all f -divergences listed in Table 1 for all batch-sizes (even as small as the batch size of 1). More sophisticated algorithms can be used to obtain O(\u03f5\u22126) iteration complexity Rafique et al. (1810); Zhang et al. (2022). However, such algorithms use nested loops and require more hyperparameter tunings. We provide an example of such an algorithm in Appendix G. If the fdivergence leads to a strongly concave function in A or satisfies Polyak-\u0141ojasiewicz condition (e.g., for \u03c72 divergence), a faster rate of O(\u03f5\u22125) can be obtained for this algorithm (Appendix F). In addition, if larger batch size of O(\u03f5\u22122) is used, we can further improve this rate to O(\u03f5\u22124) iteration complexity (see Appendix F). Finally, when full batch size is used, then double/triple-loop algorithms can lead to the iteration complexity bounds of O(\u03f5\u22122) in the nonconvex-strongly concave setting and O(\u03f5\u22123) in the general nonconvex-concave setting; see (Kong & Monteiro, 2021; Nouiehed et al., 2019; Ostrovskii et al., 2021b; Thekumparampil et al., 2019).\n3 ROBUST f -FERM IN THE PRESENCE OF DISTRIBUTION SHIFTS In the previous section, we assumed that the training and test domains have the same distribution. However, this assumption is not necessarily valid in certain applications (Fang et al., 2020). In\nparticular, a model that behaves fairly on the training data distribution may have an unfair performance in the test phase. To address this issue, this section develops stochastic algorithms for fair empirical risk minimization via f -divergences in the presence of the distribution shifts.\nAssume that P\u0302s,y(s, y\u0302) is the joint distribution of sensitive attributes and predictions on the training data. The distributionally robust fair empirical risk minimization via f -divergences is formulated as:\nmin \u03b8\n1\nn n\u2211 i=1 \u2113(y\u0302\u03b8(xi), yi) s.t. max P\u2208B Df ( P(y\u0302\u03b8(x), s)|| P(y\u0302\u03b8(x))\u2297 P(s) ) \u2264 \u03ba. (6)\nB = B(P\u0302, \u03b4) is the distributional uncertainty set defined as a certain ball around the training distribution P\u0302 with radius \u03b4. This formulation guarantees that the model fairness is preserved (up to a violence of f-divergence less than \u03ba) even when the test distribution slightly changes. With a slight change of notation, P\u0302 refers to the training distribution, whereas P is the optimization parameter.\nOne can define the uncertainty set through an \u03f5 neighborhood around the joint distribution of the training data characterized by a distance measure such as \u2113p norms, Wasserstein distance, or MMD distance. While these distributionally robust uncertainty sets are thoroughly analyzed for empirical risk minimization (ERM) (Kuhn et al., 2019; Blanchet et al., 2019; Levy et al., 2020), the DRO formulation for ERM is limited to the Wasserstein distance for the fair logistic regression (Taskesen et al., 2020) and MMD distance (Wang et al., 2023) on the distribution curvature as a heuristic for robustness. Unfortunately, none of these approaches offer a convergent algorithm with stochastic updates. Further, some of these approaches are limited to special loss functions and heuristics. On the other hand, we study imposing the distributionally robust fairness via f -divergences for a general loss function where the uncertainty set is characterized by \u2113p norms (Section 3.1) or f -divergences (Section 3.2). Our results show that the former approach is more suitable when lower levels of robustness for fairness are required, and the latter works better for handling larger distribution shifts.\n3.1 ROBUST f -FERM UNDER \u2113p NORMS AND SMALL DISTRIBUTION SHIFTS This section focuses on the widely studied \u2113p norms as the uncertainty set for the distributional distance between the training and test domains. In this case, Problem (6) can be written as:\nmin \u03b8\n1\nn n\u2211 i=1\n\u2113(y\u0302\u03b8(xi), yi) s.t. max ||P\u2212P\u0302||p\u2264\u03b4 ||Q\u2212Q\u0302||p\u2264\u03b4\nDf (P||Q) \u2264 \u03ba, (7)\nwhere P\u0302 represents the joint distribution of the sensitive attributes and predictions and Q\u0302 denotes the Kronecker product of the marginal distributions between sensitive attributes and predictions.\nSince handling non-convex constraints is challenging, as it is standard in training machine learning models, we consider the Lagrangian relaxation of Problem (7) as follows:\nmin \u03b8\n1\nn n\u2211 i=1\n\u2113(y\u0302\u03b8(xi), yi) + \u03bb max \u2225P\u2212P\u0302\u2225p\u2264\u03b4 \u2225Q\u2212Q\u0302\u2225p\u2264\u03b4\nDf (P||Q) (8)\nThis problem falls under the nonconvex-nonconcave, min-max optimization category and is most likely to be computationally hard for general uncertainty sets (Daskalakis et al., 2021). However, such a min-max optimization problem can be solved to stationarity when the diameter of set B is small (i.e., under small domain shift), see (Ostrovskii et al., 2021a). The core idea is to approximate the inner maximization problem with the Taylor approximation, leading to a nonconvex-concave min-max optimization, which is easier to solve (Daskalakis et al., 2021; Razaviyayn et al., 2020b). This idea has been used and been successful in machine learning (see Foret et al. (2020) for its use in Sharpness-aware minimization). Utilizing this idea, Problem (8) can be approximated as:\nmin \u03b8 max \u2225U\u2225p\u2264\u03b4 \u2225V\u2225p\u2264\u03b4\n( h(\u03b8,U,V) := 1\nn n\u2211 i=1 \u2113(y\u0302\u03b8(xi), yi) + \u03bb\u27e8U,\u2207PDf (P\u0302||Q\u0302)\u27e9+ \u03bb\u27e8V,\u2207QDf (P\u0302||Q\u0302)\u27e9\n) , (9)\nwhere we used the change of variables U := P\u2212 P\u0302 and V := Q\u2212 Q\u0302. Equivalently,\nmin \u03b8\n1\nn n\u2211 i=1 \u2113(y\u0302\u03b8(xi), yi) + \u03bb\u03b4\u2225\u2207PDf (P\u0302||Q\u0302)\u2225q + \u03bb\u03b4\u2225\u2207QDf (P\u0302||Q\u0302)\u2225q, (10)\nwhere \u2225 \u00b7 \u2225q is the dual of the \u2113p norm with 1p + 1 q = 1.\nProposition 3.1. Assume that the gradient of the loss function is L-Lipshitz, and the second-order derivative of the loss exists. Then, a given \u03f5\u2212approximate stationary solution of Problem (10) is an O(\u03f5)\u2212approximate stationary solution of Problem (8) whenever L\u03b4 \u2272 \u03f5.\nThis proposition, which is an immediate application of Ostrovskii et al. (2021a, Theorem 3.1), states that if the desired training accuracy \u03f5 is comparable with the distribution shift amount \u03b4 (i.e. small distribution shift regime), then one can solve problem (10) instead of (8). Thus, in this regime, we need to solve (10) or equivalently (9). To this end, we need to obtain the (sub)-gradients of the objective function in (9) w.r.t the \u03b8, U, and V variables. First, notice that\n\u2207Uh(\u03b8,U,V) = \u2207PDf (P\u0302||Q\u0302) = \u03b1\u2217(P\u0302, Q\u0302) and \u2207Vh(\u03b8,U,V) = \u2207QDf (P\u0302||Q\u0302) = f\u2217(\u03b1\u2217(P\u0302, Q\u0302)),\nwhere \u03b1\u2217(P\u0302, Q\u0302) \u2208 argmax\u03b1 \u2211\nj \u03b1j p\u0302j(\u03b8)\u2212 q\u0302j(\u03b8)f\u2217(\u03b1j). Here we invoked Danskin\u2019s theorem on the variational form of Df ; p\u0302j(\u03b8) and q\u0302j(\u03b8) is the j-th element of P\u0302 and Q\u0302, respectively. Next, we need to compute\u2207\u03b8h(\u03b8,U,V). Notice that the derivative of the first term in h(\u00b7) w.r.t. \u03b8 is easy to compute. We next calculate the derivative of the second term of h(\u03b8,U,V) w.r.t. \u03b8. As the derivative of the third term can be computed similarly, we omit its derivation here.\n\u2207\u03b8\u27e8U,\u2207PDf (P\u0302||Q\u0302)\u27e9 = \u2207\u03b8\u27e8U,\u03b1\u2217(P\u0302, Q\u0302)\u27e9 = \u2211 j uj q\u0302j(\u03b8)\u2207\u03b8p\u0302j(\u03b8)\u2212 p\u0302j(\u03b8)\u2207\u03b8 q\u0302j(\u03b8) q\u03022j (\u03b8)\u00d7 (f\u2217)\u2032\u2032(\u03b1)|\u03b1=\u03b1\u2217j (P\u0302,Q\u0302)\n(11)\nwhere in the last equation, we used the implicit function theorem to compute the derivative of \u03b1\u2217 w.r.t. \u03b8. Notice that an implicit assumption here is that f is differentiable (which holds for KL-divergence, \u03c72 divergence, reverse KL, Jensen-Shannon, and Squared Hellinger distance). Having access to the gradients, we can apply the standard [sub-]gradient descent-ascent algorithm to obtain a solution to Problem (10) (see Appendix H for the details).\nA semi-stochastic memory-efficient first-order training algorithm. To apply (stochastic) gradient descent-ascent algorithm (Lin et al., 2020) to problem (9), we need to have unbiased estimator of the function h(\u03b8,U,V) w.r.t. \u03b8, U, and V variables. While it seems challenging to obtain unbiased estimator w.r.t. all variables, one can notice that if p\u0302j(\u03b8) and q\u0302j(\u03b8) can be computed easily with one forward pass over all data points (i.e., in O(m\u00d7n) memory requirement). Consequently, the gradient of h(\u03b8,U,V) w.r.t. U and V can be computed with one forward pass over all data points (without the need for doing backpropagation). On the other hand, one can easily obtain unbiased estimator of \u2207\u03b8p\u0302j(\u03b8) and \u2207\u03b8 q\u0302j(\u03b8) in (11) using a small mini-batch of data. Such a task requires O(b \u00d7 d) memory with d being the number of parameters (i.e., \u03b8 \u2208 Rd) and b being the batch size. Combining this unbiased estimation with the computed values of p\u0302j(\u03b8) and q\u0302j(\u03b8) leads to an unbiased estimator of the objective of (9) w.r.t. \u03b8 variable. To summarize, we need to do one forward propagation to obtain gradients w.r.t. U and V, and we only do backpropagation for computing gradients w.r.t. \u03b8 over the mini-batch of data. Such an algorithm requires O(mn+ bd) memory requirement and thus can be used for training large models (with d, n\u226b b,m). It is known that memory requirements are the major limiting factors in training large models such as LLMs (Malladi et al., 2023).\n3.2 ROBUST f -FERM UNDER \u2113\u221e NORMS AND POTENTIALLY LARGE DISTRIBUTION SHIFTS The developed framework in the previous section assumes the distribution shift is small (the uncertainty set diameter is smaller than a certain threshold). When preserving fairness in the presence of large distribution shifts is a priority, our previous methodology might not work well. As discussed before, the formulation (8) leads to a nonconvex-nonconcave min-max optimization problem and this class of problems is hard to solve computationally in general (even to stationarity notions). Thus, we need to exploit the structure of the problem. In this section, we show that we can exploit the structure to develop a first-order algorithm under large distribution shifts. Particularly, we focus on the case where the uncertainty set is \u2113\u221e ball and the divergence satisfies certain assumptions (i.e., f\u2217(\u03b1\u2217) > 0 and \u03b1\u2217 > 0, which is satisfied for KL divergence).\nSince the functionDf is convex in P and Q, under \u2113\u221e uncertainty set on P and Q, the optimal solution of the maximization problem in (8) will be at an extreme point. Moreover, under the assumption that f\u2217(\u03b1\u2217) > 0 and \u03b1\u2217 > 0 (which is satisfied for KL divergence), one can easily see that the optimal pj = min{p\u0302j + \u03b4, 1} and qj = max{q\u0302j \u2212 \u03b4, 0} (see Appendix I for the exact proof). Notice that we need to relax the probability simplex constraint to obtain this efficient, optimal closed-form solution. Thus under this assumption, problem (8) can be reformulated as\nmin \u03b8\n1\nn n\u2211 i=1 \u2113(y\u0302\u03b8(xi), yi) + \u03bbDf (min{P+ \u03b4, 1}||max{Q\u2212 \u03b4, 0}), (12)\nwhich is a regular minimization problem and (sub)gradient descent can be utilized to solve it."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": "We use three popular notions of group fairness: demographic parity, equalized odds, and equality of opportunity violations (see Appendix A for definitions) to measure the fairness of trained models. To run Algorithm 1, we set \u03b7\u03b8 and \u03b7\u03b1 to 10\u22125 and 10\u22126 respectively in all experiments. Further, by changing \u03bb, we get different points in the trade-off curve between accuracy and fairness. The range of \u03bb depends on the f -divergence (see Appendix J for more information on tuning hyper-parameters). In the inference phase of our experiments, we use the standard maximum likelihood decoding based on the output of the softmax layer, i.e., the predicted label is the label with the highest logit value.\nAs we will see in this section, several f -divergence measures lead to reasonable fairness/accuracy tradeoffs and can outperform existing benchmarks. However, no single f -divergence measure uniformly outperforms other measures in all the experiments. Thus, we believe in applications, the choice of the f -divergence can be viewed as a hyperparameter that can be tuned by cross-validation."
        },
        {
            "heading": "4.1 FAIRNESS-ACCURACY TRADEOFFS ON BENCHMARK DATASETS",
            "text": "In the first set of experiments, we compare different f - divergence formulations for (f -FERM) to each other and several state-of-the-art approaches supporting multiple sensitive attributes. Figure 1 demonstrates the given tradeoff on the adult dataset (Becker & Kohavi, 1996) with gender and race as the sensitive attributes (black-female, black-male, white-female, white-male). To measure fairness, we use the demographic parity violation defined as:\nDPV = max i,j\u2208S |P(y\u0302 = 1|s = i)\u2212 P(y\u0302 = 1|s = j)|\nIn the case of binary sensitive attributes (e.g., gender), there is no significant variation between different f -divergences. However, when we have 2 sensitive attributes and the batch size is small (8 in Figure 1), the results significantly differ for various f - divergences. Interestingly, KL-divergence for smaller \u03bb values shows improvement in fairness violation and accuracy simultaneously. We do not observe such a phenomenon for other f -divergences and state-of-theart approaches in the literature. Further, in Figure 2, we compare one of the f -divergences (reverse KL)\nto several SOTA methods including Mary et al. (2019); Baharlouei et al. (2020); Cho et al. (2020). Other approaches such as the pre-processing method of Zemel et al. (2013), post-processing approach of Hardt et al. (2016), and several in-processing methods including Zafar et al. (2017); Donini et al. (2018); Jiang et al. (2020) demonstrate lower performance compared to the ones depicted in Figure 2 and are removed from the figure. While our approach demonstrates consistently good performance across different batch sizes (full-batch, 64, 8, 2), the performances of other methods drop significantly for smaller ones. For further experiments on other datasets (German and COMPAS) and other fairness measures (equality of opportunity and equalized odds violations), see Appendix K."
        },
        {
            "heading": "4.2 FAIRNESS-ACCURACY TRADEOFFS IN THE PRESENCE OF THE DISTRIBUTION SHIFT",
            "text": "We perform two experiments to evaluate the Algorithms developed in Section 3. In the first experiment, we randomly switch the label of genders for n% of the data points (n ranges from 1 to 20) in the\nAdult dataset. Then, we train models on the new datasets with a proportion of corrupted sensitive attributes and evaluate the performance on the test data. Figure 3 is obtained by training different models to achieve 80% accuracy on the test data and comparing their demographic parity violation. By increasing the percentage of corrupted sensitive attributes, we see that both f -DRO and f -infinity achieve less DP violation than SOTA approaches in the literature. In this specific experiment, f -DRO works better than f -infinity, and there is no significant difference between choosing KLdivergence or \u03c72 as the function f . Among the papers designed for handling distribution shifts, Rezaei et al. (2021) and Wang et al. (2020) were the only options with the available implementation.\nIn a more recently collected dataset (new adult) (Ding et al., 2021), the users are separated based on their living state. We train different fair models in a single state and evaluate the fairness-accuracy tradeoff in other states. Figure 4 depicts the performance of different methods. For each method, the center point is the average of accuracy and fairness among 50 states. The horizontal and vertical lines show the 25- percentile to 75-percentile range of performance among the states. The training fairness violation is set to 0.02 for all methods. We observe that f -infinity preserves the fairness level better than other approaches. In comparison, f -DRO has a better accuracy. Depending on the application, we suggest using f -infinity if preserving a high level of fairness is a priority and f -DRO for the cases when a better tradeoff between fairness and accuracy is expected. Note that both approaches offer better fairness-accuracy tradeoffs compared to the SOTA approaches in the literature."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "This paper presented a unified stochastic framework for fair empirical risk minimization via f - divergences (f -FERM). The key idea is to reformulate the objective function as a min-max optimization problem using Legendre-Fenchel duality of f -divergence. This enables us to develop an unbiased gradient estimator and a convergent stochastic first-order algorithm. Furthermore, we robustified f -FERM using \u2113p norm balls as the uncertainty set against distributional changes. While our empirical investigation delves into the performance and fairness distinctions among various f -divergences, a more comprehensive analysis is warranted to determine the optimal f -divergence concerning the tradeoff between performance and fairness, faster convergence, and asymptotic behaviors. Furthermore, the distributionally robust formulation of fair empirical risk minimization and the advantages of each formulation can be explored beyond f -divergences as the measure of fairness violation and \u2113p norm balls as uncertainty sets."
        },
        {
            "heading": "ACKNOWLEDGEMENTS",
            "text": "This work was supported by the NSF CAREER Award CCF2144985, the AFOSR Young Investigator Program Award FA9550-22-1-0192, a gift from the USC-Meta Center for Research and Education in AI, and a gift from Google."
        }
    ],
    "title": "FAIR EMPIRICAL RISK MINIMIZATION"
}