{
    "abstractText": "Existing multi-label classification methods have long suffered from label heterogeneity, where learning a label obscures another. By modeling multi-label classification as a multi-task problem, this issue can be regarded as a negative transfer, which indicates challenges to achieve simultaneously satisfied performance across multiple tasks. In this work, we propose the Hybrid Sharing Query (HSQ), a transformer-based model that introduces the mixture-of-experts architecture to image multi-label classification. HSQ is designed to leverage label correlations while mitigating heterogeneity effectively. To this end, HSQ is incorporated with a fusion expert framework that enables it to optimally combine the strengths of task-specialized experts with shared experts, ultimately enhancing multi-label classification performance across most labels. Extensive experiments are conducted on two benchmark datasets, with the results demonstrating that the proposed method achieves state-of-the-art performance and yields simultaneous improvements across most labels. The code is available at this URL.",
    "authors": [
        {
            "affiliations": [],
            "name": "Zihao Yin"
        },
        {
            "affiliations": [],
            "name": "Chen Gan"
        },
        {
            "affiliations": [],
            "name": "Kelei He"
        },
        {
            "affiliations": [],
            "name": "Yang Gao"
        },
        {
            "affiliations": [],
            "name": "Junfeng Zhang"
        }
    ],
    "id": "SP:fc48322da0e802a3fc80504a6d92e08a34cd1b12",
    "references": [
        {
            "authors": [
                "Shilong Liu",
                "Lei Zhang",
                "Xiao Yang",
                "Hang Su",
                "Jun Zhu"
            ],
            "title": "Query2label: A simple transformer way to multi-label classification, 2021a",
            "year": 2021
        },
        {
            "authors": [
                "Tal Ridnik",
                "Gilad Sharir",
                "Avi Ben-Cohen",
                "Emanuel Ben-Baruch",
                "Asaf Noy"
            ],
            "title": "Ml-decoder: Scalable and versatile classification head",
            "venue": "In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision,",
            "year": 2023
        },
        {
            "authors": [
                "Jin Ye",
                "Junjun He",
                "Xiaojiang Peng",
                "Wenhao Wu",
                "Yu Qiao"
            ],
            "title": "Attention-driven dynamic graph convolutional network for multi-label image recognition",
            "venue": "In Computer Vision\u2013ECCV 2020: 16th European Conference,",
            "year": 2020
        },
        {
            "authors": [
                "Jiaqi Ma",
                "Zhe Zhao",
                "Xinyang Yi",
                "Jilin Chen",
                "Lichan Hong",
                "Ed H. Chi"
            ],
            "title": "Modeling Task Relationships in Multi-task Learning with Multi-gate Mixture-of-Experts",
            "venue": "In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, KDD",
            "year": 2018
        },
        {
            "authors": [
                "Robert A Jacobs",
                "Michael I Jordan",
                "Steven J Nowlan",
                "Geoffrey E Hinton"
            ],
            "title": "Adaptive mixtures of local experts",
            "venue": "Neural Computation,",
            "year": 1991
        },
        {
            "authors": [
                "Feng Zhu",
                "Hongsheng Li",
                "Wanli Ouyang",
                "Nenghai Yu",
                "Xiaogang Wang"
            ],
            "title": "Learning spatial regularization with image-level supervisions for multi-label image classification",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Pranav Rajpurkar",
                "Jeremy Irvin",
                "Kaylie Zhu",
                "Brandon Yang",
                "Hershel Mehta",
                "Tony Duan",
                "Daisy Ding",
                "Aarti Bagul",
                "Curtis Langlotz",
                "Katie Shpanskaya"
            ],
            "title": "Chexnet: Radiologist-level pneumonia detection on chest x-rays with deep learning",
            "venue": "arXiv preprint arXiv:1711.05225,",
            "year": 2017
        },
        {
            "authors": [
                "Gao Huang",
                "Zhuang Liu",
                "Laurens Van Der Maaten",
                "Kilian Q Weinberger"
            ],
            "title": "Densely connected convolutional networks",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Jiang Wang",
                "Yi Yang",
                "Junhua Mao",
                "Zhiheng Huang",
                "Chang Huang",
                "Wei Xu"
            ],
            "title": "Cnn-rnn: A unified framework for multi-label image classification",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Zhao-Min Chen",
                "Xiu-Shen Wei",
                "Peng Wang",
                "Yanwen Guo"
            ],
            "title": "Multi-label image recognition with graph convolutional networks",
            "venue": "In Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Jack Lanchantin",
                "Tianlu Wang",
                "Vicente Ordonez",
                "Yanjun Qi"
            ],
            "title": "General multi-label image classification with transformers",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Sen Wu",
                "Hongyang R Zhang",
                "Christopher R\u00e9"
            ],
            "title": "Understanding and improving information transfer in multi-task learning",
            "venue": "In International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "Rich Caruana"
            ],
            "title": "Multitask Learning",
            "venue": "Machine Learning,",
            "year": 1997
        },
        {
            "authors": [
                "Long Duong",
                "Trevor Cohn",
                "Steven Bird",
                "Paul Cook"
            ],
            "title": "Low resource dependency parsing: Crosslingual parameter sharing in a neural network parser",
            "venue": "In Proceedings of the 53rd annual meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (volume 2: short papers),",
            "year": 2015
        },
        {
            "authors": [
                "Yongxin Yang",
                "Timothy Hospedales"
            ],
            "title": "Trace norm regularised deep multi-task learning",
            "venue": "In 5th International Conference on Learning Representations,",
            "year": 2017
        },
        {
            "authors": [
                "Ishan Misra",
                "Abhinav Shrivastava",
                "Abhinav Gupta",
                "Martial Hebert"
            ],
            "title": "Cross-stitch networks for multi-task learning",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Yuan Xie",
                "Shaohan Huang",
                "Tianyu Chen",
                "Furu Wei"
            ],
            "title": "Moec: Mixture of expert clusters",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2023
        },
        {
            "authors": [
                "Hongyan Tang",
                "Junning Liu",
                "Ming Zhao",
                "Xudong Gong"
            ],
            "title": "Progressive layered extraction (ple): A novel multi-task learning (mtl) model for personalized recommendations",
            "venue": "In Proceedings of the 14th ACM Conference on Recommender Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Noam Shazeer",
                "Azalia Mirhoseini",
                "Krzysztof Maziarz",
                "Andy Davis",
                "Quoc Le",
                "Geoffrey Hinton",
                "Jeff Dean"
            ],
            "title": "Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer",
            "venue": "In International Conference on Learning Representations,",
            "year": 2017
        },
        {
            "authors": [
                "Yanqi Zhou",
                "Tao Lei",
                "Hanxiao Liu",
                "Nan Du",
                "Yanping Huang",
                "Vincent Zhao",
                "Andrew M Dai",
                "Quoc V Le",
                "James Laudon"
            ],
            "title": "Mixture-of-experts with expert choice routing",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Clemens Rosenbaum",
                "Tim Klinger",
                "Matthew Riemer"
            ],
            "title": "Routing networks: Adaptive selection of non-linear functions for multi-task learning",
            "venue": "In International Conference on Learning Representations,",
            "year": 2018
        },
        {
            "authors": [
                "Xiaonan Nie",
                "Xupeng Miao",
                "Shijie Cao",
                "Lingxiao Ma",
                "Qibin Liu",
                "Jilong Xue",
                "Youshan Miao",
                "Yi Liu",
                "Zhi Yang",
                "Bin Cui"
            ],
            "title": "Evomoe: An evolutional mixture-of-experts training framework via dense-to-sparse gate",
            "venue": "arXiv preprint arXiv:2112.14397,",
            "year": 2021
        },
        {
            "authors": [
                "Simiao Zuo",
                "Xiaodong Liu",
                "Jian Jiao",
                "Young Jin Kim",
                "Hany Hassan",
                "Ruofei Zhang",
                "Tuo Zhao",
                "Jianfeng Gao"
            ],
            "title": "Taming sparsely activated transformer with stochastic experts",
            "venue": "International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Stephen Roller",
                "Sainbayar Sukhbaatar",
                "Jason Weston"
            ],
            "title": "Hash layers for large sparse models",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Damai Dai",
                "Li Dong",
                "Shuming Ma",
                "Bo Zheng",
                "Zhifang Sui",
                "Baobao Chang",
                "Furu Wei"
            ],
            "title": "Stablemoe: Stable routing strategy for mixture of experts",
            "venue": "In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
            "year": 2022
        },
        {
            "authors": [
                "William Fedus",
                "Barret Zoph",
                "Noam Shazeer"
            ],
            "title": "Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2022
        },
        {
            "authors": [
                "Jack Lanchantin",
                "Tianlu Wang",
                "Vicente Ordonez",
                "Yanjun Qi"
            ],
            "title": "General multi-label image classification with transformers",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Tsung-Yi Lin",
                "Michael Maire",
                "Serge Belongie",
                "James Hays",
                "Pietro Perona",
                "Deva Ramanan",
                "Piotr Doll\u00e1r",
                "C Lawrence Zitnick"
            ],
            "title": "Microsoft coco: Common objects in context",
            "venue": "In Computer Vision\u2013ECCV",
            "year": 2014
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Zhuang Liu",
                "Hanzi Mao",
                "Chao-Yuan Wu",
                "Christoph Feichtenhofer",
                "Trevor Darrell",
                "Saining Xie"
            ],
            "title": "A convnet for the 2020s",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Zhao-Min Chen",
                "Xiu-Shen Wei",
                "Xin Jin",
                "Yanwen Guo"
            ],
            "title": "Multi-label image recognition with joint class-aware map disentangling and label correlation embedding",
            "venue": "IEEE International Conference on Multimedia and Expo (ICME),",
            "year": 2019
        },
        {
            "authors": [
                "Yongcheng Liu",
                "Lu Sheng",
                "Jing Shao",
                "Junjie Yan",
                "Shiming Xiang",
                "Chunhong Pan"
            ],
            "title": "Multi-label image classification via knowledge distillation from weakly-supervised detection",
            "venue": "In Proceedings of the 26th ACM International Conference on Multimedia,",
            "year": 2018
        },
        {
            "authors": [
                "Renchun You",
                "Zhiyao Guo",
                "Lei Cui",
                "Xiang Long",
                "Yingze Bao",
                "Shilei Wen"
            ],
            "title": "Cross-modality attention with semantic graph embedding for multi-label classification",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Bin-Bin Gao",
                "Hong-Yu Zhou"
            ],
            "title": "Learning to discover multi-class attentional regions for multi-label image recognition",
            "venue": "IEEE Transactions on Image Processing,",
            "year": 2021
        },
        {
            "authors": [
                "Tianshui Chen",
                "Muxin Xu",
                "Xiaolu Hui",
                "Hefeng Wu",
                "Liang Lin"
            ],
            "title": "Learning semantic-specific graph representation for multi-label image recognition",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2019
        },
        {
            "authors": [
                "Zhao-Min Chen",
                "Quan Cui",
                "Borui Zhao",
                "Renjie Song",
                "Xiaoqin Zhang",
                "Osamu Yoshie"
            ],
            "title": "Sst: Spatial and semantic transformers for multi-label image recognition",
            "venue": "IEEE Transactions on Image Processing,",
            "year": 2022
        },
        {
            "authors": [
                "Jiazhi Xu",
                "Sheng Huang",
                "Fengtao Zhou",
                "Luwen Huangfu",
                "Daniel Zeng",
                "Bo Liu"
            ],
            "title": "Boosting multilabel image classification with complementary parallel self-distillation",
            "venue": "Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "Jialu Zhang",
                "Jianfeng Ren",
                "Qian Zhang",
                "Jiang Liu",
                "Xudong Jiang"
            ],
            "title": "Spatial context-aware objectattentional network for multi-label image classification",
            "venue": "IEEE Transactions on Image Processing,",
            "year": 2023
        },
        {
            "authors": [
                "Tal Ridnik",
                "Emanuel Ben-Baruch",
                "Nadav Zamir",
                "Asaf Noy",
                "Itamar Friedman",
                "Matan Protter",
                "Lihi Zelnik-Manor"
            ],
            "title": "Asymmetric loss for multi-label classification",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Xing Cheng",
                "Hezheng Lin",
                "Xiangyu Wu",
                "Dong Shen",
                "Fan Yang",
                "Honglin Liu",
                "Nian Shi"
            ],
            "title": "Mltr: Multi-label classification with transformer",
            "venue": "IEEE International Conference on Multimedia and Expo (ICME),",
            "year": 2022
        },
        {
            "authors": [
                "Ze Liu",
                "Yutong Lin",
                "Yue Cao",
                "Han Hu",
                "Yixuan Wei",
                "Zheng Zhang",
                "Stephen Lin",
                "Baining Guo"
            ],
            "title": "Swin transformer: Hierarchical vision transformer using shifted windows",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Haiping Wu",
                "Bin Xiao",
                "Noel Codella",
                "Mengchen Liu",
                "Xiyang Dai",
                "Lu Yuan",
                "Lei Zhang"
            ],
            "title": "Cvt: Introducing convolutions to vision transformers",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Mark Everingham",
                "SM Ali Eslami",
                "Luc Van Gool",
                "Christopher KI Williams",
                "John Winn",
                "Andrew Zisserman"
            ],
            "title": "The pascal visual object classes challenge: A retrospective",
            "venue": "International Journal of Computer Vision,",
            "year": 2015
        },
        {
            "authors": [
                "Karen Simonyan",
                "Andrew Zisserman"
            ],
            "title": "Very deep convolutional networks for large-scale image recognition",
            "venue": "In International Conference on Learning Representations. Computational and Biological Learning Society,",
            "year": 2015
        },
        {
            "authors": [
                "Hao Yang",
                "Joey Tianyi Zhou",
                "Yu Zhang",
                "Bin-Bin Gao",
                "Jianxin Wu",
                "Jianfei Cai"
            ],
            "title": "Exploit bounding box annotations for multi-label object recognition",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Yunchao Wei",
                "Wei Xia",
                "Min Lin",
                "Junshi Huang",
                "Bingbing Ni",
                "Jian Dong",
                "Yao Zhao",
                "Shuicheng Yan"
            ],
            "title": "Hcp: A flexible cnn framework for multi-label image classification",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 1901
        },
        {
            "authors": [
                "Zhouxia Wang",
                "Tianshui Chen",
                "Guanbin Li",
                "Ruijia Xu",
                "Liang Lin"
            ],
            "title": "Multi-label image recognition by recurrently discovering attentional regions",
            "venue": "In Proceedings of the IEEE International Conference on Computer Vision,",
            "year": 2017
        },
        {
            "authors": [
                "Tianshui Chen",
                "Zhouxia Wang",
                "Guanbin Li",
                "Liang Lin"
            ],
            "title": "Recurrent attentional reinforcement learning for multi-label image recognition",
            "venue": "In Proceedings of the AAAI conference on Artificial Intelligence,",
            "year": 2018
        },
        {
            "authors": [
                "Tal Ridnik",
                "Emanuel Ben-Baruch",
                "Asaf Noy",
                "Lihi Zelnik-Manor"
            ],
            "title": "Imagenet-21k pretraining for the masses",
            "venue": "arXiv preprint arXiv:2104.10972,",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "In computer vision, multi-label classification (MLC) attempts to predict multiple labels that may simultaneously appear in a single sample. It is more realistic and intuitive as a sample typically has multiple attributes in real scenarios. However, the semantic correlation and heterogeneity among different labels pose a significant challenge to MLC, resulting in the labels either complement or conflict with each other. Previous works (Liu et al., 2021a; Ridnik et al., 2023; Ye et al., 2020) achieved impressive performance via transformers or graph neural networks, trying to explore the correlation among labels with shared backbone across labels. These approaches neglected the heterogeneity among labels, which becomes the key obstacle to simultaneous improvement across labels.\nIn contrast to traditional multi-label classification approaches, MLC can be formulated as a multitask learning (MTL) problem by modeling the prediction of each label as an individual task. The correlation and heterogeneity of the labels in MLC thus correspond to the task transfer problem of MTL, where learning a new task may perfect (positive transfer) or deteriorate (negative transfer) another. Under this context, the power of MTL in mitigating negative transfer may help improve the performance of MLC.\nPrecedent works like (Ma et al., 2018) in MTL include a mixture of experts (MoE, (Jacobs et al., 1991)), which utilizes a group of learned experts to handle different tasks separately. MOE has been widely adopted in natural language processing, where experts are expected to process words of various lexical categories. We advocate employing MoE in MLC image classification, which shares commonality with lexical category handling. Furthermore, we notice that the conventional MoE approach has primarily emphasized the utilization of expert groups within a specific task, with limited attention to the exchange of expertise group knowledge across different tasks. This approach may not align seamlessly with the MLC requirements, which will be scrutinized in our work.\n\u2217: Corresponding author\nIn this work, we introduce Hybrid Sharing Query (HSQ), a MoE-based MLC method with a novel proposed fusion strategy to better exploit semantic correlation and heterogeneity among labels and generate better underlying shared representation and task-specific representation. Additionally, we prioritize the adaptive fusion of label-specific and shared features in the classification task of each label, suppressing negative transfer and enhancing performance on the majority of labels. Specifically, we employ a group of shared experts to mine correlation among labels to generate multiple distinct shared features while assigning a group of task-specialized experts to each task to extract a series of label-specific features. This design can balance label-specific and shared features across labels while also emphasizing unique label-specific features for each individual label. Moreover, we employ gate networks to adaptively re-weight and harmonize features from task-specialized experts and shared experts, enhancing positive correlations and suppressing negatives among tasks.\nExperiments show that the proposed method outperforms all tested baselines across multiple datasets on the majority of labels. The proposed method is also compatible with transformer-based MLC methods, indicating potential improvement to existing works.\nOur contribution is three-fold:\n\u2022We present MoE to the MLC task, with gated task-specialized and shared experts to capture correlation and heterogeneity adaptively by formulating the MLC as an MTL problem.\n\u2022We empirically demonstrate that the fused experts help to extract correlations between tasks, encourage positive correlation sharing and suppress negative transfer, which benefits the overall and per-label performance and mitigates cross-label performance gap.\n\u2022We verify the superiority of our proposed model on two benchmark datasets with state-of-the-art performance overall and per-label."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Multi-label classification in computer vision. Models via various approaches have been proposed to address MLC. Zhu et al. (2017) use convolutional networks on an attention map to optimize ResNet prediction. Rajpurkar et al. (2017) solve the medical multi-label problem by using DenseNet (Huang et al., 2017). Wang et al. (2016) attempt to extract features from the image and generate the label as a sequence through a learned joint embedding space. Chen et al. (2019a) introduce graph convolutional network into this task, mapping label word embedding to inter-dependent object classifiers. Lanchantin et al. (2021a); Liu et al. (2021a) introduce transformer into MLC. These methods fail to see the negative transfer and positive correlation among labels. Some works also notice a similar problem in MLC from the MTL aspect. Wu et al. (2019) try to mitigate such a problem via a different architecture. Our study aims to improve overall performance in MLC while attempting to simultaneously enhance performance on as many labels as possible.\nMulti-task learning. MLC can be recognized as a special case of MTL, treating each label as a separate classification task (Wu et al., 2019). Previous works on this topic include hard and soft parameter sharing, etc. Hard parameter sharing (Caruana, 1997) comes with a shared feature extraction backbone as a bottom and task-specialized towers as a top. Soft parameter sharing does not explicitly share network components across tasks but jointly learns other information through gradient sharing or other techniques. Duong et al. (2015); Yang and Hospedales (2017) encourage knowledge sharing across experts via different constraints like L2 norm. Cross-Stitch (Misra et al., 2016) trains two networks for two tasks and shares gradients between some layers controlled by gates. However, these architectures require more attention to the correlations among tasks, and the naive knowledge-sharing strategy may hamper the performance of models. In this work, we propose HSQ to reveal these correlations in the hope of generating a better representation for each task.\nMixture of experts in deep learning. Efforts have been made to improve models\u2019 performance by scaling up the model size with MoE (Jacobs et al., 1991), which first attempts to combine the outputs of several experts with a gate network. MMOE (Ma et al., 2018) with similar settings further decouples the seesaw phenomenon between several tasks by assigning exclusive gate and tower networks to each task. MOEC (Xie et al., 2023) adopts a clustering loss to impose variancebased constraints on the routing stage, obtaining clusters of experts with more diverse knowledge.\nPLE (Tang et al., 2020) adds shared and task-specific experts to MMOE to allow better information sharing between tasks. Traditional MoE imposes a substantial computational burden since all experts activate, even when only some tasks are required. To mitigate such a cost, the sparse MoE (Shazeer et al., 2017) strategy emerges in contrast to the regular dense one. The routing strategy determines which experts contribute to the task output. Zhou et al. (2022); Rosenbaum et al. (2018); Nie et al. (2021); Zuo et al. (2022); Roller et al. (2021); Dai et al. (2022) and others explore various routing strategies, including randomizing, hashing, expert-choosing, etc. Switch Transformer (Fedus et al., 2022) introduces a sparse MoE to the transformer layer to replace the feed-forward neural network. Our method introduces the MoE into the multi-label classification field by virtue of task-specialized and shared experts exploiting correlations among tasks. Moreover, we utilize a gate network to enhance positive correlation and suppress negative correlation in pursuit of better fusion."
        },
        {
            "heading": "3 METHOD",
            "text": "The MLC task for images is to find all possible correct labels in a pre-defined label set for a provided image. Thus, our model takes the provided image I and gives probability scoring L\u0302 \u2208 RL on all labels L. The proposed HSQ model comprises three main parts, namely, 1) feature extraction backbone, 2) query transformer, and 3) mixture-of-expert hybrid sharing head. The backbone extracts image representation with a robust replaceable network, followed by a transformer-based query model to explore underlying information between such extracted representation and each given label. The Hybrid Sharing Layers are applied to better exploit the correlations between every possible task and suppress potential negative transfer problems."
        },
        {
            "heading": "3.1 FEATURE EXTRACTION BACKBONE",
            "text": "Features in any given image will be extracted through a feature extraction backbone. Multiple preceding works have contributed to this stage. We employ various well-established models to capture global and local feature information within images more effectively. For a 3-channel input image I \u2208 R3\u00d7Hi\u00d7 Wi , Hi and Wi are the height and width of an image. A feature extractor is applied to extract feature R \u2208 RCi\u00d7H\u00d7W , where Ci denotes the number of feature embedding, with a succeeding convolutional layer linearly projecting its feature space from Ci into C."
        },
        {
            "heading": "3.2 QUERY TRANSFORMER",
            "text": "The semantic heterogeneity across labels requires the model to discern and capture unique feature representations specific to each individual task. Inspired by the remarkable performance of the query-based classifier, we employ learnable query tokens for classification to mitigate semantic conflicts between tasks. Specifically, this work employs a transformer to better extract and wrap task-specific underlying features in class-wise learnable tokens.\nGiven an extracted image representation R, an encoder-decoder standard transformer is applied to inspect features for each label. On the encoder side, the image representation from the backbone is flattened into R \u2208 RC\u00d7HW and proceeded by Ne encoder layers as tokens. To decouple different labels effectively, we endorse Liu et al. (2021a); Lanchantin et al. (2021b) to use learnable tokens as the query. On the decoder side, a learnable token is fed to the transformer decoder as the query for each possible label so that the feature of each label would be learned individually. Nd decoder layers are stacked to extract the features of input representations in accordance with each possible label. The decoder layer accepts T \u2208 RL\u00d7C for every L possible label, where C is the embedding dimension for each token. The cross-attention module in the transformer decoder performs on the query from the learnable label tokens (decoder) and the key and value from the extracted features (encoder), facilitating each label to mine respective representations."
        },
        {
            "heading": "3.3 HYBRID SHARING LAYER",
            "text": "Given the potential semantic correlations among different labels, the features extracted from corresponding tasks may exhibit a positive correlation, providing complementary information to enhance model performance. However, improper exploitation of these correlations through learning jointly may cause performance degeneration since parts of these labels conflict with each other semantically due to their inherent heterogeneity, making them hard to learn jointly. To better leverage the positive correlations while suppressing detrimental impact due to heterogeneity between different tasks, we introduce the MoE mechanism into the multi-label classification area, inspired by the success of Progressive Layered Extraction (PLE) (Tang et al., 2020). Particularly, we employ several shared and task-specialized experts to capture positively correlated features among tasks and task-specific features, respectively, with a gate network adaptively fusing these features. The design of experts and gates can be very flexible and compatible as long as the output shapes are aligned, and in this work, we employ simple but effective linear layers to illustrate our approach.\nFigure 2 depicts the details of Hybrid Sharing Layers, where L indicates the number of tasks, i.e. the number of labels in multi-label classification. For any task ti, i \u2208 {1, 2, \u00b7 \u00b7 \u00b7L}, a group of taskspecialized experts Eti,j , j \u2208 {1, 2, \u00b7 \u00b7 \u00b7nt}, is assigned to extract features for this task exclusively, where nt refers to the number of experts for this task. Apart from these task-specialized experts groups for every task, a group of shared experts Es,j , j \u2208 {1, 2, \u00b7 \u00b7 \u00b7ns} is responsible for gathering global patterns and dispatching them to those potentially positively correlated tasks. Outputs of each expert group are harmonized by gate network, respectively, so that each task would have customized control on the weight of task-specialized and shared experts\u2019 outputs.\nAlgorithm 1 outlines the detailed mechanism of the MoE mechanism that we applied. Let Xt \u2208 RN\u00d7L\u00d7di be a batched input to the mixture-of-expert layer, where N refers to batch size and di means input embedding dimension. And let Xs be the input for shared experts group with the exact same shape as Xt. The outputs of a Hybrid Sharing Layer comprise task-specialized outputs and a shared output.\nIn the task-specialized section, each label (task) is processed independently. For a batched input Xti \u2208 RN\u00d7di on task ti, a set of task-specific experts, denoted as Eti,j \u2208 Rdi\u00d7do , is utilized,\nwhere j represents the j-th expert in the group. Yti|ti,j = XtiEti,j \u2208 RN\u00d7do represents the output of expert Eti,j on task ti. The subscript of Y , separated by |, refers to the output task and expert subscript, respectively, meaning that it is the output of the j-th task-specialized expert in ti and takes inputs from task ti. Similarly, a group of shared experts, denoted as Es,j \u2208 Rdi\u00d7do , is used for task ti. Shared experts, which would be used in all tasks, also accept Xti in task ti, and Yti|s,j = XtiEs,j \u2208 RN\u00d7do represents the output of expert Es,j on task ti. A gate network, denoted as Gti \u2208 Rdi\u00d7(L\u00b7nt+ns), is employed to produce weights for outputs from all shared and task-specialized experts on task ti. The gate network takes Xti as input, and outputs Softmax(XtiGti) \u2208 RN\u00d7(nt+ns) as the weights for experts\u2019 outputs. Here, nt task-specialized experts for ti and all ns shared ones are employed. The task output is a weighted mean of all experts with activation \u03c3, as described in the following equations, where (k) stands for tensor indexing.\nYti = Concat(Yti|ti,j , Yti|s,j) \u2208 RN\u00d7do\u00d7(nt+ns) Oti = \u2211 k [ \u03c3(Yti) (k) \u2299 Softmax(XtiGti)(k) ] \u2208 RN\u00d7do (1)\nIn the shared section, all shared experts Es and task-specialized experts Eti are utilized to gather potential features, with a total of ns + L \u00d7 nt experts. These experts use shared input Xs as their input. Similar to the task-specialized part, a gate fuses shared and task-specialized features. The shared gate network, denoted as Gs, harmonizes the outputs from both shared experts and taskspecialized experts across all tasks with weights derived from the shared input Xs. Algorithm 1 described shared and task-specialized parts in the Hybrid Sharing Layer.\nYs = Concat(Ys|sj , Ys|ti,j ) \u2208 RN\u00d7d0\u00d7(L\u00b7nt+ns) Os = \u2211 k [ \u03c3(Ys) (k) \u2299 Softmax(XsGS)(k)) ] \u2208 RN\u00d7d0 (2)\nIt is worth noting that the shared and task-specialized parts receive the outputs from their respective parts in the previous layer as inputs, except the initial layer, which uses an identical input."
        },
        {
            "heading": "4 EXPERIMENT",
            "text": "We have performed extensive experiments on two datasets, MS-COCO and PASCAL VOC, to verify the superiority of our model. In accordance with the preceding works, we choose mean average precision as our primary metric. Some experiments also report some secondary metrics, including overall F1-score (OF1) and pre-category F1-score (CF1). Metrics on Top-3 are also reported. The definitions of these metrics are available in the Appendix.\nAlgorithm 1: Hybrid Sharing Layer Procedure Data: Input to shared experts Xs; Input to\ntask ti experts Xti ; Shared expert gate Gs; Task ti expert gate Gti ; Number of shared experts ns; Number of task-specialized experts per task nt; Shared experts Es,(\u00b7); Task-specialized experts Et(\u00b7),(\u00b7); Number of labels L; Activation function \u03c3\nResult: Shared output Os; Task-specialized output Oti Ys = [ ] for i\u2190 1 to L do\nYti = [ ] end for i\u2190 1 to L do\nfor j \u2190 1 to nt do Ys|ti,j = XsEti,j Ys.append(Ys|ti,j ) Yti|ti,j = XtiEti,j Yti .append(Yti|ti,j)\nend end for j \u2190 1 to ns do\nYs|sj = XsEs,j Ys.append(Ys|sj ) for i\u2190 1 to L do\nYti|sj = XtiEs,j Yti .append(Yti|sj )\nend end As \u2190 Softmax(GsXs) Y \u2032s \u2190 Concat(Ys) Os \u2190 \u2211 As \u2299 \u03c3(Ys) for i\u2190 1 to L do Y \u2032ti \u2190 Concat(Yti) Ati \u2190 Softmax(GtiXti) Oti \u2190 \u2211 Ati \u2299 \u03c3(Yti) end\nCN N \u2212 RN\nN\nVG G + SV\nM Fe v + Lv H CP RD AL RA RL SS G RL M CA R AS L AD D \u2212 G CN\nQ 2L\n\u2212 TR\nes L H SQ\n0 1 2 3 4 5 6 7 8 9 10 AP D iff er en ce (% )\nCow\u2212Horse AP(%) Difference on VOC\nFigure 3: Absolute AP performance difference between cow and horse on VOC dataset in (%)\nCN N \u2212 RN\nN\nVG G + SV\nM Fe v + Lv H CP RD AL RA RL SS G RL M CA R AS L AD D \u2212 G CN\nQ 2L\n\u2212 TR\nes L H SQ\n0\n1\n2\n3 4 AP D iff er en ce (% )\nBike\u2212Motobike AP(%) Difference on VOC\nFigure 4: Absolute AP performance difference between bike and motobike on VOC dataset in (%)"
        },
        {
            "heading": "4.1 ABLATION STUDY",
            "text": "As shown in Table 1, the number of shared experts greatly influences the performance. We choose ResNet10T as the backbone and Q2L with the same backbone as the baseline. An ablation study is performed on MS-COCO. The input image is fixed at a size of 576 \u00d7 576. The proposed model, which includes shared experts (HSQ), outperforms the baseline by 1.3% on mAP, demonstrating that including shared experts facilitates the transfer of information between tasks and mitigates negative transfer. The results also reveal that removing shared experts from the model leads to a considerable drop in performance due to the complete cutoff of sharing information among all tasks, underscoring the importance of sharing features in achieving substantial performance improvements. HSQ-Linear indicates that the hybrid sharing layers of the model are replaced by fully-connected layers with the same depth and dimensions, sharing all information across all labels without discriminating taskspecialized information. It is demonstrated that the inclusion of shared experts is a crucial factor in enhancing the performance of the proposed model compared with HSQ-Linear. The findings highlight the potential benefits of incorporating shared experts and can inform the development of future multi-label image classification models."
        },
        {
            "heading": "4.2 PERFORMANCE ON THE MS-COCO DATASET",
            "text": "MS-COCO (Lin et al., 2014) is a large dataset of 80 object classes originally for image segmentation and object detection tasks. By extracting object information in annotations, it is also widely used to evaluate various models for multi-label image classification tasks. We test our model on MS-\nCOCO to compare it with previous well-known works and state-of-the-art approaches. Results are shown in Table 2. We use ResNet101 (He et al., 2016) and ConvNeXt (Liu et al., 2022)(CvN) as the backbone and set input resolution to 576 \u00d7 576. Those backbones noted with -22k indicate that they are pre-trained on ImageNet-22k. Our HSQ model with CvN as the backbone achieves state-of-the-art performance at an mAP of 92.0%. Among all ResNet101-based approaches, our model outperforms all its counterparts. HSQ-R101 at the resolution of 576\u00d7 576 achieves an mAP of 87.1%. Please note that for this model, we employ two successive Hybrid Sharing Layers of do = 1024, 512, MLP with one hidden layer of 128 neurons as gate and one of 64 as classifier."
        },
        {
            "heading": "4.3 PERFORMANCE ON THE VOC DATASET",
            "text": "PASCAL-VOC (Everingham et al., 2015) 2007 is also a well-acknowledged dataset for multi-label image classification. It comprises images of 20 classes and is split into train-val and test sets. We follow previous work to train on train-val set and validate on test set on 2007 version. The results of\nexperiments on it are displayed in Table 3. Our model is compared against various established methods and state-of-the-art techniques. Notably, our proposed approach surpasses all its counterparts, achieving an impressive mAP score of 96.4%. The per-class average precision is also presented, with the SOTA performance bolded. Items that improve in comparison with previous works are underscored in the last two rows.\nPerformance among Labels and Cross-label Comparison Among the 20 available labels, our model exhibits superior performance in 15 of them. Compared to Q2L, another transformer-based model, our model improves 105 pairs of labels, 27 pairs more than Q2L achieved. To provide a visual comparison, we randomly select two labels with moderate performance (i.e., \u201dtable\u201d and \u201dtrain\u201d) and illustrate them in Figure 7. In this graph, each dot represents a specific approach, with dots in the upper-right corner indicating better performance. We further explore the performance difference between two pairs of labels with similar semantics, as depicted in Figure 3 and 4. Our method not only outperforms previous work but also exhibits a smaller absolute cross-label performance gap.\nRobust Performance across Multiple Image Scales. In addition to previous experiments on MSCOCO, we perform extra experiments on different image scales to verify the performance of our model improves as image resolution decreases in Figure 6. We perform experiments on 576 \u00d7 576, 488 \u00d7 448 and 384 \u00d7 384. Results confirm that our model provides consistent considerable performance as image scale decreases."
        },
        {
            "heading": "4.4 VISUALIZATION RESULT",
            "text": "The proposed model incorporates several gates to harmonize outputs from experts. We verify that different tasks would rely on different experts on PASCAL VOC. Figure 5 depicts the weights of experts\u2019 outputs on all 20 tasks and the average load across tasks on a sampled data batch. The last row represents an expert\u2019s average load across all tasks. Weights are softmax-activated values of the gate networks\u2019 outputs, presented in log scale, along 33 different experts on the X-axis, 32 of which are shared, and the last one is respective task-specialized. A lighter block color indicates an expert with more weight in the final harmonized output. It is evident to note that all tasks focus on different experts. For instance, Es,8, Es,30, Es,28 have the most significant impact on chair, dog and horse respectively. The weight distributions across experts exhibit variations among tasks, indicating that\ndistinct tasks rely on different sets of experts, each extracting distinctive representations, while even average loads on experts show that all experts are engaged during inference."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "In this paper, regarding MLC as an MTL problem, we introduce HSQ, a transformer-based multilabel image classification model, which is constituted of a feature extraction backbone, query transformer, and Hybrid Sharing Layers that provide evident information sharing among tasks with shared and task-specialized experts leveraging inter- and intra-task information, respectively. Taskspecialized experts are organized by respective gate networks, allowing each task to accept correlated information from shared experts independently. Shared experts accept input from all tasks, fusing all potentially useful information. Our model mitigates the negative transfer problem in MLC when formulating it as an MTL problem, where learning several labels jointly may hinder performance improvement. Our experiments demonstrate that HSQ provides a significant improvement on tested datasets. Furthermore, HSQ can simultaneously enhance per-label performance across multiple labels, mitigate performance gap among labels, and effectively handle semantic correlation and heterogeneity.\nReproducibility Statement In this paper, we make efforts to provide detailed information to ensure the reproducibility and completeness of our work. Figure 1 illustrates the architecture of our model. Algorithm 1 and Figure 2 provide a clear overview and procedure for our crucial component, the Hybrid Sharing Layer. Section A.3 in the Appendix describes the hyper-parameters and devices we use, including optimizer, learning rate, etc. Section 4.2 and 4.3 describe details on how we prepare our dataset, including version, partitioning strategy, etc. The code will be available upon acceptance."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "This work is partially supported by the National Natural Science Foundation of China (grant no. 62106101), and the Natural Science Foundation of Jiangsu Province (grant no. BK20210180). This work is also partially supported by the AI & AI for Science Project of Nanjing University."
        },
        {
            "heading": "A APPENDIX",
            "text": "A.1 METRICS DEFINITION\nWe here provide the definitions of metrics that are mentioned in our paper.\nPre-Category Precision: CP = 1\nC \u2211 i N ci Npi\nPre-Category Recall: CR = 1\nC \u2211 i N ci Ngi\nPre-Category F1 score: CF1 = 2\u00d7 CP\u00d7 CR CP+ CR\nOverall Precision: OP = \u2211 i N c i\u2211\ni N p i Overall Recall: OR = \u2211 i N c i\u2211\ni N g i\nOverall F1 score: OF1 = 2\u00d7OP\u00d7OR OP+OR\nwhere C stands for the number of labels, N ci refers to the number of samples that are correctly predicted for the i-th label, Npi denotes the number of predicted samples for the i-th label, and N g i means the number of ground truth samples for the i-th label.\nA.2 FLOPS AND #PARAMETERS DETAILS\nModel Backbone Resolution FLOPs #Parameters\nQ2L-R10T\u2020(Liu et al., 2021a) ResNet10T 576 8.4G 14.8M HSQ (ns = 1, nt = 1) ResNet10T 576 8.6G 14.3M\nHSQ (ns = 16, nt = 1) ResNet10T 576 8.7G 15.5M\nA.3 IMPLEMENTATION DETAILS\nUnless otherwise stated, the following setting is valid for all experiments. We resize all input images from any dataset to Hi\u00d7Wi = 576\u00d7 576. After a pre-trained backbone with timm, a convolutional layer projection would keep the embedding size C = Ci = 2048 as default.\nTwo layers of mixture-of-expert layers with output embedding dimension do = 64, 16 are implemented after a transformer with one encoder layer and two decoder layers, followed by a linear layer to make a final prediction.\nWe train the model for 100 epochs using the Adam optimizer, with weight decay of 1e-2, (\u03b21, \u03b22) = (0.9, 0.9999), and a learning rate of 1e-4. All experiments are run on 4 Tesla V100-SXM2-32GB. The pre-training details of the experiments are provided in parentheses. We, by default, do not train our model on extra data, except if it is otherwise stated. Note that models with -22k indicate that they use a pre-trained backbone on ImageNet22k (Ridnik et al., 2021b). Additionally, experiments marked with \u2020 indicate that they have been replicated in this work."
        }
    ],
    "year": 2024
}