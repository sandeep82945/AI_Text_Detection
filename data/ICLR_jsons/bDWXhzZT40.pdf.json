{
    "abstractText": "Predictive uncertainty\u2013a model\u2019s self-awareness regarding its accuracy on an input\u2013 is key for both building robust models via training interventions and for test-time applications such as selective classification. We propose a novel instance-conditional reweighting approach that captures predictive uncertainty using an auxiliary network, and unifies these trainand test-time applications. The auxiliary network is trained using a meta-objective in a bilevel optimization framework. A key contribution of our proposal is the meta-objective of minimizing dropout variance, an approximation of Bayesian predictive uncertainty, We show in controlled experiments that we effectively capture diverse specific notions of uncertainty through this meta-objective, while previous approaches only capture certain aspects. These results translate to significant gains in real-world settings\u2013selective classification, label noise, domain adaptation, calibration\u2013and across datasets\u2013Imagenet, Cifar100, diabetic retinopathy, Camelyon, WILDs, Imagenet-C,-A,-R, Clothing1M, etc. For Diabetic Retinopathy, we see upto 3.4%/3.3% accuracy AUC gains over SOTA in selective classification. We also improve upon large-scale pretrained models such as PLEX (Tran et al., 2022).",
    "authors": [],
    "id": "SP:8a669f6adbe537efd4836b7804ae2757a3813809",
    "references": [
        {
            "authors": [
                "G\u00f6rkem Algan",
                "Ilkay Ulusoy"
            ],
            "title": "Meta Soft Label Generation for Noisy Labels",
            "venue": "In ICPR,",
            "year": 2020
        },
        {
            "authors": [
                "Quentin Bertrand",
                "Quentin Klopfenstein",
                "Mathieu Blondel",
                "Samuel Vaiter",
                "Alexandre Gramfort",
                "Joseph Salmon"
            ],
            "title": "Implicit differentiation of lasso-type models for hyperparameter optimization",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Mathieu Blondel",
                "Quentin Berthet",
                "Marco Cuturi",
                "Roy Frostig",
                "Stephan Hoyer",
                "Felipe Llinares-L\u00f3pez",
                "Fabian Pedregosa",
                "Jean-Philippe Vert"
            ],
            "title": "Efficient and modular implicit differentiation",
            "venue": "Advances in neural information processing systems,",
            "year": 2022
        },
        {
            "authors": [
                "Charles Blundell",
                "Julien Cornebise",
                "Koray Kavukcuoglu",
                "Daan Wierstra"
            ],
            "title": "Weight uncertainty in neural network",
            "venue": "In International conference on machine learning,",
            "year": 2015
        },
        {
            "authors": [
                "Glenn W Brier"
            ],
            "title": "Verification of forecasts expressed in terms of probability",
            "venue": "Monthly weather review,",
            "year": 1950
        },
        {
            "authors": [
                "Wray L Buntine"
            ],
            "title": "Bayesian backpropagation",
            "venue": "Complex systems,",
            "year": 1991
        },
        {
            "authors": [
                "Aniket Das",
                "Dheeraj M Nagaraj",
                "Praneeth Netrapalli",
                "Dheeraj Baby"
            ],
            "title": "Near optimal heteroscedastic regression with symbiotic learning",
            "venue": "In The Thirty Sixth Annual Conference on Learning Theory,",
            "year": 2023
        },
        {
            "authors": [
                "John Denker",
                "Daniel Schwartz",
                "Ben Wittner",
                "Sara Solla",
                "Richard Howard",
                "Lawrence Jackel",
                "John Hopfield"
            ],
            "title": "Large automatic learning, rule extraction, and generalization",
            "venue": "Complex systems,",
            "year": 1987
        },
        {
            "authors": [
                "Stefan Depeweg",
                "Jos\u00e9 Miguel Hern\u00e1ndez-Lobato",
                "Finale Doshi-Velez",
                "Steffen Udluft"
            ],
            "title": "Uncertainty decomposition in bayesian neural networks with latent variables",
            "venue": "arXiv preprint arXiv:1706.08495,",
            "year": 2017
        },
        {
            "authors": [
                "Stefan Depeweg",
                "Jose-Miguel Hernandez-Lobato",
                "Finale Doshi-Velez",
                "Steffen Udluft"
            ],
            "title": "Decomposition of uncertainty in bayesian deep learning for efficient and risk-sensitive learning",
            "venue": "In International Conference on Machine Learning,",
            "year": 2018
        },
        {
            "authors": [
                "Ran El-Yaniv"
            ],
            "title": "On the foundations of noise-free selective classification",
            "venue": "Journal of Machine Learning Research,",
            "year": 2010
        },
        {
            "authors": [
                "Kumar"
            ],
            "title": "Trainable calibration measures for neural networks from kernel mean embeddings",
            "venue": "In ICML,",
            "year": 2018
        },
        {
            "authors": [
                "Matthew Faw",
                "Rajat Sen",
                "Karthikeyan Shanmugam",
                "Constantine Caramanis",
                "Sanjay Shakkottai"
            ],
            "title": "Mix and match: an optimistic tree-search approach for learning models from mixture distributions",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Leo Feng",
                "Mohamed Osama Ahmed",
                "Hossein Hajimirsadeghi",
                "Amir Abdi"
            ],
            "title": "Stop overcomplicating selective classification: Use max-logit",
            "venue": "arXiv preprint arXiv:2206.09034,",
            "year": 2022
        },
        {
            "authors": [
                "Angelos Filos",
                "Sebastian Farquhar",
                "Aidan N Gomez",
                "Tim GJ Rudner",
                "Zachary Kenton",
                "Lewis Smith",
                "Milad Alizadeh",
                "Arnoud De Kroon",
                "Yarin Gal"
            ],
            "title": "A systematic comparison of bayesian deep learning robustness in diabetic retinopathy",
            "year": 1912
        },
        {
            "authors": [
                "Adam Fisch",
                "Tommi Jaakkola",
                "Regina Barzilay"
            ],
            "title": "Calibrated selective classification",
            "venue": "arXiv preprint arXiv:2208.12084,",
            "year": 2022
        },
        {
            "authors": [
                "Yarin Gal",
                "Zoubin Ghahramani"
            ],
            "title": "Dropout as a bayesian approximation: Representing model uncertainty in deep learning",
            "venue": "In international conference on machine learning,",
            "year": 2016
        },
        {
            "authors": [
                "Aditya Gangrade",
                "Anil Kag",
                "Venkatesh Saligrama"
            ],
            "title": "Selective classification via one-sided prediction",
            "venue": "In International Conference on Artificial Intelligence and Statistics,",
            "year": 2021
        },
        {
            "authors": [
                "Yaroslav Ganin",
                "Evgeniya Ustinova",
                "Hana Ajakan",
                "Pascal Germain",
                "Hugo Larochelle",
                "Fran\u00e7ois Laviolette",
                "Mario Marchand",
                "Victor Lempitsky"
            ],
            "title": "Domain-adversarial training of neural networks. The journal of machine learning",
            "year": 2030
        },
        {
            "authors": [
                "Yonatan Geifman",
                "Ran El-Yaniv"
            ],
            "title": "Selective classification for deep neural networks",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Yonatan Geifman",
                "Ran El-Yaniv"
            ],
            "title": "Selectivenet: A deep neural network with an integrated reject option",
            "venue": "In International conference on machine learning,",
            "year": 2019
        },
        {
            "authors": [
                "Arindam Ghosh",
                "Thomas Schaaf",
                "Matthew Gormley"
            ],
            "title": "Adafocal: Calibration-aware adaptive focal loss",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Soumyadip Ghosh",
                "Mark Squillante",
                "Ebisa Wollega"
            ],
            "title": "Efficient stochastic gradient descent for learning with distributionally robust optimization",
            "venue": "arXiv preprint arXiv:1805.08728,",
            "year": 2018
        },
        {
            "authors": [
                "Chuan Guo",
                "Geoff Pleiss",
                "Yu Sun",
                "Kilian Q Weinberger"
            ],
            "title": "On calibration of modern neural networks",
            "venue": "In International conference on machine learning,",
            "year": 2017
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Kevin Gimpel"
            ],
            "title": "A baseline for detecting misclassified and out-of-distribution examples in neural networks",
            "year": 2017
        },
        {
            "authors": [
                "Chester Holtz",
                "Tsui-Wei Weng",
                "Gal Mishne"
            ],
            "title": "Learning sample reweighting for adversarial robustness",
            "venue": "Arxiv,",
            "year": 2021
        },
        {
            "authors": [
                "Lang Huang",
                "Chao Zhang",
                "Hongyang Zhang"
            ],
            "title": "Self-adaptive training: beyond empirical risk minimization",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Eyke H\u00fcllermeier",
                "Willem Waegeman"
            ],
            "title": "Aleatoric and epistemic uncertainty in machine learning: An introduction to concepts and methods",
            "venue": "Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Alex Kendall",
                "Yarin Gal"
            ],
            "title": "What uncertainties do we need in bayesian deep learning for computer vision",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Pang Wei Koh",
                "Shiori Sagawa",
                "Henrik Marklund",
                "Sang Michael Xie",
                "Marvin Zhang",
                "Akshay Balsubramani",
                "Weihua Hu",
                "Michihiro Yasunaga",
                "Richard Lanas Phillips",
                "Irena Gao"
            ],
            "title": "Wilds: A benchmark of in-the-wild distribution shifts",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Alex Krizhevsky",
                "Geoffrey Hinton"
            ],
            "title": "Learning multiple layers of features from tiny images",
            "year": 2009
        },
        {
            "authors": [
                "Ramnath Kumar",
                "Kushal Majmundar",
                "Dheeraj Nagaraj",
                "Arun Suggala"
            ],
            "title": "Max-margin inspired persample reweighitng for robust learning. ICLR workshop on pitfalls of limited data and computation for trustworthy ML, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Yongchan Kwon",
                "Joong-Ho Won",
                "Beom Joon Kim",
                "Myunghee Cho Paik"
            ],
            "title": "Uncertainty quantification using bayesian neural networks in classification: Application to ischemic stroke lesion segmentation",
            "venue": "In Medical Imaging with Deep Learning,",
            "year": 2018
        },
        {
            "authors": [
                "Yongchan Kwon",
                "Joong-Ho Won",
                "Beom Joon Kim",
                "Myunghee Cho Paik"
            ],
            "title": "Uncertainty quantification using bayesian neural networks in classification: Application to biomedical image segmentation",
            "venue": "Computational Statistics & Data Analysis,",
            "year": 2020
        },
        {
            "authors": [
                "Balaji Lakshminarayanan",
                "Alexander Pritzel",
                "Charles Blundell"
            ],
            "title": "Simple and scalable predictive uncertainty estimation using deep ensembles",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Daniel Levy",
                "Yair Carmon",
                "John C Duchi",
                "Aaron Sidford"
            ],
            "title": "Large-scale methods for distributionally robust optimization",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Feng Liu",
                "Bo Han",
                "Tongliang Liu",
                "Chen Gong",
                "Gang Niu",
                "Mingyuan Zhou",
                "Masashi Sugiyama"
            ],
            "title": "Probabilistic margins for instance reweighting in adversarial training",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Ziyin Liu",
                "Zhikang Wang",
                "Paul Pu Liang",
                "Russ R Salakhutdinov",
                "Louis-Philippe Morency",
                "Masahito Ueda"
            ],
            "title": "Deep gamblers: Learning to abstain with portfolio theory",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Andrey Malinin",
                "Mark Gales"
            ],
            "title": "Predictive uncertainty estimation via prior networks",
            "venue": "Advances in neural information processing systems,",
            "year": 2018
        },
        {
            "authors": [
                "Mehryar Mohri",
                "Gary Sivek",
                "Ananda Theertha Suresh"
            ],
            "title": "Agnostic federated learning",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Marcin Mo\u017cejko",
                "Mateusz Susik",
                "Rafa\u0142 Karczewski"
            ],
            "title": "Inhibited softmax for uncertainty estimation in neural networks",
            "venue": "arXiv preprint arXiv:1810.01861,",
            "year": 2018
        },
        {
            "authors": [
                "Jishnu Mukhoti",
                "Viveka Kulharia",
                "Amartya Sanyal",
                "Stuart Golodetz",
                "Philip Torr",
                "Puneet Dokania"
            ],
            "title": "Calibrating deep neural networks using focal loss",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Mahdi Pakdaman Naeini",
                "Gregory Cooper",
                "Milos Hauskrecht"
            ],
            "title": "Obtaining well calibrated probabilities using bayesian binning",
            "venue": "In Twenty-Ninth AAAI Conference on Artificial Intelligence,",
            "year": 2015
        },
        {
            "authors": [
                "Luis Oala",
                "Cosmas Hei\u00df",
                "Jan Macdonald",
                "Maximilian M\u00e4rz",
                "Wojciech Samek",
                "Gitta Kutyniok"
            ],
            "title": "Interval neural networks: Uncertainty scores",
            "venue": "arXiv preprint arXiv:2003.11566,",
            "year": 2020
        },
        {
            "authors": [
                "Tim Pearce",
                "Felix Leibfried",
                "Alexandra Brintrup"
            ],
            "title": "Uncertainty in neural networks: Approximately bayesian ensembling",
            "venue": "In International conference on artificial intelligence and statistics,",
            "year": 2020
        },
        {
            "authors": [
                "Stephan Rabanser",
                "Anvith Thudi",
                "Kimia Hamidieh",
                "Adam Dziedzic",
                "Nicolas Papernot"
            ],
            "title": "Selective classification via neural network training dynamics",
            "venue": "arXiv preprint arXiv:2205.13532,",
            "year": 2022
        },
        {
            "authors": [
                "Mengye Ren",
                "Wenyuan Zeng",
                "Bin Yang",
                "Raquel Urtasun"
            ],
            "title": "Learning to reweight examples for robust deep learning",
            "venue": "In International conference on machine learning,",
            "year": 2018
        },
        {
            "authors": [
                "Murat Sensoy",
                "Lance Kaplan",
                "Melih Kandemir"
            ],
            "title": "Evidential deep learning to quantify classification uncertainty",
            "venue": "Advances in neural information processing systems,",
            "year": 2018
        },
        {
            "authors": [
                "Jun Shu",
                "Qi Xie",
                "Lixuan Yi",
                "Qian Zhao",
                "Sanping Zhou",
                "Zongben Xu",
                "Deyu Meng"
            ],
            "title": "Metaweight-net: Learning an explicit mapping for sample weighting",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Lewis Smith",
                "Yarin Gal"
            ],
            "title": "Understanding measures of uncertainty for adversarial example detection",
            "venue": "arXiv preprint arXiv:1803.08533,",
            "year": 2018
        },
        {
            "authors": [
                "Masashi Sugiyama",
                "Shinichi Nakajima",
                "Hisashi Kashima",
                "Paul Buenau",
                "Motoaki Kawanabe"
            ],
            "title": "Direct importance estimation with model selection and its application to covariate shift adaptation",
            "venue": "Advances in neural information processing systems,",
            "year": 2007
        },
        {
            "authors": [
                "Masashi Sugiyama",
                "Taiji Suzuki",
                "Shinichi Nakajima",
                "Hisashi Kashima",
                "Paul Von B\u00fcnau",
                "Motoaki Kawanabe"
            ],
            "title": "Direct importance estimation for covariate shift adaptation",
            "venue": "Annals of the Institute of Statistical Mathematics,",
            "year": 2008
        },
        {
            "authors": [
                "Yonglong Tian",
                "Dilip Krishnan",
                "Phillip Isola"
            ],
            "title": "Contrastive multiview coding",
            "venue": "In European conference on computer vision,",
            "year": 2020
        },
        {
            "authors": [
                "Naftali Tishby",
                "Esther Levin",
                "Sara A Solla"
            ],
            "title": "Consistent inference of probabilities in layered networks: Predictions and generalization",
            "venue": "In International Joint Conference on Neural Networks,",
            "year": 1989
        },
        {
            "authors": [
                "Dustin Tran",
                "Jeremiah Liu",
                "Michael W Dusenberry",
                "Du Phan",
                "Mark Collier",
                "Jie Ren",
                "Kehang Han",
                "Zi Wang",
                "Zelda Mariet",
                "Huiyi Hu"
            ],
            "title": "Plex: Towards reliability using pretrained large model extensions",
            "venue": "arXiv preprint arXiv:2207.07411,",
            "year": 2022
        },
        {
            "authors": [
                "Matias Valdenegro-Toro"
            ],
            "title": "Deep sub-ensembles for fast uncertainty estimation in image classification",
            "venue": "arXiv preprint arXiv:1910.08168,",
            "year": 2019
        },
        {
            "authors": [
                "Matias Valdenegro-Toro",
                "Daniel Saromo Mori"
            ],
            "title": "A deeper look into aleatoric and epistemic uncertainty disentanglement",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),",
            "year": 2022
        },
        {
            "authors": [
                "Yeming Wen",
                "Dustin Tran",
                "Jimmy Ba"
            ],
            "title": "Batchensemble: an alternative approach to efficient ensemble and lifelong learning",
            "venue": "arXiv preprint arXiv:2002.06715,",
            "year": 2020
        },
        {
            "authors": [
                "Xiaobo Xia",
                "Tongliang Liu",
                "Bo Han",
                "Nannan Wang",
                "Mingming Gong",
                "Haifeng Liu",
                "Gang Niu",
                "Dacheng Tao",
                "Masashi Sugiyama"
            ],
            "title": "Part-dependent label noise: Towards instance-dependent label noise",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Tong Xiao",
                "Tian Xia",
                "Yi Yang",
                "Chang Huang",
                "Xiaogang Wang"
            ],
            "title": "Learning from massive noisy labeled data for image classification",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2015
        },
        {
            "authors": [
                "Yao-Liang Yu",
                "Csaba Szepesv\u00e1ri"
            ],
            "title": "Analysis of kernel mean matching under covariate shift",
            "venue": "In Proceedings of the 29th International Conference on Machine Learning,",
            "year": 2012
        },
        {
            "authors": [
                "Jiaru Zhang",
                "Yang Hua",
                "Zhengui Xue",
                "Tao Song",
                "Chengyu Zheng",
                "Ruhui Ma",
                "Haibing Guan"
            ],
            "title": "Robust bayesian neural networks by spectral expectation bound regularization",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Peng Zhang",
                "Jiuling Wang",
                "Ali Farhadi",
                "Martial Hebert",
                "Devi Parikh"
            ],
            "title": "Predicting failures of vision systems",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2014
        },
        {
            "authors": [
                "Zizhao Zhang",
                "Tomas Pfister"
            ],
            "title": "Learning fast sample re-weighting without reward data",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Xiao Zhou",
                "Yong Lin",
                "Renjie Pi",
                "Weizhong Zhang",
                "Renzhe Xu",
                "Peng Cui",
                "Tong Zhang"
            ],
            "title": "Model agnostic sample reweighting for out-of-distribution learning",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Xinlei Zhou",
                "Han Liu",
                "Farhad Pourpanah",
                "Tieyong Zeng",
                "Xizhao Wang"
            ],
            "title": "A survey on epistemic (model) uncertainty in supervised learning",
            "venue": "Recent advances and applications. Neurocomputing,",
            "year": 2022
        },
        {
            "authors": [
                "Xiao"
            ],
            "title": "classification given the relatively larger image size, dataset size, and number & complexity of categories. Alongside these, for input dependent label noise Inst.CIFAR-100 with \u03b5 = 0.2 Xia et al. (2020) PLEX label uncertainty datasets (IN-100H, CF-100H) Tran et al",
            "venue": "BASELINES",
            "year": 2015
        },
        {
            "authors": [
                "SN Geifman",
                "El-Yaniv"
            ],
            "title": "It proposed using an auxillary network to predict a confidence score whether model wants to predict for an instance at a pre-defined coverage rate",
            "venue": "SAT Huang et al",
            "year": 2019
        },
        {
            "authors": [
                "Brier Loss"
            ],
            "title": "Squared error betwee softmax predicted logits and the ground truth label vector",
            "venue": "Brier et al",
            "year": 1950
        },
        {
            "authors": [
                "Adafocal Ghosh"
            ],
            "title": "It updates the hyperparameter of the focal loss independently for each instance based on its value at previous step, utilizing validation feedback",
            "venue": "MWN Shu et al",
            "year": 2022
        },
        {
            "authors": [
                "Ren"
            ],
            "title": "It uses one free-parameter per training instance as the weight of its loss while updating model. These free parameters are learned using meta-learning to optimize validation performance",
            "year": 2018
        },
        {
            "authors": [
                "G ADDITIONAL BASELINES G"
            ],
            "title": "COMPARISON WITH ENTROPY BASED REGULARIZATION A recent work (Feng et al., 2022) proposed using the maximum logit directly as the uncertainty measure, of the methods trained with selective classification/learning",
            "year": 2022
        },
        {
            "authors": [
                "Ren"
            ],
            "title": "2021) for training the classifier, and used MCD at test-time for selective classification; this tells us if the training procedure",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "In applications with significant cost of error, classifiers need to accurately quantify and communicate uncertainty about their predictions. Test-time applications of uncertainty include OOD detection (Hendrycks & Gimpel, 2017), and selective classification (rejecting test instances where the model is not confident (El-Yaniv et al., 2010)). Intrinsic classifier measures of uncertainty (softmax probabilities, or derived metrics) perform poorly compared to more sophisticated approaches\u2013e.g., methods that learn an extra \u201creject\u201d category at train time (Liu et al., 2019), or estimate the Bayesian posterior predictive uncertainty given a distribution over model weights (Gal & Ghahramani, 2016). Other work show benefits of reweighting training instances or reweighting subpopulations within the data improving generalization (Zhou et al., 2022a; Faw et al., 2020; Ghosh et al., 2018). The intuition is that the weights realign the training distribution to simulate a possibly different, or worst-case target distribution (see e.g., Kumar et al. (2023).\nIn this work, we are interested in learning an instance dependent weight function (on the training set) that captures predictive uncertainty. A significant challenge is that a wide range of underlying causal factors can contribute to uncertainty such as input-dependent label noise, missing features, and distribution shift between training and test data (refer Sec. 4 for more details and analysis). For covariate shift between train and test, weights on training set related to importance sampling or kernel mean matching (Sugiyama et al., 2007; Yu & Szepesv\u00e1ri, 2012) are appropriate. Other neural net based solutions transform train to match test in distribution (Ganin et al., 2016). In the case of label noise that varies across different regions in a domain, it is known that downweighing samples with larger uncertainty (Das et al., 2023) is the best solution. In robust optimization, one tries to weigh samples that yields the worst loss (Levy et al., 2020) in an uncertainty ball around the current distribution. Of the related literature, no work, to the best of our knowledge, addresses all these diverse sources of uncertainty.\nThe question that motivates our work is: Given training and validation set, what is the best reweighing function of training instances that yields a good uncertainty measure at test time? Further, what robustness properties are achieved by the reweighted classifier?\nWe propose a novel instance dependent weight learning algorithm for learning predictive uncertainty \u2013 REVAR(Reweighting for Dropout Variance Reduction). We propose to learn an auxiliary uncertainty model p = g(x) (which we call U-SCORE) alongside training of the primary model y = f(x). U-SCORE unifies train and test-time applications of uncertainty. Our primary algorithmic insight is the use of a novel dropout based variance regularization term in the U-SCORE objective. Below we summarize our approach and key contributions.\n1. U-SCORE as an uncertainty measure: We learn an instance-conditional function p = g(x), allowing us to capture a rich notion of model uncertainty. This function is learned in a nested or bi-level optimization framework where the classifier minimizes a weighted training loss, and the U-SCORE minimizes a meta-loss from the resulting classifier on a separate meta-training dataset. Our approach strictly generalizes previous reweighting approaches based on bi-level optimization (Shu et al., 2019; Zhang & Pfister, 2021), as they cannot be used at test-time. We propose a variance reduction meta-regularization loss that forms a key part of our approach, and incentivizes the learned function w = g(x) to faithfully capture uncertainty as a function of the input instance.\n2. U-SCORE scaling with different uncertainty sources: For robust test-time prediction, a reweighting approach should ideally downweight training samples with high label noise (since this would be independent uncertainty) but emphasize (upweight) hard examples in terms of overlap with respect to validation data. We demonstrate through controlled synthetic linear regression experiments, that U-SCORE scores achieve both these desirable behaviors and even smoothly interpolates between two different scaling behaviors when different sources of uncertainty are present. Ablations show that changes to our meta-loss does not yield the same performance.\n3. Real-world applications: U-SCORE outperforms conventional and state-of-the-art measures of predictive uncertainty by significant margins in a wide range of datasets (Diabetic Retinopathy, CIFAR-100, ImageNet, Clothing1M, etc) and domain shift conditions (Camelyon, WILDS, ImagenetA,-C,-R, etc). These results mirror and strengthen the findings of the controlled study in real-life applications. As an example, in Diabetic Retinopathy, a well-studied benchmark dataset for selective classification, we show upto 3.4%/3.3% accuracy & AUC gains over state-of-the-art methods in the domain shift setting. We also improve upon large-scale pretrained models such as PLEX Tran et al. (2022) by \u223c4% relative on label uncertainty and from 7.5 to 6.2 ECE in calibration."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Uncertainty Estimation. Uncertainty in deterministic neural networks is extensively studied, e.g., via ensemble modelling (Wen et al., 2020; Valdenegro-Toro, 2019; Lakshminarayanan et al., 2017) or using prior assumptions (Oala et al., 2020; Moz\u0307ejko et al., 2018; Malinin & Gales, 2018; Sensoy et al., 2018) for calculating uncertainty in a single forward pass. Bayesian NNs offer a principled approach(Buntine, 1991; Tishby et al., 1989; Denker et al., 1987; Blundell et al., 2015; Kwon et al., 2020) by modeling a distribution over model weights. They output posterior distributions over predictions after marginalizing weight uncertainty, where the distribution spread encodes uncertainty. Gal & Ghahramani (2016) connects deterministic and Bayesian NNs by using multiple forward passes with weight dropout to approximate the posterior; several works (Kwon et al., 2018; Kendall & Gal, 2017; Kwon et al., 2020; Pearce et al., 2020) use this to quantify predictive uncertainty.\nInstance weighting and bilevel optimization. Importance weighting of training instances is popular in robust learning, e.g., OOD generalization (Zhou et al., 2022a), robustness to label noise (Shu et al., 2019; Ren et al., 2018; Zhang & Pfister, 2021), Group Distributionally Robust Optimization (Faw et al., 2020; Mohri et al., 2019; Ghosh et al., 2018), covariate shift (Sugiyama et al., 2008). Weights could be a predefined function of margin or loss (Liu et al., 2021; Kumar et al., 2023; Sugiyama et al., 2008) or learned via bi-level optimization (Ren et al., 2018; Zhang & Pfister, 2021; Zhou et al., 2022a). In these latter case, instance weights are free parameters (Ren et al., 2018), or a learned function of loss (Shu et al., 2019; Holtz et al., 2021). Bilevel optimization is widely used in many settings, and can be solved efficiently (Bertrand et al., 2020; Blondel et al., 2022).\nFormal models of uncertainty. Uncertainty in neural network predictions can be decomposed into: (a) Uncertainty in input (aleatoric) and (b) uncertainty in model parameters (epistemic). Recent works (Kendall & Gal, 2017; Kwon et al., 2018; Smith & Gal, 2018; Zhou et al., 2022b; Depeweg et al., 2018; 2017; H\u00fcllermeier & Waegeman, 2021) propose explicit models for these uncertainties in\nboth classification and regression. (Kendall & Gal, 2017) model epistemic uncertainty for regression as the variance of the predicted means per sample, and for classification using the pre-softmax layer as a Gaussian Distribution with associated mean and variance. Kwon et al. (2018) directly calculate variance in the predictive probabilities. Depeweg et al. (2017) proposed an information-theoretic decomposition into epistemic and aleatoric uncertainties in reinforcement learning. Smith & Gal (2018) proposed Mutual Information between the expected softmax output and the estimated posterior as a measure of model uncertainty. Valdenegro-Toro & Mori (2022) aim to disentangle notions of uncertainty for various quantification methods and showed that aleatoric and epistemic uncertainty are related to each other, contradicting previous assumptions. Finally, recent work (Zhang et al., 2021) proposed a regularization scheme for robustness which also minimizes epistemic uncertainty.\nSelective Classification. Selective classification offeres a model the option of rejecting test instances, within a bounded rejection rate, to maximize accuracy on predicted instances. It is a benchmark for uncertainty measures in Bayesian NNs (Filos et al., 2019). Many approaches have been studied, including risk minimization with constraints (El-Yaniv et al., 2010); optimizing selective calibration (Fisch et al., 2022); entropy regularization (Feng et al., 2022) and training-dynamics-based ensembles (Rabanser et al., 2022); a separate \u201creject\u201d output head (Deep Gamblers (Liu et al., 2019)); a secondary network for rejection (Selective Net (Geifman & El-Yaniv, 2019)); optimizing training dynamics (Self-adaptive training (SAT (Huang et al., 2020)); class-wise rejection models (OSP (Gangrade et al., 2021)); variance over monte-carlo dropout predictions (MCD (Gal & Ghahramani, 2016)). We compare against these and a range of other competitive baselines in our experiments."
        },
        {
            "heading": "3 REVAR: DROPOUT VARIANCE REDUCTION",
            "text": ""
        },
        {
            "heading": "3.1 PRELIMINARIES & OBJECTIVE",
            "text": "Given a dataset D = (X,Y ) with input-output pairs X \u2208 X , Y \u2208 Y , supervised learning aims to learn a model f\u03b8 : X 7\u2192 Y , where \u03b8 denote the model parameters. In this work, we wish to also learn a measure of predictive uncertainty g\u0398 : X 7\u2192 [0, 1] (with \u0398 denoting the parameters) for our classifier f\u03b8, as a function of the input x. The function g(x) should output high scores on inputs x for which f(\u00b7) is most uncertain, or is more likely to be incorrect. A well-calibrated measure of model uncertainty g(x) could be used for many applications in failure detection & avoidance \u2013 for instance, in selective classification (El-Yaniv et al., 2010), one can abstain from making a prediction by applying a suitable threshold \u03bb on g(\u00b7) in the following manner:\n(f, g)(x) = { f(x) if g(x) < \u03bb; can\u2019t predict if g(x) \u2265 \u03bb; (1)\nAn effective measure g(\u00b7) should maximize accuracy of f(\u00b7) on unrejected test instances given a targeted rejection rate. Similarly, one could also evaluate g(\u00b7) by checking its calibration; i.e., whether the rank-ordering of instances based on g correlates strongly with the accuracy of the classifier f .\nIn our work, we also use a specialized set (Xs,Y s) (also referred to as a validation set), separate from the training set (X ,Y ) of the classifier, to obtain an unbiased measure of classifier performance. Where available, we use a validation set that is representative of the test data; as in previous work, this helps our learned classifier to better adapt to test data in cases of distribution shifts."
        },
        {
            "heading": "3.2 THE LEARNED REWEIGHTING FRAMEWORK",
            "text": "We start with a bilevel optimization problem for learned reweighting of a supervised objective:\n\u03b8\u2217 = argmin \u03b8\n1\nN N\u2211 i=1 wi \u00b7 ltrain(yi, f\u03b8(xi)) s.t. {w\u2217i } = arg min{wi} M\u2211 j=1 lmeta(x s j , y s j , \u03b8 \u2217) (2)\nwhere xi \u2208 X , yi \u2208 Y and xsi \u2208 Xs, ysi \u2208 Y s . Here, (N,M) are the sizes of the train and validation sets, \u03b8 are model parameters for f , wi are free parameters reweighting instance losses, and (ltrain(), lmeta()) are suitably chosen loss functions (e.g., cross-entropy). The meta-loss above is an implicit function of the weights {w\u2217i }, through their influence on \u03b8\u2217. The formulation finds a (model, weights) pair such that the weighted train loss, and the unweighted validation loss for the learned f(\u00b7), are both minimized.\nOur intuition for this starting point is as follows: Bilevel optimization has been used for overcoming noisy labels in training data Ren et al. (2018), and in addressing covariate shift at instance level Sugiyama et al. (2007) and group level Mohri et al. (2019). In many of these applications, as well as in general for improved model generalization Kumar et al. (2023), there is theoretical and empirical evidence that the training instance weights should be proportional to instance hardness, or uncertainty under covariate shift. Thus, the learned reweighting objective is a good starting point for capturing instance uncertainty, at least with respect to the training data."
        },
        {
            "heading": "3.3 INSTANCE-CONDITIONAL WEIGHTS IN REVAR",
            "text": "Our goal in REVAR is to learn an instance-conditional scorer U-SCORE that is used both for traintime reweighting, and at test time as a measure of predictive uncertainty. Previous work on bilevel optimization for reweighting (Shu et al., 2019; Ren et al., 2018; Zhang & Pfister, 2021) cannot be used at test time because the learned weights are free parameters (Ren et al., 2018) or a function of instance loss (Shu et al., 2019; Zhang & Pfister, 2021). We address this challenge by learning instance weights as a direct function of the instance itself, i.e., w = g\u0398(x), allowing us to capture a much richer and unconstrained measure of model uncertainty, which is also robustly estimated using the bilevel formulation. Our bilevel formulation now becomes:\n\u03b8\u2217 = argmin \u03b8\n1\nN N\u2211 i=1 g\u0398(xi) \u00b7 l(yi, f\u03b8(xi)) s.t. \u0398\u2217 = argmin \u0398 Lmeta(Xs, Y s, \u03b8\u2217) (3)\nwhere \u03b8\u2217, \u0398\u2217 correspond to optimal parameters for classifier, U-SCORE respectively."
        },
        {
            "heading": "3.4 VARIANCE MINIMIZATION AS META-REGULARIZATION",
            "text": "We now define the meta-loss Lmeta and in particular, a novel variance-minimizing regularizer that substantially improves the ability of g(\u00b7) to capture model uncertainty. This meta-regularizer leps(\u03b8, x) is added to the cross-entropy classification loss l(y, f\u03b8(x)) that is typically part of the meta-loss, leading to the following meta-objective on the specialized set (X s,Ys):\nLmeta = Lc(Xs, Ys) + Leps(\u03b8,Xs) = M\u2211 j=1 l(ysj , f\u03b8(x s j)) + leps(\u03b8, x s j) (4)\nMinimizing Bayesian Posterior uncertainty: We take inspiration from the Bayesian NN literature which regularizes the posterior on weight distribution so as to avoid overfitting or to embed extra domain knowledge. Unlike standard neural networks which output a point estimate for a given input, Bayesian networks (Buntine, 1991; Tishby et al., 1989; Denker et al., 1987; Blundell et al., 2015; Kwon et al., 2020) learn a distribution p(\u03c9|D) over the neural network weights \u03c9 given the dataset D using maximum a posteriori probability (MAP) estimation. The predictive distribution for the output y\u2217, given the input x and D, can be then calculated by marginalisation as follows: p(y\u2217|x\u2217, D) = \u222b p(y\u2217|x\u2217, \u03c9)p(\u03c9|D)d\u03c9 \u2248 1K \u2211K k=1 p(y\n\u2217|x\u2217, \u03c9k). Here we utilize a recent result (Gal & Ghahramani, 2016) that augmenting the training of a deterministic neural network with dropout regularization yields a variational approximation for a Bayesian Neural Network. At test time, taking multiple forward passes through the neural network for different dropout masks yields a Monte-Carlo approximation to Bayesian inference, and thereby a predictive distribution. The variance over these Monte Carlo samples is therefore a measure of predictive uncertainty:\nleps(\u03b8, x) \u2248 1\nK\n( K\u2211\nk=1\n(fDk\u2299\u03b8(x)\u2212 E[fDk\u2299\u03b8(x)])2 )\n(5)\nwhere Dk denotes the dropout mask at kth sample and Dk \u2299 \u03b8 denotes the application of this dropout mask to the neural network parameters. This MCD measure is popular as an estimate of instance uncertainty (Gal & Ghahramani, 2016), and is competitive with state-of-the-art methods for selective classification (Filos et al., 2019).\nWe propose to use this variance-based estimate of posterior uncertainty as a meta-regularization term in our approach. In particular, instead of directly minimizing the posterior uncertainty on the training data w.r.t. primary model parameters \u03b8, we minimize it w.r.t. U-SCORE parameters \u0398 on the specialized set instead.\nIntuition for meta-regularization: We make two significant, intertwined technical contributions: (1) using an instance conditioning network g\u0398(x) for calculating training instance weights, and (2) using variance minimization in the meta-objective for g(x). The network g(x) can be applied to unseen test instances without labels, and enables test-time applications of uncertainty not possible with previous methods. For the variance minimization loss, note that we minimize MCD variance of the primary model f(x) on the validation set; this is data not available to the primary model during training. This loss has to be indirectly minimized through training data reweightings, which in turn are accomplished through g(x). In other words, the function g(x) is forced to focus on the most uncertain/ most informative instances x in the training data. Thus, our goal with the meta-regularizer is not, primarily, to build a classifier f(x) with low output variance; we instead use it indirectly, via the meta-objective, to improve the quality of the uncertainty measure g(x)."
        },
        {
            "heading": "3.5 META-LEARNING WITH BILEVEL LOSS",
            "text": "The modeling choices we have laid out above result in a bi-level optimization scheme involving the meta-network and classifier parameters. This is because the values of each parameter set \u03b8 and \u0398 influence the optimization objective of the other. Expressing Lmeta as a function L of inputs Xs, Y s, \u03b8, this bi-level optimization scheme can be formalized as:\nCalculating updates. Instead of solving completely for the inner loop (optimizing \u0398) for every setting of the outer parameter \u03b8, we aim to solve this bilevel optimization using alternating stochastic gradient descent updates. At a high level, the updates are:\n\u0398t+1 = \u0398t \u2212 \u03b11\u2207\u0398L(Xs, Y s, \u03b8t) ; \u03b8t+1 = \u03b8t \u2212 \u03b12 1\nN \u2207\u03b8 ( N\u2211 i=1 g\u0398(xi) \u00b7 l(yi, f\u03b8(xi)) ) (6)\nwhere \u03b11 and \u03b12 are the learning rates corresponding to these networks, l is the classification loss on the training dataset (X ,Y ) using the classifier network with parameters \u03b8. This style of stochastic optimization is commonly used for solving bilevel optimization problems in a variety of settings (Algan & Ulusoy, 2020; Shu et al., 2019). Further details, including all approximations used for deriving these equations, are provided in the appendix. Also, the U-SCORE is implemented as a standard neural network architecture like the classifier (please refer appendix)."
        },
        {
            "heading": "4 U-SCORE CAPTURES DIFFERENT SOURCES OF UNCERTAINTY",
            "text": "We now create a set of synthetic generative models for linear regression and study the performance of our algorithm for conceptual insights. We investigate three kinds of uncertainty that depends on the input instance x: 1) Samples that are atypical with respect to train but typical with respect to validation 2) Samples where label noise is higher 3) Samples where uncertainty in the label is due to some unobserved latent features that affect the label.\nUsually (1) and (3) are considered to be \u201cepistemic\u201d uncertainty while (2) is understood as \u201caleatoric\u201d uncertainty. (1) is due to covariate shift and (3) is due to missing features relevant for the label. Surprisingly, we show in this section that our algorithm\u2019s weights are proportional to uncertainty from (1) while being inversely proportional to uncertainty of type (2) and (3). This is also desirable from a theoretical perspective, as we explain below\u2013for instance, when (1) and (3) are absent, the best solution is to downweight samples with larger label noise (Das et al., 2023). Similarly, when only (1) is present, one would desire examples that are typical with respect to validation and atypical with respect to train to be weighted higher. We show that our algorithm captures these notions, and furthermore smoothly interpolates between them depending on the mix of different sources of uncertainty.\nGenerative Model: For all the results in this section, for both training and validation data for all , Y is sampled as follows. Y = WTdataX + (N (0, 1) \u00b7 [c+GTX]) (7) X \u2208 R72\u00d71. X = [XoXl], Xo \u2208 R48\u00d71, Xl \u2208 R24\u00d71. For training data, we sample Xtrain \u223c N (\u00b5,\u03a3). For validation, Xval \u223c N (\u00b5\u2032,\u03a3) where \u00b5\u2032 = \u00b5+ sN (\u00b5s,\u03a3s); here s > 0 is a scalar that determines the amount of covariate shift between training and validation. WTdata = [W T o W T l ] where Wo \u2208 R48\u00d71, Wl \u2208 R24\u00d71.\nIntuition: The above generative model1 is chosen to allow testing of the different scenarios described above, w.r.t. label noise and covariate shift. We control additive noise via the variables c (zero-mean noise) and G (scaled input-dependent noise). We can introduce covariate shift between training and validation data using s. Additionally, although the output Y depends on all components of X , we can consider scenarios where only a portion of X := [XoXl] is made available to the learner (Xo is observed while Xl are latent variables that influence Y but are unobserved). Finally, we can also create combined scenarios where more than one of these factors are at play.\nEvaluation, Baselines & Metrics: We train our method on paired train-validation datasets sampled according to different scenarios, and inspect U-SCORE scores for points x in the training set. For each scenario, we predict a theoretical ideal for the instance dependent weights, and calculate R2 score of model fits for the U-SCORE outputs against the theoretical ideal. We compare against MWN Shu et al. (2019), a baseline that calculates loss-dependent instance weights using bilevel optimization. We also measure the specific contributions of our variance-minimization regularization, by evaluating a second baseline that is identical to REVAR except for this meta-regularization term\u2013we term this Instance-Based Reweighting (IBR).\nScenario 1 - Sample Dependent Label Noise and No Shift:. c = 0, s = 0, G \u0338= 0. This represents a scenario where there is no covariate shift but label uncertainty in both train and validation depend on the sample. Label noise scales as |GTX|2, while weights of the meta-network are inversely proportional to this quantity; this weighting is shown to be optimal by recent theoretical work Das et al. (2023)). (Tab. 1).\nScenario 2 - Sample Dependent Label noise and Covariate Shift: We set c = 0, G \u0338= 0, s \u0338= 0. We expect the weights to be inversely proportional to label noise (Scenario 1); we also expect weights to be directly proportional to the uncertainty due to covariate shift, i.e., h := (x\u2212 \u00b5)2. This latter idea draws from classical work on importance sampling under covariate shift Sugiyama et al. (2007) and many follow-on papers that theoretically motivate weighting training data proportional to hardness or uncertainty. When both factors are at play, we posit a simple linear combination w(x) \u223c \u03bb1|GT x|2 + \u03bb2 \u00b7 h. as an a priori desirable weighting of training instances. Interestingly, REVAR weights in fact correlate very strongly with this linear model. Further, REVAR weights shift smoothly towards uncertainties from covariate shift as its magnitude increases Sec. 4.\nScenario 3 - Hardness due to missing relevant features: We set c = 1, G = 0, s = 0. However, only Xo is available to the learner in both train and validation. Here, the missing or latent features Xl influence the label in a way inaccessible to the learner. Interestingly this behaves much like sampledependent label noise\u2013given the features seen (Xo), the \u201clabel noise\u201d (contribution of Xl that cannot be modeled) is controlled by the conditional dependence of Xl|Xo, and is therefore proportional to WTl \u03a3(Xl|Xo)Wl. This is identical to scenario 1, and the optimal solution is again inverse weighting of instances. Indeed, the weights predicted by U-SCORE roughly scales as 1\nWTl \u03a3(Xl|Xo)Wl) (Tab. 1).\nAlthough conventionally treated as \u201cepistemic uncertainty\u201d, our meta network\u2019s weights are inversely proportional to this, as desired.\nScenario 4 - Dropping Features and covariate shift in validation set:. We set c = 1, G = 0, s > 0 and only Xo is available to the learner. In this case, the weights predicted by our meta-network\n1The exact dimensionalities are not materially relevant to our findings; we have also evaluated other settings with similar results.\nroughly follows the relationship \u03bb1 WTl \u03a3(Xl|Xo)Wl) + \u03bb2(x \u2212 \u00b5)2; this follows in a straightforward manner from the scenarios considered above. As before, from Sec. 4, when we increase the amount of covariate shift, the U-SCORE weights also reflect uncertainty from covariate shift more than that of label noise.\nScenario 5 - Spurious Feature Shift:. c = 1, G = 0, s > 0. Further We = 0. However, the learner sees Xo alone for both test and validation.We now create a validation set using another distribution N (\u00b5\u2032 , \u2211\u2032 ) such that the distribution of Xo remains same and the distribution of Xl changes. This can be understood as a distribution shift setup where the core features required for predicting output for any instance remain the same but the background features change. In this case, the weights predicted by U-SCORE are close to uniform. This is because the model has to rely on core features Xo alone, and there is no difference amongst training samples with respect to these features.\nSummary. Tab. 1 summarizes the findings\u2013our approach correlates strongly with theoretically desirable models for instance weights; further, when sources of uncertainty are mixed in different proportions, U-SCORE smoothly interpolates between them in determining instance weights (Sec. 4). Two additional key findings: the closest previous work (MWN Shu et al. (2019), which proposed loss-based reweighting using a meta-network) performs significantly worse than our approach across scenarios. Interestingly, our own baseline (IBR, instance-based reweighting) improves across scenarios on MWN, but still falls significantly short of our full method. This provides strong evidence that variance minimizing meta-regularization is the key ingredient in the success of our approach."
        },
        {
            "heading": "5 EXPERIMENTS AND RESULTS",
            "text": "Having verified that REVAR accurately captures captures sources of uncertainty under various data generation process, we now evaluate it on a wide range of real-world scenarios and datasets. Since instance-level hardness or uncertainty is difficult to quantify in real-world settings, we use tasks such as selective classification or Neural Network calibration that evaluate uncertainty measures in aggregate form. We also show the general applicability of REVAR using experiments on the large-scale pretrained PLEX model (Tran et al., 2022) that show significant gains (appendix)."
        },
        {
            "heading": "5.1 BASELINES.",
            "text": "For selective classification, we compare REVAR against several key baselines: Softmax-Response (SR) (Geifman & El-Yaniv, 2017), Monte-Carlo Dropout (MCD) (Gal & Ghahramani, 2016), SelectiveNet (SN) (Geifman & El-Yaniv, 2019), Deep Gamblers (DG) (Liu et al., 2019) and Self-Adaptive Training (SAT) (Huang et al., 2020). Please refer to Sec. 2 for more information on these methods.\nWe compare REVAR against recent proposals for calibration which show impressive results: Focal Loss (FLSD-53) (Mukhoti et al., 2020), MMCE (et al., 2018), Brier Loss (Brier et al., 1950) and AdaFocal (Ghosh et al., 2022) alongside the standard cross-entropy loss. Re-weighting Baselines: We compare our method against other bi-level optimization based reweighting baselines including Meta-Weight-Net (MWN) Shu et al. (2019), Learning to Reweight (L2R) Ren et al. (2018) and Fast Sample Re-weighting (FSR) Zhang & Pfister (2021), which have been designed explicitly label imbalance or random noise in labels setup, under various setups including selective classification (appendix), calibration (appendix) and input-dependent label uncertainty. New baselines: We design two new baselines to separately measure bilevel optimization (reweighting) and meta-regularization: (a) ERM + Variance Reduction (VR) in training loss, and (b) Margin-based reweighting (MBR) of instances 2. For both these baselines, we use softmax response for selection. Datasets. We used the Diabetic Retinopathy (DR) detection dataset (kag, 2015), a significant realworld benchmark for selective classification, alongside the APTOS DR test dataset (Society, 2019) for covariate shift analysis. We also used CIFAR-100, ImageNet-100, and ImageNet-1K datasets. For the OOD test setting, we used Camelyon, WILDS, ImageNet-C,R,A. Furthermore, we utilize the Inst.CIFAR-100 (Xia et al., 2020), Clothing1M, IN-100H CF-100H(Tran et al., 2022) datasets for input dependent noisy label settings. Please see appendix for dataset and preprocessing details."
        },
        {
            "heading": "5.2 PRIMARY REAL-WORLD SETTING: IN-DOMAIN VALIDATION, IN/OUT-DOMAIN TEST",
            "text": ""
        },
        {
            "heading": "5.2.1 REVAR OUTPERFORMS SOTA AT SELECTIVE CLASSIFICATION",
            "text": "Table 2 shows the results of the Area under the accuracy-rejection curve (Zhang et al., 2014) for REVAR and baselines on various datasets including Kaggle DR (in-dist.) & APTOS (OOD testing for model trained on Kaggle DR). Our method outperforms all other methods, showing its effectiveness as a measure of model uncertainty. In particular, we beat our own baselines VR,MBR that use variance reduction on training loss, and margin-based reweighting respectively on top of ERM. Accuracy & AUC at different coverage levels for all datasets are in the appendix.\nScaling to large datasets: Table2 shows that our method scales to large datasets such as Imagenet; we provide additional evidence (accuracy & AUC at various coverage levels) in the appendix.\nWe also compared against MCD and SAT on ImageNet-A/C/R benchmarks for robustness analysis. For all these experiments, the AUARC metric is provided in table 30."
        },
        {
            "heading": "5.2.2 REVAR IS COMPETITIVE AT CALIBRATION",
            "text": "Table 3 shows the results for this analysis for a pre-temperature-scaling setup. This is so that none of the approaches achieves any advantage of post-hoc processing and the evaluation is fair for all (see supplementary for more details). As can be observed, our results are competitive or better than SOTA for the calibration task. We also provide selective calibration (calibration at different coverages in selective classification), where we show larger gains over the baselines and demonstrate better calibration across the range\u2013see supplementary materials.\nTable 4: AUARC: In-Domain, OOD test set\nImageNet-A ImageNet-C ImageNet-R Ours MCD SAT Ours MCD SAT Ours MCD SAT\n9.98 8.44 8.91 65.9 63.7 64.2 68.8 66.8 67.1\nTable 5: AUARC: OOD val, test set\nData MCD SAT Revar Revar-PV\nCamelyon 74.99 75.16 76.32 78.12 iWildCam 76.07 76.17 77.98 79.86\nTable 6: Label Noise: Re-weighting methods\nMCD MWN L2R FSR Ours\nInst.CIFAR-100 61.12 65.89 67.12 70.21 71.87 Clothing1M 68.78 73.56 72.97 73.86 73.97\nTable 7: Label Noise: Plex Model\nPlex Plex+ours\nIN-100H 0.75 0.71 CF-100H 0.49 0.47\n2Since margin and loss are highly correlated, this is similar to loss-based reweighting"
        },
        {
            "heading": "5.3 INPUT DEPENDENT LABEL NOISE",
            "text": "We now evaluate our methods on datasets comprising instance dependent label-noise. These include instance CIFAR-100 proposed in Xia et al. (2020), Clothing1M Xiao et al. (2015) having noisy human labels and the label uncertainty setup proposed in PLEX Tran et al. (2022) paper where instead of a single label, probabilities are assigned due to complex input and KL Divergence metric is used. On the CIFAR-100, Clothing datasets we compare with the other re-weighting methods designed for removing label noise using bi-level optimization including MWN, L2R, FSR (refer Sec. 5.1). In the Plex setup, we use our model on top of PLEX and analyze the improvements.\nTab. 33, Tab. 32 shows the results. Even though the re-weighting baselines have been designed for handling label noise, they are ineffective when this label noise is instance dependent, and are better suited for label imbalance/random flip (instance-independent). Our method handles these scenarios well, with signifciant gains. This matches the findings from the controlled study (Sec. 4)."
        },
        {
            "heading": "5.4 SHIFTED VALIDATION SET",
            "text": "We now study the real-world scenario of shifted/OOD validation & test sets. We used theCamelyon, iWildCam datasets from the WILDS benchmark Koh et al. (2021) where train, validation and test sets are each drawn from different domains. All methods are trained and tested on the same data splits. Table 31 compares REVAR and MCD, SAT on AUARC for selective classification. Again, we outperform the baselines, showing that REVAR efficiently handles models of uncertainty in domain shift settings; this reinforces the findings in our controlled scenarios 2 and 4 (Sec. 4).\nUsing unlabeled test-domain samples. We now consider another setup where the labeled validation set is in-domain, but we can use unlabelled OOD samples from the test domain. Since our leps regularizer does not use labels, we propose using it for unsupervised domain adaptation on these samples. REVAR-PV pools in-domain and (unlabeled) test-domain meta-losses, while REVAR-DSV only uses test-domain samples for the meta-loss. This also corresponds to scenario 2 (Sec. 4) since the main component in determining hardness (variance minimization) is applied on OOD examples. REVAR-PV handily beats other approaches in this setting (Tab. 8), suggesting that both generalization and variance minimization are important. This aligns with Sec. 4: as we increase covariate shift in validation, the hardness coefficient \u03bb2 dominates in determining U-SCORE scores. Tab. 31 provides further evidence, where validation is OOD and shifted approximately towards test data due to weak supervision from unlabelled instances. See appendix for results on ImageNet-C,R,A datasets.\nTable 8: Comparing REVAR variants for unsupervised domain adaptation. REVAR-PV pools in-domain and out-of-domain validation data, while REVAR-DSV only uses domain-shift validation data in the meta-objective.\nCoverage REVAR REVAR-PV REVAR-DSV VR-DSV VR-PV\n1.0 86.1 88.3 85.3 87.4 87.2 0.8 88.1 90.6 87.4 88.6 88.8 0.6 89.9 91.7 88.2 89.9 89.6 0.4 91.4 93.1 88.9 91.9 91.8 Figure 2: Lesion Study (DR)"
        },
        {
            "heading": "5.5 REVAR LESION ANALYSIS",
            "text": "We examined the contribution of the various components of our proposal to the overall efficacy of REVAR in selective classification. We study the following variants: (1) REVAR-NoVarMin: Drops the variance-reduction meta-regularization, (2) REVAR-SR and REVAR-MCD: Uses REVAR classifier\u2019s logits or MCD respectively at test time instead of U-SCORE, (3) MCD: baseline.\nFigure 2 shows this comparison on the DR dataset under country shift, for the accuracy metric. We make the following observations: (1) REVAR performs best, and dropout variance reduction plays a very large role. (2) REVAR-MCD beats MCD: REVAR classifiers are inherently more robust. (3) REVAR beats REVAR-SR and REVAR-MCD (differing only in test-time scoring): U-SCORE is a better measure of uncertainty than MCD or logits, even on the more-robust REVAR classifier."
        },
        {
            "heading": "REPRODUCIBILITY STATEMENT",
            "text": "We use the ResNet-50 architecture as classifier for the diabetic retinopathy experiments. Each experiment has been run 5 times and the associated mean and standard deviation are reported. For MC-Dropout baseline and our meta-objective, we calculate the uncertainty by 10 forward passes through the classifier. We used a learning rate of 0.003 and a batch size of 64 to train each model in each of the experiments. For our re-weighting scheme, we separate 10 percent of the data as the validation set. For CiFAR-100, ImageNet-1k, Clothing1M also we have used ResNet-50 and for ImageNet-100 we use VGG-16 in all experiments, inspired by recent work (Huang et al., 2020; Liu et al., 2019). A batch size of 128 is used and an initial learning rate of 1e\u22122 with a momentum of 0.9 is used. For U-SCORE we have used a learning rate of 1e\u2212 4 with a momentum of 0.9 and batch size same as classifier for all the experiments. For clothing dataset, we have used the same processing and hyper-parameter setup as MWN Shu et al. (2019). For efficiency and better training, we update the U-SCORE for every K = 15 steps of the classifier. Also, we warm start the classifier by training it without the U-SCORE for first 25 epochs. A weight decay of 10\u22124 is used for both the networks. For all experiments, training is done for 300 epochs. For the unlabelled test instances from Kaggle data to APTOS data or ImageNet to ImageNet-A,R,C data we split the training into 230 epochs without unlabelled images and 70 epochs combining the unlabelled images along with the source data for a total of 300 epochs. Around 10% of the test images are sampled as this unlabelled setup for this setup. For the PLEX model experiments, we just apply our technique on top its existing implementation, keeping same hyper-parameters and using a learning rate of 1e\u2212 3 for the U-SCORE. For synthetic data, Wdata, Gdata are matrices with each of their elements sampled from N (5, 10), N (12, 18) respectively. The \u00b5, \u2211 for Xtrain are generated by sampling 72 values from N (1, 10), N (5, 10) respectively. For scenarios 2, 4 the values of s are kept at 25, 50 respectively."
        },
        {
            "heading": "A REVAR ALGORITHM",
            "text": "Algorithm 1 REVAR training procedure. Require: Prediction Network parameters \u03b8, U-SCORE parameters \u0398, learning rates (\u03b21, \u03b22), dropout\nrate pdrop, training data {xi, yi}Ni=1, validation data {xsi , ysi }Mi=1, U-SCORE update intervalM . Ensure: Robustly trained classifier parameters \u03b8\u2217, U-SCORE parameters \u0398\u2217 to predict uncertainty.\n1: Randomly initialize \u03b8 and \u0398, t = 1; 2: for e = 1 to E do \u25b7 E: number of epochs 3: sample a minibatch {(xi, yi)}ni=1 from training data; \u25b7 n denotes the batch size 4: if t%M == 0 then 5: Create a copy of the current prediction model, denoting parameters by \u03b8\u0302 6: sample minibatch {(xvi , yvi )}mi=1 from validation data 7: \u03b8\u0302 \u2190 \u03b8\u0302 \u2212 \u03b21\u2207\u03b8\u0302 \u2211( l(f\u03b8\u0302(x), y) ) \u25b7 Update the copy of prediction model\n8: \u0398\u2190 \u0398\u2212 \u03b22\u2207\u0398 \u2211( l(yvi , f\u03b8\u0302(x v i ) + leps(\u03b8\u0302, x v i ) )\n\u25b7 Update U-SCORE using Eq. 5 9: end if\n10: \u03b8 \u2190 \u03b8 \u2212 \u03b21\u2207\u03b8 \u2211\ng\u0398(xi)l(f\u03b8(xi), yi); \u25b7 Update the prediction model 11: \u03b8\u2217 \u2190 \u03b8; \u0398\u2217 \u2190 \u0398; t\u2190 t+ 1 12: end for"
        },
        {
            "heading": "B UPDATES FOR THE BILEVEL OPTIMIZATION",
            "text": "Revisiting the bi-level optimization objective proposed in the paper:\n\u03b8\u2217 = argmin \u03b8\n1\nN N\u2211 i=1 g\u0398(xi) \u00b7 l(yi, f\u03b8(xi))\ns.t. \u0398\u2217 = argmin \u0398 Lmeta(Xs, Y s, \u03b8\u2217)\n(8)\nwhere \u03b8,\u0398 correspond to model parameters for the primary & U-SCORE models (f\u03b8 and g\u0398 respectively), (xt, yt) denote the input-output pair corresponding to the training set and l is the cross-entropy cost function for the classifier. As discussed in the paper, the loss Lmeta(Xs, Y s, \u03b8\u2217) is as follows:\nLmeta(Xs, Y s, \u03b8\u2217) = 1\nM M\u2211 j=1 ( Leps(xsj , \u03b8\u2217) + l(yj , f\u2217\u03b8 (xsj)) ) (9)\nwhere xsj , y s j are input instance and its corresponding output belonging to the specialized set. This formulation results in a nested optimization which involves updating U-SCORE(\u0398) at the outer level using the cross entropy loss and variance of the classifier parameters \u03b8\u2217, generated by sampling different dropout masks The backpropagation based update equation for \u0398 at epoch t (\u0398t) is as follows:\n\u0398t+1 = \u0398t \u2212 \u03b1\nM M\u2211 j=1 \u2207\u0398 ( Leps(xsj , \u03b8\u2217) + l(yj , f\u03b8\u2217t (x s j)) )\n(10)\nwhere \u03b1 is the step size. The gradient term in the above equation can be further simplified to:\n\u2212 \u03b1 M \u00b7 M\u2211 j=1 \u2207\u2217\u03b8 ( Leps(xsj , \u03b8\u2217) + l(f\u03b8\u2217(xsj), ysj ) )\u2223\u2223\u2223\u2223\u2223\u2223 \u03b8\u2217t \u2207\u0398(\u03b8\u2217) \u2223\u2223\u2223\u2223\u2223\u2223\u2223 \u0398t\n(11)\nwhere \u2207\u03b8(.) \u2223\u2223 \u03b8t denotes evaluating the gradient at \u03b8 = \u03b8t. Solving this optimization is quite a time-consuming process since it requires implicit gradient \u2202\u03b8 \u2217\n\u2202\u0398 and also completely optimizing the\ninner loop for one step in outer loop. Thus, we also follow the approximations used in Shu et al. (2019) and convert this nested to an alternating optimization setup for \u0398 and \u03b8. Thus, now \u03b8\u2217 in the above equation can be replaced with \u03b8. To implement this, we again follow MWNShu et al. (2019) and update \u0398 by using a copy of the \u03b8 i.e., \u03b8\u0302 at every instant when U-SCORE is updated. This makes the optimization process easy to interpret as well as stable. At any instant t + 1, it involves first calculating \u03b8\u0302 using the following eq.:\n\u03b8\u0302 = \u03b8t \u2212 \u03b2\nN \u00b7 N\u2211 i=1 \u2207\u03b8 (g\u0398(xi) \u00b7 l(yi, f\u03b8(xi))) \u2223\u2223\u2223\u2223\u2223 \u03b8t,\u0398t\n(12)\nwhere \u03b2 is the step size. Now, differentiating this w.r.t. \u0398:\n\u2207\u0398(\u03b8\u0302) = \u2212 \u03b2\nN \u00b7 N\u2211 i=1 \u2207\u0398g\u0398(xi) \u2223\u2223\u2223\u2223\u2223 \u0398t+1 \u00b7 \u2207\u03b8l(yi, f\u03b8(xi)) \u2223\u2223\u2223\u2223\u2223\u2223 \u03b8t\n(13)\nIn eq. 11, \u03b8\u2217 is replaced by \u03b8\u0302 and the last term \u2202\u03b8\u0302\u2202\u0398 can be replaced by this last equation which will modify the equation 11 to:\n\u03b1\u03b2\nMN \u00b7 M\u2211 j=1 \u2207\u03b8\u0302( Leps(xsj ,\u03b8\u0302)+l(f\u03b8\u0302(xsj),ysj )) \u2223\u2223\u2223\u2223\u2223\u2223 \u03b8\u0302t N\u2211 i=1 \u2207\u0398g\u0398(xi) \u2223\u2223\u2223\u2223\u2223 \u0398t+1 \u00b7 \u2207\u03b8l(yi, f\u03b8(xi)) \u2223\u2223\u2223\u2223\u2223\u2223 \u03b8t\n(14)\nRearranging terms:\n\u03b1\u03b2\nMN \u00b7 N\u2211 i=1 \u2207\u0398g\u0398(xi) \u00b7 M\u2211 j=1 \u2207\u03b8 ( Leps(xsj , \u03b8) + l(f\u03b8(xsj), ysj ) ) \u00b7 \u2207\u03b8l(yi, f\u03b8(xi)) \u2223\u2223\u2223\u2223\u2223\u2223 \u03b8t,\u0398t+1 (15)\nWe now write the update equation for classifier parameters \u03b8 at time t+ 1 involving the re-weighting network parameters \u0398t+1:\n\u03b8t+1 = \u03b8t \u2212 \u03b2\nN \u00b7 N\u2211 i=1 \u2207\u03b8 (g\u0398(xi) \u00b7 l(yi, f\u03b8(xi))) \u2223\u2223\u2223\u2223\u2223 \u03b8t,\u0398t+1\n(16)\nThe equation can be further simplified since \u03b8 is not dependent on \u0398:\n\u03b8t+1 = \u03b8t \u2212 \u03b2\nN \u00b7 N\u2211 i=1 (g\u0398(xi) \u00b7 \u2207\u03b8l(yi, f\u03b8(xi))) \u2223\u2223\u2223\u2223\u2223 \u03b8t,\u0398t+1\n(17)\nThis completes the derivation for update equation. Given the bilevel optimization formulation, we choose to update \u0398 at every K updates of \u03b8 based on the assumption that these K updates of \u03b8 can be used to approximate \u03b8\u2217.\nC IMPROVING PLEX MODEL\nWe now evaluate our proposed method applied to a large pretrained model, specifically the recently proposed PLEX (Tran et al., 2022) model, pretrained on large amounts of data. Tran et al. (2022) show that the rich learned representations in PLEX yield highly reliable predictions and impressive performance on various uncertainty related benchmarks like selective classification, calibration, label uncertainty etc. These applications all require fine-tuning on target data; for our version of PLEX, we replaced their standard unweighted fine-tuning with a weighted fine-tuning combined with the U-SCORE and our associated meta objective.\nDatasets and Tasks. In addition to selective classification and calibration, the PLEX paper studies a label uncertainty task which requires estimating the KL Divergence between the predicted and actual label distribution. For Selective Classification, we compare accuracies at various coverage labels on the DR dataset with covariate shift test set. For calibration, we use the in-distribution and OOD\ndatasets used in the PLEX paper and also compare with approaches like Focal loss, MMCE on these datasets. Finally, for the label uncertainty task, we use the ImageNet-100H and CIFAR-100H datasets used in the PLEX paper.\nREVAR improves PLEX. Table 9a shows Expected Calibration Error (ECE) across datasets; REVAR improves PLEX ECE by significant margins in both In-Distribution (upto around 12%) and Out-ofDistribution (upto around 13%) datasets. Table 9b shows the result of label uncertainty experiment on the ImageNet-100H and CIFAR-100H datasets, showing KL Divergence between the available and predicted probability distribution. Again, using our approach on top of PLEX yields upto 4% gains. Table 10 shows a similar trend in selective classification where we improve PLEX performance at most coverages, and also at 100% coverage, i.e., complete data. This showcases the effectiveness of REVAR at capturing the entire range of uncertainty.\nTaken together, these results show the potential value of REVAR in easily providing gains on top of large, powerful pretrained models, particularly when such foundation models are becoming increasingly common."
        },
        {
            "heading": "D EXPERIMENTAL DETAILS AND RELATED ANALYSIS",
            "text": ""
        },
        {
            "heading": "D.1 DATASETS AND METRICS",
            "text": "We now discuss various datasets we have used to evaluate our method for different tasks.\nDiabetic Retinopathy. The recently proposed Kaggle Diabetic Retinopathy (DR) Detection Challenge (kag, 2015) dataset and the APTOS dataset (Society, 2019) are used as an uncertainty benchmark for Bayesian learning (Filos et al., 2019). The Kaggle dataset consists of 35,697 training images, 2617 validation and 42,690 testing images, whereas the APTOS dataset consists of 2917 evaluation images for testing ddomain generalization for DR detection. In particular, the APTOS dataset is collected from labs in India using different equipment, and is distributionally different from the Kaggle dataset. Both datasets label input retina images with the severity of diabetic retinopathy at 5 grades\u2013 0-4 as No DR, mild, moderate, severe and proliferative. Similar to (Filos et al., 2019), we formulate a binary classification problem grouping grades 0-2 as negative class and 3,4 as positive class. We\nfocus primarily on this dataset given its real-world value and role as a benchmark specifically relevant to selective classification (Filos et al., 2019).\nImageNet-1K and Shifts. This dataset comprises of around 1.4M images with around 1.28M for training, 50k for validation and 100k testing images. It invovles solving a classification problem with 1000 classes. Furthermore, we also include results on popular shifted versions of ImageNet: ImageNet-A comprising hard examples misclassified by Resnet-50 on ImageNet, ImageNet-C comprising 15 different kinds of noises at various severity levels simulating a practical scenario and ImageNet-R comprising changes in style/locations, blurred images or various other changes common in real-world.\nOther image datasets. We study other image classification datasets commonly used for evaluating selective classification methods. including the CIFAR-100 dataset (Krizhevsky et al., 2009) (10 categories of natural images). We also evaluate on a subset of the widely used ImageNet dataset\u2013ImageNet-100 (Tian et al., 2020)\u2013consisting of 100 randomly selected classes from the 1k classes in the original ImageNet dataset. This serves as a larger-scale stress-test of selective classification given the relatively larger image size, dataset size, and number & complexity of categories. Alongside these, for input dependent label noise Inst.CIFAR-100 with \u03f5 = 0.2 Xia et al. (2020) PLEX label uncertainty datasets (IN-100H, CF-100H) Tran et al. (2022) and Clothing1M Xiao et al. (2015) datasets are used."
        },
        {
            "heading": "D.2 BASELINES.",
            "text": "Below we briefly discuss the exhaustive list of baselines used in the paper. MCD Gal & Ghahramani (2016). It applies dropout to any neural network and take multiple passes during inference and calculates the entropy of the averaged soft-logits for uncertainty. DG Liu et al. (2019). It updates the training objective by adding a background class and t the inference time abstains from prediction if the probabity of instance being in that class is higher than some threshold. SN Geifman & El-Yaniv (2019). It proposed using an auxillary network to predict a confidence score whether model wants to predict for an instance at a pre-defined coverage rate. SAT Huang et al. (2020). It uses a target label as exponential moving average of model predictions and label throughout the training and uses an extra class as the selection function, using an updated objective to enforce uncertain examples into the extra class. Brier Loss. Squared error betwee softmax predicted logits and the ground truth label vector. Brier et al. (1950). Mea FLSD-53 Mukhoti et al. (2020). It uses focal loss and proposes a method for selecting the appropriate hyper-parameter for it. Adafocal Ghosh et al. (2022). It updates the hyperparameter of the focal loss independently for each instance based on its value at previous step, utilizing validation feedback. MWN Shu et al. (2019). It uses a bi-level optimization setup comprising a meta-network which takes loss as input and predicts weights for train instances such that validation performance is maximized.Shu et al. (2019) L2R Ren et al. (2018). It uses one free-parameter per training instance as the weight of its loss while updating model. These free parameters are learned using meta-learning to optimize validation performance. FSR Zhang & Pfister (2021). Similar to L2R excpet that it doesn\u2019t require a pre-defined clean validation set and at fixed intervals keep interchanging train and val examples based on how much updating on any instance is changing the validation set loss.\nMetrics. For selective classification, we measure and report accuracy for all datasets. In addition, for the DR dataset, we also measure AUC, a measure of performance that is robust to class imbalance, and to potential class imbalance under selective classification. The accuracy and AUC are measured on those data points selected by each method for prediction, at the specified target coverage. We also measure selective calibration, i.e., calibration error (ECE (Naeini et al., 2015)) measured on only the data points selected by the method for the specified coverage. All metrics reported are averages \u00b1 standard deviation over 5 runs of the method with different random initializations.\nTable 11 summarizes the various datasets used in our experiments, and their characteristics."
        },
        {
            "heading": "D.3 TRAINING & EVALUATION DETAILS",
            "text": "We use the ResNet-50 architecture as classifier for the diabetic retinopathy experiments. Each experiment has been run 5 times and the associated mean and standard deviation are reported. For MC-Dropout baseline and our meta-objective, we calculate the uncertainty by 10 forward passes through the classifier. We used a learning rate of 0.003 and a batch size of 64 to train each model in each of the experiments. For our re-weighting scheme, we separate 10 percent of the data as the validation set. For CiFAR-100 also we have used ResNet-50 and for ImageNet-100 we use VGG-16 in all experiments, inspired by recent work (Huang et al., 2020; Liu et al., 2019). A batch size of 128 is used and an initial learning rate of 1e\u2212 2 with a momentum of 0.9 is used. For U-SCORE we have used a learning rate of 1e\u2212 4 with a momentum of 0.9 and batch size same as classifier for all the experiments. For efficiency and better training, we update the U-SCORE for every K = 15 steps of the classifier. Also, we warm start the classifier by training it without the U-SCORE for first 25 epochs. A weight decay of 10\u22124 is used for both the networks.\nFor all experiments, training is done for 300 epochs. For the unsupervised domain adaptation from Kaggle data to APTOS data, we split the training into 230 epochs without unlabelled images and 70 epochs combining the unlabelled images along with the source data for a total of 300 epochs."
        },
        {
            "heading": "E DETAILED RESULTS ON SELECTIVE CLASSIFICATION",
            "text": ""
        },
        {
            "heading": "E.1 DIABETIC RETINOPATHY DATASET",
            "text": "We present our main results on a large real-world application of selective classification: Diabetic retinopathy detection (kag, 2015). Our evaluation considers both in-distribution data, as well as a separate test set from a different geographic region collected using different equipment\u2013 this is an evaluation of test-time generalization under domain shift, without any additional learning.\nFigure 3 shows a comparison of all methods on selective classification for the Kaggle DR dataset (first row) alongwith domain generalization results (Country shift evaluated using APTOS dataset, second row)). The columns present different metrics for each task: AUC (column 1), accuracy (column 2), and selective calibration error (column 3). We see that REVAR consistently outperforms the other methods on both tasks and all metrics. In particular, the robust gains on AUC (column 1, upto 1.5% absolute, see Table 12) for both in-distribution and domain shift tasks are compelling. Note that although the results are reflected in accuracy metrics as well (column 2, upto 2% absolute gains on the domain shift task, see Table 12), AUC is less susceptible to class imbalances and therefore a more reliable metric. Also, column 3 shows robust improvement in calibration on both in-domain and out-of-domain data (ECE metric, lower is better), suggesting that the U-SCORE indeed better represents classifier uncertainty, and thereby improves on selective classification. Finally, we note that the improvement in calibration, a widely used metric of classifiers\u2019 ability to capture and represent uncertainty, suggests that REVAR may have broad applications beyond selective classification (see e.g., (Tran et al., 2022)).\nA note of interest is that AUC for all methods reduces in the domain shift task as the selectivity is increased. This is the opposite of expected behavior, where accuracy and AUC should generally increase as the classifier becomes more selective. The data suggests a significant change in the two data distributions that appears to partially invert the ranking order\u2013i.e., all classifiers appear to be more accurate for instances they are less confident about. The robust gains of REVAR suggest that it is less susceptible to such drastic shifts in distribution.\nE.2 IMAGENET-100\nWe replicated our findings on other datasets commonly used for studying selective classification in the literature. This includes Imagenet-100 (Table 13). REVAR retains an edge over the other baselines in each of these datasets. In particular, the Imagenet-100 dataset is sufficiently complex, given the much larger larger number of classes (100) on a substantial training and evaluation set\n(a) DR dataset (in-distribution AUC)(b) DR dataset (in-distribution Accuracy) (c) DR dataset (in-distribution calibration)\n(d) DR dataset (country shift AUC) (e) DR dataset (country shift Accuracy) (f) DR dataset (country shift calibration)\nFigure 3: Selective classification results on diabetic retinopathy dataset. REVAR shows robust improvement in AUC in both in-domain and domain-shift scenarios (panels (a,b)). Accuracy measures also show similar trends, with large improvements in domain shift conditions (panels (c,d)). Finally, selective calibration error measures (calibration of selected data points, panels (e,f)) show that better calibration is a key underlying factor for REVAR\u2019s performance. See text for details.\nof higher-resolution images. REVAR\u2019s superior performance on this dataset shows its potential for scaling to harder selective classification problems.\nIn all datasets we see a pattern of increasing gap as the coverage is reduced, suggesting that REVAR is able to identify and retain the highest-confidence test instances better than the other methods.\nE.3 IMAGENET-1K\nFor a large scale demonstration of our approach, we now present the results on the ImagaNet-1K dataset for the selective classification setup. We utilize the complete 1.28M train images to update the classifier and use the 50k validation images to update the U-SCORE. Table 14 shows the results for this evaluation against the existing baselines. It contains analysis on five different coverages ranging from 0.4 to 1.0. It can be observed that our method is the best-performing at lower coverage levels (0.4,0.5) and also at moderately high coverage levels (0.8). Also, it is able to provide gains upto 1.5% in accuracy (0.5) and shows a significant gain of 0.88% over all the existing baselines at the coverage level of 0.4.\nTable 14: Comparison on the ImageNet-1k dataset\n0.4 0.5 0.6 0.8 1.0\nSAT 95.34 90.12 87.16 82.12 75.31 DG 95.27 90.53 87.27 82.06 75.44 SN 95.19 90.22 87.74 81.78 75.02\nOurs 96.22 91.67 87.64 83.38 75.21\nTable 15: U-SCORE architecture ablation.\n0.4 0.5 0.6 0.8 1.0\nRN-18 91.4 90.7 89.9 88.1 86.1 RN-32 91.3 90.5 90.1 88.2 85.9 RN-50 91.1 90.4 90.2 88.3 86.0"
        },
        {
            "heading": "E.4 FURTHER ANALYZING THE UNLABELLED TEST DOMAIN INSTANCES SCENARIO",
            "text": "We further test the importance of utilizing unlabelled examples from test domain, given the in-domain validation set setting, in our REVAR-PV variant, which has proven to be better at capturing uncertainty than REVARin the experiments provided in main paper. We further verify this by testing on the ImageNet-A,R,C datasets by using ImageNet as the training, in-domain val set. Each of them inherits a significant shift from ImageNet. The results are provided in table 16. it can be observed that again REVAR-PV comes out to be significantly better than REVARin terms of modelling the uncertainty for this complete generative setup."
        },
        {
            "heading": "F ARCHITECTURE, MODEL SIZES, COST",
            "text": ""
        },
        {
            "heading": "F.1 U-SCORE ARCHITECTURES",
            "text": "For all the experiments discussed till now, we have used a ResNet-18 (Pretrained) as the U-SCORE for all the experiments. We now perform an ablation on the choice of the U-SCORE architectures including ResNet-18, ResNet-32 and ResNet-50 for the DR dataset (OOD), in table 15. Given the limited data available to train the meta-network, big architecture might be sub-optimal, verified by the results in the table. However, for large-scale datasets like ImageNet-1K, increasing capacity can be more helpful at the cost of increased computation. All these experiments use a ResNet-50 as the classifier architecture. We also analyze a different architecture for the classifier (WRN-28-10) in the appendix.\nVision Transformers. Inspired by the recent success of Vision Transformer Models, we also analyze this architecture for the U-SCORES\u0307pecifically, we test a ViT-small based U-SCORE against the ResNet-101 based U-SCORE on the DR dataset under country shift setup, both having similar number of parameters (45M, 48M respectively). We also do a similar comparison on the ImageNet-1k dataset for further assurance. The classifier architecture is same as the U-SCORE architecture. Table 17 provides the analysis for this experiment."
        },
        {
            "heading": "F.2 CHANGING MODEL ARCHITECTURES",
            "text": "We examine the effect of backbone in evaluation of our proposed scheme. Specifically, we compare the top-2 performing baselines namely SelectiveNet (SN) and Self-Adaptive Training (SAT) with our method using a Wide-ResNet-28-10 backbone with around 1.5 times parameters compared to the ResNet-50 backbone used in the paper along with a different architecture. We do this for the Diabetic Retinopathy data as well as the Imagenet-100 data. Table 18 shows the analysis for diabetic retinopathy, both in-doamin and country shift. Again we see the trend of performance is similar as compared to Table 2 in the paper with accuracy improvements of aorund 0.3-0.5% for most cases and 0.2-0.3 % decrease for a few cases. However, the performance gap is similar to using the ResNet-50 baseline. Similarly, the trend for Imagenet-100 (Table 19) is approximately same as the paper with errors improved in the range 0.3-0.8 as compared to Table 3 in the paper. This change is visible for all the methods. This can lead to a conclusion that architecture might not be playing a major role in analyzing relative performance for selective classification. However, any concrete claims require a more rigorous testing with various state-of-the-art architectures proposed recently."
        },
        {
            "heading": "F.3 COMPUTATIONAL COMPLEXITY, CONVERGENCE, TRAINING COST:",
            "text": "Empirical cost. Per training epoch, we take around 1.2x the naive baseline\u2019s running time. The total number of epochs required are 1.2x - 1.5x of ERM classifier. This makes the training process on average 1.5 times more expensive.\nThese findings were consistent across a wide range of datasets and ranges of hyperparameters, supporting a modest, deterministic increase in running time. This increase is comparable to some selective classification baselines, e.g., 1.3x increase in epochs for SN and 1.2x for SAT. Note, in addition, that baselines such as SAT and DG only work for a pre-determined budget, and changing budget requires retraining from scratch. We only require a one-time training cost.\nConvergence. The Meta-Weight-Net paper (Appendix C) proves convergence of a general bi-level optimization formulation under specific assumptions \u2013 namely that the training and meta-loss are lipschitz smooth with bounded gradient. These conditions apply to our meta-loss as well, and the convergence guarantees also transfer."
        },
        {
            "heading": "F.4 CONTROLLING FOR U-SCORE PARAMETERS",
            "text": "To control for the extra parameters used by U-SCORE, we compare all baselines trained on ResNet101 (44M), with our method trained on ResNet-50 (23M) + ResNet-18 (11M) meta-network for our method. The AUARC (Area under accuracy rejection curve) metrics are provided below. With noticeably fewer parameters, we still outperform the baselines."
        },
        {
            "heading": "G ADDITIONAL BASELINES",
            "text": ""
        },
        {
            "heading": "G.1 COMPARISON WITH ENTROPY BASED REGULARIZATION",
            "text": "A recent work (Feng et al., 2022) proposed using the maximum logit directly as the uncertainty measure, of the methods trained with selective classification/learning to abstain objectives, instead of their predicted scores. So for a given method, e.g. SelectiveNet(Geifman & El-Yaniv, 2019), it just combines classifier trained with that method with Softmax Response (SR)(Geifman & El-Yaniv, 2017) at response time. It further proposes an entropy regularization loss in addition to cross entropy loss to penalize low max-logit scores. We now analyze the effect of this entropy regularization on the selective classification baselines and our method, comparing them for Kaggle Diabetic Retinopathy data (kag, 2015) used in the paper. For the baselines at the inference time, we follow the strategy proposed in this method, using SR, whereas for ours we go with the U-SCORE at the inference time. Table 22 shows the analysis for this experiment. It can be observed that our method (with the U-SCORE ) is still significantly more effective when trained with entropy regularization as compared to these baselines. Also, using variance reduction based prediction scores are a better criteria as compared to directly applying SR technique for these selective classifiers."
        },
        {
            "heading": "G.2 COMPARISON WITH RE-WEIGHTING METHODS",
            "text": "As explained in the paper, these methods are train-time-only reweightings, since they learn free parameters for each training instance Ren et al. (2018); Zhang & Pfister (2021), or as a function of instance loss (requiring true label) Shu et al. (2019). In contrast, we learn a neural network which can readily be applied on unseen instances As a compromise, we used Ren et al. (2018); Shu et al. (2019); Zhang & Pfister (2021) for training the classifier, and used MCD at test-time for selective classification; this tells us if the training procedure in these results in better classifiers. For ours,\nwe still use our meta-network to select the instances to classify. The Area under accuracy rejection curve (AUARC metric) is provided in table 23 (under No Var Min in Baselines). It can be observed that our method significantly outperform these methods. To further differentiate the contributions of our U-SCORE, and our meta-loss, we add our variance minimization loss to these re-weighting schemes and also report the results in table 23 (under Var Min). Still our method performs the best thereby proving that both our contributions, i.e., instance-conditioning and variance minimization hold significant importance in performance improvement."
        },
        {
            "heading": "G.3 COMPARISON WITH SIMPLE CALIBRATORS",
            "text": "We compared against ProbOut, Platt scaling, and also its single single parameter version (temperature scaling) which was shown to be better at calibration Guo et al. (2017). We report the mean and std (AUARC) of 5 different runs. Results on all the datasets are provided in table 24. Our method is able to provide significant gains (upto 2.3%) as compared to all of these methods."
        },
        {
            "heading": "G.4 CORRELATION BETWEEN INSTANCE WEIGHT AND PREDICTIVE ENTROPY",
            "text": "We calculated the correlation between weights and predictive entropy in table. Further, we also evaluated entropy itself as an uncertainty measure. The results are provided in table 25. The correlations are substantial, conforming to the claim that we capture model uncertainty. However,\nwe outperform entropy, suggesting that entropy is by itself not the gold standard for uncertainty measurement, and a 100% correlation with it is not desirable."
        },
        {
            "heading": "H CONTROLS FOR SELECTIVE CLASSIFICATION",
            "text": ""
        },
        {
            "heading": "H.1 SELECTIVE CLASSIFICATION ON HARD SAMPLES",
            "text": "A concern with selective classification might be that significant initial gains may be obtained by quickly rejecting only the (rare) hard sasmples, while ranking the remaining examples poorly. To control for this, we compared selective accuracy (Area under accuracy rejection curve) for Imagenettrained classifiers on the naturally occurring hard-example test set Imagenet-A. In this test set, all samples are in some sense hard samples, and there are no shortcuts to good selective classification accuracy. The results are provided in table 26 Even among hard samples, our method is able to better order instances according to uncertainty."
        },
        {
            "heading": "H.2 MATCHED TEST SETS FOR SELECTIVE CLASSIFICATION",
            "text": "Another challenge in selective classification is that each method can choose to reject different instances, and end up reporting accuracy on nonidentical sets of data. To control for this, we use the ImageNet-A dataset for testing so that the test set comprises mostly hard examples. We apply our selection method using the U-SCORE to select examples for each coverage and then test our classifier as well as the other baselines\u2019 classifier on the same set of chosen examples. The results are reported in table 27. The column PSS (previous Selection scheme) denotes the result of previous comparison whereas column OSS (our selection scheme) denotes the result when our selection scheme for each of the baseline training methods is used. The results show that our selection scheme is capable of identifying less erroneous examples quite better than other selection schemes, since our selection improves each method\u2019s accuracy. Further, our classifier is also more accurate on the selected set, suggesting two separate sets of benefits from our method. Our U-SCORE can identify erroneous examples (intrinsically hard examples) better than other methods \u2013 this is a measure of uncertainty. The modeled uncertainty is of course best for the classifier jointly trained with it but is partially applicable to other classifiers too.\nThe AUARC metric is as follows:"
        },
        {
            "heading": "I SHARED WEIGHTS",
            "text": "We now analyze the effect of sharing parameters between the meta-network and the classifier. Here the classifier encoder is used by the meta-network upto the fully connected layer and then a separate K-layered fully connected neural network is used as the U-SCORE. We call this version as Ours (shared). The results for selective classification task with different values of K are provided in in table 28 (comprising the AUARC metric).\nIt can be observed that this shared weights setup is somewhat sub-optimal in nature. Here the encoder part is only updated w.r.t. predictor loss, otherwise the optimization becomes unstable given the bi-level objective. Only the K layers are updated using the loss for the U-SCORE. A potential explanation is that U-SCORE uses a markedly different set of features from the image compared to the primary classifer\u2013the classifier needs to find discriminative features, whereas the U-SCORE needs to identify features, potentially across classes, that flag instance hardness. We also tried other configurations where we tried attaching the meta-network Fully-connected neural network to different intermediate representations of the classifier encoder architecture but no improvements were observed."
        },
        {
            "heading": "J FINE-TUNING ON VAL SET FOR OOD SETUPS",
            "text": "We now analyze another baseline against our method in the OOD setup. Here instead of our bi-level optimization setup, ERM trained baseline is directly tuned on the validation set. We have three kinds of OOD setups in the paper: one where the train, val and test are from different domains (iWildCam, Camelyon). Second where the train and val from same domain and test from different DR (OOD). Third, where we have some unlabelled examples available from the same domain as the OOD test set and train, val sets are in-domain. We now analyze the above discussed baseline on setups 1 and 3 where we can have OOD examples in the validation set. In none of the setups, we have labelled examples from the test domain. Thus, we can\u2019t naively tune ERM on the test domain. For the first setup, we directly fine-tune the ERM trained classifier on the validation set alongside unlabelled example from the test set using our unsupervised variance minimization objective. For the third setup, since the OOD examples are unlabelled, we instead tune the ERM trained classifier using our unsupervised variance minimization objective. The results are provided in table 29. For selective classification we use MCD on top this baseline. We term this baseline as MCD (val-tuned).\nIt can be observed that the MCD(val tuned) baseline performs worse than ours in all cases, further advocating the usefulness of our proposed scheme."
        },
        {
            "heading": "K ERROR BARS FOR TABLES",
            "text": "In the main paper, due to lack of space, some tables (4-8) donot contain the std values from the five runs of each exp. Only mean values are reported. We provide the std values alongside the mean values in the corresponding tables below."
        },
        {
            "heading": "L FURTHER ANALYSIS FOR SELECTIVE CLASSIFICATION WITH OTHER ARCHITECTURES",
            "text": "We further analyze our method with the ViT-Small, Wide ResNet-28-10 (WRN-28-10) architectures for the classifier by replicating Tab. 2, comprising the area under accuracy rejection curve (AUARC) metric, from the main paper using them. Tab. 34, Tab. 35 denote the tables corresponding to these architectures respectively. The U-SCORE is same as the one used for Tab. 2. It can be observed that, upon going to these bigger architectures, there is either a minor improvement in performance gains or they remain same as Tab. 2. This further verifies the effectiveness of our method and advocates for its usefulness for various tasks requiring different kinds of neural network architectures."
        }
    ],
    "year": 2023
}