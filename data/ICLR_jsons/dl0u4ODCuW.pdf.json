{
    "abstractText": "Retrosynthesis is the task of proposing a series of chemical reactions to create a desired molecule from simpler, buyable molecules. While previous works have proposed algorithms to find optimal solutions for a range of metrics (e.g. shortest, lowest-cost), these works generally overlook the fact that we have imperfect knowledge of the space of possible reactions, meaning plans created by the algorithm may not work in a laboratory. In this paper we propose a novel formulation of retrosynthesis in terms of stochastic processes to account for this uncertainty. We then propose a novel greedy algorithm called retro-fallback which maximizes the probability that at least one synthesis plan can be executed in the lab. Using in-silico benchmarks we demonstrate that retro-fallback generally produces better sets of synthesis plans than the popular MCTS and retro* algorithms.",
    "authors": [],
    "id": "SP:ff20a4344552b491d482eb28a12c55a684a629a3",
    "references": [
        {
            "authors": [
                "John Bradshaw",
                "Brooks Paige",
                "Matt J Kusner",
                "Marwin Segler",
                "Jos\u00e9 Miguel Hern\u00e1ndez-Lobato"
            ],
            "title": "A model to search for synthesizable molecules",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "John Bradshaw",
                "Brooks Paige",
                "Matt J Kusner",
                "Marwin Segler",
                "Jos\u00e9 Miguel Hern\u00e1ndez-Lobato"
            ],
            "title": "Barking up the right tree: an approach to search over molecule synthesis dags",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Nathan Brown",
                "Marco Fiscato",
                "Marwin HS Segler",
                "Alain C Vaucher"
            ],
            "title": "Guacamol: benchmarking models for de novo molecular design",
            "venue": "Journal of chemical information and modeling,",
            "year": 2019
        },
        {
            "authors": [
                "PP Chakrabarti"
            ],
            "title": "Algorithms for searching explicit and/or graphs and their applications to problem reduction search",
            "venue": "Artificial Intelligence,",
            "year": 1994
        },
        {
            "authors": [
                "Binghong Chen",
                "Chengtao Li",
                "Hanjun Dai",
                "Le Song"
            ],
            "title": "Retro*: learning retrosynthetic planning with neural guided a* search",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Shuan Chen",
                "Yousung Jung"
            ],
            "title": "Deep retrosynthetic reaction prediction using local reactivity and global attention",
            "venue": "JACS Au,",
            "year": 2021
        },
        {
            "authors": [
                "Connor W Coley",
                "Luke Rogers",
                "William H Green",
                "Klavs F Jensen"
            ],
            "title": "Scscore: synthetic complexity learned from a reaction corpus",
            "venue": "Journal of chemical information and modeling,",
            "year": 2018
        },
        {
            "authors": [
                "Connor W Coley",
                "William H Green",
                "Klavs F Jensen"
            ],
            "title": "Rdchiral: An rdkit wrapper for handling stereochemistry in retrosynthetic template extraction and application",
            "venue": "Journal of chemical information and modeling,",
            "year": 2019
        },
        {
            "authors": [
                "Connor W Coley",
                "Dale A Thomas III",
                "Justin AM Lummiss",
                "Jonathan N Jaworski",
                "Christopher P Breen",
                "Victor Schultz",
                "Travis Hart",
                "Joshua S Fishman",
                "Luke Rogers",
                "Hanyu Gao"
            ],
            "title": "A robotic platform for flow synthesis of organic compounds informed by ai planning",
            "venue": "eaax1566,",
            "year": 2019
        },
        {
            "authors": [
                "Elias James Corey",
                "W Todd Wipke"
            ],
            "title": "Computer-assisted design of complex organic syntheses: Pathways for molecular synthesis can be devised with a computer and equipment for graphical communication",
            "year": 1969
        },
        {
            "authors": [
                "Hanjun Dai",
                "Chengtao Li",
                "Connor Coley",
                "Bo Dai",
                "Le Song"
            ],
            "title": "Retrosynthesis prediction with conditional graph logic network",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Peter Ertl",
                "Ansgar Schuffenhauer"
            ],
            "title": "Estimation of synthetic accessibility score of drug-like molecules based on molecular complexity and fragment contributions",
            "venue": "Journal of cheminformatics,",
            "year": 2009
        },
        {
            "authors": [
                "Wenhao Gao",
                "Roc\u0131\u0301o Mercado",
                "Connor W Coley"
            ],
            "title": "Amortized tree generation for bottom-up synthesis planning and synthesizable molecular design",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Samuel Genheden",
                "Amol Thakkar",
                "Veronika Chadimov\u00e1",
                "Jean-Louis Reymond",
                "Ola Engkvist",
                "Esben Bjerrum"
            ],
            "title": "Aizynthfinder: a fast, robust and flexible open-source software for retrosynthetic planning",
            "venue": "Journal of cheminformatics,",
            "year": 2020
        },
        {
            "authors": [
                "Sai Krishna Gottipati",
                "Boris Sattarov",
                "Sufeng Niu",
                "Yashaswi Pathak",
                "Haoran Wei",
                "Shengchao Liu",
                "Simon Blackburn",
                "Karam Thomas",
                "Connor Coley",
                "Jian Tang"
            ],
            "title": "Learning to navigate the synthetically accessible chemical space using reinforcement learning",
            "venue": "In International conference on machine learning,",
            "year": 2020
        },
        {
            "authors": [
                "Aric Hagberg",
                "Pieter Swart",
                "Daniel S Chult"
            ],
            "title": "Exploring network structure, dynamics, and function using networkx",
            "venue": "Technical report,",
            "year": 2008
        },
        {
            "authors": [
                "Charles R Harris",
                "K Jarrod Millman",
                "St\u00e9fan J Van Der Walt",
                "Ralf Gommers",
                "Pauli Virtanen",
                "David Cournapeau",
                "Eric Wieser",
                "Julian Taylor",
                "Sebastian Berg",
                "Nathaniel J Smith"
            ],
            "title": "Array programming with numpy",
            "year": 2020
        },
        {
            "authors": [
                "Abraham Heifets",
                "Igor Jurisica"
            ],
            "title": "Construction of new medicines via game proof search",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2012
        },
        {
            "authors": [
                "Ross Irwin",
                "Spyridon Dimitriadis",
                "Jiazhen He",
                "Esben Jannik Bjerrum"
            ],
            "title": "Chemformer: a pretrained transformer for computational chemistry",
            "venue": "Machine Learning: Science and Technology,",
            "year": 2022
        },
        {
            "authors": [
                "Pablo Jim\u00e9nez",
                "Carme Torras"
            ],
            "title": "An efficient algorithm for searching implicit and/or graphs with cycles",
            "venue": "Artificial Intelligence,",
            "year": 2000
        },
        {
            "authors": [
                "Richard M. Karp"
            ],
            "title": "Reducibility among Combinatorial Problems, pp. 85\u2013103",
            "year": 1972
        },
        {
            "authors": [
                "Junsu Kim",
                "Sungsoo Ahn",
                "Hankook Lee",
                "Jinwoo Shin"
            ],
            "title": "Self-improved retrosynthetic planning",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Akihiro Kishimoto",
                "Beat Buesser",
                "Bei Chen",
                "Adi Botea"
            ],
            "title": "Depth-first proof-number search with heuristic edge cost and application to chemical synthesis planning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Baiqing Li",
                "Hongming Chen"
            ],
            "title": "Prediction of compound synthesis accessibility based on reaction knowledge",
            "venue": "graph. Molecules,",
            "year": 2022
        },
        {
            "authors": [
                "Cheng-Hao Liu",
                "Maksym Korablyov",
                "Stanis\u0142aw Jastrzebski",
                "Pawe\u0142 W\u0142odarczyk-Pruszyn\u015bki",
                "Yoshua Bengio",
                "Marwin Segler"
            ],
            "title": "Retrognn: fast estimation of synthesizability for virtual screening and de novo design by learning from slow retrosynthesis software",
            "venue": "Journal of Chemical Information and Modeling,",
            "year": 2022
        },
        {
            "authors": [
                "Songtao Liu",
                "Zhengkai Tu",
                "Minkai Xu",
                "Zuobai Zhang",
                "Lu Lin",
                "Rex Ying",
                "Jian Tang",
                "Peilin Zhao",
                "Dinghao Wu"
            ],
            "title": "FusionRetro: Molecule representation fusion via in-context learning for retrosynthetic planning",
            "venue": "Proceedings of the 40th International Conference on Machine Learning,",
            "year": 2041
        },
        {
            "authors": [
                "David JC MacKay"
            ],
            "title": "A practical bayesian framework for backpropagation networks",
            "venue": "Neural computation,",
            "year": 1992
        },
        {
            "authors": [
                "Krzysztof Maziarz",
                "Austin Tripp",
                "Guoqing Liu",
                "Megan Stanley",
                "Shufang Xie",
                "Piotr Gai\u0144ski",
                "Philipp Seidl",
                "Marwin Segler"
            ],
            "title": "Re-evaluating retrosynthesis algorithms with syntheseus",
            "venue": "arXiv preprint arXiv:2310.19796,",
            "year": 2023
        },
        {
            "authors": [
                "Christos H Papadimitriou",
                "Mihalis Yannakakis"
            ],
            "title": "Shortest paths without a map",
            "venue": "Theoretical Computer Science,",
            "year": 1991
        },
        {
            "authors": [
                "Adam Paszke",
                "Sam Gross",
                "Francisco Massa",
                "Adam Lerer",
                "James Bradbury",
                "Gregory Chanan",
                "Trevor Killeen",
                "Zeming Lin",
                "Natalia Gimelshein",
                "Luca Antiga"
            ],
            "title": "Pytorch: An imperative style, highperformance deep learning library",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Judea Pearl"
            ],
            "title": "Heuristics: intelligent search strategies for computer problem solving",
            "venue": "Addison-Wesley Longman Publishing Co., Inc.,",
            "year": 1984
        },
        {
            "authors": [
                "F. Pedregosa",
                "G. Varoquaux",
                "A. Gramfort",
                "V. Michel",
                "B. Thirion",
                "O. Grisel",
                "M. Blondel",
                "P. Prettenhofer",
                "R. Weiss",
                "V. Dubourg",
                "J. Vanderplas",
                "A. Passos",
                "D. Cournapeau",
                "M. Brucher",
                "M. Perrot",
                "E. Duchesnay"
            ],
            "title": "Scikit-learn: Machine learning in Python",
            "venue": "Journal of Machine Learning Research,",
            "year": 2011
        },
        {
            "authors": [
                "David Rogers",
                "Mathew Hahn"
            ],
            "title": "Extended-connectivity fingerprints",
            "venue": "Journal of chemical information and modeling,",
            "year": 2010
        },
        {
            "authors": [
                "Miko\u0142aj Sacha",
                "Miko\u0142aj B\u0142az",
                "Piotr Byrski",
                "Pawe\u0142 Dabrowski-Tumanski",
                "Miko\u0142aj Chrominski",
                "Rafa\u0142 Loska",
                "Pawe\u0142 W\u0142odarczyk-Pruszynski",
                "Stanis\u0142aw Jastrzebski"
            ],
            "title": "Molecule edit graph attention network: modeling chemical reactions as sequences of graph edits",
            "venue": "Journal of Chemical Information and Modeling,",
            "year": 2021
        },
        {
            "authors": [
                "Philippe Schwaller",
                "Riccardo Petraglia",
                "Valerio Zullo",
                "Vishnu H Nair",
                "Rico Andreas Haeuselmann",
                "Riccardo Pisoni",
                "Costas Bekas",
                "Anna Iuliano",
                "Teodoro Laino"
            ],
            "title": "Predicting retrosynthetic pathways using transformer-based models and a hyper-graph exploration strategy",
            "venue": "Chemical science,",
            "year": 2020
        },
        {
            "authors": [
                "Marwin HS Segler",
                "Mark P Waller"
            ],
            "title": "Neural-symbolic machine learning for retrosynthesis and reaction prediction",
            "venue": "Chemistry\u2013A European Journal,",
            "year": 2017
        },
        {
            "authors": [
                "Marwin HS Segler",
                "Mike Preuss",
                "Mark P Waller"
            ],
            "title": "Planning chemical syntheses with deep neural networks and symbolic",
            "venue": "ai. Nature,",
            "year": 2018
        },
        {
            "authors": [
                "Philipp Seidl",
                "Philipp Renz",
                "Natalia Dyubankova",
                "Paulo Neves",
                "Jonas Verhoeven",
                "Marwin Segler",
                "J\u00f6rg K Wegner",
                "Sepp Hochreiter",
                "G\u00fcnter Klambauer"
            ],
            "title": "Modern hopfield networks for few-and zero-shot reaction template prediction",
            "venue": "arXiv preprint arXiv:2104.03279,",
            "year": 2021
        },
        {
            "authors": [
                "Ryosuke Shibukawa",
                "Shoichi Ishida",
                "Kazuki Yoshizoe",
                "Kunihiro Wasa",
                "Kiyosei Takasu",
                "Yasushi Okuno",
                "Kei Terayama",
                "Koji Tsuda"
            ],
            "title": "Compret: a comprehensive recommendation framework for chemical synthesis planning with algorithmic enumeration",
            "venue": "Journal of cheminformatics,",
            "year": 2020
        },
        {
            "authors": [
                "Grzegorz Skoraczy\u0144ski",
                "Mateusz Kitlas",
                "B\u0142a\u017cej Miasojedow",
                "Anna Gambin"
            ],
            "title": "Critical assessment of synthetic accessibility scores in computer-assisted synthesis planning",
            "venue": "Journal of Cheminformatics,",
            "year": 2023
        },
        {
            "authors": [
                "Megan Stanley",
                "Marwin Segler"
            ],
            "title": "Fake it until you make it? generative de novo design and virtual screening of synthesizable molecules",
            "venue": "Current Opinion in Structural Biology,",
            "year": 2023
        },
        {
            "authors": [
                "Felix Strieth-Kalthoff",
                "Frederik Sandfort",
                "Marwin HS Segler",
                "Frank Glorius"
            ],
            "title": "Machine learning the ropes: principles, applications and directions in synthetic chemistry",
            "venue": "Chemical Society Reviews,",
            "year": 2020
        },
        {
            "authors": [
                "Amol Thakkar",
                "Veronika Chadimov\u00e1",
                "Esben Jannik Bjerrum",
                "Ola Engkvist",
                "Jean-Louis Reymond"
            ],
            "title": "Retrosynthetic accessibility score (rascore)\u2013rapid machine learned synthesizability classification from ai driven retrosynthetic planning",
            "venue": "Chemical Science,",
            "year": 2021
        },
        {
            "authors": [
                "Zhengkai Tu",
                "Thijs Stuyver",
                "Connor W Coley"
            ],
            "title": "Predictive chemistry: machine learning for reaction deployment, reaction development, and reaction discovery",
            "venue": "Chemical Science,",
            "year": 2023
        },
        {
            "authors": [
                "Pauli Virtanen",
                "Ralf Gommers",
                "Travis E Oliphant",
                "Matt Haberland",
                "Tyler Reddy",
                "David Cournapeau",
                "Evgeni Burovski",
                "Pearu Peterson",
                "Warren Weckesser",
                "Jonathan Bright"
            ],
            "title": "Scipy 1.0: fundamental algorithms for scientific computing in python",
            "venue": "Nature methods,",
            "year": 2020
        },
        {
            "authors": [
                "GE Vleduts"
            ],
            "title": "Concerning one system of classification and codification of organic reactions",
            "venue": "Information Storage and Retrieval,",
            "year": 1963
        },
        {
            "authors": [
                "Christopher KI Williams",
                "Carl Edward Rasmussen"
            ],
            "title": "Gaussian processes for machine learning",
            "year": 2006
        },
        {
            "authors": [
                "Shufang Xie",
                "Rui Yan",
                "Peng Han",
                "Yingce Xia",
                "Lijun Wu",
                "Chenjuan Guo",
                "Bin Yang",
                "Tao Qin"
            ],
            "title": "Retrograph: Retrosynthetic planning with graph search",
            "venue": "In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining,",
            "year": 2022
        },
        {
            "authors": [
                "Yemin Yu",
                "Ying Wei",
                "Kun Kuang",
                "Zhengxing Huang",
                "Huaxiu Yao",
                "Fei Wu"
            ],
            "title": "Grasp: Navigating retrosynthetic planning with goal-driven policy",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Weihe Zhong",
                "Ziduo Yang",
                "Calvin Yu-Chian Chen"
            ],
            "title": "Retrosynthesis prediction using an end-toend graph generative architecture for molecular graph editing",
            "venue": "Nature Communications,",
            "year": 2023
        },
        {
            "authors": [
                "Zipeng Zhong",
                "Jie Song",
                "Zunlei Feng",
                "Tiantao Liu",
                "Lingxiang Jia",
                "Shaolun Yao",
                "Min Wu",
                "Tingjun Hou",
                "Mingli Song"
            ],
            "title": "Root-aligned smiles: a tight representation for chemical reaction prediction",
            "venue": "Chemical Science,",
            "year": 2022
        },
        {
            "authors": [
                "Kishimoto"
            ],
            "title": "Description of algorithm Proposed",
            "year": 2019
        },
        {
            "authors": [
                "Chen"
            ],
            "title": "F.4 RESULTS ON RETRO* 190 HARD MOLECULES Figure F.9 shows the success probability over time for harder molecules",
            "year": 2024
        },
        {
            "authors": [
                "R G f"
            ],
            "title": "Average success probability over time for 190 \u201chard\u201d molecules from Chen et al. (2020) using the optimistic heuristic (top) and SAscore heuristic (bottom)",
            "year": 2020
        },
        {
            "authors": [
                "Liu"
            ],
            "title": "2023b) call this metric \u201cexact set-wise matching,\u201d but we will call it precursor matching because we think this name is less ambiguous. Because this metric depends on the purchasable molecules, we use a buyability model derived from the inventory of Liu et al. (2023b) instead of the model derived from eMolecules used for all other experiments. This is a deterministic model: molecules in the inventory",
            "year": 2023
        },
        {
            "authors": [
                "Chen"
            ],
            "title": "2023b) retrains the models using",
            "year": 2020
        },
        {
            "authors": [
                "2021 Kim et al",
                "2022 Yu et al",
                "Liu"
            ],
            "title": "2023a). A potential point of confusion is that some of these works describe their contribution as a \u201cretrosynthesis algorithm\u201d or \u201cretrosynthetic planning algorithm.\u201d Given that the end product of these papers is a value function that is plugged into a previously proposed algorithm (typically MCTS or retro*)",
            "year": 2023
        },
        {
            "authors": [
                "retrosynthesis. G"
            ],
            "title": "FUSIONRETRO One work which does not fit nicely into any of the previous subsections is FusionRetro (Liu et al., 2023b). On one level, the paper describes a reaction prediction model based on a transformer, which is essentially a single-step reaction prediction model (G.3)",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Retrosynthesis (planning the synthesis of organic molecules via a series of chemical reactions) is a common task in chemistry with a long history of automation (Vleduts, 1963; Corey & Wipke, 1969). Although the combinatorially large search space of chemical reactions makes naive brute-force methods ineffective, recently significant progress has been made by developing modern machinelearning based search algorithms for retrosynthesis (Strieth-Kalthoff et al., 2020; Tu et al., 2023; Stanley & Segler, 2023). However, there remain obstacles to translating the output of retrosynthesis algorithms into real-world syntheses. One significant issue is that these algorithms have imperfect knowledge of the space of chemical reactions. Because the underlying physics of chemical reactions cannot be efficiently simulated, retrosynthesis algorithms typically rely on data-driven reaction prediction models which can \u201challucinate\u201d unrealistic outputs (Zhong et al., 2023), akin to hallucinated outputs in other domains (OpenAI, 2023). This results in synthesis plans which cannot actually be executed.\nAlthough future advances in modelling may reduce the prevalence of infeasible reactions, we think it is unlikely that they will ever be eliminated entirely, as even the plans of expert chemists do not always work on the first try. One possible workaround to failing plans is to produce multiple synthesis plans instead of just a single one: the other plans can act as backup plans in case the primary plan fails. Although existing algorithms may find multiple synthesis plans, they are generally not designed to do so, and there is no reason to expect the plans found will be suitable as backup plans (e.g. they may share steps with the primary plan and thereby fail alongside it).\nIn this paper, we present several advancements towards retrosynthesis with backup plans. First, in section 3 we explain how uncertainty about whether a synthesis plan will work in the wet lab can be quantified with stochastic processes. We then propose an evaluation metric called successful synthesis probability (SSP) which quantifies the probability that at least one synthesis plan found by an algorithm will work. This naturally captures the idea of producing backup plans. Second, in section 4 we present a novel search algorithm called retro-fallback which greedily optimizes SSP, and explain qualitatively how it avoids potential failure modes of other algorithms. Finally, in section 6 we demonstrate quantitatively that retro-fallback outperforms existing algorithms on an in-silico benchmark. Together, we believe these contributions form a notable advancement towards translating results from retrosynthesis algorithms into the lab."
        },
        {
            "heading": "2 BACKGROUND: WHAT IS RETROSYNTHESIS?",
            "text": "LetM represent the space of molecules andR represent the space of reactions, where each reaction transforms a set of reactant molecules in 2M into a product molecule in M. Retrosynthesis is usually formalized as a search problem on a graph G, defined implicitly via a backward reaction model B : M 7\u2192 2R which defines all possible reactions for a given molecule. Because G is combinatorially large, most search algorithms only store a small explicit subgraph G\u2032 \u2286 G. We refer to nodes which may have children in G but have no children in G\u2032 as tip nodes.1 In general, search algorithms alternate between selecting tip nodes in G\u2032 and querying B to add new nodes to G\u2032 until the computational budget is exhausted (a process called expansion).\nThere are multiple ways to define the nodes and edges of G. We choose G to be an AND/OR graph: a directed graph containing nodes for molecules and reactions (Heifets & Jurisica, 2012). Edges exist only from reactions to their reactant molecules and from molecules to reactions that produce them, making G bipartite.2 An example of an AND/OR graph is shown in Figure B.1. Reactions can be naturally associated with \u201cAND nodes\u201d because all of their reactant molecules must be synthesized for the reaction to work, while molecules are associated with \u201cOR nodes\u201d because any reaction can be used to synthesize it. However, to avoid confusion we will simply refer to the nodes of G as molecules and reactions. Further details on AND/OR graphs are given in Appendix B.\nGiven a target molecule mt \u2208 M, the primary goal of retrosynthesis algorithms is to find synthesis plans: subtrees T \u2286 G rooted at mt containing at most one reaction to produce each molecule. For these plans to be executable, all tip nodes of T must be contained in an inventory I \u2286M of buyable molecules. Among all synthesis plans, those with minimum cost or highest quality are preferred, commonly formalized with a scalar cost/value function (Segler et al., 2018; Chen et al., 2020)."
        },
        {
            "heading": "3 A FORMULATION FOR RETROSYNTHESIS WITH UNCERTAINTY",
            "text": "In this section we present our first main contribution: a formalism to represent uncertainty about whether a synthesis plan will work. We use this to construct a novel evaluation metric called successful synthesis probability and explain how it can be computed."
        },
        {
            "heading": "3.1 BINARIZED VIEW: \u201cFEASIBLE\u201d REACTIONS AND \u201cBUYABLE\u201d MOLECULES",
            "text": "To account for synthesis plans not working in the lab, we must first define what it means for a synthesis plan to \u201cwork\u201d. As mentioned in the introduction, the most obvious failure mode is that one of the reactions in the plan cannot be performed. This could happen for a variety of reasons: it may not produce the desired product, produce dangerous by-products, have a low yield, or require specialized expertise or equipment. Rather than trying to explicitly model these factors, we propose to collapse all nuance into a binary outcome: a reaction is either feasible or infeasible. Not only is this easier to model, we note that ultimately if a chemist performs a reaction they will either move to the next step in the synthesis plan or admit defeat and abandon the synthesis plan (a binary outcome). Therefore, for a given chemist and lab, we postulate the existence of a binary \u201cfeasibility\u201d function f\u2217 : R 7\u2192 {0, 1} which we will use to create and evaluate synthesis plans. A second reason why a synthesis plan may not work is the inability to buy one of the starting molecules. This is usually not a significant issue since vendors update their inventories in real time and offer fast delivery, especially for common chemicals. However, this is not always the case: for example, some companies offer large \u201cvirtual libraries\u201d with billions of molecules which they believe they can synthesize upon request, but not with 100% reliability. To account for this, we therefore define a binary \u201cbuyability\u201d function b\u2217 :M 7\u2192 {0, 1} analogously to f\u2217."
        },
        {
            "heading": "3.2 REPRESENTING UNCERTAINTY WITH STOCHASTIC PROCESSES",
            "text": "If we knew f\u2217 and b\u2217 then retrosynthesis would simply be a search problem (albeit a large one). However, in practice they are unknown. A natural response is therefore to model our epistemic\n1In contrast with leaf nodes which have no children in G, e.g. molecules where no reactions are possible. 2Note that in retrosynthesis, the edges go in the opposite direction of the chemical reactions.\nuncertainty about f\u2217 and b\u2217. One approach is to model point-wise uncertainties, using some mechanism to predict P (f\u2217(r) = 1) and P (b\u2217(m) = 1) for all r and m. Unfortunately, this approach is fundamentally incapable of capturing beliefs about correlations between different outcomes. Instead, we propose to model uncertainty about f\u2217 and b\u2217 directly in function space using stochastic processes (essentially distributions over functions). We define a feasibility model \u03bef to be a binary stochastic process over R, and define a buyability model \u03beb to be a binary stochastic process over M. This model class is very general: the fully-deterministic formulation from section 2 is a special case where \u03bef and \u03beb are degenerate distributions3, while independent outcomes are a special case where f /b are independent Bernoulli random variables at all points. Other more interesting stochastic processes which induce correlations could be constructed by putting a prior over the parameters of a model (e.g. Bayesian neural networks (MacKay, 1992)) or using non-parametric models like Gaussian processes (Williams & Rasmussen, 2006).\nImportantly, we do not claim to know what the best model for \u03bef /\u03beb is (or even what a good model is). Considering that predicting reaction outcomes is an active research area, and that modelling \u03bef implies not only predicting reaction outcomes but also their correlations, it is unrealistic to expect a perfect model here. Because of this, we will generally discuss \u03bef /\u03beb in a model-agnostic way and consider a range of different feasibility models in our experiments."
        },
        {
            "heading": "3.3 NEW EVALUATION METRIC: SUCCESSFUL SYNTHESIS PROBABILITY",
            "text": "Given f and b, a successful synthesis plan T \u2286 G must have f(r) = 1 for all r \u2208 T and b(m) = 1 for every tip molecule in T . However, if we are uncertain about f and b then the distinction between \u201csuccessful\u201d and \u201cunsuccessful\u201d synthesis plans is not binary: every T could have some probability of succeeding. What then should be the goal of retrosynthesis?\nThere is no objectively correct answer to this question. Although one could try to find the synthesis plan with the highest probability of succeeding, we instead propose the goal of maximizing the probability that any synthesis plan T \u2286 G\u2032 is valid. Assuming that a chemist would be willing to try all the synthesis plans in G\u2032, this goal not only captures the spirit of finding good synthesis plans, but backup plans too. Specifically, let s(m;G\u2032, f, b) represent the successful synthesis of a molecule m: 1 if m can be bought or synthesized using only feasible reactions in G\u2032, otherwise 0. We write s(m) when G\u2032, f, b are clear from context. If s(mt;G\u2032, f, b) = 1, this implies there is a successful synthesis plan for the target molecule. We then define the successful synthesis probability (SSP) as\ns\u0304(m;G\u2032, \u03bef , \u03beb) = Pf\u223c\u03bef ,b\u223c\u03beb [s(m;G\u2032, f, b) = 1] (1)\nand propose using SSP to evaluate the success of retrosynthesis algorithms. Unfortunately, although this metric is sensible, it is not easy to compute exactly.\nTheorem 3.1. Unless P = NP , there does not exist an algorithm to compute s\u0304(mt;G\u2032, \u03bef , \u03beb) for arbitrary \u03bef , \u03beb whose time complexity grows polynomially with the number of nodes in G\u2032.\nThe proof of this result is given in Appendix D.1. Essentially we show that every instance of the 3- SAT problem (which is known to be NP-hard) is equivalent to computing whether s\u0304(mt;G\u2032, \u03bef , \u03beb) > 0 for some choice of G\u2032, \u03bef , \u03beb, thereby showing that computing SSP must be at least as hard as 3- SAT. Although this result may appear to show that SSP is not a practical evaluation metric, it does not preclude the existence of an efficient randomized algorithm to estimate SSP. This is exactly what we propose. First, note that given f/b, if we define a similar concept of success for reactions then s can be defined recursively in terms of its children in G\u2032 (provided by the function ChG\u2032 ):\ns(m;G\u2032, f, b) = max [ b(m), max\nr\u2208ChG\u2032 (m) s(r;G\u2032, f, b)\n] (2)\ns(r;G\u2032, f, b) = f(r) \u220f\nm\u2208ChG\u2032 (r)\ns(m;G\u2032, f, b) . (3)\nThis suggests that dynamic programming can be used to compute s(mt;G\u2032, f, b) in polynomial time (we prove this in Appendix D.2). Second, observe that if f \u223c \u03bef , b \u223c \u03beb then s(mt;G\u2032, f, b) is a\n3Specifically, \u03bef contains only f(r) = 1\u2203m:r\u2208B(m) and \u03beb contains only b(m) = 1m\u2208I .\nBernoulli random variable with mean s\u0304(mt;G\u2032, \u03bef , \u03beb). This suggests a natural estimator:\ns\u0302(mt;G\u2032, \u03bef , \u03beb, k) = 1\nk k\u2211 i=1 s(mt;G\u2032, fk, bk) f1, . . . , fk \u223c \u03bef , b1, . . . , bk \u223c \u03beb . (4)\nProvided samples can be drawn in polynomial time, equation 4 can also be computed in polynomial time. Of course, the output is only an approximation of the true SSP and there will be some error, but this error is well-characterized and can be made small with a modest number of samples: we discuss this further and provide an error bound in Appendix D.3."
        },
        {
            "heading": "4 RETRO-FALLBACK: A GREEDY ALGORITHM TO MAXIMIZE SSP",
            "text": "If one believes that SSP is a sensible evaluation metric, then it makes sense to configure retrosynthesis algorithms to maximize it. Of course, one can run any algorithm independently of \u03bef , \u03beb and compute SSP post hoc. However, \u201csucceeding by coincidence\u201d is not very robust: it is more desirable to use \u03bef , \u03beb during the search to try to maximize SSP. In this section, we first examine why existing algorithms may nonetheless struggle to maximize SSP, then present our second main contribution: an algorithm explicitly designed to maximize SSP which we call retro-fallback.\n4.1 EXISTING ALGORITHMS CAN FAIL TO MAXIMIZE SSP, EVEN WITH ACCESS TO \u03bef , \u03beb\nHow could existing retrosynthesis algorithms be configured to maximize SSP? A natural inclination is to set SSP as the \u201cobjective\u201d of the algorithm. Unfortunately, existing algorithms do not have an \u201cobjective\u201d which can be set arbitrarily. For example, retro* has an independent cost for each reaction and molecule (Chen et al., 2020), MCTS uses a reward function for individual plans (Segler et al., 2018), while algorithms like breadth-first search or proof-number search (Kishimoto et al., 2019) have no customizable rewards or costs of any kind. Because SSP depends on an entire graph and distributions over reaction feasibilities and molecule buyabilities (which may involve correlations), we claim it is not generally possible to directly set SSP as the objective of previouslyproposed retrosynthesis algorithms (we argue this in detail in Appendix C). Therefore, at best we can hope to use \u03bef , \u03beb to optimize some proxy for SSP.\nFor most algorithms, we believe the closest proxy for maximizing SSP is to optimize for individually successful synthesis plans, or plans containing individually feasible reactions and buyable molecules (we justify this for each algorithm in Appendix C). Intuitively these objectives may seem very similar, but it is not difficult to imagine cases where they differ. Figure 1 illustrates such a case, wherein a synthesis plan with reactions r1, r2 has been found and the algorithm must choose between expanding m3 or m4. Individually either molecule could be promising, but any new synthesis route proceeding via r3 will also use r1 and is therefore prone to fail alongside the existing plan if r1 turns out to be infeasible. Even though m4 may not be the best choice in general, an algorithm maximizing SSP would clearly need to account for the interaction between existing and prospective synthesis plans in its decision making, which simply is not possible by reasoning about individual synthesis plans in isolation. This provides compelling motivation to develop algorithms which account for interactions between synthesis plans."
        },
        {
            "heading": "4.2 INGREDIENTS FOR AN INFORMED, GREEDY SEARCH ALGORITHM",
            "text": "A natural starting point for an algorithm specifically designed to maximize SSP is to estimate how different actions might affect SSP, and choose actions accordingly. Theorem 3.1 suggests that computing this exactly will scale poorly to larger search graphs, and therefore we assert that the basis\nof any efficient algorithm must be samples from \u03bef , \u03beb. Furthermore, equations 2\u20133 show how the success of any given node in G\u2032 can be computed reasonably efficiently in terms of the success values of other nodes. This broadly suggests it might be possible to modify equations 2\u20133 to perform counterfactual reasoning: i.e. predicting what s(mt;G\u2032, f, b) could be if G\u2032 were modified. Since the \u201cactions\u201d available to a search algorithm are expanding one or more tip nodes in G\u2032, the obvious counterfactual to consider is \u201cwhat would happen if a node was expanded?\u201d For a tip molecule m, we have that s(m;G, f, b) = b(m) from equation 2. If b(m) = 1 then expanding m cannot change s(m) (it is already at its maximum value), but if b(m) = 0 then it is possible that s(m) will change to 1 upon expansion. This is a natural entry point for a search heuristic: let h :M 7\u2192 [0, 1] be a heuristic function mapping molecules4 to probabilities that s(m) will become 1 upon expansion (assuming b(m) = 0).\nThis suggests a simple greedy algorithm: at each time step, choose a tip node which is estimated to maximally improve s(mt) upon expansion. However, even without fully developing such an algorithm we can confidently say that it would not be very effective in practice. The key reason is that in equation 3, the success of a reaction requires the success of all its reactants. If a reaction has more than one reactant which is not already successful, then no single action can change that outcome, causing a greedy algorithm to never expand such molecule. This would limit the simple greedy algorithm to finding only \u201clinear\u201d plans (where at most one reactant requires synthesis), instead of more challenging \u201cconvergent\u201d plans with multiple branches.\nTo address this, we take inspiration from the greedy retro* algorithm (Chen et al., 2020) and instead consider the counterfactual of simultaneously expanding all tip nodes on an entire synthesis plan T \u2286 G\u2032. We assume that expanding a synthesis plan T amounts to setting s(m) = 1 for all nonbuyable tip nodes m \u2208 T with s(m) = 0 independently with probability h(m). We then define \u03c1(m;G\u2032, f, b, h) to be the largest expected value of s(mt;T, f, b) under this expansion scenario across all synthesis plans T \u2286 G\u2032 rooted atmt and which containm (we write \u03c1(m) when G\u2032, f, b, h are clear from context). \u03c1 essentially measures the chance that expanding m could yield a new successful synthesis plan. Although computing \u03c1may seem to require considering a combinatorially large number of synthesis plans T (an approach which would scale poorly to large G\u2032), because the expansion outcomes were assumed to be independent all optimal plans will contain sub-plans which are also optimal. This allows \u03c1 to be efficiently computed using dynamic programming.\nAs an intermediate quantity to efficiently compute \u03c1, we define \u03c8(m;G\u2032, f, b, h) to be the largest expected value of s(m;T, f, b) obtained across all synthesis plans T \u2286 G\u2032 rooted at m under the expansion scenario above (we write \u03c8(m) when G\u2032, f, b, h are clear from context). Although \u03c8 and \u03c1 have similar definitions they have an important difference: \u03c1(m) is a hypothetical success probability of a synthesis plan involving both m and mt, while \u03c8(m) is a hypothetical success probability of a synthesis plan involving just m. This makes \u03c8 effectively a less constrained version of \u03c1. Note that definition implies that \u03c8(mt) = \u03c1(mt). Because of independence, optimal plans T which produce the value of \u03c8(m) will consist of sub-plans which are also individually optimal. This allows us to define \u03c8 implicitly with the recurrence relation:\n\u03c8(m;G\u2032, f, b, h) = { max [b(m), h(m)] (Tip molecule) max [ b(m),maxr\u2208ChG\u2032 (m) \u03c8(r;G \u2032, f, b, h) ] (Non-tip molecule) (5)\n\u03c8(r;G\u2032, f, b, h) = f(r) \u220f\nm\u2208ChG\u2032 (r)\n\u03c8(m;G\u2032, f, b, h) . (6)\nThe assumption of independent expansion outcomes also implies that an optimal plan T which achieves \u03c1(n) for a node n \u2208 G\u2032 can be obtained by modifying the optimal plan for a parent of n to include the plan T \u2032 that optimally produces n (and attains \u03c8(n)). This yields the following recursive definition of \u03c1 in terms of its parents (given by PaG\u2032 ):\n\u03c1(m;G\u2032, f, b, h) = { \u03c8(m;G\u2032, f, b, h) m is root node maxr\u2208PaG\u2032 (m) \u03c1(r;G \u2032, f, b, h) all other m (7)\n\u03c1(r;G\u2032, f, b, h) = { 0 \u03c8(r;G\u2032, f, b, h) = 0 \u03c1(PaG\u2032(r))\n\u03c8(r) \u03c8(PaG\u2032 (r))\n\u03c8(r;G\u2032, f, b, h) > 0 (8)\n4In principle we could also let h depend on f and b, but we choose not to (this will allow us to reuse the heuristic across all samples from f and b later). Further discussion in Appendix E.3.\nAlgorithm 1 Retro-fallback algorithm Require: target molecule mt, max iterations L, backward reaction model B, search heuristic h Require: samples f1, . . . , fk \u223c \u03bef , b1, . . . , bk \u223c \u03beb\n1: G\u2032 \u2190 mt 2: for i in 1, . . . , L do 3: for j in 1, . . . , k do 4: Compute s(\u00b7;G\u2032, fj , bj) for all nodes using equations 2\u20133 5: Compute \u03c8(\u00b7;G\u2032, fj , bj , h) for all nodes using equations 5\u20136 6: Compute \u03c1(\u00b7;G\u2032, fj , bj , h) for all nodes using equations 7\u20138 7: end for 8: E \u2190 all tip nodes in G\u2032 9: Terminate early if |E| = 0 OR s(mt;G\u2032, fj , bj) = 1\u2200j\n10: me \u2190 argmaxm\u2208E \u03b1(m;G\u2032, \u03bef , \u03beb, h) (cf. equation 9, breaking ties arbitrarily) 11: Add all reactions and molecules from B(me) to G\u2032 12: end for 13: return G\u2032\nMore details on these equations are given in Appendix E.1. Taken together, these equations suggest a two-stage procedure to compute \u03c1 for all tip nodes: first pass information \u201cup\u201d the graph using equations 5\u20136 to compute \u03c8 for all nodes, then pass information \u201cdown\u201d the graph using equations 7\u20138 to compute \u03c1 for all nodes. The only remaining question is how efficient this procedure can be. If G\u2032 is acyclic then \u03c8 and \u03c1 can be calculated in linear time by iterating equations 5\u20136 from tip nodes to the root, then iterating equations 7\u20138 from root to tip nodes. If there are cycles then the cost could potentially be larger, but in Appendix D.2 we prove it is at most quadratic. In any case, it is clear that \u03c8 and \u03c1 can form the basis for an efficient search algorithm."
        },
        {
            "heading": "4.3 RETRO-FALLBACK: A FULL GREEDY ALGORITHM",
            "text": "Creating a full greedy algorithm requires aggregating information across many samples from \u03bef and \u03beb to decide which tip node to expand at each step. Recalling our motivation of producing synthesis plans with backup plans, we propose to greedily expand molecules which are predicted to form successful synthesis plans in scenarios where all existing synthesis plans currently fail. Specifically, we propose to choose molecules by maximizing the objective\n\u03b1(m;G\u2032, \u03bef , \u03beb, h) = Ef\u223c\u03bef ,b\u223c\u03beb [ 1s(mt;G\u2032,f,b)=0 [\u03c1(m;G \u2032, f, b, h)] ] , (9)\nwhich is proportional to the expected value of \u03c1(m) conditioned on s(mt) = 0. We call the resulting algorithm retro-fallback (from \u201cretrosynthesis with fallback plans\u201d) and state it explicitly in Algorithm 1. The sections are colour-coded for clarity. After initializing G\u2032 to just the target molecule, the algorithm performs L iterations of expansion (although this termination condition could be changed as needed). In each iteration, first the values of s, \u03c8, and \u03c1 are computed for each sample.5 Next, the algorithm checks whether there are no nodes to expand or whether the root molecule is synthesized for every sample, and if so terminates (both of these conditions mean no further improvement is possible). Finally, a tip node maximizing equation 9 is chosen and used to expand G\u2032. A practical implementation of retro-fallback may look slightly different than Algorithm 1. In particular because a full sample from \u03bef or \u03beb contains outcomes for every possible molecule or reaction, in practice one would only store outcomes for nodes in G\u2032 and sample from the posterior stochastic processes when new nodes are added. Vectorized computation could also be used instead of explicit for loops. We refer the reader to Appendix E for further discussion about the design and implementation of retro-fallback."
        },
        {
            "heading": "5 RELATED WORK",
            "text": "Mechanistically, retro-fallback most closely resembles retro* (Chen et al., 2020): both perform a bottom-up and top-down update to determine the value of each potential action and select actions\n5This order is chosen because s depends only on f & b, \u03c8 depends on s, and \u03c1 depends on \u03c8. Because the optimal algorithm to compute s, \u03c8, \u03c1 may depend on G\u2032, we only specify this computation generically.\ngreedily. In fact, in the special case where costs are defined to be negative log probabilities then the updates for \u03c8 and \u03c1 are essentially equivalent to the \u201creaction number\u201d and \u201cretro* value\u201d updates from (Chen et al., 2020). However, in general the update equations are not equivalent. Additionally, retro-fallback performs its updates for many samples from \u03bef and \u03beb and combines information from all samples to make a decision, while retro* uses only a single cost. This is what allows retrofallback to directly optimize SSP, while retro* cannot. As explained in Appendix C, this ability also distinguishes retro-fallback from other search algorithms such as MCTS (Segler et al., 2018) and proof-number search (Kishimoto et al., 2019).\nPrior works have also considered planning in stochastic graphs, albeit in other contexts. For example, the \u201cCanadian Traveller Problem\u201d and its variants Papadimitriou & Yannakakis (1991) study search on a graph where edges can be randomly deleted. However, this is an online problem, meaning that the planning algorithm learns about edge deletions during the planning process. In contrast, our algorithm assumes offline planning because chemists desire complete synthesis plans before performing any lab experiments.\nLastly, we briefly comment on several research topics which are only tangentially related (deferring fuller coverage to Appendix G). Works proposing search heuristics for retrosynthesis search algorithms (G.1) complement rather than compete with our work: such heuristics could also be applied to retro-fallback. Generative models to produce synthesis plans (G.2) effectively also function as heuristics. Finally, methods to predict individual chemical reactions are sometimes also referred to as \u201cretrosynthesis models\u201d (G.3). Retro-fallback solves a different problem: it is a multi-step search algorithm which would use a reaction prediction model to define the search graph."
        },
        {
            "heading": "6 EXPERIMENTS",
            "text": "In this section we evaluate retro-fallback experimentally. The key question we seek to answer is whether retro-fallback does indeed maximize SSP more effectively than existing algorithms. We also investigate retro-fallback\u2019s runtime and the impact of the number of samples from \u03bef , \u03beb. In the main text we present highlights and key results; see Appendix F for complete details."
        },
        {
            "heading": "6.1 EXPERIMENT SETUP",
            "text": "We have based our experiment design on the USPTO benchmark from Chen et al. (2020), which has been widely used to evaluate multi-step retrosynthesis algorithms. However, because this benchmark does not include a feasibility or buyability model we have made some adaptations to make it suitable for our problem setting. Importantly, because we do not know what the \u201cbest\u201d feasibility model is, we instead test several feasibility models in the hope that the conclusions of our experiments could potentially generalize to future, more advanced feasibility models. We summarize the setup below and refer the reader to Appendix F.1 for further details.\nWe base all of our feasibility models on the pre-trained template classifier from Chen et al. (2020) restricted to the top-50 templates. We vary our feasibility model across two axes: the marginal feasibility assigned to each reaction and the correlation between feasibility outcomes. Marginally, we consider a constant value (C) and a value which decreases with the rank (R) for marginal feasibility. For correlations we consider all outcomes being independent (I) or determined by a latent GP model (G) which positively correlates similar reactions. Details of these models are given in Appendix F.1.1. Analogous to Chen et al. (2020), we create a buyability model based on eMolecules library, although we use the September 2023 version and exclude molecules with excessively long shipping times (details in Appendix F.1.2).\nWe compare retro-fallback to breadth-first search (an uninformed search algorithm) and heuristicguided algorithms retro* (Chen et al., 2020) and MCTS (Segler et al., 2018; Genheden et al., 2020; Coley et al., 2019b). MCTS and retro* were re-implemented and adapted to maximize SSP, which most notably entailed replacing costs or rewards from the backward reaction model B with quantities derived from \u03bef and \u03beb (see Appendices C and F.1.4 for details) and standardizing their search heuristics. All algorithms are run with a budget of 200 calls to B. The presence of heuristics makes comparing algorithms difficult because the choice of heuristic will strongly influence an algorithm\u2019s behaviour. We decided to use an optimistic heuristic and a heuristic based on the synthetic acces-\nsibility (SA) score (Ertl & Schuffenhauer, 2009), which has been shown to be a good heuristic for retrosynthesis in practice despite its simplicity (Skoraczyn\u0301ski et al., 2023).\nWe test all algorithms on 500 molecules randomly selected from the GuacaMol test set (Brown et al., 2019), which contains drug-like molecules known to be synthesizable, but with a wide range of difficulties (details in Appendix F.1.3). Our primary evaluation metric is the SSP values estimated with k = 10 000 samples, averaged over the 500 molecules."
        },
        {
            "heading": "6.2 HOW EFFECTIVE IS RETRO-FALLBACK?",
            "text": "Since retro-fallback is designed to maximize SSP, the most basic question is whether it does so more effectively than other algorithms. We found that a minority of molecules are \u201ctrivial\u201d, and all algorithms achieve a SSP of \u2248 1 within a few iterations. In Figure 2 we plot the average SSP for all \u201cnon-trivial\u201d test molecules as a function of number of reaction model calls using an optimistic heuristic for all feasibility models (the \u201ctrivial\u201d molecules are plotted in Figure F.4). Retro-fallback clearly outperforms the other algorithms in all scenarios by a wider margin than the error bars. The difference is larger for the feasibility models with independent reactions than with GP-correlated reactions. We suspect this is because there are many synthesis plans with similar reactions: when reaction outcomes are uncorrelated these synthesis plans act as backup plans for each other, but not when they are correlated. The same trends can be seen when using the SA heuristic function (shown in Figure F.5) and on a different test set (Appendix F.4). Overall, this result shows us what we expect: that retro-fallback maximizes the metric it was specifically designed to maximize more effectively than baseline algorithms.\nWe investigate the origin of these performance differences in Appendix F.3. Figure F.8 plots SSP over time for a small selection of molecules (repeated over several trials). It appears that, rather than retro-fallback being consistently a little bit better, the performance gap is driven by a larger difference for a small number of molecules. This is actually not surprising: the advantage of different approaches will vary depending on the graph, and for some graphs finding individual feasible plans is probably a promising strategy.\nA natural follow-up question is whether retro-fallback also performs well by metrics other than SSP. In Figures F.6\u2013F.7 we plot SSP of the single best synthesis plan plus two metrics frequently used by previous papers: the fraction of molecules with any synthesis plan (called \u201cfraction solved\u201d in prior works) and the length of the shortest synthesis plan found (a proxy for quality). The SSP of the single best plan is generally similar for all algorithms, especially when the SAscore heuristic is used. This suggests that in general all algorithms find similar \u201cbest\u201d plans, and retro-fallback\u2019s extra success comes from finding more effective \u201cbackup\u201d plans. Retro-fallback seems slightly better than other algorithms in terms of fraction solved and similar to other algorithms in terms of shortest plan length (although retro* is better in some cases). This suggests that it functions as an effective search algorithm even by the metrics from past papers."
        },
        {
            "heading": "6.3 RUNTIME AND VARIABILITY OF RETRO-FALLBACK",
            "text": "Lastly, we analyze retro-fallback\u2019s empirical behaviour in Figure 3. The three leftmost subplots show how runtime scales with graph size for the experiments from section 6.2. Retro-fallback\u2019s scaling ranges from \u2248 O(N) to \u2248 O(N1.6) depending on the complexity of sampling from \u03bef /\u03beb. In particular, the computational complexity is higher for \u03bef with GP-correlated outcomes, which require forming a covariance matrix in O(N2) time and inverting it in O(N3) time (although the empirical scaling suggests this step does not dominate the cost). Other algorithms draw samples for only small subsets of molecules and reactions and thereby exhibit linear scaling.\nTo study the effect of the number of samples k from \u03bef and \u03beb, we run the algorithm 10 times on a sub-sample of 25 molecules with a variety of different sample sizes. The rightmost subplot shows that as k decreases, the mean SSP value achieved by retro-fallback decreases and the variance of SSP increases. This is not surprising, since when the number of samples is small the internal estimates of SSP used by retro-fallback deviate more from their expected values, enabling sub-optimal decisions. Empirically, k \u2265 1000 seems sufficient (minimal further improvement is seen for higher k)."
        },
        {
            "heading": "7 DISCUSSION, LIMITATIONS, AND FUTURE WORK",
            "text": "In this paper we presented a novel evaluation metric for retrosynthesis algorithms called \u201csuccessful synthesis probability\u201d (SSP), proposed a novel algorithm called retro-fallback to greedily maximize SSP, and showed experimentally that retro-fallback is more effective than previously-proposed algorithms. Together, these contributions compensate for the limited ability of existing algorithms to explicitly account for reaction failure and propose backup plans.\nOne challenge for deploying retro-fallback in practice is the lack of established feasibility and buyability models. To our knowledge, retro-fallback is the first algorithm which can fully utilize uncertainty on the function level, so it is not surprising that not much work has been done in this area before. We therefore do not view this as a limitation of our work, but rather as motivation for subsequent research into quantifying the uncertainty of reaction models (especially by domain experts).\nOur experiments showed that retro-fallback is slower than other algorithms, both in absolute runtime and in scaling behaviour. We believe this can be improved using more efficient algorithms to sample from \u03bef , \u03beb and update \u03c8, \u03c1. However, we note that if retro-fallback is used with a more complex reaction model (e.g. a transformer) then the computational cost of the search algorithm will be less significant. We therefore do not expect this to limit the long-term potential of the algorithm.\nOur contributions also have some important conceptual limitations. First, chemists care about the cost and length of synthesis plans in addition to whether they will work, and we do not see a way to incorporate these directly into either our stochastic process formalism or retro-fallback. However, one of the main justifications for preferring short plans is that there are fewer steps that can go wrong, and therefore we expect retro-fallback to have a strong bias towards short plans regardless. Second, while our definition of SSP considers an arbitrary number of plans, in practice chemists are unlikely to try more than \u2248 10 plans before moving on to something else. Finally, since retro-fallback uses a search heuristic there is potential to learn this heuristic using the results of past searches (\u201cself-play\u201d). We naturally expect this to improve performance and view this as an exciting direction for future work."
        },
        {
            "heading": "ETHICS",
            "text": "Our work is foundational algorithm development and we do not see any direct ethical implications. The most likely use case for our algorithm is to automate the production of synthesis plans in drug discovery, which we hope can ease the development of new medicines. We acknowledge the possibility that such algorithms could be used by bad actors to develop harmful chemicals, but do not see this as a probable outcome: countless harmful chemicals already exist and can be readily obtained. It is therefore hard to imagine why bad actors would expend significant effort to develop new harmful chemicals with complicated synthesis processes."
        },
        {
            "heading": "REPRODUCIBILITY",
            "text": "We aim for a high standard of reproducibility in this work. We explicitly state our proposed algorithm in the paper (Algorithm 1) and dedicate Appendix E to discussing its minor (but still important) details, including guidance for future implementations (E.4). Proofs of all theorems are given in Appendix D. The experimental setup is described in more detail in Appendix F (including hyperparameters, etc). Code to reproduce all experiments6 is included in the supplementary information. Our code was thoroughly tested with unit tests and builds on libraries which are widely-used, minimizing the chance that our results are corrupted by software errors. We include the results generated by our code in json format, and also include code to read the results and reproduce the plots7 from the paper. The inclusion of raw data will freely allow future researchers to perform alternative analyses."
        },
        {
            "heading": "Appendix",
            "text": ""
        },
        {
            "heading": "Table of Contents",
            "text": ""
        },
        {
            "heading": "A Summary of Notation 15",
            "text": ""
        },
        {
            "heading": "B Details of AND/OR graphs 17",
            "text": ""
        },
        {
            "heading": "C Configuring existing algorithms to optimize SSP 19",
            "text": "C.1 Breadth-first search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 C.2 Monte-Carlo Tree Search (MCTS) . . . . . . . . . . . . . . . . . . . . . . . . 19 C.3 Depth-first proof number search (with heuristic edge initialization) . . . . . . . . 19 C.4 Retro* . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 C.5 RetroGraph . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20"
        },
        {
            "heading": "D Proofs and Theoretical Results 21",
            "text": "D.1 Proof of Theorem 3.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 D.2 Computing \u201csuccess\u201d quantities in polynomial time . . . . . . . . . . . . . . . . 21 D.3 Errors for estimating Bernoulli random variables . . . . . . . . . . . . . . . . . 23"
        },
        {
            "heading": "E Further details of retro-fallback 24",
            "text": "E.1 Elaboration on definition of \u03c8 and \u03c1 . . . . . . . . . . . . . . . . . . . . . . . . 24 E.2 Justification of node selection . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 E.3 Rejected alternative algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . 25 E.4 Practical implementation details . . . . . . . . . . . . . . . . . . . . . . . . . . 25"
        },
        {
            "heading": "F Extended experiment section 27",
            "text": "F.1 Details of experimental setup . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 F.2 Additional plots for section 6.2 . . . . . . . . . . . . . . . . . . . . . . . . . . 32 F.3 Case studies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36 F.4 Results on retro* 190 hard molecules . . . . . . . . . . . . . . . . . . . . . . . 37 F.5 Results on FusionRetro benchmark . . . . . . . . . . . . . . . . . . . . . . . . 38"
        },
        {
            "heading": "G Extended related work 39",
            "text": "G.1 Search heuristics for retrosynthesis . . . . . . . . . . . . . . . . . . . . . . . . 39 G.2 Generative models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39 G.3 Single-step retrosynthesis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39 G.4 FusionRetro . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39\nH Future work 41"
        },
        {
            "heading": "A SUMMARY OF NOTATION",
            "text": "Although we endeavoured to introduce all notation in the main text of the paper in the section where it is first used, we re-state the notation here for clarity."
        },
        {
            "heading": "General Math",
            "text": "2S Power set of set S (set of all subsets of S)\n1event Indicator function: 1 if \u201cevent\u201d is True, otherwise 0\nO(Np) Big-O notation (describing scaling of an algorithm)\nO\u0303(Np) Big-O notation, omitting poly-logarithmic factors (e.g. O(N logN) is equivalent to O\u0303(N))\nMolecules and reactions\nm a molecule\nr a reaction (assumed to be single-product)\nM space of molecules R space of reactions I Inventory of buyable molecules B backward reaction model\n\u2192 Forward reaction arrow (e.g. A+B \u2192 C) \u21d2 Backward reaction arrow (e.g. C \u21d2 A+B)"
        },
        {
            "heading": "Search Graphs",
            "text": "G implicit search graph with molecule (OR) nodes inM and reaction (AND) nodes inR\nG\u2032 explicit graph stored and expanded for search. G\u2032 \u2286 G PaG\u2032(x) The parents of molecule or reaction x in G\u2032. ChG\u2032(x) The children of molecule or reaction x in G\u2032. T A synthesis plan in G or G\u2032 (conditions in Appendix B)."
        },
        {
            "heading": "Feasibility and Buyability",
            "text": "f Feasible function (assigns whether a reaction is feasible)\nb Buyable function (assigns whether a molecule is buyable)\n\u03bef feasibility stochastic process (distribution over f )\n\u03beb buyability stochastic process (distribution over b)\nf\u0304\u03be(r) Marginal feasibility of reaction r under \u03bef b\u0304\u03be(m) Marginal buyability of molecule m under \u03beb s(m;G\u2032, f, b) Whether a molecule is synthesizable using reac-\ntions/starting molecules in G\u2032, with feasible/buyable outcomes given by f, b. Takes values in {0, 1}. Defined in equations 2\u20133\ns(m) Shorthand for s(m;G\u2032, f, b) when G\u2032, f, b are clear from context.\ns\u0304(m;G\u2032, \u03bef , \u03beb) Expected value of s(m;G\u2032, f, b) when f \u223c \u03bef , b \u223c \u03beb. Defined in equation 1.\ns\u0304(m) Shorthand for s\u0304(m;G\u2032, \u03bef , \u03beb) when G\u2032, \u03bef , \u03beb are clear from context\ns\u0302(m;G\u2032, \u03bef , \u03beb) Estimate of s\u0304(m) from samples (equation 4)"
        },
        {
            "heading": "Retro-fallback",
            "text": "mt the target molecule (at the root of G\u2032) h Search heuristic functionM 7\u2192 [0, 1] \u03c8(m;G\u2032, f, b, h) Estimate of potential s(m) if one plan underm is expanded.\nDefined in equations 5\u20136\n\u03c1(m;G\u2032, f, b, h) Estimate of potential for s(mt) if one plan under m is expanded. Defined in equations 7\u20138\nWe also use the following mathematical conventions throughout the paper:\n\u2022 log 0 = \u2212\u221e \u2022 maxx\u2208\u2205 f(x) = \u2212\u221e (the maximum of an empty set is always \u2212\u221e)"
        },
        {
            "heading": "B DETAILS OF AND/OR GRAPHS",
            "text": "We make the following additional assumptions about AND/OR graphs:\n1. Edges only exist between molecules and reactions producing them, and between reactions and their reactant molecules. This makes the graph bipartite.\n2. All reactions have at least one reactant (you cannot make something from nothing). This assumption is used implicitly below.\n3. Although we explicitly store only a subgraph G\u2032 \u2286 G, we assume that G\u2032 is connected and that every node is reachable from the target molecule mt. Only molecule nodes are tip nodes: if a reaction r \u2208 G\u2032, then G\u2032 must also contain all its reactant molecules.\n4. All reaction nodes must be connected to their product and reaction molecule nodes. 5. Reactions only exactly one product molecule. This assumption is used for example in\nequation 8 which uses the single parent of a reaction. Although this may seem restrictive, the reaction A+B \u2192 C +D could simply be encoded as two reactions: A+B \u2192 C and A+B \u2192 D .\nWe do not assume in general that the molecules and reactions in G\u2032 are unique: for example, to make a tree previous works have repeated nodes (Kishimoto et al., 2019; Chen et al., 2020).\nA synthesis plan is a subgraph T \u2286 G\u2032 which conforms with all the aforementioned assumptions of G\u2032 stated above (connected, all tip nodes are molecules, etc) with the extra restrictions:\n1. mt \u2208 T (the target molecule must be part of the synthesis plan) 2. Each molecule node in T has at most one reaction child in T . 3. T contains no cycles.\nWe refer the reader to Jime\u0301nez & Torras (2000) for a more thorough discussion of these conditions.\nAn example of an AND/OR graph is given in Figure B.1."
        },
        {
            "heading": "C CONFIGURING EXISTING ALGORITHMS TO OPTIMIZE SSP",
            "text": "In this section we examine all existing retrosynthesis algorithms which we are aware of and explain why we think they cannot be configured to directly maximize SSP, and what the closest proxy likely is. We also try to give a series of \u201chacks\u201d that we thought of to modify the algorithms, and explained their disadvantages."
        },
        {
            "heading": "C.1 BREADTH-FIRST SEARCH",
            "text": "Description of algorithm A very basic search algorithm: expand tip nodes in the order they were added.\nDirectly maximizing SSP This algorithm has no parameters and therefore has no kind of objective which could be set to maximize SSP.\nClosest proxy to SSP N/A, nothing can be done.\n\u201cHacks\u201d that could be done? Nothing we can think of."
        },
        {
            "heading": "C.2 MONTE-CARLO TREE SEARCH (MCTS)",
            "text": "Description of algorithm Used in (Segler et al., 2018; Coley et al., 2019b; Genheden et al., 2020). Create a MDP where \u201cstates\u201d are set of molecules and \u201cactions\u201d are reactions which react one molecule in a state and replace it with other molecules. At each step, descend the tree choosing nodes which maximize\nW (st, a) N(st, a) + cP (st, a)\n\u221a N(st\u22121, at\u22121)\n1 +N(st, a) , (10)\nwhere W (st, a) is the total reward accumulated while taking action a to reach state st, N(st, a) is the number of times where the algorithm has performed action a to reach state st and P (st, a) is some sort of prior probability of performing action a to reach st, and c is a constant. The algorithm is designed to eventually converge to the action sequence which maximizes reward.\nDirectly maximizing SSP The algorithm uses the \u201cprior\u201d function P (also called a policy), a reward function R, and a value function estimator V to estimate the value of leaf nodes (e.g. rollouts). However, all of these functions depend on individual states/actions instead of the graph as a whole!\nClosest proxy to SSP The reward functionR can be set to the SSP of an individual plan (estimated with samples).\n\u201cHacks\u201d that could be done? One option is to make the reward and policy change over time. For example, one could have the reward be the additional SSP gained from discovering a new plan. However, it is possible that MCTS will not behave well in this scenario: the principle of MCTS is to narrow down on the best sequence of actions by slowly tightening a confidence interval around their expected return. However, if the rewards change over time then these interval estimates will likely become inaccurate. Although the intervals could be widened (e.g. by increasing c) this will result in MCTS behaving more randomly and not searching as efficiently. Also, we note that there are many possible design choices here, and further probing of alternative options might yield improved results."
        },
        {
            "heading": "C.3 DEPTH-FIRST PROOF NUMBER SEARCH (WITH HEURISTIC EDGE INITIALIZATION)",
            "text": "Description of algorithm Proposed in Kishimoto et al. (2019). Basic proof number search assigns \u201cproof numbers\u201d and \u201cdisproof numbers\u201d which represent the number of nodes needed to prove or disprove whether there exist a synthesis plan to synthesize a given molecule and selects nodes using this information. The heuristic edge initialization Kishimoto et al. (2019) uses a heuristic to initialize the proof and disproof numbers for each reaction.\nDirectly maximizing SSP This algorithm inherently takes a very binary view of retrosynthesis (seeing nodes as either proven or disproven), and therefore is not very amenable to maximizing SSP. At best one could change the heuristic values to reflect feasibilities, but since the heuristic values are only used for tip nodes this would likely not be effective.\nClosest proxy to SSP We believe there is not much that can be done, and for this reason we do not perform experiments with this algorithm.\n\u201cHacks\u201d that could be done? Aside from changing the definitions of proof/disproof numbers, we do not see any options here."
        },
        {
            "heading": "C.4 RETRO*",
            "text": "Description of algorithm An algorithm for minimum cost search in AND/OR trees (Chen et al., 2020). At each step, the algorithm selects a potential synthesis plan T whose cost is \u2211 r\u2208T\ncR(r) +\u2211 m\u2208T cM (m) (i.e. a sum of individual costs for each reaction and molecule), then expands one tip node on T . A heuristic is used to guide plan selection by replacing cM on tip molecule nodes.\nDirectly maximizing SSP The parameters of retro* are the costs of each molecule and reaction and the heuristic functions. None of these can simply be set as the SSP because the SSP cannot be decomposed into a separate factor for each node.\nClosest proxy to SSP A cost for each molecule and reaction could be determined with \u03bef and \u03beb. One option is to set the cost of each reaction/molecule to the negative log of its marginal feasibility/buyability:\ncM (m) = \u2212 logEb\u223c\u03beb [b(m)] cR(r) = \u2212 logEf\u223c\u03bef [f(r)] .\nThis option does have a somewhat principled interpretation: if all feasibility/buyability outcomes are independent then the cost of a plan is just the negative log of its joint SSP. Of course, this relationship does not hold in the general non-independent case. However, we do not see a way to adjust this formula to account for non-independence in general, so we choose to apply it in all cases.\n\u201cHacks\u201d that could be done? One could potentially change the reaction and molecule costs with time to account for changes elsewhere in the search graph. For example, reactions/molecules which are already part of a feasible plan could be re-assigned higher costs to make the algorithm search for non-overlapping plans. However, this strategy seems unlikely to work in general: for some search graphs it is possible that the best backup plans will share some reactions (either those likely to be feasible). We were unable to come up with a strategy that did not have obvious and foreseeable failure modes, so we decided not to pursue this direction."
        },
        {
            "heading": "C.5 RETROGRAPH",
            "text": "Description of algorithm Proposed in Xie et al. (2022), this algorithm functions like a modified version of retro* but on a minimal AND/OR graph instead of a tree. A graph neural network is used to prioritize nodes.\nDirectly maximizing SSP Like retro*, this method uses additive costs across reactions, and we do not believe these costs can be modified to represent SSP.\nClosest proxy to SSP Similar to retro*, we believe the closest proxy is to set costs equal to the negative log marginal feasibility/buyability and minimize this.\n\u201cHacks\u201d that could be done? We could not think of anything."
        },
        {
            "heading": "D PROOFS AND THEORETICAL RESULTS",
            "text": "This appendix contains proofs of theoretical results from the paper."
        },
        {
            "heading": "D.1 PROOF OF THEOREM 3.1",
            "text": "Theorem 3.1 is a corollary of the following theorem, which we prove below.\nTheorem D.1. Unless P = NP , there does not exist a polynomial time algorithm to determine whether s\u0304(mt;G\u2032, \u03bef , \u03beb) > 0 whose time complexity grows polynomially with the number of nodes in G\u2032 for arbitrary \u03bef , \u03beb.\nNote that Theorem D.1 is distinct from Theorem 3.1: the latter is a hardness result about computing SSP, while the former considers only the binary problem of determining whether SSP is zero or non-zero. We now state a proof of Theorem D.1:\nProof. We will show a reduction from the Boolean 3-Satisfiability Problem (3-SAT) to the problem of determining whether SSP is non-zero. As 3-SAT is known to be NP-hard (Karp, 1972), this will imply the latter is also NP-hard, completing the proof.\nTo construct the reduction, assume an instance I of 3-SAT with n variables x1, ..., xn, andm clauses c1, ..., cm, each cj consisting of three literals (where a literal is either a variable or its negation) We will construct an AND-OR graph G(I) with size O(n+m), alongside with distributions \u03bef (I) and \u03beb(I), such that the SSP in the constructed instance is non-zero if and only if I is satisfiable.\nIn our construction we first set \u03bef \u2261 1, i.e. assume all reactions described below are always feasible. We then construct a set P of 2n potentially buyable molecules, corresponding to variables xi as well as their negations \u00acxi; to simplify notation, we will refer to these molecules as xi or \u00acxi. We then set \u03beb(I) to a uniform distribution over all subsets S \u2286 P such that \u2200i|S \u2229 {xi,\u00acxi}| = 1; in other words, either xi or \u00acxi can be bought, but never both at the same time. Note that with this construction it is easy to support all necessary operations on \u03beb, such as (conditional) sampling or computing marginals.\nIt remains to translate I to G(I) in a way that encodes the clauses cj . We start by creating a root ORnode r, with a single AND-node child r\u2032. Under r\u2032 we build m OR-node children, corresponding to clauses cj ; again, we refer to these nodes as cj for simplicity. Finally, for each cj , we attach 3 children, corresponding to the literals in cj . Intuitively these 3 children would map to three molecules from the potentially buyable set P , but formally the children of cj should be AND-nodes (while P contains molecules, i.e. OR-nodes); however, this can be resolved by adding dummy single-reactant reaction nodes.\nTo see that the reduction is valid, first note that r is synthesizable only if all cj are, which reflects the fact that I is a binary AND of clauses cj . Moreover, each cj is synthesizable if at least one of its 3 children is, which translates to at least one of the literals being satisfied. Our construction of \u03beb allows any setting of variables xi as long as it\u2019s consistent with negations \u00acxi. Taken together, this means the SSP for G(I) is non-zero if and only if there exists an assignment of variables xi that satisfies I , and thus the reduction is sound.\nCorollary D.2. If a polynomial time algorithm did exist to compute the exact value of s\u0304(mt;G\u2032, \u03bef , \u03beb), this algorithm would clearly also determine whether s\u0304(mt;G\u2032, \u03bef , \u03beb) > 0 in polynomial time, violating Theorem D.1. This proves Theorem 3.1."
        },
        {
            "heading": "D.2 COMPUTING \u201cSUCCESS\u201d QUANTITIES IN POLYNOMIAL TIME",
            "text": "Retro-fallback, and more generally the calculation of SSP rely on solving a system of recursivelydefined equations: equations 2\u20133 for s, equations 5\u20136 for \u03c8, and equations 7\u20138 for \u03c1. Exactly how these equations are solved is detached from the actual algorithms: all that matters is that they are solved. Depending on the structure of G\u2032, different algorithms with different scaling properties may be applicable. Currently we are uncertain about what the overall \u201cbest\u201d algorithm is, and therefore do not advocate for a particular method in this paper. In this section we merely aim to prove a\nminimal result: that these quantities can always be computed in polynomial time (with respect to the size of the graphs).\nFirst, we state a general theorem applicable to all graphs. Theorem D.3. Let |G\u2032| = N (i.e. G\u2032 has N nodes) and that the number of outgoing edges from any node is at most K < N . There exists an algorithm with O\u0303(N2) time complexity to compute s, \u03c8, and \u03c1.\nProof. Our proof builds on a result from Chakrabarti (1994) which gives an algorithm to compute minimum costs in an algorithm called AO*, which performs minimum-cost search on AND/OR graphs. Let ct(n) denote the terminal cost of a node (analogous to its purchase cost). This will generally be \u221e for nodes which are non-terminal (e.g. non-purchasable molecules), and a nonnegative real number otherwise. Let c\u2217 denote the optimal cost of a node, and ce denote the edge cost between two nodes. AO* defines the following cost function:\nc\u2217(n) = min [ ct(n), min\nn\u2032\u2208ChG\u2032 (n) [c\u2217(n\u2032) + ce(n, n \u2032)]\n] (OR node) (11)\nc\u2217(n) = \u2211\nn\u2032\u2208ChG\u2032 (n)\n[c\u2217(n\u2032) + ce(n, n \u2032)] (AND node) (12)\nChakrabarti (1994) presents an algorithm called Iterative revisewhose worst case time complexity is O\u0303(N2). Critically, unlike previous algorithms for AO*, the algorithm from Chakrabarti (1994) does not assume a tree or acyclic graph, making it very general. Our proof strategy is to transform the equations for s, \u03c8, and \u03c1 to resemble equations 11\u201312, making Iterative revise applicable and proving our result.\nFirst, define s\u2032(\u00b7;G\u2032, f, b) = \u2212 log s(\u00b7;G\u2032, f, b). The resulting recursive equations are:\ns\u2032(m;G\u2032, f, b) = min \u2212 log b(m)\ufe38 \ufe37\ufe37 \ufe38 ct(m) , min r\u2208ChG\u2032 (m) s\u2032(r;G\u2032, f, b)  s\u2032(r;G\u2032, f, b) = \u2212 log f(r)\ufe38 \ufe37\ufe37 \ufe38\nce(r,m)\n+ \u2211\nm\u2208ChG\u2032 (r)\ns\u2032(m;G\u2032, f, b)\nThese correspond to equations 11\u201312 using ce(m, r) = 0 and the substitutions for ct and ce(r,m) shown above.8 The same transformation and correspondence can be achieved for \u03c8 by defining \u03c8\u2032(\u00b7;G\u2032, f, b, h) = \u2212 log\u03c8(\u00b7;G\u2032, f, b, h) and defining ct(m) = \u2212 logmax [b(m), h(m)].\nSecond, define \u03c1\u2032(\u00b7;G\u2032, f, b, h) = log \u03c8(mt;G \u2032,f,b,h)\n\u03c1(\u00b7;G\u2032,f,b,h) . This results in the recursive equations:\n\u03c1\u2032(m;G\u2032, f, b, h) = { 0 m = mt minr\u2208PaG\u2032 (m) \u03c1 \u2032(r;G\u2032, f, b, h) all other m\n\u03c1(r;G\u2032, f, b, h) = { \u221e \u03c8(r;G\u2032, f, b, h) = 0 \u03c1\u2032(PaG\u2032(r)) + log \u03c8(PaG\u2032 (r)) \u03c8(r) \u03c8(r;G \u2032, f, b, h) > 0\nIf the directions of all edges are flipped (so parents become children and children become parents), then these equations correspond to equations 11\u201312 with ct(n) = 0 for mt only, ce(m, r) = 0, and ce(r,m) = log \u03c8(m) \u03c8(r) (which is non-negative because of the max in equation 5).\nThis completes the proof for all three quantities.\nTheorem D.3 assumes that the number of outgoing edges in each node is bounded. This is a realistic assumption in retrosynthesis: most reactions involve 1\u20132 reactants. Reactions with 3 or more reactants are less common, and more than \u2248 10 is essentially unheard of. Although there may be a large number of possible reactions that can be done on a given molecule, a backward reaction model\n8Note that using the convention that log 0 = \u2212\u221e the costs are guaranteed to be non-negative.\nB usually limits the number of reactions which are added to the graph. Many previous works have used a limit of 50 (Segler et al., 2018; Chen et al., 2020). Therefore we think this assumption is realistic in practice.\nOne implication of Theorem D.3 is that Iterative revise could be used to directly compute s, \u03c8, and \u03c1. However, in some cases this is likely sub-optimal: for example, if G\u2032 is acyclic then these quantities can be computed in linear time using a single pass over all nodes. Although the presence of reversible reaction (e.g. A \u2192 B and B \u2192 A) make it unlikely that strictly acyclic graphs will be encountered in practice during retrosynthesis, cyclic plans will not yield optimal plans we expect very few cycles to be explored in G\u2032. Therefore we propose in practice to initialize s, \u03c8, \u03c1 to 0 and then iterate their recursive relations until convergence. At this time we do not have any proofs for the time complexity of this procedure, but in practice it appears to be sub-linear (e.g. see Figure 3). However, in Corollary D.4 we show that computing \u03c1, which is the last phase of the algorithm, can indeed be done in linearithmic time. As this optimization is not applicable to computing \u03c8, it does not improve the overall complexity of the algorithm. Corollary D.4. Let |G\u2032| = N (i.e. G\u2032 has N nodes) and that the number of outgoing edges from any node is at most K < N . There exists an algorithm with O(N logN) time complexity to compute \u03c1 from \u03c8.\nProof. Recall the reduction of computing \u03c1 to minimum-cost search in an AND/OR graph from the proof of Theorem D.3. Note that the AND nodes in the resulting graph always have at most one child (corresponding to the node parent in the original tree), thus the sum-over-children component seen for AND nodes in general AND/OR graph search does not appear. Consequently, it is easy to see this particular search problem is equivalent to finding a shortest path from mt to every other node, which can be done using Dijkstra\u2019s algorithm in O(N logN) time."
        },
        {
            "heading": "D.3 ERRORS FOR ESTIMATING BERNOULLI RANDOM VARIABLES",
            "text": "Errors of i.i.d random variables are well-studied. The rate at which the sample mean \u201cconcentrates\u201d around its expected value can be bounded using any number of concentration bounds. For example, applying the Chernoff bound using k = 10 000 yields:\nP (|\u0302s(m;G\u2032, \u03bef , \u03beb, k)\u2212 s\u0304(m;G\u2032, \u03bef , \u03beb)| > 0.025) < 10\u22125 \u2200G\u2032, \u03bef , \u03beb This means that with 10 000 samples, the SSP can be placed within a 5% interval with near-certainty in all settings. We believe that a higher level of accuracy is not likely to be useful for chemists: if s\u0304(m) is reasonably large than a 5% error is relatively small, while if s\u0304(m) is near zero then a chemist will probably just choose not try to make the molecule (and therefore the distinction between 0.1% and 5% is not actually that important)."
        },
        {
            "heading": "E FURTHER DETAILS OF RETRO-FALLBACK",
            "text": "E.1 ELABORATION ON DEFINITION OF \u03c8 AND \u03c1\nAlthough the definition of \u03c8 is fairly self-explanatory, there is one caveat: if a given molecule occurs multiple times within a synthesis plan then the value of h(m) will implicitly be used multiple times. For example, if a synthesis plan T contained reactions A+B \u21d2 C, A+D \u21d2 E, and C+E \u21d2 mt then \u03c8(mt;T ) would represent the SSP of a hypothetical synthesis plan where the success outcome of A was sampled independently for the two reactions it is involved in. Obviously this does not make sense. However, if \u03c8 is modified to not double count such cases, then its value would no longer simply depend on the \u03c8 values of neighbouring nodes, eliminating the recursive definition which enabled efficient polynomial time computation! In practice we expect synthesis plans which use the same molecule multiple times to be uncommon, so we choose to just proceed with this quirk of the definition of \u03c8. We also note that retro*\u2019s definition of \u201creaction number\u201d has the same property (Chen et al., 2020), so this assumption has precedent in the literature.\nWe believe the easiest way to understand the equations for \u03c1 is to consider \u201cdescending\u201d the tree from the root node mt. Because \u03c8 is defined as the maximum success of any synthesis root upon expansion, and \u03c1(m) is essentially \u03c8(mt) when this root is restricted to contain both m and mt, the root node is a special case where m = mt and therefore \u03c1(mt) = \u03c8(mt). If the root node is a tip node, then there are no children and we are done.\nNext, consider a reaction child r of mt. Here it is important to recall that we only consider graphs with single-product reactions (c.f. B), meaning that r will only have a single parent. There are two cases to consider:\n1. \u03c8(r) = \u03c8(mt). This implies that r lies on an optimal expansion plan (note: not the optimal expansion plan because there could be multiple equally-promising expansion plans). Therefore we should have \u03c1(r) = \u03c1(mt).\n2. \u03c8(r) < \u03c8(mt). In this case \u03c1(r) can be achieved by \u201csubstituting\u201d the plan which caused \u03c8(mt) with the optimal plan containing r. In this case, because we are substituting the entire synthesis plan we have \u03c1(r) = \u03c8(r)\nEquation 8 yields this answer in every case.\nNow consider a child m of r (a grandchild of the root node). If this child has only one parent reaction, clearly \u03c1(m) = \u03c1(r) (since any synthesis plan containing r would need to contain m). However, if m has multiple parents, it can effectively \u201cchoose\u201d which plan to be a part of, and the definition of \u03c1 implies considering only the maximum plan. Hence we would choose the parent on the best plan, which justifies the max in equation 7.\nFinally, the most complex case: a reaction child r\u2032 ofm (a great-grandchild of the root node). Again there are two cases to consider:\n1. \u03c8(r\u2032) = \u03c8(m). The same logic applies as for the root node: r\u2032 is part of an optimal plan for m, so \u03c1(r\u2032) = \u03c1(m).\n2. \u03c8(r\u2032) < \u03c8(m). This is the more complex case. Clearly \u03c1(r\u2032) would be achieved by \u201csubstituting\u201d the sub-plan under m which allows it to achieve \u03c8(m) with an optimal subtree containing r\u2032, but unlike for direct children of the plan (e.g. r) we cannot simply set \u03c1(r\u2032) = \u03c8(r\u2032) because \u03c8(r\u2032) only accounts for a sub-plan under r\u2032, and not r or other childrenm\u2032 of r (recall that reactions can have multiple reactants and we descended the tree only under a single reactantm, ignoring its siblings). Thankfully, the additional parts of the plan are already accounted for in \u03c1(m). Because \u03c1(m) will be a product of probabilities of many sub-plans, and we know that the sub-plan we wish to remove has probability \u03c8(m), we can calculate \u03c1(r\u2032) by starting with \u03c1(m), dividing by \u03c8(m), then multiplying by \u03c8(r\u2032).\nIn both cases this is exactly what equation 8 yields.\nThe argument can be continued, but as this covers all of the cases we will conclude the explanation here."
        },
        {
            "heading": "E.2 JUSTIFICATION OF NODE SELECTION",
            "text": "In this section we offer some further thoughts/justification of equation 9. When designing the algorithm, our first intuition was to instead select nodes by maximizing \u03b1\u2032:\n\u03b1\u2032(m;G\u2032, \u03bef , \u03beb, h) = Ef\u223c\u03bef ,b\u223c\u03beb [\u03c1(m;G\u2032, f, b, h)] . This is effectively choosing nodes which lie on synthesis plans that are predicted to be successful. However, we anticipated the following failure mode: in cases where s(mt;G\u2032, f, b, h) = 1, in general \u03c1(m;G\u2032, f, b, h) < \u03c1(mt;G\u2032, f, b, h) for tip nodes m. This is because the optimal synthesis plan for mt will contain nodes already shown to be a solution. In this case, \u03c1(m;G\u2032, f, b, h) represents a SSP for an alternative plan which is not likely to be used. It seemed odd to select a node on this basis. By excluding such nodes in equation 9 we were able to only select nodes on their ability to form alternative synthesis plans.\nOf course, equation 9 is clearly not the only sensible option for node selection. For example, the expected value E could presumably be replaced with a median or some other quantile. If the 1 operator is interpreted as a weight for each sample from \u03bef , \u03beb, it might also make sense to use some sort of non-binary weighting instead of a binary weight. Ultimately, although these alternatives all seem sensible, we decided to just choose the expected value and did not investigate other options further."
        },
        {
            "heading": "E.3 REJECTED ALTERNATIVE ALGORITHMS",
            "text": "The first iteration of retro-fallback (proto retro-fallback) used a search tree instead of a search graph, and assumed that the feasibility/buyability of all reactions/molecules was independent. In this special case, the values of s, \u03c8, rho can all be computed analytically using dynamic programming. However, a major weakness of this algorithm is that forcing G\u2032 to be a tree required duplicating some molecules and reactions in the graph (e.g. if both the reactions A+B \u21d2 C and A+D \u21d2 C are possible then the molecule A and any reactions under it would be duplicated). The assumption of independence meant that the feasibility of the same reactions would be sampled multiple times independently, leading to \u201cbackup plans\u201d that actually used the same reaction. In practice this was often not an issue, but it did mean that the internal estimates of s used by proto retro-fallback did not have a clear relationship to true SSP. Hence we decided to proceed using samples, which provided a natural avenue to remove the independence assumption. More details about proto retro-fallback can be given upon request to the authors.\nWhen designing the version of retro-fallback presented in this paper, we first considered sampling outcomes for tip nodes using the heuristic function, and updating using the standard equations for s. This would effectively be a random heuristic, and although other algorithms use random heuristics (e.g. rollouts in MCTS) we decided that it would be an extra source of variance, and upon realizing that the expected value of such a heuristic can be computed analytically if the outcomes are assumed to be independent then we developed the equation for \u03c8 and used that. However, if in the future other researchers wish to remove the independence assumption in the heuristic then probably its outcomes would also need to be sampled."
        },
        {
            "heading": "E.4 PRACTICAL IMPLEMENTATION DETAILS",
            "text": "Here we give further details of how retro-fallback can be implemented. This section is not a description of our specific software implementation used in this paper: that is in Appendix F.1.7. Instead, we try to give general guidance that would be applicable for alternative implementations and alleviate potential sources of confusing or ambiguities.\nGraph initialization In Algorithm 1 we start by initializing G\u2032 to contain just the target molecule. Strictly speaking this is not required: any starting graph can be used (as long as it satisfies the assumptions about AND/OR graphs in Appendix B, e.g. not having reactions as tip nodes or edges between molecules and reactions that should not be connected). All that is needed is to properly initialize the variables (s, \u03c8, \u03c1) before entering the main loop.\nFunctional vs object-oriented implementation Algorithm 1 is written implying a functional implementation (e.g. accessing functions fi, bi as needed). This was done for clarity and conciseness\nof presentation. In practice we recommend an object-oriented implementation where the values of fi, bi, si, \u03c8i, \u03c1i are stored for each molecule/reaction. This allows for the main update steps of the algorithm to be vectorized (which is sensible because the same update equations are used for all samples) and allows f, b to have simpler interfaces.\nSamples from stochastic processes Using the object-oriented implementation described above, algorithm 1 requires computing samples from the posterior stochastic process whenever new nodes are added to G\u2032. For processes where this is inexpensive (e.g. processes with independent outcomes) this is fast and the implementation is not important. However, when it is slow it is likely very important to use caching. For example, drawing samples from a GP posterior scales with O(N3): if \u03bef or \u03beb use a GP, this O(N3) operation at every step will result in an overall algorithm speed of O(N4)! To avoid this, we cached a Cholesky decomposition of the GP covariance matrix at every step and used incremental updating of the Cholesky decomposition to bring the overall complexity to at mostO(N3). For other stochastic processes different techniques may be applicable, but in general for non-independent stochastic processes we anticipate some form of caching may be necessary.\nPriority Queues In each iteration of algorithm 1 all tip nodes of G\u2032 are found, andme is chosen by maximizing \u03b1 over all tip nodes. This operation in general scales as O(N), bringing the potential scaling of the algorithm to O(N2). In practice a different implementation is desirable. Because expanding the graph usually only results in a small number of nodes being updated, in practice the values of \u03b1 for many tip nodes does not change in most iterations. Therefore, we recommend storing all tip nodes in a priority queue (sorted by \u03b1). When \u03c1 is updated, all tip nodes for which at least one value of \u03c1 changes would be removed and re-added with updated \u03b1 values. This procedure still has worst case O(N) complexity (because some updates could affect all tip nodes), but in we imagine its average complexity would be O\u0303(1), leading to overall O\u0303(N) scaling of the algorithm (neglecting potential worse scaling from other steps).\nBackward reaction model Algorithm 1 requires a backward reaction model B. This is also not necessary: all that is needed is some way to decide what reactions to add to the graph. For example, if it was possible to obtain a list of reactions from \u03bef whose marginal feasibility is non-zero, this could be used as a replacement for B.\nTermination conditions Algorithm 1 uses several termination conditions, some of which may not be necessary or could be modified:\n1. No nodes are left to expand. We believe this one is necessary. 2. L iterations of expansion are done. This is not necessary: the algorithm alternatively ter-\nminate after a fixed wallclock time, or simply never terminate until it is killed by the user. 3. All si(mt) are 1: this is a sensible point to terminate because it means that \u03b1(m) = 0 for\nall tip nodes m. However, the algorithm could easily keep running past this point; it would just expand nodes arbitrarily because all nodes would have an equivalent value of \u03b1(m). This condition could also be loosened: for example the algorithm could terminate when s\u0302(mt) > 1 \u2212 \u03f5 for some small \u03f5 > 0. This is sensible if one believes that improvement beyond a certain point is redundant."
        },
        {
            "heading": "F EXTENDED EXPERIMENT SECTION",
            "text": ""
        },
        {
            "heading": "F.1 DETAILS OF EXPERIMENTAL SETUP",
            "text": ""
        },
        {
            "heading": "F.1.1 FEASIBILITY MODELS",
            "text": "As stated in section 6, we examined four feasibility models for this work, which assign different marginal feasibility values and different correlations between feasibility outcomes. The starting point for our feasibility models was the opinion of a trained organic chemist that around 25% of the reactions outputted by the pre-trained template classification model from Chen et al. (2020) were \u201cobviously wrong\u201d. From this, we proposed the following two marginal values for feasibility:\n1. (C) A constant value of 1/2 for all reactions. This is an attempt to account for the 25% of reactions which were \u201cobviously wrong\u201d, plus an additional unknown fraction of reactions which seemed plausible but may not work in practice. Ultimately anything in the interval [0.2, 0.6] seemed sensible to use, and we chose 1/2 as a nice number.\n2. (R) Based on previous work with template classifiers suggesting that the quality of the proposed reaction decreases with the softmax value (Segler & Waller, 2017; Segler et al., 2018), we decided to assign higher feasibility values to reactions with high softmax values. To avoid overly high or low feasibility values, we decided to values based on the rank of the outputted reaction, designed the following function which outputs a high feasibility (\u224875%) for the top reaction and decreases to (\u224810%) for lower-ranked reactions:\np(rank) = 0.75\n1 + rank/10 . (13)\nNote that \u201crank\u201d in the above equation starts from 0.\nWe then added correlations on top of these marginal feasibility values. The independent model (I) is simple: reaction outcomes are sampled independently using the marginal feasibility values described above. To introduce some correlations without changing the marginal probabilities, we created the following probabilistic model which assigns feasibility outcomes by applying a threshold to the value of a latent Gaussian process (Williams & Rasmussen, 2006):\noutcome(z) = 1z>0 (14) z(r) \u223c GP (\u00b5(\u00b7),K(\u00b7, \u00b7)) (15) \u00b5(r) = \u03a6\u22121 (p(r)) (16)\nK(r, r) = 1 \u2200r (17) Here, \u03a6 represents the CDF of the standard normal distribution and p(r) represents the desired marginal probability function. Because of equation 17, the marginal distribution of each reaction\u2019s z value is N (\u03a6\u22121(p(r)), 1) which will be positive with probability p(r). This ensures consistency with any desired marginal distribution for any kernelK with diagonal values of 1. IfK is the identity kernel (i.e. K(r1, r2) = 1r1=r2 ) then this model implies all outcomes are independent. However, non-zero off-diagonal values of K will induce correlations (positive or negative).\nWe aimed to design a model which assigns correlations very conservatively: only reactions involving similar molecules and which induce similar changes in the reactant molecules will be given a high positive correlation; all other correlations will be near zero. We therefore chose a kernel as a product of two simpler kernels:\nKtotal(r1, r2) = Kmol(r1, r2)Kmech(r1, r2) . We choseKmol(r1, r2) to be the Jaccard kernel k(x, x\u2032) = \u2211 imin(xi, x \u2032 i)\u2211\nimin(xi, x \u2032 i)\nbetween the Morgan fin-\ngerprints (Rogers & Hahn, 2010) with radius 1 of the entire set of product and reactant molecules.9 We chose Kmech(r1, r2) to be the Jaccard kernel of the difference between the product and reactant fingerprints individually. This is sensible because the difference between fingerprint vectors corresponds to a set of subgraphs which are added/removed as part of the reaction. Reactions which perform the same kinds of transformation will induce the same kinds of difference vectors.\n9This is the same as adding the fingerprint vectors for all component molecules.\nFigure F.1: Sample pairs of reactions where Ktotal > 0.8. Top: both reactions join a COCl group to an NH group in a ring to form molecules which differ only by the location of the Cl atom on the right side ring (far away from the reaction site). Middle: two reactions transforming a tert-butyl ester into a ketone with a fluorine-containing ring (difference between reactions is the location of the Cl atom on the ring far away from the reaction site). Bottom: two reactions removing a fluorine atom from an aromatic ring on similar molecules (difference is between the Cl and OH groups). Summary: these pairs of reactions are all very similar.\nWe illustrate some outputs of this kernel in Figures F.1\u2013F.3. Figure F.1 shows that reactions with a high kernel value (> 0.8) are generally quite similar, both in product and in mechanism. Figure F.2 shows that reactions with modest similarity values in [0.4, 0.6] have some similarities but are clearly less related. Figure F.3 shows that reactions with low similiarity values in [0.05, 0.1] are generally quite different. After a modest amount of exploratory analysis we were satisfied that this kernel behaved as we intended, and therefore used it in our experiments without considering further alternatives. However, we imagine there is room for improvement of the kernel in future work to better align with the beliefs of chemists."
        },
        {
            "heading": "F.1.2 BUYABILITY MODELS",
            "text": "Following Chen et al. (2020) we based our buyability models on the inventory of eMolecules: a chemical supplier which acts as a middleman between more specialized suppliers and consumers. According to eMolecule\u2019s promotional material they offer 6 \u201ctiers\u201d of molecules:\n0. (Accelerated Tier). \u201cDelivered in 2 days or less, guaranteed. Most reliable delivery service. Compound price is inclusive of a small service fee, credited back if not delivered on time. Available in the US only.\u201d\n1. \u201cShipped within 1- 5 business days. Compounds from suppliers proven to ship from their location in < 5 days.\u201d\n2. \u201cShipped within 10 business days. Compounds from suppliers proven to ship from across the globe in < 10 days\u201d\n3. \u201cShipped within 4 weeks. Shipped from suppliers further from your site and often with more complex logistics. Synthesis may be required using proven reactions.\u201d\n4. \u201cShipped within 12 weeks. Usually requires custom synthesis on demand.\u201d 5. \u201cVaried ship times. Requires custom synthesis for which a quote can be provided on re-\nquest.\u201d\nMuch like machine learning researchers, chemists usually want to complete experiments as quickly as possible and probably would prefer not to wait 12 weeks for a rare molecule to be shipped to them. Such molecules could arguably be considered less \u201cbuyable\u201d on this subjective basis alone,\nFigure F.2: Examples of reactions where 0.4 \u2264 Ktotal \u2264 0.6. Top: similar conjugation reactions, but the reactant on the right side is now a COO\u2212 anion instead of a COCl group. Middle: similar reaction, although on the right reaction has a Zn+ on the ring instead of F. Bottom: two reactions which remove a fluorine atom from an aromatic ring but on molecules which are much less similar than Figure F.1. Summary: these pairs of reactions have similarities but are less similar than the reactions in Figure F.1.\nFigure F.3: Examples of reactions where 0.05 \u2264 Ktotal \u2264 0.1. The pairs of reactions are generally quite different.\nso we decided to create buyability models based on the tier of molecule. Unfortunately, the public repository for retro* does not contain any information on the tier of each molecule, and because their inventory was downloaded in 2019 this information is no longer available on eMolecules\u2019 website. Therefore we decided to re-make the inventory using the latest data.\nWe downloaded data from eMolecules downloads page10, specifically their \u201corderable\u201d molecules and \u201cbuilding blocks\u201d with quotes. After filtering out a small number of molecules (31407) whose SMILES were not correctly parsed by rdkit we were left with 14903392 molecules with their associated purchase tiers. Based on this we created 2 buyability models:\n\u2022 Basic: all molecules in tiers 0-2 are purchasable with 100% probability. Corresponds to realistic scenario where chemists want to do a synthesis and promptly.\n\u2022 Complex: molecules are independently purchasable with probability that depends on the tier (100% for tiers 0-2, 50% for tier 3, 20% for tier 4, 5% for tier 5). These numbers were chosen as subjective probabilities that the compounds would be delivered within just 2 weeks (shorter than the longer times advertised). This still corresponds to a chemist wanting to do the synthesis within 2 weeks, but being willing to risk ordering a molecule whose stated delivery time is longer.\nThe experiments in section 6 use only the basic buyability model. We performed some preliminary experiments with the complex buyability model but found that in most cases there was no difference. This makes sense: eMolecules is a real, profit-driven company and there is a clear financial incentive to quickly ship molecules which are useful for a wide range of syntheses. Molecules with longer shipping times are used more rarely, so one would expect them to only be useful in a smaller number of cases. Because of the page limit for conference papers, we decided to prioritize other experiments for this manuscript, and therefore do not show any results for this buyability model.\nIn the future, we believe that better buyability models could be formed by introducing correlations between molecules coming from the same supplier, but we do not investigate that here (chiefly because the eMolecules data we downloaded does not contain information about suppliers)."
        },
        {
            "heading": "F.1.3 TEST MOLECULES",
            "text": "The test molecules were generated with the following procedure:\n1. Download the publicly available test set from Brown et al. (2019)\n2. Filter our all molecules available in the eMolecules inventory (F.1.2)\n3. Shuffle all molecules and take the first 500\nCode to reproduce this process, and the entire test set in shuffled order is included in our supplementary material.\nWe note that although many previous works have evaluated their methods on the 190 molecule test set from Chen et al. (2020), this test set is small and contains only molecules where finding any single synthesis plan is difficult, which only occurs for a small minority of molecules. It was unclear to us whether this would be a good test set: in particular, it is likely that the set of molecules where finding one synthesis plan is hard does not completely overlap with the set of molecules where finding multiple synthesis plans is hard. By using a more \u201ctypical\u201d set of molecules we avoid this bias. However, we do report results for the retro* 190 test set in Appendix F.4."
        },
        {
            "heading": "F.1.4 ALGORITHM CONFIGURATION",
            "text": "Retro-fallback was run with k = 1000 samples from \u03bef , \u03beb. All other algorithms were run as per the \u201cbest proxy\u201d description in Appendix C. In particular, this means:\n\u2022 Breadth-first search was run with no modifications.\n\u2022 retro* was run using \u2212 logEf [f(r)] as the reaction cost and \u2212 logEb[b(m)]\n10Downloaded 2023-09-08.\n\u2022 MCTS was run using s\u0302(m;T, \u03bef , \u03beb) as the reward for finding synthesis plan T (i.e. the empirical SSP for individual synthesis plans). To allow the algorithm to best make use of its budget of reaction model calls, we only expanded nodes after they were visited 10 times. The marginal feasibility value of reach reaction was used as the policy in the upperconfidence bound. We used an exploration constant of c = 0.01 to avoid \u201cwasting\u201d reaction model calls on exploration, and only gave non-zero rewards for up to 100 visits to the same synthesis plan to avoid endlessly re-visiting the same solutions.\nWe chose not to compare with proof-number search (Kishimoto et al., 2019) because we did not see a way to configure it to optimize SSP (see Appendix C.3). We chose not to compare with algorithms requiring some degree of learning from self-play (including RetroGraph and the methods discussed in Appendix G.1) due to computational constraints, and because it seemed inappropriate to compare with self-play methods without also learning a heuristic for retro-fallback with self-play.\nBecause retro-fallback runs on a minimal AND/OR graph, we used a modified version of retro* which also operates on an AND/OR graph. This modified version is not our original creation (it is explained in section 3.5 of Chen et al. (2020)) and is fully consistent with the original tree-based version in that it estimates the same costs and expands the same nodes, it just does not store large duplicate subtrees and uses an alternative shortest-path algorithm to perform updating. We also run breadth-first search on the minimal AND/OR graph (although this requires no special modifications)."
        },
        {
            "heading": "F.1.5 HEURISTIC FUNCTIONS",
            "text": "The heuristic obviously plays a critical role in heuristic-guided search algorithms! Ideally one would control for the effect of the heuristic by using the same heuristic for different methods. However, this is not possible when comparing algorithms from different families because the heuristics are interpreted differently! For example, in retro-fallback the heuristic is interpreted as a SSP in [0, 1] (higher is better), while in retro* it is interpreted as a cost between [0,\u221e) (lower is better). If we used literally the same heuristic it would give opposite signals to both of these algorithms, which is clearly not desirable or meaningful. Therefore, we tried our best to design heuristics which were \u201cas similar as possible.\u201d\nOptimistic heuristic Heuristics which predict the best possible value are a common choice of naive heuristic. Besides being an important baseline, optimistic heuristics are always admissible (i.e. they never overestimate search difficulty), which is a requirement for some algorithms like A* to converge to the optimal solution (Pearl, 1984). For retro-fallback, the most optimistic heuristic is hrfb(m) = 1, while for retro* it is hr*(m) = 0, as these represent the best possible values for SSP and cost respectively. For MCTS, the heuristics is a function of a partial plan T \u2032 rather than a single molecule. We choose the heuristic to be Ef\u223c\u03bef [minr\u2208T \u2032 f(r)], which is the expected SSP of the plan T \u2032 if it were completed by making every tip molecule buyable.11 In practice this quantity was estimated from k samples (same as retro-fallback).\nSA score heuristic SA score gives a molecule a score between 1 and 10 based on a dictionary assigning synthetic difficulties to different subgraphs of a molecule (Ertl & Schuffenhauer, 2009). A score of 1 means easy to synthesize, while a score of 10 means difficult to synthesize. For retrofallback, we let the estimated SSP decrease linearly with the SA score:\nhrfb(m) = 1\u2212 SA(m)\u2212 1\n10 .\nBecause the reaction costs in retro* were set to negative log feasibility values, we thought a natural extension to retro* would be to use hr*(m) = \u2212 log hrfb(m). This choice has the advantage of preserving the interpretation of total cost as the negative log joint probability, which also perfectly matches retro-fallback\u2019s interpretation of the heuristic (recall that in section 4.2 the heuristic values were assumed to be independent). We designed MCTS\u2019s heuristic to also match the interpretation\n11Note that the min function will be 1 if all reactions are feasible, otherwise 0. Using \u220f\nr instead of minr would yield the same output.\nof \u201cjoint probability\u201d:\nhMCTS(T \u2032) = Ef\u223c\u03bef   minr\u2208T \u2032 f(r)\ufe38 \ufe37\ufe37 \ufe38\nreactions feasible\n \u220f m\u2208tip(T \u2032),b(m)=0 hrfb(m)  which is the expected SSP of the plan if all non-purchasable molecules are made purchasable independently with probability hrfb(m)."
        },
        {
            "heading": "F.1.6 ANALYSIS",
            "text": "Our primary analysis metric was the SSP. For algorithms that use AND/OR graphs (e.g. retrofallback, retro*), we computed the SSP using equations 2\u20133 with k = 10 000 samples from \u03bef , \u03beb.\nFor algorithms which use OR trees the best method for analysis is somewhat ambiguous. One option is to extract all plans T \u2286 G\u2032 and calculate whether each plan succeeds on a series of samples fi, bi. A second option is to convert G\u2032 into an AND/OR graph and analyze it like other AND/OR graphs. Although they seem similar, these options are subtly different: an OR graph may contain reactions in different locations which are not connected to form a synthesis plan, but could form a synthesis plan if connected. The process of converting into an AND/OR graph would effectively form all possible synthesis plans which could be made using reactions in the original graph, even if they are not actually present in the original graph. We did implement both methods and found that converting to an AND/OR graph tends to increase performance, so this choice does make a meaningful difference. We think the most \u201crealistic\u201d option is unclear, so for consistency with other algorithms we chose to just convert to an AND/OR graph.\nAll analysis metrics involving individual synthesis routes were calculated by enumerating the routes in best-first order using a priority queue. More complex algorithms could also be used to do this (Shibukawa et al., 2020)."
        },
        {
            "heading": "F.1.7 SOFTWARE IMPLEMENTATION",
            "text": "Our code is included in the supplementary material of this paper. We built our code around the open-source library SYNTHESEUS12 (Maziarz et al., 2023) and used its implementations of retro* and MCTS in our experiments. The exact template classifier from Chen et al. (2020) was used by copying their code and using their model weights. Our code benefitted from the following libraries:\n\u2022 pytorch (Paszke et al., 2019), rdkit13 and rdchiral (Coley et al., 2019a). Used in the template classifier.\n\u2022 networkx (Hagberg et al., 2008). Used to store search graphs and for analysis. \u2022 numpy (Harris et al., 2020), scipy (Virtanen et al., 2020), and scikit-learn (Pe-\ndregosa et al., 2011). Used for array programming and linear algebra (e.g. in the feasibility models)."
        },
        {
            "heading": "F.2 ADDITIONAL PLOTS FOR SECTION 6.2",
            "text": "See Figures F.4, F.5, F.6, F.7. These figures are discussed in section 6.2.\n12https://github.com/microsoft/syntheseus/ 13Specifically version 2022.09.4 (Landrum et al., 2023).\n100 101 102 num. calls to B\n0.5\n0.6\n0.7\n0.8\n0.9 1.0 av er ag e SS\nP f: C, I (trivial mols)\nretro-fb BFS retro* MCTS\n100 101 102 num. calls to B\nf: R, I (trivial mols)\n100 101 102 num. calls to B\nf: C, G (trivial mols)\n100 101 102 num. calls to B\nf: R, G (trivial mols)\n100 101 102 num. calls to B\n0.2\n0.4\n0.6\n0.8\nav er\nag e\nSS P\nf: C, I (all mols)\nretro-fb BFS retro* MCTS\n100 101 102 num. calls to B\nf: R, I (all mols)\n100 101 102 num. calls to B\nf: C, G (all mols)\n100 101 102 num. calls to B\nf: R, G (all mols)\nFigure F.4: Extension of Figure 2: results for trivial molecules and all molecules.\n100 101 102 num. calls to B\n0.0\n0.2\n0.4\n0.6\nav er\nag e\nSS P\nf: C, I (non-trivial mols) retro-fb BFS retro* MCTS\n100 101 102 num. calls to B\nf: R, I (non-trivial mols)\n100 101 102 num. calls to B\nf: C, G (non-trivial mols)\n100 101 102 num. calls to B\nf: R, G (non-trivial mols)\n100 101 102 num. calls to B\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nav er\nag e\nSS P\nf: C, I (trivial mols)\nretro-fb BFS retro* MCTS\n100 101 102 num. calls to B\nf: R, I (trivial mols)\n100 101 102 num. calls to B\nf: C, G (trivial mols)\n100 101 102 num. calls to B\nf: R, G (trivial mols)\n100 101 102 num. calls to B\n0.2\n0.4\n0.6\n0.8\nav er\nag e\nSS P\nf: C, I (all mols)\nretro-fb BFS retro* MCTS\n100 101 102 num. calls to B\nf: R, I (all mols)\n100 101 102 num. calls to B\nf: C, G (all mols)\n100 101 102 num. calls to B\nf: R, G (all mols)\nFigure F.5: Average SSP for algorithms using SAscore heuristic (interpretation is the same as Figure 2).\n100 101 102 num. calls to B\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFr ac\ntio n\nso lv\ned\nf: C, I\nretro-fb BFS retro* MCTS\n100 101 102 num. calls to B\nf: R, I\n100 101 102 num. calls to B\nf: C, G\n100 101 102 num. calls to B\nf: R, G\nBFS MCTS retro-fb retro*\n5\n10\n15\n20\nSh or\nte st\nro ut\ne\nC, I\nBFS MCTS retro-fb retro*\nR, I\nBFS MCTS retro-fb retro*\nC, G\nBFS MCTS retro-fb retro*\nR, G\nBFS MCTS retro-fb retro* 0.0\n0.2\n0.4\n0.6\n0.8\nM os\nt s uc\nce ss\nfu l r\nou te\nC, I\nBFS MCTS retro-fb retro*\nR, I\nBFS MCTS retro-fb retro*\nC, G\nBFS MCTS retro-fb retro*\nR, G\nFigure F.6: Alternative success metrics for algorithms with optimistic heuristic.\n100 101 102 num. calls to B\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFr ac\ntio n\nso lv\ned\nf: C, I\nretro-fb BFS retro* MCTS\n100 101 102 num. calls to B\nf: R, I\n100 101 102 num. calls to B\nf: C, G\n100 101 102 num. calls to B\nf: R, G\nBFS MCTS retro-fb retro*\n5\n10\n15\n20\nSh or\nte st\nro ut\ne\nC, I\nBFS MCTS retro-fb retro*\nR, I\nBFS MCTS retro-fb retro*\nC, G\nBFS MCTS retro-fb retro*\nR, G\nBFS MCTS retro-fb retro* 0.0\n0.2\n0.4\n0.6\n0.8\nM os\nt s uc\nce ss\nfu l r\nou te\nC, I\nBFS MCTS retro-fb retro*\nR, I\nBFS MCTS retro-fb retro*\nC, G\nBFS MCTS retro-fb retro*\nR, G\nFigure F.7: Alternative success metrics for algorithms with SAscore heuristic.\n100 101 102 num. calls to B\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSS P\nf: C, I (SMILES #3)\nretro-fb BFS retro*\n100 101 102 num. calls to B\nf: R, I (SMILES #3)\n100 101 102 num. calls to B\nf: C, G (SMILES #3)\n100 101 102 num. calls to B\nf: R, G (SMILES #3)\n100 101 102 num. calls to B\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSS P\nf: C, I (SMILES #5) retro-fb BFS retro*\n100 101 102 num. calls to B\nf: R, I (SMILES #5)\n100 101 102 num. calls to B\nf: C, G (SMILES #5)\n100 101 102 num. calls to B\nf: R, G (SMILES #5)\n100 101 102 num. calls to B\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSS P\nf: C, I (SMILES #11) retro-fb BFS retro*\n100 101 102 num. calls to B\nf: R, I (SMILES #11)\n100 101 102 num. calls to B\nf: C, G (SMILES #11)\n100 101 102 num. calls to B\nf: R, G (SMILES #11)\n100 101 102 num. calls to B\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSS P\nf: C, I (SMILES #20)\nretro-fb BFS retro*\n100 101 102 num. calls to B\nf: R, I (SMILES #20)\n100 101 102 num. calls to B\nf: C, G (SMILES #20)\n100 101 102 num. calls to B\nf: R, G (SMILES #20)\nFigure F.8: Success probability for some individual molecules. Solid lines represent the median value across 10 runs, and boundaries of the shaded region are the 25th and 75th percentiles."
        },
        {
            "heading": "F.3 CASE STUDIES",
            "text": "See Figure F.8 and its caption."
        },
        {
            "heading": "F.4 RESULTS ON RETRO* 190 HARD MOLECULES",
            "text": "Figure F.9 shows the success probability over time for harder molecules from Chen et al. (2020). All details of the setup are identical to those in section 6.2 except for the test molecules and the runtime (we extended it to 500 reaction model calls to match Chen et al. (2020)). Qualitatively, the results are very similar to those on the GuacaMol test set. Quantitatively, it appears that the SSP values are on average lower (which makes sense because these molecules are chosen to be chllenging to solve) and that the difference between retro-fallback and the other algorithms is larger. These results clearly support the claim that retro-fallback outperforms other algorithms at maximizing SSP.\n100 101 102 num. calls to B\n0.0\n0.2\n0.4\n0.6\nav er\nag e\nSS P\nf: C, I (all mols) retro-fb BFS retro* MCTS\n100 101 102 num. calls to B\nf: R, I (all mols)\n100 101 102 num. calls to B\nf: C, G (all mols)\n100 101 102 num. calls to B\nf: R, G (all mols)\n100 101 102 num. calls to B\n0.0\n0.2\n0.4\n0.6\nav er\nag e\nSS P\nf: C, I (all mols) retro-fb BFS retro* MCTS\n100 101 102 num. calls to B\nf: R, I (all mols)\n100 101 102 num. calls to B\nf: C, G (all mols)\n100 101 102 num. calls to B\nf: R, G (all mols)\nFigure F.9: Average success probability over time for 190 \u201chard\u201d molecules from Chen et al. (2020) using the optimistic heuristic (top) and SAscore heuristic (bottom)."
        },
        {
            "heading": "F.5 RESULTS ON FUSIONRETRO BENCHMARK",
            "text": "Here we present results on the benchmark dataset from FusionRetro, which is derived from USPTO routes and contains 5838 test molecules (Liu et al., 2023b). In addition to SSP and the fraction solved, we also evaluate performance by checking whether the outputted synthesis plans use the same starting molecules as a known ground-truth synthesis route. Liu et al. (2023b) call this metric \u201cexact set-wise matching,\u201d but we will call it precursor matching because we think this name is less ambiguous. Because this metric depends on the purchasable molecules, we use a buyability model derived from the inventory of Liu et al. (2023b) instead of the model derived from eMolecules used for all other experiments. This is a deterministic model: molecules in the inventory are independently buyable with probability 1, and all other molecules are not buyable. We use the rank-independent feasibility model from section 6.2. All other details are kept the same.\nThe results after 50 reaction model calls are tabulated in Table F.1. As expected, retro-fallback attains higher SSP scores than the baseline retro* and breadth-first search methods, regardless of the heuristic. Just like on the GuacaMol test set from section 6.2, retro-fallback also finds at least one potential synthesis route for a higher fraction of molecules than the baselines. Finally, the precursor matching for the single most feasible synthesis plan is extremely similar for all methods (around 18%, with differences between the methods not being statistically significant).14 This is what one would expect: the best synthesis plans found by all methods will likely be similar; the difference between retro-fallback and the other algorithms is in the secondary synthesis plans that it finds. Overall, these results show that retro-fallback outperforms baseline algorithms on the SSP metric while being no worse than the baselines on metrics which involve only single synthesis plans.\nAlgorithm Heuristic Mean SSP (%) Solved (%) Precursor Match (%)\nBFS N/A 68.38\u00b10.55 76.14\u00b10.56 17.75\u00b10.50 retro* optimistic 73.73\u00b10.52 80.22\u00b10.52 18.24\u00b10.51 retro* SAScore 74.66\u00b10.52 80.56\u00b10.52 18.14\u00b10.50 retro-fallback optimistic 75.88\u00b10.50 82.67\u00b10.50 17.85\u00b10.50 retro-fallback SAScore 78.32\u00b10.48 84.31\u00b10.48 18.14\u00b10.50\nTable F.1: Results on 5838 test molecules FusionRetro benchmark (Liu et al., 2023b). Experimental details and metrics are explained in section F.5. Larger values are better for all metrics. \u00b1 values indicate standard error of the mean estimate. The largest value within each column is marked in bold (when not within the standard error of any other values).\n14Readers familiar with Liu et al. (2023b) may wonder why the precursor matching scores are much lower than what is reported in Table 1 of Liu et al. (2023b). This is because we used the same pre-trained reaction model from Chen et al. (2020) as our single-step model, whereas Liu et al. (2023b) retrains the models using their training dataset. We did not re-train the model because it also forms the basis for our feasibility model, which was loosely calibrated with the inspections of an expert chemist."
        },
        {
            "heading": "G EXTENDED RELATED WORK",
            "text": "Here we cite and discuss papers which are worth commenting on, but were not discussed in the main text."
        },
        {
            "heading": "G.1 SEARCH HEURISTICS FOR RETROSYNTHESIS",
            "text": "Many papers propose search heuristics for retrosynthesis algorithms, including rollouts (Segler et al., 2018), parametric models (Chen et al., 2020), and a variety of heuristics informed by chemical knowledge (Schwaller et al., 2020; Ertl & Schuffenhauer, 2009; Thakkar et al., 2021; Li & Chen, 2022). Many papers also propose to learn heuristics using techniques from machine or reinforcement learning, where a heuristic is learned based on previous searches or data (Coley et al., 2018; Liu et al., 2022; Kim et al., 2021; Yu et al., 2022; Liu et al., 2023a). A potential point of confusion is that some of these works describe their contribution as a \u201cretrosynthesis algorithm\u201d or \u201cretrosynthetic planning algorithm.\u201d Given that the end product of these papers is a value function that is plugged into a previously proposed algorithm (typically MCTS or retro*), we think these papers should be more accurately viewed as proposing heuristics. The heuristic is orthogonal to the underlying search algorithm, so we view these works as complementary rather than competitive. We hope in the future to investigate learning heuristics for retro-fallback using similar principles."
        },
        {
            "heading": "G.2 GENERATIVE MODELS",
            "text": "Several works propose parametric generative models of synthesis plans (Bradshaw et al., 2019; 2020; Gottipati et al., 2020; Gao et al., 2021). Although this resembles the goal of explicit search algorithms, such generative models are fundamentally limited by their parametrization: they have no guarantee to find a synthesis plan if it exists, and are often observed to fail to produce a valid synthesis plan in practice (Gao et al., 2021). We think such models are best viewed as trying to amortize the output of an explicit planning algorithms, making them more similar in spirit to search heuristics (G.1)."
        },
        {
            "heading": "G.3 SINGLE-STEP RETROSYNTHESIS",
            "text": "Many models have been proposed to predict possible chemical reactions, including template classifiers (Segler & Waller, 2017; Seidl et al., 2021), graph editing methods (Dai et al., 2019; Sacha et al., 2021; Chen & Jung, 2021), and transformers (Irwin et al., 2022; Zhong et al., 2022; Liu et al., 2023b). Such models are a useful component of a retrosynthesis algorithm, but do not themselves perform multi-step retrosynthesis."
        },
        {
            "heading": "G.4 FUSIONRETRO",
            "text": "One work which does not fit nicely into any of the previous subsections is FusionRetro (Liu et al., 2023b). On one level, the paper describes a reaction prediction model based on a transformer, which is essentially a single-step reaction prediction model (G.3). However, unlike other models which just condition on a single input molecule, in FusionRetro the predictions are conditioned on all predecessor molecules in a multi-step search graph. The paper describes an inference procedure to make predictions from the model autoregressively, which resembles both a generative model for synthesis routes (G.2) or a pruning heuristic for breadth-first search (G.1). A significant portion of the paper also describes benchmarking and evaluation of route quality.\nWe think that FusionRetro and retro-fallback can both be viewed as responses to unrealistically lenient evaluation metrics used in prior works on retrosynthesis (chiefly reporting success if a \u201csolution\u201d is found without any regard to whether the solution is realistic). Liu et al. (2023b)\u2019s general response is to evaluate the quality of entire routes rather than individual steps, and perform this evaluation using entire synthesis routes from the literature. The advantage of this approach is that it is close to ground-truth data, but has the disadvantage that high-quality ground truth data is fairly scarce, especially for long routes involving rare reactions. In contrast, our response is to model uncertainty about reactions and use this uncertainty in evaluation (to define SSP). The advantage of our approach is that it does not [necessarily] require any data, while the disadvantage is that it requires\na good model of reaction uncertainty, which we currently do not have (and creating such a model is likely to be difficult).\nCritically, the approaches described in these papers are not mutually exclusive: a backward reaction model which depends on the entire search graph G\u2032 (such as FusionRetro) could be used in retro-fallback, while the quality of synthesis routes proposed by a method like FusionRetro could be evaluated using SSP. We leave combining and building upon these works in more realistic retrosynthesis programs to future work."
        },
        {
            "heading": "H FUTURE WORK",
            "text": "Relaxing assumptions If one wishes to re-insert the \u201cnuance\u201d lost by defining feasibility and buyability as binary outcomes, one could potentially explicitly model factors such as yields and shipping times and build a binary stochastic process on top of this. We do not have a clear idea of how retro-fallback or SSP could be generalized into some sort of continuous \u201cdegree of success\u201d, but imagine future work in this area could be useful. Relaxing the independence assumption of the heuristic function was discussed in Appendix E.3. The heuristic could potentially also be modified to depend on the remaining compute budget. Finally, using a separate feasibility and buyability model implicitly assumes that these outcomes are independent. We think this is a reasonable assumption because reaction feasibility is uncertain due to not fully understanding the physical system or not having a reliable model B, while uncertainty in buyability would originate from issues of shipping, etc. That being said, \u201cvirtual libraries\u201d are one area where a molecule not being buyable meant that somebody else was unable to synthesize it. This may impact which reactions a chemist would consider feasible (although it seems unlikely in practice that a vendor would tell you the reactions that they tried). Nonetheless, if one wanted to account for this \u03bef and \u03beb could be merged into a joint feasibility-buyability model \u03befb from which functions f and b are simultaneously sampled.\nFurther Proofs We suspect that it is possible to give a theoretical guarantee that retro-fallback\u2019s worst-case performance is better than that of retro* by formalizing the scenario in section 4.1. However, we were unable to complete such a proof at the time of submission. We also expect it could be possible (and useful) to theoretically characterize how the behaviour retro-fallback with a finite number of samples k deviates from the behaviour of \u201cexact\u201d retro-fallback in the limit of k \u2192 \u221e where all estimates of SSP are exact. Such analysis might provide insight into how large k should be set to for more general feasibility models."
        }
    ],
    "title": "RETRO-FALLBACK: RETROSYNTHETIC PLANNING IN AN UNCERTAIN WORLD",
    "year": 2023
}