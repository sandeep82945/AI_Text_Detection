{
    "abstractText": "Scene graph generation (SGG) models have suffered from inherent problems regarding the benchmark datasets such as the long-tailed predicate distribution and missing annotation problems. In this work, we aim to alleviate the long-tailed problem of SGG by utilizing unannotated triplets. To this end, we introduce a Self-Training framework for SGG (ST-SGG) that assigns pseudo-labels for unannotated triplets based on which the SGG models are trained. While there has been significant progress in self-training for image recognition, designing a self-training framework for the SGG task is more challenging due to its inherent nature such as the semantic ambiguity and the long-tailed distribution of predicate classes. Hence, we propose a novel pseudo-labeling technique for SGG, called Class-specific Adaptive Thresholding with Momentum (CATM), which is a model-agnostic framework that can be applied to any existing SGG models. Furthermore, we devise a graph structure learner (GSL) that is beneficial when adopting our proposed self-training framework to the state-of-the-art message-passing neural network (MPNN)-based SGG models. Our extensive experiments verify the effectiveness of ST-SGG on various SGG models, particularly in enhancing the performance on fine-grained predicate classes. Our code is available on https://anonymous.4open.science/r/torch-st-sgg-6F0F",
    "authors": [],
    "id": "SP:d95aff6997950fbcb95015d1ded3392243c440ad",
    "references": [
        {
            "authors": [
                "Yuan Yao",
                "Tat-Seng Chua"
            ],
            "title": "Finegrained scene graph generation with data transfer",
            "year": 2022
        },
        {
            "authors": [
                "Eric Arazo",
                "Diego Ortego",
                "Paul Albert",
                "Noel E. O\u2019Connor",
                "Kevin McGuinness"
            ],
            "title": "Pseudo-labeling and confirmation bias in deep semi-supervised learning",
            "venue": "In 2020 International Joint Conference on Neural Networks (IJCNN),",
            "year": 2020
        },
        {
            "authors": [
                "Yoshua Bengio",
                "Nicholas L\u00e9onard",
                "Aaron Courville"
            ],
            "title": "Estimating or propagating gradients through stochastic neurons for conditional computation",
            "venue": "arXiv preprint arXiv:1308.3432,",
            "year": 2013
        },
        {
            "authors": [
                "Yidong Wang"
            ],
            "title": "Flexmatch: Boosting semi-supervised learning with curriculum pseudo labeling",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Michael Bernstein Cewu Lu",
                "Ranjay Krishna",
                "Li Fei-Fei"
            ],
            "title": "Visual relationship detection with language priors",
            "venue": "European conference on computer vision,",
            "year": 2016
        },
        {
            "authors": [
                "Tianshui Chen",
                "Weihao Yu",
                "Riquan Chen",
                "Liang Lin"
            ],
            "title": "Knowledge-embedded routing network for scene graph generation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Meng-Jiun Chiou",
                "Henghui Ding",
                "Hanshu Yan",
                "Changhu Wang",
                "Roger Zimmermann",
                "Jiashi Feng"
            ],
            "title": "Recovering the unbiased scene graphs from the biased ones",
            "venue": "In Proceedings of the 29th ACM International Conference on Multimedia,",
            "year": 2021
        },
        {
            "authors": [
                "Alakh Desai",
                "Tz-Ying Wu",
                "Subarna Tripathi",
                "Nuno Vasconcelos"
            ],
            "title": "Learning of visual relations: The devil is in the tails",
            "venue": "In ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "Xingning Dong",
                "Tian Gan",
                "Xuemeng Song",
                "Jianlong Wu",
                "Yuan Cheng",
                "Liqiang Nie"
            ],
            "title": "Stacked hybrid-attention and group collaborative learning for unbiased scene graph generation",
            "year": 2022
        },
        {
            "authors": [
                "Lan-Zhe Guo",
                "Yu-Feng Li"
            ],
            "title": "Class-imbalanced semi-supervised learning with adaptive thresholding",
            "venue": "Proceedings of the 39th International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Yuyu Guo",
                "Lianli Gao",
                "Xuanhan Wang",
                "Yuxuan Hu",
                "Xing Xu",
                "Xu Lu",
                "Heng Tao Shen",
                "Jingkuan Song"
            ],
            "title": "From general to specific: Informative scene graph generation via balance adjustment",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV),",
            "year": 2021
        },
        {
            "authors": [
                "Eric Jang",
                "Shixiang Gu",
                "Ben Poole"
            ],
            "title": "Categorical reparameterization with gumbel-softmax",
            "venue": "arXiv preprint arXiv:1611.01144,",
            "year": 2016
        },
        {
            "authors": [
                "David Berthelot"
            ],
            "title": "Fixmatch: Simplifying semi-supervised learning with consistency and confidence",
            "venue": "In 34th Conference on Neural Information Processing Systems (NeurIPS 2020).,",
            "year": 2020
        },
        {
            "authors": [
                "Ranjay Krishna",
                "Yuke Zhu",
                "Oliver Groth",
                "Justin Johnson",
                "Kenji Hata",
                "Joshua Kravitz",
                "Stephanie Chen",
                "Yannis Kalantidis",
                "Li-Jia Li",
                "David A Shamma"
            ],
            "title": "Visual genome: Connecting language and vision using crowdsourced dense image annotations",
            "venue": "International journal of computer vision,",
            "year": 2017
        },
        {
            "authors": [
                "Alina Kuznetsova",
                "Hassan Rom",
                "Neil Alldrin",
                "Jasper Uijlings",
                "Ivan Krasin",
                "Jordi Pont-Tuset",
                "Shahab Kamali",
                "Stefan Popov",
                "Matteo Malloci",
                "Alexander Kolesnikov"
            ],
            "title": "The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale",
            "year": 1956
        },
        {
            "authors": [
                "Zhengfeng Lai",
                "Chao Wang",
                "Sen-ching Cheung",
                "Chen-Nee Chuah"
            ],
            "title": "Sar: Self-adaptive refinement on pseudo labels for multiclass-imbalanced semi-supervised learning",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),",
            "year": 2022
        },
        {
            "authors": [
                "Dong-Hyun Lee"
            ],
            "title": "Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks",
            "venue": "In Workshop on challenges in representation learning, ICML,",
            "year": 2013
        },
        {
            "authors": [
                "Lin Li",
                "Long Chen",
                "Yifeng Huang",
                "Zhimeng Zhang",
                "Songyang Zhang",
                "Jun Xiao"
            ],
            "title": "The devil is in the labels: Noisy label correction for robust scene graph generation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Rongjie Li",
                "Songyang Zhang",
                "Bo Wan",
                "Xuming He"
            ],
            "title": "Bipartite graph network with adaptive message passing for unbiased scene graph generation",
            "year": 2021
        },
        {
            "authors": [
                "Tsung-Yi Lin",
                "Michael Maire",
                "Serge Belongie",
                "Lubomir Bourdev",
                "Ross Girshick",
                "James Hays",
                "Pietro Perona",
                "Deva Ramanan",
                "C. Lawrence Zitnick",
                "Piotr Doll\u00e1r"
            ],
            "title": "Microsoft coco: Common objects in context, 2014. URL http://arxiv.org/abs/1405.0312. cite arxiv:1405.0312Comment: 1) updated annotation pipeline description and figures; 2) added new section describing datasets splits; 3) updated author list",
            "year": 2014
        },
        {
            "authors": [
                "Tsung-Yi Lin",
                "Priya Goyal",
                "Ross Girshick",
                "Kaiming He",
                "Piotr Doll\u00e1r"
            ],
            "title": "Focal loss for dense object detection",
            "venue": "In Proceedings of the IEEE international conference on computer vision,",
            "year": 2017
        },
        {
            "authors": [
                "Xin Lin",
                "Changxing Ding",
                "Jinquan Zeng",
                "Dacheng Tao"
            ],
            "title": "Gps-net: Graph property sensing network for scene graph generation",
            "year": 2020
        },
        {
            "authors": [
                "Xinyu Lyu",
                "Lianli Gao",
                "Yuyu Guo",
                "Zhou Zhao",
                "Hao Huang",
                "Heng Tao Shen",
                "Jingkuan Song"
            ],
            "title": "Fine-grained predicates learning for scene graph generation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2022
        },
        {
            "authors": [
                "Chris J Maddison",
                "Andriy Mnih",
                "Yee Whye Teh"
            ],
            "title": "The concrete distribution: A continuous relaxation of discrete random variables",
            "venue": "arXiv preprint arXiv:1611.00712,",
            "year": 2016
        },
        {
            "authors": [
                "Shaoqing Ren",
                "Kaiming He",
                "Ross Girshick",
                "Jian Sun"
            ],
            "title": "Faster r-cnn: Towards real-time object detection with region proposal networks",
            "venue": "Advances in neural information processing systems,",
            "year": 2015
        },
        {
            "authors": [
                "Dimitris Kanellopoulos"
            ],
            "title": "Handling imbalanced datasets: A review",
            "venue": "pp. 25\u201336,",
            "year": 2006
        },
        {
            "authors": [
                "Rajat Koner"
            ],
            "title": "Relationformer: A unified framework for image-to-graph generation",
            "year": 2022
        },
        {
            "authors": [
                "Kaihua Tang",
                "Hanwang Zhang",
                "Baoyuan Wu",
                "Wenhan Luo",
                "Wei Liu"
            ],
            "title": "Learning to compose dynamic tree structures for visual contexts",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Kaihua Tang",
                "Yulei Niu",
                "Jianqiang Huang",
                "Jiaxin Shi",
                "Hanwang Zhang"
            ],
            "title": "Unbiased scene graph generation from biased training",
            "year": 2020
        },
        {
            "authors": [
                "Yidong Wang",
                "Hao Chen",
                "Qiang Heng",
                "Wenxin Hou",
                "Marios Savvides",
                "Takahiro Shinozaki",
                "Bhiksha Raj",
                "Zhen Wu",
                "Jindong Wang"
            ],
            "title": "Freematch: Self-adaptive thresholding for semi-supervised learning",
            "venue": "arXiv preprint arXiv:2205.07246,",
            "year": 2022
        },
        {
            "authors": [
                "Chen Wei",
                "Kihyuk Sohn",
                "Clayton Mellina",
                "Alan Yuille",
                "Fan Yang"
            ],
            "title": "Crest: A classrebalancing self-training framework for imbalanced semi-supervised learning",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2021
        },
        {
            "authors": [
                "Qizhe Xie",
                "Zihang Dai",
                "Eduard Hovy",
                "Thang Luong",
                "Quoc Le"
            ],
            "title": "Unsupervised data augmentation for consistency training",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Qizhe Xie",
                "Minh-Thang Luong",
                "Eduard Hovy",
                "Quoc V Le"
            ],
            "title": "Self-training with noisy student improves imagenet classification",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Saining Xie",
                "Ross Girshick",
                "Piotr Doll\u00e1r",
                "Zhuowen Tu",
                "Kaiming He"
            ],
            "title": "Aggregated residual transformations for deep neural networks",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Danfei Xu",
                "Yuke Zhu",
                "Christopher B Choy",
                "Li Fei-Fei"
            ],
            "title": "Scene graph generation by iterative message passing",
            "year": 2017
        },
        {
            "authors": [
                "Shaotian Yan",
                "Chen Shen",
                "Zhongming Jin",
                "Jianqiang Huang",
                "Rongxin Jiang",
                "Yaowu Chen",
                "XianSheng Hua"
            ],
            "title": "Pcpl: Predicate-correlation perception learning for unbiased scene graph generation",
            "venue": "In Proceedings of the 28th ACM International Conference on Multimedia,",
            "year": 2020
        },
        {
            "authors": [
                "Jianwei Yang",
                "Jiasen Lu",
                "Stefan Lee",
                "Dhruv Batra",
                "Devi Parikh"
            ],
            "title": "Graph r-cnn for scene graph generation",
            "year": 2018
        },
        {
            "authors": [
                "Lei Shang. Dash"
            ],
            "title": "Semisupervised learning with dynamic thresholding",
            "venue": "In Proceedings of the 38 th International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Kanghoon Yoon",
                "Kibum Kim",
                "Jinyoung Moon",
                "Chanyoung Park"
            ],
            "title": "Unbiased heterogeneous scene graph generation with relation-aware message passing neural network",
            "year": 2023
        },
        {
            "authors": [
                "Alireza Zareian",
                "Svebor Karaman",
                "Shih-Fu Chang"
            ],
            "title": "Bridging knowledge graphs to generate scene graphs",
            "venue": "In Computer Vision\u2013ECCV 2020: 16th European Conference,",
            "year": 2020
        },
        {
            "authors": [
                "Rowan Zellers",
                "Mark Yatskar",
                "Sam Thomson",
                "Yejin Choi"
            ],
            "title": "Neural motifs: Scene graph parsing with global context",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Ji Zhang",
                "Kevin J Shih",
                "Ahmed Elgammal",
                "Andrew Tao",
                "Bryan Catanzaro"
            ],
            "title": "Graphical contrastive losses for scene graph parsing",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Chaofan Zheng",
                "Xinyu Lyu",
                "Lianli Gao",
                "Bo Dai",
                "Jingkuan Song"
            ],
            "title": "Prototype-based embedding network for scene graph generation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2023
        },
        {
            "authors": [
                "Fixmatch (Kihyuk Sohn"
            ],
            "title": "2020) employ a pre-defined constant threshold during training. However, these approaches fail to consider the learning state of a model, aggravating the confirmation bias as the model predicts with higher confidence at the later part of the training step. To resolve the issue, Dash (Yi Xu, 2021) presents an adaptive threshold that is decreased as the iteration goes on, and Flexmatch (Bowen Zhang, 2021) employs a curriculum learning approach to reflect the learning",
            "year": 2021
        },
        {
            "authors": [
                "Lai"
            ],
            "title": "2021) assume the situation with an extremely imbalanced class distribution in which the model is prone to a severe confirmation bias. They present thresholding techniques, which change the threshold according to the number of assigned pseudo-labels",
            "year": 2021
        },
        {
            "authors": [
                "Li"
            ],
            "title": "SGG approaches for addressing the biased prediction issue. One line of research aims to alleviate the long-tailed predicate distribution problem",
            "year": 2022
        },
        {
            "authors": [
                "Additionally",
                "Tang"
            ],
            "title": "2022) uses the transfer learning approaches to mitigate the bias towards general predicates",
            "venue": "Most recently,",
            "year": 2022
        },
        {
            "authors": [
                "Xie"
            ],
            "title": "2020a): \u03c4 = \u03c4 con is a pre-defined constant threshold used in Fixmatch (Kihyuk Sohn, 2020). Due to the fact that the SGG model predicts the bg class with significantly higher confidence than the remaining classes, setting a global threshold such as \u03c4 con = 0.95 would fail. Thus, we set the threshold as the top-1% confidence among the confidences for all classes computed on the validation",
            "year": 2020
        },
        {
            "authors": [
                "Chua"
            ],
            "title": "2022) addressing the long-tailed distribution focus on enhancing the performance",
            "year": 2022
        },
        {
            "authors": [
                "G predicates"
            ],
            "title": "LIMITATION & FUTURE WORK Although our current work (including existing SGG models) focuses on utilizing benchmark scene graph datasets, ST-SGG has a potential to leverage external resources. For instance, ST-SGG can benefit from leveraging localization datasets",
            "venue": "(Lin et al.,",
            "year": 2014
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Scene graph generation (SGG) is a task designed to provide a structured understanding of a scene by transforming a scene of an image into a compositional representation that consists of multiple triplets in the form of \u27e8subject, predicate, object\u27e9. Existing SGG methods have faced challenges due to inherent issues in the benchmark datasets (Krishna et al., 2017; Cewu Lu & Fei-Fei, 2016), such as the long-tailed predicate distribution and missing annotations for predicates (see Fig. 1). Specifically, the long-tailed predicate distribution in SGG refers to a distribution in which general predicates (e.g. \u201con\u201d) frequently appear, while fine-grained predicates (e.g. \u201cwalking in\u201d) are rarely present. Owing to such a long-tailed predicate distribution inherent in the dataset, existing SGG models tend to make accurate predictions for general predicates, while making incorrect predictions for fine-grained predicates. However, scene graphs primarily composed of general predicates are less informative in depicting a scene, which in turn diminishes their utility across a range of SGG downstream applications. Besides the challenge posed by the long-tailed predicate distribution, benchmark scene graph datasets (e.g., Visual Genome (VG) (Krishna et al., 2017)) encounter the problem of missing annotations. Specifically, missing annotations provide incorrect supervision (H. Zhang & Savvides) to SGG models as unannotated triplets are carelessly assigned to the background class (i.e., bg), even though some unannotated triplets should have been indeed annotated with another class (See Fig. 1(a)). For example, treating the missing annotation between person and sidewalk as bg may confuse SGG models that are already trained with a triplet \u27e8person,walking in, sidewalk\u27e9. In addition, the prevalence of triplets with the bg predicates between person and sidewalk throughout the dataset would exacerbate the problem. Addressing the missing annotation problem is especially important as a large of volume of benchmark scene graph datasets includes the bg class (e.g., 95.5% of the triplets are annotated with the bg class in VG dataset.\nTo alleviate the challenge posed by the long-tailed predicate distribution, recent SGG methods have presented resampling (Desai et al., 2021; Li et al., 2021), reweighting (Yan et al., 2020; Lyu et al., 2022), and various debiasing strategies (Tang et al., 2020; Guo et al., 2021; Chiou et al., 2021; Dong et al., 2022). However, these approaches simply adjust the proportion of the training\ndata or the weight of the training objective without explicitly increasing the diversity of the data, which eventually leads to the model\u2019s overfitting to minority classes. IE-Trans (Ao Zhang & Chua, 2022) is the most recent method proposed to increase the variation of the training data, either by replacing general predicates (i.e., the majority) with the fine-grained predicates (i.e., the minority) or by filling in missing annotations with fine-grained predicates. This study demonstrates that discovering informative triplets that have not been annotated helps alleviate the problem caused by the long-tailed predicate distribution, and improves the model\u2019s generalization performance. However, IE-trans involves many incorrect pseudo-labels in the training process as it relies solely on the initial parameters of the pre-trained model to generate pseudo-labels of the entire dataset in a one-shot manner. Hence, IE-trans fails to fully exploit the true labels of unannotated triplets.\nIn this paper, we aim to effectively utilize the unannotated triplets in benchmark scene graph datasets by assigning them accurate pseudo-labels. To this end, we introduce a self-training framework for SGG, called ST-SGG, which assigns pseudo-labels to confident predictions among unannotated triplets, and iteratively trains the SGG model based on them (Fig. 1.(b)). ST-SGG reduces the number of incorrect pseudo-labels by iteratively updating both the pseudo-labels and the SGG model at every batch step. This in turn mutually enhances the quality of pseudo-labels and the SGG model, which results in an effective use of unannotated triplets. While there has been significant progress in self-training for image recognition (Kihyuk Sohn, 2020; Xie et al., 2020b; Lee et al., 2013; Yi Xu, 2021), designing a self-training framework for the SGG task is more challenging due to the uniqueness of the SGG nature, where the following factors that need to be considered when setting a proper threshold that determines confident predictions for unannotated triplets: \u2022 Semantic Ambiguity of Predicate Classes: A group of predicate classes in benchmark scene\ngraph datasets are highly related, e.g., \u201con\u201d, \u201cstanding on\u201d and \u201cwalking on,\u201d which share similar semantics. This implies that the prediction probabilities for predicate classes are not sharpened for a specific predicate class (Fig. 1.(b)), and thus setting a proper threshold for determining a confident prediction is non-trivial. This problem becomes severe due to the fact that unannotated triplets in benchmark SGG datasets are annotated with the bg class, and this accounts for 95.5% of the entire triplets in VG dataset, resulting in SGG models to inevitably produce highly confident predictions for the bg class. Hence, pseudo-labeling methods used for image recognition (e.g., FixMatch (Kihyuk Sohn, 2020)) that assign the same threshold (e.g., 0.95) for all classes is not suitable for the SGG model (as demonstrated by Motif-\u03c4 con in Sec. 3.3). Therefore, it is crucial to develop a pseudo-labeling strategy specifically designed for the SGG task, which is capable of autonomously determining the criteria for how reliable its model predictions are for each class, i.e., class-specific adaptive threshold.\n\u2022 Long-tailed Predicate Distribution: The long-tailed nature of benchmark scene graph datasets poses a challenge in designing a self-training framework for SGG models. More precisely, an SGG model trained on long-tailed datasets tends to mainly generate highly confident pseudolabels from the majority classes, and retraining the model with them would further exacerbate the bias towards the majority classes, causing self-training to fail (as demonstrated by Fig. 2 of Sec. 3.3). Therefore, for the SGG task, it is crucial to develop a pseudo-labeling strategy for selftraining that assigns accurate pseudo-labels to the minority classes, while preventing bias towards the majority classes.\nTo cope with the above challenges of pseudo-labeling in SGG, we propose a novel thresholding strategy for the SGG task, called class-specific adaptive thresholding with momentum (CATM), which adaptively adjusts the threshold by considering not only the bias towards majority classes, but also the highly related predicates and the presence of the bg class. In Sec. 3.3, we empirically demonstrate that the above factors are indeed mandatory for designing a self-training framework for SGG. It is worth noting that ST-SGG is a model-agnostic framework that can be applied\nto any existing SGG models including resampling and reweighting methods such as (Li et al., 2021) and (Sotiris Kotsiantis, 2006).\nAdditionally, we particularly focus on building upon message-passing neural networks (MPNN)based SGG models, as they have recently shown to be the state-of-the-art (Li et al., 2021; Yoon et al., 2023). MPNN-based SGG models aim to learn structural relationships among entities based on the message propagation on a scene graph, which is shown to improve the representation quality of entities and relationships. We discovered that enriching the given scene graph structure, i.e., adding missing edges and removing noisy edges, is beneficial when adopting our proposed self-training framework to MPNN-based SGG models, particularly when setting the class-specific thresholds for pseudo-labeling unannotated triplets. Hence, we devise a graph structure learner (GSL) that learns to enrich the given scene graph structure, and incorporate it into our MPNN-based ST-SGG framework, which allows the messages to be propagated based on the enriched structure learned by the GSL. We show that this results in decreasing the model confidence on the bg class while increasing that on the other classes, both of which are helpful for setting the class-specific threshold. Through extensive experiments on VG and Open Images V6 (OI-V6), we verify that ST-SGG is effective when applied to existing SGG models, particularly enhancing the performance on fine-grained predicate classes.\n\u2022 To the best of our knowledge, this is the first work that adopts self-training for SGG, which is challenging due to the difficulty in setting the threshold for pseudo-labeling in the SGG nature.\n\u2022 We develop a novel thresholding technique CATM and the graph structure learner (GSL), which are beneficial for setting thresholds when ST-SGG is applied to any SGG models, e.g., Motif (Zellers et al., 2018) and VCTree (Tang et al., 2019), and MPNN-based models, e.g., BGNN (Li et al., 2021) and HetSGG (Yoon et al., 2023), respectively.\n\u2022 Through extensive experiments on VG and OI-V6, we verify the effectiveness of ST-SGG compared with the state-of-the-art debiasing methods, particularly enhancing the performance on fine-grained predicate classes."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "In this section, we briefly describe the related works and present the focus of our research. Please see Appendix. A for the extended explanation of related works.\nSelf-Training. In the field of image recognition, self-training is one of the prominent semisupervised approaches that utilize a large amount of unlabeled samples. The main idea is to assign pseudo-labels to unlabeled samples whose confidence is above a specified threshold, and use them for model training to improve the generalization performance of the model. Recent works have focused on reliving the confirmation bias (Arazo et al., 2020) arising from using incorrect pseudo-labels for model training, which in turn undesirably increases the confidence of incorrect predictions. Hence, numerous confidence-based thresholding techniques are proposed to accurately assign pseudo-labels to unlabeled samples (Xie et al., 2020a; Kihyuk Sohn, 2020; Yi Xu, 2021; Bowen Zhang, 2021; Lai et al., 2022; Guo et al., 2021; Wei et al., 2021). However, none of the above methods addresses the unique challenges of applying a self-training framework in the context of SGG (we will discuss more details of the challenges in Sec. 3.3). In this work, we propose a novel thresholding technique that considers the inherent nature of SGG, where the semantic ambiguity of predicates and the bias caused by long-tailed distribution are present.\nScene Graph Generation. Existing SGG methods point out that SGG models accurately predict general predicates, while rarely making correct predictions for fine-grained predicates. Desai et al. (2021); Li et al. (2021) and Desai et al. (2021); Li et al. (2021) aim to alleviate the long-tailed predicate distribution problem by using resampling and reweighting, respectively. Several debiasing methods are presented by Tang et al. (2020); Dong et al. (2022); Guo et al. (2021); Chiou et al. (2021). Most recently, IE-trans (Ao Zhang & Chua, 2022) proposed to replace general predicates with fine-grained predicates and fill in missing annotations with fine-grained predicates to mitigate the problem caused by the long-tailed predicate distribution. However, IE-trans fails to fully exploit the true labels of unannotated triplets as it uses the initial model parameter of the pre-trained model to generate pseudo-labels. On the other hand, we propose a novel pseudo-labeling technique for SGG to effectively utilize the unannotated triplets, which helps reduce the number of incorrect pseudo-labels by iteratively updating both the pseudo-labels and the SGG model. Further, we\nextend ST-SGG for message-passing neural network (MPNN)-based SGG models (Zellers et al., 2018; Tang et al., 2019; Xu et al., 2017; Yang et al., 2018; Lin et al., 2020; Li et al., 2021; Yoon et al., 2023; Suprosanna Shit, 2022), which are considered as state-of-the-art SGG models utilizing the advanced SGG architecture.\n3 SELF-TRAINING FRAMEWORK FOR SGG (ST-SGG)"
        },
        {
            "heading": "3.1 PRELIMINARIES",
            "text": "Notations. Given an image I, a scene graph G is represented as a set of triplets {(si,pi,oi)}Mi=1, where M is the number of triplets in G. A subject si is associated with a class label si,c \u2208 Ce and a bounding box position si,b \u2208 R4, where Ce is the set of possible classes for an entity. Likewise, an object oi is associated with oi,c \u2208 Ce and oi,b \u2208 R4. A predicate pi denotes the relationship between si and oi, and it is associated with a class label pi,c \u2208 Cp, where Cp is the set of possible classes for predicates. Note that Cp includes the \u201cbackground\u201d class, which represents there exists no relationship between si and oi (i.e., pi,c = bg).\nSGG Task. Our goal is to train an SGG model f\u03b8 : I \u2192 G, which generates a scene graph G from an image I. Generally, an SGG model first generates entity and relation proposals using a pretrained object detector such as Faster R-CNN (Ren et al., 2015). Specifically, an entity proposal xe is represented by the output of feedforward network that takes a bounding box, its visual feature, and the word embedding of the entity class. The predicted probability of the entity class p\u0302e \u2208 R|Ce| is estimated by an entity classifier fe\u03b8 (i.e., p\u0302 e = fe\u03b8 (x e)). Moreover, a relation proposal xs,o between two entities (i.e., subject s and object o) is represented by the union box of the visual feature of the two entity proposals. The predicted probability of the predicate class p\u0302p \u2208 R|Cp| between two entities is estimated by a predicate classifier fp\u03b8 (i.e., p\u0302 p = fp\u03b8 (x s,o)). Finally, the SGG model selects the most probable triplets {(si,pi,oi)}Mi=1 as a generated scene graph.\n3.2 PROBLEM FORMULATION OF ST-SGG\nWe introduce a self-training framework for SGG (ST-SGG), aiming to exploit a large volume of unannotated triplets, which is formulated under the setting of semi-supervised learning.\nGiven B batches of scene graphs {G1, ...,Gb, ...,GB}, each batch Gb contains a set of annotated triplets GAb = {(si,pi,oi)|pi \u0338= bg}, and a set of unannotated triplets GUb = {(si,pi,oi)|pi = bg}. Note that only a small portion of triplets is annotated, i.e., |GUb | \u226b |GAb |. For example, only 4.5% of relationships between objects are annotated in the VG dataset (Krishna et al., 2017).\nTo exploit the unannotated triplets, we adopt a self-training approach for SGG that assigns pseudolabels to predicates in the unannotated triplets. Given a model prediction p\u0302p \u2208 R|Cp|, we define a confidence q\u0302 of the model prediction by the maximum value of p\u0302p, i.e., q\u0302 = max(p\u0302p) \u2208 R, and denote the corresponding predicate class as q\u0303 \u2208 R|Cp|, which is a one-hot vector. We assign q\u0303 to predicates in the set of unannotated triplets GUb if the model confidence is greater than a threshold \u03c4 , and otherwise leave them as bg. Given the labeled triplets in GAb and pseudo-labeled triplets in GUb , we retrain the SGG model at every batch step (i.e., iteration). Formally, the loss for training the SGG model under the self-training framework is divided into the following three losses:\nL = E1\u2264b\u2264B Ei\u2208GAb [\u2212pi \u00b7 log(p\u0302pi )]\ufe38 \ufe37\ufe37 \ufe38 Loss for annotated predicates +Ei\u2208GU b |q\u0302i<\u03c4 [ \u2212pbgi \u00b7 log(p\u0302 p i ) ] \ufe38 \ufe37\ufe37 \ufe38 Loss for bg class +\u03b2 Ei\u2208GU b |q\u0302i\u2265\u03c4 [\u2212q\u0303i \u00b7 log(p\u0302 p i )]\ufe38 \ufe37\ufe37 \ufe38 Loss for pseudo-labeled predicates  , (1) where \u03b2 is a coefficient for controlling the weight of the pseudo-label loss, and pbg \u2208 R|Cp| is a onehot vector that represents the bg class. Despite the fact that there may exist true relationships rather than bg in the unannotated triplets, most existing SGG models simply consider all the unannotated triplets as belonging to the bg class, which means the third loss is merged with the second loss (with \u03b2 = 1) in Equation 1. It is important to note that ST-SGG is a model-agnostic framework that can be applied to any existing SGG models."
        },
        {
            "heading": "3.3 CHALLENGES OF SELF-TRAINING FOR SGG",
            "text": "Training an SGG model based on Equation 1 facilitates the self-training of the model by exploiting unannotated triplets with pseudo-labels. However, unless the threshold \u03c4 is carefully set, SGG models deteriorate since they are prone to confirmation bias, which is caused by confidently assigning incorrect pseudo-labels to unannotated triplets. In Fig. 2, we empirically investigate the effectiveness of various thresholding techniques by applying them to ST-SGG. Specifically, we first pre-train Motif (Zellers et al., 2018) on the VG dataset, and subsequently fine-tune it using Equation 1 while employing various thresholding techniques including constant thresholding (Kihyuk Sohn, 2020) (i.e., \u03c4 = \u03c4 con), fixed class-specific thresholding (i.e., \u03c4 = \u03c4 clsc ), class frequency-weighted fixed class-specific thresholding (Wei et al., 2021) that considers the long-tailed distribution (i.e., \u03c4 = \u03c4 ltc ), and class-specific adaptive thresholding (Yi Xu, 2021) (i.e., \u03c4 = \u03c4 ada c ). Please refer to Appendix B.2 for details on the thresholding techniques.\nWe have the following three observations: 1) Motif-\u03c4 con, Motif-\u03c4 clsc and Motif-\u03c4 ltc show lower performance than even Motif-vanilla (without self-training) over all metrics, failing to benefit from the unannotated triplets through self-training. This implies that thresholding techniques that are widely\nused in image recognition are not suitable for ST-SGG. We argue that this is mainly due to highly related predicate classes, the presence of bg class, and the long-tailed distribution. As these problems co-exist, determining an appropriate threshold becomes difficult when adopting self-training to SGG models. 2) In Fig. 2(b), we further report the behavior of Motif-\u03c4 clsc to investigate the reason for its failure. We observe that as the training progresses, the number of majority predicate classes being assigned as pseudo-labels increases, while minority predicate classes are not assigned at all. This implies that although we set the different thresholds for the majority and minority predicate classes to address the long-tailed problem, it is non-trivial due to the fact that other issues, such as the semantic ambiguity of predicate classes and the presence of the bg classs, are entangled with the setting of a proper threshold in the nature of SGG. We observe similar behaviors for Motif-\u03c4 con and Motif-\u03c4 ltc . 3) The performance of Motif-\u03c4 adac decreases at the early stage of training, but its final performance is similar to that of Motif-Vanilla at the later stage. We attribute this to the incorrect pseudo-label assignment in the early training steps and the saturation of the adaptive threshold in the later training steps, which prevents the model from further assigning any pseudo-labels. This implies that a naive implementation of the class-specific adaptive thresholding technique fails to fully exploit the unannotated triplets in self-training.\nIn summary, it is challenging to determine an appropriate threshold for adopting ST-SGG due to the inherent nature of SGG, mainly incurred by the semantic ambiguity of predicates and the long-tailed predicate distribution.\n4 CLASS-SPECIFIC ADAPTIVE THRESHOLDING WITH MOMENTUM (CATM)\nTo address the aforementioned challenges of applying self-training to the SGG task, we devise a novel pseudo-labeling technique for SGG, called Class-specific Adaptive Thresholding with Momentum (CATM), which adaptively adjusts the class-specific threshold by considering not only the long-tailed predicate distribution but also the model\u2019s learning state of each predicate class."
        },
        {
            "heading": "4.1 CLASS-SPECIFIC ADAPTIVE THRESHOLDING",
            "text": "Our approach to adjusting the threshold involves using the model\u2019s confidence of the predicate prediction on unannotated triplets, as these predictions reflect the per class learning state of the model. We argue that relying on the model prediction to determine the threshold has two advantages. First, it addresses the long-tailed nature of predicates that causes significant differences in learning states across predicate classes. Second, although model prediction probabilities for a certain predicate\nmay not be sharpened (or confident) due to the semantic ambiguity of predicate classes, it enables us to set an appropriate threshold value based on the model predictions on other instances of the same predicate class. A straightforward approach to estimating the model prediction-based threshold would be to compute the average of the model\u2019s confidence in the validation set, and set it as the threshold at every iteration or at a regular iteration interval. However, it requires a significant computational cost, and the learning state of the model between iteration intervals can not be reflected (Refer to Appendix C.2).\nTo this end, we employ the exponential moving average (EMA) (Wang et al., 2022) to adjust the threshold at each iteration, in which the threshold is either increased or decreased depending on the confidence of the prediction and the threshold at the previous iteration. More formally, we use \u03c4 tc to denote the threshold of a predicate class c at iteration t, and use PUc to denote the set of predicates in unannotated triplets in all B batches, i.e., {GU1 , ...,GUb , ...,GUB}, that are predicted as belonging to the predicate class c. Our main goal is to assign pseudo-labels to unannotated triplets while filtering out incorrect pseudo-labels. Specifically, we increase the threshold \u03c4 tc for a given a predicate class c, if the confidence q\u0302 of the prediction is greater than the previous threshold \u03c4 t\u22121c (See the first condition in Equation 2). On the other hand, if the confidence q\u0302 for all instances of the predicate class c is lower than the previous threshold \u03c4 t\u22121c , we decrease the threshold for the predicate class c, since the current threshold \u03c4 tc is likely to be over-estimated (See the second condition in Equation 2). The threshold of the predicate class c at iteration t, i.e., \u03c4 tc , is updated based on EMA as follows:\n\u03c4 tc =  (1\u2212 \u03bbinc) \u00b7 \u03c4 t\u22121c + \u03bbinc \u00b7 Ei\u2208PUc [q\u0302i] , if \u2203i \u2208 P U c where q\u0302i \u2265 \u03c4 t\u22121c .(\u03c4 t\u22121c increases) (1\u2212 \u03bbdec) \u00b7 \u03c4 t\u22121c + \u03bbdec \u00b7 Ei\u2208PUc [q\u0302i] , if q\u0302i < \u03c4 t\u22121 c for all i \u2208 PUc . (\u03c4 t\u22121c decreases)\n\u03c4 t\u22121c , if PUc = \u2205. (\u03c4 t\u22121c remains)\n(2)\nwhere \u03bbinc and \u03bbdec are the momentum coefficients for increasing and decreasing the threshold, respectively. Note that \u03c4 tc is updated only if there exists at least one predicate that is predicted as belonging to the predicate class c in any of the batches, and otherwise it is maintained (See the third condition in Equation 2).\nAs the class-specific threshold reflects the learning state of the model for each predicate class at each iteration, the above EMA approach that adaptively adjusts the class-specific threshold captures the differences in the learning states of the model across the predicate classes. This in turn allows the self-training framework to be applied in the SGG nature. Moreover, the EMA approach is superior to the naive method as it establishes a more stable threshold by considering the model\u2019s learning state at every iteration and the accumulated confidence in samples. It is also computationally efficient and does not require computing the confidence on additional datasets, such as a validation set."
        },
        {
            "heading": "4.2 CLASS-SPECIFIC MOMENTUM",
            "text": "However, we observed that if the same momentum hyperparameter is used for all predicate classes (i.e., setting the same \u03bbinc and \u03bbdec for all c \u2208 Cp), unannotated triplets are mainly pseudo-labeled with majority predicate classes, whereas minority predicate classes receive less attention (See Fig. 9(a) in Appendix E.7). We argue that this incurs confirmation bias and aggravates the problem of long-tailed predicate distribution in SGG. To cope with this challenge, we design the class-specific momentum, which sets different \u03bbincc and \u03bb dec c for each c \u2208 Cp based on the frequency of predicate classes in the training set. The main idea is to increase the threshold of majority predicate classes more rapidly than that of minority predicate classes so that unannotated triplets are pseudo-labeled with minority predicate classes as the training progresses. Conversely, when decreasing the threshold, we want the threshold of majority predicate classes to decrease more slowly than that of minority predicate classes. More formally, let N1, ..., N|Cp| be the number of instances that belong to the predicate classes c1, ...c|Cp| in the training set, respectively, which is sorted in descending order (i.e., N1 > N2 > ... > N|Cp|). We set the increasing momentum\ncoefficient and decreasing momentum coefficient as \u03bbincc = ( Nc N1 )\u03b1inc , \u03bbdecc = ( N(|Cp|+1\u2212c) N1 )\u03b1dec , where \u03b1inc, \u03b1dec \u2208 [0, 1] control the increasing and decreasing rate, respectively, and NcN1 and N(|Cp|+1\u2212c)\nN1 are the imbalance ratio and reverse imbalance ratio of class c. For example, assume that we are given three classes with [N1, N2, N3] = [50, 40, 10] where class 1 and 2 are head predicate classes, and class 3 is a tail predicate class. In this case, the imbalance ratio is [1.0, 0.8, 0.2], and the reverse imbalance ratio is [0.2, 0.8, 1.0]. Hence, the increasing rates of class 1 and class 3 are\n\u03bbinc1 = (1.0) \u03b1inc and \u03bbinc3 = (0.2) \u03b1inc , respectively, which implies that the threshold of class 1 increases more rapidly than that of class 3. On the other hand, the decreasing rates of class 1 and class 3 are \u03bbdec1 = (0.2) \u03b1dec and \u03bbinc3 = (1.0) \u03b1dec , respectively, which implies that the threshold of class 1 decreases more slowly than that of class 3. Please refer to Fig. 10 of Appendix E.8 regarding the change of \u03bbincc and \u03bb dec c based on \u03b1\ninc and \u03b1dec. As a result of setting the class-specific momentum coefficients, a small number of pseudo-labels are being assigned to head predicate classes, while tail predicate classes are assigned more actively (See Fig. 9(b) of Appendix E.7), which relieves the long-tailed the problem of predicates. We believe that pseudo-labeling the unannotated triplets with more tail predicate classes than head predicate classes is acceptable since the original training set already contains a large volume of instances that belong to head predicate classes."
        },
        {
            "heading": "4.3 GRAPH STRUCTURE LEARNER FOR CONFIDENT PSEUDO-LABELS",
            "text": "We discovered that enriching the given scene graph structure, i.e., adding missing edges and removing noisy edges, is beneficial when adopting our self-training framework to MPNN-based SGG models. In Fig. 3, we report the confidence of predicate classes after training BGNN (Li et al., 2021), a SOTA MPNN-based SGG method, on the fully connected scene graph (i.e., BGNN w/o GSL) and on a scene graph whose structure\nis enriched by a graph structure learner (i.e., BGNN w/ GSL). A graph structure learner (GSL) is a method to learn relevant and irrelevant relations between entities in a scene graph, allowing MPNN-based methods to propagate messages only through relevant relations. We observe that BGNN with GSL generates relatively low confident predictions on the \u201cbg\u201d class, allowing the model to make more confident predictions on the remaining predicate classes, which is particularly beneficial when setting the class-specific thresholds for pseudo-labeling unannotated triplets. To this end, we devise a GSL that learns to enrich the given scene graph structure, and incorporate it into our MPNN-based ST-SGG framework. Please refer to Appendix D for more details. Note that we consider only the relevant relations predicted by GSL as candidates for pseudo-labeling."
        },
        {
            "heading": "5 EXPERIMENT",
            "text": "We compare ST-SGG with state-of-the-arts methods that alleviate the long-tailed problem on commonly used benchmark datasets, VG and OI-V6. We report only the result on VG due to the page limit. Please refer to the result on OI-V6 in Appendix F.3. More details of the experimental setups are described in Appendix E and F. Note that ST-SGG involves CATM by default."
        },
        {
            "heading": "5.1 COMPARISON WITH BASELINES ON VISUAL GENOME",
            "text": "In Table 1, we apply ST-SGG to widely-used SGG models, such as Motif (Zellers et al., 2018) and VCTree (Tang et al., 2019), and compare them with baselines. Based on the result, we have the following observations: 1) ST-SGG exhibits model-agnostic adaptability to SGG models. Motif+ST-SGG and VCTree+ST-SGG improve their performance in terms of mR@K and F@K, implying that ST-SGG greatly increases the performance on tail predicates while retaining that of head predicates. 2) ST-SGG that employs debiasing methods is competitive with the-state-of-art SGG models. Specifically, we employ resampling and I-Trans (Ao Zhang & Chua, 2022) to Motif under ST-SGG. In terms of F@K, Motif+Resamp.+ST-SGG and Motif+I-Trans+ST-SGG outperform the performance of DT2-ACBS and PCPL in SGCls and SGDet despite the simple architecture of Motif. This implies that utilizing unannotated triplets with simple debiasing methods is powerful without advancing the architecture of SGG models. 3) Compared to a previous pseudo-labeling method, i.e., IE-Trans, ST-SGG+I-Trans achieves better performance in terms of mR@K and F@K. This demonstrates that ST-SGG assigns accurate pseudo-labels to unannotated triplets, and relieves the long-tailed problem. We attribute this to the fact that pseudo-labeling in an iterative manner is more accurate than pseudo-labeling in a one-shot manner.\nTable 1: Performance (%) comparison of ST-SGG with state-of-the art SGG models on three tasks. Resam. denotes the re-sampling (Li et al., 2021) method. The best performance is shown in bold.\nPredCls SGCls SGDetMethod R@50 / 100 mR@50 / 100 F@50 / 100 R@50 / 100 mR@50 / 100 F@50 / 100 R@50 / 100 mR@50 / 100 F@50 / 100 DT2-ACBS (Desai et al., 2021) 23.3 / 25.6 35.9 / 39.7 28.3 / 31.1 16.2 / 17.6 24.8 / 27.5 19.6 / 21.5 15.0 / 16.3 22.0 / 24.0 17.8 / 19.4 PCPL (Yan et al., 2020) 50.8 / 52.6 35.2 / 37.8 41.6 / 44.0 27.6 / 28.4 18.6 / 19.6 22.2 / 23.2 14.6 / 18.6 9.5 / 11.7 11.5 / 14.4 KERN (Chen et al., 2019) 65.8 / 67.6 17.7 / 19.2 27.9 / 29.9 36.7 / 37.4 9.4 / 10.0 15.0 / 15.8 27.1 / 29.8 6.4 / 7.3 10.4 / 11.7\nSp ec\nifi c\nGBNet (Zareian et al., 2020) 66.6 / 68.2 22.1 / 24.0 33.2 / 35.5 37.3 / 38.0 12.7 / 13.4 18.9 / 19.8 26.3 / 29.9 7.1 / 8.5 11.2 / 13.2 PE-Net (Zheng et al., 2023) 64.9 / 67.2 31.5 / 33.8 42.4 / 45.0 39.4 / 40.7 17.8 / 18.9 24.5 / 25.8 30.7 / 35.2 12.4 / 14.5 17.7 / 20.5 Motif (Zellers et al., 2018) 65.3 / 67.1 17.8 / 19.2 28.0 / 29.9 36.9 / 38.1 9.0 / 9.6 14.5 / 15.3 31.9 / 36.4 6.4 / 7.6 10.7 / 12.6 +ST-SGG 63.4 / 65.4 22.4 / 24.1 33.1 / 35.2 36.8 / 37.8 12.1 / 12.8 18.2 / 19.1 29.7 / 34.8 8.5 / 10.1 13.2 / 15.7 +Resam. (Li et al., 2021) 62.3 / 64.3 26.1 / 28.5 36.8 / 39.5 36.1 / 37.0 13.7 / 14.7 19.9 / 21.0 30.4 / 34.8 10.5 / 12.3 15.6 / 18.2 +Resam.+ST-SGG 53.9 / 57.7 28.1 / 31.5 36.9 / 40.8 33.4 / 34.9 16.9 / 18.0 22.4 / 23.8 26.7 / 30.7 11.6 / 14.2 16.2 / 19.4 +TDE (Tang et al., 2020) 46.2 / 51.4 25.5 / 29.1 32.9 / 37.2 27.7 / 29.9 13.1 / 14.9 17.8 / 19.9 16.9 / 20.3 8.2 / 9.8 11.0 / 13.2 +DLFE (Chiou et al., 2021) 52.5 / 54.2 26.9 / 28.8 35.6 / 37.6 32.3 / 33.1 15.2 / 15.9 20.7 / 21.5 25.4 / 29.4 11.7 / 13.8 16.0 / 18.8 +GCL (Dong et al., 2022) 42.7 / 44.4 36.1 / 38.2 39.1 / 41.1 26.1 / 27.1 20.8 / 21.8 23.2 / 24.1 18.4 / 22.0 16.8 / 19.3 17.6 / 20.6 +NICE (Li et al., 2022) 55.1 / 57.2 29.9 / 32.3 38.8 / 41.3 33.1 / 34.0 16.6 / 17.9 22.1 / 23.5 27.8 / 31.8 12.2 / 14.4 17.0 / 19.8 +IE-Trans (Ao Zhang & Chua, 2022) 54.7 / 56.7 30.9 / 33.6 39.5 / 42.2 32.5 / 33.4 16.8 / 17.9 22.2 / 23.3 26.4 / 30.6 12.4 / 14.9 16.9 / 20.0 +I-Trans (Ao Zhang & Chua, 2022) 55.2 / 57.1 29.1 / 31.9 38.1 / 40.9 32.5 / 33.4 15.7 / 16.9 21.2 / 22.4 27.0 / 31.3 11.4 / 14.0 16.0 / 19.3 +I-Trans+ST-SGG 50.5 / 52.8 32.5 / 35.1 41.7 / 42.5 31.2 / 32.1 18.0 / 19.3 22.8 / 24.1 25.7 / 29.8 12.9 / 15.8 17.2 / 20.7 VCTree (Tang et al., 2019) 65.5 / 67.2 17.2 / 18.6 27.3 / 29.1 38.1 / 38.8 9.6 / 10.2 15.3 / 16.2 31.4 / 35.7 7.3 / 8.6 11.9 / 13.9 +ST-SGG 64.2 / 66.2 21.5 / 22.9 32.2 / 34.0 37.5 / 38.4 12.0 / 12.5 18.2 / 18.9 30.4 / 34.7 8.7 / 10.1 13.5 / 15.6 +Resam. (Li et al., 2021) 61.2 / 63.5 27.2 / 29.2 37.7 / 40.0 35.7 / 36.5 13.8 / 14.4 19.9 / 20.7 29.7 / 33.9 10.2 / 11.8 15.2 / 17.5 +Resam.+ST-SGG 54.0 / 57.0 32.2 / 34.6 40.3 / 43.0 32.2 / 33.4 16.9 / 18.3 22.2 / 23.6 24.6 / 29.6 12.3 / 14.8 16.4 / 19.7 +TDE (Tang et al., 2020) 47.2 / 51.6 25.4 / 28.7 33.0 / 36.9 25.4 / 27.9 12.2 / 14.0 16.5 / 18.6 19.4 / 23.2 9.3 / 11.1 12.6 / 15.0 +DLFE (Chiou et al., 2021) 51.8 / 53.5 25.3 / 27.1 34.0 / 36.0 33.5 / 34.6 18.9 / 20.0 24.2 / 25.3 22.7 / 26.3 11.8 / 13.8 15.5 / 18.1 +GCL (Dong et al., 2022) 40.7 / 42.7 37.1 / 39.1 38.8 / 40.8 27.7 / 28.7 22.5 / 23.5 24.8 / 25.8 17.4 / 20.7 15.2 / 17.5 16.2 / 19.0 +NICE (Li et al., 2022) 55.0 / 56.9 30.7 / 33.0 39.4 / 41.8 37.8 / 39.0 19.9 / 21.3 26.1 / 27.6 27.0 / 30.8 11.9 / 14.1 16.5 / 19.3 +IE-Trans (Ao Zhang & Chua, 2022) 53.0 / 55.0 30.3 / 33.9 38.6 / 41.9 32.9 / 33.8 16.5 / 18.1 22.0 / 23.6 25.4 / 29.3 11.5 / 14.0 15.8 / 18.9 +I-Trans (Ao Zhang & Chua, 2022) 54.0 / 55.9 30.2 / 33.1 38.7 / 41.6 37.2 / 38.3 19.0 / 20.6 25.1 / 26.8 25.5 / 29.4 11.2 / 13.7 15.6 / 18.7 M od el -A gn os tic\n+I-Trans+ST-SGG 52.5 / 54.3 32.7 / 35.6 40.3 / 43.0 36.3 / 37.3 21.0 / 22.4 26.6 / 27.9 20.7 / 24.9 12.6 / 15.1 15.7 / 18.8\nTable 2: Performance (%) comparison of ST-SGG with state-of-the art MPNN-based SGG models on three tasks. Resam. denotes the re-sampling (Li et al., 2021) method.\nPredCls SGCls SGDetMethod R@50 / 100 mR@50 / 100 F@50 / 100 R@50 / 100 mR@50 / 100 F@50 / 100 R@50 / 100 mR@50 / 100 F@50 / 100 BGNN+Resam. (Li et al., 2021) 57.8 / 60.0 29.2 / 31.7 38.8 / 41.5 36.9 / 38.1 14.6 / 16.0 20.9 / 22.5 30.0 / 34.7 11.4 / 13.3 16.5 /19.2 +ST-SGG 48.0 / 51.4 33.0 / 35.1 39.1 / 41.7 32.3 / 34.0 17.9 / 19.0 23.1 / 24.4 20.6 / 27.3 13.6 / 16.1 16.4 / 20.3 +ST-SGG+GSL 48.9 / 51.4 34.1 / 36.2 40.2 / 42.5 33.5 / 34.7 18.0 / 19.4 23.4 / 24.9 26.5 / 31.4 14.1 / 16.6 18.4 / 21.7 BGNN+IE-Trans (Ao Zhang & Chua, 2022) 54.5 / 56.6 29.7 / 32.4 38.4 / 41.2 33.2 / 34.1 16.3 / 17.6 21.9 / 23.2 24.2 / 27.9 11.2 / 13.7 15.3 / 18.4 BGNN+I-Trans (Ao Zhang & Chua, 2022) 54.9 / 57.0 28.7 / 31.6 37.7 / 40.7 33.5 / 34.5 15.5 / 16.9 21.2 / 22.7 24.2 / 28.1 10.5 / 13.2 14.6 / 18.0 BGNN+I-Trans+ST-SGG+GSL 52.7 / 54.7 31.5 / 34.5 39.4 / 42.3 31.4 / 32.3 17.6 / 18.9 22.6 / 23.9 22.6 / 26.2 12.0 / 14.6 15.7 / 18.8 HetSGG+Resam. (Yoon et al., 2023) 58.0 / 60.1 30.0 / 32.2 39.5 / 41.9 37.6 / 38.5 15.8 / 17.7 22.2 / 24.3 30.2 / 34.5 11.5 / 13.5 16.7 / 19.4 +ST-SGG 49.5 / 52.5 32.6 / 35.2 39.3 / 42.1 33.2 / 34.7 18.0 / 18.9 23.4 / 24.5 20.2 / 27.2 11.9 / 14.6 15.0 / 19.0 +ST-SGG+GSL 50.2 / 53.2 33.6 / 35.8 40.3 / 42.8 35.5 / 36.5 18.2 / 19.1 24.1 / 25.1 26.5 / 31.9 12.9 / 15.0 17.4 / 20.4 HetSGG+IE-Trans (Ao Zhang & Chua, 2022) 53.1 / 55.1 31.3 / 34.5 39.4 / 42.4 33.9 / 34.9 16.7 / 18.1 22.4 / 23.8 23.8 / 27.4 11.5 / 14.0 15.5 / 18.5 HetSGG+I-Trans (Ao Zhang & Chua, 2022) 52.1 / 54.1 30.8 / 34.2 38.7 / 41.9 33.0 / 33.9 16.3 / 17.7 21.8 / 23.3 23.9 / 27.6 10.7 / 13.7 14.8 / 18.3 M PN N -b as ed M od el\nHetSGG+I-Trans+ST-SGG+GSL 49.6 / 51.4 32.9 / 35.9 39.6 / 42.3 31.2 / 32.2 18.0 / 19.3 22.8 / 24.1 22.3 / 25.6 12.3 / 14.9 15.9 / 18.8\nMoreover, we investigate the result for each predicate. Fig. 4 shows that ST-SGG greatly improves the performance for the tail predicates while retaining that for the head predicates by pseudolabeling the unannotated triplet to a tail predicates. It is worth noting that predicting fine-grained predicates (i.e., tail predicates) is more\nimportant than predicting general predicates as tail predicates are informative in depicting a scene. In this regard, ST-SGG is an effective framework that generates informative scene graphs by utilizing unannotated triplets."
        },
        {
            "heading": "5.2 COMPARISON WITH MPNN-BASED MODELS ON VISUAL GENOME",
            "text": "Table 2 shows the result of the state-of-art MPNN-based SGG models. Herein, we apply the graph structure learner (GSL) to existing MPNN-based models including BGNN (Li et al., 2021) and HetSGG (Yoon et al., 2023). We have the following observations: 1) The mR@K and F@K of BGNN+ST-SGG and HetSGG+ST-SGG are increased compared to those of BGNN and HetSGG, respectively, which implies that ST-SGG effectively utilizes unannotated triplets when applied to MPNN-based SGG models. 2) When MPNN-based models employ ST-SGG+GSL, they achieve state-of-art performance among SGG models, outperforming the models that solely use ST-SGG. This implies that GSL identifies relevant relationships, and the relations to which pseudo-labels are to be assigned.\n5.3 ABLATION STUDY ON MODEL COMPONENTS OF ST-SGG\nIn Table 3, we ablate the component of ST-SGG to analyze the effect of each component. We select Motif and BGNN trained with resampling (Li et al., 2021) as the backbone SGG model, and train the following models for SGCls task. 1) ST-SGG w/o EMA: we remove the EMA of adaptive threshold, and apply the fixed threshold \u03c4 clsc used in Sec. 3.3. 2) ST-SGG w/o \u03bbincc , \u03bbdecc : we remove\nthe class-specific momentum by setting \u03bbincc and \u03bbdecc to 0.5. When the Motif backbone is used, we observed that the performance of STSGG w/o EMA severely decreases in terms of R@K and mR@K compared to Vanilla Motif since the fixed threshold cannot properly assign correct pseudo-labels. This implies that adaptively updating the threshold through EMA is important for ST-SGG. Beside, compared with ST-SGG, ST-SGG w/o \u03bbincc , \u03bbdecc shows\ninferior performance over all metrics. This degradation is due to the fact that removing class-specific momentum incurs bias to majority classes as shown in Fig. 9 of Appendix E.7. This implies that adjusting the threshold in a class-specific manner, which rapidly/slowly adjusts the threshold for head/tail predicates is important to alleviating the long-tailed problem. When the BGNN backbone is used, similar results are observed. Additionally, we confirmed that ST-SGG+GSL generally outperforms ST-SGG in terms of R@K, mR@K, and F@K on the BGNN backbone, implying that enriching the scene graph structure through GSL helps identify the relations to which pseudo-labels are to be assigned. We further analyze the effect of \u03b1inc and \u03b1dec Appendix E.8, and Appendix E.9.\n5.4 QUALITATIVE ANALYSIS ON CATM\nWe conducted the qualitative analysis to demonstrate the effectiveness of our proposed CATM in assigning accurate pseudo-labels. In Fig 5(a), we observed that the range of the class-specific threshold values defined by CATM is diverse. Interestingly, we found that the threshold is not related to the frequency of classes. This shows the difficulty of determining appropriate class-specific thresholds for pseudo-labeling, and we argue that this is the main reason of the failure of the thresholding techniques used in image recognition in the SGG task. Moreover, let us consider a triplet \u27e8motorcycle, ?, street\u27e9 shown in Fig. 5(b), which is a case of true background, i.e., it should be labeled with the bg class because the motorcycle in the blue box is far from the street in the green box. We observe that while ST-SGG predicts it correctly as bg, IE-Trans incorrectly predicts it as parked on. This is the due to the design of IE-Trans that tries to assign pseudo-labels to as many unannotated triplets as possible (Ao Zhang & Chua, 2022). Next, let us consider a triplet \u27e8bear, ?, snow\u27e9 shown in Fig. 5(c), which is a case of missing annotation, i.e., it should be labeled with a class other than bg because the bear in the blue box is close to the snow in the green box. We observe that IE-Trans initially predicts it as on, and then re-assigns it to bg, due to the behavior of IE-Trans that only pseudo-labels predicates belonging to the minority classes; as on belongs to the majority class, it is re-assigned to bg. On the other hand, ST-SGG predicts it correctly as walking in, which indicates that ST-SGG is a framework that effectively utilizes unannotated triplets by providing accurate pseudo-labels."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "Although self-training has shown advancements in computer vision applications like image recognition, its application in SGG has received limited attention. Simply adapting existing self-training methods for SGG is ineffective due to challenges posed by the semantic ambiguity and the long-tailed distribution of predicates. Consequently, we propose a novel self-training framework specifically designed for SGG, called ST-SGG. This framework effectively harnesses unannotated triplets even in the SGG nature, resulting in substantial improvements in fine-grained predicates. Our work offers a new perspective on training SGG models by leveraging existing benchmark scene graph datasets, which often contain missing annotations. For the limitation of this study, please refer to Appendix G."
        },
        {
            "heading": "ETHICS STATEMENT",
            "text": "In compliance with the ICLR Code of Ethics, to the best of our knowledge, we have not encountered any ethical issues throughout this paper. Moreover, all the datasets and pre-trained models utilized in our experiments are openly accessible to the public."
        },
        {
            "heading": "REPRODUCIBILITY STATEMENT",
            "text": "To ensure reproducibility of experiment results, we describe the details of datasets, experimental setting, and implementation details in Appendix E and F. Furthermore, we provide a source code in https://anonymous.4open.science/r/torch-st-sgg-6F0F along with accessible datasets and our pretrained models."
        },
        {
            "heading": "Supplementary Material",
            "text": "- Adaptive Self-training Framework for Fine-grained Scene Graph Generation -"
        },
        {
            "heading": "A Related Work 14",
            "text": ""
        },
        {
            "heading": "B Regarding Challenges of Applying Self-training for SGG 15",
            "text": "B.1 Thresholding Techniques in Image Recognition Task . . . . . . . . . . . . . . . . 15\nB.2 Details of Thresholding Techniques in Sec. 3.3 . . . . . . . . . . . . . . . . . . . 15\nB.3 Detailed Discussion on Result . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16"
        },
        {
            "heading": "C ST-SGG 17",
            "text": "C.1 Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\nC.2 Naive Model Prediction-based Adaptive Thresholding . . . . . . . . . . . . . . . . 17"
        },
        {
            "heading": "D Details on Graph Structure Learner (GSL) 18",
            "text": "D.1 Graph Structure Learner . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\nD.2 Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19"
        },
        {
            "heading": "E Experiment on Visual Genome 19",
            "text": "E.1 Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\nE.2 Experimental Setting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\nE.3 Implementation Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\nE.4 Result with Reweighting Methods . . . . . . . . . . . . . . . . . . . . . . . . . . 20\nE.5 Zero-shot Performance Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\nE.6 Effect of Initial Value for Class-specific Threshold . . . . . . . . . . . . . . . . . . 20\nE.7 Effect of Class-specific Momentum . . . . . . . . . . . . . . . . . . . . . . . . . 21\nE.8 Effect of \u03b1inc and \u03b1dec . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\nE.9 Effect of \u03b2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\nE.10 Complexity Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23"
        },
        {
            "heading": "F Experiment on Open Image V6 23",
            "text": "F.1 Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\nF.2 Experimental Setting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\nF.3 Comparison with the state-of-art models . . . . . . . . . . . . . . . . . . . . . . . 23\nG Limitation & Future Work 24"
        },
        {
            "heading": "A RELATED WORK",
            "text": "Self-Training. Self-training is one of the prominent semi-supervised approaches that utilize a large amount of unlabeled samples, and it has been widely studied in the field of image recognition. The main idea is to assign pseudo-labels to unlabeled samples whose confidence is above a specified threshold and use them for model training to improve the generalization performance of the model. Recent works have focused on reliving the confirmation bias (Arazo et al., 2020) arising from using incorrect pseudo-labels for model training, which in turn undesirably increases the confidence of incorrect predictions. Hence, numerous confidence-based thresholding techniques are proposed to accurately assign pseudo-labels to unlabeled samples. Specifically, UDA (Xie et al., 2020a) and Fixmatch (Kihyuk Sohn, 2020) employ a pre-defined constant threshold during training. However, these approaches fail to consider the learning state of a model, aggravating the confirmation bias as the model predicts with higher confidence at the later part of the training step. To resolve the issue, Dash (Yi Xu, 2021) presents an adaptive threshold that is decreased as the iteration goes on, and Flexmatch (Bowen Zhang, 2021) employs a curriculum learning approach to reflect the learning state of the model into the pseudo-labeling process. In addition, Lai et al. (2022); Guo et al. (2021); Wei et al. (2021) assume the situation with an extremely imbalanced class distribution in which the model is prone to a severe confirmation bias. They present thresholding techniques, which change the threshold according to the number of assigned pseudo-labels per class, to reduce the bias towards majority classes. However, none of the above methods addresses the unique challenges of applying a self-training framework in the context of SGG. In this work, we propose a novel thresholding technique that considers the inherent nature of SGG, where the semantic ambiguity of predicates and the bias caused by long-tailed distribution are present.\nScene Graph Generation. Existing SGG methods point out that SGG models accurately predict general predicates, while rarely making correct prediction for fine-grained predicates. There are two mainstream SGG approaches for addressing the biased prediction issue. One line of research aims to alleviate the long-tailed predicate distribution problem. Specifically, Desai et al. (2021); Li et al. (2021) develop triplet-level resampling methods to balance the distribution of general and fine-grained predicates. Yan et al. (2020); Lyu et al. (2022) propose reweighting losses that consider interclass dependencies of predicates to improve the performance on fine-grained predicates. Additionally, Tang et al. (2020) introduces a causal inference framework, and Dong et al. (2022) uses the transfer learning approaches to mitigate the bias towards general predicates. Guo et al. (2021); Chiou et al. (2021) present post-processing methods to address the bias. Most recently, IE-trans (Ao Zhang & Chua, 2022) proposed to replace general predicates with fine-grained predicates and fill in missing annotations with fine-grained predicates to mitigate the problem caused by the long-tailed predicate distribution. However, IE-trans fails to fully exploit the true labels of unannotated triplets as it uses the initial model parameter of the pretrained model to generate pseudo-labels. On the other hand, we propose a novel pseudo-labeling technique for SGG to effectively utilize the unannotated triplets, which helps reduce the number of incorrect pseudo-labels by iteratively updating both the pseudo-labels and the SGG model.\nAnother line of research focuses on designing advanced SGG architectures to improve the generalization performance of SGG models. The main idea is to refine the representations of entity and relation proposals obtained from the object detector with message-passing neural networks (MPNN). Specifically, some works enhance the representations by employing sequential models such as RNN and TreeLSTM to capture the visual context (Zellers et al., 2018; Tang et al., 2019; Xu et al., 2017). Rather sequentially capturing the context from neighboring entities, recent works propagate messages from the neighbors using graph neural networks whose representation includes structured context between neighbors (Yang et al., 2018; Lin et al., 2020; Li et al., 2021; Yoon et al., 2023). Relformer (Suprosanna Shit, 2022) develops one-stage SGG framework using relational transformer to represent local and global semantic in image as a graph. In this work, we propose a modelagnostic self-training framework for SGG to facilitate the use of many unannotated triplets in the scene graph. We further extend ST-SGG for MPNN-based SGG models based on graph structure learning (GSL), which accurately assigns pseudo-labels based on the presence of relationships.\nDifference from IE-Trans. IE-Trans (Ao Zhang & Chua, 2022) is the most recent work that addresses the inherent issues in scene graph datasets aiming at alleviating the long-tailed problem. Specifically, IE-Trans performs an internal transfer that replaces general predicates with fine-grained predicates within the annotated triplets, and an external transfer that fills in unannotated triplets, i.e., background classes, with fine-grained predicates. More specifically, IE-Trans\nutilizes a pretrained SGG model, such as Motif (Zellers et al., 2018), VCTree (Tang et al., 2019), and BGNN (Li et al., 2021), to generate the confidence for all possible predicates, and sorts them in descending order. Subsequently, it conducts the internal transfer on the top 70% of annotated triplets and external transfer on the top 100% of unannotated triplets (i.e., all unannotated triplets). It is worth noting that IE-Trans assigns pseudo-labels based on the initial parameter of the pre-trained model, and trains a new model from scratch.\nOn the other hand, ST-SGG focuses only on the effective utilization of unannotated triplets to alleviate the long-tailed problem. That is, we only focus on the \u201cexternal transfer.\u201d In order to accomplish this, ST-SGG learns class-specific thresholds to determine which instances should be pseudo-labeled at every batch, and retraining the model\u2019s parameters using the dataset enriched with pseudo-labeled predicates.\nHerein, we clarify the difference between ST-SGG and the external transfer of IE-Trans, as they both serve a similar purpose, which is to make use of unannotated triplets.\n1. While ST-SGG sets a different threshold for each predicate class, IE-Trans assigns pseudolabels to predicates based on their confidence values without considering class-specific thresholds. This approach overlooks the diverse distribution of confidence across classes, leading to imprecise pseudo-labeling. For example, in Fig. 6.(a), we observed that the top-1% confidence of carrying is around 0.4, which is relatively high among confidences of overall predicate classes, while that of painted on below 0.05, which is relatively low. This implies that when performing external transfer to unannotated triplets sorted in descending order by the confidence, IE-Trans would rarely pseudo-label a triplet with painted on, as its confidence is generally low. We conjecture that, to avoid this problem, IE-Trans performed external transfer on the top 100% of unannotated triplets (i.e., all unannotated triplets) if there is at least a slight overlap between two bounding boxes. In fact, Figure 7.(b) of the IE-Trans paper Ao Zhang & Chua (2022) shows that IE-Trans is only successful when the external transfer is performed on the top-100% of unannotated triplets, while the performance degrades significantly otherwise. To make it worse, assigning pseudo-labels to all unannotated triplets leads to the generation of a significant number of incorrect pseudo-labels as the true background relations exist in the scene graphs. On the other hand, ST-SGG addresses these problems by proposing CATM that considers the different confidence distributions across the classes and the presence of true background.\n2. ST-SGG is computationally more efficient compared with IE-Trans. ST-SGG only needs to check whether the confidence produced at each batch is greater than the class-specific thresholds to compute the loss for pseudo-labeled triplets. On the other hand, IE-Trans needs to perform an additional data transfer stage, which produces confidence of all possible relationships in the dataset to perform the internal and external transfers. Due to the large number of triplets in the scene graph dataset, this additional stage requires a lot of computational time during the model\u2019s inference. This will be further discussed in detail in Appendix E.10."
        },
        {
            "heading": "B REGARDING CHALLENGES OF APPLYING SELF-TRAINING FOR SGG",
            "text": ""
        },
        {
            "heading": "B.1 THRESHOLDING TECHNIQUES IN IMAGE RECOGNITION TASK",
            "text": "As self-training has been widely explored in the field of image recognition, a straightforward approach to designing ST-SGG is to adopt existing thresholding techniques. In the following subsection, we evaluate the existing thresholding techniques (Kihyuk Sohn, 2020; Wei et al., 2021; Yi Xu, 2021) when they are applied to ST-SGG."
        },
        {
            "heading": "B.2 DETAILS OF THRESHOLDING TECHNIQUES IN SEC. 3.3",
            "text": "1. Constant thresholding (Kihyuk Sohn, 2020; Xie et al., 2020a): \u03c4 = \u03c4 con is a pre-defined constant threshold used in Fixmatch (Kihyuk Sohn, 2020). Due to the fact that the SGG model predicts the bg class with significantly higher confidence than the remaining classes, setting a global threshold such as \u03c4 con = 0.95 would fail. Thus, we set the threshold as the top-1% confidence among the confidences for all classes computed on the validation set.\n2. Fixed class-specific thresholding (Kihyuk Sohn, 2020): \u03c4 = \u03c4 clsc is a variant of constant thresholding that defines the threshold in a class-specific manner. The threshold for each predicate class c is set to the top-1% confidence per predicate computed on the validation set as in Fig. 6(a).\n3. Class Frequency-weighted Fixed class-specific thresholding (Wei et al. (2021)): \u03c4 = \u03c4 ltc is designed to consider the nature of long-tailed predicate distribution in SGG to prevent the model from assigning pseudo-labels mainly to majority classes. We set \u03c4 ltc as the linear combination of \u03c4 clsc and the normalized frequency of predicate class as in Fig. 6(b), which imposes a penalty for assigning pseudo-labels to majority classes.\n4. Class-specific adaptive thresholding (Yi Xu (2021); Guo & Li (2022)): \u03c4 = \u03c4 adac is a threshold that adaptively changes according to the learning state of the model. Following Dash (Yi Xu, 2021), we gradually increase the class-specific threshold starting from \u03c4 clsc to reduce the number of incorrect pseudo-labels as the training proceeds (Please refer to Fig. 6(e))."
        },
        {
            "heading": "B.3 DETAILED DISCUSSION ON RESULT",
            "text": "Here, we delve deeper into the results of Sec 3.3 and explore the reasons why each thresholding technique failed to improve the performance of SGG during self-training.\nDespite Motif-\u03c4 clsc containing reliable confidence levels for each class (i.e., top-1% confidence per predicate class), we observed that the model only assigns pseudo-labels to predicates belonging to the majority classes (Please see Fig.2(c)), leading to an increased bias towards the majority classes. This indicates that the long-tailed problems must be addressed to design a self-training framework for SGG. To address this, \u03c4 ltc was introduced to penalize assigning pseudo-labels with majority predicate classes, but it resulted in a significant performance drop in terms of Recall@100 compared to Motif-\u03c4 clsc in Fig. 2(a). This is because, as shown in Fig. 6(c), Motif-\u03c4 lt c cannot produce pseudo-labels with majority and minority predicate classes and rather generates pseudo-labels with intermediate classes, leading to the bias towards intermediate classes. It is natural that when the model is biased towards the intermediate classes, the Recall@100 for majority classes decreases since it degrades the generalization performance for majority classes. This result implies that simply penalizing the majority classes is not a solution for providing appropriate thresholds for each predicate class.\nOn the other hand, \u03c4 adac adjusts the threshold over iterations to control the number of pseudo-labels. However, as depicted in Fig. 6(d), Motif-\u03c4 adac exhibits a bias towards the majority classes in the early iterations and later on, due to the saturated thresholds, it ends up not assigning any pseudo-labels, resulting in performance similar to that of Motif without self-training i.e., Motif-Vanilla. From the experiment, we validated that a new thresholding technique is required for ST-SGG, considering the long-tailed problem and the nature of SGG.\nAlgorithm 1 ST-SGG\nRequire: Pretrained SGG model f\u03b8, increasing/decreasing rate \u03b1inc, \u03b1dec, coefficient of loss for pseudo-labeled predicates \u03b2, maximum iteration T , and sorted number of instances that belongs to each predicate class N1, N2, ..., N|Cp|.\n1: Compute class-specific momentum: \u03bbincc = ( Nc N1 )\u03b1 inc , \u03bbdecc = ( N|Cp|+1\u2212c N1 )\u03b1 dec \u2200c \u2208 Cp 2: for t = 1 to T do 3: Given a batch of images I1, ..., IB , 4: GA = [GA1 ,G A 2 , ...,G A B ] and G U = [GU1 ,G U 2 , ...,G U B ] 5: for all (si,pi,oi) \u2208 GU do 6: Compute q\u0302i = max f p \u03b8 (x s,o i ) and assign pseudo-label q\u0303i = c. 7: end for 8: if use GSL then 9: Compute s\u0302s,oi using GSL\n10: L = Ei\u2208GA [\u2212pi \u00b7 log(p\u0302pi )]\ufe38 \ufe37\ufe37 \ufe38 Loss for annotated predicates +Ei\u2208GU |(q\u0302i<\u03c4t\u22121q\u0303i )\u2228(s\u0302 s,o i <0.5)\n[ \u2212pbgi \u00b7 log(p\u0302 p i ) ]\n\ufe38 \ufe37\ufe37 \ufe38 Loss for bg class\n11: +\u03b2 Ei\u2208GU |(q\u0302i\u2265\u03c4t\u22121q\u0303i )\u2227(s\u0302 s,o i \u22650.5) [\u2212q\u0303i \u00b7 log(p\u0302pi )]\ufe38 \ufe37\ufe37 \ufe38 Loss for pseudo-labeled predicates with GSL 12: else 13: L = Ei\u2208GA [\u2212pi \u00b7 log(p\u0302pi )]\ufe38 \ufe37\ufe37 \ufe38\nLoss for annotated predicates\n+Ei\u2208GU |q\u0302i<\u03c4t\u22121q\u0303i\n[ \u2212pbgi \u00b7 log(p\u0302 p i ) ]\n\ufe38 \ufe37\ufe37 \ufe38 Loss for bg class\n14: +\u03b2 Ei\u2208GU |q\u0302i\u2265\u03c4t\u22121q\u0303i [\u2212q\u0303i \u00b7 log(p\u0302pi )]\ufe38 \ufe37\ufe37 \ufe38 Loss for pseudo-labeled predicates 15: end if 16: Update class-specific adaptive threshold \u25b7 Refer to Algorithm 2 17: Train the SGG model f\u03b8 w/ gradient descent: 18: \u03b8t+1 \u2190 \u03b8t \u2212 \u03b7 dLd\u03b8t 19: end for"
        },
        {
            "heading": "C ST-SGG",
            "text": ""
        },
        {
            "heading": "C.1 ALGORITHM",
            "text": "For better understanding of ST-SGG, we provide the details of the procedure in Algorithm 1 and Algorithm 2. In Algorithm 1, we process a batch of images by first segregating the batch into an annotated scene graph GA and an unannotated scene graph GU (Line 4). During each batch iteration, we evaluate confidences and assign pseudo-labels to all the unannotated predicates in GU (Line 6). Finally, we calculate three separate losses for the annotated predicates, background predicates, and pseudo-labeled predicates (Line 13). Specifically, for annotated predicates, we employ the standard cross-entropy loss. For unannotated predicates, if their confidence is above the current threshold \u03c4 tc , we calculate the cross-entropy using the assigned pseudo-label as supervision. If the confidence is below the threshold, the predicate is classified within the background relation class. Notably, we update the thresholds dynamically after each batch, as explained in Algorithm 2."
        },
        {
            "heading": "C.2 NAIVE MODEL PREDICTION-BASED ADAPTIVE THRESHOLDING",
            "text": "In Sec 4.1, we mentioned that a straightforward approach to estimating the model prediction-based threshold would be to compute the average of the model\u2019s confidence in the validation set, and set it as the threshold at every iteration or at a regular iteration interval. Herein, we implement the naive model prediction-based thresholding by setting the class-specific threshold as the averaged confidence computed on the validation set. We computed the confidence with 100 regular intervals since it is impractical to set the class-specific threshold from the validation set at every iteration due to the expensive computation cost. In Fig 7, we observe that the performance of the naive thresholding method significantly degrades between 0 and 100 iterations. We attribute this degradation to the\nAlgorithm 2 CATM\nRequire: Set of confidence q\u0302i for GU , increasing/decreasing rate \u03b1inc, \u03b1dec, and sorted number of instances that belongs to each predicate class N1, N2, ..., N|Cp|.\nfact that in SGG it is crucial to reflect the learning state of the model at each iteration, but the naive approach failed to do so. Therefore, the naive approach is inefficient and ineffective compared to the EMA-based approach in terms of reflecting the learning state as well as time complexity."
        },
        {
            "heading": "D DETAILS ON GRAPH STRUCTURE LEARNER (GSL)",
            "text": ""
        },
        {
            "heading": "D.1 GRAPH STRUCTURE LEARNER",
            "text": "For more details of the graph structure learner mentioned in Section 4.3, we formally describe the GSL which is adopted to the MPNN-based SGG model. The main idea of GSL is to enrich the structure of the scene graph by discovering relevant neighbors or removing irrelevant neighbors.\nBased on the subject and object representations (i.e., xs,xo), GSL uses an MLP to generate a scalar ss,o \u2208 [0, 1] representing the link probability of the relation between a subject s and an object o as follows: ss,o = sigmoid(MLP([xs;xo]), where [;] is the concatenation operation. A straightforward approach for improving the representation of the relations would be to assign a weight to a message based on the link probability ss,o, so as to treat the messages differently according to their importance. However, as ss,o is a soft value that lies between 0 and 1, this approach fails to completely prevent irrelevent messages from being propagated among entities, i.e., this approach essentially treats a graph as a fully connected graph where the edge weights are between 0 and 1. To this end, we propose to sample relevant relations from a scene graph based on ss,o so that messages are only allowed to be propagated through the sampled relations. However, as sampling is a discrete process that is not differentiable, which hinders the end-to-end training of our model, we apply the Gumbel-softmax reparameterization trick (Maddison et al., 2016; Jang et al., 2016) as s\u0302s,o = Bernoulli [ 1\n1+exp(\u2212(log ss,o+\u03b5)/\u03c4)\n] , where Bernoulli(\u00b7) is the Bernoulli approximation,\n\u03b5 \u223c Gumbel(0, 1) is the Gumbel noise for reparameterization, and \u03c4 is the temperature hyperparameter. In the forward pass, we sample the relation between s and o, if s\u0302s,o > 0.5, while in the backward pass, we employ the straight-through gradient estimator (Bengio et al., 2013) so that the gradient can be passed through the relaxed ss,o. That is, only for s and o with s\u0302s,o > 0.5, we allow message passing between s and o, and consider the relation between between s and o as a candidate for pseudo-labeling."
        },
        {
            "heading": "D.2 TRAINING",
            "text": "We train the GSL by optimizing the following binary classification loss:\nLGSL = \u2212 B\u2211\nb=1 |Gb|\u2211 k=1 ysk,ok(1\u2212 ssk,ok)\u03b3 \u00b7 log(ssk,ok) (3)\nwhere ysk,ok is the binary value that equals to 1 if a relation exists between subject sk and object ok, and otherwise 0 (i.e., when bg), and \u03b3 is a hyperparameter. As the number of bg greatly outnumbers that of the remaining classes, i.e., class imbalance, we use the focal loss (Lin et al., 2017) instead of the binary cross-entropy loss."
        },
        {
            "heading": "E EXPERIMENT ON VISUAL GENOME",
            "text": ""
        },
        {
            "heading": "E.1 DATASET",
            "text": "We follow the commonly used pre-processing strategies that have been extensively employed for evaluating SGG (Yoon et al., 2023; Li et al., 2021; Zellers et al., 2018; Xu et al., 2017). The Visual Genome dataset comprising 108K images is divided into a 70% training set and a 30% test set, with 5K images from the training set utilized for the validation set. Based on the frequency of occurrence, we only consider the top 150 most frequently occurring object classes and the top 50 predicate classes. Following the pre-processing steps, the average number of objects per image is 11.6, while the average number of predicates is 6.2."
        },
        {
            "heading": "E.2 EXPERIMENTAL SETTING",
            "text": "Evaluation Metric. We evaluate SGG models on three metrics: (1) Recall@K (R@K) is the conventional metric, which calculates the proportion of top-K predicted triplets that are in ground truth. (2) mean Recall@K (R@K) calculates the average of the recall for each predicate class, which is designed to measure the performance of SGG models under the long-tailed predicate class distribution. (3) F@K calculates the harmonic average of R@K and mR@K to jointly consider R@K and mR@K without trade-off between these metrics. Recent SGG studies (Li et al., 2021; Ao Zhang & Chua, 2022) addressing the long-tailed distribution focus on enhancing the performance of the minority predicate classes (measured by mean R@K) since they usually include more informative descriptions in depicting a scene (e.g., \u201cwalking in\u201d, \u201cplaying\u201d), than the majority classes (e.g., \u201con\u201d, \u201chas\u201d) does. However, there is a trade-off between R@K and mR@K. In other words, if a model deliberately lowers the number of predictions for head predicate classes (e.g., \u201con\u201d) while increasing it for tail predicate classes (e.g., \u201cstanding on\u201d, \u201cwalking on\u201d, and \u201cwalking in\u201d), we would encounter a decrease in R@K and an increase in mR@K. That being said, we can deliberately increase R@K at the expense of reduced mR@K and vice versa. Thus, we focus on enhancing the F@K, considering the trade-off between Recall@K and mean Recall@K.\nEvaluation Protocol. We evaluate under three conventional SGG tasks : (1) Predicate Classification (PredCls) provides the ground truth bounding box and the class of entities, and then evaluate the performance of SGG models in terms of recognizing the predicate class. (2) Scene Graph Classification (SGCls) only provides the bounding box, and requires the model to predict the class of entities and predicates between them. (3) In Scene Graph Detection (SGDet), SGG models generate entity proposals, and predict the classes of entities and predicates between them.\nBaselines. We include the baselines that can be classified into 1) model-agnostic framework and 2) specific models. For the model-agnostic framework, we compare ST-SGG with unbiased SGG models such as TDE (Tang et al., 2020), DLFE (Chiou et al., 2021), Re-sampling (Li et al., 2021), NICE (Li et al., 2022), and IE-Trans (Ao Zhang & Chua, 2022). Moreover, we also include the variant of IE-Trans ( i.e., I-Trans), which excludes the pseudo-labeling mechanism (i.e., E-Trans) on unannotated triplets, to further compare the effectiveness of its pseudo-labeling with ST-SGG. For the specific model, we include the state-of-the-art models such as DT2-ACBS (Desai et al., 2021), PCPL (Yan et al., 2020), KERN (Chen et al., 2019), and GBNet (Zareian et al., 2020).\nE.3 IMPLEMENTATION DETAILS\nFollowing previous studies (Tang et al., 2020; Yoon et al., 2023; Li et al., 2021; Ao Zhang & Chua, 2022), we employ Faster R-CNN (Ren et al., 2015) with ResNeXt-101-FPN (Xie et al., 2017) backbone network as the object detector, whose pretrained parameters are frozen while training the SGG model. In SGDet task, we select the top 80 entity proposals sorted by scores computed by object detector, and use per-class non-maximal suppression (NMS) at IoU 0.5. For ST-SGG, we conduct a grid search for the rate in momentum \u03b1inc and \u03b1dec with an interval of 0.2 (Sec. 4.2), and set the coefficient of the loss for pseudo-labeled predicates \u03b2 to 1.0 (Equation 1) in the base SGG model. On the other hand, we set the \u03b2 to 0.1 when adopting the re-weight loss in Appendix E.4. Following the self-training framework, we initially train the SGG model based on the annotated triplets and then re-train it using both annotated triplets and pseudo-labeled triplets. We set the initial value of \u03c4 (t)c for all predicate classes to 0 based on our observation that the performance of ST-SGG is not sensitive to the initial value. We set the maximum number of pseudo-labeled instances per class to 3 in an image. Similar to IE-Trans, we give pseudo-labels on unannotated triplets only when the bounding boxes of the subject and object overlap. For the graph structure learner, we set the temperature \u03c4 to 0.5 and \u03b3 in focal loss to 2.0. For each experiment, we used the A6000 GPU device."
        },
        {
            "heading": "E.4 RESULT WITH REWEIGHTING METHODS",
            "text": "In addition to the experiments with re-sampling (Li et al., 2021) and I-Trans (Ao Zhang & Chua, 2022) approach, which is presented in Table 1 of the main paper, we further conduct experiments for ST-SGG with a conventional re-weight loss (Rwt) and show the results in Table 4. Note that we follow the conventional re-weight loss used in IE-Trans (Ao Zhang & Chua, 2022). We observe that Motif/BGNN+Rwt+ST-SGG generally improves the performance on mR@K and F@K compared to Motif/BGNN+Rwt. Furthermore, Motif/BGNN+Rwt+I-Trans+ST-SGG achieves even better performance. These findings, consistent with the insights discussed in Section 5.1, support that ST-SGG with a simple debiasing method (i.e., re-weight) shows the effectiveness of utilizing unannotated triplets, resulting in further alleviating the long-tailed problem."
        },
        {
            "heading": "E.5 ZERO-SHOT PERFORMANCE ANALYSIS",
            "text": "Following the zero-shot settings (Tang et al., 2020), we compute the recall@K values for specific (subject, predicate, object) pairs, which are not included in the training set but emerge in the testing phase. Table 5 reveals that debiasing techniques, including resampling and IE-Trans, do not enhance the zero-shot performance of SGG models. To elaborate, the limitation of the resampling approach lies in its simplistic alteration of predicate class frequencies, which does not augment any variance in the data. As for IE-Trans, it generates pseudo-labels only for minority predicates classes, which restricts the ability of finding zero-shot triplets. Conversely, the application of ST-SGG shows improvements in the zero-shot performance for both Motif and VCTree models. This improvement suggests that ST-SGG can generate pseudo-labels that are instrumental in enhancing the model\u2019s generalization to unseen triplet patterns. The effectiveness of ST-SGG in this context underscores its potential as a robust tool for improving the generalization capabilities of SGG models, where the ability to predict novel patterns is crucial."
        },
        {
            "heading": "E.6 EFFECT OF INITIAL VALUE FOR CLASS-SPECIFIC THRESHOLD",
            "text": "We set the initial value of \u03c4c for all predicate classes to 0 based on our observation that the performance of ST-SGG is not sensitive to the initial value. We further showcase the effect of the initial\nTable 5: Zero-shot (zR) Performance when Motif and VCTree backbones are used.\nMethod zR@50 / 100 Method zR@50 / 100\nMotif 16.3 / 19.4 VCTree 16.7 / 19.7 Motif+Resampling 17.0 / 19.5 VCTree+Resampling 16.3 / 19.4 Motif+IE-Trans 13.0 / 15.9 VCTree+IE-Trans 14.0 / 16.5 Motif+ST-SGG 18.0 / 21.0 VCTree+ST-SGG 19.8 / 21.0\no f\nh as\nm o\nu n\nte d\no n\nag ai\nn st\nT h\nre sh\no ld\nT h\nre sh\no ld\nT h\nre sh\no ld\nT h\nre sh\no ld\nIteration\nFigure 8: Adaptive threshold values over iterations on SGCls task when \u03c40c =1, \u03c4 0 c =0, and \u03c4 0 c = \u03c4 cls c . Motif backbone is used.\nvalue of the class-specific adaptive threshold. Specifically, we train Motif with bi-level sampling, and consider three cases: initial thresholds for all predicates are one (i.e., Motif-\u03c40c = 1), zero (i.e., Motif-\u03c40c = 0), and the mean value of the confidence computed on the validation set (i.e., Motif\u03c40c =Motif-\u03c4 cls c in the main paper.). In Table 6, we observe that the performance of ST-SGG is not sensitive to the selection of the initial threshold in terms of F@K. Moreover, in Fig. 8, CATM eventually adjusts thresholds for all predicates in a narrow range. These results imply that CATM rapidly finds the proper threshold wherever they started from and, in turn, makes the value of \u03c4c stabilize within a few iterations."
        },
        {
            "heading": "E.7 EFFECT OF CLASS-SPECIFIC MOMENTUM",
            "text": "We conducted an analysis on the effect of class-specific momentum which is described in Section 4.2 of the main paper. Fig. 9(a) shows that the SGG model without the class-specific momentum produces a significantly more pseudo-labels for majority predicates compared to minority predicates, exacerbating the long-tailed problem, which leads to a decrease in performance, as shown in Table 3 of the main paper. On the other hand, Fig. 9(b) demonstrates that unannotated triplets are pseudolabeled more with predicates from minority classes compared to majority classes. This demonstrates\nthat applying class-specific momentum relieves the long-tailed problem by generating the pseudolabels annotated with minority predicates classes.\nE.8 EFFECT OF \u03b1inc AND \u03b1dec\nFig. 11 shows the performance (F@100) according to the change of \u03b1inc (y-axis) and \u03b1dec (xaxis), which control the increasing and decreasing rate in EMA, respectively. Note that the smaller the value of \u03b1inc or \u03b1dec is, the more aggressive the threshold of minority classes increases or decreases, which is described in detail in Fig. 10. We observe that a high value of \u03b1inc (row 1, 2) leads to a decrease in performance since a high value of \u03b1inc severely restricts the increase of the threshold for minority classes, resulting in incorrect pseudo-label assignment for minority classes. Compared to the performance on the high value of \u03b1inc, the high value of \u03b1dec shows relatively competitive performance. This suggests that a gradual decrease in the threshold for minority classes is acceptable since it keeps assigning the confident pseudo-labels. However, the best performance is achieved with \u03b1inc = 0.4 and \u03b1dec = 0.4, rather than \u03b1dec = 1.0 or 0.8, indicating that in the self-training for SGG task, it is beneficial to appropriately increase or decrease the threshold while assigning more pseudo-labels on minority classes.\nE.9 EFFECT OF \u03b2\nFig. 12 shows the performance with respect to the coefficient of the loss for the pseudo-labeled predicates i.e., \u03b2 in Equation 1. Here, we also employ the Motif backbone network trained with bi-level resampling techniques (Li et al., 2021). We observed consistent performance of ST-SGG ranging from 0.1 to 1.0 for the coefficient \u03b2, which indicates that ST-SGG is not sensitive to the coefficient of the loss for the pseudo-labeled predicates. Moreover, we noticed a performance drop when \u03b2 becomes too large, suggesting that it is preferable to prioritize the existing annotated predicates over the pseudo-labeled predicates to train SGG models."
        },
        {
            "heading": "E.10 COMPLEXITY ANALYSIS",
            "text": "Table 7 shows the total training time of Motif-Vanilla, IE-Trans, and ST-SGG required for 20,000 training iterations. Note that the 20,000 iterations include \u201cPseudolabeling,\u201d and \u201cRe-training.\u201d We observed that ST-SGG requires a shorter training time than IE-Trans for pseudo-labeling and re-training. This implies that the iterative pseudo-labeling process of STSGG is computationally more efficient than the pseudo-labeling process adopted by IE-Trans.\nPlease note that ST-SGG utilizes the confidence originally produced by the backbone SGG model during the training stage, resulting in very low additional computational cost. For example, when Motif-vanilla generates the confidence to compute the loss for annotated triplets, ST-SGG compares it with the learned thresholds and determines whether to include it in the loss for pseudo-labeled predicates. In other words, ST-SGG only requires additional computations for calculating the gradients of the loss for pseudo-labeled predicates. On the other hand, IE-Trans has additional inference stage after training Motif-Vanilla, and stores confidence for every entity pair in the dataset to obtain the rank of all confidence."
        },
        {
            "heading": "F EXPERIMENT ON OPEN IMAGE V6",
            "text": ""
        },
        {
            "heading": "F.1 DATASET",
            "text": "We closely follow the data processing of previous works (Li et al., 2021; Yoon et al., 2023) for Open Image V6 (OI-V6). After preprocessing, OI-V6 is split into 126,368 train images, 1,813 validation images, and 6,322 test images, and contains 301 object classes, and 31 predicate classes."
        },
        {
            "heading": "F.2 EXPERIMENTAL SETTING",
            "text": "Evaluation Metric. Following (Zhang et al., 2019), we additionally evaluate SGG models on three metrics: wmAPrel is weighted mean average precision of relationships, wmAPphr is weighted mean average precision of phrase, and the final score is computed by scorewtd = 0.2 \u00d7 R@50 + 0.4 \u00d7 wmAPrel+0.4\u00d7wmAPphr. Specifically, wmAPrel evaluates the average precision of subject, predicate and object, where both the ground truth boxes of subject and object have an IOU greater than 0.5 with the ground truth bounding boxes, and compute the weighted sum of the average precision scaled with each frequency of predicate class. wmAPphr is similarly computed to wmAPrel, but it computes single bounding box enclosing both subject and object with IOU greater than 0.5.\nEvaluation Protocol. Following the previous works (Li et al., 2021; Lin et al., 2020; Zhang et al., 2019), we evaluate on Scene Graph Detection (SGDet) task, where SGG models generate entity and relation proposals, and predict the classes of entities and the class of predicates between them."
        },
        {
            "heading": "F.3 COMPARISON WITH THE STATE-OF-ART MODELS",
            "text": "Table 8 shows the result in SGDet task on Open ImageV6 datasets. We have the following observations: 1) ST-SGG generally improves Motif, Motif with the resampling, and BGNN in terms of mR@50 and F@50, implying that the long-tailed problem is alleviated by our pseudo-labeling method. 2) Interestingly, Motif+ST-SGG shows competitive performance among those with Motif backbone in terms of R@50 and wmAP. It is important to note that R@50 and wmAP1 metrics primarily emphasize the performance of head predicates, which is in contrast to the objective of addressing the long-tailed problem. This implies that our proposed framework retains the performance on head predicates while achieving the state-of-art performance on tail predicates. This implication\n1The wmAP metric is computed by APrel \u00d7 weight, where the weight is based on the frequency in the test data.\ncan be further explored by referring to Fig. 13, which displays the performance APrel for each class. Specifically, for the head predicates, Motif+ST-SGG achieves competitive results, particularly in the case of the contain predicate. For the tail predicates, Motif+ST-SGG significantly enhances the performance, particularly in the cases where Motif struggles to make accurate predictions, such as ski predicates."
        },
        {
            "heading": "G LIMITATION & FUTURE WORK",
            "text": "Although our current work (including existing SGG models) focuses on utilizing benchmark scene graph datasets, ST-SGG has a potential to leverage external resources. For instance, ST-SGG can benefit from leveraging localization datasets (Lin et al., 2014) containing only the bounding box information without annotation. Specifically, we can include these unannotated bounding boxes obtained from localization datasets in the second and third terms in the loss (Equation 1), i.e., add them to GUb . This approach is promising in utilizing resources to enrich the benchmark scene graph datasets, where acquiring annotated triplets is costly."
        }
    ],
    "year": 2023
}