{
    "abstractText": "Standard language models generate text by selecting tokens from a fixed, finite, and standalone vocabulary. We introduce a novel method that selects contextaware phrases from a collection of supporting documents. One of the most significant challenges for this paradigm shift is determining the training oracles, because a string of text can be segmented in various ways and each segment can be retrieved from numerous possible documents. To address this, we propose to initialize the training oracles using linguistic heuristics and, more importantly, bootstrap the oracles through iterative self-reinforcement. Extensive experiments show that our model not only outperforms standard language models on a variety of knowledge-intensive tasks but also demonstrates improved generation quality in open-ended text generation. For instance, compared to the standard language model counterpart, our model raises the accuracy from 23.47% to 36.27% on OpenbookQA, and improves the MAUVE score from 42.61% to 81.58% in open-ended text generation. Remarkably, our model also achieves the best performance and the lowest latency among several retrieval-augmented baselines. In conclusion, we assert that retrieval is more accurate generation and hope that our work will encourage further research on this new paradigm shift.",
    "authors": [
        {
            "affiliations": [],
            "name": "Bowen Cao"
        },
        {
            "affiliations": [],
            "name": "\u2217Deng"
        },
        {
            "affiliations": [],
            "name": "Cai"
        },
        {
            "affiliations": [],
            "name": "\u2020Leyang Cui"
        },
        {
            "affiliations": [],
            "name": "Xuxin Cheng"
        },
        {
            "affiliations": [],
            "name": "Wei Bi"
        },
        {
            "affiliations": [],
            "name": "Yuexian Zou"
        },
        {
            "affiliations": [],
            "name": "Shuming Shi"
        }
    ],
    "id": "SP:d55dcf574ccc283439ea7b02d7c0c272bfac37be",
    "references": [
        {
            "authors": [
                "Akari Asai",
                "Sewon Min",
                "Zexuan Zhong",
                "Danqi Chen"
            ],
            "title": "Retrieval-based language models and applications",
            "venue": "In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 6: Tutorial Abstracts),",
            "year": 2023
        },
        {
            "authors": [
                "Ilya Sutskever",
                "Dario Amodei"
            ],
            "title": "Language models are few-shot learners",
            "venue": "Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems",
            "year": 2020
        },
        {
            "authors": [
                "Deng Cai",
                "Yan Wang",
                "Wei Bi",
                "Zhaopeng Tu",
                "Xiaojiang Liu",
                "Wai Lam",
                "Shuming Shi"
            ],
            "title": "Skeletonto-response: Dialogue generation guided by retrieval memory. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 2019a",
            "year": 2019
        },
        {
            "authors": [
                "Deng Cai",
                "Yan Wang",
                "Wei Bi",
                "Zhaopeng Tu",
                "Xiaojiang Liu",
                "Shuming Shi"
            ],
            "title": "Retrieval-guided dialogue response generation via a matching-to-generation framework",
            "venue": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),",
            "year": 2019
        },
        {
            "authors": [
                "Deng Cai",
                "Yan Wang",
                "Huayang Li",
                "Wai Lam",
                "Lemao Liu"
            ],
            "title": "Neural machine translation with monolingual translation memory",
            "venue": "In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),",
            "year": 2021
        },
        {
            "authors": [
                "Peter Clark",
                "Isaac Cowhey",
                "Oren Etzioni",
                "Tushar Khot",
                "Ashish Sabharwal",
                "Carissa Schoenick",
                "Oyvind Tafjord"
            ],
            "title": "Think you have solved question answering? try arc, the ai2 reasoning challenge",
            "venue": "ArXiv preprint,",
            "year": 2018
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova"
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "year": 2019
        },
        {
            "authors": [
                "Chris Dyer",
                "Adhiguna Kuncoro",
                "Miguel Ballesteros",
                "Noah A. Smith"
            ],
            "title": "Recurrent neural network grammars",
            "venue": "In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
            "year": 2016
        },
        {
            "authors": [
                "Ruiqi Guo",
                "Sanjiv Kumar",
                "Krzysztof Choromanski",
                "David Simcha"
            ],
            "title": "Quantization based fast inner product search",
            "venue": "Proceedings of the 19th International Conference on Artificial Intelligence and Statistics,",
            "year": 2016
        },
        {
            "authors": [
                "Kelvin Guu",
                "Kenton Lee",
                "Zora Tung",
                "Panupong Pasupat",
                "Ming-Wei Chang"
            ],
            "title": "Realm: Retrievalaugmented language model pre-training",
            "venue": "ArXiv preprint,",
            "year": 2020
        },
        {
            "authors": [
                "Tatsunori B. Hashimoto",
                "Kelvin Guu",
                "Yonatan Oren",
                "Percy Liang"
            ],
            "title": "A retrieve-and-edit framework for predicting structured outputs",
            "venue": "Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems",
            "year": 2018
        },
        {
            "authors": [
                "Ari Holtzman",
                "Jan Buys",
                "Li Du",
                "Maxwell Forbes",
                "Yejin Choi"
            ],
            "title": "The curious case of neural text degeneration",
            "venue": "In 8th International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Gautier Izacard",
                "Patrick Lewis",
                "Maria Lomeli",
                "Lucas Hosseini",
                "Fabio Petroni",
                "Timo Schick",
                "Jane Dwivedi-Yu",
                "Armand Joulin",
                "Sebastian Riedel",
                "Edouard Grave"
            ],
            "title": "Few-shot learning with retrieval augmented language models",
            "venue": "ArXiv preprint,",
            "year": 2022
        },
        {
            "authors": [
                "Di Jin",
                "Eileen Pan",
                "Nassim Oufattole",
                "Wei-Hung Weng",
                "Hanyi Fang",
                "Peter Szolovits"
            ],
            "title": "What disease does this patient have? a large-scale open domain question answering dataset from medical exams",
            "venue": "Applied Sciences,",
            "year": 2021
        },
        {
            "authors": [
                "Peng Jin",
                "Hao Li",
                "Zesen Cheng",
                "Kehan Li",
                "Xiangyang Ji",
                "Chang Liu",
                "Li Yuan",
                "Jie Chen"
            ],
            "title": "Diffusionret: Generative text-video retrieval with diffusion model",
            "venue": "arXiv preprint arXiv:2303.09867,",
            "year": 2023
        },
        {
            "authors": [
                "Jeff Johnson",
                "Matthijs Douze",
                "Herv\u00e9 J\u00e9gou"
            ],
            "title": "Billion-scale similarity search with gpus",
            "venue": "IEEE Transactions on Big Data,",
            "year": 2019
        },
        {
            "authors": [
                "Jean Kaddour"
            ],
            "title": "The minipile challenge for data-efficient language models, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Vladimir Karpukhin",
                "Barlas Oguz",
                "Sewon Min",
                "Patrick Lewis",
                "Ledell Wu",
                "Sergey Edunov",
                "Danqi Chen",
                "Wen-tau Yih"
            ],
            "title": "Dense passage retrieval for open-domain question answering",
            "venue": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
            "year": 2020
        },
        {
            "authors": [
                "Urvashi Khandelwal",
                "Omer Levy",
                "Dan Jurafsky",
                "Luke Zettlemoyer",
                "Mike Lewis"
            ],
            "title": "Generalization through memorization: Nearest neighbor language models",
            "venue": "In 8th International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Urvashi Khandelwal",
                "Angela Fan",
                "Dan Jurafsky",
                "Luke Zettlemoyer",
                "Mike Lewis"
            ],
            "title": "Nearest neighbor machine translation",
            "venue": "In 9th International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Tian Lan",
                "Deng Cai",
                "Yan Wang",
                "Heyan Huang",
                "Xian-Ling Mao"
            ],
            "title": "Copy is all you need",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Jinhyuk Lee",
                "Mujeen Sung",
                "Jaewoo Kang",
                "Danqi Chen"
            ],
            "title": "Learning dense representations of phrases at scale",
            "venue": "In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),",
            "year": 2021
        },
        {
            "authors": [
                "Jinhyuk Lee",
                "Alexander Wettig",
                "Danqi Chen"
            ],
            "title": "Phrase retrieval learns passage retrieval, too",
            "venue": "In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2021
        },
        {
            "authors": [
                "Hao Li",
                "Curise Jia",
                "Peng Jin",
                "Zesen Cheng",
                "Kehan Li",
                "Jialu Sui",
                "Chang Liu",
                "Li Yuan"
            ],
            "title": "Freestyleret: Retrieving images from style-diversified queries",
            "venue": "arXiv preprint arXiv:2312.02428,",
            "year": 2023
        },
        {
            "authors": [
                "Huayang Li",
                "Yixuan Su",
                "Deng Cai",
                "Yan Wang",
                "Lemao Liu"
            ],
            "title": "A survey on retrieval-augmented text generation",
            "venue": "ArXiv preprint,",
            "year": 2022
        },
        {
            "authors": [
                "Yafu Li",
                "Leyang Cui",
                "Jianhao Yan",
                "Yongjing Yin",
                "Wei Bi",
                "Shuming Shi",
                "Yue Zhang"
            ],
            "title": "Explicit syntactic guidance for neural text generation",
            "venue": "In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics",
            "year": 2023
        },
        {
            "authors": [
                "Stephanie Lin",
                "Jacob Hilton",
                "Owain Evans"
            ],
            "title": "TruthfulQA: Measuring how models mimic human falsehoods",
            "venue": "In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume",
            "year": 2022
        },
        {
            "authors": [
                "Todor Mihaylov",
                "Peter Clark",
                "Tushar Khot",
                "Ashish Sabharwal"
            ],
            "title": "Can a suit of armor conduct electricity? a new dataset for open book question answering",
            "venue": "In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2018
        },
        {
            "authors": [
                "Tomas Mikolov",
                "Martin Karafi\u00e1t",
                "Lukas Burget",
                "Jan Cernock\u1ef3",
                "Sanjeev Khudanpur"
            ],
            "title": "Recurrent neural network based language model",
            "venue": "In Interspeech,",
            "year": 2010
        },
        {
            "authors": [
                "Tom\u00e1s Mikolov",
                "Ilya Sutskever",
                "Kai Chen",
                "Gregory S. Corrado",
                "Jeffrey Dean"
            ],
            "title": "Distributed representations of words and phrases and their compositionality",
            "venue": "Advances in Neural Information Processing Systems",
            "year": 2013
        },
        {
            "authors": [
                "Sewon Min",
                "Danqi Chen",
                "Hannaneh Hajishirzi",
                "Luke Zettlemoyer"
            ],
            "title": "A discrete hard EM approach for weakly supervised question answering",
            "venue": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),",
            "year": 2019
        },
        {
            "authors": [
                "Sewon Min",
                "Weijia Shi",
                "Mike Lewis",
                "Xilun Chen",
                "Wen-tau Yih",
                "Hannaneh Hajishirzi",
                "Luke Zettlemoyer"
            ],
            "title": "Nonparametric masked language modeling",
            "venue": "ArXiv preprint,",
            "year": 2022
        },
        {
            "authors": [
                "James L Morgan",
                "Elissa L Newport"
            ],
            "title": "The role of constituent structure in the induction of an artificial language",
            "venue": "Journal of verbal learning and verbal behavior,",
            "year": 1981
        },
        {
            "authors": [
                "Aaron van den Oord",
                "Yazhe Li",
                "Oriol Vinyals"
            ],
            "title": "Representation learning with contrastive predictive coding",
            "venue": "ArXiv preprint,",
            "year": 2018
        },
        {
            "authors": [
                "Ankit Pal",
                "Logesh Kumar Umapathi",
                "Malaikannan Sankarasubbu"
            ],
            "title": "Medmcqa: A large-scale multi-subject multi-choice dataset for medical domain question answering",
            "venue": "Proceedings of the Conference on Health, Inference, and Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Krishna Pillutla",
                "Swabha Swayamdipta",
                "Rowan Zellers",
                "John Thickstun",
                "Sean Welleck",
                "Yejin Choi",
                "Za\u0131\u0308d Harchaoui"
            ],
            "title": "MAUVE: measuring the gap between neural text and human text using divergence frontiers",
            "venue": "Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems",
            "year": 2021
        },
        {
            "authors": [
                "Alec Radford",
                "Jeffrey Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "venue": "OpenAI blog,",
            "year": 2019
        },
        {
            "authors": [
                "Marc\u2019Aurelio Ranzato",
                "Sumit Chopra",
                "Michael Auli",
                "Wojciech Zaremba"
            ],
            "title": "Sequence level training with recurrent neural networks",
            "venue": "In Yoshua Bengio and Yann LeCun (eds.), 4th International Conference on Learning Representations,",
            "year": 2016
        },
        {
            "authors": [
                "S. Robertson",
                "H. Zaragoza"
            ],
            "title": "The probabilistic relevance framework: Bm25 and beyond",
            "venue": "Found. Trends Inf. Retr.,",
            "year": 2009
        },
        {
            "authors": [
                "Stephen Robertson",
                "Hugo Zaragoza"
            ],
            "title": "The probabilistic relevance framework: Bm25 and beyond",
            "venue": "Foundations and Trends\u00ae in Information Retrieval,",
            "year": 2009
        },
        {
            "authors": [
                "Gerard Salton",
                "Christopher Buckley"
            ],
            "title": "Term-weighting approaches in automatic text retrieval",
            "venue": "Information processing & management,",
            "year": 1988
        },
        {
            "authors": [
                "Minjoon Seo",
                "Jinhyuk Lee",
                "Tom Kwiatkowski",
                "Ankur P Parikh",
                "Ali Farhadi",
                "Hannaneh Hajishirzi"
            ],
            "title": "Real-time open-domain question answering with dense-sparse phrase index",
            "year": 1906
        },
        {
            "authors": [
                "Chufan Shi",
                "Haoran Yang",
                "Deng Cai",
                "Zhisong Zhang",
                "Yifan Wang",
                "Yujiu Yang",
                "Wai Lam"
            ],
            "title": "A thorough examination of decoding methods in the era of llms",
            "venue": "arXiv preprint arXiv:2402.06925,",
            "year": 2024
        },
        {
            "authors": [
                "Anshumali Shrivastava",
                "Ping Li"
            ],
            "title": "Asymmetric LSH (ALSH) for sublinear time maximum inner product search (MIPS)",
            "venue": "Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems",
            "year": 2014
        },
        {
            "authors": [
                "Yixuan Su",
                "Nigel Collier"
            ],
            "title": "Contrastive search is what you need for neural text generation",
            "venue": "arXiv preprint arXiv:2210.14140,",
            "year": 2022
        },
        {
            "authors": [
                "Yixuan Su",
                "Tian Lan",
                "Yan Wang",
                "Dani Yogatama",
                "Lingpeng Kong",
                "Nigel Collier"
            ],
            "title": "A contrastive framework for neural text generation",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Shufan Wang",
                "Yixiao Song",
                "Andrew Drozdov",
                "Aparna Garimella",
                "Varun Manjunatha",
                "Mohit Iyyer"
            ],
            "title": "Knn-lm does not improve open-ended text generation",
            "venue": "ArXiv preprint,",
            "year": 2023
        },
        {
            "authors": [
                "Sean Welleck",
                "Kiant\u00e9 Brantley",
                "Hal Daum\u00e9 III",
                "Kyunghyun Cho"
            ],
            "title": "Non-monotonic sequential text generation",
            "venue": "In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Sean Welleck",
                "Ilia Kulikov",
                "Stephen Roller",
                "Emily Dinan",
                "Kyunghyun Cho",
                "Jason Weston"
            ],
            "title": "Neural text generation with unlikelihood training",
            "venue": "In 8th International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Jason Weston",
                "Emily Dinan",
                "Alexander Miller"
            ],
            "title": "Retrieve and refine: Improved sequence generation models for dialogue",
            "venue": "In Proceedings of the 2018 EMNLP Workshop SCAI: The 2nd International Workshop on Search-Oriented Conversational AI,",
            "year": 2018
        },
        {
            "authors": [
                "Yu Wu",
                "Furu Wei",
                "Shaohan Huang",
                "Yunli Wang",
                "Zhoujun Li",
                "Ming Zhou"
            ],
            "title": "Response generation by context-aware prototype editing",
            "venue": "In The Thirty-Third AAAI Conference on Artificial Intelligence,",
            "year": 2019
        },
        {
            "authors": [
                "Qichen Ye",
                "Bowen Cao",
                "Nuo Chen",
                "Weiyuan Xu",
                "Yuexian Zou"
            ],
            "title": "Fits: Fine-grained two-stage training for knowledge-aware question answering",
            "venue": "arXiv preprint arXiv:2302.11799,",
            "year": 2023
        },
        {
            "authors": [
                "Dani Yogatama",
                "Cyprien de Masson d\u2019Autume",
                "Lingpeng Kong"
            ],
            "title": "Adaptive semiparametric language models",
            "venue": "Transactions of the Association for Computational Linguistics,",
            "year": 2021
        },
        {
            "authors": [
                "Susan Zhang",
                "Stephen Roller",
                "Naman Goyal",
                "Mikel Artetxe",
                "Moya Chen",
                "Shuohui Chen",
                "Christopher Dewan",
                "Mona Diab",
                "Xian Li",
                "Xi Victoria Lin"
            ],
            "title": "Opt: Open pre-trained transformer language models",
            "venue": "arXiv preprint arXiv:2205.01068,",
            "year": 2022
        },
        {
            "authors": [
                "Wen Zhang",
                "Yang Feng",
                "Fandong Meng",
                "Di You",
                "Qun Liu"
            ],
            "title": "Bridging the gap between training and inference for neural machine translation",
            "venue": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,",
            "year": 2019
        },
        {
            "authors": [
                "Zexuan Zhong",
                "Tao Lei",
                "Danqi Chen"
            ],
            "title": "Training language models with memory augmentation",
            "venue": "In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2022
        },
        {
            "authors": [
                "MedMCQA (Pal"
            ],
            "title": "2022) is a comprehensive, high-quality dataset designed for biomedical question-answering. We use its validation split, which consists of 4,183 questions. Med-USMILE (Jin et al., 2021) encompasses 12,723 multiple-choice questions, each with four options, originally sourced from the National Medical Board Examination in the USA",
            "year": 2021
        },
        {
            "authors": [
                "Coherence (Su",
                "Collier",
                "Su"
            ],
            "title": "2022) measures the semantic coherence between the prompt x and the generated text x\u0302 by calculating the average log-likelihood",
            "year": 2022
        },
        {
            "authors": [
                "Su et al",
                "Lan"
            ],
            "title": "2023) measures the repetition in generated text at different n-gram levels by computing the proportion of unique n-grams to total n-grams",
            "venue": "Diversity (Welleck et al.,",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Memorization or generalization, that is the question.\nStandard language models (LMs) break down the text generation process into sequential token predictions (Mikolov et al., 2010; Brown et al., 2020; OpenAI, 2022). Each token is a word (or subword) selected from a fixed, finite, and standalone vocabulary. To make the generation more attributable and accelerate the inference speed, Lan et al. (2023) propose a method named CoG that retrieves phrases from similar contexts, where the term \u201cphrase\u201d refers to any contiguous text segments of variable lengths. It is worth noting that, similar to other retrieval-augmented generation frameworks (Li et al., 2022; Asai et al., 2023), CoG still employs a two-stage pipeline, specifically document retrieval followed by grounded phrase extraction. The final performance is constrained by the quality and quantity of the return from the first stage. In this paper, we propose a new paradigm that completely removes the dependence on document retrieval. To our best knowledge, our work is the first that performs text generation through direct phrase retrieval.\nOne core challenge of adopting this novel approach is the construction of the training oracles. That is a function mapping a string of text to an action sequence for creating training examples. For a given text, there exist numerous different ways to segment it into phrases, with each potential phrase being retrievable from a vast array of documents. To better align the generation process and the supporting documents, we introduce a two-fold approach: first, we leverage linguistics-motivated heuristics to initialize the training oracles. Second, we implement a bootstrapping mechanism through iterative self-reinforcement, gradually refining the oracles with each iteration.\nUnlike Lan et al. (2023) which only evaluates the generation fluency in open-ended text generation, we carry out comprehensive and rigorous evaluation in a wide range of knowledge-intensive\n\u2217Work done during an internship at Tencent AI Lab. \u2020Corresponding author.\ntasks, e.g., open-domain question answering. Our proposed model exhibits superior zero-shot performance, outperforming the baseline method. For example, on the OpenbookQA dataset, our model dramatically improves upon base LM, presenting an increase in accuracy from 23.47% to 36.27% (Table 1). Our model also demonstrates improved quality in open-ended text generation, as evidenced by the improvement of 38.97% in the MAUVE score (Table 4). Moreover, it shows even better performance when switching to an enlarged (Table 2) or domain-specific (Table 3) phrase table, without any further training. In addition, our model attains the fastest generation speed among retrieval-augmented baselines (Table 4). We believe that our study can inspire future research to build more efficient and accurate LMs that harness the power of retrieval-based approaches.\nIn summary, the contributions of this paper can be summarized as follows:\n\u2022 We introduce a new approach for language modeling that focuses on directly selecting contextaware phrases from a set of supporting documents.\n\u2022 We propose a novel method for decomposing text generation into sequential next-phrase retrieval by linguistics-driven heuristics and iterative self-reinforced bootstrapping.\n\u2022 We validate the effectiveness of our models on various downstream tasks, including opendomain and domain-specific question answering, as well as open-ended text generation, highlighting substantial improvements over standard LMs and several retrieval-augmented baselines."
        },
        {
            "heading": "2 A UNIFIED VIEW OF GENERATION AND RETRIEVAL",
            "text": "Standard language models (LMs) factorize the generation probability of a sequence x = [x1, x2, . . . , xn] into a series of conditional probabilities p(x) = \u220fn i=1 p(xi|x<i). Hence, the generation is often performed by repeatedly predicting the next token based on the generated sequence thus far (i.e., prefix). The next-token prediction probabilities are computed as\np(xi|x<i) = exp(Ep(x<i) \u00b7 Ec(xi))\u2211\nx\u2032\u2208V exp(Ep(x<i) \u00b7 Ec(x\u2032)) , (1)\nwhere Ep(x<i) is a vector representation of the prefix x<i, Ec(x) denotes a vector representation of the token x, and V stands for the token vocabulary. Through the above notations, we can see that the standard LMs can be viewed as a dual-encoder matching network connecting different prefixes and tokens. Typically, as shown in the left part of Figure 1, the source encoder Ep is implemented by a multi-layer neural network (e.g., Transformers) while the target encoder Ec is simply a token embedding layer. As seen, the design of the dual-encoder network is heavily unbalanced; The source side is much more complex than the target side.\nRecently, a retrieval-augmented LM, CoG (Lan et al., 2023), has been proposed. In addition to token selection, CoG also allows for phrase retrieval (i.e., variable-length n-grams) from a collection of supporting documents. From our point of view, CoG augments the target side of conventional LMs. First, the candidate pool is enlarged to include phrases of variable lengths. Second, the target encoder not only considers the candidates themselves but also their contexts.\nHowever, searching phrases from large-scale corpora is resource-intensive. Therefore, CoG adopts a two-stage search strategy: relevant documents are first retrieved to reduce the search space for phrase selection. To construct the training oracles, CoG uses a forward maximum matching algorithm to find the longest matching phrases from the retrieved documents. Despite promising results, CoG cannot guarantee to provide a globally optimal solution for phrase retrieval, and is highly dependent on the external tool for document retrieval. In contrast, we present a new paradigm that generates text directly through phrase retrieval."
        },
        {
            "heading": "3 THE PROPOSED METHOD",
            "text": ""
        },
        {
            "heading": "3.1 OVERVIEW",
            "text": "Our research aims to enhance the interpretability and factuality of language models (LMs) by transitioning from token generation to phrase retrieval. First, the semantics of phrases are enhanced by their surrounding contexts (Mikolov et al., 2013), leading to a more discriminative representation for inference. Second, each retrieved phrase can be traced back to its original document, enhancing the accountability of the output.\nTo link a given prefix with a set of variable-length phrases, our model follows the dual-encoder structure as described in Section 2 but emphasizes a balanced design in contrast to standard LMs that heavily favor the source side (see Figure 1). Specifically, the source encoder Ep(\u00b7) is a multilayer neural network (e.g., Transformer) as usual. The target encoder Ec(\u00b7) is also a multi-layer neural network to learn context-aware representation for phrases in supporting documents.\nSimilar to standard LMs, we employ dot product as the matching measure. During inference, we can use efficient maximum inner product search (MIPS) algorithms (Shrivastava & Li, 2014; Guo et al., 2016; Seo et al., 2019) to retrieve from a large pool of candidate phrases. The overall framework is depicted in Figure 1. The remaining question is how to train our models."
        },
        {
            "heading": "3.2 TRAINING ORACLES",
            "text": "We break down text generation into a series of next-phrase retrieval. Formally, each step takes the current prefix p as its state, an oracle policy \u03c0\u2217 maps the state to an action \u03c0\u2217(p) \u2192 (f, s), where f is a follow-up phrase and s is a copy of the phrase f in a supporting document.\nAs illustrated in Figure 2, to create such triplets (p, f, s) from raw corpora presents two challenges. First, the boundary of the phrase f is unclear given a continuation can be divided in various ways. Second, the source of each phrase s is unclear because a phrase can appear numerous times across a vast number of documents. On the other hand, the variety of generation paths for a given text also indicates that training oracles are crucial for optimal and quick convergence of our models.\nTo tackle the above problems, we first present a set of linguistics-motivated heuristics to initialize the training oracles (Section 3.2.1), then describe how we allow the model to refine its generation paths in a self-reinforcement manner (Section 3.2.2)."
        },
        {
            "heading": "3.2.1 LINGUISTICS-MOTIVATED HEURISTICS",
            "text": "We start to design the training oracles through the following principles.\nSyntactic Structure. Inspired by the syntactic structure of language and its implications on language generation (Chomsky, 1957; Dyer et al., 2016; Li et al., 2023b), we restrict the phrase to a contiguous sequence of words that corresponds to a constituent unit in a syntactic parse tree. This approach ensures that each phrase possesses a relatively complete and well-defined meaning, while avoiding arbitrary word combinations that could result in semantic ambiguity or nonsensical formations (Morgan & Newport, 1981).\nDistributional Sparsity. The inclusion of high-frequency phrases significantly inflates the size of the candidate pool. This is due to our treatment of lexically identical phrases in different contexts as distinct entries in the pool. Consequently, a single high-frequency phrase could potentially introduce tens of thousands, or even millions, of entries. In our analysis of Wikipedia, we discovered that eliminating just the top 1% of high-frequency phrases could reduce the total number of entries by 50%. However, these high-frequency phrases, such as \u2019as well as\u2019, often lack specific meanings. Their inclusion may result in imbalanced training, which could adversely affect the model\u2019s overall performance. Regarding phrases with extremely low frequency, we consider them to be rare usages with limited practical use. Including them would notably increase the complexity of training. Therefore, we also choose to exclude them.\nSemantic Similarity. Although a lexically identical copy of a phrase can be located in various places, it is crucial to account for polysemy (Cruse, 1986), as lexically identical phrases can exhibit different meanings depending on their contexts. Moreover, even when lexically identical phrases share similar meanings, subtle nuances can arise from different contexts, necessitating a thorough evaluation of semantic similarity when selecting the most appropriate matching (Min et al., 2019).\nSpecifically, we first run the Stanford Parser1 to extract constituents from the training data. We then filter these constituents based on the following criteria: (1) remove trivial constituents with labels such as WHADJP, WHADVP; (2) exclude constituents that are too short (< 2 words) or too long (> 10 words); (3) discard constituents with excessively high or low Inverse Document Frequency (IDF) (Salton & Buckley, 1988) values. Notably, we apply a more lenient IDF threshold for longer constituents. Next, we group lexically identical phrases and compute the pairwise semantic similarities using BM25 (Robertson et al., 2009) and an off-the-shelf phrase encoder (Lee et al., 2021b). Consequently, we can identify the most suitable next phrase for each prefix based on the scores. For more detailed information, please refer to the Appendix A."
        },
        {
            "heading": "3.2.2 ITERATIVE SELF-REINFORCEMENT",
            "text": "The generation paths determined by the above heuristics are model-agnostic and could be noisy and sub-optimal (Welleck et al., 2019). To further improve performance, we allow the model to adjust its own generation paths based on the capabilities it has acquired. That is, transitioning from imitating the oracles to reinforcing its own preferences. In particular, we propose a bootstrapping algorithm to iteratively adjust the target phrases. For each prefix p, we first let the model retrieve the k-best phrases in the entire candidate pool using its current policy. Then, we choose the valid phrase with the highest semantic matching score from these k phrases as the new target. If no such phrase is found, i.e., none of the k-best phrases match the ground-truth continuation, we retain the previous target. The above process is repeated periodically. We present an example in Appendix B."
        },
        {
            "heading": "3.3 TRAINING OBJECTIVES",
            "text": "We optimize our model using the InfoNCE loss (Oord et al., 2018; Karpukhin et al., 2020), for which a negative phrase set N (p) is introduced for each triplet (p, f, s).\nLp = exp(Ep(p) \u00b7 Ec(s)) exp(Ep(p) \u00b7 Ec(s)) + \u2211 t\u2208N(p) exp(Ep(p) \u00b7 Ec(t)) (2)\nThe construction of the negative phrase set N (p) is detailed below. To preserve the ability for tokenlevel generation, we also train our model with the standard next-token prediction loss Lt (Lan et al., 2023). The training objective is formulated as Lp + \u03b1Lt.\n1https://stanfordnlp.github.io/stanza/\nNegative Sampling. We incorporate two types of negative examples to improve the model\u2019s ability to differentiate phrases: (1) In-batch negatives: We regard all other candidate phrases in the same training batch as this type of negative example. These negatives help the model learn more discriminative representations on a large scale without incurring considerable costs. (2) Hard negatives: Recall that in Section 3.2.2, we periodically update the generation targets by retrieving top-k candidate phrases for each prefix. Among these k phrases, despite one may be chosen as the new generation target, the remaining phrases can serve as strong negatives because they are likely to confuse the model.\nNote that the above negatives may contain false negatives, which are not chosen as targets but still make a valid follow-up. To minimize the risk, we remove all phrases that constitute a prefix of the groundtruth continuation."
        },
        {
            "heading": "3.4 MODELS",
            "text": "Prefix Encoder. We treat the prefix as a sequence of tokens with previously predicted phrases split into tokens. This token sequence is encoded using the standard Transformer architecture with causal attention (Vaswani et al., 2017; Radford et al., 2019). The prefix representation is obtained through a linear projection of the last-layer representation of the final token in the sequence.\nPhrase Encoder. We employ a deep bidirectional Transformer (Vaswani et al., 2017; Devlin et al., 2019) to generate contextualized token representations of a supporting document. The representation of a phrase is obtained by concatenating the representations of its first and last tokens, followed by projecting the concatenated representation to the same dimension as the prefix representation. To preserve the ability to compose output using single tokens, we also add the token vocabulary to our phrase table. These standalone tokens can be considered as special phrases, and their representations are obtained through the standard embedding layer of the LM."
        },
        {
            "heading": "4 EXPERIMENT SETUP",
            "text": ""
        },
        {
            "heading": "4.1 IMPLEMENTATION DETAILS",
            "text": "We train our model on the training set of MiniPile2(Kaddour, 2023), and use the English Wikipedia dump March 1, 20223 as supporting documents. Specifically, we split each Wikipedia article into multiple, disjoint text blocks of up to 128 words as documents, which results in 29,488,431 documents. The size of our phrase index is 137,101,097. We use GPT-2 (Radford et al., 2019) and DensePhrases4 (Lee et al., 2021b) to initialize the prefix encoder and the phrase encoder, respectively. For efficiency, we solely fine-tune the prefix encoder. This avoid the computational burden of re-computing phrase embeddings associated with updating the phrase encoder. While revising the training oracles via self-reinforcement, we retrieve the top k = 128 phrases for each prefix."
        },
        {
            "heading": "4.2 INFERENCE DETAILS",
            "text": "During inference, we employ FAISS (Johnson et al., 2019), a library for vector similarity search and clustering, for efficient retrieval.\nContinuation Generation. For text generation, we directly retrieve top-k candidates from the entire phrase table (including both context-aware phrases and standalone tokens). We then apply a softmax function to the matching scores of these candidates, creating a next-phrase probability distribution (Shi et al., 2024), and use top-p sampling (Holtzman et al., 2020) for selecting the next phrase. In all experiments, we set k to 128 (see the analysis on k in Table 7 in Appendix G) and p to 0.95. To control the ratio of phrase retrieval, we filter out phrases with probabilities below a threshold. The threshold is set to \u03d5 = 0.4 if not otherwise specified.\n2https://huggingface.co/datasets/JeanKaddour/minipile 3https://huggingface.co/datasets/wikipedia 4https://huggingface.co/princeton-nlp/densephrases-multi\nLikelihood Estimation. To calculate the likelihood of a given text, we approximate the likelihood by summing all possible generation paths. For instance, given the sentence \u201dThe Moon rises\u201d, the following generation paths may exist: (1) The\u2192moon\u2192rises; (2) The moon\u2192rises; (3) The moon rises. The probability of each path is the product of the probabilities of all phrases (tokens) along that path. For example, the probability of the path (2) is calculated by p(rises|The moon) \u00b7 p(The moon). The probabilities of each step are obtained in the same way as we construct the next-phrase probability distribution for continuation generation. Note that the sum of all possible paths can be computed efficiently using dynamic programming with time complexity O(n2), where n represents the number of tokens in the text."
        },
        {
            "heading": "4.3 BASELINES",
            "text": "We compare the proposed method with standard LM in the zero-shot setting, also drawing the following state-of-the-art retrieval-augmented methods as baselines:\nBase LM is the standard token-level language model using the Transformer (Vaswani et al., 2017) architecture. We fine-tune the pre-trained GPT-25 (Radford et al., 2019).\nkNN-LM (Khandelwal et al., 2020) is a retrieval-augmented LM that interpolates the next-token distribution of the base LM with a k-nearest neighbors (kNN) model.\nRETRO (Borgeaud et al., 2022)6 is a retrieval-augmented LM incorporated with a pre-trained document retriever, a document encoder and a cross-attention mechanism.\nCoG (Lan et al., 2023)7 is another retrieval-augmented LM that adopts a two-stage search pipeline. It first retrieves semantically-relevant documents, and then considers all n-grams within them as candidate phrases."
        },
        {
            "heading": "5 EXPERIMENTS",
            "text": "We verify the effectiveness of our methods on a set of knowledge-intensive tasks and open-ended text generation tasks without fine-tuning."
        },
        {
            "heading": "5.1 KNOWLEDGE-INTENSIVE TASKS",
            "text": ""
        },
        {
            "heading": "5.1.1 DATASETS",
            "text": "We employ five knowledge-insensitive datasets, including three open-domain QA datasets: OpenbookQA (Mihaylov et al., 2018), ARC-Challenge (Clark et al., 2018), and TruthfulQA (Lin et al., 2022); and two domain-specific (medical) datasets: MedMCQA (Pal et al., 2022) and MedUSMILE (Jin et al., 2021). The details for these datasets can be found in Appendix C.\nIn line with prior research (Brown et al., 2020; Sanh et al., 2022), we adopt a classification with options methodology to quantify the model performance. This approach involves presenting the model with a range of options and calculating the likelihood of each option being the correct response. The option with the highest probability is selected as the model\u2019s prediction. We then report the accuracy of the model\u2019s predictions."
        },
        {
            "heading": "5.1.2 RESULTS",
            "text": "We compare our methods with baselines in knowledge-intensive tasks across several settings.\nMain Results. As shown in Table 1, our model consistently outperforms various baseline models across all datasets. Compared with base LM, our model improves the accuracy of the TruthfulQA and OpenBookQA datasets from 29.73% to 34.27% and 23.47% to 36.27%, respectively. When we eliminate the phrase retrieval from our model and only use standalone tokens (Ours w/o phrase), there is a considerable drop in performance, demonstrating the effectiveness of incorporating phrase\n5https://huggingface.co/gpt2 6https://github.com/lucidrains/RETRO-pytorch 7https://github.com/gmftbyGMFTBY/Copyisallyouneed\nretrieval in our methods. Note that the models presented in Table 1 are initialized from pre-trained LMs. To analyze the role of pre-trained models in our framework, we train all models from scratch with random initialization. The results are shown in Table 8 in Appendix G, our model outperforms the baselines across all datasets. For example, our model achieves a 12.8% absolute improvement on OpenbookQA over base LM, suggesting that our training framework is not heavily dependent on pre-trained models. To elucidate the role of phrase retrieval in knowledge-intensive tasks, we delve into a case study depicted in Appendix D.\nEnlarged Phrase Index. Recall that we exclude phrases with excessively high or low IDF values (Section 3.2.1). This strategy not only stabilizes the training process but also improves training efficiency. However, the phrases initially filtered out can be repurposed to augment our phrase index in a training-free manner. This expanded phrase index, now three times larger than the original, underscores the scalability of our approach. As evidenced in Table 2, this expansion boosts our model\u2019s performance, such as a 5.32% increase in accuracy on TruthfulQA. This not only highlights our model\u2019s potential to generalize to unseen phrases and documents but also emphasizes its plugand-play feature, capable of adapting to a larger phrase table without the need for re-training.\nDomain Adaption. The plug-and-play property of the phrase index further motivates us to employ a domain-specific index for the QA tasks in the medical domain without any domain-specific training. To this end, we construct an index consisting of 3 million phrases by extracting phrases from a small text collection of the medical domain8. For comparison purpose, we also\nfine-tune the base LM on it for fair comparison. As illustrated in Table 3, despite the considerable reduction in index size compared to the original Wikipedia index (3 million vs 137 million), our model exhibits even better performance on two medical QA datasets. This result underscores our model\u2019s capability to enhance its performance in specific domains by leveraging a domain-specific, well-curated phrase index in a training-free manner."
        },
        {
            "heading": "5.2 OPEN-ENDED TEXT GENERATION",
            "text": "We conduct open-ended text generation experiments on the test set of MiniPile (Kaddour, 2023). For each document in the test set, we adopt the first 128 tokens as the prefix. The baselines and our model are required to generate text continuations of 128 tokens in length based on the same prefix.\n8https://huggingface.co/datasets/gamino/wiki medical terms"
        },
        {
            "heading": "5.2.1 EVALUATION METRICS",
            "text": "Following previous works (Welleck et al., 2020; Su et al., 2022; Lan et al., 2023), we utilize three automatic evaluation metrics to measure the quality of the generated texts: (i) MAUVE (Pillutla et al., 2021) captures the overall usefulness of the generated text by estimating the average utility of the content; (ii) Coherence measures the logical consistency and flow of the generated text, ensuring that the output is well-structured and easy to understand; and (iii) Diversity evaluates the variety of generated content, promoting the generation of unique and creative text. We report MAUVE and diversity as percentages (%). The details for these metrics can be found in Appendix E. We also measure the average time cost for a model to decode a continuation consisting of 128 tokens given a prefix of 128 tokens, referred to as latency."
        },
        {
            "heading": "5.2.2 RESULTS",
            "text": "As shown in Table 4, our model attains the highest MAUVE score among all models, demonstrating the high quality of the generated text. Other retrieval-augmented methods underperform base LM in the MAUVE score due to text degeneration, which aligns with findings in previous work (Wang et al., 2023). Our model also shows a strong balance between coherence and diversity. The coherence score of our model is 3.25, which outperforms most baselines except for CoG. However, we find that CoG often generates lexically similar, meaningless sentences, which is reflected in its low diversity score of 55.04%. Meanwhile, our model\u2019s diversity score is 76.26%, which is slightly lower than some baseline models, but these models often generate incoherent sentences, as reflected in their lower coherence scores.\nHuman Evaluation. To gain further insights, we randomly sample 100 cases and evaluate the results of the base LM, the base LM without fine-tuning (w/o FT), and our model from four perspectives: fluency, coherence, informativeness, and grammar. Each aspect is scored on a Likert scale from 1 to 4 (1 represents \u201dbad\u201d, 2 stands for \u201dfair\u201d, 3 is considered \u201dgood\u201d, and 4 signifies \u201dvery good\u201d). We report the average scores in table 5. As we can see, our method outperforms the base LM in all four categories, especially in coherence and informativeness. This indicates that our model, based on phrase retrieval, is better at following the preceding context and providing more informative content. As for the lower scores of the base LM compared to the base LM (w/o FT), we find that they are largely due to formatting issues. Further analysis can be found in Appendix F.\nGeneration Speed. We now discuss the generation latency of different models. In Table 4, we report the relative latency, taking the base LM as the baseline. kNN-LM incurs the highest cost due to the need for interpolating the base LM\u2019s token distribution with another distribution computed using its datastore. The CoG model also exhibits a notable overhead as it involves extracting all n-grams from the retrieved documents, applying softmax over tokens and all n-grams, and sampling from the resulting probability distribution. The RETRO model, although faster than the previous two, still\nrequires time for applying the representations of retrieved text chunks in attention computation. Our method stands out with the highest generation speed, since it directly retrieves and utilizes phrases.\nEffect of Self-reinforcement. Ablation studies on the effect of the Self-Reinforcement (SR) mechanism reveal significant insights into the performance of our model. In the case of knowledge-intensive tasks, we do not observe a significant impact of SR on our model\u2019s performance (refer to Table 9 in Appendix G). This suggests that our framework is inherently effective in handling such tasks, even without the aid of SR. However, the scenario differs for open-ended text gen-\neration. Table 6 shows that models trained with SR exhibit substantial improvements in the MAUVE scores across multiple rounds, which indicates the importance of SR in enhancing the quality of text generation. After the second round, we do not observe noticeable improvements with additional rounds of SR iteration, suggesting that the model converges to its optimal state."
        },
        {
            "heading": "6 RELATED WORK",
            "text": "Standard language models (LMs) (Radford et al., 2019; Brown et al., 2020) are trained to predict the next token given a text prefix. With a vast amount of training corpora and model parameters, these models show strong zero-shot performance on various downstream tasks, serving as a unified solution for natural language processing. However, scaling up the model parameters and training corpora can be very expensive and cannot be done in a timely manner.\nTo tackle the above issues, there has been an increasing body of work that enhances the parametric LM with a non-parametric component (Li et al., 2022). Guu et al. (2020); Lewis et al. (2020); Borgeaud et al. (2022); Izacard et al. (2022) ground the next token prediction on a set of relevant documents obtained using retrieval techniques (Robertson & Zaragoza, 2009; Karpukhin et al., 2020). Khandelwal et al. (2020); Yogatama et al. (2021); Zhong et al. (2022) augment the output probability distribution with non-parametric nearest neighborhood estimation. Also, the retrievethen-generate paradigm has been extensively studied in specific downstream tasks, such as code generation (Hashimoto et al., 2018), question answering (Ye et al., 2023; Karpukhin et al., 2020; Lee et al., 2021a), open-domain dialogue systems (Weston et al., 2018; Wu et al., 2019; Cai et al., 2019a;b), and machine translation (Khandelwal et al., 2021; Cai et al., 2021), multimodal retrieval (Jin et al., 2023; Li et al., 2023a).\nThe work most closely related to ours is that of Min et al. (2022) and Lan et al. (2023). The former explores a similar idea in the area of masked language models to enhance natural language understanding. Lan et al. (2023), on the other hand, allows the copy of phrases from the grounding documents. However, their approach still relies on a two-stage pipeline, grounding the generation on a small set of retrieved documents only. While Lan et al. (2023) simply employs the longest common subsequence algorithm to find phrases that can be copied from the retrieved documents, we present heuristics-based and self-reinforced mechanisms to construct reliable training oracles. Also, Lan et al. (2023) only evaluates the performance on open-ended text generation tasks."
        },
        {
            "heading": "7 CONCLUSION",
            "text": "We presented a novel retrieval-based text generation approach using context-aware phrase retrieval. Our method addresses the primary challenge of constructing training oracles through heuristic-based initialization and iterative self-reinforcement. Experiments on knowledge-intensive tasks and openended text generation tasks show that the proposed method outperforms the standard LM and stateof-the-art retrieval-augmented methods. Moreover, our model exhibits superior performance with either an enlarged or a smaller, domain-specific index, and achieves the lowest generation latency compared to other retrieval-augmented baselines. This work contributes to the NLP research community by promoting a paradigm shift towards more accurate generation via retrieval. As we continue to explore and refine the paradigm, we invite readers to consider the limitations of our current work, as detailed in Appendix H, to fully appreciate the scope of future research."
        },
        {
            "heading": "A PHRASE TABLE PRUNING AND PHRASE MATCHING",
            "text": "It is noteworthy that syntactic parsing is a very well-studied task in NLP as well as its cross-domain and cross-language generalization. For example, the Universal Dependencies 9 project provides consistent grammatical annotation across over 100 languages. To our knowledge, the state-of-the-art parsing accuracies are pretty high for major languages such as English, Chinese, Italian, Japanese, Portuguese, etc. Nevertheless, we anticipate performance degradation for languages and domains when the parser accuracy is relatively low. For situations where a syntactic parser is unavailable, alternative methods may be utilized such as unsupervised syntactic parsing and unsupervised tokenization methods (e.g., BPE, sentencepiece).\nAfter extracting constituents from the training data and supporting documents, we filter these constituents based on the following criteria: (1) remove trivial spans with the following constituent labels: \u2018X\u2019, \u2018PRT\u2019, \u2018CC\u2019, \u2018DT\u2019, \u2018EX\u2019, \u2018FRAG\u2019, \u2018GW\u2019, \u2018HYPH\u2019, \u2018IN\u2019, \u2018INTJ\u2019, \u2018LS\u2019, \u2018LST\u2019, \u2018MD\u2019, \u2018NFP\u2019, \u2018NML\u2019, \u2018PDT\u2019, \u2018POS\u2019, \u2018PP\u2019, \u2018PRP\u2019, \u2018PRP$\u2019, \u2018PPZ\u2019, \u2018RB\u2019, \u2018RBR\u2019, \u2018RBS\u2019, \u2018RP\u2019, \u2018S\u2019, \u2018SYM\u2019, \u2018TO\u2019, \u2018WDT\u2019, \u2018WHADJP\u2019, \u2018WHADVP\u2019, \u2018WHNP\u2019, \u2018WHPP\u2019, \u2018WP\u2019, \u2018WP$\u2019, \u2018WRB\u2019, \u2018#\u2019, \u2018$\u2019, \u2018\u201d\u2019, \u201c\u2018\u2019, \u2018-LRB-\u2019, \u2018-RRB-\u2019, \u2018,\u2019, \u2018.\u2019, \u2018:\u2019; (2) exclude constituents that are too short (< 2 words) or too long (> 10 words); (3) discard constituents with excessively high or low Inverse Document Frequency (IDF) values. The (minimum, maximum) thresholds for constituents with different numbers of words are: 2: (10.50, 14.08), 3: (11.09, 14.08), 4: (11.77, 14.30), 5: (12.10, 14.30), 6: (12.32, 14.30), 7: (12.51, 14.59), 8: (12.59, 14.59), 9: (12.64, 14.59), 10: (12.69, 14.59). Next, we group lexically identical phrases and retrieve the top-10 candidates for each phrase using the BM25 algorithm (Robertson et al., 2009). We then calculate the semantic similarities between the original phrase and the retrieved candidate phrases using an off-the-shelf phrase encoder (Lee et al., 2021b). As a result, we can identify the most appropriate next phrase for each prefix based on the scores.\nThe entire preprocessing process, including syntactic parsing, phrase selection, and semantic matching, takes approximately 24 hours on 8 V100 GPUs. The overhead is small compared to the cost of training the model."
        },
        {
            "heading": "B EXAMPLE FOR ITERATIVE SELF-REINFORCEMENT",
            "text": "Suppose we have a prefix p = \u201dGo right for the top when you\u201d. The ground truth for this prefix is \u201dGo right for the top when you want to make things happen\u201d. The initial target phrase determined\n9https://universaldependencies.org/\nby the heuristics might be \u201dwant\u201d. In the iterative self-reinforcement process, we would first let the model retrieve the k-best phrases for the prefix from the entire candidate pool. Supposing that the k-best phrases are [\u201dwant\u201d, \u201dwant to\u201d, \u201dwant to make things happen\u201d, \u201dneed\u201d, \u201dcan\u201d], only \u201dwant\u201d, \u201dwant to\u201d, and \u201dwant to make things happen\u201d are considered as valid ones. If the model\u2019s semantic matching score is highest for \u201dwant to make things happen\u201d, we would update the target phrase for the prefix to this phrase. If none of the k-best phrases are valid, we will retain the previous target \u201dwant\u201d."
        },
        {
            "heading": "C DETAILS OF TASK PHRASING AND SPECIFICATIONS",
            "text": "The statistics of the datasets we select are as follows:\nOpenbookQA (Mihaylov et al., 2018) is a collection of 5,957 multiple-choice questions, each with four options, centered around elementary scientific knowledge. We utilize the test split, which comprises 500 questions.\nARC-Challenge (Clark et al., 2018) includes 7,787 authentic, grade-school level, multiple-choice science questions. These questions span a wide range of topics in science and history, among others. Our experiments focus on the test split of its Challenge Set, which contains 1,172 hard questions.\nTruthfulQA (Lin et al., 2022) is a distinctive dataset emphasizing the truthfulness of answers. We employ the test split of the multiple-choice option, which includes 817 questions.\nMedMCQA (Pal et al., 2022) is a comprehensive, high-quality dataset designed for biomedical question-answering. We use its validation split, which consists of 4,183 questions.\nMed-USMILE (Jin et al., 2021) encompasses 12,723 multiple-choice questions, each with four options, originally sourced from the National Medical Board Examination in the USA. We utilize its test split, which includes 1,273 questions.\nGiven a question with several candidate answers, we concatenate the question with each candidate answer to form options, and then ask the model to select the most accurate one among all the options. We remove questions from these datasets where all candidate answers are single words to ensure the inclusion of phrases in the retrieval process."
        },
        {
            "heading": "D CASE STUDY",
            "text": "To elucidate the role of phrase retrieval in knowledge-intensive tasks, we delve into a case study depicted in Figure 3. As previously discussed in Section 4.2, our approach involves retrieving phrases for each token in an option, enabling us to estimate the probabilities of alternative generation paths beyond simply generating the token sequence. In this specific case from the MedUSMILE dataset, options are formed by concatenating the question with each candidate answer. We find that the phrases retrieved for the final token of the question include the answer, a proper noun requiring medical knowledge for understanding. This introduces a new generation path:\nquestion \u2192 Schizoid personality disorder. We observe that the contexts of the retrieved phrases, such as \u201cSchizoid personality disorder (SPD) is characterized by a lack of interest in social relationships ...\u201d, align closely with the context of the question, \u201cShe does not have friends and spends most of the time reading by herself ...\u201d. These contextually encoded phrases benefit answer selection, thereby showcasing the interpretability of our model. It also highlights the model\u2019s ability to leverage contextual information effectively, particularly in tasks that require specialized knowledge."
        },
        {
            "heading": "E DETAILS FOR AUTOMATIC EVALUATION METRICS",
            "text": "In this section, we provide a detailed introduction to MAUVE, as well as the concepts of coherence and diversity.\nMAUVE (Pillutla et al., 2021) measures how closely the token distribution in generated text matches that in human-written text across the entire test set.\nCoherence (Su & Collier, 2022; Su et al., 2022) measures the semantic coherence between the prompt x and the generated text x\u0302 by calculating the average log-likelihood as: coherence(x\u0302;x) = 1 |x\u0302| \u2211|x\u0302| i=1 log pM (x\u0302i|[x : x\u0302<i]), where [:] is the concatenation operation and M is a pre-trained LM. We follow prior work and set M as the OPT-2.7B model (Zhang et al., 2022). In our implementation, we introduce a slight modification by taking the negative of the average log-likelihood. This adjustment transforms the typically negative log-likelihood into a positive value, facilitating a more intuitive interpretation of the results.\nDiversity (Welleck et al., 2020; Su et al., 2022; Lan et al., 2023) measures the repetition in generated text at different n-gram levels by computing the proportion of unique n-grams to total n-grams in the generated text. It is defined as: diversity = \u220f4 n=2(1.0 \u2212 rep-n 100 ), where rep-n = 100 \u00d7 (1.0 \u2212 |unique n-grams(x\u0302)| |total n-grams(x\u0302)| ), and x\u0302 is the text generated by the model."
        },
        {
            "heading": "F DETAILED HUMAN EVALUATION",
            "text": "Following the results in Table 5, upon manual analysis, we find that the outputs from our method often have a tighter connection with the preceding text (coherence) and exhibit stronger knowledge characteristics (informativeness). For instance, they often include specialized terms, which are not observed in the outputs from the base LM. As for the lower scores of the base LM compared to the base LM (w/o FT), one major reason is that the fine-tuned model frequently outputs \u201dReferences: xxx\u201d and \u201dExternal Links: xxx\u201d. This is related to the characteristics of the Wikipedia dataset, where each article typically ends with references and external links. To further investigate this, we retest the Base LM, excluding all outputs containing \u201dReferences\u201d and \u201dExternal Links\u201d (which accounts for 26.77% of the cases). The resulting MAUVE score is 60.23, slightly lower than the 69.68 of the base LM (w/o FT), but substantially higher than the previous score of 42.61. This also suggests that one possible reason for the significant drop in the MAUVE score after (c.f. Table 4) fine-tuning the base model is due to these extraneous outputs."
        },
        {
            "heading": "G ABLATION STUDIES",
            "text": "The impact of k. As shown in Table 7, k does not have a significant impact on the performance of our model on knowledge-intensive tasks. Since we retrieve the top 128 phrases during the selfreinforcement process, we set k = 128 throughout all experiments.\nThe impact of pre-trained models. The results are given in Table 8. Our model outperforms the base LM across all datasets, achieving a 12.8% absolute improvement on OpenbookQA. This suggests that our training framework is not heavily dependent on pre-trained models.\nThe impact of phrase retrieval threshold \u03d5. The phrase retrieval threshold, which filters out phrases with probabilities below it, influences the proportion of tokens (token rate for short) used in the text generation process. This section explores the intriguing relationship between token rate and the quality of the generated text.\nA lower token rate boosts the model\u2019s inference efficiency, as phrases typically contain multiple tokens, thereby reducing the number of decoding steps. However, this efficiency can sometimes compromise the quality of the text. This degradation occurs due to the inherent uncertainty during generation, particularly when all candidate probabilities are low or when an inappropriate candidate is sampled. In such cases, the selection of an incorrect phrase, given its length, can significantly disrupt the generation pro-\ncess, a phenomenon known as exposure bias (Ranzato et al., 2016; Zhang et al., 2019). Conversely, the impact of choosing a sub-optimal token is less severe. Therefore, up to a certain point (approximately 0.89), increasing the token rate can stabilize the generation process, as shown in Figure 4. Beyond this point, the model\u2019s performance peaks.\nFurthermore, even without phrases (i.e., the token rate is 1), our model can generate high-quality text, suggesting that our method also enhances token prediction learning. However, it\u2019s important\nto note that while our model achieves a high MAUVE score based solely on token prediction, the factuality of the generated text is lower than when phrase retrieval is integrated. This highlights the need for more innovative metrics to precisely measure the quality of generated text.\nThe impact of self-reinforcement on knowledge-intensive tasks. Based on the results presented in Table 9, it can be concluded that the utilization of SR does not significantly affect the performance of our model on knowledge-intensive tasks. This implies that our framework is inherently capable of effectively addressing such tasks, even in the absence of SR."
        },
        {
            "heading": "H LIMITATIONS AND FUTURE WORK",
            "text": "While our proposed framework has shown promising results in efficiently generating accurate and coherent text, it is important to acknowledge the limitations of its current form. First of all, the presented result is just a proof-of-concept of the new paradigm. Future work should focus on (1) Scalability. In our current experiments, we train our models and build the phrase index on the English Wikipedia corpus. When scaling up to larger corpora, we may encounter computational challenges due to the significantly increased amount of possible phrases. To make our approach scalable, some potential solutions include clustering, dimensionality reduction, and fast vector search algorithms with sub-linear time complexity. (2) Alignment. Recent studies have shown that alignment (i.e., fine-tuning language models to following human instructions) is important to make language models universally useful. Thus, incorporating alignment techniques into our approach is an important future research direction."
        }
    ],
    "year": 2024
}