{
    "abstractText": "Federated learning is a powerful paradigm for large-scale machine learning, but it faces significant challenges due to unreliable network connections, slow communication, and substantial data heterogeneity across clients. FEDAVG and SCAFFOLD are two prominent algorithms to address these challenges. In particular, FEDAVG employs multiple local updates before communicating with a central server, while SCAFFOLD maintains a control variable on each client to compensate for \u201cclient drift\u201d in its local updates. Various methods have been proposed to enhance the convergence of these two algorithms, but they either make impractical adjustments to the algorithmic structure or rely on the assumption of bounded data heterogeneity. This paper explores the utilization of momentum to enhance the performance of FEDAVG and SCAFFOLD. When all clients participate in the training process, we demonstrate that incorporating momentum allows FEDAVG to converge without relying on the assumption of bounded data heterogeneity even using a constant local learning rate. This is novel and fairly surprising as existing analyses for FEDAVG require bounded data heterogeneity even with diminishing local learning rates. In partial client participation, we show that momentum enables SCAFFOLD to converge provably faster without imposing any additional assumptions. Furthermore, we use momentum to develop new variance-reduced extensions of FEDAVG and SCAFFOLD, which exhibit state-of-the-art convergence rates. Our experimental results support all theoretical findings.",
    "authors": [
        {
            "affiliations": [],
            "name": "Ziheng Cheng"
        },
        {
            "affiliations": [],
            "name": "Xinmeng Huang"
        },
        {
            "affiliations": [],
            "name": "Pengfei Wu"
        },
        {
            "affiliations": [],
            "name": "Kun Yuan"
        }
    ],
    "id": "SP:f7d9688e826468557e4b5efd948b28936618d139",
    "references": [
        {
            "authors": [
                "Sulaiman A Alghunaim"
            ],
            "title": "Local exact-diffusion for decentralized optimization and learning",
            "year": 2023
        },
        {
            "authors": [
                "Sulaiman A Alghunaim",
                "Kun Yuan"
            ],
            "title": "A unified and refined convergence analysis for non-convex decentralized learning",
            "venue": "arXiv preprint arXiv:2110.09993,",
            "year": 2021
        },
        {
            "authors": [
                "Yossi Arjevani",
                "Yair Carmon",
                "John C. Duchi",
                "Dylan J. Foster",
                "Nathan Srebro",
                "Blake E. Woodworth"
            ],
            "title": "Lower bounds for non-convex stochastic optimization",
            "year": 1912
        },
        {
            "authors": [
                "Tianyi Chen",
                "Xiao Jin",
                "Yuejiao Sun",
                "Wotao Yin"
            ],
            "title": "Vafl: a method of vertical asynchronous federated learning",
            "venue": "arXiv preprint arXiv:2007.06081,",
            "year": 2020
        },
        {
            "authors": [
                "Xuxing Chen",
                "Tesi Xiao",
                "Krishnakumar Balasubramanian"
            ],
            "title": "Optimal algorithms for stochastic bilevel optimization under relaxed smoothness conditions",
            "venue": "arXiv preprint arXiv:2306.12067,",
            "year": 2023
        },
        {
            "authors": [
                "Yujing Chen",
                "Yue Ning",
                "Martin Slawski",
                "Huzefa Rangwala"
            ],
            "title": "Asynchronous online federated learning for edge devices with non-iid data",
            "venue": "IEEE International Conference on Big Data (Big Data),",
            "year": 2020
        },
        {
            "authors": [
                "Ashok Cutkosky",
                "Francesco Orabona"
            ],
            "title": "Momentum-based variance reduction in non-convex sgd",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Rudrajit Das",
                "Anish Acharya",
                "Abolfazl Hashemi",
                "Sujay Sanghavi",
                "Inderjit S Dhillon",
                "Ufuk Topcu"
            ],
            "title": "Faster non-convex federated learning via global and local momentum",
            "venue": "In Uncertainty in Artificial Intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "P. Di Lorenzo",
                "G. Scutari"
            ],
            "title": "Next: In-network nonconvex optimization",
            "venue": "IEEE Transactions on Signal and Information Processing over Networks,",
            "year": 2016
        },
        {
            "authors": [
                "Alp Emre Durmus",
                "Zhao Yue",
                "Matas Ramon",
                "Mattina Matthew",
                "Whatmough Paul",
                "Saligrama Venkatesh"
            ],
            "title": "Federated learning based on dynamic regularization",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Alireza Fallah",
                "Aryan Mokhtari",
                "Asuman Ozdaglar"
            ],
            "title": "Personalized federated learning with theoretical guarantees: A model-agnostic meta-learning approach",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Cong Fang",
                "Chris Junchi Li",
                "Zhouchen Lin",
                "Tong Zhang"
            ],
            "title": "Spider: Near-optimal non-convex optimization via stochastic path integrated differential estimator",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Ilyas Fatkhullin",
                "Alexander Tyurin",
                "Peter Richt\u00e1rik"
            ],
            "title": "Momentum provably improves error feedback",
            "venue": "arXiv preprint arXiv:2305.15155,",
            "year": 2023
        },
        {
            "authors": [
                "Zhishuai Guo",
                "Yi Xu",
                "Wotao Yin",
                "Rong Jin",
                "Tianbao Yang"
            ],
            "title": "A novel convergence analysis for algorithms of the adam family",
            "venue": "arXiv preprint arXiv:2112.03459,",
            "year": 2021
        },
        {
            "authors": [
                "Farzin Haddadpour",
                "Mohammad Mahdi Kamani",
                "Aryan Mokhtari",
                "Mehrdad Mahdavi"
            ],
            "title": "Federated learning with compression: Unified analysis and sharp guarantees",
            "venue": "In International Conference on Artificial Intelligence and Statistics,",
            "year": 2021
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2016
        },
        {
            "authors": [
                "Yutong He",
                "Xinmeng Huang",
                "Yiming Chen",
                "Wotao Yin",
                "Kun Yuan"
            ],
            "title": "Lower bounds and accelerated algorithms in distributed stochastic optimization with communication compression",
            "venue": "arXiv preprint arXiv:2305.07612,",
            "year": 2023
        },
        {
            "authors": [
                "Yutong He",
                "Xinmeng Huang",
                "Kun Yuan"
            ],
            "title": "Unbiased compression saves communication in distributed optimization: When and how much",
            "venue": "In Thirty-seventh Conference on Neural Information Processing Systems,",
            "year": 2023
        },
        {
            "authors": [
                "Tzu-Ming Harry Hsu",
                "Hang Qi",
                "Matthew Brown"
            ],
            "title": "Measuring the effects of non-identical data distribution for federated visual classification",
            "year": 1909
        },
        {
            "authors": [
                "Kun Huang",
                "Xiao Li",
                "Shi Pu"
            ],
            "title": "Distributed stochastic optimization under a general variance condition",
            "venue": "arXiv preprint arXiv:2301.12677,",
            "year": 2023
        },
        {
            "authors": [
                "Xinmeng Huang",
                "Ping Li",
                "Xiaoyun Li"
            ],
            "title": "Stochastic controlled averaging for federated learning with communication compression",
            "venue": "In The Twelfth International Conference on Learning Representations,",
            "year": 2024
        },
        {
            "authors": [
                "Jiayin Jin",
                "Jiaxiang Ren",
                "Yang Zhou",
                "Lingjuan Lyu",
                "Ji Liu",
                "Dejing Dou"
            ],
            "title": "Accelerated federated learning with decoupled adaptive optimization",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Peter Kairouz",
                "H Brendan McMahan",
                "Brendan Avent",
                "Aur\u00e9lien Bellet",
                "Mehdi Bennis",
                "Arjun Nitin Bhagoji",
                "Kallista Bonawitz",
                "Zachary Charles",
                "Graham Cormode",
                "Rachel Cummings"
            ],
            "title": "Advances and open problems in federated learning",
            "venue": "Foundations and Trends\u00ae in Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Sai Praneeth Karimireddy",
                "Martin Jaggi",
                "Satyen Kale",
                "Mehryar Mohri",
                "Sashank J Reddi",
                "Sebastian U Stich",
                "Ananda Theertha Suresh"
            ],
            "title": "Mime: Mimicking centralized stochastic algorithms in federated learning",
            "year": 2020
        },
        {
            "authors": [
                "Sai Praneeth Karimireddy",
                "Satyen Kale",
                "Mehryar Mohri",
                "Sashank Reddi",
                "Sebastian Stich",
                "Ananda Theertha Suresh"
            ],
            "title": "Scaffold: Stochastic controlled averaging for federated learning",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Prashant Khanduri",
                "Pranay Sharma",
                "Haibo Yang",
                "Mingyi Hong",
                "Jia Liu",
                "Ketan Rajawat",
                "Pramod Varshney"
            ],
            "title": "Stem: A stochastic two-sided momentum algorithm achieving near-optimal sample and communication complexities for federated learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Geeho Kim",
                "Jinkyu Kim",
                "Bohyung Han"
            ],
            "title": "Communication-efficient federated learning with acceleration of global momentum",
            "year": 2022
        },
        {
            "authors": [
                "Anastasia Koloskova",
                "Nicolas Loizou",
                "Sadra Boreiri",
                "Martin Jaggi",
                "Sebastian Stich"
            ],
            "title": "A unified theory of decentralized sgd with changing topology and local updates",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Jakub Kone\u010dn\u1ef3",
                "H Brendan McMahan",
                "Felix X Yu",
                "Peter Richt\u00e1rik",
                "Ananda Theertha Suresh",
                "Dave Bacon"
            ],
            "title": "Federated learning: Strategies for improving communication efficiency",
            "venue": "arXiv preprint arXiv:1610.05492,",
            "year": 2016
        },
        {
            "authors": [
                "A. Krizhevsky",
                "G. Hinton"
            ],
            "title": "Learning multiple layers of features from tiny images",
            "venue": "Master\u2019s thesis,",
            "year": 2009
        },
        {
            "authors": [
                "Tian Li",
                "Anit Kumar Sahu",
                "Manzil Zaheer",
                "Maziar Sanjabi",
                "Ameet Talwalkar",
                "Virginia Smith"
            ],
            "title": "Federated optimization in heterogeneous networks",
            "venue": "Proceedings of Machine learning and systems,",
            "year": 2020
        },
        {
            "authors": [
                "Xiang Li",
                "Kaixuan Huang",
                "Wenhao Yang",
                "Shusen Wang",
                "Zhihua Zhang"
            ],
            "title": "On the convergence of fedavg on non-iid data",
            "venue": "In International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "Xianfeng Liang",
                "Shuheng Shen",
                "Jingchang Liu",
                "Zhen Pan",
                "Enhong Chen",
                "Yifei Cheng"
            ],
            "title": "Variance reduced local sgd with lower communication complexity",
            "year": 1912
        },
        {
            "authors": [
                "Tao Lin",
                "Sebastian U Stich",
                "Kumar Kshitij Patel",
                "Martin Jaggi"
            ],
            "title": "Don\u2019t use large mini-batches, use local sgd",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Yanli Liu",
                "Yuan Gao",
                "Wotao Yin"
            ],
            "title": "An improved analysis of stochastic gradient descent with momentum",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Brendan McMahan",
                "Eider Moore",
                "Daniel Ramage",
                "Seth Hampson",
                "Blaise Aguera y Arcas"
            ],
            "title": "Communication-efficient learning of deep networks from decentralized data",
            "venue": "In Artificial Intelligence and Statistics,",
            "year": 2017
        },
        {
            "authors": [
                "H.B. McMahan",
                "Eider Moore",
                "Daniel Ramage",
                "Seth Hampson",
                "Blaise Ag\u00fcera y Arcas"
            ],
            "title": "Communication-efficient learning of deep networks from decentralized data",
            "venue": "In AISTATS,",
            "year": 2017
        },
        {
            "authors": [
                "Aritra Mitra",
                "Rayana Jaafar",
                "George J Pappas",
                "Hamed Hassani"
            ],
            "title": "Linear convergence in federated learning: Tackling client heterogeneity and sparse gradients",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Tomoya Murata",
                "Taiji Suzuki"
            ],
            "title": "Bias-variance reduced local sgd for less heterogeneous federated learning",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Edward Duc Hien Nguyen",
                "Sulaiman A Alghunaim",
                "Kun Yuan",
                "C\u00e9sar A Uribe"
            ],
            "title": "On the performance of gradient tracking with local updates",
            "year": 2022
        },
        {
            "authors": [
                "Lam M Nguyen",
                "Jie Liu",
                "Katya Scheinberg",
                "Martin Tak\u00e1\u010d. Sarah"
            ],
            "title": "A novel method for machine learning problems using stochastic recursive gradient",
            "venue": "In International Conference on Machine Learning,",
            "year": 2017
        },
        {
            "authors": [
                "Kumar Kshitij Patel",
                "Lingxiao Wang",
                "Blake E Woodworth",
                "Brian Bullins",
                "Nati Srebro"
            ],
            "title": "Towards optimal communication complexity in distributed non-convex optimization",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Krishna Pillutla",
                "Kshitiz Malik",
                "Abdel-Rahman Mohamed",
                "Mike Rabbat",
                "Maziar Sanjabi",
                "Lin Xiao"
            ],
            "title": "Federated learning with partial model personalization",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Boris T Polyak"
            ],
            "title": "Some methods of speeding up the convergence of iteration methods",
            "venue": "Ussr computational mathematics and mathematical physics,",
            "year": 1964
        },
        {
            "authors": [
                "Shi Pu",
                "Angelia Nedi\u0107"
            ],
            "title": "Distributed stochastic gradient tracking methods",
            "venue": "Mathematical Programming,",
            "year": 2020
        },
        {
            "authors": [
                "Sashank J Reddi",
                "Zachary Charles",
                "Manzil Zaheer",
                "Zachary Garrett",
                "Keith Rush",
                "Jakub Kone\u010dn\u1ef3",
                "Sanjiv Kumar",
                "Hugh Brendan McMahan"
            ],
            "title": "Adaptive federated optimization",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Amirhossein Reisizadeh",
                "Aryan Mokhtari",
                "Hamed Hassani",
                "Ali Jadbabaie",
                "Ramtin Pedarsani"
            ],
            "title": "Fedpaq: A communication-efficient federated learning method with periodic averaging and quantization",
            "venue": "In International Conference on Artificial Intelligence and Statistics,",
            "year": 2020
        },
        {
            "authors": [
                "Elsa Rizk",
                "Stefan Vlaski",
                "Ali H Sayed"
            ],
            "title": "Privatized graph federated learning",
            "year": 2022
        },
        {
            "authors": [
                "Sebastian Urban Stich"
            ],
            "title": "Local sgd converges fast and communicates little",
            "venue": "In International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "Canh T Dinh",
                "Nguyen Tran",
                "Josh Nguyen"
            ],
            "title": "Personalized federated learning with moreau envelopes",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Alysa Ziying Tan",
                "Han Yu",
                "Lizhen Cui",
                "Qiang Yang"
            ],
            "title": "Towards personalized federated learning",
            "venue": "IEEE Transactions on Neural Networks and Learning Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Jianyu Wang",
                "Gauri Joshi"
            ],
            "title": "Cooperative sgd: A unified framework for the design and analysis of local-update sgd algorithms",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2021
        },
        {
            "authors": [
                "Jianyu Wang",
                "Qinghua Liu",
                "Hao Liang",
                "Gauri Joshi",
                "H Vincent Poor"
            ],
            "title": "Tackling the objective inconsistency problem in heterogeneous federated optimization",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Jianyu Wang",
                "Vinayak Tantia",
                "Nicolas Ballas",
                "Michael Rabbat"
            ],
            "title": "SlowMo: Improving communication-efficient distributed sgd with slow momentum",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Ran Xin",
                "Usman A Khan",
                "Soummya Kar"
            ],
            "title": "An improved convergence analysis for decentralized online stochastic non-convex optimization",
            "venue": "IEEE Transactions on Signal Processing,",
            "year": 2020
        },
        {
            "authors": [
                "Chenhao Xu",
                "Youyang Qu",
                "Yong Xiang",
                "Longxiang Gao"
            ],
            "title": "Asynchronous federated learning on heterogeneous devices: A survey",
            "venue": "arXiv preprint arXiv:2109.04269,",
            "year": 2021
        },
        {
            "authors": [
                "J. Xu",
                "S. Zhu",
                "Y.C. Soh",
                "L. Xie"
            ],
            "title": "Augmented distributed gradient methods for multi-agent optimization under uncoordinated constant stepsizes",
            "venue": "In IEEE Conference on Decision and Control (CDC),",
            "year": 2015
        },
        {
            "authors": [
                "Jing Xu",
                "Sen Wang",
                "Liwei Wang",
                "Andrew Chi-Chih Yao"
            ],
            "title": "FedCM: Federated learning with client-level momentum",
            "year": 2021
        },
        {
            "authors": [
                "Yan Yan",
                "Tianbao Yang",
                "Zhe Li",
                "Qihang Lin",
                "Yi Yang"
            ],
            "title": "A unified analysis of stochastic momentum methods for deep learning",
            "venue": "In International Joint Conference on Artificial Intelligence,",
            "year": 2018
        },
        {
            "authors": [
                "Haibo Yang",
                "Minghong Fang",
                "Jia Liu"
            ],
            "title": "Achieving linear speedup with partial worker participation in non-iid federated learning",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Hao Yu",
                "Rong Jin",
                "Sen Yang"
            ],
            "title": "On the linear speedup analysis of communication efficient momentum SGD for distributed non-convex optimization",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Hao Yu",
                "Sen Yang",
                "Shenghuo Zhu"
            ],
            "title": "Parallel restarted sgd with faster convergence and less communication: Demystifying why model averaging works for deep learning",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2019
        },
        {
            "authors": [
                "Kun Yuan",
                "Bicheng Ying",
                "Xiaochuan Zhao",
                "Ali H. Sayed"
            ],
            "title": "Exact diffusion for distributed optimization and learning\u2014part i: Algorithm development",
            "venue": "IEEE Transactions on Signal Processing,",
            "year": 2019
        },
        {
            "authors": [
                "Kun Yuan",
                "Sulaiman A Alghunaim",
                "Bicheng Ying",
                "Ali H Sayed"
            ],
            "title": "On the influence of biascorrection on distributed stochastic optimization",
            "venue": "IEEE Transactions on Signal Processing,",
            "year": 2020
        },
        {
            "authors": [
                "Kun Yuan",
                "Yiming Chen",
                "Xinmeng Huang",
                "Yingya Zhang",
                "Pan Pan",
                "Yinghui Xu",
                "Wotao Yin"
            ],
            "title": "DecentLaM: Decentralized momentum sgd for large-batch deep training",
            "venue": "International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Kun Yuan",
                "Sulaiman A Alghunaim",
                "Xinmeng Huang"
            ],
            "title": "Removing data heterogeneity influence enhances network topology dependence of decentralized sgd",
            "venue": "Journal of Machine Learning Research,",
            "year": 2023
        },
        {
            "authors": [
                "Nesterov Yurri"
            ],
            "title": "Introductory Lectures on Convex Optimization: A Basic Course",
            "year": 2004
        },
        {
            "authors": [
                "Xinwei Zhang",
                "Mingyi Hong",
                "Sairaj Dhople",
                "Wotao Yin",
                "Yang Liu"
            ],
            "title": "FedPD: A federated learning framework with adaptivity to non-iid data",
            "venue": "IEEE Transactions on Signal Processing,",
            "year": 2021
        },
        {
            "authors": [
                "Fan Zhou",
                "Guojing Cong"
            ],
            "title": "On the convergence properties of a k-step averaging stochastic gradient descent algorithm for nonconvex optimization",
            "venue": "arXiv preprint arXiv:1708.01012,",
            "year": 2017
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Federated learning (FL) is a powerful paradigm for large-scale machine learning (Konec\u030cny\u0300 et al., 2016; McMahan et al., 2017a). In situations where data and computational resources are dispersed among a diverse range of clients, including phones, tablets, sensors, hospitals, and other devices and agents, federated learning facilitates local data processing and collaboration among these clients (Kairouz et al., 2021). Consequently, a centralized model can be trained without transmitting decentralized data from clients directly to servers, thereby ensuring a fundamental level of privacy.\nFederated learning encounters several significant challenges in algorithmic development. Firstly, the reliability and relatively slow nature of network connections between the server and clients pose obstacles to efficient communication during the training process. Secondly, the dynamic availability of only a small subset of clients for training at any given time demands strategies that can adapt to this variable environment. Lastly, the presence of substantial heterogeneity of non-iid data across different clients further complicates the training process.\nFEDAVG (Konec\u030cny\u0300 et al., 2016; McMahan et al., 2017a; Stich, 2019; Yu et al., 2019a; Lin et al., 2020; Wang & Joshi, 2021) emerges as a prevalent algorithm for FL, leveraging multiple stochastic gradient descent (SGD) steps within each client before communicating with a central server. While FEDAVG is readily implementable and succeeds in certain applications, its performance is notably\n\u2217Equal Contribution. This work is supported in part by National Natural Science Foundation of China (NSFC) Grant 12301392, 92370121, and 12288101\n\u2020Corresponding Author. Kun Yuan is also affiliated with National Engineering Labratory for Big Data Analytics and Applications, and AI for Science Institute, Beijing, China.\nhindered by the presence of data heterogeneity, i.e., non-iid clients, even when all clients participate in the training process (Li et al., 2019; Yang et al., 2021). To mitigate the influence of data heterogeneity, SCAFFOLD (Karimireddy et al., 2020b) maintains a control variable on each client to compensate for \u201cclient drift\u201d in its local SGD updates, making convergence more robust to data heterogeneity and client sampling. Due to their practicality and effectiveness, FEDAVG and SCAFFOLD have become foundational algorithms in federated learning, leading to the development of numerous variants that cater to decentralized (Koloskova et al., 2020; Rizk et al., 2022; Nguyen et al., 2022; Alghunaim, 2023), compressed (Haddadpour et al., 2021; Reisizadeh et al., 2020; Mitra et al., 2021), asynchronous (Chen et al., 2020a;b; Xu et al., 2021a), and personalized (Fallah et al., 2020; Pillutla et al., 2022; Tan et al., 2022; T Dinh et al., 2020) federated learning scenarios.\nVarious methods have been proposed to enhance the convergence of FEDAVG, SCAFFOLD, and their variance-reduced1 extensions. While exhibiting superior convergence rates, these approaches typically make impractical adjustments to algorithmic structures. For instance, STEM (Khanduri et al., 2021) requires increasing either the batch size or the number of local steps with algorithmic iterations. Similarly, CE-LSGD (Patel et al., 2022) and MIME (Karimireddy et al., 2020a) mandate computing a large-batch or even full-batch local gradient per round for each client. Additionally, FEDPROX (Li et al., 2020), FEDPD (Zhang et al., 2021), and FEDDYN (Durmus et al., 2021) rely on solving \u201clocal problems\u201d to an extremely high precision. These adjustments may not align with the practical constraints in federated learning setups.\nFurthermore, many of these algorithms, including FEDAVG, STEM, FEDPROX, MIME, and CESGD, still rely on the assumption of bounded data heterogeneity. When this assumption is violated, their theoretical analyses become invalid. While some algorithms, such as LED (Alghunaim, 2023) and VRL-SGD (Liang et al., 2019), can handle unbounded data heterogeneity, their convergence rates are not state-of-the-art, as stated in Table 1. These limitations motivate us to develop novel strategies that are easy to implement, robust to data heterogeneity, and exhibit superior convergence."
        },
        {
            "heading": "1.1 MAIN RESULTS AND CONTRIBUTIONS",
            "text": "This paper examines the utilization of momentum to enhance the performance of FEDAVG and SCAFFOLD. To ensure simplicity and practicality in implementations, we only introduce momentum to the local SGD steps, avoiding any inclusion of impractical elements, such as gradient computation of multiple minibatches or solving local problems to high precision. Remarkably, this straightforward approach effectively alleviates the necessity for stringent assumptions regarding bounded data heterogeneity, leading to noteworthy improvements in convergence rates. The main findings and contributions of this paper are summarized below.\nFirst, when all clients participate in the training process: \u2022 We show that incorporating momentum allows FEDAVG and its variance-reduced extension to\nconverge under unbounded data heterogeneity, even using constant local learning rates. This is rather surprising as, to our knowledge, all existing analyses for FEDAVG, e.g., Karimireddy et al. (2020b); Yang et al. (2021); Wang et al. (2020b) require bounded data heterogeneity even with diminishing local learning rates.\n\u2022 We further establish that, by effectively removing the influence of data heterogeneity on convergence, momentum empowers FEDAVG and its variance-reduced extension with state-ofthe-art convergence rates in the context of full client participation.\nSecond, when partial clients participate in the training process per iteration: \u2022 The proposed SCAFFOLD-M that incorporates momentum into SCAFFOLD achieves prov-\nably faster convergence. To our knowledge, this is the first result that improves SCAFFOLD without imposing additional assumptions beyond those in Karimireddy et al. (2020b).\n\u2022 We further introduce momentum to SCAFFOLD with variance reduction, obtaining the first variance-reduced FL algorithm that converges without bounded data heterogeneity. This method attains the state-of-the-art convergence rate in the context of partial client participation and unbounded data heterogeneity.\nTables 1 and 2 present a comprehensive comparison of the convergence rates and associated assumptions of prior algorithms and our proposed methods. It is observed that by simply adding momentum\n1Throughout the paper, variance reduction refers to techniques aiming to mitigate the influence of withinclient gradient stochasticity, as opposed to the inter-client data heterogeneity.\nto local steps, FEDAVG, SCAFFOLD, and their variance-reduced variants all attain state-of-the-art convergence rates without resorting to further assumptions such as bounded data heterogeneity. We support our theoretical findings with extensive numerical experiments."
        },
        {
            "heading": "1.2 RELATED WORK",
            "text": "FL with homogeneous clients. FEDAVG is a well-known algorithm introduced by McMahan et al. (2017b) as a heuristic to enhance communication efficiency and data privacy in federated learning. Numerous subsequent studies have focused on analyzing its convergence under the assumption of homogeneous datasets, where clients are independent and identically distributed (iid) and all clients participate fully (Stich, 2019; Yu et al., 2019b; Wang & Joshi, 2021; Lin et al., 2020; Zhou & Cong, 2017). However, when dealing with heterogeneous clients and partial client participation, FEDAVG is found to be vulnerable to data heterogeneity because of the \u201dclient drift\u201d effect (Karimireddy et al., 2020b; Yang et al., 2021; Wang et al., 2020b; Li et al., 2019).\nFL with heterogeneous clients. Numerous research efforts are devoted to mitigating the impact of data heterogeneity in FL. For example, Li et al. (2020) propose FEDPROX, which introduces a proximal term to the objective function. Yang et al. (2021) utilize a two-sided learning rate approach, while Wang et al. (2020a) propose FEDNOVA, a normalized averaging method. Additionally, Zhang et al. (2021) presents FEDPD, which addresses data heterogeneity from a primal-dual optimization perspective. Notably, Karimireddy et al. (2020b) introduces SCAFFOLD, an effective algorithm that employs control variables to mitigate the influence of data heterogeneity and partial client participation. FEDGATE (Haddadpour et al., 2021) and LED (Alghunaim, 2023) are two recent effective algorithms that have alleviated the impact of data heterogeneity, utilizing gradient tracking (Xu et al.,\n2015; Di Lorenzo & Scutari, 2016; Pu & Nedic\u0301, 2020; Xin et al., 2020; Alghunaim & Yuan, 2021; Huang et al., 2024) and exact-diffusion (Yuan et al., 2019; 2020; 2023) techniques, respectively. FL with momentum. The momentum mechanism dates back to Nesterov\u2019s acceleration (Yurri, 2004) and Polyak\u2019s heavy-ball method (Polyak, 1964), which later flourishes in the stochastic optimization (Yan et al., 2018; Yu et al., 2019a; Liu et al., 2020) and other areas (Yuan et al., 2021; He et al., 2023b;a; Chen et al., 2023; Huang et al., 2024). Extensive research has explored incorporating momentum into FL (Reddi et al., 2021; Wang et al., 2020b; Karimireddy et al., 2020a; Khanduri et al., 2021; Patel et al., 2022; Das et al., 2022; Yu et al., 2019a; Xu et al., 2021b), and have demonstrated its impact on enhancing the empirical performance of FL methods (Wang et al., 2020b; Xu et al., 2021b; Reddi et al., 2021; Jin et al., 2022; Kim et al., 2022). However, whether momentum can offer theoretical benefits to FL, especially in mitigating the impact of data heterogeneity, remains unclear. This work demonstrates that momentum can benefit non-iid federated learning simply and provably. Notably, the utility of momentum is demonstrated in domains other than FL. For instance, Guo et al. (2021) proves that momentum can correct the bias experienced by the ADAM method, while recently Fatkhullin et al. (2023) shows that momentum can improve the error feedback technique in communication compression. The analysis presented in this work distinguishes from prior works including Guo et al. (2021); Fatkhullin et al. (2023) due to the unique challenges encountered in FL including multiple local updates, data heterogeneity, and partial client participation."
        },
        {
            "heading": "2 PROBLEM SETUP",
            "text": "This section formulates the problem of non-iid federated learning. Formally, we consider minimizing the following objective with the fewest number of client-server communication rounds:\nmin x\u2208Rd\nf(x) := 1\nN N\u2211 i=1 fi(x) where fi(x) := E\u03bei\u223cDi [F (x; \u03bei)].\nHere, the random variable \u03bei represents a local data point available at client i, while the function fi(x) denotes the non-convex local loss function associated with client i. This function takes expectation concerning the local data distribution Di. In practice, the local data distributions Di among different clients typically differ from each other, resulting in the inequality fi(x) \u0338= fj(x) for any pair of nodes i and j. This phenomenon is commonly referred to as data heterogeneity. If all local clients were homogeneous, meaning that all local data samples follow the same distribution D, we would have fi(x) = fj(x) for any i and j. In addition, throughout the paper, we assume that the function f is bounded from below and possesses a global minimum f\u2217. To facilitate convergence analysis, we also introduce the following standard assumptions. Assumption 1 (STANDARD SMOOTHNESS). Each local objective fi is L-smooth, i.e., \u2225\u2207fi(x)\u2212\u2207fi(y)\u2225 \u2264 L\u2225x\u2212 y\u2225, for any x, y \u2208 Rd and 1 \u2264 i \u2264 N . Assumption 2 (SAMPLE-WISE SMOOTHNESS). Each sample-wise objective F (x; \u03be) is L-smooth, i.e., \u2225\u2207F (x; \u03bei)\u2212\u2207F (y; \u03bei)\u2225 \u2264 L\u2225x\u2212 y\u2225 for any x, y \u2208 Rd, 1 \u2264 i \u2264 N , and \u03bei iid\u223c Di.\nAlgorithm 1 FEDAVG-M: FEDAVG with momentum Require: initial model x0 and gradient estimate g0, local learning rate \u03b7, global learning rate \u03b3,\nmomentum \u03b2 for r = 0, \u00b7 \u00b7 \u00b7 , R\u2212 1 do\nfor each client i \u2208 {1, . . . , N} in parallel do Initialize local model xr,0i = x r\nfor k = 0, \u00b7 \u00b7 \u00b7 ,K \u2212 1 do Compute gr,ki = \u03b2\u2207F (x r,k i ; \u03be r,k i ) + (1\u2212 \u03b2)gr \u25b7 \u03b2 = 1 implies FEDAVG\nUpdate local model xr,k+1i = x r,k i \u2212 \u03b7g r,k i\nend for end for Aggregate local updates gr+1 = 1\u03b7NK \u2211N i=1 ( xr \u2212 xr,Ki ) Update global model xr+1 = xr \u2212 \u03b3gr+1\nend for\nIt is worth noting that Assumption 2 implies Assumption 1, which is typically used in variancereduced algorithms, e.g., Karimireddy et al. (2020a); Khanduri et al. (2021); Fang et al. (2018); Cutkosky & Orabona (2019). We will utilize either Assumption 1 or 2 in different algorithms. It is worth highlighting that, these are the only assumptions required for all our theoretical analyses.\nAssumption 3 (STOCHASTIC GRADIENT). There exists \u03c3 \u2265 0 such that for any x \u2208 Rd and 1 \u2264 i \u2264 N , E\u03bei [\u2207F (x; \u03bei)] = \u2207fi(x) and E\u03bei [\u2225\u2207F (x; \u03bei)\u2212\u2207fi(x)\u22252] \u2264 \u03c32 where \u03bei iid\u223c Di."
        },
        {
            "heading": "3 ACCELERATING FEDAVG WITH MOMENTUM",
            "text": "This section focuses on full client participation. We will introduce momentum to both FEDAVG and its variance-reduced extension. Furthermore, we will justify that the incorporation of momentum effectively mitigates the impact of data heterogeneity, leading to improved convergence rates."
        },
        {
            "heading": "3.1 FEDAVG WITH MOMENTUM",
            "text": "Algorithm. We introduce momentum to enhance the estimation of the stochastic gradient, resulting in the algorithm FEDAVG-M, as presented in Algorithm 1. In FEDAVG-M, the subscript i represents the client index, while the superscripts r and k denote the outer loop index and inner local update index, respectively. The structure of FEDAVG-M remains identical to the vanilla FEDAVG, except for the inclusion of momentum in gradient computation (see highlight in Algorithm 1):\ngr,ki = \u03b2\u2207F (x r,k i ; \u03be r,k i ) + (1\u2212 \u03b2)g r, (1) where \u03b2 \u2208 [0, 1] is the momentum coefficient, and gr represents a global gradient estimate updated in the outer loop r. It is important to note that FEDAVG-M will reduce to the vanilla FEDAVG when \u03b2 = 1. Furthermore, FEDAVG-M is easy to implement, as it maintains the same algorithmic structure and incurs no additional uplink communication overhead compared to FEDAVG. Notably, no extra downlink commmunication cost is needed if clients store the last iterate model xr so that momentum gr+1 can recovered through (xr+1 \u2212 xr)/\u03b3.\nConvergence property. The inclusion of momentum in FEDAVG yields notable theoretical improvements. Firstly, it eliminates the need for the data heterogeneity assumption, also known as the gradient similarity assumption. The assumption can be expressed as\n1\nN N\u2211 i=1 \u2225\u2207fi(x)\u2212\u2207f(x)\u22252 \u2264 \u03b62, \u2200x \u2208 Rd (Bounded data heterogeneity)\nwhere \u03b62 measures the magnitude of data heterogeneity. By incorporating momentum, the above assumption is no longer required for the convergence analysis of FEDAVG. Secondly, momentum enables FEDAVG to converge at a state-of-the-art rate. These improvements are justified as follows: Theorem 1. Under Assumption 1 and 3, if we set g0 = 0, \u03b2, \u03b3, and \u03b7 as in (5), FEDAVG-M enjoys\n1\nR R\u22121\u2211 r=0 E[\u2225\u2207f(xr)\u22252] \u2272 \u221a L\u2206\u03c32 NKR + L\u2206 R ,\nwhere \u2206 \u225c f(x0)\u2212minx f(x) and \u2272 absorbs numeric numbers. See proof in Appendix B.1.\nComparison with FEDAVG. Table 1 compares FEDAVG-M with prior algorithms when all clients participate in the training process. The results demonstrate that FEDAVG-M attains the most favorable convergence rate without relying on any assumption of data heterogeneity. Moreover, this rate matches the lower bound provided by Arjevani et al. (2019). Notably, a recent work (Huang et al., 2023) establishes the convergence of FEDAVG by relaxing the bounded data heterogeneity to a bound on f\u22c6 \u2212 1N \u2211N i=1 f \u22c6 i where f\n\u22c6 \u225c minx f(x) and f\u22c6i \u225c fi(x). However, their convergence does not benefit from local updates. Moreover, it still suffers from data heterogeneity f\u22c6 \u2212 1N \u2211N i=1 f \u22c6 i and gets slow as the number of clients N increases, resulting in a suboptimal rate. Comparison with FEDCM. FEDAVG-M coincides with the FEDCM algorithm proposed by Xu et al. (2021b). However, our result outperforms that of Xu et al. (2021b) in several aspects. First, our convergence only utilizes the standard smoothness of objectives and gradient stochasticity while Xu et al. (2021b) additionally require bounded data heterogeneity and bounded gradients which are rarely valid in practice, suggesting the limitation of their result. Second, the convergence established by Xu et al. (2021b) is significantly weaker than ours and cannot even asymptotically approach the customary rate O(1/ \u221a NKR) in non-convex FL, as demonstrated by the results stated in Table 1.\nConstant local learning rate. Based on Theorem 1, it can be inferred that when R \u2273 NKL\u2206/\u03c32, FEDAVG-M allows the utilization of constant local learning rate \u03b7 which does not necessarily decay as the number of communication rounds R increases. This characteristic eases the tuning of the local learning rate and improves empirical performance. In contrast, many existing convergence results of FEDAVG necessitate the adoption of local learning rates that diminish as R increases, as exemplified by e.g., Yang et al. (2021); Li et al. (2019); Karimireddy et al. (2020b); Koloskova et al. (2020). Intuition on the effectiveness of momentum. The momentum mechanism relies on an accumulated gradient estimate gr, which is updated through gr+1 = \u03b2NK \u2211N i=1 \u2211K\u22121 k=0 \u2207F (x r,k i ; \u03be r,k i ) + (1\u2212 \u03b2)gr. While gr is a biased gradient estimate, it exhibits reduced variance due to its accumulation nature compared to a stochastic gradient \u2207F (xr,ki ; \u03be r,k i ) computed with a single data minibatch. Importantly, by utilizing directions \u03b2\u2207F (xr,ki ; \u03be r,k i ) + (1\u2212 \u03b2)gr for local updates, an \u201canchoring\u201d effect is achieved, effectively mitigating the \u201cclient-drift\u201d phenomenon. In the extreme case where \u03b2 = 0, all clients remain synchronized in their local updates, eliminating the drift incurred by data heterogeneity in the vanilla FEDAVG. By appropriately tuning the coefficient \u03b2, FEDAVG-M maintains the same convergence rate as (Yang et al., 2021) while removing the requirement of data heterogeneity assumption utilized in their analysis."
        },
        {
            "heading": "3.2 VARIANCE-REDUCED FEDAVG WITH MOMENTUM",
            "text": "When each local loss function is further assumed to be sample-wise smooth (i.e., Assumption 2), we can replace the local descent direction in Algorithm 1 with a variance-reduced momentum direction\ngr,ki = \u2207F (x r,k i ; \u03be r,k i ) + (1\u2212 \u03b2)(g r \u2212\u2207F (xr\u22121; \u03ber,ki )) (2) to further enhance convergence, leading to variance-reduced FEDAVG with momentum, or FEDAVG-M-VR for short, see the detailed algorithm in Appendix B.2. The variable xr\u22121 is the last-iterate global model maintained in the server. The construction of the variance-reduced direction (2) effectively mitigates the influence of within-client gradient noise and can be traced back to SARAH (Nguyen et al., 2017) and STORM (Cutkosky & Orabona, 2019) in stochastic optimization; more discussion can be found in Tan et al. (2022). Same as FEDAVG-M, turning off the variancereduced momentum of FEDAVG-M-VR, i.e., setting \u03b2 = 1, recovers FEDAVG. FEDAVG-M-VR shares the same algorithmic structure and uplink communication workload as FEDAVG.\nTheorem 2. Under Assumption 2 and 3, if we take g0 = 1NB \u2211N i=1 \u2211B b=1 \u2207F (x0; \u03bebi )2 with {\u03bebi }Bb=1 iid\u223c Di and set \u03b2, \u03b3, \u03b7, and B as in (8), FEDAVG-M-VR enjoys\n1\nR R\u22121\u2211 r=0 E[\u2225\u2207f(xr)\u22252] \u2272 ( L\u2206\u03c3 NKR )2/3 + L\u2206 R .\nComparison with prior works. FEDAVG-M-VR outperforms existing variance-reduced FL methods in convergence rate, as justified by the results listed in Table 1. Additionally, compared to BVR-L-SGD (Murata & Suzuki, 2021) and CE-LSGD (Patel et al., 2022), FEDAVG-M-VR conducts each local update using 1 + 1/K = O(1) minibatches on average, contrasting with the\n2We use B data minibatches per client to initialize the gradient estimate g0 with small variance E[\u2225g0 \u2212 \u2207f(x0)\u22252], after which only one minibatch is utilized per local gradient computation. The same applies below.\nAlgorithm 2 SCAFFOLD-M: SCAFFOLD with momentum Require: initial model x0, gradient estimator g0, control variables {c0i }Ni=1 and c0, local learning\nrate \u03b7, global learning rate \u03b3, momentum \u03b2 for r = 0, \u00b7 \u00b7 \u00b7 , R\u2212 1 do\nUniformly sample clients Sr \u2286 {1, \u00b7 \u00b7 \u00b7 , N} with |Sr| = S for each client i \u2208 Sr in parallel do\nInitialize local model xr,0i = x r for k = 0, \u00b7 \u00b7 \u00b7 ,K \u2212 1 do Compute gr,ki = \u03b2(\u2207F (x r,k i ; \u03be r,k i )\u2212 cri + cr) + (1\u2212 \u03b2)gr \u25b7\u03b2 = 1 implies SCAFFOLD\nUpdate local model xr,k+1i = x r,k i \u2212 \u03b7g r,k i\nend for Update control variable cr+1i := 1 K \u2211K\u22121 k=0 \u2207F (x r,k i ; \u03be r,k i ) (for i /\u2208 Sr, c r+1 i = c r i )\nend for Aggregate local updates gr+1 = 1\u03b7SK \u2211 i\u2208Sr ( xr \u2212 xr,Ki ) Update global model xr+1 = xr \u2212 \u03b3gr+1 Update control variable cr+1 = cr + 1N \u2211 i\u2208Sr (c r+1 i \u2212 cri )\nend for\nO(K) minibatches in BVR-L-SGD and CE-LSGD. Furthermore, in comparison to STEM (Khanduri et al., 2021), FEDAVG-M-VR does not rely on the assumption of bounded data heterogeneity.\nBased on discussions in Sections 3.1 and 3.2, we demonstrate that FEDAVG-M and FEDAVG-MVR, in the context of full client participation, can achieve the state-of-the-art convergence rate without resorting to any stronger assumption, e.g., bounded data heterogeneity or impractical algorithmic structures such as a large number of minibatches in local gradient computation."
        },
        {
            "heading": "4 ACCELERATING SCAFFOLD WITH MOMENTUM",
            "text": "This section addresses the scenario where a random subset of clients participates in each training round. To tackle the challenges of partial participation, SCAFFOLD employs a control variable in each client to counteract the \u201cclient drift\u201d effect during local updates. We will introduce momentum to both SCAFFOLD and its variance-reduced extension to gain better convergence results."
        },
        {
            "heading": "4.1 SCAFFOLD WITH MOMENTUM",
            "text": "Algorithm. We introduce momentum to enhance the estimation of the stochastic gradient, resulting in the newly proposed algorithm SCAFFOLD-M, outlined in Algorithm 2. In SCAFFOLDM, S clients are randomly selected from a pool of N clients for each training iteration. The control variables ci and c are maintained by the client and server, respectively. In SCAFFOLD, the local descent direction is given by \u2207F (xr,ki ; \u03ber.ki ) \u2212 cri + cr. In contrast, SCAFFOLD-M incorporates momentum directions for local updates:\ngr,ki = \u03b2(\u2207F (x r,k i ; \u03be r,k i )\u2212 c r i + c r) + (1\u2212 \u03b2)gr, (3) where gr represents the global stochastic gradient vector maintained by the server. It is worth noting that SCAFFOLD-M can reduce to SCAFFOLD by setting \u03b2 = 1. Convergence property. Our momentum yields notable theoretical improvements to SCAFFOLD:\nTheorem 3. Under Assumption 1 and 3, if we take g0 = 0, c0i = 1 B \u2211B b=1 \u2207F (x0; \u03bebi ) with {\u03bebi }Bb=1 iid\u223c Di, c0 = 1N \u2211N i=1 c 0 i and set \u03b2, \u03b3, \u03b7, and B as in (9), SCAFFOLD-M enjoys\n1\nR R\u22121\u2211 r=0 E[\u2225\u2207f(xr)\u22252] \u2272 \u221a L\u2206\u03c32 SKR + L\u2206 R ( 1 + N2/3 S ) .\nComparison with SCAFFOLD. Compared to SCAFFOLD, SCAFFOLD-M exhibits provably faster convergence under partial participation, as justified in the comparison in Table 2. Specifically, when the gradients are noiseless (i.e., \u03c32 = 0), achieving the same level of stationarity E[\u2225\u2207f(x\u0302)\u22252] requires a ratio, between SCAFFOLD-M and SCAFFOLD, of communication rounds:\n1 +N2/3/S\n(N/S)2/3 =\n( S\nN\n)2/3 + 1\nS1/3 .\nThus, if S \u224d N2/3, SCAFFOLD-M achieves up to N2/9 times improvement in comparison to the vanilla SCAFFOLD, when aiming for the same level of stationarity. This improvement is significant as N , the number of clients in FL, is typically large. It is also worth highlighting that prior to our SCAFFOLD-M, SCAFFOLD was the only known non-iid FL method, to the best of our knowledge, that is robust to both unbounded data heterogeneity and partial client sampling, and capable of attaining linear speedup without relying on impractical algorithmic structures. The development of SCAFFOLD-M provides an alternative and superior choice."
        },
        {
            "heading": "4.2 VARIANCE-REDUCED SCAFFOLD WITH MOMENTUM",
            "text": "Similar to FEDAVG-M-VR, when the loss functions further enjoy the sample-wise smoothness property, we can obtain SCAFFOLD-M-VR by replacing momentum directions in Algorithm 2 with variance-reduced momentum directions\ngr,ki = \u2207F (x r,k i ; \u03be r,k i )\u2212 \u03b2(c r i \u2212 cr) + (1\u2212 \u03b2)(gr \u2212\u2207F (xr\u22121; \u03be r,k i )).\nThe detailed algorithm is in Appendix C.2, and the convergence is shown below.\nTheorem 4. Under Assumption 2 and 3, if we take c0i = 1 B \u2211B b=1 \u2207F (x0; \u03bebi ) with {\u03bebi }Bb=1\niid\u223c Di, g0 = c0 = 1N \u2211N i=1 c 0 i and set \u03b2, \u03b3, \u03b7, and B as in (11), SCAFFOLD-M-VR enjoys\n1\nR R\u22121\u2211 r=0 E[\u2225\u2207f(xr)\u22252] \u2272 ( L\u2206\u03c3 S \u221a KR )2/3 + L\u2206 R ( 1 + N1/2 S ) .\nComparison with variance-reduced methods. SCAFFOLD-M-VR outperforms all existing variance-reduced federated learning methods under partial participation in terms of convergence rate when data heterogeneity is severe (i.e., \u03b62 is large), see results listed in Table 2. Moreover, SCAFFOLD-M-VR has the following additional advantages. Compared to MIMELITEMVR (Karimireddy et al., 2020a), SCAFFOLD-M-VR does not need access to noiseless (full-batch) local gradients per iteration. Compared to MB-STORM (Patel et al., 2022) and CE-LSGD (Patel et al., 2022), SCAFFOLD-M-VR does not require bounded data heterogeneity and conducts each local update using 1 + 1/K = O(1) minibatches on average, instead of O(K). Based on Sections 4.1 and 4.2, we demonstrate that SCAFFOLD-M and SCAFFOLD-M-VR, in the context of partial client participation, can achieve state-of-the-art convergence rates without resorting to any stronger assumption, e.g., bounded data heterogeneity or impractical algorithmic structures such as a large number of minibatches in local gradient computation."
        },
        {
            "heading": "5 EXPERIMENTS",
            "text": "We present experiments on the CIFAR-10 dataset (Krizhevsky & Hinton, 2009) with two neural networks (three-layer MLP, ResNet-18) to justify the efficacy of our proposed algorithms. We evaluate them along with baselines including FEDAVG (Konec\u030cny\u0300 et al., 2016), SCAFFOLD (Karimireddy et al., 2020b), MB-STORM, CE-LSGD (Patel et al., 2022). Parameters (such as learning rates) in our implementation are set by grid search. We defer more experimental details and results (e.g., investigating the impact of momentum value \u03b2, setups with large N ) to Appendix D."
        },
        {
            "heading": "5.1 MLP EXPERIMENTS",
            "text": "The MLP experiments involve K = 32 local updates and N = 10 clients with data generated via the Dirichlet distribution (Hsu et al., 2019) with a parameter of 0.5 and 0.2 for full and partial client participation, respectively (small parameter value implies severe heterogeneity).\nFirstly, we compare the performance of FEDAVG-M and SCAFFOLD-M with their momentumfree counterparts, namely the vanilla FEDAVG and SCAFFOLD, under full client participation.\nThe results are presented in Figure 1(a), in which it can be observed that incorporating momentum significantly accelerates the convergence of both FEDAVG and SCAFFOLD. Secondly, we compare four momentum-based variance-reduced methods: MINIBATCH-STORM, CE-LSGD, FEDAVG-M-VR (our Algorithm 3), and SCAFFOLD-M-VR (our Algorithm 4), under full client participation. The comparison is illustrated in Figure 1(b). Our proposed methods outperform MINIBATCH-STORM and CE-LSGD with substantial margins. Lastly, we investigate the case of partial client participation with S = 1 and compare the performance of SCAFFOLD-M and SCAFFOLD-M-VR with vanilla SCAFFOLD. The results are presented in Figure 1(c). Once again, we observe that the introduction of momentum leads to significant improvements even when only a few clients participate in each round of training."
        },
        {
            "heading": "5.2 RESNET18 EXPERIMENTS",
            "text": "We further compare the above algorithms with a larger model: ResNet18 (He et al., 2016) under varying data heterogeneity by setting the parameter of Dirichlet distribution as 0.5 and 0.1, respectively, where a small parameter value suggests severe data heterogeneity. The experiment involves N = 10 clients and K = 16 local updates. We set S = 2 in partial client participation.\nFigure 2(a) reports the test accuracy of full and partial client participation under mild data heterogeneity while Figure 2(b) presents the counterparts under severe data heterogeneity, where the bottom right one is smoothed by plotting the best-so-far result. Again, we observe that FEDAVG-M and SCAFFOLD-M significantly outperform the vanilla FEDAVG and SCAFFOLD. Moreover, for ResNet18 and severe data heterogeneity, FEDAVG-M and SCAFFOLD-M exhibit notably greater advantages over their momentumless counterparts than for MLP scenarios under milder data heterogeneity. The observation demonstrates amplified advantages of the introduced momentum in larger models and severely heterogeneous data, which is aligned with our theoretical predictions and suggests the promising utility of our proposed methods in real-world applications."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "We propose momentum variants of FEDAVG and SCAFFOLD under various client participation situations and objectives\u2019 smoothness. All the momentum variants make simple and practical adjustments to FEDAVG and SCAFFOLD yet obtain state-of-the-art performance among their peers, especially under severe data heterogeneity or small gradient variance. In particular, FEDAVG-M converges under unbounded data heterogeneity and admits constant local learning rates, giving the first neat convergence for FEDAVG-type methods; SCAFFOLD-M is the first FL method that outperforms SCAFFOLD unconditionally. Experiments conducted support our theoretical findings."
        },
        {
            "heading": "A PRELIMINARIES OF PROOFS",
            "text": "Let F0 = \u2205 and Fr,ki := \u03c3({x r,j i }0\u2264j\u2264k \u222a Fr) and Fr+1 := \u03c3(\u222aiF r,K i ) for all r \u2265 0 where \u03c3(\u00b7) indicates the \u03c3-algebra. Let Er[\u00b7] := E[\u00b7|Fr] be the expectation, conditioned on the filtration Fr, with respect to the random variables {Sr, {\u03ber,ki }1\u2264i\u2264N,0\u2264k<K} in the r-th iteration. We also use E[\u00b7] to denote the global expectation over all randomness in algorithms. Through out the proofs, we use \u2211 i to represent the sum over i \u2208 {1, . . . , N}, while \u2211 i\u2208Sr denotes the sum over i \u2208 Sr.\nSimilarly, we use \u2211\nk to represent the sum of k \u2208 {0, . . . ,K \u2212 1}. For all r \u2265 0, we define the following auxiliary variables to facilitate proofs:\nEr := E[\u2225\u2207f(xr)\u2212 gr+1\u22252],\nUr := 1\nNK \u2211 i \u2211 k E[\u2225xr,ki \u2212 x r\u2225]2,\n\u03b6r,ki := E[x r,k+1 i \u2212 x r,k i |F r,k i ],\n\u039er := 1\nN N\u2211 i=1 E[\u2225\u03b6r,0i \u2225 2],\nVr := 1\nN N\u2211 i=1 E[\u2225cri \u2212\u2207fi(xr\u22121)\u22252].\nWe remark that quantity Vr is only used in the analysis of SCAFFOLD-based algorithms. Throughout the appendix, we let \u2206 := f(x0) \u2212 f\u2217, G0 := 1N \u2211 i \u2225\u2207fi(x0)\u22252, x\u22121 := x0 and E\u22121 := E[\u2225\u2207f(x0)\u2212 g0\u22252]. We will use the following foundational lemma for all our algorithms. Lemma 5. Under Assumption 1, if \u03b3L \u2264 124 , the following holds all r \u2265 0:\nE[f(xr+1)] \u2264 E[f(xr)]\u2212 11\u03b3 24 E[\u2225\u2207f(xr)\u22252] + 13\u03b3 24 Er.\nProof. Since f is L-smooth, we have\nf(xr+1) \u2264f(xr) + \u27e8\u2207f(xr), xr+1 \u2212 xr\u27e9+ L 2 \u2225xr+1 \u2212 xr\u22252\n=f(xr)\u2212 \u03b3\u2225\u2207f(xr)\u22252 + \u03b3\u27e8\u2207f(xr),\u2207f(xr)\u2212 gr+1\u27e9+ L\u03b3 2\n2 \u2225gr+1\u22252.\nSince xr+1 = xr \u2212 \u03b3gr+1, using Young\u2019s inequality, we further have f(xr+1)\n\u2264f(xr)\u2212 \u03b3 2 \u2225\u2207f(xr)\u22252 + \u03b3 2 \u2225\u2207f(xr)\u2212 gr+1\u22252 + L\u03b32(\u2225\u2207f(xr)\u22252 + \u2225\u2207f(xr)\u2212 gr+1\u22252) \u2264f(xr)\u2212 11\u03b3 24 \u2225\u2207f(xr)\u22252 + 13\u03b3 24 \u2225\u2207f(xr)\u2212 gr+1\u22252,\nwhere the last inequality is due to \u03b3L \u2264 124 . Taking the global expectation completes the proof.\nTo handle local updates and client sampling, we will also use the following technical lemmas.\nLemma 6 (Karimireddy et al. (2020b)). Suppose {X1, \u00b7 \u00b7 \u00b7 , X\u03c4} \u2282 Rd be random variables that are potentially dependent. If their marginal means and variances satisfy E[Xi] = \u00b5i and E[\u2225Xi \u2212 \u00b5i\u22252] \u2264 \u03c32, then it holds that\nE \u2225\u2225\u2225\u2225\u2225 \u03c4\u2211\ni=1\nXi \u2225\u2225\u2225\u2225\u2225 2  \u2264 \u2225\u2225\u2225\u2225\u2225 \u03c4\u2211 i=1 \u00b5i \u2225\u2225\u2225\u2225\u2225 2 + \u03c42\u03c32.\nIf they are correlated in the Markov way such that E[Xi|Xi\u22121, \u00b7 \u00b7 \u00b7X1] = \u00b5i and E[\u2225Xi \u2212 \u00b5i\u22252 | \u00b5i] \u2264 \u03c32, i.e., the variables {Xi \u2212 \u00b5i} form a martingale. Then the following tighter bound holds:\nE \u2225\u2225\u2225\u2225\u2225 \u03c4\u2211\ni=1\nXi \u2225\u2225\u2225\u2225\u2225 2  \u2264 2E \u2225\u2225\u2225\u2225\u2225 \u03c4\u2211\ni=1\n\u00b5i \u2225\u2225\u2225\u2225\u2225 2 + 2\u03c4\u03c32.\nLemma 7. Given vectors v1, \u00b7 \u00b7 \u00b7 , vN \u2208 Rd and v\u0304 = 1N \u2211N\ni=1 vi, if we sample S \u2282 {1, \u00b7 \u00b7 \u00b7 , N} uniformly randomly such that |S| = S, then it holds that\nE \u2225\u2225\u2225\u2225\u2225 1S \u2211 i\u2208S vi \u2225\u2225\u2225\u2225\u2225 2  = \u2225v\u0304\u22252 + N \u2212 S S(N \u2212 1) 1 N N\u2211 i=1 \u2225vi \u2212 v\u0304\u22252.\nProof. Letting 1{i \u2208 S} be the indicator for the event i \u2208 Sr, we prove this lemma by direct calculation as follows:\nE \u2225\u2225\u2225\u2225\u2225 1S \u2211 i\u2208S vi \u2225\u2225\u2225\u2225\u2225 2  = E \u2225\u2225\u2225\u2225\u2225 1S N\u2211 i=1 vi1{i \u2208 S} \u2225\u2225\u2225\u2225\u2225 2 \n= 1\nS2 E \u2211 i \u2225vi\u222521{i \u2208 S}+ 2 \u2211 i<j v\u22a4i vj1{i, j \u2208 S}  = 1\nSN N\u2211 i=1 \u2225vi\u22252 + 1 S2 S(S \u2212 1) N(N \u2212 1) 2 \u2211 i<j v\u22a4i vj\n= 1\nSN N\u2211 i=1 \u2225vi\u22252 + 1 S2 S(S \u2212 1) N(N \u2212 1) \u2225\u2225\u2225\u2225\u2225 N\u2211 i=1 vi \u2225\u2225\u2225\u2225\u2225 2 \u2212 N\u2211 i=1 \u2225vi\u22252 \n= N \u2212 S S(N \u2212 1) 1 N N\u2211 i=1 \u2225vi\u22252 + N(S \u2212 1) S(N \u2212 1) \u2225v\u22252\n= N \u2212 S S(N \u2212 1) 1 N N\u2211 i=1 \u2225vi \u2212 v\u22252 + \u2225v\u22252.\nIn the following subsections, we present complete proofs of our main results. For FEDAVG-M and SCAFFOLD-M, our proofs only rely on Assumption 1 and 3, while for FEDAVG-M-VR and SCAFFOLD-M-VR, our proofs rely on Assumption 2 and 3."
        },
        {
            "heading": "B FEDAVG WITH MOMENTM",
            "text": ""
        },
        {
            "heading": "B.1 FEDAVG-M",
            "text": "In this subsection, we present the proofs for the FEDAVG-M algorithm.\nLemma 8. If \u03b3L \u2264 \u03b26 , the following holds for r \u2265 1: Er \u2264 ( 1\u2212 8\u03b2\n9\n) Er\u22121 + 4\u03b32L2\n\u03b2 E[\u2225\u2207f(xr\u22121)\u22252] + 2\u03b2\n2\u03c32\nNK + 4\u03b2L2Ur."
        },
        {
            "heading": "Additionally, it holds for r = 0 that",
            "text": "E0 \u2264 (1\u2212 \u03b2)E\u22121 + 2\u03b22\u03c32\nNK + 4\u03b2L2U0.\nProof. For r \u2265 1, Er = E[\u2225\u2207f(xr)\u2212 gr+1\u22252]\n= E \u2225\u2225\u2225\u2225\u2225(1\u2212 \u03b2)(\u2207f(xr)\u2212 gr) + \u03b2 ( \u2207f(xr)\u2212 1 NK \u2211 i \u2211 k \u2207F (xr,ki ; \u03be r,k i ) )\u2225\u2225\u2225\u2225\u2225 2  = E [ \u2225(1\u2212 \u03b2)(\u2207f(xr)\u2212 gr)\u22252 ] + \u03b22E  \u2225\u2225\u2225\u2225\u2225\u2225\u2207f(xr)\u2212 1NK \u2211 i, k \u2207F (xr,ki ; \u03be r,k i ) \u2225\u2225\u2225\u2225\u2225\u2225 2 \n+ 2\u03b2E \u2329(1\u2212 \u03b2)(\u2207f(xr)\u2212 gr),\u2207f(xr)\u2212 1 NK \u2211 i, k \u2207f(xr,ki ) \u232a . Note that {\u2207F (xr,ki ; \u03be r,k i )}0\u2264k<K are sequentially correlated. Applying the AM-GM inequality and Lemma 6, we have\nEr \u2264 ( 1 + \u03b2\n2\n) E[\u2225(1\u2212 \u03b2)(\u2207f(xr)\u2212 gr)\u22252] + 2\u03b2L2Ur + 2\u03b22 ( \u03c32\nNK + L2Ur\n) .\nUsing the AM-GM inequality again and Assumption 1, we have Er \u2264 (1\u2212 \u03b2)2 ( 1 + \u03b2\n2\n)[( 1 + \u03b2\n2\n) Er\u22121 + ( 1 + 2\n\u03b2\n) L2E[\u2225xr \u2212 xr\u22121\u22252] ] + 2\u03b22\u03c32\nNK + 4\u03b2L2Ur\n\u2264 (1\u2212 \u03b2)Er\u22121 + 2 \u03b2 L2E[\u2225xr \u2212 xr\u22121\u22252] + 2\u03b2\n2\u03c32\nNK + 4\u03b2L2Ur \u2264 ( 1\u2212 8\u03b2\n9\n) Er\u22121 + 4 \u03b32L2\n\u03b2 E[\u2225\u2207f(xr\u22121)\u22252] + 2\u03b2\n2\u03c32\nNK + 4\u03b2L2Ur,\nwhere we plug in \u2225xr \u2212 xr\u22121\u22252 \u2264 2\u03b32(\u2225\u2207f(xr\u22121)\u22252 + \u2225gr \u2212 \u2207f(xr\u22121)\u22252) and use \u03b3L \u2264 \u03b26 in the last inequality. Similarly for r = 0,\nE0 \u2264 ( 1 + \u03b2\n2\n) E[\u2225(1\u2212 \u03b2)(\u2207f(x0)\u2212 g0)\u22252] + 2\u03b2L2U0 + 2\u03b22 ( \u03c32\nNK + L2U0 ) \u2264 (1\u2212 \u03b2)E\u22121 + 2\u03b22\u03c32\nNK + 4\u03b2L2U0.\nLemma 9. If \u03b7LK \u2264 1\u03b2 , the following holds for r \u2265 0: Ur \u2264 2eK2\u039er +K\u03b72\u03b22\u03c32(1 + 2K3L2\u03b72\u03b22).\nProof. Recall that \u03b6r,ki := E[x r,k+1 i \u2212 x r,k i |F r,k i ] = \u2212\u03b7 ( (1\u2212 \u03b2)gr + \u03b2\u2207fi(xr,ki ) ) . Then we have\nE[\u2225\u03b6r,ji \u2212 \u03b6 r,j\u22121 i \u2225 2] \u2264 \u03b72L2\u03b22E[\u2225xr,ji \u2212 x r,j\u22121 i \u2225 2]\n\u2264 \u03b72L2\u03b22(\u03b72\u03b22\u03c32 + E[\u2225\u03b6r,j\u22121i \u2225 2).\nFor any 1 \u2264 j \u2264 k \u2212 1 \u2264 K \u2212 2, using \u03b7L \u2264 1\u03b2K \u2264 1 \u03b2(k+1) , we have\nE[\u2225\u03b6r,ji \u2225 2] \u2264\n( 1 + 1\nk\n) E[\u2225\u03b6r,j\u22121i \u2225 2] + (1 + k)E[\u2225\u03b6r,ji \u2212 \u03b6 r,j\u22121 i \u2225 2]\n\u2264 ( 1 + 2\nk\n) E[\u2225\u03b6r,j\u22121i \u2225 2] + (k + 1)L2\u03b74\u03b24\u03c32\n\u2264 e2E[\u2225\u03b6r,0i \u2225 2] + 4k2L2\u03b74\u03b24\u03c32, where the last inequality is by unrolling the recursive bound and using ( 1 + 2k )k \u2264 e2. By Lemma 6, it holds that for k \u2265 2,\nE[\u2225xr,ki \u2212 x r\u22252] \u2264 2E  \u2225\u2225\u2225\u2225\u2225\u2225 k\u22121\u2211 j=0 \u03b6r,ji \u2225\u2225\u2225\u2225\u2225\u2225 2 + 2k\u03b72\u03b22\u03c32\n\u2264 2k k\u22121\u2211 j=0 E[\u2225\u03b6r,ki \u2225 2] + 2k\u03b72\u03b22\u03c32\n\u2264 2e2k2E[\u2225\u03b6r,0i \u2225 2] + 2k\u03b72\u03b22\u03c32(1 + 4k3L2\u03b72\u03b22).\nThis is also valid for k = 0, 1. Summing up over i and k finishes the proof.\nLemma 10. If 288e(\u03b7KL)2((1\u2212 \u03b2)2 + e(\u03b2\u03b3LR)2) \u2264 1, then it holds for r \u2265 0 that R\u22121\u2211 r=0 \u039er \u2264 1 72eK2L2 R\u22122\u2211 r=\u22121 (Er + E[\u2225\u2207f(xr)\u22252]) + 2\u03b72\u03b22eRG0.\nProof. Note that \u03b6r,0i = \u2212\u03b7((1\u2212 \u03b2)gr + \u03b2\u2207fi(xr)),\n1\nN N\u2211 i=1 \u2225\u03b6r,0i \u2225 2 \u2264 2\u03b72\n( (1\u2212 \u03b2)2\u2225gr\u22252 + \u03b22 1\nN N\u2211 i=1\n\u2225\u2207fi(xr)\u22252 ) .\nUsing Young\u2019s inequality, we have for any q > 0 that E[\u2225\u2207fi(xr)\u22252] \u2264 (1 + q)E[\u2225\u2207fi(xr\u22121)\u22252] + (1 + q\u22121)L2E[\u2225xr \u2212 xr\u22121\u22252]\n\u2264 (1 + q)E[\u2225\u2207fi(xr\u22121)\u22252] + 2(1 + q\u22121)\u03b32L2(Er\u22121 + E[\u2225\u2207f(xr\u22121)\u22252])\n\u2264 (1 + q)rE[\u2225\u2207fi(x0)\u22252] + 2\nq \u03b32L2 r\u22121\u2211 j=0 (Ej + E[\u2225\u2207f(xj)\u22252)(1 + q)r\u2212j .\nTake q = 1r and we have\nE[\u2225\u2207fi(xr)\u22252] \u2264 eE[\u2225\u2207fi(x0)\u22252] + 2e(r + 1)\u03b32L2 r\u22121\u2211 j=0 (Ej + E[\u2225\u2207f(xj)\u22252). (4)\nNote that this inequality is valid for r = 0. Therefore, using (4), we have R\u22121\u2211 r=0 \u039er \u2264 R\u22121\u2211 r=0 2\u03b72E [ (1\u2212 \u03b2)2\u2225gr\u22252 + \u03b22 1 N N\u2211 i=1 \u2225\u2207fi(xr)\u22252 ]\n\u2264 R\u22121\u2211 r=0 2\u03b72\n( 2(1\u2212 \u03b2)2(Er\u22121 + E[\u2225\u2207f(xr\u22121)\u22252]) + \u03b22 1\nN N\u2211 i=1 E[\u2225\u2207fi(xr)\u22252]\n)\n\u2264 R\u22121\u2211 r=0 4\u03b72(1\u2212 \u03b2)2(Er\u22121 + E[\u2225\u2207f(xr\u22121)\u22252])\n+ 2\u03b72\u03b22 R\u22121\u2211 r=0  e N N\u2211 i=1 E[\u2225\u2207fi(x0)\u22252] + 2e(r + 1)(\u03b3L)2 r\u22121\u2211 j=0 (Ej + E[\u2225\u2207f(xj)\u22252]) \n\u2264 4\u03b72(1\u2212 \u03b2)2 R\u22121\u2211 r=0 (Er\u22121 + E[\u2225\u2207f(xr\u22121)\u22252])\n+ 2\u03b72\u03b22 ( eRG0 + 2e(\u03b3LR)\n2 R\u22122\u2211 r=0 (Er + E[\u2225\u2207f(xr)\u22252])\n) .\nRearranging the equation and applying the upper bound of \u03b7 completes the proof.\nTheorem 11. Under Assumption 1 and 3, if we take g0 = 0,\n\u03b2 = min { , \u221a NKL\u2206\n\u03c32R\n} for any constant c \u2208 (0, 1], \u03b3 = min { 1\n24L , \u03b2 6L\n} ,\n\u03b7KL \u2272 min { 1, 1\n\u03b2\u03b3LR ,\n( L\u2206\nG0\u03b23R\n)1/2 ,\n1\n(\u03b2N)1/2 ,\n1\n(\u03b23NK)1/4 } (5) then FEDAVG-M converges as\n1\nR R\u22121\u2211 r=0 E[\u2225\u2207f(xr)\u22252] \u2272 \u221a L\u2206\u03c32 NKR + L\u2206 R .\nHere G0 := 1N \u2211N i=1 \u2225\u2207fi(x0)\u22252.\nProof. Combining Lemma 8 and 9, we have Er \u2264 ( 1\u2212 8\u03b2\n9\n) Er\u22121 + 4 (\u03b3L)2\n\u03b2 E[\u2225\u2207f(xr\u22121)\u22252] + 2\u03b2\n2\u03c32\nNK + 4\u03b2L2 ( 2eK2\u039er +K\u03b7 2\u03b22\u03c32(1 + 2K3L2\u03b72\u03b22 ) .\nand\nE0 \u2264 (1\u2212 \u03b2)E\u22121 + 2\u03b22\u03c32\nNK + 4\u03b2L2\n( 2eK2\u039e0 +K\u03b7 2\u03b22\u03c32(1 + 2K3L2\u03b72\u03b22) ) .\nSumming over r from 0 to R\u2212 1 and applying Lemma 10, R\u22121\u2211 r=0 Er \u2264 ( 1\u2212 8\u03b2 9 ) R\u22122\u2211 r=\u22121 Er + 4 (\u03b3L)2 \u03b2 R\u22122\u2211 r=0 E[\u2225\u2207f(xr)\u22252] + 2\u03b2 2\u03c32 NK R\n+ 4\u03b2L2 ( 2eK2\nR\u22121\u2211 r=0 \u039er +RK\u03b7 2\u03b22\u03c32(1 + 2K3L2\u03b72\u03b22)\n)\n\u2264 ( 1\u2212 7\u03b2\n9 ) R\u22122\u2211 r=\u22121 Er + ( 4 (\u03b3L)2 \u03b2 + \u03b2 9 ) R\u22122\u2211 r=\u22121 E[\u2225\u2207f(xr)\u22252] + 16\u03b23(e\u03b7KL)2RG0\n+ 2\u03b22\u03c32\nNK R+ 4\u03b23(\u03b7KL)2\n( 1\nK + 2(\u03b7KL\u03b2)2\n) \u03c32R\n\u2264 ( 1\u2212 7\u03b2\n9 ) R\u22122\u2211 r=\u22121 Er + 2\u03b2 9 R\u22122\u2211 r=\u22121 E[\u2225\u2207f(xr)\u22252] + 16\u03b23(e\u03b7KL)2RG0 + 4\u03b22\u03c32 NK R.\nHere in the last inequality we apply 4\u03b2(\u03b7KL)2 ( 1\nK + 2(\u03b7KL\u03b2)2\n) \u2264 2\nNK and \u03b3L \u2264 \u03b2 6 .\nTherefore, R\u22121\u2211 r=0 Er \u2264 9 7\u03b2 E\u22121 + 2 7 E[ R\u22122\u2211 r=\u22121 \u2225\u2207f(xr)\u22252] + 144 7 (e\u03b2\u03b7KL)2G0R+ 36\u03b2\u03c32 7NK R.\nCombine this inequality with Lemma 5 and we get\n1 \u03b3 E[f(xR)\u2212 f(x0)] \u2264 \u22121 7 R\u22121\u2211 r=0 E[\u2225\u2207f(xr)\u22252] + 39 56\u03b2 E\u22121 + 78 7 (e\u03b2\u03b7KL)2G0R+ 39\u03b2\u03c32 14NK R.\nFinally, noticing that g0 = 0 implies E\u22121 \u2264 2L(f(x0)\u2212 f\u2217) = 2L\u2206, we obtain 1\nR R\u22121\u2211 r=0 E[\u2225\u2207f(xr)\u22252] \u2272 L\u2206 \u03b3LR + E\u22121 \u03b2R + (\u03b2\u03b7KL)2G0 + \u03b2\u03c32 NK\n\u2272 L\u2206\nR +\nL\u2206 \u03b2R +\n\u03b2\u03c32 NK + (\u03b2\u03b7KL)2G0\n\u2272 L\u2206\nR +\n\u221a L\u2206\u03c32\nNKR ."
        },
        {
            "heading": "B.2 FEDAVG-M-VR",
            "text": "In this subsection, we present the proofs for the FEDAVG-M-VR algorithm, shown as in Algorithm 3.\nLemma 12. If \u03b3L \u2264 \u221a\n\u03b2NK 54 , the following holds for r \u2265 1:\nEr \u2264 (1\u2212 8\u03b2\n9 )Er\u22121 +\n4 \u03b2 L2Ur +\n3\u03b22\u03c32\nNK +\n6(\u03b3L)2\nNK E[\u2225\u2207f(xr\u22121)\u22252."
        },
        {
            "heading": "Also for r = 0, it holds that",
            "text": "E0 \u2264 (1\u2212 \u03b2)E\u22121 + 4\n\u03b2 L2Ur +\n3\u03b22\u03c32\nNK .\nAlgorithm 3 FEDAVG-M-VR: FEDAVG with variance-reduced momentum Require: initial model x\u22121 = x0 and gradient estimate g0, local learning rate \u03b7, global learning\nrate \u03b3, momentum \u03b2 for r = 0, \u00b7 \u00b7 \u00b7 , R\u2212 1 do\nfor each client i \u2208 {1, . . . , N} in parallel do Initial local model xr,0i = x r\nfor k = 0, \u00b7 \u00b7 \u00b7 ,K \u2212 1 do Compute direction gr,ki = \u2207F (x r,k i ; \u03be r,k i ) + (1\u2212 \u03b2)(gr \u2212\u2207F (xr\u22121; \u03be r,k i ))\nUpdate local model xr,k+1i = x r,k i \u2212 \u03b7g r,k i\nend for end for Aggregate local updates gr+1 = 1\u03b7NK \u2211N i=1 ( xr \u2212 xr,Ki ) Update global model global xr+1 = xr \u2212 \u03b3gr+1\nend for"
        },
        {
            "heading": "Proof.",
            "text": "Er = E  \u2225\u2225\u2225\u2225\u2225\u2225 1NK \u2211 i, k \u2207F (xr,ki ; \u03be r,k i ) + (1\u2212 \u03b2) gr \u2212 1 NK \u2211 i, k \u2207F (xr\u22121; \u03ber,ki ) \u2212\u2207f(xr) \u2225\u2225\u2225\u2225\u2225\u2225 2 \n=E [\u2225\u2225\u2225\u2225\u2225(1\u2212 \u03b2)(gr \u2212\u2207f(xr\u22121)) + 1NK \u2211 i, k \u2207F (xr,ki ; \u03be r,k i )\u2212\u2207f(x r)\n+ (1\u2212 \u03b2) \u2207f(xr\u22121)\u2212 1 NK \u2211 i, k \u2207F (xr\u22121; \u03ber,ki ) \u2225\u2225\u2225\u2225\u2225 2]\n=(1\u2212 \u03b2)2Er\u22121 + 2E \u2329(1\u2212 \u03b2)(gr \u2212\u2207f(xr\u22121)), 1 NK \u2211 i, k \u2207fi(xr,ki )\u2212\u2207f(x r) \u232a \ufe38 \ufe37\ufe37 \ufe38\n\u039b1\n+ E \u2225\u2225\u2225\u2225\u2225\u2225 1NK \u2211 i, k \u2207F (xr,ki ; \u03be r,k i )\u2212\u2207f(x r) + (1\u2212 \u03b2) \u2207f(xr\u22121)\u2212 1 NK \u2211 i, k \u2207F (xr\u22121; \u03ber,ki ) \u2225\u2225\u2225\u2225\u2225\u2225 2\n\ufe38 \ufe37\ufe37 \ufe38 \u039b2\n.\nBy the AM-GM inequality and Assumption 2,\n\u039b1 \u2264 \u03b2(1\u2212 \u03b2)2Er\u22121 + 1\n\u03b2 L2Ur.\nBy Assumption 2,\n\u039b2 = E [\u2225\u2225\u2225\u2225\u2225 1NK \u2211 i, k (\u2207F (xr,ki ; \u03be r,k i )\u2212\u2207F (x r; \u03ber,ki )) + \u03b2  1 NK \u2211 i, k \u2207F (xr,ki ; \u03be r,k i )\u2212\u2207f(x r)  + (1\u2212 \u03b2)  1 NK \u2211 i, k (\u2207F (xr; \u03ber,ki )\u2212\u2207F (x r\u22121; \u03ber,ki ))\u2212\u2207f(x r) +\u2207f(xr\u22121) \u2225\u2225\u2225\u2225\u2225 2]\n\u2264 3L2Ur + 3 \u03b22\u03c32 NK + 3(1\u2212 \u03b2)2 L 2 NK E[\u2225xr \u2212 xr\u22121\u22252.\nTherefore, for r \u2265 1,\nEr \u2264 (1\u2212 \u03b2)Er\u22121 + 4\n\u03b2 L2Ur +\n3\u03b22\u03c32\nNK + 3(1\u2212 \u03b2)2 L\n2\nNK E[\u2225xr \u2212 xr\u22121\u22252]\n\u2264 (1\u2212 8\u03b2 9 )Er\u22121 + 4 \u03b2 L2Ur +\n3\u03b22\u03c32\nNK +\n6(\u03b3L)2\nNK E[\u2225\u2207f(xr\u22121)\u22252].\nThe last inequality is derived by \u2225xr \u2212 xr\u22121\u22252 \u2264 2\u03b32(\u2225\u2207f(xr\u22121)\u22252 + \u2225gr \u2212 \u2207f(xr\u22121)\u22252) and \u03b3L \u2264 \u221a \u03b2NK 54 . Similarly, for r = 0, we can obtain\nE0 \u2264 (1\u2212 \u03b2)E\u22121 + 4\n\u03b2 L2U0 +\n3\u03b22\u03c32\nNK .\nLemma 13. If \u03b7KL \u2264 14e , the following holds: Ur \u2264 4eK2\u039er + 8(\u03b7K)2(2(\u03b7KL)2 +K\u22121) ( \u03b22\u03c32 + 2L2E[\u2225xr \u2212 xr\u22121\u22252] ) .\nProof. Note that \u03b6r,ki = \u2212\u03b7(\u2207fi(x r,k i ) + (1\u2212 \u03b2)(gr \u2212\u2207fi(xr\u22121)). Then we have\nE[\u2225\u03b6r,ji \u2212 \u03b6 r,j\u22121 i \u2225 2] \u2264 \u03b72L2E[\u2225xr,ji \u2212 x r,j\u22121 i \u2225 2] = \u03b72L2 ( E[\u2225\u03b6r,j\u22121i \u2225 2] + E[Var[xr,ji \u2212 x r,j\u22121 i |F r,j\u22121 i ]] ) .\nHere we use bias-variance decomposition and Var[\u00b7|\u00b7] stands for the conditional variance. Since E[Var[xr,ji \u2212 x r,j\u22121 i |F r,j\u22121 i ]]\n=\u03b72E [\u2225\u2225\u2225\u2207F (xr,j\u22121i ; \u03ber,j\u22121i )\u2212\u2207fi(xr,j\u22121i )\u2212 (1\u2212 \u03b2)(\u2207F (xr\u22121; \u03ber,j\u22121i )\u2212\u2207fi(xr\u22121))\u2225\u2225\u22252]\n\u2264\u03b72 ( 2\u03b22\u03c32 + 2(1\u2212 \u03b2)2L2E[\u2225xr\u22121 \u2212 xr,j\u22121i \u2225 2 ) ,\nthen E[\u2225\u03b6r,ji \u2212 \u03b6 r,j\u22121 i \u2225 2]\n\u2264 \u03b72L2 ( E[\u2225\u03b6r,j\u22121i \u2225 2 + 2\u03b22\u03b72\u03c32 + 2\u03b72(1\u2212 \u03b2)2L2E[\u2225xr\u22121 \u2212 xr,j\u22121i \u2225 2] )\n\u2264 \u03b72L2 ( E[\u2225\u03b6r,j\u22121i \u2225 2] + 2\u03b22\u03b72\u03c32 + 4\u03b72L2E[\u2225xr\u22121 \u2212 xr\u22252 + \u2225xr \u2212 xr,j\u22121i \u2225 2] ) .\nTherefore for any 1 \u2264 j \u2264 k \u2212 1 \u2264 K \u2212 2,\nE\u2225\u03b6r,ji \u2225 2 \u2264 (1 + 1 k )E[\u2225\u03b6r,j\u22121i \u2225 2 + (1 + k)E[\u2225\u03b6r,ji \u2212 \u03b6 r,j\u22121 i \u2225 2]\n\u2264 ( 1 + 2\nk\n) E\u2225\u03b6r,j\u22121i \u2225 2 + (k + 1)\u03b72L2 ( 2\u03b22\u03b72\u03c32 + 4\u03b72L2E[\u2225xr\u22121 \u2212 xr\u22252 + \u2225xr \u2212 xr,j\u22121i \u2225 2] )\n\u2264e2E\u2225\u03b6r,0i \u2225 2 + 8k2L2\u03b74(2\u03b22\u03c32 + 4L2E[\u2225xr \u2212 xr\u22121\u22252]) + 4e2k(\u03b7L)4 j\u22121\u2211 j\u2032=0 E[\u2225xr,j \u2032 i \u2212 x r\u22252]. (6) Here the second inequality is by \u03b7L \u2264 1K \u2264 1 k+1 . The last inequality is by unrolling the recursive\nbound and using ( 1 + 2k )k \u2264 e2. By Lemma 6, it holds that E[\u2225xr,ki \u2212 x r\u22252]\n\u22642E  \u2225\u2225\u2225\u2225\u2225\u2225 k\u22121\u2211 j=0 \u03b6r,ji \u2225\u2225\u2225\u2225\u2225\u2225 2 + 2 k\u22121\u2211 j=0 E[Var[xr,j+1i \u2212 x r,j i |F r,j i ]] \u22642k k\u22121\u2211 j=0 E[\u2225\u03b6r,ji \u2225 2] + 2 k\u22121\u2211 j=0 ( 2\u03b22\u03b72\u03c32 + 4\u03b72L2E[\u2225xr\u22121 \u2212 xr\u22252 + \u2225xr \u2212 xr,ji \u2225 2] ) . (7)\nSumming up (7) over k = 0, . . . ,K \u2212 1, using (6) and 8(\u03b7L)2 + 8e2(\u03b7KL)4 \u2264 12 due to the condition on \u03b7, we have\n1\n2K K\u22121\u2211 k=0 E[\u2225xr,ki \u2212x r\u22252 \u2264 2eK2E[\u2225\u03b6r,0i \u2225 2]+(8(\u03b7K)4L2+4\u03b72K) ( \u03b22\u03c32 + 2L2E[\u2225xr \u2212 xr\u22121\u22252] ) . This implies Ur \u2264 4eK2\u039er + 8(\u03b7K)2(2(\u03b7KL)2 +K\u22121) ( \u03b22\u03c32 + 2L2E[\u2225xr \u2212 xr\u22121\u22252] ) .\nLemma 14. If \u03b3L \u2264 124 and 288e(\u03b7KL) 2 ( 289 72 (1\u2212 \u03b2) 2 + 8e(\u03b3\u03b2LR)2 ) \u2264 \u03b22, then the following holds: R\u22121\u2211 r=0 \u039er \u2264 \u03b22 288eK2L2 R\u22122\u2211 r=\u22121 (Er + E[\u2225\u2207f(xr)\u22252]) + 4\u03b72\u03b22eRG0.\nProof. Recall that \u03b6r,0i = \u2212\u03b7((1\u2212 \u03b2)(gr \u2212\u2207fi(xr\u22121)) +\u2207fi(xr)). Consequently, we have \u2225\u03b6r,0i \u2225 2 \u2264 2\u03b72 ( (1\u2212 \u03b2)2\u2225gr\u22252 + \u2225\u2207fi(xr)\u2212 (1\u2212 \u03b2)\u2207fi(xr\u22121)\u22252 ) \u2264 2\u03b72(1\u2212 \u03b2)2(1 + 2(\u03b3L)2)\u2225gr\u22252 + 4\u03b72\u03b22\u2225\u2207fi(xr)\u22252\n\u2264 289 144\n\u03b72(1\u2212 \u03b2)2\u2225gr\u22252 + 4\u03b72\u03b22\u2225\u2207fi(xr)\u22252. Using Young\u2019s inequality, we can obtain that for any q > 0,\nE[\u2225\u2207fi(xr)\u22252] \u2264 (1 + q)E[\u2225\u2207fi(xr\u22121)\u22252] + (1 + q\u22121)L2E\u2225xr \u2212 xr\u22121\u22252\n\u2264 (1 + q)E[\u2225\u2207fi(xr\u22121)\u22252] + 2(1 + q\u22121)(\u03b3L)2(Er\u22121 + E[\u2225\u2207f(xr\u22121)\u22252])\n\u2264 (1 + q)rE[\u2225\u2207fi(x0)\u22252] + 2\nq (\u03b3L)2 r\u22121\u2211 j=0 (Ej + E[\u2225\u2207f(xj)\u22252])(1 + q)r\u2212j .\nTaking q = 1r in the above, we have\nE[\u2225\u2207fi(xr)\u22252] \u2264 eE[\u2225\u2207fi(x0)\u22252] + 2e(r + 1)(\u03b3L)2 r\u22121\u2211 j=0 (Ej + E[\u2225\u2207f(xj)\u22252]).\nThis inequality holds as well trivially for r = 0. Therefore, we have R\u22121\u2211 r=0 \u039er \u2264 R\u22121\u2211 r=0 E\n[ 289\n144 \u03b72(1\u2212 \u03b2)2\u2225gr\u22252 + 4\u03b72\u03b22 1 N N\u2211 i=1\n\u2225\u2207fi(xr)\u22252 ]\n\u2264 R\u22121\u2211 r=0 289 72 \u03b72(1\u2212 \u03b2)2(Er\u22121 + E[\u2225\u2207f(xr\u22121)\u22252])\n+ 4\u03b72\u03b22 R\u22121\u2211 r=0  e N \u2211 i E[\u2225\u2207fi(x0)\u22252] + 2e(r + 1)(\u03b3L)2 r\u22121\u2211 j=0 (Ej + E[\u2225\u2207f(xj)\u22252])  \u2264 289\n72 \u03b72(1\u2212 \u03b2)2 R\u22121\u2211 r=0 (Er\u22121 + E[\u2225\u2207f(xr\u22121)\u22252])\n4\u03b72\u03b22 ( eRG0 + 2e(\u03b3LR)\n2 R\u22122\u2211 r=0 (Er + E[\u2225\u2207f(xr)\u22252])\n)\n\u2264 \u03b2 2\n288eK2L2 R\u22122\u2211 r=\u22121 (Er + E[\u2225\u2207f(xr)\u22252]) + 4\u03b72\u03b22eRG0.\nHere the last inequality is due to the upper bound of \u03b7.\nTheorem 15. Under Assumption 2 and 3, if we take g0 = 1NB \u2211N i=1 \u2211B b=1 \u2207F (x0; \u03bebi ) with {\u03bebi }Bb=1 iid\u223c Di and set\n\u03b2 = min { c, ( NKL2\u22062\n\u03c34R2\n)1/3} for any constant c \u2208 (0, 1], \u03b3 = min { 1\n24L ,\n\u221a \u03b2NK\n54L2\n} ,\n\u03b7KL \u2272 min\n{( L\u2206\nG0\u03b3LR\n)1/2 , ( \u03b2\nN\n)1/2 , ( \u03b2\nNK\n)1/4} , B = \u2308 K\nR\u03b22\n\u2309 ,\n(8)\nFEDAVG-M-VR converges as\n1\nR R\u22121\u2211 r=0 E[\u2225\u2207f(xr)\u22252] \u2272 ( L\u2206\u03c3 NKR )2/3 + L\u2206 R .\nAlternatively, if B = \u0398(KR) and \u03b2 = min { 1 R , ( NKL2\u22062 \u03c34R2 )1/3} , then FEDAVG-M-VR converges\nas\n1\nR R\u22121\u2211 r=0 E[\u2225\u2207f(xr)\u22252] \u2272 ( L\u2206\u03c3 NKR )2/3 + \u03c32 NKR + L\u2206 R .\nProof. Combine Lemma 12, 13 and we have\nEr \u2264 (1\u2212 8\u03b2\n9 )Er\u22121 +\n(6\u03b3L)2\nNK E[\u2225\u2207f(xr\u22121)\u22252] + 3\u03b2\n2\u03c32\nNK\n+ 4 \u03b2 L2 ( 4eK2\u039er + 8(\u03b7K) 2(2(\u03b7KL)2 +K\u22121)(\u03b22\u03c32 + 2L2E[\u2225xr \u2212 xr\u22121\u22252]) )\nE0 \u2264 (1\u2212 \u03b2)E\u22121 + 3\u03b2 2\u03c32 NK + 4 \u03b2L 2 ( 4eK2\u039e0 + 8(\u03b7K) 2(2(\u03b7KL)2 +K\u22121))\u03b22\u03c32 )\nSumming over r from 0 to R\u2212 1 and applying Lemma 14, R\u22121\u2211 r=0 Er\n\u2264(1\u2212 8\u03b2 9 ) R\u22122\u2211 r=\u22121 Er + 6(\u03b3L)2 NK E [ R\u22122\u2211 r=0 \u2225\u2207f(xr)\u22252 ] + 3\u03b22\u03c32 NK R\n+ 4\n\u03b2 L2\n( 4eK2\nR\u22121\u2211 r=0 \u039er + 8(\u03b7K) 2(2(\u03b7KL)2 + 1 K )\n( R\u03b22\u03c32 + 2L2\nR\u22121\u2211 r=0 E[\u2225xr \u2212 xr\u22121\u22252]\n))\n\u2264(1\u2212 7\u03b2 9 ) R\u22122\u2211 r=\u22121 Er + ( 6(\u03b3L)2 NK + \u03b2 9 ) E[ R\u22122\u2211 r=\u22121 \u2225\u2207f(xr)\u22252] + 64\u03b2(e\u03b7KL)2RG0\n+ 3\u03b22\u03c32\nNK R+ 32\u03b2(\u03b7KL)2\n( 1\nK + 2(\u03b7KL)2\n) \u03c32R\n\u2264(1\u2212 7\u03b2 9 ) R\u22122\u2211 r=\u22121 Er + 2\u03b2 9 E [ R\u22122\u2211 r=\u22121 \u2225\u2207f(xr)\u22252 ] + 64\u03b2(e\u03b7KL)2RG0 + 4\u03b22\u03c32 NK R. Here in the second inequality, we apply 32\u03b2(\u03b7KL)2( 1K + 2(\u03b7KL) 2) \u2264 \u03b2 2 NK , 128(\u03b7KL)2 \u03b2 ( 1 K + 2(\u03b7KL) 2)(\u03b3L)2 \u2264 \u03b218 , \u03b3L \u2264 \u221a\n\u03b2NK 54 .\nTherefore, we obtain R\u22121\u2211 r=0 Er \u2264 9 7\u03b2 E\u22121 + 2 7 E [ R\u22122\u2211 r=\u22121 \u2225\u2207f(xr)\u22252 ] + 576 7 (e\u03b7KL)2G0R+ 36\u03b2\u03c32 7NK R.\nCombine this inequality with Lemma 5 and we get\n1 \u03b3 E[f(xR)\u2212 f(x0)] \u2264 \u22121 7 R\u22121\u2211 r=0 E[\u2225\u2207f(xr)\u22252] + 39 56\u03b2 E\u22121 + 312 7 (e\u03b7KL)2G0R+ 39\u03b2\u03c32 14NK R.\nFinally, for B = \u2308\nK R\u03b22 \u2309 , noticing that g0 = 1NB \u2211 i \u2211B b=1 \u2207F (x0; \u03bebi ) implies E\u22121 \u2264 \u03c32\nNB \u2264 \u03b22\u03c32R NK and thus\n1\nR R\u22121\u2211 r=0 E[\u2225\u2207f(xr)\u22252] \u2272 L\u2206 \u03b3LR + E\u22121 \u03b2R + (\u03b7KL)2G0 + \u03b2\u03c32 NK\n\u2272 L\u2206\n\u03b3LR +\n\u03b2\u03c32\nNK\n\u2272 L\u2206\nR + L\u2206\u221a \u03b2NKR + \u03b2\u03c32 NK\n\u2272 L\u2206\nR +\n( L\u2206\u03c3\nNKR )2/3 Similarly, for B = KR, E\u22121 \u2264 \u03c3 2 NB \u2264 \u03c32 NKR , and we have\n1\nR R\u22121\u2211 r=0 E[\u2225\u2207f(xr)\u22252] \u2272 L\u2206 \u03b3LR + E\u22121 \u03b2R + (\u03b7KL)2G0 + \u03b2\u03c32 NK\n\u2272 L\u2206\n\u03b3LR +\n\u03c32\n\u03b2NKR2 +\n\u03b2\u03c32\nNK\n\u2272 L\u2206\nR + L\u2206\u221a \u03b2NKR + \u03c32 \u03b2NKR2 + \u03b2\u03c32 NK\n\u2272 L\u2206\nR +\n( L\u2206\u03c3\nNKR\n)2/3 + \u03c32\nNKR ."
        },
        {
            "heading": "C SCAFFOLD WITH MOMENTUM",
            "text": ""
        },
        {
            "heading": "C.1 SCAFFOLD-M",
            "text": "In this subsection, we present the proofs for the SCAFFOLD-M algorithm.\nLemma 16. If \u03b3L \u2264 \u03b212 , the following holds for r \u2265 1: Er \u2264 ( 1\u2212 8\u03b2\n9\n) Er\u22121 + 16\n\u03b2 (\u03b3L)2E[\u2225\u2207f(xr\u22121)\u22252] + 4\u03b2\n2\u03c32\nSK + 10\u03b2L2Ur + 6\u03b2 2 N \u2212 S S(N \u2212 1) Vr."
        },
        {
            "heading": "In addition,",
            "text": "E0 \u2264 (1\u2212 \u03b2)E\u22121 + 4\u03b22\u03c32\nSK + 8\u03b2L2U0 + 4\u03b2 2 N \u2212 S S(N \u2212 1) V0.\nProof. Note that 1N \u2211N i=1 c r i = c r holds for any r \u2265 0. Using Lemma 7, we have\nEr = E  \u2225\u2225\u2225\u2225\u2225\u2225\u2207f(xr)\u2212 1NK \u2211 i, k gr,ki \u2225\u2225\u2225\u2225\u2225\u2225 2 + N \u2212 S S(N \u2212 1) 1 N N\u2211 i=1 E  \u2225\u2225\u2225\u2225\u2225\u2225 1K \u2211 k gr,ki \u2212 1 NK \u2211 j,k gr,kj \u2225\u2225\u2225\u2225\u2225\u2225 2 \n= E  1\u2212 \u03b2)(\u2207f(xr)\u2212 gr) + \u03b2  1 NK \u2211 i, k \u2207F (xr,ki ; \u03be r,k i )\u2212\u2207f(x r) \u2225\u2225\u2225\u2225\u2225\u2225 2 \n\ufe38 \ufe37\ufe37 \ufe38 \u039b1\n+ \u03b22(N \u2212 S) S(N \u2212 1) 1 N N\u2211 i=1 E  \u2225\u2225\u2225\u2225\u2225\u2225 1K \u2211 k \u2207F (xr,ki ; \u03be r,k i )\u2212 1 NK \u2211 j,k \u2207F (xr,kj ; \u03be r,k j )\u2212 (c r i \u2212 cr) \u2225\u2225\u2225\u2225\u2225\u2225 2 \n\ufe38 \ufe37\ufe37 \ufe38 \u039b2\n.\nFor r \u2265 1, similar to the proof of Lemma 8, we have\n\u039b1 \u2264 (1\u2212 \u03b2)Er\u22121 + 2 \u03b2 L2E[\u2225xr \u2212 xr\u22121\u22252] + 2\u03b2\n2\u03c32\nNK + 4\u03b2L2Ur.\nBesides, by AM-GM inequality and Lemma 6,\n\u039b2 \u2264 1\nN N\u2211 i=1 E \u2225\u2225\u2225\u2225\u2225 1K \u2211 k \u2207F (xr,ki ; \u03be r,k i )\u2212 c r i \u2225\u2225\u2225\u2225\u2225 2 \n\u2264 2\u03c3 2\nK +\n2\nN \u2211 i E \u2225\u2225\u2225\u2225\u2225 1K \u2211 k \u2207fi(xr,ki )\u2212 c r i \u2225\u2225\u2225\u2225\u2225 2 \n\u2264 2\u03c3 2\nK + 6(L2Ur + L 2E[\u2225xr \u2212 xr\u22121\u22252] + Vr). Since E[\u2225xr \u2212 xr\u22121\u22252] \u2264 2\u03b32(Er\u22121 + E[\u2225\u2207f(xr\u22121)\u22252]) and ( 2 \u03b2 + 6\u03b2 2 N\u2212S S(N\u22121) ) 2(\u03b3L)2 \u2264\n16 \u03b2 (\u03b3L) 2 \u2264 \u03b29 , we have Er \u2264 ( 1\u2212 8\u03b2\n9\n) Er\u22121 + 16\n\u03b2 (\u03b3L)2E[\u2225\u2207f(xr\u22121)\u22252] + 4\u03b2\n2\u03c32\nSK + 10\u03b2L2Ur + 6\u03b2 2 N \u2212 S S(N \u2212 1) Vr.\nThe case for r = 0 is similar.\nLemma 17. If \u03b3L \u2264 1\u221a 2\u03b2 and \u03b7KL \u2264 1\u03b2 , it holds for all r \u2265 1 that Ur \u2264 \u03b72K2 ( 8e(Er\u22121 + 2E[\u2225\u2207f(xr\u22121)\u22252] + \u03b22Vr) + \u03b22\u03c32(K\u22121 + 2(\u03b2\u03b7KL)2)) ) .\nProof. Since \u03b6r,ki = E[x r,k+1 i \u2212 x r,k i |F r,k i ] = \u2212\u03b7(\u03b2\u2207fi(x r,k i ) + (1 \u2212 \u03b2)gr \u2212 \u03b2(cri \u2212 cr)) and Var[xr,k+1i \u2212 x r,k i |F r,k i ] \u2264 \u03b22\u03b72\u03c32, with exactly the same procedures of Lemma 9, we have\nUr \u2264 2eK2\u039er +K\u03b72\u03b22\u03c32(1 + 2K3L2\u03b72\u03b22).\nAdditionally, by AM-GM inequality,\n\u039er = \u03b72\nN \u2211 i E[\u2225\u03b2\u2207fi(xr) + (1\u2212 \u03b2)gr \u2212 \u03b2(cri \u2212 cr)\u22252]\n= \u03b72\nN \u2211 i E [ \u2225\u03b2(\u2207fi(xr)\u2212\u2207fi(xr\u22121)) + (1\u2212 \u03b2)(gr \u2212\u2207f(xr\u22121))\n\u2212\u03b2 ( cri \u2212 cr \u2212\u2207fi(xr\u22121) +\u2207f(xr\u22121) ) +\u2207f(xr\u22121)\u22252 ] \u2264 4\u03b72 ( \u03b22L2E[\u2225xr \u2212 xr\u22121\u22252] + (1\u2212 \u03b2)2Er\u22121 + \u03b22Vr + E[\u2225\u2207f(xr\u22121)\u22252]\n) \u2264 4\u03b72(Er\u22121 + 2E[\u2225\u2207f(xr\u22121)\u22252] + \u03b22Vr).\nPlug this inequality into the above bound completes the proof.\nLemma 18. Under the same conditions of Lemma 17, if \u03b2\u03b7KL \u2264 1 24K1/4 and \u03b7K \u2264 N5S \u03b3, then we have\nR\u22121\u2211 r=0 Vr \u2264 3N S\n( V0 + 4SR\nNK \u03c32 +\n8N\nS (\u03b3L)2 R\u22122\u2211 r=\u22121 (Er + E[\u2225\u2207f(xr)\u22252])\n) .\nProof. Since\ncr+1i = { cri with probability 1\u2212 SN 1 K \u2211 k \u2207F (x r,k i ; \u03be r,k i ) with probability S N ,\nusing Young\u2019s inequality repeatedly, we have\nVr+1 =\n( 1\u2212 S\nN\n) 1\nN N\u2211 i=1 E[\u2225cri \u2212\u2207fi(xr)\u22252] + S N 1 N N\u2211 i=1 E \u2225\u2225\u2225\u2225\u2225 1K \u2211 k \u2207F (xr,ki ; \u03be r,k i )\u2212\u2207fi(x r) \u2225\u2225\u2225\u2225\u2225 2 \n\u2264 ( 1\u2212 S\nN\n) 1\nN N\u2211 i=1 E[\u2225cri \u2212\u2207fi(xr)\u22252] + S N ( 2\u03c32 K + 2L2Ur )\n\u2264 ( 1\u2212 S\nN\n) 1\nN N\u2211 i=1 E [( 1 + S 2N ) \u2225cri \u2212\u2207fi(xr\u22121)\u22252 + ( 1 + 2N S ) L2\u2225xr \u2212 xr\u22121\u22252 ] + 2S\nN\n( \u03c32\nK + L2Ur ) \u2264 ( 1\u2212 S\n2N\n) Vr + 2N\nS L2E[\u2225xr \u2212 xr\u22121\u22252] + 2S\u03c3\n2\nNK +\n2S N L2Ur.\nHere we apply Lemma 6 to obtain the second inequality. Combine this with Lemma 17, Vr+1 \u2264 ( 1\u2212 S\n2N + 16e\nS N (\u03b2\u03b7KL)2\n) Vr + 2\u03c3 2 ( S\nNK +\n2S N (\u03b2\u03b7KL)2(K\u22121 + 2(\u03b2\u03b7KL)2) ) + ( 4N\nS (\u03b3L)2 +\n32eS\nN (\u03b7KL)2\n) (Er\u22121 + E[\u2225\u2207f(xr\u22121)\u22252])\n\u2264 ( 1\u2212 S\n3N\n) Vr + 4S\nNK \u03c32 +\n8N\nS (\u03b3L)2(Er\u22121 + E[\u2225\u2207f(xr\u22121)\u22252]),\nwhere we apply the upper bound of \u03b7. Therefore, we finish the proof by summing up over r from 0 to R\u2212 1 and rearranging the inequality.\nTheorem 19. Under Assumption 1 and 3, if we take g0 = 0, c0i = 1 B \u2211B b=1 \u2207F (x0; \u03bebi ) with {\u03bebi }Bb=1 iid\u223c Di, c0 = 1N \u2211N i=1 c 0 i and set\n\u03b3 = \u03b2\nL , \u03b2 = min c, SN2/3 , \u221a L\u2206SK \u03c32R , \u221a L\u2206S2\nG0N  , \u03b7KL \u2272 min { 1\nS1/2 ,\n1 \u03b2K1/4 , S1/2 N\n} , B = \u2308 NK\nSR\n\u2309 ,\n(9)\nthen SCAFFOLD-M converges as\n1\nR R\u22121\u2211 r=0 E[\u2225\u2207f(xr)\u22252] \u2272 \u221a L\u2206\u03c32 SKR + L\u2206 R ( 1 + N2/3 S ) .\nProof. By Lemma 16, sum over r from 0 to R\u2212 1 and plug Lemma 17, Lemma 18 in, R\u22121\u2211 r=0 Er \u2264 ( 1\u2212 8\u03b2 9 ) R\u22122\u2211 r=\u22121 Er + 16 \u03b2 (\u03b3L)2 R\u22122\u2211 r=0 E[\u2225\u2207f(xr)\u22252]\n+ 4\u03b22\u03c32\nSK R+ 10\u03b2L2 R\u22121\u2211 r=0 Ur + 6\u03b2 2 N \u2212 S S(N \u2212 1) R\u22121\u2211 r=0 Vr\n\u2264 ( 1\u2212 8\u03b2\n9 + 80e\u03b2(\u03b7KL)2 ) R\u22122\u2211 r=\u22121 Er + ( 16 \u03b2 (\u03b3L)2 + 160e\u03b2(\u03b7KL)2) R\u22122\u2211 r=0 E[\u2225\u2207f(xr)\u22252]\n+ \u03b22\u03c32R\n( 4\nSK + 10(\u03b7KL)2(K\u22121 + 2(\u03b2\u03b7KL)2)\n) +\n+ \u03b22 ( 6\nN \u2212 S S(N \u2212 1)\n+ 80e\u03b2(\u03b7KL)2 )R\u22121\u2211\nr=0\nVr\n\u2264 ( 1\u2212 7\u03b2\n9 ) R\u22122\u2211 r=\u22121 Er + ( 16 \u03b2 (\u03b3L)2 + \u03b2 9 )R\u22122\u2211 r=0 E[\u2225\u2207f(xr)\u22252] + 80\u03b2 2\u03c32 SK R+ 30\u03b22N S2 V0.\nHere the coefficients in the last inequality are derived by the following bounds: 160e\u03b2(\u03b7KL)2 + 24(\u03b2\u03b3LNS ) 2 ( 6 N\u2212SS(N\u22121) + 80e\u03b2(\u03b7KL) 2 ) \u2264 \u03b29 , 10(\u03b7KL)2(K\u22121 + 2(\u03b2\u03b7KL)2) + 960e\u03b2K\u22121(\u03b7KL)2 \u2264 4SK , 80e\u03b2(\u03b7KL)2 \u2264 4S , which can be guaranteed by  \u03b3L \u2272 S3/2 \u03b21/2N ,\n\u03b7KL \u2272 1 S1/2 .\nTherefore, R\u22121\u2211 r=0 Er \u2264 9 7\u03b2 E\u22121 + 2 7 E [ R\u22122\u2211 r=\u22121 \u2225\u2207f(xr)\u22252 ] + 270\u03b2N 7S2 V0 + 720\u03b2\u03c32 7SK R.\nCombining this inequality with Lemma 5, we obtain\n1 \u03b3 E[f(xR)\u2212 f(x0)] \u2264 \u22121 7 R\u22121\u2211 r=0 E[\u2225\u2207f(xr)\u22252] + 39 56\u03b2 E\u22121 + 585\u03b2N 28S2 V0 + 390\u03b2\u03c32 7SK R.\nFinally, noticing that g0 = 0 implies E\u22121 \u2264 2L\u2206 and ci = 1B \u2211 b \u2207F (x0; \u03bebi ) implies V0 \u2264 \u03c32\nB \u2264 SR\u03c32\nNK , we reach\n1\nR R\u22121\u2211 r=0 E[\u2225\u2207f(xr)\u22252] \u2272 L\u2206 \u03b3LR + E\u22121 \u03b2R + \u03b2N S2R V0 + \u03b2\u03c32 SK\n\u2272 L\u2206\n\u03b2R +\nL\u2206\nS3/2R N\u03b21/2 +\n\u03b2\u03c32\nSK\n\u2272 L\u2206\nR\n( 1 + N2/3\nS\n) + \u221a L\u2206\u03c32\nSKR ."
        },
        {
            "heading": "C.2 SCAFFOLD-M-VR",
            "text": "In this subsection, we present the proofs for the SCAFFOLD-M-VR algorithm, shown as in Algorithm 4.\nAlgorithm 4 SCAFFOLD-M-VR: SCAFFOLD with variance-reduced momentum Require: initial model x\u22121 = x0, gradient estimator g0, control variables {c0i }Ni=1 and c0, local\nlearning rate \u03b7, global learning rate \u03b3, momentum \u03b2 for r = 0, \u00b7 \u00b7 \u00b7 , R\u2212 1 do\nUniformly sample clients Sr \u2286 {1, \u00b7 \u00b7 \u00b7 , N} with |Sr| = S for each client i \u2208 Sr in parallel do\nInitialize local model xr,0i = x r for k = 0, \u00b7 \u00b7 \u00b7 ,K \u2212 1 do Compute gr,ki = \u2207F (x r,k i ; \u03be r,k i )\u2212 \u03b2(cri \u2212 cr) + (1\u2212 \u03b2)(gr \u2212\u2207F (xr\u22121; \u03be r,k i ))\nUpdate local model xr,k+1i = x r,k i \u2212 \u03b7g r,k i\nend for Update control variable cr+1i := 1 K \u2211 k \u2207F (x r,k i ; \u03be r,k i ) (for i /\u2208 Sr, c r+1 i = c r i )\nend for Aggregate local updates gr+1 = 1\u03b7SK \u2211 i\u2208Sr ( xr \u2212 xr,Ki ) Update global model xr+1 = xr \u2212 \u03b3gr+1 Update control variable cr+1 = cr + 1N \u2211 i\u2208Sr (c r+1 i \u2212 cri )\nend for\nLemma 20. If \u03b3L \u2264 \u221a\n\u03b2S 126 , then the following holds for r \u2265 1:\nEr \u2264 (1\u2212 8\u03b2\n9 )Er\u22121 +\n14(\u03b3L)2\nS E[\u2225\u2207f(xr\u22121)\u22252] + 8 \u03b2 L2Ur +\n7\u03b22\u03c32\nSK + 4(N \u2212 S) S(N \u2212 1) \u03b22Vr."
        },
        {
            "heading": "In addition,",
            "text": "E0 \u2264 (1\u2212 \u03b2)E\u22121 + 8\n\u03b2 L2U0 +\n7\u03b22\u03c32\nSK + 4(N \u2212 S) S(N \u2212 1) \u03b22V0.\nProof. By Lemma 6, we have\nEr \u2264 E  \u2225\u2225\u2225\u2225\u2225\u2225\u2207f(xr)\u2212 1NK \u2211 i, k [ \u2207F (xr,ki ; \u03be r,k i ) + (1\u2212 \u03b2)(g r \u2212\u2207F (xr\u22121; \u03ber,ki )) ]\u2225\u2225\u2225\u2225\u2225\u2225 2 \n\ufe38 \ufe37\ufe37 \ufe38 \u039b1\n+ N \u2212 S S(N \u2212 1) 1 N N\u2211 i=1 E \u2225\u2225\u2225\u2225\u2225 1K \u2211 k [ \u2207F (xr,ki ; \u03be r,k i )\u2212 (1\u2212 \u03b2)\u2207F (x r\u22121; \u03ber,ki ) ] \u2212 \u03b2cri \u2225\u2225\u2225\u2225\u2225 2 \n\ufe38 \ufe37\ufe37 \ufe38 \u039b2\n.\nApplying the same derivation as Lemma 12, we can show that\n\u039b1 \u2264 (1\u2212 \u03b2)Er\u22121 + 4\n\u03b2 L2Ur + 3\n\u03b22\u03c32\nNK + 3(1\u2212 \u03b2)2 L\n2\nNK E[\u2225xr \u2212 xr\u22121\u22252].\nAdditionally, by the AM-GM inequality,\n\u039b2 \u2264 1\nN N\u2211 i=1 4E \u2225\u2225\u2225\u2225\u2225 1K \u2211 k \u2207F (xr,ki ; \u03be r,k i )\u2212\u2207F (x r; \u03ber,ki ) \u2225\u2225\u2225\u2225\u2225 2\n+ \u03b22 \u2225\u2225\u2225\u2225\u2225 1K \u2211 k \u2207F (xr; \u03ber,ki )\u2212\u2207fi(x r) \u2225\u2225\u2225\u2225\u2225 2 + \u03b22\u2225\u2207fi(xr\u22121)\u2212 cri \u22252\n+ \u2225\u2225\u2225\u2225\u2225\u03b2(\u2207fi(xr)\u2212\u2207fi(xr\u22121)) + 1\u2212 \u03b2K \u2211 k \u2207F (xr; \u03ber,ki )\u2212\u2207F (x r\u22121; \u03ber,ki ) \u2225\u2225\u2225\u2225\u2225 2 \n\u2264 4 ( L2Ur + \u03b22\u03c32\nK + \u03b22Vr + L\n2E[\u2225xr \u2212 xr\u22121\u22252] ) .\nFurther notice that for r \u2265 1, E[\u2225xr \u2212 xr\u22121\u22252 \u2264 2\u03b32(Er\u22121 + E[\u2225\u2207f(xr\u22121)\u22252]) and\n(\u03b3L)2( 8(N \u2212 S) S(N \u2212 1) + 6(1\u2212 \u03b2)2 NK ) \u2264 14(\u03b3L) 2 S \u2264 \u03b2 9 .\nHence we obtain\nEr \u2264 (1\u2212 8\u03b2\n9 )Er\u22121 +\n14(\u03b3L)2\nS E[\u2225\u2207f(xr\u22121)\u22252 + 8 \u03b2 L2Ur +\n7\u03b22\u03c32\nSK + 4(N \u2212 S) S(N \u2212 1) \u03b22Vr.\nThe case for r = 0 can be established similarly.\nLemma 21. If \u03b7KL \u2264 14e , \u03b7K \u2264 \u03b3N 10S , and \u03b3L \u2264 1 24 , then it holds that\nR\u22121\u2211 r=0 Vr \u2264 3N S\n( V0 + 4SR\nNK \u03c32 +\n6N\nS (\u03b3L)2 R\u22122\u2211 r=\u22121 (Er + E[\u2225\u2207f(xr)\u22252])\n) .\nProof. Note that \u03b6r,ki = \u2212\u03b7(\u2207fi(x r,k i ) + (1 \u2212 \u03b2)(gr \u2212\u2207fi(xr\u22121)) \u2212 \u03b2(cri \u2212 cr)), with the same procedures in Lemma 13, we have Ur \u2264 4eK2\u039er + 8(\u03b7K)2(2(\u03b7KL)2 +K\u22121) ( \u03b22\u03c32 + 2L2E[\u2225xr \u2212 xr\u22121\u22252] ) . Additionally, by the AM-GM inequality,\n\u039er = \u03b72\nN \u2211 i E[\u2225\u2207fi(xr) + (1\u2212 \u03b2)(gr \u2212\u2207fi(xr\u22121)\u2212 \u03b2(cri \u2212 cr)\u22252]\n= \u03b72\nN \u2211 i E [\u2225\u2225(\u2207fi(xr)\u2212\u2207fi(xr\u22121)) + (1\u2212 \u03b2)(gr \u2212\u2207f(xr\u22121))\n\u2212\u03b2 ( cri \u2212 cr \u2212\u2207fi(xr\u22121) +\u2207f(xr\u22121) ) +\u2207f(xr\u22121) \u2225\u22252] \u2264 4\u03b72E [ L2\u2225xr \u2212 xr\u22121\u22252 + (1\u2212 \u03b2)2Er\u22121 + \u03b22Vr + \u2225\u2207f(xr\u22121)\u22252\n] \u2264 8\u03b72(Er\u22121 + E[\u2225\u2207f(xr\u22121)\u22252] + \u03b22Vr).\nHence, by applying 32(2(\u03b7KL)2 +K\u22121)(\u03b3L)2 \u2264 96(\u03b3L)2 \u2264 2, we obtain Ur \u2264 32e(\u03b7K)2(Er\u22121 + E[\u2225\u2207f(xr\u22121)\u22252] + \u03b22Vr)\n+ 8(\u03b7K)2(2(\u03b7KL)2 +K\u22121) ( \u03b22\u03c32 + 2L2E[\u2225xr \u2212 xr\u22121\u22252] ) \u2264 90(\u03b7K)2(Er\u22121 + E[\u2225\u2207f(xr\u22121)\u22252] + \u03b22Vr) + 8(\u03b2\u03b7K)2(2(\u03b7KL)2 +K\u22121)\u03c32.\n(10)\nAlso, similar to Lemma 18, it still holds that Vr+1 \u2264 ( 1\u2212 S\n2N\n) Vr + 2N\nS L2E[\u2225xr \u2212 xr\u22121\u22252 + 2S\u03c3\n2\nNK +\n2S N L2Ur.\nCombine this with the upper bound of Ur, Vr+1\n\u2264 ( 1\u2212 S\n2N +\n180(\u03b2\u03b7KL)2S\nN\n) Vr + ( 4N(\u03b3L)2\nS +\n180(\u03b7KL)2S\nN\n) (Er\u22121 + E[\u2225\u2207f(xr\u22121)\u22252])\n+ \u03c32 ( 2S\nNK + 8(\u03b2\u03b7KL)2(2(\u03b7KL)2 +K\u22121) ) \u2264 ( 1\u2212 S\n3N\n) Vr + 6N(\u03b3L)2\nS (Er\u22121 + E[\u2225\u2207f(xr\u22121)\u22252]) +\n4S\u03c32 NK ,\nwhere we apply the upper bound of \u03b7 in the last inequality. Iterating the above inequality completes the proof.\nTheorem 22. Under Assumption 2 and 3, if we take c0i = 1 B \u2211B b=1 \u2207F (x0; \u03bebi ) with {\u03bebi }Bb=1\niid\u223c Di, g0 = c0 = 1N \u2211N i=1 c 0 i and set\n\u03b3 = min\n{ 1\nL ,\n\u221a \u03b2S\nL\n} , \u03b2 = min { S\nN ,\n( KL\u2206\n\u03c32R\n)2/3 S1/3 } ,\n\u03b7KL \u2272 min\n{( \u03b2\nS\n)1/2 , ( \u03b2\nSK\n)1/4} , B = \u2308 max { SK\nNR\u03b22 , NK SR\n}\u2309 ,\n(11)\nSCAFFOLD-M-VR converges as\n1\nR R\u22121\u2211 r=0 E[\u2225\u2207f(xr)\u22252] \u2272 ( L\u2206\u03c3 S \u221a KR )2/3 + L\u2206 R ( 1 + N1/2 S ) .\nAlternatively, if R \u2273 NS and \u03b2 = min { 1 R , ( KL\u2206 \u03c32R )2/3 S1/3 } , B = \u0398(SKRN ), SCAFFOLD-M-VR converges as\n1\nR R\u22121\u2211 r=0 E[\u2225\u2207f(xr)\u22252] \u2272 ( L\u2206\u03c3 S \u221a KR )2/3 + L\u2206 R ( 1 + N1/2 S + \u03c32 SKR ) .\nProof. By Lemma 20, sum over r from 0 to R\u2212 1 and plug (10), Lemma 21 in, R\u22121\u2211 r=0 Er \u2264 (1\u2212 8\u03b2 9 ) R\u22122\u2211 r=\u22121 Er + 14(\u03b3L)2 S R\u22122\u2211 r=0 E[\u2225\u2207f(xr)\u22252] + 7\u03b2 2\u03c32 SK R\n+ 8\n\u03b2 L2 R\u22121\u2211 r=0 Ur + 4\u03b2 2 N \u2212 S S(N \u2212 1) R\u22121\u2211 r=0 Vr\n\u2264 (1\u2212 8\u03b2 9 + 720 (\u03b7KL)2 \u03b2 ) R\u22122\u2211 r=\u22121 Er + ( 14(\u03b3L)2 S + 720 (\u03b7KL)2 \u03b2 ) R\u22122\u2211 r=0 E[\u2225\u2207f(xr)\u22252]\n+ \u03b22\u03c32R\n( 7\nSK +\n64(\u03b7KL)2\n\u03b2 (K\u22121 + 2(\u03b7KL)2) ) + \u03b22 ( 4(N \u2212 S) S(N \u2212 1) + 720 (\u03b7KL)2 \u03b2 )R\u22121\u2211 r=0 Vr\n\u2264 (1\u2212 7\u03b2 9 ) R\u22122\u2211 r=\u22121 Er + ( 14(\u03b3L)2 S + \u03b2 9 ) R\u22122\u2211 r=0 E[\u2225\u2207f(xr)\u22252] + 60\u03b2 2\u03c32 SK R+ 15 \u03b22N S2 V0.\nHere the coefficients in the last inequality are derived by the following bounds: 720 (\u03b7KL) 2 \u03b2 + 18( \u03b2\u03b3LN S ) 2 ( 4 N\u2212SS(N\u22121) + 720 (\u03b7KL)2 \u03b2 ) \u2264 \u03b29 , 64 (\u03b7KL) 2 \u03b2 (K \u22121 + 2(\u03b7KL)2) + 8640 (\u03b7KL) 2 \u03b2K \u2264 5 SK ,\n720 (\u03b7KL) 2\n\u03b2 \u2264 1 S ,\nwhich can be guaranteed by  \u03b3L \u2272 S 3/2 \u03b21/2N , \u03b7KL \u2272 min{ \u221a\n\u03b2 S , ( \u03b2 SK ) 1/4}. Therefore, it holds that\nR\u22121\u2211 r=0 Er \u2264 9 7\u03b2 E\u22121 + 2 7 E [ R\u22122\u2211 r=\u22121 \u2225\u2207f(xr)\u22252 ] + 135\u03b2N 7S2 V0 + 540\u03b2\u03c32 7SK R.\nCombine this inequality with Lemma 5 and we get\n1 \u03b3 E[f(xR)\u2212 f(x0)] \u2264 \u22121 7 R\u22121\u2211 r=0 E[\u2225\u2207f(xr)\u22252] + 39 56\u03b2 E\u22121 + 585\u03b2N 56S2 V0 + 585\u03b2\u03c32 14SK R.\nFinally, for B = \u2308 max { SK\nNR\u03b22 , NK SR }\u2309 , noticing that g0 = 1NB \u2211 i,b \u2207F (x0; \u03bebi ) implies E\u22121 \u2264\n\u03c32 NB \u2264 \u03b22\u03c32R SK and ci = 1 B \u2211 b \u2207F (x0; \u03bebi ) implies V0 \u2264 \u03c32 B \u2264 SR\u03c32 NK , we reach\n1\nR R\u22121\u2211 r=0 E[\u2225\u2207f(xr)\u22252] \u2272 L\u2206 \u03b3LR + E\u22121 \u03b2R + \u03b2N S2R V0 + \u03b2\u03c32 SK\n\u2272 L\u2206\nR +\nL\u2206\n(\u03b2S)1/2R +\nL\u2206\nS3/2R N\u03b21/2 +\n\u03b2\u03c32\nSK\n\u2272 L\u2206\nR\n( 1 + N1/2\nS\n) + ( L\u2206\u03c3\nS \u221a KR\n)2/3 .\nSimilarly, for B = SKRN and R \u2273 N S , E\u22121 \u2264\n\u03c32 NB \u2264 \u03c32 SKR , V0 \u2264 \u03c32 B \u2264 N\u03c32 SKR and thus we have\n1\nR R\u22121\u2211 r=0 E[\u2225\u2207f(xr)\u22252] \u2272 L\u2206 \u03b3LR + E\u22121 \u03b2R + \u03b2N S2R V0 + \u03b2\u03c32 SK\n\u2272 L\u2206\nR +\nL\u2206\n(\u03b2S)1/2R +\nL\u2206\nS3/2R N\u03b21/2 +\n\u03c32\n\u03b2SKR2 +\n\u03b2\u03c32\nSK\n\u2272 L\u2206\nR\n( 1 + N1/2\nS\n) + ( L\u2206\u03c3\nS \u221a KR\n)2/3 + \u03c32\nSKR .\nD IMPLEMENTATION DETAILS & MORE EXPERIMENTS"
        },
        {
            "heading": "D.1 TRAINING SETUP OF MLP",
            "text": "We generate non-iid data for the clients, we sample label ratios from the Dirichlet distribution (Hsu et al., 2019) with a parameter of 0.5 for the full participation setting and 0.2 for the partial participation setting. Our experimental setup involves N = 10 clients and K = 32 local updates. The weight decay is set as 10\u22124. The global learning rate is fixed as \u03b3 = \u03b7K for all the algorithms, and we perform a grid search for the local learning rate \u03b7 in values {0.005, 0.01, 0.05, 0.1, 0.5}. Similarly, we search for the momentum parameter \u03b2 in values {0.1, 0.2, 0.5, 0.8}."
        },
        {
            "heading": "D.2 TRAINING SETUP OF RESNET18",
            "text": "We generate non-iid data by setting the parameter of Dirichlet distribution as 0.1, which implies higher heterogeneity. The experiment involves N = 10 clients and K = 16 local updates. We set S = 2 in the partial participation setting. The local learning is fixed as \u03b7\u0302 = 0.001 and global learning rate is \u03b3\u0302 = \u03b7\u0302K. The momentum parameter is \u03b2 = 0.1 and batchsize is 128.\nReparameterizing momentum. The update rule of FEDAVG-M in (1) is equivalent to, with a transformation of hyperparameters g\u0302r,ki := g r,k i /\u03b2, g\u0302 r := gr/\u03b2, \u03b7\u0302 := \u03b2\u03b7, \u03b3\u0302 := \u03b2\u03b3,\ng\u0302r,ki = \u2207F (x r,k i ; \u03be r,k i ) + (1\u2212 \u03b2)g\u0302 r,\nxr,k+1i = x r,k i \u2212 \u03b7\u0302 g\u0302 r,k i , x r+1 = xr \u2212 \u03b3\u0302g\u0302r+1. This is typically used in the current Pytorch implementation of momentum-based methods. When \u03b2 = 1, it still reduces to vanilla FEDAVG. Similarly, in SCAFFOLD-M, the update rule (3) is equivalent to\ng\u0302r,ki = (\u2207F (x r,k i ; \u03be r,k i )\u2212 c r i + c r) + (1\u2212 \u03b2)g\u0302r\nIn all the experiments on ResNet18, we implement our proposed FEDAVG-M and SCAFFOLD-M with this reparameterization."
        },
        {
            "heading": "D.3 MORE EXPERIMENTS OF RESNET18",
            "text": "We conduct more algorithms under mild heterogeneity with the parameter of Dirichlet distribution being 0.5. We set S = 5 in the partial participation setting. Other hyperparameters are the same as described in Section D.2. We plot the evolution of test loss in Figures 3(a) and 3(b), respectively. Again, we observe that our proposed FEDAVG-M and SCAFFOLD-M outperform the vanilla FEDAVG and SCAFFOLD with evident margins. We also evaluate VR methods in terms of test accuracy in the context of full client participation. The results are presented in Figure 4, which demonstrates the advantage of our proposed VR methods over the prior methods."
        },
        {
            "heading": "D.4 EXPERIMENTS WITH MORE CLIENTS",
            "text": "We further conduct experiments with N = 100 on the MNIST dataset and two-layer fully connected ReLU neural network. The parameter of Dirichlet distribution is 0.2 and the batchsize is 32. We set the number of local steps K = 16. For FEDAVG-M and SCAFFOLD-M, we set \u03b2 = 0.2. We plot the test loss and test accuracy of our proposed algorithms in the regime of full participation (S = N ) and partial participation (S = 10 and S = 5). The results are shown in Figure 5 and 6. Compared to former experiments where N , we observe that our proposed momentum-based algorithms scale well to FL setups with large N . Moreover, we observe that the advantage of our momentum-based variants over the vanilla FEDAVG and SCAFFOLD becomes more evident when fewer clients participate in training, suggesting a great utility of our algorithms in practical FL setups.\nD.5 IMPACT OF MOMENTUM VALUE \u03b2\nTo further illustrate the effect of momentum, we examine different choices of \u03b2 in both FEDAVGM and SCAFFOLD-M under partial participation setting with S = 5 and N = 100. We again simulate with the MNIST dataset and two-layer fully connected ReLU neural networks. The results are shown in Figure 7 and 8. It is worth noting that when \u03b2 \u2192 1, the momentum will anneal down to off, recovering the vanilla FEDAVG and SCAFFOLD. We observe that the stronger the momentum used, the better performance we eventually obtain. This directly demonstrates the benefit of momentum."
        }
    ],
    "year": 2024
}