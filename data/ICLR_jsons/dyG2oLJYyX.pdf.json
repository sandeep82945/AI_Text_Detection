{
    "abstractText": "Existing NAS methods suffer from either an excessive amount of time for repetitive sampling and training of many task-irrelevant architectures. To tackle such limitations of existing NAS methods, we propose a paradigm shift from NAS to a novel conditional Neural Architecture Generation (NAG) framework based on diffusion models, dubbed DiffusionNAG. Specifically, we consider the neural architectures as directed graphs and propose a graph diffusion model for generating them. Moreover, with the guidance of parameterized predictors, DiffusionNAG can flexibly generate task-optimal architectures with the desired properties for diverse tasks, by sampling from a region that is more likely to satisfy the properties. This conditional NAG scheme is significantly more efficient than previous NAS schemes which sample the architectures and filter them using the property predictors. We validate the effectiveness of DiffusionNAG through extensive experiments in two predictor-based NAS scenarios: Transferable NAS and Bayesian Optimization (BO)-based NAS. DiffusionNAG achieves superior performance with speedups of up to 20\u00d7 when compared to the baselines on Transferable NAS benchmarks. Furthermore, when integrated into a BO-based algorithm, DiffusionNAG outperforms existing BO-based NAS approaches, particularly in the large MobileNetV3 search space on the ImageNet 1K dataset.",
    "authors": [],
    "id": "SP:caa7c498f09d5313a20b8024c50c348b2e4e8252",
    "references": [
        {
            "authors": [
                "Brian D.O. Anderson"
            ],
            "title": "Reverse-time diffusion equation models",
            "venue": "Stochastic Processes and their Applications,",
            "year": 1982
        },
        {
            "authors": [
                "James Bergstra",
                "Yoshua Bengio"
            ],
            "title": "Random search for hyper-parameter optimization",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2012
        },
        {
            "authors": [
                "Han Cai",
                "Ligeng Zhu",
                "Song Han"
            ],
            "title": "ProxylessNAS: Direct neural architecture search on target task and hardware",
            "venue": "In International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "Han Cai",
                "Chuang Gan",
                "Tianzhe Wang",
                "Zhekai Zhang",
                "Song Han"
            ],
            "title": "Once-for-all: Train one network and specialize it for efficient deployment",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Huiwen Chang",
                "Han Zhang",
                "Lu Jiang",
                "Ce Liu",
                "William T Freeman"
            ],
            "title": "Maskgit: Masked generative image transformer",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Xiangning Chen",
                "Ruochen Wang",
                "Minhao Cheng",
                "Xiaocheng Tang",
                "Cho-Jui Hsieh"
            ],
            "title": "DrNAS: Dirichlet neural architecture search",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Alexander Cowen-Rivers",
                "Wenlong Lyu",
                "Rasul Tutunov",
                "Zhi Wang",
                "Antoine Grosnit",
                "Ryan-Rhys Griffiths",
                "Alexandre Maravel",
                "Jianye Hao",
                "Jun Wang",
                "Jan Peters",
                "Haitham Bou Ammar"
            ],
            "title": "HEBO: Pushing the limits of sample-efficient hyperparameter optimisation",
            "venue": "Journal of Artificial Intelligence Research,",
            "year": 2022
        },
        {
            "authors": [
                "Francesco Croce",
                "Matthias Hein"
            ],
            "title": "Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks",
            "venue": "In International conference on machine learning,",
            "year": 2020
        },
        {
            "authors": [
                "Ekin D Cubuk",
                "Barret Zoph",
                "Dandelion Mane",
                "Vijay Vasudevan",
                "Quoc V Le"
            ],
            "title": "Autoaugment: Learning augmentation policies from data",
            "year": 2018
        },
        {
            "authors": [
                "J. Deng",
                "W. Dong",
                "R. Socher",
                "L.-J. Li",
                "K. Li",
                "L. Fei-Fei"
            ],
            "title": "ImageNet: A Large-Scale Hierarchical Image Database",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2009
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova"
            ],
            "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
            "venue": "Association for Computational Linguistics,",
            "year": 2019
        },
        {
            "authors": [
                "Terrance DeVries",
                "Graham W Taylor"
            ],
            "title": "Improved regularization of convolutional neural networks with cutout",
            "venue": "arXiv preprint arXiv:1708.04552,",
            "year": 2017
        },
        {
            "authors": [
                "Prafulla Dhariwal",
                "Alexander Nichol"
            ],
            "title": "Diffusion models beat gans on image synthesis",
            "venue": "Advances in neural information processing systems,",
            "year": 2021
        },
        {
            "authors": [
                "Xuanyi Dong",
                "Yi Yang"
            ],
            "title": "One-shot neural architecture search via self-evaluated template network",
            "venue": "In Proceedings of the IEEE International Conference on Computer Vision (CVPR),",
            "year": 2019
        },
        {
            "authors": [
                "Xuanyi Dong",
                "Yi Yang"
            ],
            "title": "Searching for a robust neural architecture in four gpu hours",
            "venue": "In Proceedings of the IEEE Conference on computer vision and pattern recognition (CVPR),",
            "year": 2019
        },
        {
            "authors": [
                "Xuanyi Dong",
                "Yi Yang"
            ],
            "title": "Nas-bench-201: Extending the scope of reproducible neural architecture search",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2020
        },
        {
            "authors": [
                "Xuanyi Dong",
                "Yi Yang"
            ],
            "title": "Nas-bench-201: Extending the scope of reproducible neural architecture search",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2020
        },
        {
            "authors": [
                "\u0141ukasz Dudziak",
                "Thomas Chau",
                "Mohamed S Abdelfattah",
                "Royson Lee",
                "Hyeji Kim",
                "Nicholas D Lane"
            ],
            "title": "Brp-nas: Prediction-based nas using gcns. Advances in neural information processing systems (NeurIPS), 2020",
            "year": 2020
        },
        {
            "authors": [
                "Stefan Falkner",
                "Aaron Klein",
                "Frank Hutter"
            ],
            "title": "BOHB: Robust and efficient hyperparameter optimization at scale",
            "venue": "In International Conference on Machine Learning,",
            "year": 2018
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
            "year": 2016
        },
        {
            "authors": [
                "Jonathan Ho",
                "Tim Salimans"
            ],
            "title": "Classifier-free diffusion guidance",
            "venue": "arXiv preprint arXiv:2207.12598,",
            "year": 2022
        },
        {
            "authors": [
                "Jonathan Ho",
                "Ajay Jain",
                "Pieter Abbeel"
            ],
            "title": "Denoising diffusion probabilistic models",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Emiel Hoogeboom",
                "V\u0131ctor Garcia Satorras",
                "Cl\u00e9ment Vignac",
                "Max Welling"
            ],
            "title": "Equivariant diffusion for molecule generation in 3d",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Jie Hu",
                "Li Shen",
                "Gang Sun"
            ],
            "title": "Squeeze-and-excitation networks",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Gao Huang",
                "Yu Sun",
                "Zhuang Liu",
                "Daniel Sedra",
                "Kilian Q Weinberger"
            ],
            "title": "Deep networks with stochastic depth",
            "venue": "In ECCV,",
            "year": 2016
        },
        {
            "authors": [
                "Aapo Hyv\u00e4rinen"
            ],
            "title": "Estimation of non-normalized statistical models by score matching",
            "venue": "J. Mach. Learn. Res.,",
            "year": 2005
        },
        {
            "authors": [
                "Myeonghun Jeong",
                "Hyeongju Kim",
                "Sung Jun Cheon",
                "Byoung Jin Choi",
                "Nam Soo Kim"
            ],
            "title": "Diff-tts: A denoising diffusion model for text-to-speech",
            "venue": "In Interspeech 2021,",
            "year": 2021
        },
        {
            "authors": [
                "Jaehyeong Jo",
                "Seul Lee",
                "Sung Ju Hwang"
            ],
            "title": "Score-based generative modeling of graphs via the system of stochastic differential equations",
            "venue": "In Proceedings of the 39th International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Steffen Jung",
                "Jovita Lukasik",
                "Margret Keuper"
            ],
            "title": "Neural architecture design and robustness: A dataset",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Zhifeng Kong",
                "Wei Ping",
                "Jiaji Huang",
                "Kexin Zhao",
                "Bryan Catanzaro"
            ],
            "title": "Diffwave: A versatile diffusion model for audio synthesis",
            "venue": "In ICLR,",
            "year": 2021
        },
        {
            "authors": [
                "Alex Krizhevsky"
            ],
            "title": "Learning multiple layers of features from tiny images",
            "year": 2009
        },
        {
            "authors": [
                "Hayeon Lee",
                "Eunyoung Hyung",
                "Sung Ju Hwang"
            ],
            "title": "Rapid neural architecture search by learning to generate graphs from datasets",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Hayeon Lee",
                "Sewoong Lee",
                "Song Chong",
                "Sung Ju Hwang"
            ],
            "title": "Hardware-adaptive efficient latency prediction for nas via meta-learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Juho Lee",
                "Yoonho Lee",
                "Jungtaek Kim",
                "Adam Kosiorek",
                "Seungjin Choi",
                "Yee Whye Teh"
            ],
            "title": "Set transformer: A framework for attention-based permutation-invariant neural networks",
            "venue": "In International Conference on Machine Learning (ICML),",
            "year": 2019
        },
        {
            "authors": [
                "Seul Lee",
                "Jaehyeong Jo",
                "Sung Ju Hwang"
            ],
            "title": "Exploring chemical space with score-based out-ofdistribution generation",
            "venue": "Proceedings of the 40th International Conference on Machine Learning,",
            "year": 2023
        },
        {
            "authors": [
                "Chaojian Li",
                "Zhongzhi Yu",
                "Yonggan Fu",
                "Yongan Zhang",
                "Yang Zhao",
                "Haoran You",
                "Qixuan Yu",
                "Yue Wang",
                "Cong Hao",
                "Yingyan Lin"
            ],
            "title": "Hw-nas-bench: Hardware-aware neural architecture search benchmark",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Liam Li",
                "Ameet Talwalkar"
            ],
            "title": "Random search and reproducibility for neural architecture search",
            "venue": "In Uncertainty in Artificial Intelligence,",
            "year": 2019
        },
        {
            "authors": [
                "Marius Lindauer",
                "Frank Hutter"
            ],
            "title": "Best practices for scientific research on neural architecture search",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2020
        },
        {
            "authors": [
                "Hanxiao Liu",
                "Karen Simonyan",
                "Yiming Yang"
            ],
            "title": "Darts: Differentiable architecture search",
            "venue": "In In International Conference on Learning Representations (ICLR),",
            "year": 2019
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter"
            ],
            "title": "Sgdr: Stochastic gradient descent with warm restarts",
            "venue": "arXiv preprint arXiv:1608.03983,",
            "year": 2016
        },
        {
            "authors": [
                "Zhichao Lu",
                "Kalyanmoy Deb",
                "Erik Goodman",
                "Wolfgang Banzhaf",
                "Vishnu Naresh Boddeti"
            ],
            "title": "NSGANetV2: Evolutionary multi-objective surrogate-assisted neural architecture search",
            "venue": "In European Conference on Computer Vision (ECCV),",
            "year": 2020
        },
        {
            "authors": [
                "Renqian Luo",
                "Fei Tian",
                "Tao Qin",
                "Enhong Chen",
                "Tie-Yan Liu"
            ],
            "title": "Neural architecture optimization. In Advances in neural information processing systems (NeurIPS), 2018",
            "year": 2018
        },
        {
            "authors": [
                "Subhransu Maji",
                "Esa Rahtu",
                "Juho Kannala",
                "Matthew Blaschko",
                "Andrea Vedaldi"
            ],
            "title": "Fine-grained visual classification of aircraft",
            "venue": "arXiv preprint arXiv:1306.5151,",
            "year": 2013
        },
        {
            "authors": [
                "David Marwood",
                "Shumeet Baluja",
                "Yair Alon"
            ],
            "title": "Diversity and diffusion: Observations on synthetic image distributions with stable diffusion",
            "venue": "arXiv preprint arXiv:2311.00056,",
            "year": 2023
        },
        {
            "authors": [
                "Willie Neiswanger",
                "Kirthevasan Kandasamy",
                "Barnabas Poczos",
                "Jeff Schneider",
                "Eric Xing"
            ],
            "title": "Probo: Versatile bayesian optimization using any probabilistic programming language",
            "year": 1901
        },
        {
            "authors": [
                "Alexander Quinn Nichol",
                "Prafulla Dhariwal",
                "Aditya Ramesh",
                "Pranav Shyam",
                "Pamela Mishkin",
                "Bob McGrew",
                "Ilya Sutskever",
                "Mark Chen"
            ],
            "title": "GLIDE: towards photorealistic image generation and editing with text-guided diffusion models",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Xuefei Ning",
                "Yin Zheng",
                "Tianchen Zhao",
                "Yu Wang",
                "Huazhong Yang"
            ],
            "title": "A generic graph-based neural architecture encoding scheme for predictor-based nas",
            "venue": "In Computer Vision\u2013ECCV 2020: 16th European Conference,",
            "year": 2020
        },
        {
            "authors": [
                "Xuefei Ning",
                "Changcheng Tang",
                "Wenshuo Li",
                "Zixuan Zhou",
                "Shuang Liang",
                "Huazhong Yang",
                "Yu Wang"
            ],
            "title": "Evaluating efficient performance estimators of neural architectures",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Chenhao Niu",
                "Yang Song",
                "Jiaming Song",
                "Shengjia Zhao",
                "Aditya Grover",
                "Stefano Ermon"
            ],
            "title": "Permutation invariant graph generation via score-based generative modeling",
            "venue": "In International Conference on Artificial Intelligence and Statistics,",
            "year": 2020
        },
        {
            "authors": [
                "Chenhao Niu",
                "Yang Song",
                "Jiaming Song",
                "Shengjia Zhao",
                "Aditya Grover",
                "Stefano Ermon"
            ],
            "title": "Permutation invariant graph generation via score-based generative modeling",
            "venue": "In AISTATS,",
            "year": 2020
        },
        {
            "authors": [
                "Omkar M. Parkhi",
                "Andrea Vedaldi",
                "Andrew Zisserman",
                "C.V. Jawahar"
            ],
            "title": "Cats and dogs",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2012
        },
        {
            "authors": [
                "Hieu Pham",
                "Melody Y Guan",
                "Barret Zoph",
                "Quoc V Le",
                "Jeff Dean"
            ],
            "title": "Efficient neural architecture search via parameter sharing",
            "venue": "In International Conference on Machine Learning (ICML),",
            "year": 2018
        },
        {
            "authors": [
                "Aditya Ramesh",
                "Mikhail Pavlov",
                "Gabriel Goh",
                "Scott Gray",
                "Chelsea Voss",
                "Alec Radford",
                "Mark Chen",
                "Ilya Sutskever"
            ],
            "title": "Zero-shot text-to-image generation",
            "venue": "In International Conference on Machine Learning. PMLR,",
            "year": 2021
        },
        {
            "authors": [
                "Esteban Real",
                "Alok Aggarwal",
                "Yanping Huang",
                "Quoc V Le"
            ],
            "title": "Regularized evolution for image classifier architecture search",
            "venue": "In Proceedings of the aaai conference on artificial intelligence (AAAI),",
            "year": 2019
        },
        {
            "authors": [
                "Robin Rombach",
                "Andreas Blattmann",
                "Dominik Lorenz",
                "Patrick Esser",
                "Bj\u00f6rn Ommer"
            ],
            "title": "Highresolution image synthesis with latent diffusion models",
            "venue": "In IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Binxin Ru",
                "Xingchen Wan",
                "Xiaowen Dong",
                "Michael Osborne"
            ],
            "title": "Interpretable neural architecture search via bayesian optimisation with weisfeiler-lehman kernels",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Gresa Shala",
                "Thomas Elsken",
                "Frank Hutter",
                "Josif Grabocka"
            ],
            "title": "Transfer NAS with meta-learned bayesian surrogates",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Jasper Snoek",
                "Hugo Larochelle",
                "Ryan P Adams"
            ],
            "title": "Practical bayesian optimization of machine learning algorithms",
            "venue": "arXiv preprint arXiv:1206.2944,",
            "year": 2012
        },
        {
            "authors": [
                "Jascha Sohl-Dickstein",
                "Eric Weiss",
                "Niru Maheswaranathan",
                "Surya Ganguli"
            ],
            "title": "Deep unsupervised learning using nonequilibrium thermodynamics",
            "venue": "In International Conference on Machine Learning,",
            "year": 2015
        },
        {
            "authors": [
                "Yang Song",
                "Stefano Ermon"
            ],
            "title": "Generative modeling by estimating gradients of the data distribution",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Yang Song",
                "Jascha Sohl-Dickstein",
                "Diederik P. Kingma",
                "Abhishek Kumar",
                "Stefano Ermon",
                "Ben Poole"
            ],
            "title": "Score-based generative modeling through stochastic differential equations",
            "venue": "In ICLR,",
            "year": 2021
        },
        {
            "authors": [
                "Yang Song",
                "Jascha Sohl-Dickstein",
                "Diederik P Kingma",
                "Abhishek Kumar",
                "Stefano Ermon",
                "Ben Poole"
            ],
            "title": "Score-based generative modeling through stochastic differential equations",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Simo S\u00e4rkk\u00e4",
                "Arno Solin"
            ],
            "title": "Applied Stochastic Differential Equations. Institute of Mathematical Statistics Textbooks",
            "year": 2019
        },
        {
            "authors": [
                "Mingxing Tan",
                "Bo Chen",
                "Ruoming Pang",
                "Vijay Vasudevan",
                "Mark Sandler",
                "Andrew Howard",
                "Quoc V Le"
            ],
            "title": "Mnasnet: Platform-aware neural architecture search for mobile",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Clement Vignac",
                "Igor Krawczuk",
                "Antoine Siraudin",
                "Bohan Wang",
                "Volkan Cevher",
                "Pascal Frossard"
            ],
            "title": "Digress: Discrete denoising diffusion for graph generation",
            "venue": "arXiv preprint arXiv:2209.14734,",
            "year": 2022
        },
        {
            "authors": [
                "Hanrui Wang",
                "Zhanghao Wu",
                "Zhijian Liu",
                "Han Cai",
                "Ligeng Zhu",
                "Chuang Gan",
                "Song Han"
            ],
            "title": "Hat: Hardware-aware transformers for efficient natural language processing",
            "venue": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
            "year": 2020
        },
        {
            "authors": [
                "Wei Wen",
                "Hanxiao Liu",
                "Yiran Chen",
                "Hai Li",
                "Gabriel Bender",
                "Pieter-Jan Kindermans"
            ],
            "title": "Neural predictor for neural architecture search",
            "venue": "In European Conference on Computer Vision,",
            "year": 2020
        },
        {
            "authors": [
                "Colin White",
                "Sam Nolen",
                "Yash Savani"
            ],
            "title": "Local search is state of the art for nas benchmarks",
            "venue": "arXiv preprint arXiv:2005.02960,",
            "year": 2005
        },
        {
            "authors": [
                "Colin White",
                "Willie Neiswanger",
                "Yash Savani"
            ],
            "title": "Bananas: Bayesian optimization with neural architectures for neural architecture search",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Colin White",
                "Arber Zela",
                "Robin Ru",
                "Yang Liu",
                "Frank Hutter"
            ],
            "title": "How powerful are performance predictors in neural architecture search",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Colin White",
                "Mahmoud Safari",
                "Rhea Sukthanker",
                "Binxin Ru",
                "Thomas Elsken",
                "Arber Zela",
                "Debadeepta Dey",
                "Frank Hutter"
            ],
            "title": "Neural architecture search: Insights from 1000 papers",
            "venue": "arXiv preprint arXiv:2301.08727,",
            "year": 2023
        },
        {
            "authors": [
                "Ronald J Williams"
            ],
            "title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning",
            "venue": "Machine learning,",
            "year": 1992
        },
        {
            "authors": [
                "Yuhui Xu",
                "Lingxi Xie",
                "Xiaopeng Zhang",
                "Xin Chen",
                "Guo-Jun Qi",
                "Qi Tian",
                "Hongkai Xiong"
            ],
            "title": "Pc-darts: Partial channel connections for memory-efficient architecture search",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2020
        },
        {
            "authors": [
                "Shen Yan",
                "Kaiqiang Song",
                "Fei Liu",
                "Mi Zhang"
            ],
            "title": "Cate: Computation-aware neural architecture encoding with transformers",
            "venue": "In ICML,",
            "year": 2021
        },
        {
            "authors": [
                "Shen Yan",
                "Colin White",
                "Yash Savani",
                "Frank Hutter"
            ],
            "title": "Nas-bench-x11 and the power of learning curves",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Arber Zela",
                "Julien Niklas Siems",
                "Lucas Zimmer",
                "Jovita Lukasik",
                "Margret Keuper",
                "Frank Hutter"
            ],
            "title": "Surrogate nas benchmarks: Going beyond the limited search spaces of tabular nas benchmarks",
            "venue": "In Tenth International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Muhan Zhang",
                "Shali Jiang",
                "Zhicheng Cui",
                "Roman Garnett",
                "Yixin Chen"
            ],
            "title": "D-VAE: A variational autoencoder for directed acyclic graphs",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Barret Zoph",
                "Quoc V Le"
            ],
            "title": "Neural architecture search with reinforcement learning",
            "venue": "International Conference on Learning Representations (ICLR),",
            "year": 2017
        },
        {
            "authors": [
                "Barret Zoph",
                "Vijay Vasudevan",
                "Jonathon Shlens",
                "Quoc V Le"
            ],
            "title": "Learning transferable architectures for scalable image recognition",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR),",
            "year": 2018
        },
        {
            "authors": [
                "Yan"
            ],
            "title": "2021a), we employ L transformer blocks (T) (Vaswani et al., 2017) to parameterize the score network. These blocks utilize an attention mask M \u2208 RN\u00d7N",
            "year": 2017
        },
        {
            "authors": [
                "Lee"
            ],
            "title": "2021a), we employ a dataset encoder based on Set Transformer (Lee et al., 2019) to accurately capture the characteristics of the target dataset. This dataset encoder is specifically designed to process input sets of varying sizes and output a fixed size latent code, denoted as zD, that effectively encapsulates the information within the dataset. The dataset encoder consists of two stacked modules that are permutation-invariant, meaning output",
            "year": 2019
        },
        {
            "authors": [
                "Lee"
            ],
            "title": "2021a), we report results obtained from ResNet56. Random Search (RS, Bergstra & Bengio (2012)) is a basic NAS approach that involves randomly sampling architectures from the search space and selecting the top-performing one. Another basic NAS method we consider is REA (Real et al., 2019)",
            "year": 2019
        },
        {
            "authors": [
                "loss. GDAS (Dong",
                "Yang"
            ],
            "title": "2019b) is a differentiable neural architecture sampler that utilizes the Gumbel-Softmax relaxation technique. GDAS sampler is designed to be trainable and optimized based on the validation loss after training the sampled architecture. This allows the sampler to be trained in an end-to-end fashion using gradient descent",
            "venue": "PC-DARTS (Xu et al.,",
            "year": 2020
        },
        {
            "authors": [
                "Lee"
            ],
            "title": "Aircraft (Maji et al., 2013), and Oxford IIT Pets (Parkhi et al., 2012), as described in Section 3.2 of the main paper. In this section, we present a detailed description of each dataset used in our study",
            "venue": "(Krizhevsky,",
            "year": 2009
        },
        {
            "authors": [
                "Cai"
            ],
            "title": "Oxford-IIIT Pets datasets, following the experimental setup",
            "year": 2020
        },
        {
            "authors": [
                "Lee"
            ],
            "title": "Specifically, we utilized SGD optimization with cross-entropy loss, setting the weight decay to 0.0005. The learning rate decayed from 0.1 to 0 using a cosine annealing schedule (Loshchilov & Hutter, 2016). We resized images as 224\u00d7224 pixels and applied dataset augmentations, including random horizontal flip, cutout DeVries & Taylor (2017) with a length of 16, AutoAugment",
            "venue": "Cubuk et al",
            "year": 2020
        },
        {
            "authors": [
                "Yan"
            ],
            "title": "2016) with a rate of 0.2. To ensure a fair comparison, we applied the same training pipeline to all architectures obtained by our TransferNAG and the baseline NAS methods. In Section 3.2 of the main paper, we leveraged a surrogate strategy to report the performance of architectures, which is inspired by the existing surrogate NAS benchmarks Li et al",
            "venue": "Zela et al",
            "year": 2021
        },
        {
            "authors": [
                "Cai"
            ],
            "title": "1K. By training an accuracy predictor using these [architecture",
            "year": 2020
        },
        {
            "authors": [
                "White"
            ],
            "title": "By utilizing an ensemble of predictors {\u0125\u03c8m}m=1, we can estimate the mean and standard deviation of predictions for each architecture in the search space (Please refer to Algorithms 1 and 2 in Appendix C.6)",
            "venue": "Following Neiswanger et al",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "While Neural Architecture Search (NAS) approaches automate neural architecture design, eliminating the need for manual design process with trial-and-error (Zoph & Le, 2017; Liu et al., 2019; Cai et al., 2019; Luo et al., 2018; Real et al., 2019; White et al., 2020), they mostly suffer from the high search cost, which often includes the full training with the searched architectures. To address this issue, many previous works have proposed to utilize parameterized property predictors (Luo et al., 2018; White et al., 2021a;b; 2023; Ning et al., 2020; 2021; Dudziak et al., 2020; Lee et al., 2021a;b) that can predict the performance of an architecture without training. However, existing NAS approaches still result in large waste of time as they need to explore an extensive search space and the property predictors mostly play a passive role such as the evaluators that rank architecture candidates provided by a search strategy to simply filter them out during the search process.\nTo overcome such limitations, we propose a paradigm shift from NAS (Neural Architecture Search) to a novel conditional NAG (Neural Architecture Generation) framework that enables the generation of desired neural architectures. Specifically, we introduce a novel predictor-guided Diffusion-based Neural Architecture Generative framework called DiffusionNAG, which explicitly incorporates the predictors into generating architectures that satisfy the objectives (e.g., high accuracy or robustness against attack). To achieve this goal, we employ the diffusion generative models (Ho et al., 2020; Song et al., 2021b), which generate data by gradually injecting noise into the data and learning to reverse this process. They have demonstrated remarkable generative performance across a wide range of domains. Especially, we are inspired by their parameterized model-guidance mechanism (Sohl-Dickstein et al., 2015; Vignac et al., 2022) that allows the diffusion generative models to excel in conditional generation over diverse domains such as generating images that match specific labels (Ramesh et al., 2021) or discovering new drugs meeting particular property criteria (Lee et al., 2023).\nIn this framework, we begin by training the base diffusion generative model to generate architectures that follow the distribution of a search space without requiring expensive label information, e.g., accuracy. Then, to achieve our primary goal of generating architectures that meet the specified target\ncondition, we deploy the trained diffusion model to diverse downstream tasks, while controlling the generation process with property predictors. Specifically, we leverage the gradients of parameterized predictors to guide the generative model toward the space of the architectures with desired properties. The proposed conditional NAG framework offers the key advantages compared with the conventional NAS methods as follows: Firstly, our approach facilitates efficient search by generating architectures that follow the specific distribution of interest within the search space, minimizing the time wasted exploring architectures that are less likely to have the desired properties. Secondly, DiffusionNAG, which utilizes the predictor for both NAG and evaluation purposes, shows superior performance compared to the traditional approach, where the same predictor is solely limited to the evaluation role. Lastly, DiffusionNAG is easily applicable to various types of NAS tasks (e.g., latency or robustness-constrained NAS) as we can swap out the predictors in a plug-and-play manner without retraining the base generative model, making it practical for diverse NAS scenarios.\nAdditionally, to ensure the generation of valid architectures, we design a novel score network for neural architectures. In previous works on NAS, neural architectures have been typically represented as directed acyclic graphs (Zhang et al., 2019) to model their computational flow where the input data sequentially passes through the multiple layers of the network to produce an output. However, existing graph diffusion models (Niu et al., 2020a; Jo et al., 2022) have primarily focused on undirected graphs, which represent structure information of graphs while completely ignoring the directional relationships between nodes, and thus cannot capture the computational flow in architectures. To address this issue, we introduce a score network that encodes the positional information of nodes to capture their order connected by directed edges.\nWe demonstrate the effectiveness of DiffusionNAG with extensive experiments under two key predictor-based NAS scenarios: 1) Transferable NAS and 2) Bayesian Optimization (BO)-based NAS. For Transferable NAS using transferable dataset-aware predictors, DiffusionNAG achieves superior or comparable performance with the speedup of up to 20\u00d7 on four datasets from Transferable NAS benchmarks, including the large MobileNetV3 (MBv3) search space and NAS-Bench-201. Notably, DiffusionNAG demonstrates superior generation quality compared to MetaD2A (Lee et al., 2021a), a closely related unconditional generation-based method. For BO-based NAS with task-specific predictors, DiffusionNAG outperforms existing BO-based NAS approaches that rely on heuristic acquisition optimization strategies, such as random architecture sampling or architecture mutation, across four acquisition functions. This is because DiffusionNAG overcomes the limitation of existing BO-based NAS, which samples low-quality architectures during the initial phase, by sampling from the space of the architectures that satisfy the given properties. DiffusionNAG obtains especially large performance gains on the large MBv3 search space on the ImageNet 1K dataset, demonstrating its effectiveness in restricting the solution space when the search space is large. Furthermore, we verify that our score network generates 100% valid architectures by successfully capturing their computational flow, whereas the diffusion model for undirected graphs (Jo et al., 2022) almost fails.\nOur contributions can be summarized as follows:\n\u2022 We propose a paradigm shift from conventional NAS approaches to a novel conditional Neural Architecture Generation (NAG) scheme, by proposing a framework called DiffusionNAG. With the guidance of the property predictors, DiffusionNAG can generate task-optimal architectures for diverse tasks.\n\u2022 DiffusionNAG offers several advantages compared with conventional NAS methods, including efficient and effective search, superior utilization of predictors for both NAG and evaluation purposes, and easy adaptability across diverse tasks.\n\u2022 Furthermore, to ensure the generation of valid architectures by accurately capturing the computational flow, we introduce a novel score network for neural architectures that encodes positional information in directed acyclic graphs representing architectures.\n\u2022 We have demonstrated the effectiveness of DiffusionNAG in Transferable NAS and BO-NAS scenarios, achieving significant acceleration and improved search performance in extensive experimental settings. DiffusionNAG significantly outperforms existing NAS methods in such experiments."
        },
        {
            "heading": "2 METHOD",
            "text": "In Section 2.1, we first formulate the diffusion process for the generation of the architectures that follow the distribution of the search space. In Section 2.2, we propose a conditional diffusion\nframework for NAG that leverages a predictor for guiding the generation process. Finally, we extend the architecture generation framework for Transferable NAS in Section 2.3. Representation of Neural Architectures A neural architecture A in the search spaceA is typically considered as a directed acyclic graph (DAG) (Dong & Yang, 2020b). Specifically, the architecture A withN nodes is defined by its operator type matrix V \u2208 RN\u00d7F and upper triangular adjacency matrix E \u2208 RN\u00d7N , as A = (V ,E) \u2208 RN\u00d7F \u00d7 RN\u00d7N , where F is the number of predefined operator sets. In the MobileNetV3 search space (Cai et al., 2020), N represents the maximum possible number of layers, and the operation sets denote a set of combinations of the kernel size and width."
        },
        {
            "heading": "2.1 NEURAL ARCHITECTURE DIFFUSION PROCESS",
            "text": "As a first step, we formulate an unconditional neural architecture diffusion process. Following Song et al. (2021b), we define a forward diffusion process that describes the perturbation of neural architecture distribution (search space) to a known prior distribution (e.g., Gaussian normal distribution) modeled by a stochastic differential equation (SDE), and then learn to reverse the perturbation process to sample the architectures from the search space starting from noise.\nForward process We define the forward diffusion process that maps the neural architecture distribution p(A0) to the known prior distribution p(AT ) as the following It\u00f4 SDE:\ndAt = ft(At)dt+ gtdw, (1) where t-subscript represents a function of time (Ft(\u00b7) := F (\u00b7,t)), ft(\u00b7) : A \u2192 A is the linear drift coefficient, gt : A \u2192 R is the scalar diffusion coefficient, and w is the standard Wiener process. Following Jo et al. (2022), we adopt a similar approach where architectures are regarded as entities embedded in a continuous space. Subsequently, during the forward diffusion process, the architecture is perturbed with Gaussian noise at each step.\nReverse process The reverse-time diffusion process corresponding to the forward process is modeled by the following SDE (Anderson, 1982; Song et al., 2021b):\ndAt = [ ft(At)\u2212 g2t\u2207At log pt(At) ] dt+ gtdw\u0304, (2)\nwhere pt denotes the marginal distribution under the forward diffusion process, dt represents an infinitesimal negative time step and w\u0304 is the reverse-time standard Wiener process.\nIn order to use the reverse process as a generative model, the score network s\u03b8 is trained to approximate the score function \u2207At log pt(At) with the following score matching (Hyv\u00e4rinen, 2005; Song et al., 2021a) objective, where \u03bb(t) is a given positive weighting function:\n\u03b8\u2217 = argmin \u03b8\nEt { \u03bb(t)EA0EAt|A0 \u2225s\u03b8(At, t)\u2212\u2207At log pt(At)\u2225 2 2 } . (3)\nOnce the score network has been trained, we can generate neural architectures that follow the original distribution p(A0) using the reverse process of Equation (2). To be specific, we start from noise sampled from the known prior distribution and simulate the reverse process, where the score function is approximated by the score network s\u03b8\u2217(At, t). Following various continuous graph diffusion models (Niu et al., 2020a; Jo et al., 2022), we discretize the entries of the architecture matrices using the operator 1>0.5 to obtain discrete 0-1 matrices after generating samples by simulating the reverse diffusion process. Empirically, we observed that the entries of the generated samples after simulating the diffusion process do not significantly deviate from integer values of 0 and 1.\nScore Network for Neural Architectures To generate valid neural architectures, the score network should capture 1) the dependency between nodes, reflecting the computational flow (Dong & Yang, 2020a; Zhang et al., 2019), and 2) the accurate position of each layer within the overall architecture to comply with the rules of a specific search space. Inspired by Yan et al. (2021a) on architecture encoding, we use L transformer blocks (T) with an attention mask M \u2208 RN\u00d7N that indicates the dependency between nodes, i.e., an upper triangular matrix of DAG representation (Dong & Yang, 2020a; Zhang et al., 2019), to parameterize the score network. (See Appendix B for more detailed descriptions) Furthermore, we introduce positional embedding Embpos(vi) to more accurately capture the topological ordering of layers in architectures, which leads to the generation of valid architectures adhering to specific rules within the given search space as follows:\nEmbi = Embops (vi) +Embpos (vi) +Embtime (t) , where vi : i-th row of V for i \u2208 [N ], (4)\ns\u03b8 (At, t) = MLP (HL) , where H0i = Embi,H l = T ( H l\u22121,M ) and H l = [H l1 \u00b7 \u00b7 \u00b7H lN ], (5)\nwhere, Embops(vi) and Embtime(t) are embeddings of each node (operation) vi and time t, respectively.\nWhile simulating Equation (2) backward in time can generate random architectures within the entire search space, random generation is insufficient for the main goal of DiffusionNAG. Therefore, we introduce a conditional NAG framework to achieve this goal in the following section."
        },
        {
            "heading": "2.2 CONDITIONAL NEURAL ARCHITECTURE GENERATION",
            "text": "Inspired by the parameterized model-guidance scheme (Sohl-Dickstein et al., 2015; Vignac et al., 2022; Dhariwal & Nichol, 2021), we incorporate a parameterized predictor in our framework to actively guide the generation toward architectures that satisfy specific objectives. Let y be the desired property (e.g., high accuracy or robustness against attacks) we want the neural architectures to satisfy. Then, we include the information of y into the score function. To be specific, we generate neural architectures from the conditional distribution pt(At|y) by solving the following conditional reverse-time SDE (Song et al., 2021b):\ndAt = [ ft(At)\u2212 g2t\u2207At log pt(At|y) ] dt+ gtdw\u0304. (6)\nHere, we can decompose the conditional score function\u2207At log pt(At|y) in Equation (6) as the sum of two gradients that is derived from the Bayes\u2019 theorem p(At|y) \u221d p(At) p(y|At):\n\u2207At log pt(At|y) = \u2207At log pt(At) +\u2207At log pt(y|At). (7)\nBy approximating the score function \u2207At log pt(At) with the score network s\u03b8\u2217 , the conditional generative process of Equation (6) can be simulated if the term\u2207At log pt(y|At) could be estimated. Since log pt(y|At) represents the log-likelihood that the neural architecture At satisfies the target property y, we propose to model log pt(y|At) using a pre-trained predictor f\u03d5(y|At) parameterized by \u03d5, which predicts the desired property y given a perturbed neural architecture At:\n\u2207At log pt(y|At) \u2248 \u2207At log f\u03d5(y|At). (8)\nAs a result, we construct the guidance scheme with the predictor as follows, where kt is a constant that determines the scale of the guidance of the predictor:\ndAt = { ft(At)\u2212 g2t [ s\u03b8\u2217(At, t) + kt\u2207At log f\u03d5(y|At) ]} dt+ gtdw\u0304. (9)\nIntuitively, the predictor guides the generative process by modifying the unconditional score function which is estimated by s\u03b8\u2217 at each sampling step. The key advantage of this framework is that we only need to train the score network once and can generate architectures with various target properties by simply changing the predictor. Our approach can reduce significant computational overhead for the conditional NAG compared to the classifier-free guidance scheme (Hoogeboom et al., 2022) that requires retraining the diffusion model every time the conditioning properties change."
        },
        {
            "heading": "2.3 TRANSFERABLE CONDITIONAL NEURAL ARCHITECTURE GENERATION",
            "text": "Transferable NAS (Lee et al., 2021a; Shala et al., 2023) offers practical NAS capabilities for diverse real-world tasks, by simulating human learning. They acquire a knowledge from past NAS tasks to improve search performance on new tasks. In this section, to achieve highly efficient Transferable NAS, we extend the conditional NAG framework discussed earlier into a diffusion-based transferable\nNAG method by combining our framework with the transferable dataset-aware predictors from Transferable NAS methods (Lee et al., 2021a; Shala et al., 2023). A dataset-aware predictor f\u03d5(D,At) is conditioned on a dataset D. In other words, even for the same architecture, if datasets are different, the predictor can predict accuracy differently. The predictor is meta-learned with Equation (21) over the task distribution p(T ) utilizing a meta-dataset S := {(A(i), yi,Di)}Ki=1 with K tasks consisting of (dataset, architecture, accuracy) triplets for each task. We use the meta-dataset collected by Lee et al. (2021a). The key advantage is that by exploiting the knowledge learned from the task distribution, we can conduct fast and accurate predictions for unseen datasets without additional predictor training. We integrate the meta-learned dataset-aware predictor f\u03d5(D,At) into the conditional neural architecture generative process (Equation (9)) for an unseen dataset D\u0303 as follows:\ndAt = { ft(At)\u2212 g2t [ s\u03b8\u2217(At, t) + kt\u2207At log f\u03d5(y|D\u0303,At) ]} dt+ gtdw\u0304. (10)"
        },
        {
            "heading": "3 EXPERIMENT",
            "text": "We validate the effectiveness of DiffusionNAG on two predictor-based NAS scenarios: Transferable NAS (Section 3.1) and BO-based NAS (Section 3.2). In Section 3.3, we demonstrate the effectiveness of the proposed score network. In Section 3.4, we analyze DiffusionNAG\u2019s adaptation ability. Search Space We validate our framework on two Transferable NAS benchmark search spaces (Lee et al., 2021a): MobileNetV3 (MBv3) (Cai et al., 2020) and NAS-Bench-201 (NB201) (Dong & Yang, 2020b). Especially, MBv3 is a large search space, with approximately 1019 architectures. (Please see Appendix C.1 for detailed explanations.) Training Score Network The score network is trained only once for all experiments conducted within each search space. Note that training the score network only requires architectures (graph) without the need for accuracy which is expensive information. The training process required 21.33 GPU hours (MBv3) and 3.43 GPU hours (NB201) on Tesla V100-SXM2, respectively."
        },
        {
            "heading": "3.1 COMPARISON WITH TRANSFERABLE NAS METHODS",
            "text": "Experimental Setup Transferable NAS methods (Shala et al., 2023; Lee et al., 2021a) are designed to leverage prior knowledge learned from previous NAS tasks, making NAS more practical on an unseen task. To achieve this, all Transferable NAS methods, including our DiffusionNAG, utilize a transferable dataset-aware accuracy predictor, as described in Section 2.3. The dataset-aware predictor is meta-trained on the meta-dataset provided by Lee et al. (2021a), which consists of 153,408/4,230 meta-training tasks for MBv3/NB201, respectively. For more details, please refer to Lee et al. (2021a). MetaD2A (Lee et al., 2021a), which is the most closely related to our work, includes an unconditional architecture generative model that explicitly excludes the dataset-aware predictor during the generation process. Instead, MetaD2A needs to search for optimal architectures across multiple tasks, train these architectures to obtain their accuracy data, and use this costly accuracy collection to train its generative model. Besides, it uses the dataset-aware predictor only during the subsequent evaluation stage to rank the generated architectures. During the test phase, it first objective-unconditionally generates 500 candidate architectures and then evaluates the top\nDiffusionNAG (Ours) 94.37\u00b10.00 5 73.51\u00b10.00 5 59.63\u00b10.92 2 41.32\u00b10.84 2 \u2217 We report the search time of one-shot NAS methods in Appendix C.3.\nFigure 2: The distribution of generated architectures.\narchitectures using its predictor. TNAS (Shala et al., 2023) enhances the meta-learned datasetaware predictor\u2019s adaptability to unseen datasets by utilizing BO with the deep-kernel GP strategy without involving any generation process (Please see Appendix C.2 for details of the baselines.). DiffusionNAG conditionally generates 500 architectures with the diffusion model guided by the dataset-aware predictor. Our generation process takes up to 4.35 GPU minutes on Tesla V100-SXM2. Finally, we select the top architectures sorted by the predictor among the generated candidates. We conduct experiments on Transferable NAS benchmarks (Lee et al., 2021a) such as four unseen datasets - CIFAR-10, CIFAR-100, Aircraft (Maji et al., 2013), and Oxford IIT Pets (Parkhi et al., 2012) from large search space MBv3 (Table 1) and, NB201 (Table 2).\nResults on MBv3 Search Space In Table 1, MetaD2A, TNAS, and DiffusionNAG obtain the top 30 neural architectures for each datasets, following the descriptions in the Experimental Setup section. Subsequently, we train these architectures on the datasets following the training pipeline described in Appendix C.5. Once the architectures are trained, we analyze the accuracy statistics for each method\u2019s group of architectures. Additionally, we calculate p-value to assess the statistical significance of performance differences between the architecture groups obtained via DiffusionNAG and each method. A p-value of 0.05 or lower denotes that a statistically meaningful difference exists in the performances of the generated architectures between the two groups.\nThe results demonstrate that, except for the Aircraft dataset, DiffusionNAG consistently provides architectures with superior maximum accuracy (max) compared to other methods across three datasets. Additionally, the mean accuracy and minimum accuracy (min) of architectures within the DiffusionNAG group are higher across all datasets. In particular, the p-values obtained from comparing the groups of architectures suggested by DiffusionNAG and those from other baselines are consistently below the 0.05 threshold across all datasets. This indicates that the architectures generated by DiffusionNAG have shown statistically significant performance improvements compared to those provided by the baseline methods when using transferable dataset-aware predictors. Furthermore, the results clearly support the superiority of the proposed predictor-guided conditional architecture generation method compared with either excluding predictors during generation (MetaD2A) or relying solely on predictors without generating architectures (TNAS).\nResults on NB201 Search Space We highlight two key aspects from the results of Table 2. Firstly, the architectures generated by DiffusionNAG attain oracle accuracies of 94.37% and 73.51% on CIFAR-10 and CIFAR-100 datasets, respectively, and outperform architectures obtained by the\nbaseline methods on Aircraft and Oxford-IIIT Pets datasets. While MetaD2A and TNAS achieve accuracies of 59.15%/57.71% and 40.00%/39.04% on Aircraft and Oxford-IIIT Pets datasets, respectively, DiffusionNAG gains even better accuracies of 59.63% and 41.32%, demonstrating its superiority. Secondly, DiffusionNAG significantly improves the search efficiency by minimizing the number of architectures that require full training (Trained Archs) to obtain a final accuracy (For CIFAR-10 and CIFAR-100, an accuracy is retrieved from NB201 benchmarks) compared to all baselines. Specifically, when considering the Aircraft and Oxford-IIIT Pets datasets, DiffusionNAG only needs to train 2 architectures for each dataset to complete the search process while MetaD2A and TNAS require 40/26 and 6/40 architectures, respectively. This results in a remarkable speedup of at least 8.4\u00d7 and up to 20\u00d7 on average. Further Anaylsis We further analyze the accuracy statistics of the distribution of architectures generated by each method within the NB201 search space. Specifically, we conduct an in-depth study by generating 1,000 architectures using each method and analyzing their distribution, as presented in Table 3 and Figure 2. We compare DiffusionNAG with two other methods: random architecture sampling (Random) and MetaD2A. Additionally, to assess the advantage of using a predictor in both the NAG and evaluation phases compared to an approach where the predictor is solely used in the evaluation phase, we unconditionally generate 10,000 architectures and then employ the predictor to select the top 1,000 architectures (Uncond. + Sorting). DiffusionNAG (Cond.) leverages the dataset-aware predictor f\u03d5(D,At) to guide the generation process following Equation (10).\nThe results from Table 3 and Figure 2 highlight three key advantages of our model over the baselines. Firstly, our model generates a higher proportion of high-performing architectures for each target dataset, closely following the Oracle Top-1000 distribution within the search space. Secondly, our model avoids generating extremely low-accuracy architectures, unlike the baseline methods, which generate architectures with only 10% accuracy. This suggests that our model is capable of focusing on a target architecture distribution by excluding underperforming architectures. Lastly, as shown in Figure 2, DiffusionNAG (Cond.) outperforms sorting after the unconditional NAG process (Uncond. + Sorting). These results highlight the value of involving the predictor not only in the evaluation phase but also in the NAG process, emphasizing the necessity of our conditional NAG framework."
        },
        {
            "heading": "3.2 IMPROVING EXISTING BAYESIAN OPTIMIZATION-BASED NAS",
            "text": "In this section, we have demonstrated that DiffusionNAG significantly outperforms existing heuristic architecture sampling techniques used in Bayesian Optimization (BO)-based NAS approaches, leading to improved search performance in BO-based NAS. BO-based NAS The typical BO algorithm for NAS (White et al., 2023) is as follows: 1) Start with an initial population containing neural architecture-accuracy pairs by uniformly sampling n0\narchitectures and obtaining their accuracy. 2) Train a predictor using architecture-accuracy pairs in the population, and 3) Sample c candidate architectures by the Acquisition Optimization strategy (AO strategy) (White et al., 2021a) and choose the one maximizing an acquisition function based on the predictions of the predictor. 4) Evaluate the accuracy of the selected architecture after training it and add the pair of the chosen architecture and its obtained accuracy to the population. 5) Repeat steps 2) to 4) during N iterations, and finally, select the architecture with the highest accuracy from the population as the search result. (For more details, refer to Algorithm 1 in the Appendix.)\nOur primary focus is on replacing the existing AO strategy in step 3) with DiffusionNAG to improve the search efficiency of BO-based NAS approaches. Baseline AO Strategy: The simplest AO strategy is randomly sampling architecture candidates (Random). Another representative AO strategy is Mutation, where we randomly modify one operation in the architecture with the highest accuracy in the population. Mutation + Random combines two aforementioned approaches. Guided Gen (Ours): Instead of relying on these heuristic strategies, we utilize DiffusionNAG to generate the candidate architectures. Specifically, we train a predictor f\u03d5(y|At), as described in Equation (8), using architecture-accuracy pairs in the population. The trained predictor guides the generation process of our diffusion model to generate architectures. We then provide these generated architecture candidates to the acquisition function in step 3) (See Algorithm 2 in the Appendix.)\nComparison Results with Existing AO Strategies The left and middle sides in Figure 3 illustrates our comparison results with existing AO strategies. These results clearly highlight the effectiveness of DiffusionNAG (Guided Gen (Ours)), as it significantly outperforms existing AO strategies such as Random, Mutation, and Mutation + Random on the CIFAR100 dataset from NB201 and the large-scale ImageNet 1K (Deng et al., 2009) dataset within the extensive MBv3 search space. In particular, BO-based NAS methods employing Random or Mutation strategies often suffer from the issue of wasting time on sampling low-quality architectures during the initial phase (White et al., 2020; Zela et al., 2022). In contrast, DiffusionNAG effectively addresses this issue by offering relatively high-performing architectures right from the start, resulting in a significant reduction in search times. As a result, as shown in the right side of Figure 3, our approach outperforms existing BO-based NAS methods, by effectively addressing the search cost challenge of them.\nExperimental Results on Various Acquisition Functions In addition to the Probability of Improvement (PI) used in Figure 3, we investigate the benefits of DiffusionNAG across various acquisition functions, such as Expected Improvement (EI), Independent Thompson sampling (ITS), and Upper Confidence Bound (UCB) as shown in Figure 4. (Please see Appendix D.2 for more details on acquisition functions.). The experimental results verify that DiffusionNAG (Ours) consistently outperforms heuristic approaches, including Mutation, Random, and Mutation + Random approaches, on four acquisition functions: PI, EI, ITS, and UCB, in the large MBv3 search space."
        },
        {
            "heading": "3.3 THE EFFECTIVENESS OF SCORE NETWORK FOR NEURAL ARCHITECTURES",
            "text": "In this section, we validate the ability of the proposed score network to generate architectures that follow the distribution of NB201 and MBv3 search spaces. For NB201, we construct the training set by randomly selecting 50% of the architectures from the search space, while for MBv3, we randomly sample 500,000 architectures. We evaluate generated architectures using three metrics (Zhang et al., 2019): Validity, Uniqueness, and Novelty. Validity measures the proportion of valid architectures generated by the model, Uniqueness quantifies the proportion of unique architectures among the valid ones, and Novelty indicates the proportion of valid architectures that are not present in the training set. As shown in Table 4, our score network generates valid architectures with 100% Validity, whereas GDSS (Jo et al., 2022), a state-of-the-art graph diffusion model designed for undirected graphs, fails to generate valid architectures, with the validity of only 4.56% and 0.00% for NB201 and MBv3, respectively. Furthermore, our positional embedding yields significant improvements, indicating that it successfully captures the topological ordering of nodes within the architectures.\nNotably, in the MBv3, Validity improves from 42.17% to 100.00%, highlighting the necessity of positional embedding for generating architectures with a large number of nodes (a.k.a. \"long-range\"). Additionally, our framework generates 49.20%/100.00% novel architectures that are not found in the training set, as well as unique architectures 98.70%/100.00% for NB201 and MBv3, respectively."
        },
        {
            "heading": "3.4 ADAPTATION ACROSS DIFFERENT OBJECTIVES",
            "text": "In Table 5, we demonstrate that DiffusionNAG easily adapts to new tasks by swapping the task-specific predictor in a plug-and-play manner without retraining the score network. We train three predictors with different objectives: Clean accuracy, robust accuracy against an adversarial attack such as APGD (Croce & Hein, 2020) with a perturbation magnitude of \u03f5 = 2, and robust accuracy against glass Blur corruption. For each objective, we randomly select 50 neural architectures from the NB201 and collect their corresponding test accuracy data on CIFAR10 as training data (i.e., (architecture, corresponding\ntest accuracy) pairs). Subsequently, we train each predictor using their respective training data, denoted as f\u03d5Clean , f\u03d5APGD , and f\u03d5Blur . With the guidance of these predictors, we generate a pool of 20 architectures for each. For each generated architecture pool, we provide statistics of all objectives (i.e., test accuracies), including robust accuracies under the APGD attack and glass Blur corruption and clean accuracy from the NB201-based robustness benchmark (Jung et al., 2023).\nThe maximum (max) and mean accuracies of the pool of architectures generated by using the predictor trained on the target objective are higher than those obtained by using predictors trained with other objectives. For example, architectures generated with the guidance of f\u03d5Clean have poor robust performance on the APGD attack and Blur corruption, with a max accuracy of 2.6% and 37.10% for each. In contrast, the guidance of f\u03d5APGD and f\u03d5Blur yield better architectures with a max accuracy of 26.60% and 56.90% on the APGD attack and Blur corruption, respectively, demonstrating the superior adaptation ability of DiffusionNAG across various objectives."
        },
        {
            "heading": "4 RELATED WORK",
            "text": "Neural Architecture Search NAS is an automated architecture search process (Ning et al., 2021; Zoph & Le, 2017) and roughly can be categorized into reinforcement learning-based (Zoph & Le, 2017; Zoph et al., 2018; Pham et al., 2018), evolutionary algorithm-based (Real et al., 2019; Lu et al., 2020), and gradient-based methods (Luo et al., 2018; Liu et al., 2019; Dong & Yang, 2019b; Xu et al., 2020; Chen et al., 2021). Recently, Shala et al. (2023); Lee et al. (2021a) have proposed Transferable NAS to rapidly adapt to unseen tasks by leveraging prior knowledge. However, they still suffer from the high search cost. DiffusionNAG addresses these limitations by generating architectures satisfying the objective with a guidance scheme of a meta-learned dataset-aware predictor. Diffusion Models Diffusion models, as demonstrated in prior work (Song & Ermon, 2019; Ho et al., 2020; Song et al., 2021b), are designed to reverse the data perturbation process, enabling them to generate samples from noisy data. They have achieved success in a variety of domains, including images (Nichol et al., 2022; Rombach et al., 2022), audio (Jeong et al., 2021; Kong et al., 2021), and graphs (Niu et al., 2020b; Jo et al., 2022). However, existing diffusion models are not well-suited for Neural Architecture Generation (NAG) because their primary focus is on unconditionally generating undirected graphs. To overcome this limitation, this study introduces a conditional diffusion-based generative framework tailored for generating architectures represented as directed acyclic graphs that meet specified conditions, such as accuracy requirements."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "This study introduced a novel approach called DiffusionNAG, which is the paradigm shift from existing NAS methods. With the guidance of a predictor for a given task, DiffusionNAG can efficiently generate task-optimal architectures. Additionally, the introduction of a score network ensures the generation of valid architectures. Extensive experiments demonstrated that DiffusionNAG outperforms existing NAS, especially effective in the extensive search space. For reproducibility, we include \u201cNAS Best Practices Checklist\u201d in Appendix A and the code in the supplementary file."
        },
        {
            "heading": "A NAS BEST PRACTICE CHECKLIST",
            "text": "In this section, we provide a description of how we cover each point in the NAS Best Practice Checklist (Lindauer & Hutter, 2020).\n1. Best Practices for Releasing Code For all experiments you report:\n(a) Did you release the code for the training pipeline used to evaluate the final architectures? We released the code for the training pipeline along with the supplementary file.\n(b) Did you release the code for the search space? In our main experiments, we utilized the NAS-Bench-201 search space, which is publicly available along with its description and code.\n(c) Did you release the hyperparameters used for the final evaluation pipeline, as well as random seeds? We use the NAS-Bench-201 pipeline and hyperparameters as our final evaluation pipeline. It can be found in the released code.\n(d) Did you release code for your NAS method? The code for our NAS method can be found in the supplementary file.\n(e) Did you release hyperparameters for your NAS method, as well as random seeds? Yes, it can be found in the code we provide.\n2. Best practices for comparing NAS methods\n(a) For all NAS methods you compare, did you use exactly the same NAS benchmark, including the same dataset (with the same training-test split), search space and code for training the architectures and hyperparameters for that code? Yes, for a fair comparison, we used the same evaluation pipeline for all NAS methods we compare.\n(b) Did you control for confounding factors (different hardware, versions of DL libraries, different runtimes for the different methods)? To control for confounding factors, we ensured that all methods were executed on the same hardware and environment.\n(c) Did you run ablation studies? Yes. (d) Did you use the same evaluation protocol for the methods being compared? Yes. (e) Did you compare performance over time? Yes. (f) Did you compare to random search? Performance comparison to random search and\nother baselines can be found in Table 2 of the main paper. (g) Did you perform multiple runs of your experiments and report seeds? For each of the\nexperiments we performed three runs with different seeds (777,888,999). (h) Did you use tabular or surrogate benchmarks for in-depth evaluations? We used\nNAS-Bench-201 (Dong & Yang, 2020b) as a tabular benchmark.\n3. Best practices for reporting important details\n(a) Did you report how you tuned hyperparameters, and what time and resources this required? Yes.\n(b) Did you report the time for the entire end-to-end NAS method (rather than, e.g., only for the search phase)? Yes.\n(c) Did you report all the details of your experimental setup? The details of our experimental setup can be found in Appendix C."
        },
        {
            "heading": "B DETAILS OF DIFFUSIONNAG",
            "text": ""
        },
        {
            "heading": "B.1 OVERVIEW",
            "text": "In this section, we first describe the forward diffusion process that perturbs neural architectures towards random graphs in Appendix B.2. Subsequently, we illustrate our specialized score network for estimating neural architecture scores in Appendix B.3. Finally, we elaborate the components of the dataset-aware predictor in DiffusionNAG in Appendix B.4."
        },
        {
            "heading": "B.2 DIFFUSION PROCESS",
            "text": "Our approach aims to apply continuous-time generative diffusion processes to neural architectures. Specifically, we utilize the Variance Exploding (VE) SDE (Song et al., 2021b) to model the forward diffusion process for our neural architecture generation framework. The following SDE describes the process of the VE SDE (Jo et al., 2022):\ndA = \u03c3min ( \u03c3max \u03c3min )t\u221a 2 log \u03c3max \u03c3min dw, (11)\nwhere \u03c3min, \u03c3max, and t \u2208 (0,1] are hyperparameters. The transition distribution of the process is Gaussian, given that Equation (11) has a linear drift coefficient. We can obtain the mean and covariance using the result of Equation (5.50) and (5.51) of S\u00e4rkk\u00e4 & Solin (2019) as follows:\npt|0(At|A0) = N ( At; A0 , \u03c3 2 min ( \u03c3max \u03c3min )2t I ) . (12)\nDuring the forward diffusion process, Gaussian noise is applied to perturb the architecture, following the noise scheduling modeled by the prescribed SDE. In order to employ the reverse process, which corresponds to the forward process, as a generative model, the score network s\u03b8 is trained to approximate the score function\u2207At log pt(At)."
        },
        {
            "heading": "B.3 SCORE NETWORK FOR NEURAL ARCHITECTURES",
            "text": "As proposed in Section 2.1 of the main paper, we introduce a score network specifically designed for neural architectures. The distinctive aspect of our score network is the incorporation of positional embedding for nodes within directed acyclic graphs (DAGs), which allows for capturing the topological ordering of neural architectures. The input embedding and computation of this score network are as follows:\nEmbi = Embops (vi) +Embpos (vi) +Embtime (t) , where vi : i-th row of V for i \u2208 [N ], (13)\ns\u03b8 (At, t) = MLP (HL) , where H0i = Embi,H l = T ( H l\u22121,M ) and H l = [H l1 \u00b7 \u00b7 \u00b7H lN ]\u22a4. (14)\nHere, we denote Embops (vi) as the embedding of each node (operation) vi, and Embtime (vi) as the embedding of time t. Additionally, we introduce the positional embedding Embpos (vi) to incorporate positional information of the node into the input embeddings. Inspired by the previous work (Yan et al., 2021a), we employ L transformer blocks (T) (Vaswani et al., 2017) to parameterize the score network. These blocks utilize an attention mask M \u2208 RN\u00d7N , where N represents the number of nodes in the architecture. In our approach, a pair of nodes (operations) in the architecture is considered dependent if a directed edge is connected. The attention mask M in the transformer block is determined by the value of the adjacency matrix E of a neural architecture A following the equation:\nMi,j = { 0, if Ei,j = 1 \u2212109, if Ei,j = 0\n(15)\nEach Transformer block (T) consists of nhead attention heads. The computation of the l-th Transformer block is described as follows:\nQp =H l\u22121W lqp,Kp = H l\u22121W lkp,Vp = H l\u22121W lvp (16)\nH\u0302 lp = softmax\n( QpK\n\u22a4 p\u221a\ndh +M\n) Vp (17)\nH\u0302 l = concatenate ( H\u0302 l1, H\u0302 l 2, . . . , H\u0302 l nhead ) (18)\nH l = ReLU ( H\u0302 lW l1 + b l 1 ) W l2 + b l 2 (19)\nwhere the size of H l and H\u0302 lp is N \u00d7 dh and N \u00d7 (dh/nhead), respectively. Additionally, in the attention operation of the p-th head, we denote Qp, Kp, and Vp as the \u201cQuery\u201d, \u201cKey\u201d, and \u201cValue\u201d matrices, respectively. Moreover, we utilize W l1 and W l 2 to represent the weights in the feed-forward layer."
        },
        {
            "heading": "B.4 DATASET-AWARE PREDICTOR IN DIFFUSIONNAG",
            "text": "To ensure that the predictor is dataset-aware, we extend its design by incorporating not only the architecture encoder but also a dataset encoder. In this section, we provide a detailed explanation of this dataset-aware predictor configuration utilized in Section 3.1 of the main paper.\nNeural Architecture Encoding For encoding neural architectures, we employ the DiGCN layer (Wen et al., 2020), which is a modified version of Graph Convolutional Networks (GCNs) specifically designed for directed graphs. This layer is specifically designed to effectively capture structural information and dependencies within the architecture. To encode the architecture A, represented as (V ,E) \u2208 RN\u00d7F \u00d7 RN\u00d7N , we first obtain the normalized adjacency matrix E\u0302 by adding an identity matrix (representing the self-cycles) to the original matrix E and normalizing it. Subsequently, to enable bidirectional information flow, we utilize two GCN layers. One GCN layer utilizes the normalized adjacency matrix E\u0302 to propagate information in the \"forward\" direction, while the other GCN layer uses E\u0302 \u22a4 to propagate information in the \"reverse\" direction. The outputs from these two GCN layers are then averaged. In the l-th layer of the architecture encoding module, the computation is performed as follows:\nH l = 1\n2 ReLU\n( E\u0302H l\u22121W l\u22121+ ) + 1\n2 ReLU\n( E\u0302 \u22a4 H l\u22121W l\u22121\u2212 ) , (20)\nwhere H0 = V and W l\u22121+ and W l\u22121\u2212 are trainable weight matrices. By stacking multiple layers, we can effectively learn high-quality representations of directed graphs. After applying the graph convolutional layers, we utilize average pooling on the architecture representations obtained from the final layer to aggregate information across the graph. Finally, one or more fully connected layers can be attached to obtain the latent vector zA of the neural architecture A.\nDataset Encoding Following Lee et al. (2021a), we employ a dataset encoder based on Set Transformer (Lee et al., 2019) to accurately capture the characteristics of the target dataset. This dataset encoder is specifically designed to process input sets of varying sizes and output a fixed size latent code, denoted as zD, that effectively encapsulates the information within the dataset. The dataset encoder consists of two stacked modules that are permutation-invariant, meaning output of the dataset encoder is always the same regardless order of elements in the input set. Additionally, these modules leverage attention-based learnable parameters. The lower-level module, known as the \u201cintra-class\u201d encoder, captures class prototypes that represent each class information present in the dataset. On the other hand, the higher-level module, referred to as the \u201cnter-class\u201d encoder, considers the relationships between these class prototypes and aggregates them into a latent vector zD. The utilization of this hierarchical dataset encoder enables us to effectively model high-order interactions among the instances within the dataset, thereby allowing us to capture and summarize crucial information about the dataset.\nUltimately, the encoded representation of the dataset zD is combined with the encoded architecture representation zA through MLP layers. This integration enables the predictor to make conditional predictions by considering both the architecture and the dataset information."
        },
        {
            "heading": "B.5 TRANSFERABLE TASK-GUIDED NEURAL ARCHITECTURE GENERATION",
            "text": "We aim to design a diffusion-based transferable NAG method that can generate optimal neural architectures for unseen datasets. In order to achieve this, the surrogate model needs to be conditioned on a dataset D (a.k.a. \"dataset-aware\") (Lee et al., 2021a; Shala et al., 2023). For the surrogate model to be dataset-aware, we train it over the task distribution p(T ) utilizing a meta-dataset S consisting of (dataset, architecture, accuracy) triplets. To be precise, we define the meta-dataset as S := {(A(i), yi,Di)}Ki=1 consisting of K tasks, where each task is created by randomly sampling architecture A(i) along with their corresponding accuracy yi on the dataset Di. We train the datasetaware accuracy surrogate model using S by minimizing the following MSE loss function, L:\n\u03d5\u2217 \u2208 argmin \u03d5 K\u2211 i=1 L(yi, f\u03d5(Di,A(i))). (21)\nAn important aspect is that this surrogate model only requires a one-time training phase since it is trained over the task distribution. Thereby, the surrogate model can make accurate predictions even for unseen datasets. After one-time training, we utilize this meta-learned dataset-aware accuracy surrogate model f\u03d5\u2217(D,At) as a guiding surrogate. To be precise, we integrate the dataset-aware surrogate model f\u03d5\u2217(D,At) trained with the objective function (Equation (21)) into the conditional generative process (Equation (9)), which enables us to control the neural architecture generation process for an unseen task, including unseen datasets with the following generation process:\ndAt = { ft(At)\u2212 g2t [ s\u03b8\u2217(At, t) + kt\u2207At log f\u03d5\u2217(y|D,At) ]} dt+ gtdw\u0304. (22)"
        },
        {
            "heading": "C EXPERIMENTAL DETAILS",
            "text": ""
        },
        {
            "heading": "C.1 SEARCH SPACE",
            "text": "As explained in Section 2 of the main paper, the architecture A consisting of N nodes is defined by its operator type matrix V \u2208 RN\u00d7F and upper triangular adjacency matrix E \u2208 RN\u00d7N . Thus, A can be represented as (V ,E) \u2208 RN\u00d7F \u00d7 RN\u00d7N , where F denotes the number of predefined operator sets. Detailed statistics, including the number of nodes (N ) and the number of operation types (F ) for each search space utilized in our experiments (excluding Section 3.4), are provided in Table 6. In Section 3.4, the only difference is that we train the score network using a subset of 7,812 architectures randomly selected from the NAS-Bench-201 search space. This subset is selected due to utilizing the Novelty metric as part of our evaluation process. Further elaboration on the search space is presented below.\nNAS-Bench-201 search space consists of cell-based neural architectures represented by directed acyclic graphs (DAGs). Each cell in this search space originally consists of 4 nodes and 6 edges. The edges offer 5 operation candidates, including zeroize, skip connection, 1-by-1 convolution, 3-by-3 convolution, and 3-by-3 average pooling. Consequently, there are a total of 15,626 possible architectures within this search space. To utilize the NAS-Bench201 search space, we convert the original cell representation into a new graph representation. In the new graph, each node represents an operation (layer), and the edges represent layer connections. After the conversion, each cell contains eight nodes and seven operation types for each node. An additional input and output operation are added to the existing set of 5 operations. To represent the neural architectures in the NAS-Bench-201 search space, we adopt an 8\u00d77 operator type matrix (V) and an 8\u00d78 upper triangle adjacency matrix (E). Furthermore, the macro skeleton is constructed by stacking various components, including one stem cell, three stages consisting of 5 repeated cells each, residual blocks (He et al., 2016) positioned between the stages, and a final classification\nlayer. The final classification layer consists of an average pooling layer and a fully connected layer with a softmax function. The stem cell, which serves as the initial building block, comprises a 3-by-3 convolution with 16 output channels, followed by a batch normalization layer. Each cell within the three stages has a varying number of output channels: 16, 32, and 64, respectively. The intermediate residual blocks feature convolution layers with a stride of 2, enabling down-sampling.\nMobileNetV3 search space is designed as a layer-wise space, where each building block incorporates MBConvs, squeeze and excitation Hu et al. (2018), and modified swish nonlinearity to create a more efficient neural network. The search space consists of 5 stages, and within each stage, the number of building blocks varies from 2 to 4. As a result, the maximum number of layers possible in MobileNetV3 is calculated as 5\u00d7 4 = 20. For each building block, the kernel size can be chosen from the set {3, 5, 7}, and the expansion ratio can be chosen from {3, 4, 6}. This implies that there are a total of 3\u00d7 3 = 9 possible operation types available for each layer. To represent the neural architectures in the MobileNetV3 search space, we utilize a 20\u00d79 operator type matrix (V) and a 20\u00d720 upper triangle adjacency matrix (E). Furthermore, the search space contains approximately 1019 architectures, reflecting the extensive range of choices available."
        },
        {
            "heading": "C.2 BASELINES",
            "text": "In this section, we provide the description of various Neural Architecture Search (NAS) baselines in Section 3.1 and of the main paper.\nBasic approach We begin by discussing the basic NAS baseline approaches. ResNet (He et al., 2016) is a popular deep learning architecture commonly used in computer vision tasks. Following Lee et al. (2021a), we report results obtained from ResNet56. Random Search (RS, Bergstra & Bengio (2012)) is a basic NAS approach that involves randomly sampling architectures from the search space and selecting the top-performing one. Another basic NAS method we consider is REA (Real et al., 2019). REA utilizes evolutionary aging with tournament selection as its search strategy. REINFORCE (Williams, 1992) is a reinforcement learning based NAS method. In this approach, the validation accuracy obtained after 12 training epochs is used as the reward signal.\nOne-shot NAS We also compare our method with various one-shot methods, which have shown strong empirical performance for NAS. SETN (Dong & Yang, 2019a) focuses on selectively sampling competitive child candidates by training a model to evaluate the quality of these candidates based on their validation loss. GDAS (Dong & Yang, 2019b) is a differentiable neural architecture sampler that utilizes the Gumbel-Softmax relaxation technique. GDAS sampler is designed to be trainable and optimized based on the validation loss after training the sampled architecture. This allows the sampler to be trained in an end-to-end fashion using gradient descent. PC-DARTS (Xu et al., 2020) is a gradient-based NAS method that improves efficiency by partially sampling channels. By reducing redundancy and incorporating edge normalization, PC-DARTS enables more efficient architecture search. DrNAS (Chen et al., 2021) is a differentiable architecture search method that formulates the search as a distribution learning problem, using Dirichlet distribution to model the architecture mixing weight. By optimizing the Dirichlet parameters with gradient-based optimization and introducing a progressive learning scheme to reduce memory consumption, DrNAS achieves improved generalization ability.\nBayesian Optimization based NAS A variety of Bayesian Optimization (BO) methods have been proposed for NAS, and in our study, we conduct a comprehensive comparison of these methods. One approach involves using a vanilla Gaussian Process (GP) surrogate (Snoek et al., 2012), where the Upper Confidence Bound (UCB) acquisition function is employed (GP-UCB). Additionally, we evaluate our method against other approaches such as HEBO (Cowen-Rivers et al., 2022), which is a black-box Hyperparameter Optimization (HPO) method that addresses challenges like heteroscedasticity and non-stationarity through non-linear input and output warping and robust acquisition maximizers. Another BO-based NAS method, BANANAS (White et al., 2021a), utilizes an ensemble of fully-connected neural networks as a surrogate and employs path encoding for neural architectures. BANANAS initiates a new architecture search for each task. On the other hand, NASBOWL (Ru et al., 2021) combines the Weisfeiler-Lehman graph kernel with a Gaussian process surrogate, allowing efficient exploration of high-dimensional search spaces.\nTransferNAS and TransferNAG The key distinction between TransferNAS(NAG) methods and conventional NAS methods lies in their ability to leverage prior knowledge for rapid adaptation to unseen datasets. Unlike conventional NAS methods that start the architecture search from scratch for each new dataset, TransferNAS(NAG) methods utilize prior knowledge gained from previous tasks to accelerate the search process on new datasets. By leveraging this prior knowledge, these methods can expedite the search process and potentially yield better performance on unseen datasets.\nOne of the TransferNAG methods called MetaD2A (Lee et al., 2021a), employs an amortized meta-learning approach to stochastically generate architectures from a given dataset within a crossmodal latent space. When applied to a test task, MetaD2A generates 500 candidate architectures by conditioning on the target dataset and then selects the top architectures based on its datasetaware predictor. TNAS (Shala et al., 2023), another TransferNAS method, is a subsequent work of MetaD2A that aims at enhancing the dataset-aware predictor\u2019s adaptability to unseen test datasets utilizing BO with the deep-kernel GP. TNAS achieves this by making the deep kernel\u2019s output representation be conditioned on both the neural architecture and the characteristics of a dataset. The deep kernel is meta-trained on the same training dataset used in MetaD2A and DiffusionNAG (Ours). In the test phase, TNAS adapts to the test dataset using BO with the deep-kernel GP strategy. As described in Shala et al. (2023), TNAS starts the BO loop by selecting the top-5 architectures with the highest performance on the meta-training set. However, this starting point can be detrimental when searching for an optimal architecture on the unseen target dataset since it may start the BO loop from architectures that perform poorly on the target dataset. As a result, it can lead to a long search time and high computational cost to obtain the target performance on the target dataset."
        },
        {
            "heading": "C.3 SEARCH TIME OF ONE-SHOT NAS",
            "text": "As described in Table 2 of the main paper, we provide the search time for each dataset of the one-shot NAS methods in Table 7."
        },
        {
            "heading": "C.4 DATASETS",
            "text": "We evaluate our approach on four datasets following Lee et al. (2021a): CIFAR-10 (Krizhevsky, 2009), CIFAR-100 (Krizhevsky, 2009), Aircraft (Maji et al., 2013), and Oxford IIT Pets (Parkhi et al., 2012), as described in Section 3.2 of the main paper. In this section, we present a detailed description of each dataset used in our study. 1) CIFAR-10 consists of 32\u00d732 color images from 10 general object classes. The training set has 50,000 images (5,000 per class), and the test set has 10,000 images (1,000 per class). 2) CIFAR-100 contains colored images from 100 fine-grained general object classes. Each class has 500 training images and 100 test images. 3) Aircraft is a fine-grained classification benchmark dataset with 10,000 images from 30 aircraft classes. All images are resized to 32\u00d732. 4) Oxford-IIIT Pets is a dataset for fine-grained classification, including 37 breeds of pets with approximately 200 instances per class. The dataset is divided into 85% for training and 15% for testing. All images are resized to 32\u00d732. For CIFAR-10 and CIFAR-100, we use the predefined splits from the NAS-Bench-201 benchmark. For Aircraft and Oxford-IIIT Pets, we create random validation and test splits by dividing the test set into two equal-sized subsets."
        },
        {
            "heading": "C.5 TRAINING PIPELINE FOR NEURAL ARCHITECTURES",
            "text": "In this section, we provide the architecture training pipeline for both the NAS-Bench-201 and MobileNetV3 search spaces.\nNAS-Bench-201 Following the training pipeline presented in Dong & Yang (2020b), we train each architecture using SGD with Nesterov momentum and employ the cross-entropy loss for 200 epochs. For regularization, we set the weight decay to 0.0005 and decay the learning rate from 0.1 to 0 using a cosine annealing schedule (Loshchilov & Hutter, 2016). We maintain consistency by utilizing the same set of hyperparameters across different datasets. Data augmentation techniques such as random flip with a probability of 0.5, random cropping of a 32\u00d732 patch with 4 pixels padding on each border, and normalization over RGB channels are applied to each dataset.\nMobileNetV3 In our training pipeline, we fine-tuned a subnet of a pretrained supernet from the MobileNetV3 search space on the ImageNet 1K dataset for CIFAR-10, CIFAR-100, Aircraft, and Oxford-IIIT Pets datasets, following the experimental setup by Cai et al. (2020). The process involved several steps. Firstly, we activated the subnet and its parameters pretrained on ImageNet 1K for each neural architecture. Then, we randomly initilized the fully-connected layers for the classifier, where the output size of the classifier is matched with the number of classes in the target dataset. Next, we conducted fine-tuning on the target dataset for 20 epochs. We follow the training settings of MetaD2A Lee et al. (2021a) and NSGANetV2 Lu et al. (2020). Specifically, we utilized SGD optimization with cross-entropy loss, setting the weight decay to 0.0005. The learning rate decayed from 0.1 to 0 using a cosine annealing schedule (Loshchilov & Hutter, 2016). We resized images as 224\u00d7224 pixels and applied dataset augmentations, including random horizontal flip, cutout DeVries & Taylor (2017) with a length of 16, AutoAugment Cubuk et al. (2018), and droppath Huang et al. (2016) with a rate of 0.2. To ensure a fair comparison, we applied the same training pipeline to all architectures obtained by our TransferNAG and the baseline NAS methods.\nIn Section 3.2 of the main paper, we leveraged a surrogate strategy to report the performance of architectures, which is inspired by the existing surrogate NAS benchmarks Li et al. (2021); Zela et al. (2022); Yan et al. (2021b) that utilize predictors to estimate the performance of architectures within a search space. This approach eliminates the need for exhaustively evaluating all architectures in the large search spaces. For evaluating the performance of each neural architecture, we utilized an accuracy predictor provided by Cai et al. (2020). They randomly sampled 16K subnets from the MobileNetV3 search space with diverse architectures. The accuracy of these subnets was measured on a validation set comprising 10K images randomly selected from the original training set of ImageNet 1K. By training an accuracy predictor using these [architecture, accuracy] pairs, Cai et al. (2020) obtained a model capable of predicting the accuracy of a given architecture. Notably, Cai et al. (2020) demonstrated that the root-mean-square error (RMSE) between the predicted and estimated accuracy on the test set is as low as 0.21%. To ensure a fair comparison, we applied an identical training pipeline to all architectures obtained by our method and the baseline NAS methods."
        },
        {
            "heading": "C.6 ACQUISITION FUNCTION OPTIMIZATION STRATEGY IN BAYESIAN OPTIMIZATION BASED",
            "text": "NAS\nIn Section 3.2 of the main paper, we address the limitation of existing Bayesian Optimization (BO) based NAS methods by incorporating our conditional architecture generative framework into them. Specifically, we replace the conventional acquisition optimization strategy with our conditional generative framework, guided by the predictor. To facilitate understanding, we present an algorithm for General Bayesian Optimization NAS that utilizes an ensemble of neural predictors. Additionally, we demonstrate an algorithm for Bayesian Optimization NAS with DiffusionNAG, where our conditional generative framework replaces the conventional acquisition optimization strategy.\nAlgorithm 1: General Bayesian Optimization NAS Input: Search space A, dataset D, parameters n0, N , c, acquisition function \u03c6, function h : A \u2192 R returning validation accuracy of an architecture A \u2208 A. Draw n0 architectures A(1), . . . ,A(n0) uniformly from the search space A and train them on D\nto construct Bn0 = {(A(1), h(A(1))), . . . , (A(n0), h(A(n0)))}. for n = n0, . . . , N \u2212 1 do\nTrain an ensemble of M surrogate model {h\u0302\u03c8m}Mm=1 based on the current population Bn: \u03c8\u2217m \u2208 argmin\u03c8m \u2211n i=1 ( h(A(i))\u2212 h\u0302\u03c8m(A(i)) )2 for m = 1, . . . ,M . Generate a set of c candidate architectures from A using an acquisition optimization strategy. For each candidate architecture A, evaluate the acquisition function \u03c6(A) with {h\u0302\u03c8m}Mm=1. Select architecture A(n+1) which maximizes \u03c6(A) . Train architecture A(n+1) to get the accuracy h(A(n+1)) and add it to the population Bn: Bn+1 = Bn \u222a {(A(n+1), h(A(n+1)))}.\nOutput: A\u2217 = argmaxn=1,...,N h(A(n))\nAlgorithm 2: Bayesian Optimization with DiffusionNAG Input: Pre-trained score network s\u03b8\u2217 , search space A, dataset D, parameters n0, N , c,\nacquisition function \u03c6, function h : A \u2192 R returning validation accuracy of an architecture A \u2208 A.\nDraw n0 architectures A(1), . . . ,A(n0) uniformly from the search space A and train them on D to construct Bn0 = {(A(1), h(A(1))), . . . , (A(n0), h(A(n0)))}. for n = n0, . . . , N \u2212 1 do Train an ensemble of M surrogate model {h\u0302\u03c8m}Mm=1 based on the current population Bn:\n\u03c8\u2217m \u2208 argmin\u03c8m \u2211n i=1 ( h(A(i))\u2212 h\u0302\u03c8m(A(i)) )2 for m = 1, . . . ,M .\n\u00b5\u0302(A(i)) := 1M \u2211M m=1 h\u0302\u03c8\u2217m(A (i))\n\u03c3\u0302i(A (i)) :=\n\u221a\u2211M m=1((h\u0302\u03c8\u2217m (A\n(i))\u2212\u00b5(A(i)))2 M\u22121\np(y|A(i);D) := N (y; \u00b5\u0302((A(i))), \u03c3\u0302(A(i))2). /* Generate c candidate architecture with the score network s\u03b8\u2217. */ S \u2190 \u2205 Minimize \u2211n i=1\u2212 log p\u03be(yi|A(i)) w.r.t \u03be, where yi = h(A(i)). for i = 1, . . . , c do Draw a random noise A(n+i)T from the prior distribution. Generate architecture A(n+i) with s\u03b8\u2217 by the reverse process: dA\n(n+i) t ={ ft(A (n+i) t )\u2212 g2t [ s\u03b8\u2217(A\n(n+i) t , t) + kt\u2207A(n+i)t log p\u03be(y|A (n+i) t )\n]} dt+ gtdw\u0304\nS \u2190 S \u222a {A(n+i)0 } For each candidate architecture A \u2208 S, evaluate \u03c6(A) with p(y|A(i);D). Select architecture A(n+1) which maximizes \u03c6(A). Train architecture A(n+1) to get the accuracy h(A(n+1)) and add it to the population Bn: Bn+1 = Bn \u222a {(A(n+1), h(A(n+1)))}.\nOutput: A\u2217 = argmaxn=1,...,N h(A(n))\nC.7 IMPLEMETATION DETAILS\nIn this section, we provide a detailed description of the DiffusionNAG implementation and the hyperparameters used in our experiments.\nDiffusion Process In Equation (11), which describes the diffusion process in DiffusionNAG, we set the minimum diffusion variance, denoted as \u03c3min, to 0.1 and the maximum diffusion variance, denoted as \u03c3max, to 5.0. These values determine the range of diffusion variances used during the\ndiffusion process. Additionally, we utilize 1000 diffusion steps. This means that during the diffusion process, we iterate 1000 times to progressively generate and update the architectures. Furthermore, we set the sampling epsilon \u03f5 to 1\u00d710\u22125. This value is used in the sampling step to ensure numerical stability (Song et al., 2021b).\nScore Network To train the score network s\u03b8 in DiffusionNAG, we employ the set of hyperparameters in Table 8. We conduct a grid search to tune the hyperparameters. Specifically, we tune the number of transformer blocks from the set {4, 8, 12}, the number of heads from the set {4, 8, 12}, and the learning rate from the set {2\u00d7 10\u22122, 2\u00d7 10\u22123, 2\u00d7 10\u22124, 2\u00d7 10\u22125}. Additionally, we apply Exponential Moving Average (EMA) to the model parameters, which helps stabilize the training process and improve generalization.\nDataset-aware predictor To train the dataset-aware predictor f\u03d5 in DiffusionNAG, we employ the set of hyperparameters in Table 9. We conduct a grid search to tune the hyperparameters. Specifically, we tune the number of DiGCN layers from the set {1, 2, 3, 4}, the hidden dimension of DiGCN layers from the set {36, 72, 144, 288}, the hidden dimension of dataset encoder from the set {32, 64, 128, 256, 512}, and the learning rate from the set {1\u00d7 10\u22121, 1\u00d7 10\u22122, 1\u00d7 10\u22123}."
        },
        {
            "heading": "D ADDITIONAL EXPERIMENTAL RESULTS",
            "text": ""
        },
        {
            "heading": "D.1 DISTRIBUTION OF GENERATED NEURAL ARCHITECTURES",
            "text": "In Section 3.1 of the main paper, we evaluate the effectiveness of DiffusionNAG in generating high-performing architectures. We generate 1,000 architectures using each method and analyze their distribution. The target datasets (D) are CIFAR10/CIFAR100 (Krizhevsky, 2009), for which we have access to the Oracle distribution through the NAS-Bench-201 benchmark (Dong & Yang, 2020b). Alongside the architecture distribution for CIFAR100 depicted in Figure 2 of the main paper, we present the architecture distribution for CI-\nFAR10 in Figure 5. Similar to CIFAR100, DiffusionNAG produces a greater number of highperforming architectures compared to other baselines for the CIFAR10 dataset as well. This observation confirms that the three advantages discussed in Section 3.1 of the main paper also extend to CIFAR10.\nD.2 IMPROVING EXISTING BAYESIAN OPTIMIZATION NAS ACROSS VARIOUS ACQUISITION FUNCTIONS\nLet the actual validation accuracy of an architecture A in the search spaceA be denoted as h(A), and let Bn represent the set of trained models accumulated during the previous Bayesian Optimization (BO) loops, given by Bn := {(A(1), h(A(1))), . . . , (A(n), h(A(n)))}. Moreover, we define the mean of predictions as \u00b5\u0302(A) := 1M \u2211M m=1 h\u0302\u03c8\u2217m(A), where h\u0302\u03c8\u2217m(A) represents the prediction made by the m-th predictor from an ensemble of M predictors. Additionally, we define the standard\ndeviation of predictions as \u03c3\u0302(A) :=\n\u221a\u2211M m=1((h\u0302\u03c8\u2217m (A)\u2212\u00b5\u0302(A)) 2\nM\u22121 . By utilizing an ensemble of predictors\n{h\u0302\u03c8m}Mm=1, we can estimate the mean and standard deviation of predictions for each architecture in the search space (Please refer to Algorithms 1 and 2 in Appendix C.6).\nFollowing Neiswanger et al. (2019) and White et al. (2021a), we utilize the following estimates of acquisition functions for an input architecture A from the search space A:\n\u03c6PI(A) = E [1 [\u00b5\u0302(A) > ymax]] = \u222b \u221e ymax N ( \u00b5\u0302(A), \u03c3\u03022(A) ) dy (23)\n\u03c6EI(A) = E [1 [\u00b5\u0302(A) > ymax] (\u00b5\u0302(A)\u2212 ymax)] = \u222b \u221e ymax (\u00b5\u0302(A)\u2212 ymax)N ( \u00b5\u0302(A), \u03c3\u03022(A) ) dy (24)\n\u03c6ITS(A) = h\u0303(A), h\u0303(A) \u223c N ( \u00b5\u0302(A), \u03c3\u03022(A) ) (25)\n\u03c6UCB(A) = \u00b5\u0302(A) + \u03b2\u03c3\u0302(A), (26)\nwhere ymax = h(Ai \u2217 ) , i\u2217 = argmaxi\u2208[n] h(A (i)).\nWe conduct the same experiment as Figure 4 of the main paper in Figure 6 on the different search space, NB201. The experimental results of different acquisition function optimization strategies across various acquisition functions in NB201 search space are presented in Figure 6. The results demonstrate that our proposed acquisition optimization strategy with the conditional generative framework (Guided Gen (Ours)) consistently shows superior or comparable performance compared to conventional acquisition optimization strategies. This improvement is observed across various acquisition functions, including Probability of Improvement (PI), which is used in Section 3.2 of the main paper, as well as Expected Improvement (EI), Independent Thompson sampling (ITS), and Upper Confidence Bound (UCB) in the NB201 search space."
        },
        {
            "heading": "D.3 ANALYSIS OF GENERATED NEURAL ARCHITECTURES ACROSS DIFFERENT OBJECTIVES",
            "text": "In the Adaptation Across Different Objectives part of Section 3.4 in the main paper, we demonstrate the capability of our conditional generative framework to easily adapt to new tasks. This adaptability is achieved by seamlessly swapping the predictor, trained with the specific target objective, without the need to retrain the score network.\nBy leveraging predictors trained with different objectives, such as clean accuracy, robust accuracy against adversarial attacks (e.g., APGD with perturbation magnitude \u03f5 = 2), and robust accuracy against glass blur corruption (Blur), we generate a pool of 20 neural architectures in the section. In this section, we analyze the architecture pool generated for each objective. Specifically, we examine the occurrence of each operation type for each node in the architecture pool guided by each objective.\nInterestingly, each generated architecture pool exhibits distinct characteristics, as illustrated in Figure 7. In Figure 7(a), we observe that the architecture pool guided by the Clean Accuracy objective has a higher proportion of 3-by-3 convolution operation types compared to the other architecture pools. Furthermore, in Figure 7(b), we can observe that the architecture pool guided by the Robust Accuracy against APGD Attack objective exhibits distinct characteristics. Specifically, nodes 1, 2, and 3 are dominated by the zeroize, 3-by-3 average pooling, and skip connection operation types, while nodes 4, 5, and 6 are predominantly occupied by the 3-by-3 average pooling operation type. Lastly, in Figure 7(c), depicting the architecture pool guided by the Robust Accuracy against Glass Blur Corruption objective, we observe that nodes 1, 2, and 3 are predominantly occupied by the zeroize operation type. Based on our analysis, it is intriguing to observe that the characteristics of the generated architectures vary depending on the objective that guides the architecture generation process."
        },
        {
            "heading": "D.4 GENERATED NEURAL ARCHITECTURES DETAILS",
            "text": "In this section, we provide detailed information on the top-5 architectures selected by the datasetaware predictor following the generation process. The specific architectural details of the generated neural architectures can be found in Figure 8."
        },
        {
            "heading": "D.5 ADAPTATION ACROSS DIFFERENT OBJECTIVES: CORRUPTIONS",
            "text": "In Table 5 of the main paper, we evaluate the adaptation abilities of DiffusionNAG to new tasks by swapping the task-specific predictor in a plug-and-play manner without retraining the score network. In the Section, we conduct further experiments to explore a broader range of corruptions introduced\nnot only in glass blur but also in various corruptions in the NB201-based robustness benchmark (Jung et al., 2023). We conduct experiments to find robust neural architectures for 15 diverse corruptions at severity level 2 in CIFAR-10-C. Similar to Table 5 of the main paper, we demonstrated the capabilities of DiffusionNAG to easily adapt to new tasks without retraining the score network. We achieved this by replacing task-specific predictors in a plug-and-play manner. Specifically, we train a clean\npredictor, f\u03d5clean ), to predict the clean accuracy of neural architectures using randomly sampled pairs of architectures and their corresponding CIFAR-10 clean accuracy from the NB201-based robustness benchmark. Afterward, we train the target corruption predictor, f\u03d5corruption , to predict the robust accuracy of neural architectures under specific corruption types. We create the training dataset by including pairs of architectures and their respective robust accuracy for each corruption scenario from the NB201-based robustness benchmark. With the guidance of the trained f\u03d5clean , we generated a pool of 20 architectures (Sclean. Similarly, with the guidance of the task-specific predictors (f\u03d5corruption ), we generate another pool of 20 architectures for each target corruption (Scorruption). Subsequently, we retrieve the robust accuracy on both f\u03d5clean -guided and f\u03d5corruption -guided architecture pools (Sclean and Scorruption) from the NB201-based robustness benchmark for each corruption scenario.\nThe maximum accuracies of the architecture pool generated using the predictor f\u03d5corruption trained for the target corruption surpass those obtained using the accuracy predictor f\u03d5clean trained for clean accuracy. For 13 out of the 15 corruptions, excluding fog and frost corruption, the maximum accuracy of the architecture pool guided by the f\u03d5corruption predictor exceeds that of the pool guided by f\u03d5clean . The average maximum accuracy increased by 3.6%, reaching 59.0% and 62.6%, respectively."
        },
        {
            "heading": "D.6 SEARCH STRATEGY FOR THE GUIDANCE STRENGTH",
            "text": "To effectively define the search range for guidance strength, we employ a following strategy:\nCoarse grid search: Inspired by Lee et al. (2023), we first perform a coarse grid search on the guidance strength within a log scale, identifying a strength where validity becomes stable. For instance, within the NB201 search space under the Transferable NAS scenario (Table 11), a guidance strength of 104 yielded 100.0% valid neural architectures.\nFine-grained grid search: Building upon the coarse grid search, we perform a more fine-grained grid search around the point identified in the coarse grid search phase. This involves incrementally adjusting the guidance strength until we identify the optimal point where validity remains intact.\nSubsequently, we employ a balanced exploitation-exploration sampling strategy that iteratively decreases the guidance strength from this identified point through the fine-grained grid search. Influenced by the analysis of guidance strength and fidelity/diversity in previous studies (Dhariwal & Nichol, 2021; Ho & Salimans, 2022; Marwood et al., 2023), we designed this strategy to balance the trade-off between exploitation and exploration: Higher guidance strength focuses more on the modes of the guiding predictor, prioritizing fidelity over diversity, which is similar to exploitation. On the contrary, utilizing lower guidance strength reduces dependence on the guiding predictor, enabling increased engagement in exploration.\nAblation studies on the impact of guidance strength. To empirically verify how this guidance strength affects the performance and further provide insights for future follow-up studies, we compared the neural architecture distributions generated using our sampling strategy (balanced exploitation-exploration sampling strategy), both when the guidance scale was fixed at a large value and when it was fixed at a small value (Please see Table 12). We conducted experiments on the CIFAR-10 dataset within the NB201 search space under the Transferable NAS scenario. The fixed large and small values correspond to the maximum and minimum guidance strengths in our strategy of gradually decreasing guidance strength with a scale 5000, respectively (i.e., 15000, 10000, 5000). As we mentioned in Section 3.1 in the main paper, after generating neural architectures by each sampling strategy, we rank the generated architecture pools using a meta-learned dataset-aware predictor and also provide the average accuracy of the Top-5 neural architectures.\nAs evident from Table 12, the average accuracy of Top-5 neural architectures generated with the Fixed at a large value strategy (93.262%) is lower than the average accuracy of Top-5 neural architectures (94.156%) achieved by Ours, possibly due to heavily engaging in exploitation. On the other hand, fixing the guidance strength to one small value allows for greater engagement in exploration, leading to the generation of neural architectures with more diversity. However, since it guides towards regions far from the target sub-region, the average accuracy of the generated Top-5 neural architectures (93.978%) is lower than the result obtained by Ours, i.e., when the guidance strength is 0, it can be considered equivalent to random sampling. Given the trade-off between exploration and exploitation depending on the guidance strength, we progressively adjust it during the generation of neural architectures. This sampling approach offers the advantage of balancing exploitation-exploration by gradually reducing the guidance strength.\nPitfalls of solely relying on a fixed large guidance strength. Using only one fixed large guidance strength to generate neural architectures may not lead to the generation of an optimal neural architecture. In other words, utilizing only one large guidance strength can lead to a heavy reliance on the guiding predictor (exploitation), which may result in phenomena corresponding to model collapse. However, by adopting our aforementioned sampling strategy that gradually decreases the guidance strength for generating neural architectures, we can achieve a balance between exploitation and exploration. As the guidance strength diminishes, the bias towards the predictor decreases, mitigating the risk of model collapse. Empirically, as seen in Table R2, the Standard Deviation values for the accuracy of the set of neural architectures generated by the fixed at a large value method and the proposed sampling method are 0.395 and 0.658, respectively. The fixed at a large value method tends to produce relatively less diverse neural architecture structures. As a result, the proposed method (Top-5 accuracy: 94.154%) outperforms the fixed at a large value method (Top-5 accuracy: 93.262%)."
        },
        {
            "heading": "E FUTURE WORK",
            "text": "In this section, we explore potential future research directions using DiffusionNAG. Firstly, we consider employing a discrete diffusion model as our primary framework, departing from the continuous diffusion model utilized in the main paper. Then, we discuss the feasibility of generating optimal generative models using the DiffusionNAG framework based on the discrete diffusion model. Finally, we discuss the application of DiffusionNAG as a post-training structured pruning approach to design sparse foundation models."
        },
        {
            "heading": "E.1 DIFFUSIONNAG TO DESIGN OPTIMAL GENERATIVE MODELS",
            "text": "We explore the feasibility of generating optimal generative models using a discrete diffusion model under the proposed DiffusionNAG framework. While there are a lot of design choices to generate diffusion models, we consider transformer-based generative models. Inspired by Wang et al. (2020), a NAS work focusing on designing vanilla transformers (although not generation model-based NAS work), we can define a search space that encompasses key architectural design choices, such as the number of transformer blocks, the number of MLP layers within each block, the number of attention layers within each block, and the dimension of hidden states in attention or MLP layers. Similar to a set of visual tokens (Chang et al., 2022) or vocab tokens in the NLP field (Devlin et al., 2019), we can create a set of architectural tokens by representing all combinations of architectural design choices as architectural tokens (i.e., one-hot vectors). Similar to dividing a vacant canvas into image patches and predicting visual tokens for each patch (Chang et al., 2022), in NAS, we can consider that the macro skeleton of architectures (the entire graph) corresponds to the vacant canvas, where nodes of the graph correspond to image patches. Subsequently, we perform architectural token predictions for each node in the graph. To train the score model, our approach involves sampling graphs from the search space, partially masking nodes, and predicting architectural tokens for the masked nodes. This transforms the problem into a cross-entropy minimization task, predicting probabilities for each masked token and computing the cross-entropy between the ground truth one-hot token and the predicted token. The performance metric in this scenario may vary depending on the target generative task. For instance, we can consider FID or IS as performance metrics for image synthesis. We can train the predictor guiding the architecture generation process of DiffusionNAG to predict the performance of an architecture for the target generative task. By utilizing the trained predictor, we can guide the generation of DiffusionNAG to generate an optimal architecture of a generative model for the target task."
        },
        {
            "heading": "E.2 DIFFUSIONNAG TO DESIGN SPARSE FOUNDATION MODELS",
            "text": "To explore efficient sparse architectures for foundation models, we can leverage DiffusionNAG, similar to the research idea discussed in Appendix D.4. Applying diffusion model to generate diffusion models. Among various potential directions, one illustrative example is to consider DiffusionNAG as\na post-training structured pruning approach designed to induce sparsity in Large Language Models (LLMs). This post-training structured pruning method starts from pre-trained models and eliminates redundant architecture elements along with their parameters. We find this approach reasonable, particularly when considering the impracticality of iteratively pre-training or iteratively pruning models at the scale of LLMs with hundreds of billions of parameters. Specifically, since the majority of LLMs are transformer-based models, we can explore sparse architectures within the search space that was defined in our response to Appendix D.4. Our goal would be to identify the optimal architecture that minimizes the accuracy drops within a resource budget (e.g., memory footprint or inference time). In other words, we aim to find the best architecture that enhances inference speed or reduces memory footprint without compromising accuracy. We can address this problem by employing the proposed DiffusionNAG method. First, we can train the score model of DiffusionNAG in a manner similar to that explained in (Tan et al., 2019). Secondly, we can formulate a multiobjective function to evaluate the performance of sparse architecture candidates. For example, we may consider existing multi-objectives proposed by ProxylessNAS (Cai et al., 2019) or MNASNet (Tan et al., 2019) to find an optimal balance between accuracy and latency. Using the multi-objective function, we can train the predictor to predict the performance of given architecture candidates. Guided by the trained predictor, DiffusionNAG can generate an optimal sparse architecture (graph). In the final step, we selectively activate only the architectural components and parameters of LLMs that align with the generated graph. In other words, we prune (deactivate) architectural components such as blocks, MLP layers, and attention layers that do not correspond to the graph. This process results in a resource-efficient model, enhancing the sparsity of the model and contributing to improved efficiency in terms of inference latency or memory utilization."
        }
    ],
    "title": "DIFFUSIONNAG: PREDICTOR-GUIDED NEURAL ARCHI-",
    "year": 2023
}