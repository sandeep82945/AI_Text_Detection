{
    "abstractText": "In federated learning (FL), data heterogeneity is one key bottleneck that causes model divergence and limits performance. Addressing this, existing methods often regard data heterogeneity as an inherent property and propose to mitigate its adverse effects by correcting models. In this paper, we seek to break this inherent property by generating data to complement the original dataset to fundamentally mitigate heterogeneity level. As a novel attempt from the perspective of data, we propose federated learning with consensus-oriented generation (FedCOG). FedCOG consists of two key components at the client side: complementary data generation, which generates data extracted from the shared global model to complement the original dataset, and knowledge-distillation-based model training, which distills knowledge from global model to local model based on the generated data to mitigate over-fitting the original heterogeneous dataset. FedCOG has two critical advantages: 1) it can be a plug-and-play module to further improve the performance of most existing FL methods, and 2) it is naturally compatible with standard FL protocols such as Secure Aggregation since it makes no modification in communication process. Extensive experiments on classical and real-world FL datasets show that FedCOG consistently outperforms state-of-the-art methods .",
    "authors": [],
    "id": "SP:5fb9b1cf82bc7e7d91288600b58d56384eaca90f",
    "references": [
        {
            "authors": [
                "Durmus Alp Emre Acar",
                "Yue Zhao",
                "Ramon Matas",
                "Matthew Mattina",
                "Paul Whatmough",
                "Venkatesh Saligrama"
            ],
            "title": "Federated learning based on dynamic regularization",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Keith Bonawitz",
                "Vladimir Ivanov",
                "Ben Kreuter",
                "Antonio Marcedone",
                "H Brendan McMahan",
                "Sarvar Patel",
                "Daniel Ramage",
                "Aaron Segal",
                "Karn Seth"
            ],
            "title": "Practical secure aggregation for privacypreserving machine learning",
            "venue": "In proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security,",
            "year": 2017
        },
        {
            "authors": [
                "L\u00e9on Bottou",
                "Frank E Curtis",
                "Jorge Nocedal"
            ],
            "title": "Optimization methods for large-scale machine learning",
            "venue": "Siam Review,",
            "year": 2018
        },
        {
            "authors": [
                "Haokun Chen",
                "Ahmed Frikha",
                "Denis Krompass",
                "Volker Tresp"
            ],
            "title": "Fraug: Tackling federated learning with non-iid features via representation augmentation",
            "venue": "arXiv preprint arXiv:2205.14900,",
            "year": 2022
        },
        {
            "authors": [
                "Yae Jee Cho",
                "Jianyu Wang",
                "Gauri Joshi"
            ],
            "title": "Client selection in federated learning: Convergence analysis and power-of-choice selection strategies",
            "venue": "arXiv preprint arXiv:2010.01243,",
            "year": 2020
        },
        {
            "authors": [
                "Liam Collins",
                "Hamed Hassani",
                "Aryan Mokhtari",
                "Sanjay Shakkottai"
            ],
            "title": "Exploiting shared representations for personalized federated learning",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Liam Collins",
                "Hamed Hassani",
                "Aryan Mokhtari",
                "Sanjay Shakkottai"
            ],
            "title": "Fedavg with fine tuning: Local updates lead to representation learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Ittai Dayan",
                "Holger R Roth",
                "Aoxiao Zhong",
                "Ahmed Harouni",
                "Amilcare Gentili",
                "Anas Z Abidin",
                "Andrew Liu",
                "Anthony Beardsworth Costa",
                "Bradford J Wood",
                "Chien-Sung Tsai"
            ],
            "title": "Federated learning for predicting clinical outcomes in patients with covid-19",
            "venue": "Nature medicine,",
            "year": 2021
        },
        {
            "authors": [
                "Enmao Diao",
                "Jie Ding",
                "Vahid Tarokh"
            ],
            "title": "Heterofl: Computation and communication efficient federated learning for heterogeneous clients",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Gongfan Fang",
                "Jie Song",
                "Chengchao Shen",
                "Xinchao Wang",
                "Da Chen",
                "Mingli Song"
            ],
            "title": "Data-free adversarial distillation",
            "venue": "arXiv preprint arXiv:1912.11006,",
            "year": 2019
        },
        {
            "authors": [
                "Matt Fredrikson",
                "Somesh Jha",
                "Thomas Ristenpart"
            ],
            "title": "Model inversion attacks that exploit confidence information and basic countermeasures",
            "venue": "In Proceedings of the 22nd ACM SIGSAC conference on computer and communications security,",
            "year": 2015
        },
        {
            "authors": [
                "Weituo Hao",
                "Mostafa El-Khamy",
                "Jungwon Lee",
                "Jianyi Zhang",
                "Kevin J Liang",
                "Changyou Chen",
                "Lawrence Carin Duke"
            ],
            "title": "Towards fair federated learning with zero-shot data augmentation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Andrew Hard",
                "Kanishka Rao",
                "Rajiv Mathews",
                "Swaroop Ramaswamy",
                "Fran\u00e7oise Beaufays",
                "Sean Augenstein",
                "Hubert Eichner",
                "Chlo\u00e9 Kiddon",
                "Daniel Ramage"
            ],
            "title": "Federated learning for mobile keyboard prediction",
            "venue": "arXiv preprint arXiv:1811.03604,",
            "year": 2018
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Samuel Horvath",
                "Stefanos Laskaridis",
                "Mario Almeida",
                "Ilias Leontiadis",
                "Stylianos Venieris",
                "Nicholas Lane"
            ],
            "title": "Fjord: Fair and accurate federated learning under heterogeneous targets with ordered dropout",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Tzu-Ming Harry Hsu",
                "Hang Qi",
                "Matthew Brown"
            ],
            "title": "Measuring the effects of non-identical data distribution for federated visual classification",
            "year": 1909
        },
        {
            "authors": [
                "Divyansh Jhunjhunwala",
                "Shiqiang Wang",
                "Gauri Joshi"
            ],
            "title": "Fedexp: Speeding up federated averaging via extrapolation",
            "venue": "arXiv preprint arXiv:2301.09604,",
            "year": 2023
        },
        {
            "authors": [
                "Peter Kairouz",
                "H Brendan McMahan",
                "Brendan Avent",
                "Aur\u00e9lien Bellet",
                "Mehdi Bennis",
                "Arjun Nitin Bhagoji",
                "Kallista Bonawitz",
                "Zachary Charles",
                "Graham Cormode",
                "Rachel Cummings"
            ],
            "title": "Advances and open problems in federated learning",
            "venue": "arXiv preprint arXiv:1912.04977,",
            "year": 2019
        },
        {
            "authors": [
                "Sai Praneeth Karimireddy",
                "Satyen Kale",
                "Mehryar Mohri",
                "Sashank Reddi",
                "Sebastian Stich",
                "Ananda Theertha Suresh"
            ],
            "title": "Scaffold: Stochastic controlled averaging for federated learning",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Alex Krizhevsky",
                "Geoffrey Hinton"
            ],
            "title": "Learning multiple layers of features from tiny images",
            "year": 2009
        },
        {
            "authors": [
                "Solomon Kullback",
                "Richard A Leibler"
            ],
            "title": "On information and sufficiency",
            "venue": "The annals of mathematical statistics,",
            "year": 1951
        },
        {
            "authors": [
                "Daliang Li",
                "Junpu Wang"
            ],
            "title": "Fedmd: Heterogenous federated learning via model distillation",
            "venue": "arXiv preprint arXiv:1910.03581,",
            "year": 2019
        },
        {
            "authors": [
                "Qinbin Li",
                "Bingsheng He",
                "Dawn Song"
            ],
            "title": "Model-contrastive federated learning",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Tian Li",
                "Anit Kumar Sahu",
                "Ameet Talwalkar",
                "Virginia Smith"
            ],
            "title": "Federated learning: Challenges, methods, and future directions",
            "venue": "IEEE Signal Processing Magazine,",
            "year": 2020
        },
        {
            "authors": [
                "Tian Li",
                "Anit Kumar Sahu",
                "Manzil Zaheer",
                "Maziar Sanjabi",
                "Ameet Talwalkar",
                "Virginia Smith"
            ],
            "title": "Federated optimization in heterogeneous networks",
            "venue": "Proceedings of Machine Learning and Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Tian Li",
                "Maziar Sanjabi",
                "Ahmad Beirami",
                "Virginia Smith"
            ],
            "title": "Fair resource allocation in federated learning",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Tian Li",
                "Shengyuan Hu",
                "Ahmad Beirami",
                "Virginia Smith"
            ],
            "title": "Ditto: Fair and robust federated learning through personalization",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Xiang Li",
                "Kaixuan Huang",
                "Wenhao Yang",
                "Shusen Wang",
                "Zhihua Zhang"
            ],
            "title": "On the convergence of fedavg on non-iid data",
            "venue": "In International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "Jianhua Lin"
            ],
            "title": "Divergence measures based on the shannon entropy",
            "venue": "IEEE Transactions on Information theory,",
            "year": 1991
        },
        {
            "authors": [
                "Tao Lin",
                "Lingjing Kong",
                "Sebastian U Stich",
                "Martin Jaggi"
            ],
            "title": "Ensemble distillation for robust model fusion in federated learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Guodong Long",
                "Yue Tan",
                "Jing Jiang",
                "Chengqi Zhang"
            ],
            "title": "Federated learning for open banking",
            "venue": "In Federated Learning: Privacy and Incentive,",
            "year": 2020
        },
        {
            "authors": [
                "Aravindh Mahendran",
                "Andrea Vedaldi"
            ],
            "title": "Understanding deep image representations by inverting them",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2015
        },
        {
            "authors": [
                "Aravindh Mahendran",
                "Andrea Vedaldi"
            ],
            "title": "Visualizing deep convolutional neural networks using natural pre-images",
            "venue": "International Journal of Computer Vision,",
            "year": 2016
        },
        {
            "authors": [
                "Brendan McMahan",
                "Eider Moore",
                "Daniel Ramage",
                "Seth Hampson",
                "Blaise Aguera y Arcas"
            ],
            "title": "Communication-efficient learning of deep networks from decentralized data",
            "venue": "In Artificial intelligence and statistics,",
            "year": 2017
        },
        {
            "authors": [
                "Gaurav Kumar Nayak",
                "Konda Reddy Mopuri",
                "Vaisakh Shaj",
                "Venkatesh Babu Radhakrishnan",
                "Anirban Chakraborty"
            ],
            "title": "Zero-shot knowledge distillation in deep networks",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Zhe Qu",
                "Xingyu Li",
                "Rui Duan",
                "Yao Liu",
                "Bo Tang",
                "Zhuo Lu"
            ],
            "title": "Generalized federated learning via sharpness aware minimization",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Sashank J. Reddi",
                "Zachary Charles",
                "Manzil Zaheer",
                "Zachary Garrett",
                "Keith Rush",
                "Jakub Kone\u010dn\u00fd",
                "Sanjiv Kumar",
                "Hugh Brendan McMahan"
            ],
            "title": "Adaptive federated optimization",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Yujun Shi",
                "Jian Liang",
                "Wenqing Zhang",
                "Vincent YF Tan",
                "Song Bai"
            ],
            "title": "Towards understanding and mitigating dimensional collapse in heterogeneous federated learning",
            "venue": "arXiv preprint arXiv:2210.00226,",
            "year": 2022
        },
        {
            "authors": [
                "Jinhyun So",
                "Chaoyang He",
                "Chien-Sheng Yang",
                "Songze Li",
                "Qian Yu",
                "Ramy E Ali",
                "Basak Guler",
                "Salman Avestimehr"
            ],
            "title": "Lightsecagg: a lightweight and versatile design for secure aggregation in federated learning",
            "venue": "Proceedings of Machine Learning and Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Congzheng Song",
                "Filip Granqvist",
                "Kunal Talwar"
            ],
            "title": "Flair: Federated learning annotated image repository",
            "venue": "In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track,",
            "year": 2022
        },
        {
            "authors": [
                "Zhenheng Tang",
                "Yonggang Zhang",
                "Shaohuai Shi",
                "Xin He",
                "Bo Han",
                "Xiaowen Chu"
            ],
            "title": "Virtual homogeneity learning: Defending against data heterogeneity in federated learning",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Hongyi Wang",
                "Mikhail Yurochkin",
                "Yuekai Sun",
                "Dimitris Papailiopoulos",
                "Yasaman Khazaeni"
            ],
            "title": "Federated learning with matched averaging",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Jianyu Wang",
                "Qinghua Liu",
                "Hao Liang",
                "Gauri Joshi",
                "H Vincent Poor"
            ],
            "title": "Tackling the objective inconsistency problem in heterogeneous federated optimization",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Jianyu Wang",
                "Zachary Charles",
                "Zheng Xu",
                "Gauri Joshi",
                "H Brendan McMahan",
                "Maruan Al-Shedivat",
                "Galen Andrew",
                "Salman Avestimehr",
                "Katharine Daly",
                "Deepesh Data"
            ],
            "title": "A field guide to federated optimization",
            "venue": "arXiv preprint arXiv:2107.06917,",
            "year": 2021
        },
        {
            "authors": [
                "Han Xiao",
                "Kashif Rasul",
                "Roland Vollgraf"
            ],
            "title": "Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms",
            "venue": "arXiv preprint arXiv:1708.07747,",
            "year": 2017
        },
        {
            "authors": [
                "Chencheng Xu",
                "Zhiwei Hong",
                "Minlie Huang",
                "Tao Jiang"
            ],
            "title": "Acceleration of federated learning with alleviated forgetting in local training, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Jie Xu",
                "Benjamin S Glicksberg",
                "Chang Su",
                "Peter Walker",
                "Jiang Bian",
                "Fei Wang"
            ],
            "title": "Federated learning for healthcare informatics",
            "venue": "Journal of Healthcare Informatics Research,",
            "year": 2021
        },
        {
            "authors": [
                "Qiang Yang",
                "Yang Liu",
                "Tianjian Chen",
                "Yongxin Tong"
            ],
            "title": "Federated machine learning: Concept and applications",
            "venue": "ACM Transactions on Intelligent Systems and Technology (TIST),",
            "year": 2019
        },
        {
            "authors": [
                "Timothy Yang",
                "Galen Andrew",
                "Hubert Eichner",
                "Haicheng Sun",
                "Wei Li",
                "Nicholas Kong",
                "Daniel Ramage",
                "Fran\u00e7oise Beaufays"
            ],
            "title": "Applied federated learning: Improving google keyboard query suggestions",
            "venue": "arXiv preprint arXiv:1812.02903,",
            "year": 2018
        },
        {
            "authors": [
                "Rui Ye",
                "Zhenyang Ni",
                "Fangzhao Wu",
                "Siheng Chen",
                "Yanfeng Wang"
            ],
            "title": "Personalized federated learning with inferred collaboration",
            "year": 2023
        },
        {
            "authors": [
                "Hongxu Yin",
                "Pavlo Molchanov",
                "Jose M Alvarez",
                "Zhizhong Li",
                "Arun Mallya",
                "Derek Hoiem",
                "Niraj K Jha",
                "Jan Kautz"
            ],
            "title": "Dreaming to distill: Data-free knowledge transfer via deepinversion",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Jaehong Yoon",
                "Wonyong Jeong",
                "Giwoong Lee",
                "Eunho Yang",
                "Sung Ju Hwang"
            ],
            "title": "Federated continual learning with weighted inter-client transfer",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Hao Yu",
                "Sen Yang",
                "Shenghuo Zhu"
            ],
            "title": "Parallel restarted sgd with faster convergence and less communication: Demystifying why model averaging works for deep learning",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2019
        },
        {
            "authors": [
                "Haolin Yuan",
                "Bo Hui",
                "Yuchen Yang",
                "Philippe Burlina",
                "Neil Zhenqiang Gong",
                "Yinzhi Cao"
            ],
            "title": "Addressing heterogeneity in federated learning via distributional transformation",
            "venue": "In Computer Vision\u2013ECCV 2022: 17th European Conference,",
            "year": 2022
        },
        {
            "authors": [
                "Mikhail Yurochkin",
                "Mayank Agarwal",
                "Soumya Ghosh",
                "Kristjan Greenewald",
                "Nghia Hoang",
                "Yasaman Khazaeni"
            ],
            "title": "Bayesian nonparametric federated learning of neural networks",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Jie Zhang",
                "Chen Chen",
                "Bo Li",
                "Lingjuan Lyu",
                "Shuang Wu",
                "Shouhong Ding",
                "Chunhua Shen",
                "Chao Wu"
            ],
            "title": "Dense: Data-free one-shot federated learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Lin Zhang",
                "Li Shen",
                "Liang Ding",
                "Dacheng Tao",
                "Ling-Yu Duan"
            ],
            "title": "Fine-tuning global model via data-free knowledge distillation for non-iid federated learning",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Xiaojin Zhang",
                "Yan Kang",
                "Kai Chen",
                "Lixin Fan",
                "Qiang Yang"
            ],
            "title": "Trading off privacy, utility and efficiency in federated learning",
            "venue": "arXiv preprint arXiv:2209.00230,",
            "year": 2022
        },
        {
            "authors": [
                "Yue Zhao",
                "Meng Li",
                "Liangzhen Lai",
                "Naveen Suda",
                "Damon Civin",
                "Vikas Chandra"
            ],
            "title": "Federated learning with non-iid data",
            "venue": "arXiv preprint arXiv:1806.00582,",
            "year": 2018
        },
        {
            "authors": [
                "Ligeng Zhu",
                "Zhijian Liu",
                "Song Han"
            ],
            "title": "Deep leakage from gradients",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Zhuangdi Zhu",
                "Junyuan Hong",
                "Jiayu Zhou"
            ],
            "title": "Data-free knowledge distillation for heterogeneous federated learning",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "MOON (Li"
            ],
            "title": "2021a): the feature regularization strength \u03bc is set to 0.01",
            "year": 2021
        },
        {
            "authors": [
                "Finetuning",
                "FedRep (Collins et al",
                "Ditto (Li"
            ],
            "title": "2021b). We conduct experiments on CIFAR-10 NIID-1 and the results are shown in Table 9. From the table, we see that FedCOG outperforms all these strong and representative baselines. Table 9: Accuracy(%) comparisons with four representative personalized FL methods across CIFAR-10 dataset at NIID-1 heterogeneity level",
            "year": 2021
        },
        {
            "authors": [
                "Wang"
            ],
            "title": "error\u2019s upper bound approaches 0. Also, given a finite T , there exists a best \u03c4 that minimizes the error\u2019s upper bound. These analyses show that our method can achieve the same convergence rate as most methods, such as FedAvg",
            "year": 2020
        },
        {
            "authors": [
                "Wang"
            ],
            "title": "2020b) and (25) uses bounded variance assumption in Assumption A.3",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Federated learning (FL) is an emerging privacy-preserving training paradigm that enables multiple clients to train a global model collaboratively without directly accessing their raw data (McMahan et al., 2017). With the increasing privacy concerns and legislation, FL has attracted much attention from industry and academia (Kairouz et al., 2019; Yang et al., 2019). FL can be widely used in a diverse of real-world applications, including keyboard prediction (Hard et al., 2018; Yang et al., 2018), healthcare (Dayan et al., 2021; Xu et al., 2021), and finance (Long et al., 2020).\nAs one of the most common and critical issues in FL, data heterogeneity fundamentally limits FL\u2019s practical performance (McMahan et al., 2017) as clients\u2019 datasets could be distinctly different from each other (Li et al., 2020a). To tackle this issue, enormous previous works focus on model-level correction at either the client or server side. At the client side, many methods propose to enhance the consistency among local models through regularization in model space (Li et al., 2020b; Acar et al., 2020) or feature space (Li et al., 2021a; Shi et al., 2022), or local gradient correction (Karimireddy et al., 2020). At the server side, some methods introduce update momentum (Hsu et al., 2019; Reddi et al., 2021), apply knowledge distillation based on public dataset (Lin et al., 2020; Li & Wang, 2019), or adjust aggregation manner (Wang et al., 2020b; Li et al., 2020c). However, these previous works regard the data heterogeneity as an inherent property and attempt to mitigate its negative effects via model correction, leaving the training adversely affected throughout the FL process as the inherent heterogeneity persists to cause client drift (Karimireddy et al., 2020).\nIn this paper, we seek an orthogonal approach to address the data heterogeneity issue: correcting the data itself to mitigate the heterogeneity level. To correct the data, our core idea is to generate data from the shared global model as consensus to complement the original data, which contributes to mitigate the effects of data heterogeneity by making all local datasets more homogeneous (e.g., local categorical distributions are more balanced).\nFollowing this spirit of data correction, we propose a new FL algorithm, Federated Learning with Consensus-Oriented Generation, denoted as FedCOG. FedCOG includes two novel components:\ncomplementary data generation and knowledge-distillation-based model training. 1) During complementary dataset generation, each client generates data that is accurately predicted by global model but incorrectly predicted by local model. In this case, the generated data not only contains consensual knowledge in the global model but also serves as an informative dataset complement, mitigating the level of data heterogeneity. 2) During local model training, beside minimizing the conventional task-driven loss on the original dataset, each client distills the knowledge from global model to current local model based on the generated dataset. Such knowledge distillation contributes to enhance the consensus among local models, which further alleviates the impact of data heterogeneity. Overall, FedCOG improves the performance by mitigating both data heterogeneity level and its effects.\nFedCOG has two critical properties: 1) plug-and-play property and 2) compatibility with conventional real-world FL protocol. First, FedCOG focuses on data correction, which is orthogonal to most existing FL methods (e.g., those that focus on model correction (Li et al., 2020b; Karimireddy et al., 2020)) and thus can be easily combined with them to further enhance the performance. Second, FedCOG is compatible with practical standard FL protocols such as Secure Aggregation (Bonawitz et al., 2017) since it does not modify the communication process on standard FL, making it convenient to deploy in real-world application.\nWith extensive experiments across different representative datasets including real-world multi-label FL dataset FLAIR (Song et al., 2022), heterogeneity types and heterogeneity levels, we show that our proposed FedCOG consistently outperforms nine representative baselines and is of plug-andplay property. Besides, we conduct further empirical analysis to provide more insights in FedCOG from the perspective of alleviated model dissimilarity and enhanced local model performance.\nThe main contribution is as follows:\n\u2022 We propose a novel attempt to tackle data heterogeneity in FL from the perspective of data correction, which seek to mitigate its adverse effects from the dataset itself.\n\u2022 We propose a novel algorithm, FedCOG, which mitigates the issue of data heterogeneity by generating data to complement original data and distilling knowledge from global model to local model through the generated data.\n\u2022 With extensive experiments, we show FedCOG consistently achieves the best and helps relieve model dissimilarity and forgetting."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Data Heterogeneity in Federated Learning. Data heterogeneity is one of the most fundamental and essential challenges in FL (McMahan et al., 2017; Kairouz et al., 2019), where data distributions of clients could distinctly vary from each other, and is shown to affect the performance of FL empirically (Zhao et al., 2018; Li et al., 2020b) and theoretically (Li et al., 2019; Yu et al., 2019). Addressing this, many methods are proposed from the aspects of local model correction and global model adjustment. 1) Local model correction aims for more similar local models at the client side. FedProx (Li et al., 2020b) and FedDyn (Acar et al., 2020) propose regularizing the distance between local and global model. SCAFFOLD (Karimireddy et al., 2020) introduces control variate to correct local gradients, MOON (Li et al., 2021a), and FedDecorr (Shi et al., 2022) regularize the local models from the feature space via feature alignment or feature correlation regularization. 2) Global model adjustment aims for better-performed global model at the server side. FedAvgM (Hsu et al., 2019) and FedOPT (Reddi et al., 2021) introduce momentum to stabilize global model updating. FedNova (Wang et al., 2020b) and FedExP (Jhunjhunwala et al., 2023) modify the aggregation rules by adjusting aggregation weights or global learning rate. Others may apply knowledge distillation based on a public dataset (Lin et al., 2020) or design client sampling strategies (Cho et al., 2020).\nDifferent from these two aspects, our method addresses this issue in a novel way by directly reducing heterogeneity level from the aspect of data. We propose to generate additional data samples to complement the original heterogeneous dataset for each client, which is achieved by inversely optimizing inputs given the global and local model.\nData Generation and Augmentation in FL. (1) Recently, some methods (Zhang et al., 2022a;b; Zhu et al., 2021; Hao et al., 2021) apply data generation at the server side to tune the global model for better performance, which generate data based on all local models. However, this could be un-\nacceptable in practice since Secure Aggregation technique (Bonawitz et al., 2017; So et al., 2022) is often required so that the server can only receive the aggregated global model, making these methods less practical and privacy-preserving. In contrast, our proposed FedCOG is compatible with Secure Aggregation and fundamentally mitigates the data heterogeneity by generating complementary data. (2) Some methods (Tang et al., 2022; Chen et al., 2022; Yuan et al., 2022) apply data augmentation, though, they all introduce additional communication objects other than model parameters (McMahan et al., 2017), which increases communication cost and/or risks of privacy leakage. Others (Xu et al., 2022; Yoon et al., 2021) may generate data to address the problem of forgetting. Unlike these, FedCOG makes no compromise on privacy and communication cost, and is a novel method that adopts task-specific data generation to tackle data heterogeneity.\nData Generation from Model. There are a line of works that focus on data generation from a single model (Fredrikson et al., 2015; Mahendran & Vedaldi, 2015; 2016) to understand the representations of neural networks. Some works extend data generation to achieve data-free knowledge distillation (Fang et al., 2019; Nayak et al., 2019; Mordvintsev et al., 2015; Yin et al., 2020). For example, DeepDream (Mordvintsev et al., 2015) generates data by penalizing classification loss, total variance and \u21132 norm while DeepInversion (Yin et al., 2020) additionally adds a feature regularization and divergence term. These works are orthogonal to ours as they do not consider FL scenarios and our method can be further enhanced by combining with more advanced generation techniques."
        },
        {
            "heading": "3 PROBLEM FORMULATION",
            "text": "The objective of federated learning (FL) is to enable multiple clients collaboratively train a global model under the coordination of a server without directly accessing raw data (Wang et al., 2021). Specifically, suppose that there are K clients, where each client k holds a private local original dataset Dk, the global objective of FL is: min\u03b8 \u2211K k=1 pkLc(\u03b8;Dk), where pk = |Dk|\u2211 i |Di|\nis the relative dataset size,Lc(\u00b7) is the task-driven loss function. Conventionally, this optimization problem is solved by the two iterative steps, including 1) client step: client downloads global model \u03b8 from the server and conducts training on local dataset Dk to obtain local model \u03b8k, 2) server step: server receives local models {\u03b8k} from clients and aggregates them to obtain global model \u03b8 := \u2211 k pk\u03b8k.\nSince clients\u2019 datasets {Dk} can be heterogeneous, the trained local models at client step can be significantly different and eventually results in slow convergence and limited performance of global model. Some methods are proposed to mitigate the difference between local models {\u03b8k} at client step (Li et al., 2020b; Karimireddy et al., 2020) while some to improve the global model \u03b8 at the server step (Reddi et al., 2021; Jhunjhunwala et al., 2023).\nHowever, they do not consider modification on the original datasets {Dk}, which is the root cause of heterogeneity issue. In this paper, we propose a novel attempt for mitigating the effects of data heterogeneity by generating data to complement the original dataset for each client."
        },
        {
            "heading": "4 METHODOLOGY",
            "text": "We propose federated learning with consensus-oriented generation (FedCOG), which mitigates data heterogeneity by generating complementary data and refining training at the client side. FedCOG follows the standard FL protocol that consists of client and server steps (McMahan et al., 2017). The novel designs of FedCOG lie in the client step, which is composed of two key modules: generating data to complement the original dataset and conducting knowledge distillation on the local model, which are elaborated in the following; also see details in Algorithm 1 and illustration in Figure 1."
        },
        {
            "heading": "4.1 COMPLEMENTARY DATASET GENERATION",
            "text": "As a novel attempt to mitigate the adverse effects of data heterogeneity from the perspective of data, the key step in FedCOG is generating data to complement the original dataset of each client, reducing the heterogeneity levels. We propose to generate task-specific and client-specific data by learning to produce samples that can be accurately predicted by the current global model and falsely predicted by the previous local model. Such generated data can contain consensual knowledge for the targeted task and complement the client dataset to increase data diversity.\nTo obtain the desired samples, we set the inputs of the model as learnable parameters and fix the model parameters of global model and local model, where the learnable inputs are optimized by the guidance of task-driven loss and disagreement loss. Specifically, let \u03b8t be the global model at round t and \u03b8t\u22121,rk be client k\u2019s local model at previous round t\u2212 1 after r SGD iterations of local model training. We use \u03b8\u2217|t to represent that the model parameters are fixed and \u03b8t to represent that the model parameters are learnable. Denote Dk = {(x\u0302i, yi)|i \u2208 N} as the generated dataset of client k where N is the number of samples to generate, x\u0302i \u2208 RH\u00d7W\u00d7C (H,W,C denotes height, width and number of channels) is a learnable tensor and yi is an arbitrary pre-defined target label. Each client k generates data by solving the following optimization problem:"
        },
        {
            "heading": "D\u0302tk := argmin",
            "text": "Dk\nLgen(\u03b8\u2217|t,\u03b8\u2217|t\u22121,\u03c4k ;Dk) = 1 |Dk| \u2211\n(x\u0302,y)\u2208Dk\nLc(\u03b8\u2217|t; x\u0302, y)+\u03bbdisLdis(\u03b8\u2217|t,\u03b8\u2217|t\u22121,\u03c4k ; x\u0302),\n(1) where Lgen(\u00b7) is the overall generating objective. Lc(\u00b7) is the task-driven loss, \u03bbdis is a hyperparameter, and Ldis(\u00b7) is the defined disagreement loss. Taking single-label classification task as a concrete example, we define the task-driven loss as a cross-entropy loss and the disagreement loss based on Jensen-Shannon divergence (Lin, 1991; Yin et al., 2020), which is formulated as:\nLdis(\u03b8\u2217|t,\u03b8\u2217|t\u22121,\u03c4k ; x\u0302) := 1\u2212 1\n2\n( KL ( p(\u03b8\u2217|t; x\u0302),p ) +KL ( p(\u03b8\n\u2217|t\u22121,\u03c4 k ; x\u0302),p\n)) , (2)\nwhere p(\u03b8; x\u0302) denotes the output of model \u03b8, p = 12 ( p(\u03b8\u2217|t; x\u0302) + p(\u03b8 \u2217|t\u22121,\u03c4 k ; x\u0302) ) is the averaged output, and KL(\u00b7) is the Kullback\u2013Leibler divergence (Kullback & Leibler, 1951). The proposed two terms contribute to generating consensual and reasonable and data to diversify and complement the local client dataset. 1) The first task-driven loss encourages generating data that is accurately recognized by the global model. Since the global model is the knowledge aggregation of multiple local models, such generated data is task-specific and can be seen as consensual knowledge of all clients. 2) The second disagreement term encourages the generated data to cause global-local model disagreement, serving as a better complement for the original client dataset as the generated data is generally \u2019unfamiliar\u2019 for the client\u2019s local model. The generated dataset D\u0302k is subsequently utilized for the process of local model training; see Section 4.2.\nDetails of designing target labels for generated data. One basic generating strategy is uniformly generating data for all possible target labels. Taking CIFAR-10 as a concrete task example, the target label list can be [0, ..., 0\ufe38 \ufe37\ufe37 \ufe38\nn , 1, ..., 1\ufe38 \ufe37\ufe37 \ufe38 n , ..., 9, ..., 9\ufe38 \ufe37\ufe37 \ufe38 n ], resulting in 10\u00d7 n samples to generate. Though, there\ncan also be more specific designs for particular tasks. See Section A.1.1."
        },
        {
            "heading": "4.2 LOCAL MODEL TRAINING WITH KNOWLEDGE DISTILLATION",
            "text": "After data generation, the pivotal next step involves incorporating the generated data into local model training. A simple and straightforward solution is to merge the generated dataset with the original dataset, creating a new dataset for local model training. However, since there are inherent discrepancies between generated and real-world data, direct training on the generated data in a hard-labeled\nAlgorithm 1 FedCOG: federated learning with consensus-oriented generation. 1: Initialization: number of clients K, number of rounds T , number of iterations \u03c4 . 2: for t = 0, 1, . . . , T \u2212 1 do \u25b7 FL rounds in sequence 3: Send global model \u03b8t to each client \u25b7 Model communication 4: for k = 0, 1, . . . ,K \u2212 1 in parallel do \u25b7 Clients in parallel 5: D\u0302k \u2190 argminD Lgen(\u03b8\u2217|t,\u03b8 \u2217|t\u22121,\u03c4 k ;D) \u25b7 Complementary data generation\n6: \u03b8t,0k := \u03b8 t \u25b7 Model synchronization 7: for r = 0, 1, . . . , \u03c4 \u2212 1 do \u25b7 Iterations in sequence 8: \u03b8t,r+1k := \u03b8 t,r k \u2212 \u03b7\u2207 ( Lc(\u03b8t,rk ) + \u03bbkdLkd(\u03b8\u2217|t,\u03b8 t,r k ) ) \u25b7 KD-based model training\n9: Send local model \u03b8t,\u03c4k to server \u25b7 Model communication 10: \u03b8t+1 := \u2211K k=1 pk\u03b8 t,\u03c4 k \u25b7 Model aggregation 11: Return: \u03b8T\nformat may result in overly stringent supervision. Thus, we advocate using the global model\u2019s soft labels for guidance, striking a balance between learning from valuable insights from the generated data while circumventing the potential biases from the generation process.\nSpecifically, client k initially employs the global model to reinitialize the local model: \u03b8t,0k := \u03b8 t, and subsequently launches local model training on both of the original dataset Dk and the generated dataset D\u0302k, with the optimization objective defined as:\nmin \u03b8\n1 |Dk| \u2211\n(xi,yi)\u2208Dk\nLc(\u03b8;xi, yi) + \u03bbkd\n|D\u0302k| \u2211 (x\u0302i,yi)\u2208D\u0302k KL ( p(\u03b8\u2217|t; x\u0302),p(\u03b8; x\u0302) ) , (3)\nwhere p(\u03b8\u2217|t; x\u0302) is the output of global model, p(\u03b8; x\u0302) is the output of currently optimizing local model, and \u03bbkd is a hyper-parameter. Note that the outputs of the global model given generated data can be extracted from the previous generation step without the need for recomputation. Solving the optimization problem for \u03c4 steps of standard SGD in total, each client obtains the trained local model \u03b8t,\u03c4k , which is then uploaded to the server for further model aggregation.\nIn this objective function, the optimization of the local model is balanced between task-driven loss (optimizing on the original real-world dataset) and knowledge distillation loss (optimizing on the generated dataset). This approach serves two purposes: first, to prevent over-fitting on the locally heterogeneous original dataset; and second, to preserve the knowledge of the global model and prevent excessive loss of consensus knowledge during local training."
        },
        {
            "heading": "4.3 DISCUSSIONS",
            "text": "Compatibility. One advantage of FedCOG is that it is compatible with standard FL communication protocols including Secure Aggregation (SA) (Bonawitz et al., 2017; So et al., 2022) since it does not modify the communication process compared with standard FL (McMahan et al., 2017). However, a series of previous works are not compatible with SA since they assume that the server can directly access each individual local model (Lin et al., 2020; Zhang et al., 2022a;b); while SA requires that the server can only obtain the summation of local models to enhance security and privacy (Zhu et al., 2019). Besides, FedCOG has the plug-and-play property that can be combined with many existing FL works (McMahan et al., 2017; Hsu et al., 2019) and potentially FedCOG can be applied in future works to further improve their performance; see empirical evidence in Table 3.\nCommunication, privacy, and computation. As pointed in (Kairouz et al., 2019), privacy and communication efficiency are two first-order concerns in FL, our proposed FedCOG does not compromise on either of these two aspects since it does not introduce any additional communication objects other than model parameters as FedAvg (McMahan et al., 2017). As there is no free lunch for privacy, utility, and efficiency in FL (Zhang et al., 2022c), we search to enhance utility by slightly sacrificing computation efficiency, which is a relatively minor concern in FL. Though, the introduced computation overhead is acceptable as 1) the introduced generator is lightweight; see the comparison of training cost in Table 4; and 2) generating data for only a few rounds is adequate to bring evident performance improvement; see experiments in Table 3. We provide further discussions in A.4.3."
        },
        {
            "heading": "5 EXPERIMENTS",
            "text": "We conduct comprehensive comparisons with many FL baselines on multiple datasets, including real-world FL dataset FLAIR (Song et al., 2022). See more details and results in Section A."
        },
        {
            "heading": "5.1 IMPLEMENTATION DETAILS",
            "text": "Heterogeneous datasets. Overall, we consider three classical datasets and one real-world FL dataset. 1) For the three classical datasets, including Fashion-MNIST (Xiao et al., 2017), CIFAR10/100 (Krizhevsky et al., 2009), we consider two types of heterogeneity, namely NIID-1 and NIID2. NIID-1 follows Dirichlet distribution Dir\u03b2 (default \u03b2 = 0.1), which is a common setting in FL (Yurochkin et al., 2019; Wang et al., 2020a). NIID-2 makes each client hold data of only partial labels (McMahan et al., 2017; Li et al., 2020b) (2 for Fashion-MNIST and CIFAR-10, 10 for CIFAR-100). 2) FLAIR (Song et al., 2022) is a recently released real-world FL multi-label dataset, where each client is a user from Flickr and we adopt the task with 17 labels. It is a challenging task since each user has different data distribution and few data samples. See more in Section A.2.\nTraining setups. For the three classical datasets, we consider 70 communication rounds, K = 10 clients, \u03c4 = 400 iterations of model training, and apply a simple CNN (Li et al., 2021a). For the more challenging FLAIR (Song et al., 2022) dataset, we consider 400 communication rounds and apply ResNet18 (He et al., 2016) model. Besides, we sample K = 200 clients in total and 10 clients in each FL round, and set \u03c4 = 10 because each client holds relatively few data samples. For all datasets, we use SGD as the optimizer with learning rate 0.01. For FedCOG, we apply the most computation-efficient way by setting the inputs as learnable, supervised by 256 pre-defined target labels. The default settings of \u03bbdis and \u03bbkd are 0.1 and 0.01, respectively.\nBaselines. We compare with ten baselines. FedAvg (McMahan et al., 2017) is the basical baseline. FedProx (Li et al., 2020b), SCAFFOLD (Karimireddy et al., 2020), MOON (Li et al., 2021a), FedSAM (Qu et al., 2022), FedDecorr (Shi et al., 2022) focus on local model correction, VHL (Tang et al., 2022) and FedReg (Xu et al., 2022) focus on data augmentation, FedAvgM (Hsu et al., 2019) and FedExP (Jhunjhunwala et al., 2023) focus on global model adjustment. Among these, FedDecorr and FedExP are recently published baselines, and SCAFFOLD requires double communication cost."
        },
        {
            "heading": "5.2 COMPARISONS WITH STATE-OF-THE-ART METHODS",
            "text": "Applicability on standard datasets. Here, we conduct experiments under two types of heterogeneous scenarios on three standard datasets used in FL literature (Li et al., 2020b; 2021a; Shi et al., 2022; Jhunjhunwala et al., 2023), including Fashion-MNIST (Xiao et al., 2017), CIFAR10/100 (Krizhevsky et al., 2009). We run 50 rounds of FedAvg before running 20 rounds of FedCOG for higher computation efficiency. We report accuracy comparisons in Table 1. From the table, we see that 1) FedCOG consistently performs the best across different datasets and heterogeneous scenarios, indicating the effectiveness of tackling data heterogeneity from the perspective of data. 2) For the relatively more challenging scenario, NIID-2, some methods (e.g., SCAFFOLD) even\nperform worse than FedAvg; while in contrast, our proposed FedCOG brings significantly larger performance gain. This indicates that complementary data generation is more effective in cases where local dataset misses data from several categories (i.e., more heterogeneous).\nApplicability on the real-world FL multi-label dataset, FLAIR (Song et al., 2022). We run additional 5 rounds of FedCOG based on the global model trained by 400 rounds of FedAvg, for the sake of computation efficiency. We compared with other baselines with 405 rounds for fair comparison by evaluating per-class (denoted by C-) precision, recall, F1 score, and per-sample (denoted by O-) F1 score in Table 2. From the table, we see that 1) FedCOG consistently achieves the best performance under different evaluation metrics, indicating the effectiveness of data generation in FedCOG for multi-label task. 2) While some methods perform worse than the vanilla FL method FedAvg or even inapplicable to this task (e.g., VHL (Tang et al., 2022) and FedReg (Xu et al., 2022)), our proposed FedCOG even brings larger performance improvement for such real-world multi-label task.\nSpecifically, it outperforms FedAvg by 14% and 15% relatively with regard to C-F1 and O-F1."
        },
        {
            "heading": "5.3 EFFICIENT PLUG-AND-PLAY PROPERTY",
            "text": "One valuable advantage of FedCOG is its plug-and-play nature since FedCOG focuses on datalevel modification, which is orthogonal to most existing methods. In this section, we select five representative baseline methods, including FedAvg (McMahan et al., 2017), FedAvgM (Hsu et al., 2019), FedProx (Li et al., 2020b), SCAFFOLD (Karimireddy et al., 2020) and MOON (Li et al., 2021a), to investigate the compatibility of FedCOG with existing methods.\nHere, we first conduct 50 rounds of training using the baseline method X. Subsequently, on one hand, we continue training method X for 1 round to obtain Model A, and on the other hand, we conduct 1 round of training of method X+FedCOG to obtain Model B. Finally, we compare the performances of Model A and B in Table 3. From the table, we see that 1) FedCOG consistently brings performance gain when combined with these baseline methods, indicating its plug-and-play property. Specifically, it can bring 19% O-F1 improvement to FedProx (Li et al., 2020b) on FLAIR dataset. 2) there is already evident performance improvement through only 1 round of FedCOG, indicating that FedCOG can take effects in a computation-efficient way."
        },
        {
            "heading": "5.4 EMPIRICAL ANALYSIS OF FEDCOG",
            "text": "We dive into the training process by evaluating model difference, generalization, and personalization of local models to provide more deeper insights. Experiments are conducted on a common setting (Shi et al., 2022; Jhunjhunwala et al., 2023), NIID-1 on CIFAR-10. We start from the same global model initialization (trained by 50 rounds of FedAvg (McMahan et al., 2017)), then launches 1 round of FedAvg, FedProx (Li et al., 2020b), and FedCOG, respectively. See more in Section A.4.\nModel difference of local models. Here, we measure the \u21132 difference between each local model and the aggregated global model, which is a reasonable indicator of the effects of data heterogene-\nity (Li et al., 2020b). From Figure 2(a), we see that FedCOG achieves the lowest model difference between local and global model, indicating that FedCOG can significantly reduce the effects of data heterogeneity on the consistency of local models.\nGeneralization ability of local models. Here, we evaluate each local model on the uniform test dataset (used in Table 1) to examine the preservation of general knowledge during local training. From Figure 2(b), we see that FedCOG consistently and significantly achieves the highest accuracy of local models, indicating that FedCOG preserves the generalization ability best since it generates data from global model to complement the original dataset. Additionally, the achieved global model accuracies of FedAvg, FedProx and FedCOG are 63.34, 64.57 and 65.59, respectively, indicating that FedCOG achieves best generalization performance of both local models and global model.\nPersonalization ability of local models. Here we evaluate each local model on the personalized test dataset, which follows the data distribution of its training dataset. Interestingly, from Figure 2(c), we find that FedCOG also consistently achieves the highest personalization performance, indicating that FedCOG can also enhances the fitting ability on local dataset. Such a performance gain of FedCOG could be attributed to two aspects: 1) From a global view, FedCOG leverages data generation and knowledge distillation to achieve a well-performing aggregated global model. This global model is then used as the initialization for local model training, contributing to improved performance. 2) From a local perspective, the generated data serves two purposes: it enhances model learning by providing additional knowledge, and it mitigates the risk of over-fitting. See the further explanations and comparisons of FedCOG against several representative personalized FL methods in A.4.1.\nComputational cost. We compare the total local time required by each client throughout 51 rounds and the final accuracy in Table 4. Note that we have include the generation time (which takes only 0.81 seconds for each round) required for FedCOG for fair comparison. 1) Results show that FedCOG introduces moderate training and generating time while achieving the highest accuracy. 2) This is acceptable as communication and privacy are more important in FL (Kairouz et al., 2019) while FedCOG makes no compromise on these two aspects, and the utility is improved with evident gain. Specifically, compared with\nFedProx and MOON, FedCOG achieves much higher performance with the least computation time.\nApplicability on different heterogeneity levels. Here, we tune the argument \u03b2 \u2208 {0.1, 0.5, 1.0, 5.0} for Dirichlet distribution in NIID-1 and compare with two representative baselines in Figure 3(a). Note that a smaller \u03b2 denotes a more heterogeneous setting. From the figure, we see that 1) generally, FL methods have worse performance under the more heterogeneous scenarios, indicating that data heterogeneity is a critical issue that limits FL performance. 2) FedCOG consistently performs the best across different heterogeneity levels, indicating its effectiveness on tackling data heterogeneity."
        },
        {
            "heading": "5.5 ABLATION STUDY",
            "text": "To evaluate the impact of each module on the final performance of global model, we conduct experiments on CIFAR-10 for ablation study. See effects of FL arguments in Section A.3.\nComplementary data from noise or generation. To validate the effectiveness of taskspecific data generation from global model and local model, we also run experiments on pure Gaussian noise. From \u2460 & \u2463 in Table 5, we see that data generation from global model and local model leads to significantly better performance than Gaussian noise, indicating the effectiveness of data generation through optimizing the inputs.\nGenerating data from global model or global & local models. By comparing \u2461 with \u2463 in Table 5, we can see that generating data from merely global model achieves moderate performance while adding the disagreement term between global and local model contributes to better performance, indicating the effectiveness of disagreement loss in FedCOG. More detailed analysis is in Figure 3(b).\nSupervision in hard- or soft-label format. By comparing \u2462 and \u2463 in Table 5, we see that softlabel format supervision leads to better performance, indicating the effectiveness of knowledgedistillation-based model training in FedCOG. More detailed analysis is in Figure 3(c).\nEffects of disagreement in generation and knowledge distillation in model training. Under two heterogeneous settings on CIFAR-10, we tune \u03bbdis \u2208 {0.01, 0.1, 1.0, 10.0} and \u03bbkd \u2208 {0.001, 0.01, 0.1, 1.0}, showing results in Figure 3(b) and 3(c), respectively. From the figure, we see that generally \u03bbdis = 0.1 and \u03bbkd = 0.01 can lead to better performance for both settings."
        },
        {
            "heading": "6 CONCLUSION AND FUTURE WORKS",
            "text": "In this paper, we seek to tackle the issue of data heterogeneity in FL from the novel perspective of modifying local dataset. To achieve this, we propose a novel FL algorithm, federated learning with consensus-oriented generation (FedCOG), to mitigate the heterogeneity level. FedCOG consists of two key components, complementary data generation to reduce heterogeneity level and knowledgedistillation-based model training to mitigate the effects of heterogeneity. FedCOG is plug-and-play in most existing FL methods, is compatible with standard FL protocol such as Secure Aggregation, and makes no compromise on communication cost and privacy while improves utility. Extensive experiments on classical and real-world FL datasets show FedCOG consistently outperforms stateof-the-art methods. We believe that FedCOG can inspire more future subsequent works along this line via adding regularization terms during generation or introducing advanced generative models."
        },
        {
            "heading": "A APPENDIX",
            "text": ""
        },
        {
            "heading": "A.1 METHODOLOGY",
            "text": ""
        },
        {
            "heading": "A.1.1 DETAILS OF DESIGNING TARGET LABELS FOR GENERATING DATA",
            "text": "One basic generating strategy is uniformly generating data for all possible target labels. Taking CIFAR-10 as a concrete task example, the target label list can be [0, ..., 0\ufe38 \ufe37\ufe37 \ufe38\nn , 1, ..., 1\ufe38 \ufe37\ufe37 \ufe38 n , ..., 9, ..., 9\ufe38 \ufe37\ufe37 \ufe38 n ], result-\ning in 10\u00d7 n samples to generate. Though, there can also be more specific designs for particular tasks such that the generated dataset can better complement the original heterogeneous dataset. Here, we take a 5-way classification task as an example, where the ultimate objective is training a global model that works well fairly across categories while different clients have different preference on categories. Suppose the categorical distribution of client k is dk = [500, 0, 400, 200, 400], where the i-th element dk,i denotes the number of samples that belongs to category i. Comparing with generating equal number of samples across categories, a more reasonable and targeted strategy is to generate more samples for those categories with fewer samples, better complementing the original dataset and empowering the client to perform better on those minor categories.\nSpecifically, in order to make the complimented dataset more uniform, we generate data according to the following complementary categorical data distribution d\u0302k = max(dk) \u2212 dk, that is, d\u0302k = 500 \u2212 dk = [0, 500, 100, 300, 100]. Then, given the budget of generating samples N , we will allocate N \u00d7 d\u0302k,i\n|d\u0302k| label i\u2019s in the target label list, e.g., generating N \u00d7 5001000 samples for category 1."
        },
        {
            "heading": "A.2 EXPERIMENTAL DETAILS",
            "text": "A.2.1 IMPLEMENTATION DETAILS\nDatasets and Heterogeneity. Overall, we consider three classical datasets and one real-world FL dataset.\n1. Fashion-MNIST (Xiao et al., 2017) is a 10-category classification dataset that consists of 10 types of clothes;\n2. CIFAR-10 and CIFAR-100 (Krizhevsky et al., 2009) are two classification datasets that consists of 10 and 100 types of objects, respectively;\n3. FLAIR (Song et al., 2022) is a recently released real-world FL multi-label dataset, where each client is a user from Flickr with multiple images and corresponding tags. We adopt the task of predicting the existence of 17 categories of objects in each image.\nFor the three classical datasets, we consider two types of data heterogeneity, namely NIID-1 and NIID-2.\n1. NIID-1 follows Dirichlet distribution Dir\u03b2 (default \u03b2 = 0.1), which is a common setting in FL (Yurochkin et al., 2019; Wang et al., 2020a;b; Li et al., 2021a; Shi et al., 2022). The argument \u03b2 controls the level of data heterogeneity, where a smaller \u03b2 corresponds to a more heterogeneous level and \u03b2 is set to 0.1 for default setting. The distribution for these datasets are shown in Figure. 4\n2. NIID-2 makes each client hold data of only partial labels (McMahan et al., 2017; Li et al., 2020b) (2 for Fashion-MNIST and CIFAR-10, 10 for CIFAR-100).\nFor FLAIR (Song et al., 2022), the data heterogeneity is inherent as different local dataset is drawn from different user while different user has different preference and thus different data distribution.\nModels. 1) For Fashion-MNIST and CIFAR-10, we use a simple convolutional neural network (CNN) that is frequently used in FL literature (Li et al., 2021a). The data flow in this model is: 5 \u00d7 5 convolution, max-pooling, 5 \u00d7 5 convolution, three fully-connected layer with hidden size\nof 120, 84 and 10 respectively. 2) For CIFAR-100, we use a slightly different simple CNN. The data flow in this model is: 3 \u00d7 3 convolution, max-pooling, 3 \u00d7 3 convolution, max-pooling, 3 \u00d7 3 convolution, two fully-connected layer with hidden size of 128 and 100 respectively. 3) For FLAIR, we use ResNet18 (He et al., 2016) from PyTorch API."
        },
        {
            "heading": "A.2.2 HYPER-PARAMETER-FREE KNOWLEDGE DISTILLATION FOR IMAGE CLASSIFICATION.",
            "text": "Following the example in A.1.1, for a classification task, we also propose a hyper-parameter-free knowledge distillation manner. Specifically, denote the number of samples from the original real dataset as Nreal = |dk| and the number of samples required to complement a balanced dataset Ngen = |d\u0302k|, we set the coefficient of task-driven loss Lc as NrealNreal+Ngen and the coefficient of knowledge-distillation loss Lkd as NgenNreal+Ngen . In this case, samples from all categories are supervised with similar intensity, relieving the issue of imbalanced learning and contributing to enhanced overall performance.\nBaselines and hyper-parameters. Here, we list the applied hyper-parameters for the compared baselines.\n1. FedAvgM (Hsu et al., 2019): the momentum coefficient is set to 0.1.\n2. FedProx (Li et al., 2020b): the model regularization strength \u00b5 is set to 0.01.\n3. MOON (Li et al., 2021a): the feature regularization strength \u00b5 is set to 0.01.\n4. FedSAM (Qu et al., 2022): the constant to control the radius of the perturbation \u03c1 is set to 0.5.\n5. VHL (Tang et al., 2022): the coefficient of conditional distribution mismatch penalty \u03bb is set to 1.0 and the number of epochs for feature alignment is set to 5.\n6. FedExP (Jhunjhunwala et al., 2023): the \u03f5 is set to 1e\u22123. Note that in our experiments, the performance of FedExP is unstable. Through careful checking, we find that the experiments in the original paper of FedExP are conducted by setting local iteration \u03c4 = 20. While in our paper, we are more interested in larger number of iterations (e.g., \u03c4 = 400) which is common in FL. We conjecture that such large iteration number causes the model update more divergent and thus leads to unstable performance of FedExP.\n7. FedDecorr (Shi et al., 2022): the feature regularization strength \u03b2 is set to 0.01."
        },
        {
            "heading": "A.3 EFFECTS OF FL ARGUMENTS",
            "text": "In order to validate the robustness of our method for various federated learning settings, we conducted experiments using the CIFAR-10 dataset with NIID-1 distribution.\nIteration number of local model training. Here we evaluate the effect of the number of iteration for local training on CIFAR-10 with NIID-1 data distribution. The results are shown in Table 6. From the table, we see that 1) when \u03c4 is relatively small, the accuracy of all methods rise with the\nnumber of local iteration increasing; while when the number of local iteration gets too large (e.g., 800), the accuracy of all approaches drops due to the drift of local updates. 2) Nevertheless, our method FedCOG consistently outperforms the other methods in all scenarios.\nPartial client participation scenarios. For this experiment, we consider K = 50 clients in total and sample partial clients to be available at each round. We compare FedCOG with FedAvg (McMahan et al., 2017), FedProx (Li et al., 2020b), MOON (Li et al., 2021a) here and report the results in Table 7. From the table, we see that 1) as the participation rate increases, the performance of all methods increase since more clients are available at each round. 2) FedCOG consistently outperforms the other methods across all participation rates. Specifically, FedCOG outperforms the second-best method (FedProx (Li et al., 2020b)) by 14% relatively under 0.2 participation rate."
        },
        {
            "heading": "K = 50 0.1 0.2 0.3 0.4",
            "text": "Client number. We investigate the effect of the number of clients for participation for CIFAR-10, see results in Table 8. Our method consistently outperforms all other approaches across different numbers of clients."
        },
        {
            "heading": "A.4 FURTHER ANALYSIS OF FEDCOG",
            "text": ""
        },
        {
            "heading": "A.4.1 PERSONALIZATION ANALYSIS",
            "text": "Methodology. FedCOG provides a better global model, which further provides a better initialization for local, personalized models. Specifically, with data generation and knowledge distillation, FedCOG achieves a better-performed aggregated global model, which is subsequently used as initialization for local model training. Such a better-performed global model provides a better initial point for local model training and thus benefits the model training (improves the personalization). This phenomenon is actually common as FedAvg with Fine-tuning is generally a strong baseline\nin personalized FL (Collins et al., 2021; Ye et al., 2023), which is also explained in (Collins et al., 2022).\nExperiments. (1) In Figure 2(c), we have shown the advantages of FedCOG in the personalized setting by comparing it with two baselines in personalized FL: FedAvg/FedProx with Fine-tuning (local models before aggregation in FedAvg). Both baseline methods are strong and have been intensively verified in many personalized FL papers (Collins et al., 2021; Ye et al., 2023; Collins et al., 2022). Through the figure, we see that FedCOG consistently achieves the highest accuracy across clients. (2) Besides, we conduct further evaluations on personalization by comparing with several representative personalized FL methods, namely, FedAvg with Finetuning, FedProx with Finetuning, FedRep (Collins et al., 2021), and Ditto (Li et al., 2021b). We conduct experiments on CIFAR-10 NIID-1 and the results are shown in Table 9. From the table, we see that FedCOG outperforms all these strong and representative baselines."
        },
        {
            "heading": "A.4.2 GENERATED DATA ANALYSIS",
            "text": "Here, we provide analysis on the generated data.\nVisualization of generated data. We visualize the generated data after various generation steps. CIFAR-10 consists of 32x32 colour images in 10 classes: airplane, automobile, bird, cat, deer, dog, frog, horse, ship and truck. From Figure 5, we see that 1) when the generation step is set to 0, the synthetic data produced is purely random noise. As the number of iterations increases, the generated images start to contain more specific patterns related to their corresponding classes. For instance, when generating images of ships and horses, the abstract shape of a hull and a horse are included in the output. 2) In this experiment, we validate the federated learning framework is privacy preserving, clients could not recover detailed private data from either client side or server side. The patterns in each image are highly abstract and contain certain category-specific features, but overall, we cannot invert a trained network to generate class-specific input images from random noise.\nEffect of generated data. (1) The synthetic data is optimized to be correctly predicted by the global model, thus, it can be seen as a representative of the knowledge of global model. Then, during training, we use the synthetic data to distill knowledge from global model to local model, which basically forces the local model to not forget what the global model has already known. This would benefit the updating process of global model and will not be affected even if the synthetic data is not visually the same as real data. (2) We do not directly use the generated data to train the local model (i.e., using cross entropy loss to train local model without guidance from global model), which prevents the local model from overly fitting the generated data that could contain some unrelated properties.\nNumber of generation steps. Here we experiment on how the number of generation steps affects performance on CIFAR-10 and report the results in Table 10. From the table, we see that 1) generally, a moderate size (e.g., 100) of generation step contributes to better performance. 2) The performance is degraded when the generation step is too small or too large. This is reasonable as when the generation step is small, the image is similar to pure noise that may fail to well complement the original dataset; when the generation step is large, the optimizing process leads to over-fitting too many details and thus could even bring negative impacts.\nEffects of introducing generator. In this experiment, we analyze the effects of introducing a learnable generator into the process of data generation. We insert a learnable convolutional neural network between the learnable noisy inputs and the taks-specific models. The introduced generator consists of a linear layer with hidden size 8192, a upsampling and three 3 \u00d7 3 convolution layers. After introducing the generator, we keep using the same hyper-parameters as before. We see that introducing a generator brings performance gain to NIID-2 setting (1.5% absolute accuracy im-\nprovement), but degrades performance to NIID-1 setting (0.93% absolute accuracy deterioration). This indicates that using the same set of hyper-parameters, FedCOG without generator and FedCOG with a learnable generator perform on par. Such results are reasonable since we do not carefully tune hyper-parameters and the generator is training from scratch. However, we believe that with careful hyper-parameter tuning and per-trained generator, the performance can be further improved, though, we leave this to future works."
        },
        {
            "heading": "A.4.3 FEDCOG UNDER RESOURCE HETEROGENEITY",
            "text": "In practice, FL clients may vary in their available resources. Several methods can be employed to fit this scenario, including:\nFirst, we can use a small batch size during data generation for resource-constrained clients, thereby mitigating memory constraints on these clients. By adjusting the batch size to match the limited memory capacity, we enable efficient data generation without necessitating any alterations to the underlying algorithm. An illustrative example from our CIFAR-100 experiment underscores the feasibility of this approach. Using a simple convolutional neural network, the required number of learnable parameters for local model training is 69380. While for instance, when employing a batch size of 1, the number of learnable parameters during local model training dramatically reduces to 3x32x32=3072, thereby rendering it practical to generate data for clients with restricted resources.\nSecond, another interesting angle is to use different model sizes for different clients. In a system where different clients have different computing resources, we can tailor model sizes to match individual capabilities (Diao et al., 2020; Horvath et al., 2021). By embracing this dynamic model-sizing strategy, clients constrained by limited resources can harness models with reduced dimensions, such as employing sub-sampled CNNs with fewer channels (Diao et al., 2020).\nThird, we introduce a new avenue for future exploration under our framework; that is, the adoption of variable generator sizes based on the clients\u2019 resource profiles. Specifically, resource-constrained clients solely focus on learning the input tensors (no generator, as done in our paper). While clients with more resources could learn to generate data with the help of a pre-trained generative model. We believe that this approach would be an interesting future direction."
        },
        {
            "heading": "A.4.4 GENERATION TIME V.S. NUMBER OF GENERATED SAMPLES",
            "text": "The process of data generation in FedCOG is efficient as the only learnable parameters are the inputs (which is relatively small compared to model parameters) and the required optimization steps is only 100. To further verify this, we adjust the number of generated samples and record the required optimization time for each client at each round. We vary the number of generated samples in the range of [16, 32, 64, 128, 256, 512, 1024] and report the corresponding generation time in Table 11. From the table, we see that the generation process is quite efficient. Throughout the ppaer, we set the number of generated samples as 256, which costs only 0.81 seconds. As a reference, the conventional SGD-based local model training takes 14.81 seconds. Also, even when the number of generated samples is 1024, the required generation time is still only 1.27 seconds, indicating that our method is efficient.\nA.5 VISUALIZATION OF CONVERGENCE CURVES\nHere, we visualize curves of different methods, where x-axis denotes the round index and the y-axis denotes the test accuracy of global model. In this experiment, our method is based on intial 50 rounds of FedAvg and 20 rounds of FedCOG to see a clearer improvement brought by FedCOG. From Figure 6, we see that 1) after 70 rounds of communication, the performance of global models of all methods saturate after 70 rounds of communication. 2) We see that our method consistently performs the best.\nPlease note that at this case, we are more interested at the performance improvement brought by FedCOG over FedAvg, which is significant through the figures. FedCOG can be further integrated with other methods such as FedProx Li et al. (2020b) and MOON Li et al. (2021a); please refer to Table 3."
        },
        {
            "heading": "A.6 THEORETICAL CONVERGENCE ANALYSIS",
            "text": "For simplicity, assume that all the clients share the same generated consensus data (this can be achieved by generating data based on only global model and sharing the same random seed). We denote the global objective function as\n\u03a6(\u03b8;D) = K\u2211\nk=1\npk\u03a6k(\u03b8;D) = K\u2211\nk=1\npk [Fk(\u03b8) +Qk(\u03b8;D)] , (4)\nwhere Fk is task loss, Qk is KD loss on generated dataset D, and \u2211K\nk=1 pk = 1. Then, we show four conventional assumptions in FL theoretical literature Wang et al. (2020b).\nAssumption A.1 (Smoothness). Each loss function \u03a6k(\u03b8;D) is Lipschitz-smooth. Assumption A.2 (Bounded Scalar). \u03a6k(\u03b8;D) is bounded below by \u03a6inf . Assumption A.3 (Unbiased Gradient and Bounded Variance). For each client, the stochastic gradient is unbiased: E\u03be[gk(\u03b8|\u03be)] = \u2207\u03a6k(\u03b8;D), and has bounded variance: E\u03be[||gk(\u03b8|\u03be) \u2212 \u2207\u03a6k(\u03b8;D)||2] \u2264 \u03c32. Assumption A.4 (Bounded Dissimilarity). For any set of weights {pk \u2265 0}Kk=1 subject to\u2211K\nk=1 pk = 1, there exists constants \u03b2 2 \u2265 1 and \u03ba2 \u2265 0 such that \u2211K k=1 pk||\u2207\u03a6k(\u03b8;D)||2 \u2264\n\u03b22||\u2207\u03a6(\u03b8;D)||2 + \u03ba2.\nAdditionally, we show a lemma that is critical for the derivation of our theoretical analysis.\nLemma A.5 (Non-Increasing Global Loss).\n\u03a6(\u03b8(t+1);D(t+1)) \u2264 \u03a6(\u03b8(t+1);D(t)). (5)\nProof. We first expand both two sides of equation 5.\n\u03a6(\u03b8(t+1);D(t+1)) = F (\u03b8(t+1)) +Q(\u03b8(t+1);D(t+1)) = F (\u03b8(t+1)) +KL(\u03b8(t+1)||\u03b8(t+1\u2217),D(t+1)), (6)\n\u03a6(\u03b8(t+1);D(t)) = F (\u03b8(t+1)) +Q(\u03b8(t+1);D(t)) = F (\u03b8(t+1)) +KL(\u03b8(t+1)||\u03b8(t\u2217),D(t)), (7)\nSince KL(\u03b8(t+1)||\u03b8(t+1\u2217),D(t+1)) = 0 and KL(\u03b8(t+1)||\u03b8(t\u2217),D(t)) \u2265 0, we naturally have:\n\u03a6(\u03b8(t+1);D(t+1)) \u2264 \u03a6(\u03b8(t+1);D(t)). (8)\nBased on these assumptions and this lemma, we can finally prove the following theorem: Theorem A.6 (Optimization bound of the global objective function). Under these assumptions, if we set \u03b7L \u2264 min{ 12\u03c4 , 1\u221a 2\u03c4(\u03c4\u22121)(2\u03b22+1) }, the optimization error will be bounded as follows:\nmin t\nE||\u2207\u03a6(\u03b8(t);D(t))||2\n\u22644[\u03a6(\u03b8 (0);D(0))\u2212 \u03a6inf ]\n\u03c4\u03b7T + 4\u03b7L\u03c32 K\u2211 k=1 p2k + 3(\u03c4 \u2212 1)\u03b72\u03c32L2 + 6\u03c4(\u03c4 \u2212 1)\u03b72L2\u03ba2, (9)\nwhere \u03b7 is learning rate for local model training and \u03c4 is the number of local model updates.\nTheorem A.6 establishes that when the number of communication rounds, T , tends towards infinity (T \u2192 \u221e), the expectation of the optimization error remains bounded by a constant number for fixed \u03b7; see detailed proof in Appendix A.6.2. Further, based on Theorem A.6, we can deduce the following corollary when a suitable learning rate, denoted as \u03b7, is established: Corollary A.7 (Convergence of the global objective function). By setting \u03b7 = 1\u221a\n\u03c4T , our method can\nconverge to a stationary point given an infinite number of communication rounds T . Specifically, the bound could be reformulated as follows:\nmin t\nE||\u2207\u03a6(\u03b8(t);D(t))||2\n\u2264 4[\u03a6(\u03b8(0);D(0))\u2212 \u03a6inf ] + 4L\u03c32\n\u2211K k=1 p\n2 k\u221a\n\u03c4T +\n3(\u03c4 \u2212 1)\u03c32L2\n\u03c4T +\n6(\u03c4 \u2212 1)L2\u03ba2\nT (10)\n=O( 1\u221a \u03c4T ) +O( 1 T ) +O( \u03c4 T ). (11)\nThis corollary indicates that as T \u2192 \u221e, the error\u2019s upper bound approaches 0. Also, given a finite T , there exists a best \u03c4 that minimizes the error\u2019s upper bound. These analyses show that our method can achieve the same convergence rate as most methods, such as FedAvg Li et al. (2019); Wang et al. (2020b). Please see detailed proof in Appendix A.6.2."
        },
        {
            "heading": "A.6.1 PRELIMINARIES",
            "text": "For ease of writing, we use gk(\u03b8) to denote mini-batch gradient gk(\u03b8|\u03be) and \u2207\u03a6k(\u03b8) to denote full-batch gradient. We further define the following two notions:\nAveraged Mini-batch Gradient: d(t)k = 1\n\u03c4 \u03c4\u22121\u2211 r=0 gk(\u03b8 (t,r) k ), (12)\nAveraged Full-batch Gradient: h(t)k = 1\n\u03c4 \u03c4\u22121\u2211 r=0 \u2207\u03a6k(\u03b8(t,r)k ). (13)\nThen, the update of the global model between two rounds is as follows:\n\u03b8(t+1) \u2212 \u03b8(t) = \u2212\u03c4\u03b7 K\u2211\nk=1\npkd (t) k . (14)"
        },
        {
            "heading": "A.6.2 PROOF OF THEOREM",
            "text": "According to the Lipschitz-smooth property in Assumption A.1, we have its equivalent form Bottou et al. (2018):\nE [ \u03a6(\u03b8(t+1);D(t)) ] \u2212 \u03a6(\u03b8(t);D(t))\n\u2264 E [\u2329 \u2207\u03a6(\u03b8(t);D(t)),\u03b8(t+1) \u2212 \u03b8(t) \u232a] \u2212 L 2 E [\u2225\u2225\u2225\u03b8(t+1) \u2212 \u03b8(t)\u2225\u2225\u22252] (15)\n= \u2212\u03c4\u03b7 E [\u2329 \u2207\u03a6(\u03b8(t);D(t)),\nK\u2211 k=1 pkd (t) k \u232a] \ufe38 \ufe37\ufe37 \ufe38\nT1\n+ L\u03c42\u03b72\n2 E \u2225\u2225\u2225\u2225\u2225 K\u2211\nk=1\npkd (t) k \u2225\u2225\u2225\u2225\u2225 2 \n\ufe38 \ufe37\ufe37 \ufe38 T2\n, (16)\nwhere the expectation is taken over mini-batches \u03be(t,r)k , \u2200k \u2208 1, 2, ...,K, r \u2208 0, 1, ..., \u03c4 \u2212 1.\nFor ease of writing, we use \u03a6(\u03b8(t)) and \u03a6k(\u03b8(t)) to denote \u03a6(\u03b8(t);D(t)) and \u03a6k(\u03b8(t);D(t)) when it causes no ambiguity for the rest of paper.\nBounding T1 in (16):\nT1 = E [\u2329 \u2207\u03a6(\u03b8(t)),\nK\u2211 k=1 pk(d (t) k \u2212 h (t) k )\n\u232a] + E [\u2329 \u2207\u03a6(\u03b8(t)),\nK\u2211 k=1 pkh (t) k\n\u232a] (17)\n= E [\u2329 \u2207\u03a6(\u03b8(t)),\nK\u2211 k=1 pkh (t) k\n\u232a] (18)\n= 1\n2 \u2225\u2225\u2225\u2207\u03a6(\u03b8(t))\u2225\u2225\u22252 + 1 2 E \u2225\u2225\u2225\u2225\u2225 K\u2211\nk=1\npkh (t) k \u2225\u2225\u2225\u2225\u2225 2 \u2212 1 2 E \u2225\u2225\u2225\u2225\u2225\u2207\u03a6(\u03b8(t))\u2212 K\u2211\nk=1\npkh (t) k \u2225\u2225\u2225\u2225\u2225 2  , (19)\nwhere (18) uses the unbiased gradient assumption in Assumption A.3, such that E[d(t)k \u2212 h (t) i ] = h (t) k \u2212 h (t) i = 0. (19) uses the fact that 2 \u27e8a, b\u27e9 = \u2225a\u2225 2 + \u2225b\u22252 \u2212 \u2225a\u2212 b\u22252.\nBounding T2 in (16):\nT2 = E \u2225\u2225\u2225\u2225\u2225 K\u2211\nk=1\npk(d (t) k \u2212 h (t) k ) + K\u2211 k=1 pkh (t) k \u2225\u2225\u2225\u2225\u2225 2  (20)\n\u2264 2E \u2225\u2225\u2225\u2225\u2225 K\u2211\nk=1\npk(d (t) k \u2212 h (t) k ) \u2225\u2225\u2225\u2225\u2225 2 + 2E \u2225\u2225\u2225\u2225\u2225 K\u2211\nk=1\npkh (t) k \u2225\u2225\u2225\u2225\u2225 2  (21)\n= 2 K\u2211 k=1 p2kE [\u2225\u2225\u2225d(t)k \u2212 h(t)k \u2225\u2225\u22252]+ 2E \u2225\u2225\u2225\u2225\u2225 K\u2211 k=1 pkh (t) k \u2225\u2225\u2225\u2225\u2225 2  (22)\n= 2\n\u03c42 K\u2211 k=1 p2kE \u2225\u2225\u2225\u2225\u2225 \u03c4\u22121\u2211 r=0 (gk(\u03b8 (t,r) k )\u2212\u2207\u03a6k(\u03b8 (t,r) k )) \u2225\u2225\u2225\u2225\u2225 2 + 2E \u2225\u2225\u2225\u2225\u2225 K\u2211 k=1 pkh (t) k \u2225\u2225\u2225\u2225\u2225 2  (23)\n= 2\n\u03c42 K\u2211 k=1 p2k \u03c4\u22121\u2211 r=0 E [\u2225\u2225\u2225gk(\u03b8(t,r)k )\u2212\u2207\u03a6k(\u03b8(t,r)k )\u2225\u2225\u22252]+ 2E \u2225\u2225\u2225\u2225\u2225 K\u2211 k=1 pkh (t) k \u2225\u2225\u2225\u2225\u2225 2  (24)\n\u2264 2\u03c3 2\n\u03c4 K\u2211 k=1 p2k + 2E \u2225\u2225\u2225\u2225\u2225 K\u2211 k=1 pkh (t) k \u2225\u2225\u2225\u2225\u2225 2  (25)\nwhere (21) uses \u2225a+ b\u22252 \u2264 2 \u2225a\u22252 + 2 \u2225b\u22252, (22) uses the fact that clients are independent to each other so that E \u2329 d (t) k \u2212 h (t) k ,d (t) n \u2212 h(t)n \u232a = 0,\u2200k \u0338= n. (24) uses Lemma 2 in Wang et al. (2020b) and (25) uses bounded variance assumption in Assumption A.3.\nPlug (19) and (25) back into (16), we have\nE [ \u03a6(\u03b8(t+1);D(t)) ] \u2212 \u03a6(\u03b8(t);D(t))\n\u2264\u2212 \u03c4\u03b7 2 \u2225\u2225\u2225\u2207\u03a6(\u03b8(t))\u2225\u2225\u22252 \u2212 \u03c4\u03b7 2 (1\u2212 2\u03c4\u03b7L)E \u2225\u2225\u2225\u2225\u2225 K\u2211\nk=1\npkh (t) k \u2225\u2225\u2225\u2225\u2225 2 \n+ L\u03c4\u03b72\u03c32 K\u2211\nk=1\np2k + \u03c4\u03b7\n2 E \u2225\u2225\u2225\u2225\u2225\u2207\u03a6(\u03b8(t))\u2212 K\u2211\nk=1\npkh (t) k \u2225\u2225\u2225\u2225\u2225 2  . (26)\nWhen 1\u2212 2\u03c4\u03b7L \u2265 0, we have\nE [ \u03a6(\u03b8(t+1);D(t)) ] \u2212 \u03a6(\u03b8(t);D(t))\n\u2264 \u2212\u03c4\u03b7 2 \u2225\u2225\u2225\u2207\u03a6(\u03b8(t))\u2225\u2225\u22252 + L\u03c4\u03b72\u03c32 K\u2211 k=1 p2k + \u03c4\u03b7 2 E \u2225\u2225\u2225\u2225\u2225\u2207\u03a6(\u03b8(t))\u2212 K\u2211 k=1 pkh (t) k \u2225\u2225\u2225\u2225\u2225 2  (27)\n= \u2212\u03c4\u03b7 2 \u2225\u2225\u2225\u2207\u03a6(\u03b8(t))\u2225\u2225\u22252 + L\u03c4\u03b72\u03c32 K\u2211 k=1 p2k + \u03c4\u03b7 2 E \u2225\u2225\u2225\u2225\u2225 K\u2211 k=1 pk(\u2207\u03a6k(\u03b8(t))\u2212 h(t)k ) \u2225\u2225\u2225\u2225\u2225 2  (28)\n\u2264 \u2212\u03c4\u03b7 2 \u2225\u2225\u2225\u2207\u03a6(\u03b8(t))\u2225\u2225\u22252 + L\u03c4\u03b72\u03c32 K\u2211 k=1 p2k + \u03c4\u03b7 2 K\u2211 k=1 pk E [\u2225\u2225\u2225\u2207\u03a6k(\u03b8(t))\u2212 h(t)k \u2225\u2225\u22252]\ufe38 \ufe37\ufe37 \ufe38\nT3\n, (29)\nwhere (29) uses Jensen\u2019s Inequality \u2225\u2225\u2225\u2211Kk=1 pkxk\u2225\u2225\u22252 \u2264\u2211Kk=1 pk \u2225xk\u22252.\nBounding T3 in (29):\nE [\u2225\u2225\u2225\u2207\u03a6k(\u03b8(t))\u2212 h(t)k \u2225\u2225\u22252] = E \u2225\u2225\u2225\u2225\u2225\u2207\u03a6k(\u03b8(t))\u2212 1\u03c4 \u03c4\u22121\u2211 r=0 \u2207\u03a6k(\u03b8(t,r)k ) \u2225\u2225\u2225\u2225\u2225 2  (30)\n= E \u2225\u2225\u2225\u2225\u22251\u03c4 \u03c4\u22121\u2211 r=0 (\u2207\u03a6k(\u03b8(t))\u2212\u2207\u03a6k(\u03b8(t,r)k )) \u2225\u2225\u2225\u2225\u2225 2  (31)\n\u2264 1 \u03c4 \u03c4\u22121\u2211 r=0 E [\u2225\u2225\u2225\u2207\u03a6k(\u03b8(t))\u2212\u2207\u03a6k(\u03b8(t,r)k )\u2225\u2225\u22252] (32)\n\u2264 L 2\n\u03c4 \u03c4\u22121\u2211 r=0 E [\u2225\u2225\u2225\u03b8(t) \u2212 \u03b8(t,r)k \u2225\u2225\u22252]\ufe38 \ufe37\ufe37 \ufe38\nT4\n, (33)\nwhere (32) uses Jensen\u2019s Inequality and (33) follows Lipschitz-smooth property.\nBounding T4 in (33):\nE [\u2225\u2225\u2225\u03b8(t) \u2212 \u03b8(t,r)k \u2225\u2225\u22252] = \u03b72E \u2225\u2225\u2225\u2225\u2225 r\u22121\u2211 s=0 gk(\u03b8 (t,s) k ) \u2225\u2225\u2225\u2225\u2225 2  (34)\n\u22642\u03b72E \u2225\u2225\u2225\u2225\u2225 r\u22121\u2211 s=0 ( gk(\u03b8 (t,s) k )\u2212\u2207\u03a6k(\u03b8 (t,s) k ) )\u2225\u2225\u2225\u2225\u2225 2 + 2\u03b72E \u2225\u2225\u2225\u2225\u2225 r\u22121\u2211 s=0 \u2207\u03a6k(\u03b8(t,s)k ) \u2225\u2225\u2225\u2225\u2225 2  (35) =2\u03b72 r\u22121\u2211 s=0 E [\u2225\u2225\u2225gk(\u03b8(t,s)k )\u2212\u2207\u03a6k(\u03b8(t,s)k )\u2225\u2225\u22252]+ 2\u03b72E \u2225\u2225\u2225\u2225\u2225 r\u22121\u2211 s=0 \u2207\u03a6k(\u03b8(t,s)k ) \u2225\u2225\u2225\u2225\u2225 2  (36)\n\u22642r\u03b72\u03c32 + 2\u03b72E \u2225\u2225\u2225\u2225\u2225r r\u22121\u2211 s=0 1 r \u2207\u03a6k(\u03b8(t,s)k ) \u2225\u2225\u2225\u2225\u2225 2  (37) \u22642r\u03b72\u03c32 + 2r\u03b72 r\u22121\u2211 s=0 E [\u2225\u2225\u2225\u2207\u03a6k(\u03b8(t,s)k )\u2225\u2225\u22252] (38)\n\u22642r\u03b72\u03c32 + 2r\u03b72 \u03c4\u22121\u2211 s=0 E [\u2225\u2225\u2225\u2207\u03a6k(\u03b8(t,s)k )\u2225\u2225\u22252] (39)\nwhere (35) uses \u2225a+ b\u22252 \u2264 2 \u2225a\u22252 + 2 \u2225b\u22252, (36) uses Lemma 2 in Wang et al. (2020b), (37) uses the bounded variance assumption in Assumption A.3, (38) uses Jensen\u2019s Inequality.\nPlug (39) back into (33) and use this equation \u2211\u03c4\u22121\nr=0 r = \u03c4(\u03c4\u22121) 2 , we have\nE [\u2225\u2225\u2225\u2207\u03a6k(\u03b8(t))\u2212 h(t)k \u2225\u2225\u22252]\n\u2264L 2\n\u03c4 \u03c4\u22121\u2211 r=0 E [\u2225\u2225\u2225\u03b8(t) \u2212 \u03b8(t,r)k \u2225\u2225\u22252] (40)\n\u2264(\u03c4 \u2212 1)L2\u03b72\u03c32 + (\u03c4 \u2212 1)L2\u03b72 \u03c4\u22121\u2211 s=0 E [\u2225\u2225\u2225\u2207\u03a6k(\u03b8(t,s)k )\u2225\u2225\u22252]\ufe38 \ufe37\ufe37 \ufe38\nT5\n, (41)\nwhere T5 in (41) can be further bounded.\nBounding T5 in (41): E [\u2225\u2225\u2225\u2207\u03a6k(\u03b8(t,s)k )\u2225\u2225\u22252]\n\u22642E [\u2225\u2225\u2225\u2207\u03a6k(\u03b8(t,s)k )\u2212\u2207\u03a6k(\u03b8(t))\u2225\u2225\u22252]+ 2E [\u2225\u2225\u2225\u2207\u03a6k(\u03b8(t))\u2225\u2225\u22252] (42)\n\u22642L2E [\u2225\u2225\u2225\u03b8(t) \u2212 \u03b8(t,s)k \u2225\u2225\u22252]+ 2E [\u2225\u2225\u2225\u2207\u03a6k(\u03b8(t))\u2225\u2225\u22252] , (43)\nwhere (42) uses \u2225a+ b\u22252 \u2264 2 \u2225a\u22252 + 2 \u2225b\u22252, (43) uses Lipschitz-smooth property. Plug (43) back to (41), we have\nL2\n\u03c4 \u03c4\u22121\u2211 r=0 E [\u2225\u2225\u2225\u03b8(t) \u2212 \u03b8(t,r)k \u2225\u2225\u22252]\n\u2264(\u03c4 \u2212 1)L2\u03b72\u03c32 + 2(\u03c4 \u2212 1)\u03b72L4 \u03c4\u22121\u2211 s=0 E [\u2225\u2225\u2225\u03b8(t,0)k \u2212 \u03b8(t,s)\u2225\u2225\u22252]+ 2(\u03c4 \u2212 1)\u03b72L2 \u03c4\u22121\u2211 s=0 E [\u2225\u2225\u2225\u2207\u03a6k(\u03b8(t))\u2225\u2225\u22252]\n(44)\nAfter rearranging, we have\nE [\u2225\u2225\u2225\u2207\u03a6k(\u03b8(t))\u2212 h(t)k \u2225\u2225\u22252]\n\u2264L 2\n\u03c4 \u03c4\u22121\u2211 r=0 E [\u2225\u2225\u2225\u03b8(t) \u2212 \u03b8(t,r)k \u2225\u2225\u22252] (45)\n\u2264 (\u03c4 \u2212 1)\u03b7 2\u03c32L2\n1\u2212 2\u03c4(\u03c4 \u2212 1)\u03b72L2 +\n2\u03c4(\u03c4 \u2212 1)\u03b72L2 1\u2212 2\u03c4(\u03c4 \u2212 1)\u03b72L2 E [\u2225\u2225\u2225\u2207\u03a6k(\u03b8(t))\u2225\u2225\u22252] (46)\n= (\u03c4 \u2212 1)\u03b72\u03c32L2\n1\u2212A +\nA 1\u2212A E [\u2225\u2225\u2225\u2207\u03a6k(\u03b8(t))\u2225\u2225\u22252] , (47)\nwhere we define A = 2\u03c4(\u03c4 \u2212 1)\u03b72L2 < 1. Then\n\u03c4\u03b7\n2 K\u2211 k=1 pkE [\u2225\u2225\u2225\u2207\u03a6k(\u03b8(t))\u2212 h(t)k \u2225\u2225\u22252]\n\u2264\u03c4\u03b7 2 K\u2211 k=1 { pk [ (\u03c4 \u2212 1)\u03b72\u03c32L2 1\u2212A + A 1\u2212A E [\u2225\u2225\u2225\u2207\u03a6k(\u03b8(t))\u2225\u2225\u22252]]} (48)\n\u2264\u03c4(\u03c4 \u2212 1)\u03c3 2L2\u03b73\n2(1\u2212A) +\nA\u03c4\u03b7\u03b22 2(1\u2212A) E [\u2225\u2225\u2225\u2207\u03a6(\u03b8(t))\u2225\u2225\u22252]+ A\u03c4\u03b7\u03ba2 2(1\u2212A) , (49)\nwhere (49) follows bounded dissimilarity assumption in Assumption A.4. Plug (49) back to (29), we have\nE [ \u03a6(\u03b8(t+1);D(t)) ] \u2212 \u03a6(\u03b8(t);D(t))\n\u2264\u2212 \u03c4\u03b7 2 \u2225\u2225\u2225\u2207\u03a6(\u03b8(t);D(t))\u2225\u2225\u22252 + L\u03c4\u03b72\u03c32 K\u2211 k=1 p2k\n+ \u03c4(\u03c4 \u2212 1)\u03c32L2\u03b73\n2(1\u2212A) +\nA\u03c4\u03b7\u03b22 2(1\u2212A) E [\u2225\u2225\u2225\u2207\u03a6(\u03b8(t);D(t))\u2225\u2225\u22252]+ A\u03c4\u03b7\u03ba2 2(1\u2212A) (50)\n=\u2212 \u03c4\u03b7 2 (1\u2212 A\u03b2\n2 1\u2212A ) \u2225\u2225\u2225\u2207\u03a6(\u03b8(t);D(t))\u2225\u2225\u22252 + L\u03c4\u03b72\u03c32 K\u2211\nk=1\np2k + \u03c4(\u03c4 \u2212 1)\u03c32L2\u03b73\n2(1\u2212A) +\nA\u03c4\u03b7\u03ba2\n2(1\u2212A) .\n(51)\nIf A\u03b2 2\n1\u2212A \u2264 1 2 , then 1 1\u2212A \u2264 1 + 1 2\u03b22 and we have E [ \u03a6(\u03b8(t+1);D(t)) ] \u2212 \u03a6(\u03b8(t);D(t))\n\u2264\u2212 \u03c4\u03b7 4 \u2225\u2225\u2225\u2207\u03a6(\u03b8(t);D(t))\u2225\u2225\u22252 + L\u03c4\u03b72\u03c32 K\u2211 k=1 p2k + \u03c4(\u03c4 \u2212 1)\u03c32L2\u03b73 2 (1 + 1 2\u03b22 ) + A\u03c4\u03b7\u03ba2 2 (1 + 1 2\u03b22 )\n(52)\n\u2264\u2212 \u03c4\u03b7 4 \u2225\u2225\u2225\u2207\u03a6(\u03b8(t);D(t))\u2225\u2225\u22252 + L\u03c4\u03b72\u03c32 K\u2211 k=1 p2k + 3 4 \u03c4(\u03c4 \u2212 1)\u03c32L2\u03b73 + 3 4 A\u03c4\u03b7\u03ba2, (53)\nwhere (53) follows \u03b2 \u2265 1 in Assumption A.4. Then E [ \u03a6(\u03b8(t+1);D(t+1)) ] \u2212 \u03a6(\u03b8(t);D(t))\n\u2264E [ \u03a6(\u03b8(t+1);D(t)) ] \u2212 \u03a6(\u03b8(t);D(t)) (54)\n\u2264\u2212 \u03c4\u03b7 4 \u2225\u2225\u2225\u2207\u03a6(\u03b8(t);D(t))\u2225\u2225\u22252 + L\u03c4\u03b72\u03c32 K\u2211 k=1 p2k + 3 4 \u03c4(\u03c4 \u2212 1)\u03c32L2\u03b73 + 3 4 A\u03c4\u03b7\u03ba2, (55)\nwhere (54) follows our key lemma in Lemma A.5.\nFinally, by taking the average expectation across all rounds, we finish the proof of our Theorem A.6:\nmin t\nE \u2225\u2225\u2225\u2207\u03a6(\u03b8(t);D(t))\u2225\u2225\u22252 \u2264 1\nT T\u22121\u2211 t=0 E \u2225\u2225\u2225\u2207\u03a6(\u03b8(t);D(t))\u2225\u2225\u22252 (56)\n\u2264 4 [ \u03a6(\u03b8(0,0))\u2212 \u03a6inf ] \u03c4\u03b7T + 4\u03b7L\u03c32 K\u2211\nk=1\np2k + 3(\u03c4 \u2212 1)\u03b72\u03c32L2 + 6\u03c4(\u03c4 \u2212 1)\u03b72L2\u03ba2. (57)\nConstraints on local learning rate. Here, we summarize the constraints on local learning rate \u03b7:\n1\u2212 2\u03c4\u03b7L \u22650, (58) 1\n1\u2212 2\u03c4(\u03c4 \u2212 1)\u03b72L2 \u22641 + 1 2\u03b22 , (59)\nthat is,\n\u03b7L \u2264 min\n{ 1\n2\u03c4 , 1\u221a 2\u03c4(\u03c4 \u2212 1)(2\u03b22 + 1)\n} . (60)"
        },
        {
            "heading": "A.6.3 COROLLARY",
            "text": "By setting \u03b7 = 1\u221a \u03c4T , the above bound can be written as\nmin t\nE \u2225\u2225\u2225\u2207\u03a6(\u03b8(t);D(t))\u2225\u2225\u22252 \u22644[\u03a6(\u03b8(0,0);D0)\u2212 \u03a6inf ] + 4L2\u03c32 \u2211Kk=1 p2k\u221a\n\u03c4T\n+ 3(\u03c4 \u2212 1)\u03c32L2\n\u03c4T +\n6\u03c4(\u03c4 \u2212 1)L2\u03ba2\n\u03c4T (61)\n=O( 1\u221a \u03c4T ) +O( 1 T ) +O( \u03c4 T ). (62)"
        }
    ],
    "title": "FAKE IT TILL MAKE IT: FEDERATED LEARNING WITH CONSENSUS-ORIENTED GENERATION",
    "year": 2023
}