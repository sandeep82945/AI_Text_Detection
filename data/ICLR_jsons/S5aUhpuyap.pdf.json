{
    "abstractText": "Despite many successful examples in which probabilistic inference can account for perception, we have little understanding of how the brain represents and uses structured priors that capture the complexity of natural input statistics. Here we construct a recurrent circuit model that can implicitly represent priors over latent variables, and combine them with sensory and contextual sources of information to encode task-specific posteriors. Inspired by the recent success of diffusion models as means of learning and using priors over images, our model uses dendritic nonlinearities optimized for denoising, and stochastic somatic integration with the degree of noise modulated by an oscillating global signal. Combining these elements into a recurrent network yields a dynamical system that samples from the prior at a rate prescribed by the period of the global oscillator. Additional inputs reflecting sensory or top-down contextual information alter these dynamics to generate samples from the corresponding posterior, with different input gating patterns selecting different inference tasks. We demonstrate that this architecture can sample from low dimensional nonlinear manifolds and multimodal posteriors. Overall, the model provides a new framework for circuit-level representation of probabilistic information, in a format that facilitates flexible inference.",
    "authors": [],
    "id": "SP:fdc57cef5390641ffc3674a8b783961a1142c47e",
    "references": [
        {
            "authors": [
                "Laurence Aitchison",
                "M\u00e1t\u00e9 Lengyel"
            ],
            "title": "The Hamiltonian Brain: Efficient Probabilistic Inference with Excitatory-Inhibitory Neural Circuit Dynamics",
            "venue": "PLOS Computational Biology,",
            "year": 2016
        },
        {
            "authors": [
                "Laurence Aitchison",
                "Jannes Jegminat",
                "Jorge Aurelio Menendez",
                "Jean-Pascal Pfister",
                "Alexandre Pouget",
                "Peter E. Latham"
            ],
            "title": "Synaptic plasticity as Bayesian inference",
            "venue": "Nature Neuroscience,",
            "year": 2021
        },
        {
            "authors": [
                "Nikitas Angeletos Chrysaitis",
                "Peggy"
            ],
            "title": "Seri\u00e8s. 10 years of Bayesian theories of autism: A comprehensive review",
            "venue": "Neuroscience & Biobehavioral Reviews,",
            "year": 2023
        },
        {
            "authors": [
                "Pietro Berkes",
                "Gerg\u0151 Orb\u00e1n",
                "M\u00e1t\u00e9 Lengyel",
                "J\u00f3zsef Fiser"
            ],
            "title": "Spontaneous Cortical Activity Reveals Hallmarks of an Optimal",
            "venue": "Internal Model of the Environment. Science,",
            "year": 2011
        },
        {
            "authors": [
                "Colin Bredenberg",
                "Eero Simoncelli",
                "Cristina Savin"
            ],
            "title": "Learning efficient task-dependent representations with synaptic plasticity",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Shirui Chen",
                "Linxin Preston Jiang",
                "Rajesh P.N. Rao",
                "Eric Shea-Brown"
            ],
            "title": "Expressive probabilistic sampling in recurrent neural networks. arXiv",
            "year": 2023
        },
        {
            "authors": [
                "Timothy R. Darlington",
                "Jeffrey M. Beck",
                "Stephen G. Lisberger"
            ],
            "title": "Neural implementation of Bayesian inference in a sensorimotor behavior",
            "venue": "Nature Neuroscience,",
            "year": 2018
        },
        {
            "authors": [
                "Prafulla Dhariwal",
                "Alexander Nichol"
            ],
            "title": "Diffusion Models Beat GANs on Image Synthesis",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Thomas Durrant",
                "Yul Kang",
                "Daniel Wolpert",
                "Mate Lengyel"
            ],
            "title": "Here, there, but not everywhere: multimodal beliefs captured by a population code during navigation",
            "venue": "Bernstein conference proceedings,",
            "year": 2023
        },
        {
            "authors": [
                "Rodrigo Echeveste",
                "Laurence Aitchison",
                "Guillaume Hennequin",
                "M\u00e1t\u00e9 Lengyel"
            ],
            "title": "Cortical-like dynamics in recurrent circuits optimized for sampling-based probabilistic inference",
            "venue": "Nature Neuroscience,",
            "year": 2020
        },
        {
            "authors": [
                "Brian J. Fischer",
                "Jos\u00e9 Luis Pe\u00f1a"
            ],
            "title": "Owl\u2019s behavior and neural representation predicted by Bayesian inference",
            "venue": "Nature Neuroscience,",
            "year": 2011
        },
        {
            "authors": [
                "J\u00f3zsef Fiser",
                "Pietro Berkes",
                "Gerg\u0151 Orb\u00e1n",
                "M\u00e1t\u00e9 Lengyel"
            ],
            "title": "Statistically optimal perception and learning: From behavior to neural representations",
            "venue": "Trends in cognitive sciences,",
            "year": 2010
        },
        {
            "authors": [
                "Deep Ganguli",
                "Eero P. Simoncelli"
            ],
            "title": "Efficient Sensory Encoding and Bayesian Inference with Heterogeneous Neural Populations",
            "venue": "Neural Computation,",
            "year": 2014
        },
        {
            "authors": [
                "Peggy Gerardin",
                "Zoe Kourtzi",
                "Pascal Mamassian"
            ],
            "title": "Prior knowledge of illumination for 3D perception in the human brain",
            "venue": "Proceedings of the National Academy of Sciences,",
            "year": 2010
        },
        {
            "authors": [
                "Ahna R Girshick",
                "Michael S Landy",
                "Eero P Simoncelli"
            ],
            "title": "Cardinal rules: Visual orientation perception reflects knowledge of environmental statistics",
            "venue": "Nature Neuroscience,",
            "year": 2011
        },
        {
            "authors": [
                "Michael Hahn",
                "Xue-Xin Wei"
            ],
            "title": "A unifying theory explains seemingly contradicting biases in perceptual estimation, December 2022",
            "year": 2022
        },
        {
            "authors": [
                "Guillaume Hennequin",
                "Laurence Aitchison",
                "M\u00e1t\u00e9 Lengyel"
            ],
            "title": "Fast sampling for Bayesian inference in neural circuits",
            "year": 2014
        },
        {
            "authors": [
                "Jonathan Ho",
                "Tim Salimans"
            ],
            "title": "Classifier-Free Diffusion Guidance",
            "venue": "NeurIPS",
            "year": 2021
        },
        {
            "authors": [
                "Jonathan Ho",
                "Ajay Jain",
                "Pieter Abbeel"
            ],
            "title": "Denoising Diffusion Probabilistic Models",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Neil M.T. Houlsby",
                "Ferenc Husz\u00e1r",
                "Mohammad M. Ghassemi",
                "Gerg\u0151 Orb\u00e1n",
                "Daniel M. Wolpert",
                "M\u00e1t\u00e9 Lengyel"
            ],
            "title": "Cognitive Tomography Reveals Complex, Task-Independent Mental Representations",
            "venue": "Current Biology,",
            "year": 2013
        },
        {
            "authors": [
                "Patrik Hoyer",
                "Aapo Hyv\u00e4rinen"
            ],
            "title": "Interpreting Neural Response Variability as Monte Carlo Sampling of the Posterior",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2002
        },
        {
            "authors": [
                "Monika P. Jadi",
                "Bardia F. Behabadi",
                "Alon Poleg-Polsky",
                "Jackie Schiller",
                "Bartlett W. Mel"
            ],
            "title": "An Augmented Two-Layer Model Captures Nonlinear Analog Spatial Integration Effects in Pyramidal Neuron Dendrites",
            "venue": "Proceedings of the IEEE. Institute of Electrical and Electronics Engineers,",
            "year": 2014
        },
        {
            "authors": [
                "Mehrdad Jazayeri",
                "Srdjan Ostojic"
            ],
            "title": "Interpreting neural computations by examining intrinsic and embedding dimensionality of neural activity",
            "venue": "Current Opinion in Neurobiology,",
            "year": 2021
        },
        {
            "authors": [
                "Zahra Kadkhodaie",
                "Eero Simoncelli"
            ],
            "title": "Stochastic Solutions for Linear Inverse Problems using the Prior Implicit in a Denoiser",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Zahra Kadkhodaie",
                "Eero P. Simoncelli"
            ],
            "title": "Solving Linear Inverse Problems Using the Prior Implicit in a Denoiser",
            "year": 2020
        },
        {
            "authors": [
                "Kishore V. Kuchibhotla",
                "Jonathan V. Gill",
                "Grace W. Lindsay",
                "Eleni S. Papadoyannis",
                "Rachel E. Field",
                "Tom A. Hindmarsh Sten",
                "Kenneth D. Miller",
                "Robert C. Froemke"
            ],
            "title": "Parallel processing by cortical inhibition enables context-dependent behavior",
            "venue": "Nature Neuroscience,",
            "year": 2017
        },
        {
            "authors": [
                "Baowang Li",
                "Matthew R. Peterson",
                "Ralph D. Freeman"
            ],
            "title": "Oblique Effect: A Neural Basis in the Visual Cortex",
            "venue": "Journal of Neurophysiology,",
            "year": 2003
        },
        {
            "authors": [
                "Wei Ji Ma"
            ],
            "title": "Organizing probabilistic models of perception",
            "venue": "Trends in Cognitive Sciences,",
            "year": 2012
        },
        {
            "authors": [
                "Wei Ji Ma",
                "Jeffrey M Beck",
                "Peter E Latham",
                "Alexandre Pouget"
            ],
            "title": "Bayesian inference with probabilistic population codes",
            "venue": "Nature Neuroscience,",
            "year": 2006
        },
        {
            "authors": [
                "Pascal Mamassian",
                "Michael Landy",
                "Laurence T Maloney"
            ],
            "title": "Bayesian modelling of visual perception",
            "year": 2002
        },
        {
            "authors": [
                "Paul Masset",
                "Jacob A. Zavatone-Veth",
                "J. Patrick Connor",
                "Venkatesh N. Murthy",
                "Cengiz Pehlevan"
            ],
            "title": "Natural gradient enables fast sampling in spiking neural networks",
            "venue": "Advances in neural information processing systems,",
            "year": 2022
        },
        {
            "authors": [
                "Masanori Murayama",
                "Enrique P\u00e9rez-Garci",
                "Thomas Nevian",
                "Tobias Bock",
                "Walter Senn",
                "Matthew E. Larkum"
            ],
            "title": "Dendritic encoding of sensory stimuli controlled by deep cortical interneurons",
            "year": 2009
        },
        {
            "authors": [
                "Radford M. Neal"
            ],
            "title": "Sampling from multimodal distributions using tempered transitions",
            "venue": "Statistics and Computing,",
            "year": 1996
        },
        {
            "authors": [
                "Ilya Nemenman",
                "F. Shafee",
                "William Bialek"
            ],
            "title": "Entropy and Inference, Revisited",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2001
        },
        {
            "authors": [
                "Alexander Quinn Nichol",
                "Prafulla Dhariwal"
            ],
            "title": "Improved Denoising Diffusion Probabilistic Models",
            "venue": "In Proceedings of the 38th International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Gerg\u0151 Orb\u00e1n",
                "Pietro Berkes",
                "J\u00f3zsef Fiser",
                "M\u00e1t\u00e9 Lengyel"
            ],
            "title": "Neural Variability and Sampling-Based Probabilistic Representations in the Visual Cortex",
            "year": 2016
        },
        {
            "authors": [
                "Panayiota Poirazi",
                "Terrence Brannon",
                "Bartlett W. Mel"
            ],
            "title": "Pyramidal Neuron as Two-Layer",
            "venue": "Neural Network. Neuron,",
            "year": 2003
        },
        {
            "authors": [
                "Alon Polsky",
                "Bartlett W Mel",
                "Jackie Schiller"
            ],
            "title": "Computational subunits in thin dendrites of pyramidal cells",
            "venue": "Nature Neuroscience,",
            "year": 2004
        },
        {
            "authors": [
                "Bo Qiang",
                "Yuxuan Song",
                "Minkai Xu",
                "Jingjing Gong",
                "Bowen Gao",
                "Hao Zhou",
                "Wei-Ying Ma",
                "Yanyan Lan"
            ],
            "title": "Coarse-to-Fine: A Hierarchical Diffusion Model for Molecule Generation in 3D",
            "venue": "In Proceedings of the 40th International Conference on Machine Learning,",
            "year": 2023
        },
        {
            "authors": [
                "Aditya Ramesh",
                "Mikhail Pavlov",
                "Gabriel Goh",
                "Scott Gray",
                "Chelsea Voss",
                "Alec Radford",
                "Mark Chen",
                "Ilya Sutskever"
            ],
            "title": "Zero-Shot Text-to-Image Generation",
            "venue": "In Proceedings of the 38th International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Martin Raphan",
                "Eero P. Simoncelli"
            ],
            "title": "Least Squares Estimation Without Priors or Supervision",
            "venue": "Neural Computation,",
            "year": 2011
        },
        {
            "authors": [
                "David P. Reichert",
                "Peggy Series",
                "Amos J. Storkey"
            ],
            "title": "Unifying low-level mechanistic and high-level Bayesian explanations of bistable perceptions: Neuronal adaptation for cortical inference",
            "venue": "BMC Neuroscience,",
            "year": 2011
        },
        {
            "authors": [
                "Lisa Roux",
                "Gy\u00f6rgy Buzs\u00e1ki"
            ],
            "title": "Tasks for inhibitory interneurons in intact brain",
            "venue": "circuits. Neuropharmacology,",
            "year": 2015
        },
        {
            "authors": [
                "Camille Rull\u00e1n Bux\u00f3",
                "Cristina Savin"
            ],
            "title": "A sampling-based circuit for optimal decision making",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Chitwan Saharia",
                "William Chan",
                "Saurabh Saxena",
                "Lala Li",
                "Jay Whang",
                "Emily L. Denton",
                "Kamyar Ghasemipour",
                "Raphael Gontijo Lopes",
                "Burcu Karagol Ayan",
                "Tim Salimans",
                "Jonathan Ho",
                "David J. Fleet",
                "Mohammad Norouzi"
            ],
            "title": "Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Cristina Savin",
                "Sophie Den\u00e8ve"
            ],
            "title": "Spatio-temporal Representations of Uncertainty in Spiking Neural Networks",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2014
        },
        {
            "authors": [
                "Cristina Savin",
                "Peter Dayan",
                "M\u00e1t\u00e9 Lengyel"
            ],
            "title": "Optimal Recall from Bounded Metaplastic Synapses: Predicting Functional Adaptations in Hippocampal Area CA3",
            "venue": "PLoS Computational Biology,",
            "year": 2014
        },
        {
            "authors": [
                "Peggy Seri\u00e8s",
                "Aaron R. Seitz"
            ],
            "title": "Learning what to expect (in visual perception)",
            "venue": "Frontiers in Human Neuroscience,",
            "year": 2013
        },
        {
            "authors": [
                "Jascha Sohl-Dickstein",
                "Eric Weiss",
                "Niru Maheswaranathan",
                "Surya Ganguli"
            ],
            "title": "Deep Unsupervised Learning using Nonequilibrium Thermodynamics",
            "venue": "In Proceedings of the 32nd International Conference on Machine Learning,",
            "year": 2015
        },
        {
            "authors": [
                "Yang Song",
                "Stefano Ermon"
            ],
            "title": "Generative Modeling by Estimating Gradients of the Data Distribution",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Lucas Theis",
                "A\u00e4ron van den Oord",
                "Matthias Bethge"
            ],
            "title": "A note on the evaluation of generative models",
            "venue": "In International Conference on Machine Learning,",
            "year": 2016
        },
        {
            "authors": [
                "Bal\u00e1zs B. Ujfalussy",
                "Judit K. Makara",
                "M\u00e1t\u00e9 Lengyel",
                "Tiago Branco"
            ],
            "title": "Global and Multiplexed Dendritic Computations under In Vivo-like",
            "venue": "Conditions. Neuron,",
            "year": 2018
        },
        {
            "authors": [
                "Pascal Vincent"
            ],
            "title": "A Connection Between Score Matching and Denoising Autoencoders",
            "venue": "Neural Computation,",
            "year": 2011
        },
        {
            "authors": [
                "Xue-xin Wei",
                "Alan A Stocker"
            ],
            "title": "Efficient coding provides a direct link between prior and likelihood in perceptual Bayesian inference",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2012
        },
        {
            "authors": [
                "Louise Whiteley",
                "Maneesh Sahani"
            ],
            "title": "Attention in a Bayesian Framework",
            "venue": "Frontiers in Human Neuroscience,",
            "year": 2012
        },
        {
            "authors": [
                "Thilo Womelsdorf",
                "Taufik A. Valiante",
                "Ned T. Sahin",
                "Kai J. Miller",
                "Paul Tiesinga"
            ],
            "title": "Dynamic circuit motifs underlying rhythmic gain control, gating and integration",
            "venue": "Nature Neuroscience,",
            "year": 2014
        },
        {
            "authors": [
                "Richard Zemel",
                "Peter Dayan"
            ],
            "title": "Distributional Population Codes and Multiple Motion Models",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 1998
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "The organization of past experience in the form of internal models is crucial for making sense of noisy and incomplete sensory information. The framework of Bayesian inference successfully formalizes this knowledge in terms of probabilistic priors, and perception and action as probabilistic inference. However, despite many successes in explaining behavioral (Mamassian et al., 2002; Li et al., 2003; Gerardin et al., 2010; Girshick et al., 2011; Wei & Stocker, 2012; Ma, 2012; Hahn & Wei, 2022; Angeletos Chrysaitis & Serie\u0300s, 2023) and neural (Zemel & Dayan, 1998; Ma et al., 2006; Fiser et al., 2010; Savin & Dene\u0300ve, 2014; Savin et al., 2014; Orba\u0301n et al., 2016; Echeveste et al., 2020; Aitchison et al., 2021) observations through the lens of Bayesian inference, the circuit implementation of such computations remains poorly understood. In particular, we know relatively little about how priors are represented and the mechanics of how they affect perception.\nBrain priors come in many flavors. For instance, priors over faces are structurally complex and invariant across tasks within subjects, despite substantial across subject variability (Houlsby et al., 2013). Such structural priors reflect natural world statistics; they allow for efficient learning and powerful generalization across a range of tasks (Gerardin et al., 2010; Houlsby et al., 2013). In contrast, contextual priors are relatively short-lived and adapt to reflect the needs of the task at hand (Serie\u0300s & Seitz, 2013; Darlington et al., 2018). Due to these differences, it is possible for different types of priors to have different neural representations (Rulla\u0301n Buxo\u0301 & Savin, 2021) and be accessed through separate neural mechanisms during inference.\nNeural representation of priors are generally assumed to be implicit, with the exact form reflecting different schools of thought on neural correlates of uncertainty (Ma et al., 2006; Fiser et al., 2010). One set of models take the view that priors are represented in the stimulus encoding population, with more neural resources dedicated to higher probability stimuli (Fischer & Pen\u0303a, 2011; Girshick et al., 2011; Ganguli & Simoncelli, 2014; Bredenberg et al., 2020). The (log) prior could also be\nencoded in a separate subpopulation, e.g. in the form of a probabilistic population code and then linearly combined with the (log) sensory evidence (Ma et al., 2006). These theoretical schemes focus largely on the probabilistic representation of a single, one-dimensional stimulus feature, and cannot be readily generalized to the high-dimensional structured priors needed for representing natural stimuli (Houlsby et al., 2013; Durrant et al., 2023). Such complex distributions can be represented in stochastic recurrent circuits whose activity over time encodes samples from the posterior distributions (Hoyer & Hyva\u0308rinen, 2002; Fiser et al., 2010), or\u2014in the limit of vanishing sensory evidence\u2014samples from the prior (Berkes et al., 2011). In this neural sampling framework, prior information is implicitly encoded in the recurrent collaterals between the neurons that encode the inferred latents. However, the worked out examples of neural sampling dynamics have so far been restricted to priors and posteriors with relatively simple structure, e.g. Gaussian scale mixtures to account for statistics of gratings in early vision (Orba\u0301n et al., 2016), or multimodal distributions to model bistable percepts (Reichert et al., 2011; Savin et al., 2014). Finally, some priors could be encoded downstream of sensory representations (Ma, 2012), affecting perception through feedback mechanisms akin to attention (Whiteley & Sahani, 2012), although the circuit implementation of such effects remains underspecified. Overall, we are still missing circuit level models of probabilistic computation that capture the statistical complexity of natural priors and can flexibly use them across tasks as demonstrated behaviorally.\nThe problem of representing complex high-dimensional distributions has close correspondents in machine learning. For example, digital images lie in a high dimensional pixel space, yet not all points in this space correspond to veridical images. Within this huge ambient dimensionality, naturalistic images trace low dimensional nonlinear manifolds, whose local coordinates represent continuous deformations or intensity variations. Among recent machine learning attempts at modeling such structure, diffusion models have emerged as particularly expressive and stable generative neural network models (Sohl-Dickstein et al., 2015; Kadkhodaie & Simoncelli, 2020; Ho et al., 2020), outperforming previous generative solutions such as VAEs, normalizing flows and GANs on the task of image synthesis (Dhariwal & Nichol, 2021). Their training involves local denoising operations, which can translate into relatively simple local learning objectives (Raphan & Simoncelli, 2011). Furthermore, they can be combined flexibly with different likelihoods to support inference (SohlDickstein et al., 2015; Kadkhodaie & Simoncelli, 2021; Song & Ermon, 2019; Nichol & Dhariwal, 2021). Taken together, these features make diffusion models a valuable source of inspiration for building new classes of neural circuits for probabilistic computation.\nHere we adapt several ideas developed in the context of diffusion models to construct a recurrent circuit model that can implicitly represent sensory priors and combine them with other sources of information to encode task-specific posteriors. Our solution relies on dendritic nonlinearities, optimized for denoising, and stochastic somatic activity modulated by a global oscillation that determines the effective rate of sampling. Additional inputs into the circuit which provide sensory or top-down contextual information shape the dynamics to generate samples from the corresponding posteriors. In simulations, we demonstrate several scenarios of prior and posterior encoding, including nonlinear manifolds embedded in a higher dimensional ambient space as priors and several likelihoods corresponding to bottom-up and top-down evidence. We also identify potential neural signatures of such probabilistic inference that might be testable experimentally."
        },
        {
            "heading": "2 ENCODING COMPLEX PRIORS IN RECURRENT NEURAL CIRCUITS",
            "text": "Representing priors with diffusion models. Diffusion models (DMs) are generative neural networks that develop an implicit representation of data statistics by learning to denoise noisy versions of the data across multiple noise levels (Sohl-Dickstein et al., 2015; Kadkhodaie & Simoncelli, 2020; Ho et al., 2020). While there is more than one way to describe DMs, the essence of the process involves a predetermined forward process, which iteratively adds noise with known statistics to the input, and a learned reverse process which aims to \u201cundo\u201d the effects of the added noise.\nFormally, the forward process transforms N -dimensional samples from a data distribution {x0} \u223c p(x0), e.g. natural images, by progressively adding small amounts of Gaussian noise over a number of steps \u039b following a pre-determined variance schedule {\u03c3\u03bb} \u2208 (0, 1) that increases in magnitude with index \u03bb = 1, \u00b7 \u00b7 \u00b7 ,\u039b. Across these steps, noise gradually accumulates to ultimately overwhelm the signal, and the image distribution morphs into a standard N -dimensional multivariate Gaussian\n(Fig. 1A, B; blue). Each step of this process corresponds to the application of a stochastic forward transition operator that maps noisy versions of the data sample x\u03bb\u22121 into noisier versions x\u03bb,\np(x\u03bb | x\u03bb\u22121) = N (x\u03bb; \u221a 1\u2212 \u03c3\u03bbx\u03bb\u22121, \u03c3\u03bbI), (1)\nAnalogously, the reverse process involves a series of applications of a Gaussian reverse transition operator that partially denoises noisy versions of the image x\u03bb+1 into less noisy versions x\u03bb (Fig. 1A, B; red),\np\u03b8(x\u03bb | x\u03bb+1) = N (x\u03bb;\u00b5\u03b8(x\u03bb+1, \u03bb), \u03c3\u03bbI). (2)\nThe mean of the reverse transition operator \u00b5\u03b8(x\u03bb+1, \u03bb) is parameterized by a feedforward neural network that takes as input the noisy image from the previous iteration x\u03bb+1 and the current noise index \u03bb, with a matching, but reversed, variance schedule. The network learns this iterative mapping by optimizing a denoising objective that minimizes the reconstruction error, or equivalently, the L2\ndistance between the estimated and true noise present in the image: L = E\u03bb\u223c[1,\u039b],x0, \u03f5\u03bb [ \u2225\u00b5\u0303\u03bb(x\u03bb, \u03f5\u03bb)\u2212 \u00b5\u03b8(x\u03bb, \u03bb)\u222522 ] (3)\n= E\u03bb\u223c[1,\u039b],x0, \u03f5\u03bb [ \u2225\u03f5\u03bb \u2212 \u03f5\u03b8(x\u03bb, \u03bb)\u222522 ] , (4)\nwhere \u03f5\u03bb \u223c N (0, I), \u00b5\u0303\u03bb is the mean of the forward transition operator at step \u03bb when conditioned on a clean image x0, and \u03f5\u03b8 is the network\u2019s estimate of the noise present in the noisy image. This objective is equivalent to learning the flow field associated with the score of the reverse transition operator at every noise level \u2207x\u03bb log p(x\u03bb | x\u03bb+1) (Vincent, 2011). At the end of training, the model can generate novel sample images from the same distribution, by starting with i.i.d. multivariate Gaussian noise and iteratively applying the reverse transition operator following the same variance schedule. Each run of this procedure generates a new independent image sample.\nA diffusion-based circuit for sampling-based probabilistic computation. Several properties of diffusion models seem appealing from a biological perspective. In particular, the iterative nature of the operations make them naturally map into recurrent circuit dynamics. In this view, the data distribution follows a nonlinear manifold attractor, embedded in the N -dimensional space of sensory features encoded by the network (not necessarily image pixels). The forward process injects noise that pushes data samples away from the manifold, generating signals for learning, while the reverse process implements denoising. Here, we chose to focus specifically on the circuit dynamics and the mechanism for sample generation. We leave the learning aspect of the problem for future work.\nA recurrent circuit can implement something akin to the DM reverse process, by assuming that the the nonlinear reverse operator p\u03b8(x\u03bb | x\u03bb+1) is computed in the dendritic trees of a set of N cortical pyramidal neurons. At each time step, nonlinear dendritic computations (Poirazi et al., 2003; Polsky et al., 2004; Jadi et al., 2014; Ujfalussy et al., 2018) yield somatic currents that reflects the mean of the Gaussian transition operator \u00b5\u03b8, which is subsequently corrupted by somatic noise. As an analogue for the variance schedule, we introduce a global time-varying signal \u03b2t \u2208 (0, 1) which modulates the nonlinear operation of the dendrites and the magnitude of somatic noise. Mechanistically, this type of modulations could be implemented by local inhibitory subunits (Murayama et al., 2009; Roux & Buzsa\u0301ki, 2015). We adopt the convention of associating increasing \u03b2t with a reduction in noise, although this choice is somewhat arbitrary. Scalar \u03b2t is a sinusoid which fluctuates over time between 0 and 1, in anti-phase to the DM noise schedule \u03c3\u03bb, to account for the fact that downstream circuits read out information at the peak of the local oscillation (Savin et al., 2014):\n\u03b2t = 1\n2\n( 1 + cos ( 2\u03c0t\nT\n)) = 1\u2212 \u03c3\u03bb \u2212 \u03c31\n\u03c3\u039b \u2212 \u03c31 , (5)\nwhere T is the number of time steps in a cycle. Putting everything together, recurrent circuit dynamics take the form : xt+1 = f(xt, \u03b2t)+ (1\u2212\u03b2t)\u03f5t, where f(\u00b7, \u00b7) reflects the nonlinear function computed by the dendrites, itself modulated by the global oscillation, and \u03f5t \u223c N (0, I) are i.i.d. samples from an isotropic Gaussian. Following Poirazi et al. (2003), dendritic computations are modeled using a tree-structured artificial neural network (ANN), whose parameters we train via gradient descent using the standard DM denoising objective (Fig. 1D). Unlike previous ANN-based models of dendritic processing, we use ReLU nonlinearities to facilitate training, with the shape of the dendritic tree parameterized by its depth and branching factors at each level.1 This biologically-motivated solution represents an architectural departure from standard DMs, which assume all-to-all connectivity. The ability to represent priors is relatively robust to variations in tree geometry, although learning speed can vary with architecture. In particular, varying tree depth and width while keeping the total number of parameters constant identifies shallow architectures as the fastest learners in our example (see Suppl. B.1). This implies that one can determine an optimal architecture for encoding any given prior, with potential implications for cortical dendritic morphology.\nAnother key distinction between traditional DMs and biology is the initial condition for the reverse process during sampling. In DMs, initial conditions are drawn independently from a standard Gaussian, which would yield unrealistic state jumps in the circuit dynamics. In contrast, in the recurrent circuit the same neural units need to support the dynamics at all times. As a result, we use the same neural transition operator, p\u03b8(xt | xt\u22121) := N (xt;\u00b5\u03b8, (1\u2212 \u03b2t)I), but under an increasing\n1Unless otherwise specified, simulations use a depth of 7 and branching factor of 3, except in the most proximal section, which has a branching factor 4.\nnoise schedule to reinitialize the reverse process and generate another sample from the prior (Fig. 1E). If we were to use the DM forward process for this operation, the end effect would be statistically equivalent to the original (since noise dominates the signal, by construction, the initial conditions are forgotten). The proposed neural dynamics do not necessarily come with the same mathematical guarantees, but they do significantly simplify the circuit\u2019s operations. In this scheme, sampling from the prior entails running the same stochastic nonlinear recurrent dynamics, at a pace set by an oscillatory \u03b2t signal, with one new sample from the prior generated once per cycle (Fig. 1F). 2 The precise structure of these oscillations can be relaxed to account for frequency and amplitude fluctuations seen biologically (Suppl. Sec. B.5).\nWe numerically tested the quality of the samples generated by our neural circuit in a toy example of a two-dimensional nonlinear manifold (shaped as a \u201cswiss-roll\u201d, see Fig. 1D inset) with linear dimensionality 3, embedded in an ambient feature space with dimensionality N = 10. This simple example both captures the essence of priors of interest (i.e., nonlinear low-dimensional manifold embedded in a larger feature space) and is simple enough to exploit its structure when assessing the quality of the generated samples, something which is generally difficult for complex distributions (Theis et al., 2016). First, we tested the effects of the neural approximation on the autocorrelation function of the generated samples and found that this remains steadily around zero even with the shortest lags (Fig. 1G, left), proving that, despite the approximation, the generated samples are essentially independent. Second, to test the variability of the generated samples we numerically estimated the density along the curved axis of the manifold, which\u2014by construction\u2014has a uniform distribution. We found that the empirical density constructed using the neural samples closely matched the ground truth (Fig. 1G). This is also true for the marginal along the orthogonal dimension (Suppl. B.2). While the quality of samples if harder to estimate, we also find good quality representations of a high dimensional prior trained on the MNIST dataset () (see Suppl.Sec. B.6). Overall, these results indicate that the constraints imposed by biology may have a minimal effect on the quality of neural sampling as compared to traditional DMs.\n2In all simulations we initialize the dynamics randomly and discard the first cycle as burn-in period."
        },
        {
            "heading": "3 FLEXIBLE INFERENCE",
            "text": "Using the same prior-encoding circuit for inference. Machine learning has proposed several algorithmic solutions for combining DM-based prior representations with different likelihood functions, with practical applications such as in-painting, super-resolution, compressed sensing, or class-specific conditioning (Sohl-Dickstein et al., 2015; Dhariwal & Nichol, 2021; Kadkhodaie & Simoncelli, 2021; Song & Ermon, 2019; Ho & Salimans, 2021). In order to maximize the flexibility of our circuit implementation, we take advantage of a modular approach that reuses the dynamics that sample from the prior to do flexible inference. In particular, posterior samples are constructed by altering the flow-fields of the DM-prior, in the form of state-dependent additive changes to the DM reverse transition operator (Sohl-Dickstein et al., 2015; Dhariwal & Nichol, 2021).\nIn general, our approach entails defining a posterior neural transition operator which reflects how additional evidence changes the beliefs about the represented latent variables (Fig.2A). Formally, posterior sampling dynamics take the form (see Suppl. A.1 for derivation):\np(xt | xt\u22121,y) := N (\u00b5\u03b8(xt\u22121) + \u03b3(1\u2212 \u03b2t)gt(xt\u22121,y), (1\u2212 \u03b2t)I), (6)\nwhere the effects of the prior are reflected in the mean \u00b5\u03b8, as before, but shifted by the (log)likelihood gradient gt(xt\u22121,y) = \u2207xt\u22121 log p\u03d5(y | xt\u22121), which, unlike (Dhariwal & Nichol, 2021), is evaluated with respect to a noisy data sample at the previous time step xt\u22121. Scalar \u03b3 is a hyperparameter that weighs the relative contribution of the log prior and log likelihood. This hyperparameter is set to 1 in Sohl-Dickstein et al. (2015) and varied in Dhariwal & Nichol (2021) to optimize a trade-off between sample fidelity and diversity (here, \u03b3 = 1). The relative influence of the additional evidence on the circuit dynamics varies across the cycle, scaled by (1 \u2212 \u03b2t). Mechanistically, this solution means that the local recurrent circuit representing the prior receives an external input drive, itself gated by the global oscillation, which shifts circuits dynamics to sampling from the corresponding posterior. The nature of the input signal is specific to each of the sources of evidence, and implemented in a separate feedforward sub-circuit, which interacts bi-directionally with the primary recurrent network. Functionally, this sub-circuit implements the computation of gt(xt\u22121,y) (Fig. 2B). The functional form of this circuit is problem specific, determined either in closed form (see sensory inference example below), or itself learned (e.g., in the context cue example). Different sources of evidence can operate in parallel by providing their own additive contribution to the circuit drive.\nInferring latents from sensory evidence. As a first concrete example of inference, we show a simplified version of a sensory perception task in which the input is only partially observed and the corresponding features need to be inferred given this incomplete observation (as would be the case in occlusion, for instance). In particular, we take the sensory observation to induce a linear constraint on the latent variable (N = 2 for easy visualization), M\u22a4s (x\u2212 xc) = 0, parameterized by a unit norm linear operator Ms and offset xc, with Gaussian-distributed uncertainty around it (Fig.2B, right). Formally, the corresponding likelihood score is orthogonal to the constraint manifold and linearly rescaled by distance to it, gt(xt\u22121, s) = 1\u03c32s MsM \u22a4 s (xt\u22121 \u2212 xc); parameter \u03c3s defines the degree of sensory uncertainty. As was the case for the prior, the DM implementation of this inference problem generates samples starting from the isotropic N -dimensional Gaussian, with the posterior reverse dynamics bringing the samples towards the posterior, here concentrated at the intersection of the two manifolds corresponding to the prior and likelihood, respectively (Fig. 2C, top). The neural posterior dynamics find a similar solution (Fig. 2C, bottom), but with initial conditions reflecting the likelihood. This is due to the sequential sampling process which has the same dynamics operating in both forward and reverse phases of the oscillation, and to the fact that the likelihood is gated oppositely to the prior (due to the 1\u2212 \u03b2t scaling, see Eq. 6). When it comes to inference, the sampling dynamics of the neural circuit approximate DM posterior dynamics in two important ways. First, as for the prior, they introduce sequential effects due to the continuous use of the same operator across all phases of the cycle. Second, the computation of the likelihood is itself approximate, as gt is evaluated at point xt\u22121 instead of the \u00b5\u03b8(xt\u22121) required in the original posterior flow field derivation (Dhariwal & Nichol, 2021). This switch was needed to account for temporal causality and the fact that the sensory area providing this input does not have access to local somatic currents \u00b5\u03b8(xt\u22121). In principle, both effects should diminish with a finer discretization of time, but we also numerically quantified the effects of each approximation individually (Fig. 2E, top) and together (Fig. 2E, bottom). In particular, we collected samples\ngenerated according to either the DM reverse operator (using \u00b5\u03b8(xt\u22121)) or the neural posterior sampler (using xt\u22121). We quantified the similarity of the resulting sample distributions as a function of \u03b2t, using a naive KL divergence estimator (10000 samples each, with space discretized into 225 bins). We found that discrepancies between the distributions are largest at the start of the reverse process. In this regime, the noise and flow fields are large leading to substantial differences between xt\u22121 and \u00b5\u03b8(xt\u22121) (Fig. 2E, top). In principle, this might lead to catastrophic accumulation of errors and large sampling biases; however, the attractor dynamics prevent this from happening. The effects of the approximation reduce over the cycle, with the KL divergence approaching zero. We see largely the same effects when the neural likelihood approximation is used in conjunction with the neural prior approximation (same operator throughout; Fig. 2E, bottom). Overall, these numerical results suggest that, at least in the simple toy examples considered here, the effects of neural approximations on inference are minimal.\nInference with multiple sources of information. The main benefit of separating the neural substrates implementing the prior from the computation of the likelihood is that it makes inference flexible. Gating signals from different sources of evidence allows the model to reuse the same prior information across multiple tasks. Beyond the sensory example above, another important source of evidence that shapes perception/inference comes from contextual priors, reflecting expectations about which of the latent features are likely to occur (e.g. attention). To illustrate this idea, we introduce a discrete context random variable, c, whose value leads to different expectations about the latent variable x, formally making the prior a mixture of 1D manifolds, with data uniformly distributed along each (Fig. 3A, top).3 This additional complexity does not affect the neural dynamics sampling from this prior (Fig. 3A, bottom), but allows us to demonstrate richer inference scenarios.\nSampling from the context conditioned posterior p(x|c) proceeds very similarly to the sensory inference example above, but the drive to the circuit is given by a separate top-down feedforward sub-circuit which implements a different gt(xt\u22121, c) = \u2207x log p\u03d5(c | xt\u22121)|x=xt\u22121 . 4 The corresponding flow field and posterior samples are shown in (Fig. 3B), where we see the dynamics being strongly attracted to the region of the latent space corresponding to the cued context (the \u2018S\u2019 manifold). Finally, top-down contextual signals can be combined with the bottom-up sensory evidence (Fig. 3C) to jointly constrain the posterior distribution from which we draw samples (Fig. 3D). When multiple sources of evidence, y(i), are available, their net effect on the likelihood combines additively, gt(xt\u22121,y) = \u2211 i \u03b3ig (i) t (xt\u22121, y\n(i)). This modular parametrization is at the root of the circuit\u2019s inferential flexibility, with individual components gated in or out depending on availability of information and current task demands (Womelsdorf et al., 2014; Kuchibhotla et al., 2017)."
        },
        {
            "heading": "4 NEURAL SIGNATURES",
            "text": "One unique feature of the inferential process proposed here is how the phase of the circuit oscillation modulates the relative contribution of different inputs to the circuit. At one extreme (\u03b2t = 0), the effect of the local attractor dynamics is weak and the corresponding prior unstructured, with neural activity dominated by the likelihood component. At the other extreme (\u03b2t = 1), the effect of the likelihood on the dynamics is weak, and neural activity corresponds to samples from the posterior (Fig. 4A; see also Eq. 6). This dialing up and down of different sources of evidence opens the door for experimental validation of the model. Concretely testing such dynamics involves assessing patterns of population activity during perception, at different phases of the dominant circuit oscillation, as samples from the intermediate posterior. Samples from the prior p(x) can be obtained from spontaneous activity, in the absence of a sensory stimulus and outside a specific task context, while a reasonable measure of the effects of the likelihood is given by the activity at the lowest phase of the posterior dynamics, p(xt|y, \u03b2t = 0). Given these statistics, the change in the contribution of the likelihood and the prior on this posterior can be estimated as the KL divergence between the activity of the circuit during inference, and these two reference distributions. The precise assessment of differences between potentially complex and structured distributions based on empirical measurements may seem impractical, but there is precedent in the literature for estimating KL divergences in population activity statistics using coarse discretizations of the space (e.g. temporal\n3The ambient space is 2D, for simplicity. 4Note here that the direction of the conditioning is switched from the natural causal one, which means that\nthis map can be learned from labeled examples by optimization (see Suppl. A.1 for derivation).\nbinning and thresholding of multiunit activity) (Berkes et al., 2011). For simplicity, here we draw 105 from each distribution, discretize the latent space into 400 bins and compute a naive estimate just based on the resulting histograms, although more sample efficient estimators are certainly available (Nemenman et al., 2001). Despite the coarseness of the measure, the intuition about the effects of \u03b2t holds: population activity statistics become increasingly similar to the prior, and dissimilar from the likelihood, over the ascending phase of the oscillation (Fig. 4B). With a more precise understanding of the mechanics of gating different type of signals into the circuit, one could further envision causal manipulations that precisely bias the outcome in one or the other direction."
        },
        {
            "heading": "5 DISCUSSION",
            "text": "Natural sensory inputs are often structured into complex nonlinear manifolds in high dimensions. How does the brain represent such prior knowledge and uses it across tasks? Conceivably, one\ncould solve this problem by learning a mapping between stimuli and responses de novo for each task. However, a more efficient representation would consolidate and leverage universally useful information across tasks. Here, we have proposed a circuit implementation of across-task inference in which a common prior is encoded in the form of a recurrent neural circuit with dendritic nonlinearities optimized for denoising. The stochastic dynamics of this circuit provide samples from the prior at the peak of an ongoing local oscillation, while additional inputs (carrying sensory or contextual information) switch the dynamics to sampling from the corresponding posterior. This solution offers a first circuit model for reusing priors across tasks, as seen behaviorally (Houlsby et al., 2013), and has measurable neural signatures based on neural population recordings in animals.\nTheories of probabilistic brain computation based on neural sampling suffer from two main practical challenges. The first is time, as estimating posterior expectations for decision making requires integrating samples over time (Rulla\u0301n Buxo\u0301 & Savin, 2021). This makes the speed of sampling a key constraint on plausible neural sampling dynamics (Hennequin et al., 2014; Savin & Dene\u0300ve, 2014; Aitchison & Lengyel, 2016; Masset et al., 2022). Second, although in principle sampling makes most sense for distributions with complex structure that cannot be well-represented parametrically (Fiser et al., 2010), in practice circuit models of sampling tend to be restricted to relatively simple distributions (Chen et al., 2023). The dendritic nonlinearities and DM-inspired oscillatory sampling schedule in our solution naturally overcome both limitations: by construction, the circuit is designed to sample from complex distributions, e.g. involving low dimensional nonlinear manifolds embedded in high dimensional spaces, and mixture distributions with low probability gaps between the components, which pose fundamental challenges for classic Markov Chain Monte Carlo approaches (Aitchison & Lengyel, 2016). Also by construction, DMs are designed to provide independent samples from the target distribution at the end of each reverse run, something that our neural circuit approximation seems to preserve to a large degree.\nThe specific role of oscillations in sampling resembles a previous model of sampling in the hippocampus (Savin et al., 2014) inspired by tempered transitions (Neal, 1996). In this view, \u03b2 functions as an inverse temperature annealing the target distribution so as to reduce the gap (and thus increase the likelihood of transitions) between modes; this may inform the theoretical analysis of the sampler\u2019s properties. Additionally, theta oscillations naturally set the tempo in the hippocampus, which may instruct the experimental validation of our proposal. The concrete approach for testing our experimental prediction was strongly influenced by Berkes et al. (2011), who originally used KL divergences to measure the calibration of (sampling based) probabilistic models of early vision over development. While segregating the responses by oscillatory phase brings additional data and estimation challenges, the Berkes result, together with the evidence of hippocampal theta phase sampling from Savin et al. (2014), provides reassurance that experimental validation of the model might be possible in practice.\nWhile our proposed solution relies on nonlinearities in the dendrites as an implementation of the reverse operator, it is not clear that this is the only way to map DM operations to biology. In principle, each of our units could be themselves a small feedforward subnetwork, recursively interacting between them. This version has the advantage of fewer restrictions on the architecture, but requires additional biological accommodations. In particular, for consistency noise would need to be distributed equally in all the neurons of the network (as opposed to just the last layer in DMs and our solution). Computationally, this solution remains viable (Suppl. B.4), although the architecture is arguably more contrived. Models of learning might better distinguish between these architectures in the future.\nHere, we have purposefully abstracted away the nature of the features, so that the modeled circuit could be any processing stage in the cortical hierarchy, receiving evidence from one or several sensory domains bottom-up, and contextual top-down information from higher areas. This modular architecture opens the way for hierarchical probabilistic representations, by stacking such recurrent circuits with appropriate conditioning between them (Qiang et al., 2023). Such an architecture would also help limit the computational complexity of the neural nonlinearities involved in any stage: pixel-level DMs models of images are huge, with parameters on the order of billions (Ramesh et al., 2021; Saharia et al., 2022), but models of abstract image features are likely more tractable. Future work on hierarchical sampling neural circuits might lead not only to better accounts of brain representations but also to more compact machine learning models.\nReproducibility Statement We have included a link to an anonymized code repository that contains all the code for implementing and training the proposed model. This repository also contains jupyter\nnotebooks that can recreate all main text and supplementary figures. The datasets used in our model can be generated using the code provided.\nhttps://anonymous.4open.science/r/neuralmodel"
        },
        {
            "heading": "A SUPPLEMENTARY DERIVATIONS",
            "text": ""
        },
        {
            "heading": "A.1 POSTERIOR SAMPLING DYNAMICS",
            "text": "We modify the original posterior flow field derivation by Sohl-Dickstein et al. (2015) to allow for a biologically plausible posterior sampling algorithm. We wish to derive the posterior neural transition operator p(xt | xt\u22121,y) by combining the prior neural transition operator p(xt | xt\u22121) with an arbitrary likelihood signal p\u03d5(y | xt). Recall that our prior neural transition operator is Gaussian:\np(xt | xt\u22121) = N (\u00b5\u03b8, (1\u2212 \u03b2t)I)\nlog p(xt | xt\u22121) = \u2212 1\n2(1\u2212 \u03b2t) \u2225xt \u2212 \u00b5\u03b8,t\u22252 + C\nwhere C is a constant. Under some mild assumptions, we can approximate the log likelihood log p\u03d5(y | xt) by Taylor expanding around xt = \u00b5\u03b8,t, where \u00b5\u03b8,t = \u00b5\u03b8(xt\u22121):\nlog p\u03d5(y | xt) \u2248 log p\u03d5(y | xt) \u2223\u2223 xt=\u00b5\u03b8,t + (xt \u2212 \u00b5\u03b8,t)\u2207xt log p\u03d5(y | xt) \u2223\u2223 xt=\u00b5\u03b8,t\n= (xt \u2212 \u00b5\u03b8,t)gt(xt\u22121,y) + C1\nwhere the likelihood flow field gt(\u00b5\u03b8,t,y) = \u2207xt log p\u03d5(y | xt) \u2223\u2223 xt=\u00b5\u03b8,t\nand C1 is a constant. Note that in this derivation, the gradient of the log likelhood is evaluated at \u00b5\u03b8(xt\u22121), the local somatic current at the current time step. This makes a direct biological interpretation of the model problematic since the sensory area providing this likelihood signal cannot access the somatic current and only has access to information from the previous time step. In order for our model to obey temporal causality and local access to information, we suppose instead that the likelihood flow field receives as input a point xt\u22121 from the previous time step which is used to evaluate the likelihood flow field gt(xt\u22121,y) = \u2207x log p\u03d5(y | xt\u22121)|x=xt\u22121 . This approximation becomes more exact the closer we are to the data manifold: since \u00b5\u03b8(xt\u22121) can be thought of as the prior network\u2019s current guess of a point on the prior manifold, the error from substituting xt\u22121 for \u00b5\u03b8,t goes to zero as xt approaches the prior manifold. We demonstrate this numerically in Fig. 2E.\nThe posterior neural transition operator combines the prior transition operator and likelihood signal using Bayes rule:\nlog p(xt | xt\u22121,y) = log p\u03b8(xt | xt\u22121) + log p\u03d5(y | xt) + C1\n= \u2212 1 2(1\u2212 \u03b2t) \u2225xt \u2212 \u00b5\u03b8,t\u22252 + (xt \u2212 \u00b5\u03b8,t)gt + C2\n= \u2212 1 2(1\u2212 \u03b2t) \u2225xt \u2212 \u00b5\u03b8,t \u2212 (1\u2212 \u03b2t)gt\u22252 + g2t (1\u2212 \u03b2t) 2 + C2\n= \u2212 1 2(1\u2212 \u03b2t) \u2225xt \u2212 (\u00b5\u03b8,t + (1\u2212 \u03b2t)gt)\u22252 + C3\n= logN (xt;\u00b5\u03b8,t + (1\u2212 \u03b2t)gt, (1\u2212 \u03b2t)I) + C4\nwhere C1\u22124 are constants that do not contain xt. As we can see, the posterior transition operator is again a Gaussian and C4 corresponds to its normalizing constant. The effect of the external likelihood signal is to shift the mean of the prior transition operator by an amount (1\u2212 \u03b2t)gt. To sample from the context conditioned posterior p(x | c), we learn a map from xt to c by training a feedforward neural network to classify points xt via optimization. For the trimodal prior in Fig. 3, this is done by assigning one of three class labels to samples from the corresponding mode. We construct the classifier training set by adding i.i.d. multivariate Gaussian noise of varying degrees of variance to clean samples from the prior using the DM forward process. As with the unimodal swiss-roll prior, at large degrees of noise most of the signal is removed and the distribution resembles a white Gaussian. The classifier network is then trained via backpropagation to receive xt as input and identify the class of the corresponding clean data sample. Since the last layer of the classifier is a Softmax layer that normalizes the outputs to one, we interpret the values of the classifier output as the probability distribution p(c | xt). gt(xt\u22121, c) is evaluated using the torch autograd package."
        },
        {
            "heading": "B SUPPLEMENTARY RESULTS",
            "text": ""
        },
        {
            "heading": "B.1 EFFECTS OF DENDRITIC TREE MORPHOLOGY",
            "text": "We studied how the morphology of the dendritic networks affected the speed of learning by conducting a grid search over dendritic architectures that differed in width and depth. To isolate the effect of the architecture choice, we kept the total number of parameters roughly the same across all networks, meaning that as we increased network depth we also decreased network width.\nA dendritic architecture can be fully specified by an array of branching factors, which denotes the number of children nodes at each layer. The array gives branch factors in order of distal to proximal layers. The six architectures are defined as follows:"
        },
        {
            "heading": "Depth Morphology Num. of parameters",
            "text": "2 [59, 59] 35048 4 [8, 8, 7, 7] 33154 5 [5, 5, 5, 5, 5] 34372 6 [4, 4, 4, 4, 4, 3] 34814 7 [3, 3, 3, 3, 3, 3, 4] 34986\n10 [2, 2, 2, 2, 2, 2, 2, 2, 3, 3] 32234\nTable S1: Morphology of dendritic networks.\nWe trained four iterations of each network architecture with different random seeds. The networks were trained on the 2D swiss roll manifold using the Adam optimizer with a learning rate of 3e-4 over 1.5e6 epochs. We quantified the similarity of the network to the ground truth manifold as a function of training epoch using a naive KL divergence estimator (10000 samples each, with space discretized into 255 bins).\n0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 training epochs 1e6\n0\n2\n4\n6\n8\n10\n12\n14\n16\nKL d\niv er\nge nc\ne\n2 4 5 6 7 10\nFigure S1: Network performance over training. Each line corresponds to the performance of a model architecture of a particular depth. We see that the KL divergence of shallower networks (depths 2, 4) decreased the fastest, while deeper network converges slower. Note that the speed of learning does not scale monotonically with network depth; for example, the 7-layer network learns faster than the 5 layer network.\nWe see that shallower but wider networks learned faster than deeper but thinner networks, and converged to a KL divergence value much lower than the other networks. However, we cannot conclude that shallower networks perform better overall: the KL divergence value for the 6 layer network even towards the end of the 1.5e6 training epochs indicates that the networks have not reached the global minimum of their loss functions, and suggests that with more training epochs the network performance may continue to improve."
        },
        {
            "heading": "B.2 TESTING NEURAL SAMPLING QUALITY BY MARGINAL STATISTICS",
            "text": "The data manifold on which the model in Fig. 1A was trained has three embedding dimensions but is embedded in a 10-dimensional ambient space (using the terminology in Jazayeri & Ostojic (2021)). We ensured the data have non-zero values in every dimension by rotating the three dimensional manifold by \u03c0/4 around each pair of axes.\nThe three dimensional swiss-roll manifold is uniformly distributed along the linear dimension (i.e. the dimension orthogonal to the swiss-roll cross-section). We evaluated the marginal statistics of the model output along this dimension by comparing the cumulative distribution function (cdf) of this model output against a reference uniform cdf.\nTo recover the model\u2019s marginal density along the linear dimension, we first projected the network output onto the three basis vectors of the embedding space, which we calculated by transforming unit vectors by the same rotation operators as the data manifold. We then projected the model output along the third basis vector and computed the cdf along this dimension.\nThe empirical cdf is close to the reference cdf, indicating that the data are likely distributed according to the uniform distribution. We can quantify this using the Kolmogorov-Smirnov goodness-of-fit test, which is a nonparametric measure of the similarity between a continuous empirical sample distribution and a specified reference distribution. We applied the KS test to both the empirical distribution along the nonlinear dimension in Fig. 1G and along the linear dimension in Fig. S2. For a sample size of 1000 each, the p-value along the nonlinear dimension is 0.145 while the p-value along the linear dimension is 0.125, making them statistically indistinguishable from the uniform distribution.\n0.4 0.2 0.0 0.2 0.4\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0 reference cdf empirical cdf\nFigure S2: Marginal statistics along the dimension orthogonal to the swiss roll cross-section."
        },
        {
            "heading": "B.3 POSTERIOR AUTOCORRELATION",
            "text": "We numerically evaluated posterior samples generated by the neural circuit. We see in Fig. S3 that the samples generated from neural sampling result in autocorrelation functions that rapidly decrease and remains steadily at zero. This indicates that the posterior samples, like the prior samples in Fig. 1G, are largely independent over time despite the approximations. This remains true even when the neural sampling dynamics are influenced by contextual and sensory evidence."
        },
        {
            "heading": "B.4 STOCHASTIC NEURAL NETWORK",
            "text": "To assess whether the nonlinear operations of the neural sampling process could be instantiated by a population of neurons rather than in the dendritic arbor of pyramidal neurons, we trained a Stochastic Neural Network (SNN) model, a fully connected feedforward neural network comprising neurons with linear weights, ReLU nonlinearity and somatic stochasticity. As in our dendritic model, the somatic stochasticity in each neuron is additive Gaussian, with the degree of stochasticity modulated by the phase of the global oscillation. The operation performed by the neurons in each hidden layer\nFigure S3: Autocorrelation comparison of the posterior samples. (A) Posterior samples generated from the two dimensional trimodal dataset as a result of observing contextual evidence (left), sensory evidence (middle), and both forms of contextual and sensory evidence (right). The context cue for the context-conditioned posterior is the discrete context random variable c2 indicating the swiss roll manifold. The sensory likelihood, as before, is a two dimensional linear constraint with a Gaussian-distributed uncertainty around it. (B) The autocorrelation functions of samples generated by the DM reverse process (top row) and neural sampling (middle row) for the different posterior distributions. The difference between the two autocorrelation functions is shown in the bottom row.\ncan be expressed as hl = ReLU(Wl(hl\u22121, t)) + (1\u2212 \u03b2t)\u03f5t, (7)\nwhere hl is the output of hidden layer l and Wl is the weight matrix at layer l. \u03f5t is a sample of a Gaussian with identity covariance.\nOur instantiation of this model architecture has three hidden layers, each with a width of 32 neurons. The degree of noise as given by the oscillator \u03b2t is concatenated with the output of the previous hidden layer. We trained the model via backpropagation using the Adam optimizer with a learning rate of 1e-4 over 5000 epochs. We show the distribution of samples generated at intermediate points of the ascending phase below, in Fig. S4.\nFigure S4: DM reverse process using the SNN architecture. The samples generated by the SNN result in samples that lie on the two dimensional swiss roll manifold."
        },
        {
            "heading": "B.5 MORE BIOLOGICALLY REALISTIC OSCILLATIONS",
            "text": "In the brain, oscillations fluctuate in both amplitude and frequency and do not have the perfect sinusoid structure assumed by the model. To test the effects of such fluctuations on the circuit\u2019s\nability to sample from priors, we introduced variability in the amplitudes and/or period of the \u03b2t oscillation. The original oscillation cycled between 0 and 1 with a period of 200 timesteps (Fig. S5A). In these variations, the amplitude and period of the oscillation were defined as random variables. The amplitude was drawn from a truncated normal distribution A \u223c N (Aoriginal, \u03c3A), A > 0, i.i.d for each cycle (Fig. S5B), where samples were obtained by rejection sampling. The period of each cycle was similarly drawn from another truncated normal distribution T \u223c N (Toriginal, \u03c3T ), T > 0 (Fig. S5C). Finally, we also considered the combined effect of simultaneous fluctuations in both amplitude and frequency (Fig. S5D). Using the denoising-trained 2D swiss-roll model, we used the induced \u03b2t values for circuit sampling. We assessed the effect of the generated samples by comparing the empirical distribution given by the resulting samples against the ground truth distribution, via the KL divergence. We find that the quality of the samples is largely unaffected by the deviations from the perfect sinusoid (Fig. S5E). The KL divergence remains largely flat for a range of noise levels. This suggests that the sampling procedure can be reasonably robust for naturalistic oscillatory signals.\n0.00 0.05 0.10 0.15 0.20 magnitude of fluctuations (s.d./mean)\n0.04\n0.05\n0.06\n0.07\n0.08\n0.09\n0.10\nKL d\niv er\nge nc\ne\nrandom periods random amplitudes both random\n0.0\n0.5\n1.0\n0.0\n0.5\n1.0\n0 250 500 750 1000 1250 1500 1750 2000\ntimestep\n0.0\n0.5\n1.0\n0.0\n0.5\n1.0\nbe ta\nA\nB\nC\nD\nE\nFigure S5: (A) Reference \u03b2t fluctuations for the simple model. (B) Example draw for a more naturalistic oscillations with fluctuations in amplitude (\u03c3A = 0.15). (C) Example draw for a more naturalistic oscillations with fluctuations in period (\u03c3T = 30). (D) Example draw for a more naturalistic oscillations with fluctuations in amplitude (\u03c3A = 0.15 and \u03c3T = 30). (E) Deviations of empirical samples from ground truth distribution for varying degrees of oscillation fluctuations. A standard deviation corresponds to the deterministic setting. KL estimates use 5000 samples per run, with shading showing SEM estimated over 20 runs."
        },
        {
            "heading": "B.6 MNIST DATASET",
            "text": "To assess whether our model scales to higher dimensional priors, we trained our circuit model on the MNIST dataset, which contains 28 by 28 pixel images of handwritten digits. We were able to train several dendritic architectures to successfully generate digits from the MNIST dataset (Fig.S6, top two rows). They were trained by the same denoising objective, using the Adam optimizer with a learning rate of 4e-3 for 5000 epochs. The best performing architectures were two and three layer architectures with branch factors [12, 12] and [5, 5, 5] respectively.\nWe sampled a class-conditioned posterior by combining the model with a separate classifier trained on MNIST digits (Fig.S6, top two rows). We used approximate posterior dynamics described in Section 3 and described in Supplementary section A.1. Taken together, these MNIST results suggest that the biologically-motivated deviations from traditional diffusion models can scale to higher dimensional distributions.\nA\nB\nFigure S6: Results for an an MNIST trained neural circuit. (A) Example prior samples, one per class. (B) Example samples from the posterior conditioned on class \u20183\u2019."
        }
    ],
    "title": "COMPLEX PRIORS AND FLEXIBLE INFERENCE IN RECURRENT CIRCUITS WITH DENDRITIC NONLINEARITIES",
    "year": 2023
}