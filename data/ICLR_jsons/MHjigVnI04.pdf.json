{
    "abstractText": "We rigorously study the joint evolution of training dynamics via stochastic gradient descent (SGD) and the spectra of empirical Hessian and gradient matrices. We prove that in two canonical classification tasks for multi-class high-dimensional mixtures and either 1 or 2-layer neural networks, the SGD trajectory rapidly aligns with emerging low-rank outlier eigenspaces of the Hessian and gradient matrices. Moreover, in multi-layer settings this alignment occurs per layer, with the final layer\u2019s outlier eigenspace evolving over the course of training, and exhibiting rank deficiency when the SGD converges to sub-optimal classifiers. This establishes some of the rich predictions that have arisen from extensive numerical studies in the last decade about the spectra of Hessian and information matrices over the course of training in overparametrized networks.",
    "authors": [],
    "id": "SP:4a4db4de6385894a8f1ce56c6ea487e7d20d0472",
    "references": [
        {
            "authors": [
                "Greg W Anderson",
                "Alice Guionnet",
                "Ofer Zeitouni"
            ],
            "title": "An introduction to random matrices",
            "venue": "Number 118. Cambridge university press,",
            "year": 2010
        },
        {
            "authors": [
                "Antonio Auffinger",
                "G\u00e9rard Ben Arous"
            ],
            "title": "Complexity of random smooth functions on the highdimensional sphere",
            "venue": "Ann. Probab.,",
            "year": 2013
        },
        {
            "authors": [
                "Antonio Auffinger",
                "G\u00e9rard Ben Arous",
                "Ji\u0159\u0131\u0301 \u010cern\u00fd"
            ],
            "title": "Random matrices and complexity of spin glasses",
            "venue": "Comm. Pure Appl. Math.,",
            "year": 2013
        },
        {
            "authors": [
                "Jinho Baik",
                "G\u00e9rard Ben Arous",
                "Sandrine"
            ],
            "title": "P\u00e9ch\u00e9. Phase transition of the largest eigenvalue for nonnull complex sample covariance matrices",
            "venue": "The Annals of Probability,",
            "year": 2005
        },
        {
            "authors": [
                "G\u00e9rard Ben Arous",
                "Song Mei",
                "Andrea Montanari",
                "Mihai Nica"
            ],
            "title": "The landscape of the spiked tensor model",
            "venue": "Comm. Pure Appl. Math.,",
            "year": 2019
        },
        {
            "authors": [
                "Gerard Ben Arous",
                "Reza Gheissari",
                "Aukosh Jagannath"
            ],
            "title": "High-dimensional limit theorems for SGD: Effective dynamics and critical scaling",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "L\u00e9on Bottou"
            ],
            "title": "On-Line Learning and Stochastic Approximations",
            "year": 1999
        },
        {
            "authors": [
                "Anna Choromanska",
                "MIkael Henaff",
                "Michael Mathieu",
                "Gerard Ben Arous",
                "Yann LeCun"
            ],
            "title": "The Loss Surfaces of Multilayer Networks",
            "venue": "Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics,",
            "year": 2015
        },
        {
            "authors": [
                "Jeremy Cohen",
                "Simran Kaur",
                "Yuanzhi Li",
                "J Zico Kolter",
                "Ameet Talwalkar"
            ],
            "title": "Gradient descent on neural networks typically occurs at the edge of stability",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Alexandru Damian",
                "Jason Lee",
                "Mahdi Soltanolkotabi"
            ],
            "title": "Neural networks can learn representations with gradient descent",
            "venue": "Proceedings of Thirty Fifth Conference on Learning Theory,",
            "year": 2022
        },
        {
            "authors": [
                "Yann N. Dauphin",
                "Razvan Pascanu",
                "Caglar Gulcehre",
                "Kyunghyun Cho",
                "Surya Ganguli",
                "Yoshua Bengio"
            ],
            "title": "Identifying and attacking the saddle point problem in high-dimensional non-convex optimization",
            "venue": "In Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2,",
            "year": 2014
        },
        {
            "authors": [
                "Zhou Fan",
                "Zhichao Wang"
            ],
            "title": "Spectra of the conjugate kernel and neural tangent kernel for linearwidth neural networks",
            "venue": "In Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS\u201920, Red Hook, NY, USA,",
            "year": 2020
        },
        {
            "authors": [
                "Jonathan Frankle",
                "Michael Carbin"
            ],
            "title": "The lottery ticket hypothesis: Finding sparse, trainable neural networks",
            "venue": "In International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "Behrooz Ghorbani",
                "Shankar Krishnan",
                "Ying Xiao"
            ],
            "title": "An investigation into neural net optimization via hessian eigenvalue density",
            "venue": "Proceedings of the 36th International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Sebastian Goldt",
                "Madhu Advani",
                "Andrew M Saxe",
                "Florent Krzakala",
                "Lenka Zdeborov\u00e1"
            ],
            "title": "Dynamics of stochastic gradient descent for two-layer neural networks in the teacher-student setup",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Guy Gur-Ari",
                "Daniel A. Roberts",
                "Ethan Dyer"
            ],
            "title": "Gradient descent happens in a tiny subspace, 2019",
            "venue": "URL https://openreview.net/forum?id=ByeTHsAqtX",
            "year": 2019
        },
        {
            "authors": [
                "X.Y. Han",
                "Vardan Papyan",
                "David L. Donoho"
            ],
            "title": "Neural collapse under MSE loss: Proximity to and dynamics on the central path",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Arthur Jacot",
                "Franck Gabriel",
                "Clement Hongler"
            ],
            "title": "Neural tangent kernel: Convergence and generalization in neural networks",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Arthur Jacot",
                "Franck Gabriel",
                "Clement Hongler"
            ],
            "title": "The asymptotic spectrum of the hessian of dnn throughout training",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Yann A. LeCun",
                "L\u00e9on Bottou",
                "Genevieve B. Orr",
                "Klaus-Robert M\u00fcller"
            ],
            "title": "Efficient BackProp, pp. 9\u201348",
            "year": 2012
        },
        {
            "authors": [
                "Xinyan Li",
                "Qilong Gu",
                "Yingxue Zhou",
                "Tiancong Chen",
                "Arindam Banerjee"
            ],
            "title": "Hessian based analysis of SGD for Deep Nets: Dynamics and Generalization, pp. 190\u2013198",
            "venue": "URL https://epubs.siam.org/doi/abs/10.1137/1",
            "year": 1611
        },
        {
            "authors": [
                "Zhenyu Liao",
                "Michael W. Mahoney"
            ],
            "title": "Hessian eigenspectra of more realistic nonlinear models",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Bruno Loureiro",
                "Gabriele Sicuro",
                "Cedric Gerbelot",
                "Alessandro Pacco",
                "Florent Krzakala",
                "Lenka Zdeborova"
            ],
            "title": "Learning gaussian mixtures with generalized linear models: Precise asymptotics in high-dimensions",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Antoine Maillard",
                "G\u00e9rard Ben Arous",
                "Giulio Biroli"
            ],
            "title": "Landscape complexity for the empirical risk of generalized linear models",
            "venue": "Proceedings of The First Mathematical and Scientific Machine Learning Conference,",
            "year": 2020
        },
        {
            "authors": [
                "Charles H. Martin",
                "Michael W. Mahoney"
            ],
            "title": "Traditional and heavy tailed self regularization in neural network models, 2019",
            "venue": "URL https://openreview.net/forum?id=SJeFNoRcFQ",
            "year": 2019
        },
        {
            "authors": [
                "Song Mei",
                "Yu Bai",
                "Andrea Montanari"
            ],
            "title": "The landscape of empirical risk for nonconvex losses",
            "venue": "Ann. Statist., 46(6A):2747\u20132774,",
            "year": 2018
        },
        {
            "authors": [
                "Francesca Mignacco",
                "Florent Krzakala",
                "Pierfrancesco Urbani",
                "Lenka Zdeborov\u00e1"
            ],
            "title": "Dynamical mean-field theory for stochastic gradient descent in gaussian mixture classification",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Marvin Minsky",
                "Seymour Papert"
            ],
            "title": "An introduction to computational geometry",
            "venue": "Cambridge tiass., HIT,",
            "year": 1969
        },
        {
            "authors": [
                "Alireza Mousavi-Hosseini",
                "Sejun Park",
                "Manuela Girotti",
                "Ioannis Mitliagkas",
                "Murat A Erdogdu"
            ],
            "title": "Neural networks efficiently learn low-dimensional representations with SGD",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Vardan Papyan"
            ],
            "title": "Measurements of three-level hierarchical structure in the outliers in the spectrum of deepnet hessians",
            "venue": "Proceedings of the 36th International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Vardan Papyan"
            ],
            "title": "Traces of class/cross-class structure pervade deep learning spectra",
            "venue": "Journal of Machine Learning Research,",
            "year": 2020
        },
        {
            "authors": [
                "Vardan Papyan",
                "X.Y. Han",
                "David L. Donoho"
            ],
            "title": "Prevalence of neural collapse during the terminal phase of deep learning training",
            "venue": "Proceedings of the National Academy of Sciences,",
            "year": 2020
        },
        {
            "authors": [
                "Courtney Paquette",
                "Kiwon Lee",
                "Fabian Pedregosa",
                "Elliot Paquette"
            ],
            "title": "SGD in the large: Averagecase analysis, asymptotics, and stepsize criticality",
            "venue": "In Conference on Learning Theory,",
            "year": 2021
        },
        {
            "authors": [
                "S. P\u00e9ch\u00e9"
            ],
            "title": "The largest eigenvalue of small rank perturbations of hermitian random matrices",
            "venue": "Probability Theory and Related Fields,",
            "year": 2006
        },
        {
            "authors": [
                "Jeffrey Pennington",
                "Yasaman Bahri"
            ],
            "title": "Geometry of neural network loss surfaces via random matrix theory",
            "venue": "Proceedings of the 34th International Conference on Machine Learning,",
            "year": 2017
        },
        {
            "authors": [
                "Jeffrey Pennington",
                "Pratik Worah"
            ],
            "title": "The spectrum of the fisher information matrix of a singlehidden-layer neural network",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Maria Refinetti",
                "Sebastian Goldt",
                "Florent Krzakala",
                "Lenka Zdeborov\u00e1"
            ],
            "title": "Classifying highdimensional gaussian mixtures: Where kernel methods fail and neural networks succeed",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Herbert Robbins",
                "Sutton Monro"
            ],
            "title": "A stochastic approximation method",
            "venue": "Ann. Math. Statistics,",
            "year": 1951
        },
        {
            "authors": [
                "David Saad",
                "Sara Solla"
            ],
            "title": "Dynamics of on-line gradient descent learning for multilayer neural networks",
            "venue": "Advances in neural information processing systems,",
            "year": 1995
        },
        {
            "authors": [
                "David Saad",
                "Sara A Solla"
            ],
            "title": "On-line learning in soft committee machines",
            "venue": "Physical Review E,",
            "year": 1995
        },
        {
            "authors": [
                "Levent Sagun",
                "Leon Bottou",
                "Yann LeCun"
            ],
            "title": "Eigenvalues of the hessian in deep learning: Singularity and beyond, 2017a. URL https://openreview.net/forum?id=B186cP9gx",
            "year": 2017
        },
        {
            "authors": [
                "Levent Sagun",
                "Utku Evci",
                "V. Ugur G\u00fcney",
                "Yann N. Dauphin",
                "L\u00e9on Bottou"
            ],
            "title": "Empirical analysis of the hessian of over-parametrized neural networks",
            "venue": "CoRR, abs/1706.04454,",
            "year": 2017
        },
        {
            "authors": [
                "Yan Shuo Tan",
                "Roman Vershynin"
            ],
            "title": "Online stochastic gradient descent with arbitrary initialization solves non-smooth, non-convex phase retrieval",
            "year": 1910
        },
        {
            "authors": [
                "Rodrigo Veiga",
                "Ludovic Stephan",
                "Bruno Loureiro",
                "Florent Krzakala",
                "Lenka Zdeborov\u00e1"
            ],
            "title": "Phase diagram of stochastic gradient descent in high-dimensional two-layer neural networks",
            "venue": "arXiv preprint arXiv:2202.00293,",
            "year": 2022
        },
        {
            "authors": [
                "Roman Vershynin"
            ],
            "title": "High-dimensional probability: An introduction with applications in data science, volume 47",
            "year": 2018
        },
        {
            "authors": [
                "Sumio Watanabe"
            ],
            "title": "Almost all learning machines are singular",
            "venue": "IEEE Symposium on Foundations of Computational Intelligence, pp",
            "year": 2007
        },
        {
            "authors": [
                "Zeke Xie",
                "Qian-Yuan Tang",
                "Mingming Sun",
                "Ping Li"
            ],
            "title": "On the overlooked structure of stochastic gradients",
            "venue": "In Thirty-seventh Conference on Neural Information Processing Systems,",
            "year": 2023
        },
        {
            "authors": [
                "Zhihui Zhu",
                "Tianyu Ding",
                "Jinxin Zhou",
                "Xiao Li",
                "Chong You",
                "Jeremias Sulam",
                "Qing Qu"
            ],
            "title": "A geometric analysis of neural collapse with unconstrained features",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Stochastic gradient descent (SGD) and its many variants, are the backbone of modern machine learning algorithms (see e.g., Bottou (1999)). The training dynamics of neural networks, however, are still poorly understood in the non-convex and high-dimensional settings that are frequently encountered. A common explanation for the staggering success of neural networks, especially when overparameterized, is that the loss landscapes that occur in practice have many \u201cflat\u201d directions and a hidden low-dimensional structure within which the bulk of training occurs.\nTo understand this belief, much attention has been paid to the Hessian of the empirical risk (and related matrices formed via gradients) along training. This perspective on the training dynamics of neural networks was proposed in LeCun et al. (2012), and numerically analyzed in depth in Sagun et al. (2017a;b). The upshot of these studies was a broad understanding of the spectrum of the Hessian of the empirical risk, that we summarize as follows:\n1. It has a bulk that is dependent on the network architecture, and is concentrated around 0, becoming more-so as the model becomes more overparametrized;\n2. It has (relatively) few outlier eigenvalues that are dependent on the data, and evolve nontrivially along training while remaining separated from the bulk;\nSince those works, these properties of the Hessian and related spectra over the course of training have seen more refined and large-scale experimentation. Papyan (2019) found a hierarchical decomposition to the Hessian for deep networks, attributing the bulk, emergent outliers, and a minibulk to three different \u201cparts\u201d of the Hessian. Perhaps most relevant to this work, Gur-Ari et al. (2019) noticed that gradient descent tends to quickly align with a low-dimensional outlier subspace of the Hessian matrix, and stay in that subspace for long subsequent times. They postulated that this common low-dimensional structure to the SGD and Hessian matrix may be key to many classification tasks in machine learning. For a sampling of other empirical investigations of spectra of Hessians and information matrices along training, see e.g., Ghorbani et al. (2019); Papyan (2020); Li et al.; Martin & Mahoney (2019); Cohen et al. (2021); Xie et al. (2023).\nFrom a theoretical perspective, much attention has been paid to the Hessians of deep networks using random matrix theory approaches. Most of this work has focused on the spectrum at a fixed point in parameter space, most commonly at initialization. Early works in the direction include Watanabe (2007); Dauphin et al. (2014). Choromanska et al. (2015) noted similarities of neural net Hessians to\nspin glass Hessians, whose complexity (exponential numbers of critical points) has been extensively studied see e.g., Auffinger & Ben Arous (2013); Auffinger et al. (2013). More recently, the expected complexity has been investigated in statistical tasks like tensor PCA and generalized linear estimation Ben Arous et al. (2019); Maillard et al. (2020). In Pennington & Worah (2018) and Pennington & Bahri (2017), the empirical spectral distribution of Hessians and information matrices in single layer neural networks were studied at initialization. Liao & Mahoney (2021) studied Hessians of some non-linear models, which they referred to as generalized GLMs, also at initialization.\nUnder the infinite-width neural tangent kernel limit of Jacot et al. (2018), Fan & Wang (2020) derived the empirical spectral distribution at initialization, and Jacot et al. (2020) studied its evolution over the course of training. In this limit the input dimension is kept fixed compared to the parameter dimension, while our interest in this paper is when the input dimension, parameter dimension, and number of samples all scale together.\nAn important step towards understanding the evolution of Hessians along training in the highdimensional setting, is of course understanding the training dynamics themselves. Since the classical work of Robbins & Monro (1951), there has been much activity studying limit theorems for stochastic gradient descent. In the high-dimensional setting, following Saad & Solla (1995a;b), investigations have focused on finding a finite number of functions (sometimes called \u201cobservables\u201d or \u201csummary statistics\u201d), whose dynamics under the SGD are asymptotically autonomous in the highdimensional limit. For a necessarily small sampling of this rich line of work, we refer to Goldt et al. (2019); Veiga et al. (2022); Paquette et al. (2021); Arnaboldi et al. (2023); Tan & Vershynin (2019); Ben Arous et al. (2022). Of particular relevance, it was shown by Damian et al. (2022); MousaviHosseini et al. (2023) that for multi-index models, SGD predominantly lives in the low-dimensional subspace spanned by the ground truth parameters.\nA class of tasks whose SGD is amenable to this broad approach is classification of Gaussian mixture models (GMMs). With various losses and linearly separable class structures, the minimizer of the empirical risk landscape with single-layer networks was studied in Mignacco et al. (2020); Loureiro et al. (2021). A well-studied case of a Gaussian mixture model needing a two-layer network is under an XOR-type class structure; the training dynamics of SGD for this task were studied in Refinetti et al. (2021) and Ben Arous et al. (2022) and it was found to have a particularly rich structure with positive probability of convergence to bad classifiers among other degenerate phenomena.\nStill, a simultaneous understanding of high-dimensional SGD and the Hessian and related matrices\u2019 spectra along the training trajectory has remained largely open."
        },
        {
            "heading": "1.1 OUR CONTRIBUTIONS",
            "text": "In this paper, we study the interplay between the training dynamics (via SGD) and the spectral compositions of the empirical Hessian matrix (2.2) and an empirical gradient second moment matrix, or simply G-matrix (2.3) (similar in spirit to an information matrix) over the course of training. We rigorously show the following phenomenology in two canonical high-dimensional classification tasks with k \u201chidden\u201d classes:\n1. Shortly into training, the empirical Hessian and empirical G-matrices have C(k) many outlier eigenvalues, and the SGD itself predominantly lives in their eigenspace. Here C(k) is explicit and depends on the performance of the classifier to which the SGD converges.\n2. In multi-layer settings, this alignment happens within each layer, i.e., the first layer parameters align with the outlier eigenspaces of the corresponding blocks of the empirical Hessian and G-matrices, and likewise for the second layer parameters.\n3. This alignment is not predicated on success at the classification task: when the SGD converges to a sub-optimal classifier, the empirical Hessian and G matrices have lower rank outlier eigenspaces, and the SGD aligns with those rank deficient spaces.\nThe first model we consider is the basic example of supervised classification of general k-component Gaussian mixture models with k linearly independent classes by a single layer neural network. In Theorem 3.1, we establish alignment of the form of Item 1 above, between each of the k one-vs-all classifiers and their corresponding blocks in the empirical Hessian and G-matrices. See also the depictions in Figures 3.1\u20133.2. In order to show this, we show that the matrices have an outlier-\nminibulk-bulk structure throughout the parameter space, and derive limiting dynamical equations for the trajectory of appropriate summary statistics of the SGD trajectory following the approach of Ben Arous et al. (2022). Importantly, the same low-dimensional subspace is at the heart of both the outlier eigenspaces and the summary statistics. At this level of generality, the SGD can behave very differently within the outlier eigenspace. As an example of the refined phenomenology that can arise, we further investigate the special case where the means are orthogonal; here the SGD aligns specifically with the single largest outlier eigenvalue, which itself has separated from the other k 1 outliers along training. This is proved in Theorem 3.3 and depicted in Figure 3.3. These results are presented in Section 3.\nTo demonstrate our results in more complex multi-layer settings, we consider supervised classification of a GMM version of the famous XOR problem of Minsky & Papert (1969). This is one of the simplest models that requires a two-layer neural network to solve. We use a two-layer architecture with a second layer of width K. As indicated by Item 2 above, in Theorems 4.1\u20134.2 the alignment of the SGD with the matrices\u2019 outlier eigenspaces occurs within each layer, the first layer having an outlier space of rank two, and the second layer having an outlier space of rank 4 when the dynamics converges to an optimal classifier. This second layer\u2019s alignment is especially rich, as when the model is overparametrized (K large), its outlier space of rank 4 is not present at initialization, and only separates from its rank-K bulk over the course of training. This can be interpreted as a dynamical version of the well-known spectral phase transition in spiked covariance matrices of Baik et al. (2005): see Figure 4.2 for a visualization. Moreover, the SGD for this problem is known to converge to sub-optimal classifiers with probability bounded away from zero Ben Arous et al. (2022), and we find that in these situations, the alignment still occurs but the outlier eigenspaces into which the SGD moves are rank-deficient compared to the number of hidden classes, 4: see Figure 4.3. These results are presented in Section 4."
        },
        {
            "heading": "2 PRELIMINARIES",
            "text": "Before turning to our results, let us begin by introducing the following framework and notation that will be relevant throughout this work. We suppose that we are given data from a distribution PY over pairs Y = (y, Y ) where y 2 Rk is a one-hot \u201clabel\u201d vector that takes the value 1 on a class (sometimes identified with the element of [k] = {1, ..., k} on which it is 1), and Y 2 Rd is a corresponding feature vector. In training we take as loss a function of the form L(x,Y) : Rp\u21e5Rk+d ! R+ , where x 2 Rp represents the network parameter. (As we are studying supervised classification, in both settings this loss will be the usual cross-entropy loss corresponding to the architecture used.)\nWe imagine we have two data sets, a training set (Y`)M`=1 and a test set ( eY`) fM `=1, all drawn i.i.d. from PY . Let us first define the stochastic gradient descent trained using (Y`). In order to ensure the SGD doesn\u2019t go off to infinity we add an `2 penalty term (as is common in practice) with Lagrange multiplier . The (online) stochastic gradient descent with initialization x0 and learning rate, or step-size, , will be run using the training set (Y`)M`=1 as follows:\nx` = x` 1 rL(x` 1,Y`) x` 1 . (2.1)\nOur aim is to understand the behavior of SGD with respect to principal subspaces, i.e., outlier eigenvectors, of the empirical Hessian matrix and empirical second moment matrix of the gradient. This latter matrix is exactly the information matrix when L is the log-likelihood; in our paper L is taken to be a cross-entropy loss, so we simply refer to this as the G-matrix henceforth. We primarily consider the empirical Hessian and empirical G-matrices generated out of the test data, namely:\nr2 bR(x) = 1fM\nfMX\n`=1\nr2L(x, eY`) . (2.2)\nbG(x) = 1fM\nfMX\n`=1\nrL(x, eY`)\u23262 . (2.3)\n(Notice that in our online setting, it is as natural to generate these matrices with test data as with train data. See Remark 5.1 for how our results extend when training data is used.) When the parameter\nspace naturally splits into subsets of its indices (e.g., the first-layer weights and the second-layer weights), for a subset I of the parameter coordinates, we use subscripts r2I,I bR and bGI,I to denote the block corresponding to that subset.\nTo formalize the notion of alignment between the SGD and the principal directions of the Hessian and G-matrices, we introduce the following language. For a subspace B, we let PB denote the orthogonal projection onto B; for a vector v, we let kvk be its `2 norm; and for a matrix A, let kAk = kAkop be its `2 ! `2 operator norm. Definition 2.1. The alignment of a vector v with a subspace B is the ratio \u21e2(v,B) = ||PBv||/||v||. We say a vector v lives in a subspace B up to error \" if \u21e2(v,B) 1 \".\nFor a matrix A, we let Ek(A) denote the span of the top k eigenvectors of A, i.e., the span of the k eigenvectors of A with the largest absolute values. We also use the following. Definition 2.2. We say a matrix A lives in a subspace B up to error \" if there exists M such that Im(A M) \u21e2 B with kMkop  \"kAkop, where kAkop denotes the `2-to-`2 operator norm."
        },
        {
            "heading": "3 CLASSIFYING LINEARLY SEPARABLE MIXTURE MODELS",
            "text": "We begin by illustrating our results on (arguably) the most basic problem of high-dimensional multiclass classification, namely supervised classification of a k component Gaussian mixture model with constant variance and linearly independent means using a single-layer network. (This is sometimes used as a toy model for the training dynamics of the last layer of a deep network via the common ansatz that the output of the second-to-last layer of a deep network behaves like a linearly separable mixture of Gaussians: see e.g., the neural collapse phenomenon posited by Papyan et al. (2020).)"
        },
        {
            "heading": "3.1 DATA MODEL",
            "text": "Let C = [k] be the collection of classes, with corresponding distinct class means (\u00b5a)a2[k] 2 Rd, covariance matrices Id/ , where > 0 can be viewed a signal-to-noise parameter, and corresponding probabilities 0 < (pa)a2[k] < 1 such that P a2[k] pa = 1. The number of classes k = O(1) is fixed (here and throughout the paper o(1), O(1) and \u2326(1) notations are with respect to the dimension parameter d, and may hide constants that are dimension independent such as k, ).\nFor the sake of simplicity we take the means to be unit norm. Further, in order for the task to indeed be solvable with the single-layer architecture, we assume that the means are linearly independent, say with a fixed (i.e., d-independent) matrix of inner products (mab)a,b = (\u00b5a \u00b7 \u00b5b)a,b. Our data distribution PY is a mixture of the form P c pcN (\u00b5c, Id/ ), with an accompanying class label y 2 Rk. Namely, our data is given as Y = (y, Y ) where:\ny \u21e0 X\na2[k]\npa 1a , and Y \u21e0 X\na2[k]\nya\u00b5a + Z , (3.1)\nand where Z \u21e0 N (0, Id/ ). We perform classification by training a single layer network formed by k \u201call-vs-one\u201d classifiers using the cross entropy loss (equivalently, we are doing multi-class logistic regression):\nL(x,Y) = X\nc2[k]\nycx c \u00b7 Y + log\nX\nc2[k]\nexp(xc \u00b7 Y ) , (3.2)\nwhere x = (xc)c2C are the parameters, each of which is a vector in Rd, i.e., x 2 Rdk. (Note that we can alternatively view x as a k \u21e5 d matrix.)"
        },
        {
            "heading": "3.2 RESULTS AND DISCUSSION",
            "text": "Our first result is that after some linearly many steps, the SGD finds the subspace generated by the outlier eigenvalues of the Hessian and/or G-matrix of the test loss and lives there for future times. Theorem 3.1. Consider the mixture of k-Gaussians with loss function from (3.2), and SGD (2.1) with learning rate = O(1/d), regularizer > 0, initialized from N (0, Id/d). There exists \u21b50, 0\nsuch that if 0, and fM \u21b50d, the following hold. For every \" > 0, there exists T0(\") such that for any time horizon T0 < Tf < M/d, with probability 1 od(1),\nxc` lives in Ek(r2cc bR(x`)) and in Ek( bGcc(x`)) , for every c 2 [k], up to O(\"+ 1) error, for all ` 2 [T0 1, Tf 1].\nThis result is demonstrated in Figure 3.1 which plots the alignment of the training dynamics xc` with the principal eigenspaces of the Hessian and G for each c 2 [k]. As we see the alignment increases to near 1 rapidly for all blocks in both matrices. This theorem, and all our future results, are stated using a random Gaussian initialization, scaled such that the norm of the parameters is O(1) in d. The fact that this is Gaussian is not relevant to the results, and similar results hold for other uninformative initializations with norm of O(1).\nTheorem 3.1 follows from the following theorem that describes the SGD trajectory, its Hessian and G-matrix (and their top k eigenspaces), all live up to O(1/ ) error in Span(\u00b51, ..., \u00b5k). Theorem 3.2. In the setup of Theorem 3.1, the following live in Span(\u00b51, ..., \u00b5k) up to O(\"+ 1) error with probability 1 od(1):\n1. The state of the SGD along training, xc` for every c;\n2. The b, c blocks of the empirical test Hessian, r2bc bR(x`) for all b, c 2 [k];\n3. The b, c blocks of the empirical test G-matrix bGbc(x`) for all b, c 2 [k].\nWe demonstrate this result in Figure 3.2, which shows the coordinate-wise values of a fixed block of the SGD, the Hessian, and the G-matrix.\nInside the low-rank space spanned by \u00b51, ..., \u00b5k, the training dynamics, Hessian and G-matrix spectra can display different phenomena depending on the relative locations of \u00b51, ..., \u00b5k and weights p1, ..., pk. To illustrate the more refined alignment phenomena, let us take as a concrete example pc = 1 k for all c, and \u00b51, ..., \u00b5k orthonormal.\nWith this concrete choice, we can analyze the limiting dynamical system of the SGD without much difficulty and its relevant dynamical observables have a single stable fixed point, to which the SGD converges in linearly many, i.e., O( 1), steps. This allows us to show more precise alignment that occurs within Span(\u00b51, ..., \u00b5k) over the course of training. Theorem 3.3. In the setting of Theorem 3.1, with the means (\u00b51, ..., \u00b5k) being orthonormal, the estimate xc` has \u2326(1), positive, inner product with the top eigenvector of both r2cc bR(x`) and bGcc(x`) (and negative, \u2326(1) inner product with the k 1 next largest eigenvectors). Also, the top eigenvector of r2cc bR(x`), as well as that of bGcc(x`), live in Span(\u00b5c) up to O(\"+ 1) error.\nPut together, the above three theorems describe the following rich scenario for classification of the k-GMM. At initialization, and throughout the parameter space in each class-block, the Hessian and G-matrices decompose into a rank-k outlier part spanned by \u00b51, ..., \u00b5k, and a correction term of size O(1/ ) in operator norm. Furthermore, when initialized randomly, the SGD is not aligned with the outlier eigenspaces, but does align with them in a short O( 1) number of steps. Moreover, when the means are orthogonal, each class block of the SGD xc` in fact correlates strongly with the specific mean for its class \u00b5c, and simultaneously, in the Hessian and G-matrices along training, the eigenvalue corresponding to \u00b5c becomes distinguished from the other k 1 outliers. We illustrate these last two points in Figure 3.3. We also refer the reader to Section F for further numerical demonstrations. Each of these phenomena appear in more general contexts, and in even richer manners, as we will see in the following section."
        },
        {
            "heading": "4 CLASSIFYING XOR-TYPE MIXTURE MODELS VIA TWO-LAYER NETWORKS",
            "text": "With the above discussion in mind, let us now turn to more complex classification tasks that are not linearly separable and require the corresponding network architecture to be multilayer."
        },
        {
            "heading": "4.1 DATA MODEL",
            "text": "For our multilayer results, we consider the problem of classifying a 4-component Gaussian mixture whose class labels are in a so-called XOR form. More precisely, consider a mixture of four Gaussians with means \u00b5, \u00b5, \u232b, \u232b where k\u00b5k = k\u232bk = 1 and, say for simplicity, are orthogonal, and variances Id/ . There are two classes, class label 1 for Gaussians with mean \u00b1\u00b5, and 0 for Gaussians with mean \u00b1\u232b. To be more precise, our data distribution PY is\ny \u21e0 1 2 0 + 1 2 1 and Y \u21e0\n\u21e2 1\n2 N (\u00b5, Id/ ) + 12N ( \u00b5, Id/ ) y = 1 1 2 N (\u232b, Id/ ) + 12N ( \u232b, Id/ ) y = 0 . (4.1)\nThis is a Gaussian version of the famous XOR problem of Minsky & Papert (1969). It is one of the simplest examples of a classification task requiring a multi-layer network to express a good classifier.\nWe therefore use a two-layer architecture with the intermediate layer having width K 4 (any less and a Bayes-optimal classifier would not be expressible), ReLu activation function g(x) = x _ 0\nand then sigmoid activation function (x) = 1 1+e x . The parameter space is then x = (W, v) 2 RKd+K , where the first layer weights are denoted by W = W (x) 2 RK\u21e5d and the second layer weights are denoted by v = v(x) 2 RK . We use the binary cross-entropy loss on this problem,\nL(x;Y) = yv \u00b7 g(WY ) + log(1 + ev\u00b7g(WY )) , (4.2)\nwith g applied entrywise. The SGD for this classification task was studied in some detail in Refinetti et al. (2021) and Ben Arous et al. (2022), with \u201ccritical\u201d step size = \u21e5(1/d)."
        },
        {
            "heading": "4.2 RESULTS AND DISCUSSION",
            "text": "We begin our discussion with the analogue of Theorem 3.1 in this setting. As the next theorem demonstrates, in this more subtle problem, the SGD still finds and lives in the principal directions of the empirical Hessian and G-matrices.1 Here, the principal directions vary significantly across the parameter space, and the alignment phenomenon differs depending on which fixed point the SGD converges to. Moreover, the relation between the SGD and the principal directions of the Hessian and G-matrices can be seen per layer. Theorem 4.1. Consider the XOR GMM mixture with loss function (4.2) and the corresponding SGD (2.1) with 2 (0, 1/8), learning rate = O(1/d), initialized from N (0, Id/d). There exist \u21b50, 0 such that if 0, and fM \u21b50d, the following hold. For every \" > 0, there exists T0(\") such that for any time horizon T0 < Tf < M/d, with probability 1 od(1), for all i 2 {1, ...,K},\n1. Wi(x`) lives in E2(r2WiWi bR(x`)) and in E2( bGWiWi(x`)), and\n2. v(x`) lives in E4(r2vv bR(x`)) and E4( bGvv(x`)),\nup to O(\"+ 1/2)2 error, for all ` 2 [T0 1, Tf 1]. Remark 1. The restriction to < 1/8 in Theorems 4.1\u20134.2 is because when > 1/8 the regularization is too strong for the SGD to be meaningful; in particular, the SGD converges ballistically to the origin in parameter space, with no discernible preference for the directions corresponding to \u00b5, \u232b as the other directions. The above theorems are still valid there if the notion of error for living in a space from Definition 2.1 were additive instead of multiplicative (i.e., at most \u270f rather than \u270fkvk).\nThis theorem is demonstrated in Figure 4.1. There we have plotted the alignment of the rows in the intermediate layer with the space spanned by the top two eigenvectors of the corresponding first-layer blocks of the Hessian and G-matrices, and similarly for the final layer.\nAs before, the above theorem follows from the following theorem that describes both the SGD trajectory, its Hessian, and its G-matrix, living up to O(\u270f+ 1/2) error in their first-layer blocks in\n1As the second derivative of ReLU is only defined in the sense of distributions, care must be taken when studying these objects. This singularity is not encountered by the SGD trajectory almost surely.\n2In this theorem and in Theorem 4.2, the big-O notation also hides constant dependencies on the initial magnitudes of v(x0), and on Tf .\nSpan(\u00b5, \u232b) and in their second layer blocks in\nSpan(g(W (x`)\u00b5), g( W (x`)\u00b5), g(W (x`)\u232b), g( W (x`)\u232b)) ,\nwhere g is applied entrywise.\nTheorem 4.2. In the setting of Theorem 4.1, up to O(\"+ 1/2) error with probability 1 od(1), the following live in Span(\u00b5, \u232b),\n\u2022 The first layer weights, Wi(x`) for each i 2 {1, ...,K},\n\u2022 The first-layer empirical test Hessian r2WiWi bR(x`) for each i 2 {1, ...,K},\n\u2022 The first-layer empirical test G-matrix bGWiWi(x`) for each i 2 {1, ...,K},\nand the following live in Span(g(W (x`)\u00b5), g( W (x`)\u00b5), g(W (x`)\u232b), g( W (x`)\u232b))\n\u2022 The second layer weights v(x`),\n\u2022 The second-layer empirical test Hessian r2vv bR(x`),\n\u2022 The second-layer empirical test G-matrix bGvv(x`).\nLet us discuss a bit more the phenomenology of the alignment in the second layer. First of all, we observe that the subspace in which the alignment occurs is a random\u2014depending on the initialization and trajectory (in particular, choice of fixed point the SGD converges to)\u20144-dimensional subspace of RK . Furthermore, if we imagine K to be much larger than 4 so that the model is overparametrized, at initialization, unlike the 1-layer case studied in Section 3, the Hessian and Gmatrices do not exhibit any alignment in their second layer blocks. In particular, the second layer blocks of the Hessian and G-matrices look like (non-spiked) Gaussian orthogonal ensemble and Wishart matrices3 in K dimensions at initialization, and it is only over the course of training that they develop 4 outlier eigenvalues as the first layer of the SGD begins to align with the mean vectors and the vectors (g(W (x`)#))#2{\u00b1\u00b5,\u00b1\u232b} in turn get large enough to generate outliers. This crystallization of the last layer around these vectors over the course of training is reminiscent of the neural collapse phenomenon outlined in Papyan et al. (2020) (see also Han et al. (2022); Zhu et al. (2021)). The simultaneous emergence, along training, of outliers in the Hessian and G-matrix spectra can be seen as a dynamical version of what is sometimes referred to as the BBP transition after Baik et al. (2005) (see also Pe\u0301che\u0301 (2006)). This dynamical transition is demonstrated in Figure 4.2\nFinally, we recall that Ben Arous et al. (2022) found a positive probability (uniformly in d, but shrinking as the architecture is overparametrized by letting K grow) that the SGD converges to suboptimal classifiers from a random initialization. When this happens, g(W (x`)#) remains small for the hidden classes # 2 {\u00b1\u00b5,\u00b1\u232b} that are not classifiable with SGD output. In those situations, Theorem 4.2 shows that the outlier subspace in the vv-blocks that emerges will have rank smaller\n3These are two of the most classical random matrix models, see Anderson et al. (2010) for more.\nthan 4, whereas when an optimal classifier is found it will be of rank 4: see Figure 4.3. Knowing that the classification task entails a mixture of 4 means, this may provide a method for devising a stopping rule for classification tasks of this form by examining the rank of the outlier eigenspaces of the last layer Hessian or G-matrix. While the probability of such sub-optimal classification is bounded away from zero, the probability exponentially goes to zero as the model is overparametrized via K ! 1, and that gives more chances to allow the second layer SGD, Hessian, and G-matrices to exhibit full 4-dimensional principal spaces. This serves as a concrete and provable manifestation of the lottery ticket hypothesis of Frankle & Carbin (2019)."
        },
        {
            "heading": "5 OUTLINE AND IDEAS OF PROOF",
            "text": "The proofs of our main theorems break into three key steps.\n1. In Sections A\u2013B, we show that the population Hessian and G-matrices, have bulks (and possibly minibulks) that are O(1/ ) in operator norm, and finite C(k)-rank parts. We can explicitly characterize the low-rank part\u2019s eigenvalues and eigenvectors up to O(1/ ) corrections, as functions of the parameter space, to see exactly where their emergence as outliers occurs depending on the model.\n2. In Section C, we analyze the SGD trajectories for the k-GMM and XOR classification problems. We do this using the limiting effective dynamics theorem proven in Ben Arous et al. (2022) for finite families of summary statistics. We derive ODE limits for these summary statistics, notably for the general k-GMM which was not covered in that paper: see Theorem C.7. We then pull back the limiting dynamics to finite d and expand its - finite trajectory about its = 1 solution. These latter steps involve understanding some of the stability properties of the dynamical system limits of the SGD\u2019s summary statistics.\n3. In Section D, we prove concentration for the empirical Hessian and G-matrices about their population versions, in operator norm, throughout the parameter space. In some related settings, including binary mixtures of Gaussians, Mei et al. (2018) established concentration of the empirical Hessian about the population Hessian uniformly in the parameter space assuming polylogarithmic sample complexity. Our proofs are based on \u270f-nets and concentration inequalities for uniformly sub-exponential random variables, albeit with some twists due, for instance, to non-differentiability of the ReLU function.\nRemark 5.1. Our results are stated for the empirical Hessian and G-matrices generated using test data, along the SGD trajectory generated from training data. Since we are considering online SGD, the empirical Hessian and G-matrices with training data are no more relevant than those generated from test data, but the reader may still wonder whether the same behavior holds. A straightforward modification of our arguments in Section D extends our results for the k-GMM model to Hessian and G-matrices generated from train data, if we assume that M & d log d rather than simply M & d. See the full version for a sketch of this extension. It is an interesting mathematical question to drop this extra logarithmic factor. The extension in the XOR case is technically more involved due to the regularity of the ReLU function. See Section F for numerical demonstrations that the phenomena in the empirical matrices generated from train data are identical to those generated from test data.\nAUTHOR CONTRIBUTIONS\nAll authors contributed equally to this work."
        }
    ],
    "year": 2023
}