{
    "abstractText": "One fascinating aspect of pre-trained vision-language models (VLMs) learning under language supervision is their impressive zero-shot generalization capability. However, this ability is hindered by distribution shifts between the training and testing data. Previous test time adaptation (TTA) methods for VLMs in zeroshot classification rely on minimizing the entropy of model outputs, tending to be stuck in incorrect model predictions. In this work, we propose TTA with feedback to rectify the model output and prevent the model from becoming blindly confident. Specifically, a CLIP model is adopted as the reward model during TTA and provides feedback for the VLM. Given a single test sample, the VLM is forced to maximize the CLIP reward between the input and sampled results from the VLM output distribution. The proposed reinforcement learning with CLIP feedback (RLCF) framework is highly flexible and universal. Beyond the classification task, with task-specific sampling strategies and a proper reward baseline choice, RLCF can be easily extended to not only discrimination tasks like retrieval but also generalization tasks like image captioning, improving the zero-shot generalization capacity of VLMs. According to the characteristics of these VL tasks, we build different fully TTA pipelines with RLCF to improve the zero-shot generalization ability of various VLMs. Extensive experiments along with promising empirical results demonstrate the effectiveness of RLCF. The code is available at https://github.com/mzhaoshuai/RLCF.",
    "authors": [
        {
            "affiliations": [],
            "name": "Shuai Zhao"
        },
        {
            "affiliations": [],
            "name": "Xiaohan Wang"
        },
        {
            "affiliations": [],
            "name": "Linchao Zhu"
        },
        {
            "affiliations": [],
            "name": "Yi Yang"
        }
    ],
    "id": "SP:0f39f91744efb8d46d8ccac0dfefb6192c38ccdb",
    "references": [
        {
            "authors": [
                "Harsh Agrawal",
                "Karan Desai",
                "Yufei Wang",
                "Xinlei Chen",
                "Rishabh Jain",
                "Mark Johnson",
                "Dhruv Batra",
                "Devi Parikh",
                "Stefan Lee",
                "Peter Anderson"
            ],
            "title": "Nocaps: Novel object captioning at scale",
            "year": 2019
        },
        {
            "authors": [
                "Yuntao Bai",
                "Andy Jones",
                "Kamal Ndousse",
                "Amanda Askell",
                "Anna Chen",
                "Nova DasSarma",
                "Dawn Drain",
                "Stanislav Fort",
                "Deep Ganguli",
                "Tom Henighan"
            ],
            "title": "Training a helpful and harmless assistant with reinforcement learning from human feedback",
            "venue": "arXiv preprint arXiv:2204.05862,",
            "year": 2022
        },
        {
            "authors": [
                "Ting Chen",
                "Simon Kornblith",
                "Mohammad Norouzi",
                "Geoffrey Hinton"
            ],
            "title": "A simple framework for contrastive learning of visual representations",
            "venue": "In ICML,",
            "year": 2020
        },
        {
            "authors": [
                "Jaemin Cho",
                "Seunghyun Yoon",
                "Ajinkya Kale",
                "Franck Dernoncourt",
                "Trung Bui",
                "Mohit Bansal"
            ],
            "title": "Fine-grained image captioning with clip reward",
            "venue": "In Findings of NAACL,",
            "year": 2022
        },
        {
            "authors": [
                "J. Deng",
                "W. Dong",
                "R. Socher",
                "L.-J. Li",
                "K. Li",
                "L. Fei-Fei"
            ],
            "title": "ImageNet: A Large-Scale Hierarchical Image Database",
            "venue": "In CVPR,",
            "year": 2009
        },
        {
            "authors": [
                "Mingkai Deng",
                "Jianyu Wang",
                "Cheng-Ping Hsieh",
                "Yihan Wang",
                "Han Guo",
                "Tianmin Shu",
                "Meng Song",
                "Eric Xing",
                "Zhiting Hu"
            ],
            "title": "RLPrompt: Optimizing discrete text prompts with reinforcement learning",
            "venue": "In EMNLP,",
            "year": 2022
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "Lucas Beyer",
                "Alexander Kolesnikov",
                "Dirk Weissenborn",
                "Xiaohua Zhai",
                "Thomas Unterthiner",
                "Mostafa Dehghani",
                "Matthias Minderer",
                "Georg Heigold",
                "Sylvain Gelly",
                "Jakob Uszkoreit",
                "Neil Houlsby"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
            "venue": "In ICLR,",
            "year": 2021
        },
        {
            "authors": [
                "Amelia Glaese",
                "Nat McAleese",
                "Maja Tr\u0119bacz",
                "John Aslanides",
                "Vlad Firoiu",
                "Timo Ewalds",
                "Maribeth Rauh",
                "Laura Weidinger",
                "Martin Chadwick",
                "Phoebe Thacker"
            ],
            "title": "Improving alignment of dialogue agents via targeted human judgements",
            "venue": "arXiv preprint arXiv:2209.14375,",
            "year": 2022
        },
        {
            "authors": [
                "Chuan Guo",
                "Geoff Pleiss",
                "Yu Sun",
                "Kilian Q. Weinberger"
            ],
            "title": "On calibration of modern neural networks",
            "venue": "In Doina Precup and Yee Whye Teh (eds.),",
            "year": 2017
        },
        {
            "authors": [
                "Jia Guo",
                "Minghao Chen",
                "Yao Hu",
                "Chen Zhu",
                "Xiaofei He",
                "Deng Cai"
            ],
            "title": "Reducing the teacherstudent gap via spherical knowledge disitllation",
            "venue": "arXiv preprint arXiv:2010.07485,",
            "year": 2020
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "year": 2016
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Steven Basart",
                "Norman Mu",
                "Saurav Kadavath",
                "Frank Wang",
                "Evan Dorundo",
                "Rahul Desai",
                "Tyler Zhu",
                "Samyak Parajuli",
                "Mike Guo",
                "Dawn Song",
                "Jacob Steinhardt",
                "Justin Gilmer"
            ],
            "title": "The many faces of robustness: A critical analysis of out-of-distribution generalization",
            "venue": "In ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Kevin Zhao",
                "Steven Basart",
                "Jacob Steinhardt",
                "Dawn Song"
            ],
            "title": "Natural adversarial examples",
            "venue": "In CVPR, pp",
            "year": 2021
        },
        {
            "authors": [
                "Jack Hessel",
                "Ari Holtzman",
                "Maxwell Forbes",
                "Ronan Le Bras",
                "Yejin Choi"
            ],
            "title": "CLIPScore: a reference-free evaluation metric for image captioning",
            "venue": "In EMNLP,",
            "year": 2021
        },
        {
            "authors": [
                "Geoffrey Hinton",
                "Oriol Vinyals",
                "Jeff Dean"
            ],
            "title": "Distilling the knowledge in a neural network",
            "venue": "arXiv preprint arXiv:1503.02531,",
            "year": 2015
        },
        {
            "authors": [
                "Fangzhou Hong",
                "Mingyuan Zhang",
                "Liang Pan",
                "Zhongang Cai",
                "Lei Yang",
                "Ziwei Liu"
            ],
            "title": "Avatarclip: zero-shot text-driven generation and animation of 3d avatars",
            "venue": "ACM Trans. Graph.,",
            "year": 2022
        },
        {
            "authors": [
                "Chao Jia",
                "Yinfei Yang",
                "Ye Xia",
                "Yi-Ting Chen",
                "Zarana Parekh",
                "Hieu Pham",
                "Quoc V. Le",
                "Yun-Hsuan Sung",
                "Zhen Li",
                "Tom Duerig"
            ],
            "title": "Scaling up visual and vision-language representation learning with noisy text supervision",
            "year": 2021
        },
        {
            "authors": [
                "Andrej Karpathy",
                "Li Fei-Fei"
            ],
            "title": "Deep visual-semantic alignments for generating image descriptions",
            "venue": "In CVPR,",
            "year": 2015
        },
        {
            "authors": [
                "Ngan Le",
                "Vidhiwar Singh Rathour",
                "Kashu Yamazaki",
                "Khoa Luu",
                "Marios Savvides"
            ],
            "title": "Deep reinforcement learning in computer vision: a comprehensive survey",
            "venue": "Artificial Intelligence Review,",
            "year": 2022
        },
        {
            "authors": [
                "Dong-Hyun Lee"
            ],
            "title": "Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks",
            "venue": "In Workshop on challenges in representation learning,",
            "year": 2013
        },
        {
            "authors": [
                "Harrison Lee",
                "Samrat Phatale",
                "Hassan Mansoor",
                "Kellie Lu",
                "Thomas Mesnard",
                "Colton Bishop",
                "Victor Carbune",
                "Abhinav Rastogi"
            ],
            "title": "Rlaif: Scaling reinforcement learning from human feedback with ai feedback, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Wei Li",
                "Linchao Zhu",
                "Longyin Wen",
                "Yi Yang"
            ],
            "title": "Decap: Decoding CLIP latents for zero-shot captioning via text-only training",
            "venue": "In ICLR,",
            "year": 2023
        },
        {
            "authors": [
                "Tsung-Yi Lin",
                "Michael Maire",
                "Serge Belongie",
                "James Hays",
                "Pietro Perona",
                "Deva Ramanan",
                "Piotr Doll\u00e1r",
                "C Lawrence Zitnick"
            ],
            "title": "Microsoft coco: Common objects in context",
            "venue": "In ECCV,",
            "year": 2014
        },
        {
            "authors": [
                "Wei Lin",
                "Muhammad Jehanzeb Mirza",
                "Mateusz Kozinski",
                "Horst Possegger",
                "Hilde Kuehne",
                "Horst Bischof"
            ],
            "title": "Video test-time adaptation for action recognition",
            "year": 2023
        },
        {
            "authors": [
                "Yuejiang Liu",
                "Parth Kothari",
                "Bastien Van Delft",
                "Baptiste Bellot-Gurlet",
                "Taylor Mordan",
                "Alexandre Alahi"
            ],
            "title": "Ttt++: When does self-supervised test-time training fail or thrive? NeurIPS, 2021",
            "year": 2021
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter"
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "In ICLR,",
            "year": 2019
        },
        {
            "authors": [
                "Shu Manli",
                "Nie Weili",
                "Huang De-An",
                "Yu Zhiding",
                "Goldstein Tom",
                "Anandkumar Anima",
                "Xiao Chaowei"
            ],
            "title": "Test-time prompt tuning for zero-shot generalization in vision-language models",
            "venue": "NeurIPS,",
            "year": 2022
        },
        {
            "authors": [
                "Matthias Minderer",
                "Josip Djolonga",
                "Rob Romijnders",
                "Frances Hubis",
                "Xiaohua Zhai",
                "Neil Houlsby",
                "Dustin Tran",
                "Mario Lucic"
            ],
            "title": "Revisiting the calibration of modern neural networks",
            "venue": "NeurIPS,",
            "year": 2021
        },
        {
            "authors": [
                "Ron Mokady",
                "Amir Hertz",
                "Amit H Bermano"
            ],
            "title": "Clipcap: Clip prefix for image captioning",
            "venue": "arXiv preprint arXiv:2111.09734,",
            "year": 2021
        },
        {
            "authors": [
                "Shuaicheng Niu",
                "Jiaxiang Wu",
                "Yifan Zhang",
                "Yaofo Chen",
                "Shijian Zheng",
                "Peilin Zhao",
                "Mingkui Tan"
            ],
            "title": "Efficient test-time model adaptation without forgetting",
            "year": 2022
        },
        {
            "authors": [
                "Shuaicheng Niu",
                "Jiaxiang Wu",
                "Yifan Zhang",
                "Zhiquan Wen",
                "Yaofo Chen",
                "Peilin Zhao",
                "Mingkui Tan"
            ],
            "title": "Towards stable test-time adaptation in dynamic wild world",
            "year": 2023
        },
        {
            "authors": [
                "David Nukrai",
                "Ron Mokady",
                "Amir Globerson"
            ],
            "title": "Text-only training for image captioning using noise-injected clip",
            "venue": "In Findings of EMNLP,",
            "year": 2022
        },
        {
            "authors": [
                "Long Ouyang",
                "Jeffrey Wu",
                "Xu Jiang",
                "Diogo Almeida",
                "Carroll Wainwright",
                "Pamela Mishkin",
                "Chong Zhang",
                "Sandhini Agarwal",
                "Katarina Slama",
                "Alex Ray"
            ],
            "title": "Training language models to follow instructions with human",
            "year": 2022
        },
        {
            "authors": [
                "Andr\u00e9 Susano Pinto",
                "Alexander Kolesnikov",
                "Yuge Shi",
                "Lucas Beyer",
                "Xiaohua Zhai"
            ],
            "title": "Tuning computer vision models with task rewards",
            "venue": "arXiv preprint arXiv:2302.08242,",
            "year": 2023
        },
        {
            "authors": [
                "Bryan A Plummer",
                "Liwei Wang",
                "Chris M Cervantes",
                "Juan C Caicedo",
                "Julia Hockenmaier",
                "Svetlana Lazebnik"
            ],
            "title": "Flickr30k entities: Collecting region-to-phrase correspondences for richer imageto-sentence models",
            "venue": "In ICCV,",
            "year": 2015
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark",
                "Gretchen Krueger",
                "Ilya Sutskever"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "year": 2021
        },
        {
            "authors": [
                "Benjamin Recht",
                "Rebecca Roelofs",
                "Ludwig Schmidt",
                "Vaishaal Shankar"
            ],
            "title": "Do imagenet classifiers generalize to imagenet",
            "venue": "In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.),",
            "year": 2019
        },
        {
            "authors": [
                "Steven J Rennie",
                "Etienne Marcheret",
                "Youssef Mroueh",
                "Jerret Ross",
                "Vaibhava Goel"
            ],
            "title": "Self-critical sequence training for image captioning",
            "year": 2017
        },
        {
            "authors": [
                "Aneeshan Sain",
                "Ayan Kumar Bhunia",
                "Pinaki Nath Chowdhury",
                "Subhadeep Koley",
                "Tao Xiang",
                "Yi-Zhe Song"
            ],
            "title": "CLIP for all things zero-shot sketch-based image retrieval, fine-grained or not",
            "year": 2023
        },
        {
            "authors": [
                "Steffen Schneider",
                "Evgenia Rusak",
                "Luisa Eck",
                "Oliver Bringmann",
                "Wieland Brendel",
                "Matthias Bethge"
            ],
            "title": "Improving robustness against common corruptions by covariate shift adaptation",
            "venue": "NeurIPS,",
            "year": 2020
        },
        {
            "authors": [
                "John Schulman",
                "Filip Wolski",
                "Prafulla Dhariwal",
                "Alec Radford",
                "Oleg Klimov"
            ],
            "title": "Proximal policy optimization algorithms",
            "venue": "arXiv preprint arXiv:1707.06347,",
            "year": 2017
        },
        {
            "authors": [
                "Nisan Stiennon",
                "Long Ouyang",
                "Jeffrey Wu",
                "Daniel Ziegler",
                "Ryan Lowe",
                "Chelsea Voss",
                "Alec Radford",
                "Dario Amodei",
                "Paul F Christiano"
            ],
            "title": "Learning to summarize with human feedback",
            "venue": "NeurIPS,",
            "year": 2020
        },
        {
            "authors": [
                "Yixuan Su",
                "Tian Lan",
                "Yahui Liu",
                "Fangyu Liu",
                "Dani Yogatama",
                "Yan Wang",
                "Lingpeng Kong",
                "Nigel Collier"
            ],
            "title": "Language models can see: plugging visual controls in text generation",
            "venue": "arXiv preprint arXiv:2205.02655,",
            "year": 2022
        },
        {
            "authors": [
                "Yu Sun",
                "Xiaolong Wang",
                "Zhuang Liu",
                "John Miller",
                "Alexei A. Efros",
                "Moritz Hardt"
            ],
            "title": "Test-time training with self-supervision for generalization under distribution shifts",
            "venue": "In ICML,",
            "year": 2020
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "NeurIPS,",
            "year": 2017
        },
        {
            "authors": [
                "Ramakrishna Vedantam",
                "C Lawrence Zitnick",
                "Devi Parikh"
            ],
            "title": "Cider: Consensus-based image description evaluation",
            "venue": "In CVPR,",
            "year": 2015
        },
        {
            "authors": [
                "Dequan Wang",
                "Evan Shelhamer",
                "Shaoteng Liu",
                "Bruno A. Olshausen",
                "Trevor Darrell"
            ],
            "title": "Tent: Fully test-time adaptation by entropy minimization",
            "venue": "In ICLR,",
            "year": 2021
        },
        {
            "authors": [
                "Haohan Wang",
                "Songwei Ge",
                "Zachary Lipton",
                "Eric P Xing"
            ],
            "title": "Learning robust global representations by penalizing local predictive power",
            "venue": "NeurIPS,",
            "year": 2019
        },
        {
            "authors": [
                "Wenxiao Wang",
                "Minghao Chen",
                "Shuai Zhao",
                "Long Chen",
                "Jinming Hu",
                "Haifeng Liu",
                "Deng Cai",
                "Xiaofei He",
                "Wei Liu"
            ],
            "title": "Accelerate cnns from three dimensions: A comprehensive pruning framework",
            "venue": "In ICML,",
            "year": 2021
        },
        {
            "authors": [
                "Ronald J Williams"
            ],
            "title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning",
            "venue": "Machine learning,",
            "year": 1992
        },
        {
            "authors": [
                "Mitchell Wortsman",
                "Gabriel Ilharco",
                "Samir Ya Gadre",
                "Rebecca Roelofs",
                "Raphael Gontijo-Lopes",
                "Ari S Morcos",
                "Hongseok Namkoong",
                "Ali Farhadi",
                "Yair Carmon",
                "Simon Kornblith"
            ],
            "title": "Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time",
            "year": 2022
        },
        {
            "authors": [
                "Jiazheng Xu",
                "Xiao Liu",
                "Yuchen Wu",
                "Yuxuan Tong",
                "Qinkai Li",
                "Ming Ding",
                "Jie Tang",
                "Yuxiao Dong"
            ],
            "title": "Imagereward: Learning and evaluating human preferences for text-to-image generation",
            "venue": "arXiv preprint arXiv:2304.05977,",
            "year": 2023
        },
        {
            "authors": [
                "Lu Yuan",
                "Dongdong Chen",
                "Yi-Ling Chen",
                "Noel Codella",
                "Xiyang Dai",
                "Jianfeng Gao",
                "Houdong Hu",
                "Xuedong Huang",
                "Boxin Li",
                "Chunyuan Li",
                "Ce Liu",
                "Mengchen Liu",
                "Zicheng Liu",
                "Yumao Lu",
                "Yu Shi",
                "Lijuan Wang",
                "Jianfeng Wang",
                "Bin Xiao",
                "Zhen Xiao",
                "Jianwei Yang",
                "Michael Zeng",
                "Luowei Zhou",
                "Pengchuan Zhang"
            ],
            "title": "Florence: A new foundation model for computer vision",
            "venue": "arXiv preprint arXiv:2111.11432,",
            "year": 2021
        },
        {
            "authors": [
                "Luca Zancato",
                "Alessandro Achille",
                "Tian Yu Liu",
                "Matthew Trager",
                "Pramuditha Perera",
                "Stefano Soatto"
            ],
            "title": "Train/test-time adaptation with retrieval",
            "year": 2023
        },
        {
            "authors": [
                "Marvin Zhang",
                "Sergey Levine",
                "Chelsea Finn"
            ],
            "title": "Memo: Test time robustness via adaptation and augmentation",
            "venue": "NeurIPS,",
            "year": 2022
        },
        {
            "authors": [
                "Susan Zhang",
                "Stephen Roller",
                "Naman Goyal",
                "Mikel Artetxe",
                "Moya Chen",
                "Shuohui Chen",
                "Christopher Dewan",
                "Mona Diab",
                "Xian Li",
                "Xi Victoria Lin"
            ],
            "title": "Opt: Open pre-trained transformer language models",
            "venue": "arXiv preprint arXiv:2205.01068,",
            "year": 2022
        },
        {
            "authors": [
                "Tianjun Zhang",
                "Xuezhi Wang",
                "Denny Zhou",
                "Dale Schuurmans",
                "Joseph E Gonzalez"
            ],
            "title": "Tempera: Test-time prompt editing via reinforcement learning",
            "venue": "In ICLR,",
            "year": 2023
        },
        {
            "authors": [
                "Borui Zhao",
                "Quan Cui",
                "Renjie Song",
                "Yiyu Qiu",
                "Jiajun Liang"
            ],
            "title": "Decoupled knowledge distillation",
            "year": 2022
        },
        {
            "authors": [
                "Kaiyang Zhou",
                "Jingkang Yang",
                "Chen Change Loy",
                "Ziwei Liu"
            ],
            "title": "Learning to prompt for visionlanguage models",
            "venue": "CoRR, abs/2109.01134,",
            "year": 2021
        },
        {
            "authors": [
                "Kaiyang Zhou",
                "Jingkang Yang",
                "Chen Change Loy",
                "Ziwei Liu"
            ],
            "title": "Conditional prompt learning for vision-language models",
            "year": 2022
        },
        {
            "authors": [
                "Daniel M Ziegler",
                "Nisan Stiennon",
                "Jeffrey Wu",
                "Tom B Brown",
                "Alec Radford",
                "Dario Amodei",
                "Paul Christiano",
                "Geoffrey Irving"
            ],
            "title": "Fine-tuning language models from human",
            "year": 1909
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Pre-trained vision-language models (VLMs) learning under language supervision (Radford et al., 2021; Jia et al., 2021; Yuan et al., 2021) exhibit promising zero-shot transferability. This encourages researchers to explore the capabilities of VLMs across a number of tasks in a zero-shot fashion. For example, Hong et al. (2022) employ CLIP for zero-shot text-driven avatar generation, Sain et al. (2023) adapt CLIP for zero-shot sketch-based image retrieval, and Li et al. (2023) achieve zero-shot image captioning without images. Nonetheless, the large domain gap between training and test data is still challenging for VLMs in a zero-shot circumstance. In this work, we investigate how to fulfill the domain gap during test time in various tasks without task-specific training corpus, namely, test time adaptation (TTA) for VLMs with a zero-shot prerequisite.\nOne pioneer TTA work in improving the zero-shot classification ability of VLMs is test time prompt tuning (TPT) (Manli et al., 2022). Given a single test sample, TPT optimizes the learnable prefix tokens by minimizing the entropy of model outputs to bootstrap its generalization capacity. Nevertheless, making the model confident in its predictions is a double-edged sword. It does reduce the test error and close the domain gap at a certain level (Wang et al., 2021a), but it makes the model stick to its incorrect predictions and unable to get out of the dilemma by itself as shown in the top of Figure 1a. Entropy minimization tends to make the model blindly confident.\n\u2217Part of this work is done during an internship at Baidu Inc. Yi Yang is the corresponding author.\nInspired by the success of the feedback mechanism in language and vision tasks (Ouyang et al., 2022; OpenAI, 2023; Pinto et al., 2023), we introduce feedback during test time to rectify the VLM output as shown in the bottom of Figure 1a. Previous feedback methods leverage labeled preference data to train a reward model (Ouyang et al., 2022; Lee et al., 2023) or use labels to calculate the reward (Cho et al., 2022; Pinto et al., 2023). Without ground truth, we refer to the well-recognized CLIP (Radford et al., 2021) model as the feedback resource. CLIP shows powerful generalization capacity across many VL tasks. The outputs of CLIP are also well-calibrated (without fine-tuning on a specific dataset) (Minderer et al., 2021), i.e., the score from CLIP accurately reflects its uncertainty about the input sample. This makes CLIP a reliable reward model. One more question is why feedback rather than directly tuning with CLIP supervision? Ouyang et al. (2022) demonstrate that model learning with feedback has better generalization abilities than a supervised fine-tuning model. We get the same conclusion from our empirical results. Furthermore, CLIP supervision cannot be directly used in generation tasks like image captioning, while the feedback mechanism is versatile.\nOur proposed framework, coined as reinforcement learning with CLIP feedback (RLCF), is flexible and universal for TTA with different VLMs in various tasks. With task-specific sampling strategies and a proper reward baseline choice, RLCF is applicable across zero-shot classification, text-image retrieval, and image captioning. In these tasks, the model is given a single test sample, we then sample K candidates from the output distribution. For discrimination tasks like classification and retrieval, the top-K sampling is applied; for the caption generalization task, a beam search method is adopted. Assuming the input is an image, like Figure 1b, the CLIP model first gives the CLIPScore (Hessel et al., 2021) between the image and all candidate sentences. As CLIPScore is always non-negative, the average score is subtracted from the calculated scores. This average baseline aims to distinguish which model behaviors are encouraged and which are discouraged. Then the learnable parameters in the TTA model are optimized by REINFORCE (Williams, 1992) algorithm.\nWhile the reward design and learning algorithm remain consistent across various tasks, the TTA pipelines are tailored to each specific task. For classification, we inherit the data augmentation and confidence selection pipeline from TPT (Manli et al., 2022), making it work for not only prefix tuning but also backbone adaptation. For retrieval, considering a large number of candidate entries, we only update the parameters w.r.t. the query for efficiency. For instance, we only tune the branch w.r.t. the input modality for a two-branch VLM like CLIP. For image captioning, we construct the TTA pipeline with two methods (Mokady et al., 2021; Nukrai et al., 2022) built upon large language models (LLMs). During TTA, we only tune the projector which projects the image into the LLM token embedding space. Plus, several task-agnostic practical tricks are applied, i.e., multiple reward models, episodic TTA (Wang et al., 2021a), and momentum buffer for incremental learning.\nTo summarize our contributions: 1) To the best of our knowledge, RLCF is the first universal fully TTA framework for improving the zero-shot generalization capacity of VLMs across different tasks. 2) We develop a novel reward function for test time RL with CLIP. It is simple yet effective. Compared to previous methods (Cho et al., 2022) in the training stage, it demonstrates that CLIP can be used as a practical reward model alone, even with a single test sample. 3) We design task-specific TTA pipelines for three VL tasks with RLCF. Extensive experiments with promising results validate the effectiveness of RLCF in boosting the zero-shot performance of different VLMs."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Reinforcement learning in language and vision The most well-known application of RL in natural language process is reinforcement learning with human feedback (RLHF) (Ouyang et al., 2022; OpenAI, 2023). A reward model is trained with preference data collected from humans, and it is used to fine-tune the LLM via proximal policy optimization (PPO, Schulman et al. (2017)). Similar approaches are applied in (Ziegler et al., 2019; Stiennon et al., 2020; Bai et al., 2022; Glaese et al., 2022). In prompt engineering for language models, RLPrompt (Deng et al., 2022) and TEMPERA (Zhang et al., 2023) search for discrete text prompts by RL. RL has also been widely applied in vision and multi-modal research. A comprehensive study of deep RL in computer vision can be found at (Le et al., 2022). Recently, Pinto et al. (Pinto et al., 2023) optimize vision task metrics using RL and achieve promising results, demonstrating the effectiveness of RL in vision. In the multi-modal area, ImageReward (Xu et al., 2023) collects preference data and trains a reward model for text-to-image generation tasks, similar to RLHF in GPT models. SCST (Rennie et al., 2017) apply CIDEr metric as a reward function with REINFORCE (Williams, 1992) algorithm to improve the generation quality in image captioning during training. Cho et al. (Cho et al., 2022) explore the possibility of using CLIPScore (Hessel et al., 2021) as the reward in image captioning. Their empirical results show that CLIPScore cannot be an independent reward function and should be combined with a grammar regularization or CIDEr metric. Nevertheless, the training of the grammar head and calculation of CIDEr metric both need reference text, which is unavailable at test time.\nTest-time adaptation Test-time adaptation (TTA) aims to address the distribution shift between training and test data during test time (Sun et al., 2020; Liu et al., 2021; Wang et al., 2021a). Testtime training (TTT, Sun et al. (2020)) allows modifications to the training pipeline. In such cases, self-supervised auxiliary tasks are incorporated to help the model adapt to the distribution of test data (Sun et al., 2020; Liu et al., 2021; Lin et al., 2023). For example, TTT+ (Liu et al., 2021) utilizes instance discrimination tasks from contrastive learning (Chen et al., 2020). On the other hand, fully TTA assumes that the training pipeline cannot be modified as the training data is unavailable (Wang et al., 2021a). Two popular techniques in fully TTA are normalization layer adaptation and entropy minimization. Normalization layer adaptation updates data statistics or parameters of the normalization layer based on batched test samples (Wang et al., 2021a; Schneider et al., 2020; Niu et al., 2023) or augmented data views from a single test sample (Zhang et al., 2022a). Entropy minimization aims to make the model confident in its predictions to reduce generalization error (Wang et al., 2021a; Zhang et al., 2022a; Manli et al., 2022; Niu et al., 2022; 2023). There is also a retrieval-augmented TTA method (Zancato et al., 2023), which uses CLIP to retrieve informative data from an external large dataset and update the decision boundary during test time."
        },
        {
            "heading": "3 METHOD",
            "text": ""
        },
        {
            "heading": "3.1 PRELIMINARIES",
            "text": "Fully test-time adaptation in vision-language tasks Let f\u03b8(\u00b7) represent a VLM trained on imagetext pairs Dtrain = {(ti,vi)}Ni=1 with parameter \u03b8, where ti \u2208 Ttrain (training text space) and vi \u2208 Vtrain (training image space). The objective of TTA (Sun et al., 2020; Wang et al., 2021a) is to boost f\u03b8(t) or f\u03b8(v) on domain-shifted test samples Dtest = {(tj}Mj=1 or Dtest = {(vj}Mj=1, where tj \u2208 Ttest (testing text space), vj \u2208 Vtest (testing image space), Ttest \u0338= Ttrain, and Vtest \u0338= Vtrain. We assume that the VLM takes either text or image as input and outputs the other modality. In fully TTA, the training data are unavailable, and the training pipeline cannot be modified. Following TPT (Manli et al., 2022) and MEMO (Zhang et al., 2022a), the adaptation is conducted with a single test point, i.e., the VLM is exposed to only one tj or vj .\nContrastive Language-Image Pre-training (CLIP) CLIP (Radford et al., 2021) comprises an image encoder g(\u00b7) and a text encoder h(\u00b7). CLIP is pre-trained using a contrastive loss that encourages similarity between feature vectors of paired images and text, aligning them in a shared embedding space. Once pre-trained, CLIP can assess the similarity between the text t and image v as follows:\nCLIP(t,v) = cos(h(t), g(v)), (1)\nwhere cos(\u00b7, \u00b7) represents the cosine similarity. For image classification with CLIP, the input text consists of the prompt plus the class names, i.e., t = {pt; \"dog\"}, where prompt pt = \"a photo of a\"."
        },
        {
            "heading": "3.2 TEST-TIME ADAPTATION WITH CLIP REWARD",
            "text": ""
        },
        {
            "heading": "3.2.1 REINFORCEMENT LEARNING WITH CLIP FEEDBACK",
            "text": "Without loss of generality, we first consider the case where the VLM f\u03b8(\u00b7) takes an image v as input and maps it to text t. During TTA, our goal is to learn a conditional distribution P (t|v, \u03b8) = f\u03b8(v) that maximizes a reward functionR(\u00b7, \u00b7). Formally, the optimization problem during TTA is:\nmax \u03b8\nEt\u223cP (\u00b7|v,\u03b8)R(t,v). (2)\nDifferent from previous methods (Rennie et al., 2017; Cho et al., 2022; Pinto et al., 2023) which maximizes the expected reward over batched training samples, here we only maximize the expected reward over a single test sample v \u2208 Vtest. Policy gradient with REINFORCE To compute the gradient of the non-differentiable reward function, REINFORCE (Williams, 1992) is adopted to calculate \u2207\u03b8Et\u223cP [R(t,v)]. It uses the so-called \"log-derivative trick\" to estimate the gradient of the expected reward for a given input:\n\u2207\u03b8Et\u223cP [R(t,v)] = Et\u223cP [R(t,v)\u2207\u03b8 logP (t|v; \u03b8)]. (3)\nIn a VL task, the input and output modalities are closely related, e.g., the input is an image and the output is the description of the image. Therefore, we can use CLIP to evaluate the similarity between the input and output, and the model can maximize this similarity to align with task goals. Similar to Cho et al. (2022), we use CLIPScore (Hessel et al., 2021) as the reward:\nCLIP-S(t,v) = w \u00d7max(CLIP(t,v), 0), (4)\nwhere w = 2.5 is a constant. CLIPScore is always non-negative, which means it encourages all model behaviors. However, for an irrelevant sampled image-text pair in Figure 1b, we expect the reward model to provide negative feedback to discourage such behavior. Cho et al. (Cho et al., 2022) adopt a greedy search baseline which needs to be combined with a grammar regularization or CIDEr metric to be a practical reward function. In this work, we demonstrate that with proper sampling strategies and baseline, CLIPScore can also be used as the sole reward function in different VL tasks. Specifically, we set the reward baseline as the average CLIPScore of sampled image-text pairs. The reward function with baseline becomes:\nR(t,v) = CLIP-S(t,v)\u2212 Et\u223cP [CLIP-S(t,v)]. (5)\nIt is straightforward to get the reward function for a VLM which takes text t as input and return an image v according to Eq. (5). The sampling strategies will be presented in the next section."
        },
        {
            "heading": "3.2.2 TASK-SPECIFIC FULLY TEST-TIME ADAPTATION",
            "text": "RLCF is flexible and applicable across various VL tasks, and we apply RLCF to three different VL tasks in this work. For all tasks, the VLM solely learns through REINFORCE with Eq. (5) as the reward function during test time. However, VLMs and sampling strategies vary with tasks. Next, we introduce our task-specific fully TTA pipelines.\nZero-shot image classification on OOD data Figure 2 illustrates the fully TTA pipeline for zeroshot image classification with RLCF. Without loss of generality, we also choose CLIP as the classifier. The TTA pipelines include two adaptation manners: prompt tuning and image encoder tuning. TPT (Manli et al., 2022) shows that entropy minimization for image encoder tuning results in inferior performance compared to prompt tuning. By contrast, RLCF works both with prompt tuning and image encoder tuning, demonstrating its versatility.\nIn Figure 2, given a test image v, it is first operated with data augmentors {\u03c41, \u03c42, . . . , \u03c4n} for multiple different views. Following TPT (Manli et al., 2022) and SAR (Niu et al., 2023), we only reserve the confident samples with low-entropy predictions, namely, the entropy H(P (t|\u03c4(v))) of the selected view should be low. High-entropy predictions are considered unreliable as they lack confidence in their outputs. In practice, we use the bottom 10th percentile of n = 64 augmented views with low entropies as TPT (Manli et al., 2022). For each low-entropy view, class names of the top-K predictions are used to calculate their CLIP rewards according to Eq. (5). The learnable parameters are then optimized to maximize the expected reward by gradient descent as Eq. (3).\nOne point that needs to be clarified is why using the CLIP reward as feedback rather than directly fine-tuning the model with CLIP supervision. For example, methods like knowledge distillation (KD, Hinton et al. (2015)) or pseudo-label (Lee et al., 2013). InstructGPT (Ouyang et al., 2022) demonstrates that model learning with feedback has better generalization capabilities compared to a supervised fine-tuning model. In our context, KD or pseudo-label requires a weak model (student) to mimic a strong model (teacher). However, it is worth noting that the student may be correct while the teacher may be incorrect. For instance, given an image of a dog, the top-3 predictions of the student and teacher models are {dog, horse, tree} and {cat, dog, horse}, respectively. For KD or pseudo-label, the student will be forced to follow the incorrect behaviors of the teacher. In contrast, the feedback mechanism only assesses the sampled results from the student, less likely to alter the correct prediction. In such cases, the feedback mechanism combines the merits of both the student and the teacher. Another important reason is that CLIP supervision cannot be directly used in generalization tasks like image captioning, while the feedback mechanism is universal.\nZero-shot text-image retrieval The fully TTA pipeline for zero-shot retrieval with RLCF is presented in Figure 3. CLIP also serves as the zero-shot retrieval model. For retrieval, the number of candidates is usually large, so we only update the parameters with respect to the query for efficiency.\nFor text-to-image retrieval, the image encoder remains fixed, while the text encoder is frozen in the other case. Given a query, top-K sampling is employed to the returned results to calculate the reward. Unlike image classification, no augmentations are used for the input query. The retrieval task requires a holistic understanding of the input query rather than identifying a single object. Augmentations like crop and flip may lead to corrupt semantics.\nZero-shot and cross-domain image captioning Figure 4 illustrates the fully TTA pipeline for image captioning with RLCF. The captioning TTA pipeline is built upon two LLM-based methods: CapDec (Nukrai et al., 2022) and CLIPCap (Mokady et al., 2021). CapDec is trained only with text and CLIPCap is trained with images. TTA with CapDec is undertaken with a zero-shot prerequisite, and TTA with CLIPCap is cross-domain. During the test, CapDec and CLIPCap will be given unseen and domain-shifted images, respectively. Both CapDec and CLIPCap utilize a projector (e.g., an MLP or transformer (Vaswani et al., 2017)) to project CLIP feature vectors into the token embedding space of the LLM. Only the projector is updated through policy gradient, while the LLM remains fixed during TTA. Beam search is employed to sample K generated captions for reward calculation."
        },
        {
            "heading": "3.2.3 TEST-TIME ADAPTATION TRICKS",
            "text": "In this section, we introduce several general TTA techniques applicable across different tasks.\nMultiple reward models with weights By default, a single CLIP-ViT-L/14 is used as the reward model. An ensemble of multiple reward models can be used for better feedback. We assign scores based on human preference for different CLIP models: {CLIP-ViT-L/14-336: 10, CLIP-ViT-L/14: 5, CLIP-RN50\u00d764: 3}. These scores are then normalized to sum up to 1, serving as weights for the ensemble. CLIP-RN uses a ResNet (He et al., 2016) as the image encoder, while CLIP-ViT adopts a vison transformer (Dosovitskiy et al., 2021).\nEpisodic TTA The model is exposed to only a single test sample once, making the learned knowledge unreliable for other samples. Hence, after each TTA process, the model parameters \u03b8 are reset to the initial state \u03b8\u22c6 like (Wang et al., 2021a; Manli et al., 2022). It is called episodic TTA.\nMomentum buffer While episodic TTA ensures reliability, it limits the incremental learning ability of the model. To address this issue, we introduce a momentum buffer \u03be, initialized as \u03be \u2190 \u03b8\u22c6. After a TTA process, \u03b8 becomes \u03b8, and \u03be is updated by \u03be \u2190 m\u03be + (1 \u2212 m)\u03b8, where m \u2208 [0, 1) is a momemtum coefficient. Every Bs samples, we update \u03b8\u22c6 \u2190 \u03be. At the start of the next TTA process, \u03b8 \u2190 \u03b8\u22c6, allowing the utilization of the learned knowledge. The momentum buffer functions similarly to an ensemble of different models, resembling model soups (Wortsman et al., 2022)."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": "This section presents the experimental TTA results in three tasks. For variants of our method, RLCF uses a CLIP-ViT-L/14 as the reward model, RLCF-S adopts weighted reward sum of {CLIP-ViTL/14-336, CLIP-ViT-L/14, CLIP-RN50\u00d764}, and RLCF-S-M adds the momentum buffer."
        },
        {
            "heading": "4.1 ZERO-SHOT IMAGE CLASSIFICATION ON OOD DATA",
            "text": "Datasets Following CLIP and TPT, we test RLCF on ImageNet (Deng et al., 2009) and its four variant test sets with distribution shifts: ImageNet-A (Hendrycks et al., 2021b), ImageNet-V2 (Recht et al., 2019), ImageNet-R (Hendrycks et al., 2021a), and ImageNet-Sketch (Wang et al., 2019). ImageNet-A consists 7,500 natural adversarial images misclassified by a ResNet-50. ImageNet-V2 contains 10,000 natural images from different sources. ImageNet-R collects 30,000 images with artistic renditions. ImageNet-Sketch includes 50,000 black and white sketch images.\nTable 1: Top-1 accuracy of zero-shot image classification with TTA on OOD data. KD uses a\nCLIP-ViT-L/14 as the teacher. The best and second-best results are highlighted. Improvement in accuracy of RLCF compared to the baselines (zero-shot CLIP-ViT-B/16 or CoOp) is in (\u2191blue).\nBaselines We compare RLCF with few-shot prompt tuning methods for CLIP \u2014 CoOp (Zhou et al., 2021) and CoCoOp (Zhou et al., 2022) (16 shots on ImageNet), state-of-the-art test-time prompt tuning methods \u2014 TPT (Manli et al., 2022), and knowledge distillation (KD (Hinton et al., 2015), ATKD (Guo et al., 2020)), which use the reward model as the teacher during test time. TPT + CoOp means TPT adopts the learned prompts of CoOp as the initialization, otherwise, TPT uses token embedding of a hard prompt \"a photo of a\" as initial weights. For all prompt tuning methods, the length of learnable prompts is 4. Results of Pseudo-label (Lee et al., 2013) are also presented.\nImplementation details For prompt tuning, the learning rate is 7e-3, the weight decay value is 5e-4, and the optimizer is AdamW (Loshchilov & Hutter, 2019). For image encoder tuning, the learning rate is decreased to 1e-5. Given a test sample, the parameters will be optimized for 3 steps to maximize the reward of the top-3 (sampling factor K = 3) predictions. The momentum coefficient m = 0.9998 and update interval Bs = 64 for the momentum buffer.\nResults In Table 1, RLCF largely improves the zero-shot generalization capacity of CLIP-ViTB/16 and outperforms previous methods. Notably, on ImageNet-A/V2/R, RLCF with CLIP-ViTB/16 surpasses the reward model \u2014 CLIP-ViT-L/14. This shows that RLCF effectively combines the capabilities of both the TTA model and the reward model through the feedback mechanism, something that KD or pseudo-label cannot achieve. RLCF significantly outperforms the entropy minimization method \u2014 TPT. TPT can only learn from the TTA model itself and lacks awareness of the correctness of its predictions. Figure 5a presents the expected calibration error (ECE) (Guo et al., 2017) of TPT and RLCF. The ECE of the two both increases along with the TTA steps, but the ECE of RLCF is clearly lower. This means the output of RLCF better reflects its uncertainty about the input and is more reliable. In Figure 1a&8, RLCF provides multiple positive scores for various objects, preventing the model from becoming blindly confident. In Figure 5b, the top-5 accuracy of TPT drops with more steps. The model is stuck in its incorrect predictions and pushes away the ground truth as shown in Figure 1a. By contrast, there is no such issue for RLCF in Figure 5c.\nAblation study about sampling factors and reward model choices can be found in Appendix B."
        },
        {
            "heading": "4.2 ZERO-SHOT TEXT-IMAGE RETRIEVAL",
            "text": "Implementation details For text-image retrieval, we use the test set of Flickr30K (Plummer et al., 2015) and test split of MS-COCO (Lin et al., 2014) divided by Karpathy et al. (Karpathy & Fei-Fei, 2015). Each image in the two test sets corresponds to 5 sentences. CLIP-ViT-B/16 is adopted as the retrieval model. The learning rate is 1e-6, the weight decay value is 5e-4, and AdamW optimizer is used. For MS-COCO, the sampling factor K = 12 for text-to-image retrieval, and K = 20 for the other case. For Flickr30K, K = 12 and K = 16 for text-to-image and image-to-text retrieval, respectively. The adaptation steps are 8. For the momentum buffer, m = 0.9998 and Bs = 64. We also compare RLCF with knowledge distillation (KD) with CLIP-ViT-L/14 as the teacher.\nResults Table 2 presents the retrieval results on MS-COCO and Flickr30K. RLCF demonstrates significant improvement compared to the zero-shot baseline and even outperforms the most powerful CLIP-ViT-L/14-336. Similar phenomena are also observed in zero-shot classification. The feedback mechanism reserves the merits of the TTA model and makes the TTA model improve with the reward model. In contrast, KD or pseudo-label forces the student to mimic the teacher regardless of the correctness of the teacher as discussed in Sec. 3.2.2. In KD for supervised classification (Hinton et al., 2015; Zhao et al., 2022; Wang et al., 2021b), the student is generally worse than the teacher due to their capacity gap and incomplete learning. Nevertheless, RLCF can surpass the powerful reward model with the feedback mechanism during test time in a zero-shot circumstance."
        },
        {
            "heading": "4.3 IMAGE CAPTIONING",
            "text": "Datasets To test the adaptation ability of RLCF for captioning models in a zero-shot or crossdomain condition, we train the captioning model on MS-COCO train set (Lin et al., 2014) and test it on the test set of Flickr30K (Plummer et al., 2015) and validation set of NoCaps (Agrawal et al., 2019). NoCaps validation set contains three splits according to whether contains MS-COCO objects: in domain contains only MS-COCO objects, near domain contains both MS-COCO and novel objects, and out domain contains only novel objects.\nImplementation details CLIPCap (Mokady et al., 2021) and CapDec (Nukrai et al., 2022), two LLM-based methods, are chosen as the captioning models. The two have the same architecture, while CLIPCap is trained with CLIP-ViT-B/16 image embedding and CapDec is trained with CLIPViT-B/16 text embedding. The projector in Figure 4 is an 8-layer transformer encoder that contains about 43M parameters. The LLM is an OPT-125M (Zhang et al., 2022b). During TTA, we only tune the parameters of the projector. For CLIPCap, the learning rate is 2e-6, and sampling factor K = 10; for CapDec, the learning rate is 5e-6 on Flickr30K, 3e-6 on NoCaps, and K = 6. No weight decay is applied. The optimizer is AdamW. The TTA step is 4. After TTA, captions are generated with a beam search with a width of 5 and the final caption is the one with the highest score.\nResults Table 3 presents results for image captioning. A weakly supervised method \u2014 MAGIC (Su et al., 2022) and a zero-shot method \u2014DeCap (Li et al., 2023), are included for reference. The reported metrics include BLEU@4, CIDEr, SPICE, and RefCLIPScore (Hessel et al., 2021). RefCLIPScore reflects the similarity between generated text and reference captions. The improvements in CIDEr metric (Vedantam et al., 2015) are highlighted. For all metrics, both CapDec and CLIPCap with RLCF significantly improve upon the baselines. This demonstrates the strong generalization ability of RLCF in image captioning, even with a single test sample. It is noteworthy that CLIPCap\nachieves greater improvements in CIDEr (up to 9.2) compared to CapDec. CLIPCap can also use a large sampling factor K. This is possible because CLIPCap can generate higher-quality candidate captions. The results of RLCF-S-M are not shown as it is no better than RLCF-S.\nQualitative results Figure 6 displays the intermediate-generated captions and their corresponding rewards. The visualization reveals that the CLIP reward model favors captions that provide a holistic description of the image. Through feedback, the generation of such captions is encouraged. During TTA, captions aligned with the preferences of CLIP are given higher priority. Please refer to Figure 7 in Appendix A for more visualization cases."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "In this work, we introduce reinforcement learning with CLIP feedback (RLCF) to improve the zeroshot generalization ability of VLMs on the fly. A novel reward function with CLIP is developed. We instantiate three TTA pipelines for image classification, text-image retrieval, and image captioning with task-specific sampling strategies and parameter tuning manners. With RLCF, the zero-shot generalization capacity of various VLMs is boosted significantly. We hope RLCF can provide heuristic information for future research that employs TTA with feedback from large foundation models."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "This work was supported in part by the Australian Research Council (ARC) under Grant DP200100938. Thanks Chao Liang for his helpful discussions."
        },
        {
            "heading": "B ABLATION STUDY",
            "text": "B.1 SAMPLING FACTOR\nWhen using RLCF with TTA for various VL tasks, we sample K candidates from output distribution for reward calculations. In this section, we examine how K affects different tasks and models.\nImage classification on OOD data Figure 9 shows the average top-1 and top-5 accuracy on ImageNet-A, ImageNet-V2, and ImageNet-R for different K in image classification. RLCF reduces to pseudo-label (Lee et al., 2013) when K = 1. A larger K improves top-5 accuracy, but not top1. Too many sampled classes may make the optimization process difficult for policy gradient. For example, when K = 5 and only one class gets a positive score and other classes get negative scores, pushing away 4 negative classes may cause unpredictable behavior and make the model miss the ground truth after gradient updating.\nZero-shot text-image retrieval Table 4 presents the effect of sampling factor K in zero-shot textimage retrieval. Similar to image classification, a larger K generally leads to better Recall@5 and Recall@10 compared to a smaller K. However, a smaller K tends to produce better Recall@1 in\nmost cases. In MS-COCO and Flickr30K, one image has 5 reference captions, so the sampling factor for image-to-text retrieval is larger than K for text-to-image retrieval.\nImage captioning Table 5 illustrates the effect of different values of sampling factor K in crossdomain image captioning. The optimal K varies for different image captioning models. CLIPCap has better captioning capabilities than CapDec, so it can produce better candidates. Therefore, a larger K is suitable for CLIPCap.\nFrom the ablation study of sampling factor K, we find that the choice of K depends on the tasks and VLMs. Different tasks and models require various sampling strategies.\nB.2 DIFFERENT REWARD MODELS\nRLCF relies on the good quality of the CLIP reward models. In this section, we show the influence of different CLIP reward models in image classification and image captioning.\nAs shown in Table 6, RLCF is robust to different reward models. Compared to the baseline CoOp, RLCF can achieve improvements even with a CLIP-RN50\u00d74 as the reward model, which is worse than the prompt tuning model CLIP-ViT-B/16. When the prompt tuning model and the reward model are the same, RLCF is also better than the state-of-the-art test-time prompt tuning method \u2014 TPT (Manli et al., 2022). With CLIP-ViT-L/14 as the reward model, RLCF with prompt tuning is slightly better than the ensemble result. It is worth noting that RLCF with image encoder tuning in Table 1 is obviously better than the ensemble results in the OOD average performance. Compared to the ensemble method, RLCF can adapt to the test distribution with the feedback mechanism. This is why RLCF shows better performance than the ensemble results.\nWe also test different reward models in image captioning. Results are shown in Table 7. CapDec and CLIPCap both use CLIP-ViT-B/16 as the image embedding extractor. RLCF with different reward\nmodels can always achieve significant improvements in the near and out domain data. This shows the robustness of RLCF to open domain scenarios with different CLIP reward models.\nB.3 GPU RUNTIME AND MEMORY\nEfficiency is also important in TTA. We provide the GPU time and memory in Table 8.\nCompared to TPT, the inference time of RLCF increases by a constant amount for different TTA steps and datasets, i.e., roughly 0.03s per sample. For each sample, the CLIP reward model only needs to run the image encoder once. This is the source of the 0.03s increase. The text features of CLIP reward model are always the same because the class names are fixed.\nImageNet-A has 200 classes, while ImageNet-V2 has 1000 classes. For prompt tuning on ImageNetV2, the input batch size of the CLIP text encoder is 1000, and we need to re-run the text encoder to update the text features after each TTA step. This is why prompt tuning is slower and consumes more memory than image encoder tuning. For image encoder tuning, the text features are unchanged and the image encoder only has a single image as input."
        }
    ],
    "title": "ZERO-SHOT GENERALIZATION IN VISION-LANGUAGE MODELS",
    "year": 2024
}