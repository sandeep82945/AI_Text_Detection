{
    "abstractText": "Recent works have shown that deep neural networks are vulnerable to adversarial examples that find samples close to the original image but can make the model misclassify. Even with access only to the model\u2019s output, an attacker can employ black-box attacks to generate such adversarial examples. In this work, we propose a simple and lightweight defense against black-box attacks by adding random noise to hidden features at intermediate layers of the model at inference time. Our theoretical analysis confirms that this method effectively enhances the model\u2019s resilience against both score-based and decision-based black-box attacks. Importantly, our defense does not necessitate adversarial training and has minimal impact on accuracy, rendering it applicable to any pre-trained model. Our analysis also reveals the significance of selectively adding noise to different parts of the model based on the gradient of the adversarial objective function, which can be varied during the attack. We demonstrate the robustness of our defense against multiple black-box attacks through extensive empirical experiments involving diverse models with various architectures.",
    "authors": [],
    "id": "SP:7b91655130530b4e6fe4a1a1bd259faefa2db7d3",
    "references": [
        {
            "authors": [
                "Abdullah Al-Dujaili",
                "Una-May O\u2019Reilly"
            ],
            "title": "Sign bits are all you need for black-box attacks",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Maksym Andriushchenko",
                "Francesco Croce",
                "Nicolas Flammarion",
                "Matthias Hein"
            ],
            "title": "Square attack: a query-efficient black-box adversarial attack via random search",
            "venue": "In Computer Vision\u2013ECCV 2020: 16th European Conference,",
            "year": 2020
        },
        {
            "authors": [
                "Anish Athalye",
                "Logan Engstrom",
                "Andrew Ilyas",
                "Kevin Kwok"
            ],
            "title": "Synthesizing robust adversarial examples",
            "venue": "Proceedings of the 35th International Conference on Machine Learning,",
            "year": 2018
        },
        {
            "authors": [
                "Junyoung Byun",
                "Hyojun Go",
                "Changick Kim"
            ],
            "title": "On the effectiveness of small input noise for defending against query-based black-box attacks",
            "venue": "IEEE/CVF Winter Conference on Applications of Computer Vision (WACV),",
            "year": 2022
        },
        {
            "authors": [
                "Nicholas Carlini",
                "David Wagner"
            ],
            "title": "Towards evaluating the robustness of neural networks",
            "venue": "In IEEE Symposium on Security and Privacy (SP),",
            "year": 2017
        },
        {
            "authors": [
                "Jinghui Chen",
                "Quanquan Gu"
            ],
            "title": "Rays: A ray searching method for hard-label adversarial attack",
            "venue": "In Proceedings of the 26rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,",
            "year": 2020
        },
        {
            "authors": [
                "Pin-Yu Chen",
                "Huan Zhang",
                "Yash Sharma",
                "Jinfeng Yi",
                "Cho-Jui Hsieh"
            ],
            "title": "Zoo: Zeroth order optimization based black-box attacks to deep neural networks without training substitute models",
            "venue": "In Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security,",
            "year": 2017
        },
        {
            "authors": [
                "Sizhe Chen",
                "Zhehao Huang",
                "Qinghua Tao",
                "Yingwen Wu",
                "Cihang Xie",
                "Xiaolin Huang"
            ],
            "title": "Adversarial attack on attackers: Post-process to mitigate black-box score-based query attacks",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Steven Chen",
                "Nicholas Carlini",
                "David Wagner"
            ],
            "title": "Stateful detection of black-box adversarial attacks",
            "venue": "In Proceedings of the 1st ACM Workshop on Security and Privacy on Artificial Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Weilun Chen",
                "Zhaoxiang Zhang",
                "Xiaolin Hu",
                "Baoyuan Wu"
            ],
            "title": "Boosting decision-based blackbox adversarial attacks with random sign flip",
            "venue": "In Proceedings of the European Conference on Computer Vision,",
            "year": 2020
        },
        {
            "authors": [
                "Jeremy Cohen",
                "Elan Rosenfeld",
                "Zico Kolter"
            ],
            "title": "Certified adversarial robustness via randomized smoothing",
            "venue": "In international conference on machine learning,",
            "year": 2019
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "Lucas Beyer",
                "Alexander Kolesnikov",
                "Dirk Weissenborn",
                "Xiaohua Zhai",
                "Thomas Unterthiner",
                "Mostafa Dehghani",
                "Matthias Minderer",
                "Georg Heigold",
                "Sylvain Gelly"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
            "venue": "International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Ian J. Goodfellow",
                "Jonathon Shlens",
                "Christian Szegedy"
            ],
            "title": "Explaining and harnessing adversarial examples",
            "venue": "In Proceedings of the 3rd International Conference on Learning Representations (ICLR),",
            "year": 2015
        },
        {
            "authors": [
                "Chuan Guo",
                "Jacob Gardner",
                "Yurong You",
                "Andrew Gordon Wilson",
                "Kilian Weinberger"
            ],
            "title": "Simple black-box adversarial attacks",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2016
        },
        {
            "authors": [
                "Zhezhi He",
                "Adnan Siraj Rakin",
                "Deliang Fan"
            ],
            "title": "Parametric noise injection: Trainable randomness to improve deep neural network robustness against adversarial attack",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Andrew Ilyas",
                "Logan Engstrom",
                "Anish Athalye",
                "Jessy Lin"
            ],
            "title": "Black-box adversarial attacks with limited queries and information",
            "venue": "In International conference on machine learning,",
            "year": 2018
        },
        {
            "authors": [
                "Andrew Ilyas",
                "Logan Engstrom",
                "Aleksander Madry"
            ],
            "title": "Prior convictions: Black-box adversarial attacks with bandits and priors",
            "venue": "In International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "Alex Krizhevsky",
                "Geoffrey Hinton"
            ],
            "title": "Learning multiple layers of features from tiny images",
            "venue": "Technical Report",
            "year": 2009
        },
        {
            "authors": [
                "Huiying Li",
                "Shawn Shan",
                "Emily Wenger",
                "Jiayun Zhang",
                "Haitao Zheng",
                "Ben Y Zhao"
            ],
            "title": "Blacklight: Scalable defense for neural networks against {Query-Based}{Black-Box} attacks",
            "venue": "In 31st USENIX Security Symposium (USENIX Security",
            "year": 2022
        },
        {
            "authors": [
                "Sijia Liu",
                "Pin-Yu Chen",
                "Xiangyi Chen",
                "Mingyi Hong"
            ],
            "title": "signsgd via zeroth-order oracle",
            "venue": "In International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "Xuanqing Liu",
                "Minhao Cheng",
                "Huan Zhang",
                "Cho-Jui Hsieh"
            ],
            "title": "Towards robust neural networks via random self-ensemble",
            "venue": "In European Conference on Computer Vision,",
            "year": 2017
        },
        {
            "authors": [
                "Aleksander Madry",
                "Aleksandar Makelov",
                "Ludwig Schmidt",
                "Dimitris Tsipras",
                "Adrian Vladu"
            ],
            "title": "Towards deep learning models resistant to adversarial attacks",
            "venue": "In Proceedings of the 6th International Conference on Learning Representations (ICLR),",
            "year": 2018
        },
        {
            "authors": [
                "Ren Pang",
                "Xinyang Zhang",
                "Shouling Ji",
                "Xiapu Luo",
                "Ting Wang"
            ],
            "title": "Advmind: Inferring adversary intent of black-box attacks",
            "venue": "In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining,",
            "year": 2020
        },
        {
            "authors": [
                "Tianyu Pang",
                "Kun Xu",
                "Chao Du",
                "Ning Chen",
                "Jun Zhu"
            ],
            "title": "Improving adversarial robustness via promoting ensemble diversity",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Nicolas Papernot",
                "Patrick McDaniel",
                "Ananthram Swami",
                "Richard Harang"
            ],
            "title": "Crafting adversarial input sequences for recurrent neural networks",
            "venue": "In IEEE Military Communications Conference,",
            "year": 2016
        },
        {
            "authors": [
                "Nicolas Papernot",
                "Patrick D. McDaniel",
                "Somesh Jha",
                "Matt Fredrikson",
                "Z. Berkay Celik",
                "Ananthram Swami"
            ],
            "title": "The limitations of deep learning in adversarial settings",
            "venue": "In Proceedings of the IEEE European Symposium on Security and Privacy (EuroS&P),",
            "year": 2016
        },
        {
            "authors": [
                "Nicolas Papernot",
                "Patrick McDaniel",
                "Ian Goodfellow",
                "Somesh Jha",
                "Z Berkay Celik",
                "Ananthram Swami"
            ],
            "title": "Practical black-box attacks against machine learning",
            "venue": "In Proceedings of the ACM on Asia Conference on Computer and Communications Security,",
            "year": 2017
        },
        {
            "authors": [
                "Zeyu Qin",
                "Yanbo Fan",
                "Hongyuan Zha",
                "Baoyuan Wu"
            ],
            "title": "Random noise defense against query-based black-box attacks",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Ali Rahmati",
                "Seyed-Mohsen Moosavi-Dezfooli",
                "Pascal Frossard",
                "Huaiyu Dai"
            ],
            "title": "Geoda: A geometric framework for black-box adversarial attacks",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2020
        },
        {
            "authors": [
                "Hadi Salman",
                "Jerry Li",
                "Ilya Razenshteyn",
                "Pengchuan Zhang",
                "Huan Zhang",
                "Sebastien Bubeck",
                "Greg Yang"
            ],
            "title": "Provably robust deep learning via adversarially trained smoothed classifiers",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Karen Simonyan",
                "Andrew Zisserman"
            ],
            "title": "Very deep convolutional networks for large-scale image recognition",
            "venue": "In Yoshua Bengio and Yann LeCun (eds.), 3rd International Conference on Learning Representations,",
            "year": 2015
        },
        {
            "authors": [
                "Aman Sinha",
                "Hongseok Namkoong",
                "John Duchi"
            ],
            "title": "Certifiable some distributional robustness with principled adversarial training",
            "venue": "International Conference on Learning Representations (ICLR),",
            "year": 2017
        },
        {
            "authors": [
                "Christian Szegedy",
                "Wojciech Zaremba",
                "Ilya Sutskever",
                "Joan Bruna",
                "Dumitru Erhan",
                "Ian J. Goodfellow",
                "Rob Fergus"
            ],
            "title": "Intriguing properties of neural networks",
            "venue": "In Proceedings of the 2nd International Conference on Learning Representations (ICLR), Banff, Canada,",
            "year": 2014
        },
        {
            "authors": [
                "Hugo Touvron",
                "Matthieu Cord",
                "Matthijs Douze",
                "Francisco Massa",
                "Alexandre Sablayrolles",
                "Herve Jegou"
            ],
            "title": "Training data-efficient image transformers & distillation through attention",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Florian Tram\u00e8r",
                "Alexey Kurakin",
                "Nicolas Papernot",
                "Dan Boneh",
                "Patrick McDaniel"
            ],
            "title": "Ensemble adversarial training: Attacks and defenses",
            "venue": "International Conference on Learning Representations (ICLR),",
            "year": 2018
        },
        {
            "authors": [
                "Binghui Wang",
                "Xiaoyu Cao",
                "Neil Zhenqiang Gong"
            ],
            "title": "On certifying robustness against backdoor attacks via randomized smoothing",
            "venue": "arXiv preprint:2002.11750,",
            "year": 2020
        },
        {
            "authors": [
                "Huanrui Yang",
                "Jingyang Zhang",
                "Hongliang Dong",
                "Nathan Inkawhich",
                "Andrew Gardner",
                "Andrew Touchet",
                "Wesley Wilkes",
                "Heath Berry",
                "Hai Li"
            ],
            "title": "Dverge: diversifying vulnerabilities for enhanced robust generation of ensembles",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Dinghuai Zhang",
                "Mao Ye",
                "Chengyue Gong",
                "Zhanxing Zhu",
                "Qiang Liu"
            ],
            "title": "Black-box certification with randomized smoothing: A functional optimization based framework",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Pang"
            ],
            "title": "We leave these to future works. Similar to the related works on randomized defenses, our empirical evaluation has limitation for it is designed to assess the relative effectiveness of randomized models",
            "venue": "From the adversary perspective,",
            "year": 2020
        },
        {
            "authors": [
                "Qin"
            ],
            "title": "2021) for their analysis. We also provided the error of the first-order approximation in Section F.1. The histogram of the error implies that the added noise",
            "year": 2021
        },
        {
            "authors": [
                "Byun"
            ],
            "title": "2022), our threat model focuses on defending against black-box, query-based attacks, as it is a more realistic scenario in practice",
            "year": 2022
        },
        {
            "authors": [
                "W Carlini",
                "Wagner",
                "PGD Madry"
            ],
            "title": "2018) with l\u221e constraint \u03b5 = 0.03. As observed in Table 13, the proposed defense can boost the robustness against these white-box attacks while having a negligible degradation in clean accuracy. We conjecture that adding stochasticity to the model can transform it into a smoothed classifier and therefore reduce the adversarial effect",
            "venue": "ROBUSTNESS CHARACTERISTICS OF LAYERS",
            "year": 2018
        }
    ],
    "sections": [
        {
            "text": "Recent works have shown that deep neural networks are vulnerable to adversarial examples that find samples close to the original image but can make the model misclassify. Even with access only to the model\u2019s output, an attacker can employ black-box attacks to generate such adversarial examples. In this work, we propose a simple and lightweight defense against black-box attacks by adding random noise to hidden features at intermediate layers of the model at inference time. Our theoretical analysis confirms that this method effectively enhances the model\u2019s resilience against both score-based and decision-based black-box attacks. Importantly, our defense does not necessitate adversarial training and has minimal impact on accuracy, rendering it applicable to any pre-trained model. Our analysis also reveals the significance of selectively adding noise to different parts of the model based on the gradient of the adversarial objective function, which can be varied during the attack. We demonstrate the robustness of our defense against multiple black-box attacks through extensive empirical experiments involving diverse models with various architectures."
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "Modern deep neural networks have demonstrated remarkable performance in various complex tasks, including image classification and face recognition, among others. However, prior works have pointed out that deep learning models are sensitive to small changes in the input and can be fooled by carefully chosen and imperceptible perturbations Szegedy et al. (2014); Goodfellow et al. (2015); Papernot et al. (2016b); Madry et al. (2018). These adversarial attacks can be generally classified into white-box and black-box attacks. In a white-box setting, strong attacks such as Projected Gradient Descent (PGD) Madry et al. (2018) can generate effective adversarial examples by levering the information inside the model. However, in practical scenarios such as machine learning as a service (MLaas), the well-trained models and the training datasets are often inaccessible to the users, especially in the era of large models. Hence, query-based black-box attacks become the primary threats in most real-world applications, where the adversary is assumed to have no knowledge of the model architecture and parameters.\nThis paper proposes a lightweight, plug-and-play defensive method that can significantly decrease the success rate of query-based black-box attacks, including both score-based and decision-based attacks Ilyas et al. (2018; 2019); Andriushchenko et al. (2020); Guo et al. (2019); Al-Dujaili & O\u2019Reilly (2020); Liu et al. (2019); Chen & Gu (2020); Chen et al. (2020b); Rahmati et al. (2020). Adversarial examples generated through query-based attacks involve iterative procedures that rely on either local search techniques involving small incremental modifications to the input or optimization methods leveraging estimated gradients of the adversary\u2019s loss concerning the input. However, the process of requesting numerous queries is time-consuming and sometimes may raise suspicions with the presence of multiple similar queries. Hence, the objective of defense is to perplex the adversary when attempting to generate adversarial examples. Our proposed method accomplishes this by introducing noise into the feature space. While Qin et al. (2021) study the robustness of randomized input, this paper provides both theoretical analysis and empirical evidence to demonstrate improved robustness of randomized features. Our analysis also highlights the importance of strategically intro-\nducing noise to specific components of the model based on the gradient of the adversarial objective function, which can be dynamically adjusted throughout the attack process.\nOur contributions can be summarized as follows:\n\u2022 We investigate the impact of randomized perturbations in the feature space and its connection to the robustness of the model to black-box attacks.\n\u2022 We design a simple yet effective and lightweight defense strategy that hampers the attacker\u2019s ability to approximate the direction toward adversarial samples. As a result, the success rate of the attacks is significantly reduced.\n\u2022 We extensively evaluate our approach through experiments on both score-based and decision-based attacks. The results validate our analysis and demonstrate that our method enhances the robustness of the randomized model against query-based attacks."
        },
        {
            "heading": "2 RELATED WORKS",
            "text": ""
        },
        {
            "heading": "2.1 ADVERSARIAL ATTACKS",
            "text": "Extensive research has been conducted on white-box attacks, focusing on the generation of adversarial examples when the attacker possesses complete access to the target model. Over the years, various notable methods have emerged as representative approaches in this field, including fast gradient sign method (FGSM) Goodfellow et al. (2015), Jacobian-based saliency Map Attack (JSMA) Papernot et al. (2016a), and PGD Madry et al. (2018).\nIn contrast to white-box attacks, the black-box scenario assumes that the attacker lacks access to the target model, making it a more challenging situation. However, this is also a more realistic setting in real-world applications where the adversary would not have access to the model parameters. One approach in black-box attacks involves utilizing white-box techniques on substitute models to create adversarial examples, which can subsequently be applied to black-box target models Papernot et al. (2017). However, the effectiveness of transfer-based attacks can vary significantly due to several practical factors, such as the initial training conditions, model hyperparameters, and constraints involved in generating adversarial samples Chen et al. (2017). This paper focuses on the defense against query-based attacks instead."
        },
        {
            "heading": "2.2 QUERY-BASED BLACK-BOX ATTACKS",
            "text": "Query-based attacks can be largely divided into score-based attacks and decision-based attacks, based on the accessible model output information. Score-based attacks leverage the output probability or logit of the targeted model, allowing the attacker to manipulate the scores associated with different classes. On the other hand, decision-based queries provide the attacker with hard labels, restricting the access to only the final predictions without any probability or confidence values.\nWe list the query-based attacks used in this paper below:\nNatural Evolutionary Strategies (NES) Ilyas et al. (2018) is one of the first query-based attacks that use natural evolutional strategies to estimate the gradient of the model with respect to an image x. By exploring the queries surrounding x, NES effectively gauges the model\u2019s gradient, enabling it to probe and gain insights into the model\u2019s behavior.\nSignHunt Al-Dujaili & O\u2019Reilly (2020) is another score-based attack, which flips the sign of the perturbation based on the sign of the estimated gradient to improve the query efficiency.\nSquare attack Andriushchenko et al. (2020) is a type of score-based attack that differs from gradient approximation techniques. Instead, it employs random search to update square-shaped regions located at random positions within the images. This approach avoids relying on gradient information and introduces a localized square modification to the image.\nRayS Chen & Gu (2020) is a decision-based attack that solves a discrete problem to find the direction with the smallest distance to the decision boundary while using a fast check step to avoid unnecessary searches.\nSignFlip Chen et al. (2020b) is an \u2113\u221e decision based attack that alternately projects the perturbation to a smaller \u2113\u221e ball and flips the sign of some randomly selected entries in the perturbation."
        },
        {
            "heading": "2.3 DEFENSIVE METHODS AGAINST QUERY-BASED ATTACKS",
            "text": "In the recent literature, several defensive solutions have been proposed to counter adversarial examples. One such solution involves the detection of malicious queries by comparing them with previously observed normal queries Chen et al. (2020a); Li et al. (2022); Pang et al. (2020). This approach aims to identify anomalous patterns in queries and flag them as potential adversarial examples. Additionally, adversarial training has also been utilized to enhance the model\u2019s robustness Cohen et al. (2019); Wang et al. (2020); Sinha et al. (2017); Zhang et al. (2020). Adversarial training involves training the model on both regular and adversarial examples to improve its ability to withstand adversarial attacks. However, it is computationally expensive, especially when dealing with large and complex datasets. In some cases, adversarial training may also inadvertently harm the model\u2019s overall performance.\nIn contrast, this paper focuses on approaches that involve incorporating noise or randomness into the model, thereby providing the adversary with distorted information. The underlying intuition behind these defense mechanisms is to deceive the attacker by introducing perturbations in the model\u2019s prediction process. By altering certain signals, the defenses aim to mislead the attacker and divert them from their intended direction. To achieve this, various techniques are employed to modify the input data or manipulate the model\u2019s internal workings. For instance, some defenses may introduce random noise or distortion to the input samples Liu et al. (2017); He et al. (2019); Salman et al. (2019), making them less susceptible to adversarial perturbations. This noise acts as a smokescreen, confusing the attacker and making it harder for them to generate effective adversarial examples.\nWe list the defensive methods evaluated in this paper below:\nRandom Noise Defense (RND) Qin et al. (2021) is a lightweight defense that adds Gaussian noise to the input for each query. This work also theoretically shows RND\u2019s effectiveness against querybased attacks. Byun et al. (2021) proposes Small Noise Defense (SND), whose randomization method is identical to RND, thus reporting performance valuation for RND covers both of these works.\nAdversarial Attack on Attackers (AAA) Chen et al. (2022) directly optimizes the model\u2019s logits to confound the attacker towards incorrect attack directions."
        },
        {
            "heading": "3 METHOD",
            "text": ""
        },
        {
            "heading": "3.1 PROBLEM FORMULATIONS",
            "text": "Adversarial attack. Let f : Rd \u2192 RK be the victim model, where d is the input dimension, K is the number of classes, fk(x) is the predicted score of class k for input x. Given an input example (x, y), the goal of adversarial attack is to find a sample x\u2032 such that\nargmax k\nf(x\u2032) \u0338= y, s.t d(x, x\u2032) \u2264 \u03f5, (1)\nwhere d(x, x\u2032) is distance between samples x and x\u2032. In practice, the distance can be the \u21132\u2212norm, \u2225x\u2212 x\u2032\u22252, or the \u2113\u221e\u2212norm, \u2225x\u2212 x\u2032\u2225\u221e. This adversarial task can be framed as a constrained optimization problem. More particularly, the attacker tries to solve the following objective\nmin x\u2032 L(f(x\u2032), y), s.t d(x, x\u2032) \u2264 \u03f5, (2)\nwhere L(., .) is a loss function designed by the attacker. In practice, a common loss function L is the max-margin loss, as follows:\nL(f(x), y) = fy(x)\u2212max i\u0338=y fi(x). (3)\nScore-based attack. For the query-based attack, an attacker can only access the input and output of the model; thus, the attacker cannot compute the gradient of the objective function with respect to\nthe input x. However, the attacker can approximate the gradient using the finite difference method:\n\u2207\u0302L = \u2211 u L(f(x+ \u03b7u), y)\u2212 L(f(x), y) \u03b7 u, where u \u223c N (0, \u00b5I). (4)\nAnother approach to minimize the objective function is via random search. Specifically, the attacker proposes an update u and computes the value of L of this update to determine if u can help improve the value of the objective function. Formally, the proposed u is selected if L(f(x+ u), y)\u2212 L(f(x), y) < 0, otherwise it is rejected. Decision-based attack. In contrast to score-based attacks, hard-label attacks find the direction that has the shortest distance to the decision boundary. The objective function of an untargeted hard-label attack can be formulated as follows:\nmin d g(d) where g(d) = min { r : argmax\nk f(x+ rd/\u2225d\u22252) \u0338= y\n} . (5)\nThis objective function can be minimized using binary search, in which the attacker queries the model to find the distance r for a particular direction d. To improve the querying efficiency, binary search can be combined with fine-grained search, in which the radius is iteratively increased until the attacker finds an interval that contains g(d). Hence, the gradient of g(d) can also be approximated by the finite difference method\n\u2207\u0302g(d) = \u2211 u g(d+ \u03b7u)\u2212 g(d) \u03b7 u. (6)\nSimilar to the case of score-based attacks, the attacker can also search for the optimal direction. Given the current best distance ropt, a proposed direction d is eliminated if it cannot flip the prediction using the current best distance ropt; otherwise the binary search is used to compute g(d), which is the new best distance.\nRandomized model. In this work, we consider a randomized model frand : Rd \u2192 P(RK) that maps a sample x \u2208 Rd to a probability distribution on RK . Given an input x and an attack query, the corresponding output is a vector drawn from frand(x). We assume that the randomized model frand is \u2019nice\u2019; that is, the mean and variance of frand(x) exist for every x.\nFinally, we define adversarial samples for a randomized model. Since the model has stochasticity, the prediction returned by the model of a sample x can be inconsistent at different queries; i.e., the same sample can be correctly predicted at one application of frand and be incorrectly predicted later in another application of frand. For this reason, adversarial attacks are successful if the obtained adversarial example can fool the randomized model in the majority of its applications on the example.\nDefinition 1 (Attack Success on Randomized Model). Given a datapoint x with label y and a positive real number \u03f5, a point x\u2032 is called adversarial samples in a closed ball of radius \u03f5 around x with respect to the model frand if \u2225x\u2032 \u2212 x\u2225p < \u03f5 and\nargmaxE[frand(x\u2032)] \u0338= y."
        },
        {
            "heading": "3.2 RANDOMIZED FEATURE DEFENSE",
            "text": "Our method is based on the assumption that the attacker relies on the model\u2019s output to find the update vector toward an adversarial example. Consequently, if the attacker receives unreliable feedback from the model, it will be more challenging for the attacker to infer good search directions toward the adversarial sample.\nIn contrast to the previous inference-time randomization approaches, we introduce stochasticity to the model by perturbing the hidden features of the model. Formally, let hl be the l\u2212th layer of the model, we sample an independent noise vector \u03b4 and forward hl(x) + \u03b4 to the next layer. For simplicity, \u03b4 is sampled from Gaussian distribution N (0,\u03a3), where \u03a3 is a diagonal matrix, or N (0, \u03bdI), \u03bd \u2208 R. The detailed algorithm is presented in Algorithm 1.\nAlgorithm 1 Randomized Feature Defense Input: a model f , input data x,\nnoise statistics \u03a3, a set of perturbed layers H = {hl0 , hl1 , . . . , hln}\nOutput: logit vector l z0 \u2190 x for layer hi in the model do\nif hi \u2208 H then \u03b4 \u223c N (0,\u03a3) zi \u2190 hi(zi\u22121) + \u03b4\nend if end for l\u2190 zn\nLet frand be the proposed randomized model corresponding to the original f . When the variance of injected noise is small, we can assume that small noise diffuses but does not shift the prediction. Particularly, we can make the following assumption. Assumption 1. Mean of the randomized model frand with input x is exactly the prediction of the original model for x\nE[frand(x)] = f(x).\nGiven Assumption 1, by Definition 1, adversarial samples of the original model are adversarial samples of the randomized model. Therefore, the direction that the attacker seeks is also that of the original model. Recall that the attacker finds this direction by either finite difference or random search.\nIn our method, when the model is injected with an independent noise, the value of objective L is affected. If L(frand(x + \u03b7u), y) \u2212 L(frand(x), y) oscillates among applications of frand, the attacker is likely misled and selects a wrong direction. For random-search attacks, when the sign of L(frand(x + \u03b7u), y) \u2212 L(frand(x), y) and the sign of L(f(x + \u03b7u), y) \u2212 L(f(x), y) are different, the attacker chooses the opposite action to the optimal one. In other words, the attacker can either accept a bad update or reject a good one in a random search."
        },
        {
            "heading": "3.3 ROBUSTNESS TO SCORE-BASED ATTACKS",
            "text": "In this section, we present the theoretical analysis of the proposed defense against score-based attacks. Theorem 1. Assuming the proposed random vector u is sampled from a Gaussian N (0, \u00b5I), the model is decomposed into f = g \u25e6 h, and the defense adds a random noise \u03b4 \u223c N (0, \u03bdI) to the output of h. At input x, the probability that the attacker chooses an opposite action positively correlates with\narctan ( \u2212 ( 2\u03bd\n\u00b5 \u2225\u2207h(x)(L \u25e6 g)\u222522 \u2225\u2207x(L \u25e6 f)\u222522\n)\u22120.5) .\nThis theorem states that the robustness of the randomized model is controlled by both (i) the ratio between the defense and attack noises and (ii) the ratio of the norm of the gradient with respect to the feature h(x) and the norm of the gradient with respect to the input x. Since arctan is monotonically increasing, the model becomes more robust if the ratio 2\u03bd\u00b5 \u2225\u2207h(x)(L\u25e6g)\u222522 \u2225\u2207x(L\u25e6f)\u222522\nis high. Intuitively, the perturbations added by the attacker and by the defense induce a corresponding noise in the output; if the attack noise is dominated by the defense noise, the attacker cannot perceive how its update affects the model. Note that the arctan function is bounded, which means at some point the robustness saturates when the ratio increases.\nWhile the first ratio is predetermined before an attack, the second ratio varies during the attack when the input x is sequentially perturbed since it depends on the gradient of the objective function. To understand this behavior of the randomized model during the attack, we perform the following experiment. First, we compute the ratio of the norms of gradients at h(x) and x. To simulate an attacker, we perform a single gradient descent step with respect to L. The distributions of the ratios on the raw and perturbed images at different layers are shown in Figure 1. We can observe that these ratios become higher when the data are perturbed toward the adversarial samples. In other words, the randomized model is more robust during the attack. We also illustrate the accuracy under Square attack when adding noise to each layer, verifying our analysis."
        },
        {
            "heading": "3.4 ROBUSTNESS TO DECISION-BASED ATTACKS",
            "text": "In decision-based attacks, the attacker finds the optimal direction dopt and the corresponding distance ropt to the decision boundary such that ropt is minimal. We use the objective function L(f(x), y) to understand how our method affects the decision-based attacks. Indeed, L measures how close the prediction is to the true label: L \u2264 0 if the prediction is incorrect andL > 0 otherwise.\nTo estimate g(d), the attacker can use binary search. Similar to score-based attacks, when noise is injected into the model, the function g(d) becomes stochastic, which eventually affects the attack. Unfortunately, the distribution of g(d) (under binary search with randomness) does not have an analytical form. Nevertheless, we can still use a similar analysis to the last section to understand the robustness of our method. To avoid performing a binary search on uninformative directions, the attacker relies on best-radius searching. Given the current best distance ropt, for every new direction d, the attacker verifies if the distance along d to the boundary is shorter than ropt by querying x + roptd/\u2225d\u22252. When adding noise to features h(x) of f = g \u25e6 h and linearizing the function at the current input x, we have\nL(frand(x+ roptd/||d||2), y) \u2248 L(g(h(x) + roptJh(x)d/||d||2 + \u03b4), y) (7) \u2248 L(f(x), y) + ropt\u2207xL(f(x), y)d/||d||2 +\u2207h(x)L(g(h(x)), y)\u03b4\n(8) \u2248 (ropt \u2212 g(d))\u2207xL(f(x), y)d/||d||2 +\u2207h(x)L(g(h(x)), y)\u03b4, (9)\nwhere Jh(x) is the Jacobian matrix of h evaluated at x, since L(f(x), y) + g(d)\u2207xL(f(x), y)d/||d||2 \u2248 L(f(x + g(d)d/||d||2), y) = 0. If \u03b4 \u223c N (0, \u03bdI), the variance of \u2207h(x)L(g(h(x)), y)\u03b4 is \u03bd\u2225\u2207h(x)L(g(h(x)), y)\u222522. When this value is large, it can dominate the other terms and increase the chance of flipping the sign of the loss function L. In other words, when L has a high variance, the attacker is more likely to misjudge the direction."
        },
        {
            "heading": "3.5 THE EFFECT OF RANDOMIZED FEATURES ON ACCURACY",
            "text": "Let D be the data distribution, without any attack or defense, the accuracy of the model is\nAcc(f) := E (x,y)\u223cD [1(f(x) = y)] = E (x,y)\u223cD [1(L(f(x), y) > 0)]. (10)\nWhen injecting noise into the model, it becomes a robust, stochastic model frand : Rd \u2192 P(RK). The clean accuracy of the randomized model is\nAcc(frand) = E (x,y)\u223cD E y\u2032\u223cfrand(x) [1(y\u2032 = y)] = E (x,y)\u223cD E y\u2032\u223cfrand(x) [1(L(y\u2032, y) > 0)]. (11)\nAdding noise \u03b42 \u223c N (0, \u03bd2I) to the features at layer h of the model f = g \u25e6 h results in:\nAcc(frand) = E (x,y)\u223cD E \u03b4\u223cN (0,\u03bd2I) [1(L(g(h(x) + \u03b42), y) > 0)] (12)\n\u2248 E (x,y)\u223cD E \u03b42\u223cN (0,\u03bd2I) [1(L(f(x), y) +\u2207h(x)(L \u25e6 g)\u03b42 > 0)] (13)\n= E (x,y)\u223cD E \u03b4\u20322\u223cN (0,\u03bd2)\n[1(L(f(x), y)/\u2225\u2207h(x)(L \u25e6 g)\u22252 + \u03b4\u20322 > 0)]. (14)\nIt means that the accuracy of a randomized model depends on the objective function and its gradient, which vary for different data points. These ratios of L and its gradient computed at the input and hidden layers are different. If L is small at samples that have a large gradient norm when noise is injected at a layer, these samples will be likely misclassified while the correctly classified samples have a low magnitude of robustness (i.e., \u03bd\u2225\u2207h(x)(L \u25e6 g)\u222522 is small, as discussed in Theorem 1 and Section 3.4). In contrast, if the gradient norm with respect to the randomized layer is large for samples that have large L, the robustness of the model for the correctly classified samples will be high; thus, adding noise to this layer makes the model more robust against black-box attacks.\nWe conduct the following experiment to understand how the defense affects the whole dataset. We first compute the ratios ofL and its gradient for all samples and keep the top 99% values. Essentially, the standard deviation of defensive noises that makes the accuracy drop by 1% is proportional to the value at which 1% of the ratios in the dataset are smaller. The product of this value and the norm of gradient represents the robustness of datasets, which are shown in Figure 2. We also illustrate the accuracy under Square attack when adding noise to each layer.\nWe can observe that the ratio distributions when randomizing the input and the hidden features are similar at the first few layers of the model; however, these ratios at the deeper layers of the model are higher. This means that randomizing the model at these layers makes it more robust than adding noise to the input layer when the defenders desire similar clean accuracy in the randomized models."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": "In this section, we evaluate the empirical performance of the proposed randomized feature defense."
        },
        {
            "heading": "4.1 EXPERIMENTAL SETUP",
            "text": "Datasets. We perform our experiments on two widely used benchmark datasets in adversarial robustness: CIFAR10 Krizhevsky & Hinton (2009) and ImageNet Russakovsky et al. (2015). We randomly select 1000 images that contain every class from the studied dataset in each experiment.\nDefenses. In addition to the proposed defense, we also include the related input defenses Qin et al. (2021); Byun et al. (2021) in our evaluation. Note that, the empirical robustness comparison of all adversarial defenses is beyond the scope of the paper since our objective is to theoretically and empirically study the effectiveness of the randomized feature defense. We also evaluate AAA defense Chen et al. (2022) against decision-based attacks and compare them with randomized defenses.\nIn all experiments, our defense randomizes only the penultimate layers of the base models, since our theoretical observations in Section 3.3 (Figure 1), and empirical results, provided in Table 14 (Supplementary), show that randomizing a deeper layer is consistently more effective.\nAttacks. For score-based attacks, we consider the gradient-estimation methods, NES Ilyas et al. (2018), and the random-search methods, Square Andriushchenko et al. (2020), SignHunt Al-Dujaili & O\u2019Reilly (2020). For decision-based attacks, we consider RayS Chen & Gu (2020) and SignFlip Chen et al. (2020b).\nModels. We consider 6 victim models on ImageNet, including 2 convolution models that are VGG19 Simonyan & Zisserman (2015) and ResNet50 He et al. (2016), 2 transformer models that are ViT Dosovitskiy et al. (2021) and DeiT Touvron et al. (2021). For the experiments on CIFAR10, we finetuned VGG19, ResNet50, ViT, DeiT with an input size of 224\u00d7 224. Evaluation protocol. For a fair comparison, we report each defense\u2019s robustness performance results at the corresponding configuration of hyperparameters that achieves a specific drop (i.e., \u22481% or \u22482%) in clean-data accuracy. In practice, a defender always considers the trade-off between robustness and clean-data performance, with a priority on satisfactory clean-data performance; thus, achieving higher robustness but a significant drop in clean-data accuracy is usually not acceptable."
        },
        {
            "heading": "4.2 PERFORMANCE AGAINST SCORE-BASED ATTACKS",
            "text": "On ImageNet, we report the accuracy under the attack of 6 models and 3 score-based attacks in Table 1. As we can observe, while the attacks achieve close to 0% failure rate on the base models (i.e., without any defense), both randomized feature and input defenses significantly improve the models\u2019 robustness against score-based attacks. Furthermore, for Square attack and SignHunt, which are strong adversarial attack baselines, randomized feature defense consistently achieves better perfor-\nmance on all 6 models, which supports our theoretical analysis in Section 3. For instance, while the base VGG19 models are severely vulnerable, our randomized feature defense achieves 22.2% in robust accuracy after 10000 query, also significantly better than the randomized input defense (17.8% robust accuracy). On the transformer-based DeiT, our randomized feature defense has 69.1% robust accuracy under Square attack, while the robust accuracy of the randomized input defense is 2% lower. For the NES attack, the randomized-feature VGG19 shows the best robustness. In summary, randomized feature defense consistently achieves high robustness on most models except ResNet50 where the robustness is similar to randomized input defense.\nTable 1: Defense Performance in ImageNet. The clean-data accuracy of the robust models is allowed to drop either \u2248 1% or \u2248 2%.\nModel Method Acc Square NES SignHunt\n1000 10000 1000 10000 1000 10000\nResNet50 Base 80.37 3.5 0.2 36.2 4.3 6.6 0.4\nInput 79.18 (\u2248 1%) 40.3 39.5 63.8 23.9 47.6 45.478.46 (\u2248 2%) 41.1 39.8 69.4 41.5 49.3 47.2\nFeature 79.70 (\u2248 1%) 37.0 36.0 56.7 16.8 46.3 43.478.43 (\u2248 2%) 42.0 41.5 65.6 40.6 51.3 49.3\nVGG19 Base 74.21 0.1 0.0 19.6 0.0 0.4 0.0\nInput 73.24 (\u2248 1%) 7.7 6.9 32.1 1.5 18.3 17.071.43 (\u2248 2%) 18.7 17.8 47.4 11.5 28.3 27.1\nFeature 72.66 (\u2248 1%) 22.4 21.6 50.1 18.5 34.6 32.971.21 (\u2248 2%) 23.3 22.2 55.1 28.4 36.5 32.8\nDeiT Base 82.00 6.4 0.0 46.7 0.8 22.3 0.0\nInput 80.10 (\u2248 1%) 67.7 67.2 75.8 65.9 64.4 63.679.60 (\u2248 2%) 66.6 66.0 75.7 67.1 64.9 64.3\nFeature 80.80 (\u2248 1%) 69.7 69.1 75.0 59.1 66.4 64.179.76 (\u2248 2%) 69.3 69.0 75.1 65.3 66 64.3\nViT Base 79.15 5.7 0.0 45.7 7.3 5.1 0.0\nInput 78.28 (\u2248 1%) 58.8 58.1 70.8 51.4 53.1 52.277.09 (\u2248 2%) 61.3 60.9 70.6 59.2 53.7 52.7\nFeature 78.20 (\u2248 1%) 60.6 60.2 69.1 47.5 54.0 52.977.18 (\u2248 2%) 63.7 62.9 72.2 58.1 57.0 55.3\nTable 2: Defense Performance in CIFAR10. The clean-data accuracy of the robust models is allowed to drop either \u2248 2% or \u2248 4%.\nModel Method Acc Square NES SignHunt\n1000 10000 1000 10000 1000 10000\nResNet50 Base 97.66 0.8 0.1 71.7 21.7 3.7 0.2\nInput 95.98 (\u2248 2%) 50.5 48.8 93.1 85.4 26.8 2693.42 (\u2248 4%) 56.4 54.8 90.0 85.0 31.1 29.8\nFeature 95.95 (\u2248 2%) 54.9 52.8 93.2 86.2 32.5 30.693.48 (\u2248 4%) 56.7 53.4 89.9 83.9 37.1 35.7\nVGG19 Base 96.28 0.6 0.1 68.8 16.6 3.2 0.3\nInput 94.92 (\u2248 2%) 30.6 27.1 89.5 58.0 22.7 21.893.52 (\u2248 4%) 42.2 39.8 90.3 68.4 27.5 26.8\nFeature 94.93 (\u2248 2%) 61.0 58.4 92.2 77.9 43.2 42.493.58 (\u2248 4%) 64.2 62.8 91.2 80.1 49.2 46.9\nDeiT Base 98.40 3.2 0.0 81.9 34.2 7.9 0.2\nInput 96.59 (\u2248 2%) 66.9 67.6 95.2 90.0 40.2 39.294.81 (\u2248 4%) 70.6 68.8 92.6 87.7 40.3 38.5\nFeature 96.29 (\u2248 2%) 69.1 67.9 94.1 88.3 45.7 43.494.91 (\u2248 4%) 68.9 66.1 93.5 87.6 43.6 40.4\nViT Base 97.86 5.1 0.0 84.8 43.6 6.1 0.0\nInput 95.80 (\u2248 2%) 63.0 61.2 93.5 87.0 34.8 33.393.40 (\u2248 4%) 62.6 61.1 89.7 85.5 33.4 32.2\nFeature 95.96 (\u2248 2%) 63.9 62.7 93.7 85.6 42.5 40.793.39 (\u2248 4%) 66.2 65.6 92.9 85.3 44.8 43.8\nWe also observe similar robustness results on CIFAR10 experiments with ResNet50, VGG19, DeiT, and ViT for 3 attacks. As we can observe in Table 2, randomized feature and input defenses are effective against score-based attacks. Similar to ImageNet, randomized feature defense achieves significantly better robustness than randomized input defense in most experiments. For Square attacks on ResNet50 and DeiT, while the best robustness is achieved by randomized input defense, randomized feature defense is more robust when the defender sacrifices 2% clean-data accuracy.\nAttack \u00b5 VGG ViT\nSmall \u03bd Large \u03bd Small \u03bd Large \u03bd\nInput Feature Input Feature Input Feature Input Feature\nSquare 0.05 30.6 61.0 42.2 64.2 63.0 63.9 62.6 66.2 0.1 47.4 65.8 54.6 65.5 69.3 70.2 68.8 69.6 0.2 32.1 59.7 43.9 64.0 56.1 58.0 56.8 58.6 0.3 27.0 54.9 38.1 59.7 47.1 51.9 47.7 50.4\nNES 0.001 93.4 93.9 90.1 91.4 93.7 94.8 90.3 93.5 0.01 89.5 92.2 90.3 91.2 93.5 93.7 89.7 92.9 0.1 88.0 90.0 86.7 89.6 87.9 91.4 86.7 90.6 0.2 93.6 93.0 92.6 91.4 91.0 93.8 87.6 92.0\nSignHunt 0.01 91.6 91.0 91.3 88.0 89.1 90.9 85.4 91.3 0.05 22.7 43.2 27.5 49.2 34.8 42.5 33.4 44.8 0.075 5.6 19.7 8.1 25.6 13.6 22.5 13.7 24.3 0.1 1.2 7.9 2.4 12.1 5.5 11.3 5.2 12.7\nthan randomized input defense; this improved robustness again can be explained by the analysis in Section 3.3 and 3.5. A larger attack perturbation may also cause the approximation in the attack to be less accurate, which leads to a drop in the attack\u2019s effectiveness; for example, the robust-\nness increases from 89.6% to 91.4% when the NES\u2019s perturbation magnitude increases in VGG19 experiments (similar observations in ViT).\nCombined with Adversarial Training (AT). We evaluate the combination of our defense and AT on CIFAR10/ResNet20 model against under score-based attacks with 1000 queries and observe significantly improved robustness, as shown in Table 4."
        },
        {
            "heading": "4.3 PERFORMANCE AGAINST DECISION-BASED ATTACKS",
            "text": "Table 5 reports the performance of VGG19 and ResNet50 against 2 decision-based attacks on CIFAR10. Besides randomized feature and input defenses, we also include AAA defense, which optimizes the perturbation that does not change the prediction. While AAA is optimized for score-based attacks directly and thus is successful in fooling these attacks (as seen in Table 3 in Supplementary), the results show that AAA is not effective in defending against decision-based attacks, while randomized feature and input defenses improve the robustness. An interesting observation is that RayS attack is more effective than score-based attacks although it only uses hard labels, even when there are defenses."
        },
        {
            "heading": "4.4 RELATIONSHIP BETWEEN THE GRADIENT NORM AND THE ROBUSTNESS TO SCORE-BASED ATTACKS",
            "text": "In Table 6, we provide the corresponding accuracy under attack on CIFAR10 with 1000 queries (for when a single layer is randomized with a fixed value of \u03bd) and the mean of the gradient norm at that layer. As we can observe, as the gradient norm increases (also as we originally observe in Figure 1), the robustness also increases, thus verifying our theoretical results."
        },
        {
            "heading": "4.5 PERFORMANCE AGAINST ADAPTIVE ATTACKS",
            "text": "We conduct experiments with adaptive attacks that apply Expectation Over Transformation (EOT) Athalye et al. (2018) in which the attacker queries a sample M times and averages the outputs to cancel the randomness. Tables 7 show the robust accuracy of VGG19 and ResNet50 on CIFAR10 against EOT attack with M = 5 and M = 10. Note that with EOT, the number of updates in the attack is M times less than that of a normal attack with the same query budget. For this reason, we report the results for adaptive attacks with both 1000 queries and M \u00d7 1000 queries. We can observe that EOT can mitigate the effect of randomized defenses even with the same number of queries; however, feature defense still yields better performance."
        },
        {
            "heading": "5 CONCLUSION AND FUTURE WORK",
            "text": "In this work, we study the effectiveness of random feature defense against query-based attacks, including score-based and decision-based attacks. We provide an analysis that connects the robustness to the variance of noise and the local behavior of the model. Our empirical results show that random defense helps improve the performance of the model under query-based attacks with a trade-off in clean accuracy. Future works will be directed toward the analysis covering black-box attacks that transfer adversarial samples from the surrogate model to the target model."
        },
        {
            "heading": "APPENDIX",
            "text": "This Appendix provides additional details, analysis, and experimental results to support the main paper. We begin by discussing the limitations in Section A and societal impacts in Section B. Then, we provide the proof for Theorem 1 in the main paper in Section C. Next, the detailed experimental setup is provided in Section D, which is followed by additional robustness experiments to demonstrate the effectiveness of the proposed defense. Finally, we provide additional visualization of the robust behavior of the models with randomized features in Section F."
        },
        {
            "heading": "A LIMITATIONS",
            "text": "As discussed, we focus on studying the effectiveness of the randomized feature defense for DNNs against black-box attacks, including score-based and decision-based attacks. We do not include black-box attacks that utilize the transferability from surrogate to target models since our threat model does not make any assumptions about the network architecture and the training dataset.\nOur proposed defense adds another layer of protection to DNNs against adversarial attacks, and it would be interesting to study the adversarial robustness of the combination of our randomized feature defense and existing defense strategies, including those that have been developed for transferbased attacks Pang et al. (2019); Trame\u0300r et al. (2018); Yang et al. (2020). We leave these to future works.\nSimilar to the related works on randomized defenses, our empirical evaluation has limitation for it is designed to assess the relative effectiveness of randomized models. From the adversary perspective, the attack should only need to make the system fail once, during the querying process. Correspondingly, it means that the attack can query the model repeatedly, and by chance, the defense fails at some point; however, this failure, equivalently meaning the attack is successful, is not due to the perturbed input being an adversarial example but rather comes from the added randomness of the defense. Here, there are 2 scenarios: (1) if the input (or the perturbed query) is far from the decision boundary, randomization is much less likely to shift the input to the other side of the boundary, making this chance very low. On the other hand, (2) for input close to the decision boundary, this \u201crepeated\u201d attack will be more effective, unfortunately; one potential solution is to preemptively stop this attack if the system recognizes the same input is repeatedly forwarded to the model. We leave this to future work and urge practitioners to research this inherent problem of randomized models.\nIn practice, an attack has a cost, and if the attack cost is higher than the potential gain, the attacker will more likely stop; thus a defense should increase this cost as much as possible. A randomized approach like ours increases the attack cost by confusing their optimization trajectory. Note that as explained earlier, if the perturbed query is in scenario (1) (or the randomized radius is small enough), our randomization would likely not make the attack successful due to chance; on the other hand, the defense would make it more costly for the attack to push this query to scenario (2)."
        },
        {
            "heading": "B SOCIETAL IMPACTS",
            "text": "Deep neural networks (DNN) rapidly transform our daily lives in various domains and applications. Unfortunately, most well-trained DNNs are vulnerable to adversarial attacks, which decreases confidence in their deployment. Among the existing adversarial attacks, query-based attacks pose a severe threat to users since these attacks are effective and only require access to the model\u2019s feedback; the attackers do not know the trained parameters or model architectures.\nOur work tackles this defense challenge against query-based attacks by proposing a lightweight adversarial defense for existing DNNs. We conduct a detailed theoretical analysis of our defense and show its superior performance compared to other randomized defenses in extensive empirical experiments across a wide range of DNN architectures, query-based attacks, and benchmark datasets. Most importantly, our method can be directly integrated into any existing off-the-shelf DNN. In summary, the proposed randomized feature defense can boost the adversarial robustness of existing DNNs against most query-based attacks, further improving the users\u2019s confidence when using them in practice."
        },
        {
            "heading": "C PROOF OF SECTION 3.2",
            "text": "Theorem 1. Assuming the proposed random vector u is sampled from a Gaussian N (0, \u00b5I), the model is decomposed into f = g \u25e6 h, and the defense adds a random noise \u03b4 \u223c N (0, \u03bdI) to the output of h. At input x, the probability that the attacker chooses an opposite action positively correlates with\narctan ( \u2212 ( 2\u03bd\n\u00b5 \u2225\u2207h(x)(L \u25e6 g)\u222522 \u2225\u2207x(L \u25e6 f)\u222522\n)\u22120.5) .\nProof. For score-based attacks, the attacker finds the direction by computingL(f(x+u))\u2212L(f(x)). When applying randomized defense, the direction instead relies on L(frand(x+u))\u2212L(frand(x)). As discussed in Section 3, the probability that the attacker chooses an opposite action is\nP [ L(frand(x+ u))\u2212 L(frand(x)) L(f(x+ u))\u2212 L(f(x)) < 0 ] . (15)\nIf the defense adds a random noise \u03b4 \u223c N (0, \u03bdI) to the output of layer h of the model f = g \u25e6 h, we have L(frand(x)) = L(g(h(x) + \u03b4). Since \u03b4 and u are small, we can linearly approximate the objection function\nL(f(x+ u)) \u2248 L(f(x)) +\u2207x(L \u25e6 f)\u22bau, (16) L(g(h(x) + \u03b4) \u2248 L(f(x)) +\u2207h(x)(L \u25e6 g)\u22ba\u03b4, (17)\nL(g(h(x+ u) + \u03b4) \u2248 L(g(h(x) + Jh(x)u+ \u03b4) (18) \u2248 L(f(x)) +\u2207h(x)(L \u25e6 g)\u22baJh(x)u+\u2207h(x)(L \u25e6 g)\u22ba\u03b4 (19) = L(f(x)) +\u2207x(L \u25e6 f)\u22bau+\u2207h(x)(L \u25e6 g)\u22ba\u03b4, (20)\nwhere\u2207h(x)(L \u25e6 g) is the gradient of L \u25e6 g evaluated at h(x),\u2207x(L \u25e6 f) is the gradient of L \u25e6 f at x, Jh(x) is the Jacobian matrix of h at x.\nAt a different application of frand, the randomized model samples a new noise vector. Let \u03b41, \u03b42 be the sampled noises when querying L(frand(x+u)) and L(frand(x)), the ratio can be approximated by\nL(frand(x+ u))\u2212 L(frand(x)) L(f(x+ u))\u2212 L(f(x)) \u2248 \u2207x(L \u25e6 f)\u22bau+\u2207h(x)(L \u25e6 g)\u22ba\u03b41 \u2212\u2207h(x)(L \u25e6 g)\u22ba\u03b42 \u2207x(L \u25e6 f)\u22bau (21)\n= 1 + \u2207h(x)(L \u25e6 g)\u22ba(\u03b41 \u2212 \u03b42)\n\u2207x(L \u25e6 f)\u22bau . (22)\nSince \u03b41 and \u03b42 are independent, we have \u03b41 \u2212 \u03b42 \u223c N (0, 2\u03bdI), thus\n\u2207h(x)(L \u25e6 g)\u22ba(\u03b41 \u2212 \u03b42) \u223c N (0, 2\u03bd\u2225\u2207h(x)(L \u25e6 g)\u222522), (23) \u2207x(L \u25e6 f)\u22bau \u223c N (0, \u00b5\u2225\u2207x(L \u25e6 f)\u222522). (24)\nThe noises \u03b41, \u03b42 added by the defense and the noise u added by the attacker are independent, therefore the ratio of two independent normal variables \u2207h(x)(L\u25e6g)\n\u22ba(\u03b41\u2212\u03b42) \u2207x(L\u25e6f)\u22bau follows Cauchy distribution with location 0 and scale \u221a\n2\u03bd \u00b5 \u2225\u2207h(x)(L\u25e6g)\u22252 \u2225\u2207x(L\u25e6f)\u22252 . In this case, the probability that the attacker is fooled\ncan be approximated by P [ L(frand(x+ u))\u2212 L(frand(x)) L(f(x+ u))\u2212 L(f(x)) < 0 ] \u2248 P [\u2207h(x)(L \u25e6 g)\u22ba(\u03b41 \u2212 \u03b42) \u2207x(L \u25e6 f)\u22bau < \u22121 ]\n(25)\n= 1\n\u03c0 arctan\n( \u2212 ( 2\u03bd\n\u00b5 \u2225\u2207h(x)(L \u25e6 g)\u222522 \u2225\u2207x(L \u25e6 f)\u222522\n)\u22120.5) + 1\n2 .\n(26)"
        },
        {
            "heading": "C.1 ANALYSIS OF THE APPROXIMATION ERROR",
            "text": "Due to the high nonlinearity of neural networks, it is difficult to have a complete analysis of the convexity of the boundary. However, if the model is k1\u2212Lipschitz, the change in the prediction when a noise u is added is bounded by k1 and u.\n||f(x+ u)\u2212 f(x)||22 \u2264 k1||u||22.\nIf k1 is not too large, since ||u||22 is very small, in expectation the prediction of the randomized models stays still. Furthermore, we empirically validate whether the majority of the perturbed inputs belong to the original class, as shown in the experiment below.\nFor each input, we sample 20 pairs of perturbation with opposite directions. It\u2019s similar to antithetic sampling employed in Ilyas et al. (2018) for a better estimation of the effect of the injected noise. Intuitively, if the majority of the perturbed inputs are still in the original class, the average of the predictions is the same as the output of the original model. We also consider the extreme case (or worst case) where an input is marked as misclassified if any of those 40 noise vectors misleads the model. The table below shows the original accuracy, the average accuracy, and the extreme accuracy on VGG19/CIFAR10. As can be observed in Table 8, the decrease in accuracy of the expected case is trivial (0.37%); and even in the extreme case, this decrease is still small (2.3%) even in the extreme case. This supports our assumption.\nThe perturbation added by the defense induces a negligible effect on the prediction of the model; since \u00b5 is constrained by the adversarial constraint (i.e., within the Lp ball), it is typically smaller than \u03bd, thus inducing even small impact than \u03bd on the prediction. Furthermore, if the gradient of the loss function L(x) with respect to the input is k2\u2212Lipschitz continuous, we can bound the error of the approximation by k2 and the norm of the noise u\nL(x+ u) \u2264 L(x) +\u2207xL \u00b7 u+ 1\n2 k2||u||22.\nIf k and the noise are small, the error is also small. This assumption is also used in Qin et al. (2021) for their analysis. We also provided the error of the first-order approximation in Section F.1. The histogram of the error implies that the added noise is small enough for a close approximation, thus our analysis is valid."
        },
        {
            "heading": "D EXPERIMENTAL SETUP",
            "text": ""
        },
        {
            "heading": "D.1 DATASET",
            "text": "In this work, we conduct experiments on two widely used datasets in adversarial attacks, CIFAR10 and ImageNet. We randomly selected 1000 images per dataset such that the test sets cover all classes, each of which has equal size.\n\u2022 CIFAR101 consists of 60, 000 images from 10 different classes where the training set has 50, 000 images and the test set has 10, 000 images.\n\u2022 ImageNet (ILSVRC) 20122 is a large-scale dataset that consists of 1000 classes. The training set includes 1, 281, 167 images, the validation set includes 50, 000 images, and the test set has 100, 000 images.\nFor all experiments, we resize the images to 224\u00d7 224 resolution. 1https://www.cs.toronto.edu/\u02dckriz/cifar.html 2https://www.image-net.org/download.php"
        },
        {
            "heading": "D.2 MODELS",
            "text": "As discussed in the main text, we consider 4 models that have various architectures, including ResNet50 He et al. (2016), VGG19 Simonyan & Zisserman (2015), ViT base Dosovitskiy et al. (2021), DeiT base Touvron et al. (2021). We use the pretrained weights from timm package3 for ImageNet, and finetune ResNet50, VGG19, ViT base, DeiT base for CIFAR10.\nD.3 IMPLEMENTATION OF THE BLACK-BOX ATTACKS\nWe perform experiments on 3 score-based black box attacks (Square attack Andriushchenko et al. (2020), NES Ilyas et al. (2018), and Signhunter Al-Dujaili & O\u2019Reilly (2020)) and 2 decisionbased attacks (RayS Chen & Gu (2020) and SignFlip Chen et al. (2020b)). For \u2113\u221e attacks, we find adversarial samples within the \u2113\u221e ball of radius 0.05, for \u21132 attacks we set the radius to 5. The detailed hyperparameters of each attack are as follows:\n\u2022 Square attack: The initial probability of pixel change is 0.05 for \u2113\u221e attack and 0.1 for \u21132 attack.\n\u2022 NES: We estimate the gradient by finite difference with 60 samples for \u2113\u221e attack and 30 for \u21132 attack. The step size of finite difference is 0.01 and 0.005, and the learning rate is set to 0.005 and 1 for \u2113\u221e and \u21132 attack, respectively."
        },
        {
            "heading": "D.4 EVALUATION",
            "text": "According to Section 3, a sample is considered as adversarial if it can fool the model in the majority of its application. Since the randomized model has stochasticity, for any datapoint there is a chance that the prediction is flipped at some application. Therefore, an attacker can stop the attack before finding the true adversarial sample. To alleviate this issue, when deciding whether to stop the attack, the query is repeated multiple times, and the attack is considered to be successful if the prediction is consistently flipped in most of the runs. The experiment applies 9 query runs for verification, and these extra runs are not included in the total number of queries."
        },
        {
            "heading": "E ADDITIONAL EXPERIMENTS",
            "text": "E.1 PERFORMANCE AGAINST \u21132 ATTACKS\nWe provide the results of randomized feature defense against \u21132 attacks on CIFAR10 in Table 9. As we can observe, \u21132 attacks are quite successful in fooling the model; however, randomized feature defense improves the robustness of the models to these attacks."
        },
        {
            "heading": "E.2 PERFORMANCE AGAINST DECISION-BASED ATTACKS",
            "text": "Table 10 shows the performance of the model on ImageNet under decision-based attacks. Similar to CIFAR10, randomized feature defense is effective against decision-based attacks while AAA Chen\n3https://github.com/huggingface/pytorch-image-models\net al. (2022) defense is not helpful in this case, since decision-based attacks only rely on the label that the model returns and AAA defense keeps the output label be the same. We also provide the results for RayS on ViT/CIFAR10 in Table 11.\nAAA\u2019s Performance against score-based attacks. We also provide the evaluation of AAA under score-based attacks in Table 12. Since AAA is optimized for score-based attacks directly, it is successful in fooling the attack. However, under a general setting where the defender does not know the type of attack is currently performed (a more realistic scenario), AAA failed miserably as shown above, while our defense performs well regardless of the attack."
        },
        {
            "heading": "E.3 PERFORMANCE AGAINST WHITE-BOX ATTACKS",
            "text": "As mentioned in the main paper, similar to previous works Byun et al. (2021); Qin et al. (2021); Chen et al. (2022), our threat model focuses on defending against black-box, query-based attacks, as it is a more realistic scenario in practice. Nevertheless, in this section, we provide an additional study about the performance of our defense against white-box attacks, those that require access to the model\u2019s architecture and its parameters. We evaluate the performance of our method against C&W Carlini & Wagner (2017) and PGD Madry et al. (2018) with \u2113\u221e constraint \u03f5 = 0.03. As observed in Table 13, the proposed defense can boost the robustness against these white-box attacks while having a negligible degradation in clean accuracy. We conjecture that adding stochasticity to the model can transform it into a smoothed classifier and therefore reduce the adversarial effect."
        },
        {
            "heading": "E.4 ROBUSTNESS CHARACTERISTICS OF LAYERS",
            "text": "As discussed in Section 3.3 in the main paper, the gradient norm varies during the sequence of queries of an attack on an input. Figure 1 suggests that, in deeper layers, the ratio of the gradient norm increases during the attack, which is related to the model\u2019s robustness as seen in Theorem 1; thus the model becomes more resilient to black-box attacks. Here, we additionally provide the performance evaluation on CIFAR10 with 1000 attack queries when each layer is perturbed alone (with \u03bd such that the clean accuracy drops within a similar threshold), as well as the mean rate of change in the gradient norm\u2019s ratios during the sequence of queries. Table 14 implies that deeper layers induce higher change and lead to better robustness, which confirms our analysis."
        },
        {
            "heading": "E.5 PERFORMANCE WHEN THE ATTACK IS LUCKY",
            "text": "Our empirical experiments focus on evaluating whether the attack can truly find adversarial examples. For non-randomized models, when an attack arrives at the decision that x is an \u201cadversarial\u201d example, x unquestionably is on the other side of the decision boundary. However, for a randomized model, x could be still on the correct side of the decision boundary, but the added randomization shifts it to the other side of the decision boundary; thus, in principle, x is still not an adversarial example, and if we use 1 single application for evaluation, an attack could be lucky or a randomized defense could be unlucky. Consequently, for a fair evaluation of the effectiveness of a defense, we forward x multiple times and decide that it is an adversarial example if the majority of the results say so, as seen in our paper.\nNevertheless, we understand that in the case where we ignore fair evaluation, the attack is allowed to be lucky and just has to fool the defense once. Therefore, we also provide the experiments when forwarding x only once in Table 15. As we can observe, our defense is still effective against the attacks."
        },
        {
            "heading": "F BEHAVIOR OF MODELS WITH RANDOMIZED FEATURES",
            "text": ""
        },
        {
            "heading": "F.1 APPROXIMATION ERROR OF OUR ANALYSIS",
            "text": "In our theoretical analysis, we apply first-order approximation to study the behavior of the model. To show that the error is negligible, we calculate the difference between the loss when the input is\nTable 15: The results of Square attack on ViT/ImageNet with one forward each iteration and budget query of N .\nMethod Acc N=500 N=1000 N=5000 N=10000\nBase 79.15 13.4 10.3 0.2 0.0 Input 78.28 48.0 46.6 44.8 44.0 Feature 78.20 48.2 47.0 45.7 44.8\n7 6 5 4 3 2 1 Approximation error on log scale\n0\n5\n10\n15\n20\nNu m\nbe r o\nf s am\npl es\nFigure 3: The error of first-order approximation of ViT on ImageNet at log10 scale.\nshifted by a vector u and the value computed by first-order approximation. Figure 3 illustrates the histogram of the error of ViT on ImageNet at log10 scale, showing that our analysis is based on a good approximation."
        },
        {
            "heading": "F.2 THE RATIO OF THE NORM OF THE GRADIENT",
            "text": "We report the ratio 2\u03bd\u00b5 \u2225\u2207h(x)(L\u25e6g)\u222522 \u2225\u2207x(L\u25e6f)\u222522\nbefore and after perturbed at hidden layers of VGG19 and ViT on ImageNet/CIFAR10 in Figure 6 and 7. The results show that, on both datasets, when the perturbed sample moves close to adversarial samples, the probability that randomized feature defense can fool the attacker increases while the probability of randomized input defense does not change. This explains the effectiveness of randomized feature defense against score-based attacks. Figure 7 shows this ratio on ViT. On ImageNet, the robustness of the defense still increases during the attack; however, on CIFAR10, such behaviors between the original and perturbed samples are not significantly different.\nWe also include the ratio on ResNet50 and DeiT on CIFAR10 in Figure 4 and Figure 5. As can be observed, the ratios on ResNet50 and DeiT do not increase much during the attack, leading to small improvements."
        },
        {
            "heading": "F.3 THE MAGNITUDE OF THE ROBUSTNESS AT INPUT AND HIDDEN LAYERS",
            "text": "Figure 8 and 9 show the robustness of randomized feature and randomized input defenses by multiplying the norm of the gradient with the relative magnitude of the defense noise. As we can observe,\nthe robustness when injecting noise to the hidden layers is generally higher than when injecting noise in the input. Such robustness behaviors are more visible in the deeper layers."
        }
    ],
    "year": 2023
}