{
    "abstractText": "Training AI models that generalize across tasks and domains has long been among the open problems driving AI research. The emergence of Foundation Models made it easier to obtain expert models for a given task, but the heterogeneity of data that may be encountered at test time often means that any single expert is insufficient. We consider the Fusion of Experts (FoE) problem of fusing outputs of expert models with complementary knowledge of the data distribution and formulate it as an instance of supervised learning. Our method is applicable to both discriminative and generative tasks and leads to significant performance improvements in image and text classification, text summarization, multiple-choice QA, and automatic evaluation of generated text. We also extend our method to the \u201cfrugal\u201d setting where it is desired to reduce the number of expert model evaluations at test time. Our implementation is available here.",
    "authors": [],
    "id": "SP:bfb1c628f63544a6d5ff596c8377d27e3fb62af1",
    "references": [
        {
            "authors": [
                "James Atwood",
                "Yoni Halpern",
                "Pallavi Baljekar",
                "Eric Breck",
                "D Sculley",
                "Pavel Ostyakov",
                "Sergey I Nikolenko",
                "Igor Ivanov",
                "Roman Solovyev",
                "Weimin Wang"
            ],
            "title": "The inclusive images competition",
            "venue": "In The NeurIPS\u201918 Competition: From Machine Learning to Intelligent Conversations,",
            "year": 2020
        },
        {
            "authors": [
                "Edward Beeching",
                "Cl\u00e9mentine Fourrier",
                "Nathan Habib",
                "Sheon Han",
                "Nathan Lambert",
                "Nazneen Rajani",
                "Omar Sanseviero",
                "Lewis Tunstall",
                "Thomas Wolf"
            ],
            "title": "Open llm leaderboard",
            "venue": "https: //huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard,",
            "year": 2023
        },
        {
            "authors": [
                "Shiori Sagawa",
                "Keshav Santhanam",
                "Andy Shih",
                "Krishnan Srinivasan",
                "Alex Tamkin",
                "Rohan Taori",
                "Armin W. Thomas",
                "Florian Tram\u00e8r",
                "Rose E. Wang",
                "William Wang",
                "Bohan Wu",
                "Jiajun Wu",
                "Yuhuai Wu",
                "Sang Michael Xie",
                "Michihiro Yasunaga",
                "Jiaxuan You",
                "Matei Zaharia",
                "Michael Zhang",
                "Tianyi Zhang",
                "Xikun Zhang",
                "Yuhui Zhang",
                "Lucia Zheng",
                "Kaitlyn Zhou",
                "Percy Liang"
            ],
            "title": "On the Opportunities and Risks of Foundation Models",
            "venue": "[cs],",
            "year": 2021
        },
        {
            "authors": [
                "Leo Breiman"
            ],
            "title": "Stacked regressions",
            "venue": "Machine learning,",
            "year": 1996
        },
        {
            "authors": [
                "Sutskever",
                "Dario Amodei"
            ],
            "title": "Language Models are Few-Shot Learners",
            "venue": "[cs],",
            "year": 2020
        },
        {
            "authors": [
                "Sebastian Buschj\u00e4ger",
                "Lukas Pfahler",
                "Katharina Morik"
            ],
            "title": "Generalized negative correlation learning for deep ensembling",
            "venue": "arXiv preprint arXiv:2011.02952,",
            "year": 2020
        },
        {
            "authors": [
                "Annie S Chen",
                "Yoonho Lee",
                "Amrith Setlur",
                "Sergey Levine",
                "Chelsea Finn"
            ],
            "title": "Confidence-based model selection: When to take shortcuts for subpopulation shifts",
            "venue": "arXiv preprint arXiv:2306.11120,",
            "year": 2023
        },
        {
            "authors": [
                "Lingjiao Chen",
                "Matei Zaharia",
                "James Zou"
            ],
            "title": "FrugalML: How to use ML prediction APIs more accurately and cheaply",
            "venue": "In Proceedings of the 34th International Conference on Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Lingjiao Chen",
                "Matei Zaharia",
                "James Zou"
            ],
            "title": "Frugalgpt: How to use large language models while reducing cost and improving performance",
            "venue": "arXiv preprint arXiv:2305.05176,",
            "year": 2023
        },
        {
            "authors": [
                "Daixuan Cheng",
                "Shaohan Huang",
                "Furu Wei"
            ],
            "title": "Adapting large language models via reading comprehension",
            "venue": "arXiv preprint arXiv:2309.09530,",
            "year": 2023
        },
        {
            "authors": [
                "Aakanksha Chowdhery",
                "Sharan Narang",
                "Jacob Devlin",
                "Maarten Bosma",
                "Gaurav Mishra",
                "Adam Roberts",
                "Paul Barham",
                "Hyung Won Chung",
                "Charles Sutton",
                "Sebastian Gehrmann"
            ],
            "title": "Palm: Scaling language modeling with pathways",
            "venue": "arXiv preprint arXiv:2204.02311,",
            "year": 2022
        },
        {
            "authors": [
                "Nachshon Cohen",
                "Oren Kalinsky",
                "Yftah Ziser",
                "Alessandro Moschitti"
            ],
            "title": "Wikisum: Coherent summarization dataset for efficient human evaluation",
            "year": 2021
        },
        {
            "authors": [
                "Thomas G Dietterich"
            ],
            "title": "Ensemble methods in machine learning",
            "venue": "In International workshop on multiple classifier systems,",
            "year": 2000
        },
        {
            "authors": [
                "Alexander R Fabbri",
                "Wojciech Kry\u015bci\u0144ski",
                "Bryan McCann",
                "Caiming Xiong",
                "Richard Socher",
                "Dragomir Radev"
            ],
            "title": "Summeval: Re-evaluating summarization evaluation",
            "venue": "Transactions of the Association for Computational Linguistics,",
            "year": 2021
        },
        {
            "authors": [
                "William Fedus",
                "Barret Zoph",
                "Noam Shazeer"
            ],
            "title": "Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2022
        },
        {
            "authors": [
                "Yoav Freund",
                "Robert E Schapire"
            ],
            "title": "Experiments with a new boosting algorithm",
            "venue": "In icml,",
            "year": 1996
        },
        {
            "authors": [
                "Jerome H. Friedman"
            ],
            "title": "Greedy function approximation: A gradient boosting machine",
            "venue": "The Annals of Statistics,",
            "year": 2001
        },
        {
            "authors": [
                "Yao Fu",
                "Hao Peng",
                "Litu Ou",
                "Ashish Sabharwal",
                "Tushar Khot"
            ],
            "title": "Specializing Smaller Language Models towards Multi-Step Reasoning",
            "venue": "In Proceedings of the 40th International Conference on Machine Learning,",
            "year": 2023
        },
        {
            "authors": [
                "Mudasir A Ganaie",
                "Minghui Hu",
                "AK Malik",
                "M Tanveer",
                "PN Suganthan"
            ],
            "title": "Ensemble deep learning: A review",
            "venue": "Engineering Applications of Artificial Intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "Yonatan Geifman",
                "Ran El-Yaniv"
            ],
            "title": "Selective Classification for Deep Neural Networks",
            "venue": "[cs],",
            "year": 2017
        },
        {
            "authors": [
                "Max Grusky",
                "Mor Naaman",
                "Yoav Artzi"
            ],
            "title": "Newsroom: A dataset of 1.3 million summaries with diverse extractive strategies",
            "venue": "arXiv preprint arXiv:1804.11283,",
            "year": 2018
        },
        {
            "authors": [
                "Chuan Guo",
                "Geoff Pleiss",
                "Yu Sun",
                "Kilian Q Weinberger"
            ],
            "title": "On calibration of modern neural networks",
            "venue": "In International conference on machine learning,",
            "year": 2017
        },
        {
            "authors": [
                "Lars Kai Hansen",
                "Peter Salamon"
            ],
            "title": "Neural network ensembles",
            "venue": "IEEE transactions on pattern analysis and machine intelligence,",
            "year": 1990
        },
        {
            "authors": [
                "Trevor Hastie",
                "Robert Tibshirani",
                "Jerome H Friedman"
            ],
            "title": "The elements of statistical learning: data mining, inference, and prediction, volume",
            "year": 2009
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep Residual Learning for Image Recognition",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2016
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Collin Burns",
                "Steven Basart",
                "Andy Zou",
                "Mantas Mazeika",
                "Dawn Song",
                "Jacob Steinhardt"
            ],
            "title": "Measuring massive multitask language understanding",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Cheng-Yu Hsieh",
                "Chun-Liang Li",
                "Chih-Kuan Yeh",
                "Hootan Nakhost",
                "Yasuhisa Fujii",
                "Alexander Ratner",
                "Ranjay Krishna",
                "Chen-Yu Lee",
                "Tomas Pfister"
            ],
            "title": "Distilling step-by-step! outperforming larger language models with less training data and smaller model sizes",
            "venue": "arXiv preprint arXiv:2305.02301,",
            "year": 2023
        },
        {
            "authors": [
                "Robert A Jacobs",
                "Michael I Jordan",
                "Steven J Nowlan",
                "Geoffrey E Hinton"
            ],
            "title": "Adaptive mixtures of local experts",
            "venue": "Neural computation,",
            "year": 1991
        },
        {
            "authors": [
                "Joel Jang",
                "Seungone Kim",
                "Seonghyeon Ye",
                "Doyoung Kim",
                "Lajanugen Logeswaran",
                "Moontae Lee",
                "Kyungjae Lee",
                "Minjoon Seo"
            ],
            "title": "Exploring the benefits of training expert language models over instruction tuning",
            "venue": "arXiv preprint arXiv:2302.03202,",
            "year": 2023
        },
        {
            "authors": [
                "Dongfu Jiang",
                "Xiang Ren",
                "Bill Yuchen Lin"
            ],
            "title": "Llm-blender: Ensembling large language models with pairwise ranking and generative fusion",
            "venue": "In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
            "year": 2023
        },
        {
            "authors": [
                "Michael I Jordan",
                "Robert A Jacobs"
            ],
            "title": "Hierarchical mixtures of experts and the em algorithm",
            "venue": "Neural computation,",
            "year": 1994
        },
        {
            "authors": [
                "Peter Kairouz",
                "H Brendan McMahan",
                "Brendan Avent",
                "Aur\u00e9lien Bellet",
                "Mehdi Bennis",
                "Arjun Nitin Bhagoji",
                "Kallista Bonawitz",
                "Zachary Charles",
                "Graham Cormode",
                "Rachel Cummings"
            ],
            "title": "Advances and open problems in federated learning",
            "venue": "Foundations and Trends\u00ae in Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Pang Wei Koh",
                "Shiori Sagawa",
                "Henrik Marklund",
                "Sang Michael Xie",
                "Marvin Zhang",
                "Akshay Balsubramani",
                "Weihua Hu",
                "Michihiro Yasunaga",
                "Richard Lanas Phillips",
                "Sara Beery",
                "Jure Leskovec",
                "Anshul Kundaje",
                "Emma Pierson",
                "Sergey Levine",
                "Chelsea Finn",
                "Percy Liang"
            ],
            "title": "WILDS: A Benchmark of in-the-Wild Distribution Shifts",
            "venue": "[cs],",
            "year": 2020
        },
        {
            "authors": [
                "Alex Krizhevsky",
                "Geoffrey Hinton"
            ],
            "title": "Learning multiple layers of features from tiny images",
            "year": 2009
        },
        {
            "authors": [
                "Anders Krogh",
                "Jesper Vedelsby"
            ],
            "title": "Neural network ensembles, cross validation, and active learning",
            "venue": "Advances in neural information processing systems,",
            "year": 1994
        },
        {
            "authors": [
                "Ananya Kumar",
                "Aditi Raghunathan",
                "Robbie Jones",
                "Tengyu Ma",
                "Percy Liang"
            ],
            "title": "Fine-Tuning can Distort Pretrained Features and Underperform Out-of-Distribution",
            "year": 2022
        },
        {
            "authors": [
                "Balaji Lakshminarayanan",
                "Alexander Pritzel",
                "Charles Blundell"
            ],
            "title": "Simple and scalable predictive uncertainty estimation using deep ensembles",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Mike Lewis",
                "Yinhan Liu",
                "Naman Goyal",
                "Marjan Ghazvininejad",
                "Abdelrahman Mohamed",
                "Omer Levy",
                "Ves Stoyanov",
                "Luke Zettlemoyer"
            ],
            "title": "Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension",
            "year": 1910
        },
        {
            "authors": [
                "Daliang Li",
                "Junpu Wang"
            ],
            "title": "Fedmd: Heterogenous federated learning via model distillation",
            "venue": "arXiv preprint arXiv:1910.03581,",
            "year": 2019
        },
        {
            "authors": [
                "Chin-Yew Lin",
                "Eduard Hovy"
            ],
            "title": "Automatic evaluation of summaries using n-gram co-occurrence statistics",
            "venue": "In Proceedings of the 2003 human language technology conference of the North American chapter of the association for computational linguistics,",
            "year": 2003
        },
        {
            "authors": [
                "Tao Lin",
                "Lingjing Kong",
                "Sebastian U Stich",
                "Martin Jaggi"
            ],
            "title": "Ensemble distillation for robust model fusion in federated learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Zachary C. Lipton",
                "Yu-Xiang Wang",
                "Alex Smola"
            ],
            "title": "Detecting and Correcting for Label Shift with Black Box Predictors",
            "year": 2018
        },
        {
            "authors": [
                "Yong Liu",
                "Xin Yao"
            ],
            "title": "Ensemble learning via negative correlation",
            "venue": "Neural networks,",
            "year": 1999
        },
        {
            "authors": [
                "Saeed Masoudnia",
                "Reza Ebrahimpour"
            ],
            "title": "Mixture of experts: a literature survey",
            "venue": "Artificial Intelligence Review,",
            "year": 2014
        },
        {
            "authors": [
                "Joshua Maynez",
                "Shashi Narayan",
                "Bernd Bohnet",
                "Ryan Thomas Mcdonald"
            ],
            "title": "On faithfulness and factuality in abstractive summarization",
            "venue": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,",
            "year": 2020
        },
        {
            "authors": [
                "Brendan McMahan",
                "Eider Moore",
                "Daniel Ramage",
                "Seth Hampson",
                "Blaise Aguera y Arcas"
            ],
            "title": "Communication-efficient learning of deep networks from decentralized data",
            "venue": "In Artificial intelligence and statistics,",
            "year": 2017
        },
        {
            "authors": [
                "Ibrahim Naji"
            ],
            "title": "TSATC: Twitter Sentiment Analysis Training Corpus",
            "venue": "In thinknook,",
            "year": 2012
        },
        {
            "authors": [
                "Tim Pearce",
                "Alexandra Brintrup",
                "Jun Zhu"
            ],
            "title": "Understanding softmax confidence and uncertainty",
            "venue": "arXiv preprint arXiv:2106.04972,",
            "year": 2021
        },
        {
            "authors": [
                "Liudmila Prokhorenkova",
                "Gleb Gusev",
                "Aleksandr Vorobev",
                "Anna Veronika Dorogush",
                "Andrey Gulin"
            ],
            "title": "Catboost: unbiased boosting with categorical features",
            "venue": "Advances in neural information processing systems,",
            "year": 2018
        },
        {
            "authors": [
                "Mathieu Ravaut",
                "Shafiq Joty",
                "Nancy F Chen"
            ],
            "title": "Summareranker: A multi-task mixture-of-experts re-ranking framework for abstractive summarization",
            "venue": "arXiv preprint arXiv:2203.06569,",
            "year": 2022
        },
        {
            "authors": [
                "Jie Ren",
                "Jiaming Luo",
                "Yao Zhao",
                "Kundan Krishna",
                "Mohammad Saleh",
                "Balaji Lakshminarayanan",
                "Peter J Liu"
            ],
            "title": "Out-of-distribution detection and selective generation for conditional language models",
            "venue": "In International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Stuart J Russell"
            ],
            "title": "Artificial intelligence a modern approach",
            "venue": "Pearson Education, Inc.,",
            "year": 2010
        },
        {
            "authors": [
                "Shibani Santurkar",
                "Dimitris Tsipras",
                "Aleksander Madry"
            ],
            "title": "BREEDS: Benchmarks for Subpopulation Shift",
            "year": 2020
        },
        {
            "authors": [
                "Lingfeng Shen",
                "Lemao Liu",
                "Haiyun Jiang",
                "Shuming Shi"
            ],
            "title": "On the evaluation metrics for paraphrase generation",
            "venue": "arXiv preprint arXiv:2202.08479,",
            "year": 2022
        },
        {
            "authors": [
                "Zheyan Shen",
                "Jiashuo Liu",
                "Yue He",
                "Xingxuan Zhang",
                "Renzhe Xu",
                "Han Yu",
                "Peng Cui"
            ],
            "title": "Towards out-of-distribution generalization: A survey",
            "venue": "arXiv preprint arXiv:2108.13624,",
            "year": 2021
        },
        {
            "authors": [
                "Emily Sheng",
                "David Uthus"
            ],
            "title": "Investigating societal biases in a poetry composition system",
            "venue": "arXiv preprint arXiv:2011.02686,",
            "year": 2020
        },
        {
            "authors": [
                "Zenglin Shi",
                "Le Zhang",
                "Yun Liu",
                "Xiaofeng Cao",
                "Yangdong Ye",
                "Ming-Ming Cheng",
                "Guoyan Zheng"
            ],
            "title": "Crowd counting with deep negative correlation learning",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Nitish Srivastava",
                "Geoffrey Hinton",
                "Alex Krizhevsky",
                "Ilya Sutskever",
                "Ruslan Salakhutdinov"
            ],
            "title": "Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning",
            "year": 1929
        },
        {
            "authors": [
                "Alex Wang",
                "Kyunghyun Cho",
                "Mike Lewis"
            ],
            "title": "Asking and answering questions to evaluate the factual consistency of summaries",
            "venue": "arXiv preprint arXiv:2004.04228,",
            "year": 2020
        },
        {
            "authors": [
                "Hongyi Wang",
                "Mikhail Yurochkin",
                "Yuekai Sun",
                "Dimitris Papailiopoulos",
                "Yasaman Khazaeni"
            ],
            "title": "Federated learning with matched averaging",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Jianyu Wang",
                "Zachary Charles",
                "Zheng Xu",
                "Gauri Joshi",
                "H Brendan McMahan",
                "Maruan Al-Shedivat",
                "Galen Andrew",
                "Salman Avestimehr",
                "Katharine Daly",
                "Deepesh Data"
            ],
            "title": "A field guide to federated optimization",
            "venue": "arXiv preprint arXiv:2107.06917,",
            "year": 2021
        },
        {
            "authors": [
                "David H Wolpert"
            ],
            "title": "Stacked generalization",
            "venue": "Neural networks,",
            "year": 1992
        },
        {
            "authors": [
                "Weizhe Yuan",
                "Graham Neubig",
                "Pengfei Liu"
            ],
            "title": "Bartscore: Evaluating generated text as text generation",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Hongyi Zhang",
                "Moustapha Cisse",
                "Yann N. Dauphin",
                "David Lopez-Paz"
            ],
            "title": "Mixup: Beyond Empirical Risk Minimization",
            "venue": "In International Conference on Learning Representations,",
            "year": 2018
        },
        {
            "authors": [
                "Jingqing Zhang",
                "Yao Zhao",
                "Mohammad Saleh",
                "Peter Liu"
            ],
            "title": "Pegasus: Pre-training with extracted gap-sentences for abstractive summarization",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Le Zhang",
                "Zenglin Shi",
                "Ming-Ming Cheng",
                "Yun Liu",
                "Jia-Wang Bian",
                "Joey Tianyi Zhou",
                "Guoyan Zheng",
                "Zeng Zeng"
            ],
            "title": "Nonlinear regression via deep negative correlation learning",
            "venue": "IEEE transactions on pattern analysis and machine intelligence,",
            "year": 2019
        },
        {
            "authors": [
                "Tianyi Zhang",
                "Varsha Kishore",
                "Felix Wu",
                "Kilian Q Weinberger",
                "Yoav Artzi"
            ],
            "title": "Bertscore: Evaluating text generation with bert",
            "venue": "arXiv preprint arXiv:1904.09675,",
            "year": 2019
        },
        {
            "authors": [
                "Ming Zhong",
                "Yang Liu",
                "Da Yin",
                "Yuning Mao",
                "Yizhu Jiao",
                "Pengfei Liu",
                "Chenguang Zhu",
                "Heng Ji",
                "Jiawei Han"
            ],
            "title": "Towards a unified multi-dimensional evaluator for text generation",
            "venue": "arXiv preprint arXiv:2210.07197,",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": "1 INTRODUCTION\nTraditional training of ML/AI models, i.e., empirical risk minimization, allows obtaining experts specialized for a particular task and domain. However, in practice, the distribution of the test data often differs leading to significant performance degradation (Koh et al., 2020; Santurkar et al., 2020). The emergence of Foundation Models (Bommasani et al., 2021) allows obtaining expert models with simple fine-tuning on a handful of data, but such expert models still face generalization issues (Kumar et al., 2022; Jang et al., 2023; Cheng et al., 2023). In the case of Large Language Models (LLMs), although most advanced commercial models can perform well on a range of tasks, they are closed-source, expensive to use, and can be outperformed by smaller specialized models (Hsieh et al., 2023; Gunasekar et al., 2023; Jang et al., 2023; Fu et al., 2023). In other words, training expert models have become extremely effective, while generalization with a single model remains challenging. In this paper, we aim to develop methods for combining the strengths of models with complementary expertise to push their collective generalization capabilities.\nCombining experts is a long-standing and successful practice in ML: classical approaches like ensembling (Dietterich, 2000; Ganaie et al., 2022) or Mixture of Experts (MoE) (Jacobs et al., 1991; Jordan & Jacobs, 1994; Masoudnia & Ebrahimpour, 2014) often lead to performance improvements. These techniques typically consider experts trained and tested on the same data distribution, although improvements in terms of out-of-distribution (OOD) generalization have also been observed (Lakshminarayanan et al., 2017; Shen et al., 2021). In contrast, in this paper, we revisit the practice of combining experts in a new setting where we are given pre-trained experts with complementary expertise, and our emphasis is on generalization to test data distributions where none of the experts perform well individually. See Figure 1 for an illustration.\nMore specifically, we consider a problem where the data distribution is comprised of K domains, and we have access to K expert models, one for each of the domains. We do not make any assumptions about how experts were trained to make it compatible with modern practices of fine-tuning Foundation Models to obtain high-quality experts. At test time, the data is sampled from the mixture of the K domains, i.e., it contains data from every domain, and thus any individual expert will be sub-optimal.\nOur goal is to train a model using expert outputs that produces final predictions or generations either by choosing one of the experts or by combining their outputs, for a given input data point. We refer to such models as the Fusion of Experts (FoE) models. In our experiments, we consider tasks such as image/text classification, text generation, and automatic evaluation of generated summaries (Mani, 2001). Finally, recognizing that obtaining the outputs of every expert can be expensive, we consider the \u201cfrugal\u201d setting (Chen et al., 2020) and propose an extension of our method to reduce the number of expert evaluations. Our contributions are summarized below:\n1. We formulate the Fusion of Experts (FoE) problem of fusing outputs of models with complementary expertise and cast it as a supervised learning problem. Our approach is applicable to both discriminative and generative use cases.\n2. We further extend the problem to present Frugal Fusion of Experts (FrugalFoE). In specific, we extend our formulation by casting it as a graph shortest path problem that allows us to efficiently perform expert fusion while only evaluating a subset of experts at test time.\n3. We demonstrate the efficacy of our method through extensive experimental evaluations on image classification with standard deep learning methods, text classification, summarization, and question answering using Large Language Models (LLMs), and automatic evaluation of generated summaries. Our proposed fusion method can greatly improve the performance of individual experts, while also reducing the number of expert evaluations at test time."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Ensemble learning. Ensemble learning combines several individual models (e.g., either model outputs or model weights directly) to obtain better performance (Breiman, 1996a;b; Ganaie et al., 2022), which has also been justified by theoretical analysis (Hansen & Salamon, 1990; Krogh & Vedelsby, 1994). Classical ensemble methods include bagging (bootstrap aggregating), boosting, and stacking (model blending) (Breiman, 1996a; Freund et al., 1996; Friedman, 2001; Wolpert, 1992). Negative correlation learning, which encourages ensemble models to learn diverse aspects from the training data, has also been widely used for deep ensemble methods (Liu & Yao, 1999; Shi et al., 2018; Zhang et al., 2019a; Buschj\u00e4ger et al., 2020). Especially in deep neural networks, the ensemble effect can be introduced implicitly by various methods, e.g., Dropout (Srivastava et al., 2014). Most works in this group implicitly assume that models in the ensemble have similar expertise, thus it is beneficial to aggregate their predictions. To combine models with complementary expertise, averaging their outputs might be detrimental due to most of them being not suitable for an input.\nMixture of experts (MoE). The basic idea of MoE is a learned weighted ensemble among \u201cexpert models\" where the expert(s) choice is made via a \u201cgating mechanism\", i.e., that controls the expert selection for a certain inference query/instance or the weight coefficients to combine various experts (Jacobs et al., 1991; Jordan & Jacobs, 1994; Masoudnia & Ebrahimpour, 2014). The idea of MoE has recently been extended to LLMs where multiple MLP experts are integrated after each multi-head self-attention in the Transformer encoder/decoder blocks (Fedus et al., 2022; Chowdhery et al., 2022). The use of MoE in LLMs has been demonstrated to effectively scale the model sizes up further without costing proportional increase in the computation complexity, as the computation in MoE is effectively sparse. The majority of works in this group are designed for the joint training of \u201cexperts\u201d and the gating/aggregation module. In our problem setting, the experts are pre-trained on their respective domains and serve as a starting point for the Fusion of Experts.\nFederated/collaborative Learning. Federated/collaborative learning allows various clients/agents to jointly learn using their own (mostly private) local data (Kairouz et al., 2021; Wang et al., 2021). During the federated learning, participating clients conduct in-situ learning and computing before their locally learned models are communicated to a central server for model aggregation or fusion (McMahan et al., 2017). Common federated model fusion/aggregation methods include various averaging schemes, ensemble-based schemes, and MoE type of gating mechanisms (McMahan et al., 2017; Wang et al., 2020b; Li & Wang, 2019; Lin et al., 2020). Our work can be seen as a special case of federated learning where clients train their own models locally and share it with the central server for training the FoE model to aggregate them.\nCombining pre-trained models. One common way a practitioner can interact with a pre-trained model is via an API. Such models typically vary in performance and cost. FrugalML (Chen et al., 2020) aims to maximize the usage of the cheapest API on \u201ceasy\u201d inputs, while only querying the more expensive ones when needed. This work has also been extended to LLMs (Chen et al., 2023b). In our setting, the expert models have similar costs and complementary expertise, i.e. no expert is\nbetter than any other across all of the data distributions. Finally, (Ravaut et al., 2022; Jiang et al., 2023) train auxiliary LLMs to combine generations of general (non-expert) LLMs, however, do not consider the \u201cfrugal selection of experts\"."
        },
        {
            "heading": "3 FUSING MODELS WITH COMPLEMENTARY EXPERTISE",
            "text": "The real data distributions are often diverse and challenging to represent with a single dataset and/or mimicked by a single AI model (Santurkar et al., 2020; Koh et al., 2020). For example, in image recognition, different geographic regions has different patterns such as colors of traditional attires (Atwood et al., 2020). In text summarization, models are typically fine-tuned for specific domains such as news articles from a particular publisher, e.g., CNN (Lewis et al., 2019), or WikiHow articles (Cohen et al., 2021). To model such distributions, we consider a mixture model with K components:\nD = K\u2211 k=1 \u03b1kDk, \u2211 k \u03b1k = 1, \u03b1k > 0\u2200 k, (3.1)\nwhere each Dk represents a subdomain of D such as images from a specific geographic location. Our goal is to train an AI system that can perform well on D leveraging the recent advances in Foundation Models that allow us to obtain powerful models (experts) for any given Dk even when training data from Dk is scarce (e.g., via few-shot learning) (Brown et al., 2020). Such experts alone typically struggle to perform well on D due to the performance drop on any domain apart from the one they were fine-tuned on (Jang et al., 2023). In this work, we explore strategies for fusing different expert outputs to improve performance on D. Thus, the input to our problem is a set of models {fk : X \u2192 Y}Kk=1 and some amount of validation data {xki \u2208 X , yki \u2208 Y} nk i=1 from every domain k = 1, . . . ,K, where X and Y are input and output spaces correspondingly. We consider supervised and generative scenarios, i.e., Y can be numerical, categorical, or representative natural language."
        },
        {
            "heading": "3.1 FUSION OF CLASSIFICATION EXPERTS",
            "text": "Let us consider a supervised learning scenario with C classes, i.e., Y is the probability simplex \u2206C\u22121. Our goal is to fuse the expert outputs, i.e., to learn a mapping from {fk(x)}Kk=1 to associated label y. We review common measures to combine expert predictions, then present our fusing approach.\nThe traditional approach to combine expert predictions is to average their outputs, i.e., ensembling: 1 K \u2211 k fk(x). Another popular choice is to use the most confident model (Pearce et al., 2021; Chen et al., 2023a), i.e., predict with the model that outputs the largest probability for a class: fk\u2217(x), k\n\u2217 = argmaxk maxc [fk(x)]c. Model confidence is often used in selective classification (Geifman & El-Yaniv, 2017) to decide when to abstain from making a prediction. However, we found that both ensembling and model confidence are not well-suited for combining experts with complementary expertise as expert confidence on OOD, i.e., on data that they did not see during training, can be poor or misleading (Koh et al., 2020).\nIn this work, we propose Fusion of Experts (FoE), a simple learning problem for training a fuser using validation data from the K domains that we found to perform very well in practice. Let {xki \u2208 X , yki \u2208 Y} nk i=1 be validation data from every domain k = 1, . . . ,K. We construct features for every input by concatenating expert outputs, i.e., f(x) = [f1(x), . . . , fK(x)] and train the fuser F\u03b8 parameterized by \u03b8 via empirical risk minimization:\nmin \u03b8 \u2211 k \u2211 i \u2113(F\u03b8(f(x k i )), y k i ), (3.2)\nwhere \u2113 is a suitable loss function such as cross-entropy. The procedure is domain agnostic, and the fuser can be a small fully-connected neural network, that ingests the outputs of all experts to produce a class prediction."
        },
        {
            "heading": "3.2 FUSION OF GENERATIVE EXPERTS",
            "text": "Next, we discuss FoE for generative experts in the context of LLMs. The key difference is that the output space is a lot more complicated, e.g., natural text in LLM applications. This makes the strategies discussed above non-trivial to apply.\nEnsembling LLMs can be done by averaging the next token probabilities during generation, however, this requires that all LLMs share the tokenizer and vocabulary, which is rarely true for open-source models and essentially constraints one to fine-tuning the same model for different domains to obtain\nexperts. Confidence-based expert selection is fairly straightforward to use with LLMs by comparing the log-likelihoods (or perplexities) of LLMs generating corresponding outputs (Ren et al., 2023).\nDirectly applying the fusing strategy from supervised learning 3.2 would essentially mean training a new LLM, i.e., a model that inputs concatenated texts generated by expert models and generates text matching the reference outputs. Jiang et al. (2023) explored such \u201cgenerative fusion\u201d in the context of a single domain and regular (non-expert) LLMs. However, training a new generative LLM fuser that would be good on all K domains contradicts our problem motivation, i.e., that it is challenging to train a model simultaneously suitable for all domains. Instead, we opt for a simpler fusing approach where we learn which expert to use for a given input.\nFirst, we make a modification to how an input is represented using the expert models. Instead of concatenating their generated text outputs, we concatenate the embeddings of the input and generated tokens from the last layer of the corresponding transformer models (the predominant architecture of modern LLMs). Let fek(x) denote the average of fk\u2019s last-layer token embeddings of the input text x and the output text fk(x). Such embeddings were previously used by Ren et al. (2023) for OOD detection with LLMs, thus suggesting that they contain information about data distributions we need to identify the correct expert.\nOur representation of an input using experts is the concatenation of these averaged embeddings: f(x) = [fe1 (x), . . . , f e K(x)]. We train the FoE model via empirical risk minimization to predict the index of the correct expert: min \u03b8 \u2211 k \u2211 i \u2113(F\u03b8(f(x k i )), k), (3.3)\nwhere \u2113 is the cross entropy loss for a K-way classification problem. As before, F\u03b8 can be a simple fully-connected neural network. In this formulation, the fuser F\u03b8 ingests a vector of concatenated embeddings f(x) and outputs the index of an expert corresponding to the input x. The expert is then used to produce the final generation."
        },
        {
            "heading": "3.3 WHEN FUSING EXPERTS IS A GOOD IDEA",
            "text": "Using only expert outputs may seem restrictive, but it is actually not too harmful in the applications we consider. To keep things simple, we consider the task of choosing one of the experts as in 3.3. As we shall see, as long as (the expertise of) the experts are complementary, we expect their outputs to be sufficiently informative.\nConsider the expert fusion problem as a multi-way hypothesis testing problem. Define F\u2217(X) as the ground truth best expert for input X:\nF\u2217(x) \u225c argmink\u2208[K]E [ \u2113(fk(x), Y ) | X = x ] (3.4)\nand let F (f(X)) be an approximation to F\u2217 that only uses expert outputs as inputs. Fano\u2019s inequality provides a lower bound on the accuracy of F \u25e6 f :\nP{F (f(X)) \u0338= F\u2217(X)} \u2265 H(F\u2217(X))\u2212 I(f(X), F\u2217(X))\u2212 log 2\nlog(K \u2212 1) , (3.5)\nwhere H(F\u2217(X)) is the (Shannon) entropy of F\u2217(X) and I(f(X), F\u2217(X)) is the mutual information between f(X) and F\u2217(X). From this lower bound, we see that the larger the mutual information between F\u2217(X) and f(X), the higher the accuracy we can expect from F \u25e6 f . Intuitively, I(f(X), F\u2217(X)) is large whenever it is possible to recover F\u2217(X) from expert outputs f(X), and this is exactly the case when the experts are complementary.\nFor example, consider a classification task: the experts themselves are classifiers fk : X \u2192 \u2206C\u22121, and the label is one-hot encoded. As long as there is label shift (Lipton et al., 2018) among the domains, we expect E [ f(Xk) ] \u2248 E [ Yk ] to vary across domains. Thus it is possible to distinguish between inputs from different domains from f(X) (so I(f(X), F\u2217(X)) is large), and we expect good expert selection performance."
        },
        {
            "heading": "4 FRUGAL FUSION OF EXPERTS (FRUGALFOE)",
            "text": "Producing a prediction or conditional generation output, from an input x via FoE, can be summarized as follows: we first query K experts, extracting a vector of concatenated outputs f(x), and then use a fuser F\u03b8 that produces a final output based on f(x). This leads to high performance when applied to\ntest data but with the limitation that querying all experts can be costly. We now introduce a sequential algorithm called FrugalFoE to select a small yet sufficient set of experts. For the rest of this section, we assume that querying an expert fk incurs a non-negative cost ck, which might include energy consumption if experts are run locally or API costs when a third party provides experts."
        },
        {
            "heading": "4.1 INTRODUCING FRUGALFOE",
            "text": "We start with some definitions and then present our approach in a general form that covers both applications to classification and generative tasks. Let (X,Z) represent a random pair of inputs and outputs; at test time, the input X , e.g., an image of a cat or a long news article, is fixed at x while the output Z is unknown. In this setup, Z can be either a label Y , in the classification case, or a domain membership variable, in the generative case. Let F be the set of all experts {f1, \u00b7 \u00b7 \u00b7 , fK} in the classification case and the set of all embedders obtained from experts {fe1 , \u00b7 \u00b7 \u00b7 , feK} in the generative case. To ease the exposition of this section, we (i) refer to elements of F as \u201cexperts\u201d in both classification and generative cases and (ii) drop the superscript \u201ce\u201d in the generative case. Furthermore, define fS(x) \u225c [ fk(x) ] fk\u2208S\nas the concatenation of the outputs of all experts in S. Finally, the conditional expected loss of predicting with a set of experts S given the event {fS\u0303(X) = fS\u0303(x)}, i.e., after querying all experts in S\u0303 and observing their outputs, is defined as\nL(x,S, S\u0303) \u225c E [ \u2113 ( F\u03b8(fS(X)), Z ) | fS\u0303(X) = fS\u0303(x) ] + \u03bb \u2211 k:fk\u2208S ck, (4.1)\nwhere F\u03b8 represents a given fuser parameterized by \u03b8 and \u03bb > 0 is a constant that controls the trade-off between querying costs and errors made by the fuser. Realize that the cost sum is taken over k for fk \u2208 S, i.e., it depends on a set S of experts we are evaluating. Note that the used fuser F\u03b8 depends on S through the inputs fS(x). This implies that we need a fuser for each S \u2208 2F . We assume that fusers for all S are available and discuss this aspect further in Section 4.2. Now we are ready to introduce the idea behind FrugalFoE.\nSuppose that, for a fixed input x and up to a certain instant, we have queried the experts S\u0303. The expected loss of stopping at this point and applying a fuser on top of fS\u0303(x) is given by L(x, S\u0303, S\u0303). The question is whether to query an extra expert or not: can we find an extra expert f\u0304 \u0338\u2208 S\u0303 such that L(x, S\u0303 \u222a {f\u0304}, S\u0303) < L(x, S\u0303, S\u0303)? That is, given all the information collected up to that point, can we do better by querying more experts? If yes, we should find the extra expert f\u0304 that minimizes L(x, S\u0303 \u222a {f\u0304}, S\u0303) and query it. If not, we should stop and apply F\u03b8 on the top of S\u0303 to get a final prediction. In practice, however, we cannot evaluate the conditional expected loss L(x,S, S\u0303), for any sets S and S\u0303 , because the conditional distribution of \u2113 ( F\u03b8(fS(X)), Y ) given {fS(X) = fS(x)} is usually unknown. In spite of that, we can estimate it on the fly using a k-nearest-neighbors non-parametric estimator for conditional expectations (Hastie et al., 2009) using validation data:\nL\u0302(x,S, S\u0303) \u225c 1 M \u2211 (xm,zm)\u2208NM (x,S\u0303) \u2113(F\u03b8(fS(xm)), zm) + \u03bb \u2211 k:fk\u2208S ck. (4.2)\nThe neighborhood set NM (x, S\u0303) are the M nearest neighbors (and corresponding targets) of the input point x among the validation set, where the distance between points is the Euclidean distance in the space of the queried experts\u2019 outputs, i.e., dS\u0303(x, x\n\u2032) = \u2225fS\u0303(x)\u2212 fS\u0303(x\u2032)\u22252. Thus, we assume access to the experts\u2019 outputs for all data points in a validation set, i.e., for each combination data point/expert, we have a different output."
        },
        {
            "heading": "4.2 IMPLEMENTING FRUGALFOE",
            "text": "Starting expert. In our problem setting we interact with input x only through the expert outputs. Thus, we should select an expert to call first, which will be the same for every input. We can make this selection by simply considering the loss of 1-expert fusers on the validation data {xki , zki } nk i=1 (zki = y k i in classification and z k i = k in generation):\nargminf\u0304\u2208F \u2211 i,k \u2113(F\u03b8(f\u0304(x k i )), z k i ). (4.3)\nThat is, we choose the expert that has the best average performance in the validation set. For interpretability reasons, we use 0-1 loss as \u2113 in our implementation of FrugalFoE.\nSubsequent experts. Let S\u0303 be the set of experts queried so far for x. Before deciding on the next expert, we update the current estimate of the quality of S\u0303 . We use L\u0302(x, S\u0303, S\u0303) from (4.2) as the quality estimate. Next, we find the expert f\u0304\u2217 that we expect to provide the maximum improvement:\nf\u0304\u2217 = argminf\u0304\u2208F\\S\u0303L\u0302(x, S\u0303 \u222a {f\u0304}, S\u0303). (4.4)\nIf L\u0302(x, S\u0303 \u222a{f\u0304\u2217}, S\u0303)\u2212 L\u0302(x, S\u0303, S\u0303) > 0 we terminate the search and return F\u03b8(fS\u0303(x)). Otherwise, we evaluate expert f\u0304\u2217 on the input x, update S\u0303 = S\u0303 \u222a {f\u0304\u2217}, and continue the search. In our experiments, we consider the cost ck of all experts to equal 0.01. Then \u03bb can be interpreted as the minimal error rate reduction we want to achieve when deciding whether to query an additional expert.\nObtaining fusers for subsets of experts. So far we have assumed that we have access to fusers F\u03b8(fS(\u00b7)) for all S \u2208 2F . These fusers must be trained before FrugalFoE deployment using training data and objective functions in equations 3.2 and 3.3. In general, this is only feasible for a small number of experts, or if we restrict FrugalFoE to always terminate after a small number of expert calls (out of a potentially bigger set of experts). It is, however, possible to bypass this limitation by using kNN classifiers as the fuser models as those do not require training and can be evaluated on the fly at test time. Specifically, let xm \u2208 NM (x, S\u0303) be a point from the validation dataset that is a neighbor of a test input x based on the expert outputs in S\u0303. We can evaluate the loss on this point required for FrugalFoE as follows:\n\u2113(F\u03b8(fS\u0303(xm)), zm) = \u2113(kNN(xm), zm), (4.5)\nwhere kNN(xm) is the majority output corresponding to \u03ba nearest neighbors of xm in the validation data excluding itself. The neighbors for each validation point can be precomputed using the outputs of all experts F before deployment. Evidently, this procedure bypasses the need to train 2F fusers."
        },
        {
            "heading": "4.3 FRUGALFOE AS A GRAPH SHORTEST-PATH PROBLEM SOLVER",
            "text": "FrugalFoE is an algorithm that can be motivated as a shortest-path problem solver with connections to the A\u2217 algorithm (Russell, 2010). We first need to frame the sequential expert selection problem as a graph search problem to see that. Consider a weighted directed graph G = (V, E) with 2K + 1 vertices: 2K vertices corresponding to all subsets (including the empty set) of the K experts, and an additional target vertex. We label each vertex (except the target vertex) with (the set of indices of) the subset of the experts associated with the vertex. For example, the vertex {1, 2} is associated with the subset {f1, f2}. Each vertex associated with a subset of experts S is connected to vertices associated with the subsets of size |S| + 1, including S. The length of each edge between two vertices associated with subsets of the experts is the cost of querying the additional expert, scaled by \u03bb, in the current vertex. For example, if K = 3, the vertex {1} is connected to the vertices {1, 2} and {1, 3}, and the lengths of the edges are \u03bbc2 and \u03bbc3. Finally, all 2K vertices associated with subsets of the experts are connected to the terminal vertex. For a given input x and set of fusers, the length of the edge connecting a vertex associated with a set of experts S to the terminal vertex is 1 M \u2211 (xm,zm)\u2208NM (x,S\u0303) \u2113(F\u03b8(fS(xm)), zm) if we have gathered outputs from all experts in S\u0303 (4.2).\nFrom here, we can draw connections to the A\u2217 algorithm. In A\u2217, for any prospective vertex n to be visited on the graph, the estimated distance traversing through n from the initial to the terminal vertex can be dissected into the sum of the known distance from the initial vertex to n, denoted as g(n), and an estimate of the distance from n to the terminal vertex, denoted as h(n). Among all prospective vertices, we opt for n\u2217 that yields the smallest sum g(n\u2217)+h(n\u2217). In FrugalFoE, if we are currently at a vertex representing the set of experts S\u0303 , the next step is to query some extra expert f\u0304 or opt for the terminal vertex, which we denote by T . If we decide to query f\u0304\u2217, using the logic explained in equation 4.4, then g(S\u0303, f\u0304\u2217) = \u03bb \u2211 k:fk\u2208S\u0303\u222a{f\u0304\u2217} ck and h(S\u0303, f\u0304 \u2217) = 1M \u2211 (xm,zm)\u2208NM (x,S\u0303) \u2113(F\u03b8(fS(xm)), zm). If we decide on the terminal vertex (i.e., to conclude the search), g(S\u0303, T ) = L\u0302(x, S\u0303, S\u0303) and h(S\u0303, T ) = 0. The connection between FrugalFoE and the A\u2217 algorithm is clear once we define g and h; however, there is no perfect correspondence between the two algorithms since in FrugalFoE the functions g and h depend on the current set of vertices S\u0303 as opposed to the classical A\u2217 version where these functions only depend on the candidate vertex. This difference is mainly due to the fact that we can not \u201cundo\u201d querying an expert, so it is more reasonable to try to utilize all expert outputs available at any given decision point."
        },
        {
            "heading": "5 EXPERIMENTS",
            "text": "We evaluated the efficacy of the FoE method using a wide range of experiments, including image classification, summarization, text generation evaluation, sentiment analysis, and massive multitask language understanding (MMLU) (Hendrycks et al., 2021). Additionally, we investigated how to enhance FoE\u2019s efficiency, aiming to achieve the desired accuracy while consulting the fewest number of experts, i.e., FrugalFoE. FoE manages to achieve close (and matches on some tasks) performance compared to the \u201coracle model\" (i.e., always selecting the most suitable expert for a given task) and consistently surpasses individual experts and other popular baseline methods (including FrugalML (Chen et al., 2020), ensemble) by a notable margin. Interestingly, FoE inherently tends to be frugal in summarization and sentiment analysis tasks. Specifically, relying on information from just one expert often yields results as effective as consulting all experts. For tasks where FoE is not naturally frugal, our FrugalFoE algorithm proves effective. For instance, in the CIFAR experiment, FrugalFoE queries only 37.5% of the experts to match the accuracy achieved by querying all experts."
        },
        {
            "heading": "5.1 THE CIFAR-100 SUPER-CLASS CLASSIFICATION TASK",
            "text": "We start with an image classification experiment on CIFAR-100. While this is a relatively simple task and on a small scale, it effectively demonstrates the capabilities of FoE.\nWe partition CIFAR-100 into 20 sections where each section contains all images from 30 subclasses (Krizhevsky et al., 2009). Out of these 30 sub-classes, 20 come exclusively from one sub-class of each super-class (for instance, beaver from aquatic mammals, bottles from food containers, etc), while the remaining 10 are picked at random from all the other sub-classes. Thus, these sections have overlapping images, meaning they are not mutually exclusive. This partitioning strategy (inspired partly by Santurkar et al. (2020)) ensures that each expert excels in classifying one sub-class within each super-class, however, they have some shared knowledge (more details in Appendix A.2).\nWe train 20 experts, each of them using ResNet-18 on their own data partition to predict superclasses (He et al., 2016). We use 40k images from the CIFAR-100 training set for partitioning and expert training and hold 10k images from the training set out as a validation set to train our fusing strategy by solving equation 3.2. The final test accuracy is measured using the CIFAR test set.\nOur results are shown in Table 1. In the confidence-based fusing baseline, we used the maximum softmax score as an indicator of a model\u2019s confidence and then chose the model with the highest confidence for predictions, where models were trained with mixup, a method that effectively calibrates the model\u2019s confidence (Guo et al., 2017; Zhang et al., 2018). Additionally, we show the ac-\ncuracy of an oracle classifier, which operates under the assumption that each test sample is always evaluated using the ideal expert \u2014 that is, the expert trained on the data containing the subclass of the the test sample. Our results demonstrate that using FoE markedly improves accuracy over individual experts. FoE also outperforms the confidence-based fusion approach and a basic ensemble method, both in prediction accuracy and expert selection. Notably, FoE\u2019s performance comes close to matching the accuracy of the oracle expert.\nFrugalFoE for image classification. FoE performs well on the CIFAR task, however, for each test sample, it needs to query all 20 experts, which is computationally expensive. We seek to make it frugal using our FrugalFoE algorithm described in Section 4.2 using the kNN fusing approach 4.5 with \u03ba = {7, 9, 11} (FoE\u2019s performance does not seem to be sensitive to different \u03ba values). We vary the selection of M and \u03bb to control the number of experts we end up with querying. The FrugalMoE results are shown in Figure 2 where one can observe that FrugalFoE can use 37.5% of the experts to reach the accuracy of using the entire 20 experts on the CIFAR task.\nWhen querying the same number of experts, FrugalFoE outperforms FrugalML, which is also limited to up to two experts. In Figure 5 we present results with a neural network as a fuser where we limit the maximum number of expert calls to 5 to make the problem feasible."
        },
        {
            "heading": "5.2 THE SENTIMENT ANALYSIS TASK",
            "text": "Table 2: Sentiment analysis task experiments.\nMethod TFN Poem Auditor Reviews Sent. Avg.\nFoE 87.54% 85.71% 81.71% 95.20% 91.88%\nTFN Expert 87.54% 0.0% 58.54% 0.40% 26.85%\nPoem Expert 6.69% 85.71% 15.24% 59.76% 46.95%\nAuditor Expert 51.98% 45.71% 81.71% 49.65% 53.50%\nReviews Sent. Expert 71.12% 62.86% 32.93% 95.20% 79.44%\nFoE w/ TFN Expert features only 86.93% 85.71% 82.32% 95.20% 91.81%\nFoE w/ Poem Expert features only 87.23% 82.86% 81.10% 95.20% 91.75% We consider language model applications, starting with the sentiment analysis task. We use fine-tuned sentiment classification models from Hugging Face (more details in Appendix A.4) as experts and the corresponding datasets, Twitter Sentiment Analysis, Twitter Financial News, Poem Sentiment, Data Reviews Sentiment Analysis, Auditor Sentiment (Naji, 2012; Sheng & Uthus, 2020). Though sentiment analysis is essentially a classification task, we train the fuser using the generative model strategy 3.3. To test we combined the test sets from all tasks. Results for per task and average accuracies are in Table 2 (upper part). FoE achieves 99.1% accuracy in expert selection. FoE almost reaches the best sentiment classification accuracy on each downstream task, i.e., the oracle expert performance.\nOur other finding is that the expert outputs, i.e., the embeddings of language models are highly informative such that only querying a single expert is sufficient for the FoE to predict the expertto-use surprisingly well. In Table 2 (lower part) we present results when using a single expert, i.e., extreme frugal case, noting that performance is almost equal to using all experts. Results using other stand-alone experts are in Table 13."
        },
        {
            "heading": "5.3 THE SUMMARIZATION TASK",
            "text": "In this section, we present our experiments on generative tasks, starting with the summarization task. We use fine-tuned Pegasus models on six downstream summarization tasks as experts (Zhang et al., 2020). We use the combined validation datasets from all six summarization tasks to train the generative fuser 3.3 for expert prediction. For testing, we combined the test sets from all tasks and used ROUGE-2 (Lin & Hovy, 2003) scores to assess the performance. Results for per task and average accuracies are in Table 3 (upper part). Expert selection accuracy of FoE is 99.1% with the six experts. One can see that FoE almost reaches the best ROUGE-2 score on each downstream task, i.e., FoE almost reaches the performance of the oracle expert.\nSimilar to our sentiment analysis experiment, querying a single expert is almost as good as using all experts (see Table 3 lower part and Table 14 for the remaining experts)."
        },
        {
            "heading": "5.4 THE MMLU TASK",
            "text": "In all previous experiments, we were fusing experts properly fine-tuned for their respective domains. Here we consider a weaker notion of experts, i.e., general LLMs that happen to perform better than others on a particular domain. Specifically, we consider the MMLU (Hendrycks et al., 2021) multiple-choice QA task which consists of 57 categories (elementary mathematics, US history, etc.). As experts, we use 15 open-source LLMs with sizes of \u223c 7 billion parameters from the Open LLM Leaderboard (Beeching et al., 2023). We consider an LLM with the highest accuracy on a category to be an expert for this category. FoE is trained as a generative fuser 3.3 (details are in Appendix A.3). The test set is the combination of test sets from all 57 categories.\nThe embedding dimension of expert LLMs is fairly high (4096), while there is only a total of 1700 samples for training the fuser, thus we use single expert outputs for FoE in this experiment. Results are presented in Table 4. We see that the results with the weak experts are worse as we no longer can match the oracle performance, however, FoE still outperforms the strongest of the considered LLMs. Next, unlike our previous experiments, we notice a discrepancy in performance when using outputs of different\nweak experts, especially in terms of expert selection accuracy. Overall, we conclude that weak experts are not as effective, although still can benefit from FoE. We also tested the generalization capability of FoE to unseen domains on the MMLU task, the results can be found in Appendix B.1."
        },
        {
            "heading": "5.5 THE TEXT GENERATION EVALUATION TASK",
            "text": "We investigate the potential of using complementary experts to evaluate machinegenerated summaries. For this experiment, we use human-annotated summary datasets namely SummEval (Fabbri et al., 2021), Newsroom (Grusky et al., 2018), QAGS-CNN/XSUM (Wang\net al., 2020a), and HALL-XSUM (Maynez et al., 2020). Each dataset has human ratings for specific dimensions such as coherence and consistency. To train and evaluate experts for each one of the datasets/domains, we: (i) extract some automatic metrics, e.g., BARTScore (Yuan et al., 2021), BERTScoreFree (Shen et al., 2022), UniEval (Zhong et al., 2022), from summaries; (ii) randomly select training and test points from each dataset; (iii) for each pair domain/dimension, we train a linear model (expert) that optimally combines precomputed metrics. We compare the performances of FoE and individual experts in evaluating the consistency1 (or factuality) of machine-generated summaries. In Table 5, we can see that, for individual tasks, FoE does not lead to the highest correlations; however on the aggregated test set (last column) our approach delivers the overall best results. We present additional details and comparison to individual metrics in Appendix A.1.\nWe note that our FrugalFoE approach is not directly applicable in this setting due to the design of the experts. In this case, they are simple linear models that use various automatic metrics as features. The experts themselves are cheap to evaluate, however, it may be desirable to reduce the number of their input features, i.e., automatic metrics based on LLMs, to improve test-time efficiency. We leave the extension of FrugalFoE to such expert design for future work."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "Our work demonstrates the benefits of fusing experts fine-tuned for different data subdomains. Obtaining high-quality experts has become relatively easy with the emergence of foundation models, thanks to their few-shot learning capabilities. The fusing procedure is also fairly simple: using appropriate model outputs it suffices to train a simple neural network, as supported by our experiments. We have seen that it is even beneficial to fuse general LLMs that happen to perform well on various domains, although not as effective as the fusion of proper expert models.\nWe have also proposed a frugal extension of the expert fusion for the cases where querying all experts at test time is too costly. Our solution based on kNN classifiers and the graph shortest path analogy allowed us to bypass the combinatorial nature of the problem that limited prior frugal methods to consider at most 2-3 expert/API calls (Chen et al., 2020; 2023b). Interestingly, in the considered LLM applications, using only the outputs of an arbitrary single expert for all test inputs was sufficient to identify the correct expert for the input. This observation suggests that LLM embeddings contain a lot of information that can potentially be utilized in other applications. This observation extends the findings of Ren et al. (2023), who demonstrated that LLM embeddings are good for OOD detection, i.e., distinguishing their own domain from everything else.\n1Consistency/factuality measure if the facts presented in the summary are consistent with those in the source text. In the Newsroom dataset, this dimension is termed as \u201crelevance\u201d. See Table 6 for the full set of results."
        },
        {
            "heading": "CONTENTS OF THE APPENDIX",
            "text": ""
        },
        {
            "heading": "A More Details on Experimental Setups 14",
            "text": "A.1 More details on text generation evaluation . . . . . . . . . . . . . . . . . . . . . . 14 A.2 More details on the CIFAR experiment . . . . . . . . . . . . . . . . . . . . . . . . 17 A.3 More details on the MMLU experiment . . . . . . . . . . . . . . . . . . . . . . . 17 A.4 More details on the sentiment analysis experiment . . . . . . . . . . . . . . . . . . 19"
        },
        {
            "heading": "B Additional Experimental Results 20",
            "text": "B.1 Generalization to new domains with weak experts . . . . . . . . . . . . . . . . . . 20 B.2 Additional Results on FrugalFoE of the CIFAR-100 task . . . . . . . . . . . . . . 21 B.3 Additional Experiment of the Sentiment Analysis Task . . . . . . . . . . . . . . . 21 B.4 Additional Experiment of the Summarization Task . . . . . . . . . . . . . . . . . . 21"
        },
        {
            "heading": "C Details on The Hyperparameters 22",
            "text": ""
        },
        {
            "heading": "A MORE DETAILS ON EXPERIMENTAL SETUPS",
            "text": ""
        },
        {
            "heading": "A.1 MORE DETAILS ON TEXT GENERATION EVALUATION",
            "text": "The first observation we need to make is that, because each dataset has a different format for the human annotations, experts\u2019 outputs are not comparable, and we cannot compute the aggregate correlation of predictions and human labels for a given task (pair dataset/dimension) when evaluating FoE. Instead, we estimate the expected conditional correlation between predictions and human labels in the following way: using the outputs from the eleven experts and for each domain, we classify each text in the test set according to their dataset membership, and according to the empirical distribution of classes induced by the classifier, we compute a weighted average of the correlations achieved by experts in that specific task. Now, we can describe our experiment in detailed steps.\nThis experiment is repeated 25 times with different random seeds. The final correlations/numbers are the averages across the 25 repetitions. For each random seed b, we do the following:\n1. For all datasets, extract automatic metrics, i.e., BARTScore (Yuan et al., 2021), BERTScore.Free (Zhang et al., 2019b; Shen et al., 2022), Unieval (Zhong et al., 2022). For BARTScore, we use four variations: the first one is BARTScore-CNN (used by Yuan et al. (2021)), and for the last three, we use Pegasus (Zhang et al., 2020) pre-trained on CNN-DM, Newsroom, and XSUM as the backbone model. We work only with reference-free metrics, and therefore we do not use UniEval to evaluate relevance;\n2. For each dataset, randomly select training and test samples. We select 50 test points for all datasets, but the number of training points depends on how big the datasets are. The biggest training set has 370 data points;\n3. For each task (pair dataset/dimension), train a linear model using non-negative least squares to predict human rating from automatic metrics. In total, we have eleven experts;\n4. Evaluate the performance (Pearson correlation) of experts and individual metrics on each one of the tasks using the test sets;\n5. Append all training sets with the eleven experts\u2019 outputs as columns and train a CatBoost classifier (Prokhorenkova et al., 2018) to predict from each dataset each point has come. Because the test set is balanced, we weigh the loss to guarantee that each class has the same importance. The average confusion matrix (across Monte Carlo repetitions) can be seen in Figure 3;\n6. To evaluate Model Fuser we do the following for each task (pair dataset/dimension): using the outputs from the eleven experts and for each domain, we classify each text in the test set using the trained CatBoost classifier according to their dataset membership; according to the empirical distribution of classes induced by the classifier, we compute a weighted average of the correlations achieved by experts in that specific task. For example, if task=\"summeval_coherence\" and the classifier outputs the class \"summeval\" 90% of the times and the class \"newsroom\" 10% of the times, the Model Fuser score on that task will be .9 \u00d7 \u03c1SE + .1 \u00d7 \u03c1NR, where \u03c1SE is the SummEval\u2019s expert score and \u03c1NR is the Newsroom\u2019s expert score in that specific task;\nIn Table 6, we can see the full set of results. The row \u201cavg\" computes the average of all available methods, while \u201cavg_con\" only takes into account the dimension \u201cconsistency\" (factuality), and\n\u201cavg_no_con\" takes into account all dimensions except \u201cconsistency\". For the \u201coracle\" columns, we always select the correct expert, instead of running the classifier.\nU nderreview as a conference paperatIC L R 2024"
        },
        {
            "heading": "A.2 MORE DETAILS ON THE CIFAR EXPERIMENT",
            "text": "In this section, we provide more information about the CIFAR-100 experiment presented in the main paper. To construct features for training the fusing strategy in FoE (i.e., a classifier to directly predict labels), we use the softmax scores of the last layer for each expert. The detailed partitions used in our CIFAR-100 experiments are shown in Table 7. The overlapping of sub-classes among partitions is shown in Figure 4."
        },
        {
            "heading": "A.3 MORE DETAILS ON THE MMLU EXPERIMENT",
            "text": "In this section, we provide more information about the MMLU experiment presented in the main paper. To construct the feature to learn the fusing strategy, we use the input (without the answer choices) in the MMLU test as the prompt and generate output with maximum sequence length of 16.\nWe average across the sequence length dimension (prompt and generated tokens) of the last decoder block and use that as the feature for each expert.\nThe selected LLMs. The selected LLMs in our experiments are:\n\u2022 Aspik101/trurl-2-7b-pl-instruct_unload \u2022 Charlie911/vicuna-7b-v1.5-lora-mctaco \u2022 Fredithefish/RedPajama-INCITE-Chat-3B-Instruction-Tuning-with-GPT-4 \u2022 GOAT-AI/GOAT-7B-Community \u2022 TheTravellingEngineer/bloom-1b1-RLHF \u2022 ashercn97/manatee-7b \u2022 garage-bAInd/Platypus2-7B \u2022 golaxy/gogpt-7b-bloom \u2022 julianweng/Llama-2-7b-chat-orcah \u2022 lmsys/vicuna-7b-v1.3 \u2022 lmsys/vicuna-7b-v1.5-16k \u2022 medalpaca/medalpaca-7b \u2022 rombodawg/LosslessMegaCoder-llama2-7b-mini \u2022 togethercomputer/GPT-JT-6B-v0 \u2022 togethercomputer/GPT-JT-6B-v1\nThe complete category scores. The complete MMLU category scores for average experts, FoE, and the oracle expert are reported in Table 8 and Table 9.\nMMLU scores on all individual LLMs experimented in our experiments. The MMLU scores of individual experts used in our experiments are reported in Table 10."
        },
        {
            "heading": "A.4 MORE DETAILS ON THE SENTIMENT ANALYSIS EXPERIMENT",
            "text": "We select the models with their associated datasets available on Hugging Face for the sentiment analysis experiments. To train the fusing strategy, we use the input embedding (averaged across the context length dimension) as the feature. Similar to the summarization experiment, the true label indicates the source downstream task of the input text sequence/article."
        },
        {
            "heading": "The selected models.",
            "text": "\u2022 nickmuchi/finbert-tone-finetuned-fintwitter-classification\n\u2022 joheras/clasificador-poem-sentiment\n\u2022 FinanceInc/auditor_sentiment_finetuned\n\u2022 Kaludi/Reviews-Sentiment-Analysis\nThe selected sentiment analysis dataset. For the Twitter Financial News Sentiment and Auditor Sentiment datasets, there is no split between validation and test sets. We thus split 60% of the data as the validation set and 40% of the data as the test set.\n\u2022 zeroshot/twitter-financial-news-sentiment \u2022 poem_sentiment \u2022 Kaludi/data-reviews-sentiment-analysis \u2022 FinanceInc/auditor_sentiment\nDetails on the semantic label alignment. The goal of the semantic label alignment is to make sure all experimented models are aligned on the predicted labels. Our objective is to make the \u201c0\" for \u201cnegative\" (or equivalent) and \u201c1\" for \u201cpositive\" (or equivalent) for the adjusted and aligned labels. See Table 11."
        },
        {
            "heading": "B ADDITIONAL EXPERIMENTAL RESULTS",
            "text": ""
        },
        {
            "heading": "B.1 GENERALIZATION TO NEW DOMAINS WITH WEAK EXPERTS",
            "text": "One natural question regarding the proposed FoE method is what will happen when encountering a previously unseen domains. We note that for FoE to generalize, it needs to have access to experts that are capable of performing well on (some) samples from the new domain.\nTo evaluate generalization of FoE to new domains, we modify our MMLU experiment with weak experts. This setting is the most suitable to test generalization as the considered (weak) experts are essentially generalist models capable of handling MMLU categories beyond their strongest areas of expertise. We modified our MMLU experiment by explicitly removing a subset of MMLU categories when training the fuser. Specifically, we removed five categories (i.e., high school european history, business ethics, clinical knowledge, medical genetics, and high\nschool us history) out of 57 MMLU categories. Consequently, during the test phase, data from these categories represent unseen tasks or categories. This modification allows us to assess FoE\u2019s generalization capabilities to new data or tasks. In Table 12 we compare the performance of FoE on the aforementioned 5 MMLU categories when they are included and excluded when training the fuser. We see that excluding these categories results in only minor performance decrease, thus demonstrating that FoE is able to generalize to new domains. FoE noticeably improves upon the average expert performance and sometimes even approaches the Oracle expert accuracy."
        },
        {
            "heading": "B.2 ADDITIONAL RESULTS ON FRUGALFOE OF THE CIFAR-100 TASK",
            "text": "The results of FrugalFoE on the CIFAR-100 super-class classification task (using neural networks as the fuser model) are shown in Figure 5."
        },
        {
            "heading": "B.3 ADDITIONAL EXPERIMENT OF THE SENTIMENT ANALYSIS TASK",
            "text": "The complete results on FrugalFoE of our sentiment analysis task is reported in Table 13."
        },
        {
            "heading": "B.4 ADDITIONAL EXPERIMENT OF THE SUMMARIZATION TASK",
            "text": "The complete results on FrugalFoE of our summarization task is reported in Table 14."
        },
        {
            "heading": "C DETAILS ON THE HYPERPARAMETERS",
            "text": "Fuser training hyperparameters. For training the fusers in FoE we use the AdamW optimizer with an initial learning rate at 0.001 and weight decay at 10\u22124. We use a batch size of {64, 128} across various tasks in our experiments. We train the fuser until convergence in all our experiments, which usually takes 10\u2212 50 epochs. We also use the cosine annealing learning rate scheduler for all fuser training.\nFuser model hyperparameters. For using neural networks as fusers, we simply use a three-layer MLP as the neural network architecture with ReLU as the activation function. We use a Dropout layer with a dropout rate of 0.5 before the very last fully connected layer in the MLP. The hidden dimensions in our experiments range from 2048 to 8192."
        }
    ],
    "year": 2023
}