{
    "abstractText": "Centre for Medical Image Computing, UCL, London, United Kingdom Microsoft Research, Cambridge ABSTRACT Scientific discovery hinges on the effective integration of metadata, which refers to a set of conceptual operations such as determining what information is relevant for inquiry, and data, which encompasses physical operations such as observation and experimentation. This paper introduces the Causal Modelling Agent (CMA), a novel framework that synergizes the metadata-based reasoning capabilities of Large Language Models (LLMs) with the data-driven modelling of Deep Structural Causal Models (DSCMs) for the task of causal discovery. We evaluate the CMA\u2019s performance on a number of benchmarks, as well as on the real-world task of modelling the clinical and radiological phenotype of Alzheimer\u2019s Disease (AD). Our experimental results indicate that the CMA can outperform previous purely data-driven or metadata-driven approaches to causal discovery. In our real-world application, we use the CMA to derive new insights into the causal relationships among biomarkers of AD.",
    "authors": [
        {
            "affiliations": [],
            "name": "COVERY THROUGH"
        },
        {
            "affiliations": [],
            "name": "SYNERGISING METADATA"
        },
        {
            "affiliations": [],
            "name": "Ahmed Abdulaal"
        },
        {
            "affiliations": [],
            "name": "Adamos Hadjivasiliou"
        },
        {
            "affiliations": [],
            "name": "Nina Monta\u00f1a-Brown"
        },
        {
            "affiliations": [],
            "name": "Tiantian He"
        },
        {
            "affiliations": [],
            "name": "Ayodeji Ijishakin"
        },
        {
            "affiliations": [],
            "name": "Ivana Drobnjak"
        },
        {
            "affiliations": [],
            "name": "Daniel C. Castro"
        },
        {
            "affiliations": [],
            "name": "Daniel C. Alexander"
        }
    ],
    "id": "SP:d2d97fa361364880fb85ab9107f47f2c85d05548",
    "references": [
        {
            "authors": [
                "Herv\u00e9 Abdi",
                "Lynne J Williams"
            ],
            "title": "Tukey\u2019s honestly significant difference (HSD) test",
            "venue": "Encyclopedia of research design,",
            "year": 2010
        },
        {
            "authors": [
                "Eldar D Abraham",
                "Karel D\u2019Oosterlinck",
                "Amir Feder",
                "Yair Gat",
                "Atticus Geiger",
                "Christopher Potts",
                "Roi Reichart",
                "Zhengxuan Wu"
            ],
            "title": "CEBaB: Estimating the causal effects of real-world concepts on nlp model behavior",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Matthew Ashman",
                "Chao Ma",
                "Agrin Hilmkil",
                "Joel Jennings",
                "Cheng Zhang"
            ],
            "title": "Causal reasoning in the presence of latent confounders via neural ADMG learning",
            "venue": "arXiv preprint arXiv:2303.12703,",
            "year": 2023
        },
        {
            "authors": [
                "Brian B Avants",
                "Nick Tustison",
                "Gang Song"
            ],
            "title": "Advanced normalization tools (ANTS)",
            "venue": "Insight j,",
            "year": 2009
        },
        {
            "authors": [
                "Abhishek Ankur Balmik",
                "Subashchandrabose Chinnathambi"
            ],
            "title": "Methylation as a key regulator of tau aggregation and neuronal health in alzheimer\u2019s disease",
            "venue": "Cell Communication and Signaling,",
            "year": 2021
        },
        {
            "authors": [
                "Taiyu Ban",
                "Lyvzhou Chen",
                "Xiangyu Wang",
                "Huanhuan Chen"
            ],
            "title": "From query tools to causal architects: Harnessing large language models for advanced causal discovery from data",
            "venue": "arXiv preprint arXiv:2306.16902,",
            "year": 2023
        },
        {
            "authors": [
                "David Barber"
            ],
            "title": "Bayesian reasoning and machine learning",
            "year": 2012
        },
        {
            "authors": [
                "Susan C Bates",
                "Baylor Fox-Kemper",
                "Steven R Jayne",
                "William G Large",
                "Samantha Stevenson",
                "Stephen G Yeager"
            ],
            "title": "Mean biases, variability, and trends in air\u2013sea fluxes and sea surface temperature in the CCSM4",
            "venue": "Journal of climate,",
            "year": 2012
        },
        {
            "authors": [
                "Lukas Berglund",
                "Meg Tong",
                "Max Kaufmann",
                "Mikita Balesni",
                "Asa Cooper Stickland",
                "Tomasz Korbak",
                "Owain Evans"
            ],
            "title": "The reversal curse: LLMs trained on \u201cA is B\u201d fail to learn \u201cB is A",
            "venue": "arXiv preprint arXiv:2309.12288,",
            "year": 2023
        },
        {
            "authors": [
                "Axel Bouchon",
                "Cristina Hern\u00e1ndez-Munain",
                "Marina Cella",
                "Marco Colonna"
            ],
            "title": "A DAP12-mediated pathway regulates expression of CC chemokine receptor 7 and maturation of human dendritic cells",
            "venue": "The Journal of experimental medicine,",
            "year": 2001
        },
        {
            "authors": [
                "Rachel F Buckley",
                "Matthew R Scott",
                "Heidi IL Jacobs",
                "Aaron P Schultz",
                "Michael J Properzi",
                "Rebecca E Amariglio",
                "Timothy J Hohman",
                "Danielle V Mayblyum",
                "Zoe B Rubinstein",
                "Lyssa Manning"
            ],
            "title": "Sex mediates relationships between regional tau pathology and cognitive decline",
            "venue": "Annals of neurology,",
            "year": 2020
        },
        {
            "authors": [
                "Lingjiao Chen",
                "Matei Zaharia",
                "James Zou"
            ],
            "title": "How is chatgpt\u2019s behavior changing over time",
            "venue": "arXiv preprint arXiv:2307.09009,",
            "year": 2023
        },
        {
            "authors": [
                "Kristy Choi",
                "Chris Cundy",
                "Sanjari Srivastava",
                "Stefano Ermon"
            ],
            "title": "LMPriors: Pre-trained language models as task-specific priors",
            "venue": "arXiv preprint arXiv:2210.12530,",
            "year": 2022
        },
        {
            "authors": [
                "Saloni Dash",
                "Vineeth N Balasubramanian",
                "Amit Sharma"
            ],
            "title": "Evaluating and mitigating bias in image classifiers: A causal perspective using counterfactuals",
            "venue": "In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision,",
            "year": 2022
        },
        {
            "authors": [
                "Laurent Dinh",
                "Jascha Sohl-Dickstein",
                "Samy Bengio"
            ],
            "title": "Density estimation using Real NVP",
            "venue": "5th International Conference on Learning Representations, ICLR 2017 - Conference Track Proceedings,",
            "year": 2017
        },
        {
            "authors": [
                "Nicola K. Dinsdale",
                "Emma Bluemke",
                "Stephen M. Smith",
                "Zobair Arya",
                "Diego Vidaurre",
                "Mark Jenkinson",
                "Ana I.L"
            ],
            "title": "Namburete. Learning patterns of the ageing brain in MRI using deep convolutional networks",
            "venue": "NeuroImage, 224:117401,",
            "year": 2021
        },
        {
            "authors": [
                "Conor Durkan",
                "Artur Bekasov",
                "Iain Murray",
                "George Papamakarios"
            ],
            "title": "Neural spline flows",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "AR Esteves",
                "AM Palma",
                "R Gomes",
                "D Santos",
                "DF Silva",
                "SM Cardoso"
            ],
            "title": "Acetylation as a major determinant to microtubule-dependent autophagy: Relevance to alzheimer\u2019s and parkinson disease pathology",
            "venue": "Biochimica et Biophysica Acta (BBA)-Molecular Basis of Disease,",
            "year": 2008
        },
        {
            "authors": [
                "Michael Ewers",
                "Nicolai Franzmeier",
                "Marc Su\u00e1rez-Calvet",
                "Estrella Morenas-Rodriguez",
                "Miguel Angel Araque Caballero",
                "Gernot Kleinberger",
                "Laura Piccio",
                "Carlos Cruchaga",
                "Yuetiva Deming",
                "Martin Dichgans"
            ],
            "title": "Increased soluble trem2 in cerebrospinal fluid is associated with reduced cognitive and clinical decline in alzheimer\u2019s disease",
            "venue": "Science translational medicine,",
            "year": 2019
        },
        {
            "authors": [
                "Amir Feder",
                "Nadav Oved",
                "Uri Shalit",
                "Roi Reichart"
            ],
            "title": "Causalm: Causal model explanation through counterfactual language models",
            "venue": "Computational Linguistics,",
            "year": 2021
        },
        {
            "authors": [
                "Amir Feder",
                "Katherine A Keith",
                "Emaad Manzoor",
                "Reid Pryzant",
                "Dhanya Sridhar",
                "Zach WoodDoughty",
                "Jacob Eisenstein",
                "Justin Grimmer",
                "Roi Reichart",
                "Margaret E Roberts"
            ],
            "title": "Causal inference in natural language processing: Estimation, prediction, interpretation and beyond",
            "venue": "Transactions of the Association for Computational Linguistics,",
            "year": 2022
        },
        {
            "authors": [
                "Fabia Filipello",
                "Claire Goldsbury",
                "Shih Feng You",
                "Alberto Locca",
                "Celeste M Karch",
                "Laura Piccio"
            ],
            "title": "Soluble TREM2: Innocent bystander or active player in neurological diseases",
            "venue": "Neurobiology of disease,",
            "year": 2022
        },
        {
            "authors": [
                "Mehmet F\u0131rat",
                "Saniye Kuleli"
            ],
            "title": "What if GPT4 became autonomous: The Auto-GPT project and use cases",
            "venue": "Journal of Emerging Computer Technologies,",
            "year": 2023
        },
        {
            "authors": [
                "Katharina Flach",
                "Ellen Ramminger",
                "Isabel Hilbrich",
                "Annika Arsalan-Werner",
                "Franziska Albrecht",
                "Lydia Herrmann",
                "Michel Goedert",
                "Thomas Arendt",
                "Max Holzer"
            ],
            "title": "Axotrophin/march7 acts as an e3 ubiquitin ligase and ubiquitinates tau protein in vitro impairing microtubule binding",
            "venue": "Biochimica et Biophysica Acta (BBA)-Molecular Basis of Disease,",
            "year": 2014
        },
        {
            "authors": [
                "Morten Frydenberg"
            ],
            "title": "The chain graph markov property",
            "venue": "Scandinavian Journal of Statistics, pp",
            "year": 1990
        },
        {
            "authors": [
                "Dan Geiger",
                "Judea Pearl"
            ],
            "title": "On the Logic of Causal Models",
            "venue": "Machine Intelligence and Pattern Recognition, 9(C):3\u201314,",
            "year": 1990
        },
        {
            "authors": [
                "Clark Glymour",
                "Kun Zhang",
                "Peter Spirtes"
            ],
            "title": "Review of causal discovery methods based on graphical models",
            "venue": "Frontiers in genetics,",
            "year": 2019
        },
        {
            "authors": [
                "Peter Godfrey-Smith"
            ],
            "title": "Theory and reality: An introduction to the philosophy of science",
            "year": 2009
        },
        {
            "authors": [
                "Walter Gulisano",
                "Daniele Maugeri",
                "Marian A Baltrons",
                "Mauro F\u00e0",
                "Arianna Amato",
                "Agostino Palmeri",
                "Luciano D\u2019Adamio",
                "Claudio Grassi",
                "DP Devanand",
                "Lawrence S Honig"
            ],
            "title": "Role of amyloid-\u03b2 and tau proteins in Alzheimer\u2019s disease: confuting the amyloid cascade",
            "venue": "Journal of Alzheimer\u2019s Disease,",
            "year": 2018
        },
        {
            "authors": [
                "Norwood Russell Hanson"
            ],
            "title": "Patterns of discovery: An inquiry into the conceptual foundations of science",
            "venue": "CUP Archive,",
            "year": 1965
        },
        {
            "authors": [
                "John A Hardy",
                "Gerald A Higgins"
            ],
            "title": "Alzheimer\u2019s disease: the amyloid cascade hypothesis",
            "year": 1992
        },
        {
            "authors": [
                "Karl Herrup"
            ],
            "title": "The case for rejecting the amyloid cascade hypothesis",
            "venue": "Nature neuroscience,",
            "year": 2015
        },
        {
            "authors": [
                "Yiyi Huang",
                "Matth\u00e4us Kleindessner",
                "Alexey Munishkin",
                "Debvrat Varshney",
                "Pei Guo",
                "Jianwu Wang"
            ],
            "title": "Benchmarking of data-driven causality discovery approaches in the interactions of arctic sea ice and atmosphere",
            "venue": "Frontiers in big Data,",
            "year": 2021
        },
        {
            "authors": [
                "Yiyi Huang",
                "Matth\u00e4us Kleindessner",
                "Alexey Munishkin",
                "Debvrat Varshney",
                "Pei Guo",
                "Jianwu Wang"
            ],
            "title": "Benchmarking of data-driven causality discovery approaches in the interactions of arctic sea ice and atmosphere",
            "venue": "Frontiers in big Data,",
            "year": 2021
        },
        {
            "authors": [
                "Hyekyoung Hwang",
                "Eunbyung Park",
                "Jitae Shin"
            ],
            "title": "Chain graph explanation of neural network based on feature-level class confusion",
            "venue": "Applied Sciences,",
            "year": 2022
        },
        {
            "authors": [
                "Zara A Ioannides",
                "Peter A Csurhes",
                "Andrew Swayne",
                "Philippe Foubert",
                "Blake T Aftab",
                "Michael P Pender"
            ],
            "title": "Correlations between macrophage/microglial activation marker strem-2 and measures of t-cell activation, neuroaxonal damage and disease severity in multiple sclerosis",
            "venue": "Multiple Sclerosis Journal\u2013Experimental, Translational and Clinical,",
            "year": 1977
        },
        {
            "authors": [
                "Fabian Isensee",
                "Marianne Schell",
                "Irada Pflueger",
                "Gianluca Brugnara",
                "David Bonekamp",
                "Ulf Neuberger",
                "Antje Wick",
                "Heinz Peter Schlemmer",
                "Sabine Heiland",
                "Wolfgang Wick",
                "Martin Bendszus",
                "Klaus H. Maier-Hein",
                "Philipp Kickingereder"
            ],
            "title": "Automated brain extraction of multisequence MRI using artificial neural networks",
            "venue": "Human Brain Mapping, 40(17):4952\u20134964,",
            "year": 2019
        },
        {
            "authors": [
                "Arthur B Kahn"
            ],
            "title": "Topological sorting of large networks",
            "venue": "Communications of the ACM,",
            "year": 1962
        },
        {
            "authors": [
                "Marcus Kaiser",
                "Maksim Sipos"
            ],
            "title": "Unsuitability of NOTEARS for causal graph discovery when dealing with dimensional quantities",
            "venue": "Neural Processing Letters,",
            "year": 2022
        },
        {
            "authors": [
                "Ilyes Khemakhem",
                "Ricardo Monti",
                "Robert Leech",
                "Aapo Hyvarinen"
            ],
            "title": "Causal autoregressive flows",
            "venue": "In International conference on artificial intelligence and statistics,",
            "year": 2021
        },
        {
            "authors": [
                "Emre K\u0131c\u0131man",
                "Robert Ness",
                "Amit Sharma",
                "Chenhao Tan"
            ],
            "title": "Causal reasoning and large language models: Opening a new frontier for causality",
            "venue": "arXiv preprint arXiv:2305.00050,",
            "year": 2023
        },
        {
            "authors": [
                "Diederik P. Kingma",
                "Jimmy Lei Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "In 3rd International Conference on Learning Representations, ICLR 2015 - Conference Track Proceedings,",
            "year": 2015
        },
        {
            "authors": [
                "Gernot Kleinberger",
                "Yoshinori Yamanishi",
                "Marc Su\u00e1rez-Calvet",
                "Eva Czirr",
                "Ebba Lohmann",
                "Elise Cuyvers",
                "Hanne Struyfs",
                "Nadine Pettkus",
                "Andrea Wenninger-Weinzierl",
                "Fargol Mazaheri"
            ],
            "title": "TREM2 mutations implicated in neurodegeneration impair cell surface transport and phagocytosis",
            "venue": "Science translational medicine,",
            "year": 2014
        },
        {
            "authors": [
                "Anne-Brita Knapskog",
                "Kristi Henjum",
                "Ane-Victoria Idland",
                "Rannveig Sakshaug Eldholm",
                "Karin Persson",
                "Ingvild Saltvedt",
                "Leiv Otto Watne",
                "Knut Engedal",
                "Lars NG Nilsson"
            ],
            "title": "Cerebrospinal fluid strem2 in alzheimer\u2019s disease: comparisons between clinical presentation and at classification",
            "venue": "Scientific Reports,",
            "year": 2020
        },
        {
            "authors": [
                "Andrew Kyle Lampinen",
                "Stephanie CY Chan",
                "Ishita Dasgupta",
                "Andrew J Nam",
                "Jane X Wang"
            ],
            "title": "Passive learning of active causal strategies in agents and language models",
            "venue": "arXiv preprint arXiv:2305.16183,",
            "year": 2023
        },
        {
            "authors": [
                "Steffen L Lauritzen"
            ],
            "title": "Graphical models, volume 17",
            "year": 1996
        },
        {
            "authors": [
                "Moeketsi Letseka",
                "Davison Zireva"
            ],
            "title": "Thinking: Lessons from John Dewey\u2019s How We Think",
            "venue": "Academic Journal of Interdisciplinary Studies,",
            "year": 2013
        },
        {
            "authors": [
                "Patrick Lewis",
                "Ethan Perez",
                "Aleksandra Piktus",
                "Fabio Petroni",
                "Vladimir Karpukhin",
                "Naman Goyal",
                "Heinrich K\u00fcttler",
                "Mike Lewis",
                "Wen-tau Yih",
                "Tim Rockt\u00e4schel"
            ],
            "title": "Retrieval-augmented generation for knowledge-intensive nlp tasks",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Yujia Li",
                "Jiong Shi",
                "S Kevin Zhou"
            ],
            "title": "Causal image synthesis of brain",
            "venue": "MR in 3D. arXiv preprint arXiv:2303.14349,",
            "year": 2023
        },
        {
            "authors": [
                "Zongyu Li",
                "Zhenfeng Zhu",
                "Xiaobo Guo",
                "Shuai Zheng",
                "Zhenyu Guo",
                "Siwei Qiang",
                "Yao Zhao"
            ],
            "title": "A survey of deep causal models and their industrial applications",
            "venue": "arxiv,",
            "year": 2023
        },
        {
            "authors": [
                "Ruibo Liu",
                "Jason Wei",
                "Shixiang Shane Gu",
                "Te-Yen Wu",
                "Soroush Vosoughi",
                "Claire Cui",
                "Denny Zhou",
                "Andrew M Dai"
            ],
            "title": "Mind\u2019s eye: Grounded language model reasoning through simulation",
            "venue": "arXiv preprint arXiv:2210.05359,",
            "year": 2022
        },
        {
            "authors": [
                "Stephanie Long",
                "Alexandre Pich\u00e9",
                "Valentina Zantedeschi",
                "Tibor Schuster",
                "Alexandre Drouin"
            ],
            "title": "Causal discovery with language models as imperfect experts",
            "venue": "arXiv preprint arXiv:2307.02390,",
            "year": 2023
        },
        {
            "authors": [
                "Stephanie Long",
                "Tibor Schuster",
                "Alexandre Pich\u00e9"
            ],
            "title": "Can large language models build causal graphs",
            "venue": "arXiv preprint arXiv:2303.05279,",
            "year": 2023
        },
        {
            "authors": [
                "You Lu",
                "Bert Huang"
            ],
            "title": "Structured Output Learning with Conditional Generative Flows",
            "venue": "AAAI 2020 - 34th AAAI Conference on Artificial Intelligence, pp. 5005\u20135012,",
            "year": 2019
        },
        {
            "authors": [
                "Alessandro Magrini",
                "Stefano Di Blasi",
                "Federico Mattia Stefanini"
            ],
            "title": "A conditional linear Gaussian network to assess the impact of several agronomic settings on the quality of Tuscan Sangiovese grapes",
            "venue": "Biometrical Letters,",
            "year": 2017
        },
        {
            "authors": [
                "Freddie M\u00e1rquez",
                "Michael A Yassa"
            ],
            "title": "Neuroimaging biomarkers for alzheimer\u2019s disease",
            "venue": "Molecular neurodegeneration,",
            "year": 2019
        },
        {
            "authors": [
                "Varun Nair",
                "Elliot Schumacher",
                "Geoffrey Tso",
                "Anitha Kannan"
            ],
            "title": "DERA: enhancing large language model completions with dialog-enabled resolving agents",
            "venue": "arXiv preprint arXiv:2303.17071,",
            "year": 2023
        },
        {
            "authors": [
                "Meike Nauta",
                "Doina Bucur",
                "Christin Seifert"
            ],
            "title": "Causal discovery with attention-based convolutional neural networks",
            "venue": "Machine Learning and Knowledge Extraction,",
            "year": 2019
        },
        {
            "authors": [
                "Rik Ossenkoppele",
                "Gil D Rabinovici",
                "Ruben Smith",
                "Hanna Cho",
                "Michael Sch\u00f6ll",
                "Olof Strandberg",
                "Sebastian Palmqvist",
                "Niklas Mattsson",
                "Shorena Janelidze",
                "Alexander Santillo"
            ],
            "title": "Discriminative accuracy of [18f] flortaucipir positron emission tomography for alzheimer disease",
            "venue": "vs other neurodegenerative disorders. Jama,",
            "year": 2018
        },
        {
            "authors": [
                "Joon Sung Park",
                "Joseph C O\u2019Brien",
                "Carrie J Cai",
                "Meredith Ringel Morris",
                "Percy Liang",
                "Michael S Bernstein"
            ],
            "title": "Generative agents: Interactive simulacra of human behavior",
            "venue": "arXiv preprint arXiv:2304.03442,",
            "year": 2023
        },
        {
            "authors": [
                "Nick Pawlowski",
                "Daniel C. Castro",
                "Ben Glocker"
            ],
            "title": "Deep structural causal models for tractable counterfactual inference",
            "venue": "In Advances in Neural Information Processing Systems",
            "year": 2020
        },
        {
            "authors": [
                "Judea Pearl"
            ],
            "title": "Causality: Models, reasoning, and inference",
            "year": 2011
        },
        {
            "authors": [
                "Judea Pearl"
            ],
            "title": "Theoretical impediments to machine learning with seven sparks from the causal revolution",
            "venue": "arXiv preprint arXiv:1801.04016,",
            "year": 2018
        },
        {
            "authors": [
                "Jonas Peters",
                "Dominik Janzing",
                "Bernhard Sch\u00f6lkopf"
            ],
            "title": "Elements of causal inference: foundations and learning algorithms",
            "year": 2017
        },
        {
            "authors": [
                "R.C. Petersen",
                "P.S. Aisen",
                "L.A. Beckett",
                "M.C. Donohue",
                "A.C. Gamst",
                "D.J. Harvey",
                "C.R. Jack",
                "W.J. Jagust",
                "L.M. Shaw",
                "A.W. Toga",
                "J.Q. Trojanowski",
                "M.W. Weiner"
            ],
            "title": "Alzheimer\u2019s Disease Neuroimaging Initiative (ADNI): Clinical characterization",
            "venue": "ISSN 1526632X",
            "year": 2010
        },
        {
            "authors": [
                "Jacob C. Reinhold",
                "Aaron Carass",
                "Jerry L. Prince"
            ],
            "title": "A structural causal model for MR images of multiple sclerosis",
            "venue": "Medical Image Computing and Computer Assisted Intervention \u2013 MICCAI 2021,",
            "year": 2021
        },
        {
            "authors": [
                "Danilo Rezende",
                "Shakir Mohamed"
            ],
            "title": "Variational inference with normalizing flows",
            "venue": "In International conference on machine learning,",
            "year": 2015
        },
        {
            "authors": [
                "Thomas S Richardson",
                "James M Robins"
            ],
            "title": "Single world intervention graphs (SWIGs): A unification of the counterfactual and graphical approaches to causality. Center for the Statistics and the Social Sciences",
            "year": 2013
        },
        {
            "authors": [
                "Karen Sachs",
                "Omar Perez",
                "Dana Pe\u2019er",
                "Douglas A Lauffenburger",
                "Garry P Nolan"
            ],
            "title": "Causal protein-signaling networks derived from multiparameter single-cell data",
            "year": 2005
        },
        {
            "authors": [
                "Pritish Sahu",
                "Michael Cogswell",
                "Yunye Gong",
                "Ajay Divakaran"
            ],
            "title": "Unpacking large language models with conceptual consistency",
            "venue": "arXiv preprint arXiv:2209.15093,",
            "year": 2022
        },
        {
            "authors": [
                "Pedro Sanchez",
                "Sotirios A. Tsaftaris"
            ],
            "title": "Diffusion causal models for counterfactual estimation",
            "venue": "In Proceedings of the First Conference on Causal Learning and Reasoning,",
            "year": 2022
        },
        {
            "authors": [
                "Yuesong Shen",
                "Daniel Cremers"
            ],
            "title": "A chain graph interpretation of real-world neural networks",
            "venue": "arXiv preprint arXiv:2006.16856,",
            "year": 2020
        },
        {
            "authors": [
                "Shohei Shimizu",
                "Patrik O Hoyer",
                "Aapo Hyv\u00e4rinen",
                "Antti Kerminen",
                "Michael Jordan"
            ],
            "title": "A linear non-Gaussian acyclic model for causal discovery",
            "venue": "Journal of Machine Learning Research,",
            "year": 2006
        },
        {
            "authors": [
                "Noah Shinn",
                "Beck Labash",
                "Ashwin Gopinath"
            ],
            "title": "Reflexion: an autonomous agent with dynamic memory and self-reflection",
            "venue": "arXiv preprint arXiv:2303.11366,",
            "year": 2023
        },
        {
            "authors": [
                "Kihyuk Sohn",
                "Honglak Lee",
                "Xinchen Yan"
            ],
            "title": "Learning structured output representation using deep conditional generative models",
            "venue": "Advances in neural information processing systems,",
            "year": 2015
        },
        {
            "authors": [
                "Peter Spirtes",
                "Clark N Glymour",
                "Richard Scheines"
            ],
            "title": "Causation, prediction, and search",
            "venue": "MIT press,",
            "year": 2000
        },
        {
            "authors": [
                "Marc Su\u00e1rez-Calvet",
                "Estrella Morenas-Rodr\u0131\u0301guez",
                "Gernot Kleinberger",
                "Kai Schlepckow",
                "Miguel \u00c1ngel Araque Caballero",
                "Nicolai Franzmeier",
                "Anja Capell",
                "Katrin Fellerer",
                "Brigitte Nuscher",
                "Erden Eren"
            ],
            "title": "Early increase of CSF sTREM2 in Alzheimer\u2019s disease is associated with tau related-neurodegeneration but not with amyloid-\u03b2 pathology",
            "venue": "Molecular neurodegeneration,",
            "year": 2019
        },
        {
            "authors": [
                "Lucas Theis",
                "A\u00e4ron Van Den Oord",
                "Matthias Bethge"
            ],
            "title": "A note on the evaluation of generative models",
            "venue": "4th International Conference on Learning Representations, ICLR 2016 - Conference Track Proceedings,",
            "year": 2015
        },
        {
            "authors": [
                "Brian L Trippe",
                "Richard E Turner"
            ],
            "title": "Conditional density estimation with bayesian normalising flows",
            "venue": "arXiv preprint arXiv:1802.04908,",
            "year": 2018
        },
        {
            "authors": [
                "Sean Trott",
                "Cameron Jones",
                "Tyler Chang",
                "James Michaelov",
                "Benjamin Bergen"
            ],
            "title": "Do large language models know what humans know",
            "venue": "Cognitive Science,",
            "year": 2023
        },
        {
            "authors": [
                "Ruibo Tu",
                "Kun Zhang",
                "Bo Bertilson",
                "Hedvig Kjellstrom",
                "Cheng Zhang"
            ],
            "title": "Neuropathic pain diagnosis simulator for causal discovery algorithm evaluation",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Ruibo Tu",
                "Chao Ma",
                "Cheng Zhang"
            ],
            "title": "Causal-discovery performance of chatgpt in the context of neuropathic pain diagnosis",
            "venue": "arXiv preprint arXiv:2301.13819,",
            "year": 2023
        },
        {
            "authors": [
                "Nicholas J. Tustison",
                "Brian B. Avants",
                "Philip A. Cook",
                "Yuanjie Zheng",
                "Alexander Egan",
                "Paul A. Yushkevich",
                "James C. Gee"
            ],
            "title": "N4ITK: improved N3 bias correction",
            "venue": "IEEE transactions on medical imaging,",
            "year": 2010
        },
        {
            "authors": [
                "Tinu Varghese",
                "R Sheelakumari",
                "Jija S James",
                "Pavagada S Mathuranath"
            ],
            "title": "A review of neuroimaging biomarkers of alzheimer\u2019s disease",
            "venue": "Neurology Asia,",
            "year": 2013
        },
        {
            "authors": [
                "Victor Veitch",
                "Alexander D\u2019Amour",
                "Steve Yadlowsky",
                "Jacob Eisenstein"
            ],
            "title": "Counterfactual invariance to spurious correlations in text classification",
            "venue": "Advances in neural information processing systems,",
            "year": 2021
        },
        {
            "authors": [
                "Guanzhi Wang",
                "Yuqi Xie",
                "Yunfan Jiang",
                "Ajay Mandlekar",
                "Chaowei Xiao",
                "Yuke Zhu",
                "Linxi Fan",
                "Anima Anandkumar"
            ],
            "title": "Voyager: An open-ended embodied agent with large language models",
            "venue": "arXiv preprint arXiv:2305.16291,",
            "year": 2023
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "Fei Xia",
                "Ed Chi",
                "Quoc V Le",
                "Denny Zhou"
            ],
            "title": "Chain-of-thought prompting elicits reasoning in large language models",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Moritz Willig",
                "Matej Ze\u010devi\u0107",
                "Devendra Singh Dhami",
                "Kristian Kersting"
            ],
            "title": "Can foundation models talk causality",
            "venue": "arXiv preprint arXiv:2206.10591,",
            "year": 2022
        },
        {
            "authors": [
                "Edward N Wilson",
                "Michelle S Swarovski",
                "Patricia Linortner",
                "Marian Shahid",
                "Abigail J Zuckerman",
                "Qian Wang",
                "Divya Channappa",
                "Paras S Minhas",
                "Siddhita D Mhatre",
                "Edward D Plowey"
            ],
            "title": "Soluble trem2 is elevated in parkinson\u2019s disease subgroups with increased csf",
            "year": 2020
        },
        {
            "authors": [
                "Christina Winkler",
                "Daniel Worrall",
                "Emiel Hoogeboom",
                "Max Welling"
            ],
            "title": "Learning likelihoods with conditional normalizing flows",
            "venue": "arXiv preprint arXiv:1912.00042,",
            "year": 2019
        },
        {
            "authors": [
                "Julie K Wisch",
                "Karin L Meeker",
                "Brian A Gordon",
                "Shaney Flores",
                "Aylin Dincer",
                "Elizabeth A Grant",
                "Tammie L Benzinger",
                "John C Morris",
                "Beau M Ances"
            ],
            "title": "Sex-related differences in tau positron emission tomography (pet) and the effects of hormone therapy (ht)",
            "venue": "Alzheimer disease and associated disorders,",
            "year": 2021
        },
        {
            "authors": [
                "Steve Yadlowsky",
                "Lyric Doshi",
                "Nilesh Tripuraneni"
            ],
            "title": "Pretraining data mixtures enable narrow model selection capabilities in transformer models",
            "venue": "arXiv preprint arXiv:2311.00871,",
            "year": 2023
        },
        {
            "authors": [
                "Yan Yan",
                "Xinming Wang",
                "Dale Chaput",
                "Min-Kyoo Shin",
                "Yeojung Koh",
                "Li Gan",
                "Andrew A Pieper",
                "Jung-AA Woo",
                "David E Kang"
            ],
            "title": "X-linked ubiquitin-specific peptidase 11 increases tauopathy vulnerability in women",
            "year": 2022
        },
        {
            "authors": [
                "Shunyu Yao",
                "Jeffrey Zhao",
                "Dian Yu",
                "Nan Du",
                "Izhak Shafran",
                "Karthik Narasimhan",
                "Yuan Cao"
            ],
            "title": "React: Synergizing reasoning and acting in language models",
            "venue": "arXiv preprint arXiv:2210.03629,",
            "year": 2022
        },
        {
            "authors": [
                "Yang You",
                "Jing Li",
                "Sashank Reddi",
                "Jonathan Hseu",
                "Sanjiv Kumar",
                "Srinadh Bhojanapalli",
                "Xiaodan Song",
                "James Demmel",
                "Kurt Keutzer",
                "Cho-Jui Hsieh"
            ],
            "title": "Large batch optimization for deep learning: Training BERT",
            "venue": "minutes. arXiv preprint arXiv:1904.00962,",
            "year": 2019
        },
        {
            "authors": [
                "Yue Yu",
                "Jie Chen",
                "Tian Gao",
                "Mo Yu"
            ],
            "title": "DAG-GNN: DAG structure learning with graph neural networks",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Matej Ze\u010devi\u0107",
                "Moritz Willig",
                "Devendra Singh Dhami",
                "Kristian Kersting"
            ],
            "title": "Causal parrots: Large language models may talk causality but are not causal",
            "venue": "Transactions in Machine Learning Research (TMLR),",
            "year": 2023
        },
        {
            "authors": [
                "Bin Zhang",
                "Chris Gaiteri",
                "Liviu-Gabriel Bodea",
                "Zhi Wang",
                "Joshua McElwee",
                "Alexei A Podtelezhnikov",
                "Chunsheng Zhang",
                "Tao Xie",
                "Linh Tran",
                "Radu Dobrin"
            ],
            "title": "Integrated systems approach identifies genetic nodes and networks in late-onset Alzheimer\u2019s disease",
            "year": 2013
        },
        {
            "authors": [
                "Kun Zhang",
                "Lai-Wan Chan"
            ],
            "title": "Extensions of ICA for causality discovery in the Hong Kong stock market",
            "venue": "In International Conference on Neural Information Processing,",
            "year": 2006
        },
        {
            "authors": [
                "Kun Zhang",
                "Aapo Hyv\u00e4rinen"
            ],
            "title": "Causality discovery with additive disturbances: An informationtheoretical perspective",
            "venue": "ECML PKDD",
            "year": 2009
        },
        {
            "authors": [
                "Aonan Zhao",
                "Yang Jiao",
                "Guanyu Ye",
                "Wenyan Kang",
                "Lan Tan",
                "Yuanyuan Li",
                "Yulei Deng",
                "Jun Liu"
            ],
            "title": "Soluble TREM2 levels associate with conversion from mild cognitive impairment to Alzheimer\u2019s disease",
            "venue": "The Journal of Clinical Investigation,",
            "year": 2022
        },
        {
            "authors": [
                "Xun Zheng",
                "Bryon Aragam",
                "Pradeep K Ravikumar",
                "Eric P Xing"
            ],
            "title": "DAGs with NO TEARS: Continuous optimization for structure learning",
            "venue": "Advances in neural information processing systems,",
            "year": 2018
        },
        {
            "authors": [
                "Lyu Zhiheng",
                "Zhijing Jin",
                "Rada Mihalcea",
                "Mrinmaya Sachan",
                "Bernhard Sch\u00f6lkopf"
            ],
            "title": "Can large language models distinguish cause from effect",
            "venue": "In UAI 2022 Workshop on Causal Representation Learning,",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Scientific discovery is the output of successful scientific inquiry, and its objects include events, causes, processes, and hypotheses. Arguably, the main mechanism of scientific reasoning is the generation of novel hypotheses that align with or explain observed datasets, or that allow for the derivation of testable consequences (Schickore, 2014). The \u2018pragmatic logics of discovery\u2019 refer to a set of reasoning strategies which are used in knowledge generation (Letseka & Zireva, 2013), and include both mental operations such as determination of relevant information for a given inquiry, and physical operations such as observation and experimentation (Schickore, 2014; Schiller, 1917). We refer to the former operations as \u2018metadata\u2019 and the latter as \u2018data\u2019. Both metadata and data are foundational to producing, communicating, and validating conjectures, and are important aspects of several theories of the scientific method (Hanson, 1965; Godfrey-Smith, 2009). Whilst we can represent conjectures or hypotheses as Directed Acyclic Graphs (DAGs) in several fields (Spirtes et al., 2000; Sachs et al., 2005; Zhang et al., 2013), we are often faced with the challenging problem of inferring causal structure from its empirical implications, which is known as the causal discovery problem (Peters et al., 2017).\nThere are a number of algorithms which attempt to solve the causal discovery problem by identifying the correct DAG given a dataset (Zheng et al., 2018; Yu et al., 2019; Nauta et al., 2019). However, in attempting to identify the generative process by leveraging asymmetries in the numerical data alone, even state-of-the-art causal discovery algorithms can be ineffective on real-world datasets (Tu et al., 2019; Huang et al., 2021b; Kaiser & Sipos, 2022), and can struggle in the setting where the data consists of as few as five synthetic variables, where each pair of variables can at most have a single confounder (Ashman et al., 2023).\nMore recently, it was demonstrated that Large Language Models (LLMs) establish new state-of-theart performance on multiple causal benchmarks including counterfactual reasoning, actual causality, and causal discovery (K\u0131c\u0131man et al., 2023; Lampinen et al., 2023). It is hypothesized that LLMs are able to capture domain knowledge (as encoded in natural language), which can then be translated into causal graphs or used to identify background causal context. In essence, this describes metadatabased reasoning; a task previously assumed to be restricted to humans (Sahu et al., 2022; Trott et al., 2023).\n\u2217rmapabd@ucl.ac.uk\nCoextensive to the causal discovery problem, causal modelling has seen numerous innovations in creating increasingly flexible models capable of causal reasoning on complex or even multi-modal data. For example, recent advances in probabilistic generative modelling have led to the emergence of Deep Structural Causal Models (DSCMs) (Pawlowski et al., 2020; Khemakhem et al., 2021; Sanchez & Tsaftaris, 2022; Dash et al., 2022), which combine modular Deep Learning (DL) elements with Structural Causal Models (SCMs). DSCMs can perform interventional and counterfactual queries in high-dimensional data settings, including in the imaging space (Pawlowski et al., 2020). However, they are limited in that they require the causal relationships between the variables (i.e., the causal graph) to be known a priori, which is rarely the case for real-world problems. Another limitation is the assumption of no unmeasured confounding, which requires yet more flexible types of graphical models (such as chain graph models) to account for hidden variables.\nIn this work, we investigate the effectiveness of combining LLM-based methods with a generalization of the DSCM framework by proposing the Causal Modelling Agent (CMA). The CMA combines the data-based modelling from DSCMs with the complementary and distinct metadata-based reasoning that LLMs utilise for the task of causal discovery, including for multi-modal datasets. Combining LLM-based methods with data-driven causal modelling approaches is beneficial in two main ways: 1) The LLMs can act as proxies of human knowledge and allow for the efficient exploration of causal graph space, and 2) data-driven causal methods can allow LLMs to formalize, communicate, and ground their reasoning (K\u0131c\u0131man et al., 2023). We assess this framework on a number of synthetic experiments and causal discovery benchmarks, before applying it to the real-world task of modelling the clinical and radiological phenotype of Alzheimer\u2019s disease (AD). Our main contributions are: 1) A unified framework for causal discovery which combines LLMs with deep probabilistic graphical models, and in which the LLMs act as priors, critics, and post-processors over the training of such models; 2) a generalised approach for the automatic construction of a DSCM from an experiment-description file, which allows for flexible experimentation; 3) a novel modelling strategy which parametrises chain graphs with deep learning elements to account for unmeasured confounding; 4) in our real-world application, we use the CMA to derive new insights into the causal relationships among biomarkers of AD."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Deep Structural Causal Models A multi-modal DSCM framework is proposed by Pawlowski et al. (2020). Whilst previous work has extended DSCMs and similar causal models to a number of application areas (Reinhold et al., 2021; Li et al., 2023a), previous DSCMs require that the causal graph be known a priori. As a general model class for counterfactual inference in multi-modal data, DSCMs do not directly attempt to solve the causal discovery problem and have not previously been used as part of a causal discovery algorithm. Additionally, there is no method with which to handle unmeasured confounding/associative relationships.\nLLMs and Causality Data-driven causal reasoning methods are well-established (Li et al., 2023b), but LLMs\u2019 application in this field is recent (Willig et al., 2022). Zec\u030cevic\u0301 et al. (2023) found that LLMs may recite causal knowledge instead of reason per se, with improved performance noted under Chain of Thought (CoT) prompting (Wei et al., 2022). Yadlowsky et al. (2023) showed LLMs\u2019 capacity to learn new tasks deteriorates with tasks increasingly divergent from their pretraining data. Contrarily, (Lampinen et al., 2023) observed that transformer-based agents can passively learn and apply generalizable causal strategies if allowed intervention during tests. In light of this nascent research field, a number of encouraging results have been demonstrated for the task of causal discovery. For example, Long et al. (2023b) demonstrated that LLMs can construct correct 3\u20134 variable graphs. Tu et al. (2023) considered causal discovery for a neuropathic pain dataset (Tu et al., 2019) using LLMs alone. Choi et al. (2022) demonstrated that LLMs can produce a prior hypothesis which improved the accuracy of data-based causal discovery algorithms, and Zhiheng et al. (2022) attempted to partially redefine the causal discovery problem such that it included relevant metadata. In these cases, LLMs were used in one stage of the causal modelling process, centring mostly around producing a causal graph alone or creating a causal graph which is used as a prior, for example in Ban et al. (2023). Long et al. (2023a) demonstrated that LLMs can reduce the size of a Markov equivalence class assuming an optimal output from a discovery algorithm; their work viewed LLMs as a post-processing step alone. K\u0131c\u0131man et al. (2023) investigated the graph discovery capabilities of LLMs over a broader set of real-world datasets; however, they did not assess combinations of LLMs with existing causal methods.\nAgents and Reasoning In our setting, an agent is an LLM which has access to one or more \u2018tools\u2019, such as an internet search engine. We briefly summarise recent agent frameworks for reasoning tasks and compare them with our agent, whose tool is a causal modelling framework. The ReAct framework (Yao et al., 2022) aimed to combine reasoning and acting capabilities in language models by using reasoning traces to design and update action plans. Reflexion (Shinn et al., 2023) built on ReAct by adding a \u2018self-reflection\u2019 mechanism to aid in inferring future actions, and AutoGPT (F\u0131rat & Kuleli, 2023) is a framework which decomposed overarching goals into sub-goals, carrying them out in a ReAct-like loop. DERA (Nair et al., 2023) attempted to enhance task completion using dialogue between two LLM agents, and Generative Agents (Park et al., 2023) stored agent experiences as memories which can be retrieved for future planning, however actions here were not executable. An extension of these ideas was Voyager (Wang et al., 2023), an LLM-based embodied agent which combined an automatic curriculum with an iterative prompting mechanism. These frameworks did not consider LLMs for the task of causal modelling, or indeed for inferring causal structure from its empirical implications.\nRelated topics Additional related topics include works by Feder et al. which introduced the statistical challenge of estimating causal effects with text (Feder et al., 2022), and investigated how language representation models can effectively learn a counterfactual representation for a given concept (Feder et al., 2021). Veitch et al. (2021) investigated counterfactual invariance in the context of text classification, and Abraham et al. (2022) introduced a benchmark for assessing concept-based explanation methods in natural language processing (NLP). Mind\u2019s Eye (Liu et al., 2022) attempted to ground language model reasoning through simulation. The MuJoCo physics engine was used to simulate outcomes for given physics questions, and the results were passed to the LLM to improve model reasoning ability. However, this process did not iterate nor consider a causal modelling task. To the best of our knowledge, this is the first work which combines LLM-based methods with a general deep causal modelling approach in which the LLM acts as a prior, critic and hypothesis engine, as well as post-processor. Additionally, in contrast to previous work (Long et al., 2023a; Ban et al., 2023), the CMA does not make any explicit assumptions about the initial graph structure, and is assessed on complex data scenarios beyond synthetic cases."
        },
        {
            "heading": "3 CAUSAL MODELLING AGENT (CMA)",
            "text": "We introduce the CMA framework (Figure 1), which unifies metadata- and data-based modelling paradigms to reason over a dataset and its associated data-generating process. Conceptually, the framework is defined as a function that takes as input a dataset D and associated metadata M , which represents the knowledge in the LLM\u2019s training corpus, and outputs a trained causal model and associated causal graph. The framework makes use of four concepts: 1) Hypothesis generation, 2) Model fitting, 3) Post-processing, and 4) Hypothesis amendment.\nHypothesis generation In hypothesis generation, we produce a causal graph by use of an LLM. The only required input is a set of variable names. In our case, during the first iteration (t = 0), the language model function LLM acts as a prior which proposes a representation of a causal graph Gt=0 of the current variables given metadata M and an empty graph G\u2205 (that is, a causal graph with no relationships between variables): Gt=0 = LLM(G\u2205;M). Graph Gt=0 can be encoded into an appropriate structured format Gst=0 = e(Gt=0), where e(\u00b7) is an encoding function and Gst=0 is a structured causal graph (Figure 1a).\nModel fitting At iteration t, the model fitting stage uses a data-driven approach to calculate a metric of fit Ft. As an example, the metric of fit could represent the log-likelihood of a dataset D under the model, given a structured graph Gst : Ft = logPGst (D). Indeed, this will be the metric of fit considered in this work. In our instantiation of a CMA, we use a modelling approach which requires that we account for the order in which modules are constructed to avoid errors and model misspecifications. Therefore, let K be a function which takes as input Gst and transmutes it into a (correctly specified) computational graph CGt, that is, CGt = K(Gst ). Then, the model fitting stage is defined: Ft := logPGst (D) = ME(D,CGt), (1) where ME is a \u2018Modelling Engine\u2019 function which is described in more detail (alongside the motivations and specific choices for the function K) in Appendix A.1.4 (Figure 1c). The model fit Ft is stored at this stage.\nPost-processing The post-processing stage is used to produce a \u2018memory\u2019 \u00b5. This stage requires as input the graph at the current iteration Gst , the graph at the previous iteration Gst\u22121, and their associated metrics of fit, Ft and Ft\u22121, respectively. The function LLM\u00b5 produces a memory at iteration t as \u00b5t = LLM\u00b5(Gst ,Gst\u22121, Ft, Ft\u22121). Memories encode information about changes to the causal graph, their implications, and their impact on model fit, and are stored at this stage.\nHypothesis amendment Hypothesis amendment is divided into two phases, which we call the \u2018global\u2019 and \u2018local\u2019 phases. In the global phase, the function LLM acts as a critic which proposes a set of amendments Aglob by taking as input the (structured) graph at t \u2212 1, metadata M , and an optional memory \u00b5t\u22121: Aglob = LLM(Gst\u22121, \u00b5t\u22121;M). This phase is expected to keep track of high-level relationships and account for the overarching structure of the DAG, and can be guided by previous amendments through the memory system (Figure 1a). It should be noted that the hypothesis generation stage can be seen as a special case of the global amendment phase, which takes an empty graph as input (with no memory).\nThe local phase considers pairwise comparisons between vertices u, v \u2208 V, u \u0338= v, where V is the set of vertices in the structured causal graph. This phase makes amendments Aloc as:\nAloc(u, v) = {\nADJUST(u, v;M) if edge exists between u and v ASSESS(u, v;M) otherwise,\n(2)\nwhere ADJUST(u, v;M) is an LLM-based function that outputs one of three actions: {keep, remove, reverse}, and ASSESS(u, v;M) is a similar function which outputs one of the following options: {No direct causality, u \u2192 v, v \u2192 u}. We can augment the metadata of the LLM to M \u222a C, where C is additional domain-specific context. This is known as a Retrieval Augmented Generation (RAG) pattern (Lewis et al., 2020) and can be used to encourage or enforce specific relationships according to domain expertise (Figure 1b). This is expected to introduce edges (under specific domain considerations), and otherwise to critique, validate, and/or prune the edge outputs from the global phase. Hypothesis amendment at iteration t, HAt, can be seen as a compositional set of amendments to the structured graph from iteration t\u2212 1:\nHAt(Gst\u22121, \u00b5t\u22121;M) = {Aglob} \u222a {Aloc(u, v) | \u2200u, v \u2208 V, u \u0338= v}. (3)\nThe hypothesis amendment, model fitting, and post-processing stages (Figure 1a-b, 1c, and 1d) iterate throughout a given experiment. The order of iteration is shown in Algorithm 1. In our implementation of the CMA, the model fitting stage creates a DSCM (Pawlowski et al., 2020), which enables reasoning over multi-modal datasets. We additionally propose a novel modelling approach, whereby we parameterise chain graph models with DL elements to represent associative but not necessarily causal links (for example, due to unmeasured confounding between the endogenous variables). Additional background on DSCMs and chain graphs, as well as further implementation details, can be found in Appendix A.1.\nAlgorithm 1 Iterative procedure of the CMA Framework\nRequire: Empty graph G\u2205, data D and metadata M Iteration 0\n1: Gs0 := e(LLM(G\u2205, \u2205;M)) \u25b7 Hypothesis Generation (no memory) 2: F0 := logPGs0 (D) \u25b7 Model Fitting\nIteration 1 1: Gs1 := e(HA1(G0, \u2205;M)) \u25b7 Hypothesis Amendment (no memory) 2: F1 := logPGs1 (D) \u25b7 Model Fitting 3: \u00b51 := LLM\u00b5(Gs1 ,Gs0 , F1, F0) \u25b7 Post-processing\nIteration \u2265 2 1: while \u00ac Early stopping criterion do \u25b7 Early stopping criterion 2: Gst := e(HAt(Gt\u22121, \u00b5t\u22121;M)) \u25b7 Hypothesis Amendment (with memory) 3: Ft := logPGst (D) \u25b7 Model Fitting 4: \u00b5t := LLM\u00b5(Gst ,Gst\u22121, Ft, Ft\u22121) \u25b7 Post-processing 5: end while"
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": "Our overarching hypothesis is that the CMA is effective at the task of causal discovery. The experimental logic is as follows: First, we assess the constituent elements of the CMA to ensure they are appropriate for reasoning over causal graphs. The synthetic neuropathic protein experiment assesses the data-driven module of the CMA (described in more detail in Appendix A.1.4). We hypothesise that correctly defined causal graphs lead to higher data likelihoods under the model (up to Markov equivalence; see Appendix A.2.3) than misspecified graphs using a simple DAG setting. Whilst we focus on the data-driven module of the CMA, we present additional results on LLM behavioural patterns in Appendix A.2.4.\nSecond, we wish to assess the CMA relative to other data- or metadata-driven approaches for causal discovery. We assess the performance of various methods on three causal discovery benchmarks where a ground-truth graph is known a priori: 1) The Arctic Sea Ice benchmark; 2) the Sangiovese benchmark; 3) an Alzheimer\u2019s Disease (AD) benchmark. We develop the AD benchmark in collaboration with 5 domain experts, with expertise in either clinical neurology (with a specialist interest in AD) or neuroradiology, with a specialist interest in neurodegenerative diseases. Additional details can be found in Appendix A.5.1.\nFinally, to assess the CMA in a real-world setting with multi-modal data, we apply it to data from the Alzheimer\u2019s Disease Neuroimaging Initiative (ADNI), where the task is to jointly model the clinical and radiological phenotype of the disease. In contrast to the AD benchmark, the ADNI dataset contains noisy real-world data representing the complex aetiological process of the disease, for which the ground-truth ultimately remains contentious (Herrup, 2015; Gulisano et al., 2018). Additionally, we demonstrate the CMA\u2019s ability to perform a causal discovery task in a multi-modal data setting (tabular data and images), which is not possible for any of the other baseline methods. We assess the implications of the causal graph proposed by the CMA and extract various insights relating to biomarkers of the disease."
        },
        {
            "heading": "4.1 SYNTHETIC NEUROPATHIC PROTEIN EXPERIMENT",
            "text": "Experimental setup To validate the modelling engine for the model fitting stage (Appendix A.1.4), we consider a simple DAG whose node description relates to the health sciences.\nPhosphorylated-tau (P-tau) and \u03b2-amyloid are proteins which are involved in neurodegenerative processes in the human brain, and both are affected by the ageing process. We consider the DAG \u2018Age\u2192AV45\u2019, \u2018Age\u2192 P-tau\u2019, and \u2018AV45\u2192 P-tau\u2019, as per the classical amyloid cascade hypothesis (Hardy & Higgins, 1992). Here, AV45 refers to \u03b2-amyloid levels as measured by the Florbetapir F 18 tracer following Positron Emission Tomography (PET) scanning (Varghese et al., 2013; Ma\u0301rquez & Yassa, 2019). Based on the synthetic distributions, we generate 10,000 independent and identically distributed (i.i.d.) data points from the resulting graph using ancestral sampling and test the hypothesis that a correctly defined DAG can appropriately induce the observational distribution. We train each model 100 times on the data points and calculate an average data likelihood. Additional implementation details as well as results relating to Markov equivalence in graphs and the CMA\u2019s behavioural patterns can be found in Appendix A.2.\nResults Figure 2 illustrates the ground-truth DAG alongside three modelling strategies. We assess the three strategies by comparing the data likelihoods under each model. The first DAG (DAG 1) is a perfect map of the data-generating process, which is defined as an equivalence of independence statements in the graph and the ground-truth distribution. In other words, the graph matches the data-generating process. This model has an average data likelihood of 4402.68, with a standard deviation of \u00b126.70, and this acts as the reference ground-truth likelihood. The second DAG (DAG 2) removes a causal link which exists in the data-generating process, thereby adding a graphical independence statement not present in the ground-truth distribution. The second model is, therefore, a deficient representation of the ground-truth distribution, and has an average data likelihood of 3857.53 \u00b1 18.98. The third DAG (DAG 3) adds a chain component between AV45 and P-tau, which represents an associative but not necessarily causal link. This model has an average data likelihood of 3757.55 \u00b1 60.80. The Tukey HSD test (Abdi & Williams, 2010) is used for pairwise model comparisons whilst ensuring the overall error rate for the family of comparisons is kept to the 0.05 threshold by control of the Familywise Error Rate (FWER). There is a statistically significant difference in likelihood between all three models, with the model representing the data-generating process having the highest data likelihood. Detailed results of the comparisons are shown in Table 1. As can be seen, the results validate our initial hypothesis: The model that aligns most closely with the true data-generating process produces the highest data likelihood. As expected, we find that this is only valid up to the Markov equivalence class of the ground-truth DAG (see Appendix A.2.3)."
        },
        {
            "heading": "4.2 BENCHMARKING EXPERIMENTS",
            "text": "Experimental setup We consider the task of recovering causal graphs on a number of causal discovery datasets. The Arctic sea ice dataset (Huang et al., 2021b) is from the field of atmospheric science and is an increasingly popular dataset for the task of full causal graph discovery (K\u0131c\u0131man et al., 2023). This dataset considers the relations of several geophysical variables to sea ice thick-\nness (12 nodes; 48 true edges). A full description can be found in Appendix A.3.1. The Sangiovese dataset is from the field of agricultural science and is a conditional linear Gaussian Bayesian Network from the popular bnlearn R package (Magrini et al., 2017). The DAG considers several variables that relate to grape quality in Sangiovese vineyards in Tuscany (15 nodes; 55 true edges). Additional information can be found in Appendix A.4.1. The Alzheimer\u2019s dataset is another conditional linear Gaussian Bayesian Network that we developed in collaboration with 5 domain experts. The synthetically generated dataset considers demographic, clinical, imaging-based biomarker, and cognitive assessment variables, and how they relate to the aetiology of AD (11 nodes; 19 true edges). We construct the ground-truth graph based on a consensus heuristic, for which full details can be found in Appendix A.5.1. We compare the CMA with commonly used, state-of-the-art, data-driven algorithms including NOTEARS (Zheng et al., 2018), DAG-GNN (Yu et al., 2019), and TCDF (for the Arctic sea ice dataset; more details in A.3.2) (Nauta et al., 2019). We also compare the CMA with metadata-based benchmarks as per K\u0131c\u0131man et al. (2023). The experimental setup for the metadata-based (LLM) benchmarks is given in Appendix A.3.2.\nResults The results of the benchmarking experiments can be seen in Table 2. We report the normalised Hamming distance (NHD), which is defined for a predicted graph G\u2032 and ground-truth G as 1/m2 \u2211m i,j=1 I[Gi,j \u0338= G\u2032i,j ]: The edges in one graph and not the other, normalised by the total number of possible edges. It should be noted that the NHD depends on the number of edges reported by a causal discovery algorithm. Consequently, we use the approach described in K\u0131c\u0131man et al. (2023), whereby we report the ratio of the NHD with a Baseline Hamming distance (BHD). The BHD is defined as a graph that contains the same number of edges as G, but all of them are incorrect. A lower NHD/BHD ratio is the multiple by which the causal discovery algorithm outperforms the \u2018floor\u2019 baseline graph.\nThe CMA outperforms causal discovery techniques which rely exclusively on data- or metadata (LLM)-driven approaches. In multiple instances, the CMA is capable of proposing additional causal relations within \u2018internal\u2019 variables, which we define as variables for which we have direct access to data. We give an illustrative example from the Arctic sea ice dataset. The CMA proposes that there\nis a 70% probability that \u2018net longwave flux at the surface\u2019 (LW) should have a causal relationship to \u2018sensible plus latent heat flux\u2019 (HFLX). The CMA\u2019s reasoning trace is shown in Appendix Figure 12. Observational samples as well as counterfactual inference under the causal model which encodes this edge suggest a positive relationship between the variables (Appendix A.3.4). Whilst this relationship is not present in the original causal graph, nor are its dynamics present in the domain literature used to construct it (Huang et al., 2021b), increased absorption of longwave radiation (LW) produces tropospheric warming, leading to an increase in sea surface temperature (SST), until SSTdependent cooling increases to establish equilibrium, which occurs through LW\u2019s effect on latent and sensible heat fluxes (HFLX) (Bates et al., 2012). We therefore contend this is an example of a missing edge from the ground-truth graph detected by the CMA. Extended results for the Arctic sea ice, Sangiovese, and AD benchmarks can be found in Appendix sections A.3.3, A.4.3, and A.5.3, respectively."
        },
        {
            "heading": "4.3 CASE STUDY: ALZHEIMER\u2019S DISEASE NEUROIMAGING EXPERIMENT",
            "text": "Experimental setup In this experiment, a CMA is applied to a challenging real-world dataset from the Alzheimer\u2019s Disease Neuroimaging Initiative (ADNI) (Petersen et al., 2010). We consider the same variables as in the AD benchmark experiment above (Section 4.2), with the exception that we link the clinical and radiological phenotypes of AD by considering how the variables might impact a brain Magnetic Resonance Image (MRI) for a given participant. This allows us to assess the effects of interventions and counterfactual queries in the imaging space. For example, if we intervene on brain volume by reducing it, we would expect a reduction in brain size on the MRI image in a reasonable model. We assess the counterfactual implications of the model proposed by the CMA and illustrate the framework\u2019s utility for identifying potentially useful external variables for inclusion in the causal graph. Additional experimental setup details can be found in Appendix A.6.\nResults The CMA outputs a trained SCM that encodes functional causal relationships between the disease covariates and the presentation of the disease in MRI images. Figure 3a illustrates the CMA\u2019s proposed causal graph, whilst 3c illustrates the biologically plausible output of counterfactual queries in the imaging space for qualitative evaluation. For example, increasing age produces visible cortical degeneration in the counterfactual image (Figure 3c; row 2). It should be noted that increasing the age value not only demonstrates cortical neurodegeneration, but the ventricles are also expanded, which is the expected neuro-radiological result of increasing age (Dinsdale et al., 2021). Directly intervening on whole brain and ventricular volumes (Figure 3c; rows 1 and 4, respectively) leads to expected morphological effects in both instances.\nAs with the Arctic sea ice dataset, we note interesting edges proposed for internal variables in the causal graph. In particular, we note the relationship \u2018biological sex\u2192tau pathology\u2019. This relationship was not proposed by any of the domain experts for the AD benchmarking task in Section 4.2. The graphs proposed by the experts can be seen in Appendix A.5. Nonetheless, counterfactual inference using the model trained by the CMA (under the ADNI dataset) provides evidence of increased P-tau protein levels in females (Figure 3b). Counterfactually intervening on sex led to a statistically significant shift in P-tau levels following Welch\u2019s t-test of unequal variances. This was observed when intervening on both males and females (t = \u22126.84, p < 0.001, and t = 6.05, p < 0.001, respectively). Despite not reflecting current expert consensus, there is emerging evidence that such a mechanism exists. In recent work by Yan et al. (2022), it was shown in both in-vitro and invivo models that the X chromosome-linked protein ubiquitin-specific peptidase 11 (USP11) alters tau aggregation patterns by deubiquitination initiated at lysine-218. Deubiquitination (removal of ubiquitin, a 76 amino acid protein), enhances tau buildup, which produces a damaging effect on brain tissue. USP11 \u2018escapes\u2019 complete X-inactivation (Yan et al., 2022), meaning females exhibit greater levels than males, and delimits the causal relationship between having an XX genotype and increased tau pathology.\nAs the CMA leverages LLMs to produce hypotheses, it is capable of proposing potentially confounding or modulating variables for which we do not have direct access to data (\u2018external\u2019 variables). For example, whilst assessing a potential relationship between the APOE4 gene and tau pathology (APOE4\u2192tau pathology), the CMA proposes that the TREM2 gene may be a potential confounder. The reasoning trace is shown in full in Appendix A.7. We accrue data for the soluble form of TREM2 (sTREM2) from the ADNI dataset and include it in the causal graph with a directed edge to P-tau (sTREM2\u2192tau pathology). By counterfactual inference, we find a statistically significant positive relationship between sTREM2 and P-tau (t = 6.55, P < 0.001). Whilst several associational studies exist with inconclusive results (Sua\u0301rez-Calvet et al., 2019; Zhao et al., 2022), we believe this analysis to be the first counterfactual-inference-based approach to analysing this relationship. TREM2 is an immune receptor expressed by support cells of the central nervous system (Bouchon et al., 2001), and is broken down by ADAM metalloprotease (Kleinberger et al., 2014) to produce its soluble form. Whether this breakdown (and therefore reduction of TREM2 on the cell surface membrane) leads to increased tau pathology, or whether sTREM2 itself exerts a specific biological function is unclear (Filipello et al., 2022). Whilst our work proposes a potential relation from sTREM2 to tau deposition, the underlying mechanism is currently unknown (Filipello et al., 2022)."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "We introduce the CMA, a modular framework for causal discovery with large language models and deep SCMs (DSCMs), enabling causal discovery for multi-modal data. Through a diverse set of experiments on synthetic and real-world datasets from a range of scientific fields (agriculture, geophysics, and health), we demonstrate that the CMA outperforms existing data- or metadatadriven approaches for causal discovery. The CMA is capable of proposing insightful relationships between variables internal and external to the DSCMs, and we provide a number of illustrative examples in the geophysics and health domains.\nFuture work should address several important limitations. Whilst discrete variables with parents currently require continuous relaxation, DSCMs could support discrete mechanisms with a Gumbel\u2013max parametrisation (Pawlowski et al. (2020), Appendix C). In our AD case study, our model used 2D axial slices of the brain, which may not be optimal for producing counterfactuals based on 3D volumes; hence, incorporating 3D volume modelling in DSCMs is a potential extension. Although the observational distribution is determined by a graph\u2019s Markov Equivalence Class (MEC), there is evidence that LLMs can effectively reduce the MEC by ruling out implausible graphs (Long et al., 2023a). We discuss these ideas in more detail in Appendix A.2.4. Finally, The CMA relies on DSCMs to fit data, which assumes a Markovian DAG. Whilst we extend the framework to include chain graph elements, in practice, we found that LLMs struggle to represent chains in a fully automated manner- instead, the CMA proposes potentially bidirectional relationships and human intervention is necessary to allow appropriate Deep Chain Graph modelling. Investigating techniques to enable fully automated chain graph modelling, and indeed more generally extending deep SCMs to represent more flexible, non-Markovian causal graphs (for example, models which allow for feedback loops) represents a natural avenue of future research."
        },
        {
            "heading": "REPRODUCIBILITY STATEMENT",
            "text": "To increase reproducibility, we have included all implementation details in Appendix A.1. We also include implementation and prompting code at https://anonymous.4open.science/r/ causal_modelling_agent-F443/."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "This work is supported by an EPSRC Industrial Case grant [EP/W522077/1] and the EPSRCfunded UCL Centre for Doctoral Training in Intelligent, Integrated Imaging in Healthcare (i4health) [EP/S021930/1]. AA is supported by a Microsoft Research PhD Scholarship. Wellcome Trust award 221915/Z/20/Z, JPND and MRC award MR/T046422/1 and the NIHR ULCH Biomedical Research Centre support DCA\u2019s work on this topic."
        },
        {
            "heading": "A APPENDIX",
            "text": ""
        },
        {
            "heading": "Summary of Appendices.",
            "text": "\u2022 A.1: Causal Modelling Agent\n\u2022 A.2: Synthetic neuropathic protein experiment\n\u2022 A.3: Benchmark: Arctic Sea Ice\n\u2022 A.4: Benchmark: Sangiovese\n\u2022 A.5: Benchmark: Alzheimer\u2019s Disease\n\u2022 A.6: Case Study: Alzheimer\u2019s Disease Neuroimaging Experiment"
        },
        {
            "heading": "A.1 CAUSAL MODELLING AGENT",
            "text": "In this section, we provide additional background on DSCMs and chain graph models, before introducing our novel parameterisation of a chain graph with deep learning elements. We then provide implementation details for our global and local hypothesis amendment phases, the modelling engine, and the post-processing phase of the CMA."
        },
        {
            "heading": "A.1.1 ADDITIONAL BACKGROUND",
            "text": "Deep Structural Causal Models A structural causal model (SCM) C := (S, P\u03f5) consists of a collection S = (f1, . . . , fD) of mechanisms Xj := fj(PAj , \u03f5j), j = 1, . . . , D, where PAj \u2286 {X1, . . . , XD} \\ {Xj} are assumed to be direct causes (parents) of Xj , and a jointly independent distribution over exogenous noise (\u03f5) variables P\u03f5 = \u220fD d=1 P (\u03f5d), where D is the number of variables for which there is associated exogenous noise. An SCM C entails a unique distribution over {Xd}Dd=1, denoted by PCX. SCM C satisfies the Markov condition (Geiger & Pearl, 1990), whereby every node is independent of its non-descendents given its parents. Each conditional probability\ncan be seen as being defined by its corresponding mechanism and noise P (Xd|PAd) = fd(\u03f5d;PAd) (Pawlowski et al., 2020; Peters et al., 2017). We perform interventions by altering the mechanism for variable Xk such that Xk := f\u0303(P\u0303Ak, \u03f5\u0303k). An intervention induces a new interventional distribution PC \u2032\nX =: P C;do(Xk:=f\u0303(P\u0303Ak,\u03f5\u0303k)) X (Peters et al., 2017). Interventions which place a point mass\non real value m are called atomic interventions and are written as PC;do(Xk:=m)X . In the scenario where we already have some observations, we can define a SCM in which the distribution of noise variables has been modified as CX=x := (S, P C|X=x \u03f5 ), where P C|X=x \u03f5 := P\u03f5|X=x, and X = x refers to a specific observation of random variable X. Performing an intervention in this setting amounts to performing a counterfactual query (Pearl, 2011; Peters et al., 2017). Computing counterfactual queries can be performed in three steps known as abduction, action, and prediction (Pearl, 2011): 1) In abduction, we infer the exogenous noise which is compatible with the observations x, i.e., compute PC|X=x\u03f5 ; 2) Next, we perform an intervention of interest, such as do(xI,1 := x\u2032I,1); 3) Finally, it is possible to infer a quantity of interest. If we are interested in quantity z, we could compute P C|X=x;do(xI,1:=x\u2032I,1) z (Peters et al., 2017; Pearl, 2011; 2018).\nFor DSCMs, mechanisms are parameterized by invertible DL elements so that the exogenous noise variables can be computed as per the abduction step. Conditional normalizing flows (Rezende & Mohamed, 2015; Trippe & Turner, 2018) are used to learn bijective mappings between the observed variables and their exogenous noise. The mappings operate in the data space, which means they are costly for modelling high-dimensional data (e.g., images). Pawlowski et al. (2020) propose instead to decompose such a mechanism fk into invertible hk and non-invertible gk functions. The noise is correspondingly decomposed as ek = (uk, zk), with p(ek) = p(uk)p(zk). The non-invertible noise term zk is computed by the recognition model of a conditional variational autoencoder (CVAE) (Sohn et al., 2015). This approach has been termed an \u2018amortized, explicit-likelihood mechanism\u2019. Additional details can be found in Pawlowski et al. (2020).\nDeep Chain Graph Models Chain graphs (CGs) are a form of probabilistic graphical model which contains both directed and undirected edges. A CG can be more expressive than either SCMs or Markov networks by introducing conditional independence statements not possible by either model type alone (Barber, 2012; Lauritzen, 1996). A CG can be interpreted as a Directed Acyclic Graph (DAG) over chain components (Frydenberg, 1990), where the chain components of a graph G can be identified by first removing all directed edges from G to produce graph G\u2032. The remaining connected components in G\u2032 (i.e., those vertices with undirected edges connecting them) are chain components \u03c4 . Each component represents a distribution over the variables within it, conditioned on parental components (Barber, 2012):\nPX = \u220f \u03c4 p(X\u03c4 |PA\u03c4 ), p(X\u03c4 |PA\u03c4 ) \u221d \u220f\nd\u2208D\u03c4 p(xd |PAd) \u220f c\u2208C\u03c4 \u03d5(Xc), (4)\nwhere D\u03c4 is the set of variables in component \u03c4 with directed terms, and C\u03c4 denotes the union of the cliques in \u03c4 . Unlike a fully specified DAG on singleton nodes, CGs can also be used to represent potential unmeasured confounding between two or more variables. We can take advantage of this increased flexibility by operationalising them using modular DL elements. Namely, we use (conditional) multivariate normalizing flows (Lu & Huang, 2019; Winkler et al., 2019) as the mechanisms fi \u2208 S on maximal cliques. We call this model a Deep Chain Graph Model (DCGM). Training a DCGM proceeds with the same likelihood-based objective as the DSCM. Whilst previous work has focussed on interpreting neural networks or elements within them as CGs (Shen & Cremers, 2020; Hwang et al., 2022), as far as we know, this is the first time that a CG is explicitly defined and then parameterized with DL modules such that it remains compatible with the counterfactual inference procedure described in A.1.1."
        },
        {
            "heading": "A.1.2 GLOBAL HYPOTHESIS AMENDMENT",
            "text": "The global phase implementation requires two inputs, an input JSON and an optional memory. During the first two iterations of the CMA, the memory is empty, as can be seen in the main text. The output of the call is a JSON with the same format as the input JSON. To ensure an appropriately structured output, we leverage gpt-4-0613\u2019s \u2018function calling\u2019 capability (OpenAI, 2023b;a), which represents the encoding function e(\u00b7) that operates on the representation of a given graph G\nto produce a structured graph Gs, as in the main text. The parameters of the input JSON are the nodes of the input graph, which have as properties an array called \u2018parents\u2019. The LLM is instructed to fill the \u2018parents\u2019 array of each node. Where a memory is included, it is embedded into the prompt directly. The algorithm and system prompt for the global phase are shown in Algorithm 2 and Listing 1, respectively.\nAlgorithm 2 Global Hypothesis Amendment Require: An input JSON G representing the initial graph and a Memory \u00b5.\n1: procedure GLOBAL PHASE(G(V,E), \u00b5) 2: if First iteration of CMA then 3: Initialize \u00b5 as empty 4: end if 5: if Second iteration of CMA then 6: Initialize \u00b5 as empty 7: end if 8: Extract nodes and their properties from G 9: if \u00b5 exists then\n10: Embed the memory \u00b5 into the prompt directly 11: end if 12: for each node n in G do 13: Make API call to gpt-4-0613 with n to get updated edges 14: Update G with the response from the API call 15: end for 16: return modified JSON G\u2032(V,E\u2032) with updated edges for each node. 17: end procedure\nListing 1: System prompt and schema for the global phase. 1 \"\"\" 2 Schema and system prompt for the global hypothesis amendment phase 3 \"\"\" 4 # Schema for function calling output 5 schema {\u2019name\u2019: \u2019change_DAG\u2019, \u2019description\u2019: \u2019The new improved DAG json\u2019,\n\u2019parameters\u2019: {\u2019type\u2019: \u2019object\u2019, \u2019properties\u2019: {\u2019node1\u2019: {\u2019type\u2019: \u2019 object\u2019, \u2019id\u2019: \u2019node1\u2019, \u2019parents\u2019: {\u2019type\u2019: \u2019array\u2019, \u2019description\u2019: \" List of parents of node1. Think about what variables of [\u2019node2\u2019, \u2019 node3\u2019,...] can affect it \"}}, \u2019node2\u2019: {\u2019type\u2019: \u2019object\u2019, \u2019id\u2019: \u2019 node2\u2019, \u2019parents\u2019: ...}.\n6 7 # Base prompt 8 prompt = f\"\"\"Consider the following JSON object: {input_json_dict}. Are\nthere any links you\u2019d like to add? Any you think should be removed? Any whose direction should be reversed? Consider directed changes of the variables in the JSON ONLY. Do not propose changes on variables not in the given JSON. Output a new valid JSON that you think would be an improved representation of the causal relationships between the variables.\"\"\"\n9 10 ====nodes list: 11 {nodes} 12 ======= 13 14 Do not give an explanation. Only output a JSON. Background knowledge:\nThis JSON describes a directed acyclic graph (DAG), which describes the causal relationships between nodes. Each node may be an ancestor (have an empty parent array), or may be the child of some other nodes (where you can see the parents in the JSON).\"\"\"\n15 16 # if memory 17 prompt = f\"{prompt}\\n\\n{memory}\""
        },
        {
            "heading": "A.1.3 LOCAL HYPOTHESIS AMENDMENT",
            "text": "This section outlines the details of the local hypothesis amendment phase. As described in the main text, this phase considers pairwise comparisons between the variables. Let V be the set of vertices in the structured causal graph Gs. For each pairwise comparison between the vertices in the graph, (u, v),\u2200 u, v \u2208 V, u \u0338= v, the local phase outputs a set of probabilities over all possible actions. The action space varies based on two types of pairwise comparisons: 1) \u2018Relationship assessment\u2019, and 2) \u2018Relationship adjustment\u2019. Relationship assessment assumes that there is currently no causal link between variables u and v. We ask the LLM to output three probability values based on the assumption that u and v are not related causally. The probabilities reflect three possible decisions:\n1. \u2018No direct causality\u2019: There is no direct causal relationship between the variables. 2. \u2018u causes v\u2019: Changing u causes a change in v in a causal manner. 3. \u2018v causes u\u2019: Changing v causes a change in u in a causal manner.\nIf the probability values for \u2018u causes v\u2019 and \u2018v causes u\u2019 are within a 5% threshold of each other, and are both individually greater than the probability that there is \u2018No direct causality\u2019, then the CMA saves these variables as potentially having a \u2018bidirectional\u2019 causal relationship. The CMA randomly chooses a direction in this case (as long as the condition that the output is a Markovian DAG is satisfied), and implements a causal edge. Following training, a user can assess whether the flagged \u2018bidirectional\u2019 relationships warrant additional modelling with a Deep Chain Graph model. An example output of a relationship assessment is shown below:\n1 {\"NO DIRECT CAUSALITY\": 90, \"U CAUSES V\": 5, \"V CAUSES U\": 5}\nRelationship adjustment, on the other hand, assumes that a causal relationship already exists in either direction between variables u and v. The LLM outputs probability values which reflect three different possible decisions:\n1. \u2018Keep\u2019: The causal relationship is correct in the current direction and should be kept. 2. \u2018Remove\u2019: Remove this relationship completely; there is no causal relationship between u\nand v. 3. \u2018Reverse\u2019: Flip the direction of this relationship. There is evidence of a causal relationship\nin the opposite direction to the currently proposed direction.\nAn example output of a relationship adjustment is shown below:\n1 {\"KEEP\": 5, \"FLIP\": 0, \"REMOVE\": 95}\nNaively, one could make API calls to the LLM for each CMA iteration (after each global phase) to obtain the relevant probabilities for each edge (and non-edge). However, this could be less efficient and may incur significant costs. Instead, to obtain probabilities for use in decision-making from the LLM, an initial (two-stage) \u2018pre-computation\u2019 phase occurs before the first CMA iteration. We gather all relevant probabilities as a preliminary step and apply them as appropriate for each subsequent iteration of the CMA. First, for each pairwise comparison (v, u), two API calls are made to gpt-4-0613 (OpenAI, 2023b). Specifically, one call is made which assumes that the variables are independent (relationship assessment), and one call is made assuming a relationship already exists (relationship adjustment); i.e., v \u2192 u. A run is defined as a complete iteration of all pairwise comparisons in the graph (two API calls are made for each comparison). Multiple runs are made to mitigate potential variability or inconsistency in the outputs. We use gpt-4-0613 with a temperature setting of 0.6 for all experiments.\nIn addition, the local phase can optionally utilise a Retrieval Augmentation Generation (RAG) pattern, whereby each edge in the DAG is described using natural language (for example, for the edge \u2018precipitation \u2192 humidity\u2019, we can write \u2018precipitation has a causative effect on humidity\u2019). The edge descriptions are embedded into a vector space using the text-embedding-ada-002 model, where all embeddings are normalized to unitary length. The embeddings are used to perform semantic text search with a vector store to retrieve the most relevant information which relates to the current edge. Retrieval occurs by calculating the cosine similarity between the edge description and\nthe document embeddings in the vector store as cos(\u03c9) = \u27e8x,y\u27e9/\u2225x\u2225\u2225y\u2225, which simplifies to a dotproduct due to the normalisation. The retrieved documents can be passed into the \u2018system prompt\u2019 as additional context. Aside from the (optional) additional context, the system prompt (Listing 2) contains instructions to consider the monotonicity, temporality, subjectivity, and potential spillover effects for each relationship, as well as potential confounders or mediating factors when outputting probability values.\nThe output probabilities are stored in JSON format. An optional explanation which represents the reasoning trace of the model can also be saved. The output is a JSON file for every pair of variables, with the name: pair[0]pair[1] temp prompt loop (if exists).json. In the second stage of the pre-computation phase, the probabilities stored in the JSONs for every combination are grouped, averaged over, and saved into two tables, one for \u2018empty\u2019 relationships (when there is no prior causal link assumed) and another for \u2018existing\u2019 relationships (assuming a prior causal relationship in either direction). During the CMA iterations, for any given pairwise comparison, if an edge already exists (e.g. from a previous iteration), the CMA looks into the \u2018existing\u2019 relationships table, otherwise into the \u2018empty\u2019 table. A final decision is made by sampling from a uniform distribution and then mapping the value to the probabilities output by the LLM. Algorithms 3 and 4 give an exposition of the pre-computation and local amendment operations, respectively.\nListing 2: System prompt for the local phase. Includes output options for both existing and empty relationships.\n1 \"\"\" 2 System prompt for the local hypothesis amendment phase 3 \"\"\" 4 5 \"You are an expert {INSERT DOMAIN} with a sub-specialist interest in {\nINSERT SUB-FIELD}. Your task is to explore the hypothesis space of the relationships that might exist between a number of variables relating to {INSERT SPECIFIC TOPIC}.\"\n6 7 # The options when assuming a pre-existing causal relationship: 8 Please decide the probabilistic nature of the relationship using the following options: 9 KEEP: \"The causal relationship is correct in the current direction and\nshould be kept.\" 10 FLIP: \"FLIP the direction of this relationship. There is evidence to\nsuggest that it is plausible that changing {pair[1]} causes a change to {pair[0]}.\"\n11 REMOVE: \"Remove this relationship completely; that is, no causal relationship exists between {pair[0]} and {pair[1]}.\"\n12 13 14 # The options when assuming no prior causal relationship: 15 Please decide the probabilistic nature of the relationship using the following options: 16 NO DIRECT CAUSALITY: \"There is no direct causal relationship between the {pair[0]} and {pair[1]}.\" 17 changing {pair[0]} causes changes to {pair[1]}: \"Changing {pair[0]} causes a change in {pair[1]} in a causal manner.\" 18 changing {pair[1]} causes changes to {pair[0]}: \"Changing {pair[1]} causes a change in {pair[0]} in a causal manner.\" 19 20 21 ================================ 22 To make your decision, consider the following criteria: 23 # The first criterion applies if an edge already exists. 24 Reversal and Causality: \"Could the direction of this relationship be\ninverted, such that the {pair[1]} might cause changes to {pair[0]} instead? If yes, the probability of option [FLIP] should be high.\"\n25 Confounding: \"Could this relationship be confounded by any other variables?\"\n26 Mediation_or_Modulation: \"Are there any additional variables that might either mediate or moderate this relationship?\" 27 Monotonicity: \"Do you think the relationship is monotonic?\" 28 Temporality: \"Is there a time-lag between causation?\" 29 Subjectivity: \"Could this relationship be subjective?\" 30 Spillover: \"Could this relationship be spillover?\" 31 ================================ 32 33 34 You may want to consider the following contextual information about {pair [0]} and {pair[1]}:\\n\\n 35 {CONTEXT} 36 \"\nAlgorithm 3 Local Hypothesis Precomputation Phase 1: for each node v \u2208 V do 2: for each node u \u2208 V where u \u0338= v do 3: Formulate the API requests with pair (v, u) 4: Make API call to gpt-4-0613 assuming no causal link between v and u 5: Store output probabilities in JSON file in ASSESSED-FOLDER 6: Make API call to gpt-4-0613 for potential causal relationship in direction v \u2192 u 7: Store output probabilities in JSON file in ADJUSTED-FOLDER 8: end for 9: end for 10: Process JSON files in ASSESSED-FOLDER to group and average probabilities 11: Store averaged probabilities in ASSESSED-TABLE CSV 12: Process JSON files in ADJUSTED-FOLDER to group and average probabilities 13: Store averaged probabilities in ADJUSTED-TABLE CSV\nAlgorithm 4 Local Hypothesis Amendment Require: An input JSON G representing the output graph from the global phase.\n1: procedure LOCAL PHASE(G(V,E)) 2: for each node v \u2208 V do 3: for each node u \u2208 V where u \u0338= v do 4: if directed edge exists between v and u then 5: Retrieve probabilities from ADJUSTED-TABLE CSV for relationship v \u2192 u 6: else 7: Retrieve probabilities from ASSESSED-TABLE CSV for the pair (v, u) 8: end if 9: Decide relationships based on retrieved probabilities and threshold\n10: if adding or flipping a directed edge creates a cycle then 11: Skip this amendment 12: end if 13: end for 14: end for 15: return modified JSON G\u2032(V,E\u2032) with amended hypothesis. 16: end procedure"
        },
        {
            "heading": "A.1.4 MODELLING ENGINE",
            "text": "We use the term Modelling Engine (ME) to refer to a software abstraction of both DSCMs and DCGMs, whereby either model type can be specified using a single JavaScript Object Notation (JSON) file which includes the name of each variable, its parent nodes, its type (binary/discrete/continuous/image), and whether it exists in a chain component \u03c4 . Because SCMs are specified by DAGs, and CGs can be interpreted as DAGs over chain components (Barber, 2012), it is important to account for the order in which we define the modules, as they must reflect the child\u2192parent pathways\nin the DAG. For example, attempting to construct a conditional network for a child node without first having constructed the network(s) for its parent(s) can lead to errors/model misspecification.\nWe implement the function K from the main text using Kahn\u2019s topological sorting algorithm (Kahn, 1962). In this case, the purpose of K is to sort the nodes and order network construction. As previously discussed, a structured graph Gst is taken as input toK to produce a computational graph CGt. Let V be the set of vertices in the structured causal graph, and E be the set of edges. The in-degree of vertex a \u2208 V is defined as the number of incoming edges; in-degree(a) = |{b \u2208 V : (b, a) \u2208 E}|. We identify a list N of nodes with in-degree(n) = 0. These nodes are removed from N alongside any out-going edges they have and added to list L. This process iterates until there are no nodes in N , leaving a sorted list in L. The computational graph is then defined by iteratively constructing the appropriate neural networks, hypernetworks, normalizing flows, and CVAE architectures where appropriate for each variable in the order given by L."
        },
        {
            "heading": "A.1.5 POST-PROCESSING",
            "text": "The post-processing phase consists of three steps:\n1. In the first step, we compare the graph JSONs from the current and previous iterations. This allows us to identify differences in relationships, which we then convert into textual descriptions.\n2. In the second step, we compare the average data likelihoods from the DSCM models of both the current and previous iterations. This comparison allows us to gauge the impact of changes on data likelihood.\n3. The final step involves using the identified differences in relationships and likelihood fit to make an API call to gpt-4-0613 using the prompt in Listing 3. We instruct the LLM to generate an explanation about the potential causes of the observed differences in fit. This explanation constitutes a \u2018memory\u2019, and is saved as a text file. The memory file can be used to guide the global phase of the subsequent iteration.\nThe post-processing phase is summarised in Algorithm 5.\nListing 3: System prompt for the memory generation. 1 \"You are an expert {INSERT DOMAIN} with a sub-specialist interest in {\nINSERT SUB-FIELD}. Your task is to explore the hypothesis space of the relationships that might exist between a number of variables relating to {INSERT SPECIFIC TOPIC}.\"\n2 3 I am going to present you two JSONs in <XML> tags that represent directed acyclic graphs which relate to {INSERT DOMAIN}. 4 For each node, you can see its parents in its relevant \u2019parents\u2019 array.\nNote that if two nodes have a non-zero chain number, they are in a \u2019 chain\u2019, which means they have an associative but NOT a causal relationship.\n5 6 <PREVIOUS GRAPH> { (previous_graph_topo_sorted)} </PREVIOUS GRAPH> 7 8 <CURRENT GRAPH> { (current_graph_topo_sorted)} </CURRENT GRAPH> 9\n10 In an experiment where we fit these graphs onto real data, 11 {string_comparison} 12 Based on your expert knowledge, try to provide an explanation for why\nthis might be the case by considering all the individual differences between both graphs. Here is a summary of the differences you should focus on:\n13 <GRAPH DIFFERENCES>{string_differences}</GRAPH DIFFERENCES> 14 Contextualize your analysis by thinking about the current literature on {\nadverb}. Provide specific advice for improving the current graph using these variables, given the literature. IF there are chain differences, do you recommend making the relationships between the variables causal or associative? Explain your reasoning.\nAlgorithm 5 Post-processing and memory generation Require: 2 input JSONs Gt, Gt\u22121 and 2 numbers Ft, Ft\u22121 representing the graphs and likelihood\nfits produced in current and previous iterations. 1: procedure POST-PROCESSING(Gt, Gt\u22121, Ft, Ft\u22121) 2: Find differences Dg between graphs Gt, Gt\u22121 3: Convert Dg into text 4: Calculate difference Df between Ft, Ft\u22121 5: Embed Df and Dg into an API call to gpt-4-0613 to get a memory \u00b5t 6: Save memory \u00b5t in a text file 7: end procedure"
        },
        {
            "heading": "A.2 SYNTHETIC NEUROPATHIC PROTEIN EXPERIMENT",
            "text": ""
        },
        {
            "heading": "A.2.1 DATA GENERATION",
            "text": "As described in the manuscript, we consider the following DAG structure: \u2018Age\u2192 AV45\u2019, \u2018Age\u2192 P-tau\u2019, and \u2018AV45\u2192 P-tau\u2019. The data-generating process is defined as follows:\nage := f\u2217age(\u03f5 \u2217 age) = 55 + \u03f5 \u2217 age, \u03f5 \u2217 age \u223c N (0, 1),\nav45 := f\u2217av45(\u03f5 \u2217 av45; age) = (age 2 + \u03f5\u2217av45 + 90)/60, \u03f5 \u2217 av45 \u223c N (0, 0.3),\ntau := f\u2217tau(\u03f5 \u2217 tau; av45, age) = (age 3 + av45 + \u03f5\u2217tau + 110)/6, \u03f5 \u2217 tau \u223c N (0, 0.2),\n(5)"
        },
        {
            "heading": "A.2.2 EXPERIMENTAL SETUP",
            "text": "For mechanisms S = (f1, . . . , fd), all continuous singleton nodes are represented by (conditional) rational spline normalizing flows (Reinhold et al., 2021; Pawlowski et al., 2020), whereas continuous nodes in chains are represented by (conditional) multivariate normalizing flows (Lu & Huang, 2019; Winkler et al., 2019). The flows are composed of components that fit the distribution as well as constrain the support of the output distribution. We consider two variables K and V , where K is an ancestor with no parents, and V is a child of K. Their mechanisms are defined as:\nK := fK(\u03f5K) = (exp \u25e6 AffineNormalisation \u25e6 Spline\u03b8)(\u03f5K), (6) V := fV (\u03f5V ;K) = (exp \u25e6 AffineNormalisation \u25e6 ConditionalTransform\u03b8(K\u0302))(\u03f5V ), (7)\nwhere the subscript \u03b8 denotes elements with learnable parameters. The Spline\u03b8 transformation is a first-order neural spline flow (Durkan et al., 2019), and ConditionalTransform\u03b8(\u00b7) is a conditional transformation which might represent a conditional affine transformation in the case of univariate nodes, or a combination of spline coupling and conditional spline transformations [SplineCoupling\u03b8,ConditionalSpline\u03b8] in the case of chain components (Lu & Huang, 2019; Winkler et al., 2019). We require conditional flows for singleton nodes or chains with parents. Conditional flows use hyper-networks, which are multi-layer perceptrons (MLPs) that predict the transformation parameters of otherwise invertible functions. The MLPs are composed of two hidden layers, with 10 and 16 nodes in the first and second layers, respectively. All variables are singly bounded and therefore an exponential transform is applied to constrain the unbounded values following a fixed AffineNormalisation transformation. The AffineNormalisation has the location and scale parameters set to the logarithm of the mean and variance of the training data, respectively, and is equivalent to a whitening operation in unbounded log-space (Pawlowski et al., 2020). Note the hat \u00b7\u0302 in equation (8), which refers to an unconstrained value (i.e., we invert the constraint transforms of exponentiation and AffineNormalisation). For all exogenous noise variables \u03f5 \u223c P\u03f5, we use unit Gaussians as base distributions. All learnable flow parameters were optimized by maximizing the likelihood using the AdamW optimizer (You et al., 2019) with a learning rate of 3 \u00d7 10\u22123 for 300 epochs."
        },
        {
            "heading": "A.2.3 ADDITIONAL RESULTS I - MARKOV EQUIVALENCE",
            "text": "Ground Truth\nDAG 1: Perfect Map\nDAG 2: Markov Equivalent\nDAG 3: Dependence Map\nDAG 4: Chain graph\nIn this section, we explicitly define the notion of equivalence classes for SCMs and present further numerical results as they relate to such a class.\nMarkov equivalence in graphs Given a DAG G and a distribution P , the distribution is said to be Markovian if it satisfies the local (or global) Markov property. The local Markov property states that all variables are independent of their non-descendants given their parents. The global Markov property with respect to G is defined as:\nX \u22a5 G Y |Z =\u21d2 X \u22a5 Y |Z (8)\nfor all disjoint vertex sets X,Y,Z (\u22a5 G denotes d-separation in the graph) (Peters et al., 2017). Let M(G) be the set of all distributions that are Markovian with respect to G. Two DAGs Gi and Gj are Markov equivalent ifM(Gi) = M(Gj). The set of all DAGs which are Markov equivalent to a DAG G is called the Markov Equivalence Class (MEC) of G (Pearl, 2011). This implies that a MEC for a given DAG is the set of DAGs which entail the same set of (conditional) independence statements.\nFrom a graphical perspective, two DAGs are Markov equivalent if they have the same skeleton and v-structure. A skeleton is defined as the set of edges in the graph after all directed edges are converted into undirected ones, and a v-structure is any structure of the form A\u2192 C \u2190 B; here, C is a collider, and A and B are not connected.\nMEC experiment We consider an expanded DAG from Figure 2 which includes a causal parent (CP) and causal ancestor (CA) of AV45. We perform a similar experiment as in section 4.1, however, we additionally consider a graph from the MEC of the expanded DAG. Figure 4 illustrates all DAGs considered for analysis. We conducted an ANOVA test for group differences and a Tukey HSD test for pairwise assessments. The ANOVA indicates a statistically significant difference between model likelihoods (p < 0.01). Table 3 shows the results of the pairwise Tukey HSD test. There is a statistically significant difference in data likelihood between all pairs of DAGs except the perfect map graph and its Markov equivalent graph. This would indicate that the reward signal produced by the Modelling Engine (the \u2018data-driven\u2019 element of the CMA; section A.1.4) is valid only up to the MEC of the data-generating process."
        },
        {
            "heading": "A.2.4 ADDITIONAL RESULTS II - LLM BEHAVIOURAL PATTERNS",
            "text": "Here, we discuss the use of LLMs in the context of a MEC and highlight early evidence of the utility of using LLMs to reduce the size of the MEC by excluding implausible graphs. Finally, we present additional analyses which delimit the behavioural patterns of the LLM portion of the CMA.\nThe role of LLMs in the context of Markov Equivalence Due to any given DAG having a MEC, it is generally not possible to learn the correct graph for a given dataset of observational data alone (Peters et al., 2017). Learning the correct graph given a dataset is known as the identifiability problem. Classically, there have been two approaches to this issue (K\u0131c\u0131man et al., 2023). First, one could assume that the data-generating process follows a specific functional form under which identifiability of a single graph might be possible (Glymour et al., 2019), for example by assuming a linear functional form and adding non-gaussian noise (Shimizu et al., 2006), or assuming a non-linear\nfunctional form with additive noise (Zhang & Chan, 2006; Zhang & Hyva\u0308rinen, 2009). However, even the simple setting of a dataset with linear equations and Gaussian noise is non-identifiable (Peters et al., 2017). The second approach is to jointly model all variables using deep learning techniques. This approach is not empirically more effective on many real-world datasets and does not resolve the identifiability issue. K\u0131c\u0131man et al. (2023) hypothesise that LLMs might represent a powerful utility to alleviate this issue by leveraging metadata-based reasoning to construct causal graphs. Indeed, there is early empirical evidence that LLMs can reduce the size of the MEC of a given DAG with a high probability of leaving behind the ground-truth graph when the LLMs are used as a post-processing step (Long et al., 2023a). To better understand this phenomenon, we investigate LLM behavioural patterns for reasoning over edges in the causal graph space.\nBehavioural experiment Figure 5 illustrates hypothesis amendments across three different types of relationship. We begin by considering the relationship between the degenerative proteins \u03b2amyloid and P-tau. If the starting point is an \u2018empty\u2019 relationship, the CMA is inclined to output the edge Amyloid\u2192Tau the majority of the time. This is also the case where an edge already exists in this direction (Figure 5; top-left and top-centre, respectively). This is in line with the \u2018amyloid cascade\u2019 hypothesis, which states that amyloid aggregation is upstream, and may also be causative of tau protein deposition (Hardy & Higgins, 1992; Gulisano et al., 2018). On the other hand, if the proposed edge is Tau\u2192Amyloid, the CMA either removes this relationship or \u2018flips\u2019 it to align with the amyloid cascade hypothesis a majority of the time. Nonetheless, it should be noted that this edge is \u2018kept\u2019 in many instances (Figure 5; top-right). Overall, the CMA can be seen to prefer the causal relationship Amyloid\u2192Tau, however will consider the opposite relationship Tau\u2192Amyloid a non-trivial proportion of the time, irrespective of the initial orientation and/or existence of the edge. This likely reflects recent doubt in the literature about the veracity of the amyloid cascade hypothesis (Herrup, 2015), and whether there might be a feedback loop running from Tau\u2192Amyloid, or whether both proteins may be confounded by other variables (Gulisano et al., 2018). However, this is unlikely to be the full story. In recent work, Berglund et al. (2023) described a phenomenon dubbed the \u2018reversal curse\u2019, which exposes a failure of generalization in auto-regressive LLMs. In brief, if a model is trained on \u2018A is B\u2019, it will not automatically generalize to \u2018B is A\u2019. The authors do not specifically assess the implications of this phenomenon for causal questions. Nevertheless, we hypothesize that the differences in model behaviour may in part be explained by the order in which a causal relationship is presented. Additional work is required to investigate the \u2018reversal curse\u2019 in the context of causal reasoning.\nThe second row of Figure 5 considers the relationship between biological sex and P-tau. The relationship Sex\u2192P-tau has found some empirical support in recent work (Yan et al., 2022). Where the initial relationship is empty, or already in the direction Sex\u2192P-tau, the CMA will consider a potential causative path from sex to P-tau a majority of the time, however, will also posit that there may not be a causal relationship in many cases. This relationship is interesting because one would expect that P-tau should not have a causal effect on sex, and indeed if the relationship P-tau\u2192Sex is proposed to the CMA (e.g. from a previous iteration), the model will either flip or remove it in almost all instances.\nThe third row of Figure 5 illustrates results for a relationship which should not plausibly exist in either direction: Biological sex and chronological age. Indeed, we see the CMA does not introduce this edge if it does not exist, and if the edge already exists (e.g. from a previous iteration), it is nearly always removed. The LLM modules of the CMA thus behave in a broadly expected manner; where relationships are not fully understood, there is often a default to current hypotheses, however, the CMA can make proposals which do not necessarily align with expert consensus in a conventional manner. Relationships which should not exist altogether are not suggested and/or removed.\nFinally, we assess the effect of Retrieval Augmented Generation (RAG) patterns on causal graph construction. We use gpt-4-0613 (with no RAG pattern) for all our main experiments, which has a cut-off date of September 2021. Utilizing a RAG pattern could allow it to account for more recent scientific literature. We consider the relationship between biological sex and P-tau once more, this time allowing the CMA to utilise a RAG pattern which accesses a vector store that contains embeddings of recently published abstracts to PubMed, including the works by (Yan et al., 2022). Results are illustrated in Figure 6. As can be seen, the additional context strongly enforces the new edge Sex\u2192P-tau; this underscores the large effect additional in-context learning can provide.\nWhilst the CMA represents relationships in a probabilistic manner, a strong enforcement can be made nonetheless based on a RAG pattern."
        },
        {
            "heading": "A.3 BENCHMARK: ARCTIC SEA ICE",
            "text": ""
        },
        {
            "heading": "A.3.1 DATASET",
            "text": "The dataset describes Arctic sea ice concentration in relation to a number of atmospheric variables such as total cloud water path, relative humidity, wind components, and geopotential height (Huang et al., 2021a). The full dataset was originally collected by analysing sea ice concentration levels provided by the National Snow and Ice Data Center, and the atmospheric data was obtained via the European Centre for Medium-Range Weather Forecasts\u2019 Integrated Forecast System (IFS) (Huang et al., 2021a). Additionally, a domain-knowledge graph with 12 nodes and 48 edges was proposed to describe causal relationships between the atmospheric variables and the sea ice concentration. Full details of the dataset are provided at (Huang et al., 2021a). We summarise variables in the dataset in Table 4."
        },
        {
            "heading": "A.3.2 EXPERIMENTAL SETUP",
            "text": "Data-driven methods We compare the CMA to the Non-combinatorial Optimization via Trace Exponential and Augmented lagrangian for Structure learning (NOTEARS) (Zheng et al., 2018), DAG Structure Learning with Graph Neural Networks (DAG-GNN) (Yu et al., 2019), and the Temporal Causal Discovery Frameworks (TCDF) (Nauta et al., 2019).\nWe perform a grid hyperparameter search over the two principal hyperparameters of NOTEARS, \u03bb and t (the L1 penalty parameter and the threshold parameter, respectively). We consider the hyperparameter sets: \u03bb = {0.001, 0.01, 0.1} and t = {0, 0.1, 0.3}. This contains the default hyperparameter set [0.1, 0.3] (Zheng et al., 2018). We use the implementation available at https: //github.com/xunzheng/notears. We use the default hyperparameters for DAG-GNN, and report results over two threshold parameters, t = {0.1, 0.3}. Whilst Huang et al. (2021a) and K\u0131c\u0131man et al. (2023) both report the performance of \u2018temporal\u2019 versions NOTEARS and DAG-GNN, it should be noted that both of these techniques assume i.i.d. observations of the variables. For the \u2018static\u2019 models, all observations at different time points are considered i.i.d., and fed directly into the algorithms. The \u2018temporal\u2019 versions require that the data is augmented by adding lagged versions of each variable, then considering the newly created lagged versions as a set of i.i.d. observations, before running NOTEARS and DAG-GNN on the augmented datasets. The TCDF algorithm requires time-series data and is therefore given temporal data from the Arctic sea ice dataset directly.\nMetadata-driven methods We report LLM benchmarking results based on the approach developed by K\u0131c\u0131man et al. (2023). Specifically, we use the \u2018single prompt\u2019 approach ((K\u0131c\u0131man et al., 2023); Appendix A.1, Table 14), which has been modified for the task of full graph discovery ((K\u0131c\u0131man et al., 2023); section 3.2.1; p13). The full prompt alongside an example is shown in table 5. We define two types of error for a given LLM output: 1) A missing answer tag, and 2) an incor-\nrectly formatted answer tag. For example, the output \u201c<Answer>A and B</Answer>\u201d is not correct, because the model should choose a single output."
        },
        {
            "heading": "A.3.3 ADDITIONAL RESULTS I - BENCHMARKS",
            "text": "Results for the data-driven approaches on the Arctic Sea Ice dataset are reported in Table 6. The original graph contains 48 edges; the maximum and minimum number of edges found by the NOTEARS benchmark was 15 and 0, respectively, with the maximum and minimum number of true positives being 7 and 0, respectively. An illustration of the best-performing NOTEARS output can be seen in Figure 7. The DAG-GNN algorithm discovered a higher number of total and true-positive edges with a threshold of 0.1, with a similar result over the threshold of 0.3. The best performing DAGGNN output is shown in Figure 8. Table 7 shows the results of the LLM benchmark for the Arctic Sea Ice dataset. The minimum number of predicted edges is 47, and the maximum number is 64. Error rates were as high as 16.6% with gpt-3.5-turbo at a temperature of 1.0. Results show that gpt-3.5-turbo (Figure 9) has worse performance in comparison to gpt-4 (Figure 10), and that the metadata-driven (LLM-based) approaches outperform the data-driven techniques. The CMA (Figure 11) outperforms both metadata- or data-driven approaches alone, with fewer false positives in particular than the LLM benchmarks.\nIt is interesting to note that these results vary from the metadata-based results reported by (K\u0131c\u0131man et al., 2023). We believe this to be down to two reasons. First, K\u0131c\u0131man et al. (2023) do not explicitly describe their error handling procedure, which may differ from ours (described above). Second, it appears that gpt-3.5-turbo and gpt-4 performance on a multitude of tasks can change over time, as demonstrated by (Chen et al., 2023), which may partly explain the observed differences."
        },
        {
            "heading": "A.3.4 ADDITIONAL RESULTS II - LW AND HFLX",
            "text": "The CMA identifies a high probability that net longwave flux at the surface (LW) has a direct effect on the sensible and latent heat flux (HFLX). The reasoning trace is given in Figure 12. Whilst an observational relationship exists between the variables (as shown in Figure 13, t = 4.72, p < 0.01), to investigate this relationship from a causal perspective, the output of the model trained by the CMA is shown in Figure 14. This illustrates that by counterfactual inference, an increase in LW leads to an increased measurement of HFLX. There is a borderline statistically significant difference between the observational and counterfactual distributions (t = \u22121.89, p = 0.059). We contend that this likely represents a missing edge from the ground-truth graph in Huang et al. (2021a)."
        },
        {
            "heading": "PC N/A N/A 17 0.326 0.381 0.855 9 0.529 0.188 0.277",
            "text": ""
        },
        {
            "heading": "A.4 BENCHMARK: SANGIOVESE",
            "text": ""
        },
        {
            "heading": "A.4.1 DATASET",
            "text": "The Sangiovese dataset is a causal discovery benchmark derived from the conditional linear Gaussian network model built in Magrini et al. (2017) from data collected about grape quality in Sangiovese vineyards in Tuscany. In summary, the original dataset consisted of the measurement of various characteristics of grapes collected during growth and post-harvest, such as their pH, polyphenol content, or the mean grape weight. Data was collected between 2007 and 2009. Additionally, different well-established interventions for canopy management were applied to different parts of the vineyard. Three intervention types were considered; defoliation and bunch thinning (not applied or applied at 50%), and harvest time (technological or late). The original DAG consists of 15 nodes and 55 edges; the original DAG and its constituent nodes are described in Table 8."
        },
        {
            "heading": "A.4.2 EXPERIMENTAL SETUP",
            "text": "The experimental setup is the same as in the Arctic sea ice benchmark, described in Appendix A.3.2."
        },
        {
            "heading": "A.4.3 ADDITIONAL RESULTS",
            "text": "Results for NOTEARS and DAG-GNN are shown in Table 9. The maximum and minimum number of edges found by the NOTEARS algorithm were 22 and 1, respectively, with the maximum and minimum number of true positives being 13 and 1, respectively. No significant trend was observed over the NHD for NOTEARS across different hyperparameter settings. A similar performance was observed for DAG-GNN and NOTEARS run at \u03bb=0.001. The best performing NOTEARS output (hyperparameters \u03bb=0.001, t=0.3) is shown in 15, with results for the DAG-GNN shown in Figure 16. As can be seen, both algorithms show similar patterns in true positives and false negatives.\nTable 10 shows the results of the LLM benchmarks. Whilst gpt-3.5-turbo is competitive with the data-driven causal discovery approaches, gpt-4 outperforms them across several temperature settings with an average error rate of 0.25%. Figures 17 and 18 illustrate the predicted adjacency matrices as compared to the ground-truth for gpt-3.5-turbo and gpt-4, respectively.\nBoth data- and metadata-driven approaches struggle with the Sangiovese dataset. This would suggest that: 1) Establishing causal structure from asymmetries in the dataset is challenging, AND 2) there is a lack of evidence to support making causal claims for many of the variables, which likely reflects less robust domain knowledge in general. As such, the CMA also struggles here and produces only marginally better results than either approach alone 19."
        },
        {
            "heading": "PC N/A N/A 25 0.244 0.347 0.706 14 0.560 0.275 0.368",
            "text": ""
        },
        {
            "heading": "A.5 BENCHMARK: ALZHEIMER\u2019S DISEASE",
            "text": ""
        },
        {
            "heading": "A.5.1 DATASET",
            "text": "Variable description The variables considered for this benchmark are described in table 11.\nData generation The ground-truth DAG is created in collaboration with 5 domain experts, with expertise in either clinical and/or academic neurology (with a sub-specialist interest in neurodegenerative/Alzheimer\u2019s disease) or neuroradiology with a subspecialist interest in neurodegenerative disease. To accrue expert causal graphs, we build a front-end application in the JavaScript programming language (Figure 20). We include edges in the final ground-truth DAG which are proposed by at least 2/5 experts as our consensus heuristic. Figure 21 shows individual expert-based causal graphs, and Figure 22 shows a stacked graph as well as the final ground-truth DAG."
        },
        {
            "heading": "A.5.2 EXPERIMENTAL SETUP",
            "text": "The experimental setup is the same as in the Arctic sea ice benchmark, described in Appendix A.3.2."
        },
        {
            "heading": "A.5.3 ADDITIONAL RESULTS",
            "text": "Results for the NOTEARS and DAG-GNN algorithm can be found in Table 12. The best NOTEARS output is visualized in Figure 23, whilst the DAG-GNN result is shown in Figure 24. The LLM benchmark results are shown in Table 13. The metadata-based (LLM) methods outperform the data-driven approaches. The CMA output is visualised in Figure 27. As can be seen, the CMA outperform both data- and metadata-driven approaches alone."
        },
        {
            "heading": "PC N/A N/A 16 0.198 0.321 0.615 8 0.500 0.500 0.500",
            "text": ""
        },
        {
            "heading": "A.5.4 SENSITIVITY ANALYSIS: DEGRADATION OF CAUSAL LINKS",
            "text": "In this sensitivity analysis we perturb the dataset variables using Gaussian noise with increasingly large standard deviation values, which has the effect of reducing the total variation explained by any given variable on its children in the causal graph. The effect of these perturbations can be visualised in Figure 28. We run the NOTEARS, DAG-GNN, and CMA algorithms on each noise level and compare their performances.\nNOTEARS results Inspecting results for NOTEARS in Tables 14, 15, and 16 reveals a number of patterns. First, causal discovery performance is worse with increasing values of the lambda parameter, and this is trend is consistent across all noise levels. Second, performance broadly degrades with higher noise levels. The best performance is observed at the \u2018L1\u2019 noise level (Figure 28; L1) with parameters \u03bb=0.001 and t=0.1. The worst results were osberved with at the highest noise level L3, with parameters \u03bb=0.1 and t=0.3.\nDAG-GNN results Inspecting results for DAG-GNN in Tables 14, 15, and 16 showcases the relative robustness of DAG-GNN across noise levels. The same number of true positives are detected across all levels, with less variability observed across other metrics.\nCMA results Similarly to the DAG-GNN algorithm, the CMA remains relatively robust across all noise levels, however outperforms both approaches with a lower NHD/BHD ratio. Indeed, it is likely this improved performance is partly due to the metadata-based modules of the framework, which can still propose reasonable causal structures despite weak causal relationships in the dataset. A visual summary of these results is illustrated in Figure 29."
        },
        {
            "heading": "A.6 CASE STUDY: ALZHEIMER\u2019S DISEASE NEUROIMAGING EXPERIMENT",
            "text": ""
        },
        {
            "heading": "A.6.1 EXPERIMENTAL SETUP",
            "text": "Image pre-processing Structural T1-weighted MRI scans were collected for all participants and linked to their relevant demographic, disease biomarker, and cognitive assessment variables. The earliest Inversion Recovery Spoiled Gradient echo sequence (SAG IR-SPGR) MRI was accrued for each participant, skull-stripped using the HD-BET brain extraction tool (Isensee et al., 2019), and bias-field corrected with the N4 software package (Tustison et al., 2010). All images were resampled to the size of the MNI ICBM152 brain atlas in the NiLearn software package (Nil) (197\u00d7233\u00d7189) with linear interpolation. The resampled images were subsequently rigidly registered to the atlas using ANTs (Avants et al., 2009). The middle 10 axial slices of each MRI were extracted and their intensity values were normalized by rescaling the minimum and maximum values of each slice to [0, 255]. Each 2D image slice is then saved as a PNG file for training. During training, the image slices were uniformly dequantised by the addition of Gaussian noise (Theis et al., 2015). Images were randomly cropped from their original size to 192\u00d7 192 and downsampled to 64\u00d7 64 during training to prevent overfitting. The slices were centre-cropped during counterfactual image inference.\nImaging mechanisms Mechanisms S were defined in the same way as in the synthetic experiments A.2.2. By decomposing the image mechanism as per 2.1, we model the image as the invertible function\nHimg(uimg;PAimg) = [Preprocessing \u25e6 ConditionalAffine\u03b8(PAimg)](uimg), (9)\nwhere the Preprocessing follows RealNVP (Dinh et al., 2016), and the hyper-network for ConditionalAffine\u03b8(\u00b7) is the non-invertible mechanism gimg(zimg;PAimg) which is implemented as a decoder that outputs the bias for the ConditionalAffine\u03b8(\u00b7) transformation with fixed logarithmicvariance of log \u03c32 = \u22125. We require an encoder function eimg(img;PAimg) to generate the latent zimg. The images are therefore modelled using a CVAE architecture where both encoder and decoder functions are composed of 5 modules of 3 blocks of (LeakyReLU(0.1),BN\u03b8,Conv\u03b8), where Conv is a convolutional layer, BN is batch normalisation, and LeakyReLU(\u03d5) is a leaky rectified linear unit with an angle of negative slope parameter \u03d5. For all other non-imaging mechanisms, we use a similar setup as in the synthetic experiments (Appendix A.2.2).\nBinary variables such as biological sex require that we learn the binary probability by sampling from a Bernoulli distribution (female = 1, male = 0). Discrete variables such as APOE4 status and MRI image slice number are sampled from uniform distributions (APOE4 status in {0, 1, 2}, and minimum to maximum number of slices, respectively), as per Reinhold et al. (2021). All learnable parameters in the flows and the CVAE architecture were optimised by a stochastic variational inference approach to estimate the evidence lower bound (ELBO; estimated using 4 Monte Carlo (MC) samples) using the Adam optimizer (Kingma & Ba, 2015) with learning rates of 10\u22125 and 5\u00d710\u22123, respectively. For counterfactual inference, 32 MC samples were taken and the inference result was their average. All learnt mechanisms fi \u2208 S were fixed during inference and the single world intervention graph (SWIG) formalism was used to produce counterfactuals (Richardson & Robins, 2013). Experiments were parallelized across two NVIDIA RTX 3090 GPUs and one NVIDIA RTX 4090 GPU."
        },
        {
            "heading": "A.7 ADDITIONAL RESULTS",
            "text": "Figure 30 illustrates the CMA\u2019s reasoning trace for the relationship between APOE4 and P-tau. The model proposes that the TREM2 gene may confound this relationship. We subsequently accrued data for the soluble form of TREM2 (sTREM2) from the Alzheimer\u2019s Disease NeuroImaging dataset and trained a DSCM with a causal link between sTREM2 and P-tau. We perform a counterfactual intervention and find a statistically significant relationship between sTREM2 levels and P-tau levels following Welch\u2019s ANOVA test of unequal variance (F = 43.52, p < 0.01). Figure 31 illustrates counterfactual interventions against the null intervention. As can be seen, there is a positive relationship between increasing sTREM2 levels and P-tau deposition.\nWe additionally conduct an observational analysis in two stages. First we regress sTREM2 on P-tau levels, and then we add APOE4 as a categorical variable. Results are illustrated in Figure 32 and\nFigure 33, respectively. As can be seen, there is a positive association between sTREM2 levels and P-tau (t = 5.15, p < 0.01). As expected, patients with \u2265 1 copies of the APOE4 gene display greater levels of P-tau deposition (t = 2.76, p = 0.006), with the greatest levels observable in homozygous present patients (i.e., those who have two copies of the gene). Finally, we conduct an observational analysis by regressing biological sex onto P-tau levels. We can be seen in Figure 34, females have a higher level of P-tau burden (t = 2.29, p = 0.023)."
        },
        {
            "heading": "A.7.1 REVIEW OF THE EFFECTS OF STREM2 AND SEX ON THE TAU PROTEIN",
            "text": "sTREM2 \u2192 Neuropathic tau protein Triggering receptor expressed on myeloid cells 2 (TREM2) is an immune receptor in the central nervous system (CNS). It appears to have a positive role in cellular proliferation and survival (Ewers et al., 2019). It has a soluble form (sTREM2), which can be produced from the shedding of TREM2. There are a number of studies which show elevated levels of sTREM2 in AD (Knapskog et al., 2020; Ioannides et al., 2021; Wilson et al., 2020). For example, Sua\u0301rez-Calvet et al. (2019) demonstrated that tau pathology is associated with an increase in sTREM2 in the cerebrospinal fluid (CSF). However, presence of amyloid protein alone was not associated with an increase in CSF sTREM2. There is also recent evidence of elevated sTREM2 levels in other pathologies including multiple sclerosis (Ioannides et al., 2021) and Parkinson\u2019s disease subgroups with increased CSF tau (Wilson et al., 2020).\nBiological sex \u2192 Neuropathic tau protein In our analysis, the CMA (fit to the ADNI dataset) proposed that there may be a direct causal link between being biologically female (XX chromosome profile), and neuropathic protein deposition. Recent work by Buckley et al. (2020) aimed to investigate sex differences in tau distribution across multiple brain regions of older adults using Positron Emission Tomography (PET) scanning. They used the Alzheimer\u2019s Disease Neuroimaging Initiative (ADNI) and Harvard Aging Brain Study (HABS) datasets. Neuropathic tau protein levels were measured by use of [18F]flortaucipir (FTP), which is a tracer agent that allows in vivo quantification of paired helical filament tau (Ossenkoppele et al., 2018). Their work suggested that women showed statistically significantly higher FTP-signal (greater tau levels) in multiple regions of the cortical mantle (p < 0.007). They additionally wanted to assess whether composite FTP signals in Regions of Interest (ROIs) across the brain were associated with a more rapid cognitive decline. In their study, women with higher FTP signals had a borderline significant higher rate of cognitive decline than men (p = 0.04). Following this, it was thought that perhaps these sex differences were due to sex hormone profiles. To assess this, Wisch et al. (2021) conducted a cross-sectional neuroimaging study which compared cortical tau deposition (using PET) between cognitively normal males and females. In addition, they also compared preclinical Alzheimer\u2019s pathology between females who had and had not used hormone therapy (HT). They observed greater tau deposition in females. However, they also observed decreased tau burden females who were HT users, and highlighted that this relationship should be investigated longitudinally. It was therefore unclear whether hormonal profiles modulated tau protein deposition, or whether this was more directly mediated by genomic/epigenomic markers. deposition. Tau clearance itself is controlled by methylation (Balmik & Chinnathambi, 2021), then (Esteves et al., 2019), and finally ubiquitination (Flach et al., 2014). Yan et al. (2022) demonstrated that the X-linked gene Ubiquitin Specific Peptidase 11 (USP11) removes ubiquitin from tau (the final step required for its clearance), which led to its aggregation in-vivo. USP11 has elevated expression in females, and correlated with tau brain pathology. Whilst this might not yet be the full story, it is an exciting research direction, as the mechanism behind the sexual dimorphism in tau burden is thought to be a foundational event in AD development Yan et al. (2022)."
        }
    ],
    "title": "CAUSAL MODELLING AGENTS: CAUSAL GRAPH DIS-",
    "year": 2024
}