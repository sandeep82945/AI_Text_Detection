{
    "abstractText": "Large language models (LLM), such as Google\u2019s Minerva and OpenAI\u2019s GPT families, are becoming increasingly capable of solving mathematical quantitative reasoning problems. However, they still make unjustified logical and computational errors in their reasoning steps and answers. In this paper, we leverage the fact that if the training corpus of LLMs contained sufficiently many examples of formal mathematics (e.g. in Isabelle, a formal theorem proving environment), they can be prompted to translate i.e. autoformalize informal mathematical statements into formal Isabelle code \u2014 which can be verified automatically for internal consistency. This provides a mechanism to automatically reject solutions whose formalized versions are inconsistent within themselves or with the formalized problem statement. We evaluate our method on GSM8K, MATH and MultiArith datasets and demonstrate that our approach provides a consistently better heuristic than vanilla majority voting \u2014 the previously best method to identify correct answers, by more than 12% on GSM8K. In our experiments it improves results consistently across all datasets and LLM model sizes.",
    "authors": [],
    "id": "SP:72b3d1bb0d5df814e5d5fd587bc124afbd5671c6",
    "references": [
        {
            "authors": [
                "Ayush Agrawal",
                "Siddhartha Gadgil",
                "Navin Goyal",
                "Ashvni Narayanan",
                "Anand Tadipatri"
            ],
            "title": "Towards a mathematics formalisation assistant using large language models",
            "venue": "arXiv preprint arXiv:2211.07524,",
            "year": 2022
        },
        {
            "authors": [
                "Zhangir Azerbayev",
                "Bartosz Piotrowski",
                "Hailey Schoelkopf",
                "Edward W Ayers",
                "Dragomir Radev",
                "Jeremy Avigad"
            ],
            "title": "Proofnet: Autoformalizing and formally proving undergraduate-level mathematics",
            "venue": "arXiv preprint arXiv:2302.12433,",
            "year": 2023
        },
        {
            "authors": [
                "Yuntao Bai",
                "Andy Jones",
                "Kamal Ndousse",
                "Amanda Askell",
                "Anna Chen",
                "Nova DasSarma",
                "Dawn Drain",
                "Stanislav Fort",
                "Deep Ganguli",
                "Tom Henighan"
            ],
            "title": "Training a helpful and harmless assistant with reinforcement learning from human feedback",
            "venue": "arXiv preprint arXiv:2204.05862,",
            "year": 2022
        },
        {
            "authors": [
                "Bruno Barras",
                "Samuel Boutin",
                "Cristina Cornes",
                "Judica\u00ebl Courant",
                "Jean-Christophe Filliatre",
                "Eduardo Gimenez",
                "Hugo Herbelin",
                "Gerard Huet",
                "Cesar Munoz",
                "Chetan Murthy"
            ],
            "title": "The Coq proof assistant reference manual: Version 6.1",
            "venue": "PhD thesis, Inria,",
            "year": 1997
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Davide Castelvecchi"
            ],
            "title": "Mathematicians welcome computer-assisted proof in \u2018grand unification\u2019theory",
            "venue": "Nature, 595(7865):18\u201319,",
            "year": 2021
        },
        {
            "authors": [
                "Wenhu Chen",
                "Xueguang Ma",
                "Xinyi Wang",
                "William W. Cohen"
            ],
            "title": "Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Aakanksha Chowdhery",
                "Sharan Narang",
                "Jacob Devlin",
                "Maarten Bosma",
                "Gaurav Mishra",
                "Adam Roberts",
                "Paul Barham",
                "Hyung Won Chung",
                "Charles Sutton",
                "Sebastian Gehrmann"
            ],
            "title": "Palm: Scaling language modeling with pathways",
            "venue": "arXiv preprint arXiv:2204.02311,",
            "year": 2022
        },
        {
            "authors": [
                "Karl Cobbe",
                "Vineet Kosaraju",
                "Mohammad Bavarian",
                "Mark Chen",
                "Heewoo Jun",
                "Lukasz Kaiser",
                "Matthias Plappert",
                "Jerry Tworek",
                "Jacob Hilton",
                "Reiichiro Nakano"
            ],
            "title": "Training verifiers to solve math word problems",
            "venue": "arXiv preprint arXiv:2110.14168,",
            "year": 2021
        },
        {
            "authors": [
                "Antonia Creswell",
                "Murray Shanahan",
                "Irina Higgins"
            ],
            "title": "Selection-inference: Exploiting large language models for interpretable logical reasoning, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Leonardo de Moura",
                "Soonho Kong",
                "Jeremy Avigad",
                "Floris Van Doorn",
                "Jakob von Raumer"
            ],
            "title": "The lean theorem prover (system description)",
            "venue": "In Automated Deduction-CADE-25: 25th International Conference on Automated Deduction, Berlin,",
            "year": 2015
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova. Bert"
            ],
            "title": "Pre-training of deep bidirectional transformers for language understanding",
            "venue": "arXiv preprint arXiv:1810.04805,",
            "year": 2018
        },
        {
            "authors": [
                "Yao Fu",
                "Hao Peng",
                "Ashish Sabharwal",
                "Peter Clark",
                "Tushar Khot"
            ],
            "title": "Complexity-based prompting for multi-step reasoning, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Luyu Gao",
                "Aman Madaan",
                "Shuyan Zhou",
                "Uri Alon",
                "Pengfei Liu",
                "Yiming Yang",
                "Jamie Callan",
                "Graham Neubig"
            ],
            "title": "Pal: Program-aided language models",
            "venue": "arXiv preprint arXiv:2211.10435,",
            "year": 2022
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Collin Burns",
                "Saurav Kadavath",
                "Akul Arora",
                "Steven Basart",
                "Eric Tang",
                "Dawn Song",
                "Jacob Steinhardt"
            ],
            "title": "Measuring mathematical problem solving with the math dataset",
            "venue": "arXiv preprint arXiv:2103.03874,",
            "year": 2021
        },
        {
            "authors": [
                "Ari Holtzman",
                "Jan Buys",
                "Li Du",
                "Maxwell Forbes",
                "Yejin Choi"
            ],
            "title": "The curious case of neural text degeneration",
            "year": 1904
        },
        {
            "authors": [
                "Shima Imani",
                "Liang Du",
                "Harsh Shrivastava"
            ],
            "title": "Mathprompter: Mathematical reasoning using large language models",
            "venue": "arXiv preprint arXiv:2303.05398,",
            "year": 2023
        },
        {
            "authors": [
                "Gautier Izacard",
                "Patrick Lewis",
                "Maria Lomeli",
                "Lucas Hosseini",
                "Fabio Petroni",
                "Timo Schick",
                "Jane Dwivedi-Yu",
                "Armand Joulin",
                "Sebastian Riedel",
                "Edouard Grave"
            ],
            "title": "Atlas: Few-shot learning with retrieval augmented language models",
            "venue": "arXiv preprint arXiv,",
            "year": 2022
        },
        {
            "authors": [
                "Albert Qiaochu Jiang",
                "Wenda Li",
                "Jesse Michael Han",
                "Yuhuai Wu"
            ],
            "title": "Lisa: Language models of isabelle proofs",
            "venue": "In 6th Conference on Artificial Intelligence and Theorem Proving,",
            "year": 2021
        },
        {
            "authors": [
                "Albert Qiaochu Jiang",
                "Sean Welleck",
                "Jin Peng Zhou",
                "Wenda Li",
                "Jiacheng Liu",
                "Mateja Jamnik",
                "Timoth\u2019ee Lacroix",
                "Yuhuai Wu",
                "Guillaume Lample"
            ],
            "title": "Draft, Sketch, and Prove: Guiding formal theorem provers with informal proofs",
            "venue": "In International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Jaehun Jung",
                "Lianhui Qin",
                "Sean Welleck",
                "Faeze Brahman",
                "Chandra Bhagavatula",
                "Ronan Le Bras",
                "Yejin Choi"
            ],
            "title": "Maieutic prompting: Logically consistent reasoning with recursive explanations",
            "venue": "In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2022
        },
        {
            "authors": [
                "Aishwarya Kamath",
                "Rajarshi Das"
            ],
            "title": "A survey on semantic parsing",
            "venue": "arXiv preprint arXiv:1812.00978,",
            "year": 2018
        },
        {
            "authors": [
                "Tushar Khot",
                "Harsh Trivedi",
                "Matthew Finlayson",
                "Yao Fu",
                "Kyle Richardson",
                "Peter Clark",
                "Ashish Sabharwal"
            ],
            "title": "Decomposed prompting: A modular approach for solving complex tasks, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Gerwin Klein",
                "Kevin Elphinstone",
                "Gernot Heiser",
                "June Andronick",
                "David Cock",
                "Philip Derrin",
                "Dhammika Elkaduwe",
                "Kai Engelhardt",
                "Rafal Kolanski",
                "Michael Norrish"
            ],
            "title": "sel4: Formal verification of an os kernel",
            "venue": "In Proceedings of the ACM SIGOPS 22nd symposium on Operating systems principles,",
            "year": 2009
        },
        {
            "authors": [
                "Takeshi Kojima",
                "Shixiang Shane Gu",
                "Machel Reid",
                "Yutaka Matsuo",
                "Yusuke Iwasawa"
            ],
            "title": "Large language models are zero-shot reasoners",
            "venue": "Advances in neural information processing systems,",
            "year": 2022
        },
        {
            "authors": [
                "Angeliki Lazaridou",
                "Elena Gribovskaya",
                "Wojciech Stokowiec",
                "Nikolai Grigorev"
            ],
            "title": "Internetaugmented language models through few-shot prompting for open-domain question answering",
            "venue": "arXiv preprint arXiv:2203.05115,",
            "year": 2022
        },
        {
            "authors": [
                "Aitor Lewkowycz",
                "Anders Johan Andreassen",
                "David Dohan",
                "Ethan Dyer",
                "Henryk Michalewski",
                "Vinay Venkatesh Ramasesh",
                "Ambrose Slone",
                "Cem Anil",
                "Imanol Schlag",
                "Theo Gutman-Solo",
                "Yuhuai Wu",
                "Behnam Neyshabur",
                "Guy Gur-Ari",
                "Vedant Misra"
            ],
            "title": "Solving quantitative reasoning problems with language models",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Reiichiro Nakano",
                "Jacob Hilton",
                "Suchir Balaji",
                "Jeff Wu",
                "Long Ouyang",
                "Christina Kim",
                "Christopher Hesse",
                "Shantanu Jain",
                "Vineet Kosaraju",
                "William Saunders"
            ],
            "title": "Webgpt: Browser-assisted question-answering with human feedback",
            "venue": "arXiv preprint arXiv:2112.09332,",
            "year": 2021
        },
        {
            "authors": [
                "Tobias Nipkow",
                "Markus Wenzel",
                "Lawrence Charles Paulson"
            ],
            "title": "Isabelle/hol: A proof assistant for higher-order logic",
            "year": 2002
        },
        {
            "authors": [
                "Lawrence C Paulsson",
                "Jasmin C Blanchette"
            ],
            "title": "Three years of experience with sledgehammer, a practical link between automatic and interactive theorem provers",
            "venue": "In Proceedings of the 8th International Workshop on the Implementation of Logics (IWIL-2010), Yogyakarta, Indonesia. EPiC,",
            "year": 2012
        },
        {
            "authors": [
                "Subhro Roy",
                "Dan Roth"
            ],
            "title": "Solving general arithmetic word problems",
            "venue": "arXiv preprint arXiv:1608.01413,",
            "year": 2016
        },
        {
            "authors": [
                "William Saunders",
                "Catherine Yeh",
                "Jeff Wu",
                "Steven Bills",
                "Long Ouyang",
                "Jonathan Ward",
                "Jan Leike"
            ],
            "title": "Self-critiquing models for assisting human evaluators",
            "venue": "arXiv preprint arXiv:2206.05802,",
            "year": 2022
        },
        {
            "authors": [
                "Timo Schick",
                "Jane Dwivedi-Yu",
                "Roberto Dess\u0131",
                "Roberta Raileanu",
                "Maria Lomeli",
                "Luke Zettlemoyer",
                "Nicola Cancedda",
                "Thomas Scialom"
            ],
            "title": "Toolformer: Language models can teach themselves to use tools",
            "venue": "arXiv preprint arXiv:2302.04761,",
            "year": 2023
        },
        {
            "authors": [
                "Weijia Shi",
                "Sewon Min",
                "Michihiro Yasunaga",
                "Minjoon Seo",
                "Rich James",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Wen-tau Yih"
            ],
            "title": "Replug: Retrieval-augmented black-box language models",
            "venue": "arXiv preprint arXiv:2301.12652,",
            "year": 2023
        },
        {
            "authors": [
                "Christian Szegedy"
            ],
            "title": "A promising path towards autoformalization and general artificial intelligence",
            "venue": "In Intelligent Computer Mathematics: 13th International Conference,",
            "year": 2020
        },
        {
            "authors": [
                "Qingxiang Wang",
                "Chad Brown",
                "Cezary Kaliszyk",
                "Josef Urban"
            ],
            "title": "Exploration of neural machine translation in autoformalization of mathematics in mizar",
            "venue": "In Proceedings of the 9th ACM SIGPLAN International Conference on Certified Programs and Proofs,",
            "year": 2020
        },
        {
            "authors": [
                "Xuezhi Wang",
                "Jason Wei",
                "Dale Schuurmans",
                "Quoc Le",
                "Ed Chi",
                "Denny Zhou"
            ],
            "title": "Self-consistency improves chain of thought reasoning in language models",
            "venue": "arXiv preprint arXiv:2203.11171,",
            "year": 2022
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "brian ichter",
                "Fei Xia",
                "Ed H. Chi",
                "Quoc V Le",
                "Denny Zhou"
            ],
            "title": "Chain of thought prompting elicits reasoning in large language models",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Sean Welleck",
                "Jiacheng Liu",
                "Ximing Lu",
                "Hannaneh Hajishirzi",
                "Yejin Choi"
            ],
            "title": "Naturalprover: Grounded mathematical proof generation with language models, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Freek Wiedijk"
            ],
            "title": "Formal proof sketches",
            "venue": "Lecture notes in computer science,",
            "year": 2004
        },
        {
            "authors": [
                "Yuhuai Wu",
                "Albert Qiaochu Jiang",
                "Wenda Li",
                "Markus Rabe",
                "Charles Staats",
                "Mateja Jamnik",
                "Christian Szegedy"
            ],
            "title": "Autoformalization with large language models",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Zhuosheng Zhang",
                "Aston Zhang",
                "Mu Li",
                "Alex Smola"
            ],
            "title": "Automatic chain of thought prompting in large language models",
            "venue": "arXiv preprint arXiv:2210.03493,",
            "year": 2022
        },
        {
            "authors": [
                "Chuanyang Zheng",
                "Zhengying Liu",
                "Enze Xie",
                "Zhenguo Li",
                "Yu Li"
            ],
            "title": "Progressive-hint prompting improves reasoning in large language models, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Kunhao Zheng",
                "Jesse Michael Han",
                "Stanislas Polu"
            ],
            "title": "Minif2f: a cross-system benchmark for formal olympiad-level mathematics",
            "venue": "arXiv preprint arXiv:2109.00110,",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Recently, language models (Devlin et al., 2018; Brown et al., 2020; Chowdhery et al., 2022) have advanced significantly in many natural language processing tasks such as machine translation, question answering, summarization, etc. More recent large language models (LLMs) such as Minerva (Lewkowycz et al., 2022), GPT3.5 (OpenAI) and GPT4 (OpenAI, 2023) have also become increasingly capable of solving quantitative reasoning problems, ranging from middle school math word problems (Cobbe et al., 2021) to challenging high school mathematical competition problems (Hendrycks et al., 2021). By training or finetuning the model on high-quality natural language mathematical and scientific text, these LLMs can generate self-contained step-by-step solutions to quantitative reasoning problems without relying on external tools. However, just like human beings, the solutions LLMs generate are prone to simple calculation errors and unjustified logical leaps.\nFollowing Wang et al. (2022); Lewkowycz et al. (2022), one can sample many proposed solutions, extract the final answer from each, and select the most common answer. While aggregating answers like this improves performance at the problem level, the most common answer is sometimes wrong. Ideally, we would like a better heuristic to identify the correct answer. In fact, there are well-known techniques for computers to verify mathematical reasoning using formalization (Wiedijk, 2008). Those methods involve translating the problem and sometimes the solution into a formal language. Unfortunately this translation is difficult and time-intensive enough that formalization methods are less frequently leveraged.\nRecently it was discovered that large language models, with few-shot prompting, can automatically formalize natural mathematical language into formal languages (Agrawal et al., 2022; Azerbayev et al., 2023; Wu et al., 2022; Jiang et al., 2023; Wang et al., 2020). It is conjectured this capability arises because the training data for these LLMs contains sufficiently many examples of computer code and/or formal mathematics. While the translation is far from perfect and can fail on many complex statements, the correctness of the statements produced can be checked using formal theorem proving systems such as Isabelle (Nipkow et al., 2002) and Lean (de Moura et al., 2015).\nIn this paper, we demonstrate that in spite of its issues, autoformalization is already capable enough to identify correct answers for many quantitative reasoning problems. We call our method Don\u2019t Trust:"
        },
        {
            "heading": "Informal Statement",
            "text": ""
        },
        {
            "heading": "Informal Solution",
            "text": ""
        },
        {
            "heading": "Informal Reasoning Formal Reasoning",
            "text": "Verify (DTV) and provide an overview in Figure 1. Intuitively, instead of naively taking majority voting of all generated natural language solutions, we only aggregate solutions that can be verified with autoformalization. To carry out verification, both a formal statement and solution are needed. Therefore, we attempt to translate the plain text problem and solution into formal language using large language models. However, because language models can erroneously translate incorrect statements into correct ones, we develop both symbolic and neural filters to improve the statement translation reliability. Additionally, even with a correct formal statement, a formal solution directly translated from informal solution can fail to prove it. In particular, correct informal solutions (whether generated by humans or LLMs) skip low-level steps that are necessary for formal reasoning. To this end, we instead generate a formal solution sketch following Wiedijk (2004); Jiang et al. (2023) and employ an automated theorem prover (ATP) to fill in the low-level gaps.\nWe evaluate DTV on GSM8K (Cobbe et al., 2021), three subsets of MATH (Hendrycks et al., 2021) following prior work (Zheng et al., 2021), and MultiArith (Roy & Roth, 2016) datasets. The results show that our method consistently outperforms vanilla majority voting (Wang et al., 2022; Lewkowycz et al., 2022), with a more than 12% improvement on GSM8K. We demonstrate that DTV improves performance across various model sizes and categories. Additionally, we provide case studies on our method identifying correct answers as well as informal solutions. Finally, we discuss the limitations of our approach inherited from LLM and theorem proving environments."
        },
        {
            "heading": "2 BACKGROUND AND RELATED WORK",
            "text": "Informal quantitative reasoning with language models. A number of large language models such as PaLM (Chowdhery et al., 2022), Minerva (Lewkowycz et al., 2022), GPT3.5 (OpenAI) and GPT4 (OpenAI, 2023) have demonstrated their impressive quantitative reasoning abilities by pretraining or finetuning on mathematical and science data. To improve informal reasoning performance, chain-of-thought prompting (Wei et al., 2022) is typically used to encourage language models outputting intermediate reasoning steps before arriving at an answer. A diverse set of prompting methods have since been proposed for informal reasoning that tries to find better examples for prompting (Fu et al., 2023; Zhang et al., 2022) or better strategies for decoding from the model such as conditioning on references, multi-step decoding (Creswell et al., 2022; Zheng et al., 2023; Welleck et al., 2022; Khot et al., 2023). Cobbe et al. (2021) explores training informal verifiers to judge the correctness of reasoning. Additionally, to alleviate erroneous reasoning with language models, techniques that explore multi-sample consistency in the informal reasoning (Wang et al., 2022; Jung et al., 2022) have also been proposed. Our proposed autoformalization approach does not require training and is complementary to the above methods since we rely on the consistency of\na formal theorem proving system to identify correct reasoning paths and improve the reliability of large language model reasoning.\nAugmented language models. Besides elucidating rationales via chain-of-thought prompting, recent research has also been devoted to augmenting LLMs with external tools such as web search engine (Nakano et al., 2021; Lazaridou et al., 2022; Schick et al., 2023), external memory retrieval (Shi et al., 2023; Izacard et al., 2022) and programming-based calculators (Chen et al., 2022; Gao et al., 2022; Imani et al., 2023) to bolster their downstream performance and reduce hallucination. Our work augments language models with a formal theorem proving environment that goes beyond simple arithmetic (Chen et al., 2022; Imani et al., 2023) and Boolean logic (Jung et al., 2022).\nInteractive theorem proving and autoformalization. Modern interactive theorem provers such as Isabelle (Nipkow et al., 2002), Coq (Barras et al., 1997), and Lean (de Moura et al., 2015), provide an interactive environment to encode and mechanically verify mathematical proofs. Success stories with handcrafted proofs include the verification of industrial software systems (Klein et al., 2009) and research-level mathematics (Castelvecchi et al., 2021). Szegedy (2020) argues for automatically obtaining formal mathematics from their informal counterparts by translation i.e. autoformalization. Since then, researchers have shown the feasibility of autoformalization using neural networks in particular large language models (Agrawal et al., 2022; Azerbayev et al., 2023; Wu et al., 2022; Jiang et al., 2023; Wang et al., 2020). Our work which aims to ground LLM reasoning with formal theorem proving environments is distinct from Jiang et al. (2023) that improves theorem proving performance with LLM. DTV assumes a much more difficult but realistic setting where only natural language data is available. We are the first to demonstrate it is possible to automatically verify correct answer and informal solutions with autoformalization."
        },
        {
            "heading": "3 DON\u2019T TRUST: VERIFY",
            "text": "In this section, we describe our approach Don\u2019t Trust: Verify (DTV). Given a mathematical question and several solutions to it phrased in natural language, we assume the question has a well-defined answer and each solution contains an answer that may or may not be correct. Our goal is to understand which informal solutions are more likely to be correct. An overview of our method can be found in Figure 1."
        },
        {
            "heading": "3.1 STATEMENT FORMALIZATION",
            "text": "We begin by finding a formal statement that corresponds to the description of the informal problem statement. In many mainstream formal theorem proving environments such as Isabelle (Nipkow et al., 2002), Lean (de Moura et al., 2015), and Coq (Barras et al., 1997), a formal statement needs to be an assertion rather than a question. For example, a formal statement cannot be in the form of Is \u03c0 irrational? but rather Show that \u03c0 is irrational. Since quantitative reasoning problems typically ask for an answer rather than a proof, we first extract the answer from each proposed informal solution. To facilitate answer extraction, we include the following answer format in the few-shot prompt examples: The final answer is [placeholder]. and use regular expressions to extract the answer. In certain scenarios, extraction might be more challenging and not only restricted to the final answer. In this case, we could leverage various semantic parsing methods (Kamath & Das, 2018) and possibly use a language model to extract the informal reasoning as seen in Kojima et al. (2022) as well. The formal statement is then generated conditioned on both the plain text statement and the extracted answer. Following Wu et al. (2022), we leverage the few-shot learning capability of large language models to generate formal representation of the informal statements. Specifically, we provide a few informal-formal statement translation pairs and prompt the language model to complete the subsequent problem statement formalization."
        },
        {
            "heading": "3.2 SOLUTION FORMALIZATION AND VERIFICATION",
            "text": "The formalized statement itself does not tell us its correctness without a formal solution or proof. Because of this, we seek to generate a piece of formal solution to verify the statement correctness. Ideally, formal solution steps could be directly translated from natural language solution steps by sharing a similar level of abstraction. However, this could fail to prove the statement since formal reasoning steps require additional low-level justification than their natural language counterparts\neither written by human or LLMs. To address this issue, we generate a formal solution sketch following Wiedijk (2004); Jiang et al. (2023) (see Figure 1 (left)). The formal solution sketch contains high-level steps that are based on natural language counterparts and leaves low-level justifications to an automated theorem prover (indicated as [ATP] in Figure 1). Similar to statement formalization, we few-shot prompt a large language model to automatically generate such formal solution sketches.\nTo perform verification, we attempt to prove the formal statement with individual solution steps in a formal theorem proving environment. We leverage the consistency of the formal environment and its automated theorem prover to sequentially verify the steps. At any step, if the automated theorem prover fails to close the gap, we consider the formal statement not verified and hence incorrect. Besides, if there are still remaining goals to be proved after verifying every step, the formal statement is also treated as unverified. For example, in Figure 1 (right) formal solution #2, although both steps are correct, the formal statement cannot be proved since the original statement is an if and only if statement, and right-to-left direction is clearly false. If formal statement is considered verified, the corresponding informal solution and answer are also considered verified. To arrive at the final answer to the quantitative reasoning problem, we take the most common answer among the verified informal solutions. If no solution can be verified, we fallback to majority voting across all unverified solutions."
        },
        {
            "heading": "3.3 FILTERING UNFAITHFUL STATEMENT FORMALIZATIONS",
            "text": "For DTV to achieve good performance, it is crucial that informal and formal statements match precisely with each other. This is because an answer that is definitely incorrect to one problem could still be correct to an altered problem. For example, in the problem statement of Figure 1, if the constraint of x being positive is omitted, the answer x = \u00b12 would be correct instead. This is detrimental as the erroneously translated formal statements with incorrect answers can get verified by DTV. We call such statements unfaithful. To mitigate this issue, we propose to employ two types of filters to discard formal statements that are potentially unfaithful.\nVacuous statement filter. In our preliminary experiments, we found that vacuous formal statements form one common category of verified but unfaithful statement translations. By vacuous we mean that the formalized statement\u2019s hypotheses are contradictory, and can thus be used to prove anything by contradiction. The contradictory hypotheses are usually due to language model translation mistakes rather than the original problem itself being contradictory. For instance, a translated statement could assume a variable x simultaneously being x > 0 and x < 0, leading to a contradiction. To address this issue, we replace the formal statement goal with a simple statement of False and apply the automated theorem prover. Any formal statement that can be proved after this substitution is vacuous. We assume it is very unlikely that the natural language question is legitimately vacuous and we discard such vacuous formalizations.\nSelf-critique filter. There are other categories of unfaithful translations we find harder to identify symbolically with automated theorem provers. Formalization attempts generated by LLMs can sometimes be irrelevant to the problem or will even modify a formula to make the statement correct. Inspired by the fact that large language models are capable of critiquing their own output (Saunders et al., 2022), we explicitly ask the language model to decide whether the formal statement is a faithful translation of the informal statement. The likelihood of a yes versus no followed by a justification is compared. If the likelihood of no outweighs yes, the formal statement is considered unfaithful and discarded. It is worth noting that the self-critique filter can reject faithful formal statements as well as miss unfaithful formal statements. We empirically verify its effectiveness in the experiment section."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "4.1 DATASET AND FORMAL ENVIRONMENT",
            "text": "We evaluate DTV on three quantitative reasoning datasets: GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al., 2021) and MultiArith (Roy & Roth, 2016). These datasets have been used extensively to evaluate the reasoning capability of large language models (Cobbe et al., 2021; Wei et al., 2022; Lewkowycz et al., 2022; Zhang et al., 2022; OpenAI, 2023). GSM8K and MultiArith datasets contain grade-school arithmetic word problems. MATH dataset consists of high school mathematical competition problems drawn from AMC 10, AMC 12, AIME, etc that are more challenging. The problems in MATH have also been grouped into 7 categories: Prealgebra, Algebra, Number"
        },
        {
            "heading": "Generated Formal Statement:",
            "text": ""
        },
        {
            "heading": "Generated Formal Statement:",
            "text": ""
        },
        {
            "heading": "Generated Formal Statement:",
            "text": ""
        },
        {
            "heading": "Generated Formal Statement:",
            "text": "Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus. Following prior work (Zheng et al., 2021) which only draws problems from algebra and number theory categories, we evaluate on Prealgebra, Algebra and Number Theory subsets of the MATH dataset, which are most applicable in current theorem proving environments.\nSimilar to Wu et al. (2022); Jiang et al. (2023), we use Isabelle (Nipkow et al., 2002) as the formal theorem proving environment for the experiments since it has one of the largest formal corpora that enables formalization of challenging problems as well as powerful automated theorem provers to close low-level details in the formal solution sketches. We do not see fundamental limitations of our DTV framework applying to other formal languages such as Lean and Mizar and we leave this to future work. We adopt the Portal-to-ISAbelle (Jiang et al., 2021) interface to interact with Isabelle."
        },
        {
            "heading": "4.2 BASELINES AND EVALUATION PROTOCOL",
            "text": "We compare DTV with two baselines: single sample and multiple samples with majority voting (Wang et al., 2022; Lewkowycz et al., 2022). In single sample, for each problem, only one informal solution is sampled greedily with T = 0. For the latter, multiple informal solutions are sampled with a non-zero temperature. To evaluate the performance, we consider a problem being solved correctly if the most common answer matches the ground truth answer. For single sample baseline, the most common answer is the answer extracted from the solution whereas for multiple samples with majority vote, a grouping of solutions based on the answer is performed and the most frequent answer is chosen."
        },
        {
            "heading": "4.3 EXPERIMENTAL SETUP",
            "text": "Informal solution generation. We few-shot prompt large language models to generate n = 64 informal solutions per problem conditioned on the informal problem statement. We experiment with the 8B, 62B and 540B Minerva models (Lewkowycz et al., 2022). We use the default sampling configuration (T = 0.6, nucleus sampling (Holtzman et al., 2019) p = 0.95) reported in Lewkowycz et al. (2022) for Minerva models.\nStatement formalization. We prepare 25 paired (informal statement, Isabelle formal statement) examples for each of the three categories from MATH dataset, GSM8K and MultiAirth datasets as the candidates for few-shot demonstrations. When prompting the model to formalize an informal statement, we randomly draw and permute 10 examples to form the few-shot prompt. We always ensure the problem to be translated does not appear in the few-shot prompt. To reduce randomness of statement formalization process, for each informal solution, we perform 10 statement formalization attempts. All few-shot demonstration examples can be found in the supplementary materials.\nSolution formalization and verification. We further select 10 problems from each dataset and manually write complete formalization examples including both statements and solutions in the form of (informal statement, formal statement, informal solution, formal solution sketch) as examples for solution formalization. We randomly select 3 examples for each few-shot prompt. We query the language model once for solution formalization for each formal statement sample so that each informal solution is in total formalized 10\u00d71 times. We then use Isabelle to verify the formal solution sketch against the generated formal statement. Sledgehammer (Paulsson & Blanchette, 2012) along with 11 common tactics (auto, simp, blast, fastforce, force, eval, presburger, sos, arith, linarith, auto simp: field simps) are used to close low-level open conjectures in the formal solution sketch. We consider the formal solution and consequently the corresponding original informal solution correct if Sledgehammer and tactics succeed. An informal solution is kept if any of the 10 formalization attempts is verified successfully.\nUnfaithful statement filters. For the vacuous statement filter, we change the goal to be proved in generated formal statement to false. This is accomplished by replacing text after show clause to show false and check whether Sledgehammer and tactics can prove the modified formal statement. Statements proved this way are discarded due to being vacuous. To create the self-critique filter, we take 5 faithful and 5 unfaithful statement formalization examples generated by the model in the preliminary experiments as few-shot demonstrations. The unfaithful formalization examples are also accompanied by what errors it have. This few shot demonstration is kept fixed throughout the experiments. Generated formal statements are discarded if the likelihood of an answer of yes is lower than no for the language model."
        },
        {
            "heading": "4.4 EXPERIMENTAL RESULTS",
            "text": "Table 1 shows the percentage of problems solved for baselines and DTV. The informal solutions are generated with Minerva 62B model (Lewkowycz et al., 2022). We consider two autoformalization model choices: Minerva 62B and GPT3.5 (OpenAI). Due to the API inference time and cost, we only use GPT3.5 to generate the formal statements, which is the most crucial component in our approach. Minerva 62B is queried for both solution formalization and self-critique filter.\nIt can be seen that majority voting is a strong baseline that significantly improves the language model performance over single sample generation from an average of 42.8% to 61.5%, matching the observations in Wang et al. (2022); Lewkowycz et al. (2022). With autoformalization and\nverification in the formal environment, DTV outperforms majority voting baseline and achieves an average performance of 65.0% using the same Minerva 62B without any finetuning, suggesting the effectiveness of autoformalization. Since Minerva was mainly trained on natural language math content, its autoformalization capability could be limited. To this end, with the same informal solutions generated by Minerva, we switch the statement formalization model to GPT3.5. This leads to an even larger improvement of average solve rate to 68.2%. We observe that the boost in performance is consistent across all datasets and categories of problems. A larger improvement could potentially be obtained by switching solution formalization and self-critique filter to GPT3.5."
        },
        {
            "heading": "4.5 ABLATION",
            "text": "Size of informal solution models. Table 2 shows how baselines and DTV performs when varying the size of model that generates informal solutions. Specifically, we experiment with all three sizes of the Minerva model: 8B, 62B and 540B on Number theory, GSM8K and MultiArith problems. Not surprisingly, as the informal solution model scales up, both single sample and majority voting baselines improve their performance, suggesting larger models have generally stronger quantitative reasoning capability than smaller models. For DTV, we keep the formalization model the same as in Table 1 and do not vary its size. DTV consistently outperforms the baselines by a large margin across all three informal reasoning model sizes on all three datasets, indicating the scalability of DTV. The improvement is particularly significant on Minerva 8B, almost doubling its problem solve rate for GSM8K and number theory. Interestingly, the results also demonstrate that it is beneficial to autoformalize informal solutions from a larger model (Minerva 540B) even with a weaker model such as Minerva 62B, which opens up the possibility of reducing autoformalization runtime and cost.\nEffect of solution formalization and filters. For quantitative reasoning problems, it is possible to directly prove a translated formal statement with the automated theorem prover in absence of a\nstep-by-step solution sketch. This is because once the statement has been formalized correctly, the proposition to be proved is not \u201dfar\u201d from the assumption. Proving the proposition could just involve simplification and evaluation that the automated theorem prover can be capable of. We observe that by not asking DTV to formalize informal solutions, we can still achieve a strong problem solve rate (see Table 3). However, in this case only the informal solution answer is checked by DTV and correct formal statements that require elaborate formal solution steps will not be proved successfully. As shown in Table 3, both filters that are used to detect unfaithful formalization are beneficial for improving the problem solve rate."
        },
        {
            "heading": "4.6 QUALITATIVE ANALYSIS",
            "text": "DTV translates problems that majority voting fails. In Figure 2, we show four case study examples that majority voting fails to solve due to the correct answer not being the most common answer. DTV, however, successfully formalizes the informal statement and uses automated theorem prover to prove these propositions, making the correct answer as the majority answer. Each example statement formalized require clear and precise understanding of the informal statement and the language model is capable of formalizing diverse types of problems. For example, in the first example, the model needs to understand inclusion-exclusion relationship between different types of students and precisely translates them into the formal environment. In Appendix A.1, we provide more case study examples of statement formalization.\nDTV solves additional problems leveraging correct informal solution steps. In Figure 3, we showcase a number theory problem whose statement and informal solution are both formalized correctly by the model. The formal statement itself is beyond the capability of automated theorem prover and requires formal solution steps to solve it. It can be seen that by translating individual informal solution steps correctly, the problem is successfully solved. Additionally, we observe that for this problem, there are low-quality informal solutions that simply output a final answer without justification and their corresponding formal solutions cannot prove the proposition. This suggests that autoformalization could potentially be used to identify high-quality informal solutions. We provide more examples in Appendix A.2."
        },
        {
            "heading": "4.7 LIMITATIONS AND FUTURE WORK",
            "text": "One limitation of our approach is that it requires the theorem proving environment to support the problem domain in question. Currently, most environments are still limited in scope, a shortcoming that DTV naturally inherits. For example, Isabelle has limited support for formalizing geometric and probabilistic reasoning. Statements involving such arguments usually need to be built upon measure theory and advanced calculus, which is far beyond the math required to solve these high school problems. This is also the reason why these subsets in the MATH dataset have been excluded by prior work (Zheng et al., 2021) and us. As more support in such areas becomes available, we expect DTV to exhibit similar gains but a thorough evaluation is currently left as future work. Until then, for practical purposes, one could imagine training a simple classifier that predicts if a particular problem belongs to the domain that is well suited for DTV.\nThe second inherent limitation of DTV is the LLM\u2019s capability to translate a theorem, written in natural language, into a formal statement. Mistakes can be subtle. For example, 1 \u2212 2 + 3 could be erroneously formalized to (1::nat) - 2 + 3, which evaluates to 3 instead of 2 in Isabelle because a variable of the type nat cannot be negative (i.e. (1::nat) - 2 = 0). Future"
        },
        {
            "heading": "Generated Formal Statement and Formal Solution:",
            "text": "directions could explore employing more effective filters and leveraging reinforcement learning from human feedback (RLHF) (Bai et al., 2022) to further finetune language models on aligned mathematical formalizations directly."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "In this paper, we show that by leveraging the autoformalization capability of large language models through few-shot prompting, we can identify the correct answer among many informal solution samples generated by the same large language models. Our approach, Don\u2019t Trust: Verify, utilizes the internal consistency of formal theorem proving environments to check for correct answers. Besides, DTV is lightweight with no training or finetuning required. We demonstrate the feasibility and effectiveness of DTV by reaching state-of-the-art performance on GSM8K, subsets of MATH and MultiArith datasets. DTV consistently outperforms vanilla majority voting, the best previous approach, and leads to improvement across different model sizes from 8B, 64B and 540B. DTV is also complementary to different prompting methods such as Zheng et al. (2023); Fu et al. (2023) that only process reasoning in informal domain. We seek to combine these approaches with DTV for future work."
        },
        {
            "heading": "APPENDIX",
            "text": ""
        },
        {
            "heading": "A ADDITIONAL CASE STUDY EXAMPLES",
            "text": ""
        },
        {
            "heading": "A.1 FAITHFUL STATEMENT FORMALIZATION",
            "text": "Informal Statement: Gina has five pairs of white socks, three pairs of black socks, and two pairs of red socks. What percent of her socks are red? Show that it is 20%."
        },
        {
            "heading": "Generated Formal Statement:",
            "text": "theorem assumes \"card whites = 10\" and \"card blacks = 6\" and \"card reds = 4\" shows \"100 * (card reds) / (card whites + card blacks + card reds) = 20\"\nInformal Statement: What is the sum of the units digits of all the multiples of 3 between 0 and 50? Show that it is 78."
        },
        {
            "heading": "Generated Formal Statement:",
            "text": "theorem \"sum_list [x mod 10 . x \\<leftarrow> [0..50], 3 dvd x] = (78::nat)\"\nInformal Statement: Blake and Jenny each took four 100-point tests. Blake averaged 78 on the four tests. Jenny scored 10 points higher than Blake on the first test, 10 points lower than him on the second test, and 20 points higher on both the third and fourth tests. What is the difference between Jenny\u2019s average and Blake\u2019s average on these four tests? Show that it is 10."
        },
        {
            "heading": "Generated Formal Statement:",
            "text": "theorem fixes b1 b2 b3 b4 j1 j2 j3 j4 :: real assumes \"b1 + b2 + b3 + b4 = 4 * 78\" and \"j1 = b1 + 10\" and \"j2 = b2 - 10\" and \"j3 = b3 + 20\" and \"j4 = b4 + 20\" shows \"(j1 + j2 + j3 + j4)/4 - (b1 + b2 + b3 + b4)/4 = 10\"\nInformal Statement: What is the perimeter, in units, of a rhombus if its area is 120 square units and one diagonal is 10 units? Show that it is 52."
        },
        {
            "heading": "Generated Formal Statement:",
            "text": "theorem fixes d1 d2 :: real assumes \"d1 > 0\" and \"d2 > 0\" and \"d1 = 10\" and \"d1 * d2 / 2 = 120\" shows \"2 * sqrt ((d1/2)\u02c62 + (d2/2)\u02c62) * 2 = 52\"\nInformal Statement: Sam is carrying a 2 gallon bucket of milk to his house from the barn, which is 3 miles away from his house. However, the bucket has a leak in it. For each mile he walks, there is 2 3 as much milk in the bucket as at the beginning of the mile. How many gallons of milk will be in the bucket when Sam gets home? Show that it is 16\n27 ."
        },
        {
            "heading": "Generated Formal Statement:",
            "text": "theorem fixes x :: real assumes \"x = (2/3)\u02c63 * 2\" shows \"x = 16/27\"\nInformal Statement: Expand (2x5 + 3x2)(x4 \u2212 4x2 + 3x\u2212 8). Show that it is 2x9 \u2212 8x7 + 9x6 \u2212 16x5 \u2212 12x4 + 9x3 \u2212 24x2."
        },
        {
            "heading": "Generated Formal Statement:",
            "text": "theorem fixes x :: real shows \"(2 * x\u0302 5 + 3 * x\u0302 2) * (x\u0302 4 - 4 * x\u0302 2 + 3 * x - 8)\n= 2 * x\u0302 9 - 8 * x\u0302 7 + 9 * x\u0302 6 - 16 * x\u0302 5 - 12 * x\u0302 4 + 9 * x\u0302 3 - 24 * x\u0302 2\"\nInformal Statement: The function f(n) = n2 + n+ 17 for 0 \u2264 n \u2264 15 generates prime numbers. What is the value of f(10)\u2212 f(9)? Show that it is 20."
        },
        {
            "heading": "Generated Formal Statement:",
            "text": "theorem fixes n :: nat assumes \"\\<forall> n \\<in> {0..15}. prime (n\u0302 2 + n + 17)\" shows \"((10::nat)\u02c62 + 10 + 17) - ((9::nat)\u02c62 + 9 + 17) = 20\"\nInformal Statement: Reversing the digits in John\u2019s age results in his father\u2019s age. The positive difference between their ages is 27 years. If the sum of the digits in each age is 5, what is the number of years in John\u2019s father\u2019s age? Show that it is 41."
        },
        {
            "heading": "Generated Formal Statement:",
            "text": "theorem fixes j f j1 f1 :: nat assumes \"j = 10 * j1 + f1\" and \"f = 10 * f1 + j1\" and \"f - j = 27\" and \"j1 + f1 = 5\" shows \"f = 41\""
        },
        {
            "heading": "Generated Formal Statement:",
            "text": ""
        },
        {
            "heading": "Generated Formal Statement:",
            "text": ""
        },
        {
            "heading": "A.2 FAITHFUL SOLUTION FORMALIZATION",
            "text": "Informal Statement: Kailin\u2019s favorite restaurant has a special offer on a three-course meal. A meal consists of an appetizer, an entree and a dessert. The menu features five appetizers, eight entrees and four desserts. How many different three-course meals can be formed with these choices? Show that it is 160.\nGenerated Informal Solution: We have to choose one of the five appetizers, one of the eight entrees, and one of the four desserts. We have 5 choices for the appetizers, 8 choices for the entrees, and 4 choices for the dessert. Therefore, the number of possible meals is 5 \u00b7 8 \u00b7 4 = 160. Final Answer: The final answer is 160. I hope it is correct."
        },
        {
            "heading": "Generated Formal Statement and Formal Solution:",
            "text": "theorem assumes \"card appetizers = 5\" and \"card entrees = 8\" and \"card desserts = 4\" shows \"card (appetizers \\<times> entrees \\<times> desserts) = 160\" proof - (* We have to choose one of the five appetizers, one of the eight entrees, and one of the four desserts. *) have \"card (appetizers \\<times> entrees \\<times> desserts)\n= card appetizers * card entrees * card desserts\" by (simp add: card_cartesian_product)\n(* Therefore, the number of possible meals is 5 \u00b7 8 \u00b7 4 = 160. *) then have \"card (appetizers \\<times> entrees \\<times> desserts)\n= 5 * 8 * 4\" using assms by auto [ATP]\nalso have \"\\<dots> = 160\" by simp [ATP]\n(* Final Answer: The final answer is 160. I hope it is correct. *) finally show ?thesis by auto [ATP] qed\nInformal Statement: Simplify \u221a 192. Show that it is 8 \u221a 3.\nGenerated Informal Solution: We can factor out a perfect square from 192: 192 = 64 \u00b7 3 = 82 \u00b7 3. Thus, we have \u221a 192 = \u221a 82 \u00b7 3 = 8 \u221a 3. Final Answer: The final answer is 8 \u221a 3. I hope it is correct."
        },
        {
            "heading": "Generated Formal Statement and Formal Solution:",
            "text": "theorem fixes x :: real assumes \"x = 192\" shows \"sqrt x = 8 * sqrt (3::real)\" proof - (* We can factor out a perfect square from 192: 192 = 64 \u00b7 3 = 82 \u00b7 3. Thus, we have \u221a 192 = \u221a 82 \u00b7 3 = 8 \u221a 3. *)\nhave \"x = 64 * 3\" unfolding assms by simp [ATP] then have \"sqrt x = sqrt (64 * 3)\" by simp [ATP] also have \"\\<dots> = sqrt 64 * sqrt 3\" using real_sqrt_mult by blast [ATP] also have \"\\<dots> = 8 * sqrt 3\" by simp [ATP] finally show ?thesis by simp [ATP]\nqed\nInformal Statement: What is 3\u22124 \u00b7 3\u22128 \u00b7 314? Show that it is 9.\nGenerated Informal Solution: By the product rule for exponents, we have that\n3\u22124 \u00b7 3\u22128 \u00b7 314 = 3(\u22124)+(\u22128)+14 = 32 = 9 .\nFinal Answer: The final answer is 9. I hope it is correct."
        },
        {
            "heading": "Generated Formal Statement and Formal Solution:",
            "text": "theorem fixes x y z :: real assumes \"x = 3 powr (-4)\" and \"y = 3 powr (-8)\" and \"z = 3 powr 14\" shows \"x * y * z = 9\" proof - (* By the product rule for exponents, we have that *) have \"x * y * z = 3 powr (-4) * 3 powr (-8) * 3 powr 14\" by (smt (z3) assms(1) assms(2) assms(3)) [ATP] (* 3\u02c6{-4} \\cdot 3\u02c6{-8} \\cdot 3\u02c6{14} = 3\u02c6{(-4)+(-8)+14} = 3\u02c6{2} = \\boxed{9}. *) also have \"\\<dots> = 3 powr (-4 + -8 + 14)\" by eval [ATP]\nalso have \"\\<dots> = 3 powr 2\" by auto [ATP] also have \"\\<dots> = 9\" by auto [ATP]\n(* Final Answer: The final answer is 9. I hope it is correct. *) finally show ?thesis by auto [ATP] qed"
        },
        {
            "heading": "Generated Formal Statement and Formal Solution:",
            "text": ""
        },
        {
            "heading": "B FEW-SHOT PROMPT DATA",
            "text": "We provide few-shot prompts used in the experiments in the supplementary material zip file. Below is a few-shot prompt example for statement formalization.\nInformal Statement: Evaluate log2 64. Show that it is 6. Formal Statement: theorem shows \"log 2 64 = 6\"\nInformal Statement: What is the distance between the points with coordinates (\u22125, 5) and (5,\u22125)? Express your answer in simplest radical form. Show that it is 10 \u221a 2."
        },
        {
            "heading": "Formal Statement:",
            "text": "theorem fixes x1 x2 y1 y2 :: real assumes \"(x1, y1) = (-5, 5)\" and \"(x2, y2) = (5, -5)\" shows \"sqrt ((y2 - y1)\u02c62 + (x2 - x1)\u02c62) = 10 * sqrt(2)\"\nInformal Statement: If three flicks are equivalent to eight flecks, and six flocks are equivalent to four flecks, how many flocks are equivalent to 12 flicks? Show that it is 48. Formal Statement: theorem fixes flick fleck flock :: real assumes \"flick > 0\" and \"flock > 0\" and \"fleck > 0\" assumes \"3 * flick = 8 * fleck\" and \"6 * flock = 4 * fleck\" shows \"48 * flock = 12 * flick\"\n... <Informal statement to be formalized here>"
        },
        {
            "heading": "C ADDITIONAL DISCUSSION",
            "text": "Potential Improvement of Filters. In the main text, we discuss and experiment with one symbolic filter that filters out vacuous formal statements. The filter can be leveraged to detect unfaithful statement translations. It is also possible to expand this into a set of filters that attempt to detect trivial formal statements, which are very likely to be unfaithful translations. One type of trivial statements could have the same goal appearing in the assumption already, which makes it provable immediately following the assumption. Another type could have the left-hand side of the goal being the same as that of right-hand side. Implementing these filters could potentially further improve the performance at a diminishing rate.\nDTV helps filter incorrect answer solutions. Besides problem solve rate, we study to what extent DTV can reject incorrect answer solutions present in the informal solution samples. As seen in Table 4, without autoformalization, the portion of correct answer solutions from Minerva 62B is between 24.3% and 33.9% for different categories. By performing autoformalization and only considering informal solutions whose formal counterparts are proved, the percentage of correct answer solutions increase by more than 20%, indicating the effectiveness of our approach in identifying correct answer solutions. GPT3.5 is better than Minerva 62B at this, which also supports the fact that GPT3.5 leads to higher problem solve rate in Table 1."
        }
    ],
    "title": "DON\u2019T TRUST: VERIFY \u2013 GROUNDING LLM QUANTI-",
    "year": 2023
}