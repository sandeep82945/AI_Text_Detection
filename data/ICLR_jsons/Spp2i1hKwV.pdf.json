{
    "abstractText": "In-context learning is a promising paradigm that utilizes in-context examples as prompts for the predictions of large language models. These prompts are crucial for achieving strong performance. However, since the prompts need to be sampled from a large volume of annotated examples, finding the right prompt may result in high annotation costs. To address this challenge, this paper introduces an influence-driven selective annotation method that aims to minimize annotation costs while improving the quality of in-context examples. The essence of our method is to select a pivotal subset from a large-scale unlabeled data pool to annotate for the subsequent sampling of prompts. Specifically, a directed graph is first constructed to represent unlabeled data. Afterward, the influence of candidate unlabeled subsets is quantified with a diffusion process. A simple yet effective greedy algorithm for unlabeled data selection is lastly introduced. It iteratively selects the data if it provides a maximum marginal gain with respect to quantified influence. Compared with previous efforts on selective annotations, our influencedriven method works in an end-to-end manner, avoids an intractable explicit balance between data diversity and representativeness, and enjoys theoretical support. Experiments confirm the superiority of the proposed method on various benchmarks, achieving better performance under lower time consumption during subset selection. The project page is available at https://skzhang1.github.io/IDEAL/.",
    "authors": [
        {
            "affiliations": [],
            "name": "Shaokun Zhang"
        },
        {
            "affiliations": [],
            "name": "Xiaobo Xia"
        },
        {
            "affiliations": [],
            "name": "Zhaoqing Wang"
        },
        {
            "affiliations": [],
            "name": "Ling-Hao Chen"
        },
        {
            "affiliations": [],
            "name": "Jiale Liu"
        },
        {
            "affiliations": [],
            "name": "Qingyun Wu"
        },
        {
            "affiliations": [],
            "name": "Tongliang Liu"
        }
    ],
    "id": "SP:e2a16d3f8f08dfc106b9d630472438831f4f41af",
    "references": [
        {
            "authors": [
                "Ekin Aky\u00fcrek",
                "Dale Schuurmans",
                "Jacob Andreas",
                "Tengyu Ma",
                "Denny Zhou"
            ],
            "title": "What learning algorithm is in-context learning? investigations with linear models",
            "venue": "In ICLR,",
            "year": 2023
        },
        {
            "authors": [
                "Yu Bai",
                "Fan Chen",
                "Huan Wang",
                "Caiming Xiong",
                "Song Mei"
            ],
            "title": "Transformers as statisticians: Provable in-context learning with in-context algorithm selection",
            "venue": "arXiv preprint arXiv:2306.04637,",
            "year": 2023
        },
        {
            "authors": [
                "Jason Baldridge",
                "Miles Osborne"
            ],
            "title": "Active learning and the total cost of annotation",
            "venue": "In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2004
        },
        {
            "authors": [
                "Hritik Bansal",
                "Karthik Gopalakrishnan",
                "Saket Dingliwal",
                "Sravan Bodapati",
                "Katrin Kirchhoff",
                "Dan Roth"
            ],
            "title": "Rethinking the role of scale for in-context learning: An interpretability-based case study at 66 billion scale",
            "year": 2022
        },
        {
            "authors": [
                "Luisa Bentivogli",
                "Peter Clark",
                "Ido Dagan",
                "Danilo Giampiccolo"
            ],
            "title": "The fifth pascal recognizing textual entailment challenge",
            "year": 2009
        },
        {
            "authors": [
                "Sid Black",
                "Leo Gao",
                "Phil Wang",
                "Connor Leahy",
                "Stella Biderman"
            ],
            "title": "Gpt-neo: Large scale autoregressive language modeling with mesh-tensorflow",
            "year": 2021
        },
        {
            "authors": [
                "Pawe\u0142 Budzianowski",
                "Tsung-Hsien Wen",
                "Bo-Hsiang Tseng",
                "Inigo Casanueva",
                "Stefan Ultes",
                "Osman Ramadan",
                "Milica"
            ],
            "title": "Ga\u0161i\u0107. Multiwoz\u2013a large-scale multi-domain wizard-of-oz dataset for taskoriented dialogue modelling",
            "year": 2018
        },
        {
            "authors": [
                "Stephanie Chan",
                "Adam Santoro",
                "Andrew Lampinen",
                "Jane Wang",
                "Aaditya Singh",
                "Pierre Richemond",
                "James McClelland",
                "Felix Hill"
            ],
            "title": "Data distributional properties drive emergent in-context learning in transformers",
            "venue": "In NeurIPS,",
            "year": 2022
        },
        {
            "authors": [
                "Ting-Yun Chang",
                "Robin Jia"
            ],
            "title": "Data curation alone can stabilize in-context learning",
            "venue": "In ACL,",
            "year": 2023
        },
        {
            "authors": [
                "Mark Chen",
                "Jerry Tworek",
                "Heewoo Jun",
                "Qiming Yuan",
                "Henrique Ponde de Oliveira Pinto",
                "Jared Kaplan",
                "Harri Edwards",
                "Yuri Burda",
                "Nicholas Joseph",
                "Greg Brockman"
            ],
            "title": "Evaluating large language models trained on code",
            "venue": "arXiv preprint arXiv:2107.03374,",
            "year": 2021
        },
        {
            "authors": [
                "Yanda Chen",
                "Ruiqi Zhong",
                "Sheng Zha",
                "George Karypis",
                "He He"
            ],
            "title": "Meta-learning via language model in-context tuning",
            "venue": "In ACL,",
            "year": 2022
        },
        {
            "authors": [
                "Hyunsoo Cho",
                "Hyuhng Joon Kim",
                "Junyeob Kim",
                "Sang-Woo Lee",
                "Sang-goo Lee",
                "Kang Min Yoo",
                "Taeuk Kim"
            ],
            "title": "Prompt-augmented linear probing: Scaling beyond the limit of few-shot in-context learners",
            "venue": "In AAAI,",
            "year": 2023
        },
        {
            "authors": [
                "Christopher Clark",
                "Kenton Lee",
                "Ming-Wei Chang",
                "Tom Kwiatkowski",
                "Michael Collins",
                "Kristina Toutanova"
            ],
            "title": "Boolq: Exploring the surprising difficulty of natural yes/no questions",
            "venue": "In NAACLHLT,",
            "year": 2019
        },
        {
            "authors": [
                "Justin Cui",
                "Ruochen Wang",
                "Si Si",
                "Cho-Jui Hsieh"
            ],
            "title": "Scaling up dataset distillation to imagenet-1k with constant memory",
            "venue": "In ICML,",
            "year": 2023
        },
        {
            "authors": [
                "Shizhe Diao",
                "Pengcheng Wang",
                "Yong Lin",
                "Tong Zhang"
            ],
            "title": "Active prompting with chain-of-thought for large language models",
            "venue": "arXiv preprint arXiv:2302.12246,",
            "year": 2023
        },
        {
            "authors": [
                "William Dolan",
                "Chris Quirk",
                "Chris Brockett",
                "Bill Dolan"
            ],
            "title": "Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources",
            "venue": "In ACL,",
            "year": 2004
        },
        {
            "authors": [
                "Qingxiu Dong",
                "Lei Li",
                "Damai Dai",
                "Ce Zheng",
                "Zhiyong Wu",
                "Baobao Chang",
                "Xu Sun",
                "Jingjing Xu",
                "Zhifang Sui"
            ],
            "title": "A survey for in-context learning",
            "venue": "arXiv preprint arXiv:2301.00234,",
            "year": 2023
        },
        {
            "authors": [
                "Jiawei Du",
                "Yidi Jiang",
                "Vincent YF Tan",
                "Joey Tianyi Zhou",
                "Haizhou Li"
            ],
            "title": "Minimizing the accumulated trajectory error to improve dataset distillation",
            "venue": "In CVPR,",
            "year": 2023
        },
        {
            "authors": [
                "Sean P Engelson",
                "Ido Dagan"
            ],
            "title": "Minimizing manual annotation cost in supervised training from corpora",
            "venue": "arXiv preprint cmp-lg/9606030,",
            "year": 1996
        },
        {
            "authors": [
                "Vitaly Feldman",
                "Chiyuan Zhang"
            ],
            "title": "What neural networks memorize and why: Discovering the long tail via influence estimation",
            "venue": "In NeurIPS,",
            "year": 2020
        },
        {
            "authors": [
                "Tianyu Gao",
                "Adam Fisch",
                "Danqi Chen"
            ],
            "title": "Making pre-trained language models better few-shot learners",
            "venue": "arXiv preprint arXiv:2012.15723,",
            "year": 2020
        },
        {
            "authors": [
                "Matt Gardner",
                "Yoav Artzi",
                "Victoria Basmova",
                "Jonathan Berant",
                "Ben Bogin",
                "Sihao Chen",
                "Pradeep Dasigi",
                "Dheeru Dua",
                "Yanai Elazar",
                "Ananth Gottumukkala"
            ],
            "title": "Evaluating models\u2019 local decision boundaries via contrast sets",
            "venue": "In Findings of EMNLP,",
            "year": 2020
        },
        {
            "authors": [
                "Shivam Garg",
                "Dimitris Tsipras",
                "Percy S Liang",
                "Gregory Valiant"
            ],
            "title": "What can transformers learn in-context? a case study of simple function classes",
            "venue": "In NeurIPS,",
            "year": 2022
        },
        {
            "authors": [
                "Jacob Goldenberg",
                "Barak Libai",
                "Eitan Muller"
            ],
            "title": "Talk of the network: A complex systems look at the underlying process of word-of-mouth",
            "venue": "Marketing letters,",
            "year": 2001
        },
        {
            "authors": [
                "Frank Harary"
            ],
            "title": "Graph Theory (on Demand Printing Of 02787)",
            "year": 2018
        },
        {
            "authors": [
                "Muyang He",
                "Shuo Yang",
                "Tiejun Huang",
                "Bo Zhao"
            ],
            "title": "Large-scale dataset pruning with dynamic uncertainty",
            "venue": "arXiv preprint arXiv:2306.05175,",
            "year": 2023
        },
        {
            "authors": [
                "Lingxiao Huang",
                "Shaofeng H-C Jiang",
                "Jian Li",
                "Xuan Wu"
            ],
            "title": "Epsilon-coresets for clustering (with outliers) in doubling metrics",
            "venue": "In FOCS,",
            "year": 2018
        },
        {
            "authors": [
                "Lingxiao Huang",
                "Shaofeng H-C Jiang",
                "Jianing Lou",
                "Xuan Wu"
            ],
            "title": "Near-optimal coresets for robust clustering",
            "venue": "In ICLR,",
            "year": 2023
        },
        {
            "authors": [
                "Zhuo Huang",
                "Chang Liu",
                "Yinpeng Dong",
                "Hang Su",
                "Shibao Zheng",
                "Tongliang Liu"
            ],
            "title": "Machine vision therapy: Multimodal large language models can enhance visual robustness via denoising in-context learning",
            "venue": "arXiv preprint arXiv:2312.02546,",
            "year": 2023
        },
        {
            "authors": [
                "Zhuo Huang",
                "Xiaobo Xia",
                "Li Shen",
                "Bo Han",
                "Mingming Gong",
                "Chen Gong",
                "Tongliang Liu"
            ],
            "title": "Harnessing out-of-distribution examples via augmenting content and style",
            "venue": "In ICLR,",
            "year": 2023
        },
        {
            "authors": [
                "Zhuo Huang",
                "Miaoxi Zhu",
                "Xiaobo Xia",
                "Li Shen",
                "Jun Yu",
                "Chen Gong",
                "Bo Han",
                "Bo Du",
                "Tongliang Liu"
            ],
            "title": "Robust generalization against photon-limited corruptions via worst-case sharpness minimization",
            "venue": "In CVPR,",
            "year": 2023
        },
        {
            "authors": [
                "David Kempe",
                "Jon Kleinberg",
                "\u00c9va Tardos"
            ],
            "title": "Maximizing the spread of influence through a social network",
            "venue": "In SIGKDD, pp",
            "year": 2003
        },
        {
            "authors": [
                "Kiana Kheiri",
                "Hamid Karimi"
            ],
            "title": "Sentimentgpt: Exploiting gpt for advanced sentiment analysis and its departure from current machine learning",
            "venue": "arXiv preprint arXiv:2307.10234,",
            "year": 2023
        },
        {
            "authors": [
                "Hyuhng Joon Kim",
                "Hyunsoo Cho",
                "Junyeob Kim",
                "Taeuk Kim",
                "Kang Min Yoo",
                "Sang-goo Lee"
            ],
            "title": "Self-generated in-context learning: Leveraging auto-regressive language models as a demonstration generator",
            "venue": "arXiv preprint arXiv:2206.08082,",
            "year": 2022
        },
        {
            "authors": [
                "Young-Jun Lee",
                "Chae-Gyun Lim",
                "Ho-Jin Choi"
            ],
            "title": "Does gpt-3 generate empathetic dialogues? a novel in-context example selection method and automatic evaluation metric for empathetic dialogue generation",
            "venue": "In COLING,",
            "year": 2022
        },
        {
            "authors": [
                "Jens Lehmann",
                "Robert Isele",
                "Max Jakob",
                "Anja Jentzsch",
                "Dimitris Kontokostas",
                "Pablo N Mendes",
                "Sebastian Hellmann",
                "Mohamed Morsey",
                "Patrick Van Kleef",
                "S\u00f6ren Auer"
            ],
            "title": "Dbpedia\u2013a largescale, multilingual knowledge base extracted from wikipedia",
            "venue": "Semantic web,",
            "year": 2015
        },
        {
            "authors": [
                "Fangqi Li",
                "Chong Di",
                "Wenwen Xia"
            ],
            "title": "On the submodularity of diffusion models: Equivalent conditions and applications",
            "venue": "arXiv preprint arXiv:2002.00845,",
            "year": 2019
        },
        {
            "authors": [
                "Xiaonan Li",
                "Xipeng Qiu"
            ],
            "title": "Finding supporting examples for in-context learning",
            "venue": "arXiv preprint arXiv:2302.13539,",
            "year": 2023
        },
        {
            "authors": [
                "Yingcong Li",
                "Muhammed Emrullah Ildiz",
                "Dimitris Papailiopoulos",
                "Samet Oymak"
            ],
            "title": "Transformers as algorithms: Generalization and stability in in-context learning",
            "venue": "In ICML,",
            "year": 2023
        },
        {
            "authors": [
                "Yuchen Li",
                "Ju Fan",
                "Yanhao Wang",
                "Kian-Lee Tan"
            ],
            "title": "Influence maximization on social graphs: A survey",
            "venue": "IEEE Transactions on Knowledge and Data Engineering,",
            "year": 2018
        },
        {
            "authors": [
                "Yunshui Li",
                "Binyuan Hui",
                "Xiaobo Xia",
                "Jiaxi Yang",
                "Min Yang",
                "Lei Zhang",
                "Shuzheng Si",
                "Junhao Liu",
                "Tongliang Liu",
                "Fei Huang"
            ],
            "title": "One shot learning as instruction data prospector for large language models",
            "venue": "arXiv preprint arXiv:2312.10302,",
            "year": 2023
        },
        {
            "authors": [
                "Chin-Yew Lin"
            ],
            "title": "Rouge: A package for automatic evaluation of summaries",
            "venue": "In Text summarization branches out,",
            "year": 2004
        },
        {
            "authors": [
                "Hui Lin",
                "Jeff A Bilmes"
            ],
            "title": "How to select a good training-data subset for transcription: submodular active selection for sequences",
            "venue": "In Interspeech,",
            "year": 2009
        },
        {
            "authors": [
                "Jiachang Liu",
                "Dinghan Shen",
                "Yizhe Zhang",
                "Bill Dolan",
                "Lawrence Carin",
                "Weizhu Chen"
            ],
            "title": "What makes good in-context examples for gpt-3",
            "venue": "arXiv preprint arXiv:2101.06804,",
            "year": 2021
        },
        {
            "authors": [
                "Stuart Lloyd"
            ],
            "title": "Least squares quantization in pcm",
            "venue": "IEEE Transactions on Information Theory,",
            "year": 1982
        },
        {
            "authors": [
                "Noel Loo",
                "Ramin Hasani",
                "Mathias Lechner",
                "Daniela Rus"
            ],
            "title": "Dataset distillation with convexified implicit gradients",
            "venue": "In ICML,",
            "year": 2023
        },
        {
            "authors": [
                "Yao Lu",
                "Max Bartolo",
                "Alastair Moore",
                "Sebastian Riedel",
                "Pontus Stenetorp"
            ],
            "title": "Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity",
            "venue": "arXiv preprint arXiv:2104.08786,",
            "year": 2021
        },
        {
            "authors": [
                "Yuexiao Ma",
                "Taisong Jin",
                "Xiawu Zheng",
                "Yan Wang",
                "Huixia Li",
                "Yongjian Wu",
                "Guannan Jiang",
                "Wei Zhang",
                "Rongrong Ji"
            ],
            "title": "Ompq: Orthogonal mixed precision quantization",
            "venue": "In Proceedings of the AAAI conference on artificial intelligence,",
            "year": 2023
        },
        {
            "authors": [
                "Yuexiao Ma",
                "Huixia Li",
                "Xiawu Zheng",
                "Xuefeng Xiao",
                "Rui Wang",
                "Shilei Wen",
                "Xin Pan",
                "Fei Chao",
                "Rongrong Ji"
            ],
            "title": "Solving oscillation problem in post-training quantization through a theoretical perspective",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2023
        },
        {
            "authors": [
                "Andrew Maas",
                "Raymond E Daly",
                "Peter T Pham",
                "Dan Huang",
                "Andrew Y Ng",
                "Christopher Potts"
            ],
            "title": "Learning word vectors for sentiment analysis",
            "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,",
            "year": 2011
        },
        {
            "authors": [
                "Leland McInnes",
                "John Healy",
                "Nathaniel Saul",
                "Lukas"
            ],
            "title": "Grossberger. Umap: Uniform manifold approximation and projection",
            "venue": "The Journal of Open Source Software,",
            "year": 2018
        },
        {
            "authors": [
                "Sewon Min",
                "Xinxi Lyu",
                "Ari Holtzman",
                "Mikel Artetxe",
                "Mike Lewis",
                "Hannaneh Hajishirzi",
                "Luke Zettlemoyer"
            ],
            "title": "Rethinking the role of demonstrations: What makes in-context learning work",
            "year": 2022
        },
        {
            "authors": [
                "Swaroop Mishra",
                "Daniel Khashabi",
                "Chitta Baral",
                "Hannaneh Hajishirzi"
            ],
            "title": "Cross-task generalization via natural language crowdsourcing instructions",
            "venue": "In ACL,",
            "year": 2022
        },
        {
            "authors": [
                "Shashi Narayan",
                "Shay B Cohen",
                "Mirella Lapata"
            ],
            "title": "Don\u2019t give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization",
            "year": 2018
        },
        {
            "authors": [
                "Adam Paszke",
                "Sam Gross",
                "Francisco Massa",
                "Adam Lerer",
                "James Bradbury",
                "Gregory Chanan",
                "Trevor Killeen",
                "Zeming Lin",
                "Natalia Gimelshein",
                "Luca Antiga"
            ],
            "title": "Pytorch: An imperative style, highperformance deep learning library",
            "venue": "NeurIPS,",
            "year": 2019
        },
        {
            "authors": [
                "Nils Reimers",
                "Iryna Gurevych"
            ],
            "title": "Sentence-bert: Sentence embeddings using siamese bertnetworks",
            "venue": "arXiv preprint arXiv:1908.10084,",
            "year": 2019
        },
        {
            "authors": [
                "David Rolnick",
                "Jonathan Weed"
            ],
            "title": "Greedy maximization of submodular functions",
            "year": 2014
        },
        {
            "authors": [
                "Ohad Rubin",
                "Jonathan Herzig",
                "Jonathan Berant"
            ],
            "title": "Learning to retrieve prompts for in-context learning",
            "venue": "In NAACL,",
            "year": 2022
        },
        {
            "authors": [
                "Ozan Sener",
                "Silvio Savarese"
            ],
            "title": "Active learning for convolutional neural networks: A core-set approach",
            "venue": "In ICLR,",
            "year": 2018
        },
        {
            "authors": [
                "Bingkang Shi",
                "Xiaodan Zhang",
                "Dehan Kong",
                "Yulei Wu",
                "Zongzhen Liu",
                "Honglei Lyu",
                "Longtao Huang"
            ],
            "title": "General phrase debiaser: Debiasing masked language models at a multi-token level",
            "venue": "arXiv preprint arXiv:2311.13892,",
            "year": 2023
        },
        {
            "authors": [
                "Seongjin Shin",
                "Sang-Woo Lee",
                "Hwijeen Ahn",
                "Sungdong Kim",
                "HyoungSeok Kim",
                "Boseop Kim",
                "Kyunghyun Cho",
                "Gichang Lee",
                "Woomyoung Park",
                "Jung-Woo Ha"
            ],
            "title": "On the effect of pretraining corpora on in-context learning by a large-scale language model",
            "year": 2022
        },
        {
            "authors": [
                "Seungjae Shin",
                "Heesun Bae",
                "Donghyeok Shin",
                "Weonyoung Joo",
                "Il-Chul Moon"
            ],
            "title": "Loss-curvature matching for dataset selection and condensation",
            "venue": "In AISTATS,",
            "year": 2023
        },
        {
            "authors": [
                "Rion Snow",
                "Brendan O\u2019connor",
                "Dan Jurafsky",
                "Andrew Y Ng"
            ],
            "title": "Cheap and fast\u2013but is it good? evaluating non-expert annotations for natural language tasks",
            "venue": "In Proceedings of the 2008 conference on empirical methods in natural language processing,",
            "year": 2008
        },
        {
            "authors": [
                "Richard Socher",
                "Alex Perelygin",
                "Jean Wu",
                "Jason Chuang",
                "Christopher D Manning",
                "Andrew Y Ng",
                "Christopher Potts"
            ],
            "title": "Recursive deep models for semantic compositionality over a sentiment treebank",
            "venue": "In EMNLP,",
            "year": 2013
        },
        {
            "authors": [
                "Ben Sorscher",
                "Robert Geirhos",
                "Shashank Shekhar",
                "Surya Ganguli",
                "Ari Morcos"
            ],
            "title": "Beyond neural scaling laws: beating power law scaling via data pruning",
            "venue": "In NeurIPS,",
            "year": 2022
        },
        {
            "authors": [
                "Aarohi Srivastava",
                "Abhinav Rastogi",
                "Abhishek Rao",
                "Abu Awal Md Shoeb",
                "Abubakar Abid",
                "Adam Fisch",
                "Adam R Brown",
                "Adam Santoro",
                "Aditya Gupta",
                "Adri\u00e0 Garriga-Alonso"
            ],
            "title": "Beyond the imitation game: Quantifying and extrapolating the capabilities of language models",
            "venue": "Transactions on Machine Learning Research,",
            "year": 2023
        },
        {
            "authors": [
                "Hongjin Su",
                "Jungo Kasai",
                "Chen Henry Wu",
                "Weijia Shi",
                "Tianlu Wang",
                "Jiayi Xin",
                "Rui Zhang",
                "Mari Ostendorf",
                "Luke Zettlemoyer",
                "Noah A Smith"
            ],
            "title": "Selective annotation makes language models better few-shot learners",
            "venue": "In ICLR,",
            "year": 2023
        },
        {
            "authors": [
                "Mariya Toneva",
                "Alessandro Sordoni",
                "Remi Tachet des Combes",
                "Adam Trischler",
                "Yoshua Bengio",
                "Geoffrey J Gordon"
            ],
            "title": "An empirical study of example forgetting during deep neural network learning",
            "venue": "In ICLR,",
            "year": 2019
        },
        {
            "authors": [
                "Ben Wang",
                "Aran Komatsuzaki"
            ],
            "title": "GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/mesh-transformer-jax",
            "year": 2021
        },
        {
            "authors": [
                "Boshi Wang",
                "Xiang Deng",
                "Huan Sun"
            ],
            "title": "Iteratively prompt pre-trained language models for chain of thought",
            "venue": "In EMNLP,",
            "year": 2022
        },
        {
            "authors": [
                "Jindong Wang",
                "Cuiling Lan",
                "Chang Liu",
                "Yidong Ouyang",
                "Tao Qin",
                "Wang Lu",
                "Yiqiang Chen",
                "Wenjun Zeng",
                "Philip Yu"
            ],
            "title": "Generalizing to unseen domains: A survey on domain generalization",
            "venue": "IEEE Transactions on Knowledge and Data Engineering,",
            "year": 2022
        },
        {
            "authors": [
                "Tongzhou Wang",
                "Jun-Yan Zhu",
                "Antonio Torralba",
                "Alexei"
            ],
            "title": "A Efros. Dataset distillation",
            "venue": "arXiv preprint arXiv:1811.10959,",
            "year": 2018
        },
        {
            "authors": [
                "Yizhong Wang",
                "Swaroop Mishra",
                "Pegah Alipoormolabashi",
                "Yeganeh Kordi",
                "Amirreza Mirzaei",
                "Anjana Arunkumar",
                "Arjun Ashok",
                "Arut Selvan Dhanasekaran",
                "Atharva Naik",
                "David Stap"
            ],
            "title": "Super-naturalinstructions: Generalization via declarative instructions on",
            "venue": "EMNLP,",
            "year": 2022
        },
        {
            "authors": [
                "Jason Wei",
                "Maarten Bosma",
                "Vincent Y Zhao",
                "Kelvin Guu",
                "Adams Wei Yu",
                "Brian Lester",
                "Nan Du",
                "Andrew M Dai",
                "Quoc V Le"
            ],
            "title": "Finetuned language models are zero-shot learners",
            "venue": "In ICLR,",
            "year": 2022
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "Fei Xia",
                "Ed Chi",
                "Quoc V Le",
                "Denny Zhou"
            ],
            "title": "Chain-of-thought prompting elicits reasoning in large language models",
            "venue": "In NeurIPS,",
            "year": 2022
        },
        {
            "authors": [
                "Adina Williams",
                "Nikita Nangia",
                "Samuel R Bowman"
            ],
            "title": "A broad-coverage challenge corpus for sentence understanding through inference",
            "venue": "arXiv preprint arXiv:1704.05426,",
            "year": 2017
        },
        {
            "authors": [
                "Thomas Wolf",
                "Lysandre Debut",
                "Victor Sanh",
                "Julien Chaumond",
                "Clement Delangue",
                "Anthony Moi",
                "Pierric Cistac",
                "Tim Rault",
                "R\u00e9mi Louf",
                "Morgan Funtowicz"
            ],
            "title": "Huggingface\u2019s transformers: State-of-the-art natural language processing",
            "year": 1910
        },
        {
            "authors": [
                "Zhiyong Wu",
                "Yaoxiang Wang",
                "Jiacheng Ye",
                "Lingpeng Kong"
            ],
            "title": "Self-adaptive In-context Learning",
            "year": 2022
        },
        {
            "authors": [
                "Xiaobo Xia",
                "Tongliang Liu",
                "Bo Han",
                "Mingming Gong",
                "Jun Yu",
                "Gang Niu",
                "Masashi Sugiyama"
            ],
            "title": "Sample selection with uncertainty of losses for learning with noisy labels",
            "venue": "In ICLR,",
            "year": 2022
        },
        {
            "authors": [
                "Xiaobo Xia",
                "Jiale Liu",
                "Jun Yu",
                "Xu Shen",
                "Bo Han",
                "Tongliang Liu"
            ],
            "title": "Moderate coreset: A universal method of data selection for real-world data-efficient deep learning",
            "venue": "In ICLR,",
            "year": 2023
        },
        {
            "authors": [
                "Xiaobo Xia",
                "Jiale Liu",
                "Shaokun Zhang",
                "Qingyun Wu",
                "Tongliang Liu"
            ],
            "title": "Coreset selection with prioritized multiple objectives",
            "venue": "arXiv preprint arXiv:2311.08675,",
            "year": 2023
        },
        {
            "authors": [
                "Sang Michael Xie",
                "Aditi Raghunathan",
                "Percy Liang",
                "Tengyu Ma"
            ],
            "title": "An explanation of in-context learning as implicit bayesian inference",
            "venue": "In ICLR,",
            "year": 2022
        },
        {
            "authors": [
                "Shuo Yang",
                "Zeke Xie",
                "Hanyu Peng",
                "Min Xu",
                "Mingming Sun",
                "Ping Li"
            ],
            "title": "Dataset pruning: Reducing training data by examining generalization influence",
            "venue": "In ICLR,",
            "year": 2023
        },
        {
            "authors": [
                "Junjie Ye",
                "Xuanting Chen",
                "Nuo Xu",
                "Can Zu",
                "Zekai Shao",
                "Shichun Liu",
                "Yuhan Cui",
                "Zeyang Zhou",
                "Chao Gong",
                "Yang Shen"
            ],
            "title": "A comprehensive capability analysis of gpt-3 and gpt-3.5 series models",
            "venue": "arXiv preprint arXiv:2303.10420,",
            "year": 2023
        },
        {
            "authors": [
                "Kang Min Yoo",
                "Junyeob Kim",
                "Hyuhng Joon Kim",
                "Hyunsoo Cho",
                "Hwiyeol Jo",
                "Sang-Woo Lee",
                "Sanggoo Lee",
                "Taeuk Kim"
            ],
            "title": "Ground-truth labels matter: A deeper look into input-label demonstrations",
            "year": 2022
        },
        {
            "authors": [
                "John M Zelle",
                "Raymond J Mooney"
            ],
            "title": "Learning to parse database queries using inductive logic programming",
            "venue": "In Proceedings of the National Conference on Artificial Intelligence,",
            "year": 1996
        },
        {
            "authors": [
                "Rowan Zellers",
                "Ari Holtzman",
                "Yonatan Bisk",
                "Ali Farhadi",
                "Yejin Choi"
            ],
            "title": "Hellaswag: Can a machine really finish your sentence",
            "year": 2019
        },
        {
            "authors": [
                "Ruiqi Zhang",
                "Spencer Frei",
                "Peter L Bartlett"
            ],
            "title": "Trained transformers learn linear models in-context",
            "venue": "arXiv preprint arXiv:2306.09927,",
            "year": 2023
        },
        {
            "authors": [
                "Shaokun Zhang",
                "Xiawu Zheng",
                "Chenyi Yang",
                "Yuchao Li",
                "Yan Wang",
                "Fei Chao",
                "Mengdi Wang",
                "Shen Li",
                "Jun Yang",
                "Rongrong Ji"
            ],
            "title": "You only compress once: Towards effective and elastic bert compression via exploit-explore stochastic nature gradient",
            "venue": "arXiv preprint arXiv:2106.02435,",
            "year": 2021
        },
        {
            "authors": [
                "Shaokun Zhang",
                "Yiran Wu",
                "Zhonghua Zheng",
                "Qingyun Wu",
                "Chi Wang"
            ],
            "title": "Hypertime: Hyperparameter optimization for combating temporal distribution shifts",
            "venue": "arXiv preprint arXiv:2305.18421,",
            "year": 2023
        },
        {
            "authors": [
                "Bo Zhao",
                "Konda Reddy Mopuri",
                "Hakan Bilen"
            ],
            "title": "Dataset condensation with gradient matching",
            "year": 2024
        },
        {
            "authors": [
                "2022 Xie et al",
                "2022 Shin et al",
                "2023a Zhang et al",
                "2023 Bai et al",
                "Huang"
            ],
            "title": "2023b). A series of works attempts to revise, enhance, and understand ICL, which include but are not limited to prompt tuning (Kim et al., 2022; Wang et al., 2022a; Mishra et al., 2022), analyzing intrinsic mechanism (Bansal et al., 2022",
            "venue": "(Dong et al.,",
            "year": 2022
        },
        {
            "authors": [
                "2022 mains (Chen et al",
                "2022 Lee et al",
                "Cho"
            ],
            "title": "2023), and etc. Despite in-context learning has shown impressive performance in various domains, its efficacy is sensitive to the selection of in-context learning examples (Zhao et al., 2021b; Lu et al., 2021)",
            "year": 2021
        },
        {
            "authors": [
                "Liu et al",
                "Gao"
            ],
            "title": "2020) Alternatively, some methods aim to find a set of examples once for all queries on the same task (Li & Qiu, 2023; Diao et al., 2023). Specifically, (Wu et al., 2022) formally defines the problem of self-adaptive In-context learning, which aims to search for the best In-context learning examples and corresponding order for each input",
            "year": 2023
        },
        {
            "authors": [
                "a. Input: Lars Nielsen"
            ],
            "title": "born 3 November 1960 in Copenhagen) is a Danish rower",
            "year": 1960
        }
    ],
    "sections": [
        {
            "text": "In-context learning is a promising paradigm that utilizes in-context examples as prompts for the predictions of large language models. These prompts are crucial for achieving strong performance. However, since the prompts need to be sampled from a large volume of annotated examples, finding the right prompt may result in high annotation costs. To address this challenge, this paper introduces an influence-driven selective annotation method that aims to minimize annotation costs while improving the quality of in-context examples. The essence of our method is to select a pivotal subset from a large-scale unlabeled data pool to annotate for the subsequent sampling of prompts. Specifically, a directed graph is first constructed to represent unlabeled data. Afterward, the influence of candidate unlabeled subsets is quantified with a diffusion process. A simple yet effective greedy algorithm for unlabeled data selection is lastly introduced. It iteratively selects the data if it provides a maximum marginal gain with respect to quantified influence. Compared with previous efforts on selective annotations, our influencedriven method works in an end-to-end manner, avoids an intractable explicit balance between data diversity and representativeness, and enjoys theoretical support. Experiments confirm the superiority of the proposed method on various benchmarks, achieving better performance under lower time consumption during subset selection. The project page is available at https://skzhang1.github.io/IDEAL/."
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "In-context learning (ICL) entails presenting a small set of examples with demonstrations as prompts (called in-context examples) to large language models (LLMs), before making predictions on test inputs (Wei et al., 2022a; Min et al., 2022; Akyu\u0308rek et al., 2023). This emerging few-shot learning paradigm is an appealing alternative to supervised fine-tuning as it can avoid heavy parameter updates of language models while improving accuracy (Liu et al., 2021; Yoo et al., 2022).\nRecent studies indicate that obtaining prompts from a vast collection of annotated examples is crucial to achieving strong performance (Rubin et al., 2022). Notably, these studies have illuminated the substantial performance improvements when retrieving analogous examples (under specific embedding criteria) as in-context examples tailored for each individual test input. Since different test scenarios need distinct in-context examples, and each of them is equipped with its pertinent annotations, the necessity of a large volume of annotated examples is emphasized (Su et al., 2023). However, obtaining large-scale annotated examples for ICL requires substantial manpower and financial resources (Baldridge & Osborne, 2004; Engelson & Dagan, 1996; Snow et al., 2008). This is because humans not only need to annotate the true label for each example but also need to provide the example demonstration in the annotation process (Wei et al., 2022b).\n\u2217Equal contributions. \u2020Corresponding authors.\nTo reduce the annotation cost, the previous effort Vote-k (Su et al., 2023) made attempts by proposing to select a diverse and representative subset from a large-scale unlabeled data pool to annotate. Particularly, Vote-k initially selects a small portion of data for diversity and annotates them manually. Then, these annotated data act as prompts for predictions on all other unlabeled data, and choose the remaining ones that need to be annotated, based on diverse confidence scores. However, despite its strong performance in empirical evaluations, Vote-k is still unsatisfactory in practice. We detail the issues from three aspects. (1) The data selection procedure of Vote-k is not end-to-end. This results in inconvenience, increased processing complexity, and added inference costs due to the predictions on unlabeled data. (2) Diversity and representativeness need to be balanced carefully (Su et al., 2023). Highlighting diversity in data selection is crucial for comprehensive coverage, but may sacrifice representativeness by overlooking exemplary data. Besides, the excessive emphasis on diversity of Vote-k causes the selection of outliers (see evidence in Appendix C.2). (3) Vote-k lacks theoretical guarantees, making it challenging to assess the algorithm\u2019s reliability in realistic tasks and constraining its practical utility.\nIn this paper, to minimize annotation costs for ICL and address the issues of existing work, an innovative data selection method is introduced, where we utilize influence-driven selective annotations to empower in-context learners (IDEAL). In essence, IDEAL aims to identify a subset of data that acts as a proxy and closely approximates the vast unlabeled dataset. Once annotated, these selected data can be considered a viable substitute for the large annotated examples in subsequent ICL tasks. In further detail, our method works in an unsupervised and end-to-end manner. We first construct a directed graph, where its vertices represent unlabeled data and its edges bridge different data based on their similarities. Inspired by influence maximization that aims to select a vertex set at key positions in social graphs (Li et al., 2018), we then propose to quantify the influence of each candidate unlabeled subset in our constructed graph, through a classic independent-cascade diffusion model illustrated in Figure 2. To find the subset with high influence, a simple greedy algorithm for unlabeled data selection is introduced. The algorithm does not need a delicate trade-off between diversity and representativeness. Instead, it iteratively selects a vertex if it provides a maximum marginal gain to the influence metric, until the selection is completed based on the annotation budget.\nTheoretically, under the influence-driven selective paradigm, we provide the lower bound for the subset influence selected by our method, demonstrating it is at least as large as a certain proportion of the influence of the optimal solution. Empirically, we conduct comprehensive experiments over 9 datasets across diverse tasks (covering classification, commonsense reasoning, dialogue, and text/code generation). Various LLMs and prompt retrieval technologies are included in evaluations. Experimental results demonstrate that our IDEAL can achieve better performance than Vote-k in 17 out of 18 cases in the experiments, with only 13% time consumption during subset selection. This creates a strong baseline of selective annotations for follow-up research. Source codes have been attached for the reproducibility of results."
        },
        {
            "heading": "2 METHODOLOGY",
            "text": "In this section, to reduce the annotation cost of ICL, a framework of influence-driven selective annotations is formulated. We discuss how examples should be selected to annotate, leading to better in-context learners for LLMs."
        },
        {
            "heading": "2.1 PROBLEM SETUP",
            "text": "We begin by defining notations and setting up the research problem. Specifically, LLMs perform in-context learning tasks based on a task-specific prompt Z = [z1, . . . , zc], where each zi represents one example (xi, yi) consisting of the instance xi and label yi, with c examples in total. LLMs generate the prediction for one test input xtest conditioned on the prompt Z followed by xtest, i.e., ytest = argmaxy\u2208CP (y|Z,xtest), where C denotes the label space. As each prompt needs distinct annotations, the importance of having a substantial number of annotated examples is stressed, resulting in huge annotation costs. This motivates us to investigate selective annotations.\nGiven a pool of unlabeled instances Du = {xi}ni=1, where n is the number of unlabeled instances, the aim of selective annotations is to select a subset Su \u2282 Du to make manual annotations, such that performing ICL using prompts retrieved from the selected subset can yield good performance on an unseen test set Dtest. The size of Su is controlled by the annotation budget m, i.e., |Su| = m."
        },
        {
            "heading": "2.2 INFLUENCE-DRIVEN SELECTIVE ANNOTATIONS",
            "text": "Overview. For selective annotations in ICL, we need to identify a subset that approximates vast unlabeled data. Therefore, quantifying the coverage of each candidate subset is critical. To achieve this, we construct a directed graph using the embeddings of unlabeled data and portray their relationships using the edges in the graph. We then quantify the influence of each candidate subset in the constructed graph. An information diffusion model is used for this purpose. Through the information diffusion model to quantifying the influence of each candidate subset, we avoid the delicate trade-off between diversity and representativeness. After the quantification, we can search the subset with maximum influence, which most closely approximates the unlabeled data. Below we detail the above procedure step by step.\nConstructing the directed graph. We first compute a vector embedding for each unlabeled instance using Sentence-BERT (Reimers & Gurevych, 2019)1. The obtained embeddings are employed to build a directed graph G = (V,E,P), where the vertices V = {vi}ni=1 represent the embeddings of the unlabeled instances, E denotes the set of edges in the graph, and P denotes the set of weights assigned to edges. In more detail, for each vertex v \u2208 V, we connect it to its k nearest successors2 in terms of the cosine similarity between the embeddings and then get E. For the edge (v,u) \u2208 E that connects v and its successor u, we assign the weight p(v,u) = cos(v,u)/ \u2211 z\u2208N (v,k) cos(v, z) with p \u2208 P, where N (v, k) represents the set including k nearest successors of v, and cos(\u00b7, \u00b7) is a function that calculates the cosine similarity of two embeddings. The constructed graph depicts the relationships between unlabeled examples in terms of the embedding similarity.\nQuantifying subset influence. Here we propose to quantify each candidate subset within the constructed graph, which is detailed in Algorithm 1. Specifically, given the constructed graph G and a candidate subset S, the quantification algorithm simulates the progression of information diffusion originating from S. The number of influenced vertices can be considered as a measure of the influence of the candidate subset. In other words, the subset that influences more vertices within the graph can provide a better approximation of the vast unlabeled data. The diffusion process unfolds discretely, progressing through multiple steps. At the beginning, the subset S is activated. Then at each step, each vertex v activates its successors that remained inactive in the last step with a probability defined by p(v,u). The activation can be conceptualized as a coin toss where the outcome is determined by the head probability p(v,u). If the result is the head, the vertex v becomes activated; otherwise, it remains inactive. Starting from S, the diffusion terminates when no further vertex can be activated in the graph. Lastly, we quantify the influence of the set with the number of activated vertices, where a larger number corresponds to greater influence. In order to get a stable result, we\n1https://huggingface.co/sentence-transformers/all-mpnet-base-v2. 2In graph theory (Harary, 2018), a vertex u is the successor of a vertex v if it is at the end of an outgoing\ndirected edge (v,u).\nAlgorithm 1: Subset influence quantification. Input : Directed graph G = (V,E,P), subset S. Output: Number of influenced vertices by S in G. Sactive \u2190 S, Snew \u2190 \u2205, L = 0; while Sactive \u0338= \u2205 do\nfor each node v in Sactive do for each successor u of v in G do\nif u not in S then Generate random number \u03c4 \u2208 [0, 1]; if \u03c4 \u2264 p(v,u) then S \u2190 S \u222a u; Snew \u2190 Snew \u222a u;\nSactive \u2190 Snew; L\u2190 L+ |Snew|; Snew \u2190 \u2205; return L.\nAlgorithm 2: Searching the subset with maximum influence. Input: The directed graph G = (V,E,P), the annotation budget m. Result: The set Su that includes m examples to annotate. Initialize S0 \u2190 \u2205, t = 0; while t < m do\nvt \u2190 argmaxv\u2208V\\St fG(St \u222a {v}); St+1 \u2190 St \u222a vt; t\u2190 t+ 1;\nObtain Su with Sm using the correspondence between embeddings and instances; return Su.\nrepeat this process ten times and take the average influence. To help understand the procedure of Algorithm 1, we provide an illustration as shown in Figure 2. For convenience, we express Algorithm 1 as an influence function fG(S) for the graph G that takes example set S as inputs, and returns the number of activated vertices L.\nSearching the subset with maximum influence. We exploit a simple yet effective greedy algorithm (Kempe et al., 2003) to search the subset with maximum influence, which is illustrated in Algorithm 2. Specifically, the algorithm is initialized with an empty set, and iteratively involves an instance if it can provide the maximum marginal gain to the influence function. The search algorithm terminates when the selected subset meets the annotation budget. Finally, we achieve the set Su that includesm examples to annotate, using the correspondence between embeddings and instances. It is worth mentioning that this searching process aims to maximize the influence of the whole selected subset rather than considering each example separately. This is because combining all the individual high-impact examples together does not necessarily achieve the highest-impact subset."
        },
        {
            "heading": "2.3 PROMPT RETRIEVAL",
            "text": "After the above influence-driven selective annotations, the subset Su is achieved. By making manual annotations on Su, a set of annotated examples is obtained. We can then retrieve examples from the annotated set as in-context examples for each test input. Following previous studies (Liu et al., 2021; Su et al., 2023), we will calculate embeddings for all annotated examples using SentenceBERT (Reimers & Gurevych, 2019) and identify the most similar instances to each test input based on the cosine similarity. Notice that, the proposed method is agnostic to prompt retrieval methods. As demonstrated in \u00a74.3.3, our method can be combined with any other prompt retrieval technologies. Better prompt retrieval technologies can further boost final performance."
        },
        {
            "heading": "3 THEORETICAL ANALYSIS",
            "text": "In this section, we perform theoretical analysis on the influence of the subset searched by our algorithm and provide the corresponding lower bound. For any constructed graph G, we exploit \u03c8v(S) to denote the influence improvement of the subset S after adding v into S , i.e., \u03c8v(S) = fG(S \u222a v) \u2212 fG(S). For convenience, we use \u03c8t = fG(St) \u2212 fG(St\u22121) (t \u2265 1) to denote the incremental value of the influence function fG after adding vt into St\u22121. Also, we employ S\u2217m to represent the subset with the optimal influence value in the graph G with annotation budget m. Afterward, the optimal solution we expect to search in Algorithm 2 can be regarded as\nS\u2217m = argmax S\u2282V fG(S), s.t. |S| = m. (1)\nIn the following, we present the submodular condition to facilitate theoretical analysis of our method. Condition 1 (submodular condition). In the problem of selective annotations, given any graph G constructed by our procedure, the influence function fG is a submodular function which satisfies, for \u2200v \u2208 V, \u2200Sa \u2282 Sb \u2282 V,\nfG(Sa \u222a v)\u2212 fG(Sa) \u2265 fG(Sb \u222a v)\u2212 fG(Sb). (2)\nRemark 1. Intuitively speaking, given any graph G, we say the influence function fG satisfies the submodular condition if adding one data point to a smaller subset provides more influence than adding the same data point to a larger subset. In other words, it reflects the principle of diminishing returns: the marginal gain of including a data point in a set decreases as the size of the set increases. This condition can hold within the influence function (Li et al., 2019). Considering an extreme case, when subset S = V, the influence improvement of adding any data point to S will be zero. Proposition 1. In Algorithm 2, if the influence function fG satisfies Condition 1, then for fG(S\u2217m),\n\u2200t \u2208 [0,m\u2212 1), fG(S\u2217m) \u2264 fG(St) +m\u03c8t+1. (3)\nRemark 2. Proposition 1 proposes an upper bound for fG(S\u2217m) in the form of the influence fG(St) and its improvement at next step t+ 1, when Algorithm 2 is applied to selective annotations. Theorem 1. In Algorithm 2, if influence function fG satisfies Condition 1, when the algorithm terminates at the step m\u2212 1, fG(Sm) has a lower bound:\nfG(Sm) \u2265 (1\u2212 (1\u2212 1/m)m)fG(S\u2217m). (4)\nRemark 3. Theorem 1 provides an approximation guarantee for the influence of the selected subset returned by our method. The influence of the selected subset is at least as large as a certain proportion of the influence of the optimal solution, i.e., 1\u2212(1\u22121/m)m. With the annotation budgetm increases, this fraction gets closer to 1\u2212 1/e.\nFor the proofs of Proposition 1 and Theorem 1, readers can refer to Appendix B."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": "In this section, we evaluate our method (IDEAL) on multiple datasets that have different categories of tasks. Experimental setups are first introduced (\u00a74.1). We then demonstrate that the proposed method can find a better selective annotation subset in a more efficient way compared with baselines (\u00a74.2). Moreover, we perform in-depth investigations to provide a better understanding of the superiority of the proposed method (\u00a74.3). Finally, a case study is also provided to further evaluate the selected subset from our method in an automatic annotation scenario (\u00a74.4)."
        },
        {
            "heading": "4.1 EXPERIMENTAL SETUPS",
            "text": "Datasets and tasks. Following previous work (Su et al., 2023), we employ 9 datasets for the evaluations, which can be categorized into 4 different tasks, including classification, multi-choice, dialogue, and generation. The details of the datasets are provided in Appendix D.1. For each dataset, the original \u201ctrain/dev/test\u201d split from the Transformer library (Wolf et al., 2019) is utilized. We use test data for evaluation if they are available publicly (SST-5 (Socher et al., 2013), DBpedia(Lehmann et al., 2015), MWoZ (Budzianowski et al., 2018), and Xsum (Narayan et al., 2018)). Otherwise, we follow the same setting in (Su et al., 2023) and use the development set. We use accuracy as metric for all classifications and multiple choices tasks, joint accuracy (Budzianowski et al., 2018) for MWoZ, test suite accuracy (Zhong et al., 2020) for GeoQuery (Zelle & Mooney, 1996), and ROUGE-L (Lin, 2004) for Xsum.\nModels. If not otherwise specified, we run all experiments on the GPT-J 6B model (Wang & Komatsuzaki, 2021) except the GeoQuery and MWoZ datasets where we use Text-devinci-002 (Chen et al., 2021). We also provide experiments on other models including GPT-Neo 2.7B (Black et al., 2021) and more advanced models GPT-3.5-Turbo (Openai, 2023) in \u00a74.3.4. Our implementation is detailed in Appendix D.2.\nBaselines. In the main experiments, we perform a comprehensive evaluation of our method that is compared with previous state-of-the-art selective annotation baselines, i.e., Vote-k (Su et al., 2023) and random selection (abbreviated as \u201cRandom\u201d below). Note that, in \u00a74.3.2, we also compare our method with alternative methods that can select a coreset from large-scale unlabeled data on typical datasets. For the baseline Vote-k, we conduct experiments by running its official code3."
        },
        {
            "heading": "4.2 MAIN RESULTS",
            "text": "Measurement on performance. We first perform the evaluations for Random, Vote-k, and our method. The annotation budget is set to 18 and 100 respectively following the same setting as Vote-k. Note that we include 18 as the annotation budget considering all annotated examples can be fit to the prompt of the large language models within context limits. Therefore, the prompt retrieve stage can be ignored and the evaluation results can naturally represent the quality of the selected examples. We provide ex-\nperimental results in Table 1. As can be seen, our method achieves better performance than baselines\n3https://github.com/HKUNLP/icl-selective-annotation.\nin most of the evaluation scenarios (17 out of 18). Interestingly, we find that random selection outperforms Vote-k in 3 out of 18 cases. We conjecture that, under some ideal circumstances, the selected subset by random selection can approximate the distribution of full data. If test data follows the same distribution, good performance can be achieved. Note that we also illustrate selected examples and label distributions in selective annotations in Appendix C.1 and Appendix C.4 respectively.\nMeasurement on time cost. Previous work Vote-k (Su et al., 2023) encompasses generating prediction for most unlabeled data with a set of selected examples as prompts and performs data selection according to the confidence scores of the prediction. However, this process results in large unnecessary costs at inference time. Meanwhile, LLMs are often used as a service and an extra charge will appear with the usage of the token in both the input and output. In Figure 3, we compare the time cost of subset selection in our method against Vote-k on all tasks with the same hardware. The annotation budget is set to 18. We can observe that our method saves a tremendous amount of cost compared to Vote-k. Specifically, under the same hardware conditions, IDEAL achieves a 7.8\u00d7 lead on average over Vote-k. The speed improvement benefits from the fact that the proposed method does not need to perform example selection by generating predictions on a large number of unlabeled examples and is completely unsupervised."
        },
        {
            "heading": "4.3 MORE ANALYSIS",
            "text": "4.3.1 LARGER INFLUENCE BRINGS BETTER PERFORMANCE\nWe conduct experiments to investigate the correlation between subset influence and its corresponding in-context learning performance. Specifically, we randomly select a collection of example subsets from a large unlabeled data pool. We then evaluate each subset as a prompt and record its performance and influence in the constructed graph, resulting in a set of influence-performance pairs. Our goal is to analyze the correlation between these two metrics. To achieve this, we perform experiments on SST-5 and MNLI. We sample 30 subsets and order them according to their influences, where each subset in-\ncludes 5 examples. We divide this sorted subset sequence equally into three influence levels, with each level containing 10 subsets. We visualize the performance of subsets in each influence level in Figure 4. Our analysis reveals that subsets with larger influence levels achieve better average, median, and worst-case performance. This finding further demonstrates that quantifying the influence of each potential subset is an effective metric in the selective annotation problem."
        },
        {
            "heading": "4.3.2 COMPARISONS WITH ALTERNATIVE METHODS",
            "text": "We also compare our method with other alternative methods that can select the coreset from large-scale unlabeled data. We perform the evaluations on MRPC, MNLI and HellaSwag. We include the following alternative methods (1) KMeans (Lloyd, 1982), which groups all examples into m clusters, and selects the centroid example from each cluster. (2) Maximizing facility location (MFL) (Lin & Bilmes, 2009), which aims at optimizing the representative-\nness of the selected subset. (3) Fast Vote-k (Su et al., 2023), which is an efficient alternative to Vote-k which directly picks m examples with the largest Vote-k scores.\nMethod Datasets Selection Retrieval MRPC MNLI HellaSwag\nVote-k Similar 64.6 38.9 67.9 IDEAL Similar 66.4 41.0 68.6 Vote-k Random 60.7 37.8 64.6 IDEAL Random 62.5 39.0 66.8\nTable 3: Comparison of random and similar prompt retrieval with Vote-k and IDEAL on MRPC, MNLI, and HellaSwag. The subset selection method with a similar prompt retrieve achieves better performance compared with its version with a random prompt retrieve method. The best performance in each case is bolded.\nMethod Models Test DomainIMDb BoolQ Cst. Vote-k GPT-Neo 71.1 56.4 IDEAL GPT-Neo 72.2 58.0 Vote-k GPT-J 76.4 56.1 IDEAL GPT-J 76.8 56.4\nTable 4: The evaluations on out-of-distribution tasks. We show the performance of different methods on IMDb and BoolQ Contrast Set (target domains). In the evaluations, the prompts consist of selected SST-2 and BoolQ training examples, respectively (source domains). The best performance in each case is bolded.\nWe show the results in Table 2. We can observe IDEAL consistently outperforms the baselines in all datasets, demonstrating its superiority. Note that, the graph-based methods (Vote-k, Fast Votek, and our IDEAL) outperform the methods non-graph-based methods (K-Means and MFL) in all cases. This phenomenon suggests that graph-based methods are suitable for capturing similarity relationships between examples in the selective annotation problem, which can lead to better results."
        },
        {
            "heading": "4.3.3 EVALUATION WITH DIFFERENT RETRIEVAL METHODS",
            "text": "In previous experiments, we used a similarity-based prompt retrieval method by default. In this section, we conduct experiments to quantify the effect of different prompt retrieval methods under the annotation 100. We present the results in Table 3. We observe that both Vote-k and IDEAL suffer from a significant performance drop when the prompt retrieval method is changed from similaritybased to random selection. Notably, IDEAL also achieves better performance than Vote-k when combined with random retrieval in all datasets. It suggests that IDEAL can cultivate a more stable training subset (Chang & Jia, 2023) for in-context learning tasks. Note that we also show that our IDEAL is more stable and robust against the order of in-context examples in Appendix C.5."
        },
        {
            "heading": "4.3.4 EVALUATION ON OTHER LANGUAGE MODELS",
            "text": "Here we evaluate IDEAL on other language models, including GPT-Neo 2.7B (Black et al., 2021), and the advanced chat model GPT-3.5-Turbo where we use the same instruction as other language models for each dataset. While GPT-3.5-Turbo has mainly been optimized for chat, it also performs well on traditional completion tasks (Kheiri & Karimi, 2023). To conduct experiments, we select three classification tasks (MRPC, MNLI, and RTE), considering they are easier for prompting GPT3.5-Turbo to return responses without pleasantries or explanatory content.\nThe evaluation results are presented in Figure 5. Our evaluations reveal that IDEAL consistently outperforms the baselines across all models tested. This demonstrates the versatility of our method in the context of learning tasks using models of varying sizes. Notably, we observe that the largest model, i.e., GPT-3.5-Turbo, performs worse than GPT-Neo and GPT-J. This situation arises because GPT-3.5-Turbo is primarily optimized for chat tasks and faces challenges in following human instructions for classification. This scenario also has been identified in Ye et al. (2023)."
        },
        {
            "heading": "4.3.5 EVALUATION ON OUT-OF-DISTRIBUTION TASKS",
            "text": "We further evaluate our method on out-of-distribution tasks (Zhou et al., 2022; Wang et al., 2022b; Zhang et al., 2023b; Huang et al., 2023c;d), where there is a distribution shift between the selective annotation data and test data. Following (Chang & Jia, 2023), we compare IDEAL and Vote-k using SST-2 (Socher et al., 2013), BoolQ (Clark et al., 2019) as source tasks, and IMDb (Maas et al., 2011), BoolQ Contrast Set (Gardner et al., 2020) as target tasks, respectively. In all evaluations, we set the annotation budget as 18 and use the similarity-based retrieve to perform the evaluations on the test set in target domains. We use GPT-J 6B and GPT-Neo 2.7B here and show the results in\nTable 4. We can observe that IDEAL still outperforms baselines on all datasets with two models, implying that IDEAL could select the subset which could depict the invariant properties of this kind of tasks and generalize to out-of-distribution scenarios."
        },
        {
            "heading": "4.4 CASE STUDY: AUTOMATIC ANNOTATION",
            "text": "In previous experiments, we used a small set of manually annotated examples as candidate prompts to make predictions. In contrast, here we are interested in a case study that utilizes the subset selected by IDEAL to annotate all available unlabeled data automatically, leading to a larger set of candidate prompts. Specifically, we first choose an initial subset from the pool of unlabeled data using IDEAL and manually label this selected subset. Afterward, we simulate the information diffusion process from the initial subset to all other data, where we employ those activated data as prompts to predict upcoming activated data at each step and label them accordingly with prediction results. This process ultimately\nmakes a fully labeled training dataset. Finally, all examples (including manual labeling and automatic labeling ) are utilized as potential prompts in conjunction with the prompt retrieve technique for final testing. We name this paradigm as Auto-IDEAL and compare it with Vote-k and origin IDEAL on all classification datasets. We choose 300 training data for each dataset to perform experiments. The manual annotation budget is set to 150, i.e., half of the labels of the candidate prompts in Auto-IDEAL are annotated automatically. Experimental results are provided in Table 5. As can be observed, Auto-IDEAL even achieves better performance than IDEAL in 4 of 5 cases. Notably, although the performance is worse on MNLI, it is still competitive (better than Vote-k). It suggests that expanding the candidate prompts through automatic annotation following the diffusion process can further boost the performance of IDEAL. It benefits from the fact that information only diffuses between similar examples. Therefore, unlabeled examples will be automatically annotated using the most similar annotated examples as prompts leading to a promising annotation success rate."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "A series of recent works have confirmed the powerful ability of in-context learning for large language models. We investigate the ability from the perspective of selective annotations and propose an influence-driven method that selects a subset of data that acts as a proxy and closely approximates full data. Theoretical analysis is provided to establish an upper limit for the global optimal solution, and demonstrate that our greedy search algorithm selects a subset with influence at least as substantial as a specific proportion of the optimal solution\u2019s influence. Empirical evaluations illustrate the superiority of our method across a range of benchmarks, delivering superior performance while largely reducing the time required for subset selection. We hope this work can help researchers and practitioners understand the promise and potential of selective annotations in in-context learning, and facilitate them in the efficient conceptualization of novel language-based challenges."
        },
        {
            "heading": "ACKNOWLEDGEMENTS",
            "text": "Tongliang Liu is partially supported by the following Australian Research Council projects: FT220100318, DP220102121, LP220100527, LP220200949, and IC190100031."
        },
        {
            "heading": "A RELATED WORK",
            "text": "A.1 IN-CONTEXT LEARNING\nIn-context learning (ICL) has become a new paradigm for natural language processing (NLP), where large language models make predictions only based on contexts augmented with a few examples (Dong et al., 2023; Xie et al., 2022; Shin et al., 2022; Zhang et al., 2023a; Bai et al., 2023; Huang et al., 2023b). A series of works attempts to revise, enhance, and understand ICL, which include but are not limited to prompt tuning (Kim et al., 2022; Wang et al., 2022a; Mishra et al., 2022), analyzing intrinsic mechanism (Bansal et al., 2022; Chan et al., 2022; Li et al., 2023a; Garg et al., 2022), evaluations (Srivastava et al., 2023; Wang et al., 2022c), applications in multiple domains (Chen et al., 2022; Lee et al., 2022; Cho et al., 2023), and etc.\nDespite in-context learning has shown impressive performance in various domains, its efficacy is sensitive to the selection of in-context learning examples (Zhao et al., 2021b; Lu et al., 2021). Considering this, multiple methods have been proposed to select the optimal in-context learning examples to achieve optimal performance. These methods retrieve the most relevant examples subset with/without specific order for each query (Wu et al., 2022; Liu et al., 2021; Gao et al., 2020) Alternatively, some methods aim to find a set of examples once for all queries on the same task (Li & Qiu, 2023; Diao et al., 2023). Specifically, (Wu et al., 2022) formally defines the problem of self-adaptive In-context learning, which aims to search for the best In-context learning examples and corresponding order for each input query. While, Li & Qiu (2023); Diao et al. (2023) focus on finding one fixed set of examples for each task. However, these methods rely on the assumption that large-scale annotated training examples are always available.\nDifferent from them, this paper studies selective annotations for ICL, which can effectively reduce the annotation cost in ICL. Furthermore, compared with recent work (Su et al., 2023), as discussed in the main paper, this work is superior in many aspects, such as the end-to-end manner, mitigation of the trade-off between diversity and representativeness, theoretical guarantees, and better empirical performance."
        },
        {
            "heading": "A.2 CORESET SELECTION",
            "text": "Coreset selection focuses on selecting a small but highly informative subset from a large dataset for follow-up tasks, which can significantly reduce the data storage cost and training consumption (Huang et al., 2018; 2023a; Feldman & Zhang, 2020; Sorscher et al., 2022; Xia et al., 2023b; Li et al., 2023b; Xia et al., 2022). Most of the works on coreset selection target the scenes of supervised learning and classification (Sener & Savarese, 2018; Toneva et al., 2019; He et al., 2023). Only a few works extend coreset selection into unsupervised cases (Sorscher et al., 2022; Su et al., 2023). This paper studies unsupervised data selection for annotations in ICL, which reduces the annotation expenses of prompts and helps large language models become better few-shot learners. Also, it enjoys theoretical support. Therefore, this work is different from previous efforts and contributes to the research community."
        },
        {
            "heading": "A.3 DATA DISTILLATION",
            "text": "Data distillation (Wang et al., 2018; Zhao et al., 2021a; Shin et al., 2023; Cui et al., 2023; Du et al., 2023; Loo et al., 2023) is an alternative approach for dataset compression and curation, which is inspired by knowledge distillation which could be categorized into efficient ML (Ma et al., 2023a;b; Zheng et al., 2021; Zhang et al., 2021; Zheng et al., 2023). Different from coreset selection, this series of works target synthesizing a small but informative dataset as an alternative to the original dataset. However, data distillation is criticized for only synthesizing a small number of data points due to computational source limitations (Xia et al., 2023a; Yang et al., 2023). The performances of data distillation and data selection are therefore not compared directly. Besides, it is under-explored about how to perform data distillation in an unsupervised manner on natural language processing tasks. Based on this analysis, the data distillation strategy is not involved in empirical evaluations."
        },
        {
            "heading": "B PROOFS",
            "text": ""
        },
        {
            "heading": "B.1 PRELIMINARY THEORETICAL RESULTS",
            "text": "We first present some preliminary theoretical results and their corresponding proofs for the subsequential proofs of Proposition 1 and Theorem 1.\nB.1.1 LEMMA 1\nLemma 1. Given a graph G = (V,E,P), if the influence function meets Condition 1, then for \u2200Si,Sj \u2286 V:\nfG(Si)\u2212 fG(Sj) \u2264 \u2211\nv\u2208Si\u2212Sj\n\u03c8v(Sj)\u2212 \u2211\nv\u2208Sj\u2212Si\n\u03c8v(Si \u222a Sj \u2212 v), (5)\nwhere \u03c8v(Sj) := fG(Sj \u222a v)\u2212 fG(Sj).\nProof. The proof is inspired by (Rolnick & Weed, 2014). We first let\nSi \u2212 Sj = {a1, ...,ar} (6)\nand Sj \u2212 Si = {b1, ...,bq}, (7)\nwhere r \u2208 N+ and q \u2208 N+. According to Eq. (6), for the subsets Si and Sj , we have\nSj \u222a Si = Sj \u222a {a1, ...,ar}. (8)\nAfterward, we obtain\nfG(Sj \u222a Si)\u2212 fG(Sj) = fG(Sj \u222a {a1, ...,ar})\u2212 fG(Sj). (9)\nAt a high level, Eq. (9) is to calculate the influence improvement of Sj after adding data points {a1, ...,ar} into Sj . As the influence improvement of adding one sequence of data points is equal to the sum of the influence improvement at each step, we have,\nfG(Sj \u222a Si)\u2212 fG(Sj) (10)\n= fG(Sj \u222a a1)\u2212 fG(Sj) + r\u2211\nk=2\n[fG(Sj \u222a {a1, ...,ak})\u2212 fG(Sj \u222a {a1, ...,ak\u22121})]\n= \u03c8a1(Sj) + r\u2211\nk=2\n\u03c8ak(Sj \u222a {a1, ...,ak\u22121}).\nUnder Condition 1, as Sj \u2282 Sj \u222a {a1, ...,ak\u22121}, we have\nfG(Sj \u222a Si)\u2212 fG(Sj) = \u03c8a1(Sj) + r\u2211\nk=2\n\u03c8ak(Sj \u222a {a1, ...,ak\u22121}) (11)\n\u2264 r\u2211\nk=1\n\u03c8ak(Sj) = \u2211\na\u2208Si\u2212Sj\n\u03c8a(Sj).\nSimilarly,\nfG(Sj \u222a Si)\u2212 fG(Si) (12)\n= \u03c8b1(Si) + q\u2211\nk=2\n\u03c8bk(Si \u222a {b1, ...,bk\u22121}) \u2265 q\u2211\nk=1\n\u03c8bk(Si \u222a Sj \u2212 bk) = \u2211\nb\u2208Sj\u2212Si\n\u03c8b(Si).\nBy subtracting (12) from (10), we have fG(Si)\u2212 fG(Sj) \u2264 \u2211\nv\u2208Si\u2212Sj\n\u03c8v(Sj)\u2212 \u2211\nv\u2208Sj\u2212Si\n\u03c8v(Si \u222a Sj \u2212 v). (13)\nB.1.2 LEMMA 2\nLemma 2. Given a graph G = (V,E,P), for any subset S \u2282 V and any v \u2208 V, the influence function fG satisfies\n\u03c8v(S) = fG(S \u222a v)\u2212 fG(S) \u2265 0 (14)\nProof. We consider two cases to finish the proof.\nCase 1 (v \u2208 V\u2227v /\u2208 S). In this case, the influence improvement is at least 1 since v itself has been included, i.e.,\n\u03c8v(S) = fG(S \u222a v)\u2212 fG(S) \u2265 1. (15)\nCase 2 (v \u2208 V \u2227 v \u2208 S). In this case, the influence improvement is 0 since v has already been included in S, i.e.,\n\u03c8v(S) = fG(S \u222a v)\u2212 fG(S) = 0. (16)\nCombining the above two cases, we conclude that, for \u2200v \u2208 V, the influence function fG satisfies\nfG(S \u222a v)\u2212 fG(S) \u2265 0. (17)"
        },
        {
            "heading": "B.2 PROOF OF PROPOSITION 1",
            "text": "Proof. Given a graph G = (V,E,P), for \u2200Si,Sj \u2282 V, according to Lemma 2, we have\u2211 v\u2208Si\u2212Sj \u03c8v(Si \u222a Sj \u2212 v) \u2265 0. (18)\nTaking (18) into Lemma 1, we have fG(Si)\u2212 fG(Sj) \u2264 \u2211\nv\u2208Si\u2212Sj\n\u03c8v(Sj). (19)\nWe use S\u2217m to denote the optimal solution as discussed in the main paper. At any step t in Algorithm 2, we substitute S\u2217m (resp. St) into Si (resp. Sj) in (19), we can derive\nfG(S\u2217m) \u2264 fG(St) + \u2211\nv\u2208S\u2217m\u2212St\n\u03c8v(St). (20)\nAccording to Condition 1, \u03c8v(St) \u2265 \u03c8v(St+1) (21)\nholds. Taking both (20) and (21) into (19), we have for any t,\nfG(S\u2217m) \u2264 fG(St) +m\u03c8t+1. (22)"
        },
        {
            "heading": "B.3 PROOF OF THEOREM 1",
            "text": "Proof. Recall that\n\u03c8t = fG(St)\u2212 fG(St\u22121). (23)\nAccording to Proposition 1, we have\nfG(S\u2217m)\u2212 fG(St) \u2264 m\u03c8t+1 = m(fG(St+1)\u2212 fG(St)). (24)\nAfterwards, (24) equals to,\nfG(S\u2217m)\u2212 fG(St)\u2212 (fG(S\u2217m)\u2212 fG(St+1)) \u2265 1\nm (fG(S\u2217m)\u2212 fG(St)) (25)\n\u21d0\u21d2 fG(S\u2217m)\u2212 fG(St+1) \u2264 m\u2212 1 m (fG(S\u2217m)\u2212 fG(St)).\nBased on (25), we have\nfG(S\u2217m)\u2212 fG(St+1) \u2264 m\u2212 1 m (fG(S\u2217m)\u2212 fG(St)) (26)\n\u2264 (m\u2212 1 m )2(fG(S\u2217m)\u2212 fG(St\u22121)) \u2264 ... \u2264 (m\u2212 1 m )t+1(fG(Sm\u2217 )\u2212 fG(S0)).\nSince fG(S0) = fG(\u2205) = 0, we have\nfG(S\u2217m)\u2212 fG(St+1) fG(S\u2217m) \u2264 (m\u2212 1 m )t+1. (27)\nWhen Algorithm 2 terminates at step t = m\u2212 1, we have,\nfG(Sm) \u2265 (1\u2212 (1\u2212 1/m)m)fG(S\u2217m). (28)"
        },
        {
            "heading": "C SUPPLEMENTARY EXPERIMENTAL RESULTS",
            "text": ""
        },
        {
            "heading": "C.1 SELECTED EXAMPLES",
            "text": "In Table 6, for illustration purposes, we provide a few examples from the selection by our method, when the annotation size is 18.\nC.2 VISUALIZATION OF SELECTED EXAMPLES\nHere we provide a umap (McInnes et al., 2018) visualization of selected examples. To avoid the denseness, we choose the annotation budget as 5. The visualization can be checked in Figure 6. First, comparing subfigures (a) and (b), we can clearly see that the selection of Vote-k is much biased, and our IDEAL can identify a subset that is more favorable to be a proxy of full data. Second, comparing subfigures (c) and (d), we can see that the selected subset by Vote-k is distributed on the right of full data. By comparison, our IDEAL can select a subset that is distributed more uniformly."
        },
        {
            "heading": "C.3 DETAILED EXPERIMENTAL RESULTS IN TABLE 1",
            "text": "In the main paper (Table 1), we report the mean evaluation results for different methods over three random trials. Here we provided the detailed results of Table 1 with mean/maximum/minimum values. We can observe IDEAL achieves stable results compared with baselines. Moreover, the worst-case performance of IDEAL is obviously better compared with baselines in most cases."
        },
        {
            "heading": "C.4 LABEL DISTRIBUTIONS IN SELECTIVE ANNOTATIONS",
            "text": "Recall that the process of selective annotations is based entirely on similarities derived from sentence embeddings without labels. Therefore, we investigate whether the selected examples have label skew. Under an annotation budget of 100, we collect all selected examples in three classification tasks (MRPC, MNLI, and RTE) and show the numbers of different labels for different methods in Table 9. We also present the label statistics of the original training data. We observe that random selection shows a great variance. However, in an ideal case, it should achieve a similar distribution as the original training data. Notably, IDEAL achieves the smallest ratio between the numbers of the most frequent class and the least frequent class in 2 out of 3 cases (MNLI and RTE). This demonstrates that IDEAL can indeed balance the label distribution in the selected subset and mitigate the problem of label skew."
        },
        {
            "heading": "C.5 PROMPT ORDER IN SELECTIVE ANNOTATION",
            "text": "As pointed out by (Lu et al., 2021), the performance of in-context learning is influenced not only by the selection of prompts but also by the order in which the prompts are presented to models. Although this work focuses solely on selective annotation problems, we are interested in explor-\ning whether the selected subset can still lead to better performance when the order of prompts is permuted. Under an annotation budget of 18, we first retrieve prompts for each test instance from selected subsets achieved by different selective annotation methods. We then permute the order of prompts for each test instance 10 times, resulting in 10 different experimental trials. We show the average performance of these 10 trials and make a comparison between different selective annotation methods. We conduct experiments on MRPC, SST-5, and RTE datasets and present the results in Table 10. The results show that IDEAL outperforms baselines in 2 out of 3 cases, suggesting that our method can choose more stable and robust subsets against changed prompt orders."
        },
        {
            "heading": "D SUPPLEMENTARY DESCRIPTIONS OF EXPERIMENTAL SETTINGS",
            "text": ""
        },
        {
            "heading": "D.1 DETAILS OF DATASETS",
            "text": "In this paper, to demonstrate the superiority of our method, we employ 9 datasets typical datasets that have been widely used in previous NLP works (Su et al., 2023; Shi et al., 2023; Zhang et al., 2021) which can be categorized into 4 different tasks, including classification (MRPC (Dolan et al., 2004), SST-5 (Socher et al., 2013), MNLI (Williams et al., 2017), DBpedia (Lehmann et al., 2015), and RTE (Bentivogli et al., 2009)), multi-choice (HellaSwag (Zellers et al., 2019)), dialogue (MWoZ (Budzianowski et al., 2018)), and generation (GeoQuery (Zelle & Mooney, 1996) and Xsum (Narayan et al., 2018)). We list the datasets and the models used in Table 11.\nDatasets Task Models\nClassification MRPC (Dolan et al., 2004) Paraphrase Detection GPT-Neo, GPT-J, GPT-3.5-Turbo SST-5 (Socher et al., 2013) Sentiment Analysis GPT-J DBpedia (Lehmann et al., 2015) Topic Classification GPT-J RTE (Bentivogli et al., 2009)) Natural Language Inference GPT-Neo, GPT-J, GPT-3.5-Turbo MNLI (Williams et al., 2017) Natural Language Inference GPT-Neo, GPT-J, GPT-3.5-Turbo\nTo help readers better understand the datasets and tasks, for each of these datasets, we also list one example including both the input and output.\nD.1.1 MRPC"
        },
        {
            "heading": "Input",
            "text": "Are the following two sentences \u2019equivalent\u2019 or \u2019not equivalent\u2019?\\nA federal judge yesterday disconnected a new national \\\" do-not-call \\\" list , just days before it was to take effect , saying the agency that created it lacked the authority ..\\nA federal judge yesterday struck down the national do-not-call registry slated to take effect Oct. 1 , ruling the Federal Trade Commission had no authority to create the list ..\\nanswer:"
        },
        {
            "heading": "Output",
            "text": "equivalent\nD.1.2 SST-5\nInput\nHow do you feel about the following sentence?\\nsmug , artificial , ill-constructed and fatally overlong ... it never finds a consistent tone and lacks bite , degenerating into a pious , preachy soap opera .\\nanswer:"
        },
        {
            "heading": "Output",
            "text": "neutral\nD.1.3 MNLI"
        },
        {
            "heading": "Input",
            "text": "yeah well the Cardinals i don\u2019t know i think the Cowboys probably have a a better team they just at the end of the season the kind of got messed up with Aikman getting hurt because uh Laufenberg just couldn\u2019t never really get it together at all of course he sat along the sidelines all season he never really got in a game never did a whole lot. Based on that information, is the claim The Cowboys should have started Laufenberg all season. \\\"True\\\", \\\"False\\\", or \\\" Inconclusive\\\"?\\nanswer:"
        },
        {
            "heading": "Output",
            "text": "Inconclusive"
        },
        {
            "heading": "D.1.4 DBPEDIA",
            "text": ""
        },
        {
            "heading": "Input",
            "text": "title: V\\u00edctor David Loubriel; content: V\\u00edctor David Loubriel Ort\\u00edz is a Puerto Rican politician and former member of the Senate of Puerto Rico for the New Progressive Party (PNP). Loubriel presented his candidacy for the Senate of Puerto Rico before 2004. He ran for a candidate slot in the 2003 primaries obtaining the most votes in his district (Arecibo).In the 2004 general election Loubriel won a seat in the 23rd Senate of Puerto Rico to represent the district of Arecibo along with Jos\\u00e9 Emilio Gonz\\u00e1lez Vel \\u00e1zquez."
        },
        {
            "heading": "Output",
            "text": "office holder\nD.1.5 RTE"
        },
        {
            "heading": "Input",
            "text": "MEXICO CITY (Reuters) - A deadly strain of swine flu never seen before has broken out in Mexico, killing as many as 60 people and raising fears it is spreading across North America. The World Health Organization said it was concerned about what it called 800 \\\" influenza-like\\\" cases in Mexico, and also about a confirmed outbreak of a new strain of swine flu in the United States. It said about 60 people had died in Mexico. Mexico\u2019s government said it had confirmed that at least 16 people had died of the swine flu in central Mexico and that there could be another 45 fatal victims..\\nquestion: 800 Mexicans have been affected by a new form of swine influenza.. True or False?\\nanswer:"
        },
        {
            "heading": "Output",
            "text": "True"
        },
        {
            "heading": "D.1.6 HELLASWAG",
            "text": ""
        },
        {
            "heading": "Input",
            "text": "The topic is Work World. [header] How to become a high school social studies teacher [title] Obtain your bachelor\u2019s degree in education. [ step] All schools will require you to obtain at least your bachelor\u2019s degree in education. This degree will be proof that you are capable\nof delivering information to students using the current educational best practices."
        },
        {
            "heading": "Output",
            "text": "Make sure you\u2019ve fully completed all of your course work and obtained your bachelor\u2019s degree before you seek certification or employment.\n[substeps] Your electives should be based in social studies courses."
        },
        {
            "heading": "D.1.7 MULTIWOZ",
            "text": ""
        },
        {
            "heading": "Input",
            "text": "CREATE TABLE hotel( name text, ......, internet text CHECK (internet IN (dontcare, yes, no))\n) /* 4 example rows: SELECT * FROM hotel LIMIT 4; name pricerange type parking book_number_of_days book_day book_people area stars internet a and b guest house moderate guest house dontcare 3 friday 5 east 4 yes ...... /* ...... -- Using valid SQLite, answer the following multi-turn conversational questions for the tables provided above. Example #1 [context] hotel-area: west, hotel-stars: 3, hotel-internet: yes [system] the hobsons house is available in that area . Q: [user] that sounds like it will work . can i book that for 3 nights starting wednesday ? SQL: SELECT * FROM hotel WHERE book_day = wednesday AND book_people = 1 AND book_number_of_days = 3 AND name = hobsons house; ......"
        },
        {
            "heading": "Output",
            "text": "hotel WHERE book_day = wednesday AND book_number_of_days = 4 AND name = warkworth house;\nD.1.8 GEOQ"
        },
        {
            "heading": "Input",
            "text": "CREATE TABLE \"border_info\" (\"state_name\" text, \"border\" text) /* state_name border\nalabama tennessee alabama georgia alabama florida\n*/\n...... -- Using valid SQLite, answer the following questions for the tables provided above. ...... -- what is the longest river in the state with the highest point SELECT"
        },
        {
            "heading": "Output",
            "text": "RIVERalias0.RIVER_NAME FROM HIGHLOW AS HIGHLOWalias0, RIVER AS RIVERalias0 WHERE HIGHLOWalias0.HIGHEST_ELEVATION = (SELECT MAX( HIGHLOWalias1.HIGHEST_ELEVATION) FROM HIGHLOW AS HIGHLOWalias1 ) AND RIVERalias0.TRAVERSE = HIGHLOWalias0.STATE_NAME ORDER BY RIVERalias0. LENGTH DESC LIMIT 1;\nD.1.9 XSUM"
        },
        {
            "heading": "Input",
            "text": "For decades, large numbers of Haitians have migrated - many of them without papers - to the Dominican Republic, to escape the poverty and lack of employment in their homeland.\\nIn 2013, the Dominican Republic\u2019s highest court ruled that children born there to undocumented migrants were not automatically eligible for Dominican nationality. ...... \\nThere he strips the trees for firewood to make charcoal, to sell to Dominican traders for a few dollars.\\nHe knows the practice damages the fertility of the soil, but it\u2019s the only available source of income.\\n\\\"This is the only way we can survive,\\\" he says, motioning at his family, stuck inside the world\u2019s forgotten migrant crisis.\\ nYou can hear more of Will Grant\u2019s report on Heart and Soul on the BBC World Service."
        },
        {
            "heading": "Output",
            "text": "Immigration has long been a divisive issue on Hispaniola, the Caribbean island shared by Haiti and the Dominican Republic.\nD.2 IMPLEMENTATION DETAILS\nGeneral experimental conditions. We primarily use PyTorch (Paszke et al., 2019) to implement our algorithm and baselines. For GPT-3.5-Turbo, we perform the experiments by calling the OpenAI API using a single Intel Xeon CPU. The GPT-J 6B and GPT-Neo 2.7B models are from the Huggingface transformer library (Wolf et al., 2019). We run all our experiments of GPT-J 6B and GPT-Neo 2.7B on a single NVIDIA Tesla V100 (32GB) GPU.\nDetails of getting unlabeled data. Since obtaining unlabeled examples in realistic scenarios is also a high-variance process, we follow the same setting as (Su et al., 2023) to simulate the realistic setting. We perform selective annotations from 3k instances that are randomly sub-sampled from training data for each task. For each experiment, we repeat the sub-sampling process three times and average the results over all trials to ensure comprehensive evaluations.\nDetails of the graph construction. Except for the illustration experiment in Figure 2, we construct the directed graph for all unlabeled data by connecting each vertex to its 10 nearest successors (k = 10). It is important to note that a larger k will lead to an increase in the computation cost. We have chosen this setting because it provides good performance while maintaining efficient computation costs. For Figure 2, we construct the graph by connecting each vertex to its 3 nearest successors in order to avoid denseness.\nDetails of Algorithm 1. Considering the randomness of the diffusion process, when quantifying the influence of the subset, we run Algorithm 1 10 times and use the averaged influence value. Note that we also calculate the time cost in this repeated process when reporting the final results in the main paper. As shown in Figure 3, our algorithm is still more effective than Vote-k."
        },
        {
            "heading": "E TIME COMPLEXITY ANALYSIS",
            "text": "In this section, we perform a time-complexity analysis of our method. Given the constructed graph with a nodes and b edges, and an annotation budget ofm, the whole algorithm involves the following two parts that incur the following time costs: (1) Information diffusion process. The time complexity of quantifying the influence of a specific subset isO(a+b). This is because it involves running an independent cascade diffusion process (BFS-like traversals) through the graph. (2) Greedy search. The greedy algorithm iteratively selects the example that provides the maximum marginal gain in influence. When the annotation budget ism, the time complexity isO(m\u2217 (a+b)). This is because, in the worst case, the algorithm needs to evaluate the influence of each node in the network. Besides, for each node, the algorithm needs to perform a simulation with time complexity of O(a+ b)."
        },
        {
            "heading": "F LIMITATIONS",
            "text": "Memory cost. Although in-context learning tasks avoid the heavy parameter update process, they still require a large amount of memory to load models. For example, loading GPT-J 6B into a GPU requires about 23GB GPU memory, without considering the size of the dataset. This is a relatively high cost for individual researchers.\nThe limitation of Auto-IDEAL. Auto-IDEAL outperforms IDEAL in terms of performance, but it has two main drawbacks that hinder its usability in practice. First, Auto-IDEAL suffers from a model inference cost, especially in the era of large language models as a service. Specifically, when performing automatic annotation, Auto-IDEAL has to make predictions for all unlabeled examples to achieve automatic annotation. This makes it less practical than the native IDEAL. Second, AutoIDEAL may fail when the initial examples to be labeled are not very relevant to the initial examples labeled, even though they have a similar embedding. Auto-IDEAL performs automatic annotation by following the information diffusion process from the initial annotated subset to the examples to be labeled with similar embedding. When the examples to be labeled are not relevant to the initial examples labeled, it may lead to incorrect automatic annotations and then poor performance. Future research may focus on maintaining superior performance while reducing the automatic annotation cost of IDEAL.\nPotential benchmark leakage in GPT-3.5-Turbo. There may potentially exist benchmark leakage problems in the evaluation process (Zhou et al., 2023). Specifically, due to the fact that GPT-3.5Turbo is trained using huge text datasets as of 2021, the data related to evaluation sets may potentially be used for model training. This could lead to potential risks in the evaluation process. However, as our work does not involve the training data selection, the impact should be negligible."
        }
    ],
    "title": "IDEAL: INFLUENCE-DRIVEN SELECTIVE ANNOTA- TIONS EMPOWER IN-CONTEXT LEARNERS IN LARGE LANGUAGE MODELS",
    "year": 2024
}