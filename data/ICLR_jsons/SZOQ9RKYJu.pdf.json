{
    "abstractText": "With the rapid advancement of IT operations, managing and analyzing large data volumes efficiently for practical applications has become increasingly critical. Natural Language Processing (NLP) techniques have demonstrated remarkable capabilities in various tasks, including named entity recognition, machine translation, and dialogue systems. Recently, Large Language Models (LLMs) have achieved significant improvements across various domain-specific areas. However, there is a noticeable gap in the development of specialized Large Language Models (LLMs) tailored for IT operations. In this paper, we introduce the OWL, a large language model trained on our constructed Owl-Instruct with a wide range of IT-related information. Specifically, limited by the maximum input length, we propose the Homogeneous Markov Context Extension method (HMCE). The mixture-of-adapter strategy is leveraged to improve the parameter-efficient tuning across different domains or tasks. Further, we evaluate the performance of OWL on the Owl-Bench established by us and open IT-related benchmarks. OWL demonstrates superior performance results on IT tasks, which outperforms existing models by significant margins. Moreover, we hope that the findings of our work will provide more insights to revolutionize the techniques of IT operations with specialized LLMs.1",
    "authors": [],
    "id": "SP:5a33d17ea6175da8d80774b205adb6b752bd3fa5",
    "references": [
        {
            "authors": [
                "Armen Aghajanyan",
                "Lili Yu",
                "Alexis Conneau",
                "Wei-Ning Hsu",
                "Karen Hambardzumyan",
                "Susan Zhang",
                "Stephen Roller",
                "Naman Goyal",
                "Omer Levy",
                "Luke Zettlemoyer"
            ],
            "title": "Scaling laws for generative mixed-modal language models",
            "venue": "International Conference on Machine Learning,",
            "year": 2023
        },
        {
            "authors": [
                "Bradbury",
                "Siddhartha Brahma",
                "Kevin Brooks",
                "Michele Catasta",
                "Yong Cheng",
                "Colin Cherry",
                "Christopher A. Choquette-Choo",
                "Aakanksha Chowdhery",
                "Cl\u00e9ment Crepy",
                "Shachi Dave",
                "Mostafa Dehghani",
                "Sunipa Dev",
                "Jacob Devlin",
                "Mark D\u0131\u0301az",
                "Nan Du",
                "Ethan Dyer",
                "Vladimir Feinberg",
                "Fangxiaoyu Feng",
                "Vlad Fienber",
                "Markus Freitag",
                "Xavier Garcia",
                "Sebastian Gehrmann",
                "Lucas Gonzalez"
            ],
            "title": "Palm 2 technical report",
            "venue": "CoRR, abs/2305.10403,",
            "year": 2023
        },
        {
            "authors": [
                "Jiaqi Bai",
                "Hongcheng Guo",
                "Jiaheng Liu",
                "Jian Yang",
                "Xinnian Liang",
                "Zhao Yan",
                "Zhoujun Li"
            ],
            "title": "Griprank: Bridging the gap between retrieval and generation via the generative knowledge improved passage ranking",
            "venue": "arXiv preprint arXiv:2305.18144,",
            "year": 2023
        },
        {
            "authors": [
                "Jakub Breier",
                "Jana Brani\u0161ov\u00e1"
            ],
            "title": "Anomaly detection from log files using data mining techniques",
            "venue": "In Information Science and Applications,",
            "year": 2015
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Radford",
                "Ilya Sutskever",
                "Dario Amodei"
            ],
            "title": "Language models are few-shot learners",
            "venue": "CoRR, abs/2005.14165,",
            "year": 2020
        },
        {
            "authors": [
                "Wenhu Chen",
                "Xueguang Ma",
                "Xinyi Wang",
                "William W. Cohen"
            ],
            "title": "Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning",
            "venue": "tasks. CoRR,",
            "year": 2022
        },
        {
            "authors": [
                "Zewen Chi",
                "Li Dong",
                "Furu Wei",
                "Wenhui Wang",
                "Xian-Ling Mao",
                "Heyan Huang"
            ],
            "title": "Cross-lingual natural language generation via pre-training",
            "venue": "In The Thirty-Fourth AAAI Conference on Artificial Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Moreira",
                "Rewon Child",
                "Oleksandr Polozov",
                "Katherine Lee",
                "Zongwei Zhou",
                "Xuezhi Wang",
                "Brennan Saeta",
                "Mark Diaz",
                "Orhan Firat",
                "Michele Catasta",
                "Jason Wei",
                "Kathy Meier-Hellstern",
                "Douglas Eck",
                "Jeff Dean",
                "Slav Petrov",
                "Noah Fiedel. Palm"
            ],
            "title": "Scaling language modeling with pathways",
            "venue": "CoRR, abs/2204.02311,",
            "year": 2022
        },
        {
            "authors": [
                "Aakanksha Chowdhery",
                "Sharan Narang",
                "Jacob Devlin",
                "Maarten Bosma",
                "Gaurav Mishra",
                "Adam Roberts",
                "Paul Barham",
                "Hyung Won Chung",
                "Charles Sutton",
                "Sebastian Gehrmann"
            ],
            "title": "Palm: Scaling language modeling with pathways",
            "venue": "arXiv preprint arXiv:2204.02311,",
            "year": 2022
        },
        {
            "authors": [
                "Jiaxi Cui",
                "Zongjian Li",
                "Yang Yan",
                "Bohua Chen",
                "Li Yuan"
            ],
            "title": "Chatlaw: Open-source legal large language model with integrated external knowledge",
            "venue": "bases. CoRR,",
            "year": 2023
        },
        {
            "authors": [
                "Yingnong Dang",
                "Qingwei Lin",
                "Peng Huang"
            ],
            "title": "Aiops: real-world challenges and research innovations",
            "venue": "IEEE/ACM 41st International Conference on Software Engineering: Companion Proceedings (ICSE-Companion),",
            "year": 2019
        },
        {
            "authors": [
                "Min Du",
                "Feifei Li"
            ],
            "title": "Spell: Streaming parsing of system event logs",
            "venue": "IEEE 16th International Conference on Data Mining (ICDM),",
            "year": 2016
        },
        {
            "authors": [
                "Min Du",
                "Feifei Li",
                "Guineng Zheng",
                "Vivek Srikumar"
            ],
            "title": "Deeplog: Anomaly detection and diagnosis from system logs through deep learning",
            "venue": "In Proceedings of the 2017 ACM SIGSAC conference on computer and communications security,",
            "year": 2017
        },
        {
            "authors": [
                "Zhengxiao Du",
                "Yujie Qian",
                "Xiao Liu",
                "Ming Ding",
                "Jiezhong Qiu",
                "Zhilin Yang",
                "Jie Tang"
            ],
            "title": "Glm: General language model pretraining with autoregressive blank infilling",
            "venue": "In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
            "year": 2022
        },
        {
            "authors": [
                "Qiang Fu",
                "Jian-Guang Lou",
                "Yi Wang",
                "Jiang Li"
            ],
            "title": "Execution anomaly detection in distributed systems through unstructured log analysis",
            "venue": "In 2009 ninth IEEE international conference on data mining,",
            "year": 2009
        },
        {
            "authors": [
                "Xinghua Gao",
                "Pardis Pishdad-Bozorgi"
            ],
            "title": "Bim-enabled facilities operation and maintenance: A review",
            "venue": "Advanced engineering informatics,",
            "year": 2019
        },
        {
            "authors": [
                "Deepanway Ghosal",
                "Yew Ken Chia",
                "Navonil Majumder",
                "Soujanya Poria"
            ],
            "title": "Flacuna: Unleashing the problem solving power of vicuna using FLAN fine-tuning",
            "year": 2023
        },
        {
            "authors": [
                "Hongcheng Guo",
                "Jiaheng Liu",
                "Haoyang Huang",
                "Jian Yang",
                "Zhoujun Li",
                "Dongdong Zhang",
                "Zheng Cui"
            ],
            "title": "Lvp-m3: Language-aware visual prompt for multilingual multimodal machine translation",
            "venue": "In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2022
        },
        {
            "authors": [
                "Hongcheng Guo",
                "Yuhui Guo",
                "Jian Yang",
                "Jiaheng Liu",
                "Zhoujun Li",
                "Tieqiao Zheng",
                "Liangfan Zheng",
                "Weichao Hou",
                "Bo Zhang"
            ],
            "title": "Loglg: Weakly supervised log anomaly detection via log-event graph construction",
            "venue": "In DASFAA 2023,",
            "year": 2023
        },
        {
            "authors": [
                "Jinyang Guo",
                "Jiaheng Liu",
                "Zining Wang",
                "Yuqing Ma",
                "Ruihao Gong",
                "Ke Xu",
                "Xianglong Liu"
            ],
            "title": "Adaptive contrastive knowledge distillation for BERT compression. In Findings of the Association for Computational Linguistics: ACL 2023",
            "year": 2023
        },
        {
            "authors": [
                "Junxian He",
                "Chunting Zhou",
                "Xuezhe Ma",
                "Taylor Berg-Kirkpatrick",
                "Graham Neubig"
            ],
            "title": "Towards a unified view of parameter-efficient transfer learning",
            "venue": "In The Tenth International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Pinjia He",
                "Jieming Zhu",
                "Zibin Zheng",
                "Michael R Lyu"
            ],
            "title": "Drain: An online log parsing approach with fixed depth tree",
            "venue": "IEEE international conference on web services (ICWS),",
            "year": 2017
        },
        {
            "authors": [
                "Shilin He",
                "Jieming Zhu",
                "Pinjia He",
                "Michael R Lyu"
            ],
            "title": "Loghub: a large collection of system log datasets towards automated log analytics",
            "venue": "arXiv preprint arXiv:2008.06448,",
            "year": 2020
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Collin Burns",
                "Saurav Kadavath",
                "Akul Arora",
                "Steven Basart",
                "Eric Tang",
                "Dawn Song",
                "Jacob Steinhardt"
            ],
            "title": "Measuring mathematical problem solving with the MATH dataset",
            "venue": "Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks",
            "year": 2021
        },
        {
            "authors": [
                "Neil Houlsby",
                "Andrei Giurgiu",
                "Stanislaw Jastrzebski",
                "Bruna Morrone",
                "Quentin de Laroussilhe",
                "Andrea Gesmundo",
                "Mona Attariyan",
                "Sylvain Gelly"
            ],
            "title": "Parameter-efficient transfer learning for NLP",
            "venue": "Proceedings of the 36th International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Edward J. Hu",
                "Yelong Shen",
                "Phillip Wallis",
                "Zeyuan Allen-Zhu",
                "Yuanzhi Li",
                "Shean Wang",
                "Lu Wang",
                "Weizhu Chen"
            ],
            "title": "Lora: Low-rank adaptation of large language models",
            "venue": "In The Tenth International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Yuzhen Huang",
                "Yuzhuo Bai",
                "Zhihao Zhu",
                "Junlei Zhang",
                "Jinghan Zhang",
                "Tangjun Su",
                "Junteng Liu",
                "Chuancheng Lv",
                "Yikai Zhang",
                "Jiayi Lei",
                "Yao Fu",
                "Maosong Sun",
                "Junxian He"
            ],
            "title": "C-eval: A multilevel multi-discipline chinese evaluation suite for foundation models",
            "venue": "CoRR, abs/2305.08322,",
            "year": 2023
        },
        {
            "authors": [
                "Wenxiang Jiao",
                "Wenxuan Wang",
                "Jen-tse Huang",
                "Xing Wang",
                "Zhaopeng Tu"
            ],
            "title": "Is chatgpt a good translator? a preliminary study",
            "venue": "arXiv preprint arXiv:2301.08745,",
            "year": 2023
        },
        {
            "authors": [
                "Jared Kaplan",
                "Sam McCandlish",
                "Tom Henighan",
                "Tom B. Brown",
                "Benjamin Chess",
                "Rewon Child",
                "Scott Gray",
                "Alec Radford",
                "Jeffrey Wu",
                "Dario Amodei"
            ],
            "title": "Scaling laws for neural language models",
            "year": 2001
        },
        {
            "authors": [
                "Van-Hoang Le",
                "Hongyu Zhang"
            ],
            "title": "Log parsing with prompt-based few-shot learning",
            "venue": "IEEE/ACM International Conference on Software Engineering (ICSE)",
            "year": 2023
        },
        {
            "authors": [
                "Minghao Li",
                "Tengchao Lv",
                "Jingye Chen",
                "Lei Cui",
                "Yijuan Lu",
                "Dinei Florencio",
                "Cha Zhang",
                "Zhoujun Li",
                "Furu Wei"
            ],
            "title": "Trocr: Transformer-based optical character recognition with pre-trained models",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2023
        },
        {
            "authors": [
                "Jiaheng Liu",
                "Tan Yu",
                "Hanyu Peng",
                "Mingming Sun",
                "Ping Li"
            ],
            "title": "Cross-lingual cross-modal consolidation for effective multilingual video corpus moment retrieval",
            "venue": "In Findings of the Association for Computational Linguistics: NAACL",
            "year": 2022
        },
        {
            "authors": [
                "Yilun Liu",
                "Shimin Tao",
                "Weibin Meng",
                "Jingyu Wang",
                "Wenbing Ma",
                "Yanqing Zhao",
                "Yuhang Chen",
                "Hao Yang",
                "Yanfei Jiang",
                "Xun Chen"
            ],
            "title": "Logprompt: Prompt engineering towards zero-shot and interpretable log analysis",
            "year": 2023
        },
        {
            "authors": [
                "Ziyang Luo",
                "Can Xu",
                "Pu Zhao",
                "Qingfeng Sun",
                "Xiubo Geng",
                "Wenxiang Hu",
                "Chongyang Tao",
                "Jing Ma",
                "Qingwei Lin",
                "Daxin Jiang"
            ],
            "title": "Wizardcoder: Empowering code large language models with evol-instruct",
            "year": 2023
        },
        {
            "authors": [
                "Adetokunbo AO Makanju",
                "A Nur Zincir-Heywood",
                "Evangelos E Milios"
            ],
            "title": "Clustering event logs using iterative partitioning",
            "venue": "In Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining,",
            "year": 2009
        },
        {
            "authors": [
                "Weibin Meng",
                "Ying Liu",
                "Yichen Zhu"
            ],
            "title": "Loganomaly: Unsupervised detection of sequential and quantitative anomalies in unstructured logs",
            "venue": "In IJCAI,",
            "year": 2019
        },
        {
            "authors": [
                "Weibin Meng",
                "Ying Liu",
                "Federico Zaiter"
            ],
            "title": "Logparse: Making log parsing adaptive through word classification",
            "venue": "In 2020 29th International Conference on Computer Communications and Networks (ICCCN),",
            "year": 2020
        },
        {
            "authors": [
                "Salma Messaoudi",
                "Annibale Panichella",
                "Domenico Bianculli",
                "Lionel Briand",
                "Raimondas Sasnauskas"
            ],
            "title": "A search-based approach for accurate identification of log message formats",
            "venue": "IEEE/ACM 26th International Conference on Program Comprehension (ICPC),",
            "year": 2018
        },
        {
            "authors": [
                "Ha-Thanh Nguyen"
            ],
            "title": "A brief report on lawgpt 1.0: A virtual legal assistant based on GPT-3",
            "venue": "CoRR, abs/2302.05729,",
            "year": 2023
        },
        {
            "authors": [
                "Long Ouyang",
                "Jeffrey Wu",
                "Xu Jiang",
                "Diogo Almeida",
                "Carroll Wainwright",
                "Pamela Mishkin",
                "Chong Zhang",
                "Sandhini Agarwal",
                "Katarina Slama",
                "Alex Ray"
            ],
            "title": "Training language models to follow instructions with human feedback",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "William M Rand"
            ],
            "title": "Objective criteria for the evaluation of clustering methods",
            "venue": "Journal of the American Statistical association,",
            "year": 1971
        },
        {
            "authors": [
                "Laxmi Rijal",
                "Ricardo Colomo-Palacios",
                "Mary S\u00e1nchez-Gord\u00f3n"
            ],
            "title": "Aiops: A multivocal literature review",
            "venue": "Artificial Intelligence for Cloud and Edge Computing,",
            "year": 2022
        },
        {
            "authors": [
                "Baptiste Rozi\u00e8re",
                "Jonas Gehring",
                "Fabian Gloeckle",
                "Sten Sootla",
                "Itai Gat",
                "Xiaoqing Ellen Tan",
                "Yossi Adi",
                "Jingyu Liu",
                "Tal Remez",
                "J\u00e9r\u00e9my Rapin",
                "Artyom Kozhevnikov",
                "Ivan Evtimov",
                "Joanna Bitton",
                "Manish Bhatt",
                "Cristian Canton-Ferrer",
                "Aaron Grattafiori",
                "Wenhan Xiong",
                "Alexandre D\u00e9fossez",
                "Jade Copet",
                "Faisal Azhar",
                "Hugo Touvron",
                "Louis Martin",
                "Nicolas Usunier",
                "Thomas Scialom",
                "Gabriel Synnaeve"
            ],
            "title": "Code llama: Open foundation models for code",
            "venue": "CoRR, abs/2308.12950,",
            "year": 2023
        },
        {
            "authors": [
                "Jianlin Su"
            ],
            "title": "Naive bayes-based context extension",
            "venue": "https://github.com/bojone/NBCE,",
            "year": 2023
        },
        {
            "authors": [
                "Liang Tang",
                "Tao Li",
                "Chang-Shing Perng"
            ],
            "title": "Logsig: Generating system events from raw textual logs",
            "venue": "In Proceedings of the 20th ACM international conference on Information and knowledge management,",
            "year": 2011
        },
        {
            "authors": [
                "Shimin Tao",
                "Weibin Meng",
                "Yimeng Cheng",
                "Yichen Zhu",
                "Ying Liu",
                "Chunning Du",
                "Tao Han",
                "Yongpeng Zhao",
                "Xiangguang Wang",
                "Hao Yang"
            ],
            "title": "Logstamp: Automatic online log parsing based on sequence labelling",
            "venue": "ACM SIGMETRICS Performance Evaluation Review,",
            "year": 2022
        },
        {
            "authors": [
                "Ross Taylor",
                "Marcin Kardas",
                "Guillem Cucurull",
                "Thomas Scialom",
                "Anthony Hartshorn",
                "Elvis Saravia",
                "Andrew Poulton",
                "Viktor Kerkez",
                "Robert Stojnic"
            ],
            "title": "Galactica: A large language model for science",
            "year": 2022
        },
        {
            "authors": [
                "InternLM Team"
            ],
            "title": "Internlm: A multilingual language model with progressively enhanced capabilities",
            "venue": "https://github.com/InternLM/InternLM,",
            "year": 2023
        },
        {
            "authors": [
                "Hugo Touvron",
                "Thibaut Lavril",
                "Gautier Izacard",
                "Xavier Martinet",
                "Marie-Anne Lachaux",
                "Timoth\u00e9e Lacroix",
                "Baptiste Rozi\u00e8re",
                "Naman Goyal",
                "Eric Hambro",
                "Faisal Azhar",
                "Aur\u00e9lien Rodriguez",
                "Armand Joulin",
                "Edouard Grave",
                "Guillaume Lample"
            ],
            "title": "Llama: Open and efficient foundation language models",
            "venue": "CoRR, abs/2302.13971,",
            "year": 2023
        },
        {
            "authors": [
                "Sergey Edunov",
                "Thomas Scialom"
            ],
            "title": "Llama 2: Open foundation and fine-tuned chat models",
            "venue": "CoRR, abs/2307.09288,",
            "year": 2023
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam M. Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N. Gomez",
                "Lukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "year": 2017
        },
        {
            "authors": [
                "Yaqing Wang",
                "Sahaj Agarwal",
                "Subhabrata Mukherjee",
                "Xiaodong Liu",
                "Jing Gao",
                "Ahmed Hassan Awadallah",
                "Jianfeng Gao"
            ],
            "title": "Adamix: Mixture-of-adaptations for parameter-efficient model tuning",
            "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2022
        },
        {
            "authors": [
                "Yizhong Wang",
                "Yeganeh Kordi",
                "Swaroop Mishra",
                "Alisa Liu",
                "Noah A. Smith",
                "Daniel Khashabi",
                "Hannaneh Hajishirzi"
            ],
            "title": "Self-instruct: Aligning language models with self-generated instructions",
            "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
            "year": 2023
        },
        {
            "authors": [
                "Zixiang Wang",
                "Linzheng Chai",
                "Jian Yang",
                "Jiaqi Bai",
                "Yuwei Yin",
                "Jiaheng Liu",
                "Hongcheng Guo",
                "Tongliang Li",
                "Liqun Yang",
                "Hebboul Zine El Abidine",
                "Zhoujun Li"
            ],
            "title": "Mt4crossoie: Multi-stage tuning for cross-lingual open information",
            "venue": "extraction. CoRR,",
            "year": 2023
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "Brian Ichter",
                "Fei Xia",
                "Ed H. Chi",
                "Quoc V. Le",
                "Denny Zhou"
            ],
            "title": "Chain-of-thought prompting elicits reasoning in large language models",
            "venue": "NeurIPS,",
            "year": 2022
        },
        {
            "authors": [
                "Shijie Wu",
                "Ozan Irsoy",
                "Steven Lu",
                "Vadim Dabravolski",
                "Mark Dredze",
                "Sebastian Gehrmann",
                "Prabhanjan Kambadur",
                "David S. Rosenberg",
                "Gideon Mann"
            ],
            "title": "Bloomberggpt: A large language model for finance",
            "year": 2023
        },
        {
            "authors": [
                "Can Xu",
                "Qingfeng Sun",
                "Kai Zheng",
                "Xiubo Geng",
                "Pu Zhao",
                "Jiazhan Feng",
                "Chongyang Tao",
                "Daxin Jiang"
            ],
            "title": "Wizardlm: Empowering large language models to follow complex",
            "venue": "instructions. CoRR,",
            "year": 2023
        },
        {
            "authors": [
                "Canwen Xu",
                "Daya Guo",
                "Nan Duan",
                "Julian McAuley"
            ],
            "title": "Baize: An open-source chat model with parameter-efficient tuning on self-chat data",
            "venue": "arXiv preprint arXiv:2304.01196,",
            "year": 2023
        },
        {
            "authors": [
                "Hongyang Yang",
                "Xiao-Yang Liu",
                "Christina Dan Wang"
            ],
            "title": "Fingpt: Open-source financial large language models",
            "venue": "CoRR, abs/2306.06031,",
            "year": 2023
        },
        {
            "authors": [
                "Jian Yang",
                "Yuwei Yin",
                "Shuming Ma",
                "Dongdong Zhang",
                "Zhoujun Li",
                "Furu Wei"
            ],
            "title": "High-resource language-specific training for multilingual neural machine translation",
            "venue": "Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI 2022, Vienna,",
            "year": 2022
        },
        {
            "authors": [
                "Jian Yang",
                "Shuming Ma",
                "Li Dong",
                "Shaohan Huang",
                "Haoyang Huang",
                "Yuwei Yin",
                "Dongdong Zhang",
                "Liqun Yang",
                "Furu Wei",
                "Zhoujun Li"
            ],
            "title": "Ganlm: Encoder-decoder pre-training with an auxiliary discriminator",
            "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
            "year": 2023
        },
        {
            "authors": [
                "Shenglin Zhang",
                "Ying Liu",
                "Dan Pei",
                "Yu Chen",
                "Xianping Qu",
                "Shimin Tao",
                "Zhi Zang"
            ],
            "title": "Rapidand robust impact assessment of software changes in large internet-based services",
            "venue": "ENET",
            "year": 2015
        },
        {
            "authors": [
                "Shenglin Zhang",
                "Weibin Meng"
            ],
            "title": "Syslog processing for switch failure diagnosis and prediction in datacenter networks",
            "venue": "In IEEE/ACM 25th International Symposium on Quality of Service (IWQoS\u201917),",
            "year": 2017
        },
        {
            "authors": [
                "Xu Zhang",
                "Yong Xu",
                "Qingwei Lin",
                "Bo Qiao",
                "Hongyu Zhang",
                "Yingnong Dang",
                "Chunyu Xie",
                "Xinsheng Yang",
                "Qian Cheng",
                "Ze Li"
            ],
            "title": "Robust log-based anomaly detection on unstable log data",
            "venue": "In Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering,",
            "year": 2019
        },
        {
            "authors": [
                "Lianmin Zheng",
                "Wei-Lin Chiang",
                "Ying Sheng",
                "Siyuan Zhuang",
                "Zhanghao Wu",
                "Yonghao Zhuang",
                "Zi Lin",
                "Zhuohan Li",
                "Dacheng Li",
                "Eric P. Xing",
                "Hao Zhang",
                "Joseph E. Gonzalez",
                "Ion Stoica"
            ],
            "title": "Judging llm-as-a-judge with mt-bench and chatbot",
            "venue": "arena. CoRR,",
            "year": 2023
        },
        {
            "authors": [
                "Chunting Zhou",
                "Pengfei Liu",
                "Puxin Xu",
                "Srini Iyer",
                "Jiao Sun",
                "Yuning Mao",
                "Xuezhe Ma",
                "Avia Efrat",
                "Ping Yu",
                "Lili Yu",
                "Susan Zhang",
                "Gargi Ghosh",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Omer Levy"
            ],
            "title": "LIMA: less is more for alignment",
            "venue": "CoRR, abs/2305.11206,",
            "year": 2023
        },
        {
            "authors": [
                "Yaoming Zhu",
                "Jiangtao Feng",
                "Chengqi Zhao",
                "Mingxuan Wang",
                "Lei Li"
            ],
            "title": "Counter-interference adapter for multilingual machine translation",
            "venue": "Virtual Event / Punta Cana, Dominican Republic,",
            "year": 2021
        },
        {
            "authors": [
                "Since LLaMA Touvron"
            ],
            "title": "2023a) is designed to support natural language in Latin or Cyrillic language families, it has unsatisfactory compatibility with IT operation data (The vocabulary of LLaMA only contains 32K words). When tokenization is performed on IT operation data, a terminology of IT operation is often split into multiple parts (2-3 tokens are required to combine a term of log data)",
            "year": 2023
        },
        {
            "authors": [
                "Tao"
            ],
            "title": "Evaluation employs the same metrics as LogPrompt: RandIndex",
            "year": 2020
        },
        {
            "authors": [
                "Xu"
            ],
            "title": "Leveraging the topics forged in the prior",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Large Language Models (LLMs) (Chowdhery et al., 2022b; Touvron et al., 2023a;b) have emerged as powerful tools in the field of natural language processing (NLP) and Artificial Intelligence (AI). The release of GPT-3 (Brown et al., 2020a) in 2020 demonstrated the advantages of training large auto-regressive LLMs. With 175 billion parameters, GPT-3 surpassed previous models in various LLM tasks including reading comprehension, question answering, and code generation. Similar results have been achieved by other models as well. Furthermore, evidence suggests that larger models exhibit emergent behaviors and possess abilities not present in smaller models. For instance, they can learn tasks from a few examples, a phenomenon known as few-shot prompting. This capability expands the scope of supported tasks and facilitates the automation of new language tasks for users. However, the majority of research endeavors have been directed towards constructing general Large Language Models (LLMs) that encompass a wide spectrum of subjects, certain models trained on domain-specific data have exhibited exceptional performance within their specific domains, such as science and medicine. These discoveries underscore the necessity for additional advancements in domain-specific models.\nIn the field of IT operations (Du et al., 2017; Liu et al., 2023; Guo et al., 2023a), the significance of natural language processing (NLP) technologies is steadily on the rise. This paper undertakes the crucial task of delineating a set of specific assignments within the realm of IT operations, encompassing areas such as information security, system architecture, and other domains. However, the complexity and specific terminology of IT operations pose formidable challenges, including a unique set of terminologies, processes, and contextual nuances that are not easily decipherable by conventional NLP models. Therefore, it becomes increasingly evident that there is a pressing need for the development and deployment of a Large Language Model specifically tailored to the exigencies of IT operations within such specialized domains. The fine-tuned large language model\n1We will release dataset, model, and benchmark.\ncustomized to this purpose promises to be an invaluable asset in navigating the complexities of IT operations within these highly specialized domains. Such a specialized large language model would greatly enhance the efficiency, accuracy, and comprehension of IT-related tasks and communications within these niche areas, which will ultimately advance the field of IT operations management.\nIn this paper, we introduce the OWL, a large language model trained on our collected Owl-Instruct with a wide range of knowledge from operations and maintenance (O&M), where our data contains nine prevalent domains within O&M: information security, application, system architecture, software architecture, middleware, network, operating system, infrastructure, and database. Besides, we explore the use of the data augmentation (Xu et al., 2023b; Wang et al., 2023a) strategy to enable LLMs to accurately generate large, high-quality and diverse instruction data from a set of humanannotated data samples. To maintain a stringent standard of data quality, we employ a two-pronged approach that combines GPT-4 (OpenAI, 2023) scoring with meticulous manual validation. In addition, we have embarked on the development of Owl-Bench to evaluate the performance of different LLMs on IT-related tasks, an extensive bilingual benchmark that comprises two distinct segments: a Q&A (question-answer) part consisting of 317 entries, and a multiple-choice part containing 1,000 questions. In terms of model design, constrained by the maximum input length, inspired by the recent NBCE (Su, 2023) (Naive Bayes-based Context Extension), we propose the Homogeneous Markov Context Extension (HMCE) approach. Furthermore, in order to enhance the efficacy of instruction adaptation across various tasks, we put forward the strategy of employing a Mixture-ofAdapter method to facilitate supervised fine-tuning.\nThe contributions of our paper are as follows:\n\u2022 Owl-Instruct Construction. We collect and label 3000 seed samples and then prompt ChatGPT (Ouyang et al., 2022) to generate diverse instructions. To cover practical scenarios, we curate instructions that involve both single-turn and multi-turn scenarios.\n\u2022 Owl-Bench Construction. We have established a big model test benchmark for the operation and maintenance(O&M) domain to measure LLMs capabilities, which consists of nine O&M-related domains, showing the diversity of LLMs capabilities in the domain in a hierarchical manner.\n\u2022 Model and Training. For tokenization, we expand the word vocabulary using the extra IT-related data. Besides, we propose a simple method to extend input length based on the homogeneous Markov chain. As for training, a Mixture-of-Adapter strategy is proposed to improve the instruction-tuning performance.\n\u2022 Impressive Performance. We evaluate the performance of OWL with other LLMs on multiple benchmark datasets, including Owl-Bench and open IT-related benchmarks. OWL demonstrates superior performance results on IT tasks, which outperforms existing models by significant margins and maintains effective generalization abilities on Owl-Bench."
        },
        {
            "heading": "2 OWL-INSTRUCT DATASET CONSTRUCTION",
            "text": "The quality of the data employed in training Large Language Models (LLMs) stands as a pivotal determinant influencing the ultimate performance of the language model. Zhou et al. (2023) underscore the paramount importance attributed to both the diversity and quality of data when training large-scale models. Consequently, it becomes imperative for us to curate a high-caliber instruction dataset, which is the Owl-Instruct, tailored specifically for the realm of Operations and Maintenance (O&M). The overview of constructing Owl-Instruct is in Figure 1. A statistical analysis is presented in Table 1, and a visual representation of the keywords is depicted in Figure 2."
        },
        {
            "heading": "2.1 SEED DATA COLLECTION",
            "text": "In the initial phase of our project, we engage 25 subject matter experts within the field of operations and maintenance (O&M) to meticulously craft input and output sequences, along with comprehensive instructions. These encompass a wide spectrum of common domains and tasks. Concretely, Owl-Instruct contains nine prevalent domains within O&M: information security, application, system architecture, software architecture, middleware, network, operating system, infrastructure, and database. Within each domain, a plethora of tasks are encapsulated, including but not limited to deployment, monitoring, fault diagnosis, performance optimization, log analysis, backup and recovery, among others. Finally, we gain 2,000 single-turn and 1,000 multi-turn seed data instances, which serve as the foundation for further augmenting the scale and diversity of Owl-Instruct."
        },
        {
            "heading": "2.2 DATASET AUGMENTATION",
            "text": "Single-turn Dataset Construction In our endeavor, we have constructed a comprehensive singleturn dialogue dataset tailored specifically to the domain of operations and maintenance, boasting an impressive 9,118 meticulously curated data entries. Motivated by Self-Instruct (Wang et al., 2023a), we have enriched our dataset. This enrichment involves the generation of supplementary samples derived from the seed data, a corpus painstakingly labeled by our domain experts. Besides, we consider GPT4 (OpenAI, 2023) as a reference and supervisor for ensuring the quality of data. Please refer to Appendix E.1 for cases.\nMulti-turn Dataset Construction In accordance with the methodology elucidated in Baize (Xu et al., 2023b), the generation process of our multi-turn dialogue dataset within the operations and maintenance domain encompasses the following four distinct phases (Case in Appendix E.2.): (1) Seed Data Collection, (2) Topic Generation, (3) Multi-turn Dialogue Generation, and (4) Manual and GPT4 Screening. More details can be seen in Appendix C."
        },
        {
            "heading": "2.3 DATA QUALITY",
            "text": "In order to maintain a stringent standard of data quality, we employ a two-pronged approach that combines GPT-4 (OpenAI, 2023) scoring with meticulous manual validation. This dual-validation\nprocess ensures the integrity and reliability of generated data while enhancing its overall quality. When leveraging GPT-4 for scoring, we meticulously design specific prompts tailored to our dataset. These prompts are strategically crafted to enable GPT-4 to evaluate and rate the generated data based on predefined quality criteria. This automated scoring mechanism allows us to swiftly identify and filter out any low-quality data instances. Moreover, it serves as a valuable tool for flagging potential issues and areas that require improvement within the dataset. Simultaneously, dataset undergoes rigorous manual validation. A team of expert reviewers conducts an in-depth assessment of each data entry. This manual inspection process entails a thorough examination of content, coherence, and adherence to domain-specific knowledge. Entries that do not meet our stringent quality standards are meticulously flagged and subsequently removed. Please refer to Appendix F.1 for prompt and case details."
        },
        {
            "heading": "3 OWL-BENCH BENCHMARK CONSTRUCTION",
            "text": "Overview: In the absence of a benchmark tailored for evaluating the performance of large language models in the context of Operations and Maintenance (O&M), there exists a critical gap in our ability to effectively assess and compare models within this area. To address this deficiency, we have embarked on the development of Owl-Bench, an extensive bilingual benchmark that comprises two distinct segments: a Q&A (question-answer) part consisting of 317 entries, and a multiplechoice part containing 1,000 questions in Table 2. The O&M is characterized by its vastness and multidisciplinary nature (Dang et al., 2019; Rijal et al., 2022; Gao & Pishdad-Bozorgi, 2019). Inspired by these works, we have meticulously curated the sub-fields included in Owl-Bench. We consider the multitude of real-world industrial scenarios that encompass this area, ensuring that our benchmark exhibits a comprehensive diversity. Our data collection process involves the acquisition of test data from nine distinct subdomains: information security, application, system architecture, software architecture, middleware, network, operating system, infrastructure, and database. And the data consists of Q&A pairs and multiple-choice questions. It is essential to highlight that the data incorporated into Owl-Bench significantly differs from that of Owl-Instruct. The former is derived directly from real-world scenario-based examination questions, devoid of any modification or expansion through GPT4. In Appendix E.3 and E.4, we present examples of multiple-choice questions and Q&A question across these nine domains, providing a glimpse into the rich diversity encapsulated within the benchmark."
        },
        {
            "heading": "3.1 DATA COLLECTION AND PROCESSING",
            "text": "Our main source of data consists of two parts: one part is a free practice exam available on the Internet. The other questions are carefully designed by O&M experts. More details can be seen in Appendix H. The data collected are in a variety of formats, mainly PDF or Microsoft Word documents, with a small percentage of web pages. The PDF documents are initially processed into text using OCR tools (Li et al., 2023). Some cases that are difficult to process will be manually parsed into a structured format by hand, similar to Hendrycks et al. (2021); Taylor et al. (2022).\nMiddleware Information security Infrastructure Application Operating system Database System architecture Network Software architecture\nQ&A\n#Questions 30 26 41 36 39 38 25 40 42 #Average dialogue length 301 275 287 311 297 342 286 308 298\nMultiple-choice\n#Questions 136 108 110 102 118 119 87 122 98 #Average dialogue length 212 287 264 343 247 310 255 294 301\nTable 2: Statistical analysis of the Owl-Bench."
        },
        {
            "heading": "4 MODEL",
            "text": ""
        },
        {
            "heading": "4.1 TOKENIZATION",
            "text": "Since LLaMA2 (Touvron et al., 2023b) is designed to support natural language in Latin or Cyrillic language families, it has unsatisfactory compatibility with IT operation data. For compatibility with both natural language and IT-related data, we expand the word vocabulary size from 32, 000 to 48, 553. (More details in Appendix A)"
        },
        {
            "heading": "4.2 LONG-CONTEXT INPUT",
            "text": "Limited by the maximum input length, inspired by the recent NBCE (Su, 2023) (Naive Bayes-based Context Extension), we propose the Homogeneous Markov Context Extension method (HMCE). The fundamental assumption of NBCE is the independent input contexts, but in reality, input texts are interconnected and exhibit continuity rather than independence. Consequently, we abandon the independence assumption and instead employ a homogeneous Markov chain assumption to extend NBCE, catering to the continuous nature of the input data, which is named HMCE.\nLet T be the generated text. Let S := (S1, ..., Sn) be the previous contexts, and that their overall length has exceeded the training length. We want to generate T based on S1, ..., Sn, that is, we want to estimate p(T |S). (See Appendix D.2 for more details and derivation.)\nlog p(T |S) \u221d n\u2211\ni=2 p(T |Si, Si\u22121)\u2212 n\u22121\u2211 i=2 p(T |Si) (1)\nwhere n is the number of separate content. For the original input sentences S longer than maximum length L, we slit them into multiple segments."
        },
        {
            "heading": "4.3 MIXTURE-OF-ADAPTER",
            "text": "Parameter-efficient tuning (Houlsby et al., 2019; Zhu et al., 2021; Hu et al., 2022; Yang et al., 2022; He et al., 2022; Wang et al., 2023b; 2022) is a simple but effective technique in LLMs, which enables efficient and flexible transfer by introducing task-specific modifications to a fixed pre-trained model. For the cross-domain/cross-task transfer, we use a mixture of adapters for different domains and tasks, where a group of LoRA adapters is lightweight compared to the pre-trained model. The adapters with a low-rank down-project matrix and up-project matrix can be directly inserted into the pre-trained embedding, attention, and feed-forward network. Given the source sentence x = {x1, . . . , xm} of m tokens and a group of T Adapters, we use mixture-of-adapters to learn the task-specific and domain-specific representations for diverse input:\nhLia = A\u03b8g(Li)(h Li) (2)\nwhere g(Li) are selected LoRA experts derived from the language representations. A(\u00b7) denotes the LoRA adapter module and \u03b8 = {\u03b81, . . . , \u03b8T } denotes the adapter pool. A\u03b8g(Li) is calculated by:\nA\u03b8g(Li)(h Li) = hLi + \u2211 At,Bt\u2208S(e) \u03b1\u2206WhLi (3)\nwhere \u2206W = BA is denoted by a low-rank decomposition (A \u2208 Rd\u00d7r \u2227B \u2208 Rr\u00d7d \u2227 r \u226a d). The matrices A and B are initialized by a random Gaussian distribution and zero. \u03b1 is the scaling factor and r is the inner dimension. S(e) denotes the subset.\nIn Equation 3, all experts only require fine-tuning a small number of language-specific parameters instead of all parameters of the pre-trained model. Thus, we can simultaneously train multiple experts for different languages, which all share the same freezing pre-trained parameters. We use multiple adapters from the selected subset to maximize the transfer of knowledge across languages:\ng(Li) = TopK ( exp(\u03b1Lij )\u2211T t=1 exp(\u03b1 Li t ) ) (4)\nwhere TopK(\u00b7) is the selection function, where we calculate the selection probabilities of all LoRA adapters and choose the top-k LoRA experts obeying the probability distribution. \u03b1Lij is a scalar from the representations of language Li (We use the hidden state of the special token [CLS] of each layer). S(e) = {(Ak, Bk)}Kk=1 and \u03b1 Li j is used to incorporate the different experts.\nWe project the language representation eLi of language Li into the LoRA expert distribution using the learned matrix Wa \u2208 Rd\u00d7T , where d is the hidden size and T is the number of experts. The weight of LoRA expert \u03b1Lij is calculated by:\n\u03b1Li = eLiWa (5)\nwhere \u03b1 = {\u03b1t = 1}Tt=1. For all modules of the pre-trained model, we leverage the mixtureof-adapter strategy to learn the task-sensitive representations for the different input sentences by activating top-k experts."
        },
        {
            "heading": "4.4 SUPERVISED FINE-TUNING",
            "text": "Given the multiple tasks T = {Ti}Ni=1, we construct the multi-task training corpora D = {Di}Ni=1, where each dataset contains a series of triple tuples {(x(j), y(j), I(j))}Mj=1, where x(j) and y(j) are input and output sample with the instruction I(j).\nInstruction Tuning at Scale To scale up the multi-task training corpora D = {Di}Ni=1, we adopt the self-instruction (Wang et al., 2023a) for increasing data diversity and complexity. Given the seed human-written instructions as in-context examples (randomly sampling K task instruction from the initial instruction pool), new instructions are generated and merged into the instruction pool. The process is repeated several times until no new legitimate instructions are generated. Based on task descriptions and their instructions, we expand the training data of each instruction by leveraging a large language model to output the sample input and then produce the answer. To filter out the low-quality data, we score the model-generated samples by feeding the generated sample into the LLM, and three human experts fix or drop out the illegal samples (the scoring prompt is shown in Section F.1). After the heuristic filter process, a new high-scoring instruction passing expert check will be added to the instruction pool. The model-generated training corpora Dm = {Dmi }Ni=1 are merged into original training corpora as a whole D \u22c3 Dm for multi-task training. Multi-task Training Given the supervised and model-generated instruction corpora D \u22c3 Dm, the training objective of the supervised instruction tuning can be described as:\nLm = \u2212 1\nN N\u2211 i=1 Ex,y,I\u2208{Di,Dmi } log(y (i)|I(i), x(i)) (6)\nwhere x is the sample input and y is the sample output with the instruction I from the original training corpora and model-generated training corpora."
        },
        {
            "heading": "5 EVALUATION",
            "text": "We evaluate the performance of OWL on Owl-Bench and general downstream tasks, where the OwlBench help us test our hypothesis that training on high-quality data will yield better results on the questions in operation and maintenance area. The general tasks investigate whether OWL has the generalization capability. For general downstream tasks, we chose two typical benchmarks: Log Parsing and Anomaly Detection Tasks."
        },
        {
            "heading": "5.1 EXPERIMENT SETTING",
            "text": "Our base model is LLaMA2-13b (Touvron et al., 2023b). For instruction-tuning, the learning rate is 10\u22124, a weight decay of 0.1, a batch size of 16. The sequence length is 1024. We use Adam as the optimization algorithm with \u03b21 = 0.9, \u03b22 = 0.99, and \u03b5 = 10\u22128. The training epoch is 3. The rank and alpha of LoRA (Hu et al., 2022) is 8 and 32. The dropout of LoRA is 0.05. We train LoRA for 10 epochs."
        },
        {
            "heading": "5.2 EVALUATION ON OWL-BENCH",
            "text": "In this section, we compare the results of most of the large language models (LLMs) on our benchmark (Owl-Bench). The results of the experiment consist of two main parts: the results of the multiple choice questions and the Q&A test. The multiple choice questions mainly test the model\u2019s objective domain general knowledge learning ability, and the Q&A questions mainly test the model\u2019s comprehensive processing and logic ability for O&M problems. The models we choose to compare have similar sizes to Owl and are open-sourced that the results can reproduced: ChatGLM2-6b (Du\net al., 2022), ChaGLM-6b (Du et al., 2022), LLaMA2-13b (Touvron et al., 2023b), Qwen-7b 2 and InternLM-7b (Team, 2023). Besides, we also compare our model with ChatGPT (Jiao et al., 2023)."
        },
        {
            "heading": "5.2.1 RESULTS ON Q&A TEST",
            "text": "Evaluation way Follow recent works (Huang et al., 2023; Xu et al., 2023b), there are two ways for evaluation: single-score mode and pairwise-score mode. For single-score mode, firstly, we select the model which will be tested, and let the model give the answer directly based on the given questions. Then, we choose the scoring model (GPT4 (OpenAI, 2023)), and let the scoring model give a score from 1 to 10 based on the question content. Higher scoring values indicate better responses. Please refer to the Appendix F.2 for more details.\nFor the pairwise-score mode, inspired by the work (Zheng et al., 2023), first, we select two models which will be assessed, and let the models give the answers based on the given questions. Then, let the scoring model judge which model among the assessed models is better according to the question content, the answers of the two assessed models, and the reference answer. The better model counts one win, the worse model counts one loss, and if the models answer at a similar level, they all count one tie. Please refer to the Appendix F.3 for more details.\nQ&A Performance In Table 3, we show the average scores of the Q&A test, where OWL gets the highest score, and overall the models perform well, with a small variance in the scores. As for the pairwise scores, we can see in Figure 3 that OWL also beats the rest of the models with the best performance, but the number of draws is on the high side, which means in most cases, both generate answers to the satisfaction of the GPT4."
        },
        {
            "heading": "5.2.2 RESULTS ON MULTIPLE CHOICE QUESTIONS",
            "text": "Evaluation way For the multiple-choice questions, our approach involves direct model response generation by allowing the model to select a single answer from the provided options (A, B, C, D). This way not only streamlines the evaluation process but also offers a standardized format for assessing the model\u2019s comprehension and decision-making abilities. Each question presents a set of choices, and the model\u2019s task is to make a single selection that best aligns with its understanding of the question\u2019s context and semantics. This assessment approach is widely employed in educational and evaluative contexts to gauge a model\u2019s accuracy and proficiency in selecting the correct answer among multiple possibilities. It enables a straightforward and efficient means of evaluating the model\u2019s performance in a multiple-choice question setting, facilitating a clear and concise assessment process.\n2https://github.com/QwenLM/Qwen-7B\nMultiple choice performance The results in Figure 4a and Figure 4b show that ChatGPT (Jiao et al., 2023) has the highest average scores among these 9 domains, and OWL is also just a little bit lower than ChatGPT, while outperforming the other models. Besides, OWL achieves a higher point than ChatGPT on the system architecture domain."
        },
        {
            "heading": "5.3 EVALUATION ON LONG CONTEXT INPUT",
            "text": "We have propose HMCE for training-free long-context inference based on NBCE (Su, 2023). Specifically, HMCE employs a conditional homogeneous Markov chain to extend the context processing capabilities of large language models (LLMs). Notably, this extension is model-agnostic and necessitates no fine-tuning. To substantiate the efficacy of this approach, we randomly selected Q&A pairs from the Owl-Bench and concatenated the questions to create input sequences of varying lengths. The results in Table 4 are assessed in terms of perplexity (PPL). These findings unequivocally underscore the effectiveness of HMCE. Without NMCE, the PPL experiences a proportional increase as the input length grows."
        },
        {
            "heading": "5.4 EFFECT OF MIXTURE-OF-ADAPTER",
            "text": "In Table 5, we perform ablation experiments to compare the results of multiple choice with and without Mixture-of-Adapter (MoA) strategy, and the experiments show that our MoA is effective and we also compared it with the fashion LoRA (Hu et al., 2022), which better shows the efficiency of our model. Specifically, when using instruction-tuning without MoA, the overall performance of the model is slightly degraded, and when using LoRA fine-tuning, the final performance is close to the result without MoA."
        },
        {
            "heading": "5.5 EVALUATION ON DOWNSTREAM TASK",
            "text": ""
        },
        {
            "heading": "5.5.1 LOG ANOMALY DETECTION",
            "text": "Task Description Log anomaly detection represents a crucial component of automated log analytics, employed for real-time system issue detection for large-scale IT systems. Anomaly detection, as elucidated in the work (Breier & Branis\u030cova\u0301, 2015), assumes paramount importance in scrutinizing idiosyncrasies within log data. These logs provide intricate insights into system events transpiring in real-time, as well as user intentions within the ambit of large-scale services, as articulated in the study (Zhang et al., 2015). The endeavor to pinpoint anomalous logs solely from a local perspective is fraught with potential errors.\nDatasets and Baselines We conduct experiments on LogHub (He et al., 2020). Baseline methods: DeepLog (Du et al., 2017), LogAnomaly (Meng et al., 2019), LogRobust (Zhang et al., 2019), and LogPrompt (Liu et al., 2023). The basic settings are consistent with LogPrompt to ensure the zeroshot scenario. Specifically, we use the first 4000 log messages in each dataset to train and then test on the remaining logs. The evaluation metric is the F1-score, wherein TP signifies the successful detection of an anomalous session (likewise for TN , FP , and FN ). The prompts utilized for ChatGPT and OWL are in Appendix F.5:\nResults and Analysis The results are depicted in Table 6. Remarkably, OWL outperforms existing methods in both datasets, despite the latter being trained on thousands of logs. In a comparison with ChatGPT (LogPrompt), OWL exhibits an average improvement of 6.5% in terms of F1-score across the two datasets. Nevertheless, even with the formidable Large Language Model capabilities, the performance of anomaly detection in the zero-shot scenario remains modest.\nModel results on the Log Parsing downstream task are shown in Appendix B."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "In this paper, we present the OWL, a large language model for IT operations. First, we collect the Owl-Instruct dataset, which contains diverse IT-related tasks to improve the generalization ability of LLMs on IT operations. Then, we also introduce the Owl-Bench evaluation benchmark dataset with nine operation and maintenance domains. Besides, we also introduce the HMCE method to extend the context and the mixture-of-adapter strategy to further enhance the performance. Moreover, extensive experimental results on Owl-Bench demonstrate the effectiveness of our OWL."
        },
        {
            "heading": "A TOKENIZATION",
            "text": "Since LLaMA Touvron et al. (2023a) is designed to support natural language in Latin or Cyrillic language families, it has unsatisfactory compatibility with IT operation data (The vocabulary of LLaMA only contains 32K words). When tokenization is performed on IT operation data, a terminology of IT operation is often split into multiple parts (2-3 tokens are required to combine a term of log data), which significantly reduces the efficiency of coding and decoding. For compatibility with both natural language and log data, we expand the word vocabulary using the extra data. Specifically, we train a tokenizer model on Owl-Instruct dataset and then merge the LLama tokenizer with the LLaMA native tokenizer by combining their vocabularies for a coupled tokenizer model. Consequently, we obtain a merged tokenizer with a vocabulary size of 48,553. To adapt the LLaMA model for the new tokenizer, we resize the word embeddings and language model head from shape D \u00d7 T to D\u2032 \u00d7 T , where D = 32, 000 denotes the original vocabulary size, and D\u2032 = 48, 553 is the new vocabulary size. The new rows are appended to the end of the original embedding matrices, ensuring that the embeddings of the tokens in the original vocabulary remain unaffected."
        },
        {
            "heading": "B RESULTS ON LOG PARSING TASK",
            "text": "Task description Log parsing represents a classical challenge within the realm of log analysis. Although existing approaches have made noteworthy strides in log analysis, they encounter substantial challenges when deployed in real-world scenarios. Firstly, the performance of current methods experiences a marked decline when confronted with situations characterized by a scarcity of training samples, coupled with a preponderance of previously unseen logs.\nSecondly, the pragmatic implementation of log analysis techniques is hampered by their limited interpretability. Conventional methods furnish predictions devoid of accompanying explanations. Conversely, an interpretable output from the analysis not only facilitates the identification of false alarms but also simplifies the task of tracing the root causes of issues and subsequently taking appropriate corrective measures.\nDatasets and Baselines We have conducted experiments on LogHub benchmark He et al. (2020). To assess the log parsing performance under zero-shot conditions, we adopt the same setting outlined in LogPrompt Liu et al. (2023). 10 baselines are chosen, including LKE Fu et al. (2009), LogSig Tang et al. (2011), Spell Du & Li (2016), IPLoM Makanju et al. (2009), Drain He et al. (2017), FTtree Zhang et al. (2017), MoLFI Messaoudi et al. (2018), Logstamp Tao et al. (2022), LogPPT Le & Zhang (2023), and LogPrompt Liu et al. (2023).\nEvaluation employs the same metrics as LogPrompt: RandIndex Rand (1971); Tao et al. (2022); Zhang et al. (2017); Meng et al. (2020) and fine-level F1-score. To adhere to the zero-shot paradigm, we follow the LogPrompt: for each dataset, the baselines are trained on the initial 10% of the logs and then evaluated on the remaining 90%. Specifically, LogPPT is trained on only the first 0.05% of the logs, while LogPrompt and Owl are directly tested on the remaining 90% of the logs. The prompt for parsing is in Appendix F.4.\nResults and Analysis The results are shown in Table 7. Remarkably, despite its lack of training data, Owl achieves comparable performance on the RandIndex and the best F1 scores. For RandIndex comparison, Owl exhibits only marginal performance degradation than LogStamp. In the realm of fine-level F1 comparisons, Owl outperforms other baselines significantly, displaying a remarkable capacity to accurately identify variables within previously unseen logs. Notably, the foundational model for logPrompt is ChatGPT Ouyang et al. (2022). When compared to ChatGPT under identical fundamental settings, Owl delivers superior performance, underscoring the robust generalization capabilities in operations and maintenance (O&M)."
        },
        {
            "heading": "C MORE DETAILS ON MULTI-TURN DATASET CONSTRUCTION",
            "text": "\u2022 Seed Data Collection: This initial phase involves the meticulous curation of original seed data, a collection painstakingly annotated by domain experts renowned within the operations and maintenance field.\n\u2022 Topic Generation: Building upon the seed data acquired in the preceding step, we harness the GPT-4 OpenAI (2023) to generate a myriad of topics. This process is meticulously designed to ensure that the generated content remains firmly rooted within the operations and maintenance domain, all while maintaining a desirable level of topic diversity.\n\u2022 Multi-turn Dialogue Generation: In this critical stage, we employ the Baize multi-turn dialogue generation method Xu et al. (2023b). Leveraging the topics forged in the prior phase, this method artfully crafts multi-turn dialogue data that is intrinsically tied to the operations and maintenance domain.\n\u2022 Manual and GPT4 Screening: To further bolster the quality of our generated data, we enlist the capabilities of GPT-4 OpenAI (2023). In addition to the automated screening, our dataset undergoes rigorous manual inspection and cross-validation. This meticulous process ensures that each data entry is meticulously reviewed by at least three individuals. Entries that fail to meet the high-quality standards are promptly removed. Consequently, our comprehensive multi-turn dataset comprises a total of 8,740 dialogue entries, with an average of approximately three turns per dialogue."
        },
        {
            "heading": "D MORE DETAILS ON MODEL",
            "text": ""
        },
        {
            "heading": "D.1 ROTARY EMBEDDING",
            "text": "Language models based on Transformers use a self-attention mechanism to consider the positional information of individual tokens, which facilitates the exchange of knowledge between tokens located at various positions. The multi-head attention can be described as:\nXattn = H\u2225\u2225\nh=1\nSF ( QKT\u221a\ndk\n) V (7)\nwhere SF(\u00b7) is the softmax function, and \u2225Hh=1 is the feature concatenation of the H attention heads. The input representation is projected into Q,K, V with the learned matrix.\nTo account for relative position information, we need a function g that operates on word embeddings xm, xn and their relative position (m \u2212 n) as input variables. The objective is to ensure that the inner product of query qm and key kn encode position information solely in its relative form. To incorporate position information with self-attention, we inject the position information into the query and key, respectively. We set the inner product of these two terms to a function explicitly depending on their relative distance as:\nf{q,k}(xm,m) = R d \u0398,mW{q,k}xm (8)\nwhere Rd\u0398,m is the rotary matrix with pre-defined parameters for incorporating the relative position information into the attention mechanism."
        },
        {
            "heading": "D.2 DETAILS ON HMCE",
            "text": "Let T be the text that needs to be generated. Let S := (S1, ..., Sn) be the previous contexts, and that their overall length has exceeded the training length. We want to generate T based on S1, ..., Sn, that is, we want to estimate p(T |S). We assume that S follows a conditional homogeneous Markov chain structure, that is\np(Si|S1:i\u22121, T ) = p(Si|Si\u22121, T ) (9)\nAlso, by the Bayes\u2019 formula and the conditional probability, we have\np(T |S) \u221d p(S|T )p(T ) (10)\np(S, T ) = p(S|T )p(T ) = p(Sn|S1:n\u22121, T )p(S1:n\u22121, T ) (11)\nThus we gain the final log-likelihood of P (T |S). The detailed derivation is in Appendix G.2.\nlog p(T |S) \u221d n\u2211\ni=2 p(T |Si, Si\u22121)\u2212 n\u22121\u2211 i=2 p(T |Si) (12)\nwhere n is the number of separate content. For the original input sentences S longer than maximum length L, we slit them into multiple segments and feed them into the model to compute their corresponding log-likelihoods and finally obtain the log-likelihood of the original input sentence. Besides, we compare Eq. 14 and Eq. 12 and find that NBCE is a special case of HMCE."
        },
        {
            "heading": "E OWL-INSTRUCT AND OWL-BENCH EXAMPLES",
            "text": "Here we give some specific examples from the Owl-Instruct and Owl-Bench."
        },
        {
            "heading": "E.1 SINGLE-TURN CASE IN OWL-INSTRUCT",
            "text": ""
        },
        {
            "heading": "Single-turn Example Structure:",
            "text": "{\u201dq\u201d: \u201dPlease answer the following questions: The server is running slowly and how to troubleshoot and solve it\u201d, \u201da\u201d: \u201d1. Check whether the server hardware resources are overloaded, such as CPU, memory, disk space, etc. 2. Check whether the applications running on the server are running normally, such as checking application logs, monitoring alarms, etc.\u201d }"
        },
        {
            "heading": "E.2 MULTI-TURN CASE IN OWL-INSTRUCT",
            "text": ""
        },
        {
            "heading": "Multi-turn Example Structure:",
            "text": "{\u201did\u201d: \u201d0\u201d, \u201dtopic\u201d: \u201dHow to monitor and manage logs in distributed systems?\u201d, \u201dconversations\u201d: [{\u201d[question]\u201d: \u201dI have a question about distributed systems. Can you tell me how to monitor and manage logs in distributed systems?\u201d, \u201d[answer]\u201d: \u201dWhen monitoring and managing logs in distributed systems, the following are some common practices and tools: 1. Central log collector: Use a central log collector such as Elasticsearch, Logstash, and Kibana (ELK) or Splunk and other tools...\u201d}, {\u201d[question]\u201d: \u201dAmong the tools you mentioned, I am familiar with Elasticsearch and Kibana. Can you explain in detail how to use these two tools to monitor and manage logs in distributed systems?\u201d, \u201d[answer]\u201d: \u201dWhen using Elasticsearch and Kibana to monitor and manage logs in a distributed system, you can follow the following steps:1. Install and configure Elasticsearch: First, install and configure Elasticsearch as the central log storage and search engines...\u201d}, {\u201d[question]\u201d: \u201d...\u201d, \u201d[answer]\u201d: \u201d...\u201d}] }"
        },
        {
            "heading": "E.3 Q&A QUESTION IN OWL-BENCH",
            "text": "Case of Q&A question in Owl-Bench ##Question: {\u201did\u201d: 1, \u201ddomain\u201d: \u201dInformation Security\u201d, \u201dquestion\u201d: [\u201dHow to protect system security?\u201d]} ##Reference {\u201did\u201d: 1, \u201danswer\u201d: [\u201dThe following are several key steps to protect system security: 1. Install security updates: Ensure that all security updates for the system\u2019s operating system, applications, and anti-virus software have been installed , to ensure that the system is not vulnerable to attacks due to known vulnerabilities. 2. Use strong passwords: Use strong passwords and change them regularly to keep accounts secure. 3. Antivirus and anti-malware software: Install and regularly update antivirus and Anti-malware software to ensure that the system is not infected with malware. 4. Timely backup: Back up important data regularly to prevent data loss and malware attacks. 5. Install a firewall: Install and configure a firewall to prevent unauthorized access. 6. Restrict access rights: Restrict user and administrator access rights to ensure that only authorized personnel can access systems and data. 7. Train employees: Conduct security training for employees to improve their security awareness and prevention capabilities. 8. Monitoring logs: Regularly monitor system logs to detect abnormal behaviors and attacks in a timely manner.\u201d]}"
        },
        {
            "heading": "E.4 MULTIPLE-CHOICE EXAMPLES IN OWL-BENCH",
            "text": "In Table 8, we give examples of different domains. The correct answers to the multiple-choice questions are marked in red, and each question has only one correct answer."
        },
        {
            "heading": "F PROMPT TEMPLATES",
            "text": ""
        },
        {
            "heading": "F.1 PROMPT FOR DATA QUALITY",
            "text": "The prompt we employ for GPT-4 is deliberately tailored to elicit insightful evaluations and is as follows:\nQuality Prompt: You are now a data grader. You will grade the data I provide according to my requirements, explain the reasons, and then give a piece of higher-quality data based on this piece of data. Please help me rate the following dialogue data in the field of operation and maintenance and explain the reasons. Require: 1. Scoring perspective: whether the problem belongs to the field of operation and maintenance; whether the problem description is clear; whether the answer is accurate; whether the problem has a certain meaning; whether the language is coherent; whether the problem is challenging and difficult. 2. Point scale: 5-point scale, 1 point: very poor; 2 points: slightly poor; 3 points: barely qualified; 4 points: usable; 5 points: excellent. 3. Please rate the problem and attach reasons. If the score is lower than 4 points, a higher quality data will be generated based on this piece of data. 4. Format: You can only return a parsable json format data, no other content. For example: \u201dscore\u201d: 4, \u201dreason\u201d: \u201d\u201d, \u201dmodified-data\u201d: \u201d\u201d. Among them, score represents the score for this question, reason represents the reason for the score, and states the advantages and disadvantages of the data, and modifieddata represents You generated a new, higher-quality data based on the above data. Compared with the data provided, this new data solves the shortcomings you mentioned above and is directly available. 5. All reasons are written in reason. 6. If the score is lower than 4 points, modified-data must be provided. 7. Modified-data contains a complete piece of data that is directly available, and the quality must be higher and more in line with the quality of ChatGPT\u2019s training data. If null needs to be output, replace it with None. Now please follow the above requirements to annotate the following conversation data and return your annotated results in pure json form: \u201d\u201d.\nA Case for Data Quality: Raw data: { \u201dinstruction\u201d: \u201dHow to install mysql?\u201d, \u201dinput\u201d: \u201d\u201d, \u201doutput\u201d: \u201d1. Download MySQL Community Server; 2. Install MySQL Community Server; 3. Start MySQL Community Server; 4. Set the MySQL root account password.\u201d } Score: { \u201dscore\u201d: 2, \u201dreason\u201d: \u201dThe question does not entirely belong to the field of operation and maintenance, and the description of the problem is not clear enough, and it does not explain in what environment mysql is installed. In addition, the answer is not satisfactory enough and is relatively simple. Although the operation steps are given, there is no explanation Users would like to get more extensive answers regarding the specific operation methods of each step, such as explaining how to set the MySQL root account password next.\u201d, \u201dmodified-data\u201d: { \u201dinstruction\u201d: \u201dHow to install mysql on mac? Please explain the specific installation steps in detail\u201d, \u201dinput\u201d: \u201d\u201d, \u201doutput\u201d: \u201dThere are many ways to install MySQL on Mac. Here is one of the common methods: 1. Download MySQL Community Server: Download MySQL Community Server for Mac OS X from the official MySQL website at https://dev.mysql.com/downloads/mysql/. 2. Install MySQL Community Server: Double-click the downloaded dmg file, and then follow the prompts to install. 3. Start MySQL Community Server: Open the \u201dMySQL\u201d option in \u201dSystem Preferences\u201d and start MySQL Community Server. 4. Set the MySQL root account password: Enter the following command in the terminal to set the root account password: \u201c\u2018 /usr/local/mysql/bin/mysqladmin -u root password \u2019yourpassword\u2019 \u201c\u2018 (Note: Replace \u2019yourpassword\u2019 with your own password) 5. Test MySQL: Enter the following command in the terminal to test whether MySQL has been successfully installed and running: \u201c\u2018 /usr/local/mysql/bin/mysql -u root -p \u201c\u2018 Then enter the root account password. If you successfully log in to MySQL, the installation and configuration have been completed. The above is a common way to install MySQL in Mac, but there are other methods and you can choose according to your needs and preferences. \u201d } }"
        },
        {
            "heading": "F.2 PROMPT FOR SINGLE-SCORE MODE",
            "text": ""
        },
        {
            "heading": "Prompt for single-score mode:",
            "text": "You are a helpful assistant. [Instruction] Please act as an impartial judge and evaluate the quality of the response provided by an AI assistant to the user question displayed below. Your evaluation should consider correctness and helpfulness. You will be given a reference answer and the assistant\u2019s answer. Begin your evaluation by comparing the assistant\u2019s answer with the reference answer. Identify and correct any mistakes. Be as objective as possible. After providing your explanation, you must rate the response on a scale of 1 to 10 by strictly following this format: \u201d[[rating]]\u201d, for example: \u201dRating: [[5]]\u201d. [Question]: ... [The Start of Reference Answer] .... [The End of Reference Answer] [The Start of Assistant\u2019s Answer] .... [The End of Assistant\u2019s Answer]"
        },
        {
            "heading": "F.3 PROMPT FOR PAIRWISE-SCORE MODE",
            "text": ""
        },
        {
            "heading": "Prompt for pairwise-score mode:",
            "text": "Please act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user question displayed below. Your evaluation should consider correctness and helpfulness. You will be given a reference answer, assistant A\u2019s answer, and assistant B\u2019s answer. Your job is to evaluate which assistant\u2019s answer is better. Begin your evaluation by comparing both assistants\u2019 answers with the reference answer. Identify and correct any mistakes. Avoid any positional biases and ensure that the order in which the responses were presented does not influence your decision. Do not allow the length of the responses to influence your evaluation. Do not favor certain names of the assistants. Be as objective as possible. After providing your explanation, output your final verdict by strictly following this format: \u201d[[A]]\u201d if assistant A is better, \u201d[[B]]\u201d if assistant B is better, and \u201d[[C]]\u201d for a tie. [User Question]:... [The Start of Reference Answer] ... [The End of Reference Answer] [The Start of Assistant A\u2019s Answer] ... [The End of Assistant A\u2019s Answer] [The Start of Assistant B\u2019s Answer] ... [The End of Assistant B\u2019s Answer]"
        },
        {
            "heading": "F.4 PROMPT FOR LOG PARSING",
            "text": "Log Parsing Prompt: Convert the following log into a standardized template by identifying and replacing the variable parts with a * and retain the keywords: [Input Log]"
        },
        {
            "heading": "F.5 PROMPT FOR ANOMALY DETECTION",
            "text": "Anomaly Detection prompt: Classify the given logs into normal and abnormal categories. Do it with these steps: (a) Mark it normal when values (such as memory address, floating number and register value) in a log are invalid. (b) Mark it normal when lack of information. (c) Never consider \u27e8\u2217\u27e9 and missing values as abnormal patterns. (d) Mark it abnormal when and only when the alert is explicitly expressed in textual content (such as keywords like error or interrupt)."
        },
        {
            "heading": "G MATHEMATICAL DERIVATION",
            "text": ""
        },
        {
            "heading": "G.1 DERIVATION OF NBCE",
            "text": "Assuming T is the target sentence to be generated and S = (S1, . . . , Sn) are given sets of relatively independent context separated from the original sentence S, we need to generate the target sentence T based on the independent sequence S = (S1, . . . , Sn). The target sentence can be estimated as P (T |S1, . . . , Sn):\nP (T |S) = P (S|T )P (T ) P (S) \u2248 N\u220f i=1 P (Sk|T )P (T ) \u2248 1 P (T )n\u22121 N\u220f k=1 P (Sk|T ) (13)\nwhere P (S|T )P (T )P (S) follows the independent assumption and the item P (S) is omitted. Equation 13 can be further rewrited as with P (Sk|T ) = P (T |Sk)P (T ) The log-likelihood of P (T |S) can be represented by:\nlogP (T |S) = n\u2211\nk=1\nlogP (Sk|T )\u2212 (n\u2212 1) logP (T ) (14)"
        },
        {
            "heading": "G.2 DERIVATION OF HMCE",
            "text": "Let T be the text that needs to be generated. Let S := (S1, ..., Sn) be the previous contexts, and that their overall length has exceeded the training length. We want to generate T based on S1, ..., Sn, that is, we want to estimate p(T |S). We assume that S follows a conditional homogeneous Markov chain structure, that is\np(Si|S1:i\u22121, T ) = p(Si|Si\u22121, T ) Also, by the Bayes\u2019 formula, we have\np(T |S) \u221d p(S|T )p(T ) And based on the conditional probability, we have\np(S, T ) = p(S|T )p(T ) = p(Sn|S1:n\u22121, T )p(S1:n\u22121, T ) Thus we reduce the formula\np(S|T ) = p(Sn|S1:n\u22121, T )p(S1:n\u22121|T ) = ...\n= n\u220f i=2 p(Si|Si\u22121, T )p(S1|T )\nAnd p(Si|Si\u22121, T ) \u221d p(T |Si, Si\u22121)/p(T |Si\u22121)\nHence\np(T |S) \u221d n\u220f\ni=2\np(T |Si, Si\u22121) p(T |Si\u22121) p(T |S1)\nTherefore\nlog p(T |S) \u221d n\u2211\ni=2 p(T |Si, Si\u22121)\u2212 n\u22121\u2211 i=2 p(T |Si)"
        },
        {
            "heading": "H DETAILS FOR DATA COLLECTION IN OWL-BENCH",
            "text": "Our primary data source comprises practice exams that have been made freely accessible on the Internet. These practice exams, often used for honing the skills of aspiring professionals, serve as a valuable repository of real-world questions and scenarios within the operations and maintenance domain. Additionally, to further enrich the quality and authenticity of our benchmark, we have collaborated with operations and maintenance experts. Approximately 200 questions meticulously vetted by these experts have been seamlessly integrated into the Owl-Bench. This collaborative effort not only bolsters the benchmark\u2019s credibility but also infuses it with domain-specific expertise. It is worth emphasizing our commitment to openness and knowledge sharing. All the questions within Owl-Bench are slated to be open-sourced, a testament to our dedication to fostering a collaborative and transparent environment within the research community. This initiative is poised to facilitate broader access to high-quality data for the advancement of operations and maintenancerelated research and innovation."
        },
        {
            "heading": "I RELATED WORKS",
            "text": "Language Models. Language is a distinct human skill that evolves throughout life and starts developing in early childhood (Bai et al., 2023a; Guo et al., 2023b; 2022; Liu et al., 2022). Machines lack an inherent capacity to naturally comprehend and employ human language without the aid of advanced artificial intelligence (AI) algorithms. Language modeling based on the self-supervised learning training objective and large-scale data has been widely used to acquire contextual representations. Pre-training a large Transformer encoder/decoder (Vaswani et al., 2017; Chi et al., 2020; Yang et al., 2023b) brings significant improvement for various downstream natural language processing tasks. Besides, pre-training a Transformer decoder (Brown et al., 2020b) is beneficial for unconditional text generation. Enlarging Pre-training Language Models (PLMs) by increasing their model or data size brings in huge performance improvement in various tasks, adhering to a known scaling principle. To explore this, numerous studies have pushed the boundaries by training increasingly larger PLMs (Chowdhery et al., 2022a; Anil et al., 2023; Touvron et al., 2023a;b; Xu et al., 2023a; Ghosal et al., 2023; Bai et al., 2023b), such as the 175-billion parameter GPT-3 and the 540- billion parameter PaLM. The scaling laws of large language model (Kaplan et al., 2020; Aghajanyan et al., 2023) can guide the training of the large language model.\nDespite primarily focusing on scaling model size while retaining similar architectures and pretraining tasks, these expansive PLMs exhibit distinct behaviors compared to smaller ones (Brown et al., 2020b; OpenAI, 2023). These extensive models showcase unforeseen capabilities, often referred to as \u201cemergent abilities\u201d, which enable them to excel in intricate tasks. An exemplary illustration of the LLM application is observed in ChatGPT, which adapts LLMs from the GPT series to engage in dialogues, demonstrating a remarkable conversational prowess with humans. Fine-tuned LLMs on numerous datasets show promising results (Chung et al., 2022; Wang et al., 2023a; Luo et al., 2023; Wei et al., 2022; Chen et al., 2022), where the prompts used for instruction tuning can be created by humans or by LLMs themselves, and follow-up instructions can be used to refine the generation. An approach (Wei et al., 2022; Chen et al., 2022) related to instruction tuning is chainof-thought prompting, where models are prompted to explain their reasoning when given a complex problem, in order to increase the likelihood that their final answer is correct. RLHF (Ouyang et al., 2022) has emerged as a powerful strategy for fine-tuning Large Language Models, enabling significant improvements in their performance. In our work, we collect the Owl-Instruct to train and evaluate the proposed OWL.\nSpecialized Large Language Models. The value of training specialized decoder-only large language models is widely used in many fields, such as Financial LLMs (Wu et al., 2023; Yang et al., 2023a), Code LLMs (Rozie\u0300re et al., 2023; Luo et al., 2023), and Layer LLMs (Cui et al., 2023; Nguyen, 2023). Common strategies involve training specialized models to continue to pre-train an existing model using new domain-specific data. In the field of IT operations, natural language processing (NLP) technologies have assumed a paramount role (Zhang et al., 2017; Messaoudi et al., 2018; Fu et al., 2009; Du & Li, 2016; He et al., 2017; Zhang et al., 2019), such as log analysis and parsing. The large language model for IT Operations can handle question-answering tasks in\nmany fields, such as infrastructure operation&maintenance, middleware operation&maintenance, and software architecture operation&maintenance."
        }
    ],
    "year": 2023
}