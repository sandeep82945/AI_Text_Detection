{
    "abstractText": "Contrastive Learning (CL) has emerged as one of the most successful paradigms for unsupervised visual representation learning, yet it often depends on intensive manual data augmentations. With the rise of generative models, especially diffusion models, the ability to generate realistic images close to the real data distribution has been well recognized. These generated high-equality images have been successfully applied to enhance contrastive representation learning, a technique termed \u201cdata inflation\u201d. However, we find that the generated data (even from a good diffusion model like DDPM) may sometimes even harm contrastive learning. We investigate the causes behind this failure from the perspective of both data inflation and data augmentation. For the first time, we reveal the complementary roles that stronger data inflation should be accompanied by weaker augmentations, and vice versa. We also provide rigorous theoretical explanations for these phenomena via deriving its generalization bounds under data inflation. Drawing from these insights, we propose Adaptive Inflation (AdaInf), a purely data-centric strategy without introducing any extra computation cost. On benchmark datasets, AdaInf can bring significant improvements for various contrastive learning methods. Notably, without using external data, AdaInf obtains 94.70% linear accuracy on CIFAR-10 with SimCLR, setting a new record that surpasses many sophisticated methods.",
    "authors": [],
    "id": "SP:22e1266318cb4d53de03f20623b82f8fc489d289",
    "references": [
        {
            "authors": [
                "Shekoofeh Azizi",
                "Simon Kornblith",
                "Chitwan Saharia",
                "Mohammad Norouzi",
                "David J. Fleet"
            ],
            "title": "Synthetic data from diffusion models improves imagenet classification",
            "venue": "arXiv preprint arXiv:2304.08466,",
            "year": 2023
        },
        {
            "authors": [
                "Dmitry Baranchuk",
                "Andrey Voynov",
                "Ivan Rubachev",
                "Valentin Khrulkov",
                "Artem Babenko"
            ],
            "title": "Labelefficient semantic segmentation with diffusion models",
            "venue": "In ICLR,",
            "year": 2022
        },
        {
            "authors": [
                "Victor Besnier",
                "Himalaya Jain",
                "Andrei Bursuc",
                "Matthieu Cord",
                "Patrick P\u00e9rez"
            ],
            "title": "This dataset does not exist: Training models from generated images",
            "venue": "In ICASSP,",
            "year": 2020
        },
        {
            "authors": [
                "Sam Bond-Taylor",
                "Adam Leach",
                "Yang Long",
                "Chris G. Willcocks"
            ],
            "title": "Deep generative modelling: A comparative review of vaes, gans, normalizing flows, energy-based and autoregressive models",
            "venue": "Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "Mathilde Caron",
                "Ishan Misra",
                "Julien Mairal",
                "Priya Goyal",
                "Piotr Bojanowski",
                "Armand Joulin"
            ],
            "title": "Unsupervised learning of visual features by contrasting cluster assignments",
            "venue": "NeurIPS,",
            "year": 2020
        },
        {
            "authors": [
                "Mathilde Caron",
                "Hugo Touvron",
                "Ishan Misra",
                "Herv\u00e9 J\u00e9gou",
                "Julien Mairal",
                "Piotr Bojanowski",
                "Armand Joulin"
            ],
            "title": "Emerging properties in self-supervised vision transformers",
            "venue": "In ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "Ting Chen",
                "Simon Kornblith",
                "Mohammad Norouzi",
                "Geoffrey Hinton"
            ],
            "title": "A simple framework for contrastive learning of visual representations",
            "venue": "In ICML,",
            "year": 2020
        },
        {
            "authors": [
                "Xinlei Chen",
                "Kaiming He"
            ],
            "title": "Exploring simple siamese representation learning",
            "venue": "In CVPR,",
            "year": 2021
        },
        {
            "authors": [
                "Xinlei Chen",
                "Haoqi Fan",
                "Ross Girshick",
                "Kaiming He"
            ],
            "title": "Improved baselines with momentum contrastive learning",
            "venue": "arXiv preprint arXiv:2003.04297,",
            "year": 2020
        },
        {
            "authors": [
                "Xinlei Chen",
                "Saining Xie",
                "Kaiming He"
            ],
            "title": "An empirical study of training self-supervised vision transformers",
            "venue": "In ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "Yuhua Chen",
                "Wen Li",
                "Xiaoran Chen",
                "Luc Van Gool"
            ],
            "title": "Learning semantic segmentation from synthetic data: A geometrically guided input-output adaptation approach",
            "year": 2019
        },
        {
            "authors": [
                "Fan Chung",
                "Paul Horn"
            ],
            "title": "The spectral gap of a random subgraph of a graph",
            "venue": "Internet Mathematics,",
            "year": 2007
        },
        {
            "authors": [
                "Fan RK Chung"
            ],
            "title": "Spectral graph theory, volume 92",
            "venue": "American Mathematical Soc.,",
            "year": 1997
        },
        {
            "authors": [
                "Enrico Fini",
                "Moin Nabi",
                "Nicu Sebe",
                "Elisa Ricci"
            ],
            "title": "sololearn: A library of self-supervised methods for visual representation learning",
            "venue": "Journal of Machine Learning Research,",
            "year": 2022
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "Philipp Fischer",
                "Eddy Ilg",
                "Philip H\u00e4usser",
                "Caner Hazirbas",
                "Vladimir Golkov",
                "Patrick van der Smagt",
                "Daniel Cremers",
                "Thomas Brox"
            ],
            "title": "Flownet: Learning optical flow with convolutional networks",
            "venue": "In ICCV,",
            "year": 2015
        },
        {
            "authors": [
                "Quentin Garrido",
                "Yubei Chen",
                "Adrien Bardes",
                "Laurent Najman",
                "Yann LeCun"
            ],
            "title": "On the duality between contrastive and non-contrastive self-supervised learning",
            "venue": "In ICLR,",
            "year": 2023
        },
        {
            "authors": [
                "Ian J Goodfellow",
                "Jean Pouget-Abadie",
                "Mehdi Mirza",
                "Bing Xu",
                "David Warde-Farley",
                "Sherjil Ozair",
                "Aaron Courville",
                "Yoshua Bengio"
            ],
            "title": "Generative adversarial networks",
            "venue": "NeurIPS,",
            "year": 2014
        },
        {
            "authors": [
                "Jean-Bastien Grill",
                "Florian Strub",
                "Florent Altch\u00e9",
                "C. Tallec",
                "Pierre H. Richemond",
                "Elena Buchatskaya",
                "C. Doersch",
                "Bernardo Avila Pires",
                "Zhaohan Daniel Guo",
                "Mohammad Gheshlaghi Azar",
                "B. Piot",
                "K. Kavukcuoglu",
                "R\u00e9mi Munos",
                "Michal Valko"
            ],
            "title": "Bootstrap your own latent: a new approach to self-supervised learning",
            "venue": "NeurIPS,",
            "year": 2020
        },
        {
            "authors": [
                "Xi Guo",
                "Wei Wu",
                "Dongliang Wang",
                "Jing Su",
                "Haisheng Su",
                "Weihao Gan",
                "Jian Huang",
                "Qin Yang"
            ],
            "title": "Learning video representations of human motion from synthetic data",
            "year": 2022
        },
        {
            "authors": [
                "Jeff Z HaoChen",
                "Tengyu Ma"
            ],
            "title": "A theoretical study of inductive biases in contrastive learning",
            "venue": "In ICLR,",
            "year": 2023
        },
        {
            "authors": [
                "Jeff Z HaoChen",
                "Colin Wei",
                "Adrien Gaidon",
                "Tengyu Ma"
            ],
            "title": "Provable guarantees for self-supervised deep learning with spectral contrastive loss",
            "venue": "NeurIPS,",
            "year": 2021
        },
        {
            "authors": [
                "Kaiming He",
                "Haoqi Fan",
                "Yuxin Wu",
                "Saining Xie",
                "Ross Girshick"
            ],
            "title": "Momentum contrast for unsupervised visual representation learning",
            "venue": "In CVPR,",
            "year": 2020
        },
        {
            "authors": [
                "Ruifei He",
                "Shuyang Sun",
                "Xin Yu",
                "Chuhui Xue",
                "Wenqing Zhang",
                "Philip H.S. Torr",
                "Song Bai",
                "Xiaojuan Qi"
            ],
            "title": "Is synthetic data from generative models ready for image recognition",
            "venue": "In ICLR,",
            "year": 2023
        },
        {
            "authors": [
                "Jonathan Ho",
                "Ajay Jain",
                "Pieter Abbeel"
            ],
            "title": "Denoising diffusion probabilistic models",
            "venue": "In NeurIPS,",
            "year": 2020
        },
        {
            "authors": [
                "Shahram Izadi",
                "David Kim",
                "Otmar Hilliges",
                "David Molyneaux",
                "Richard A. Newcombe",
                "Pushmeet Kohli",
                "Jamie Shotton",
                "Steve Hodges",
                "Dustin Freeman",
                "Andrew J. Davison",
                "Andrew W. Fitzgibbon"
            ],
            "title": "Kinectfusion: real-time 3d reconstruction and interaction using a moving depth camera",
            "venue": "In UIST,",
            "year": 2011
        },
        {
            "authors": [
                "Ali Jahanian",
                "Xavier Puig",
                "Yonglong Tian",
                "Phillip Isola"
            ],
            "title": "Generative models as a data source for multiview representation learning",
            "venue": "In ICLR,",
            "year": 2022
        },
        {
            "authors": [
                "Manel Baradad Jurjo",
                "Jonas Wulff",
                "Tongzhou Wang",
                "Phillip Isola",
                "Antonio Torralba"
            ],
            "title": "Learning to see by looking at noise",
            "venue": "NeurIPS,",
            "year": 2021
        },
        {
            "authors": [
                "Tero Karras",
                "Miika Aittala",
                "Janne Hellsten",
                "Samuli Laine",
                "Jaakko Lehtinen",
                "Timo Aila"
            ],
            "title": "Training generative adversarial networks with limited data",
            "venue": "NeurIPS",
            "year": 2020
        },
        {
            "authors": [
                "Tero Karras",
                "Miika Aittala",
                "Timo Aila",
                "Samuli Laine"
            ],
            "title": "Elucidating the design space of diffusionbased generative models",
            "venue": "In NeurIPS,",
            "year": 2022
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Max Welling"
            ],
            "title": "Auto-encoding variational Bayes",
            "venue": "In ICLR,",
            "year": 2014
        },
        {
            "authors": [
                "Daiqing Li",
                "Junlin Yang",
                "Karsten Kreis",
                "Antonio Torralba",
                "Sanja Fidler"
            ],
            "title": "Semantic segmentation with generative models: Semi-supervised learning and strong out-of-domain generalization",
            "year": 2021
        },
        {
            "authors": [
                "Daiqing Li",
                "Huan Ling",
                "Seung Wook Kim",
                "Karsten Kreis",
                "Sanja Fidler",
                "Antonio Torralba"
            ],
            "title": "Bigdatasetgan: Synthesizing imagenet with pixel-wise annotations",
            "year": 2022
        },
        {
            "authors": [
                "Jianxin Ma",
                "Shuai Bai",
                "Chang Zhou"
            ],
            "title": "Pretrained diffusion models for unified human motion synthesis",
            "venue": "arXiv preprint arXiv:2212.02837,",
            "year": 2022
        },
        {
            "authors": [
                "Aaron van den Oord",
                "Yazhe Li",
                "Oriol Vinyals"
            ],
            "title": "Representation learning with contrastive predictive coding",
            "venue": "arXiv preprint arXiv:1807.03748,",
            "year": 2018
        },
        {
            "authors": [
                "Zhongzheng Ren",
                "Yong Jae Lee"
            ],
            "title": "Cross-domain self-supervised multi-task feature learning using synthetic imagery",
            "year": 2018
        },
        {
            "authors": [
                "Swami Sankaranarayanan",
                "Yogesh Balaji",
                "Arpit Jain",
                "Ser Nam Lim",
                "Rama Chellappa"
            ],
            "title": "Learning from synthetic data: Addressing domain shift for semantic segmentation",
            "year": 2018
        },
        {
            "authors": [
                "Nikunj Saunshi",
                "Jordan Ash",
                "Surbhi Goel",
                "Dipendra Misra",
                "Cyril Zhang",
                "Sanjeev Arora",
                "Sham Kakade",
                "Akshay Krishnamurthy"
            ],
            "title": "Understanding contrastive learning requires incorporating inductive biases",
            "year": 2022
        },
        {
            "authors": [
                "Yonglong Tian",
                "Chen Sun",
                "Ben Poole",
                "Dilip Krishnan",
                "Cordelia Schmid",
                "Phillip Isola"
            ],
            "title": "What makes for good views for contrastive learning",
            "venue": "NeurIPS,",
            "year": 2020
        },
        {
            "authors": [
                "Yonglong Tian",
                "Lijie Fan",
                "Phillip Isola",
                "Huiwen Chang",
                "Dilip Krishnan"
            ],
            "title": "Stablerep: Synthetic images from text-to-image models make strong visual representation learners",
            "venue": "arXiv preprint arXiv:2306.00984,",
            "year": 2023
        },
        {
            "authors": [
                "Yuandong Tian",
                "Xinlei Chen",
                "S. Ganguli"
            ],
            "title": "Understanding self-supervised learning dynamics without contrastive pairs",
            "venue": "In ICML,",
            "year": 2021
        },
        {
            "authors": [
                "Nontawat Tritrong",
                "Pitchaporn Rewatbowornwong",
                "Supasorn Suwajanakorn"
            ],
            "title": "Repurposing gans for one-shot semantic part segmentation",
            "venue": "In CVPR,",
            "year": 2021
        },
        {
            "authors": [
                "G\u00fcl Varol",
                "Javier Romero",
                "Xavier Martin",
                "Naureen Mahmood",
                "Michael J. Black",
                "Ivan Laptev",
                "Cordelia Schmid"
            ],
            "title": "Learning from synthetic humans",
            "year": 2017
        },
        {
            "authors": [
                "Tongzhou Wang",
                "Phillip Isola"
            ],
            "title": "Understanding contrastive representation learning through alignment and uniformity on the hypersphere",
            "venue": "In ICML,",
            "year": 2020
        },
        {
            "authors": [
                "Yifei Wang",
                "Qi Zhang",
                "Yisen Wang",
                "Jiansheng Yang",
                "Zhouchen Lin"
            ],
            "title": "Chaos is a ladder: A new theoretical understanding of contrastive learning via augmentation overlap",
            "venue": "In ICLR,",
            "year": 2022
        },
        {
            "authors": [
                "Yawen Wu",
                "Zhepeng Wang",
                "Dewen Zeng",
                "Yiyu Shi",
                "Jingtong Hu"
            ],
            "title": "Synthetic data can also teach: Synthesizing effective data for unsupervised visual representation learning",
            "venue": "In AAAI,",
            "year": 2023
        },
        {
            "authors": [
                "Yilun Xu",
                "Shangyuan Tong",
                "Tommi S. Jaakkola"
            ],
            "title": "Stable target field for reduced variance score estimation in diffusion models",
            "venue": "In ICLR,",
            "year": 2023
        },
        {
            "authors": [
                "Jure Zbontar",
                "Li Jing",
                "Ishan Misra",
                "Yann LeCun",
                "St\u00e9phane"
            ],
            "title": "Deny. Barlow twins: Self-supervised learning via redundancy reduction",
            "venue": "In ICML,",
            "year": 2021
        },
        {
            "authors": [
                "Karras"
            ],
            "title": "Additionally, the generated data for EDM (7.29 FID",
            "year": 2022
        },
        {
            "authors": [
                "HaoChen"
            ],
            "title": "Assume the set of augmented data X is finite. Let f\u2217 \u2208 argminf :X\u2192Rk be a minimizer of the population spectral contrastive loss L(f) with k \u2208 Z. Then, for any labeling function \u0177 : X \u2190 [r] there exists a linear probe B\u2217 \u2208 Rr\u00d7k with",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Contrastive learning has to be the state-of-the-art method for self-supervised representation learning across many domains (Chen et al., 2020a; He et al., 2020). However, there still remains a noticeable gap in performance compared to its supervised counterparts (Chen et al., 2021). Among many attempts to close this gap, a recent surge of interest lies in leveraging high-quality generative models to boost contrastive learning (Wu et al., 2023; Azizi et al., 2023; Tian et al., 2023). Given an unlabeled dataset, e.g., CIFAR-10, one can train a generative model on it (e.g., DDPM (Ho et al., 2020)) to generate a lot of synthetic samples, and then perform contrastive learning on the combination of the real and generated data. This simplest way for using generated data is called \u201cdata inflation\u201d. Notably, it is orthogonal to the data augmentation process, wherein an image\u2013be it raw or generated\u2013is subjected to manual augmentations (e.g., cropping) to yield positive and negative pairs used in contrastive learning (see Figure 1(a) for an illustration of the pipeline).\nThough one may expect that generated data will benefit contrastive learning with more diverse data, we find that it is not always the case. As shown in Figure 1(b), simply inflating CIFAR-10 with 1M images generated by DDPM (the vanilla setting) leads to even worse linear probing accuracy (91.33% v.s. 90.27%). We investigate this unexpected performance degradation from two aspects: data inflation (how inflated data are constructed) and data augmentation (how to craft augmented samples with inflated data). For the former, we find that better generation quality is of limited help, while reweighting real and generated data can attain larger gains. For the latter, we discover an intriguing phenomenon that weaker data augmentation, although harmful in standard contrastive learning (Tian et al., 2020), can be very helpful with data inflation. To uncover the mysteries behind these observations, we establish the first generalization guarantees for inflated contrastive learning, and explain the benefits of weak augmentations by revealing the complementary roles between data inflation and data augmentation. Based on these insights, we propose an Adaptive Inflation (AdaInf) strategy that adaptively adjusts data augmentation strength and mixing ratio for data inflation, which can bring significant improvements in downstream performance without any introducing computation overhead. We summarize our contributions as follows:\n\u2022 We discover a failure mode of data inflation for contrastive learning, and reveal the causes of this failure from the perspective of both data inflation and data augmentation. In particular, we find that data reweighting and weak augmentation contribute significantly to improving final performance.\n\u2022 To understand these phenomena, we establish the first theoretical guarantees for inflated contrastive learning, which not only rigorously explain previous phenomena, but also reveal the complementary roles between data inflation and data augmentation.\n\u2022 We propose the Adaptive Inflation (AdaInf) strategy to adaptively adjust data augmentation strength and mixing ratio for data inflation. Extensive experiments show that the proposed approach improves downstream accuracy significantly at no extra cost, and it is particularly beneficial for data-scarce scenarios."
        },
        {
            "heading": "2 PRELIMINARY & RELATED WORK",
            "text": "Self-Supervised Learning. Given an unlabelled dataset D that contain raw samples x\u0304 \u2208 Rd, the goal of self-supervised learning is to pretrain a feature extraction f : Rd \u2192 Rz on D such that the learned representations generalize well to downstream tasks. In this paper, we mainly consider contrastive learning as a representative example. For each sample x\u0304 \u2208 D, we draw two randomly augmented samples x, x+ \u223c A(\u00b7|x\u0304) as a positive pair. The general learning objective of contrastive learning is to align the representations of positive samples while pushing negative samples apart, as in the following widely adopted InfoNCE loss (Oord et al., 2018; Wang and Isola, 2020):\nLInfoNCE(f,D) = \u2212Ex,x+,{x\u2212i }Mi=1 log exp\n( f(x)\u22a4f(x+) ) exp (f(x)\u22a4f(x+)) + \u2211M i=1 exp ( f(x)\u22a4f(x\u2212i )\n) , (1) where {x\u2212i }Mi=1 are M negative samples drawn independently from D with data augmentationA(\u00b7). Besides, some variants propose to drop negative samples and adopt asymmetric modules to encoder positive pairs to avoid feature collapses, such as BYOL (Grill et al., 2020), SimSiam (Chen and He, 2021), SwAV (Caron et al., 2020), and DINO (Caron et al., 2021). Some propose to use volume maximization regularization terms to replace the negative samples and obtain similar performance, like Barlow Twins (Zbontar et al., 2021). Recent theoretical analyses show there exists a deep connection between contrastive learning and these variants (Tian et al., 2021; Garrido et al., 2023). Therefore, we regard them as general contrastive learning methods.\nGenerative Models. Generative models refer to a broad class of models that learn the data distribution P (x). Popular generative models include GANs (Goodfellow et al., 2014), VAEs (Kingma and Welling, 2014), diffusion models (Ho et al., 2020), etc. In this paper, we mainly take diffusion models for an example due its superior generation quality. During training time, we add random Gaussian noise of scale t \u2208 [0, T ] to an image x\u0304 \u2208 D, and train a denoising network \u03b5\u03b8 : Rd \u2192 Rd (typically a U-Net) to reconstruct the ground-truth noise added to the image x\u0304, i.e.,\nLSM (g,D) = Et,x\u0304\u2208D,\u03b5t\u2225\u03b5\u03b8( \u221a \u03b1\u0304tx\u0304+ \u221a 1\u2212 \u03b1\u0304t\u03b5t, t)\u2212 \u03b5t\u22252, (2)\nwhere \u03b1\u0304t is the mixing coefficient at time t (Ho et al., 2020). In this paper, to enhance contrastive learning on unlabeled data, we train an unsupervised diffusion model with real data (e.g., CIFAR-10), sample one million generated samples from the diffusion model, and append them to the real data. During this process, we can inflate the training data from 50k samples to more than 1M samples, so we call it data inflation. Remarkably, we do not use any external data or model since the diffusion model is also trained on the same training dataset.\nLearning with Generated Data. Using synthetic data from generative models has been explored in many scenarios (Sankaranarayanan et al., 2018; Dosovitskiy et al., 2015; Ma et al., 2022). Recently, due to the rise of diffusion models that are able to synthesize high-quality images, generated data have also been extensively studied for enhancing representation learning. Jahanian et al. (2022) and Tian et al. (2023) show that using generated data alone can achieve comparable performance to real data for contrastive learning with proper configurations of the generative models. While a major drawback of these methods is that they often require problem-specific designs of the sampling process which can be costly. Instead, we explore whether generated data with a standard sampling process can help contrastive learning by enlarging the dataset. Azizi et al. (2023) recently show that this kind of generated data can significantly improve supervised learning by around 1% accuracy on ImageNet. In view of these successes, the goal of this work is to deeply investigate how generated data influence contrastive learning, and provide theoretical explanations for these effects."
        },
        {
            "heading": "3 UNCOVERING REASONS BEHIND THE FAILURE OF DATA INFLATION",
            "text": "As shown in Figure 1(b), we discover that directly adding 1M images generated by DDPM (Ho et al., 2020) may yield minimal or even negative improvements on contrastive learning. In this section, we explore the reasons behind this failure from two aspects, the generated data and data augmentation, and design effective strategies to mitigate these failures."
        },
        {
            "heading": "3.1 CAUSES IN DATA INFLATION: DATA QUALITY AND DATA REWEIGHTING",
            "text": "First, we investigate whether the failure lies in our design of data inflation. Denote the distribution of the real dataDd as Pd, and that of the generated dataDg as Pg . After inflation, the overall training distribution becomes Pt = \u03b2Pd+(1\u2212\u03b2)Pg , where \u03b2 = |Dd|/(|Dd|+ |Dg|) denotes the proportion of the real data when equally mixing them together. The distribution gap between real and generated data can be characterized by the following Theorem 3.1 (proof in Appendix D.1):\nTheorem 3.1. DTV(Pt, Pd) = (1\u2212 \u03b2)DTV(Pg, Pd), where DTV denotes the TV distance.\nFrom the above, we can see that there are two factors influencing the distribution gap: the generated data Pg and the mixing ratio \u03b2.\nGenerated Data Quality. A straightforward reason for the failure is that the generative model, DDPM, is not good enough. Since the generative model is not perfect, the distribution gap between real and generated data will be large. Thus, there will be a large mismatch between training and testing data, which prevents generalization. In turn, as long as Pg = Pd, generated data will always be helpful (with more training examples). Thus, a direct solution to the degradation is to use a better generative model with a smaller gap to real data. To validate this point, we compare four diffusion models with different generation qualities (measured by FID). Figure 2(a) shows that indeed, diffusion models with lower FID, such as STF (Xu et al., 2023), consistently bring better downstream accuracy. However, we also notice two drawbacks. First, better generative quality often requires larger models and/or slower sampling (e.g., more denoising steps) (Bond-Taylor et al., 2022), which detriments the efficiency. Second, the improvement over the baseline (91.33%) is marginal, as using the best diffusion model STF only gains +0.02% accuracy, which is less worth the effort. Therefore, in the rest of the discussion, we fix the generative model to be STF and explore how to improve downstream performance with other techniques.\nData Reweighting. Beside generated data quality, Theorem 3.1 suggests another useful strategy, data reweighting. We can upweight the real data (or downweighting the generated data) with a larger mixing ratio \u03b2 which can lead to a smaller gap DTV(Pt, Pd). In practice, we upweight the real data by replicating it N times during the mixing (equivalent to \u03b2 = N \u00b7 |Dd|/(N \u00b7 |Dd|+ |Dg|)). Figure 2(b) shows that a medium replication N = 10 yields the best linear accuracy, meaning that the optimal weight between real and generated data is 10 : 1. In other words, a real sample is roughly worth 10 generated samples. Notably, more replication beyond N = 10 leads to worse\nperformance, as the low weight of generated data (with even higher replication of real data) now hinders the benefits in data diversity brought by generated data."
        },
        {
            "heading": "3.2 CAUSES IN DATA AUGMENTATION",
            "text": "Aside from the data inflation strategy, we also wonder whether the current training protocol of contrastive learning should also be adjusted for a much larger size of data (20x bigger). Data augmentation is arguably the most important part of contrastive learning. The seminal work SimCLR (Chen et al., 2020a) shows that different data augmentations dramatically influence performance (much larger than learning objectives). Therefore, we might wonder how different choices of data augmentation affect the performance of data inflation.\nAmong commonly used augmentations, random resized crop (RRC) is the most important one (Chen et al., 2020a). Therefore, we adjust the augmentation strength by varying the minimal cropped (relative) area size, denoted as a (by default, a = 0.08), and keeping others fixed. Smaller a indicates a stronger augmentation that can crop the image to a smaller size, and vice versa. We compare four scale of training data: CIFAR-10, Half CIFAR-10 (50% random split), CIFAR-10 + 0.1M generated data (with STF), and CIFAR-10 + 1M generated data (with STF). Figure 3(a) shows a clear trend that the optimal augmentation (marked by \u22c6) is consistently weaker for larger training data (0.02 for Half CIFAR-10, 0.08 for CIFAR-10, 0.20 for CIFAR-10 + 0.1M, and 0.30 for CIFAR-10 + 1M). Therefore, more training data (especially with data inflation) requires an adaptive adjustment of augmentation strength to fully unleash its benefits. Guided by this principle, we propose a weak version\nof data augmentation (detailed in Section 5), and Figure 3(b) shows that this weak augmentation can consistently bring significant gains for generative data of different FIDs."
        },
        {
            "heading": "3.3 PROPOSED STRATEGY: ADAPTIVE INFLATION",
            "text": "Following these lessons so far, we arrive at a general guideline for data inflation: 1) we should put different weights on real and generated data, and worse quality data should have lower weights; 2) we should adopt milder data augmentations with more data. We call it Adaptive Inflation (AdaInf) to emphasize that we should adapt the training configurations according to the quality and size of generated data. In practice, to avoid exhaustive search, we adopt a default choice (called Simple AdaInf, or AdaInf for short) with 10 : 1 mixture of real and generated data, and a weak data augmentation strategy designed following the AdaInf principle (details in Section 5). This default choice, as a baseline strategy for AdaInf training, works surprisingly well across multiple datasets and generated data. A preview performance of simple AdaInf is shown in Figure 1(b). With no downstream data, one may rely on surrogate metrics for finding the adaptive strategy (e.g., ARC Wang et al. (2022))."
        },
        {
            "heading": "4 THEORETICAL CHARACTERIZATION OF DATA INFLATION",
            "text": "In Section 3, we show that different strategies in data inflation have large effects on downstream performance. In this section, we provide in-depth theoretical explanations of these phenomena."
        },
        {
            "heading": "4.1 MATHEMATICAL FORMULATION",
            "text": "To analyze the influence of data augmentation, we adopt the standard augmentation graph framework introduced by HaoChen et al. (2021), where data augmentations induce interactions (as edges) between training samples (as nodes). Different from their original setting dealing with in-domain generalization on population distribution, now we need to characterize the influence of adopting more training data and mismatched training-test distribution on downstream generalization.\nRaw Data as Subgraph. To describe the difference between using raw data and inflated data, our key insight is that when the two have the same population distribution (perfect generation), the raw data can be seen as a random subset of the inflated data. This allows us to analyze their difference through a subsampled graph perspective in the augmentation graph framework. Denote the inflated dataset as X\u0304 and its augmented dataset as X . We can define an augmentation graph over all augmented training samples in X , and its adjacency matrix A \u2208 Rn\u00d7n represents the joint probability of positive samples under data augmentation, Ax,x\u2032 = Ex\u0304\u223cPX\u0304A(x|x\u0304)A(x\n\u2032|x\u0304). The (normalized) graph Laplacian is L = I \u2212 D\u2212 12AD\u2212 12 , where D is a diagonal degree matrix with the (x, x)-th diagonal element as Dxx = \u2211 x\u2032 Ax,x\u2032 . Denote the eigenvalues of L as 0 = \u03bb1 \u2264 \u03bb2 \u2264 \u00b7 \u00b7 \u00b7 \u2264 \u03bbN \u2264 2. Ignoring the difference between real and generated data, we can regard the raw dataset X\u0304raw (real data) as a random subset of X\u0304 and accordingly, the augmentation graph of raw data is a random subgraph of A. This view allows us to use random graph theory to characterize the influence of data inflation.\nFollowing common practice, we evaluate learned features with linear probing as the downstream task, where we learn a linear classifier with weights B \u2208 Rk\u00d7r (r denotes the number of classes) as gf,B on top of pretrained features f(x) \u2208 Rk to predict the labels y \u2208 Y of augmented data. Then we define a majority voting classifier g\u0304f,B(x\u0304) := argmaxi\u2208[r] Prx\u223cA(\u00b7|x\u0304) (gf,B(x) = i) to predict real data. A smaller classification error, denoted as E(f,B), indicates better feature separability."
        },
        {
            "heading": "4.2 GUARANTEES ON INFLATED CONTRASTIVE LEARNING",
            "text": "Given the formulation above, we establish a formal guarantee for contrastive learning with data inflation. Compared to the original result in HaoChen et al. (2021), our guarantees accommodate the discrepancies between the pretraining a downstream distribution (i.e., OOD generalization).\nTheorem 4.1. With probability at least 1 \u2212 \u03b4, for the optimal encoder f\u2217 on inflated data and a learned linear head B\u2217, its linear probing error has the following upper bound,\nE(f\u2217, B\u2217) \u2264 8\u03b1 \u03bbk+1 + 16\u03b1+ 2(1\u2212 \u03b2)DTV(Pd, Pg), (3)\nwhere \u03b1 = Ex\u0304\u223cPd,x\u223cA(\u00b7|x\u0304)1 [y(x) \u0338= y(x\u0304)] denotes the labeling error caused by data augmentation, \u03bbk+1 denotes the k + 1-th smallest eigenvalue of the inflated Laplacian matrix L, DTV(Pd, Pg) = 1/2 \u00b7 \u222b x |Pd(x)\u2212 Pg(x)|dx denotes the total variation (TV) between real and generated data.1 Based on the generalization upper bound in Eq. 3 (proof in Appendix D.2), we can provide rigorous explanations for the AdaInf strategy proposed in Section 3.\n(1) Explaining the Data Inflation Strategy (Section 3.1)\nFirst of all, by involving the generated data, Theorem 4.1 has an additional error term that accounts for the distribution gap between the real and the generated data DTV(Pg, Pd), which naturally explains why utilizing a better generative model (with lower FID) brings consistently better downstream performance (Figure 2(a)). Similarly, a larger weight of raw data \u03b2 also helps close the distribution gap, which aligns well with our analysis (Figure 2(b)). 2 Thus, the distribution gap term rigorously justifies the cause from the data inflation side (Section 3.1). For ease of analysis, below we assume that two distributions are roughly the same, i.e., Pd \u2248 Pg . (2) Explaining the Data Augmentation Strategy (Section 3.2)\nInfluence on Labeling Error \u03b1. Second, one would wonder how data inflation affects the labelling error \u03b1, which, intuitively, means the probability that augmentations produce samples belonging to different classes. As shown in Figure 4(a), stronger augmentations often lead to a larger labeling error. Because \u03b1 is calculated as the expectation, inflating the data size does not affect it.\nInfluence on (Algebraic) Graph Connectivity. The most critical and interesting part of this analysis, is that we find that data inflation plays an important role in affecting the graph connectivity \u03bbk+1. From spectral graph theory, we know that Laplacian eigenvalues can serve as algebraic measures of graph connectivity. Loosely speaking, larger eigenvalues indicate better connectivity (and complete graphs have the largest). Data augmentation plays a positive role in improving graph connectivity, since stronger augmentations create more overlap between different training samples (illustrated in Figure 4(b)). Meanwhile, we also notice that using only raw data (i.e., a subset of inflated data) generally has worse connectivity, as there are generally fewer edges when restricted to the subgraph. The following lemma from Chung and Horn (2007) shows that a subsampled graph has a smaller spectral gap (usually equals to the second smallest eigenvalue \u03bb2, known as algebraic graph connectivity (Chung, 1997)). Experiments in Appendix A show that other \u03bbk\u2019s also decrease with a larger sampling ratio. Since the no-inflation graph can be seen as a subgraph of the inflation graph, it means that inflation will increase the eigenvalues of the non-inflation graph and thus bring better graph connectivity than the raw data.\nLemma 4.2 (Theorem 1.1 in Chung and Horn (2007)). Suppose G is a graph on n vertices with spectral gap \u03bb = min{\u03bb2, 2 \u2212 \u03bbN} and minimum degree dmin. A random subgraph H of G with edge-selection probability p almost surely has a spectral gap \u03bbH satisfying\n\u03bbH = \u03bb\u2212O\n(\u221a log n\npdmin +\n(log n)3/2\npdmin(log log n)3/2\n) .\n1In practice, although we only have finite samples, with the inductive bias of neural networks (Saunshi et al., 2022; HaoChen and Ma, 2023), we will not meet \u03bbk+1 = 0 that renders the bound vacuous.\n2In practice, since we only use limited real data and training epochs, too large mixing ratio \u03b2 will render the generated data almost useless during training, which also hurt model performance with reduced data diversity. Thus, the optimal reweighting is usually smaller than 0.5 but not 0.\nThe Complementary Roles between Inflation and Augmentation. Given the analysis above, we know two important facts: 1) data augmentation has a conflicting effect on downstream performance, since stronger augmentation improves graph connectivity (larger \u03bbk+1) but also improves labeling error (larger \u03b1); 2) data inflation only has a one-way effect, as it improves graph connectivity and does not change labeling error. Therefore, when data inflation can bring enough graph connectivity, in order to further minimize the generalization error, we can accordingly adopt a weaker augmentation in the sake of smaller labeling error. In turn, if the data size is too small, we need to adopt stronger augmentation to gain better connectivity. Therefore, inflation and augmentation have complementary roles for generalization, increasing one of them will decrease the need for the other, and vice versa. As a result, with more inflated data, the optimal augmentation strength will shift to a lower one, which explains why weak augmentations lead to better performance in Section 3.2."
        },
        {
            "heading": "4.3 VERIFICATION EXPERIMENTS",
            "text": "In Section 4.2, we theoretically characterize the influence of data inflation and data augmentation on the generalization error through two crucial factors: labeling error \u03b1 and connectivity \u03bbk+1. Since the augmentation graph is hard to construct for real-world data (HaoChen et al., 2021), we now validate this analysis with a synthetic experiment designed following Wang et al. (2022).\nSetting. We sample data from the isotropic Gaussian distribution with means (\u22121, 0) and (1, 0) (two classes), and variance 0.7. The augmentation here is to apply a uniform noise in a circle of radius r. Thus, r can be seen as a measure of augmentation strength. With this toy model, we can construct the augmentation graph and explicitly compute the labeling error \u03b1 and the Laplacian eigenvalues like \u03bbk+1 (k = 2 by default), allowing us to closely examine their changes.\nResults. Figure 5 shows the influence of data size and augmentation strength, which verifies our analysis from the following aspects. First, Figures 5(a) & 5(b) show that large data size and stronger augmentations are indeed complementary since they both bring better connectivity (larger \u03bbk+1).\nTable 1: Comparison linear probing accuracy (mean & stdev) of different contrastive learning methods and different Datasets. As for the generative models, we adopt STF for CIFAR-10 (1.94 FID) and CIFAR-100 (3.14 FID), and DDPM for Tiny ImageNet (18.61 FID).\n(a) Different CL Methods\nInflation SimCLR MoCo V2 BYOL Barlow Twins\nNo 91.56\u00b10.29 92.75\u00b10.43 92.46\u00b10.06 91.24\u00b10.30 Vanilla 91.38\u00b10.11 92.51\u00b10.40 92.9\u00b10.21 92.09\u00b10.10 AdaInf 93.42\u00b10.20 94.19\u00b10.19 92.87\u00b10.26 93.64\u00b10.38\n(b) Different Datasets (w/ SimCLR)\nInflation CIFAR-10 CIFAR-100 Tiny ImageNet\nNo 91.56\u00b10.29 66.81\u00b10.36 47.21\u00b10.86 Vanilla 91.38\u00b10.11 65.52\u00b10.73 41.03\u00b10.39 AdaInf 93.42\u00b10.20 69.6\u00b10.21 48.36\u00b10.46\n10k 30k 100k 500k Total Training Steps\n80\n82\n84\n86\n88\n90\n92\n94\n96\nLin ea\nr A cc ur ac\ny (%\n)\n80.4580.77\n84.58 83.72\n88.65\n90.81 91.3391.35\n93.57\n92.35 93.21\n94.7 No Inflation Vanilla AdaInf (ours)\n(a) Different Total Training Steps\n0 100k 200k 300k 400k 500k Training Steps\n75.0\n77.5\n80.0\n82.5\n85.0\n87.5\n90.0\n92.5\n95.0\nLin ea r A\ncc ur ac y (%\n)\nNo Inflation Vanilla AdaInf (ours)\n(b) Training Process (500k steps in total)\nFigure 6: CIFAR-10 results. (a): linear accuracy (%) with total different training steps (10k steps \u2248 100 epochs of no-inflation training). (b): linear accuracy along the 500k-step training process.\nSecond, Figure 5(c) shows that stronger augmentations indeed bring larger labeling error \u03b1. At last, Figure 5(d) verifies that when combined, the optimal augmentation (marked by red dots) indeed decreases when the increase of data size, which aligns well with our observation in Section 3.2."
        },
        {
            "heading": "5 EXPERIMENTS",
            "text": "Setup. We conduct experiments on three benchmark datasets: CIFAR-10, CIFAR-100, and Tiny ImageNet (100 classes). By default, we use 1M synthetic data for CIFAR-10 and CIFAR-100 generated by a high-quality diffusion model STF (Xu et al., 2023) (FIDs are 1.94 (CIFAR-10) and 3.14 (CIFAR-100)). Due to the limit of computation resource, we adopt DDPM (18.61 FID) for Tiny ImageNet. These diffusion models are unconditional since we only assume access to unlabeled data for pretraining. We evaluate the pre-trained contrastive models using the linear probing method, where only the raw dataset is used. The model training relies on the solo-learn repository (da Costa et al., 2022). We include SimCLR (Chen et al., 2020a) (default choice), MoCo V2 (Chen et al., 2020b), BYOL (Grill et al., 2020), and Barlow Twins (Zbontar et al., 2021) in this part. For a fair comparison of inflated and non-inflated training, we train the model for 100k steps in all cases, which amounts to 1,000 training epochs without inflation. We compare three inflation methods: 1) No Inflation, which serves as a baseline for our study; 2) Vanilla Inflation, which equally mixes real and generated data and adopts default augmentations; 3) our AdaInf strategy, which adopts a mixing ratio of 10 : 1 and weaker augmentations. Specifically, we weaken two most important augmentations: the min scale of random resized cropping improves from 0.08 to 0.2; the ColorJitter strength decreases from 1 to 0.5; and the probability of applying ColorJitter decreases from 0.8 to 0.4.\nResults. We summarize the benchmark results in Table 1, where we run each experiment for 3 random trials. Table 1(a) shows that compared to the no-inflation baseline, vanilla inflation sometimes leads to even worse performance (e.g., on MoCo V2), while AdaInf has consistent improvements on all datasets. Meanwhile, AdaInf outperforms vanilla inflation significantly (with more than 1% gain in accuracy) on most methods (SimCLR, MoCo V2, and Barlow Twins), while the two perform comparably on BYOL (potentially because the BYOL augmentation needs specific weakening strategy). Table 1(b) shows that AdaInf brings consistent improvements across datasets having different scales and different numbers of classes.\nTo gain further understanding of data inflation, we further take a closer look at its behaviors. Experiments in this part are conducted on CIFAR-10 with SimCLR unless specified.\nTraining Steps. We examine whether data inflation is also effective under different total training steps. As shown in Figure 6(a), AdaInf brings a very large improvement under short training (80.45 \u2192 84.58 with 10k training steps), and there still remains a clear advantage even if we train for 500k steps (around 5,000 epochs in standard training). Remarkably, SimCLR attains 94.7% linear accuracy under this setting, setting a new SSL record on CIFAR-10 with simply the SimCLR method. It shows that even the simplest method has the potential to match state-of-the-art performance by simply inflating the dataset with generated data, and combining data inflation with advanced methods may lead to further improvements.\nLearning Curve. We further examine the learning process with and without data inflation in Figure 6(b). Interestingly, we observe that vanilla inflation is inferior to non-inflated training in most time and only achieves a small improvement at last (when non-inflated training saturates). In comparison, AdaInf can consistently outperform the standard non-inflated training across the entire training process, and continue to bring improvements when non-inflated training saturates.\nAblation Study. We study the influence of the three components of AdaInf in Table 2(a): generated data, data reweighting, weak augmentation. We can observe that while all three components contribute to the final performance, their importance is: weak augmentation > data reweighting > generated data. In particular, the major improvement is brought by weak augmentation that solely brings around +2% accuracy. It shows that the interplay between inflated data and the learning algorithm has a large influence on the final performance and the adaptive co-design is very important."
        },
        {
            "heading": "5.1 APPLICATION TO DATA-SCARCE SCENARIOS",
            "text": "As generative models can provide a large amount of synthetic data, the proposed data inflation strategy can be particularly helpful when facing data scarcity issues. To show this benefit, we construct a small dataset consisting of 5,000 images (1/10 size of CIFAR-10) by randomly sampling 500 images from each class of CIFAR-10. Subsequently, we train an STF model on this subset (with 18.27 FID) and use the generated 1M samples for data inflation. As shown in Table 2(b), AdaInf obtains much higher linear accuracy than standard training (+4.32% accuracy), and it also achieves better performance than vanilla inflation (+2.2%). Thus, AdaInf is indeed a simple and effective approach for data-scarce scenarios."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "In this work, contrary to the common belief that generated data help representation learning, we show that they can be harmful when improperly used for contrastive learning. Upon our investigation, we have identified two sources of failure from the data inflation and data augmentation perspectives. To gain a better understanding of these phenomena, we have provided rigorous theoretical explanations with a careful examination of the generalization bounds. Based on these observations, we propose an adaptive data inflation strategy, Adaptive Inflation (AdaInf), that combines data reweighting and weak augmentations for inflated contrastive learning. Experiments show that this simple data-centric strategy brings significant improvements over the standard training and the vanilla inflation method without any additional cost, especially on data-scarce scenarios."
        },
        {
            "heading": "A ADDITIONAL RESULTS",
            "text": "Influence of Subsampling on Laplacian Eigenvalues. We randomly generate 10k datapoints from a two-dimensional uniform distribution [0, 1]2 as overall dataset and sample the dataset with different sample ratios. We then construct the augmentation sub-graph by drawing edged Aij = 1 for any data pair satisfying \u2225xi\u2212xj\u22252 \u2264 0.05. Figure 7 shows the Laplacian eigenvalues of the subsampled graphs. We can observe a consistent trend that a smaller sampling ratio leads to a larger decrease of eigenvalues. Therefore, equivalently speaking, using stronger data inflation (i.e., increasing from a smaller sampling ratio (the original data) to 1 (inflated data)) can lead to a larger increase of eigenvalues, and thus bring larger improvements in graph connectivity.\nInfluence of the scale of Generated Data. Figure 8 illustrates the impact of different scales of generated data on the linear accuracy of SimCLR. Consistent with the results shown in Figure 2(b), the comparison of results between \"No Replication\" and \"10:1 Replication\" in Figure 8 further highlights the significance of replication (Data Reweighting) in mitigating the distribution shift between real data and generated data. Additionally, the results indicate that using 1M generated data samples is the optimal scale when employing the AdaInf strategy. Notably, using a larger size of generated data beyond 1M when 10:1 replication actually leads to worse performance. This is because the same replication leads to a smaller mixing ratio \u03b2 with a larger size of generated data, leading to a larger distribution gap between the real and generated data, as explained in Section 4.2. Therefore, if we want to utilize a larger amount of generated data than 1M, we should use a larger replication to maintain the same mixing ratio \u03b2.\nAugmentation Matters. Figure 3(a) investigates the optimal minimal cropped area size for differentscale datasets. Based on Figure 3(a), Table 3 investigates the effects of both the minimal cropped size and color jitter which play a crucial role in contrastive learning when inflating the original dataset. The results demonstrate that the standard augmentation (the first row) is suitable for no inflation dataset (baseline). However, the weak augmentation method proves to be more effective compared to the standard augmentation. Specifically, when the crop minimum scale is set to 0.2 and the color\njitter probability is 0.4 with strength of 0.5 (relative), SimCLR achieves the best performance on inflated data. Once again, these results highlight the complementary effects between data inflation and data augmentation.\nThe Optimal Mixing Ratio Depends On The Quality Of Generated Data. Figure 9 shows the impact of data quality on the optimal mixing ratio. For generated data from STF (FID = 1.94), the optimal replication is 10 : 1 which means a real sample is roughly worth 10 generated samples. However, For lower quality generated data from DDPM (FID=3.04), the optimal replication is 15 : 1 which means a real sample is roughly worth 15 generated samples. The result is consistent with Theorem 4.1, which suggests adjusting the mixing ratio \u03b2 based on the magnitude of the distribution gap.\nResearch On High-Resolution Images. To further validate the relationship between data scale and data augmentation on high-resolution images, we compared the optimal augmentation strength between two different scale datasets: ImageNet100 (1300 images for each class) and 10%*ImageNet100 (randomly sampled 10% of from each class of ImageNet100, 130 images for each class). The Table 4 shows that for 10%*ImageNet100, the model performs better with a RRC min scale of 0.04 compared to 0.08. However, for ImageNet100, which scale is larger than 10% ImageNet100, weaker augmentation (RRC min scale of 0.08) achieved better performance.\nThe Influence Of Augmentation Under Different Trainging Steps. Figure 10 shows the influence of augmentation under different training steps. We can see that under different training steps, using\nweak augmentation after data inflation is effective. Additionally, we can see that when training is insufficient (fewer steps), generated data tend to help than hurt the performance. Instead, as training converges (100k steps \u2248 1000 epochs), \"Inf & Standard Augmentation\" underperforms no inflation but our AdaInf still performs well.\nCompare GAN And Diffusion Model. There are mainly two types of generated models with SOTA performance on CIFAR-10: diffusion model and GANs currently. Table 5 provides a comparison of data inflation on CIFAR-10 with generated data from StyleGAN2-ADA (GAN) (Karras et al., 2020) and STF (diffusion model). We find that similar to DDPM, vanilla inflation leads to worse performance, while AdaInf can bring significant improvements. Comparably, diffusion models with better generation quality (e.g., STF with lower FID) can achieve better accuracy.\nPretraining cost of the generative model. We conduct a time test for pretraining diffusion models with 4 NVIDIA GTX 3090 GPUs on CIFAR-10, and the total training time is shown in Table 6. We can see that models with better quality (EDM, STF) generally require longer training. In practice, since these models have public checkpoints, we do not need to train on our own for CIFAR-10."
        },
        {
            "heading": "B ADDITIONAL RELATED WORK",
            "text": "In addition to researches mentioned in Section 2, there are still many works that investigated the application of generated data in various tasks like semantic image segmentation (Baranchuk et al., 2022; Chen et al., 2019; Li et al., 2022; 2021; Tritrong et al., 2021), human motion understanding (Guo et al., 2022; Izadi et al., 2011; Varol et al., 2017). To address the distribution shift between generated data and real data, many works have focused on sampling methods of generative models (Besnier et al., 2020; He et al., 2023). (Jurjo et al., 2021) proposed that the color, image coherence, and FID of generative model are important factors influencing the learning representation from generated data. Moreover, (Ren and Lee, 2018) proposed learning representations by predicting information (e.g., surface normal, depth, and instance contour) of generated data through self-supervised\nlearning. Compared to previous works, our approach stands out as we do not modify existing generative models or contrastive learning models. Instead, we fully leverage generated data from the perspectives of data inflation and data augmentation, without incurring additional costs."
        },
        {
            "heading": "C EXPERIMENTAL DETAILS",
            "text": "Unless otherwise specified, all experiments are conducted using the default configuration in the codebase of the solo-learn library (da Costa et al., 2022). The CIFAR-10 dataset served as the original dataset and 1M generated data is generated by STF (1.94 FID). We adopt SimCLR with backone Resnet-18 for contrasive learning and pre-train for 100,000 steps. In AdaInf, weak augmentation is used with a minimal cropped (relative) area size of 0.2 and a color jitter probability of 0.4. The values for brightness, contrast, and saturation of color jitter is 0.4 and the hue value is 0.1. Other hyperparameters of weak augmentation remained consistent with the standard augmentation provided by the solo-learn library.\nFor the experiment in Figure 2(a), the dataset consisted of CIFAR-10 and 1M generated data. The generated data from STF was generated using the provided checkpoint from Xu et al. (2023). The generated data from EDM (1.96 FID) and DDPM was generated using the provided checkpoint from Karras et al. (2022). Additionally, the generated data for EDM (7.29 FID) was generated using a shorter training time checkpoint, where only 5017k images were trained.\nFor the experiment in Figure 3(a), the augmentation method modified the minimal cropped (relative) area size based on the standard augmentation. The dataset \"Half CIFAR-10\" was created by randomly selecting 2,500 images from each class of CIFAR-10. The dataset \"CIFAR-10 + 1M\" consisted of CIFAR-10 data and 1 million generated data from STF and \"CIFAR-10 + 0.1M\" consisted of CIFAR-10 and 0.1 million generated data from STF.\nFor the experiment in Figure 3(b), the dataset for the Vanilla method consisted of CIFAR-10 data and 1 million generated data, with standard augmentation. The dataset for the AdaInf method consisted of CIFAR-10 data replicated 10 times and 1 million generated data, with weak augmentation.\nFor the experiment in Table 1(b), the generated data for CIFAR-100 is from STF, while the generated data for Tiny-ImageNet is from DDPM. The configuration of SimCLR trained on Tiny-ImageNet followed the settings provided by the solo-learn library for imagenet-100.\nFor the experiment in Table 2(b), we sampled 5,00 images from each class of CIFAR-10 to create a small-scale dataset. We then generated 100,000 images by STF trained on this small-scale dataset, which is used for data inflation. The training steps is 10,000."
        },
        {
            "heading": "D OMITTED PROOF",
            "text": "D.1 PROOF OF THEOREM 3.1\nProof. Since Pt = \u03b2Pd + (1\u2212 \u03b2)Pg and 0 \u2264 \u03b2 \u2264 1, we have\nDTV(Pt, Pd)\n= \u222b |Pt(x)\u2212 Pd(x)|dx\n= \u222b |\u03b2Pd + (1\u2212 \u03b2)Pg \u2212 Pd(x)|dx\n=(1\u2212 \u03b2) \u222b |Pg \u2212 Pd(x)|dx\n=(1\u2212 \u03b2)DTV(Pg, Pd),\n(4)\nwhich completes the proof.\nD.2 PROOF OF THEOREM 4.1\nProof. We define a linear function with weights B \u2208 Rk\u00d7r (r represents the number of dataset classes) as gf,B : Rk \u2192 Y on top of pretrained features f(x) \u2208 Rk to predict the labels y \u2208 Y of\naugmentated data. And we define\ng\u0304f,B(x\u0304) := argmax i\u2208[r] Pr x\u223cA(\u00b7|x\u0304) (gf,B(x) = i) (5)\nto predict the labels y \u2208 Y of real data. To describe the difference between labels of two augmented data of the same natural datapoint, we define a function:\n\u03d5y := \u2211\nx,x\u2032\u2208X Axx\u2032 \u00b7 1[y(x) \u0338= y(x\u2032)]. (6)\nWe have\n\u03d5y = \u2211\nx,x\u2032\u2208X Axx\u2032 \u00b7 1[y(x) \u0338= y(x\u2032)]\n= Ex\u0304\u223cPt \u2211\nx,x\u2032\u2208X [A(x|x\u0304)A(x\u2032|x\u0304) \u00b7 1[y(x) \u0338= y(x\u2032)]\n\u2264 Ex\u0304\u223cPt \u2211\nx,x\u2032\u2208X [A(x|x\u0304)A(x\u2032|x\u0304) \u00b7 (1[y(x) \u0338= y(x\u0304)] + 1[y(x\u2032) \u0338= y(x\u0304))]\n= 2 \u00b7 Ex\u0304\u223cPt \u2211 x\u2208X [A(x|x\u0304) \u00b7 1[y(x) \u0338= y(x\u0304)]]\n= 2 \u00b7 Ex\u0304\u223cPtEx\u223cA(\u00b7|x\u0304)1[y(x) \u0338= y(x\u0304)] = 2\u03b1\n(7)\n.\nLemma D.1 (Theorem B.3 in HaoChen et al. (2021)). Assume the set of augmented data X is finite. Let f\u2217 \u2208 argminf :X\u2192Rk be a minimizer of the population spectral contrastive loss L(f) with k \u2208 Z+. Then, for any labeling function y\u0302 : X \u2190 [r] there exists a linear probe B\u2217 \u2208 Rr\u00d7k with norm \u2225B\u2217\u2225F \u2264 1/(1\u2212 \u03bbk) such that\nEx\u0304\u223cP,x\u223cA(\u00b7|x\u0304)[\u2225y\u20d7(x\u0304)\u2212B\u2217f\u2217(x)\u222522] \u2264 \u03d5y\n\u03bbk+1 + 4\u2206(y, y\u0302), (8)\nwhere y\u20d7(x\u0304) is the one-hot embedding of y(x\u0304) and \u2206(y, y\u0302) := Pr x\u0304\u223cPt,x\u223cA(\u00b7|x\u0304) (y(x) \u0338= y\u0302(x)) denotes the average disagreement between y\u0302 and the ground-truth labeling y. Furthermore, the error can be bounded by\nPr x\u0304\u223cPt,x\u223cA(\u00b7|x\u0304)\n(gf\u2217,B\u2217(x) \u0338= y(x\u0304)) \u2264 2\u03d5y\n\u03bbk+1 + 8\u03b1. (9)\nAccording to the definition of g\u0304f (x\u0304), g\u0304f\u2217,B\u2217(x\u0304) \u0338= y(x\u0304) happens only if more than half of the augmentations of x\u0304 predicts differently from y(x\u0304). Formally, for any x\u0304 \u223c Pt with g\u0304f\u2217,B\u2217(x\u0304)) \u0338= y(x\u0304), according to the definition of g\u0304f\u2217,B\u2217(x\u0304), we must have\nPr x\u223cA(\u00b7|x\u0304)\n(gf\u2217,B\u2217(x)) \u0338= y(x\u0304)) \u2265 0.5.\nThus we have\nPr x\u0304\u223cPt (g\u0304f\u2217,B\u2217(x\u0304) \u0338= y(x\u0304)) \u2264 2 \u00b7 Pr x\u0304\u223cPt,x\u223cA(\u00b7|x\u0304)\n(gf\u2217,B\u2217(x)) \u0338= y(x\u0304)) \u2264 8\u03b1\n\u03bbk+1 + 16\u03b1. (10)\nAt last, we notice that the following result is obtained when the training and test data are from the same distribution. When considering generalization from the training distribution Pt to the testing\ndistribution Pd, we have\nPr x\u0304\u223cPd\n(g\u0304f\u2217,B\u2217(x\u0304)) \u0338= y(x\u0304))\n= \u222b Pd(x)1[g\u0304f\u2217,B\u2217(x\u0304)) \u0338= y(x\u0304)]dx\u0304\n= \u222b (Pd(x)\u2212 Pt(x) + Pt(x))1[g\u0304f\u2217,B\u2217(x\u0304)) \u0338= y(x\u0304)]dx\u0304\n\u2264 \u222b |Pd(x)\u2212 Pt(x)|1[g\u0304f\u2217,B\u2217(x\u0304)) \u0338= y(x\u0304)]dx\u0304+ \u222b Pt(x)1[gf\u2217,B\u2217(x\u0304)) \u0338= y(x\u0304)]dx\u0304\n\u2264 \u222b |Pd(x)\u2212 Pt(x)|dx\u0304+ \u222b Pt(x)1[g\u0304f\u2217,B\u2217(x\u0304)) \u0338= y(x\u0304)]dx\u0304\n=2DTV(Pd, Pt) + Pr x\u0304\u223cPt\n(g\u0304f\u2217,B\u2217(x\u0304)) \u0338= y(x\u0304))\n\u22642(1\u2212 \u03b2)DTV(Pd, Pg) + 8\u03b1\n\u03bbk+1 + 16\u03b1.\n(11)\nIn the last equation, we combine the result of Theorem 3.1 and the generalization bound on training data (Eq. 10)."
        },
        {
            "heading": "E EXAMPLES OF GENERATED DATA",
            "text": "For a concrete understanding, we provide examples of the generated data with different diffusion models on different datasets below. It can be seen that the generated data used in our experiments indeed look very similar to the real data, and models with lower FID indeed have fewer artifacts."
        }
    ],
    "year": 2023
}