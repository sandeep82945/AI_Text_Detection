{
    "abstractText": "We study a strategic variant of the multi-armed bandit problem, which we coin the strategic click-bandit. This model is motivated by applications in online recommendation where the choice of recommended items depends on both the clickthrough rates and the post-click rewards. Like in classical bandits, rewards follow a fixed unknown distribution. However, we assume that the click-rate of each arm is chosen strategically by the arm (e.g., a host on Airbnb) in order to maximize the number of times it gets clicked. The algorithm designer does not know the post-click rewards nor the arms\u2019 actions (i.e., strategically chosen click-rates) in advance, and must learn both values over time. To solve this problem, we design an incentive-aware learning algorithm, UCB-S, which achieves two goals simultaneously: (a) incentivizing desirable arm behavior under uncertainty; (b) minimizing regret by learning unknown parameters. We approximately characterize all Nash equilibria of the arms under UCB-S and show a \u00d5( \u221a KT ) regret bound uniformly in every equilibrium. We also show that incentive-unaware algorithms generally fail to achieve low regret in the strategic click-bandit. Finally, we support our theoretical results by simulations of strategic arm behavior which confirm the effectiveness and robustness of our proposed incentive design.",
    "authors": [
        {
            "affiliations": [],
            "name": "COMBAT CLICKBAIT"
        },
        {
            "affiliations": [],
            "name": "Thomas Kleine Buening"
        },
        {
            "affiliations": [],
            "name": "Aadirupa Saha"
        },
        {
            "affiliations": [],
            "name": "Christos Dimitrakakis"
        },
        {
            "affiliations": [],
            "name": "Haifeng Xu"
        }
    ],
    "id": "SP:c04107bd7136aaffc54d3cbf6d0c3e1391f7d3b3",
    "references": [
        {
            "authors": [
                "Peter Auer"
            ],
            "title": "Using confidence bounds for exploitation-exploration trade-offs",
            "venue": "Journal of Machine Learning Research,",
            "year": 2002
        },
        {
            "authors": [
                "Moshe Babaioff",
                "Yogeshwer Sharma",
                "Aleksandrs Slivkins"
            ],
            "title": "Characterizing truthful multi-armed bandit mechanisms",
            "venue": "In Proceedings of the 10th ACM conference on Electronic commerce,",
            "year": 2009
        },
        {
            "authors": [
                "Moshe Babaioff",
                "Robert D Kleinberg",
                "Aleksandrs Slivkins"
            ],
            "title": "Truthful mechanisms with implicit payment computation",
            "venue": "Journal of the ACM (JACM),",
            "year": 2015
        },
        {
            "authors": [
                "Dirk Bergemann",
                "Juuso V\u00e4lim\u00e4ki"
            ],
            "title": "Dynamic mechanism design: An introduction",
            "venue": "Journal of Economic Literature,",
            "year": 2019
        },
        {
            "authors": [
                "Djallel Bouneffouf",
                "Irina Rish",
                "Charu Aggarwal"
            ],
            "title": "Survey on applications of multi-armed and contextual bandits",
            "venue": "IEEE Congress on Evolutionary Computation (CEC),",
            "year": 2020
        },
        {
            "authors": [
                "Mark Braverman",
                "Jieming Mao",
                "Jon Schneider",
                "S Matthew Weinberg"
            ],
            "title": "Multi-armed bandit problems with strategic arms",
            "venue": "In Conference on Learning Theory,",
            "year": 2019
        },
        {
            "authors": [
                "Nikhil R Devanur",
                "Sham M Kakade"
            ],
            "title": "The price of truthfulness for pay-per-click auctions",
            "venue": "In Proceedings of the 10th ACM conference on Electronic commerce,",
            "year": 2009
        },
        {
            "authors": [
                "Jing Dong",
                "Ke Li",
                "Shuai Li",
                "Baoxiang Wang"
            ],
            "title": "Combinatorial bandits under strategic manipulations",
            "venue": "In Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining,",
            "year": 2022
        },
        {
            "authors": [
                "Zhe Feng",
                "David Parkes",
                "Haifeng Xu"
            ],
            "title": "The intrinsic robustness of stochastic bandits to strategic manipulation",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Rupert Freeman",
                "David M Pennock",
                "Chara Podimata",
                "Jennifer Wortman Vaughan"
            ],
            "title": "No-regret and incentive-compatible prediction with expert advice",
            "venue": "arXiv preprint arXiv:2002.08837,",
            "year": 2020
        },
        {
            "authors": [
                "Guoju Gao",
                "He Huang",
                "Mingjun Xiao",
                "Jie Wu",
                "Yu-E Sun",
                "Sheng Zhang"
            ],
            "title": "Auction-based combinatorial multi-armed bandit mechanisms with strategic arms",
            "venue": "In IEEE INFOCOM 2021-IEEE Conference on Computer Communications,",
            "year": 2021
        },
        {
            "authors": [
                "Aur\u00e9lien Garivier",
                "Pierre M\u00e9nard",
                "Gilles Stoltz"
            ],
            "title": "Explore first, exploit next: The true shape of regret in bandit problems",
            "venue": "Mathematics of Operations Research,",
            "year": 2019
        },
        {
            "authors": [
                "Nicola Gatti",
                "Alessandro Lazaric",
                "Francesco Trov\u00f2"
            ],
            "title": "A truthful learning mechanism for contextual multi-slot sponsored search auctions with externalities",
            "venue": "In Proceedings of the 13th ACM Conference on Electronic Commerce,",
            "year": 2012
        },
        {
            "authors": [
                "Arpita Ghosh",
                "Patrick Hummel"
            ],
            "title": "Learning and incentives in user-generated content: Multi-armed bandits with endogenous arms",
            "venue": "In Proceedings of the 4th conference on Innovations in Theoretical Computer Science,",
            "year": 2013
        },
        {
            "authors": [
                "Irving L Glicksberg"
            ],
            "title": "A further generalization of the kakutani fixed point theorem, with application to nash equilibrium points",
            "venue": "Proceedings of the American Mathematical Society,",
            "year": 1952
        },
        {
            "authors": [
                "Moritz Hardt",
                "Nimrod Megiddo",
                "Christos Papadimitriou",
                "Mary Wootters"
            ],
            "title": "Strategic classification",
            "venue": "In Proceedings of the 2016 ACM conference on innovations in theoretical computer science,",
            "year": 2016
        },
        {
            "authors": [
                "Katja Hofmann",
                "Fritz Behr",
                "Filip Radlinski"
            ],
            "title": "On caption bias in interleaving experiments",
            "venue": "In Proceedings of the 21st ACM international conference on Information and knowledge management,",
            "year": 2012
        },
        {
            "authors": [
                "Jiri Hron",
                "Karl Krauth",
                "Michael I Jordan",
                "Niki Kilbertus",
                "Sarah Dean"
            ],
            "title": "Modeling content creator incentives on algorithm-curated platforms",
            "venue": "arXiv preprint arXiv:2206.13102,",
            "year": 2022
        },
        {
            "authors": [
                "Xinyan Hu",
                "Meena Jagadeesan",
                "Michael I Jordan",
                "Jacob Steinhard"
            ],
            "title": "Incentivizing high-quality content in online recommender systems",
            "venue": "arXiv preprint arXiv:2306.07479,",
            "year": 2023
        },
        {
            "authors": [
                "Kirthevasan Kandasamy",
                "Joseph E Gonzalez",
                "Michael I Jordan",
                "Ion Stoica"
            ],
            "title": "Vcg mechanism design with unknown agent values under stochastic bandit feedback",
            "venue": "Journal of Machine Learning Research,",
            "year": 2023
        },
        {
            "authors": [
                "Tze Leung Lai",
                "Herbert Robbins"
            ],
            "title": "Asymptotically efficient adaptive allocation rules",
            "venue": "Advances in applied mathematics,",
            "year": 1985
        },
        {
            "authors": [
                "Tor Lattimore",
                "Csaba Szepesv\u00e1ri"
            ],
            "title": "Bandit algorithms",
            "year": 2020
        },
        {
            "authors": [
                "Lihong Li",
                "Wei Chu",
                "John Langford",
                "Robert E Schapire"
            ],
            "title": "A contextual-bandit approach to personalized news article recommendation",
            "venue": "In Proceedings of the 19th international conference on World wide web,",
            "year": 2010
        },
        {
            "authors": [
                "Yang Liu",
                "Chien-Ju Ho"
            ],
            "title": "Incentivizing high quality user contributions: New arm generation in bandit learning",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2018
        },
        {
            "authors": [
                "Shie Mannor",
                "John N Tsitsiklis"
            ],
            "title": "The sample complexity of exploration in the multi-armed bandit problem",
            "venue": "Journal of Machine Learning Research,",
            "year": 2004
        },
        {
            "authors": [
                "Hamid Nazerzadeh",
                "Renato Paes Leme",
                "Afshin Rostamizadeh",
                "Umar Syed"
            ],
            "title": "Where to sell: Simulating auctions from learning algorithms",
            "venue": "In Proceedings of the 2016 ACM Conference on Economics and Computation,",
            "year": 2016
        },
        {
            "authors": [
                "Noam Nisan",
                "Amir Ronen"
            ],
            "title": "Algorithmic mechanism design",
            "venue": "In Proceedings of the thirty-first annual ACM symposium on Theory of computing,",
            "year": 1999
        },
        {
            "authors": [
                "Alessandro Pavan",
                "Ilya Segal",
                "Juuso Toikka"
            ],
            "title": "Dynamic mechanism design: A myersonian approach",
            "year": 2014
        },
        {
            "authors": [
                "Philip J Reny"
            ],
            "title": "On the existence of pure and mixed strategy nash equilibria in discontinuous games",
            "year": 1999
        },
        {
            "authors": [
                "Suho Shin",
                "Seungjoon Lee",
                "Jungseul Ok"
            ],
            "title": "Multi-armed bandit algorithm against strategic replication",
            "venue": "In International Conference on Artificial Intelligence and Statistics,",
            "year": 2022
        },
        {
            "authors": [
                "Aleksandrs Slivkins"
            ],
            "title": "Introduction to multi-armed bandits",
            "venue": "Foundations and Trends\u00ae in Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Huazheng Wang",
                "Qingyun Wu",
                "Hongning Wang"
            ],
            "title": "Factorization bandits for interactive recommendation",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2017
        },
        {
            "authors": [
                "Wenjie Wang",
                "Fuli Feng",
                "Xiangnan He",
                "Hanwang Zhang",
                "Tat-Seng Chua"
            ],
            "title": "Clicks can be cheating: Counterfactual recommendation for mitigating clickbait issue",
            "venue": "In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval,",
            "year": 2021
        },
        {
            "authors": [
                "Yisong Yue",
                "Rajan Patel",
                "Hein Roehrig"
            ],
            "title": "Beyond position bias: Examining result attractiveness as a source of presentation bias in clickthrough data",
            "venue": "In Proceedings of the 19th international conference on World wide web,",
            "year": 2010
        },
        {
            "authors": [
                "Hanrui Zhang",
                "Vincent Conitzer"
            ],
            "title": "Incentive-aware pac learning",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Shi Zong",
                "Hao Ni",
                "Kenny Sung",
                "Nan Rosemary Ke",
                "Zheng Wen",
                "Branislav Kveton"
            ],
            "title": "Cascading bandits for large-scale recommendation problems",
            "venue": "arXiv preprint arXiv:1603.05359,",
            "year": 2016
        }
    ],
    "sections": [
        {
            "text": "\u221a KT ) regret bound\nuniformly in every equilibrium. We also show that incentive-unaware algorithms generally fail to achieve low regret in the strategic click-bandit. Finally, we support our theoretical results by simulations of strategic arm behavior which confirm the effectiveness and robustness of our proposed incentive design."
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "Recommendation platforms act as intermediaries between vendors and users so as to recommend items from the former to the latter. On Amazon, vendors sell physical items, while on Youtube the recommended items are videos. The recommendation problem is how to select one or more items to present to each user so that they are most likely to click on at least one of them.\nHowever, vendor-chosen item descriptions are an essential aspect of the problem that is often ignored. These invite vendors to exaggerate their true value in the descriptions in order to increase their Click-Through-Rates (CTRs). As a consequence, even though online learning algorithms can generally identify relevant items, the existence of unrepresentative or exaggerated item descriptions remains a challenge (Yue et al., 2010; Hofmann et al., 2012). These include thumbnails or headlines that do not truly reflect the underlying item (see Figure 1)\u2014a well-known internet phenomenon called the clickbait (Wang et al., 2021). While moderately increasing user click-rates through attractive descriptions is often encouraged since it helps to increase the overall user activity, clickbait can be harmful to a platform as it leads to bad recommendation outcomes and damage to the platform\u2019s reputation which may exceed the value of any additional clicks. A key reason for such dishonest or exaggerated item deceptions is the strategic behavior of vendors driven by their incentive to increase their item\u2019s exposure and click probability. Thus naturally, vendors are better off carefully choosing descriptions so as to increase click-rates, which leads to phenomena such as clickbait.1\nTo address this issue, we take an approach that marries mechanism design without payments with online learning, which are two celebrated research areas, however, mostly studied as separate streams. Since clickbait is fundamentally driven by vendor incentives, we believe that the novel design of online learning policies that can carefully align vendor incentives with the platform\u2019s overall objective may help to resolve this issue from its root.\n\u2217Author is currently with Apple ML Research. 1This is possible because most platforms rely on vendors to provide descriptions about their items. For instance, the images of restaurants on Yelp, rentals on Airbnb, hotels on Expedia, title and thumbnails of Youtube videos, and descriptions of products on Amazon are all provided by the vendors.\nTo incorporate vendor-chosen item descriptions in this setting, we propose and study a natural strategic variant of the classical Multi-Armed Bandit (MAB) problem, which we call the strategic clickbandit in order to emphasize the strategic role that clicks and CTRs play in our setup.2 Concretely, in strategic click-bandits, each arm i is characterized by (a) a reward distribution with mean \u00b5i, inherent to the arm; and (b) a click probability si \u2208 [0, 1], chosen freely by the arm at the beginning. Since the learner (i.e., the recommendation system) knows neither of these values in advance, it must learn them through interaction. The learner\u2019s objective is represented through a general utility function u(si, \u00b5i) that depends on both click-rate and post-click rewards.\nWe highlight two fundamental differences between strategic click-bandits and standard MABs. First, each arm in the strategic click-bandit is a self-interested agent whose objective is to maximize the number of times it gets clicked. This captures the strategic behavior of many vendors in online recommendations, especially those who are rewarded based on user clicks (e.g., Youtube (2023)). Second, si is a freely chosen action by arm i, rather than a fixed parameter of arm i. We believe these modeling adjustments more realistically capture vendor behaviors in real applications. They also lead to intriguing mechanism design questions since the bandit algorithm not only needs to learn the unknown parameters, but also has to carefully align incentives to avoid undesired arm behavior. In summary, our contributions are:\n1. We introduce the strategic click-bandit problem, which involves strategic arms manipulating click-rates so as to maximize their own utility, and show that incentive-unaware algorithms generally fail to achieve low regret in the strategic click-bandit (Section 3, Proposition 4.1).\n2. We design an incentive-aware learning algorithm, UCB-S, that combines mechanism design and online learning techniques and effectively incentivizes desirable arm strategies while minimizing regret by making credible and justified threats to arms under uncertainty (Section 5).\n3. We characterize the set of Nash equilibria for the arms under the UCB-S mechanism and show that every arm i\u2019s strategy is O\u0303 ( max { \u2206i, \u221a K/T }) close to the desired strategy in\nequilibrium (Theorem 5.2). We then show that UCB-S achieves O\u0303 (\u221a KT )\nstrong strategic regret (Theorem 5.3) and complement this with an almost matching lower bound of \u2126 (\u221a KT ) for weak strategic regret (Theorem 5.5).\n4. We simulate strategic arm behavior through repeated interaction and gradient ascent and empirically demonstrate the effectiveness of the proposed UCB-S mechanism (Section 6)."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "The MAB problem is a well-studied online learning framework, which can be used to model decision-making under uncertainty (Lai et al., 1985; Auer, 2002). Since it inherently involves sequential actions and the exploration-exploitation trade-off, the MAB framework has been applied to online recommendations (Li et al., 2010; Zong et al., 2016; Wang et al., 2017) as well as a myriad of other domains (Bouneffouf et al., 2020). While there is much work studying strategic machine learning (e.g., Hardt et al., 2016; Freeman et al., 2020; Zhang and Conitzer, 2021), we here wish to highlight related work that connects online learning (and specifically the MAB formalism) to mechanism design (Nisan and Ronen, 1999). Additional related work is discussed in Appendix H.\nTo the best of our knowledge, Braverman et al. (2019) are the first to study a strategic variant of the MAB problem. In their model, when an arm is pulled, it receives a privately observed reward \u03bd and chooses to pass on a portion x of it to the principal, keeping \u03bd \u2212 x for itself. The goal of\n2We use the terms click-through-rate, click-rate, and click probability interchangeably.\nModel 1: The Strategic Click-Bandit Problem 1 Learner commits to algorithm M , which is shared with all arms 2 Arms choose strategies (s1, . . . , sK) \u2208 [0, 1]K (unknown to M ) 3 for t = 1, . . . , T do 4 Algorithm M selects arm it \u2208 [K] 5 Arm it is clicked with probability sit , i.e., ct,it \u223c Bern(sit) 6 if it was clicked (ct,it = 1) then 7 Arm it receives utility 1 from the click 8 M observes post-click reward rt,it drawn from a distribution with mean \u00b5it\nthe principal is then to incentivize arms to share as much reward with the principal as possible. In contrast to our work, the principal must not learn the underlying reward distribution or the arm strategies, but instead design an auction among arms based on the shared rewards. Feng et al. (2020) and Dong et al. (2022) study the robustness of bandit algorithms to strategic reward manipulations. However, neither work attempts to align incentives by designing mechanisms, but instead assume a limited manipulation budget. Shin et al. (2022) study MABs with strategic replication in which agents can submit several arms with replicas to the platform. They design an algorithm, which separately explores the arms submitted by each agent and in doing so discourages agents from creating additional arms and replicas. Another line of work studies auction-design in MAB formalisms, often motivated by applications in ad auctions (Babaioff et al., 2009; Devanur and Kakade, 2009; Babaioff et al., 2015). In these models, in every round the auctioneer selects one advertiser\u2019s item, which is subsequently clicked or not, and the goal of the auctioneer is to incentivize advertisers to truthfully bid their value-per-click by constructing selection and payment rules.\nTo the best of our knowledge, our work is the first to study the situation where the arms\u2019 strategies (as well as other parameters) are initially unobserved, and must be learned from interaction while simultaneously incentivizing arms under uncertainty without payments. As a result, while other work is usually able to precisely incentivize certain arm strategies, our mechanism design and characterization of the Nash equilibria are approximate."
        },
        {
            "heading": "3 THE STRATEGIC CLICK-BANDIT PROBLEM",
            "text": "We consider a natural strategic variant of the classical MAB, motivated by applications in online recommendation. Unlike classical MABs, strategic click-bandits feature decentralized interactions with the learner and multiple self-interested arms.\nLet [K] := {1, . . . ,K} denote the set of arms, each being viewed as a strategic agent. The strategic click-bandit proceeds in two phases. In the first phase, the learner commits to an online learning policy M , upon which each arm i chooses a description, which results in a corresponding clickrate si \u2208 [0, 1]. The second phase proceeds in rounds. At each round t: (1) the algorithm M pulls/recommends an arm it based on observed past data; (2) arm it is clicked with probability sit ; (3) if it is clicked, arm it receives utility 1 (whereas all other arms i receive utility 0) and the learner observes a post-click reward rt,it \u2208 [0, 1] drawn from it\u2019s reward distribution with mean \u00b5it \u2208 [0, 1]. If it is not clicked, all arms receive 0 utility and the learner does not observe any post-click rewards. The post-click mean \u00b5i is fixed for each arm i and captures the true value of the arm. From the learner\u2019s perspective, both si and \u00b5i of each arm are unknown but can be learned from online bandit feedback, that is, whether the recommended arm is clicked and, if so, what its realized reward is. In the following, we will also refer to the online learning policy M as a mechanism to emphasize its dual role in learning and incentive design. We summarize the interaction in Model 1."
        },
        {
            "heading": "3.1 LEARNER\u2019S UTILITY",
            "text": "The learner\u2019s utility of selecting an arm i with CTR si and post-click value \u00b5i is denoted u(si, \u00b5i). One example of this utility function is u(s, \u00b5) = s\u00b5. In this case, the learner monotonically prefers large s and does not care about how much the click-rate s differs from the post-click value \u00b5. However, we believe that the learner (e.g., a platform like Youtube or Airbnb) usually values consistency between the click-rates and the post-click values of arms. This could be captured by a penalty term\nfor how much si differs from \u00b5i; for instance, a natural choice is u(s, \u00b5) = s\u00b5\u2212\u03bb(s\u2212\u00b5)2 for some weight \u03bb > 0. Such non-monotonicity of the learner\u2019s utility u(si, \u00b5i) in si versus arm i\u2019s monotonic preference of larger click-rates forms the fundamental tension in the strategic click-bandit model and is also the reason that mechanism design is needed. We keep the above utility functions in mind as running examples, but derive our results for a much more general class of functions satisfying the following mild regularity assumptions:\n(A1) u : [0, 1]\u00d7 [0, 1]\u2192 R is L-Lipschitz w.r.t. the \u21131-norm.\n(A2) u\u2217(\u00b5) := maxs\u2208[0,1] u(s, \u00b5) is monotonically increasing.\n(A3) s\u2217(\u00b5) := argmaxs\u2208[0,1] u(s, \u00b5) is H-Lipschitz and is bounded away from zero.\nAssumption (A1) bounds the loss of selecting a suboptimal arm. (A2) states that, in the (idealized) situation when the arms choose click-rates so as to maximize the learner\u2019s utility u, then arms with larger post-click rewards \u00b5 are always preferred. (A3) then ensures that from the perspective of the learner most desired strategy s\u2217(\u00b5) does not change abruptly w.r.t. \u00b5 and the learner wishes to incentivize non-zero click-rates. In what follows, the function s\u2217(\u00b5) will play a central role as it describes the arm strategy that maximizes the learner\u2019s utility. For instance, in the case of u(s, \u00b5) = s\u00b5\u2212 \u03bb(s\u2212 \u00b5)2 it is given by s\u2217(\u00b5) = (1 + 12\u03bb )\u00b5. As such, the learner will typically try to incentivize an arm with post-click reward \u00b5i to choose strategy s\u2217(\u00b5i)."
        },
        {
            "heading": "3.2 ARMS\u2019 UTILITY AND NASH EQUILIBRIA AMONG ARMS",
            "text": "The mean post-click reward \u00b5i of each arm i is fixed, whereas arm i can freely choose the CTR si. In the strategic click-bandit, the objective of each arm i is to maximize the number of times it gets clicked \u2211T t=1 1{it=i} ct,i, which captures the objectives of vendors on internet platforms for whom user traffic typically proportionally converts to revenue.3 We now introduce the solution concept for the game among arms defined by a mechanism M and post-click rewards \u00b51, . . . , \u00b5K , often referred to as an equilibrium. Let s\u2212i denote the K \u2212 1 strategies of all arms except i. Each arm i chooses si to maximize their expected number of clicks vi(M, si, s\u2212i), which is a function of the mechanism M , their own action si as well as all other arms\u2019 actions s\u2212i. Concretely,\nvi(M, si, s\u2212i) := EM\n[ T\u2211\nt=1\n1{it=i} ct,i\n] (1)\nwhere the expectation is taken over the mechanism\u2019s decisions and the environment\u2019s randomness. We generally write s := (s1, . . . , sK) to summarize a strategy profile of the arms. Let \u03a3 denote the set of probability measures over [0, 1]. Given a mixed strategy profile \u03c3 = (\u03c3i, \u03c3\u2212i) \u2208 \u03a3K , i.e., a distribution over [0, 1]K , arm i\u2019s utility is then defined as vi(M,\u03c3i, \u03c3\u2212i) := Es\u223c\u03c3[vi(M, si, s\u2212i)]. Definition 3.1 (Nash Equilibrium). We say that \u03c3 = (\u03c31, . . . , \u03c3K) \u2208 \u03a3K is a Nash equilibrium (NE) under mechanism M if vi(M,\u03c3i, \u03c3\u2212i) \u2265 vi(M,\u03c3\u2032i, \u03c3\u2212i) for all i \u2208 [K] and strategies \u03c3\u2032i \u2208 \u03a3.\nIn other words, \u03c3 is in NE if no arm can increase its utility by unilaterally deviating to some other strategy. If some NE \u03c3 \u2208 \u03a3K has weight one on a pure strategy profile s \u2208 [0, 1]K , this equilibrium is said to be in pure-strategies. Let NE(M) := {\u03c3 \u2208 \u03a3K : \u03c3 is a NE under M} denote the set of all (possibly mixed) NE under mechanism M . Following conventions in standard economic analysis, we assume that the arms will form a NE in NE(M) in response to an algorithm M .4\nRemark 3.1 (Existence of Nash Equilibrium). In general, the arms\u2019 utility functions vi(M, si, s\u2212i) may be discontinuous in the arms\u2019 strategies due to their intricate dependence on the learning algorithm M . It is well-known that in games with discontinuous utilities, a NE may not exist (Reny, 1999). However, for all subsequently considered algorithms we will prove the existence of a NE by either explicitly describing the equilibrium or implicitly proving its existence.\n3More generally, different arms i may have a different value-per-click \u03bdi that could as well depend on \u00b5i so that vi(M, si, s\u2212i) = EM [ \u2211 t 1{it=i} ct,i \u03bdi]. This can easily be accommodated for by our model and our results readily extend to this case since each arm\u2019s goal still boils down to maximizing the number of clicks. 4For instance, a sufficient condition for the arms to find a NE is their knowledge about how far away they are from the best arm, i.e., their optimality gap in post-click rewards \u2206i := maxj\u2208[K] \u00b5j \u2212 \u00b5i."
        },
        {
            "heading": "3.3 STRATEGIC REGRET",
            "text": "The learner\u2019s goal is to maximize \u2211T\nt=1 u(sit , \u00b5it) which naturally depends on the arm strategies s1, . . . , sK . For given post-click values \u00b51, . . . , \u00b5K , the maximal utility u(s\u2217, \u00b5\u2217) is then achieved for \u00b5\u2217 := maxi\u2208[K] \u00b5i and s\u2217 := s\u2217(\u00b5\u2217), that is, u(s\u2217, \u00b5\u2217) = maxi\u2208[K] maxs\u2208[0,1] u(s, \u00b5i). With u(s\u2217, \u00b5\u2217) as a benchmark, we can define the strategic regret of a mechanism M under a pure-strategy equilibrium s \u2208 NE(M) as\nRT (M, s) := E\n[ T\u2211\nt=1\nu(s\u2217, \u00b5\u2217)\u2212 u(sit , \u00b5it)\n] . (2)\nFor some mixed-strategy equilibrium \u03c3 \u2208 NE(M), we then accordingly define strategic regret as RT (M,\u03c3) := Es\u223c\u03c3[RT (M, s)]. In general, there may exist several Nash equilibria for the arms under a given mechanism M . We can then consider the strong strategic regret of M given by the regret under the worst-case equilibrium:\nR+T (M) := max \u03c3\u2208NE(M) RT (M,\u03c3),\nor the weak strategic regret given by the regret under the most favorable equilibrium:\nR\u2212T (M) := min \u03c3\u2208NE(M) RT (M,\u03c3),\nwhere R\u2212T (M) \u2264 R + T (M). The regret upper bound of our proposed algorithm, UCB-S, holds under any equilibrium in NE(UCB-S), thereby bounding strong strategic regret (Theorem 5.3). On the other hand, the proven lower bounds (Proposition 4.1 and Theorem 5.5) hold for weak strategic regret and thus also apply to its strong counterpart."
        },
        {
            "heading": "4 LIMITATIONS OF INCENTIVE-UNAWARE ALGORITHMS",
            "text": "We start our analysis of the strategic click-bandit problem by showing that simply finding the arm with the largest post-click reward, argmaxi \u00b5i, or largest utility, argmaxi u(si, \u00b5i), is insufficient to achieve o(T ) weak strategic regret. In fact, we find that even with oracle knowledge of \u00b51, . . . , \u00b5K and s1, . . . , sK , an algorithm may suffer linear weak strategic regret if it fails to account for the arms\u2019 strategic nature. For such incentive-unaware oracle algorithms, we show a \u2126(T ) lower bound for weak strategic regret on any non-trivial problem instance.\nRecall that \u00b5\u2217 := maxi\u2208[K] \u00b5i and s\u2217 := s\u2217(\u00b5\u2217) and suppose that the arm i\u2217 = argmaxi\u2208[K] \u00b5i with maximal post-click rewards is unique. Our negative results rely on the following problemdependent gaps in terms of utility:\n\u03b2 := u(s\u2217, \u00b5\u2217)\u2212 u(1, \u00b5\u2217) and \u03b7 := u(s\u2217, \u00b5\u2217)\u2212 max i\u2208[K]\\{i\u2217} u\u2217(\u00b5i).\nHere, \u03b2 denotes the cost of the optimal arm i\u2217 deviating from the desired strategy s\u2217 = s\u2217(\u00b5\u2217) by playing si\u2217 = 1. The quantity \u03b7 denotes the gap between the maximally achievable utility u(s\u2217, \u00b5\u2217) and the utility of the second best arm.\nProposition 4.1. Let \u00b5-Oracle be the algorithm with oracle knowledge of \u00b51, . . . , \u00b5K that plays it = argmaxi\u2208[K] \u00b5i in every round t, whereas (s, \u00b5)-Oracle is the algorithm with oracle knowledge of \u00b51, . . . , \u00b5K and s1, . . . , sK that always plays it = argmaxi\u2208[K]u(si, \u00b5i) with ties broken in favor of the larger \u00b5. We then have\n(i) Under every equilibrium \u03c3 \u2208 NE(\u00b5-Oracle), the \u00b5-Oracle suffers regret \u2126 ( \u03b2T ) , i.e.,\nR\u2212T (\u00b5-Oracle) = \u2126 ( \u03b2T ) .\n(ii) Under every \u03c3 \u2208 NE((s, \u00b5)-Oracle), the (s, \u00b5)-Oracle suffers regret \u2126 ( min{\u03b2, \u03b7}T ) , i.e.,\nR\u2212T ((s, \u00b5)-Oracle) = \u2126 ( min{\u03b2, \u03b7}T ) .\nMechanism 1: UCB with Screening (UCB-S) 1 initialize: A0 = [K] 2 for t = 1, . . . , T do 3 if At\u22121 \u0338= \u2205 then 4 Select it \u2208 argmaxi\u2208At\u22121 \u00b5 t\u22121 i\n5 else 6 Select it uniformly at random from [K] 7 Arm it is clicked with probability sit , i.e., ct,it \u223c Bern(sit) 8 if it was clicked (ct,it = 1) then 9 Observe post-click reward rt,it\n10 if stit < min\u00b5\u2208[\u00b5tit ,\u00b5tit ] s \u2217(\u00b5) or stit > max\u00b5\u2208[\u00b5tit ,\u00b5 t it ] s \u2217(\u00b5) then 11 Ignore arm it in future rounds: At \u2190 At\u22121 \\ {it}\nProof Sketch. (i): We show that s = 1 is a strictly dominant strategy for arm i\u2217 under the \u00b5-Oracle. This implies that arm i\u2217 plays si\u2217 = 1 with probability one in every NE under the \u00b5-Oracle. The claimed lower bound then follows from bounding the instantaneous regret per round from below by \u03b2. (ii): Let j\u2217 \u2208 argmaxi \u0338=i\u2217 \u00b5i. It can be seen that in any NE, arm i\u2217 will play the largest s \u2208 [0, 1] such that u(s, \u00b5i\u2217) \u2265 u(sj\u2217 , \u00b5j\u2217). We then show that either si\u2217 = 1 or u(si\u2217 , \u00b5i\u2217) = u(s\u2217(\u00b5j\u2217), \u00b5j\u2217). Once again this allows us to lower bound the regret per round by min{\u03b2, \u03b7}.\nAs a concrete example of the failure of the \u00b5-Oracle and the (s, \u00b5)-Oracle, let us consider the running example of u(s, \u00b5) = s\u00b5 \u2212 \u03bb(s \u2212 \u00b5)2. In this case, letting \u03bb = 5 and \u00b5i\u2217 = 0.8 and \u00b5i \u2264 0.7 for i \u0338= i\u2217, we get \u03b2 \u2265 0.1 and \u03b7 \u2265 0.1 so that both oracles suffer \u2126(T ) regret in every equilibrium."
        },
        {
            "heading": "5 NO-REGRET INCENTIVE-AWARE LEARNING: UCB-S",
            "text": "The results of Proposition 4.1 suggest that any incentive-unaware learning algorithm that is oblivious to the strategic nature of the arms will generally fail to achieve low regret. In particular, \u201cunconditional\u201d selection of any arm will likely result in undesirable equilibria among arms. For these reasons, we deploy a conceptually simple screening idea, which threatens arms with elimination when deviating from the desired strategies.\nLet denote nt(i) be the number of times up to (and including) round t that arm i was selected by the learner, and let mt(i) denote the number of times post-click rewards were observed for arm i up to (and including) round t. Let s\u0302ti be the average observed click-rate and \u00b5\u0302 t i the average observed post-click reward for arm i. We then define the pessimistic and optimistic estimates of si and \u00b5i as\nsti = s\u0302 t i \u2212 \u221a 2 log(T )/nt(i), s t i = s\u0302 t i + \u221a 2 log(T )/nt(i),\n\u00b5ti = \u00b5\u0302 t i \u2212 \u221a 2 log(T )/mt(i), \u00b5 t i = \u00b5\u0302 t i + \u221a 2 log(T )/mt(i).\nwhere sti = \u2212\u221e and sti = +\u221e for nt(i) = 0 as well as \u00b5ti = \u2212\u221e and \u00b5 t i = +\u221e for mt(i) = 0.\nIn every round, UCB-S (Mechanism 1) selects arms optimistically according to their post-click rewards and subsequently observes if the arm is clicked, i.e., ct,it , and, if so, a post-click reward rt,it . However, if an arm\u2019s click-rate si is detected to be different from the learner\u2019s desired arm strategy s\u2217(\u00b5i), the arm is eliminated forever, expressed by the screening rule in line 10:\nstit < min \u00b5\u2208[\u00b5tit ,\u00b5 t it ] s\u2217(\u00b5) or stit > max \u00b5\u2208[\u00b5tit ,\u00b5 t it ] s\u2217(\u00b5).\nThe only exception is when all arms have been eliminated. Then, UCB-S plays them all uniformly for the remaining rounds. To ensure that the elimination of an arm is credible and justified with high probability, we leverage confidence bounds on si and \u00b5i. More precisely, if an arm is truthful and chooses si = s\u2217(\u00b5i), then with probability 1\u2212 1/T 2 it will not be eliminated by the screening rule.\nAs a prelude to the analysis of the UCB-S mechanism, we begin by showing that there always exists a NE among the arms under UCB-S. As mentioned briefly in Section 3, the existence of a NE among the arms is not guaranteed under an arbitrary mechanism due to the arms\u2019 continuous strategy space and possibly discontinuous utility function. Lemma 5.1. For any post-click rewards \u00b51, . . . , \u00b5K , there always exists a (possibly mixed) Nash equilibrium for the arms under the UCB-S mechanism."
        },
        {
            "heading": "5.1 CHARACTERIZING THE NASH EQUILIBRIA UNDER UCB-S",
            "text": "We now approximately characterize all NE for the arms under the UCB-S mechanism. In order to prove a regret upper bound for UCB-S, it will be key to ensure that each arm i plays a strategy si which is sufficiently close to the desired strategy s\u2217(\u00b5i) (i.e., the strategy that maximizes the learner\u2019s utility). This is particularly important for arms i\u2217 with maximal post-click rewards \u00b5i\u2217 = maxi\u2208[K] \u00b5i. If such arms i\u2217 were to deviate substantially from s\u2217(\u00b5i\u2217), e.g., by a constant amount, the learner would be forced to suffer constant regret even when selecting arms with maximal postclick rewards, making it impossible to achieve sublinear regret.\nIn the following, we show that under the UCB-S mechanism every NE is such that the strategies of arms with maximal post-click rewards deviate from the desired strategies by at most O\u0303( \u221a K/T ). We then also show that for suboptimal arms the difference between each arm i\u2019s strategy si and the desired strategy s\u2217(\u00b5i) is governed by their optimality gap in post-click rewards, given by \u2206i := \u00b5\u2217 \u2212 \u00b5i. Recall that H denotes the Lipschitz constant of s\u2217(\u00b5). Theorem 5.2. For all s \u2208 supp(\u03c3) with \u03c3 \u2208 NE(UCB-S) and all i \u2208 [K]:\nsi = s \u2217(\u00b5i) +O ( H \u00b7max { \u2206i, \u221a K log(T )\nT\n}) .\nIn particular, for all arms i\u2217 \u2208 [K] with \u2206i\u2217 = 0, i.e., maximal post-click rewards:\nsi\u2217 = s \u2217(\u00b5i\u2217) +O ( H \u221a K log(T )\nT\n) .\nThe derivation of Theorem 5.2 can be best understood by noting that the estimates of each arm\u2019s strategy roughly concentrate at a rate of 1/ \u221a t. Then, depending on how often an arm expects to be selected by UCB-S, it can exploit our uncertainty about its strategy and safely increase its click-rates to match our confidence. Generally, optimal arms expect at least T/K allocations while preventing elimination, which can be seen to imply NE strategies that deviate by at most \u221a K/T . On the other hand, suboptimal arms can expect roughly log(T )/\u22062i allocations as long as they can prevent elimination and all other arms act rationally, which results in the linear dependence on \u2206i. Hence, interestingly UCB-S\u2019 selection policy directly impacts the truthfulness of the arms, as arms that are selected more frequently are forced to choose strategies closer to s\u2217(\u00b5i). We thus observe a trade-off between incentivizing all arms to be truthful and recommending only the best arms. The proof of Theorem 5.2 (Appendix C) then relies on the above observation and careful and repeated application of the best response property of the Nash equilibrium."
        },
        {
            "heading": "5.2 UPPER BOUND OF THE STRONG STRATEGIC REGRET OF UCB-S",
            "text": "With the approximate NE characterization from Theorem 5.2 at our disposal, we are ready to prove a regret upper bound for UCB-S. We show that the strong strategic regret of the UCB-S mechanism is upper bounded by O\u0303 (\u221a KT ) , that is, for any \u03c3 \u2208 NE(UCB-S) the regret guarantee holds.\nTheorem 5.3. Let \u2206i := \u00b5\u2217 \u2212 \u00b5i and let L and H denote the Lipschitz constants of u(s, \u00b5) and s\u2217(\u00b5), respectively. The strong strategic regret of UCB-S is bounded as\nR+T (UCB-S) = LH \u00b7 O\n(\u221a KT log(T ) +\n\u2211 i:\u2206i>0 log(T ) \u2206i\n) . (3)\nIn other words, the above regret bound is achieved under any equilibrium \u03c3 \u2208 NE(UCB-S).\nProof Sketch. As suggested by the regret bound there are two sources of regret. Broadly speaking, the first term on the right hand side of (3) corresponds to the regret UCB-S suffers due to arms with maximal post-click rewards (i.e., \u2206i = 0) deviating from the utility-maximizing strategy s\u2217(\u00b5\u2217). For such arms Theorem 5.2 bounded the deviation by a term of order \u221a K/T , thereby leading to at\nmost order \u221a KT regret. The second term in (3) corresponds to the regret suffered from playing arms with suboptimal post-click rewards, i.e., \u2206i > 0. Using a typical UCB argument, the Lipschitzness of u(s, \u00b5) and s\u2217(\u00b5), and again Theorem 5.2 applied to |s\u2217(\u00b5\u2217) \u2212 si| \u2264 |s\u2217(\u00b5\u2217) \u2212 s\u2217(\u00b5i)| + O(H\u2206i) \u2264 H\u2206i +O(H\u2206i) we obtain the claimed upper bound.\nSimilarly to classical MABs we can state a regret bound independent of the instance-dependent quantities \u2206i and translate Theorem 5.3 into a minimax-type guarantee. Corollary 5.4. The strong strategic regret of UCB-S is bounded as\nR+T (UCB-S) = O ( LH \u221a KT log(T ) ) .\nIn other words, the above regret bound is achieved under any equilibrium \u03c3 \u2208 NE(UCB-S).\nTheorem 5.3 nicely shows that the additional cost of the incentive design and the strategic behavior of the arms is of order \u221a KT which primarily stems from arms with maximal post-click rewards deviating by roughly \u221a K/T from the desired strategy (see Theorem 5.2). The dishonesty of suboptimal arms does not notably contribute to the regret and is contained in the log(T )/\u2206i expressions as we can bound the number of times suboptimal arms are played sufficiently well. As a result, the total cost of incentive design and strategic behavior matches the minimax learning complexity of MABs so that we obtain an overall O\u0303( \u221a KT ) strategic regret bound under every equilibrium."
        },
        {
            "heading": "5.3 LOWER BOUND FOR WEAK STRATEGIC REGRET",
            "text": "Complementing our regret analysis, we prove a lower bound on weak strategic regret in the strategic click-bandit. By definition, weak strategic regret lower bounds its strong counterpart, i.e., R\u2212T (M) \u2264 R+T (M), so that the shown lower bound directly applies to strong strategic regret as well, which implies that UCB-S is near-optimal. Theorem 5.5. Let M be any mechanism with NE(M) \u0338= \u2205. There exists a utility function u satisfying (A1)-(A3) and post-click rewards \u00b51, . . . , \u00b5K such that for all Nash equilibria \u03c3 \u2208 NE(M):\nRT (M,\u03c3) = \u2126 (\u221a KT ) .\nIn other words, R\u2212T (M) = \u2126 (\u221a KT ) .\nProof Sketch. Consider the utility function u(s, \u00b5) = s\u00b5. Intuitively, for any low regret mechanism M the NE for the arms will be in (s1, . . . , sK) = (1, . . . , 1) as these strategies maximize the learner\u2019s utility u and are to the advantage of the arms. In this case, the learning problem reduces to a classical MAB and we inherit the well-known minimax \u221a KT lower bound. However, it is not directly clear that there exists no better mechanism that would, e.g., incentivize arm strategies (s1, . . . , si\u2217 , . . . , sK) = (0, . . . , 1, . . . , 0) under which i\u2217 = argmaxi \u00b5i becomes easier to distinguish from i \u0338= i\u2217. For this reason, we argue via the arms\u2019 utilities and lower bound the minimal utility a suboptimal arm must receive in any NE. This directly implies a lower bound on the number of times we must play any suboptimal arm in equilibrium, which yields the claimed result."
        },
        {
            "heading": "6 SIMULATING STRATEGIC ARM BEHAVIOR VIA REPEATED INTERACTION",
            "text": "Goal of the experiments is to analyze the effect of the proposed incentive-aware learning algorithm UCB-S on strategically responding arms. Strategic arm behavior is here modeled through decentralized gradient ascent and repeated interaction with the mechanism. Contrary to the assumption of arms playing in NE, arms follow a simple gradient ascent strategy to adapt to the mechanism, which serves as a realistic and natural model of strategic behavior. This requires no prior knowledge from the point of view of the arms and all learning is performed through sequential interaction with the mechanism. For this reason, the final strategies in our experiments may not necessarily be in NE. Despite this, we want to see whether the mechanism is still able to incentivize arms to behave in the desired manner which will also provide insight into the robustness of the proposed incentive design.\ngradient step with respect to its utility vi. We initialized the arm strategies to si = 1, however, our experiments show that other initialization, such as si = 0 or si = 0.5, yield similar results. All results are averaged over 10 complete runs and the standard deviation shown in shaded color. Results. The conducted simulations show that under natural greedy behavior as modeled by gradient ascent, the incentive design of UCB-S is still effective and desirable arm strategies incentivized (Figure 2). Most notably, the optimal arm (having the largest incentive to be truthful) converges to a strategy close to the desired strategy s\u2217(\u00b51). The suboptimal arms do not converge to a strategy close to the desired strategy and we observe that the distance to s\u2217(\u00b5i) depends on the optimality gap \u2206i, which mirrors our theoretical results (Theorem 5.2). In addition, Figure 4 shows that as the arms interact with UCB-S and adapt their strategies, the regret of UCB-S improves substantially. In contrast, incentive-unaware algorithms like UCB fail to incentivize desirable strategies (all arm strategies remain close to 1, see Figure 3) and UCB accordingly suffers large regret (Figure 4) throughout all epochs. The observation that UCB-S initially suffer larger regret than UCB can be explained by the elimination rule causing UCB-S to select arms uniformly at random when arms are notably untruthful. This threat of elimination, however, incentivizes the arms to adapt their strategies in the next epoch and eventually leads to smaller regret for UCB-S."
        },
        {
            "heading": "7 DISCUSSION",
            "text": "We study the strategic click-bandit problem in which each arm is associated with a click-rate, chosen strategically by the arms, and an immutable post-click reward. We show the necessity of incentive design in this model and design an incentive-aware online learning algorithm that incentivizes desirable arm strategies under uncertainty. As the learner has no prior knowledge of the arm strategies and the post-click rewards, the mechanism design is approximate and leaves room for arms to exploit the learner\u2019s uncertainty. This leads to an interesting regret bound which makes the intuition precise that arms can exploit the learner\u2019s uncertainty about their strategies. In our simulations we then observe that our incentive design is robust and still effective under natural greedy arm behavior and that the design of incentive-aware learning algorithms is necessary to achieve low regret under strategic arm behavior. Some interesting open questions which we leave for future work include whether the proposed incentive design remains effective under adaptive arm strategies and whether we can construct a mechanism under which there exists a desirable NE in dominant strategies.\nAcknowledgments. We thank the anonymous reviewers for their insightful and constructive comments. We also want to thank Boi Faltings for helpful discussions in the early stages of this work. Thomas Kleine Buening was partly supported by the Norwegian Research Council (Grant 302203). Haifeng Xu is supported by the AI2050 program at Schmidt Sciences (Grant G-24-66104), an NSF Award CCF-2303372, an Army Research Office Award W911NF-23-1-0030, and an Office of Naval Research Award N00014-23-1-2802."
        }
    ]
}