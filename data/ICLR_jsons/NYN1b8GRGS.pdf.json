{
    "abstractText": "Image matching is a fundamental computer vision problem. While learning-based methods achieve state-of-the-art performance on existing benchmarks, they generalize poorly to in-the-wild images. Such methods typically need to train separate models for different scene types (e.g., indoor vs. outdoor) and are impractical when the scene type is unknown in advance. One of the underlying problems is the limited scalability of existing data construction pipelines, which limits the diversity of standard image matching datasets. To address this problem, we propose GIM, a self-training framework for learning a single generalizable model based on any image matching architecture using internet videos, an abundant and diverse data source. Given an architecture, GIM first trains it on standard domain-specific datasets and then combines it with complementary matching methods to create dense labels on nearby frames of novel videos. These labels are filtered by robust fitting, and then enhanced by propagating them to distant frames. The final model is trained on propagated data with strong augmentations. Not relying on complex 3D reconstruction makes GIM much more efficient and less likely to fail than standard SfM-and-MVS based frameworks. We also propose ZEB, the first zero-shot evaluation benchmark for image matching. By mixing data from diverse domains, ZEB can thoroughly assess the cross-domain generalization performance of different methods. Experiments demonstrate the effectiveness and generality of GIM. Applying GIM consistently improves the zero-shot performance of 3 state-of-theart image matching architectures as the number of downloaded videos increases (Fig. 1 (a)); with 50 hours of YouTube videos, the relative zero-shot performance improves by 8.4% \u2212 18.1%. GIM also enables generalization to extreme crossdomain data such as Bird Eye View (BEV) images of projected 3D point clouds (Fig. 1 (c)). More importantly, our single zero-shot model consistently outperforms domain-specific baselines when evaluated on downstream tasks inherent to their respective domains. The code will be released upon acceptance.",
    "authors": [],
    "id": "SP:a97e85754ae575bfd56d46954be8ad7984e7a6b0",
    "references": [
        {
            "authors": [
                "Relja Arandjelovi\u0107",
                "Andrew Zisserman"
            ],
            "title": "Three things everyone should know to improve object retrieval",
            "venue": "In 2012 IEEE conference on computer vision and pattern recognition,",
            "year": 2012
        },
        {
            "authors": [
                "Vassileios Balntas",
                "Karel Lenc",
                "Andrea Vedaldi",
                "Krystian Mikolajczyk"
            ],
            "title": "Hpatches: A benchmark and evaluation of handcrafted and learned local descriptors",
            "year": 2017
        },
        {
            "authors": [
                "Daniel Barath",
                "Jiri Matas",
                "Jana Noskova"
            ],
            "title": "MAGSAC: marginalizing sample consensus",
            "venue": "In Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Herbert Bay",
                "Tinne Tuytelaars",
                "Luc Van Gool"
            ],
            "title": "Surf: Speeded up robust features",
            "venue": "Computer Vision \u2013 ECCV",
            "year": 2006
        },
        {
            "authors": [
                "Angela Dai",
                "Angel X. Chang",
                "Manolis Savva",
                "Maciej Halber",
                "Thomas A. Funkhouser",
                "Matthias Nie\u00dfner"
            ],
            "title": "Scannet: Richly-annotated 3d reconstructions of indoor scenes",
            "year": 2017
        },
        {
            "authors": [
                "Mihai Dusmanu",
                "Ignacio Rocco",
                "Tom\u00e1s Pajdla",
                "Marc Pollefeys",
                "Josef Sivic",
                "Akihiko Torii",
                "Torsten Sattler"
            ],
            "title": "D2-net: A trainable cnn for joint description and detection of local features",
            "year": 2019
        },
        {
            "authors": [
                "Johan Edstedt",
                "Ioannis Athanasiadis",
                "M\u00e5rten Wadenb\u00e4ck",
                "Michael Felsberg"
            ],
            "title": "Dkm: Dense kernelized feature matching for geometry estimation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Martin A. Fischler",
                "Robert C. Bolles"
            ],
            "title": "Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography",
            "venue": "Commun. ACM,",
            "year": 1981
        },
        {
            "authors": [
                "Andreas Geiger",
                "Philip Lenz",
                "Raquel Urtasun"
            ],
            "title": "Are we ready for autonomous driving? the kitti vision benchmark suite",
            "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2012
        },
        {
            "authors": [
                "Yves Grandvalet",
                "Yoshua Bengio"
            ],
            "title": "Semi-supervised learning by entropy minimization",
            "venue": "Advances in neural information processing systems,",
            "year": 2004
        },
        {
            "authors": [
                "A. Handa",
                "T. Whelan",
                "J.B. McDonald",
                "A.J. Davison"
            ],
            "title": "A benchmark for RGB-D visual odometry, 3D reconstruction and SLAM",
            "venue": "In IEEE Intl. Conf. on Robotics and Automation,",
            "year": 2014
        },
        {
            "authors": [
                "Yuhe Jin",
                "Dmytro Mishkin",
                "Anastasiia Mishchuk",
                "Jiri Matas",
                "Pascal Fua",
                "Kwang Moo Yi",
                "Eduard Trulls"
            ],
            "title": "Image Matching across Wide Baselines: From Paper to Practice",
            "year": 2020
        },
        {
            "authors": [
                "Zhengqi Li",
                "Noah Snavely"
            ],
            "title": "Megadepth: Learning single-view depth prediction from internet",
            "venue": "photos. CVPR,",
            "year": 2018
        },
        {
            "authors": [
                "David G. Lowe"
            ],
            "title": "Distinctive image features from scale-invariant keypoints",
            "venue": "IJCV, 60:91\u2013110,",
            "year": 2004
        },
        {
            "authors": [
                "Will Maddern",
                "Geoff Pascoe",
                "Chris Linegar",
                "Paul Newman"
            ],
            "title": "Year, 1000km: The Oxford RobotCar Dataset",
            "venue": "The International Journal of Robotics Research (IJRR),",
            "year": 2017
        },
        {
            "authors": [
                "John McCormac",
                "Ankur Handa",
                "Stefan Leutenegger",
                "Andrew J.Davison"
            ],
            "title": "Scenenet rgb-d: Can 5m synthetic images beat generic imagenet pre-training on indoor segmentation",
            "year": 2017
        },
        {
            "authors": [
                "A. Mishchuk",
                "Dmytro Mishkin",
                "Filip Radenovi\u0107",
                "Jiri Matas"
            ],
            "title": "Working hard to know your neighbor\u2019s margins: Local descriptor learning loss",
            "venue": "NeurIPS,",
            "year": 2017
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "venue": "In International conference on machine learning,",
            "year": 2021
        },
        {
            "authors": [
                "Ren\u00e9 Ranftl",
                "Katrin Lasinger",
                "David Hafner",
                "Konrad Schindler",
                "Vladlen Koltun"
            ],
            "title": "Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer",
            "venue": "IEEE transactions on pattern analysis and machine intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "J\u00e9r\u00f4me Revaud",
                "C\u00e9sar Roberto de Souza",
                "M. Humenberger",
                "Philippe Weinzaepfel"
            ],
            "title": "R2d2: Reliable and repeatable detector and descriptor",
            "venue": "In NeurIPS,",
            "year": 2019
        },
        {
            "authors": [
                "Ethan Rublee",
                "Vincent Rabaud",
                "Kurt Konolige",
                "Gary Bradski"
            ],
            "title": "Orb: An efficient alternative to sift or surf",
            "venue": "In 2011 International Conference on Computer Vision, pp. 2564\u20132571,",
            "year": 2011
        },
        {
            "authors": [
                "Paul-Edouard Sarlin",
                "Daniel DeTone",
                "Tomasz Malisiewicz",
                "Andrew Rabinovich"
            ],
            "title": "SuperGlue: Learning feature matching with graph neural networks",
            "venue": "In CVPR,",
            "year": 2020
        },
        {
            "authors": [
                "Torsten Sattler",
                "Will Maddern",
                "Carl Toft",
                "Akihiko Torii",
                "Lars Hammarstrand",
                "Erik Stenborg",
                "Daniel Safari",
                "Masatoshi Okutomi",
                "Marc Pollefeys",
                "Josef Sivic",
                "Fredrik Kahl",
                "Tomas Pajdla"
            ],
            "title": "Benchmarking 6DOF Outdoor Visual Localization in Changing Conditions",
            "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2018
        },
        {
            "authors": [
                "Thomas Sch\u00f6ps",
                "Johannes L. Sch\u00f6nberger",
                "Silvano Galliani",
                "Torsten Sattler",
                "Konrad Schindler",
                "Marc Pollefeys",
                "Andreas Geiger"
            ],
            "title": "A multi-view stereo benchmark with high-resolution images and multi-camera videos",
            "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2017
        },
        {
            "authors": [
                "Tianwei Shen",
                "Zixin Luo",
                "Lei Zhou",
                "Runze Zhang",
                "Siyu Zhu",
                "Tian Fang",
                "Long Quan"
            ],
            "title": "Matchable image retrieval by learning from surface reconstruction",
            "venue": "In The Asian Conference on Computer Vision (ACCV,",
            "year": 2018
        },
        {
            "authors": [
                "Jiaming Sun",
                "Zehong Shen",
                "Yuang Wang",
                "Hujun Bao",
                "Xiaowei Zhou"
            ],
            "title": "Loftr: Detector-free local feature matching with transformers",
            "venue": "In CVPR,",
            "year": 2021
        },
        {
            "authors": [
                "Hajime Taira",
                "Masatoshi Okutomi",
                "Torsten Sattler",
                "Mircea Cimpoi",
                "Marc Pollefeys",
                "Josef Sivic",
                "Tomas Pajdla",
                "Akihiko Torii"
            ],
            "title": "InLoc: Indoor visual localization with dense matching and view synthesis",
            "year": 2018
        },
        {
            "authors": [
                "Yurun Tian",
                "Bin Fan",
                "F. Wu"
            ],
            "title": "L2-net: Deep learning of discriminative patch descriptor in euclidean space",
            "year": 2017
        },
        {
            "authors": [
                "Michal J. Tyszkiewicz",
                "P. Fua",
                "Eduard Trulls"
            ],
            "title": "Disk: Learning local features with policy gradient",
            "venue": "NeurIPS,",
            "year": 2020
        },
        {
            "authors": [
                "Shimon Ullman"
            ],
            "title": "The interpretation of structure from motion",
            "venue": "Proceedings of the Royal Society of London. Series B. Biological Sciences,",
            "year": 1979
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Kaixuan Wang",
                "Shaojie Shen"
            ],
            "title": "Flow-motion and depth network for monocular stereo and beyond",
            "venue": "IEEE Robotics Autom. Lett.,",
            "year": 2020
        },
        {
            "authors": [
                "Heng Yang",
                "Wei Dong",
                "Luca Carlone",
                "Vladlen Koltun"
            ],
            "title": "Self-supervised geometric perception",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Yao Yao",
                "Zixin Luo",
                "Shiwei Li",
                "Jingyang Zhang",
                "Yufan Ren",
                "Lei Zhou",
                "Tian Fang",
                "Long Quan"
            ],
            "title": "Blendedmvs: A large-scale dataset for generalized multi-view stereo networks",
            "venue": "Computer Vision and Pattern Recognition (CVPR),",
            "year": 2020
        },
        {
            "authors": [
                "Wei Yin",
                "Chi Zhang",
                "Hao Chen",
                "Zhipeng Cai",
                "Gang Yu",
                "Kaixuan Wang",
                "Xiaozhi Chen",
                "Chunhua Shen"
            ],
            "title": "Metric3d: Towards zero-shot metric 3d prediction from a single image",
            "venue": "In ICCV,",
            "year": 2023
        },
        {
            "authors": [
                "Ekim Yurtsever",
                "Jacob Lambert",
                "Alexander Carballo",
                "Kazuya Takeda"
            ],
            "title": "A survey of autonomous driving: Common practices and emerging technologies",
            "venue": "IEEE access,",
            "year": 2020
        },
        {
            "authors": [
                "Zichao Zhang",
                "Henri Rebecq",
                "Christian Forster",
                "Davide Scaramuzza"
            ],
            "title": "Benefit of large field-of-view cameras for visual odometry",
            "venue": "IEEE International Conference on Robotics and Automation,",
            "year": 2016
        },
        {
            "authors": [
                "RobotcarWeather Maddern"
            ],
            "title": "2017) seasonal changes",
            "venue": "RobotcarNight Maddern et al. (2017) sunlight changes",
            "year": 2017
        }
    ],
    "sections": [
        {
            "text": ""
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "Image matching is a fundamental computer vision task, the backbone for many applications such as 3D reconstruction Ullman (1979), visual localization Sattler et al. (2018) and autonomous driving (Yurtsever et al., 2020).\nHand-crafted methods (Lowe, 2004; Bay et al., 2006) utilize predefined heuristics to compute and match features. Though widely adopted, these methods often yield limited matching recall and density on challenging scenarios, such as long-baselines and extreme weather. Learning-based methods have emerged as a promising alternative with a much higher accuracy (Sarlin et al., 2020; Sun et al., 2021) and matching density (Edstedt et al., 2023). However, due to the scarcity of diverse multiview data with ground-truth correspondences, current approaches typically train separate indoor and outdoor models on ScanNet and MegaDepth respectively. Such domain-specific training limits their generalization to unseen scenarios, and makes them impractical for applications with unknown scene types. Moreover, existing data construction methods, which rely on RGBD scans (Dai et al., 2017) or Structure-from-Motion (SfM) + Multi-view Stereo (MVS) (Li & Snavely, 2018), have limited efficiency and applicability, making them ineffective for scaling up the data and model training.\nTo address these issues, we propose GIM, the first framework that can learn a single image matcher generalizable to in-the-wild data from different domains. Inspired by foundation models for computer vision (Radford et al., 2021; Ranftl et al., 2020; Kirillov et al., 2023), GIM achieves zero-shot generalization by self-training (Grandvalet & Bengio, 2004) on diverse and large-scale visual data. We use internet videos as they are easy to obtain, diverse, and practically unlimited. Given any image matching architecture, GIM first trains it on standard domain-specific datasets (Li & Snavely, 2018; Dai et al., 2017). Then, the trained model is combined with multiple complementary image matching methods to generate candidate correspondences on nearby frames of downloaded videos. The final labels are generated by removing outlier correspondences using robust fitting, and propagating the correspondences to distant video frames. Strong data augmentations are applied when training the final generalizable model. Standard SfM and MVS based label generation pipelines (Li & Snavely, 2018) have limited efficiency and are prone to fail on in-the-wild videos (see Sec. 4.2 for details). Instead, GIM can efficiently generate reliable supervision signals on diverse internet videos and effectively improve the generalization of state-of-the-art models.\nTo thoroughly evaluate the generalization performance of different methods, we also construct the first zero-shot evaluation benchmark ZEB, consisting of data from 8 real-world and 4 simulated domains. The diverse cross-domain data allow ZEB to identify the in-the-wild generalization gap of existing domain-specific models. For example, we found that advanced hand-crafted methods (Arandjelovic\u0301 & Zisserman, 2012) perform better than recent learning-based methods Sarlin et al. (2020); Sun et al. (2021) on several domains of ZEB.\nExperiments demonstrate the significance and generality of GIM. Using 50 hours of YouTube videos, GIM achieves a relative zero-shot performance improvement of 9.9%, 18.1% and 8.4% respectively for SuperGlue (Sarlin et al., 2020) LoFTR (Sun et al., 2021) and DKM (Edstedt et al., 2023). The performance improves consistently with the amount of video data (Fig. 1 (a)). Despite trained only on normal RGB images, our model generalizes well to extreme cross-domain data such as BEV images of projected 3D point clouds (Fig. 1 (c)). Besides image matching robustness, a single GIM model achieves cross-the-board performance improvements on various down-stream tasks such as visual localization, homography estimation and 3D reconstruction, even comparing to in-domain baselines on their specific domains. In summary, the contributions of this work include:\n\u2022 GIM, the first framework that can learn a generalizable image matcher from internet videos.\n\u2022 ZEB, the first zero-shot image matching evaluation benchmark.\n\u2022 Experiments showing the effectiveness and generality of GIM for both image matching and various downstream tasks."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Image matching methods: Hand-crafted methods (Lowe, 2004; Bay et al., 2006; Rublee et al., 2011) use predefined heuristics to compute local features and perform matching. RootSIFT (Arand-\njelovic\u0301 & Zisserman, 2012) combined with the ratio test has achieved superior performance (Jin et al., 2020). Though robust, hand-crafted methods only produce sparse key-point matches, which contain many outliers for challenging inputs such as low overlapping images. Many methods (Tian et al., 2017; Mishchuk et al., 2017; Dusmanu et al., 2019; Revaud et al., 2019; Tyszkiewicz et al., 2020) have been proposed recently to learn better single-image local features from data. Sarlin et al. (2020) pioneered the use of Transformers (Vaswani et al., 2017) with two images as the input and achieved significant performance improvement. The output density has also been significantly improved by state-of-the-art semi-dense (Sun et al., 2021) and dense matching methods (Edstedt et al., 2023). However, existing learning-based methods train indoor and outdoor models separately, making them generalize poorly on in-the-wild data. We find that RootSIFT performs better than recent learning-based methods (Sarlin et al., 2020; Sun et al., 2021) in many in-the-wild scenarios. We show that domain-specific training and evaluation are the cause of the poor robustness, and propose a novel framework GIM that can learn generalizable image matching from internet videos. Similar to GIM, SGP (Yang et al., 2021) also applied self-training (using RANSAC + SIFT). However, it was not designed to improve generalization and still trained models on domain-specific data. Empirical results (Sec. 4.2) show that simple robust fitting without further label enhancement cannot improve generalization effectively.\nImage matching datasets: Existing image matching methods typically train separate models for indoor and outdoor scenes using MegaDepth (Li & Snavely, 2018) and ScanNet (Dai et al., 2017) respectively. These models are then evaluated on test data from the same domain. MegaDepth consists of 196 scenes reconstructed from 1 million internet photos using COLMAP (Scho\u0308nberger & Frahm, 2016). The diversity is limited since most scenes are of famous tourist attractions and hence revolve around a central object. ScanNet consists of 1613 different scenes reconstructed from RGBD images using BundleFusion. ScanNet only covers indoor scenes in schools and it is difficult to use RGBD scans to obtain diverse images from different places of the world. In contrast, we propose to use internet videos, a virtually unlimited and diverse data source to complement the scenes not covered by existing datasets. The in-domain test data used in existing methods is also limited since they lack cross-domain data with diverse scene conditions, such as aerial photography, outdoor natural environments, weather variations, and seasonal changes. To address this problem and fully measure the generalization ability of a model, we propose ZEB, a novel zero-shot evaluation benchmark for image matching with diverse in-the-wild data.\nZero-shot computer vision models: Learning generalizable models has been an important research topic recently. CLIP (Radford et al., 2021) was trained on 400 million image-text pairs collected from the internet. This massive corpus provided strong supervision, enabling the model to learn a wide range of visual-textual concepts. Ranftl et al. (2020) mixed various existing depth estimation datasets and complementing them with frames and disparity labels from 3D movies. This allowed the depth estimation model to first time generalize across different environments. SAM (Kirillov et al., 2023) was trained on SA-1B containing over 1 billion masks from 11 million diverse images. This training data was collected using a \u201cdata engine\u201d, a three-stage process involving assistedmanual, semi-automatic, and fully automatic annotation with the model in the loop. A common approach for all these methods is to efficiently generate diverse and large scale training data. This work applies a similar idea to learn generalizable image matching. We propose GIM, a self-training framework to efficiently create supervision signals on diverse internet videos."
        },
        {
            "heading": "3 METHODOLOGY",
            "text": "Training image matching models requires multi-view images and ground-truth correspondences. Data diversity and scale have been the key towards generalizable models in other computer vision problems (Radford et al., 2021; Ranftl et al., 2020; Kirillov et al., 2023). Inspired by this observation, we propose GIM (Fig. 2), a self-training framework utilizing internet videos to learn a single generalizable model based on any image matching architecture.\nThough other video sources are also applicable, GIM uses internet videos since they are naturally diverse and nearly infinite. To experiment with commonly accessible data, we download 50 hours (hundreds of hours available) of tourism videos with the Creative Commons License from YouTube, covering 26 countries, 43 cities, various lightning conditions, dynamic objects and scene types. See Appendix A for details.\nStandard image matching benchmarks are created by RGBD scans Dai et al. (2017) or COLMAP (SfM + MVS) (Scho\u0308nberger & Frahm, 2016; Li & Snavely, 2018). RGBD scans require physical access to the scene, making it hard to obtain data from diverse environments. COLMAP is effective for landmark-type scenes with dense view coverage, however, it has limited efficiency and often fails on in-the-wild data with arbitrary motions. As a result, although millions of images are available in these datasets, the diversity is limited since thousands of images come from one (small) scene. In contrast, internet videos are not landmark-centric. A one hour tourism video typically covers a range of several kilometers (e.g., a city), and has widely spread view points. As discussed later in Sec. 3.1, the temporal information in videos also allows us to enhance the supervision signal significantly."
        },
        {
            "heading": "3.1 SELF-TRAINING",
            "text": "A naive approach to learn from video data is to generate labels using the standard COLMAP-based pipeline (Li & Snavely, 2018); however, preliminary experiments show that it is inefficient and prone to fail on in-the-wild videos (see Sec. 4.2 for details). To better scale up video training, GIM relies on self-training (Grandvalet & Bengio, 2004), which first trains a model on standard labeled data and then utilizes the enhanced output (on videos) of the trained model to boost the generalization of the same architecture.\nMulti-method matching: Given an image matching architecture, GIM first trains it on standard (domain-specific) datasets (Li & Snavely, 2018; Dai et al., 2017), and uses the trained model as the \u2018base label generator\u2019. As shown in Fig. 2, for each video, we uniformly sample images every 20 frames to reduce redundancy. For each frame X , we generate base correspondences between {X,X + 20}, {X,X + 40} and {X,X + 80}. The base correspondences are generated by running robust fitting Barath et al. (2019) on the output of the base label generator. We fuse these labels with the outputs of different complementary matching methods to significantly enhance the label density. These methods can either be hand-crafted algorithms, or other architectures trained on standard datasets; see Sec. 4 for details.\nLabel propagation: Existing image matching methods typically require strong supervision signals from images with small overlaps (Sarlin et al., 2020). However, this cannot be achieved by multimethod matching since the correspondences generated by existing methods are not reliable beyond an interval of 80 frames, even with state-of-the-art robust fitting algorithms for outlier filtering. An important benefit of learning from videos is that the dense correspondences between a video frame and different nearby frames often locate at common pixels. This allows us to propagate the correspondences to distant frames, which significantly enhances the supervision signal (see Sec. 4.2 for an analysis). Formally, we define CAB \u2208 {0, 1}rA\u00d7rB as the correspondence matrix of image IA and IB , where rA and rB are the number of pixels in IA and IB . A matrix element cABij = 1 means that pixel i in IA has a corresponding pixel j in IB . Given the correspondences CAB and CBC , to obtain the propagated correspondences CAC , for each cABij in C\nAB that is 1, if we can also find a cBCj\u2032k = 1 in C\nBC , and the distance between j and j\u2032 in image IB is less than 1 pixel, we set cACik = 1 in C\nAC . Intuitively, this means that for pixel j (or j\u2032) in image IB , it matches to both pixel i in IA and pixel k in IC . Hence image IA and IC have a correspondence at location (i, k).\nTo obtain strong supervision signals, we propagate the correspondences as far as possible as long as we have more than 1024 correspondences between two images. The propagation is executed on each sampled frame (with 20 frame interval) separately. After each propagation step, we double the frame interval for each image pair that has correspondences. As an example, initially we have base correspondences between every 20, 40 and 80 frames. After 1 round of propagation, we propagate the base correspondences from every 20 frames to every 40 frames and merge the propagated correspondences with the base ones. Now we have the merged correspondences for every 40 frames, we perform the same operation to generate the merged correspondences for every 80 frames. Since we have no base correspondence beyond 80 frames, the remaining propagation rounds do not perform the merging operation and keep doubling the frame interval until we do not have more than 1024 correspondences . The reason we enforce the minimum number of correspondences is to balance the difficulty of the learning problem, so that the model is not biased towards hard or easy samples. Though the standard approach of uniform sampling from different overlapping ratios (Sun et al., 2021) can also be applied, we find it more space and computation friendly to simply limit the number of correspondences and save the most distant image pairs as the final training data.\nStrong data augmentation: To experiment with various existing architectures, we apply the same loss used for domain-specific training to train the final GIM model, but only calculate the loss on the pixels with correspondences. Empirically, we find that strong data augmentations on video data provide better supervision signals (see Sec. 4.2 for the effect). Specifically, for each pair of video frames, we perform random perspective transformations beyond standard augmentations used in existing methods. We conjecture that applying perspective transformation alleviates the problem where the camera model of two video frames is the same and the cameras are mostly positioned front-facing without too much \u201croll\u201d rotation.\nIn practice, the major computation for generating video training data lies in running matching methods, and the average processing time per frame does not increase significantly w.r.t. the input video length. The efficiency and generality allows GIM to effectively scale up training on internet videos. It can process 12.5 hours of videos per day using 16 A100 GPUs, achieving a non-trivial performance boost for various state-of-the-art architectures."
        },
        {
            "heading": "3.2 ZEB: ZERO-SHOT EVALUATION BENCHMARK FOR IMAGE MATCHING",
            "text": "Existing image matching frameworks (Sarlin et al., 2020; Sun et al., 2021; Edstedt et al., 2023) typically train and evaluate models on the same in-domain dataset (MegaDepth (Li & Snavely, 2018) for outdoor models and ScanNet (Dai et al., 2017) for indoor models). To analyze the robustness of individual models on in-the-wild data, we construct a new evaluation benchmark ZEB by merging 8 real-world datasets and 4 simulated datasets with diverse image resolutions, scene conditions and view points (see Appendix B for details).\nFor each dataset, we sample approximately 3800 evaluation image pairs uniformly from 5 image overlap ratios (from 10% to 50%). These ratios are computed using ground truth poses and depth maps. The final ZEB benchmark thus contains 46K evaluation image pairs from various scenes and overlap ratios, which has a much larger diversity and scale comparing to the 1500 in-domain image pairs used in existing methods.\nMetrics: Following the standard evaluation protocol (Edstedt et al., 2023), we report the AUC of the relative pose error within 5\u00b0, where the pose error is the maximum between the rotation angular error and translation angular error. The relative poses are obtained by estimating the essential matrix using the output correspondences from an image matching method and RANSAC (Fischler & Bolles, 1981). Following the zero-shot computer vision literature (Ranftl et al., 2020; Yin et al., 2023), we also provide the average performance ranking across the twelve cross-domain datasets."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": "We first demonstrate in Sec. 4.1 the effectiveness of GIM on the basic image matching task \u2014 relative pose estimation. We evaluate different methods on both our zero-shot benchmark ZEB and the standard in-domain benchmarks (Sarlin et al., 2020). In Sec. 4.2, we validate our design choices with ablation studies. Finally, we apply the trained image matching models to various\ndownstream tasks (Sec. 4.3). To demonstrate the generality of GIM, we apply it to 3 state-of-the-art image matching architectures with varied output density, namely, SuperGlue (Sarlin et al., 2020), LoFTR (Sun et al., 2021) and DKM (Edstedt et al., 2023).\nImplementation details: We take the official indoor and outdoor models of each architecture as the baselines. Note that the indoor model of DKM is trained on both indoor and outdoor data. For fair comparisons, we only allow the use of complementary methods that perform worse than the baseline during multi-method matching. Specifically, we use RootSIFT, RootSIFT+SuperGlue and RootSIFT+SuperGlue+LoFTR respectively to complement SuperGlue, LoFTR and DKM. We use the outdoor official model of each architecture as the base label generator in GIM. Unless otherwise stated, we use 50 hours of YouTube videos in all experiments, which provide roughly 180K pairs of training images. The GIM label generation on our videos takes 4 days on 16 A100 GPUs. To achieve the best in-domain and cross-domain performance with a single model, we train all GIM models from scratch using a mixture of original in-domain data and our video data (sampled with equal probabilities). The training code and hyper-parameters of GIM strictly follow the original repositories of the individual architectures."
        },
        {
            "heading": "4.1 MAIN RESULTS",
            "text": "Zero-shot generalization: We use the proposed ZEB benchmark to evaluate the zero-shot generalization performance. For all three architectures (Tab. 1), applying GIM produces a single zero-shot model with a significantly better performance compared to the best in-domain baseline. Specifically, the AUC improvement for SuperGlue, LoFTR and DKM is respectively 31.2\u2192 34.3, 33.1\u2192 39.1 and 46.2 \u2192 50.1. GIMSuperGlue performs even better than LoFTR (IN)/(OUT), despite using a less advanced architecture. Interestingly, the hand-crafted method RootSIFT (Arandjelovic\u0301 & Zisserman, 2012) performs better or on-par with the in-domain models on non-trivial number of ZEB subsets, e.g., GL3, BLE, KIT and GTA. GIM successfully improved the performance on these subsets, resulting in a significantly better robustness across the board. Note that the performance of GIM did not saturate yet (Fig. 1), and further improvements can be achieved by simply downloading more internet videos.\nTwo-view geometry: Qualitatively, GIM also provides much better two-view matching/reconstruction on challenging data. As shown in Fig. 3, the best in-domain baseline DKM (IN) failed to find correct matches on data with large view changes or small overlaps (both indoor and outdoor), resulting in erroneous reconstructed point clouds. Instead, GIMDKM finds a large number of reliable correspondences and manages to reconstruct dense and accurate 3D point clouds. Interestingly, the robustness of GIM also allows it to be applied to inputs completely unseen during training. In Fig. 4, we apply GIMDKM to Bird Eye View (BEV) images generated by projecting the top-down view of two point clouds into 2D RGB images. The data comes from a real mapping\napplication where we want to align the point clouds of different building levels in the same horizontal plane. Unlike the best baseline DKM (IN) that fails catastrophically, our model GIMDKM successfully registers all three pairs of point clouds even though BEV images of point clouds were never seen during training. Due to the space limit, we show the qualitative results for the other architectures in Appendix D.\nMulti-view Reconstruction: GIM also performs well for multi-view reconstruction. To demonstrate the performance on in-the-wild data, we download internet videos for both indoor and outdoor scenes, extract roughly 200 frames for each video, and run COLMAP (Scho\u0308nberger & Frahm, 2016) reconstruction but replace the SIFT matches with the ones from our experimented models. As shown\nTable 2: Ablation study.\nModels AUC@5\u00b0(%)\nGIMDKM 50.1\n1) replace 50h video to 25h 48.3 2) replace 50h video to 12.5h 47.4 3) only use RootSIFT to generate labels 49.3 4) w/o data augmentation 49.2 5) w/o propagation 47.1 6) use COLMAP labels, same computation and time as 2) 46.5 7) w/o video 46.2\nTable 3: Homography estimation.\nMethod AUC (%)\u2192 @3px @5px @10px SUPERGLUE (OUT) 53.9 68.3 81.7 GIMSuperGlue 54.4 68.8 82.3 LOFTR (OUT) 65.9 75.6 84.6 GIMLoFTR 70.6 79.8 88.0 DKM (OUT) 71.3 80.6 88.5 GIMDKM 71.5 80.9 89.1\nin Fig. 5, applying GIM allows DKM to reconstruct a much larger portion of the captured scene with denser and less noisy point clouds.\nIn-domain performance: We also evaluate different methods on the standard in-domain datasets. Due to the space limit, we report the result in Appendix C. Though the improvement is not as significant as in ZEB because individual baselines overfit well to the in-domain data, GIM still performs the best on average (over the indoor and outdoor scenes). This result also shows the importance of ZEB for measuring the generalization capability more accurately."
        },
        {
            "heading": "4.2 ABLATION STUDY",
            "text": "To analyze the effect of different GIM components, we perform an ablation study on our bestperforming model GIMDKM. As shown in row 1, 2 and 7 of Tab. 2, the performance of GIM consistently decreases with the reduction of the video data size. Meanwhile, adding only a small amount (12.5h) of videos already provides a reasonable improvement compared to the baseline (46.2% to 47.4%). This shows the importance of generating supervision signals on diverse videos. Using only RootSIFT Arandjelovic\u0301 & Zisserman (2012) to generate video labels, the performance of GIM reduces slightly. Comparing the performance between rows 3 and 1, we can see that generating labels on more diverse images is more important than having advanced base label generators. Removing label propagation reduces the performance more than lack of data augmentations and base label generation methods. Specifically, using 50 hours of videos without label propagation performs even worse than using the full GIM method on only 12.5 hours of videos.\nWe also experiment with the standard COLMAP-based label generation pipeline (Li & Snavely, 2018) (row 6). Specifically, we separate the downloaded videos into clips of 4000 frames and uniformly sample 200 frames for label generation. We apply the same GPU and time (roughly 1 day) as row 2 to run COLMAP SfM+MVS. COLMAP only manages to process 3.9 hours of videos, and fails to reconstruct 44.3% of them, resulting in only 2.2 hours of labeled videos (vs. 12.5 hours from GIM), and a low performance improvement of 46.2% to 46.5%."
        },
        {
            "heading": "4.3 APPLICATIONS",
            "text": "Homography estimation: As a classical down-stream application (Edstedt et al., 2023), we conduct experiments on homography estimation. We use the widely adopted HPatches dataset, which contains 52 outdoor sequences under significant illumination changes and 56 sequences that exhibit large variation in viewpoints. Following previous methods Dusmanu et al. (2019), we use OpenCV to compute the homography matrix with RANSAC after the matching procedure. Then, we compute the mean reprojection error of the four corners between the images warped with the estimated and the ground-truth homography as a correctness identifier. Finally, we report the area under the cumulative curve (AUC) of the corner error up to 3, 5, and 10 pixels. We take the numbers from the original paper for each baseline.\nAs illustrated in Tab. 3, the GIM models consistently outperform the baselines, even though the baselines are trained for outdoor scenes already. Among all architectures, GIM achieves the most pronounced improvement on LoFTR, achieving an absolute performance increase of 4.7%, 4.2%, and 3.4% in the three metrics.\nVisual localization: Visual localization is another important down-stream task of image matching. The goal is to estimate the 6-DoF poses of an image with respect to a 3D scene model. Following standard approaches (Sun et al., 2021), we evaluate matching models on two tracks of the LongTerm Visual Localization benchmark, namely, the Aachen-Day-Night v1.1 dataset (Sattler et al., 2018) for outdoor scenes and the InLoc dataset (Taira et al., 2018) for indoor scenes. We use the standard localization pipeline HLoc (Balntas et al., 2017) with the matches extracted by corresponding models to perform visual localization. We take the numbers from the original paper for each baseline. Since DKM did not report the result on the outdoor case, we use the outdoor baseline to obtain the performance number.\nWith a single model, GIM consistently and significantly out-performs the domain-specific baselines for both indoor (Tab. 5) and outdoor (Tab. 4) scenes. For example, we improve the absolute pose accuracy of DKM by > 5% for the (0.25m, 2\u00b0) metric in both indoor and outdoor datasets. For indoor scenarios, GIMDKM reaches a remarkable performance of 57.1 / 78.8 / 88.4 on DUC1 and 70.2 / 91.6 / 92.4 on DUC2. These results show that without the need of domain-specific training, a single GIM model can be effectively deployed to different environments."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "We have introduced a novel approach GIM, that leverages abundant internet videos to learn generalizable image matching. The key idea is to perform self-training, where we use the enhanced output of domain-specific models to train the same architecture, and improve generalization by consuming a large amount of diverse videos. We have also constructed a novel zero-shot benchmark ZEB that allows thorough evaluation of an image matching model in in-the-wild environments. We have successfully applied GIM to 3 state-of-the-art architectures. The performance improvement increases steadily with the video data size. The improved image matching performance also benefits various downstream tasks such as visual localization and 3D reconstruction. A single GIM model generalizes to applications from different domains."
        },
        {
            "heading": "A DETAILS OF VIDEO DATA",
            "text": "In this section, we show details of our video data. Tab. 6 shows the diverse geo-locations and scene types of our downloaded videos. Fig. 6 provides example training data (images and correspon-\ndences) generated on video data, which covers both indoor and outdoor scenes, urban and natural environments, various illumination conditions."
        },
        {
            "heading": "B DETAILS OF ZEB",
            "text": "This section shows the details of the proposed ZEB benchmark. Specifically, Tab. 7 shows the 12 datasets used to construct ZEB, and the diverse scene conditions and images resolution covered by these datasets. We also show in Fig. 7 sampled image pairs in ZEB, covering varied scene types, view points and lightning conditions.\nC IN-DOMAIN EVALUATION RESULT\nAs mentioned in Sec. 4.1, we also compared GIM with baselines on standard in-domain evaluation data, i.e., MegaDepth-1500 (Edstedt et al., 2023) and ScanNet-1500 (Edstedt et al., 2023). The\nevaluation metric follows existing methods (Edstedt et al., 2023; Sun et al., 2021), and we take the numbers from the paper for each in-domain baseline. As shown in Tab. 8, though in-domain baselines already overfitted well on their trained domains, GIM still achieved the best average performance over indoor and outdoor scenes. The smaller performance gap comparing to the zero-shot scenario also shows the importance of the proposed ZEB benchmark, which can clearly reflect the generalization performance."
        },
        {
            "heading": "D FURTHER QUALITATIVE RESULTS",
            "text": "In Sec. 4.1, we only have space to show baseline results for the best architecture DKM. Here we provide the ones also for LoFTR and SuperGlue. Fig. 8 shows the two-view reconstruction results on in-the-wild images. Similar to DKM, the in-domain LoFTR and SuperGlue models also generalizes poorly on challenging in-the-wild data. Fig. 9 shows the results on BEV point cloud registration.\nThe in-domain LoFTR and SuperGlue models failed to find reliable matches and the correct relative transformations between two point clouds."
        }
    ],
    "title": "GIM: LEARNING GENERALIZABLE IMAGE MATCHER",
    "year": 2023
}