{
    "abstractText": "Hierarchical world models can significantly improve model-based reinforcement learning (MBRL) and planning by enabling reasoning across multiple time scales. Nonetheless, the majority of state-of-the-art MBRL methods still employ flat, nonhierarchical models. We propose Temporal Hierarchies from Invariant Context Kernels (THICK), an algorithm that learns a world model hierarchy via discrete latent dynamics. The lower level of THICK updates parts of its latent state sparsely in time, forming invariant contexts. The higher level exclusively predicts situations involving context state changes. Our experiments demonstrate that THICK learns categorical, interpretable, temporal abstractions on the high level, while maintaining precise low-level predictions. Furthermore, we show that the emergent hierarchical predictive model seamlessly enhances the abilities of MBRL or planning methods. We believe that THICK contributes to the further development of hierarchical, context-conditioned, event-predictive world models that can enhance planning and reasoning abilities and produce more human-like behavior.",
    "authors": [],
    "id": "SP:710727ec3f4a356e9db5a5f91aaecf71b6ff8369",
    "references": [
        {
            "authors": [
                "Ahmed Akakzia",
                "C\u00e9dric Colas",
                "Pierre-Yves Oudeyer",
                "Mohamed Chetouani",
                "Olivier Sigaud"
            ],
            "title": "Grounding language to autonomously-acquired skills via goal generation",
            "venue": "ICLR 2021 - Ninth International Conference on Learning Representation,",
            "year": 2021
        },
        {
            "authors": [
                "Marcin Andrychowicz",
                "Filip Wolski",
                "Alex Ray",
                "Jonas Schneider",
                "Rachel Fong",
                "Peter Welinder",
                "Bob McGrew",
                "Josh Tobin",
                "Pieter Abbeel",
                "Wojciech Zaremba"
            ],
            "title": "Hindsight experience replay",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Pierre-Luc Bacon",
                "Jean Harb",
                "Doina Precup"
            ],
            "title": "The option-critic architecture",
            "venue": "In Proceedings of the AAAI conference on artificial intelligence,",
            "year": 2017
        },
        {
            "authors": [
                "Yoshua Bengio",
                "Nicholas L\u00e9onard",
                "Aaron Courville"
            ],
            "title": "Estimating or propagating gradients through stochastic neurons for conditional computation",
            "venue": "arXiv preprint arXiv:1308.3432,",
            "year": 2013
        },
        {
            "authors": [
                "Matthew Botvinick",
                "Ari Weinstein"
            ],
            "title": "Model-based hierarchical reinforcement learning and human action control",
            "venue": "Philosophical Transactions of the Royal Society B: Biological Sciences,",
            "year": 2013
        },
        {
            "authors": [
                "Martin V Butz"
            ],
            "title": "Toward a unified sub-symbolic computational theory of cognition",
            "venue": "Frontiers in psychology,",
            "year": 2016
        },
        {
            "authors": [
                "Chang Chen",
                "Yi-Fu Wu",
                "Jaesik Yoon",
                "Sungjin Ahn"
            ],
            "title": "Transdreamer: Reinforcement learning with transformer world models",
            "venue": "arXiv preprint arXiv:2202.09481,",
            "year": 2022
        },
        {
            "authors": [
                "Junyoung Chung",
                "Caglar Gulcehre",
                "KyungHyun Cho",
                "Yoshua Bengio"
            ],
            "title": "Empirical evaluation of gated recurrent neural networks on sequence modeling",
            "venue": "arXiv preprint arXiv:1412.3555,",
            "year": 2014
        },
        {
            "authors": [
                "Fei Deng",
                "Junyeong Park",
                "Sungjin Ahn"
            ],
            "title": "Facing off world model backbones: Rnns, transformers, and s4",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2023
        },
        {
            "authors": [
                "Manfred Eppe",
                "Christian Gumbsch",
                "Matthias Kerzel",
                "Phuong DH Nguyen",
                "Martin V Butz",
                "Stefan Wermter"
            ],
            "title": "Intelligent problem-solving as integrated hierarchical reinforcement learning",
            "venue": "Nature Machine Intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "Karl Friston",
                "Rosalyn J Moran",
                "Yukie Nagai",
                "Tadahiro Taniguchi",
                "Hiroaki Gomi",
                "Josh Tenenbaum"
            ],
            "title": "World model learning and inference",
            "venue": "Neural Networks,",
            "year": 2021
        },
        {
            "authors": [
                "Karl J Friston",
                "Richard Rosch",
                "Thomas Parr",
                "Cathy Price",
                "Howard Bowman"
            ],
            "title": "Deep temporal models and active inference",
            "venue": "Neuroscience & Biobehavioral Reviews,",
            "year": 2018
        },
        {
            "authors": [
                "Anirudh Goyal",
                "Alex Lamb",
                "Jordan Hoffmann",
                "Shagun Sodhani",
                "Sergey Levine",
                "Yoshua Bengio",
                "Bernhard Sch\u00f6lkopf"
            ],
            "title": "Recurrent independent mechanisms",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Alex Graves",
                "Greg Wayne",
                "Ivo Danihelka"
            ],
            "title": "Neural turing machines",
            "venue": "arXiv preprint arXiv:1410.5401,",
            "year": 2014
        },
        {
            "authors": [
                "Christian Gumbsch",
                "Martin V. Butz",
                "Georg Martius"
            ],
            "title": "Autonomous identification and goaldirected invocation of event-predictive behavioral primitives",
            "venue": "IEEE Transactions on Cognitive and Developmental Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Christian Gumbsch",
                "Martin V Butz",
                "Georg Martius"
            ],
            "title": "Sparsely changing latent states for prediction and planning in partially observable domains",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Christian Gumbsch",
                "Maurits Adam",
                "Birgit Elsner",
                "Georg Martius",
                "Martin V Butz"
            ],
            "title": "Developing hierarchical anticipations via neural network-based event segmentation",
            "venue": "IEEE International Conference on Development and Learning (ICDL),",
            "year": 2022
        },
        {
            "authors": [
                "Nico G\u00fcrtler",
                "Dieter B\u00fcchler",
                "Georg Martius"
            ],
            "title": "Hierarchical reinforcement learning with timed subgoals",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Danijar Hafner",
                "Timothy Lillicrap",
                "Jimmy Ba",
                "Mohammad Norouzi"
            ],
            "title": "Dream to control: Learning behaviors by latent imagination",
            "venue": "In International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "Danijar Hafner",
                "Timothy Lillicrap",
                "Ian Fischer",
                "Ruben Villegas",
                "David Ha",
                "Honglak Lee",
                "James Davidson"
            ],
            "title": "Learning latent dynamics for planning from pixels",
            "venue": "In International conference on machine learning,",
            "year": 2019
        },
        {
            "authors": [
                "Danijar Hafner",
                "Timothy P Lillicrap",
                "Mohammad Norouzi",
                "Jimmy Ba"
            ],
            "title": "Mastering atari with discrete world models",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Danijar Hafner",
                "Kuang-Huei Lee",
                "Ian Fischer",
                "Pieter Abbeel"
            ],
            "title": "Deep hierarchical planning from pixels",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Danijar Hafner",
                "Jurgis Pasukonis",
                "Jimmy Ba",
                "Timothy Lillicrap"
            ],
            "title": "Mastering diverse domains through world models, 2023",
            "year": 2023
        },
        {
            "authors": [
                "James B. Heald",
                "M\u00e1t\u00e9 Lengyel",
                "Daniel M. Wolpert"
            ],
            "title": "Contextual inference underlies the learning of sensorimotor repertoires",
            "venue": "Nature, 600:489\u2013493,",
            "year": 2021
        },
        {
            "authors": [
                "James B. Heald",
                "M\u00e1t\u00e9 Lengyel",
                "Daniel M. Wolpert"
            ],
            "title": "Contextual inference in learning and memory",
            "venue": "Trends in Cognitive Sciences,",
            "year": 2023
        },
        {
            "authors": [
                "Arnav Kumar Jain",
                "Shivakanth Sujit",
                "Shruti Joshi",
                "Vincent Michalski",
                "Danijar Hafner",
                "Samira Ebrahimi Kahou"
            ],
            "title": "Learning robust dynamics through variational sparse gating",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Dinesh Jayaraman",
                "Frederik Ebert",
                "Alexei Efros",
                "Sergey Levine"
            ],
            "title": "Time-agnostic prediction: Predicting predictable video frames",
            "venue": "In International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "Taesup Kim",
                "Sungjin Ahn",
                "Yoshua Bengio"
            ],
            "title": "Variational temporal abstraction",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Jan Koutnik",
                "Klaus Greff",
                "Faustino Gomez",
                "Juergen Schmidhuber"
            ],
            "title": "A Clockwork RNN",
            "venue": "International Conference on Machine Learning, Proceedings of Machine Learning Research,",
            "year": 2014
        },
        {
            "authors": [
                "Heinrich K\u00fcttler",
                "Nantas Nardelli",
                "Alexander Miller",
                "Roberta Raileanu",
                "Marco Selvatici",
                "Edward Grefenstette",
                "Tim Rockt\u00e4schel"
            ],
            "title": "The nethack learning environment",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Brenden M Lake",
                "Tomer D Ullman",
                "Joshua B Tenenbaum",
                "Samuel J Gershman"
            ],
            "title": "Building machines that learn and think like people",
            "venue": "Behavioral and brain sciences,",
            "year": 2017
        },
        {
            "authors": [
                "Yann LeCun"
            ],
            "title": "A path towards autonomous machine intelligence version 0.9",
            "venue": "Open Review,",
            "year": 2022
        },
        {
            "authors": [
                "Tai Sing Lee",
                "David Mumford"
            ],
            "title": "Hierarchical bayesian inference in the visual cortex",
            "venue": "JOSA A,",
            "year": 2003
        },
        {
            "authors": [
                "Andrew Levy",
                "George Konidaris",
                "Robert Platt",
                "Kate Saenko"
            ],
            "title": "Learning multi-level hierarchies with hindsight",
            "venue": "In Proceedings of International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "Marcelo G. Mattar",
                "M\u00e1t\u00e9 Lengyel"
            ],
            "title": "Planning in the brain",
            "venue": "Neuron, 110(6):914\u2013934,",
            "year": 2022
        },
        {
            "authors": [
                "Russell Mendonca",
                "Oleh Rybkin",
                "Kostas Daniilidis",
                "Danijar Hafner",
                "Deepak Pathak"
            ],
            "title": "Discovering and achieving goals via world models",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Vincent Micheli",
                "Eloi Alonso",
                "Fran\u00e7ois Fleuret"
            ],
            "title": "Transformers are sample-efficient world models",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Ofir Nachum",
                "Shixiang Shane Gu",
                "Honglak Lee",
                "Sergey Levine"
            ],
            "title": "Data-efficient hierarchical reinforcement learning",
            "venue": "Advances in neural information processing systems,",
            "year": 2018
        },
        {
            "authors": [
                "Ashvin V Nair",
                "Vitchyr Pong",
                "Murtaza Dalal",
                "Shikhar Bahl",
                "Steven Lin",
                "Sergey Levine"
            ],
            "title": "Visual reinforcement learning with imagined goals",
            "venue": "Advances in neural information processing systems,",
            "year": 2018
        },
        {
            "authors": [
                "Daniel Neil",
                "Michael Pfeiffer",
                "Shih-Chii Liu"
            ],
            "title": "Phased lstm: Accelerating recurrent network training for long or event-based sequences",
            "venue": "In Advances In Neural Information Processing Systems,",
            "year": 2016
        },
        {
            "authors": [
                "Alexander Neitz",
                "Giambattista Parascandolo",
                "Stefan Bauer",
                "Bernhard Sch\u00f6lkopf"
            ],
            "title": "Adaptive skip intervals: Temporal abstraction for recurrent dynamical models",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Shubham Pateria",
                "Budhitama Subagdja",
                "Ah-hwee Tan",
                "Chai Quek"
            ],
            "title": "Hierarchical reinforcement learning: A comprehensive survey",
            "venue": "ACM Computing Surveys (CSUR),",
            "year": 2021
        },
        {
            "authors": [
                "Vihang Prakash Patil",
                "Markus Hofmarcher",
                "Marius-Constantin Dinu",
                "Matthias Dorfer",
                "Patrick M Blies",
                "Johannes Brandstetter",
                "Jose Arjona-Medina",
                "Sepp Hochreiter"
            ],
            "title": "Align-{rudder}: Learning from few demonstrations by reward redistribution",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Vitchyr Pong",
                "Murtaza Dalal",
                "Steven Lin",
                "Ashvin V Nair"
            ],
            "title": "Multiworld: Multitask environments for rl, 2018",
            "venue": "URL https://github.com/vitchyr/multiworld",
            "year": 2018
        },
        {
            "authors": [
                "Vitchyr H Pong",
                "Murtaza Dalal",
                "Steven Lin",
                "Ashvin Nair",
                "Shikhar Bahl",
                "Sergey Levine"
            ],
            "title": "Skewfit: state-covering self-supervised reinforcement learning",
            "venue": "In Proceedings of the 37th International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Doina Precup"
            ],
            "title": "Temporal abstraction in reinforcement learning",
            "venue": "University of Massachusetts Amherst,",
            "year": 2000
        },
        {
            "authors": [
                "Gabriel A Radvansky",
                "Jeffrey M Zacks"
            ],
            "title": "Event cognition",
            "year": 2014
        },
        {
            "authors": [
                "Jan Robine",
                "Marc H\u00f6ftmann",
                "Tobias Uelwer",
                "Stefan Harmeling"
            ],
            "title": "Transformer-based world models are happy with 100k interactions",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Tim Rohe",
                "Uta Noppeney"
            ],
            "title": "Cortical hierarchies perform bayesian causal inference in multisensory perception",
            "venue": "PLoS biology,",
            "year": 2015
        },
        {
            "authors": [
                "Nicolas P Rougier",
                "David C Noelle",
                "Todd S Braver",
                "Jonathan D Cohen",
                "Randall C O\u2019Reilly"
            ],
            "title": "Prefrontal cortex and flexible cognitive control: Rules without symbols",
            "venue": "Proceedings of the National Academy of Sciences,",
            "year": 2005
        },
        {
            "authors": [
                "Reuven Rubinstein"
            ],
            "title": "The cross-entropy method for combinatorial and continuous optimization",
            "venue": "Methodology and computing in applied probability,",
            "year": 1999
        },
        {
            "authors": [
                "Noor Sajid",
                "Panagiotis Tigas",
                "Alexey Zakharov",
                "Zafeirios Fountas",
                "Karl Friston"
            ],
            "title": "Exploration and preference satisfaction trade-off in reward-free learning",
            "venue": "In ICML 2021 Workshop on Unsupervised Reinforcement Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Mikayel Samvelyan",
                "Robert Kirk",
                "Vitaly Kurin",
                "Jack Parker-Holder",
                "Minqi Jiang",
                "Eric Hambro",
                "Fabio Petroni",
                "Heinrich Kuttler",
                "Edward Grefenstette",
                "Tim Rockt\u00e4schel"
            ],
            "title": "Minihack the planet: A sandbox for open-ended reinforcement learning research",
            "venue": "In Neural Information Processing Systems Datasets and Benchmarks Track,",
            "year": 2021
        },
        {
            "authors": [
                "Cansu Sancaktar",
                "Sebastian Blaes",
                "Georg Martius"
            ],
            "title": "Curious exploration via structured world models yields zero-shot object manipulation",
            "venue": "In Advances in Neural Information Processing Systems. Curran Associates,",
            "year": 2022
        },
        {
            "authors": [
                "Vaibhav Saxena",
                "Jimmy Ba",
                "Danijar Hafner"
            ],
            "title": "Clockwork variational autoencoders",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Thomas M Scanlon"
            ],
            "title": "Thickness and theory",
            "venue": "The Journal of Philosophy,",
            "year": 2003
        },
        {
            "authors": [
                "J\u00fcrgen Schmidhuber"
            ],
            "title": "Learning complex, extended sequences using the principle of history compression",
            "venue": "Neural Computation,",
            "year": 1992
        },
        {
            "authors": [
                "Julian Schrittwieser",
                "Ioannis Antonoglou",
                "Thomas Hubert",
                "Karen Simonyan",
                "Laurent Sifre",
                "Simon Schmitt",
                "Arthur Guez",
                "Edward Lockhart",
                "Demis Hassabis",
                "Thore Graepel"
            ],
            "title": "Mastering atari, go, chess and shogi by planning with a learned model",
            "year": 2020
        },
        {
            "authors": [
                "Ramanan Sekar",
                "Oleh Rybkin",
                "Kostas Daniilidis",
                "Pieter Abbeel",
                "Danijar Hafner",
                "Deepak Pathak"
            ],
            "title": "Planning to explore via self-supervised world models",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Richard S Sutton"
            ],
            "title": "Learning to predict by the methods of temporal differences",
            "venue": "Machine learning,",
            "year": 1988
        },
        {
            "authors": [
                "Richard S Sutton",
                "Andrew G Barto"
            ],
            "title": "Reinforcement learning: An introduction",
            "venue": "MIT press,",
            "year": 2018
        },
        {
            "authors": [
                "Richard S Sutton",
                "Doina Precup",
                "Satinder Singh"
            ],
            "title": "Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning",
            "venue": "Artificial intelligence,",
            "year": 1999
        },
        {
            "authors": [
                "Momchil S Tomov",
                "Samyukta Yagati",
                "Agni Kumar",
                "Wanqian Yang",
                "Samuel J Gershman"
            ],
            "title": "Discovery of hierarchical representations for efficient planning",
            "venue": "PLoS computational biology,",
            "year": 2020
        },
        {
            "authors": [
                "Harm van Seijen",
                "Shimon Whiteson",
                "Leon Kester"
            ],
            "title": "Efficient abstraction selection in reinforcement learning",
            "venue": "Computational Intelligence,",
            "year": 2014
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141 ukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Alexander Sasha Vezhnevets",
                "Simon Osindero",
                "Tom Schaul",
                "Nicolas Heess",
                "Max Jaderberg",
                "David Silver",
                "Koray Kavukcuoglu"
            ],
            "title": "Feudal networks for hierarchical reinforcement learning",
            "venue": "In Proceedings of the 34th International Conference on Machine Learning-Volume",
            "year": 2017
        },
        {
            "authors": [
                "Marin Vlastelica",
                "Sebastian Blaes",
                "Cristina Pinneri",
                "Georg Martius"
            ],
            "title": "Risk-averse zero-order trajectory optimization",
            "venue": "In 5th Annual Conference on Robot Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Hafner"
            ],
            "title": "2022), however we made some minor adjustments to the training frequency",
            "year": 2022
        },
        {
            "authors": [
                "Kim"
            ],
            "title": "agent-environment interactions change, e.g. changing the terrain during locomotion, which lead to a temporary increase in the prediction error. Temporal abstractions from learning boundary detectors: Besides using indirect measure to segment a sequence, a straight-forward approach is to train a boundary detector that signals the boundary of subsequences",
            "venue": "(Kim et al.,",
            "year": 2019
        },
        {
            "authors": [
                "Hafner"
            ],
            "title": "p\u03c6 and q\u03c6",
            "year": 2021
        },
        {
            "authors": [
                "Hafner"
            ],
            "title": "Dreamer is also not able to discover the very sparse rewards of VisualPinPadSix on its own. Thus, we omitted VisualPinPadSix and more complicated levels. F.5 MBRL IN VISUALPINPAD: EFFECT OF EXPLORATION We analyze the effect of exploration data",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "The intricate hierarchical representations formed in our brains through sensorimotor experience (Lee & Mumford, 2003; Rougier et al., 2005; Botvinick & Weinstein, 2014; Rohe & Noppeney, 2015; Lake et al., 2017; Friston et al., 2018; 2021; Radvansky & Zacks, 2014; Butz, 2016; Tomov et al., 2020) serve as a useful blueprint for enhancing artificial agents\u2019 planning abilities via hierarchical world models (Schmidhuber, 1992; LeCun, 2022). Humans, for example, can plan their behavior on various time scales and flexibly switch between them, such as picking up a pen for writing an invitation when organizing a party. The integration of hierarchical models into MBRL could bolster planning efficiency and effectiveness in complex, real-world tasks.\nDespite recent advancements in equipping MBRL agents with the capacity to learn world models, i.e., generative forward models encoding an agent\u2019s interaction with its environment (Ha & Schmidhuber, 2018; Hafner et al., 2019b;a; 2020; 2023), these models lack a hierarchical structure. Consequently, they are restricted to predictions on predefined time scales, hampering their capability for long-horizon planning. The main challenge lies in formalizing suitable methods to learn higherlevel abstractions (Sutton, 1988; Sutton et al., 1999; Eppe et al., 2022; Precup, 2000; van Seijen\net al., 2014). Importantly, these abstractions should be tied neither to particular tasks nor to fixed nested time scales. Context-conditioned, event-predictive structures offer themselves as temporally flexible, basic compositional unit (Butz, 2016; Heald et al., 2021; 2023).\nWe present a deep learning architecture that learns hierarchical world models, which we call Temporal Hierarchies from Invariant Context Kernels (THICK1). THICK adaptively discovers higherlevel time scales by guiding the lower-level world model to update parts of its latent state only sparsely in time. The high-level model is then trained to predict scenarios involving changes in these low-level latent states. A depiction of THICK world models can be found in Fig. 1a.\nWe make the following key contributions: \u2022 We introduce the Context-specific Recurrent State Space Model (C-RSSM), which enhances\nDreamer\u2019s (Hafner et al., 2019b; 2020) Recurrent State Space Model (RSSM) by encoding context-sensitive dynamics via sparsely changing latent factors, labeled context.\n\u2022 We introduce THICK, which learns a hierarchy of world models. The high-level runs at an adaptive time scale developing higher-level actions that anticipate lower-level context changes.\n\u2022 We demonstrate the effectiveness of THICK in two planning scenarios: i) using THICK\u2019s hierarchical predictions to enhance MBRL in long-horizon tasks, and ii) using THICK\u2019s high-level predictions to set subgoals for hierarchical model-predictive planning (MPC) ."
        },
        {
            "heading": "2 METHOD",
            "text": ""
        },
        {
            "heading": "2.1 C-RSSM WORLD MODEL",
            "text": "The RSSM proposed in Hafner et al. (2019b) is a recurrent neural network (RNN) that is used for model-based reinforcement learning (Hafner et al., 2019a; 2020; 2023; Sekar et al., 2020; Mendonca et al., 2021; Sajid et al., 2021). RSSM embeds input images it and actions at into a latent state st and predicts dynamics exclusively within this state. All aspects of the latent state evolve continuously. We require sparse latent state changes to establish a hierarchical world models. Accordingly, our Context-specific RSSM (C-RSSM) integrates a sparsely changing latent state ct as context with a coarse prediction pathway (cf. Fig. 2). Our C-RSSM with trainable parameters \u03d5 is computed by:\nLatent state: st \u2190 [ct,ht, zt] (1) Coarse Dyn.: ct = g\u03d5(at\u22121, ct\u22121, zt\u22121) (2)\nPre. Dyn.: ht = f\u03d5(at\u22121, ct,ht\u22121, zt\u22121) (3)\nPre. Prior: z\u0302ht \u223c ph\u03d5 ( z\u0302ht | ct,ht ) (4)\nCoa. Prior: z\u0302ct \u223c pc\u03d5(z\u0302ct | at\u22121, ct, zt\u22121) (5) Posterior: zt \u223c q\u03d5(zt | ct,ht, it) (6)\nEquations in red are exclusive to C-RSSM.2 We separate RSSM\u2019s latent state st into three parts (Eq. 1): a stochastic state zt, a continuously updated, high-dimensional, deterministic state ht, and a sparsely changing, low-dimensional context ct. At time t the C-RSSM first updates the context ct (Eq. 2), where actual ct changes only occur sparsely in time. Next, C-RSSM updates ht via a GRU (Chung et al., 2014) cell f\u03d5 (Eq. 3). The C-RSSM makes two predictions about the next stochastic state z\u0302ht : i) a precise prior based on both ht and ct (Eq. 4), and ii) a coarse prior z\u0302 c t based on the context, stochastic state, and action, ignoring ht (Eq. 5). Given the input image it, C-RSSM updates its posterior zt (Eq. 6). Following DreamerV2 (Hafner et al., 2020), we sample zt from a vector of categorical distributions. Note that Eq. 2 and Eq. 5 do not depend on ht\u22121, thus creating a coarse processing pathway independent of h. This enables predictions using only ct as a deterministic memory, which is crucial because it i) encourages encoding prediction-relevant information in ct and ii) allows predictions without ht which we will use later (details in Suppl. D.1).\nBesides encoding latent dynamics, C-RSSM is trained to reconstruct observable variables yt of the outside world from its latent states st . Two output heads o\u03d5 generate precise and coarse predictions:\nPrecise prediction: y\u0302t \u223c o\u03d5(y\u0302t | st) (7) Coarse prediction: y\u0302ct \u223c oc\u03d5(y\u0302ct | ct, zt). (8) We predict the input image it, the reward rt, and reward discount \u03b3t3, i.e., yt \u2208 {it, rt, \u03b3t}.\n1In philosophy, \u201cthickness\u201d refers to descriptions that fuse facts with contextual elements, while \u201cthinness\u201d denotes factual, neutral descriptions (Scanlon, 2003). When adapted to world models, a THICK model presents an accurate representation of the world with contextual interpretation.\n2Removing c in all black equations recovers the equations for the RSSM (Eqns. 1,3,4,6). 3The discount \u03b3t is set to 0 if an episode terminates and to a fixed value \u03b3 otherwise.\nSparse context updates The latent context code ct is designed to change sparsely in time, ideally at distinct, environment-specific, transition points. Accordingly, the coarse dynamics g\u03d5 (Eq. 2) are modeled by a GateL0RD cell (Gumbsch et al., 2021), which learns sparsely changing latent states ct via an update gate, whose activity is L0-regularized via loss term Lsparse. Note that context ct alone is too coarse to predict the current stochastic state zt.\nLoss function Given a sequence of length T of input images i1:T , actions a1:t, rewards r1:T , with discounts \u03b31:T , the parameters \u03d5 of C-RSSM are jointly optimized to minimize the loss L(\u03d5):\nL(\u03d5) = Eq\u03d5 [ \u03b2predLpred(\u03d5) + \u03b2KLLKL(\u03d5) + \u03b2sparseLsparse(\u03d5) ] , (9)\nincluding the prediction lossLpred, the KL lossLKL, and sparsity lossLsparse with respective hyperparameters \u03b2pred, \u03b2KL, and \u03b2sparse. The prediction lossLpred drives the system to accurately predict perceptions y via its output heads o\u03d5, including context-conditioned coarse predictions (Eq. 8). The KL loss LKL minimizes the KL divergences between prior predictions ph\u03d5 and pc\u03d5 and the approximate posterior q\u03d5. The sparsity loss Lsparse encourages consistency of context ct. The exact loss functions are provided in Suppl. D.3. We set \u03b2pred and \u03b2KL to DreamerV2 defaults (Hafner et al., 2020) and modify the sparsity loss scale \u03b2sparse depending on the scenario (cf. Suppl. B)."
        },
        {
            "heading": "2.2 HIERARCHICAL WORLD MODEL",
            "text": "To learn a hierarchical world model, we leverage C-RSSM\u2019s discrete context ct updates by means of our Temporal Hierarchies from Invariant Context Kernels (THICK) algorithm. A C-RSSM world model w\u03d5 segments sequences into periods of stable context activity (ct = ct+1 = \u00b7 \u00b7 \u00b7 = c\u03c4\u22121), interspersed with sparse context updates (cf. Fig. 3a). THICK uses these discrete context dynamics as an adaptive timescale for training a high-level network W\u03b8. The core assumption is that states prompting context updates coincide with crucial changes in latent generative factors. These key states are predicted by the high-level networkW\u03b8, while states between context updates are ignored.\nTo train the high-level world model W\u03b8, we require input-target pairs for a given sequence of T images i1:T , actions a1:T , and episode termination flags d1:T . The sequence is passed through the low-level model w\u03d5 to obtain a sequence of contexts c1:T . Targets are defined as all time steps \u03c4 with context changes, i.e., where c\u03c4 \u0338= c\u03c4\u22121 or the episode ends. We define the function \u03c4(\u00b7) as\n\u03c4(t) = min ( {\u03c4 | \u03c4 > t \u2227 (c\u03c4 \u0338= c\u03c4\u22121 \u2228 d\u03c4 = 1)} ) . (10)\nThus, \u03c4(\u00b7) maps every point t to the next point in time \u03c4(t) with context change, effectively implementing a variable temporal abstraction that generates target predictions \u03c4(t) for every t.\nHigh-level targets We predict all variables at \u03c4(t) that may cause a context change or are needed for planning across a context change: z\u0302\u03c4(t)\u22121, a\u0302\u03c4(t)\u22121,\u2206\u03c4\u0302(t), r\u0302 \u03b3 t:\u03c4(t) (cf. Fig. 3b). In particular, we predict the stochastic states z\u0302\u03c4(t)\u22121 and actions a\u0302\u03c4(t)\u22121 immediately before a context change at time \u03c4(t), because both can cause an update of c\u03c4(t) (see Eq. 2). Intuitively, this means that observations, e.g. seeing something fall, as well as actions, e.g. catching something, could contribute to a change of ct. We furthermore predict the elapsed time \u2206\u03c4(t) and the accumulated discounted reward r \u03b3 t:\u03c4(t), which may account for variable duration and rewards when evaluating high-level outcomes:\nElapsed time: \u2206\u03c4(t) = \u03c4(t)\u2212 t Accumulated rewards: r\u03b3t:\u03c4(t) = \u2206\u03c4(t)\u22121\u2211\n\u03b4=0\n\u03b3\u03b4rt+\u03b4 (11)\nHigh-level inputs To predict high-level targets, we use the low-level stochastic state zt and context ct as inputs. However, we need to disambiguate different potential outcomes, which generally depend on the world and the policy pursued by the agent. Accordingly, akin to actions on the low level, we create self-organizing high-level \u201cactions\u201d At, similar to skills or options (Sutton et al., 1999). At encode a categorical distribution over probable next context changes. To learn At, the high-level world model implements a posterior action encoder Q\u03b8 and a prior action encoder P\u03b8 (cf. Fig. 3b). Overall, the high-level world model W\u03b8 with learnable parameters \u03b8 is computed by:\nPost.: At \u223c Q\u03b8(At | ct, zt, c\u03c4(t), z\u03c4(t)) (12) Action: a\u0302\u03c4(t)\u22121 \u223c F a\u0302\u03b8 ( a\u0302\u03c4(t)\u22121 |At, ct, zt ) (13)\nState: z\u0302\u03c4(t)\u22121 \u223c F z\u0302\u03b8 ( z\u0302\u03c4(t)\u22121 | At, ct, zt ) (14)\nPrior: A\u0302t \u223c P\u03b8(A\u0302t | ct, zt) (15) Time : \u2206\u03c4\u0302(t) \u223c F \u03c4\u0302\u03b8 ( \u2206\u03c4\u0302(t) |At, ct, zt ) (16)\nReward: r\u0302\u03b3t:\u03c4(t) \u223c F r\u0302\u03b8 ( r\u0302\u03b3t:\u03c4(t) | At, ct, zt ) (17)\nThe posterior Q\u03b8 receives not only ct and zt as its input but also privileged information about the actually encountered next context, i.e. c\u03c4(t) and z\u03c4(t) (Eq. 12), which leads to the emergence of individualized, result-conditioned action encodings in At. The prior P\u03b8 learns a distribution over A\u0302t approximating the posterior without the privileged information (Eq. 15). During training, THICK samples the high-level action At from Q\u03b8. During evaluation, we sample from the prior P\u03b8 instead. We model A\u0302t and At as one-hot encoded categorical variables.\nLoss function The high-level world model W\u03b8 with parameters \u03b8 is trained to minimize the loss L(\u03b8) = E [ \u03b1predLpred(\u03b8) + \u03b1KLLKL(\u03b8) ] , (18)\nwith hyperparameters \u03b1pred and \u03b1KL scaling the prediction Lpred and action LKL loss terms, respectively. The prediction loss is the summed negative log-likelihood of the high-level predictions. The action loss drives the system to minimize the KL divergence between the posterior high-level action distribution Q\u03b8 and the prior distribution P\u03b8. The exact loss functions can be found in Suppl. D.4.\nSummary Our THICK world model augments traditional flat world models by a high-level, which learns predictions of variable length, anticipating context transitions. This augmentation allows for seamless transitions between coarse, low-level and abstract, high-level predictions. Given a\ncontext ct, stochastic state zt, and sampled high-level action A\u0302t, the high-level model W\u03b8 predicts a scenario (a\u0302\u03c4(t)\u22121, z\u0302\u03c4(t)\u22121) immediately prior to the next anticipated context change. By feeding this prediction into the coarse processing pathway of C-RSSM, we can predict the subsequent, new context c\u03c4(t) (Eq. 2) and a coarse prior estimate of the corresponding stochastic state z\u0302c\u03c4(t) (Eq. 5). Longer temporal abstract roll-outs can be created by feeding c\u03c4(t) and z\u0302c\u03c4(t) again into W\u03b8 (see Fig. 4). In this way, actual context change predictions are naturally generated by C-RSSM."
        },
        {
            "heading": "2.3 DOWNSTREAM APPLICATIONS OF THICK WORLD MODELS",
            "text": "World models have been applied in many downstream tasks, including MBRL (Ha & Schmidhuber, 2018; Hafner et al., 2019a; 2020; 2023), exploration (Sekar et al., 2020; Sancaktar et al., 2022), or model-predictive control (MPC) (Hafner et al., 2019b; Vlastelica et al., 2021). With minimal changes, the hierarchical roll-outs from THICK can be seamlessly integrated where flat roll-outs were previously utilized. We exemplify this integration in two key areas: MBRL and MPC."
        },
        {
            "heading": "2.3.1 THICK DREAMER: MBRL WITH HIERARCHICAL ROLLOUTS",
            "text": "Dreamer (Hafner et al., 2019a) learns behavior by training an actor and a critic from \u201cimagined\u201d roll-outs of its RSSM world model. More specifically, Dreamer imagines a sequence of states st:t+H from a start state st given an actor-generated action sequence at:t+H . Dreamer computes the general \u03bb\u2212return V \u03bb(st) (Sutton & Barto, 2018) for every st and its critic v\u03be is trained to regress V \u03bb(st). In sparse reward tasks, one challenge is reward propagation for training the critic (Andrychowicz et al., 2017). Here, Dreamer faces a difficult trade-off: Long roll-outs (large H) speed up reward propagation but degrade the quality of the predicted roll-outs. We propose THICK Dreamer, which combines value estimates from low- and high-level predictions to boost reward propagation. THICK Dreamer maintains an additional critic v\u03c7 to evaluate temporal abstract predictions. Like Dreamer, we first imagine a low-level roll-out ofH states st:t+H . Additionally, for every time t in the roll-out, we predict a temporal abstract outcome c\u03c4(t) and z\u03c4(t) and estimate a long horizon value V long as\nV long(st) = r\u0302 \u03b3 t:\u03c4(t) + \u03b3 \u2206t\u0302 ( r\u0302c\u03c4(t) + \u03b3\u0302 c \u03c4(t)v\u03c7(c\u0302\u03c4(t), z\u0302\u03c4(t)) ) , (19)\nwith all variables predicted via the THICK world model and immediate rewards via Eq. 8 of C-RSSM given THICK\u2019s world model predictions (cf. also supplementary Alg. 1). We estimate the value of a state st as a mixture of short- and long-horizon estimates with\nV (st) = \u03c8V \u03bb(st) + (1\u2212 \u03c8)V long(st), (20)\nwhere the hyperparameter \u03c8 controls the trade-off between the two estimates. We set \u03c8 = 0.9 in all experiments and train both critics v\u03be and v\u03c7 to regress the value estimate. In sum, to speed up credit assignment when learning a value function, THICK Dreamer combines low-level roll-outs with temporal abstract predictions to additionally estimate the value of likely long-horizon outcomes."
        },
        {
            "heading": "2.3.2 THICK PLANET: HIERARCHICAL MPC",
            "text": "The original RSSM was proposed in PlaNet (Hafner et al., 2019b) as a world model for MPC. PlaNet searches for the optimal action sequence a\u2217t:t+H to maximize the predicted returns r\u0302t:t+H . Thereby, PlaNet employs zero-order trajectory optimization via the cross entropy method (CEM) (Rubinstein, 1999). Once a\u2217t:t+H is identified, the initial action a \u2217 t is executed and the procedure is repeated.\nCEM optimizes randomly sampled trajectories. Sampling a good action sequence is exponentially harder for increasing task horizons. We hypothesize that such tasks could be solved with much fewer high-level actions. For this, we propose THICK PlaNet. THICK PlaNet plans on the high level to solve the task and uses the low level to follow this plan. We define a reward functionR(\u00b7) to estimate the return of a high-level action sequence A1:K with length K recursively as\nR(Ak:K , t \u2032) = r\u0302\u03b3t\u2032:\u03c4(t\u2032) + \u03b3 \u2206t\u0302\n{ r\u0302c\u03c4(t\u2032) + \u03b3\u0302 c \u03c4(t\u2032)R(Ak+1:K , \u03c4(t\n\u2032) + 1) for k < K, r\u0302c\u03c4(t\u2032) for k = K\n(21)\nwith all variables predicted via a temporal abstract roll-out (see Sec. 2.2) starting with k = 1 and t\u2032 = t. We search for the optimal sequence A\u0302\u22171:K maximizing R(\u00b7) with Monte Carlo Tree Search. Based on the first action A\u0302\u22171 we sample a subgoal z\u0302 goal t \u223c F\u03b8(z\u0302goalt |A\u0302\u22171, ct, zt). This subgoal is valid as long as it has not been reached yet and nothing has drastically changed in the environment. Thus, we only replan on the high level when the context has changed. We apply CEM on the low level to reach zgoalt while also maximizing task return with\na\u2217t:t+H = argmax at:t+H t+H\u2211 t\u2032=t r\u0302t\u2032 + \u03ba sim(zt\u2032 , z goal t ) with r\u0302t\u2032 \u223c o\u03d5(r\u0302t\u2032 | st\u2032), (22)\nfor a planning horizon H . The function sim(\u00b7) is a similarity measure between zgoalt and zt. The hyperparameter \u03ba controls the trade-off between external and internal reward. Previously, similarity between Gaussian distributed zt of the RSSM was estimated using cosine similarity (Mendonca et al., 2021). However, for the categorically distributed zt, the cosine similarity can be low even when they stem from the same distribution. Instead we use the cosine similarity of the logits, i.e.\nsim(zt, z goal t ) = lt \u00b7 lgoalt \u2225lt\u2225\u2225lgoalt \u2225 , (23)\nwhere \u00b7 is the dot product and lt and lgoalt are the logits of the distributions that produced zt and zgoalt , respectively. Compared to other similarity measures, e.g. KL divergence, our measure has the desirable property that sim(zt, z goal t ) \u2208 [0, 1], which simplifies setting the hyperparameter \u03ba, which we set to \u03ba = 0.025 to mainly guide the behavior in the absence of external reward."
        },
        {
            "heading": "3 RESULTS",
            "text": "We empirically evaluate THICK to answer the following questions:\n\u2022 Can THICK learn temporal abstractions? We show that the learned high-level world model indeed discerns meaningful, interpretable temporal abstractions across various scenarios (Sec. 3.1).\n\u2022 Can THICK\u2019s hierarchical predictions improve MBRL? We show that THICK Dreamer achieves higher returns than Dreamer in long-horizon tasks with sparse rewards (Sec. 3.2).\n\u2022 Can THICK\u2019s world model be used to plan hierarchically? We show that MPC with THICK world models is better than flat world models at solving long-horizon tasks (Sec. 3.3).\nWe evaluate our THICK world models in various scenarios. MiniHack (Samvelyan et al., 2021) is a sandbox framework for designing RL environments based on Nethack (Ku\u0308ttler et al., 2020). We test our system on benchmark problems as well as newly created tasks. All problems, detailed in Suppl. E.1, have hierarchical structures in which subgoals need to be achieved (e.g. fetch a wand) to fulfill a task (e.g. kill a monster) to exit a dungeon and receive a sparse reward. The observation is a pixel-based, ego-centric view of \u00b12 grid-cells around the agent. MiniHack uses discrete actions. VisualPinPad (Hafner et al., 2022) is a suite of visual, long-horizon RL problems. Here an agent (black square) needs to step on a fixed sequence of pads to receive a sparse reward. We use three levels of difficulties based on the number of pads and target sequence length (three, four, five).\nMultiWorld (Pong et al., 2018) is a suite of robotic manipulation tasks for visual RL. In these tasks a Sawyer robot has to either move an object to a goal position (puck in Pusher or ball in PickUp) or open a door (Door). We use fixed goals and take the normalized distance between the to-becontrolled entity and the goal position as dense rewards (in Pusher-Dense, PickUp, Door) and thresholded distances as sparse rewards (in Pusher-Sparse). Details are provided in Suppl. E.2."
        },
        {
            "heading": "3.1 INTERPRETABLE CONTEXTS AND HIERARCHICAL PREDICTIONS",
            "text": "First, we analyze the predictions of THICK world models across diverse tasks. Example sequences are displayed in Fig. 5, in Suppl. F.1 and on our website. In MiniHack, context updates typically coincide with item collection, map changes, area exploration, or dungeon exits. In Multiworld, context changes occur due to object interactions or at workspace boundaries. In VisualPinPad, activating pads can prompt context changes. The high-level model predicts the states preceding context changes, often abstracting details, leading to blurry reconstructions. For instance, in KeyRoom, the system forecasts the agent\u2019s level exit without knowledge of the exact room layout (Fig. 5, t + 6). Nevertheless, the lower level consistently predicts the next frames accurately, as shown in Fig. 1b.\nAbstract action representations At emerge on the high level, as illustrated in Fig. 6. These actions categorically encode different agent-world interactions, e.g., grasping or pushing a ball in PickUp. The prior Q\u03b8 learns to sample actions based on the likelihood of their outcomes (red frames in Fig. 6). If there are more actions At than necessary, different actions encode the same outcome."
        },
        {
            "heading": "3.2 MODEL-BASED REINFORCEMENT LEARNING",
            "text": "We investigate whether hierarchical roll-outs can improve MBRL in the MiniHack suite by comparing THICK Dreamer to DreamerV2 (Hafner et al., 2020) and to Director (Hafner et al., 2022), a hierarchical RL method based on Dreamer. Fig. 7a\u20137d show that THICK Dreamer matches or outperforms flat Dreamer in all tasks in terms of sample efficiency or overall success rate. The advantage of THICK Dreamer is more pronounced in tasks that require completing multiple subgoals (e.g. completing five subgoals in EscapeRoom vs. finding a key to open a door in KeyRoom). Director outperforms the other methods in KeyRoom but fails to learn other MiniHack tasks. We investigate the failure cases of Director in Suppl. F.3 and show more MiniHack results in Suppl. F.2.\nWe hypothesize that task horizon length is the main factor boosting THICK Dreamer\u2019s performance. To investigate this, we systematically vary the task horizon in the KeyCorridor problem (see Fig. 7e) by modifying the corridor length. Fig. 7f\u20137g plot the mean difference in obtained rewards and success rate over 500k steps of training between THICK Dreamer and Dreamer for different corridor lengths. The performance gain of THICK Dreamer tends to increase with corridor length until at some length both approaches fail to discover rewards during training, detailed in Suppl. F.2.\nWe further analyze the effect of task horizon in VisualPinPad. VisualPinPad poses two challenges: exploration and long-horizon behavior. To analyze the latter in isolation, we sidestep the challenge of discovering the sparse rewards by initially filling the replay buffer of all models with 1M steps of exploration using Plan2Explore (Sekar et al., 2020) (details in Suppl. F.4). Fig. 8 shows the performance of THICK Dreamer, DreamerV2, and Director. THICK Dreamer matches Dreamer in PinPadThree and is slightly more sample efficient in the more challenging tasks.4 Thus, fusing hierarchical predictions to train a single policy in THICK Dreamer seems better suited for longhorizon learning than the hierarchical policies of Director or not employing hierarchies."
        },
        {
            "heading": "3.3 ZERO-SHOT MODEL-PREDICTIVE CONTROL",
            "text": "Lastly, we analyze whether our hierarchical predictions are suitable for planning by comparing THICK PlaNet and PlaNet (Hafner et al., 2019b) in Multiworld. We consider the challenging setup of MPC for models trained on an offline dataset of 1M samples collected by Plan2Explore (Sekar et al., 2020). Figure 9 shows the zero-shot performance over training. For Pusher-Dense, i.e. a short-horizon task5 with dense rewards, there is no notable difference between both methods. When rewards are sparse (Pusher-Sparse) or the task horizon is long (Door and PickUp), THICK PlaNet achieves higher returns than PlaNet. Additionally, the subgoals set by the high level can be decoded, shown in Suppl. F.6, which improves the explainability of the system\u2019s behavior.\n4Previously, Hafner et al. (2022) reported that Director outperforms Dreamer in VisualPinPad. We hypothesize that this improvement stems from more sophisticated exploration, which is not necessary in our setting.\n5Since the puck starts between the gripper and goal, the task can be solved by directly moving to the goal."
        },
        {
            "heading": "4 RELATED WORK",
            "text": "Sparsity in RNNs: Learning hierarchical RNNs from sparse activity was proposed in Schmidhuber (1992), where a high level would become active based on low-level errors. Subsequently, there has been a lot of research on fostering sparsity in RNNs (Graves et al., 2014; Neil et al., 2016; Goyal et al., 2021; Gumbsch et al., 2021; Jain et al., 2022), which we compare in Suppl. C.\nTemporal abstract predictions: One main challenge for learning temporal abstractions is segmenting a sequence into meaningful units. Discrete latent dynamics were previously used to model proactive gaze behavior (Gumbsch et al., 2022). Alternative segmentation methods are identifying easyto-predict bottleneck states (Neitz et al., 2018; Jayaraman et al., 2019; Zakharov et al., 2021), using fixed time scales (Saxena et al., 2021), prediction error-based segmentation (Gumbsch et al., 2019), or regularizing boundary detectors (Kim et al., 2019; Zakharov et al., 2022) (details in Suppl. C).\nHierarchical RL (HRL): HRL is an orthogonal research direction to hierarchical world models.In HRL a high-level policy either selects a low-level policy or provides goals or rewards for a low level (Pateria et al., 2021). In contrast, our THICK Dreamer uses high-level predictions to train a flat RL agent. Typically in HRL, the high level operates on fixed time scales (Hafner et al., 2022; Nachum et al., 2018; Vezhnevets et al., 2017; Gu\u0308rtler et al., 2021) or task-dependently based on subgoal completion (Bacon et al., 2017; Levy et al., 2019). In THICK world models, the high level is learned time- and task-independently purely from predictions and latent state regularization."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "We have introduced C-RSSM and THICK\u2014fully self-supervised methods to construct hierarchical world models. By imposing a sparsity objective, C-RSSM develops context codes that update only at critical situations, where prediction-relevant aspects of the environment change. On a higher level, THICK learns to anticipate context-altering states. Categorical high-level action codes enable the anticipation of different outcomes, accounting for multiple lower-level context transitions. As a result, THICK world models can predict both abstract context transitions and exact low-level dynamics. Additionally, we have shown that the hierarchical predictions can improve long-horizon learning.\nLimitations THICK relies on setting the hyperparameter \u03b2sparse, which determines the high-level segmentation. Ideally, this hyperparameter should be tuned for every task. However, we found that the same value works well across similar tasks. Furthermore, except for improving long-horizon learning our downstream applications have similar restrictions as the method they build upon. For example, if Dreamer never discovers a solution to a task, THICK cannot decompose it.\nFuture directions We see great potential of THICK world models as a tool to build more sophisticated agents that explore and plan their behavior across multiple time scales. A promising direction is combining MCTS with RL (Schrittwieser et al., 2020), e.g. for biologically plausible planning (Mattar & Lengyel, 2022) by searching for high-level goals that goal-condition low-level policies (Akakzia et al., 2021). Another potential lies in integrating more active epistemic-driven exploration (Sekar et al., 2020; Sancaktar et al., 2022), which could lead to a more robust consolidation of context codes and transitions between them. Future extensions could also explore richer predictions purely from the context ct. This would allow the high-level to directly predict context transitions without predicting observable state information used for intermediate queries to the lowlevel. Lastly, while we employed THICK to establish a two-level hierarchy of world models, THICK could be applied on multiple levels to recursively build an N -level world model hierarchy."
        },
        {
            "heading": "REPRODUCIBILITY STATEMENT",
            "text": "In order to ensure reproducibility we provide an anonymized link to our source code on our supplementary website: https://sites.google.com/view/thick-world-models. Additionally, we specify all our hyperparameters choices, details on the conducted hyperparameter search, and advice on how to tune the hyperparameters for novel task in Suppl. B."
        },
        {
            "heading": "Contents",
            "text": ""
        },
        {
            "heading": "A Pseudocode 16",
            "text": ""
        },
        {
            "heading": "B Hyperparameters 18",
            "text": ""
        },
        {
            "heading": "C Extended Related Work 20",
            "text": ""
        },
        {
            "heading": "D THICK World Models: Implementation Details 21",
            "text": "D.1 THICK Design Choices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\nD.2 GateL0RD . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\nD.3 C-RSSM Loss . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\nD.4 High-level World Model Training . . . . . . . . . . . . . . . . . . . . . . . . . . 23\nD.5 THICK Dreamer: Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\nD.6 THICK PlaNet: Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23"
        },
        {
            "heading": "E Environment Details 25",
            "text": "E.1 MiniHack . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\nE.2 Multiworld . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26"
        },
        {
            "heading": "F Extended Results and Experiment Details 27",
            "text": "F.1 Analysis of Contexts and Predictions . . . . . . . . . . . . . . . . . . . . . . . . . 27\nF.2 MBRL in MiniHack: Experiment details and extended results . . . . . . . . . . . . 29\nF.3 MBRL in MiniHack: Director . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\nF.4 MBRL in VisualPinPad: Experiment details . . . . . . . . . . . . . . . . . . . . . 32\nF.5 MBRL in VisualPinPad: Effect of exploration . . . . . . . . . . . . . . . . . . . . 32\nF.6 MPC: Experiment details and extended results . . . . . . . . . . . . . . . . . . . . 33\nF.7 MBRL & MPC: Scaling models vs. hierarchy . . . . . . . . . . . . . . . . . . . . 34\nF.8 Ablations and Hyperparameters . . . . . . . . . . . . . . . . . . . . . . . . . . . 34\nG Computation and Code 37"
        },
        {
            "heading": "A PSEUDOCODE",
            "text": "Algorithm 1 outlines how THICK world models make temporal abstract predictions using both levels of the hierarchy (also visualized in Fig. 4). Blue parts are only needed for MBRL or MPC (see Sec. 2.3). For temporal abstract rollouts, which are used in THICK PlaNet, the process can be repeated K times by using the output states, i.e. c\u03c4(t) and z\u0302c\u03c4(t), as inputs again.\nAlgorithm 1 THICK Temporal Abstract Prediction 1: input: context ct, stochastic state zt 2: A\u0302t \u223c P\u03b8(A\u0302t | ct, zt) \u25b7 sample high-level action 3: z\u0302\u03c4(t)\u22121 \u223c F\u03b8 ( z\u0302\u03c4(t)\u22121 | A\u0302tct, zt ) \u25b7 high-level state prediction\n4: a\u0302\u03c4(t)\u22121 \u223c F\u03b8 ( a\u0302\u03c4(t)\u22121 | A\u0302tct, zt ) \u25b7 high-level action prediction\n5: \u02c6\u2206\u03c4(t) \u223c F\u03b8 ( \u02c6\u2206\u03c4(t) | A\u0302tct, zt) \u25b7 high-level time prediction\n6: (r\u0302\u03b3t:\u03c4(t) \u223c F\u03b8 ( r\u0302\u03b3t:\u03c4(t) | A\u0302tct, zt ) \u25b7 high-level reward prediction\n7: c\u03c4(t) \u2190 g\u03d5 ( a\u0302\u03c4(t)\u22121, ct, z\u0302\u03c4(t)\u22121 ) \u25b7 low-level context\n8: z\u0302c\u03c4(t) \u223c pc\u03d5 ( z\u0302c\u03c4(t)\u22121 | a\u0302\u03c4(t)\u22121, c\u03c4(t), z\u0302\u03c4(t)\u22121 ) \u25b7 low-level coarse prior\n9: r\u0302c\u03c4(t), \u03b3\u0302 c \u03c4(t) \u223c oc\u03d5 ( r\u0302c\u03c4(t), \u03b3\u0302 c \u03c4(t) | c\u03c4(t), z\u0302c\u03c4(t) ) \u25b7 coarse reward & discount prediction\n10: output: c\u03c4(t), z\u0302c\u03c4(t), \u2206\u0302t, r\u0302 \u03b3 t\u03c4 r\u0302 c \u03c4(t), \u03b3\u0302 c \u03c4(t)\nAlgorithm 2 describes how to create input-target data for training the high-level world model. In continual learning environments with no early termination of an episode, we omit the red part.\nAlgorithm 2 THICK Training Data Generation 1: input: discount factor \u03b3, sequences of contexts c1:T , stochastic states z1:T , actions a1:T , 2: rewards r1:T , and episode termination flags d1:T 3: initialize: train data D \u2190 {}, unassigned inputs I \u2190 {} 4: for \u03c4 \u2190 1 to T do 5: if c\u03c4 \u0338= c\u03c4\u22121 or d\u03c4 = 1 then \u25b7 context change or episode is over at time \u03c4 6: for (ct, zt) \u2208 I do 7: compute passed time \u2206\u03c4 \u2190 \u03c4 \u2212 t and accumulated rewards rt:\u03c4 \u2190 \u2211\u2206t\u22121 \u03b4=1 \u03b3\n\u03b4rt+\u03b4 8: add input-target tuple ( (ct, zt), (z\u03c4\u22121,a\u03c4\u22121,\u2206t, rt:\u03c4 ) ) to D\n9: remove (ct, zt) from I 10: add potential input (c\u03c4 , z\u03c4 ) to I 11: output: train data D\nAlgorithm 3 describes the general main training and generation of behavior THICK world models. Red parts are only used for THICK PlaNet. Blue parts are only used for THICK Dreamer. In our zero-shot planning experiments using THICK PlaNet, we do not add new data to the replay buffer and only plan and execute actions during evaluation.\nAlgorithm 3 THICK World Models 1: initialize neural networks and replay buffer 2: tplan = \u2212I 3: for t\u2190 1 to tend do 4: update low-level world model state st \u223c w\u03d5(st | st\u22121,at\u22121) 5: // Behavior 6: if ct \u0338= ct\u22121 \u2227 t \u2265 tplan + I then 7: plan subgoal zgoalt using MCTS and temporal abstract rollouts (Alg. 1) 8: tplan \u2190 t 9: plan new action at using CEM given st and z goal t (Eq. 22) 10: sample new action at from actor \u03c0 given st 11: execute action at in environment and observe rt, it and dt 12: add (it,at, rt, dt) to replay buffer 13: // Train world models 14: draw sequence batch B \u2190 (it\u2032:T ,at\u2032:T , rt\u2032:T , dt\u2032:T ) from replay buffer 15: embed batch in latent state st\u2032:T \u223c w\u03d5 ( st\u2032:T | B\n) 16: update low-level world model w\u03d5 using B (Eq. 9) 17: generate high-level training batch D from (st\u2032:T ,at\u2032:T , rt\u2032:T , dt\u2032:T ) (Alg. 2) 18: update high-level world model W\u03b8 using D (Eq. 18) 19: // Train actor and critic 20: imagine trajectory (st\u2032\u2032:H ,at\u2032\u2032:H , rt\u2032\u2032:H , \u03b3t\u2032\u2032:H) using w\u03d5 from random start st\u2032\u2032 \u2208 B 21: make temporal abstract predictions for each st\u2032\u2032:H using W\u03b8 and w\u03d5 (Alg. 1) 22: compute value V (Eq. 20) 23: update critics v\u03c7 and v\u03be (Eq. 33) 24: update actor \u03c0"
        },
        {
            "heading": "B HYPERPARAMETERS",
            "text": "World model learning hyperparameters For optimizing the world models, i.e. our THICK world models and the baseline models in Dreamer and Director, we use the default DreamerV2 hyperparameters (Hafner et al., 2020) \u2013 except for minor variations. Specifically, we decreased the model size by setting the feature size of the RSSM and the dimensionality of ht to 256. As we show in Suppl. F.7 model size does not strongly affect performance in our setting. Additionally for THICK Dreamer and Dreamer we did not employ layer normalization for the GRU within the RSSM, because in pre-tests this showed increased robustness for both approaches.\nMBRL hyperparameters For training the actor and critic in THICK Dreamer and Dreamer we use the default hyperparameters of DreamerV2. For Director we mostly used its default hyperparameters (Hafner et al., 2022), however we made some minor adjustments to the training frequency to ensure a fair comparison. Director performs one training update every 16 policy steps instead of every 5 steps in DreamerV2. This was done to reduce wall-clock time but decreases sample efficiency (Hafner et al., 2022). We increase the update frequency (16\u2192 5) in order to fairly compare sample efficiency between approaches.\nMPC hyperparameters For MPC with CEM we use the hyperparameters of PlaNet (Hafner et al., 2019b). For high-level planning with MCTS, we use MuZero\u2019s (Schrittwieser et al., 2020) implementation, with mostly the same hyperparameters. However, intuitively we wouldn\u2019t expect multiple predictions to reach a goal. Thus, we decrease the number of simulations to S = 100.\nDifferences between environments The main difference between MiniHack and the other environments is that in MiniHack episodes can terminate based on task success or death of the agent. VisualPinPad and Multiworld are continual learning environments without early episode termination. As is customary with the use of DreamerV2, for environments that do not feature early episode termination, we do not predict discounts \u03b3t, nor do we prioritize the termination of episodes in the replay buffer. Importantly, we do not treat episode terminations as context changes. For action prediction, we use Categorical Cross Entropy Loss (CCE) for predicting discrete actions (Minihack and VisualPinPad), and scale down the high-level prediction loss for predicting actions and elapsed time when training purely on task-free offline data (Multiworld). Lastly, the sparsity loss scale \u03b2sparse was tuned for each suite.\nHyperparameter search For determining the sparsity loss scale \u03b2sparse, the value estimate balance \u03c8, and the long-horizon planning scale \u03ba, we ran a grid search using three random seeds and using two tasks of each suite (MiniHack: KeyRoom-Fixed-S5, WandOfDeath-Advances; Visual Pin Pad: VisuaLPinPadFour, VisuaLPinPadFive; Multiworld: Door, PickUp). We determined the best hyperparameter value for each suite depending on task performance and a qualitative inspection of the high-level predictions (see Suppl. F.8). For simplicity and to demonstrate robustness, we used the same values for each suite.\nHow to tune When tuning THICK world models for a new task, we recommend mainly searching over the sparsity loss scale \u03b2sparse \u2208 {1, 5, 10, 25, 50}. Typically, one random seed is sufficient to determine which \u03b2sparse leads to few, but not too few, context changes. Depending on horizon length and reward sparsity of the task, searching for \u03c8 \u2208 {0.8, 0.9} and \u03ba \u2208 {0.025, 0.1, 0.5} can also boost performance."
        },
        {
            "heading": "C EXTENDED RELATED WORK",
            "text": "Sparsity in RNNs: Developing hierarchical RNNs based on sparse activity was already proposed in the 90s by Ju\u0308rgen Schmidhuber (Schmidhuber, 1992). The Neural History Compressor (Schmidhuber, 1992) uses a hierarchical stack of RNNs, that autoregressively predict the next inputs. The higher levels in the hierarchy remain inactive until the lower level fails to predict the next input. Recently, there has been increasing interest in regularizing RNNs towards sparse latent updates. Alternative approaches to the L0-regularization of GateL0RD (Gumbsch et al., 2021) are using sparse attention masks (Graves et al., 2014; Goyal et al., 2021), competition among submodules (Goyal et al., 2021), regularizing update gates towards a variational prior (Jain et al., 2022), or time-dependent updates (Koutnik et al., 2014; Neil et al., 2016).\nTemporal abstractions from regularized latent dynamics: Previously, sparse changes in the latent states of a low-level model have been used to model temporal abstractions (Gumbsch et al., 2022; Saxena et al., 2021). In contrast to our work, the temporal abstractions in Gumbsch et al. (2022) were learned in much simpler settings with highly structured observations, instead of the high-dimensional, pixel-based observations examined in this work. Additionally, these temporal abstractions were only used to model goal-anticipatory gaze behavior of infants and have not been applied for MPC or MBRL. Separately, Saxena et al. (2021) introduced a hierarchical video prediction model (i.e., without action) that used different clock speeds at each level to learn long-term dependencies using pixel-based input. Although, this was apt at learning slow-moving content at higher levels of the temporal hierarchy, unlike C-RSSM and THICK, it requires the temporal abstraction factor to be defined explicitly.\nTemporal abstractions from predictability: Adaptive Skip Intervals (ASI) (Neitz et al., 2018) is a method for learning temporal abstract autoregressive predictions. In ASI, a network is trained to predict those inputs within a predefined horizon, that best allow predicting an extended sequences into the future. As a result, the model learns to skip a number of inputs towards predictable transitions. Along similar lines, Temporal-Agnostic Predictions (TAP) (Jayaraman et al., 2019) identifies frames of a video within a time horizon that are highly predictable. TAP is then trained to only predict those predictable \u201cbottleneck\u201d frames. Zakharov et al. (2021) provide a learning-free mechanism for detecting context change by evaluating how predictable future states are. Briefly, their approach detects changes in latent representation of each layer in the model hierarchy and introduces temporal abstraction by blocking bottom-up information propagation between different contexts. This is unlike THICK, where context changes are determined using a learning-based sparsity regularization. An opposing approach is using unexpected prediction errors of a forward model for self-supervised time series segmentation (Gumbsch et al., 2019). Here, the idea is that in certain states the dynamics of the agent-environment interactions change, e.g. changing the terrain during locomotion, which lead to a temporary increase in the prediction error.\nTemporal abstractions from learning boundary detectors: Besides using indirect measure to segment a sequence, a straight-forward approach is to train a boundary detector that signals the boundary of subsequences (Kim et al., 2019; Zakharov et al., 2022). Kim et al. (2019) train a boundary detectors that is regularized by specifying the maximum number of subsequences allowed and their maximal length. This requires prior knowledge about the training data and imposes hard constraints on the time scales of the learned temporal abstractions. Our sparsity loss instead implements a soft constraint. Conversely, Zakharov et al. (2022) introduced a boundary detection mechanism using a non-parametric posterior over the latent states. Here, the model learns to transition between states only if a change in the represented features had been observed \u2013 otherwise temporally-persistent states were clustered together.\nFaster credit assignment in RL: THICK Dreamer predicts long-horizon outcomes via its high level model when training the critic in imagination to boost reward propagation. This allows faster credit assignment for tasks with sparse or delayed rewards. Previously, this was tackled using reward redistribution (Patil et al., 2021)."
        },
        {
            "heading": "D THICK WORLD MODELS: IMPLEMENTATION DETAILS",
            "text": ""
        },
        {
            "heading": "D.1 THICK DESIGN CHOICES",
            "text": "Due to space constraints we explain some design choices in more detail here.\nHigh-level targets Our goal is to learn a high-level world model that predict situations in which latent generative factors are assumed to change, e.g. a door openings or object manipulations. Besides that, we want to use the use the high-level outputs to predict future rewards and reconstruct images at some time \u03c4(t). Thus, we at least need the context c\u03c4(t) and the stochastic state z\u03c4(t) to make these reconstructions via the coarse processing pathway (see Eq. 8). There are two potential ways to predict c\u03c4(t) and z\u03c4(t), either predict the state before or after the context transition.\nWe predict the states before the context transition. Our main reasoning is that the prediction of context-altering situations pose two challenges: i) learning in which situation such transitions are currently possible and ii) how these transitions affect the latent generative factors. The C-RSSM already learns to encode ii). Thus, in order to reduce redundancy and simplify the challenge, we train the high-level model to only learn i) and then prompt the low-level model for ii). One example from MiniHack would be predicting the agent standing in front of closed door and performing the a door-opening-action. We believe this is a simpler prediction compared to predicting the ego-centric view of the agent after opening a door and looking into a (potentially unknown) new room.\nCoarse predictions for contextual learning We want the context ct to encode latent information that is necessary for prediction and reconstruction. If we would omit the coarse prior predictions (Eq. 5) and coarse output reconstructions (Eq. 8) the C-RSSM would have no incentive to encode prediction-relevant latent information in ct. Instead it could purely utilize ht and avoid a sparsity penalty in Eq. 9 via Lsparse by never updating ct. Completely omitting h in the C-RSSM impedes learning, as we show in our ablations in Suppl. F.8. Thus, we instead add the coarse processing pathway. Via the coarse predictions, the C-RSSM needs to encode latent factors in ct in order to reduce the KL-loss (Eq. 31) and prediction loss (Eq. 30).\nCoarse predictions to omit ht The high-level model attempts to predict a future state of the system. The full latent state would contain the deterministic component h. However, for the highlevel model it would be very challenging to predict the unregularized high-dimensional deterministic hidden state h many time steps in the future. The coarse pathway of the C-RSSM allows to update the context dynamics ct, predict stochastic states zct , and reconstruct external variables without the deterministic hidden state ht. Thus, it is advantageous that the C-RSSM can make predictions without h. After a high-level prediction, we can feed the high-level outputs (z\u0302\u03c4(t)\u22121, a\u0302\u03c4(t)\u22121) into the low-level world model. This brings many advantages: For example, this allows us to predict rewards or discounts/episode termination at particular states after a high-level prediction, which we use in THICK Dreamer in and THICK PlaNet (see Sec. 2.3). Furthermore, we can reconstruct images to visualize predictions as shown in Sec. 3.1. Additionally, we can continue with low-level rollouts after a high-level prediction, which is a feature we have not yet utilized."
        },
        {
            "heading": "D.2 GATEL0RD",
            "text": "We want the context code ct to only change sparsely in time. Thus, we implement the discrete context dynamics g\u03d5 as a GateL0RD cell (Gumbsch et al., 2021). GateL0RD is an RNN designed to maintain sparsely changing latent states ct. In order to realize this inductive bias, GateL0RD uses two subnetworks gp\u03d5 and g g \u03d5 that control ct-updates via an internal update gate \u039bt. GateL0RD can be summarized as follows:\nCandidate proposal: c\u0302t = g p \u03d5(at\u22121, ct\u22121, zt\u22121) (24)\nUpdate gate: \u039bt = g g \u03d5(at\u22121, ct\u22121, zt\u22121) (25) Context Update: ct = \u039bt \u25e6 c\u0302t + (1\u2212\u039bt) \u25e6 ct\u22121 (26)\nwith \u25e6 denoting the Hadamard product. We use the action at\u22121 and the last stochastic state zt\u22121 as the cell inputs. Based on this cell input and the last context ct\u22121, GateL0RD proposes a new context\nc\u0302t via its proposal subnetwork g p \u03d5 (Eq. 24). Whether the context is updated depends on an update gate \u039bt \u2208 [0, 1]m (Eq. 26). This update gate \u039bt is the output of the gating subnetwork gg\u03d5 (Eq. 25) which uses a rectified tanh activation function (ReTanh), with ReTanh(x) := max(0, tanh(x)). This ensures that the gate activations are \u2208 [0, 1]m. Note that to compute \u039bt, the subnetwork gg\u03d5 internally samples from a Gaussian distribution before applying the ReTanh function. This was shown to improve robustness (Gumbsch et al., 2021). Thus, the context updates are a stochastic process.\nOriginally (Gumbsch et al., 2021), GateL0RD used a subnetwork to compute the cell output using multiplicative gating. We omit this here and instead feed to output to the GRU cell f\u03d5 as shown in Fig. 2 (right).\nThe centralized gate \u039bt of GateL0RD makes it easy to determine context changes, i.e. ct \u0338= ct\u22121. Since all context updates depend on \u039bt, we know that the context changed if \u039bt > 0. This is an advantage over other RNNs that use multiple gates for sparsely changing latent states. We use this measure to determine context changes when building the world model hierarchy."
        },
        {
            "heading": "D.3 C-RSSM LOSS",
            "text": "The loss of the C-RSSM (Eq. 9) is composed of three parts: the prediction loss Lpred, the KL loss LKL, and the sparsity loss Lsparse. Expect for the sparsity loss, we adapt these loss terms from the RSSM. However we always need to account for the coarse prediction pathways of the C-RSSM.\nWe define the prediction loss Lpred as\nLpred(\u03d5) = T\u2211\nt=1 [ \u2211 y\u2208{it,rt,\u03b3t} \u2212 log o\u03d5(y | st)\u2212 log oc\u03d5(y | ct, zt) ] . (27)\nEquations in red are exclusive to the C-RSSM. Thus, the network is trained to minimize the negative log likelihood for predicting the images it, rewards rt and future discounts \u03b3t. Here we account for both the precise predictions over the output heads o\u03d5 (Eq. 7), as well as for the coarse predictions over the output heads oc\u03d5 (Eq. 8). Following the codebase of DreamerV2 (Hafner et al., 2020), in continual learning environments when there is no early episode termination, we do not predict the discount \u03b3t, and instead use a fixed discount \u03b3 = 0.99.\nThe C-RSSM predicts two prior distributions for the next stochastic state z\u0302t: fine predictions using the full state (Eq. 4) and coarse predictions based only on the context, last action and stochastic state (Eq. 5). We need to account for both types of prediction in the KL loss LKL with\nLKL(\u03d5) = T\u2211\nt=1\nKL [ q\u03d5 ( zt | ht, it ) ||ph\u03d5 ( z\u0302t | ht )] +KL [ q\u03d5 ( zt | ht, it ) ||pc\u03d5(z\u0302ct | at\u22121, ct, zt\u22121) ] .\n(28)\nThus, we want to minimize the divergence between both the fine prior ph\u03d5 and the approximate posterior q\u03d5, as well as the divergence between the coarse prior pc\u03d5 and q\u03d5. As in DreamerV2 (Hafner et al., 2020), we use KL-balancing, which scales the prior p\u03d5 of each KL divergence by a factor \u03b2bal = 0.8, and the posterior q\u03d5 by 1\u2212\u03b2bal. This enables faster learning of the prior to avoid that the posterior is regularized towards an untrained prior.\nWe take the sparsity loss Lsparse from GateL0RD (Gumbsch et al., 2021) which is an L0regularization of the context changes \u2206ct. This is implemented as\nLsparse(\u03d5) = T\u2211\nt=1 J\u2211 j=1 \u2225\u2225\u2225\u2206cjt\u2225\u2225\u2225 0 = T\u2211 t=1 J\u2211 j=1 \u0398 ( \u039bjt ) (29)\nwhere J is the dimensionality of the context ct and \u0398 ( \u00b7 )\ndenotes the Heaviside step function. That is, an L0-regularization of the context changes is implemented as the binarization of the update gates \u039bt (Eq. 26). We estimate the gradient of the Heaviside step function using the straight-through estimator (Bengio et al., 2013). The advantage of GateL0RD\u2019s L0-regularization to other regularization\ntowards sparsity, such as using low variational prior (Jain et al., 2022), is that in fully-observable and highly predictable situations, GateL0RD will shut its gates and not change the context almost regardless of the order of magnitude of \u03b2sparse to avoid a punishment."
        },
        {
            "heading": "D.4 HIGH-LEVEL WORLD MODEL TRAINING",
            "text": "The high-level world model with parameters \u03b8 is trained to minimize both the prediction loss Lpred, of predicting the next context change state, and the KL loss LKL between the high-level action distributions. We define the prediction loss Lpred as the summed negative log likelihood (NLL) of the to be predicted action a\u03c4(t)\u22121, stochastic state z\u03c4(t)\u22121, passed time \u2206\u03c4(t), and rewards r \u03b3 t:\u03c4(t), i.e.\nLpred(\u03b8) = T\u2211 t=1 [ \u2211 Y \u2208{a\u03c4(t)\u22121,z\u03c4(t)\u22121,\u2206\u03c4(t),r\u03b3t:\u03c4(t)} \u03b1Y \u2212 logF\u03b8(Y | At, ct, zt) ] . (30)\nThe hyperparameters \u03b1Y \u2208 {\u03b1a\u03c4(t)\u22121 , \u03b1z\u03c4(t)\u22121 , \u03b1\u2206\u03c4(t), \u03b1r \u03b3 t:\u03c4(t)} can be used to scale the individual prediction losses. As default, we set \u03b1Y = 1 for all loss terms. When training the network on task-free exploration, i.e. during zero-shot MPC as described in Sec. 3.3, we found that predicting the actions a\u03c4(t)\u22121 at context changes and elapsed time \u2206\u03c4(t) was challenging. To mitigate this, during task-free exploration we set \u03b1a\u03c4(t)\u22121 = 0.1 and \u03b1\u2206\u03c4(t) = 0.1. For predicting continuous actions we sample from a Gaussian distribution of predicted actions and compute the NLL as the loss for action prediction. For discrete actions we predict a Categorical distribution from which we sample the actions, and compute the Categorical Cross Entropy (CCE) loss.\nThe KL loss LKL drives the system to minimize the divergence between the posterior high-level action distribution Q\u03b8(At | ct, zt, c\u03c4(t), z\u03c4(t)), and the prior distribution P\u03b8(A\u0302t | ct, zt) with\nLKL = T\u2211 t=1 KL [ Q\u03b8(At | ct, zt, c\u03c4(t), z\u03c4(t)) || P\u03b8(A\u0302t | ct, zt) ] . (31)\nLike the KL loss LKL on the low level (see Suppl. D.3), we use KL balancing (Hafner et al., 2020) to scale the prior part by \u03b1bal = 0.8 and the posterior part by 1\u2212 \u03b1bal."
        },
        {
            "heading": "D.5 THICK DREAMER: DETAILS",
            "text": "THICK Dreamer estimates the overall value V (st) of a state st as a mixture of short- and longhorizon estimates (Eq. 20). The short-horizon value estimate is computed as the general \u03bb-target as in DreamerV2 with\nV \u03bb(st) = r\u0302t + \u03b3\u0302t { (1\u2212 \u03bb)v\u03be(s\u0302t+1) + \u03bbV \u03bb(s\u0302t+1) for t < H, v\u03be(s\u0302t+1) for t = H\n(32)\nwhere r\u0302t and \u03b3\u0302t are sampled from the output heads o\u03d5 given st (Eq. 7) and \u03bb is a hyperparameter.\nThe overall value estimate V (Eq. 20) is a combination of the short-horizon estimate V \u03bb from the \u03bb-returns (Eq. 32) and the long horizon estimate V long based on the high-level predictions (Eq. 19). THICK Dreamer trains both critics v\u03be and v\u03c7 to regress the overall value estimate using a squared loss:\nL(\u03d1) = Ep\u03d5 [ H\u2211 t=1 1 2 ( v\u03d1(st)\u2212 sg ( V (st) ))2] , (33)\nfor the two critics v\u03d1 \u2208 {v\u03be, v\u03c7} with parameters \u03d1 \u2208 {\u03be, \u03c7}, and sg(\u00b7) the stop gradient operator. The functions V \u03bb and V long compute value targets using the critics v\u03be and v\u03c7, respectively. Like DreamerV2, we stabilize critic training by using a copy of the critics during values estimation (in Eq. 32 and Eq. 19). The copy is updated after every 100 updates."
        },
        {
            "heading": "D.6 THICK PLANET: DETAILS",
            "text": "For planning on the high level we use a MCTS implementation based on MuZero (Schrittwieser et al., 2020). We only replan on the high level if the context changes, i.e. ct \u0338= ct\u22121. Since all\nthe subgoals zgoalt are situations that lead to context changes, no additional criterion for subgoal completion is needed. Upon reaching a subgoal, e.g. touching an object, the context can sometimes change for multiple subsequent time steps. This causes the high-level to replan multiple times in a row. Too avoid high computational load from replanning and to enable smoother trajectories, we inhibit replanning for I = 3 time steps after setting a new subgoal. While this could potentially degrade performance in dynamic environments, we found this to work well in Multiworld."
        },
        {
            "heading": "E ENVIRONMENT DETAILS",
            "text": ""
        },
        {
            "heading": "E.1 MINIHACK",
            "text": "Here we provide a detailed explanation of all MiniHack problems we considered. In all settings, we restricted the action space to the minimum number of actions needed to solve the task. In all tasks the agents receives a sparse reward of 1 when exiting the room and a small punishment of \u22120.01 when performing an action that has no effect, e.g. moving against a wall. In the easier tasks (WaterCrossing-Ring, KeyRoom-Fixed-S5, KeyCorridor) the agent is allowed 200 time steps to solve the task. In all other tasks the time limit is set to 400 time steps. For aesthetic reasons we use different characters in different levels.\nWaterCrossing-Ring is a newly designed, simple level in which an agent needs to fetch a randomly placed ring of levitation and float over a river to get to the goal (Fig. 10a). When a ring is picked up in our tasks, it is automatically worn7. The level is inspired by LavaCross-Levitate-Ring-PickUp from the MiniHack benchmark suite, where a river of deadly lava blocks the exit. However, we found that Dreamer struggles to learn this task, because of the early terminations when entering the lava.\nKeyRoom-Fixed-S5 is a benchmark task, in which an agent spawns in a room at a random position and has to fetch a randomly placed key to open a door and enter a smaller room with a randomly located exit (Fig. 10b). The door position is fixed. In all our tasks, using the key opens the door from any grid cell adjacent to the door, even diagonally.\nKeyCorridor-N is a novel task, in which an agent starts in front of a locked door in the top left corner of a corridor. In the bottom right corner of the corridor is the key for the door. We vary the length N of the corridor to systematically manipulate the task horizon.\nWandOfDeath-Advanced is based on the WandOfDeath benchmark tasks, in which an exit is guarded by a minotaur, which instantly kills the agent upon contact. The agent needs to pick up a wand to attack and kill the monster. Thereby, the agent needs to carefully select the direction of the attack, because if the attack bounces off a wall, it kills the agent instead. WandOfDeath comes in multiple levels of difficulty. WandOfDeath-Advanced (Fig. 10c) is a self-created level layout, designed to be more challenging that WandOfDeath-Medium but not as difficult as WandOfDeath-Hard. In WandOfDeath-Medium the agent can only walk horizontally and the location of the wand is fixed. In WandOfDeath-Hard the map is very large, which makes this a\n7Usually, to wear a ring in MiniHack a sequence of actions needs to be performed: PUTON \u2192 RING \u2192 RIGHT, for putting the ring on the right finger. We simplify this, by automatically applying the action sequence when the ring is picked up.\nhard exploration problem. Our version is of intermediate difficulty, where the number of accessible grids (28) is roughly the same as in WandOfDeath-Medium (27), while the randomly placed wand needs to be found first.\nRiver is a benchmark task, in which an agent needs get to an exit on the other side of a river (Fig. 10d). In order to cross the river the agent needs to push boulders into the water to form a small land bridge. To solve the task the agent needs to move at least two randomly placed boulders into the river.\nEscapeRoom is a difficult new problem designed by us, which combines the challenges of many other problems (Fig. 14a). Via EscapeRoom we test the ability to learn to execute a complex event sequence of five subgoals. Nonetheless, the task can be learned without extensive exploration or large action spaces. The agent starts in a small room and the goal is to unlock a door and escape. However, in order to get the key, the agent needs to (1.) pick up a ring of levitation and (2.) float over a small patch of water into a corridor. In the corridor the agent can (3.) exchange the ring of levitation for a key. In order to get back to the door in the first room, the agent needs to (4.) push a boulder into water. Aferwards, the agent can (5.) unlock the door and exit the room. While levitating, the agent is too light to push the boulder. In EscapeRoom, the agent can only carry one new item and picking up a second item results in dropping the first one."
        },
        {
            "heading": "E.2 MULTIWORLD",
            "text": "In Multiworld we use tasks that have previously been employed to study visual reinforcement learning (Nair et al., 2018; Pong et al., 2020). All tasks in Multiworld use different action spaces and camera viewpoints for their pixel-based observation, shown in Fig. 11. In Pusher the 2-dimensional actions control the x\u2212 and y\u2212movement of the endeffector, whereas the gripper is fixed. In Door the robot has a hook instead of a gripper at its endeffector and the 3-dimensional action controls x\u2212, y\u2212, and z\u2212movement. In PickUp the 3-dimensional action controls the y\u2212 and z\u2212movement and the gripper opening. We binarized the gripper opening, to prevent accidental object drops. In all tasks the goal positions are fixed. In Pusher and PickUp they are visible in the video frames. In Door the goal is to open the door fully. For Pusher-Dense and PickUp we compute the reward rt for every time step t as\nrt = 1\u2212 \u03b4t \u03b41 , (34)\nwhere \u03b4t is the euclidean distance between object and goal at time t. For Pusher-Sparse the agent received a reward of rt = 1 when the distance euclidean distance between puck and goal \u03b4t < 0.025, otherwise rt = 0. For Door the reward rt is the current angle of the door joint."
        },
        {
            "heading": "F EXTENDED RESULTS AND EXPERIMENT DETAILS",
            "text": ""
        },
        {
            "heading": "F.1 ANALYSIS OF CONTEXTS AND PREDICTIONS",
            "text": "In this section we provide further examples of high- and low-level predictions and context codes ct.\nC-RSSM predictions and contexts Figure 12 visualizes the low-level predictions for two example sequences. The low-level world model predicts the immediate next state and the reconstructions are more accurate than the abstract high-level predictions (cf. Fig. 5). Figure 13 displays four example sequences with the corresponding contexts ct and high-level predictions.\nHigh-level actions We analyze the high-level actions At in more detail for the EscapeRoom problem. EscapeRoom is a challenging MiniHack level, designed to contain diverse agentenvironment interactions, shown in Fig. 14a and described in detail in Suppl. E.1. To illustrate the emerging high-level action representations of THICK Dreamer, we show inputs it and image recon-\nstructions i\u0302c\u03c4(t)\u22121 and predicted low-level actions a\u0302\u03c4(t)\u22121 for all high-level actions At in Fig. 14b for one exemplary sequence.\nAt specific time steps the three possible high-level actions At encode particular agent-environment interactions: A1t encodes picking up the ring of levitation (t = 3) or exiting the level (t \u2208 {22, 26}). A2t encodes crossing the water after obtaining the ability to levitate (t \u2208 {6, 10, 16}), either upwards (t \u2208 {6, 10}) or downwards (t = 16) . A3t encodes pushing the boulder into water (t \u2208 {6, 10, 16}) or stepping in front of the door (t = 22). For all other time steps, the high-level actions produce either the identity predictions (e.g.A16) or predictions that seem to encode average scene settings (cf. A23 or A 1 10). These predictions account for unexpected context shifts, which can always occur with a small chance due to the stochasticity from sampling zt and the stochastic update gates of GateL0RD (see Suppl. D.2). The predicted low-level actions for these situations seem to be mostly random. The"
        },
        {
            "heading": "Pusher-Sparse Door",
            "text": "priorQ\u03d5 (red frames and text in Fig. 14b) typically samples reasonable high-level actions. However, occasionally the prior samples an action A\u0302t leading to an identity or average prediction (e.g. t = 6) due to the randomness of the process.\nQuantifying context changes To quantify the context changes, we plot the mean percentage of time steps when context changes occur (i.e., ct \u0338= ct\u22121) over the course of training in Fig. 15. Importantly, the context changes are somewhat consistent within the same task but, as expected, can vary across tasks of the same suite despite using the same hyperparameter \u03b2sparse. Additionally, we analyze the time between context changes for some MiniHack tasks. We plot the histogram of time gaps between context changes in Fig. 16 which illustrates that different tasks also show different distributions of context durations.\nTask-relevance of contexts Lastly, we analyze whether context changes occur at task-relevant situations for some MiniHack problems. For this we generate rollouts using the fully trained policy and identify points t\u2217, that we consider to be crucial for solving the task. For WandOfDeath-Adv., WaterCross-Ring, and KeyRoom-Fixed-S5 we take the time points t\u2217 before picking up an item. For EscapeRoom, we use points in time t\u2217 when the agent stands in front of a movable boulder blocking the path to the exit. We compute the mean percentage of context changes occurring around t\u2217 (\u00b11 step) over 10 sequences and take the average over all 7 randomly seeded models. The results are shown in Table 2. The C-RSSM tends to update its context with a high probability at the identified situations. This suggests that task-relevant aspects, such as item-pickups or boulder pushes, are encoded in the contexts."
        },
        {
            "heading": "F.2 MBRL IN MINIHACK: EXPERIMENT DETAILS AND EXTENDED RESULTS",
            "text": "In Fig. 17 we plot the success rate of THICK Dreamer, DreamerV2, and Director for additional MiniHack tasks not shown in the main paper.\nTo investigate the effect of task horizon, we compare the performance gain of THICK Dreamer over Dreamer for different corridor lengths in KeyCorridor. To compute improvements for every seeds we subtract the success rate and returns of Dreamer of THICK Dreamer (visualized in Fig. 7f\u2013 7g). For corridor lengths of 6 onward the improvements of THICK Dreamer over Dreamer tend\nto increase with corridor length. However, for a corridor length of 11 most runs fail to discover the reward (see Fig. 17g), which dampens the improvement in performance. This is due to inadequate exploration, which we simply took from Dreamer. Our results in VisualPinPad indicate that if exploration is addressed, then the performance gain of THICK Dreamer also holds for longer task horizons."
        },
        {
            "heading": "F.3 MBRL IN MINIHACK: DIRECTOR",
            "text": "Director (Hafner et al., 2022) shows strong performance in the KeyRoom-Fixed-S5 task. However, Director fails to learn the other MiniHack tasks that we considered Director (Fig. 7). Which crucial aspects are different across tasks and what causes Director to fail? We identify two key problems when applying Director in MiniHack, namely i) Director\u2019s reliance on diverse initial data and ii) problems with specifying unobservable goal information.\nDiversity of initial data Director trains a goal encoder on its replay buffer from which it samples to train a goal-conditioned policy. We hypothesize that if early in training not enough diverse data is collected this is reflected in the goal space. As a result, the manager (high-level) does not set meaningful goals for the worker (low-level) and learning is severely slowed down.\nWe analyze this aspect by training Director on variants of the KeyRoom-Fixed-S5 problem. Per default, the initial positions of agent, key, and goal within the second room are randomized in KeyRoom-Fixed-S5. We create additional variants of the task where either all entities spawn at fixed positions (positions shown in Fig. 19a) or only the initial position of the key is randomized. Additionally, we train Director in the KeyCorridor-4 task, which is very similar to KeyRoom-Fixed-S5 with fixed spawn points but of much smaller size (8 grid corridor vs. two rooms with 16 and 4 grids). Thus, in KeyCorridor-4 the observations show little diversity.\nFigure 19b shows evaluation success rates of Director in the KeyRoom-variants over environment steps. Director needs more steps to solve the tasks when entities spawn at fixed positions. Director does not learn to solve KeyCorridor-4 whereas with the same training it consistently learns to solve KeyRoom-Fixed-S5. Note that KeyCorridor-4 is much smaller and has a shorter task horizon. A similar tendency can be observed in the collected returns (Fig. 19c).\nThus, we conclude that diversity in the initial observations drastically boosts Director\u2019s performance. The ego-centric views of MiniHack often contain the same or similar observations especially when traversing long corridors or empty rooms, e.g. in KeyCorridor-8 or WandOfDeath-Advanced. This similarity in observations might impede Director\u2019s learning in the MiniHack tasks we consider here.\nUnobservable aspects of goals We hypothesize that a severe problem of Director could be specifying unobservable information in the goals. The RSSM encodes both observable as well as unobservable information within its deterministic latent state ht. If the unobservable information, e.g.\nitem pick-ups in MiniHack, does not directly affect rewards or substantially influence image reconstruction, it might be encoded only locally in ht and wash out over time. In Dreamer this is not a problem because the policy can amplify task-relevant information in ht. Director, however, compresses ht into a low-dimensional goal-encoding. Thereby, task-relevant latent information could get lost. Note that all novel tasks proposed in Hafner et al. (2022), in which Director shows very strong performance, avoid long-horizon memory, e.g. by coloring walls in a maze (Egocentric Ant Maze) or by providing a visible history of past button presses (Visual Pin Pad).\nIn smaller MiniHack tasks, e.g. KeyRoom-Fixed-S5, memory can sometimes be circumvented by specifying goals via observable information. For example, if both the key and door are visible a goal would be the same observation without the key visible (picked up) and an open door. This creates a curriculum, in which the worker can first learn from such simpler situations and later learn to pick up a key and open the door automatically from the high-level goal of an open door. In larger task spaces,e.g. KeyCorridor-8, Director never encounters such simpler situations to begin with."
        },
        {
            "heading": "F.4 MBRL IN VISUALPINPAD: EXPERIMENT DETAILS",
            "text": "For the Visual Pin Pad suite we generated offline training data to sidestep the challenge of discovering the very sparse rewards. For the data collection, we used Plan2Explore (Sekar et al., 2020) with the default settings of the DreamerV2 (Hafner et al., 2020) codebase. We trained two randomly initialized models of Plan2Explore for S \u2208 {0, 250k, 500k, 1M} environment steps in each task of the Visual Pin Pad suite. For each setting, we determined the model that achieved the highest overall returns during training.We initialized the replay buffer of all new models with the S samples.\nOriginally, Visual Pin Pad has more levels of difficulty. However, in VisualPinPadSix Plan2Explore did not receive any reward during 1M steps of exploration. Besides that, the results in Hafner et al. (2022) suggest that Dreamer is also not able to discover the very sparse rewards of VisualPinPadSix on its own. Thus, we omitted VisualPinPadSix and more complicated levels."
        },
        {
            "heading": "F.5 MBRL IN VISUALPINPAD: EFFECT OF EXPLORATION",
            "text": "We analyze the effect of exploration data by varying the number of data points with which we initialize the replay buffers. For this we consider exploration data collected by S \u2208 {0, 250k, 500k} environment steps of Plan2Explore and compare THICK Dreamer to Dreamer. In PinPadThree, Dreamer and THICK Dreamer always achieve the same performance regardless of exploration data available (cf. Fig. 20a, Fig. 20b, Fig. 20d). Without exploration data, neither THICK Dreamer nor\nDreamer manage to obtain rewards in PinPadFour and PinPadFive within 600k steps. Similarly, both methods do not discover rewards when initialized with 250k steps of exploration in PinPadFive. Thus, for the more complicated problems both THICK Dreamer as well as Dreamer need sufficient exploration. Whenever there is enough exploration data to learn the more complicated tasks, THICK Dreamer manages to achieve high rewards faster than Dreamer (see Fig. 20c, Fig. 20e, Fig. 20f).\nFor the larger problems, i.e. PinPadFour and PinPadFive , we quantify the effect of exploration data on sample efficiency by determining the number of environment steps needed to reach certain level of reward. We take 95% of the highest mean reward across all our experiments as a threshold. This corresponds to a mean reward of 359 for PinPadFour and 274 for PinPadFive. Table 3 shows the number of environment steps needed for THICK Dreamer and Dreamer reach this threshold for particular environments and amount of exploration data. In sum, a medium amount of exploration data (500k) enables reaching the threshold the fastest. THICK Dreamer reaches the reward threshold faster than Dreamer in all experiments. This advantage increases with the difficulty of the levels."
        },
        {
            "heading": "F.6 MPC: EXPERIMENT DETAILS AND EXTENDED RESULTS",
            "text": "To study zero-shot planning, we generated offline datasets for every task. For data collection, we use Plan2Explore in the same way as described in Suppl. F.4. After determining one dataset for every task, we train the models purely on this data.\nBesides boosting performance for long-horizon tasks, THICK PlaNet provides the additional advantage that the subgoals proposed by the high-level network, can directly be reconstructed into images through the low-level output heads oc\u03d5. The resulting goal images are easily interpretable by hu-\nmans. Figure 21 shows exemplary goals selected by the high-level planner in the first time step of an episode. Thus, the behavior of THICK PlaNet is much more explainable than simply performing MPC in a flat world model."
        },
        {
            "heading": "F.7 MBRL & MPC: SCALING MODELS VS. HIERARCHY",
            "text": "A alternative approach to adding hierarchies to world models is to improve performance by scaling up the model size (Hafner et al., 2023; Deng et al., 2023). Could simply increasing the capacity of the world model improve performance of Dreamer or PlaNet similar to our approach of incorporating hierarchical abstraction?\nWe investigate this in the WandOfDeath-Adv. task for MBRL and in Door for zero-shot MPC. We increase the RSSM model capacity by scaling up the number of hidden units per layer (256 before) and the dimensionality of the determinstic hidden state ht (256 before) by different factors (factors: S = 0.5,M = 1,L = 2,XL = 4). Unlike the model scaling in Hafner et al. (2023), we did not increase the dimensionality of zt since both investigated environments are visually rather simple. Figure 22 plots the returns of the different model sizes over environment steps. In both tasks increasing the model size did not improve Dreamer (Fig. 22a) or PlaNet (Fig. 22b). Thus, for the investigated setups scaling up model size does not bring the same advantages as our THICK hierarchy."
        },
        {
            "heading": "F.8 ABLATIONS AND HYPERPARAMETERS",
            "text": "Ablations We ablate various components of the C-RSSM and THICK Dreamer within a MBRL setup. We evaluate the resulting systems using the two exemplary tasks of MiniHack-WandOfDeath-Advances and VisualPinPadFour. Figure 23 plots the returns of the ablated systems over environment steps. Using the C-RSSM in DreamerV2 results in roughly the same performance (WandOfDeath-Advances) or slightly better performance (VisualPinPadFour) than using the RSSM (i.e. Dreamer). However, removing the deterministic latent state ht and the precise processing pathway from the C-RSSM (i.e. C-RSSM Dreamer without h) impedes the system from learning the tasks.8 Omitting v\u03be, and only using one critic v\u03c7 for both the short- and long-horizon returns (Eq. 20), slightly degrades the performance of THICK Dreamer.\nHyperparameter \u03b2sparse Next, we compare the effect of sparsity loss scale \u03b2sparse on THICK Dreamer in VisualPinPadFour and on THICK PlaNet in Multiworld-Door.\nFigure 24a plots the mean returns of THICK Dreamer or different values for \u03b2sparse in VisualPinPadFour. Figure 24b shows the percentage of time steps with context changes over\n8For this ablation we picked higher sparsity regularization \u03b2sparse for both tasks (\u03b2sparse = 50 for WandOfDeath-Advances, \u03b2sparse = 10 for VisualPinPadFour), such that the number of time steps with open gate roughly matches that of the C-RSSM.\ntraining. For THICK Dreamer, regularizing context changes too little is not as detrimental as overly regularizing context changes. If the contexts are weakly regularized, i.e. small \u03b2sparse, then the context changes in most time steps. As a result, the high-level learns an identity mapping, and during a temporal abstract prediction the network simply predicts the next state at time t + 1 (see Alg. 1). Stronger regularization boost sample efficiency of learning long-horizon behavior. This is even true, if at some point after the behavior is sufficiently learned, the context is no longer adapted (e.g. \u03b2sparse = 10). However, overly strong regularization, which prohibits context changes early during training, impedes learning the task (e.g. \u03b2sparse = 100). In this case, the high-level predictions are essentially average state predictions, which simply contributes noisy values for learning the critic. THICK Dreamer is very robust to the choice of \u03b2sparse in VisualPinPadFour.\nFigure 24c plots zero-shot planning performance of THICK PlaNet for different values for \u03b2sparse, with the percentage of context changes shown in Fig. 24d. For THICK PlaNet both too strong as well as too weak regularization degrade performance. However, strongly regularizing the network towards sparse context changes is slightly less detrimental for THICK PlaNet than a weak sparsity regularization (cf. \u03b2sparse = 100 and \u03b2sparse = 5). For weak sparsity regularization the context changes in every time step, which prevents the high level from finding a useful subgoal sequence during planning. As a result, the low-level might be guided into the wrong direction by the proposed subgoals.\nHyperparameter \u03c8 THICK Dreamer introduces a new hyperparameter \u03c8 which balances the influence of the short-horizon value estimates V \u03bb and long-horizon value estimates V long on the overall value V (Eq. 20). Figure 25 shows how \u03c8 affects task performance. Only considering short-horizon value estimates, i.e. \u03c8 = 1, results in less sample efficient learning than taking small amounts of long-horizon value estimates into consideration, i.e. \u03c8 = 0.9 for WandOfDeath-Advances and 0.8 \u2264 \u03c8 \u2264 0.9 for VisualPinPadFour. However, relying too strongly on long-horizon estimates, i.e. \u03c8 = 0.5, impedes policy learning. This effect is less pronounced for very long-horizon tasks such as VisualPinPadFour. We set \u03c8 = 0.9 in all experiments.\nHyperparameter \u03ba THICK PlaNet introduces the hyperparameter \u03ba, which scales the influence of the subgoal proximity on the reward estimate of the low-level planner (Eq. 22). We analyze the effect of \u03ba on THICK PlaNet\u2019s performance in Multiworld-Door, shown in Fig. 26. Incentivizing subgoal proximity too strongly, i.e. \u03ba = 1, can result in the agent getting stuck at a subgoal. This reduces overall task performance. Ignoring the subgoal, i.e. \u03ba = 0, also decreases performance for long-horizon tasks such as Door. In Door, THICK PlaNet works well across a wide range of \u03ba.\nReplanning strategy THICK PlaNet proposes new goals on the high-level upon context transitions. We do this mainly to save computational cost from running MCTS at the high-level at every time step. Figure 27 compares the effect of high-level planning in every time step to replanning upon context transitions in Door. The returns seem mostly the same. Thus, replanning at every time step is as effective as setting new subgoals only upon context transitions and can be applied if computational efficiency is not a concern."
        },
        {
            "heading": "G COMPUTATION AND CODE",
            "text": "All experiments were run on an internal computing cluster. Each experiment used one GPU. Experiments using DreamerV2 took roughly 15-20 hours, whereas THICK Dreamer experiments took around 35-45 hours of wall clock time, depending on the overall number of environment steps. Experiments with Director took around 40-60 hours of wall clock time. Zero-shot MPC experiments with PlaNet took at round 30-35 hours for 106 training steps, whereas THICK PlaNet took roughly 50-60 hours for the same number of training steps. The higher wall clock time for training THICK world models stems mainly from the larger number of trainable parameters and more detailed logging. Besides that, THICK PlaNet takes longer to evaluate, due to the additional computational cost of running MCTS on the high level during planning. The code to run the experiments is available over our website."
        }
    ],
    "year": 2023
}