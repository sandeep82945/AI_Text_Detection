{
    "abstractText": "Recent studies show that vision models pre-trained in generic visual learning tasks with large-scale data can provide useful feature representations for a wide range of visual perception problems. However, few attempts have been made to exploit pretrained foundation models in visual place recognition (VPR). Due to the inherent difference in training objectives and data between the tasks of model pre-training and VPR, how to bridge the gap and fully unleash the capability of pre-trained models for VPR is still a key issue to address. To this end, we propose a novel method to realize seamless adaptation of pre-trained models for VPR. Specifically, to obtain both global and local features that focus on salient landmarks for discriminating places, we design a hybrid adaptation method to achieve both global and local adaptation efficiently, in which only lightweight adapters are tuned without adjusting the pre-trained model. Besides, to guide effective adaptation, we propose a mutual nearest neighbor local feature loss, which ensures proper dense local features are produced for local matching and avoids time-consuming spatial verification in re-ranking. Experimental results show that our method outperforms the state-of-the-art methods with less training data and training time. Our method ranks 1st on the leaderboard of MSLS place recognition challenge, and uses about only 3% retrieval runtime of the two-stage VPR method with RANSAC-based spatial verification. The code will be publicly available.",
    "authors": [],
    "id": "SP:8248deed5c22df6b93e494a4c3ae1497ee8f266d",
    "references": [
        {
            "authors": [
                "Amar Ali-bey",
                "Brahim Chaib-draa",
                "Philippe Gigu\u00e8re"
            ],
            "title": "Gsv-cities: Toward appropriate supervised visual place",
            "venue": "recognition. Neurocomputing,",
            "year": 2022
        },
        {
            "authors": [
                "Amar Ali-Bey",
                "Brahim Chaib-Draa",
                "Philippe Giguere"
            ],
            "title": "Mixvpr: Feature mixing for visual place recognition",
            "venue": "In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision,",
            "year": 2023
        },
        {
            "authors": [
                "Adrien Angeli",
                "David Filliat",
                "St\u00e9phane Doncieux",
                "Jean-Arcady Meyer"
            ],
            "title": "Fast and incremental method for loop-closure detection using bags of visual words",
            "venue": "IEEE transactions on robotics,",
            "year": 2008
        },
        {
            "authors": [
                "Relja Arandjelovic",
                "Petr Gronat",
                "Akihiko Torii",
                "Tomas Pajdla",
                "Josef Sivic"
            ],
            "title": "Netvlad: Cnn architecture for weakly supervised place recognition",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Herbert Bay",
                "Andreas Ess",
                "Tinne Tuytelaars",
                "Luc Van Gool"
            ],
            "title": "Speeded-up robust features (surf)",
            "venue": "Computer vision and image understanding,",
            "year": 2008
        },
        {
            "authors": [
                "Gabriele Berton",
                "Carlo Masone",
                "Valerio Paolicelli",
                "Barbara Caputo"
            ],
            "title": "Viewpoint invariant dense matching for visual geolocalization",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Gabriele Berton",
                "Carlo Masone",
                "Barbara Caputo"
            ],
            "title": "Rethinking visual geo-localization for largescale applications",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Gabriele Berton",
                "Riccardo Mereu",
                "Gabriele Trivigno",
                "Carlo Masone",
                "Gabriela Csurka",
                "Torsten Sattler",
                "Barbara Caputo"
            ],
            "title": "Deep visual geo-localization benchmark",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Bingyi Cao",
                "Andre Araujo",
                "Jack Sim"
            ],
            "title": "Unifying deep local and global features for image search",
            "venue": "In European Conference on Computer Vision,",
            "year": 2020
        },
        {
            "authors": [
                "Mathilde Caron",
                "Hugo Touvron",
                "Ishan Misra",
                "Herv\u00e9 J\u00e9gou",
                "Julien Mairal",
                "Piotr Bojanowski",
                "Armand Joulin"
            ],
            "title": "Emerging properties in self-supervised vision transformers",
            "venue": "In Proceedings of the IEEE/CVF international conference on computer vision,",
            "year": 2021
        },
        {
            "authors": [
                "Shoufa Chen",
                "Chongjian Ge",
                "Zhan Tong",
                "Jiangliu Wang",
                "Yibing Song",
                "Jue Wang",
                "Ping Luo"
            ],
            "title": "Adaptformer: Adapting vision transformers for scalable visual recognition",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Zetao Chen",
                "Adam Jacobson",
                "Niko S\u00fcnderhauf",
                "Ben Upcroft",
                "Lingqiao Liu",
                "Chunhua Shen",
                "Ian Reid",
                "Michael Milford"
            ],
            "title": "Deep learning features at scale for visual place recognition",
            "venue": "IEEE international conference on robotics and automation (ICRA),",
            "year": 2017
        },
        {
            "authors": [
                "Zetao Chen",
                "Fabiola Maffra",
                "Inkyu Sa",
                "Margarita Chli"
            ],
            "title": "Only look once, mining distinctive landmarks from convnet for visual place recognition",
            "venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),",
            "year": 2017
        },
        {
            "authors": [
                "Jia Deng",
                "Wei Dong",
                "Richard Socher",
                "Li-Jia Li",
                "Kai Li",
                "Fei-Fei Li"
            ],
            "title": "Imagenet: A large-scale hierarchical image database",
            "venue": "In IEEE conference on computer vision and pattern recognition,",
            "year": 2009
        },
        {
            "authors": [
                "Daniel DeTone",
                "Tomasz Malisiewicz",
                "Andrew Rabinovich"
            ],
            "title": "Superpoint: Self-supervised interest point detection and description",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition workshops,",
            "year": 2018
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "Lucas Beyer",
                "Alexander Kolesnikov",
                "Dirk Weissenborn",
                "Xiaohua Zhai",
                "Thomas Unterthiner",
                "Mostafa Dehghani",
                "Matthias Minderer",
                "Georg Heigold",
                "Sylvain Gelly"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Martin A Fischler",
                "Robert C Bolles"
            ],
            "title": "Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography",
            "venue": "Communications of the ACM,",
            "year": 1981
        },
        {
            "authors": [
                "Sourav Garg",
                "Michael Milford"
            ],
            "title": "Seqnet: Learning descriptors for sequence-based hierarchical place recognition",
            "venue": "IEEE Robotics and Automation Letters,",
            "year": 2021
        },
        {
            "authors": [
                "Sourav Garg",
                "Adam Jacobson",
                "Swagat Kumar",
                "Michael Milford"
            ],
            "title": "Improving condition-and environment-invariant place recognition with semantic place categorization",
            "venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),",
            "year": 2017
        },
        {
            "authors": [
                "Sourav Garg",
                "Niko Suenderhauf",
                "Michael Milford"
            ],
            "title": "Don\u2019t look back: Robustifying place categorization for viewpoint-and condition-invariant place recognition",
            "venue": "IEEE International Conference on Robotics and Automation (ICRA),",
            "year": 2018
        },
        {
            "authors": [
                "Yixiao Ge",
                "Haibo Wang",
                "Feng Zhu",
                "Rui Zhao",
                "Hongsheng Li"
            ],
            "title": "Self-supervising fine-grained region similarities for large-scale image localization",
            "venue": "In European conference on computer vision,",
            "year": 2020
        },
        {
            "authors": [
                "Arren J Glover",
                "William P Maddern",
                "Michael J Milford",
                "Gordon F Wyeth"
            ],
            "title": "Fab-map+ ratslam: Appearance-based slam for multiple times of day",
            "venue": "In 2010 IEEE international conference on robotics and automation,",
            "year": 2010
        },
        {
            "authors": [
                "Stephen Hausler",
                "Michael Milford"
            ],
            "title": "Hierarchical multi-process fusion for visual place recognition",
            "venue": "IEEE International Conference on Robotics and Automation (ICRA),",
            "year": 2020
        },
        {
            "authors": [
                "Stephen Hausler",
                "Sourav Garg",
                "Ming Xu",
                "Michael Milford",
                "Tobias Fischer"
            ],
            "title": "Patch-netvlad: Multi-scale fusion of locally-global descriptors for place recognition",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Neil Houlsby",
                "Andrei Giurgiu",
                "Stanislaw Jastrzebski",
                "Bruna Morrone",
                "Quentin De Laroussilhe",
                "Andrea Gesmundo",
                "Mona Attariyan",
                "Sylvain Gelly"
            ],
            "title": "Parameter-efficient transfer learning for nlp",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Edward J Hu",
                "Yelong Shen",
                "Phillip Wallis",
                "Zeyuan Allen-Zhu",
                "Yuanzhi Li",
                "Shean Wang",
                "Lu Wang",
                "Weizhu Chen"
            ],
            "title": "Lora: Low-rank adaptation of large language models",
            "venue": "arXiv preprint arXiv:2106.09685,",
            "year": 2021
        },
        {
            "authors": [
                "Herv\u00e9 J\u00e9gou",
                "Matthijs Douze",
                "Cordelia Schmid",
                "Patrick P\u00e9rez"
            ],
            "title": "Aggregating local descriptors into a compact image representation",
            "venue": "In 2010 IEEE computer society conference on computer vision and pattern recognition,",
            "year": 2010
        },
        {
            "authors": [
                "Shibo Jie",
                "Zhi-Hong Deng"
            ],
            "title": "Convolutional bypasses are better vision transformer adapters",
            "venue": "arXiv preprint arXiv:2207.07039,",
            "year": 2022
        },
        {
            "authors": [
                "Hyo Jin Kim",
                "Enrique Dunn",
                "Jan-Michael Frahm"
            ],
            "title": "Learned contextual feature reweighting for image geo-localization",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Nikhil Keetha",
                "Avneesh Mishra",
                "Jay Karhade",
                "Krishna Murthy Jatavallabhula",
                "Sebastian Scherer",
                "Madhava Krishna",
                "Sourav Garg"
            ],
            "title": "Anyloc: Towards universal visual place recognition",
            "venue": "arXiv preprint arXiv:2308.00688,",
            "year": 2023
        },
        {
            "authors": [
                "Ahmad Khaliq",
                "Shoaib Ehsan",
                "Zetao Chen",
                "Michael Milford",
                "Klaus McDonald-Maier"
            ],
            "title": "A holistic visual place recognition approach using lightweight cnns for significant viewpoint and appearance changes",
            "venue": "IEEE transactions on robotics,",
            "year": 2019
        },
        {
            "authors": [
                "Zaid Khan",
                "Yun Fu"
            ],
            "title": "Contrastive alignment of vision to language through parameter-efficient transfer learning",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Brian Lester",
                "Rami Al-Rfou",
                "Noah Constant"
            ],
            "title": "The power of scale for parameter-efficient prompt tuning",
            "venue": "arXiv preprint arXiv:2104.08691,",
            "year": 2021
        },
        {
            "authors": [
                "Mar\u0131\u0301a Leyva-Vallina",
                "Nicola Strisciuglio",
                "Nicolai Petkov"
            ],
            "title": "Generalized contrastive optimization of siamese networks for place recognition",
            "venue": "arXiv preprint arXiv:2103.06638,",
            "year": 2021
        },
        {
            "authors": [
                "Zhengqi Li",
                "Noah Snavely"
            ],
            "title": "Megadepth: Learning single-view depth prediction from internet photos",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Liu Liu",
                "Hongdong Li",
                "Yuchao Dai"
            ],
            "title": "Stochastic attraction-repulsion embedding for large scale image localization",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2019
        },
        {
            "authors": [
                "Stephanie Lowry",
                "Henrik Andreasson"
            ],
            "title": "Lightweight, viewpoint-invariant visual place recognition in changing environments",
            "venue": "IEEE Robotics and Automation Letters,",
            "year": 2018
        },
        {
            "authors": [
                "Stephanie Lowry",
                "Niko S\u00fcnderhauf",
                "Paul Newman",
                "John J Leonard",
                "David Cox",
                "Peter Corke",
                "Michael J Milford"
            ],
            "title": "Visual place recognition: A survey",
            "venue": "ieee transactions on robotics,",
            "year": 2015
        },
        {
            "authors": [
                "Feng Lu",
                "Lijun Zhang",
                "Shuting Dong",
                "Baifan Chen",
                "Chun Yuan"
            ],
            "title": "Aanet: Aggregation and alignment network with semi-hard positive sample mining for hierarchical place recognition",
            "venue": "IEEE International Conference on Robotics and Automation (ICRA),",
            "year": 2023
        },
        {
            "authors": [
                "Sven Middelberg",
                "Torsten Sattler",
                "Ole Untzelmann",
                "Leif Kobbelt"
            ],
            "title": "Scalable 6-dof localization on mobile devices",
            "venue": "In Computer Vision\u2013ECCV 2014: 13th European Conference,",
            "year": 2014
        },
        {
            "authors": [
                "Tayyab Naseer",
                "Gabriel L Oliveira",
                "Thomas Brox",
                "Wolfram Burgard"
            ],
            "title": "Semantics-aware visual localization under challenging perceptual conditions",
            "venue": "IEEE International Conference on Robotics and Automation (ICRA),",
            "year": 2017
        },
        {
            "authors": [
                "Daniel Olid",
                "Jos\u00e9 M F\u00e1cil",
                "Javier Civera"
            ],
            "title": "Single-view place recognition under seasonal changes",
            "venue": "arXiv preprint arXiv:1808.06516,",
            "year": 2018
        },
        {
            "authors": [
                "Maxime Oquab",
                "Timoth\u00e9e Darcet",
                "Th\u00e9o Moutakanni",
                "Huy Vo",
                "Marc Szafraniec",
                "Vasil Khalidov",
                "Pierre Fernandez",
                "Daniel Haziza",
                "Francisco Massa",
                "Alaaeldin El-Nouby"
            ],
            "title": "Dinov2: Learning robust visual features without supervision",
            "venue": "arXiv preprint arXiv:2304.07193,",
            "year": 2023
        },
        {
            "authors": [
                "Junting Pan",
                "Ziyi Lin",
                "Xiatian Zhu",
                "Jing Shao",
                "Hongsheng Li"
            ],
            "title": "St-adapter: Parameter-efficient image-to-video transfer learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Jungin Park",
                "Jiyoung Lee",
                "Kwanghoon Sohn"
            ],
            "title": "Dual-path adaptation from image to video transformers",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Guohao Peng",
                "Jun Zhang",
                "Heshan Li",
                "Danwei Wang"
            ],
            "title": "Attentional pyramid pooling of salient visual residuals for place recognition",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Filip Radenovi\u0107",
                "Giorgos Tolias",
                "Ond\u0159ej Chum"
            ],
            "title": "Fine-tuning cnn image retrieval with no human annotation",
            "venue": "IEEE transactions on pattern analysis and machine intelligence,",
            "year": 2018
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "venue": "In International conference on machine learning,",
            "year": 2021
        },
        {
            "authors": [
                "Paul-Edouard Sarlin",
                "Daniel DeTone",
                "Tomasz Malisiewicz",
                "Andrew Rabinovich"
            ],
            "title": "Superglue: Learning feature matching with graph neural networks",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Yanqing Shen",
                "Ruotong Wang",
                "Weiliang Zuo",
                "Nanning Zheng"
            ],
            "title": "Tcl: Tightly coupled learning strategy for weakly supervised hierarchical place recognition",
            "venue": "IEEE Robotics and Automation Letters,",
            "year": 2022
        },
        {
            "authors": [
                "Yanqing Shen",
                "Sanping Zhou",
                "Jingwen Fu",
                "Ruotong Wang",
                "Shitao Chen",
                "Nanning Zheng"
            ],
            "title": "Structvpr: Distill structural knowledge with weighting samples for visual place recognition",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Niko S\u00fcnderhauf",
                "Sareh Shirazi",
                "Feras Dayoub",
                "Ben Upcroft",
                "Michael Milford"
            ],
            "title": "On the performance of convnet features for place recognition",
            "venue": "In 2015 IEEE/RSJ international conference on intelligent robots and systems (IROS),",
            "year": 2015
        },
        {
            "authors": [
                "Akihiko Torii",
                "Josef Sivic",
                "Tomas Pajdla",
                "Masatoshi Okutomi"
            ],
            "title": "Visual place recognition with repetitive structures",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2013
        },
        {
            "authors": [
                "Akihiko Torii",
                "Relja Arandjelovic",
                "Josef Sivic",
                "Masatoshi Okutomi",
                "Tomas Pajdla"
            ],
            "title": "24/7 place recognition by view synthesis",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2015
        },
        {
            "authors": [
                "Ruotong Wang",
                "Yanqing Shen",
                "Weiliang Zuo",
                "Sanping Zhou",
                "Nanning Zheng"
            ],
            "title": "Transvpr: Transformer-based place recognition with multi-level attention aggregation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Wenhui Wang",
                "Hangbo Bao",
                "Li Dong",
                "Johan Bjorck",
                "Zhiliang Peng",
                "Qiang Liu",
                "Kriti Aggarwal",
                "Owais Khan Mohammed",
                "Saksham Singhal",
                "Subhojit Som"
            ],
            "title": "Image as a foreign language: Beit pretraining for all vision and vision-language tasks",
            "venue": "arXiv preprint arXiv:2208.10442,",
            "year": 2022
        },
        {
            "authors": [
                "Frederik Warburg",
                "Soren Hauberg",
                "Manuel Lopez-Antequera",
                "Pau Gargallo",
                "Yubin Kuang",
                "Javier Civera"
            ],
            "title": "Mapillary street-level sequences: A dataset for lifelong place recognition",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Zhe Xin",
                "Yinghao Cai",
                "Tao Lu",
                "Xiaoxia Xing",
                "Shaojun Cai",
                "Jixiang Zhang",
                "Yiping Yang",
                "Yanqing Wang"
            ],
            "title": "Localizing discriminative visual landmarks for place recognition",
            "venue": "In 2019 International conference on robotics and automation (ICRA),",
            "year": 2019
        },
        {
            "authors": [
                "Mengde Xu",
                "Zheng Zhang",
                "Fangyun Wei",
                "Han Hu",
                "Xiang Bai"
            ],
            "title": "Side adapter network for openvocabulary semantic segmentation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Ming Xu",
                "Niko Snderhauf",
                "Michael Milford"
            ],
            "title": "Probabilistic visual place recognition for hierarchical localization",
            "venue": "IEEE Robotics and Automation Letters,",
            "year": 2020
        },
        {
            "authors": [
                "Taojiannan Yang",
                "Yi Zhu",
                "Yusheng Xie",
                "Aston Zhang",
                "Chen Chen",
                "Mu Li"
            ],
            "title": "Aim: Adapting image models for efficient video action",
            "year": 2023
        },
        {
            "authors": [
                "Peng Yin",
                "Lingyun Xu",
                "Xueqian Li",
                "Chen Yin",
                "Yingli Li",
                "Rangaprasad Arun Srivatsan",
                "Lu Li",
                "Jianmin Ji",
                "Yuqing He"
            ],
            "title": "A multi-domain feature learning method for visual place recognition",
            "venue": "In 2019 International Conference on Robotics and Automation (ICRA),",
            "year": 2019
        },
        {
            "authors": [
                "Jun Yu",
                "Chaoyang Zhu",
                "Jian Zhang",
                "Qingming Huang",
                "Dacheng Tao"
            ],
            "title": "Spatial pyramid-enhanced netvlad with weighted triplet loss for place recognition",
            "venue": "IEEE transactions on neural networks and learning systems,",
            "year": 2019
        },
        {
            "authors": [
                "Lu Yuan",
                "Dongdong Chen",
                "Yi-Ling Chen",
                "Noel Codella",
                "Xiyang Dai",
                "Jianfeng Gao",
                "Houdong Hu",
                "Xuedong Huang",
                "Boxin Li",
                "Chunyuan Li"
            ],
            "title": "Florence: A new foundation model for computer vision",
            "venue": "arXiv preprint arXiv:2111.11432,",
            "year": 2021
        },
        {
            "authors": [
                "Sijie Zhu",
                "Linjie Yang",
                "Chen Chen",
                "Mubarak Shah",
                "Xiaohui Shen",
                "Heng Wang"
            ],
            "title": "R2former: Unified retrieval and reranking transformer for place recognition",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Wang"
            ],
            "title": "2022a), we use the dataset partition first presented in (Olid et al., 2018",
            "year": 2018
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Visual place recognition (VPR), also known as image localization (Liu et al., 2019) or visual geolocalization (Berton et al., 2022b), aims at coarsely estimating the location of a query place image by searching for its best match from a database of geo-tagged images. VPR has long been studied in robotics and computer vision communities, motivated by its wide applications in mobile robot localization (Xu et al., 2020) and augmented reality (Middelberg et al., 2014), etc. The main challenges of the VPR task include condition (e.g., illumination and weather) changes, viewpoint changes, and perceptual aliasing (Lowry et al., 2015) (hard to differentiate similar images from different places).\nThe VPR task is typically addressed by using image retrieval and matching approaches (Arandjelovic et al., 2016; Cao et al., 2020) with global or/and local descriptors to represent images. The aggregation algorithms like VLAD (Je\u0301gou et al., 2010; Lowry & Andreasson, 2018; Khaliq et al., 2019) are usually used to aggregate/pool local features into a vector as the global feature. Such compact global features facilitate fast place retrieval and are robust against viewpoint variations. However, these global features neglect spatial information, making VPR methods based on them prone to perceptual aliasing. A promising solution (Cao et al., 2020; Hausler et al., 2021; Wang et al., 2022a), i.e., two-stage VPR, is to retrieve top-k candidate results in the database using global features, then re-rank these candidates by matching local features. Moreover, VPR model training follows the \u201cpre-training then finetuning\u201d paradigm. Most VPR models are initialized using model parameters pre-trained on ImageNet (Deng et al., 2009) and fine-tuned on the VPR datasets, such as MSLS (Warburg et al., 2020). As models and training datasets continue to expand, the training becomes more costly in both computation and memory footprint.\nRecently, foundation models (Radford et al., 2021; Yuan et al., 2021; Oquab et al., 2023) achieved remarkable performance on many computer vision tasks given their ability to produce well-generalized representations. However, the image representation produced by the pre-trained model is susceptible\nto useless (even harmful) dynamic objects (e.g. pedestrians and vehicles), and tends to ignore some static discriminative backgrounds (e.g. buildings and vegetation), as shown in Fig. 1 (b). A robust VPR model should focus on the static discriminative landmarks (Chen et al., 2017b) rather than the dynamic foreground. This results in a gap between pre-training and VPR tasks. Meanwhile, full fine-tuning the foundation model on downstream datasets might forget previously learned knowledge and damage the excellent transferability, i.e., catastrophic forgetting. An effective method to address this issue is parameter-efficient transfer learning (Houlsby et al., 2019; Lester et al., 2021), which has not been studied in the VPR area. Besides, most foundation models do not directly produce local features, which is typically required in the re-ranking of two-stage VPR methods.\nIn this paper, we propose a novel method to realize Seamless adaptation of pre-trained foundation models for the VPR task, named SelaVPR. By adding a few tunable lightweight adapters to the frozen pre-trained model, we achieve an efficient hybrid global-local adaptation to get both global features for retrieving candidate places and local features for re-ranking. Specifically, the global adaptation is achieved by adding adapters after the multi-head attention layer and in parallel to the MLP layer in each transformer block. The local adaptation is implemented by adding up-convolutional layers after the entire transformer backbone to upsample the feature map. Additionally, we propose a mutual nearest neighbor local feature loss, which can be combined with the commonly used triplet loss to optimize the network. The proposed SelaVPR feature representation focuses on the discriminative landmarks, which is critical to identifying places. Furthermore, we can directly match the local features without spatial verification, making the re-ranking much faster than mainstream two-stage VPR methods. Our main contributions are highlighted as follows:\n1) We propose a hybrid global-local adaptation method to seamlessly adapt pre-trained foundation models to produce both global and local features for the VPR task. The proposed SelaVPR feature representation can focus on discriminative landmarks and ignore the regions irrelevant to distinguishing places, thus closing the gap between the pre-training and VPR tasks.\n2) We also propose a mutual nearest neighbor local feature loss to train the local adaptation module, which is combined with global feature loss for fine-tuning. The obtained local features can be directly used in cross-matching for re-ranking, without time-consuming geometric verification.\n3) Our method outperforms state-of-the-art methods on several VPR benchmarks (ranks 1st on MSLS challenge leaderboard) using less training data and training time. And it only consumes 3% retrieval runtime of the mainstream two-stage methods with RANSAC-based geometric verification."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Visual Place Recognition: The traditional VPR approaches perform nearest neighbor search using global features to find the most similar place. The global features are commonly produced using aggregation algorithms, such as Bag of Words (Angeli et al., 2008) and VLAD (Je\u0301gou et al., 2010), to process the hand-crafted features like SURF (Bay et al., 2008). As the advancement of deep learning techniques, many VPR approaches (Su\u0308nderhauf et al., 2015; Arandjelovic et al., 2016; Jin Kim et al., 2017; Chen et al., 2017a;b; Naseer et al., 2017; Garg et al., 2017; 2018; Xin et al., 2019; Yin et al., 2019; Leyva-Vallina et al., 2021; Ali-Bey et al., 2023) employed a variety of deep features for the VPR task. Some works integrated the aggregation methods into neural networks\n(Arandjelovic et al., 2016; Yu et al., 2019; Peng et al., 2021), and improved training strategies (Ge et al., 2020; Berton et al., 2022a; Ali-bey et al., 2022), to achieve better performance. Nevertheless, most one-stage (i.e. global retrieval) approaches are prone to perceptual aliasing due to the use of aggregated features while neglecting spatial information. One recent work (Keetha et al., 2023) also used pre-trained foundation models for VPR and got promising results. However, this work did not perform any fine-tuning, making it difficult to fully unleash the capability of these models for VPR.\nRecently, the two-stage (i.e. hierarchical) VPR methods (Hausler & Milford, 2020; Garg & Milford, 2021; Hausler et al., 2021; Berton et al., 2021; Shen et al., 2022; Wang et al., 2022a; Lu et al., 2023; Shen et al., 2023; Zhu et al., 2023) have become popular. These approaches typically retrieved top-k candidate images over the whole database using compact global feature representation (such as NetVLAD (Arandjelovic et al., 2016) or Generalized Mean (GeM) pooling (Radenovic\u0301 et al., 2018)), then re-ranked candidates by performing local matching between the query image and each candidate using local descriptors. However, most of these methods required geometric consistency verification after local matching (Cao et al., 2020; Hausler et al., 2021; Wang et al., 2022a) or taking into account spatial constraints during matching (Berton et al., 2021; Shen et al., 2022; Zhu et al., 2023), which greatly increases the runtime burden. The recent visual foundation model (Oquab et al., 2023) has shown an excellent ability to match similar semantic patch-level features across domains. In this work, we attempt to match local features produced by the foundation model for reranking without spatial verification, thereby significantly speeding up the retrieval process in VPR.\nParameter-efficient Transfer Learning: Recent work (Radford et al., 2021; Yuan et al., 2021; Caron et al., 2021; Wang et al., 2022b; Oquab et al., 2023) demonstrated the visual foundation model can produce powerful feature representation and achieve excellent performance on multiple tasks. These works commonly trained the ViT (Dosovitskiy et al., 2020) model or its variants with large quantities of parameters on huge amounts of data. The parameter-efficient transfer learning (PETL) (Houlsby et al., 2019; Lester et al., 2021; Hu et al., 2021), first proposed in natural language processing, is an effective way to adapt foundation models to various downstream tasks, which can reduce training computation costs and avoid catastrophic forgetting. The main PETL methods fall broadly into two categories: adding task-specific adapters (Houlsby et al., 2019), and prompt tuning (Lester et al., 2021). We follow the former in this work. Although several adapter-based approaches (Jie & Deng, 2022; Chen et al., 2022; Pan et al., 2022; Yang et al., 2023; Khan & Fu, 2023; Xu et al., 2023; Park et al., 2023) have been proposed to perform various computer vision tasks, to the best of our knowledge, this work is among the first to use the hybrid global-local adaptation to produce both global features and local features, and apply them to address the challenges in VPR."
        },
        {
            "heading": "3 PROPOSED METHOD",
            "text": "This section describes the proposed SelaVPR for two-stage VPR. We first introduce ViT and its use to produce place image representation. Then, we propose the global adaptation, local adaptation, and local matching re-ranking to achieve two-stage VPR. Finally, we present the loss for fine-tuning."
        },
        {
            "heading": "3.1 PRELIMINARY",
            "text": "The Vision Transformer (ViT) and its variants have proven to be powerful for a variety of computer vision tasks including VPR. In this work, we adapt the ViT-based pre-trained foundation model DINOv2 (Oquab et al., 2023) for VPR, so here we give a brief overview of ViT.\nGiven an input image, ViT first slices it into N patches and linearly projects them to D-dim patch embeddings xp \u2208 RN\u00d7D, then prepends a learnable [class] token to xp as x0 = [xclass;xp] \u2208 R(N+1)\u00d7D. After adding positional embeddings to preserve the positional information, x0 is fed into a series of transformer blocks to produce the feature representation. A standard transformer block mainly includes multi-head attention (MHA), multi-layer perceptron ( MLP), and LayerNormalization (LN) layers, as shown in Fig. 2 (a). For the input token sequence, its change process passing through a transformer block is: The MHA is first applied to compute attentional features, then MLP is utilized to realize the feature nonlinearization and dimension transformation. It is formalized as:\nx\u2032l = MHA(LN(xl\u22121)) + xl\u22121 (1)\nxl = MLP(LN(x\u2032l)) + x \u2032 l (2)\nwhere xl\u22121 and xl are the output of the (l \u2212 1)-th and l-th transformer block. For the feature map output by CNN models, a common practice of the two-stage VPR method is to use NetVLAD or GeM to aggregate it into a global feature for candidate retrieval and also treat it as dense local (patch-level) features for re-ranking. For the ViT model, the output consists of one class token and N patch tokens, where the class token can be directly used as the global feature to represent places. Meanwhile, N patch tokens can also be reshaped as a feature map (similar to CNN) to restore spatial position. In this work, instead of using the class token as the global feature, we use GeM to pool the feature map into the global feature. The reason is explained in Appendix B."
        },
        {
            "heading": "3.2 GLOBAL ADAPTATION",
            "text": "Although pre-trained foundation models are capable of powerful feature representation, direct use of them in VPR cannot fully unleash their capability due to the gap between pre-training and VPR tasks. To address it, we introduce the global adaptation to adapt the pre-trained model so that the feature representation can focus on the static discriminative regions that are beneficial to VPR.\nInspired by previous adapter-based parameter-efficient fine-tuning works (Houlsby et al., 2019; Chen et al., 2022; Yang et al., 2023), we design our global adaptation as shown in Fig. 2 (d). Specifically, we add two adapters in each transformer block. Each adapter is a bottleneck module, which first uses the fully connected layer to down-project the input to a smaller dimension, then applies a ReLU activation and up-projects it back to the original dimension. The first adapter is a serial adapter that is added after the MHA layer and has a skip-connection internally. The second adapter is a parallel adapter that is connected in parallel to the MLP layer multiplied by a scaling factor s. The computation of each global adapted transformer block can be denoted as\nx\u2032l = Adapter1(MHA(LN(xl\u22121))) + xl\u22121 (3) xl = MLP(LN(x\u2032l)) + s \u00b7 Adapter2(LN(x\u2032l)) + x\u2032l. (4)\nThe output of the last transformer block is fed into an LN layer as the final output of the entire global adapted ViT backbone. We discard the class token and reshape patch tokens as the produced feature map fm. The L2-normalized GeM global feature used to retrieve candidates can be denoted as\nfg = L2(GeM(fm)) (5)\nThe global adapted foundation model can produce feature representations that focus on discriminative landmarks and ignore dynamic interference. This bridges the gap between pre-training and VPR tasks, and greatly boosts the performance of visual foundation models in the VPR task."
        },
        {
            "heading": "3.3 LOCAL ADAPTATION",
            "text": "Two-stage VPR methods typically match dense local features to re-rank candidate places for boosting performance. In the above introduction, we have obtained the feature map output by global\nadapted ViT backbone, which can also be regarded as coarse-grained (16\u00d716) patch-level features. However, in order to achieve appreciable performance in local matching, more fine-grained dense local features are required. To achieve this, we propose the local adaptation, which is achieved by an up-sampling module after the ViT backbone (as shown in Fig .3). To be specific, this module consists of two up-convolutional (up-conv) layers and a ReLU layer in the middle. The height and width of the feature map will approximately double after passing through each up-conv layer, while the channel dimension will be reduced. Finally, this module adjusts the 16\u00d716\u00d71024 feature map output by ViT (ViT-L/14) to 61\u00d761\u00d7128, and performs L2 normalization in the channel dimension (intraL2) to yield dense local features, i.e., a dense 61\u00d761 grid of 128-dim local features f l. Formally, it can be represented as\nf l = LocalAdaptation(fm) = intraL2(up-conv2(ReLU(up-conv1(fm)))) (6)"
        },
        {
            "heading": "3.4 LOCAL MATCHING FOR RE-RANKING",
            "text": "Our two-stage place retrieval pipeline is shown in Fig 3. After obtaining the global features and local features, we first compute L2 distance to perform the similarity search in the global feature space over the database to get the top-k most similar candidate images. For the local features matching between the query image q and a candidate image c, we search for mutual nearest neighbor matches by cross-matching. Since local features are L2 normalized, the inner product equivalent to cosine similarity is used to measure local feature similarity. That is\nsqc(i, j) = f l q(i) Tf lc(j) i, j \u2208 N, (7) where f lq(i) is the i-th local feature in query image q, f l c(j) is the j-th local feature in candidate image c, and sqc(i, j) is the local feature similarity between them. The mutual nearest neighbor matches set M is defined as\nM = {(u, v) : u = argmax i sqc(i, v), v = argmax j sqc(u, j)}. (8)\nThat is, u-th feature in image q and v-th feature in image c are the best matches for each other. The obtained nearest neighbor matches set in previous VPR work (Hausler et al., 2021; Wang et al., 2022a) exists a large number of false matches. So they apply geometric verification (e.g. RANSAC (Fischler & Bolles, 1981)) to remove outliers (i.e. false matches) and use the number of inliers as the image similarity score, which is time-consuming. Due to the powerful representation ability of the foundation model and the superiority of the ViT in capturing long-distance feature dependencies, the number of false matches in this work is actually not enough to affect the performance of re-ranking. So we directly use the number of matches (i.e., |M|) as the image similarity score for re-ranking."
        },
        {
            "heading": "3.5 LOSS",
            "text": "For the loss designed to optimize the model to produce global features (denoted as global loss Lg), we follow the triplet loss used in the previous works (Arandjelovic et al., 2016; Wang et al., 2022a):\nLg = \u2211 j l(\u2225fgq \u2212 fgp \u2225+m\u2212 \u2225fgq \u2212 fgnj\u2225), (9)\nwhere l(x) = max(x, 0), i.e. hinge loss. m is the margin. fgq , f g p , and f g nj are the global features of query, positive, and hard negative samples, respectively.\nHowever, what we use to measure image similarity in local matching re-ranking is the number of mutual matches, which is a discrete integer. It is intractable to directly optimize a loss function of discrete integer variables within a deep learning model because of its non-differentiability. So we compromise to optimize the network so that the resulting mutual matching local features are more similar, and design a mutual nearest neighbor local feature loss Ll as\nLl = \u2211 j l(\u2212 \u2211 (u,v)\u2208M sqp(u, v) |M| + \u2211 (u\u2032,v\u2032)\u2208M\u2032 sqnj (u \u2032, v\u2032) |M\u2032| ). (10)\nThis local loss maximizes the average local feature similarity in the mutual matches set M of the query image and positive image, and minimizes that in the matches set M\u2032 of the query and negative images. It makes the produced local features more suitable for local matching. We obtain the final loss L through combining the global loss Lg and local loss Ll by weight \u03bb as\nL = Lg + \u03bbLl (11)"
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "4.1 DATASETS AND PERFORMANCE EVALUATION",
            "text": "Several VPR benchmark datasets mainly including Tokyo24/7, MSLS, and Pitts30k are used in our experiments. Table 1 summarizes their main information. Tokyo24/7 (Torii et al., 2015) includes about 76k database images and 315 query images captured from urban scenes with drastic illumination changes. Mapillary Street-Level Sequences (MSLS) (Warburg et al., 2020) consists of more than 1.6 mil-\nlion images collected in urban, suburban and natural scenes over 7 years. We assess models on both MSLS-val and MSLS-challenge (an online test set without released labels) sets. Pittsburgh (Pitts30k) (Torii et al., 2013) contains 30k reference images and 24k query images in the train, val and test sets, and exists severe viewpoint changes. More details are in Appendix H.\nWe evaluate the recognition performance using Recall@N (R@N), which is the percentage of queries for which at least one of the N retrieved images is the right result. The threshold is set to 25 meters and 40\u25e6 for MSLS, 25 meters for Tokyo24/7 and Pitts30k, following standard evaluation procedure (Warburg et al., 2020; Arandjelovic et al., 2016)."
        },
        {
            "heading": "4.2 IMPLEMENTATION DETAILS",
            "text": "We use the DINOv2 based on ViT-L/14 as the foundation model and conduct all experiments on an NVIDIA GeForce RTX 3090 GPU using PyTorch. Fed a 224\u00d7224 image, the model produces a 1024-dim global feature and a dense grid of 128-dim local features. The bottleneck ratio of the adapters in ViT blocks is 0.5 and the scaling factor s in Eq. 4 is set to 0.2. We use 3\u00d73 up-conv with stride=2 and padding=1 in the local adaptation module. The output channels of the first and second up-conv layers are 256 and 128, respectively. Following other two-stage methods, we re-rank the top-100 candidates to yield final results. We train our models using the Adam optimizer with the learning rate set as 0.00001 and batch size set as 4. When the R@5 on the validation set does not have improvement within 3 epochs, the training is terminated. For MSLS, we set an epoch as passing 30k queries, whereas Pitts30k is passing 5k queries. In model training, we define the potential positive images as the reference images that are within 10 meters from the query image, while the definite negative images are those further than 25 meters. Two hard negative images from 1000 randomly chosen definite negatives are used in the triplet loss. We empirically set the margin m = 0.1 in Eq. 9, the weight \u03bb = 1 in Eq. 11. For the experiments in Section 4.3, we fine-tune our models on MSLS-train to test on MSLS-val/challenge, and further fine-tune them on Pitts30k-train to test on Pitts30k-test and Tokyo24/7 (as in R2Former (Zhu et al., 2023)). For the experiment in other sections, the models tested on Pitts30k-test and Tokyo24/7 are directly fine-tuned on Pitts30k-train.\nTable 2: Comparison to state-of-the-art methods on benchmark datasets. The best is highlighted in bold and the second is underlined.\nMethod ExtractionTime (s) Matching Time (s) Total Time (s)\nSP-SuperGlue 0.042 6.639 6.681 Patch-NetVLAD-s 0.186 0.551 0.737 Patch-NetVLAD-p 0.412 10.732 11.144 TransVPR 0.008 3.010 3.018 SelaVPR (ours) 0.027 0.085 0.112 Total Runtime (s)\nR ec\nal l@\n1 (%\n)\nFigure 4: R@1-runtime comparison."
        },
        {
            "heading": "4.3 COMPARISONS WITH STATE-OF-THE-ART METHODS",
            "text": "In this section, we compare the proposed SelaVPR method with several state-of-the-art (SOTA) VPR methods, including four one-stage methods using global feature retrieval: NetVLAD (Arandjelovic et al., 2016), SFRS (Ge et al., 2020), CosPlace (Berton et al., 2022a) and MixVPR (Ali-Bey et al., 2023), as well as five two-stage methods with re-ranking: SP-SuperGlue (DeTone et al., 2018; Sarlin et al., 2020), Patch-NetVLAD (Hausler et al., 2021), TransVPR (Wang et al., 2022a), StructVPR (Shen et al., 2023) and R2Former (Zhu et al., 2023). The details of these methods are in Appendix I. Note that CosPlace and MixVPR are trained on individually constructed large-scale datasets. TransVPR and R2Former are transformer-based methods. R2Former ranked first in the leaderboard of the MSLS place recognition challenge before our method was proposed. We also show the global retrieval result without re-ranking using the proposed SelaVPR, and denote it as SelaVPR(global). The quantitative results are shown in Table 2. Since the code for StructVPR and R2Former has not yet been released, we use the results reported in their paper. The proposed method achieves the best R@1/R@5/R@10 on all datasets.\nWhen using only global features for direct retrieval, SelaVPR(global) significantly outperforms other one-stage methods on all datasets for R@5 and R@10, including SFRS using complex training strategies and CosPlace/MixVPR trained on purpose-built large-scale datasets. SelaVPR(global) also outperforms all other two-stage methods on Tokyo24/7, MSLS-val, and MSLS-challenge for R@5 and R@10. This fully demonstrates that adapting the foundation model can provide powerful feature representation, which is a novel way to achieve SOTA one-stage VPR. Although SelaVPR(global) does not achieve very good R@1 on Tokyo24/7 where the lighting changes drastically, the complete SelaVPR method outperforms other methods by a large margin after local feature re-ranking. This illustrates the necessity of using local matching (local adaptation) for VPR in extreme environments. The complete SelaVPR method significantly outperforms SOTA methods on Tokyo24/7 and Pitts30k with absolute R@1 improvement of 5.4% and 1.3% respectively. Mean-\nQuery SelaVPR (Ours) NetVLAD SFRS CosPlace Patch-NetVLAD TransVPR\nPerceptual aliasing\nViewpoint changes\nDynamic objects\nLight changes\nLight changes\nPerceptual aliasing\nViewpoint changes\nFigure 5: Qualitative results. In these challenging examples (containing condition changes, viewpoint changes, dynamic objects, etc.), the proposed SelaVPR successfully returns the right database images, while all other methods produce incorrect results.\nwhile, it also ranks 1st on the leaderboard of MSLS place recognition challenge (see Appendix J). Comparisons to SOTA methods on more datasets are in Appendix D (SelaVPR achieves nearperfect results on St. Lucia). Fig. 5 qualitatively demonstrates that our approach is highly robust in challenging scenes. Benefiting from the visual foundation model and sensible adaptation, SelaVPR achieves absolute performance advantages on a variety of datasets without complex training strategies or purpose-built large-scale training datasets.\nEfficiency is another metric for evaluating the performance of VPR methods. In Table 3, we compare the runtime (feature extraction time and matching/retrieval time) of our method with other two-stage methods (with released code) on Pitts30k-test. Patch-NetVLAD-p and TransVPR are representative two-stage methods using RANSAC for spatial verification in re-ranking. SP-SuperGlue uses neural networks to match local features. TransVPR is fast at extracting features, while SelaVPR is slower (but faster than other methods) due to the use of the ViT/L backbone. However, for feature matching, since our method does not require time-consuming spatial verification, its runtime is less than 3% of TransVPR and only about 1% of SP-SuperGlue and Patch-NetVLAD-p. The total runtime of our method is less than 4% of TransVPR. Although Patch-NetVLAD-s uses a Rapid Spatial Scoring method for fast verification, it is still significantly slower than ours. Fig. 4 simultaneously shows the total runtime and R@1 on Pitts30k. Due to the absolute advantages in both performance and efficiency, our SelaVPR method is able to pave the way for real-world large-scale VPR applications."
        },
        {
            "heading": "4.4 ABLATION STUDY",
            "text": "In this section, we perform a series of ablation experiments to demonstrate the necessity of finetuning and verify the effectiveness of the proposed global adaptation and local adaptation:\n\u2022 DINOv2-GeM: Using the pre-trained DINOv2 backbone (freeze parameter) and GeM pooling, i.e., baseline. \u2022 Tuned-DINOv2-GeM: full fine-tuned DINOv2-GeM. \u2022 Global-Adaptation: Global adapted DINOv2-GeM with our global adapation. \u2022 Local-Adaptation: Using the DINOv2-GeM (freeze parameter) to retrieve candidates, and\nadding the local adaptation module after it to produce local features for re-ranking. \u2022 SelaVPR: The complete (hybrid global-local adaptation) method.\nThe results are shown in Table 4. Note that we directly use the model fine-tuned on Pitts30k-train to test on Pitts30k and Tokyo24/7 in this section. The results are slightly lower than that in Table 2 (but still SOTA).\nThe pre-trained DINOv2-GeM achieves decent results on Pitts30k, which has few dynamic objects on the place image. The performance is improved after full fine-tuning (i.e. Tuned-DINOv2-GeM), which indicates that even without dynamic interference, it is necessary to make the produced feature representation more suitable for VPR task through fine-tuning. However, Tuned-DINOv2-GeM performs worse than DINOv2-GeM on Tokyo24/7. It is because there is a generalization gap be-\ntween Tokyo24/7 and Pitts30k-train, and full fine-tuning damages the excellent transferability of the pre-trained foundation model. Compared with Tuned-DINOv2-GeM, Global-Adaptation only tunes a few newly added adapters (with backbone frozen), which reduces the training consumption and always improves performance (retaining the generalization ability of the foundation model). Global-Adaptation achieves absolute R@1 improvement of 10.8% on Tokyo24/7. Besides, LocalAdaptation only adds a local adaptation module (with the proposed local loss for training) after the frozen DINOv2 to produce local features for re-ranking. It can also obviously improve performance, especially on Tokyo24/7 (20.6% absolute R@1 improvement), which shows day-night changes. The complete SelaVPR method combines the advantages of Glocal-Adaptation and Local-Adaptation, and achieves absolute R@1 improvement of 10% on Pitts30k and 26.3% on Tokyo24/7.\nMSLS is a dataset with many dynamic objects (e.g. vehicles and pedestrians), on which the pretrained DINOv2-GeM gets terrible results. By adding a local adaptation module after DINOv2 (Local-Adaptation) to get dense local features for re-ranking, we can get 22.1% absolute performance improvement for R@1, which is still not ideal. Using full fine-tuning can even bring significant performance improvement (34.6% for R@1), and the improvement yielded by GlobalAdaptation is more obvious (42.3% for R@1). More importantly, the complete SelaVPR method achieves 2 \u00d7 higher R@1.\nOverall, full fine-tuning the pre-trained foundation model on these datasets usually results in better performance (unless there is a domain gap between the training and test set), indicating that the first problem should be solved when applying a foundation model to the VPR task is to make the output feature representation suitable for VPR (focusing on regions that help differentiate places), i.e. bridge the gap between pre-training and VPR tasks. The second is the catastrophic forgetting that will be encountered in fine-tuning the pre-trained models, which can be addressed by parameterefficient fine-tuning instead of full fine-tuning. Additionally, the local matching for re-ranking is also necessary, especially on datasets that show drastic condition variations (e.g. Tokyo24/7). The proposed global adaptation and local adaptation can work together to solve these problems well.\nMore experiments and analyses are in the Appendix, e.g., the comparisons of tunable parameters, data efficiency, and training time."
        },
        {
            "heading": "5 CONCLUSIONS",
            "text": "In this paper, we introduced a novel hybrid adaptation method to seamlessly adapt pre-trained foundation models for the VPR task, which is composed of the global adaptation and local adaptation. The feature representation produced by the adapted foundation model is more focused on discriminative landmarks to differentiate places, thus bridging the gap between pre-training and VPR tasks. The local adaptation enables the model to output proper dense local features, which can be used for local matching re-ranking in two-stage VPR to greatly boost performance. The experimental results demonstrated that the proposed SelaVPR method outperforms previous SOTA methods on VPR benchmark datasets (with a large margin on Tokyo24/7 and Pitts30k). Meanwhile, since our approach eliminates the reliance on time-consuming spatial validation in re-ranking, it costs only 3% retrieval time of RANSAC-based two-stage methods. We believe that the proposed SelaVPR method provides a promising way to address the VPR task in real-world large-scale applications."
        },
        {
            "heading": "A COMPARISONS OF TUNABLE PARAMETERS, DATA EFFICIENCY, AND TRAINING TIME",
            "text": "In this section, we evaluate the tunable parameters, data efficiency, and training time of our model. We use the ResNet50 with a GeM pooling in the benchmark implementation (Berton et al., 2022b) (ResNet50-GeM) and the full fine-tuning DINOv2-GeM (Tuned-DINOv2-GeM) as reference. This experiment is conducted on Pitts30k and the results are shown in Table 5. The tunable parameters of our SelaVPR are only about 1/6 of full fine-tuning DINOv2 (Tuned-DINOv2-GeM). Although our model has more tunable parameters than the ResNet50-GeM (CNN backbone), we require significantly less training data and time, and achieve a 9.1% higher R@1. Note that one epoch only contains a part of the training set (5000 triples). To further study the data efficiency of our model, we train it with only 2000 triples (i.e., 0.4 epoch) and denote it as SelaVPR*. It still achieves SOTA performance."
        },
        {
            "heading": "B PERFORMANCE OF DIFFERENT GLOBAL FEATURES",
            "text": "This section compares the performance of using class token or GeM as global features in our SelaVPR method. As shown in Table 6, the performance of direct retrieval (without re-ranking) using class token as the global feature, i.e. SelaVPR-cls(global), is better than that of GeM. However, SelaVPR-GeM(re-rank) is better than SelaVPR-cls(re-rank) after re-ranking using local features. That is, the improvement of SelaVPR-cls after re-ranking is significantly lower than that of SelaVPR-GeM, and the performance of SelaVPR-cls after re-ranking is even decreased on MSLSval. This shows that when we use local features in conjunction with global features, GeM is better compatible with local features, whereas class token is not. Therefore we finally choose GeM as the global feature in SelaVPR."
        },
        {
            "heading": "C ADDITIONAL ABLATION EXPERIMENTS FOR THE GLOBAL ADAPTATION",
            "text": "In this section, we further conduct ablation experiments to study the effect of reducing the number of tunable parameters in the global adaptation. Table 7 shows the results of using different adapters, i.e. only serial adapter, only parallel adapter, or both adapters. Using both adapters achieves the best results overall, but we can still get good performance with just either adapter. Table 8 shows the results of setting different bottleneck ratios in each adapter. Setting the ratio to 0.5 gets the best results overall, but we can still achieve good performance with a 0.25 bottleneck ratio. These experimental findings show that promising results can be obtained even if the number of tunable\nparameters in the global adaptation is reduced. We can choose appropriate settings in the global adaptation according to our needs."
        },
        {
            "heading": "D COMPARISONS ON MORE DATASETS",
            "text": "In this section, we compare the proposed method with other two-stage methods (with released code) on the Nordland and St. Lucia datasets. As shown in Table 9, our method surpasses all other methods even without re-ranking, i.e., SelaVPR(global). The SelaVPR outperforms other methods by a wide margin and achieves near-perfect results on St. Lucia (99.8% R@1 and 100% R@5)."
        },
        {
            "heading": "E QUALITATIVE RESULTS OF LOCAL MATCHING",
            "text": "We further illustrate that our method makes the produced features by foundation models more suitable for local matching in this section, and show the qualitative local matching results of our SelaVPR method and the pre-trained DINOv2 in Fig. 6. To make a fair comparison, the feature maps output by the ViT backbone of the two models are used for local matching. Our SelaVPR gets more correct matches (i.e. matching point pairs) than the pre-trained DINOv2 between two images from the same place. For two images from different places, all local matches are wrong and we expect as few matches as possible. SelaVPR produces fewer wrong matches than pre-trained DINOv2."
        },
        {
            "heading": "F ADDITIONAL ATTENTION VISUALIZATION",
            "text": "Fig. 1 in the main paper shows that the feature representation of our method can precisely focus on image regions that are helpful for place recognition. Here, Fig. 7 demonstrates more challenging examples. Compared to the pre-trained model, which is disturbed by objects such as dynamic foreground, our method always pays attention to all discriminative regions (buildings and vegetation)."
        },
        {
            "heading": "G ADDITIONAL QUALITATIVE RESULTS",
            "text": "Fig. 5 in the main paper has shown a small number of qualitative results. Here, Fig. 8, Fig. 9 and Fig. 10 show more qualitative results on Tokyo24/7, MSLS, and Pitts30k, respectively. These examples show challenging cases such as severe condition changes, viewpoint changes, dynamic interference, and only small regions of discriminative landmarks or almost no landmarks. Our method obtains correct results, while other methods produce incorrect results."
        },
        {
            "heading": "H DATASET DETAILS",
            "text": "Tokyo24/7 (Torii et al., 2015). The Tokyo24/7 dataset includes 75984 database images and 315 query images captured from urban scenes. The query images are selected from 1125 images taken at 125 distinct places with 3 different viewpoints and at 3 different times of day, and mainly exist viewpoint changes and drastic condition changes (day-night changes).\nMapillary Street-Level Sequences (MSLS) (Warburg et al., 2020). The MSLS dataset is a largescale VPR dataset containing over 1.6 million images labeled with GPS coordinates and compass angles, captured from 30 cities in urban, suburban, and natural scenes over seven years. It covers various challenging visual changes due to illumination, weather, season, viewpoint, as well as\nInput images Results of DINOv2 Results of ours\ndynamic objects, and includes subsets of training, public validation (MSLS-val), and withheld test (MSLS-challenge). Following several related works (Hausler et al., 2021; Wang et al., 2022a; Zhu et al., 2023), the MSLS-val and MSLS-challenge sets are used to evaluate models.\nPittsburgh (Torii et al., 2013). The Pittsburgh dataset is collected from Google Street View panoramas, and provides 24 images with different viewpoints at each place. The images in this dataset exist large viewpoint variations and moderate condition variations. We use Pitts30k in our experiments, which is a subset of Pitts250k (but more difficult than Pitts250k) and contains 10k database images each in the training, validation, and test sets.\nNordland (Olid et al., 2018). The Nordland dataset primarily consists of suburban and natural place images, captured from the same viewpoint in the front of a train across four seasons, which allows the images to show severe condition (e.g., season and light) changes but no viewpoint variations. Its ground truth is provided by the frame-level correspondence. Following the previous works (Olid et al., 2018; Wang et al., 2022a), we use the dataset partition first presented in (Olid et al., 2018) for our experiments. The summer (reference) and winter (query) images of the down-sampled version (224\u00d7224) of the test set (3450 images per sequence) are adopted to evaluate models. St. Lucia (Glover et al., 2010; Berton et al., 2022b). The St. Lucia dataset comprises ten video sequences captured from the same suburban roadway in Brisbane. Following visual geo-localization benchmark (Berton et al., 2022b), the first and last sequences are used as the reference and query data in our experiments, and only one image is selected every 5 meters. Thus, the reference and query sequence include 1549 and 1464 images, respectively.\nI IMPLEMENTATION DETAILS OF THE COMPARED METHOD\nNetVLAD (Arandjelovic et al., 2016). NetVLAD is a classic one-stage VPR method with a differentiable VLAD layer that can be integrated into neural networks. We use the pytorch implementation1 with the released VGG16 model trained on the Pitts30k dataset.\nSFRS (Ge et al., 2020). To train a more robust NetVLAD-based model, this work utilizes selfsupervised image-to-region similarities to mine hard positive samples. The official implementation2 with the model trained on Pitts30k is used in our experiments.\nCosPlace (Berton et al., 2022a). This work introduces an extra large-scale dataset SF-XL and applies the training technique proposed in the classification task to train the VPR models. The official VGG16 model3 is used for comparisons.\nMixVPR (Ali-Bey et al., 2023). This work presents a novel holistic feature aggregation method that takes feature maps from pre-trained backbones as global features, and uses a stack of Feature-Mixer to iteratively incorporate global relationships into each individual feature map. MixVPR is a SOTA one-stage VPR method. We use the official implementation4 and the best configuration (ResNet50 with 4096-dim output features) for comparisons.\nSP-SuperGlue (DeTone et al., 2018; Sarlin et al., 2020). Hausler et al. (2021) first used this pipeline for the VPR task in the PatchNetVLAD work. SP-SuperGlue first retrieves candidate images using NetVLAD. Then, the SuperGlue (Sarlin et al., 2020) (a feature matcher based on graph neural network) is used to match SuperPoint (DeTone et al., 2018) features for re-ranking. The official implementation5 with the model trained on MegaDepth (Li & Snavely, 2018) is adopted.\nPatch-NetVLAD (Hausler et al., 2021). This approach also retrieves candidates with NetVLAD, then re-ranks the candidates using the NetVLAD-based multi-scale patch-level features. We follow the official implementation6 and use the speed-focused and performance-focused configurations for evaluation. The model trained on Pitts30k-train is tested on Pitts30k-test and Tokyo24/7, and the model trained on MSLS is tested on other datasets.\nTransVPR (Wang et al., 2022a). This work first achieves candidate retrieval using the global features produced by integrating multi-level attentions from vision transformer, then utilizes an attention mask to filter feature maps to get key-patch descriptors for re-ranking. Based on the official\n1https://github.com/Nanne/pytorch-NetVlad 2https://github.com/yxgeee/OpenIBL 3https://github.com/gmberton/CosPlace 4https://github.com/amaralibey/MixVPR 5https://github.com/magicleap/SuperGluePretrainedNetwork 6https://github.com/QVPR/Patch-NetVLAD\nimplementation7, the model trained on Pitts30k-train is evaluated on Pitts30k-test and Tokyo24/7, while that trained on MSLS is tested on others.\nStructVPR (Shen et al., 2023). To improve feature stability in a changing environment, StructVPR utilizes the segmentation images to enhance structural knowledge in global features and applies knowledge distillation to avoid online segmentation in testing. This method is combined with SPSuperGlue to form a two-stage VPR method, and achieves great performance. Since the code is not released, we use the results reported in the original paper.\nR2Former (Zhu et al., 2023). This work addresses both the retrieval and re-ranking in two-stage VPR using a novel transformer model. Its re-ranking module takes feature correlation, attention value, and coordinates into account, and does not require RANSAC-based geometric verification for re-ranking. R2Former is the SOTA two-stage VPR method. It ranked first in the MSLS challenge leaderboard before our method was proposed. Since the code is not released, we use the results reported in the original paper."
        },
        {
            "heading": "J THE SNAPSHOT OF MSLS LEADERBOARD",
            "text": "MSLS place recognition challenge 8 (Hausler et al., 2021; Zhu et al., 2023) is an authoritative competition for VPR with over 100 participants. Fig. 11 shows the snapshot of the MSLS challenge leaderboard at the time of submission. The proposed SelaVPR method (named \u201canonymous02\u201d for double-blind policy) ranks 1st.\n7https://github.com/RuotongWANG/TransVPR-model-implementation 8https://codalab.lisn.upsaclay.fr/competitions/865"
        }
    ],
    "year": 2023
}