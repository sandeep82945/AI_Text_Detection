{
    "abstractText": "Figure 1: We propose Texture UV Radiance Fields (TUVF) to render a 3D consistent texture given a 3D object shape input. TUVF provides a category-level texture representation disentangled from 3D shapes. Top three rows: TUVF can synthesize realistic textures by training from a collection of single-view images; Fourth row: Given a 3D shape input, we can render different textures on top by using different texture codes; Bottom row: We can perform editing on a given texture (adding a flag of France) and directly apply the same texture on different 3D shapes without further fine-tuning. Note that all samples are rendered under 1024\u00d71024 resolution; zoom-in is recommended. ABSTRACT",
    "authors": [],
    "id": "SP:adc214472ee4e9c2581eab8da866d40900ae52b2",
    "references": [
        {
            "authors": [
                "Ivan Anokhin",
                "Kirill Demochkin",
                "Taras Khakhulin",
                "Gleb Sterkin",
                "Victor Lempitsky",
                "Denis Korzhenkov"
            ],
            "title": "Image generators with conditionally-independent pixel synthesis",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Dejan Azinovi\u0107",
                "Ricardo Martin-Brualla",
                "Dan B Goldman",
                "Matthias Nie\u00dfner",
                "Justus Thies"
            ],
            "title": "Neural rgb-d surface reconstruction",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Chong Bao",
                "Yinda Zhang",
                "Bangbang Yang",
                "Tianxing Fan",
                "Zesong Yang",
                "Hujun Bao",
                "Guofeng Zhang",
                "Zhaopeng Cui"
            ],
            "title": "Sine: Semantic-driven image-based nerf editing with prior-guided editing field",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Jonathan T Barron",
                "Ben Mildenhall",
                "Matthew Tancik",
                "Peter Hedman",
                "Ricardo Martin-Brualla",
                "Pratul P Srinivasan"
            ],
            "title": "Mip-nerf: A multiscale representation for anti-aliasing neural radiance fields",
            "venue": "In IEEE International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Jonathan T. Barron",
                "Ben Mildenhall",
                "Dor Verbin",
                "Pratul P. Srinivasan",
                "Peter Hedman"
            ],
            "title": "Zip-nerf: Anti-aliased grid-based neural radiance fields",
            "venue": "In IEEE International Conference on Computer Vision,",
            "year": 2023
        },
        {
            "authors": [
                "Pravin Bhat",
                "Stephen Ingram",
                "Greg Turk"
            ],
            "title": "Geometric texture synthesis by example",
            "venue": "In SGP, pp",
            "year": 2004
        },
        {
            "authors": [
                "Federica Bogo",
                "Javier Romero",
                "Gerard Pons-Moll",
                "Michael J Black"
            ],
            "title": "Dynamic faust: Registering human bodies in motion",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Gunilla Borgefors"
            ],
            "title": "Hierarchical chamfer matching: A parametric edge matching algorithm",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 1988
        },
        {
            "authors": [
                "Rohan Chabra",
                "Jan E Lenssen",
                "Eddy Ilg",
                "Tanner Schmidt",
                "Julian Straub",
                "Steven Lovegrove",
                "Richard Newcombe"
            ],
            "title": "Deep local shapes: Learning local sdf priors for detailed 3d reconstruction",
            "venue": "In European Conference on Computer Vision,",
            "year": 2020
        },
        {
            "authors": [
                "Eric R Chan",
                "Marco Monteiro",
                "Petr Kellnhofer",
                "Jiajun Wu",
                "Gordon Wetzstein"
            ],
            "title": "pi-gan: Periodic implicit generative adversarial networks for 3d-aware image synthesis",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Eric R Chan",
                "Connor Z Lin",
                "Matthew A Chan",
                "Koki Nagano",
                "Boxiao Pan",
                "Shalini De Mello",
                "Orazio Gallo",
                "Leonidas J Guibas",
                "Jonathan Tremblay",
                "Sameh Khamis"
            ],
            "title": "Efficient geometry-aware 3d generative adversarial networks",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Angel X Chang",
                "Thomas Funkhouser",
                "Leonidas Guibas",
                "Pat Hanrahan",
                "Qixing Huang",
                "Zimo Li",
                "Silvio Savarese",
                "Manolis Savva",
                "Shuran Song",
                "Hao Su"
            ],
            "title": "Shapenet: An information-rich 3d model repository",
            "venue": "arXiv preprint,",
            "year": 2015
        },
        {
            "authors": [
                "Anpei Chen",
                "Zexiang Xu",
                "Fuqiang Zhao",
                "Xiaoshuai Zhang",
                "Fanbo Xiang",
                "Jingyi Yu",
                "Hao Su"
            ],
            "title": "Mvsnerf: Fast generalizable radiance field reconstruction from multi-view stereo",
            "venue": "In IEEE International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Dave Zhenyu Chen",
                "Yawar Siddiqui",
                "Hsin-Ying Lee",
                "Sergey Tulyakov",
                "Matthias Nie\u00dfner"
            ],
            "title": "Text2tex: Text-driven texture synthesis via diffusion models",
            "venue": "arXiv preprint,",
            "year": 2023
        },
        {
            "authors": [
                "Xi Chen",
                "Yan Duan",
                "Rein Houthooft",
                "John Schulman",
                "Ilya Sutskever",
                "Pieter Abbeel"
            ],
            "title": "Infogan: Interpretable representation learning by information maximizing generative adversarial nets",
            "venue": "Neural Information Processing Systems,",
            "year": 2016
        },
        {
            "authors": [
                "Zhiqin Chen",
                "Kangxue Yin",
                "Sanja Fidler"
            ],
            "title": "Auv-net: Learning aligned uv maps for texture transfer and synthesis",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "An-Chieh Cheng",
                "Xueting Li",
                "Min Sun",
                "Ming-Hsuan Yang",
                "Sifei Liu"
            ],
            "title": "Learning 3d dense correspondence via canonical point autoencoder",
            "venue": "Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "An-Chieh Cheng",
                "Xueting Li",
                "Sifei Liu",
                "Min Sun",
                "Ming-Hsuan Yang"
            ],
            "title": "Autoregressive 3d shape generation via canonical mapping",
            "venue": "In European Conference on Computer Vision,",
            "year": 2022
        },
        {
            "authors": [
                "Yen-Chi Cheng",
                "Hsin-Ying Lee",
                "Sergey Tulyakov",
                "Alexander G Schwing",
                "Liang-Yan Gui"
            ],
            "title": "Sdfusion: Multimodal 3d shape completion, reconstruction, and generation",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "George R Cross",
                "Anil K Jain"
            ],
            "title": "Markov random field texture models",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 1983
        },
        {
            "authors": [
                "Angela Dai",
                "Yawar Siddiqui",
                "Justus Thies",
                "Julien Valentin",
                "Matthias Nie\u00dfner"
            ],
            "title": "Spsg: Selfsupervised photometric scene generation from rgb-d scans",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Vincent Dumoulin",
                "Ishmael Belghazi",
                "Ben Poole",
                "Olivier Mastropietro",
                "Alex Lamb",
                "Martin Arjovsky",
                "Aaron Courville"
            ],
            "title": "Adversarially learned inference",
            "venue": "arXiv preprint,",
            "year": 2016
        },
        {
            "authors": [
                "Alexei A Efros",
                "Thomas K Leung"
            ],
            "title": "Texture synthesis by non-parametric sampling",
            "venue": "In IEEE International Conference on Computer Vision,",
            "year": 1999
        },
        {
            "authors": [
                "Simone Foti",
                "Bongjin Koo",
                "Danail Stoyanov",
                "Matthew J Clarkson"
            ],
            "title": "3d shape variational autoencoder latent disentanglement via mini-batch feature swapping for bodies and faces",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Jun Gao",
                "Tianchang Shen",
                "Zian Wang",
                "Wenzheng Chen",
                "Kangxue Yin",
                "Daiqing Li",
                "Or Litany",
                "Zan Gojcic",
                "Sanja Fidler"
            ],
            "title": "Get3d: A generative model of high quality 3d textured shapes learned from images",
            "venue": "In Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Ian Goodfellow",
                "Jean Pouget-Abadie",
                "Mehdi Mirza",
                "Bing Xu",
                "David Warde-Farley",
                "Sherjil Ozair",
                "Aaron Courville",
                "Yoshua Bengio"
            ],
            "title": "Generative adversarial networks",
            "venue": "In Neural Information Processing Systems,",
            "year": 2014
        },
        {
            "authors": [
                "Tao Hu",
                "Shu Liu",
                "Yilun Chen",
                "Tiancheng Shen",
                "Jiaya Jia"
            ],
            "title": "Efficientnerf efficient neural radiance fields",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Xun Huang",
                "Ming-Yu Liu",
                "Serge Belongie",
                "Jan Kautz"
            ],
            "title": "Multimodal unsupervised image-to-image translation",
            "venue": "In European Conference on Computer Vision,",
            "year": 2018
        },
        {
            "authors": [
                "Ananya Harsh Jha",
                "Saket Anand",
                "Maneesh Singh",
                "VSR Veeravasarapu"
            ],
            "title": "Disentangling factors of variation with cycle-consistent variational auto-encoders",
            "venue": "In European Conference on Computer Vision,",
            "year": 2018
        },
        {
            "authors": [
                "Tero Karras",
                "Samuli Laine",
                "Timo Aila"
            ],
            "title": "A style-based generator architecture for generative adversarial networks",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Tero Karras",
                "Miika Aittala",
                "Janne Hellsten",
                "Samuli Laine",
                "Jaakko Lehtinen",
                "Timo Aila"
            ],
            "title": "Training generative adversarial networks with limited data",
            "venue": "Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Tero Karras",
                "Samuli Laine",
                "Miika Aittala",
                "Janne Hellsten",
                "Jaakko Lehtinen",
                "Timo Aila"
            ],
            "title": "Analyzing and improving the image quality of stylegan",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Max Welling"
            ],
            "title": "Auto-encoding variational bayes",
            "venue": "arXiv preprint,",
            "year": 2013
        },
        {
            "authors": [
                "Johannes Kopf",
                "Chi-Wing Fu",
                "Daniel Cohen-Or",
                "Oliver Deussen",
                "Dani Lischinski",
                "Tien-Tsin Wong"
            ],
            "title": "Solid texture synthesis from 2d exemplars",
            "venue": "In ACM SIGGRAPH,",
            "year": 2007
        },
        {
            "authors": [
                "Tejas D Kulkarni",
                "William F Whitney",
                "Pushmeet Kohli",
                "Josh Tenenbaum"
            ],
            "title": "Deep convolutional inverse graphics network",
            "venue": "Neural Information Processing Systems,",
            "year": 2015
        },
        {
            "authors": [
                "Jiaxin Li",
                "Zijian Feng",
                "Qi She",
                "Henghui Ding",
                "Changhu Wang",
                "Gim Hee Lee"
            ],
            "title": "Mine: Towards continuous depth mpi with nerf for novel view synthesis",
            "venue": "In IEEE International Conference on Computer Vision, pp. 12578\u201312588,",
            "year": 2021
        },
        {
            "authors": [
                "Ruihui Li",
                "Xianzhi Li",
                "Ka-Hei Hui",
                "Chi-Wing Fu"
            ],
            "title": "Sp-gan: Sphere-guided 3d shape generation and manipulation",
            "venue": "ACM Transactions on Graphics,",
            "year": 2021
        },
        {
            "authors": [
                "Andrew Liu",
                "Shiry Ginosar",
                "Tinghui Zhou",
                "Alexei A Efros",
                "Noah Snavely"
            ],
            "title": "Learning to factorize and relight a city",
            "venue": "In European Conference on Computer Vision,",
            "year": 2020
        },
        {
            "authors": [
                "Shichen Liu",
                "Tianye Li",
                "Weikai Chen",
                "Hao Li"
            ],
            "title": "Soft rasterizer: A differentiable renderer for image-based 3d reasoning",
            "venue": "IEEE International Conference on Computer Vision,",
            "year": 2019
        },
        {
            "authors": [
                "Emile Mathieu",
                "Tom Rainforth",
                "Nana Siddharth",
                "Yee Whye Teh"
            ],
            "title": "Disentangling disentanglement in variational autoencoders",
            "venue": "In ICML,",
            "year": 2019
        },
        {
            "authors": [
                "Nelson Max"
            ],
            "title": "Optical models for direct volume rendering",
            "venue": "IEEE Transactions on Visualization and Computer Graphics,",
            "year": 1995
        },
        {
            "authors": [
                "Ben Mildenhall",
                "Pratul P. Srinivasan",
                "Matthew Tancik",
                "Jonathan T. Barron",
                "Ravi Ramamoorthi",
                "Ren Ng"
            ],
            "title": "Nerf: Representing scenes as neural radiance fields for view synthesis",
            "venue": "In European Conference on Computer Vision,",
            "year": 2020
        },
        {
            "authors": [
                "Jiteng Mu",
                "Shalini De Mello",
                "Zhiding Yu",
                "Nuno Vasconcelos",
                "Xiaolong Wang",
                "Jan Kautz",
                "Sifei Liu"
            ],
            "title": "Coordgan: Self-supervised dense correspondences emerge from gans",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Thu Nguyen-Phuoc",
                "Chuan Li",
                "Lucas Theis",
                "Christian Richardt",
                "Yong-Liang Yang"
            ],
            "title": "Hologan: Unsupervised learning of 3d representations from natural images",
            "venue": "In IEEE International Conference on Computer Vision,",
            "year": 2019
        },
        {
            "authors": [
                "Michael Niemeyer",
                "Lars Mescheder",
                "Michael Oechsle",
                "Andreas Geiger"
            ],
            "title": "Differentiable volumetric rendering: Learning implicit 3d representations without 3d supervision",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Michael Oechsle",
                "Lars Mescheder",
                "Michael Niemeyer",
                "Thilo Strauss",
                "Andreas Geiger"
            ],
            "title": "Texture fields: Learning texture representations in function space",
            "venue": "In IEEE International Conference on Computer Vision,",
            "year": 2019
        },
        {
            "authors": [
                "Michael Oechsle",
                "Songyou Peng",
                "Andreas Geiger"
            ],
            "title": "Unisurf: Unifying neural implicit surfaces and radiance fields for multi-view reconstruction",
            "venue": "In IEEE International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Jonathan Palacios",
                "Eugene Zhang"
            ],
            "title": "Rotational symmetry field design on surfaces",
            "venue": "ACM Transactions on Graphics,",
            "year": 2007
        },
        {
            "authors": [
                "Keunhong Park",
                "Konstantinos Rematas",
                "Ali Farhadi",
                "Steven M. Seitz"
            ],
            "title": "Photoshape: Photorealistic materials for large-scale shape collections",
            "venue": "ACM Transactions on Graphics,",
            "year": 2018
        },
        {
            "authors": [
                "Keunhong Park",
                "Konstantinos Rematas",
                "Ali Farhadi",
                "Steven M. Seitz"
            ],
            "title": "Photoshape: Photorealistic materials for large-scale shape collections",
            "venue": "ACM Transactions on Graphics,",
            "year": 2018
        },
        {
            "authors": [
                "Taesung Park",
                "Jun-Yan Zhu",
                "Oliver Wang",
                "Jingwan Lu",
                "Eli Shechtman",
                "Alexei Efros",
                "Richard Zhang"
            ],
            "title": "Swapping autoencoder for deep image manipulation",
            "venue": "Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Dario Pavllo",
                "Graham Spinks",
                "Thomas Hofmann",
                "Marie-Francine Moens",
                "Aurelien Lucchi"
            ],
            "title": "Convolutional generation of textured 3d meshes",
            "venue": "Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Songyou Peng",
                "Chiyu Jiang",
                "Yiyi Liao",
                "Michael Niemeyer",
                "Marc Pollefeys",
                "Andreas Geiger"
            ],
            "title": "Shape as points: A differentiable poisson solver",
            "venue": "Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Stanislav Pidhorskyi",
                "Donald A Adjeroh",
                "Gianfranco Doretto"
            ],
            "title": "Adversarial latent autoencoders",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Ben Poole",
                "Ajay Jain",
                "Jonathan T. Barron",
                "Ben Mildenhall"
            ],
            "title": "Dreamfusion: Text-to-3d using 2d diffusion",
            "venue": "In International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Amit Raj",
                "Cusuh Ham",
                "Connelly Barnes",
                "Vladimir Kim",
                "Jingwan Lu",
                "James Hays"
            ],
            "title": "Learning to generate textures on 3d meshes",
            "venue": "In IEEE Conference on Computer Vision and Pattern RecognitionW,",
            "year": 2019
        },
        {
            "authors": [
                "Robin Rombach",
                "Andreas Blattmann",
                "Dominik Lorenz",
                "Patrick Esser",
                "Bj\u00f6rn Ommer"
            ],
            "title": "Highresolution image synthesis with latent diffusion models",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "J\u00fcrgen Schmidhuber"
            ],
            "title": "Deep learning in neural networks: An overview",
            "venue": "Neural Networks,",
            "year": 2015
        },
        {
            "authors": [
                "Christoph Schuhmann",
                "Richard Vencu",
                "Romain Beaumont",
                "Robert Kaczmarczyk",
                "Clayton Mullis",
                "Aarush Katta",
                "Theo Coombes",
                "Jenia Jitsev",
                "Aran Komatsuzaki"
            ],
            "title": "Laion-400m: Open dataset of clip-filtered 400 million image-text pairs",
            "venue": "arXiv preprint,",
            "year": 2021
        },
        {
            "authors": [
                "Katja Schwarz",
                "Yiyi Liao",
                "Michael Niemeyer",
                "Andreas Geiger. Graf"
            ],
            "title": "Generative radiance fields for 3d-aware image synthesis",
            "venue": "Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Yawar Siddiqui",
                "Justus Thies",
                "Fangchang Ma",
                "Qi Shan",
                "Matthias Nie\u00dfner",
                "Angela Dai"
            ],
            "title": "Texturify: Generating textures on 3d shape surfaces",
            "venue": "In European Conference on Computer Vision,",
            "year": 2022
        },
        {
            "authors": [
                "Krishna Kumar Singh",
                "Utkarsh Ojha",
                "Yong Jae Lee"
            ],
            "title": "Finegan: Unsupervised hierarchical disentanglement for fine-grained object generation and discovery",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Ivan Skorokhodov",
                "Sergey Tulyakov",
                "Yiqun Wang",
                "Peter Wonka"
            ],
            "title": "Epigraf: Rethinking training of 3d gans",
            "venue": "Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Gabriel Taubin"
            ],
            "title": "A signal processing approach to fair surface design",
            "venue": "In ACM SIGGRAPH, pp",
            "year": 1995
        },
        {
            "authors": [
                "Alex Trevithick",
                "Bo Yang"
            ],
            "title": "Grf: Learning a general radiance field for 3d representation and rendering",
            "venue": "In IEEE International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Greg Turk"
            ],
            "title": "Texture synthesis on surfaces",
            "venue": "In ACM SIGGRAPH, pp. 347\u2013354,",
            "year": 2001
        },
        {
            "authors": [
                "Naveen Venkat",
                "Mayank Agarwal",
                "Maneesh Singh",
                "Shubham Tulsiani"
            ],
            "title": "Geometry-biased transformers for novel view synthesis",
            "venue": "arXiv preprint,",
            "year": 2023
        },
        {
            "authors": [
                "Peihao Wang",
                "Xuxi Chen",
                "Tianlong Chen",
                "Subhashini Venugopalan",
                "Zhangyang Wang"
            ],
            "title": "Is attention all nerf needs",
            "venue": "arXiv preprint,",
            "year": 2022
        },
        {
            "authors": [
                "Peng Wang",
                "Lingjie Liu",
                "Yuan Liu",
                "Christian Theobalt",
                "Taku Komura",
                "Wenping Wang"
            ],
            "title": "Neus: Learning neural implicit surfaces by volume rendering for multi-view reconstruction",
            "venue": "Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Yue Wang",
                "Yongbin Sun",
                "Ziwei Liu",
                "Sanjay E Sarma",
                "Michael M Bronstein",
                "Justin M Solomon"
            ],
            "title": "Dynamic graph cnn for learning on point clouds",
            "venue": "ACM Transactions on Graphics,",
            "year": 2019
        },
        {
            "authors": [
                "Fanbo Xiang",
                "Zexiang Xu",
                "Milos Hasan",
                "Yannick Hold-Geoffroy",
                "Kalyan Sunkavalli",
                "Hao Su"
            ],
            "title": "Neutex: Neural texture mapping for volumetric neural rendering",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Qiangeng Xu",
                "Zexiang Xu",
                "Julien Philip",
                "Sai Bi",
                "Zhixin Shu",
                "Kalyan Sunkavalli",
                "Ulrich Neumann"
            ],
            "title": "Point-nerf: Point-based neural radiance fields",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Bangbang Yang",
                "Chong Bao",
                "Junyi Zeng",
                "Hujun Bao",
                "Yinda Zhang",
                "Zhaopeng Cui",
                "Guofeng Zhang"
            ],
            "title": "Neumesh: Learning disentangled neural mesh-based implicit field for geometry and texture editing",
            "venue": "In European Conference on Computer Vision,",
            "year": 2022
        },
        {
            "authors": [
                "Linjie Yang",
                "Ping Luo",
                "Chen Change Loy",
                "Xiaoou Tang"
            ],
            "title": "A large-scale car dataset for fine-grained categorization and verification",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2015
        },
        {
            "authors": [
                "Lior Yariv",
                "Yoni Kasten",
                "Dror Moran",
                "Meirav Galun",
                "Matan Atzmon",
                "Basri Ronen",
                "Yaron Lipman"
            ],
            "title": "Multiview neural surface reconstruction by disentangling geometry and appearance",
            "venue": "Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Lior Yariv",
                "Jiatao Gu",
                "Yoni Kasten",
                "Yaron Lipman"
            ],
            "title": "Volume rendering of neural implicit surfaces",
            "venue": "Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Alex Yu",
                "Vickie Ye",
                "Matthew Tancik",
                "Angjoo Kanazawa"
            ],
            "title": "pixelnerf: Neural radiance fields from one or few images",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Rui Yu",
                "Yue Dong",
                "Pieter Peers",
                "Xin Tong"
            ],
            "title": "Learning texture generators for 3d shape collections from internet photo sets",
            "venue": "In BMVC,",
            "year": 2021
        },
        {
            "authors": [
                "Xin Yu",
                "Peng Dai",
                "Wenbo Li",
                "Lan Ma",
                "Zhengzhe Liu",
                "Xiaojuan Qi"
            ],
            "title": "Texture generation on 3d meshes with point-uv diffusion",
            "venue": "In IEEE International Conference on Computer Vision,",
            "year": 2023
        },
        {
            "authors": [
                "Xiaohui Zeng",
                "Arash Vahdat",
                "Francis Williams",
                "Zan Gojcic",
                "Or Litany",
                "Sanja Fidler",
                "Karsten Kreis"
            ],
            "title": "Lion: Latent point diffusion models for 3d shape generation",
            "venue": "In Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Han Zhang",
                "Ian Goodfellow",
                "Dimitris Metaxas",
                "Augustus Odena"
            ],
            "title": "Self-attention generative adversarial networks",
            "venue": "In ICML,",
            "year": 2019
        },
        {
            "authors": [
                "Jason Zhang",
                "Gengshan Yang",
                "Shubham Tulsiani",
                "Deva Ramanan"
            ],
            "title": "Ners: neural reflectance surfaces for sparse-view 3d reconstruction in the wild",
            "venue": "Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Lvmin Zhang",
                "Anyi Rao",
                "Maneesh Agrawala"
            ],
            "title": "Adding conditional control to text-to-image diffusion models",
            "venue": "In IEEE International Conference on Computer Vision,",
            "year": 2023
        },
        {
            "authors": [
                "Yanshu Zhang",
                "Shichong Peng",
                "Alireza Moazeni",
                "Ke Li"
            ],
            "title": "Papr: Proximity attention point rendering",
            "venue": "In Neural Information Processing Systems,",
            "year": 2023
        },
        {
            "authors": [
                "Silvia Zuffi",
                "Angjoo Kanazawa",
                "David W Jacobs",
                "Michael J Black"
            ],
            "title": "3d menagerie: Modeling the 3d shape and pose of animals",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Texturify Siddiqui"
            ],
            "title": "Compcars dataset. The model is trained with 512\u00d7512 resolution, images shown are rendered with 1024\u00d71024 resolution. The sample shown in this figure was generated using the pre-trained model provided by the authors. Notably, all the images",
            "year": 2022
        },
        {
            "authors": [
                "Texturify Siddiqui"
            ],
            "title": "Compcars dataset. The model is trained with 512\u00d7512 resolution, images shown are rendered with 1024\u00d71024 resolution. The sample shown in this figure was generated using the pre-trained model provided by the authors. Notably, all the images",
            "year": 2022
        },
        {
            "authors": [],
            "title": "MORE QUANTITATIVE RESULTS ON FULL PHOTOSHAPE We report results on the full Phtoshape dataset in Table 5, showcasing superior controllable synthesis but higher FID and KID values compared to Texturify",
            "venue": "Similar to prior works (Cheng et al.,",
            "year": 2021
        },
        {
            "authors": [
                "rokhodov"
            ],
            "title": "2022) and our method. This ensures that both approaches utilize the same geometry. Below, we provide the results comparing our method to EpiGRAF while employing the ground-truth SDF for EpiGRAF. Table 6: Comparisons with EpiGRAF using Groundtruth Geometry. KID is multiplied by 102",
            "venue": "Dataset Method",
            "year": 2022
        },
        {
            "authors": [
                "DGCNN Wang"
            ],
            "title": "2019), which contains 3 EdgeConv layers using neighborhood size 20. The output of the encoder is a global shape latent zgeo",
            "year": 2019
        },
        {
            "authors": [
                "Li"
            ],
            "title": "2021b), which regresses additional weights among the K point neighbors",
            "year": 2021
        },
        {
            "authors": [
                "Karras"
            ],
            "title": "ModFC denotes modulated fully connected layers, and tFeat",
            "year": 2019
        },
        {
            "authors": [
                "Karras"
            ],
            "title": "2020b), but modulated by the patch location and scale parameters",
            "year": 2020
        }
    ],
    "sections": [
        {
            "text": "Codes, datasets, and trained models will be made publicly available. Interactive visualizations are provided anonymously at https://TUVF4ICLR.github.io/ without web trackers."
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "3D content creation has attracted much attention given its wide applications in mixed reality, digital twins, filming, and robotics. However, while most efforts in computer vision and graphics focus on 3D shape modeling (Chabra et al., 2020; Zeng et al., 2022; Cheng et al., 2023), there is less emphasis on generating realistic textures (Siddiqui et al., 2022). Textures play a crucial role in enhancing the immersive experiences in virtual and augmented reality. While 3D shape models are abundant in simulators, animations, video games, industry manufacturing, synthetic architectures, etc., rendering realistic and 3D consistent texture on these shape models without human efforts (Figure 1 first three rows) will fundamentally advance the visual quality, functionalities, and experiences.\nGiven instances of one category, ideally, their textures should be disentangled from their shapes. This can be particularly useful in scenarios where the appearance of an object needs to be altered frequently, but the shape remains the same. For example, it is common in video games to have multiple variations of the same object with different textures to provide visual variety without creating entirely new 3D models. Thus, the synthesis process should also be controllable, i.e., we can apply different textures to the exact shape (Figure 1 fourth row) or use the same texture code for different shapes and even edit part of the texture (Figure 1 bottom row). Recently, the wide utilization of GANs (Goodfellow et al., 2014) allows training on 3D content creation with only 2D supervision (Nguyen-Phuoc et al., 2019; Siddiqui et al., 2022; Skorokhodov et al., 2022; Chan et al., 2022). While this alleviates the data and supervision problem, the learned texture representation often highly depends on the input geometry, making the synthesis process less controllable: With the same texture code or specifications, the appearance style of the generated contents changes based on the geometric inputs.\nWe propose a novel texture representation, Texture UV Radiance Fields (TUVF), for high-quality and disentangled texture generation on a given 3D shape, i.e., a sampled texture code represents a particular appearance style adaptable to different shapes. The key to disentangling the texture from geometry is to generate the texture in a canonical UV sphere space instead of directly on the shape. We train the canonical UV space for each category via a Canonical Surface Auto-encoder in a self-supervised manner so that the correspondence between the UV space and the 3D shape is automatically established during training. Unlike traditional UV mesh representation, TUVF does not suffer from topology constraints and can easily adapt to a continuous radiance field.\nGiven a texture code, we first encode it with a texture mapping network to a style embedding, which is then projected onto the canonical UV sphere as a textured UV sphere. Using correspondence, we can assign textures to arbitrary 3D shapes and construct a point-based radiance field. Consequently, we sample the points along the ray and around the object shape surface and render the RGB image. In contrast to volumetric rendering (Drebin et al., 1988; Mildenhall et al., 2020), our Texture UV Radiance Field allows efficient rendering and disentangles the texture from the 3D surface. Finally, we apply an adversarial loss using high-quality images from the same category.\nWe train our model on two real-world datasets (Yang et al., 2015; Park et al., 2018a), along with a synthetic dataset generated by our dataset pipeline. Figure 1 visualizes the results of synthesizing 3D consistent texture given a 3D shape. Our method can provide realistic texture synthesis. More importantly, our method allows complete texture disentanglement from geometry, enabling controllable synthesis and editing (Figure 1 bottom two rows). With the same shape, we evaluate how diverse the textures can be synthesized. With the same texture, we evaluate how consistently it can be applied across shapes. Our method outperforms previous state-of-the-arts significantly on both metrics."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Neural Radiance Fields. Neural Radiance Fields (NeRFs) have been widely studied on broad applications such as high fidelity novel view synthesis (Mildenhall et al., 2020; Barron et al., 2021; 2023) and 3D reconstruction (Wang et al., 2021; Yariv et al., 2021; Zhang et al., 2021). Following this line of research, the generalizable versions of NeRF are proposed for faster optimization and few-view synthesis (Schwarz et al., 2020; Trevithick & Yang, 2021; Li et al., 2021a; Chen et al., 2021; Wang et al., 2022; Venkat et al., 2023). Similarly, TUVF is trained in category-level and learn across instances. However, instead of learning from reconstruction with multi-view datasets (Yu et al., 2021a; Chen et al., 2021), our method leverages GANs for learning from 2D single-view image collections. From the rendering perspective, instead of performing volumetric rendering (Drebin et al., 1988), more efficient rendering techniques have been applied recently, including surface rendering (Niemeyer et al., 2020; Yariv et al., 2020) and rendering with point clouds (Xu et al., 2022;\nYang et al., 2022; Zhang et al., 2023b). Our work relates to the point-based paradigm: Point-Nerf (Xu et al., 2022) models a volumetric radiance field using a neural point cloud; Neu-Mesh (Yang et al., 2022) proposes a point-based radiance field using mesh vertices. However, these approaches typically require densely sampled points and are optimized for each scene. In contrast, TUVF only requires sparse points for rendering and is generalizable across scenes.\nTexture Synthesis on 3D Shapes. Texture synthesis has been an active research area in computer vision and graphics for a long time, with early works focusing on 2D image textures (Cross & Jain, 1983; Taubin, 1995; Efros & Leung, 1999) and subsequently expanding to 3D texture synthesis (Turk, 2001; Bhat et al., 2004; Kopf et al., 2007). Recently, learning-based methods (Raj et al., 2019; Siddiqui et al., 2022; Foti et al., 2022) combined with differentiable rendering techniques (Liu et al., 2019; Mildenhall et al., 2020) have shown promising results in texture synthesis on 3D shapes by leveraging generative adversarial networks (GANs) (Goodfellow et al., 2014) and variational autoencoders (VAEs) (Kingma & Welling, 2013). These paradigms have been applied to textured shape synthesis (Pavllo et al., 2020; Gao et al., 2022; Chan et al., 2022) and scene completion (Dai et al., 2021; Azinovic\u0301 et al., 2022). Motivated by these works, we also adopt GANs to supervise a novel representation for 3D texture synthesis. This allows our model to train from a collection of single-view images instead of using multi-view images for training.\nTexture Representations. Several mesh-based methods (Oechsle et al., 2019; Dai et al., 2021; Yu et al., 2021b; Chen et al., 2022; Siddiqui et al., 2022; Chen et al., 2023; Yu et al., 2023) have been proposed. AUV-Net (Chen et al., 2022) embed 3D surfaces into a 2D aligned UV space using traditional UV mesh; however, they requires shape-image pairs as supervision. Texturify (Siddiqui et al., 2022) use 4-RoSy fields (Palacios & Zhang, 2007) to generate textures on a given mesh. However, the texture representation is entangled with the input shape, and the style can change when given different shape inputs. Our approach falls into the NeRF-based methods (Chan et al., 2022; Skorokhodov et al., 2022). The tri-plane representation has been widely used in these methods. However, these methods often face a similar problem in structure and style entanglement. NeuTex (Xiang et al., 2021) provides an explicit disentangled representation. However, the representation is designed for a single scene. Our TUVF representation disentangles texture from geometry and is generalizable across instances, which allows transferring the same texture from one shape to another.\nDisentanglement of Structure and Style. The disentanglement of structure and style in generative models allows better control and manipulation in the synthesis process. Common approaches to achieve disentanglement include using Autoencoders (Kingma & Welling, 2013; Kulkarni et al., 2015; Jha et al., 2018; Mathieu et al., 2019; Liu et al., 2020; Park et al., 2020; Pidhorskyi et al., 2020) and GANs (Chen et al., 2016; Huang et al., 2018; Karras et al., 2019; Singh et al., 2019; Nguyen-Phuoc et al., 2019; Chan et al., 2021). For example, the Swapping Autoencoder (Park et al., 2020) learns disentanglement by leveraging network architecture bias and enforcing the texture branch of the network to encode co-occurrent patch statistics across different parts of the image. However, these inductive biases do not ensure full disentanglement, and the definition of disentanglement itself is not clearly defined. In the second paradigm with adversarial learning, StyleGAN (Karras et al., 2019) learns separate mappings for the structure and style of images, allowing for high-quality image synthesis with fine-grained control over image attributes. Recently, CoordGAN (Mu et al., 2022) shows that it is possible to train GANs and pixel-wise dense correspondence can automatically emerge. Our work leverages GANs to provide supervision in training, but instead of disentangling texture from 2D structures, we are learning the texture for 3D object shapes."
        },
        {
            "heading": "3 TEXTURE UV RADIANCE FIELDS",
            "text": "We introduce Texture UV Radiance Fields (TUVF) that generate a plausible texture UV representation conditioned on the shape of a given 3D object. Semantically corresponding points on different instances across the category are mapped to the same locations on the texture UV, which inherently enables applications such as texture transfer during inference. As shown in Figure 2, our texture synthesis pipeline begins with a canonical surface auto-encoder (Section 3.1) that builds dense correspondence between a canonical UV sphere and all instances in a category. Such dense correspondence allows us to synthesize textures on a shared canonical UV space using a coordinate-based generator (Section 3.2). Finally, since we do not assume known object poses for each instance, we render the generated radiance field (Section 3.3) and train the framework with adversarial learning (Section 3.4)."
        },
        {
            "heading": "3.1 CANONICAL SURFACE AUTO-ENCODER",
            "text": "The key intuition of this work is to generate texture on a shape-independent space, where we resort to a learnable UV space containing dense correspondences across different instances in a category. To this end, we learn a canonical surface auto-encoder that maps any point on a canonical UV sphere to a point on an object\u2019s surface (Cheng et al., 2021; 2022). Specifically, given a 3D object with point O, we first encode its shape into a geometry code zgeo \u2208 Rd by an encoder E (Cheng et al., 2021). For a point p on the canonical UV sphere, we feed the concatenation of its coordinates Xp and the geometry code into an implicit function f\u03b8 (Figure 2 ) to predict the coordinates of the mapped point p\u2032, denoted as Xp\u2032 , on the given object\u2019s surface. We further predict the normal Np\u2032 at p\u2032 with a separate implicit function g\u03b8 (Figure 2 ). The overall process can be denoted as follows:\nzgeo = E(O) (1) Xp\u2032 = f\u03b8(Xp; zgeo), Np\u2032 = g\u03b8(Xp\u2032 ; zgeo) (2)\nThe coordinates and normal of p\u2032 are then used for the rendering process discussed in Section 3.3.\nWe use a graph-based point encoder following DGCNN (Wang et al., 2019) and decoder architecture following (Cheng et al., 2022) for f\u03b8 and g\u03b8. As proved by (Cheng et al., 2021), correspondences emerge naturally during training, and f\u03b8 and g\u03b8 are trained end-to-end using Chamfer Distance (Borgefors, 1988) on the surface points and the L2 losses on the indicator grid discussed in Section 3.4."
        },
        {
            "heading": "3.2 TEXTURE FEATURE GENERATOR",
            "text": "The canonical UV sphere defines dense correspondences associated with all instances in a category. Thus, shape-independent textures can be formulated as generating texture features on top of this sphere space. To this end, we introduce CIPS-UV, an implicit architecture for texture mapping function h\u03b8 (Figure 2 ). Specifically, CIPS-UV takes a 3D point p on the canonical sphere Xp, and a randomly sampled texture style vector ztex \u223c N (0, 1) as inputs and generates the texture feature vector cj \u2208 Rd at point p, which are further used for rendering as discussed in Section 3.3. The style latent is injected via weight modulation, similar to StyleGAN (Karras et al., 2019). We design our h\u03b8 based on the CIPS generator (Anokhin et al., 2021), where the style vector ztex is used to modulate features at each layer. This design brings two desired properties. First, combined with the canonical UV sphere, we do not require explicit parameterization, such as unwrapping to 2D. Second, it does not include operators (e.g., spatial convolutions (Schmidhuber, 2015), up/downsampling, or self-attentions (Zhang et al., 2019)) that bring interactions between pixels. This is important because nearby UV coordinates may not correspond to exact neighboring surface points in the 3D space. As a result, our generator can better preserve the 3D semantic information and produce realistic and diverse textures on the UV sphere. Please refer to Appendix L for implementation details."
        },
        {
            "heading": "3.3 RENDERING FROM UV SPHERE",
            "text": "Efficient Ray Sampling. Surface rendering is known for its speed, while volume rendering is known for its better visual quality (Oechsle et al., 2021). Similar to (Oechsle et al., 2021; Yariv et al., 2021; Wang et al., 2021), we take advantage of both to speed up rendering while preserving the visual quality, i.e., we only render the color of a ray on points near the object\u2019s surface. To identify valid points near the object\u2019s surface, we start by uniformly sampling 256 points along a ray between the near and far planes and computing the density value \u03c3i (discussed below) for each position xi. We then compute the contribution (denoted as wi) of xi to the ray radiance as\nwi = \u03b1i \u00b7 Ti, \u03b1i = 1\u2212 exp(\u2212\u03c3i\u03b4), Ti = exp(\u2212 i\u22121\u2211 j=1 \u03b1j\u03b4j) (3)\nwhere \u03b4 is the distance between adjacent samples. If wi = 0, then xi is an invalid sample (Hu et al., 2022) and will not contribute to the final ray radiance computation. Empirically, we found that sampling only three points for volume rendering is sufficient. It is worth noting that sampling \u03c3i alone is also fast since the geometry is known in our setting. Volume Density from Point Clouds. We discuss how to derive a continuous volume density from the Canonical Surface Auto-encoder (Section 3.1), which was designed to manipulate discrete points. Given a set of spatial coordinates and their corresponding normal derived from f\u03b8 and g\u03b8, we use the Poisson Surface Reconstruction algorithm (Peng et al., 2021) to obtain indicator function values over the 3D grid. We then retrieve the corresponding indicator value dpsr(xi) for each location xi via trilinear interpolation. dpsr(xi) is numerically similar to the signed distance to the surface and can serve as a proxy for density in volume rendering. We adopt the formulation from VolSDF (Yariv et al., 2021) to transform the indicator value into density fields \u03c3 by:\n\u03c3(xi) = 1 \u03b3 \u00b7 Sigmoid(\u2212dpsr(xi) \u03b3 ) (4)\nNote that the parameter \u03b3 controls the tightness of the density around the surface boundary and is a learnable parameter in VolSDF. However, since our geometry remains fixed during training, we used a fixed value of \u03b3 = 5e\u22124. Point-based Radiance Field. To compute the radiance for a shading point xi, we query the K nearest surface points p\u2032j\u2208NK in the output space of the Canonical Surface Auto-encoder and obtain their corresponding feature vector cj\u2208NK by h\u03b8. We then use an MLPF , following (Xu et al., 2022), to process a pair-wise feature between the shading point xi and each nearby neighboring point, expressed as cj,xi = MLPF (cj , p \u2032 j \u2212 xi). Next, we apply inverse distance weighting to normalize and fuse these K features into one feature vector cxi for shading point xi:\ncxi = \u2211\nj\u2208NK\n\u03c1j\u2211 \u03c1j cj,xi , where \u03c1j = 1\u2225\u2225p\u2032j \u2212 xi\u2225\u2225 (5)\nThen we use another MLPC to output a final color value for point xi based on cj,xi and an optional viewing direction d, denoted ci = MLPC(cj,xi \u2295 d). We design MLPF and MLPC to be shared across all points, i.e., as implicit functions, so that they do not encode local geometry information. Finally, following quadrature rules (Mildenhall et al., 2020; Max, 1995), we render the pixel C(r) with points {xi|i = 1, ..., N} along the ray r as C\u0302(r) = \u2211N i=1 Ti\u03b1ici."
        },
        {
            "heading": "3.4 GENERATIVE ADVERSARIAL LEARNING",
            "text": "Patch-based Discriminator. NeRF rendering is expressive but can be computationally expensive when synthesizing high-resolution images. For GAN-based generative NeRFs, using 2D convolutional discriminators that require entire images as inputs further exacerbates this challenge. Thus, in our work, we adopt the efficient and stable patch-discriminator proposed in EpiGRAF (Skorokhodov et al., 2022). During training, we sample patches starting from a minimal scale, covering the entire image in low resolution. As the scale gradually grows, the patch becomes high-resolution image crops. As our rendering process is relatively lightweight (see Section 3.3), we use larger patches (128\u00d7128) than those used in EpiGRAF (64\u00d764), which brings better quality. Training Objectives. We train the Canonical Surface Auto-encoder (Section 3.1) and the Texture Generator (Section 3.2) in separate stages. In stage-1, we adopt a Chamfer Distance between the output and input point sets, and a L2 loss to learn the mapping dpsr(.) between points and volume density, as aforementioned:\nLCSAE = LCD(p, p \u2032 i) + LDPSR \u2225 \u03c7\u2032 \u2212 \u03c7 \u2225 (6)\nwhere (\u03c7\u2032, \u03c7) denotes the predicted and ground truth indicator function (see details in (Peng et al., 2021)). In stage-2, with R denotes rendering, we enforce a non-saturating GAN loss with R1 regularization to train the texture generator (Karras et al., 2019; Skorokhodov et al., 2022):\nLGAN = Eztex\u223cpG [f(D(R(h\u03b8(ztex, Xp))))] + EIreal\u223cpD [f(\u2212D(Ireal) + \u03bb\u2225\u2206D(Ireal)\u22252], (7) where f(u) = \u2212log(1 + exp (\u2212u))."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "4.1 DATASETS",
            "text": "CompCars & Photoshape. We used 3D shapes from ShapeNet\u2019s \u201dchair\u201d and \u201dcar\u201d categories (Chang et al., 2015). For the 2D datasets, we employed Compcars (Yang et al., 2015) for cars and Photoshape (Park et al., 2018b) for chairs. Notably, the chair category includes subsets with significantly different topologies, such as lounges and bean chairs, where finding a clear correspondence may be challenging even for humans. Consequently, we evaluate our model and the baselines\u2019 performance on the \u201dstraight chair\u201d category, one of the largest subsets in the chair dataset. For fair comparisons, we follow Texturify, splitting the 1,256 car shapes into 956 for training and 300 for testing. We apply the same split within the subset for the chair experiment, yielding 450 training and 150 testing shapes. We also provide the evaluation of full Photoshape in Appendix F.\nDiffusionCats. The above real-world dataset assumes known camera pose distributions, such as hemispheres. However, aligning in-the-wild objects into these specific poses can be time-consuming and prone to inaccuracies. Therefore, we introduce a data generation pipeline that directly synthesizes realistic texture images. We render depth maps from 3D shapes and convert these depth maps into images using pre-trained diffusion models (Rombach et al., 2022; Zhang et al., 2023a). Next, we determine the bounding box based on the depth map and feed this into an off-the-shelf segmentation model (Kirillov et al., 2023) to isolate the target object in the foreground. This pipeline eliminates the need for TUVF to depend on real-world image datasets, making it adaptable to other categories with controllable prompts. For quantitative evaluation, we use 250 shapes of cats from SMAL (Zuffi et al., 2017), which includes appearance variations and deformations, to create a new 2D-3D dataset for texture synthesis through our data generation pipeline. We discuss the details of this pipeline in Appendix P and samples of the generated dataset in Appendix Q."
        },
        {
            "heading": "4.2 BASELINES",
            "text": "Mesh-based Approaches. Texturfiy (Siddiqui et al., 2022) is a state-of-the-art prior work on texture synthesis. They proposed using a 4-rosy field as a better representation for meshes. TexFields (Oechsle et al., 2019), SPSG (Dai et al., 2021), LTG (Yu et al., 2021b), and EG3D-Mesh (Chan et al., 2022) are all mesh-based baselines. These baselines follow a similar framework, where the texture generators are conditioned on a certain shape geometry condition. The biggest difference among these methods is that they use different representations. Specifically, the TexFields (Oechsle et al., 2019) uses a global implicit function to predict texture for mesh, and SPSG (Dai et al., 2021) uses 3D convolution networks to predict voxels for textures. EG3D-Mesh (Chan et al., 2022) uses the triplane representation in EG3D (Chan et al., 2022) to predict the face colors for a given mesh. Note that all baselines require explicit geometry encoding for texture synthesis. On the other hand, our method relies on correspondence and does not directly condition texture on a given geometry. Furthermore, our learned dense surface correspondence allows for direct texture transfer. We also compare with a concurrent work, Text2Tex (Chen et al., 2023), which proposes an instance-specific approach for texture synthesis using a pre-trained diffusion model. NeRF-based Approach. We also evaluate our method against a state-of-the-art NeRF-based approach, EpiGRAF (Skorokhodov et al., 2022), which employs a tri-plane representation and a patch-based discriminator. To modify EpiGRAF into a texture generator for radiance fields, we follow TexFields (Oechsle et al., 2019) and use a point cloud encoder to encode geometry information into EpiGRAF\u2019s style-based triplane generator. For fair comparison, we employ the same discriminator and training hyper-parameters for EpiGRAF (Skorokhodov et al., 2022) and our method."
        },
        {
            "heading": "4.3 EVALUATION METRICS",
            "text": "LPIPSg and LPIPSt. We introduce two metrics to evaluate our model\u2019s ability to disentangle geometry and texture. The first metric, LPIPSg, is calculated by generating ten random latent codes for each shape in the test set and measuring the diversity of the synthesized samples. If the model struggles to disentangle, the generated samples may appear similar, leading to a lower LPIPS score. For the second metric, LPIPSt, we measure the semantic consistency after texture swapping. Specifically, we randomly sample four latent codes and transfer them among 100 test shapes. If a model successfully disentangled the geometry and texture, all samples with the same texture code should look semantically similar, leading to a lower LPIPS score. FID and KID. In addition to LPIPSg and LPIPSt, we employ two standard GAN image quality and diversity metrics, specifically the Frechet Inception Distance (FID) and Kernel Inception Distance\n(KID) scores. We follow Texturify\u2019s setup in all experiments, training on 512\u00d7512 resolution images and rendering images at a resolution of 512\u00d7512 and subsequently downsampling to 256\u00d7256 for evaluation. We employ four random views and four random texture codes for all evaluations and incorporate all available images in the FID/KID calculations.\n4.4 RESULTS\nFigure 5: Surface reconstruction with dense correspondence. The color map indicates the correspondence between each instance and the UV. Please refer to Appendix A for further results.\nCanonical Surface Auto-encoder. To the best of our knowledge, our work represents the first attempt to explore joint end-to-end canonical point auto-encoder (Cheng et al., 2021) and surface learning (Peng et al., 2021). A key concern is the smoothness of the learned correspondence and the reconstructed surface. We construct and visualize the mesh using the predicted indicator function \u03c7\u2032, and without requiring any proxy function (such as nearest neighbor search), dense surface correspondence is readily\nobtained. As a result of the Poisson surface reconstruction, P \u2032j , which holds the correspondence, naturally lies on the surface. In Figure 5 and Figure 7, we showcase that our reconstructed surface is indeed smooth and that the correspondence is both dense and smooth as well. Quantitative Texture Synthesis Results. We show the quantitative results on CompCars in Table 1, the results on Photoshape and DiffusionCats in Table 2. For the CompCars and DiffusionCats datasets, we achieve significant improvements over all the metrics. For the Photoshape dataset, while our approach is slightly worse than Texturify in FID and KID, as for the fidelity metrics, we obtain much better results on controllable synthesis. We further conduct a user study to evaluate the texture quality. Two metrics are considered: (1) General: The users compare random renders from baselines and our method, choosing the most realistic and high-fidelity method. (2) Transfer: The users compare three random renders with the same texture code, selecting the most consistent across shapes. We use Amazon Turk to collect 125 user feedback; the results are shown in Table 3.\nQualitative Texture Synthesis Results. We show our qualitative results for texture synthesis in Figure 3, which confirms that textures generated by our approach are more visually appealing, realistic, and diverse. EpiGRAF suffers from the redundancy of tri-plane representation, leading to less sharp results. We also observe that the tri-plane representation fails when objects are thin (e.g., cats). Our proposed method also shows better diversity and disentanglement than Texturify\nand EpiGRAF. We show texture transfer results in Figure 4, where Texturify and EpiGRAF failed to transfer the texture on some samples. Please refer to Appendix B for more texture synthesis results.\nAblation study. We conducted an ablation study on different texture generator architectures using the CompCars dataset. Two architectures were considered: CIPS-2D and StyleGAN2, the former being the same as the one proposed in (Anokhin et al., 2021), and the latter being a popular choice for both 2D and 3D GANs. Since the input to both generators is in 3D (i.e., sphere coordinates), an equirectangular projection was first performed to transform the coordinates into 2D. We show the results in Table 4, where CIPS-2D suffers from the explicit parameterization of unwrapping 3D to 2D. Similarly, StyleGAN2 suffers from pixel-wise interaction operators that degrade its performance as well. In contrast, our proposed generator design avoids explicit parameterization and operators that bring interactions between pixels. By preserving 3D semantic information, our generator produces realistic and diverse textures on the UV sphere. Please refer to Appendix H for more ablation studies.\nTexture Editing. Our method enables texture editing (Yang et al., 2022; Bao et al., 2023) by allowing direct modification of rendered images, such as drawing or painting. Given a synthesized texture, one can directly operate on the rendered view to edit the texture. By fine-tuning the edited image through back-propagation to the texture feature, we can obtain an edited texture that is 3D consistent across different views. As shown in Figure 6, after editing an image, we can fine-tune its texture feature and transfer it to different shapes."
        },
        {
            "heading": "5 CONCLUSION, LIMITATIONS, AND FUTURE WORK",
            "text": "In this paper, we introduce Texture UV Radiance Fields for generating versatile, high-quality textures applicable to a given object shape. The key idea is to generate textures in a learnable UV sphere space independent of shape geometry and compact and efficient as a surface representation. Specifically, we leverage the UV sphere space with a continuous radiance field so that an adversarial loss on top of rendered images can supervise the entire pipeline. We achieve high-quality and realistic texture synthesis and substantial improvements over state-of-the-art approaches to texture swapping and editing applications. We are able to generate consistent textures over different object shapes while previous approaches fail. Furthermore, we can generate more diverse textures with the same object shape compared to previous state-of-the-arts.\nDespite its merits, our method has inherent limitations. Our current correspondence assumes one-toone dense mapping. However, this assumption does not always hold in real-world scenarios. See Appendix R for more discussion regarding the limitations. To further achieve more photorealistic textures, one option is to incorporate advanced data-driven priors, such as diffusion models, which can help mitigate the distortions and improve the quality of the generated textures. Utilizing more sophisticated neural rendering architectures, such as ray transformers, can also enhance the results."
        },
        {
            "heading": "6 ETHICS STATEMENT",
            "text": "Our work follows the General Ethical Principles listed in ICLR Code of Ethics (https://iclr. cc/public/CodeOfEthics).\nWe share similar concerns as other generative models (Poole et al., 2023). Our work may raise ethical concerns related to the improper use of content manipulation or the generation of deceptive information.\nOur data generation pipeline relies on a pre-trained diffusion (Rombach et al., 2022) model, which can inherit biases in its training data. The dataset (Schuhmann et al., 2021) used to train StableDiffusion may include undesirable images. Additionally, the pre-trained diffusion model is conditioned on features from another pre-trained large language model, which could introduce unintended biases. As a result, our generated texture may also include unintended biases."
        },
        {
            "heading": "Appendix Table of Contents",
            "text": "A More Qualitative Results of Canonical Surface Auto-encoder......................................... 17\nB More Qualitative Results using our Data Generation Pipeline ........................................ 18\nC More Qualitative Results of Texture Editing ..................................................................... 19\nD More Qualitative Results of High-Resolution Synthesis ................................................... 20\nE More Qualitative Results of Texture Transfer ................................................................... 27\nF More Quantitative Results on full Photoshape .................................................................. 32\nG More Comparisons with EpiGRAF using Ground truth Geometry ................................ 33\nH Ablation Study on Canonical Surface Auto-encoder......................................................... 34\nI Ablation Study on Different UV Resolution ....................................................................... 34\nJ Implementation Details of TUVF Rendering ..................................................................... 35\nK Implementation Details of Canonical Surface Auto-encoder ........................................... 35\nL Implementation Details of Texture Generator h\u03b8 CIPS-UV............................................. 36\nM Implementation Details of Patch-based Discriminator ..................................................... 36\nN Training Details and Hyper-parameters............................................................................. 36\nO Computational Time and Model Size ................................................................................. 37\nP Implementation Details of Data Generation Pipeline........................................................ 38\nQ Samples of Our Generated Dataset..................................................................................... 38\nR Limitations............................................................................................................................. 39"
        },
        {
            "heading": "A MORE QUALITATIVE RESULTS OF CANONICAL SURFACE AUTO-ENCODER",
            "text": "Can our Canonical Surface Auto-encoder handle different topologies? We align with prior works by evaluating our approach on two categories within ShapeNet (e.g., cars, chairs) for comparisons. While using a UV sphere adds constraints on representing shape varieties, our method can still model reasonably diverse topologies. We additionally include the canonical surface autoencoding results on three different shape collections in Figure 7. Specifically, we include results on ShapeNet (Chang et al., 2015) Airplanes (top), DFAUST (Bogo et al., 2017) (middle, scanned human subjects and motions), and SMAL Zuffi et al. (2017) (bottom, articulated animals e.g., lions, tigers, and horses). Our method consistently produces high-quality shape reconstructions with dense and smooth learned correspondence, even for non-genus zero shapes such as airplanes with holes. This is not achievable for traditional mesh-based UV deformation."
        },
        {
            "heading": "B MORE QUALITATIVE RESULTS USING OUR DATA GENERATION PIPELINE",
            "text": "We provide our texture synthesis results using Canonical Surface Auto-encoder trained on SMAL (Appendix A) and 2D datasets generated by our data pipeline(Appendix P). As shown in Figure 8, we demonstrate that our model can generate photorealistic textures for various animal categories. With the pipeline, we can also control the style with different prompts. Note that for simplicity, some results in the figure are trained on a single instance or view, indicated by grey text."
        },
        {
            "heading": "C MORE QUALITATIVE RESULTS OF TEXTURE EDITING",
            "text": ""
        },
        {
            "heading": "D MORE QUALITATIVE RESULTS OF HIGH-RESOLUTION SYNTHESIS",
            "text": ""
        },
        {
            "heading": "EpiGRAF",
            "text": ""
        },
        {
            "heading": "Texturify",
            "text": ""
        },
        {
            "heading": "EpiGRAF",
            "text": ""
        },
        {
            "heading": "Texturify",
            "text": ""
        },
        {
            "heading": "EpiGRAF",
            "text": ""
        },
        {
            "heading": "Texturify",
            "text": ""
        },
        {
            "heading": "EpiGRAF",
            "text": ""
        },
        {
            "heading": "Texturify",
            "text": ""
        },
        {
            "heading": "E MORE QUALITATIVE RESULTS OF TEXTURE TRANSFER",
            "text": ""
        },
        {
            "heading": "EpiGRAF",
            "text": ""
        },
        {
            "heading": "Texturify",
            "text": ""
        },
        {
            "heading": "EpiGRAF",
            "text": ""
        },
        {
            "heading": "Texturify",
            "text": ""
        },
        {
            "heading": "EpiGRAF",
            "text": ""
        },
        {
            "heading": "Texturify",
            "text": ""
        },
        {
            "heading": "EpiGRAF",
            "text": ""
        },
        {
            "heading": "Texturify",
            "text": ""
        },
        {
            "heading": "EpiGRAF",
            "text": ""
        },
        {
            "heading": "Texturify",
            "text": ""
        },
        {
            "heading": "EpiGRAF",
            "text": ""
        },
        {
            "heading": "Texturify",
            "text": ""
        },
        {
            "heading": "EpiGRAF",
            "text": ""
        },
        {
            "heading": "Texturify",
            "text": ""
        },
        {
            "heading": "EpiGRAF",
            "text": ""
        },
        {
            "heading": "Texturify",
            "text": ""
        },
        {
            "heading": "EpiGRAF",
            "text": ""
        },
        {
            "heading": "Texturify",
            "text": ""
        },
        {
            "heading": "EpiGRAF",
            "text": ""
        },
        {
            "heading": "Texturify",
            "text": ""
        },
        {
            "heading": "EpiGRAF",
            "text": ""
        },
        {
            "heading": "Texturify",
            "text": ""
        },
        {
            "heading": "EpiGRAF",
            "text": ""
        },
        {
            "heading": "Texturify",
            "text": ""
        },
        {
            "heading": "EpiGRAF",
            "text": ""
        },
        {
            "heading": "Texturify",
            "text": ""
        },
        {
            "heading": "EpiGRAF",
            "text": ""
        },
        {
            "heading": "Texturify",
            "text": ""
        },
        {
            "heading": "EpiGRAF",
            "text": ""
        },
        {
            "heading": "Texturify",
            "text": ""
        },
        {
            "heading": "EpiGRAF",
            "text": ""
        },
        {
            "heading": "Texturify",
            "text": ""
        },
        {
            "heading": "F MORE QUANTITATIVE RESULTS ON FULL PHOTOSHAPE",
            "text": "We report results on the full Phtoshape dataset in Table 5, showcasing superior controllable synthesis but higher FID and KID values compared to Texturify. Similar to prior works (Cheng et al., 2021; 2022), our method builds upon learnable UV maps, assuming one-to-one dense correspondence between instances of a category. However, in real-world scenarios, this assumption does not always hold: There may be variations in shape (e.g., armchairs and straight chairs) or structure (e.g., lounges and bean chairs) across different instances. This introduces challenges in modeling high-fidelity shapes and detailed correspondences at the same time. Despite these challenges, our method produces reasonable results, as depicted in Figure 22.\nTable 5: Quanitative Results on Full Photoshape. While our method has slightly larger FID and KID than Texturify on full Photoshape, we achieve significantly better results in controllable synthesis. KID is multiplied by 102.\nMethod FID \u2193 KID \u2193 LPIPSg \u2191 LPIPSt \u2193 EpiGRAF (Skorokhodov et al., 2022) 89.74 6.28 3.63 6.51 Texturify (Siddiqui et al., 2022) 26.17 1.54 8.86 3.46\nTUVF (ours) 57.56 3.74 16.43 2.72\nFigure 22: Qualatative Results on Full Photoshape. Although finding correspondence for shapes with large structural differences is challenging, our method produces reasonable results."
        },
        {
            "heading": "G MORE COMPARISONS WITH EPIGRAF USING GROUND TRUTH GEOMETRY",
            "text": "For a fair comparison, we use the auto-encoded geometry as the input for both EpiGRAF (Skorokhodov et al., 2022) and our method. This ensures that both approaches utilize the same geometry. Below, we provide the results comparing our method to EpiGRAF while employing the ground-truth SDF for EpiGRAF."
        },
        {
            "heading": "H ABLATION STUDY ON CANONICAL SURFACE AUTO-ENCODER",
            "text": "One drawback of our framework is that the auto-encoded indicator grid may not be perfect. As a result, we investigated several different network designs for the stage-1 geometry pre-training, which enabled us to learn texture synthesis using the ground-truth indicator function. We considered comparing four settings in this study:\n(a) Our Canonical Surface Auto-encoder. The geometry network takes UV points as inputs and maps them to the surface. An additional function g\u03b8 is learned to predict the surface normal for each point, and an auto-encoded indicator function is obtained. Texture synthesis is performed using the auto-encoded indicator function.\n(b) The geometry network takes UV points as inputs and maps them to the surface. No g\u03b8 is used. Texture synthesis is performed using the ground-truth indicator function.\n(c) The geometry network takes UV points as inputs and maps them to the surface. No g\u03b8 is used. Texture synthesis uses the ground-truth indicator function, while points are warped to the ground-truth surface via the nearest neighbor.\n(d) The geometry network takes surface points as inputs and maps them to the UV. In this case, there is no need for g\u03b8, and texture synthesis is learned using the ground-truth indicator function.\nTwo important factors may affect the quality of synthesis. First, the surface points should lie as close to the exact surface of the indicator function as possible. This is because our MLPF takes the nearest neighbor feature and the distance between the query point and the nearest neighbor as inputs. If there is a gap between the points and the surface of the indicator function, it can confuse MLPF and harm the performance. Secondly, the surface points should be as smooth as possible, i.e., evenly distributed among the surface. This ensures that each surface point contributes to a similar amount of surface area. The results of our ablation study on the car category can be found in Table 7. We also show samples from each setting in Figure 23. The results obtained for settings without smooth correspondence (e.g., setting 3 and 4) show that the textures are more blurry and tend to have distortions. On the other hand, our method produces sharper details compared to setting 2, which is trained on proxy surface points. This study demonstrates the unique advantage of our Canonical Surface Auto-encoder design, in which we can learn UV-to-surface mapping with smooth correspondence. Therefore, learning an additional function g\u03b8 to predict point normal and obtain an auto-encoded indicator function is necessary to obtain high-fidelity textures."
        },
        {
            "heading": "I ABLATION STUDY ON DIFFERENT UV RESOLUTION",
            "text": "We investigated the effect of UV resolution on the quality of our method. To achieve this, we compared our base method with different numbers of UV resolution (1K and 2K). The results in Table 8 showed that increasing the UV resolution leads to improved performance in terms of\nproducing higher-quality fine-scale details. However, we found that using level 4 ico-sphere vertices (i.e., 2K points) is sufficient to achieve high-quality results. Further increasing the resolution would result in prohibitively long training times due to the K nearest neighbor search. For example, using level 5 ico-sphere vertices would result in 10242 points, which would significantly slow down the training speed.\nJ IMPLEMENTATION DETAILS OF TUVF RENDERING\nTo obtain samples from the UV sphere, we use the vertices of a level 4 ico-sphere, which provides us with 2562 coordinates. After passing these coordinates through the mapping functions f\u03b8, g\u03b8, and h\u03b8, we obtain 2562 surface points (Xp\u2032 ), 2562 surface normal (Np\u2032 ), a 1283 indication function grid (\u03c7\u2032), and 2562 32-dimensional texture feature vectors. To compute the final color of a ray, we first sample 256 shading points and identify the valid points using the indication function grid \u03c7\u2032. Next, we sample three valid shading points around the surface, and for each valid shading location xi, we conduct a K-nearest neighbor search on the surface points Xp\u2032 . We perform spatial interpolation of the texture feature on the K-nearest surface points to obtain the texture feature cxi for the current shading points xi. In our experiment, we set K to 4, which is not computationally expensive since we only deal with 2562 surface points.\nK IMPLEMENTATION DETAILS OF CANONICAL SURFACE AUTO-ENCODER\nShape Encoder E . Given a 3D object O, we first normalize the object to a unit cube and sample 4096 points on the surface as inputs to the Shape Encoder E Cheng et al. (2021). The encoder structure is adopted from DGCNN Wang et al. (2019), which contains 3 EdgeConv layers using neighborhood size 20. The output of the encoder is a global shape latent zgeo \u2208 Rd where d = 256.\nSurface Points Decoder f\u03b8 and g\u03b8. Both the surface points decoder f\u03b8 and surface normal decoder g\u03b8 share the same decoder architecture, which is adapted from Cheng et al. (2022). We show the detailed architecture of our decoder in Figure 25. The decoder architecture takes a set of point coordinates Xin and geometry feature zgeo as input and learns to output a set of point coordinates Xout in a point-wise manner. To process a set of point coordinates Xin and geometry feature zgeo, the decoder first creates a matrix by duplicating zgeo for each coordinate in Xin and concatenating it to each coordinate. This matrix includes both the point coordinates and geometry features. The\ndecoder has two branches: one branch uses an EdgeConv Wang et al. (2019) with an attention module to extract point-wise spatial features from the point coordinates. The attention module is adopted from Li et al. (2021b), which regresses additional weights among the K point neighbors\u2019 features as attentions. The other branch employs a nonlinear feature embedding technique to extract style features from the geometry feature. The local styles are then combined with the spatial features using adaptive instance normalization Dumoulin et al. (2016) to create fused features. The style embedding and fusion process is repeated, and finally, the fused feature is used to predict the final output Xout. It is worth noting that both the surface points decoder f\u03b8 and surface normal decoder g\u03b8 use the same geometric features as input. However, they differ in their coordinate input. Specifically, f\u03b8 takes 2562 UV coordinates as input, while g\u03b8 uses the 2562 output coordinates from f\u03b8 as input.\nL IMPLEMENTATION DETAILS OF TEXTURE GENERATOR h\u03b8 CIPS-UV\nOur generator network, which has a multi-layer perceptron-type architecture Anokhin et al. (2021), is capable of synthesizing texture features on a UV sphere. To achieve this, we use a random texture latent vector ztex that is shared across all UV coordinates, as well as the UV coordinates (u, v, w) as input. The generator then returns the 32-dim texture feature vector value c for that particular UV coordinate. Thus, to compute the entire UV sphere, the generator is evaluated at every pair of coordinates (u, v, w) while keeping the texture latent vector ztex fixed. Specifically, we utilize a mapping network to convert the random texture latent vector ztex into a style vector with the same dimension as ztex. This vector injects style into the generation process through weight modulation. We follow the Fourier positional encoding method outlined in Anokhin et al. (2021) to encode the input UV coordinates. The resulting coordinate features pass through the modulated fully connected layers (ModFC), which are controlled by the style vector mentioned above. Finally, we obtain a 32-dimensional texture feature for the input coordinate.\nM IMPLEMENTATION DETAILS OF PATCH-BASED DISCRIMINATOR\nOur discriminator is based on EpiGRAF Skorokhodov et al. (2022), which is similar to the one used in StyleGAN2 Karras et al. (2020b), but modulated by the patch location and scale parameters. We follow the patch-wise optimization approach for training, along with using Beta distribution Skorokhodov et al. (2022) for sampling the scale. We use an initial beta value of 1e\u22124 and gradually anneal it to 0.8 after processing 1e7 images."
        },
        {
            "heading": "N TRAINING DETAILS AND HYPER-PARAMETERS",
            "text": "To demonstrate the training pipeline, we use the car category in ShapeNet and the CompCars dataset as examples. All experiments are performed on a workstation equipped with an AMD EPYC 7542 32-Core Processor (2.90GHz) and 8 Nvidia RTX 3090 TI GPUs (24GB each). We implement our framework using PyTorch 1.10. For further details and training time for each stage, please refer to Algorithm 1.\nAlgorithm 1 : The training phase of our approach consists of two stages: (1) Canonical Surface Auto-encoder (2) Texture Feature Generator using adversarial objectives\n(A) CANONICAL SURFACE AUTO-ENCODER \u25b7 12 hours on ShapeNet Car dataset 1: Sub-sample points from the input point clouds as x and the canonical UV sphere \u03c0; 2: Compute ground-truth indicator function grid \u03c7; 3: Initialize weights of the encoder E , decoder f\u03b8 and g\u03b8; 4: while not converged do 5: foreach iteration do 6: zgeo \u2190 E(x); 7: x\u0302\u2190 f\u03b8([\u03c0i, zgeo]), where \u03c0i \u2208 \u03c0; 8: n\u0302\u2190 g\u03b8([x\u0302i, zgeo]), where x\u0302i \u2208 x\u0302; 9: \u03c7\u2032 \u2190 dpsr(x\u0302, n\u0302);\n10: Obtain reconstruction loss LCD(x\u0302, x) and LDPSR(\u03c7\u2032, \u03c7); 11: Update weight;\n(B) TEXTURE FEATURE GENERATOR \u25b7 36 hours on CompCars dataset 1: Sample points from the canonical sphere \u03c0; 2: Random sample shapes with point cloud x and images from dataset Ireal; 3: Load pre-trained encoder E , f\u03b8 and g\u03b8; 4: Initialize weights of the texture feature generator h\u03b8 and patch-based discriminator D; 5: while not converged do 6: foreach iteration do 7: Obtain x\u0302, n\u0302, and \u03c7\u2032 with encoder E , f\u03b8 and g\u03b8; 8: Sample ztex from multivariate normal distribution; 9: ci \u2190 h\u03b8(\u03c0i, ztex), where \u03c0i \u2208 \u03c0;\n10: Ifake \u2190 R(x\u0302, c, \u03c7\u2032, d), whereR denotes renderer and d are camera angles; 11: Obtain loss LGAN (Ifake, Ireal); 12: Update weight;"
        },
        {
            "heading": "N.1 DATA AUGMENTATIONS AND BLUR.",
            "text": "Direct applying the discriminator fails to synthesize reasonable textures since there exists a geometric distribution shift exists bet collection and rendered 2D images. Therefore, following (Chan et al., 2022; Skorokhodov et al., 2022), we apply Adpative Discriminator Augmentation (ADA) (Karras et al., 2020a) to transform both real and fake image crops before they enter the discriminator. Specifically, we use geometric transformations, such as random translation, random scaling, and random anisotropic filtering. However, we disable color transforms in ADA as they harm the generation process and result in undesired textures. In addition to ADA, we also blur the image crops, following (Chan et al., 2022; Skorokhodov et al., 2022). However, since we use larger patch sizes, we employ a stronger initial blur sigma (i.e., 60) and a slower decay schedule, where the image stops blurring after the discriminator has seen 5\u00d7 106 images."
        },
        {
            "heading": "O COMPUTATIONAL TIME AND MODEL SIZE",
            "text": "We provide a comparison of the inference time and model size of different models in Table 9. Specifically, we measure the inference time and size of each model based on the time and number\nof parameters required to generate a texture for a given shape instance and render an image of resolution 1024. All experiments are conducted on a workstation with an Intel(R) Core(TM) i712700K (5.00GHz) processor and a single NVIDIA RTX 3090 TI GPU (24GB). Texturify is a mesh-based approach and is more efficient in terms of rendering compared to NeRF-based methods. However, its feature space is heavily parameterized on the faces, which makes it memory inefficient. Similarly, EpiGRAF requires computing high-resolution triplanes, making it memory-intensive. In contrast, we only parametrize on 2K point clouds throughout all the experiments and can achieve comparable or even better fidelity. Note that we use the same rendering approach for both TUVF and EpiGRAF; therefore, EpiGRAF has a lower inference time than TUVF because it does not require KNN computation.\nP IMPLEMENTATION DETAILS OF DATA GENERATION PIPELINE\nWe utilized Stable Diffusion models Rombach et al. (2022) to generate realistic texture images, which were subsequently used as training data for TUVF. We start by rendering depth maps from synthetic objects using Blender and converting these depth maps into images using depth-conditioned Controlnet Zhang et al. (2023a). If the 3D shape contains object descriptions in its metadata (e.g., ShapeNet (Chang et al., 2015)), we use the description as text prompt guidance. After generating the image, we determine the bounding box based on the depth map and feed this into the Segment Anything Model (SAM) Kirillov et al. (2023) to mask the target object in the foreground. This results in realistic textures for synthetic renders. Our pipeline eliminates the need for perfectly aligned cameras and mitigates differences between 3D synthetic objects and 2D image sets. We will release our automatic data generation pipeline upon publication."
        },
        {
            "heading": "Q SAMPLES OF OUR GENERATED DATASET",
            "text": "We show samples of the depth map and its corresponding 2D images that Controlnet generated on four categories (e.g., cats, horses, dogs, airplanes, and cars). Our pipeline can automatically generate realistic and high-quality (1024\u00d7 1024) 2D textured images for 3D models. For the DiffusionCat dataset, we use 250 shapes from SMAL, and split them into 200 for training and 50 for testing. We use all 250 shapes to generate textured images. Specifically, we generate 2 samples for eight views for each shape, which results in 4000 images. We use 20 denoise steps for Controlnet, and the entire process takes less than 12 hours for a single Nvidia GeForce RTX 3090."
        },
        {
            "heading": "R LIMITATIONS",
            "text": "Geometry. Our work has some limitations inherited from Cheng et al. (2021) since our Canonical Surface Auto-encoder follows similar principles. Specifically, encoding the shape information of a point cloud in a global vector may cause fine details, such as corners and edges, to be blurred or holes to disappear after reconstruction. Similar to Cheng et al. (2021), we also observed that the correspondences predicted near holes or the boundaries between parts might be incorrect, possibly due to the sparsity nature of point clouds and the limitations of the Chamfer distance. Future research should address these limitations.\nCharacteristic Seams. Seams are barely noticeable in our results. There are three reasons. Firstly, unlike prior works (Chen et al., 2022), we avoid cutting the shape into pieces and instead use a unified UV across all parts, resulting in a seamless appearance without any distinct boundaries. Secondly, our UV mapper employs a non-linear mapping function trained with Chamfer loss, seamlessly connecting the UV coordinates without explicit stitching lines. Thirdly, unlike prior works that directly regress RGB values using UV features or RGB information alone, our MLPF also takes the local coordinate as an additional input, representing a local radiance field that effectively reduces the seams. However, these design choices do not completely solve the seam issue. As illustrated in Figure 23, unsmooth correspondence can still result in visible seams."
        }
    ],
    "title": "TUVF: LEARNING GENERALIZABLE TEXTURE UV RADIANCE FIELDS",
    "year": 2023
}