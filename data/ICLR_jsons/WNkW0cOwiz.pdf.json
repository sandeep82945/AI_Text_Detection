{
    "abstractText": "Diffusion models, which employ stochastic differential equations to sample images through integrals, have emerged as a dominant class of generative models. However, the rationality of the diffusion process itself receives limited attention, leaving the question of whether the problem is well-posed and well-conditioned. In this paper, we explore a perplexing tendency of diffusion models: they often display the infinite Lipschitz property of the network with respect to time variable near the zero point. We provide theoretical proofs to illustrate the presence of infinite Lipschitz constants and empirical results to confirm it. The Lipschitz singularities pose a threat to the stability and accuracy during both the training and inference processes of diffusion models. Therefore, the mitigation of Lipschitz singularities holds great potential for enhancing the performance of diffusion models. To address this challenge, we propose a novel approach, dubbed E-TSDM, which alleviates the Lipschitz singularities of the diffusion model near the zero point of timesteps. Remarkably, our technique yields a substantial improvement in performance. Moreover, as a byproduct of our method, we achieve a dramatic reduction in the Fr\u00e9chet Inception Distance of acceleration methods relying on network Lipschitz, including DDIM and DPM-Solver, by over 33%. Extensive experiments on diverse datasets validate our theory and method. Our work may advance the understanding of the general diffusion process, and also provide insights for the design of diffusion models.",
    "authors": [
        {
            "affiliations": [],
            "name": "Zhantao Yang"
        },
        {
            "affiliations": [],
            "name": "Ruili Feng"
        },
        {
            "affiliations": [],
            "name": "Han Zhang"
        },
        {
            "affiliations": [],
            "name": "Yujun Shen"
        },
        {
            "affiliations": [],
            "name": "Kai Zhu"
        },
        {
            "affiliations": [],
            "name": "Lianghua Huang"
        },
        {
            "affiliations": [],
            "name": "Yifei Zhang"
        },
        {
            "affiliations": [],
            "name": "Yu Liu"
        },
        {
            "affiliations": [],
            "name": "Deli Zhao"
        },
        {
            "affiliations": [],
            "name": "Jingren Zhou"
        },
        {
            "affiliations": [],
            "name": "Fan Cheng"
        }
    ],
    "id": "SP:54e6be42b7f25c3b6b0e537002420970860a8724",
    "references": [
        {
            "authors": [
                "Fan Bao",
                "Chongxuan Li",
                "Jiacheng Sun",
                "Jun Zhu",
                "Bo Zhang"
            ],
            "title": "Estimating the optimal covariance with imperfect mean in diffusion probabilistic models",
            "venue": "arXiv preprint arXiv:2206.07309,",
            "year": 2022
        },
        {
            "authors": [
                "Fan Bao",
                "Chongxuan Li",
                "Jun Zhu",
                "Bo Zhang"
            ],
            "title": "Analytic-dpm: an analytic estimate of the optimal reverse variance in diffusion probabilistic models",
            "venue": "arXiv preprint arXiv:2201.06503,",
            "year": 2022
        },
        {
            "authors": [
                "Andrew Brock",
                "Jeff Donahue",
                "Karen Simonyan"
            ],
            "title": "Large scale GAN training for high fidelity natural image synthesis",
            "venue": "arXiv preprint arXiv:1809.11096,",
            "year": 2018
        },
        {
            "authors": [
                "Jooyoung Choi",
                "Jungbeom Lee",
                "Chaehun Shin",
                "Sungwon Kim",
                "Hyunwoo Kim",
                "Sungroh Yoon"
            ],
            "title": "Perception prioritized training of diffusion models",
            "venue": "In IEEE Conf. Comput. Vis. Pattern Recog.,",
            "year": 2022
        },
        {
            "authors": [
                "Yunjey Choi",
                "Youngjung Uh",
                "Jaejun Yoo",
                "Jung-Woo Ha"
            ],
            "title": "Stargan v2: Diverse image synthesis for multiple domains",
            "venue": "In IEEE Conf. Comput. Vis. Pattern Recog.,",
            "year": 2020
        },
        {
            "authors": [
                "Prafulla Dhariwal",
                "Alexander Nichol"
            ],
            "title": "Diffusion models beat GANs on image synthesis",
            "venue": "Adv. Neural Inform. Process. Syst.,",
            "year": 2021
        },
        {
            "authors": [
                "Tim Dockhorn",
                "Arash Vahdat",
                "Karsten Kreis"
            ],
            "title": "Score-based generative modeling with criticallydamped langevin diffusion",
            "venue": "arXiv preprint arXiv:2112.07068,",
            "year": 2021
        },
        {
            "authors": [
                "Martin Heusel",
                "Hubert Ramsauer",
                "Thomas Unterthiner",
                "Bernhard Nessler",
                "Sepp Hochreiter"
            ],
            "title": "GANs trained by a two time-scale update rule converge to a local nash equilibrium",
            "venue": "Adv. Neural Inform. Process",
            "year": 2017
        },
        {
            "authors": [
                "Jonathan Ho",
                "Ajay Jain",
                "Pieter Abbeel"
            ],
            "title": "Denoising diffusion probabilistic models",
            "venue": "In Adv. Neural Inform. Process. Syst.,",
            "year": 2020
        },
        {
            "authors": [
                "Emiel Hoogeboom",
                "Jonathan Heek",
                "Tim"
            ],
            "title": "Salimans. simple diffusion: End-to-end diffusion for high resolution images",
            "venue": "arXiv preprint arXiv:2301.11093,",
            "year": 2023
        },
        {
            "authors": [
                "Tero Karras",
                "Timo Aila",
                "Samuli Laine",
                "Jaakko Lehtinen"
            ],
            "title": "Progressive growing of gans for improved quality, stability, and variation",
            "venue": "arXiv preprint arXiv:1710.10196,",
            "year": 2017
        },
        {
            "authors": [
                "Tero Karras",
                "Samuli Laine",
                "Timo Aila"
            ],
            "title": "A style-based generator architecture for generative adversarial networks",
            "venue": "In IEEE Conf. Comput. Vis. Pattern Recog.,",
            "year": 2019
        },
        {
            "authors": [
                "Tero Karras",
                "Miika Aittala",
                "Janne Hellsten",
                "Samuli Laine",
                "Jaakko Lehtinen",
                "Timo Aila"
            ],
            "title": "Training generative adversarial networks with limited data",
            "venue": "Adv. Neural Inform. Process. Syst.,",
            "year": 2020
        },
        {
            "authors": [
                "Tero Karras",
                "Miika Aittala",
                "Timo Aila",
                "Samuli Laine"
            ],
            "title": "Elucidating the design space of diffusionbased generative models",
            "venue": "arXiv preprint arXiv:2206.00364,",
            "year": 2022
        },
        {
            "authors": [
                "Dongjun Kim",
                "Seungjae Shin",
                "Kyungwoo Song",
                "Wanmo Kang",
                "Il-Chul Moon"
            ],
            "title": "Soft truncation: A universal training technique of score-based diffusion model for high precision score estimation",
            "venue": "arXiv preprint arXiv:2106.05527,",
            "year": 2021
        },
        {
            "authors": [
                "Dongjun Kim",
                "Seungjae Shin",
                "Kyungwoo Song",
                "Wanmo Kang",
                "Il-Chul Moon"
            ],
            "title": "Soft truncation: A universal training technique of score-based diffusion model for high precision score estimation",
            "year": 2022
        },
        {
            "authors": [
                "Diederik Kingma",
                "Tim Salimans",
                "Ben Poole",
                "Jonathan Ho"
            ],
            "title": "Variational diffusion models",
            "venue": "Adv. Neural Inform. Process. Syst.,",
            "year": 2021
        },
        {
            "authors": [
                "Alex Krizhevsky",
                "Geoffrey Hinton"
            ],
            "title": "Learning multiple layers of features from tiny images",
            "year": 2009
        },
        {
            "authors": [
                "Shengmeng Li",
                "Luping Liu",
                "Zenghao Chai",
                "Runnan Li",
                "Xu Tan"
            ],
            "title": "Era-solver: Error-robust adams solver for fast sampling of diffusion probabilistic models",
            "venue": "arXiv preprint arXiv:2301.12935,",
            "year": 2023
        },
        {
            "authors": [
                "Shanchuan Lin",
                "Bingchen Liu",
                "Jiashi Li",
                "Xiao Yang"
            ],
            "title": "Common diffusion noise schedules and sample steps are flawed",
            "venue": "arXiv preprint arXiv:2305.08891,",
            "year": 2023
        },
        {
            "authors": [
                "Cheng Lu",
                "Kaiwen Zheng",
                "Fan Bao",
                "Jianfei Chen",
                "Chongxuan Li",
                "Jun Zhu"
            ],
            "title": "Maximum likelihood training for score-based diffusion odes by high order denoising score matching",
            "venue": "In Int. Conf. Mach. Learn.,",
            "year": 2022
        },
        {
            "authors": [
                "Cheng Lu",
                "Yuhao Zhou",
                "Fan Bao",
                "Jianfei Chen",
                "Chongxuan Li",
                "Jun Zhu"
            ],
            "title": "Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps",
            "venue": "arXiv preprint arXiv:2206.00927,",
            "year": 2022
        },
        {
            "authors": [
                "Cheng Lu",
                "Yuhao Zhou",
                "Fan Bao",
                "Jianfei Chen",
                "Chongxuan Li",
                "Jun Zhu"
            ],
            "title": "Dpm-solver++: Fast solver for guided sampling of diffusion probabilistic models",
            "venue": "arXiv preprint arXiv:2211.01095,",
            "year": 2022
        },
        {
            "authors": [
                "Alexander Quinn Nichol",
                "Prafulla Dhariwal"
            ],
            "title": "Improved denoising diffusion probabilistic models",
            "venue": "In Int. Conf. Mach. Learn.,",
            "year": 2021
        },
        {
            "authors": [
                "Dustin Podell",
                "Zion English",
                "Kyle Lacey",
                "Andreas Blattmann",
                "Tim Dockhorn",
                "Jonas M\u00fcller",
                "Joe Penna",
                "Robin Rombach"
            ],
            "title": "Sdxl: improving latent diffusion models for high-resolution image synthesis",
            "year": 1952
        },
        {
            "authors": [
                "Aditya Ramesh",
                "Prafulla Dhariwal",
                "Alex Nichol",
                "Casey Chu",
                "Mark Chen"
            ],
            "title": "Hierarchical textconditional image generation with clip latents",
            "venue": "arXiv preprint arXiv:2204.06125,",
            "year": 2022
        },
        {
            "authors": [
                "Robin Rombach",
                "Andreas Blattmann",
                "Dominik Lorenz",
                "Patrick Esser",
                "Bj\u00f6rn Ommer"
            ],
            "title": "Highresolution image synthesis with latent diffusion models",
            "venue": "In IEEE Conf. Comput. Vis. Pattern Recog.,",
            "year": 2022
        },
        {
            "authors": [
                "Chitwan Saharia",
                "William Chan",
                "Saurabh Saxena",
                "Lala Li",
                "Jay Whang",
                "Emily Denton",
                "Seyed Kamyar Seyed Ghasemipour",
                "Burcu Karagol Ayan",
                "S Sara Mahdavi",
                "Rapha Gontijo Lopes"
            ],
            "title": "Photorealistic text-to-image diffusion models with deep language understanding",
            "venue": "arXiv preprint arXiv:2205.11487,",
            "year": 2022
        },
        {
            "authors": [
                "Tim Salimans",
                "Jonathan Ho"
            ],
            "title": "Progressive distillation for fast sampling of diffusion models",
            "venue": "arXiv preprint arXiv:2202.00512,",
            "year": 2022
        },
        {
            "authors": [
                "Jascha Sohl-Dickstein",
                "Eric Weiss",
                "Niru Maheswaranathan",
                "Surya Ganguli"
            ],
            "title": "Deep unsupervised learning using nonequilibrium thermodynamics",
            "venue": "In Int. Conf. Mach. Learn.,",
            "year": 2015
        },
        {
            "authors": [
                "Jiaming Song",
                "Chenlin Meng",
                "Stefano Ermon"
            ],
            "title": "Denoising diffusion implicit models",
            "venue": "arXiv preprint arXiv:2010.02502,",
            "year": 2020
        },
        {
            "authors": [
                "Yang Song",
                "Conor Durkan",
                "Iain Murray",
                "Stefano Ermon"
            ],
            "title": "Maximum likelihood training of scorebased diffusion models",
            "venue": "Adv. Neural Inform. Process. Syst.,",
            "year": 2021
        },
        {
            "authors": [
                "Yang Song",
                "Jascha Sohl-Dickstein",
                "Diederik P Kingma",
                "Abhishek Kumar",
                "Stefano Ermon",
                "Ben Poole"
            ],
            "title": "Score-based generative modeling through stochastic differential equations",
            "venue": "Int. Conf. Learn. Represent.,",
            "year": 2021
        },
        {
            "authors": [
                "Arash Vahdat",
                "Karsten Kreis",
                "Jan Kautz"
            ],
            "title": "Score-based generative modeling in latent space",
            "venue": "Adv. Neural Inform. Process. Syst.,",
            "year": 2021
        },
        {
            "authors": [
                "Fisher Yu",
                "Ari Seff",
                "Yinda Zhang",
                "Shuran Song",
                "Thomas Funkhouser",
                "Jianxiong Xiao"
            ],
            "title": "Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop",
            "venue": "arXiv preprint arXiv:1506.03365,",
            "year": 2015
        },
        {
            "authors": [
                "Han Zhang",
                "Ruili Feng",
                "Zhantao Yang",
                "Lianghua Huang",
                "Yu Liu",
                "Yifei Zhang",
                "Yujun Shen",
                "Deli Zhao",
                "Jingren Zhou",
                "Fan Cheng"
            ],
            "title": "Dimensionality-varying diffusion process",
            "venue": "In IEEE Conf. Comput. Vis. Pattern Recog.,",
            "year": 2023
        },
        {
            "authors": [
                "Lvmin Zhang",
                "Maneesh Agrawala"
            ],
            "title": "Adding conditional control to text-to-image diffusion models",
            "venue": "arXiv preprint arXiv:2302.05543,",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "The rapid development of diffusion models has been witnessed in image synthesis (Ho et al., 2020; Song et al., 2020; Ramesh et al., 2022; Saharia et al., 2022; Rombach et al., 2022; Zhang & Agrawala, 2023; Hoogeboom et al., 2023) in the past few years. Concretely, diffusion models construct a multi-step process to destroy a signal by gradually adding noises to it. That way, reversing the diffusion process (i.e., denoising) at each step naturally admits a sampling capability. In essence, the sampling process involves solving a reverse-time stochastic differential equation (SDE) through integrals (Song et al., 2021b).\nAlthough diffusion models have achieved great success in image synthesis, the rationality of the diffusion process itself has received limited attention, leaving the open question of whether the problem is well-posed and well-conditioned. In this paper, we surprisingly observe that the noiseprediction (Ho et al., 2020) and v-prediction (Salimans & Ho, 2022) diffusion models often exhibit a perplexing tendency to possess infinite Lipschitz of network with respect to time variable near the zero point. We provide theoretical proofs to illustrate the presence of infinite Lipschitz constants\n\u2020 Corresponding author, \u22c6 Work performed at Alibaba Academy, \u22c4 Project leader\nPublished as a conference paper at ICLR 2024\n\ud835\udc65! \ud835\udc65!\"# \ud835\udc65 $%&# \ud835\udc65 $%\u22ef\n\ud835\udc65 $%\"#\ud835\udc65%!\ud835\udc65%\"\"#\ud835\udc65#\ud835\udc65(\n(\u0399) DDPM\n\ud835\udc65! \ud835\udc65!\"# \ud835\udc65 $%&# \ud835\udc65 $%\u22ef\n\ud835\udc65 $%\"#\ud835\udc65%!\ud835\udc65%\"\"#\ud835\udc65#\ud835\udc65(\n(\u0399\u0399) E-TSDM \ud835\udc61 = \ud835\udc47 \u2212 1 \ud835\udc61 = ?\u0303?\nSame Procedure\nDifferent Procedure\n\ud835\udc61 = 0 \ud835\udc61 = \ud835\udc61# \u2212 1 \ud835\udc61 = ?\u0303? \u2212 1\n\ud835\udc61 = 0 \ud835\udc61 = \ud835\udc61)\n\u22ef \u22ef 0 2 4 6 8 10 12 14 16 18 20 22 24\n10 100 1,000\nE-TSDM\nDDPM 0 2 4 6 8 10 12 14 16 18 20 22 24\n10 100 1,000\n0 2 4 6 8 10 12 14 16 18 20 22 24 10 100 1,0000 2 4 6 8 10 12 14 16 18 20 22 24 10 100 1,0000 2 4 6 8 10 12 14 16 18 20 22 24 10 100 1,000 0 2 4 6 8 10 12 14 16 18 20 22 24\n10 100 1,000\nE-TSDM DDPM 0 2 4 6 8 10 12 14 16 18 20 22 24\n10 100 1,000\nE-TSDM DDPM\n0 2 4 6 8 10 12 14 16 18 20 22 24\n10 100 1,000 0 2 4 6 8 10 12 14 16 18 20 22 24\n10 100 1,000\nE-TSDM\nDDPM E-TSD 0 2 4 6 8 10 12 14 16 18 20 22 24\n10 100 1,000\nE-TSDM\nDDPM 0 2 4 6 8 10 12 14 16 18 20 22 24\n10 100 1,000\nE-TSDM\nDDPM\n24\n20\n16\n12\n8\n4\n0 0 1000\n0\nLi ps\nch itz\nC on\nst an\nts\nTimesteps\n(a) Conceptual comparison (b) Lipschitz constants\nFigure 1: (a) Conceptual comparison between DDPM (Ho et al., 2020) (I) and our proposed early timestep-shared diffusion model (E-TSDM) (II). DDPM trains the network \u03f5\u03b8(\u00b7, t) with varying timestep conditions t at each denoising step, whereas E-TSDM uniformly divides the near-zero timestep interval t \u2208 [0, t\u0303) with high Lipschitz constants into n sub-intervals and shares the condition t within each sub-interval. Here, t\u0303 denotes the length of the interval for sharing conditions. When t \u2265 t\u0303, E-TSDM follows the same procedure as DDPM. However, when t < t\u0303, E-TSDM shares timestep conditions. (b) Quantitative comparison of the Lipschitz constants between DDPM and our proposed early timestep-shared diffusion model (E-TSDM). The Lipschitz constants tend to be extremely large near zero point for DDPM. However, our sharing approach allows E-TSDM to force the Lipschitz constants in each sub-interval to be zero, thereby reducing the overall Lipschitz constants in the timestep interval t \u2208 [0, t\u0303), where t\u0303 is set as a default value 100.\nand empirical results to confirm it. Given that noise prediction and v-prediction are widely adopted by popular diffusion models (Dhariwal & Nichol, 2021; Rombach et al., 2022; Ramesh et al., 2022; Saharia et al., 2022; Podell et al., 2023), the presence of large Lipschitz constants is a significant problem for the diffusion model community.\nSince we uniformly sample timesteps for both training and inference processes, large Lipschitz constants w.r.t. time variable pose a significant threat to both training and inference processes of diffusion models. When training, large Lipschitz constants near the zero point affect the training of other parts due to the smooth nature of the network, resulting in instability and inaccuracy. Moreover, since inference requires a smooth network for integration, the large Lipschitz constants probably have a substantial impact on accuracy, particularly for faster samplers. Therefore, the mitigation of Lipschitz singularities holds great potential for enhancing the performance of diffusion models.\nFortunately, there is a simple yet effective alternative solution: by sharing the timestep conditions in the interval with large Lipschitz constants, the Lipschitz constants can be set to zero. Based on this idea, we propose a practical approach, which uniformly divides the target interval near the zero point into n sub-intervals, and uses the same condition values in each sub-interval, as shown in Figure 1 (II). By doing so, this approach can effectively reduce the Lipschitz constants near t = 0 to zero. To validate this idea, we conduct extensive experiments, including unconditional generation on various datasets, acceleration of sampling, and super-resolution task. Both qualitative and quantitative results confirm that our approach substantially alleviates the large Lipschitz constants near zero point and improves the synthesis performance compared to the DDPM baseline (Ho et al., 2020). We also compare this simple approach with other potential methods to address the challenge of large Lipschitz constants, and find our method outperforms all of these alternative methods. In conclusion, in this work, we theoretically prove and empirically observe the presence of Lipschitz singularities issue near the zero point, advancing the understanding of the diffusion process. Besides, we propose a simple yet effective approach to address this challenge and achieve impressive improvements."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "The significant advancements of diffusion models have been witnessed in recent years in the domain of image generation. (Karras et al., 2022; Lu et al., 2022b; Dockhorn et al., 2021; Bao et al., 2022b; Lu et al., 2022a; Bao et al., 2022a; Zhang et al., 2023). It (Sohl-Dickstein et al., 2015; Ho et al.,\n2020; Song et al., 2021b) defines a Markovian forward process {xt}t\u2208[0,T ] that gradually destroys the data x0 with Gaussian noise. For any t \u2208 [0, T ], the conditional distribution q0t(xt|x0) satisfies\nq0t (xt|x0) = N ( xt|\u03b1tx0, \u03c32t I ) , (1)\nwhere \u03b1t and \u03c3t are referred to as the noise schedule, satisfying \u03b12t+\u03c3 2 t = 1. Generally, \u03b1t decreases from 1 to 0 as t increases, to ensure that the marginal distribution of xt gradually changes from the data distribution q0(x0) to Gaussian. Kingma et al. (2021) further prove that the following stochastic differential equation (SDE) has the same transition distribution q0t(xt|x0) as in Equation (1) for any t \u2208 [0, T ]: dxt = f (t)xtdt+ g (t) dwt, x0 \u223c q0 (x0) , (2) where wt is the standard Wiener process, f(t) = d log\u03b1tdt and g(t) = 2\u03c3 2 t d log(\u03c3t/\u03b1t) dt .\nSong et al. (2021b) point out that the following reverse-time SDE has the same marginal distribution qt(xt) for any t \u2208 [0, T ]:\ndxt = [f (t)xt \u2212 g (t)2 \u2207xt log qt (xt)]dt+ g (t) dw\u0304t, xT \u223c qT (xT ) , (3) where w\u0304t is a standard Wiener process in the reverse time. Once the score function \u2207xt log qt(xt) is known, we can simulate Equation (3) for sampling. However, directly learning the score function is problematic, as it involves an explosion of training loss when having a small \u03c3t(Song et al., 2021b). In practice, the noise prediction model \u03f5\u03b8(xt, t) is often adopted to estimate \u2212\u03c3t\u2207xt log qt(xt). The network \u03f5\u03b8(xt, t) can be trained by minimizing the objective:\nL (\u03b8) := Et\u223cU(0,T ),x0\u223cq0(x0),\u03f5\u223cN (0,I) [ \u2225\u03f5\u03b8 (\u03b1tx0 + \u03c3t\u03f5, t)\u2212 \u03f5\u222522 ] . (4)\nIn this work, our observation of Lipschitz singularities on noise-prediction and v-prediction diffusion models reveals the inherent price of such an approach.\nNumerical stability near zero point. Achieving numerical stability is essential for high-quality samples in diffusion models, where the sampling process involves solving a reverse-time SDE.\nNevertheless, numerical instability is frequently observed near t = 0 in practice (Song et al., 2021a; Vahdat et al., 2021). To address this singularity, one possible approach is to set a small non-zero starting time \u03c4 > 0 in both training and inference (Song et al., 2021a; Vahdat et al., 2021). Kim et al. (2022) resolve the trade-off between density estimation and sample generation performance by introducing randomization to the fixed \u03c4 . In contrast, we enhance numerical stability by reducing the Lipschitz constants to zero near t = 0, which leads to improved sample quality in diffusion models. It is worth noting that the numerical issues observed by aforementioned works are mainly caused by the singularity of transition kernel q0t(xt|x0). This transition kernel will degrade to a Dirac kernel \u03b4(xt \u2212 \u03b1tx0) as \u03c3t \u2192 0. However, our observation is the infinite Lipschitz constants of the noise prediction model \u03f5\u03b8 (x, t) w.r.t time variable t, and this is caused by the explosion of d\u03c3tdt as t \u2192 0. To the best of our knowledge, this has not been observed before."
        },
        {
            "heading": "3 LIPSCHITZ SINGULARITIES IN DIFFUSION MODELS",
            "text": "Lipschitz singularities issue. In this section, we elucidate the vexing propensity of diffusion models to exhibit infinite Lipschitz near the zero point. We achieve this by analyzing the partial derivative \u2202\u03f5\u03b8(x, t)/\u2202t of the network \u03f5\u03b8(x, t). In essence, the emergence of Lipschitz singularities, characterized by lim supt\u21920+\n\u2225\u2225\u2225\u2202\u03f5\u03b8(x,t)\u2202t \u2225\u2225\u2225 \u2192 \u221e, can be attributed to the fact that the prevailing noise schedules conform to the behavior of d\u03c3t/dt \u2192 \u221e as the parameter t tends towards zero.\nTheoretical analysis. Now we theoretically prove that the infinite Lipschitz happens near the zero point in diffusion models, where the distribution of data is an arbitrary complex distribution. We focus particularly on the scenario where the network \u03f5\u03b8(x, t) is trained to predict the noises added to images (v-prediction model (Salimans & Ho, 2022) has a similar singularity problem, and is analyzed in Appendix C.2). The network \u03f5\u03b8(x, t) exhibits a relationship with the score function \u2207x log qt(x) that \u03f5\u03b8(x, t) = \u2212\u03c3t\u2207x log qt(x) (Song et al., 2021b), where \u03c3t is the standard deviation of the forward transition distribution q0t(x|x0) = N (x;\u03b1tx0, \u03c32t I). Specifically, \u03b1t and \u03c3t satisfy \u03b12t + \u03c3 2 t = 1.\nTheorem 3.1 Given a noise schedule, since \u03c3t = \u221a\n1\u2212 \u03b12t , we have d\u03c3tdt = \u2212 \u03b1t\u221a 1\u2212\u03b12t d\u03b1t dt . As t gets\nclose to 0, the noise schedule requires \u03b1t \u2192 1, leading to d\u03c3t/dt \u2192 \u221e as long as d\u03b1tdt |t=0 \u0338= 0. The partial derivative of the network can be written as\n\u2202\u03f5\u03b8 (x, t)\n\u2202t = \u03b1t\u221a 1\u2212 \u03b12t d\u03b1t dt \u2207x log qt (x)\u2212 \u2202\u2207x log qt (x) \u2202t \u03c3t. (5)\nNote that \u03b1t \u2192 1 as t \u2192 0, thus if d\u03b1tdt |t=0 \u0338= 0, and \u2207x log qt(x)|t=0 \u0338= 0, then one of the following two must stand\nlim sup t\u21920+ \u2225\u2225\u2225\u2225\u2202\u03f5\u03b8 (x, t)\u2202t \u2225\u2225\u2225\u2225\u2192 \u221e; lim sup t\u21920+ \u2225\u2225\u2225\u2225\u2202\u2207x log qt (x)\u2202t \u03c3t \u2225\u2225\u2225\u2225\u2192 \u221e. (6)\nNote that d\u03b1tdt |t=0 \u0338= 0 stands for a wide range of noise schedules, including linear, cosine, and quadratic schedules (see details in Appendix C.1). Besides, we can safely assume that qt(x) is a smooth process. Therefore, we may often have lim supt\u21920+ \u2225\u2225\u2202\u03f5\u03b8(x,t) \u2202t \u2225\u2225 \u2192 \u221e, indicating the infinite Lipschitz constants around t = 0.\nSimple case illustration. Take a simple case that the distribution of data p(x0) \u223c N (0, I) for instance, the score function for any t \u2208 [0, T ] can be written as\n\u2207x log qt (x) = \u2207x log (\n1\u221a 2\u03c0 exp\n( \u2212\u2225x\u2225 2 2\n2\n)) = \u2212x. (7)\nDue to the relationship \u03f5\u03b8(x, t) = \u2212\u03c3t\u2207x log qt(x) and the fact that the deviation d\u03c3tdt tends toward \u221e as t \u2192 0, we have \u2225\u2225\u2202\u03f5\u03b8(x,t) \u2202t \u2225\u2225\u2192 \u221e. Case in reality. After theoretically proving that diffusion models suffer infinite Lipschitz near the zero point, we show it empirically. We estimate the Lipschitz constants of a network by\nK(t, t\u2032) = Ext [\u2225\u03f5\u03b8 (xt, t)\u2212 \u03f5\u03b8 (xt, t\u2032)]\u22252]\n\u2206t , (8)\nwhere \u2206t = |t\u2212 t\u2032|. For a network \u03f5\u03b8(xt, t\u2032) of DDPM baseline (Ho et al., 2020) trained on FFHQ 256 \u00d7 256 (Karras et al., 2019) (see training details in Section 5.1 and more results of the Lipschitz constants K(t, t\u2032) on other datasets in Appendix D.1), the variation of the Lipschitz constants K(t, t\u2032) as the noise level t varies is seen in Figure 1b, showing that the Lipschitz constants K(t, t\u2032) get extremely large in the interval with low noise levels. Such large Lipschitz constants support the above theoretical analysis and pose a threat to the stability and accuracy of the diffusion process, which relies on integral operations."
        },
        {
            "heading": "4 MITIGATING LIPSCHITZ SINGULARITIES BY SHARING CONDITIONS",
            "text": "Proposed method. In this section, we propose the Early Timestep-shared Diffusion Model (ETSDM), which aims to alleviate the Lipschitz singularities by sharing the timestep conditions in the interval with large Lipschitz constants. To avoid impairing the network\u2019s ability, E-TSDM performs a stepwise operation of sharing timestep condition values. Specifically, we consider the interval near the zero point suffering from large Lipschitz constants, denoted as [0, t\u0303), where t\u0303 indicates the length of the target interval. E-TSDM uniformly divides this interval into n sub-intervals represented as a sequence T = {t0, t1, \u00b7 \u00b7 \u00b7 , tn}, where 0 = t0 < t1 < \u00b7 \u00b7 \u00b7 < tn = t\u0303 and t1 \u2212 t0 = ti \u2212\nti\u22121,\u2200i = 1, 2, \u00b7 \u00b7 \u00b7 , n. For each sub-interval, E-TSDM employs a single timestep value (the left endpoint of the sub-interval) as the condition, both during training and inference. Utilizing this strategy, E-TSDM effectively enforces zero Lipschitz constants within each sub-interval, with only the timesteps located near the boundaries of the sub-intervals having a Lipschitz constant greater than zero. As a result, the overall Lipschitz constants of the target interval t \u2208 [0, t\u0303) are significantly reduced. The corresponding training loss can be written as\nL (\u03f5\u03b8) := Et\u223cU(0,T ),x0\u223cq(x0),\u03f5\u223cN (0,I) [ \u2225\u03f5\u03b8 (\u03b1tx0 + \u03c3t\u03f5, fT (t))\u2212 \u03f5\u222522 ] , (9)\nwhere fT(t) = max1\u2264i\u2264n{ti\u22121 \u2208 T : ti\u22121 \u2264 t} for t < t\u0303, while fT(t) = t for t \u2265 t\u0303. The corresponding reverse process can be represented as\np\u03b8 (xt\u22121|xt) = N ( xt\u22121;\n\u03b1t\u22121 \u03b1t\n( xt \u2212\n\u03b2t \u03c3t \u03f5\u03b8 (xt, fT (t))\n) , \u03b72t I ) , (10)\nwhere \u03b2t = 1 \u2212 \u03b1t\u03b1t\u22121 , and \u03b7 2 t = \u03b2t. E-TSDM is easy to implement, and the algorithm details are provided in Appendix B.2.\nAnalysis of estimation error. Then we show that the estimation error of E-TSDM can be bounded by an infinitesimal, and thus the impact of E-TSDM on the estimation accuracy is insignificant. The detailed proof is shown in Appendix C.3.\nTheorem 4.1 Given the chosen fT(t), when t \u2208 [0, t\u0303), the difference between the optimal \u03f5\u03b8(x, fT(t)) denoted as \u03f5\u2217(x, fT(t)), and \u03f5(x, t) = \u2212\u03c3t\u2207x log qt(x), can be bounded by\n\u2225\u03f5\u2217 (x, fT (t))\u2212 \u03f5 (x, t)\u2225 \u2264 \u03c3t\u0303K (x)\u2206t+B (x)\u2206\u03c3max, (11)\nwhere\nK (x) = sup t\u0338=\u03c4 \u2225\u2207x log qt (x)\u2212\u2207x log q\u03c4 (x) \u2225 |t\u2212 \u03c4 | , B (x) = sup t \u2225\u2207x log qt (x) \u2225, (12)\nand \u2206\u03c3max = max1\u2264i\u2264n |\u03c3ti \u2212\u03c3ti\u22121 |. Note that K(x) and B(x) are finite and lim\u2206t\u21920 \u2206\u03c3max = 0 for any continuous \u03c3t where \u2206t:=t\u0303/n, thus the difference converges to 0 as \u2206t \u2192 0. Furthermore, the rate of convergence is at least 12 -order with respect to \u2206t.\nThe 12 -order convergence rate is relatively fast in optimization. Given this bound, we think the introduced errors of E-TSDM are controllable.\nReduction in Lipschitz constants. In Figure 1b, we present the curve of K(t, t\u2032) of E-TSDM on FFHQ 256\u00d7 256 (Karras et al., 2019) (we provide results for continuous-time diffusion models and more results on other datasets in Appendix D.1), showing that the Lipschitz constants K(t, t\u2032) are significantly reduced by applying E-TSDM.\nImprovement in stability. To further verify the stability of E-TSDM, we evaluate the impact of a small perturbation added to the input. Specifically, we add a small noise with a growing scale to the xt\u0303, where t\u0303 is set to a default value of 100, and observe the resulting difference in the predicted value of x0, for both E-TSDM and baseline. Our results, as shown in Figure 2, illustrate that E-TSDM exhibits better stability than the baseline, as its predictions are less affected by perturbations.\nComparison with some alternative methods. Although achieving impressive performance as detailed in Section 5, E-TSDM introduces no modifications to the network architecture or loss function, thereby not incurring any additional computational cost. 1) Regularization: In contrast, an alternative potential approach is imposing restrictions on the Lipschitz constants via regularization techniques. It necessitates the computation of \u2202\u03f5\u03b8(x,t)\u2202t , consequently diminishing training efficiency. 2) Modification of noise schedules: Furthermore, E-TSDM preserves the forward process unaltered. Conversely, another potential method involves the modification of noise schedules. Recall that the issue of Lipschitz singularities only arises when the noise schedule satisfies d\u03b1tdt |t=0 \u0338= 0. Therefore, it becomes feasible to adjust the noise schedule to meet the requirement d\u03b1tdt |t=0 = 0, thus mitigating the problem of Lipschitz singularities. The detailed methods for modifying noise schedules are provided in Appendix D.3.2. Although this modification seems feasible, it results in tiny amounts of noise at the beginning stages of the diffusion process, leading to inaccurate predictions. 3) Remap: In addition, remap is another possible method, which designs a remap function \u03bb = f(t) as the conditional input of the network, namely, \u03f5\u03b8(x, f(t)). By carefully designing \u03bb = f(t), it can significantly stretch the interval with large Lipschitz constants. For example, f(t) = 1/t and f\u22121(\u03bb) = sigmoid(\u03bb) are two simple choices. In this way, Remap can efficiently reduce the Lipschitz constants regarding the conditional inputs of the network, \u2202\u03f5\u03b8(x,t)\u2202\u03bb . However, since we uniformly sample t both in training and inference, what should be focused on is the Lipschitz constants regarding t, \u2202\u03f5\u03b8(x,t)\u2202t , which can not be influenced by remap. We also consider the situation of uniformly sampling \u03bb, which can significantly hurt the quality of generated images. We show the quantitative evaluation in Figure 3 and put the detailed analysis in Appendix D.3.3. Empirically, E-TSDM surpasses not only the baseline but also all of these alternative methods, where the results are demonstrated in Figure 3. For a more in-depth discussions, please refer to Section D.3."
        },
        {
            "heading": "5 EXPERIMENTS",
            "text": "In this section, we present compelling evidence that our E-TSDM outperforms existing approaches on a variety of datasets. To achieve this, we first detail the experimental setup used in our studies in Section 5.1. Subsequently, in Section 5.2, we compare the synthesis performance of E-TSDM against that of the baseline on various datasets. In Section 5.3, we conduct multiple ablation studies and quantitative analysis from two perspectives. Firstly, we demonstrate the generalizability of\nE-TSDM by implementing it on continuous-time diffusion models and varying the noise schedules. Secondly, we investigate the impact of varying the number of conditions n in t \u2208 [0, t\u0303) and the length of the interval t\u0303, which are important hyperparameters. Moreover, we demonstrate in Section 5.4 that our method can be effectively combined with popular fast sampling techniques. Finally, we show that E-TSDM can be applied to conditional generation tasks, such as super-resolution, in Section 5.5."
        },
        {
            "heading": "5.1 EXPERIMENTAL SETUP",
            "text": "Implementation details. All of our experiments utilize the settings of DDPM (Ho et al., 2020) (see more details in Appendix B.1). Besides, we utilize a more developed structure of unet (Dhariwal & Nichol, 2021) than that of DDPM (Ho et al., 2020) for all experiments containing reproduced baseline. Given that the model size is kept constant, the speed and memory requirements for training and inference for both the baseline and E-TSDM are the same. Except for the ablation studies in Section 5.3, all other experiments fix t\u0303 = 100 for E-TSDM and use five conditions (n = 5) in the interval t \u2208 [0, t\u0303), which we have found to be a relatively good choice in practice. Furthermore, all experiments are trained on NVIDIA A100 GPUs. Datasets. We implement E-TSDM on several widely evaluated datasets containing FFHQ 256 \u00d7 256 (Karras et al., 2019), CelebAHQ 256 \u00d7 256 (Karras et al., 2017), AFHQ-Cat 256\u00d7 256, AFHQ-Wild 256\u00d7 256 (Choi et al., 2020), LsunChurch 256 \u00d7 256 and Lsun-Cat 256 \u00d7 256 (Yu et al., 2015). Evaluation metrics. To assess the sampling quality of E-TSDM, we utilize the widely-adopted Frechet inception distance (FID) metric (Heusel et al., 2017). Additionally, we use the peak signal-to-noise ratio (PSNR) to evaluate the performance of the super-resolution task."
        },
        {
            "heading": "5.2 SYNTHESIS PERFORMANCE",
            "text": "We have demonstrated that E-TSDM can effectively mitigate the large Lipschitz constants near t = 0 in Figure 1 b, as detailed in Section 4. In this section, we conduct a comprehensive comparison between E-TSDM and DDPM baseline (Ho et al., 2020) on various datasets to show that E-TSDM can improve the synthesis performance. The quantitative comparison is presented in Figure 4, which clearly illustrates that E-TSDM outperforms the baseline on all evaluated datasets. Furthermore, as depicted in Appendix D.5, the samples generated by E-TSDM on various datasets demonstrate its ability to generate high-fidelity images. Remarkably, to the best of our knowledge, as shown in Table 1, we set a new state-of-the-art benchmark for diffusion models on FFHQ 256\u00d7 256 (Karras et al., 2019) using a large version of our approach (see details in Appendix B.1)."
        },
        {
            "heading": "5.3 QUANTITATIVE ANALYSIS",
            "text": "In this section, we demonstrate the generalizability of E-TSDM by implementing it on continuoustime diffusion models and varying the noise schedules. In addition, to gain a deeper understanding of the properties of E-TSDM, we investigate the critical hyperparameters of E-TSDM by varying the length of the interval t\u0303 to share the timestep conditions, and the number of sub-intervals n."
        },
        {
            "heading": "5.3.1 QUANTITATIVE ANALYSIS ON THE GENERALIZABILITY OF E-TSDM",
            "text": "To ensure the generalizability of E-TSDM beyond specific settings of DDPM (Ho et al., 2020), we conduct a thorough investigation of E-TSDM on other popular noise schedules, as well as implement\na continuous-time version of E-TSDM. Specifically, we explore the three popular ones including linear, quadratic and cosine schedules (Nichol & Dhariwal, 2021), and two newly proposed ones, which are cosine-shift (Hoogeboom et al., 2023) and zero-terminal-SNR (Lin et al., 2023) schedules.\nAs shown in Table 2, our E-TSDM achieves excellent performance across different noise schedules. Besides, the comparison of Lipschitz constants between E-TSDM and baseline on different noise schedules, as illustrated in Appendix D.1, show that E-TSDM can mitigate the Lipschitz singularities issue besides the scenario of the linear schedule, highlighting that its effects are independent of the specific noise schedule. Additionally, the continuous-time version of E-TSDM outperforms the corresponding baseline, indicating that E-TSDM is effective for both continuous-time and discretetime diffusion models. We provide the curves of the Lipschitz constants K(t, t\u2032) in Figure A1 to compare continuous-time E-TSDM with its baseline on the linear schedule, showing that E-TSDM can mitigate Lipschitz singularities in the continuous-time scenario.\n5.3.2 QUANTITATIVE ANALYSIS ON n AND t\u0303\nE-TSDM involves dividing the target interval t \u2208 [0, t\u0303) with large Lipschitz constants into n subintervals and sharing timestep conditions within each sub-interval. Accordingly, the choices of t\u0303 and n have significant impacts on the performance of E-TSDM. Intuitively, t\u0303 should be a relatively small value, therefore representing an interval near zero point. As for n, it should not be too large or too small. If n is too small, it forces the network to adapt to too many noise levels with a single timestep condition, thus leading to inaccuracy. Conversely, if the value of n is set too large, the reduction of Lipschitz constants is insufficient, where the extreme situation is baseline.\nIn this section, we meticulously assess the impacts of t\u0303 and n on various datasets. We present the outcomes on FFHQ 256\u00d7256 (Karras et al., 2019) and CelebAHQ 256\u00d7256 (Karras et al., 2017) for each hyperparameter in Figure 5, while leaving the remaining results in Appendix D.2. Specifically, in the experiments of t\u0303, we maintain the length of each sub-interval, namely, t\u0303/n, unchanged, while in the experiments of n, we maintain the t\u0303 unchanged. The results for t\u0303 in Figure 5 a demonstrate that E-TSDM performs well when t\u0303 is relatively small. However, as t\u0303 increases, the performance of E-TSDM deteriorates gradually. Furthermore, the results for n are shown in Figure 5 b, from\nwhich we observe a rise in FID when n was too small, for instance, when n = 2. Conversely, when n is too large, such as n = 100, the performance deteriorates significantly. Although E-TSDM performs well for most n and t\u0303 values, considering the results on all of the evaluated datasets (see remaining results in Appendix D.2), n = 5 and t\u0303 = 100 are recommended to be good choices to avoid cumbersome searches or a good starting point for further exploration when applying E-TSDM."
        },
        {
            "heading": "5.4 FAST SAMPLING",
            "text": "With the development of fast sampling algorithms, it is crucial that E-TSDM can be effectively combined with classic fast samplers, such as DDIM (Song et al., 2020) and DPM-Solver (Lu et al., 2022b). To this end, we incorporate both DDIM (Song et al., 2020) and DPM-Solver (Lu et al., 2022b) into E-TSDM for fast sampling in this section. It is worth noting that the presence of large Lipschitz constants can have a more detrimental impact on the efficiency of fast sampling compared to full-timestep sampling, as numerical solvers typically depend on the similarity between function values and their derivatives on adjacent steps. When using fast sampling algorithms with larger discretization steps, it becomes necessary for the functions to exhibit better smoothness, which in turn corresponds to smaller Lipschitz constants. Hence, it is anticipated that the utilization of ETSDM will lead to an improvement in the generation performance of fast sampling methods.\nAs presented in Table 3, we observe that E-TSDM significantly outperforms the baseline when using the same number of function evaluations (NFE) for fast sampling, which is under expectation. Besides, the advantage of E-TSDM becomes more pronounced when using higher order sampler (from DDIM to DPM-Solver), indicating better smoothness when compared to the baseline. Notably, for both DDIM and DPM-Solver, we observe an abnormal phenomenon for baseline, whereby the performance deteriorates as NFE increases. This phenomenon has been previously noted by several works (Lu et al., 2022b;c; Li et al., 2023), but remains unexplained. Given that this phenomenon is not observed in E-TSDM, we hypothesize that it may be related to the improvement of smoothness of the learned network. We leave further verification of this hypothesis for future work."
        },
        {
            "heading": "5.5 CONDITIONAL GENERATION",
            "text": "In order to explore the potential for extending E-TSDM to conditional generation tasks, we further investigate its performance in the super-resolution task, which is one of the most popular conditional generation tasks. Specifically, we test E-TSDM on the FFHQ 256\u00d7256 dataset, using the 64\u00d764 \u2192 256 \u00d7 256 pixel resolution as our experimental settings. For the baseline in the super-resolution task, we utilize the same network structure and hyper-parameters as those employed in the baseline presented in Section 5.1, but incorporate a low-resolution image as an additional input. Besides, for E-TSDM, we adopt a general setting with n = 5 and t\u0303 = 100. As illustrated in Figure A12, we observe that the baseline tends to exhibit a color bias compared to real images, which is mitigated by E-TSDM. Quantitatively, our results indicate that E-TSDM outperforms the baseline on the test set, achieving an improvement in PSNR from 24.64 to 25.61. These findings suggest that E-TSDM holds considerable promise for application in conditional generation tasks."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "In this paper, we elaborate on the infinite Lipschitz of the diffusion model near the zero point from both theoretical and empirical perspectives, which hurts the stability and accuracy of the diffusion process. A novel E-TSDM is further proposed to mitigate the corresponding singularities in a timestep-sharing manner. Experimental results demonstrate the superiority of our method in both performance and adaptability to the baselines, including unconditional generation, conditional generation, and fast sampling. This paper may not only improve the performance of diffusion models, but also help to make up the critical research gap in the understanding of the rationality underlying the diffusion process.\nLimitations. Although E-TSDM has demonstrated significant improvements in various applications, it has yet to be verified on large-scale text-to-image generative models. As E-TSDM reduces the large Lipschitz constants by sharing conditions, it is possible to lead to a decrease in the effectiveness of large-scale generative models. Additionally, the reduction of Lipschitz constants to zero within each sub-interval in E-TSDM may introduce unknown and potentially harmful effects."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "We would like to thank the four anonymous reviewers for spending time and effort and bringing in constructive questions and suggestions, which helped us greatly to improve the quality of the paper. We would like to also thank the Program Chairs and Area Chairs for handling this paper and providing valuable and comprehensive comments. In addition, this research was funded by the Alibaba Innovative Research (AIR) project."
        },
        {
            "heading": "APPENDIX",
            "text": ""
        },
        {
            "heading": "A OVERVIEW",
            "text": "This supplementary material is organized as follows. First, to facilitate the reproducibility of our experiments, we present implementation details, including hyper-parameters in Appendix B.1 and algorithmic details in Appendix B.2. Next, in Appendix C, we provide all details of deduction involved in the main paper. Finally, we present additional experimental results in support of the effectiveness of E-TSDM.\nB IMPLEMENTATION DETAILS"
        },
        {
            "heading": "B.1 HYPER-PARAMETERS",
            "text": "The hyper-parameters used in our experiments are shown in Table A1, and we use identical hyperparameters for all evaluated datasets for both E-TSDM and their corresponding baselines. Specifically, we follow the hyper-parameters of DDPM (Ho et al., 2020) but adopt a more advanced structure of U-Net (Dhariwal & Nichol, 2021) with residual blocks from BigGAN (Brock et al., 2018). The network employs a block consisting of fully connected layers to encode the timestep, where the dimensionality of hidden layers for this block is determined by the timestep channels shown in Table A1. Moreover, we scale up the network to achieve the state-of-the-art results of diffusion\nTable A1: Hyper-parameters of E-TSDM and our reproduced baseline.\nNormal version Large version\nT 1,000 1,000 \u03b2t linear linear Model size 131M 692M Base channels 128 128 Channels multiple (1,1,2,2,4,4) (1,1,2,4,6,8) Heads channels 64 64 Self attention 32,16,8 32,64,8 Timestep channels 512 2048 BigGAN block \u2713 \u2713 Dropout 0.0 0.0 Learning rate 1e\u22124 1e\u22124 Batch size 96 64 Res blocks 2 4 EMA 0.9999 0.9999 Warmup steps 0 0 Gradient clip \u2717 \u2717\nmodels on FFHQ 256\u00d7 256 (Karras et al., 2019), and therefore we provide the hyper-parameters of the large version of E-TSDM in Table A1."
        },
        {
            "heading": "B.2 ALGORITHM DETAILS",
            "text": "In this section, we provide a detailed description of the E-TSDM algorithm, including the training and inference procedures as shown in Algorithm A1 and Algorithm A2, respectively. Our method is simple to implement and requires only a few steps. Firstly, a suitable length of the interval t\u0303 should be selected for sharing conditions, along with the corresponding number of timestep conditions n in the target interval t \u2208 [0, t\u0303). While performing a thorough search for different datasets can achieve better performance, the default settings t\u0303 = 100 and n = 5 are recommended when E-TSDM is applied without a thorough search.\nNext, the target interval t \u2208 [0, t\u0303) should be divided into n sub-intervals, and the boundaries for each sub-interval should be calculated to generate the partition schedule T = {t0, t1, . . . , tn}. Finally,\nAlgorithm A1 Training of E-TSDM\nRequire: The length of the target interval t\u0303. Require: The number of conditions n. Require: Model \u03f5\u03b8 to be trained. Require: Data set D.\n1: Uniformly divide the target interval t \u2208 [0, t\u0303) into n sub-intervals to get the corresponding timestep partition schedule T = {t0, t1, . . . , tn}. 2: repeat 3: x0 \u223c D 4: t \u223c Uniform({1, . . . , T}) 5: if t < t\u0303 then 6: t\u0302 = max1\u2264i\u2264n{ti\u22121 \u2208 T : ti\u22121 \u2264 t} 7: else 8: t\u0302 = t 9: end if\n10: \u03f5 \u223c N (0, I) 11: Take gradient descent step on 12: \u2207\u03b8\u2225\u03f5\u2212 \u03f5\u03b8(\u03b1tx0 + \u03c3t\u03f5, t\u0302)\u22252 13: until converged\nAlgorithm A2 Sampling of E-TSDM\nRequire: The length of the target interval t\u0303. Require: The number of conditions n. Require: A trained model \u03f5\u03b8.\n1: Uniformly divide the target interval t \u2208 [0, t\u0303) into n sub-intervals to get the corresponding timestep partition schedule T = {t0, t1, . . . , tn}. 2: xT \u223c N (0, I) 3: for t = T, . . . , 1 do 4: if t < t\u0303 then 5: t\u0302 = max1\u2264i\u2264n{ti\u22121 \u2208 T : ti\u22121 \u2264 t} 6: else 7: t\u0302 = t 8: end if 9: if t > 1 then\n10: z \u223c N (0, I) 11: else 12: z = 0 13: end if 14: xt\u22121 =\n\u03b1t\u22121 \u03b1t ( xt \u2212 \u03b2t\u03c3t \u03f5\u03b8(xt, t\u0302) ) + \u03b7tz\n15: end for 16: return x0\nduring both training and sampling, the corresponding left boundary t\u0302 for each timestep in the target interval t \u2208 [0, t\u0303) should be determined according to T, and used as the conditional input of the network instead of t."
        },
        {
            "heading": "C DERIVATION OF FORMULAS",
            "text": "In this section, we provide detailed derivations as a supplement to the main paper. The derivations are divided into three parts, firstly we prove that the key assumption of the occurrence of Lipschitz singularities, d\u03b1tdt \u2223\u2223 t=0\n\u0338= 0, holds for mainstream noise schedules including linear, quadratic, and cosine schedules. Therefore, all of the diffusion models utilizing these noise schedules suffer from the issue of Lipschitz singularities. Then we show that Lipschitz singularities also plague the vprediction (Salimans & Ho, 2022) models. Considering that most of the diffusion models are noiseprediction or v-prediction models, the Lipschitz singularities problem is an important issue for the\nPublished as a conference paper at ICLR 2024\nTimestep\n0 2 4 6 8 10 12 14 16 18 20 22 24\n10 100 1,000\nE-TSDM\nDDPM 0 2 4 6 8 10 12 14 16 18 20 22 24\n10 100 1,000\nE-TSDM DDPM\n0 2 4 6 8 10 12 14 16 18 20 22 24\n10 100 1,000\n0 2 4 6 8 10 12 14 16 18 20 22 24 10 100 1,000 0 2 4 6 8 10 12 14 16 18 20 22 24 10 100 1,0000 2 4 6 8 10 12 14 16 18 20 22 24\n10 100 1,000 0 2 4 6 8 10 12 14 16 18 20 22 24\n10 100 1,000\nE-TSDM\nDDPM E-TSDM\n24\n20\n16 12 0\n100 100010 Li\nps ch\nitz C\non sta\nnt s\nFigure A1: Quantitative comparison of the Lipschitz constants between continuous-time ETSDM and continuous-time DDPM (Ho et al., 2020). Results show that E-TSDM can efficiently reduce the Lipschitz constants in continuous-time scenarios.\ncommunity of diffusion models. Finally, we demonstrate the detailed derivation of Theorem 4.1, showing that the errors introduced by E-TSDM can be bounded by an infinitesimal and thus are insignificant.\nC.1 d\u03b1t/dt FOR WIDELY USED NOISE SCHEDULES AT ZERO POINT\nWe have already shown that for an arbitrary complex distribution, given a noise schedule, if d\u03b1t dt \u2223\u2223 t=0 \u0338= 0, then we often have lim supt\u21920+ \u2225\u2225\u2202\u03f5\u03b8(x,t) \u2202t \u2225\u2225 \u2192 \u221e, indicating the infinite Lipschitz constants around t = 0. In this section, we prove that d\u03b1tdt \u2223\u2223 t=0\n\u0338= 0 stands for three mainstream noise schedules including linear schedule, quadratic schedule and cosine schedule.\nC.1.1 d\u03b1t/dt FOR LINEAR AND QUADRATIC SCHEDULES AT ZERO POINT\nLinear and quadratic schedules are first proposed by Ho et al. (2020). Both of them determine {\u03b1t}Tt=1 by a pre-designed positive sequence {\u03b2t}Tt=1 and the relationship \u03b1t := \u220ft i=1 \u221a 1\u2212 \u03b2i. Note that t \u2208 {1, 2, \u00b7 \u00b7 \u00b7 , T} is a discrete index, and {\u03b1t}Tt=1, {\u03b2t}Tt=1 are discrete parameter sequences in DDPM. However, \u03b1t in d\u03b1t/dt refers to the continuous-time parameter determined by the following score SDE (Song et al., 2021b)\ndx(\u03c4) = \u22121 2 \u03b2(\u03c4)x(\u03c4)d\u03c4 +\n\u221a \u03b2(\u03c4)dw, \u03c4 \u2208 [0, 1], (A1)\nwhere w is the standard Wiener process, \u03b2(\u03c4) is the continuous version of {\u03b2t}Tt=1 with a continuous time variable \u03c4 \u2208 [0, 1] for indexing, and the continuous-time \u03b1t = exp (\u2212 12 \u222b t 0 \u03b2(s)ds). To avoid ambiguity, let \u03b1(\u03c4), \u03c4 \u2208 [0, 1] denote the continuous version of {\u03b1t}Tt=1. Thus, d\u03b1(\u03c4)\nd\u03c4 \u2223\u2223\u2223\u2223 \u03c4=0 = \u22121 2 \u03b2(\u03c4) exp (\u22121 2 \u222b \u03c4 0 \u03b2(s)ds) \u2223\u2223\u2223\u2223 \u03c4=0 = \u22121 2 \u03b2(0). (A2)\nOnce the continuous function \u03b2(\u03c4) is determined for a specific noise schedule, we can obtain d\u03b1(\u03c4) d\u03c4 \u2223\u2223\u2223 \u03c4=0 immediately by Equation (A2).\nTo obtain \u03b2(\u03c4), we first give the expression of {\u03b2t}Tt=1 in linear and quadratic schedules (Ho et al., 2020)\nLinear: \u03b2t = \u03b2\u0304min T + ( \u03b2\u0304max T \u2212 \u03b2\u0304min T ) \u00b7 t\u2212 1 T \u2212 1 , (A3)\nQuadratic: \u03b2t = (\u221a \u03b2\u0304min T + (\u221a \u03b2\u0304max T \u2212 \u221a \u03b2\u0304min T ) \u00b7 t\u2212 1 T \u2212 1 )2 , (A4)\nLi ps\nch itz\nC on\nst an ts Li ps ch itz C on st an ts Li ps ch itz C on st an ts\nLi ps\nch itz\nC on\nst an\nts\nLi ps\nch itz\nC on\nst an ts Li ps ch itz C on st an ts\n(a) AFHQ-Cat 256\u00d7 256 (b) AFHQ-Wild 256\u00d7 256\n(c) Lsun-Cat 256\u00d7 256 (d) Lsun-Church 256\u00d7 256\n(e) CelebAHQ 256\u00d7 256 (f) FFHQ 256\u00d7 256 using quadratic schedule\nFigure A2: Quantitative comparison of Lipschitz constants between E-TSDM and DDPM baseline (Ho et al., 2020) on various datasets, including (a) AFHQ-Cat (Choi et al., 2020), (b) AFHQ-Wild (Choi et al., 2020), (c) Lsun-Cat 256 \u00d7 256 (Karras et al., 2019), (d) Lsun-Church 256 \u00d7 256 (Karras et al., 2019), and (e) CelebAHQ 256 \u00d7 256 (Karras et al., 2017) using the linear schedule. (f) Quantitative comparison of Lipschitz constants between E-TSDM and DDPM baseline (Ho et al., 2020) on FFHQ 256\u00d7 256 (Karras et al., 2019) using the quadratic schedule.\nwhere \u03b2\u0304min and \u03b2\u0304max are user-defined hyperparameters. Then, we define an auxiliary sequence {\u03b2\u0304t = T\u03b2t}Tt=1. In the limit of T \u2192 \u221e, {\u03b2\u0304t}Tt=1 becomes the function \u03b2(\u03c4) indexed by \u03c4 \u2208 [0, 1]\nLinear: \u03b2(\u03c4) = \u03b2\u0304min + ( \u03b2\u0304max \u2212 \u03b2\u0304min ) \u00b7 \u03c4, (A5)\nQuadratic: \u03b2(\u03c4) = (\u221a\n\u03b2\u0304min + (\u221a \u03b2\u0304max \u2212 \u221a \u03b2\u0304min ) \u00b7 \u03c4 )2 , (A6)\nThus, \u03b2(0) = \u03b2\u0304min for both linear and quadratic schedules, which leads to d\u03b1(\u03c4) d\u03c4 \u2223\u2223\u2223 \u03c4=0 = \u2212 12 \u03b2\u0304min. As a common setting, \u03b2\u0304min is a positive real number, thus d\u03b1(\u03c4) d\u03c4 \u2223\u2223\u2223 \u03c4=0 < 0.\n7.01\n7.49\n6.72 6.68 6.38\n6.07\n7.26\n5.5\n6\n6.5\n7\n7.5\n8\n8.5\n0 20 100 150 200 250 300\nLsun-Church 256 x 25614.69\n12.4 11.9811.84 12.46\n13.36\n11.93\n10\n11\n12\n13\n14\n15\n16\n0 20 100 150 200 250 300\nLsun-Cat 256 x 256\n7.19 6.52 6.63\n7.83 8.45\n7.04\n11.74\n5 6 7 8 9\n10 11 12\n0 20 100 150 200 250 300\nAFHQ-Cat 256 x 256\nFI D\n-1 0k\nInterval Length Interval Length Interval Length\n(a) AFHQ-Cat 256\u00d7 256 (b) LSUN-Cat 256\u00d7 256 (c) LSUN-Church 256\u00d7 256 Figure A3: Ablation study on the length of the interval t \u2208 [0, t\u0303) to share the timestep conditions, t\u0303, using FID-10k \u2193 as the evaluation metric.\n7.04 6.62 6.72\n7.4 7.51\n8.02\n7.01\n6\n6.5\n7\n7.5\n8\n8.5\n9\n1 2 5 10 20 50 100\nLsun-Church 256 x 256\n11.77 10.97\n11.98 11.7 11.8811.81\n14.69\n10\n11\n12\n13\n14\n15\n16\n1 2 5 10 20 50 100\nLsun-Cat 256 x 256\n7.17\n5.92\n6.63 7.18\n8.24\n10.18\n7.19\n5\n6\n7\n8\n9\n10\n11\n1 2 5 10 20 50 100\nAFHQ-Cat 256 x 256\nFI D\n-1 0k\n# Sub-intervals # Sub-intervals # Sub-intervals (a) AFHQ-Cat 256\u00d7 256 (b) LSUN-Cat 256\u00d7 256 (c) LSUN-Church 256\u00d7 256\nFigure A4: Ablation study on the number of sub-intervals in this interval, n, using FID-10k \u2193 as the evaluation metric.\nC.1.2 d\u03b1t/dt FOR THE COSINE SCHEDULE AT ZERO POINT\nThe cosine schedule is designed to prevent abrupt changes in noise level near t = 0 and t = T (Nichol & Dhariwal, 2021). Different from linear and quadratic schedules that define {\u03b1t}Tt=1 by a pre-designed sequence {\u03b2t}Tt=1, the cosine schedule directly defines {\u03b1t}Tt=1 as\n\u03b1t = f(t)\nf(0) , f(t) = cos\n( t/T + s\n1 + s \u00b7 \u03c0 2\n) , t = 1, 2, \u00b7 \u00b7 \u00b7 , T, (A7)\nwhere s is a small positive offset. The continuous version of {\u03b1t}Tt=1 can be obtained in the limit of T \u2192 \u221e as\n\u03b1(\u03c4) = cos\n( \u03c4 + s\n1 + s \u00b7 \u03c0 2\n) / cos ( s\n1 + s \u00b7 \u03c0 2\n) , \u03c4 \u2208 [0, 1]. (A8)\nWith Equation (A8), we can easily get d\u03b1(\u03c4)d\u03c4 \u2223\u2223\u2223 \u03c4=0\nd\u03b1(\u03c4)\nd\u03c4 \u2223\u2223\u2223\u2223 \u03c4=0 = \u2212 \u03c0 2(1 + s) tan ( s 1 + s \u00b7 \u03c0 2 ) , (A9)\nwhich leads to d\u03b1(\u03c4)d\u03c4 \u2223\u2223\u2223 \u03c4=0 < 0 since s > 0."
        },
        {
            "heading": "C.2 LIPSCHITZ SINGULARIES FOR V-PREDICTION DIFFUSION MODELS",
            "text": "In Section 3 of the main paper, we prove that noise-prediction diffusion models suffer from Lipschitz singularities issue. In this section, we show that the Lipschitz singularities issue is also an important problem for v-prediction diffusion models from both theoretical and empirical perspectives.\nLi ps\nch itz\nC on\nst an\nts\nLi ps\nch itz\nC on\nst an\nts\nTimestepTimestep (a) LSUN-Cat 256\u00d7 256 (b) FFHQ 256\u00d7 256\nFigure A5: Quantitative comparison of the Lipschitz constants between E-TSDM and DDPM (Ho et al., 2020) using v-prediction (Salimans & Ho, 2022) on Lsun-Cat 256\u00d7256 (Karras et al., 2019) and FFHQ 256\u00d7256 dataset (Karras et al., 2019). Results show that E-TSDM can efficiently reduce the Lipschitz constants in v-prediction scenarios.\nTheoretically, the optimal solution of v-prediction models is\nv(x, t) = argmin v\u03b8\nE[\u2225v\u03b8(xt, t)\u2212 (\u03b1t\u03f5\u2212 \u03c3tx0)\u222522|xt = x]\n= E[\u03b1t\u03f5\u2212 \u03c3tx0|xt = x] = E [ \u03b1t\u03f5\u2212 \u03c3t\nxt \u2212 \u03c3t\u03f5 \u03b1t\n\u2223\u2223\u2223\u2223xt = x]\n= \u2212\u03c3t \u03b1t x+ (\u03b1t + \u03c32t \u03b1t )E[\u03f5|xt = x]\n= \u2212\u03c3t \u03b1t x\u2212 \u03b1 2 t + \u03c3 2 t \u03b1t \u03c3t\u2207x log qt(x)\n= \u2212\u03c3t \u03b1t (x+\u2207x log qt(x)),\n(A10)\nwhere x+\u2207x log qt(x) is smooth under the assumption of Theorem 3.1, and ddt ( \u03c3t \u03b1t ) \u2192 d\u03c3tdt as t \u2192\n0. Thus, with the same derivation of Theorem 3.1, we can conclude that lim supt\u21920+ \u2225\u2225\u2225\u2202v(x,t)\u2202t \u2225\u2225\u2225 \u2192 \u221e. The detailed derivation goes as follows: Firstly, we can obtain the partial derivative of the v-prediction model over t as\n\u2202v(x, t) \u2202t = \u2212 d dt ( \u03c3t \u03b1t )(x+\u2207x log qt(x))\u2212 \u03c3t \u03b1t d dt (x+\u2207x log qt(x)). (A11)\nNote that ddt ( \u03c3t \u03b1t ) = 1 \u03b12t ( \u03b1t d\u03c3t dt \u2212 \u03c3t d\u03b1t dt ) \u2192 d\u03c3tdt = \u2212 \u03b1t\u221a 1\u2212\u03b12t d\u03b1t dt as t \u2192 0 under common settings\nthat \u03c30 = 0, \u03b10 = 1, and d\u03b1tdt \u2223\u2223 t=0 is finite, thus if d\u03b1tdt \u2223\u2223 t=0\n\u0338= 0, and x + \u2207x log qt(x) \u0338= 0, then one of the following two must stand\nlim sup t\u21920+ \u2225\u2225\u2225\u2225\u2202v(x, t)\u2202t \u2225\u2225\u2225\u2225\u2192 \u221e; lim sup t\u21920+ \u2225\u2225\u2225\u2225\u03c3t\u03b1t ddt (x+\u2207x log qt(x)) \u2225\u2225\u2225\u2225\u2192 \u221e. (A12)\nUnder the assumption that qt(x) is a smooth process, we can conclude that lim supt\u21920+ \u2225\u2225\u2225\u2202v(x,t)\u2202t \u2225\u2225\u2225\u2192 \u221e.\nTable A2: Quantitative comparison between E-TSDM and DDPM (Ho et al., 2020) using v-prediction on Lsun-Cat 256\u00d7256 (Karras et al., 2019) and FFHQ 256 \u00d7 256 dataset (Karras et al., 2019) evaluated with FID-10k \u2193. Experimental results indicate that E-TSDM can achieve better synthesis performance.\nBaseline E-TSDM\nFFHQ 10.85 9.00 Lsun-Cat 18.40 13.86\nTable A3: Quantitative comparison among ETSDM, DDPM (Ho et al., 2020), and DDPM using regularization techniques (DDPM-r) on FFHQ 256\u00d7 256 (Karras et al., 2019) and CelebAHQ 256\u00d7 256 (Karras et al., 2017) evaluated with FID-10k \u2193. Experimental results show that DDPM-r can slightly improve the FID but performs worse than E-TSDM.\nMethod Baseline E-TSDM DDPM-r\nFFHQ 9.50 6.62 9.18 CelebAHQ 8.05 6.99 7.97\nSince most of the diffusion models are noise-prediction and v-prediction models, the Lipschitz singularities issue is an important problem for the community of diffusion models.\nEmpirically, we can also observe the phenomenon of Lipschitz singularities for v-prediction diffusion models, where the experimental results of Lipschitz constants on FFHQ 256\u00d7256 dataset (Karras et al., 2019) and Lsun-Cat 256\u00d7256 (Karras et al., 2019) are shown in Figure A5, from which we can tell E-TSDM can effectively mitigate Lipschitz singularities in v-prediction scenario. Besides, we also provide corresponding quantitative evaluations evaluated by FID-10k in Table A2, showing that E-TSDM can also improve the synthesis performance in the v-prediction scenario."
        },
        {
            "heading": "C.3 PROOF OF THEOREM 4.1",
            "text": "Here we will first give the derivation of the upper-bound on \u2225\u03f5\u2217(x, fT(t))\u2212\u03f5(x, t)\u2225 when t \u2208 [0, t\u0303), where \u03f5\u2217(x, fT(t)) denotes the optimal \u03f5\u03b8(x, fT(t)), and \u03f5(x, t) = \u2212\u03c3t\u2207x log qt(x). Then, we will discuss the convergence rate of the error bound.\nFor any t \u2208 [0, t\u0303), there exists an i \u2208 {1, 2, \u00b7 \u00b7 \u00b7 , n} such that t \u2208 [ti\u22121, ti). For simplicity, we use h(x, t) to denote the score function \u2207x log qt(x), and use E\u03c4 [\u00b7] to denote the expectation over \u03c4 \u223c U(ti\u22121, ti). Thus, we can obtain\n\u2225\u03f5\u2217(x, f(t))\u2212 \u03f5(x, t)\u2225 = \u2225E\u03c4 [\u03f5(x, \u03c4)]\u2212 \u03f5(x, t)\u2225 = \u2225E\u03c4 [\u03c3\u03c4h(x, \u03c4)]\u2212 \u03c3th(x, t)\u2225 = \u2225E\u03c4 [\u03c3\u03c4h(x, \u03c4)\u2212 \u03c3\u03c4h(x, t) + \u03c3\u03c4h(x, t)\u2212 \u03c3th(x, t)]\u2225 \u2264 \u2225E\u03c4 [\u03c3\u03c4 (h(x, \u03c4)\u2212 h(x, t))]\u2225+ \u2225E\u03c4 [(\u03c3\u03c4 \u2212 \u03c3t)h(x, t)]\u2225 \u2264 E\u03c4 [\u03c3\u03c4\u2225h(x, \u03c4)\u2212 h(x, t)\u2225] + E\u03c4 [|\u03c3\u03c4 \u2212 \u03c3t|]\u2225h(x, t)\u2225 \u2264 \u03c3tiE\u03c4 [\u2225h(x, \u03c4)\u2212 h(x, t)\u2225] + (\u03c3ti \u2212 \u03c3ti\u22121)\u2225h(x, t)\u2225 \u2264 \u03c3tiKi(x)(ti \u2212 ti\u22121) +Bi(x)(\u03c3ti \u2212 \u03c3ti\u22121) \u2264 \u03c3t\u0303K(x)\u2206t+B(x)\u2206\u03c3max,\n(A13)\nwhere Ki(x) = supt,\u03c4\u2208[ti\u22121,ti),t\u0338=\u03c4 \u2225h(x,t)\u2212h(x,\u03c4)\u2225\n|t\u2212\u03c4 | , Bi(x) = supt\u2208[ti\u22121,ti) \u2225h(x, t)\u2225, K(x) = supt,\u03c4\u2208[0,t\u0303),t\u0338=\u03c4 \u2225h(x,t)\u2212h(x,\u03c4)\u2225 |t\u2212\u03c4 | , B(x) = supt\u2208[0,t\u0303) \u2225h(x, t)\u2225, and \u2206\u03c3max = max1\u2264i\u2264n |\u03c3ti \u2212 \u03c3ti\u22121 |. The first equality holds because\n\u03f5(x, t) = argmin \u03f5\u03b8\nE[\u2225\u03f5\u03b8(x\u03c4 , \u03c4)\u2212 \u03f5\u222522|\u03c4 = t,x\u03c4 = x]\n= E[\u03f5|\u03c4 = t,x\u03c4 = x], (A14)\nTable A4: Quantitative comparison among E-TSDM, DDPM (Ho et al., 2020), and modification of noise schedules (ModifiedNS) on FFHQ 256 \u00d7 256 dataset (Karras et al., 2019) evaluated with FID-10k \u2193. Specifically, we implement Modified-NS on linear, quadratic, and cosine schedules. Experimental results indicate that the performance of Modified-NS is unstable while E-TSDM achieves better synthesis performance.\nLinear Quadratic Cosine\nBaseline 9.50 13.79 27.17 E-TSDM 6.62 9.69 26.08 Modified-NS 8.67 17.48 26.84\nTable A5: Quantitative comparison of remap method between uniformly sampling t and uniformly sampling \u03bb, during training and inference, on FFHQ 256\u00d7256 (Karras et al., 2019) evaluated with FID-10k \u2193. Specifically, Ut is U [0, 1], and U\u03bb is U [0,K] for 1/t but U [\u2212K,K] for InverseSigmoid, where K is a large number to avoid infinity. Results show that remap is not helpful.\nTraining Inference Remap Function\nStrategy Strategy 1/t Inverse-Sigmoid\nt \u223c Ut t \u223c Ut 9.43 9.33 t \u223c Ut \u03bb \u223c U\u03bb 83.71 468.90 \u03bb \u223c U\u03bb t \u223c Ut 83.44 468.19 \u03bb \u223c U\u03bb \u03bb \u223c U\u03bb 171.06 351.89\nand our optimal \u03f5\u2217(x, f(t)) can be expressed as\n\u03f5\u2217(x, f(t)) = \u03f5\u2217(x, ti\u22121)\n= argmin \u03f5\u03b8\nE\u03c4\u223cU(ti\u22121,ti),\u03f5[\u2225\u03f5\u03b8(x\u03c4 , ti\u22121)\u2212 \u03f5\u2225 2 2|x\u03c4 = x]\n= E\u03c4\u223cU(ti\u22121,ti),\u03f5[\u03f5|x\u03c4 = x] = E\u03c4\u223cU(ti\u22121,ti)E\u03f5[\u03f5|\u03c4,x\u03c4 = x] = E\u03c4\u223cU(ti\u22121,ti)[\u03f5(x, \u03c4)].\n(A15)\nAs for the rate of convergence, it is obvious from Equation (A13) that we only need to determine the convergence rate of \u2206\u03c3max. Under common settings, \u03c3t is monotonically decreasing and concave for t \u2208 [0, T ], thus\n\u2206\u03c3max = max 1\u2264i\u2264n |\u03c3ti \u2212 \u03c3ti\u22121 | = \u03c3t1 \u2212 \u03c3t0 = \u03c3\u2206t, (A16)\nwhere the last equality holds because \u03c3t0 = \u03c30 = 0, and t1 = t\u0303/n = \u2206t as we uniformly divides [0, t\u0303) into n sub-intervals. Then, we can verify the convergence rate of \u2206\u03c3max as\nlim \u2206t\u21920 \u2206\u03c3max\u221a \u2206t = lim \u2206t\u21920 \u221a \u03c32\u2206t \u2206t\n= \u221a d\u03c32t dt \u2223\u2223\u2223\u2223\u2223 t=0\n=\n\u221a d(1\u2212 \u03b12t )\ndt\n\u2223\u2223\u2223\u2223\u2223 t=0\n= \u221a \u22122\u03b1t\nd\u03b1t dt \u2223\u2223\u2223\u2223\u2223 t=0\n= \u221a \u22122d\u03b1t\ndt \u2223\u2223\u2223\u2223\u2223 t=0 ,\n(A17)\nwhere d\u03b1tdt \u2223\u2223 t=0 is finite and d\u03b1tdt \u2223\u2223 t=0\n\u2264 0. Thus, we can conclude that \u2206\u03c3max is at least 12 -order convergence with respect to \u2206t, and the error bound \u03c3t\u0303K(x)\u2206t + B(x)\u2206\u03c3max is also at least 1 2 - order convergence. This is a relatively fast convergence speed in optimization, and demonstrates that the introduced errors of E-TSDM are controllable.\nLi ps\nch itz\nC on\nst an\nts\nTimestep\nLi ps\nch itz\nC on\nst an\nts\nTimestep\nFigure A6: Quantitative comparison of Lipschitz constants between E-TSDM and DDPM baseline (Ho et al., 2020) on FFHQ 256 \u00d7 256 (Karras et al., 2019) using the cosine shift schedule.\nLi ps\nch itz\nC on\nst an\nts\nTimestep\nLi ps\nch itz\nC on\nst an\nts\nTimestep\nFigure A7: Quantitative comparison of Lipschitz constants among E-TSDM, DDPM (Ho et al., 2020), and DDPM (Ho et al., 2020) using regularization techniques (DDPM-r) on FFHQ 256\u00d7 256 (Karras et al., 2019)."
        },
        {
            "heading": "D ADDITIONAL RESULTS",
            "text": ""
        },
        {
            "heading": "D.1 LIPSCHITZ CONSTANTS",
            "text": "In our main paper, we demonstrate the effectiveness of E-TSDM in reducing the Lipschitz constants near t = 0 by comparing its Lipschitz constants with that of DDPM baseline (Ho et al., 2020) on the FFHQ 256\u00d7 256 dataset (Karras et al., 2019). As a supplement, we provide additional comparisons of Lipschitz constants on other datasets, including AFHQ-Cat (Choi et al., 2020) (see Figure A2a), AFHQ-Wild (Choi et al., 2020) (see Figure A2b), Lsun-Cat 256 \u00d7 256 (Karras et al., 2019) (see Figure A2c), Lsun-Church 256\u00d7 256 (Karras et al., 2019) (see Figure A2d), and CelebAHQ 256\u00d7 256 (Karras et al., 2017) (see Figure A2e). These experimental results demonstrate that E-TSDM is highly effective in mitigating Lipschitz singularities in diffusion models across various datasets.\nFurthermore, we provide a comparison of Lipschitz constants between E-TSDM and the DDPM baseline (Ho et al., 2020) when using the quadratic schedule and the cosine-shift schedule (Hoogeboom et al., 2023). As shown in Figure A2f, we observe that large Lipschitz constants still exist in diffusion models when using the quadratic schedule, and E-TSDM effectively alleviates this problem. Similar improvement can also be observed when using the cosine-shift schedule as illustrated in Figure A6, highlighting the superiority of our approach over the DDPM baseline.\nD.2 QUANTITATIVE ANALYSIS OF t\u0303 AND n\nIn our main paper, we investigated the impact of two important settings for E-TSDM, the length of the interval to share conditions t\u0303, and the number of sub-intervals n in this interval. As a supplement, we provide additional results on various datasets to further investigate the optimal settings for these parameters.\nAs seen in Figure A3 and Figure A4, we observe divergence in the best choices of n and t\u0303 across different datasets. However, we find that the default settings where t\u0303 = 100 and n = 5 consistently yield good performance across a range of datasets. Based on these findings, we recommend the default settings as an ideal choice for implementing E-TSDM without the need for a thorough search. However, if performance is the main concern, researchers may conduct a grid search to explore the optimal values of t\u0303 and n for specific datasets."
        },
        {
            "heading": "D.3 ALTERNATIVE METHODS",
            "text": "In this section, we discuss three different alternative methods that possibly alleviate Lipschitz singularities. including regularization, modification of noise schedules, and remap. Although seem feasible, they have different problems, resulting in worse performance than E-TSDM.\n0\n5\n10\n15\n20\n0 200 400 600 800 1000\nCosine Linear Quadratic\nTimestep\nR at\nio o\nf S N\nR\nFigure A8: Quantitative evaluation of the ratio of SNR of Modified-NS to the SNR of the corresponding original noise schedule. Results show that Modified-NS significantly increases the SNR near zero point, and thus reduces the amounts of added noise near zero point. Specifically, for the quadratic schedule, Modified-NS seriously increases the SNR almost during the whole process.\n0\n2,000\n4,000\n6,000\n8,000\n10,000\n12,000\n0 200 400 600 800 1000\nUniformly Sampling t\nUniformly Sampling \ud835\udf40\nTimestep\nSN R\nFigure A9: Quantitative comparison of SNR for remap method between uniformly sampling t and uniformly sampling the remapped conditional input \u03bb. Results show that when using remap method, uniformly sampling \u03bb significantly increases the SNR across all of the timesteps, and thus forces the network to focus too much on the beginning stage of the diffusion process."
        },
        {
            "heading": "D.3.1 REGULARIZATION",
            "text": "As mentioned in the main paper, one alternative method is to impose restrictions on the Lipschitz constants through regularization techniques. In this section, we apply regularization on the baseline and estimate the gradient of \u03f5\u03b8(x, t) by calculating the difference K(t, t\u2032). We represent this method as DDPM-r in this paper. As shown in Figure A7, although DDPM-r can also reduce the Lipschitz constants, its capacity to do so is substantially inferior to that of E-TSDM. Additionally, DDPMr necessitates twice the calculation compared to E-TSDM. Regarding synthesis performance, as shown in Table A3, DDPM-r performs slightly better than baseline, but much worse than E-TSDM, indicating that E-TSDM is a better choice than regularization."
        },
        {
            "heading": "D.3.2 MODIFYING NOISE SCHEDULES",
            "text": "As proved in Appendix C, the mainstream noise schedules satisfy d\u03b1tdt \u2223\u2223 t=0\n\u0338= 0, leading to Lipschitz singularities as proved in Theorem 3.1. However, it is possible to modify those schedules to force them to have d\u03b1tdt \u2223\u2223 t=0\n= 0, and thus alleviate Lipschitz singularities. We represent this method as Modified-NS in this paper. However, as said in Nichol & Dhariwal (2021), d\u03b1tdt \u2223\u2223 t=0\n= 0 means tiny amounts of noise at the beginning of the diffusion process, making it hard for the network to predict accurately enough.\nTo explore the performance, we conduct experiments of Modified-NS on FFHQ 256 \u00d7 256 (Karras et al., 2019) for all of the three discussed noise schedules in Appendix C.1. Specifically, for linear and quadratic schedules, since d\u03b1(\u03c4)d\u03c4 \u2223\u2223\u2223 \u03c4=0\n= \u2212 12\u03b2(0) (as detailed in Equation (A2)), we implement Modified-NS by setting \u03b2(0) = 0. Note that for the quadratic schedule, such a modification will significantly magnify the Signal to Noise Ratio (SNR), \u03b1 2 t\n\u03c32t , across the whole\ndiffusion process, so we slightly increase \u03b2T to make its SNR at t = T similar to that of the original quadratic schedule. Meanwhile, \u03b21, . . . , \u03b2T\u22121 are also correspondingly increased due to \u03b2t = ( \u221a \u03b20 +( \u221a \u03b2T \u2212 \u221a \u03b20) t T\u22121 )\n2. As for the cosine schedule, we set the offset s in Equation (A7) to zero. Experimental results are shown in Table A4, from which we find that the performance of Modified-NS is unstable. More specifically, Modified-NS improves performance for linear and cosine schedules but significantly drags down the performance for the quadratic schedule. We further provide the comparison of SNR between Modified-NS and their corresponding original noise schedules in Figure A8 by calculating the ratio of Modified-NS\u2019s SNR to the original noise schedule\u2019s SNR. From this figure we can tell that for linear and cosine schedule, Modified-NS significantly increase the SNR near zero point while maintaining the SNR of other timesteps similar. In other\nLi ps\nch itz\nC on\nst an\nts\nTimestep\nFigure A10: Quantitative comparison of Lipschitz constants between E-TSDM and LDM (Rombach et al., 2022) on FFHQ 256 \u00d7 256(Karras et al., 2019). E-TSDM reduces the overall Lipschitz constants near t = 0, and mitigates the Lipschitz singularities occurring in LDM (Rombach et al., 2022).\nFigure A11: Qualitative results produced by E-TSDM implemented on LDM (Rombach et al., 2022) on FFHQ 256\u00d7 256(Karras et al., 2019).\nwords, on the one hand, Modified-NS seriously reduces the amount of noise added near zero point, which can be detrimental to the accurate prediction. On the other hand, Modified-NS alleviates the Lipschitz singularities, which is beneficial to the synthesis performance. As a result, for linear and cosine schedules, Modified-NS performs better than baseline but worse than E-TSDM. However, for the quadratic schedule, although we force the SNR of Modified-NS at t = T similar to the SNR of the original schedule, the SNR at other timesteps is significantly increased, leading to a worse performance of Modified-NS compared to that of baseline."
        },
        {
            "heading": "D.3.3 REMAP",
            "text": "Except for regularization and Modified-NS, remap is another possible method to fix the Lipschitz singularities issue. Recall that the inputs of network \u03f5\u03b8(x, t) is noisy image x and timestep condition t. Remap is trying to design a remap function \u03bb = f(t) on t as the conditional input of network instead of t, namely, \u03f5\u03b8(x, f(t)). The core idea of remap is to reduce \u2202\u03f5\u03b8(x,t) \u2202t by significantly stretching the interval with large Lipschitz constants. Note that although fT of E-TSDM can also be seen as a kind of remap function, there are big differences between E-TSDM and remap. Specifically, E-TSDM tries to set the numerator to zero while remap aims to significantly increase the denominator. Besides, fT has no inverse while f(t) of remap is usually a reversible function. We provide two simple choices of f(t) in this section as examples, which are f(t) = 1/t and f\u22121(\u03bb) = sigmoid(\u03bb).\nRemap can efficiently reduce the Lipschitz constants regarding the conditional inputs of the network, \u2202\u03f5\u03b8(x,t)\u2202\u03bb . However, since we uniformly sample t both in training and inference, what should be focused on is the Lipschitz constants regarding t, \u2202\u03f5\u03b8(x,t)\u2202t , which can not be influenced by remap. In other words, although remap seems to be a feasible method, it is not helpful to mitigate the Lipschitz constants we care about, unless we uniformly sample \u03bb in training and inference. However, uniformly sampling \u03bb may force the network to focus on a certain part of the diffusion process. We use f(t) = 1/t as an example to illustrate this point and show the comparison of SNR between uniformly sampling t and uniformly sampling \u03bb when using remap in Figure A9. Results show that uniformly sampling \u03bb maintains a high SNR across all of the timesteps, leading to excessive attention to the beginning stage of the diffusion process. As a result, when we uniformly sample \u03bb during training or inference, the synthesis performance gets significantly worse as shown in Table A5. Besides, when we uniformly sample t both in training and inference, remap makes no difference and thus leads to a similar performance to the baseline."
        },
        {
            "heading": "D.4 MORE DIFFUSION MODELS",
            "text": "Latent diffusion models (LDM) (Rombach et al., 2022) is one of the most renowned variants of diffusion models. In this section, we will investigate the Lipschitz singularities in LDM (Rombach et al., 2022), and apply E-TSDM to address this problem. LDM (Rombach et al., 2022) shares a resemblance with DDPM (Rombach et al., 2022) but has an additional auto-encoder to encode images into the latent space. As LDM typically employs the quadratic schedule, it is also susceptible to Lipschitz singularities, as confirmed in Figure A10.\nAs seen in Figure A10, by utilizing E-TSDM, the Lipschitz constants within each timestep-shared sub-interval are reduced to zero, while the timesteps located near the boundaries of the sub-intervals exhibit a Lipschitz constant comparable to that of baseline, leading to a decrease in overall Lipschitz constants in the target interval t \u2208 [0, t\u0303), where t\u0303 is set as the default, namely t\u0303 = 100. Consequently, E-TSDM achieves an improvement in FID-50k from 4.98 to 4.61 with the adoption of E-TSDM, when n = 20. We provide some samples generated by the E-TSDM implemented on LDM in Figure A11.\nBesides, we also implement our E-TSDM to Elucidated diffusion models (EDM) (Karras et al., 2022), which proposed several changes to both the sampling and training processes and achieves impressive performance. Specifically, we reproduce EDM and repeat it three times on CIFAR10 32 \u00d7 32 (Krizhevsky et al., 2009) to get a FID-50k of 1.904 \u00b1 0.015, which is slightly worse than the official released one. Then we apply E-TSDM to EDM and repeat it three times to get a FID-50k of 1.797 \u00b1 0.016, indicating that E-TSDM is also helpful to EDM."
        },
        {
            "heading": "D.5 GENERATED SAMPLES",
            "text": "As a supplement, we provide massive generated samples of E-TSDM trained on Lsun-Church 256\u00d7 256 (Karras et al., 2019) (see Figure A13), Lsun-Cat 256\u00d7256 (Karras et al., 2019) (see Figure A14), AFHQ-Cat 256\u00d7256 (Choi et al., 2020), AFHQ-Wild 256\u00d7256 (Choi et al., 2020) (see Figure A15), FFHQ 256 \u00d7 256 (Karras et al., 2019) (see Figure A16), and CelebAHQ 256 \u00d7 256 (Karras et al., 2017) (see Figure A17)."
        }
    ],
    "title": "LIPSCHITZ SINGULARITIES IN DIFFUSION MODELS",
    "year": 2024
}