{
    "abstractText": "Long-term forecasting presents unique challenges due to the time and memory complexity of handling long sequences. Existing methods, which rely on sliding windows to process long sequences, struggle to effectively capture long-term variations that are partially caught within the short window (i.e., outer-window variations). In this paper, we introduce a novel approach that overcomes this limitation by employing contrastive learning and enhanced decomposition architecture, specifically designed to focus on long-term variations. To this end, our contrastive loss incorporates global autocorrelation held in the whole time series, which facilitates the construction of positive and negative pairs in a self-supervised manner. When combined with our decomposition networks, our contrastive learning significantly improves long-term forecasting performance. Extensive experiments demonstrate that our approach outperforms 14 baseline models in multiple experiments over nine long-term benchmarks, especially in challenging scenarios that require a significantly long output for forecasting. Source code is available at https://github.com/junwoopark92/Self-Supervised-Contrastive-Forecsating.",
    "authors": [
        {
            "affiliations": [],
            "name": "Junwoo Park"
        },
        {
            "affiliations": [],
            "name": "Daehoon Gwak"
        },
        {
            "affiliations": [],
            "name": "Jaegul Choo"
        },
        {
            "affiliations": [],
            "name": "Edward Choi"
        },
        {
            "affiliations": [],
            "name": "Kim Jaechul"
        }
    ],
    "id": "SP:d1b084197066421f286dbc6c64feab8d6d46ac6e",
    "references": [
        {
            "authors": [
                "Ahmad S Ahmad",
                "Mohammad Y Hassan",
                "Md Pauzi Abdullah",
                "Hasimah A Rahman",
                "F Hussin",
                "Hayati Abdullah",
                "Rahman Saidur"
            ],
            "title": "A review on applications of ann and svm for building electrical energy consumption forecasting",
            "venue": "Renewable and Sustainable Energy Reviews,",
            "year": 2014
        },
        {
            "authors": [
                "Shaojie Bai",
                "J Zico Kolter",
                "Vladlen Koltun"
            ],
            "title": "An empirical evaluation of generic convolutional and recurrent networks for sequence modeling",
            "venue": "arXiv preprint arXiv:1803.01271,",
            "year": 2018
        },
        {
            "authors": [
                "Cristian Challu",
                "Kin G Olivares",
                "Boris N Oreshkin",
                "Federico Garza Ramirez",
                "Max Mergenthaler Canseco",
                "Artur Dubrawski"
            ],
            "title": "Nhits: Neural hierarchical interpolation for time series forecasting",
            "venue": "In Proc. the AAAI Conference on Artificial Intelligence (AAAI),",
            "year": 2023
        },
        {
            "authors": [
                "Chris Chatfield",
                "Haipeng Xing"
            ],
            "title": "The analysis of time series: an introduction with R",
            "venue": "CRC press,",
            "year": 2019
        },
        {
            "authors": [
                "Ting Chen",
                "Simon Kornblith",
                "Mohammad Norouzi",
                "Geoffrey Hinton"
            ],
            "title": "A simple framework for contrastive learning of visual representations",
            "venue": "In Proc. the International Conference on Machine Learning (ICML),",
            "year": 2020
        },
        {
            "authors": [
                "Robert B Cleveland",
                "William S Cleveland",
                "Jean E McRae",
                "Irma"
            ],
            "title": "Terpenning. Stl: A seasonal-trend decomposition",
            "venue": "J. Off. Stat,",
            "year": 1990
        },
        {
            "authors": [
                "Lars Dannecker"
            ],
            "title": "Energy time series forecasting: efficient and accurate forecasting of evolving time series from the energy",
            "year": 2015
        },
        {
            "authors": [
                "Rui Ding",
                "Qiang Wang",
                "Yingnong Dang",
                "Qiang Fu",
                "Haidong Zhang",
                "Dongmei Zhang"
            ],
            "title": "Yading: Fast clustering of large-scale time series data",
            "venue": "Proceedings of the VLDB Endowment,",
            "year": 2015
        },
        {
            "authors": [
                "Jean-Yves Franceschi",
                "Aymeric Dieuleveut",
                "Martin Jaggi"
            ],
            "title": "Unsupervised scalable representation learning for multivariate time series",
            "year": 2019
        },
        {
            "authors": [
                "Rob J Hyndman",
                "Earo Wang",
                "Nikolay Laptev"
            ],
            "title": "Large-scale unusual time series detection",
            "venue": "In 2015 IEEE international conference on data mining workshop (ICDMW),",
            "year": 2015
        },
        {
            "authors": [
                "Muhammad Faisal Iqbal",
                "Muhammad Zahid",
                "Durdana Habib",
                "Lizy Kurian John"
            ],
            "title": "Efficient prediction of network traffic for real-time applications",
            "venue": "Journal of Computer Networks and Communications,",
            "year": 2019
        },
        {
            "authors": [
                "Taesung Kim",
                "Jinhee Kim",
                "Yunwon Tae",
                "Cheonbok Park",
                "Jang-Ho Choi",
                "Jaegul Choo"
            ],
            "title": "Reversible instance normalization for accurate time-series forecasting against distribution shift",
            "venue": "In Proc. the International Conference on Learning Representations (ICLR),",
            "year": 2021
        },
        {
            "authors": [
                "Nowrouz Kohzadi",
                "Milton S. Boyd",
                "Bahman Kermanshahi",
                "Iebeling Kaastra"
            ],
            "title": "A comparison of artificial neural network and time series models for forecasting commodity",
            "venue": "prices. Neurocomputing,",
            "year": 1996
        },
        {
            "authors": [
                "Vincent Le Guen",
                "Nicolas Thome"
            ],
            "title": "Shape and time distortion loss for training deep time series forecasting models",
            "venue": "In Proc. the Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2019
        },
        {
            "authors": [
                "Tsung-Yi Lin",
                "Priya Goyal",
                "Ross B. Girshick",
                "Kaiming He",
                "Piotr Doll\u00e1r"
            ],
            "title": "Focal loss for dense object detection",
            "venue": "IEEE International Conference on Computer Vision (ICCV),",
            "year": 2017
        },
        {
            "authors": [
                "Shizhan Liu",
                "Hang Yu",
                "Cong Liao",
                "Jianguo Li",
                "Weiyao Lin",
                "Alex X Liu",
                "Schahram Dustdar"
            ],
            "title": "Pyraformer: Low-complexity pyramidal attention for long-range time series modeling and forecasting",
            "venue": "In Proc. the International Conference on Learning Representations (ICLR),",
            "year": 2022
        },
        {
            "authors": [
                "Yong Liu",
                "Haixu Wu",
                "Jianmin Wang",
                "Mingsheng Long"
            ],
            "title": "Non-stationary transformers: Rethinking the stationarity in time series forecasting",
            "venue": "In Proc. the Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2022
        },
        {
            "authors": [
                "Leland McInnes",
                "John Healy",
                "James Melville"
            ],
            "title": "Umap: Uniform manifold approximation and projection for dimension reduction",
            "venue": "arXiv preprint arXiv:1802.03426,",
            "year": 2018
        },
        {
            "authors": [
                "Athanasios Papoulis",
                "S Unnikrishna Pillai"
            ],
            "title": "Probability, random variables and stochastic processes",
            "year": 2002
        },
        {
            "authors": [
                "Junwoo Park",
                "Jungsoo Lee",
                "Youngin Cho",
                "Woncheol Shin",
                "Dongmin Kim",
                "Jaegul Choo",
                "Edward Choi"
            ],
            "title": "Deep imbalanced time-series forecasting via local discrepancy density",
            "venue": "In Proc. of Machine Learning and Knowledge Discovery in Databases: Research Track (ECML/PKDD),",
            "year": 2023
        },
        {
            "authors": [
                "Thanawin Rakthanmanon",
                "Bilson Campana",
                "Abdullah Mueen",
                "Gustavo Batista",
                "Brandon Westover",
                "Qiang Zhu",
                "Jesin Zakaria",
                "Eamonn Keogh"
            ],
            "title": "Addressing big data time series: Mining trillions of time series subsequences under dynamic time warping",
            "venue": "ACM Transactions on Knowledge Discovery from Data (TKDD),",
            "year": 2013
        },
        {
            "authors": [
                "Hiroaki Sakoe",
                "Seibi Chiba"
            ],
            "title": "Dynamic programming algorithm optimization for spoken word recognition",
            "venue": "IEEE transactions on acoustics, speech, and signal processing,",
            "year": 1978
        },
        {
            "authors": [
                "Sana Tonekaboni",
                "Danny Eytan",
                "Anna Goldenberg"
            ],
            "title": "Unsupervised representation learning for time series with temporal neighborhood coding",
            "venue": "arXiv preprint arXiv:2106.00750,",
            "year": 2021
        },
        {
            "authors": [
                "Jos\u00e9 F Torres",
                "Dalil Hadjout",
                "Abderrazak Sebaa",
                "Francisco Mart\u0131\u0301nez-\u00c1lvarez",
                "Alicia Troncoso"
            ],
            "title": "Deep learning for time series forecasting: a survey",
            "venue": "Big Data,",
            "year": 2021
        },
        {
            "authors": [
                "Eleni I Vlahogianni",
                "Matthew G Karlaftis",
                "John C Golias"
            ],
            "title": "Short-term traffic forecasting: Where we are and where we\u2019re going",
            "venue": "Transportation Research Part C: Emerging Technologies,",
            "year": 2014
        },
        {
            "authors": [
                "Huiqiang Wang",
                "Jian Peng",
                "Feihu Huang",
                "Jince Wang",
                "Junhui Chen",
                "Yifei Xiao"
            ],
            "title": "Micn: Multiscale local and global context modeling for long-term series forecasting",
            "venue": "In Proc. the International Conference on Learning Representations (ICLR),",
            "year": 2023
        },
        {
            "authors": [
                "Zhiyuan Wang",
                "Xovee Xu",
                "Weifeng Zhang",
                "Goce Trajcevski",
                "Ting Zhong",
                "Fan Zhou"
            ],
            "title": "Learning latent seasonal-trend representations for time series forecasting",
            "venue": "In Proc. the Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2022
        },
        {
            "authors": [
                "Gerald Woo",
                "Chenghao Liu",
                "Doyen Sahoo",
                "Akshat Kumar",
                "Steven Hoi"
            ],
            "title": "Cost: Contrastive learning of disentangled seasonal-trend representations for time series forecasting. 2022a",
            "year": 2022
        },
        {
            "authors": [
                "Gerald Woo",
                "Chenghao Liu",
                "Doyen Sahoo",
                "Akshat Kumar",
                "Steven Hoi"
            ],
            "title": "Etsformer: Exponential smoothing transformers for time-series forecasting",
            "venue": "arXiv preprint arXiv:2202.01381,",
            "year": 2022
        },
        {
            "authors": [
                "Haixu Wu",
                "Jiehui Xu",
                "Jianmin Wang",
                "Mingsheng Long"
            ],
            "title": "Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting",
            "venue": "In Proc. the Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2021
        },
        {
            "authors": [
                "Haixu Wu",
                "Tengge Hu",
                "Yong Liu",
                "Hang Zhou",
                "Jianmin Wang",
                "Mingsheng Long"
            ],
            "title": "Timesnet: Temporal 2d-variation modeling for general time series analysis",
            "venue": "In Proc. the International Conference on Learning Representations (ICLR),",
            "year": 2023
        },
        {
            "authors": [
                "Zhihan Yue",
                "Yujing Wang",
                "Juanyong Duan",
                "Tianmeng Yang",
                "Congrui Huang",
                "Yunhai Tong",
                "Bixiong Xu"
            ],
            "title": "Ts2vec: Towards universal representation of time series",
            "venue": "In Proc. the AAAI Conference on Artificial Intelligence (AAAI),",
            "year": 2022
        },
        {
            "authors": [
                "Ailing Zeng",
                "Muxi Chen",
                "Lei Zhang",
                "Qiang Xu"
            ],
            "title": "Are transformers effective for time series forecasting",
            "venue": "In Proc. the AAAI Conference on Artificial Intelligence (AAAI),",
            "year": 2023
        },
        {
            "authors": [
                "Kaiwen Zha",
                "Peng Cao",
                "Yuzhe Yang",
                "Dina Katabi"
            ],
            "title": "Supervised contrastive regression",
            "venue": "arXiv preprint arXiv:2210.01189,",
            "year": 2022
        },
        {
            "authors": [
                "Tianping Zhang",
                "Yizhuo Zhang",
                "Wei Cao",
                "Jiang Bian",
                "Xiaohan Yi",
                "Shun Zheng",
                "Jian Li"
            ],
            "title": "Less is more: Fast multivariate time series forecasting with light sampling-oriented mlp structures",
            "venue": "arXiv preprint arXiv:2207.01186,",
            "year": 2022
        },
        {
            "authors": [
                "Yunhao Zhang",
                "Junchi Yan"
            ],
            "title": "Crossformer: Transformer utilizing cross-dimension dependency for multivariate time series forecasting",
            "venue": "In Proc. the International Conference on Learning Representations (ICLR),",
            "year": 2023
        },
        {
            "authors": [
                "Haoyi Zhou",
                "Shanghang Zhang",
                "Jieqi Peng",
                "Shuai Zhang",
                "Jianxin Li",
                "Hui Xiong",
                "Wancai Zhang"
            ],
            "title": "Informer: Beyond efficient transformer for long sequence time-series forecasting",
            "venue": "In Proc. the AAAI Conference on Artificial Intelligence (AAAI),",
            "year": 2021
        },
        {
            "authors": [
                "Tian Zhou",
                "Ziqing Ma",
                "Qingsong Wen",
                "Liang Sun",
                "Tao Yao",
                "Wotao Yin",
                "Rong Jin"
            ],
            "title": "Film: Frequency improved legendre memory model for long-term time series forecasting",
            "venue": "In Proc. the Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2022
        },
        {
            "authors": [
                "Tian Zhou",
                "Ziqing Ma",
                "Qingsong Wen",
                "Xue Wang",
                "Liang Sun",
                "Rong Jin"
            ],
            "title": "Fedformer: Frequency enhanced decomposed transformer for long-term series forecasting",
            "venue": "In Proc. the International Conference on Machine Learning (ICML),",
            "year": 2022
        },
        {
            "authors": [
                "\u2208 R",
                "bchannel \u2208 R",
                "Wtime \u2208 RO\u00d7I",
                "Wchannel"
            ],
            "title": "Rd\u00d7c. To compute AutoCon, global autocorrelation must first be determined. As part of the preprocessing process, we calculate autocorrelation for the entire training series, excluding validation and test series",
            "year": 2024
        },
        {
            "authors": [
                "Wu"
            ],
            "title": "In this paper, we utilized six real-world datasets from diverse domains: mechanical systems (ETT), energy (Electricity), traffic (Traffic), weather (Weather), economics (Exchange), and disease (ILI). The statistics of each dataset are summarized and found in Table 5. As a mainstream benchmark, the ETT datasets are extensively utilized for assessing long-term forecasting methods Zhou et al",
            "year": 2023
        },
        {
            "authors": [
                "Iqbal et al",
                "Torres"
            ],
            "title": "2021). As forecasting horizons increase, the window length expands, leading to increased computational costs. Therefore, it is imperative to evaluate a model\u2019s computational efficiency. Figure 14 illustrates the time required to update the parameters by a single batch for our model in comparison with the baseline models",
            "venue": "in time-series forecasting (Dannecker,",
            "year": 2021
        },
        {
            "authors": [
                "W3. C"
            ],
            "title": "ADDITIONAL COMPARISON WITH TWO SELF-SUPERVISED LOSS We designed and provided results for two possible self-supervised objectives based on HierCon (Yue et al., 2022) and SupCon (Khosla et al., 2020) that can be incorporated into our two-stream model structure. HierCon induces the representations of two partially overlapped windows",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Time-series data presents a unique challenge due to its potentially infinite length accumulating over time, making it infeasible to process them all at once (Ding et al., 2015; Hyndman et al., 2015; Rakthanmanon et al., 2013). This requires different strategies compared to other sequence data such as natural language sentences. To address this, the sliding window approach (Kohzadi et al., 1996) is commonly employed to partition a single time-series data into shorter sub-sequences (i.e., windows) Typically, in time-series forecasting, the sliding window approach enables models to not only process the long-time series but also capture local dependencies between the past and future sequence within the windows, resulting in accurate short-term predictions.\nRecently, as the demands in the industry to predict more distant future increases (Ahmad et al., 2014; Vlahogianni et al., 2014; Zhou et al., 2021), various studies have gradually increased the window length. Transformer-based models have reduced computational costs of using long windows through improvements in the attention mechanism (Zhou et al., 2021; Wu et al., 2021; Liu et al., 2022a). Also, CNN-based models (Bai et al., 2018; Yue et al., 2022) have applied a dilation in convolution operations to learn more distant dependencies while benefiting from their efficient computational cost. Despite the remarkable progress made by these models, their effectiveness in long-term forecasting remains uncertain. Since the extended window is still shorter than the total time series length, these models may not learn the longer temporal patterns than the window length.\nIn this paper, we first analyze the limitations of existing models trained with sub-sequences (i.e., based on sliding windows) for long-term forecasting tasks. We observed that most time series often contain long-term variations with periods longer than conventional window lengths as shown in Figure 1 and\nFigure 5. If a model successfully captures these long-term variations, we expect the representations of two distant yet correlated windows to be more similar than uncorrelated ones. However, since the previous studies all treat each window independently during training, it is challenging for the model to capture such long-term variations across distinct windows. Explicitly, Figure 2 shows that the representations of existing models fail to reflect the long-term correlations between two distant windows. Still, recent methods tend to overlook long-term variations by focusing more on learning short-term variations within the window. For example, existing models based on decomposition approaches (Zeng et al., 2023; Wang et al., 2023) often treat the long-term variations partially caught in the window as simple non-periodic trends and employ a linear model to extend the past trend into the prediction. Besides, window-unit normalization methods (Kim et al., 2021; Zeng et al., 2023) can hinder long-term prediction by normalizing numerically significant values (e.g., maximum, minimum, domain-specific values in the past) that may have a long-term impact on the time series. Since these normalization methods are essential for mitigating distribution shift problems (Kim et al., 2021) caused by nonstationarity (Liu et al., 2022b), a new approach is necessary to learn long-term variations while still keeping the normalization methods.\nTherefore, we propose a novel contrastive learning to help the model capture long-term dependencies that exist across different windows. Our method builds on the fact that a mini-batch can consist of windows that are temporally far apart. It allows the interval between windows to span the entire series length, which is much longer than the window length. Section 3.1 describes the details of our contrastive loss. Moreover, we use our contrastive loss in combination with a decomposition-based model architecture, which consists two branches, namely a short-term branch and a long-term branch. Naturally, our loss is applied to the long-term branch. However, as pointed out earlier, the long-term branch in the existing decomposition architecture has been composed of a single linear layer, which is unsuitable for learning long-term representations. Thus, as explained in Section 3.2, we redesign the decomposition architecture where the long-term branch has sufficient capacity to learn long-term representation from our loss. In summary, the main contributions of our work are as follows:\n\u2022 Our findings reveal that the long-term performances of existing models are poor as those models overlooked the long-term variations beyond the window.\n\u2022 We propose AutoCon, a novel contrastive loss function to learn a long-term representation by constructing positive and negative pairs across distant windows in a self-supervised manner.\n\u2022 Extensive experiments on nine datasets demonstrate that the proposed decomposition architecture trained with AutoCon achieves performance improvements of up to 34% compared to a total of 14 concurrent models including three representation methods."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Contrastive Learning for Time-series Forecasting Contrastive learning (Chen et al., 2020; Khosla et al., 2020; Zha et al., 2022) is a type of self-supervised learning technique that helps models learn\nuseful representations of data without the need for explicit labeling of data. Motivated by the recent success of contrastive learning in computer vision, numerous methods (Tonekaboni et al., 2021; Yue et al., 2022; Woo et al., 2022a) have been proposed in time-series analysis. In contrastive learning, since how to construct positive pairs has a great influence on the performance, they mainly proposed positive pair construction strategies such as temporal consistency (Tonekaboni et al., 2021), subseries consistency (Franceschi et al., 2019), and contextual consistency (Yue et al., 2022). However, these strategies have a limitation in that only temporally close samples are selected as positives, overlooking the periodicity in the time series. Due to the periodicity, there may be more similar negative samples than positively selected samples. Recently, CoST (Woo et al., 2022a) tried to learn a representation considering periodicity through Frequency Domain Contrastive loss, but it could not consider periodicity beyond the window length because it still uses augmentation for the window. In the time-series learning framework, we focus on the fact that randomly sampled sequences in a batch can be far from each other in time. Therefore, we propose a novel selection strategy to choose not only local positive pairs but also global positive pairs between the windows in the batch.\nDecomposition-based Models for Long-term Forecasting Time-series decomposition (Cleveland et al., 1990) is a well-established technique that involves breaking down a time series into its individual components, such as trend, seasonal, and remainder components. By decomposing a time series into these components, it becomes easier to analyze each component\u2019s behavior and make more interpretable predictions. Thus, decomposition-based models (Wu et al., 2021; Zhou et al., 2022b; Wang et al., 2023) have gained popularity in time-series forecasting, as they offer robust and interpretable predictions, even when trained on complex time series. Recently, DLinear(Zeng et al., 2023) has demonstrated exceptional performance by using a decomposition block and a single linear layer for each trend and seasonal component. However, our analysis indicates that these linear models are effective in capturing high-frequency components that impact short-term predictions, while they often miss low-frequency components that significantly affect long-term predictions. Therefore, a single linear model may be sufficient for short-term prediction, but it is inadequate for long-term prediction. In light of this limitation, we propose a model architecture that includes layers with varying capacities to account for the unique properties of both components."
        },
        {
            "heading": "3 METHOD",
            "text": "Notations We first describe the forecasting task with the sliding window approach (Zhou et al., 2021; Wu et al., 2021; Park et al., 2023), which covers all possible in-output sequence pairs of the entire time series S = {s1, . . . , sT } where T denotes the length of the observed time series and st \u2208 Rc is observation with c dimension. For the simplicity in explaining our methodology, we set the dimension c to 1 throughout this paper. By sliding a window with a fixed length W on S , we obtain the windows D = {Wt}Mt=1 where Wt = (Xt,Yt) are divided into two parts: input sequence Xt = {st, . . . , st+I\u22121} with the input length I , and output sequence Yt = {st+I , . . . , st+I+O\u22121} with the output length O to predict. Also, we denote the global index sequence of Wt as Tt = {t+ i}W\u22121i=0 ."
        },
        {
            "heading": "3.1 AUTOCORRELATION-BASED CONTRASTIVE LOSS FOR LONG-TERM FORECASTING",
            "text": "Missing Long-term Dependency in the Window Many real-world time series exhibit diverse long-term and short-term variations (Wu et al., 2021; 2023; Wang et al., 2023). In such cases, a forecasting model may struggle to predict long-term variations, as these variations are not captured within the window. We first identify these long-term variations using autocorrelation, inspired by the stochastic process theory(Chatfield & Xing, 2019; Papoulis & Unnikrishna Pillai, 2002). For a real discrete-time process {St}, we can obtain the autocorrelation function RSS(h) using the following equation:\nRSS(h) = lim T\u2192\u221e\n1\nT T\u2211 t=1 StSt\u2212h (1)\nThe autocorrelation measures the correlation between observations at different times (i.e., time lag h). A strong correlation close to 1 or -1 indicates that all points separated by h in the series S are linearly related, moving in the same or opposite direction for positive or negative signs, respectively. In other words, autocorrelation can be utilized to forecast future variations that are h interval away based on current variations. Although recent methods have leveraged autocorrelation to discover periodbased dependencies (Wu et al., 2021; Wang et al., 2022), they only apply it to capture variations within the window, overlooking long-term variations that span beyond the window. But as shown in Figure 1, non-zero correlations exist outside the conventional window length. For the first time, we propose a representation learning method via contrastive learning to capture these long-term"
        },
        {
            "heading": "Value",
            "text": "variations quantified by the global autocorrelation. Note that, to distinguish our method from those that use local autocorrelation within a given window, we refer to the autocorrelation calculated across the entire time series as the global autocorrelation.\nAutocorrelation-based Contrastive Loss (AutoCon) We note that a mini-batch can consist of windows that are temporally very far apart. This time distance can be as long as the entire series length T , which is much longer than the window length W . Based on this fact, we address long-term dependencies that exist throughout the entire series by establishing relationships between windows. Concretely, we define the relationship between the two windows based on the global autocorrelation. Any two windows Wt1 and Wt2 obtained at two different times t1 and t2 each have W observations with globally indexed time sequence Tt1 = {t1 + i}W\u22121i=0 and Tt2 = {t2 + j} W\u22121 j=0 . Then, we denote time distances between all pairs of two observations in each window as a matrix D \u2208 RW\u00d7W . This matrix D contains time distance as elements Di,j = |(t2 + j) \u2212 (t1 + i)|. In the two windows, the time distances between the same phase (i.e., i = j) all have the same value |t1 \u2212 t2|, and they are represented by the diagonal terms {Di,i}W\u22121i=1 of the matrix. Therefore, based on this representativeness, we leverage the global autocorrelation RSS(|t1 \u2212 t2|) to define the relationship between the two windows as follows:\nr(Tt1 , Tt2) = |RSS(|t1 \u2212 t2|)| (2)\nwhere RSS denote the global autocorrelation calculated from train series S. Now, we design a loss to ensure that the similarities between all pairs of window representations follow the global autocorrelation measured in the data space. To achieve this, we define positive and negative samples in a relative manner inspired by SupCR (Zha et al., 2022) for regression tasks in the image domain. However, unlike SupCR which uses annotated labels to determine the relationship between images, we use the global autocorrelation RSS to determine the relationship between windows, making our approach an unsupervised method. We feed a mini-batch X \u2208 RN\u00d7I consisting of N windows to the encoder to obtain representations v \u2208 RN\u00d7I\u00d7d where v = Enc (X , T ). Indexed by the windows i, our autocorrelation-based contrastive loss, called AutoCon, is then computed over the representations {v(i)}Ni=1 with the corresponding time sequence {T (i)}Ni=1 as:\nLAutoCon = \u2212 1\nN N\u2211 i=1 1 N \u2212 1 N\u2211 j=1,j \u0338=i r(i,j) log exp\n( Sim ( v(i),v(j) ) /\u03c4 )\u2211N k=1 1[k \u0338=i,r(i,k)\u2264r(i,j)] exp ( Sim ( v(i),v(k) ) /\u03c4\n) (3)\nwhere Sim (\u00b7, \u00b7) measures the similarity between two representations (e.g., cosine similarity between max pooled v(i) along with the time axis (Yue et al., 2022)), and r(i,j) = r(T (i), T (j)) represents the global correlation between two windows. During training, there are a total of N \u00d7 (N \u2212 1) possible pairs indexed by (i, j). Each pair (i.e., as an anchor pair) designates itself as a relatively positive pair by considering any pairs that exhibit the global autocorrelation r(i,k) lower than that\nr(i,j) of the anchor pair as negative pairs. Figure 3 describes sample cases of our selection strategy in the given batch. Since a different set of windows form the batch in each iteration, we expect that the representations reflect the global autocorrelations of all possible distances. The relative selection strategy does not guarantee that the positive window has a high correlation close to one; it only requires a higher correlation than other negative windows in the same batch. Consequently, we introduce r(i,j) as weights to differentiate between positive pairs with varying degrees of correlation, similar to focal loss (Lin et al., 2017). To minimize LAutoCon, the encoder learns representations so that the pairs with high correlation are closer than the pairs with low correlation.\nOur AutoCon offers several notable advantages over conventional contrastive-based methods. First, although AutoCon is an unsupervised representation method, it does not rely on data augmentation, which is common in most contrastive-based approaches (Tonekaboni et al., 2021; Yue et al., 2022; Woo et al., 2022a). The augmentation-based methods require additional computation costs caused by the augmentation process and increase the forward-backward process for the augmented data. Also, existing contrastive learning methods consider only temporally close samples as positive pairs within windows. This ultimately fails to appropriately learn representations of the windows that are distant from each other but are similar due to long-term periodicity. Consequently, our method is computationally efficient and able to learn long-term representations, enhancing the ability to predict long-term variations effectively."
        },
        {
            "heading": "3.2 DECOMPOSITION ARCHITECTURE FOR LONG-TERM REPRESENTATION",
            "text": "Existing models commonly adopt the decomposition architecture that has a seasonal branch and a trend branch to achieve disentangled seasonal and trend prediction. To emphasize that trends are long-term variations partially caught in the window, we regard the trend branch as a long-term branch and the seasonal branch as a short-term branch. Our AutoCon method is designed to learn long-term representations, making it natural not to use it in the short-term branch to enforce long-term dependencies. However, integrating AutoCon with current decomposition architectures presents a challenge because both branches share the same representation (Wu et al., 2021; Zhou et al., 2022b; Liu et al., 2022b), or the long-term branch consists of a linear layer that is not suitable for learning representations (Zeng et al., 2023; Wang et al., 2023). Moreover, we observe that recent linearbased models (Zeng et al., 2023) outperform complicated deep models at short-term predictions, leaving doubts whether a deep model is necessary to learn the high-frequency variations. Based on these considerations, we redesign a model architecture with well-defined existing blocks to respect temporal locality for short-term and globality for long-term forecasting as shown in Figure 4. Our decomposition architecture has three main features.\nNormalization and Denormalization for Nonstationarity First, we use window-unit normalization and denormalization methods (Equation 4) (Kim et al., 2021; Zeng et al., 2023) as follows:\nXnorm = X \u2212 X\u0304 , Ypred = (Yshort + Ylong) + X\u0304 (4)\nwhere X\u0304 is the mean of the input sequence. These simple methods help to effectively alleviate the distribution shift problem (Kim et al., 2021) by nonstationarity of the real-world time series.\nShort-term Branch for Temporal Locality Next, we observe that short-period variations often repeat multiple times within the input sequence and exhibit similar patterns with temporally close sequences. This locality of short-term variations supports the recent success of linear-based models (Zeng et al., 2023), which use sequential information of adjacent sequences only. Therefore, we\nemploy the linear layer for the short-term prediction as follows:\nYshort = Linear(Xnorm). (5)\nLong-term Branch for Temporal Globality The long-term branch, designed to apply the AutoCon method, employs an encoder-decoder architecture. The encoder with sufficient capacity to learn the long-term presentation leverages both sequential information and global information (i.e., timestampbased features derived from T ) as follows:\nv = Enc(Xnorm, T ). (6)\nThe choice of network for the encoder is flexible as long as there are no issues in processing long sequences. We opted for temporal convolution networks (Bai et al., 2018) (TCNs), widely used in learning time-series representation (Yue et al., 2022), for its computational efficiency. The decoder employs the multi-scale Moving Average (MA) block (Wang et al., 2023), with different kernel sizes {ki}ni=1 to capture multiple periods based on the representation v as follows:\nY\u0302long = 1\nn n\u2211 i=1 AvgPool(Padding(MLP (v)))ki . (7)\nThe MA block at the head of the long-term branch smooths out short-term fluctuations, naturally encouraging the branch to focus on long-term information. Our redesigned architecture is optimized by the objective function L as follows:\nL = LMSE + \u03bb \u00b7 LAutoCon. (8)\nwhere the mean square error (MSE) and the AutoCon loss are combined with the weight \u03bb as a hyperparameter. The hyperparamer sensitivity analysis is available in Appendix A.6. Detailed descriptions of each operation (e.g., Linear, Padding, and MLP ) can be found in the Appendix A.1."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": "To validate our proposed method, we conducted extensive experiments on nine real-world datasets from six domains: mechanical systems (ETT), energy (Electricity), traffic (Traffic), weather (Weather), economics (Exchange), and disease (ILI). We follow standard protocol (Wu et al., 2021) and split all datasets into training, validation, and test sets in chronological order by the ratio of 6:2:2. We select the latest baseline models with different architectures categorized into linear-based (Zhou et al., 2022a; Zeng et al., 2023), CNN-based (Wu et al., 2023; Wang et al., 2023), and Transformer-based (Zhou et al., 2022b; Liu et al., 2022b; Nie et al., 2023). Additionally, we compared our model with two models (Challu et al., 2023; Zhang & Yan, 2023) that focus on learning inter-channel dependencies for multivariate forecasting. Appendix A provides more detailed information about the datasets and baseline implementations."
        },
        {
            "heading": "4.1 MAIN RESULTS",
            "text": "Extended Long-term Forecasting In our pursuit to better evaluate our model\u2019s performance in predicting long-term variations\u2014which tend to have increasing significance as the forecast length extends\u2014we designed our experiments to extend the prediction length O for each dataset. This shift from the conventional benchmark experiments, which typically predict up to 720 steps, allows us to explore the model\u2019s capability in more challenging forecast scenarios. For the datasets with longer total lengths, such as ETTh, Electricity, Traffic, and Weather, we tripled the prediction length from 720 to 2160. Also, for Exchange and ILI datasets with shorter total lengths, we extend the output length up to 1080 and 112, respectively. Overall, Table 1 shows that our model with AutoCon outperformed the state-of-the-art baselines by achieving first place 42 times in the univariate setting. When examining the performance changes according to length, our model showed significant improvement compared to other best models when predicting further into the future (e.g., on average, errors decreased by 5% at 96 and 720, and by 12% at 1440 and 2160). These results empirically demonstrate the contribution of our AutoCon in effectively capturing long-term variations that exist beyond the window.\nDataset Analysis Since our goal is to learn long-term variations, the performance improvements of our model can be affected by the magnitude and the number of long-term variations. Figure 5 shows various yearly-long business cycles and natural cycles unique to each dataset. For instance, the ETTh2 and Electricity datasets have strong long-term correlations with peaks at several lags repeated multiple times. Thus, our method on the ETTh2 and Electricity datasets exhibited significant\nTable 1: Extended long-term forecasting results with various prediction lengths O and the best input length I \u2208 {48, 96, 168, 336} for each model except for Illness dataset with I = 14. Red and blue numbers denote the best and second-best results, respectively. The full benchmarks with ETTh1 and ETTm are available in Appendix D.\nModels Ours TimesNet MICN PatchTST DLinear FiLM Nonstationary FEDformer(2023) (2023) (2023) (2023) (2022a) (2022b) (2022b)\nO MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE\nE T\nT h2 96 0.124 0.269 0.139 0.290 0.122 0.264 0.136 0.292 0.128 0.271 0.129 0.275 0.192 0.343 0.129 0.277 720 0.177 0.344 0.207 0.370 0.313 0.457 0.233 0.392 0.319 0.461 0.256 0.407 0.231 0.394 0.273 0.419 1440 0.176 0.340 0.192 0.358 0.520 0.599 0.351 0.481 0.514 0.597 0.389 0.506 0.211 0.379 0.384 0.487 2160 0.198 0.358 0.263 0.413 0.759 0.734 0.610 0.659 0.740 0.728 0.610 0.645 0.240 0.399 0.919 0.737\nE le\nct ri\nci ty 96 0.196 0.313 0.286 0.386 0.241 0.367 0.227 0.336 0.207 0.322 0.394 0.451 0.332 0.426 0.279 0.393\n720 0.275 0.386 0.417 0.471 0.336 0.446 0.332 0.426 0.304 0.412 0.467 0.504 0.505 0.533 0.417 0.486 1440 0.338 0.441 0.491 0.523 0.419 0.504 0.482 0.537 0.395 0.484 0.625 0.610 0.577 0.574 0.651 0.609 2160 0.380 0.481 0.536 0.547 0.421 0.501 0.768 0.644 0.415 0.496 0.938 0.758 0.642 0.610 0.896 0.714\nTr af\nfic 96 0.132 0.206 0.145 0.219 0.168 0.256 0.192 0.296 0.219 0.327 0.264 0.334 0.247 0.326 0.220 0.312 720 0.144 0.225 0.163 0.269 0.304 0.394 0.213 0.318 0.309 0.419 0.247 0.329 0.277 0.360 0.255 0.344 1440 0.174 0.251 0.188 0.292 0.375 0.443 0.246 0.341 0.353 0.409 0.311 0.390 0.303 0.361 0.297 0.376 2160 0.175 0.252 0.190 0.304 0.360 0.426 0.261 0.353 0.324 0.386 0.988 0.745 0.222 0.317 0.317 0.394\nW ea\nth er 96 0.521 0.522 0.584 0.536 0.569 0.525 0.545 0.539 0.579 0.529 0.589 0.533 0.636 0.567 0.703 0.625720 0.963 0.715 1.090 0.753 1.080 0.754 0.987 0.752 1.007 0.706 1.003 0.728 1.007 0.725 1.114 0.822\n1440 1.280 0.835 1.547 0.926 1.351 0.863 1.342 0.860 1.299 0.823 1.472 0.900 1.394 0.867 1.435 0.919 2160 1.415 0.887 1.744 0.994 1.544 0.937 1.506 0.924 1.454 0.887 1.712 0.988 1.598 0.944 1.786 1.054\nE xc\nha ng e 48 0.051 0.172 0.054 0.178 0.054 0.181 0.068 0.197 0.049 0.170 0.052 0.173 0.054 0.178 0.059 0.184 360 0.448 0.527 0.479 0.532 0.459 0.536 0.548 0.573 0.485 0.531 0.492 0.534 0.493 0.541 0.528 0.556 720 1.067 0.794 1.239 0.856 1.383 0.927 1.264 0.859 1.718 1.024 1.291 0.864 1.358 0.894 1.381 0.903 1080 1.004 0.792 1.327 0.900 4.874 1.972 1.255 0.873 4.982 1.973 1.670 1.010 1.774 1.058 1.600 0.980\nIL I 14 0.725 0.574 1.414 0.735 0.815 0.701 1.558 0.965 1.397 0.901 1.079 0.739 1.107 0.698 0.773 0.619 28 0.887 0.683 1.604 0.854 1.670 1.062 1.878 1.110 2.008 1.134 1.315 0.887 1.515 0.767 0.989 0.770 56 0.807 0.725 1.021 0.787 1.757 1.210 1.451 1.028 1.584 1.075 1.080 0.891 0.895 0.742 0.856 0.741 112 1.499 1.038 1.669 1.072 3.593 1.759 2.846 1.438 3.332 1.572 2.608 1.387 1.724 1.108 1.660 1.097\n1st Count 42 0 2 0 4 0 0 0\n0\n2000\n4000\n6000 Electricity\n0 5000 10000 15000 20000 25000 0.00 0.25 0.50 0.75 1.00\n0\n20\n40\n60 ETTh2\n0 2500 5000 7500 10000 12500 0.00 0.25 0.50 0.75 1.00 0.0\n0.1\n0.2 Traffic\n0 5000 10000 15000 0.00 0.25 0.50 0.75 1.00 300\n400\n500\nWeather\nLong-term variation\n0 10000 20000 30000 40000 50000 0.00 0.25 0.50 0.75 1.00 Autocorrelation Maximum Correlation Within window Outer window\nFigure 5: The outer-window autocorrelation exists in varying degrees in four datasets.\nperformance gains, which are 34% and 11% reduced error compared to the second-best model, respectively. In contrast, the Weather dataset has relatively lower correlations outside the windows than the aforementioned two datasets. This leads our model to show the least improvement with a 3% reduced error on the Weather dataset. As a result, our method\u2019s superiority manifests more strongly for the datasets with stronger long-term correlation, thus empirically validating our contribution. Extension to Multivariate Forecasting As shown in Table 2, our method is applicable in multivariate forecasting by calculating autocorrelation on a per-channel basis and then following a channel independence approach (Nie et al., 2023). Appendix A.2 describes details for multivariate setting."
        },
        {
            "heading": "4.2 MODEL ANALYSIS",
            "text": "Temporal Locality and Globality As mentioned in Section 3.2, we proposed a model architecture that combines the advantages of both linear models for locality and deep models for globality. Figure 6(a) demonstrates that, for short-term predictions up to 96 units, the linear model (DLinear) achieved a lower error rate than the deep models such as TimesNet, Nonstationary, and FEDformer. However, the error of DLinear started to diverge as the prediction length extended. Conversely, the TimesNet and Nonstationary maintained a consistent error rate even with the increase in prediction length but didn\u2019t perform as well as the linear model for short-term predictions. These observations served as the motivation for our decomposition architecture that is proficient in both short-term and long-term predictions (blue line in Figure 6(a)). Ablation Studies Here, we conducted ablation studies to validate each component of our method. Figure 6(b) shows the results of the ablation study conducted on the full model, and when the short-term branch was removed, it showed a significant error for short-term predictions. When the long-term branch was removed, it showed a significant error for long-term predictions. Also, without\nthe integration of our AutoCon, the long-term performance was degraded. As demonstrated in Table 3, these trends were consistent across a variety of datasets."
        },
        {
            "heading": "4.3 COMPARISON WITH REPRESENTATION LEARNING METHODS",
            "text": "We also demonstrate the effectiveness of our method in capturing long-term representations beyond the window compared to existing time-series representation learning methods. TS2Vec (Yue et al., 2022) and CoST (Woo et al., 2022a) are both unsupervised contrastive learning methods, with TS2Vec only considering the augmented data of the same time index as a positive pair and CoST using a loss that takes into account periodicity, but both have the limitation of only being effective within a window. Therefore, while they show competitive performance in relatively short lengths, they fail to predict accurately for long-term periods. LaST (Wang et al., 2022) is a decomposition-based\nrepresentation learning method and also shows competitive performance in short-term predictions, but fails to predict accurately for long-term periods. Figure 7 shows the learned representation by AutoCon with the other three methods. Appendix C.2 provides experimental protocols and further comparison experiments."
        },
        {
            "heading": "4.4 COMPUTATIONAL EFFICIENCY COMPARISON",
            "text": "Our proposed model shows competitive computational efficiency among other deep models. Specifically, on the ETT dataset, our model without AutoCon exhibits computational times of 31.1 ms/iter, second best after the linear models. Even with the integration of AutoCon during training, the computational cost does not increase significantly (33.2 ms/iter) since there is no augmentation process and the autocorrelation calculation occurs only once during the entire training. Consequently, our model\u2019s computational efficiency surpasses existing Transformer-based models (Nonstationary 365.7 ms/iter) and recent state-of-the-art CNN-based models (TimesNet 466.1 ms/iter). Detailed comparisons can be found in Appendix B.4."
        },
        {
            "heading": "5 DISCUSSION & LIMITATIONS",
            "text": "Our proposed method mitigates the constraint of the sliding window approach by learning the longterm variations beyond the window. Nevertheless, we examine whether the limitation of the sliding window we point out can be solved by simply increasing the window length without our method, and also elucidate a limitation of our method.\nCan we use a very long window to capture long-term variations? Given a time series S of length T , the number of windows M is T \u2212 (I +O) + 1. This implies that as the input length I (i.e., data complexity) increases, while keeping the output length O fixed, the number of data instances (i.e., windows) available for learning decreases, potentially making the model more susceptible to overfitting (Park et al., 2023) as shown in Figure 8. Consequently, it is challenging for input\nsequences to be long enough to cover all long-term variations present in the data, and models often struggle to capture variations outside the window. Therefore, the limitation we point out regarding the sliding window approach is valid in most situations and worth addressing. Appendix B.1 presents comprehensive experiments and empirical findings obtained upon increasing the window length.\nCan Autocorrelation capture all long-term variations ? While autocorrelation serves as a valuable tool for capturing certain long-term variations, its linearity assumption limits its effectiveness in dealing with the non-linear patterns and relationships prevalent in real-world time series data. By considering higher-order correlations, non-linear dependencies, and external factors, we are likely to achieve even more accurate and comprehensive long-term forecasting."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "This work was supported by the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (No.NRF-2020H1D3A2A03100945, No.NRF-2022R1A2B5B02001913), and Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government (MSIT) (No.2019-0-00075, No.2022-0-00984)."
        },
        {
            "heading": "A REPRODUCIBILITY",
            "text": ""
        },
        {
            "heading": "A.1 DETAILS OF OUR METHOD IMPLEMENTATION",
            "text": "In this subsection, we provide a detailed explanation of the operations that were used in Section 3.2. Firstly, Linear signifies a linear layer along the time dimension. When provided with an input sequence X , the output Y\u0302 is computed as:\nY\u0302 = Wtime \u00b7 X (9)\nwhere X \u2208 RI\u00d7c, Y\u0302 \u2208 RO\u00d7c, and Wtime \u2208 RO\u00d7I . Next, we describe Padding(\u00b7) and Avgpool(\u00b7) along with the time axis as follows:\nXpad = Padding(X ) (10) Xavg = Avgpool(Xpad)k (11)\nXavg[t, c] = 1\nk k\u22121\u2211 i=0 Xpad[t+ i, c] (12)\nwhere Xpad \u2208 R(I+2(k\u22121))\u00d7c is padded sequence on both sides with neighboring values to preserve input length after applying Avgpool(\u00b7). The various kernel sizes of Avgpool(\u00b7) are selected to handle the multi-periodicities, which are observed in real-world time-series data.\nMLP denotes the use of two linear layers with an activation function g, specifically GELU. Given the input sequence X , the output Y is computed as:\nY\u0302 = g(Wtime \u00b7 X + btime) \u00b7Wchannel + bchannel (13)\nwhere btime \u2208 Rd, bchannel \u2208 Rc, Wtime \u2208 RO\u00d7I , and Wchannel \u2208 Rd\u00d7c. To compute AutoCon, global autocorrelation must first be determined. As part of the preprocessing process, we calculate autocorrelation for the entire training series, excluding validation and test series. This autocorrelation comprises both long and short-period variations. However, in our pursuit of disentangled long-term representation, we intend that the long-term branch address only lowfrequency variations with long periods. Therefore, we smooth out short-period fluctuations in the series before computing the autocorrelation.\nOur training protocol is identical to conventional training methods, except for the inclusion of AutoCon as an additional loss, in addition to the forecasting loss. To clarify this further, we also present the algorithm.\nAlgorithm 1 AutoCon: Autocorrelation-based Constrastive Learning Framework\nRequire: Entire time series S = {s1, . . . , sT }, Training set D = {(Xt,Yt) , Tt}Mt=1, AutoCon weight \u03bb where T denotes a total length and M denotes the number of windows.\nCompute the global autocorrelation RSS(l) for all possible lags l \u2208 [0 : M ]\nfor all number of training iterations do Sample a mini-batch {((Xn,Yn) , Tn)}Nn=1 from D Forward {Xn, Tn}Nn=1 and get corresponding representation {v}Nn=1 and predictions {Y\u0302n}Nn=1 Compute window relationship matrix r \u2208 RN\u00d7N , r(i,j) = RSS(|T (0)i \u2212 T (0) j |)\nCompute AutoCon loss LAutoCon following Equation 3 as inputs {v(n)}Nn=1 and r Compute forecasting loss Lmse = 1N \u2211N n=1(Y\u0302n \u2212 Yn)2\nDo one training step using the full loss L = Lmse + \u03bb \u00b7 LAutoCon end for\nOur redesigned model and AutoCon were implemented based on the TSlib code repository 1. Our source code can be accessed at a zip file in the supplementary."
        },
        {
            "heading": "A.2 DETAILS OF MULTIVARIATE FORECASTING",
            "text": "There are two representative approaches to multivariate forecasting: Channel-mixing and Channelindependence approaches. The channel-mixing approach involves mapping the values of multiple channels at the same step into an embedding space and extracting temporal dependencies from this embedding sequence. This approach has been adopted by various papers (Zhou et al., 2022b; Wu et al., 2023; Zhang & Yan, 2023). The channel-independence approach, on the other hand, preserves each channel\u2019s information without mixing them and learns temporal patterns within each channel independently. Recently, this method has been adopted in high-performing models such as PatchTST (Challu et al., 2023) and Linear models (Zeng et al., 2023), demonstrating superior performance on current benchmark datasets. In terms of implementation, each channel is treated as a batch axis for computations. This effectively increases the amount of training data by the number of channels, and the model parameters are shared across multiple channels. Following the channel- independence approach, we first compute autocorrelations for each channel separately in order to calculate AutoCon. We then train the representation tailored to each channel based on these autocorrelations."
        },
        {
            "heading": "A.3 DETAILS OF DATASETS",
            "text": "In this paper, we utilized six real-world datasets from diverse domains: mechanical systems (ETT), energy (Electricity), traffic (Traffic), weather (Weather), economics (Exchange), and disease (ILI). The statistics of each dataset are summarized and found in Table 5. As a mainstream benchmark, the ETT datasets are extensively utilized for assessing long-term forecasting methods Zhou et al. (2021); Wu et al. (2021); Zhou et al. (2022b); Zeng et al. (2023); Wu et al. (2023). ETT consists of critical\n1https://github.com/thuml/Time-Series-Library\nindicators (such as oil temperature, load, and others) that are gathered over a span of two years from electricity transformers. These datasets are grouped into four distinct sets based on location (ETT1 and ETT2) and time interval (15 minutes and one hour). The Electricity dataset captures the hourly electricity consumption of 321 customers from 2012 to 2014. On the other hand, the Traffic dataset compiles hourly data from the California Department of Transportation, detailing the occupancy rates of roads as measured by different sensors on freeways in the San Francisco Bay area. The Weather dataset consists of 21 meteorological indicators, including air temperature, humidity, and others, recorded at 10-minute intervals over the course of a year. The Exchange dataset chronicles daily exchange rates of eight different countries from 1990 through 2016. Lastly, the ILI dataset includes weekly records of influenza-like illness (ILI) patient data from the Centers for Disease Control and Prevention in the United States, spanning from 2002 to 2021. This dataset illustrates the ratio of patients diagnosed with ILI relative to the total patient count."
        },
        {
            "heading": "A.4 BASELINE MODELS",
            "text": "In the realm of long-term forecasting, numerous models have been proposed since the advent of Informer. These models have demonstrated commendable performance with unique novelties. However, they were compared with underperforming models such as the models based on RNNs, and Transformer-based models, which are known to be susceptible to overfitting. Therefore, our primary focus is on high-performing and state-of-the-art models among the most recent proposals. We validate our method against seven forecasting baselines and three representation methodologies. All models were implemented using PyTorch. For the latest forecasting models, namely TimesNet2, DLinear and NLinear3, MICN4, FiLM5, Nonstationary Transformer6, and FEDformer7, we utilized the official code released by the original authors rather than implementing it from scratch.\nSimilarly, for recent representation methods, such as LaST8, CoST9, and TS2Vec10, we utilized the official codes provided by the authors instead of implementing models from scratch. We adhered to the unique hyperparameters of each model, tuning within the parameter search range that yielded optimal performance. However, certain configurations, such as input length and output length, were set uniformly for ease of comparison. More specific evaluation protocols will be presented in the following section.\n2https://github.com/thuml/Time-Series-Library 3https://github.com/cure-lab/LTSF-Linear 4https://github.com/wanghq21/MICN 5https://github.com/DAMO-DI-ML/NeurIPS2022-FiLM 6https://github.com/thuml/Nonstationary Transformers 7https://github.com/MAZiqing/FEDformer 8https://github.com/zhycs/LaST 9https://github.com/salesforce/CoST\n10https://github.com/yuezhihan/ts2vec"
        },
        {
            "heading": "A.5 EVALUATION DETAILS",
            "text": "In our experiments, we aim to assess the model\u2019s capability to capture long-term variations, so that the output length should be long enough to be affected by these variations. Increasing the output length more than those used in previous experiments, however, involves several considerations. Thus, we delineate the modifications to the standard evaluation protocol as follows:\n1. The input length I is set to 14 (for the ILI dataset), 48 (for the Exchange dataset), 192 (for the ETTm dataset), and 96 (for the others datasets). These input lengths allow us to increase the output length within the limited window length, in accordance with the total length of each dataset.\n2. The standard protocol splits all datasets into training, validation, and test sets in chronological order, with a ratio of 6:2:2 for the ETT dataset and 7:1:2 for the remaining datasets. However, due to the increased window length in the other datasets, the validation set is insufficiently populated. Therefore, we adopt a ratio of 6:2:2 for all datasets.\n3. The weather dataset contains negative values for indicators that should logically be nonnegative. These erroneous labels, if not corrected, could impede accurate evaluation due to scaling issues. We rectified these errors by filling them with neighboring values.\nWe adhered to standard protocols for all experiments, barring the exceptions outlined above."
        },
        {
            "heading": "A.6 HYPERPARAMETER SENSITIVITY",
            "text": ""
        },
        {
            "heading": "B ADDITIONAL EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "B.1 EXPERIMENTS ON WINDOW LENGTH",
            "text": "As mentioned in Section 5, we considered simply increasing the length of the window to capture the long-term variations as much as possible. Also, as the window length increases, the number of data windows used for learning decreases. After all, we hypothesize that increasing the window length increases the input complexity of the model, while reducing the number of data points, making the model vulnerable to overfitting.\nFigures 10 and 11 depict the training and testing losses when the input length increase from 192 to 920 for the purpose of predicting 720 steps in ETTh1 and ETTh2, respectively. We observed that the overall test loss tends to soar or converge, while the training loss persistently decreases when the input size is increased in five models with varying capacities and attributes.\nAlso, in the case of DLinear in Figure 11, both test and training losses decline in sync due to limited capacity. However, we regard it as an underfitting problem since test errors are higher than our method (see red line). Consequently, we empirically substantiate that merely increasing the input length does not necessarily enhance the performance of long-term forecasting. Moreover, it is noteworthy that the computational cost for complex models other than linear models increases significantly with increasing sequence length."
        },
        {
            "heading": "B.2 ADDITIONAL FIGURE 6 RESULTS ON OTHER DATASETS",
            "text": "Additionally, we provide results for ETTh1 (Figure 12) and Electricity (Figure 13), which show the long-term variations. Although there are some differences in magnitude, the overall trends are similar across the three datasets."
        },
        {
            "heading": "B.3 ADDITIONAL ABLATION RESULTS FOR LONG-TERM BRANCH",
            "text": "Increasing the complexity of the long-term branch is essential for learning long-term representations, but it is not the sole reason for the superiority of our methodology. In other words, even with the increased complexity, capturing long-term variation is not easy in the current framework that uses only forecasting loss. As the main contribution, using AutoCon is essential for learning long-term variation and leading to performance improvement. To verify this, we additionally present two ablation results: increasing complexity in DLinear and in our model.\nFirst, DLinear utilizes only a single linear layer for both long-term and short-term branches. We increase the complexity of the long-term branch by stacking linear layers with an activation function in the long-term branch. However, as evident in Table 6 below, even when stacking layers in the long term, performance tends to decrease or remain similar. This demonstrates that increasing long-term complexity is not effective in the existing decomposition architecture.\nSecond, the following Table 7 demonstrates the performance changes based on the complexity of the long-term branch in our decomposition architecture. Without Autocon, our model may be slightly better or comparable to the second-best model. The highest performance is achieved only when AutoCon is employed. This further underscores the necessity of the AutoCon we proposed to accurately predict long-term variation."
        },
        {
            "heading": "B.4 ANALYSIS OF COMPUTATIONAL COST",
            "text": "Given the real-time nature of most time-series applications, computational efficiency is a crucial factor in time-series forecasting (Dannecker, 2015; Iqbal et al., 2019; Torres et al., 2021). As forecasting horizons increase, the window length expands, leading to increased computational costs. Therefore, it is imperative to evaluate a model\u2019s computational efficiency. Figure 14 illustrates the time required to update the parameters by a single batch for our model in comparison with the baseline models. The computational cost was measured for four different output lengths, ranging from 96 to 2160, for each model. A batch size of 32 was used, and all measurements were taken independently in the same GPU and server environment. Firstly, the linear model took the least amount of time due to its minimal number of parameters and the simplicity of matrix multiplication operations. On the other hand, TimesNet required the most time as it extracts multiple periods and computes a loop for each period. The Nonstationary model, which is based on the Transformer, has a computational complexity of O(W 2) in relation to length, which explains the sharp increase in computation time with length. Overall, our model was the second fastest after the linear model, and its computation cost did not\nsignificantly increase even when training included AutoCon (from 31.1 ms/iter to 33.2 ms/iter). Hence, our method manages to achieve superior long-term forecasting performance compared to\nlinear models, while requiring less computational resources than other more complex models. The cost analysis was briefly addressed in Section 4.4 of the main paper.\nB.5 VISUALIZATION OF FORECASTING RESULTS\nFigure 15 provides a qualitative result of the five different models by visualizing the prediction results for 1440 steps in the ETTh2 dataset. In the case of linear models, the error increases as the prediction distance increases, failing to account for long-term variations. Nonstationary and TimesNet models, although better at following long-term variations than the linear model, struggle to capture high-frequency patterns effectively. Our model, on the other hand, successfully manages to capture both long-term variations and high-frequency patterns. This can be attributed to our model\u2019s structure, which is designed to benefit from both short-term and long-term predictions."
        },
        {
            "heading": "B.6 EVALUATION RESULTS WITH OTHER METRICS",
            "text": "While existing metrics (i.e., MSE and MAE) are standard metrics for long-term forecasting evaluation, they have limitations. Specifically, they may not adequately capture aspects such as the shape and temporal alignment of the time series, which are crucial for a comprehensive evaluation of a forecasting model\u2019s performance.\nTo address these limitations, we introduced two additional metrics based on Dynamic Time Warping (DTW) (Sakoe & Chiba, 1978): Shape DTW and Temporal DTW (Le Guen & Thome, 2019). Shape DTW focuses on the similarity of the pattern or shape of the predicted sequence to the actual sequence, providing insight into the model\u2019s ability to capture the underlying pattern of the time series. Temporal DTW evaluates the alignment of the predicted sequence with the actual sequence, highlighting the model\u2019s accuracy in forecasting the timing of events.\nThese additional metrics offer a more nuanced assessment of our model\u2019s performance, particularly in areas where MSE and MAE may fall short. Lower values in both Shape DTW and Temporal DTW indicate better performance, signifying lesser distortion between the predicted and actual sequences. As shown in Table 8, our method demonstrates superior performance not only in MSE and MAE but also in these shape and temporal alignment-focused metrics."
        },
        {
            "heading": "C ANALYSIS OF REPRESENTATION",
            "text": ""
        },
        {
            "heading": "C.1 DETAILS OF REPRESENTATION SIMILARITIES",
            "text": "In Figure 2, we used three baselines with our model and extracted the representations of each baseline either before the final projection layer (TimesNet) or after the encoder layer (PatchTST, FEDformer, and Ours). Our main purpose in visualizing the representation was to demonstrate the learning of long-term correlations. To display more clearly, we applied a filtering method to smooth short-term fluctuations within the given window. We also provide original representation results, which are enlarged for each baseline, without smoothing out the short-term fluctuations as shown in Figure 16. Figure 16 shows that the three baseline models learn the short-term correlations within the window, although they do not learn the long-term correlations.\nOne interesting point in this finding is that existing models have attempted to address the limitations of window length by leveraging time-stamp information. Actually, TimesNet, FEDformer, and our model obtained the representations using timestamps, incorporating them into the input sequences, while PatchTST does not utilize the timestamps. However, not only PatchTST but also both TimesNet and FEDformer do not effectively capture the annual cyclic patterns, despite utilizing timestamps as the same as our model. These failures are noteworthy, particularly considering the Electricity time series, which displays a yearly-long periodicity. These results show that it is challenging for the model to learn yearly patterns even when given input sequences and timestamps, solely relying on the existing forecasting loss. Therefore, this result demonstrates the necessity of our AutoCon loss. Furthermore, to justify the emergence of long-term representation irrespective of the model\u2019s structural aspects, we provide additional results of the representation from an ablation model that does not utilize AutoCon in our model. As shown in Figure 17, the model without AutoCon also exhibits a weak periodicity, but similar to other baselines, representation similarity remains relatively flat compared to our full model.\nC.2 VISUALIZATION OF REPRESENTATIONS\nWe benchmarked our AutoCon method against three representation learning methods proposed to enhance forecasting performance. TS2Vec and CoST have a two-stage learning framework in which they utilize a ridge regression model for time-series forecasting and deep learning-based models for representation learning. On the other hand, LaST and our method adopt an end-to-end learning framework wherein both representation and time-series forecasting learning occur concurrently.\nFigure 7 presents the representation results of four methods over the ETTh2 dataset. To investigate whether each method has learned the representation structure associated with the long-term variations, we extracted representations corresponding to all training time steps and visualized them via UMAP with the month labels derived from the timestamp. Our model clearly displays continuity between adjacent months and demonstrates well-defined clustering, attributes not seen in the other models. It seems that the other models did not learn the structure necessary for recognizing one-year long-term variations beyond the window, as their representation learning was confined within the window."
        },
        {
            "heading": "C.3 ADDITIONAL COMPARISON WITH TWO SELF-SUPERVISED LOSS",
            "text": "We designed and provided results for two possible self-supervised objectives based on HierCon (Yue et al., 2022) and SupCon (Khosla et al., 2020) that can be incorporated into our two-stream model structure. HierCon induces the representations of two partially overlapped windows to be close to each other, while SupCon encourages the encoder to learn close representations for the windows with the same month label.\nThe two SSL objectives were tested with our model architecture, replacing only the AutoCon loss. As shown in Table 9, compared to the one without any SSL loss, HierCon shows a slight improvement in performance on the ETTh1 and ETTh2 for short-term predictions with a length of 96, but it performs worse in the long-term prediction, as it emphasizes only temporal closeness. SupCon leveraged monthly information beyond the window length, leading to performance improvements even in the long-term prediction. However, SupCon can only learn a single predefined periodicity, unlike AutoCon. Consequently, SupCon shows lower performance than AutoCon in learning the periodicities existing in the time series through autocorrelation."
        },
        {
            "heading": "D FULL BENCHMARKS",
            "text": "Table 10 and Table 11 show the full benchmark results including confidence intervals."
        }
    ],
    "title": "SELF-SUPERVISED CONTRASTIVE FORECASTING",
    "year": 2024
}