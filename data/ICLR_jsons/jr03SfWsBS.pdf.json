{
    "abstractText": "Seven years ago, researchers proposed a postprocessing method to equalize the error rates of a model across different demographic groups. The work launched hundreds of papers purporting to improve over the postprocessing baseline. We empirically evaluate these claims through thousands of model evaluations on several tabular datasets. We find that the fairness-accuracy Pareto frontier achieved by postprocessing the predictor with highest accuracy contains all other methods we were feasibly able to evaluate. In doing so, we address two common methodological errors that have confounded previous observations. One relates to the comparison of methods with different unconstrained base models. The other concerns methods achieving different levels of constraint relaxation. At the heart of our study is a simple idea we call unprocessing that roughly corresponds to the inverse of postprocessing. Unprocessing allows for a direct comparison of methods using different underlying models and levels of relaxation.",
    "authors": [
        {
            "affiliations": [],
            "name": "ALGORITHMIC FAIRNESS"
        },
        {
            "affiliations": [],
            "name": "Andr\u00e9 F. Cruz"
        },
        {
            "affiliations": [],
            "name": "Moritz Hardt"
        }
    ],
    "id": "SP:3ce655be189fbb0e826b5782adbf8d3e36030460",
    "references": [
        {
            "authors": [
                "Alekh Agarwal",
                "Alina Beygelzimer",
                "Miroslav Dudik",
                "John Langford",
                "Hanna Wallach"
            ],
            "title": "A reductions approach to fair classification",
            "venue": "In International Conference on Machine Learning,",
            "year": 2018
        },
        {
            "authors": [
                "Timothy G. Armstrong",
                "Alistair Moffat",
                "William Webber",
                "Justin Zobel"
            ],
            "title": "Improvements that don\u2019t add up: ad-hoc retrieval results since 1998",
            "venue": "In Proceedings of the 18th ACM Conference on Information and Knowledge Management,",
            "year": 2009
        },
        {
            "authors": [
                "Michelle Bao",
                "Angela Zhou",
                "Samantha Zottola",
                "Brian Brubach",
                "Sarah Desmarais",
                "Aaron Horowitz",
                "Kristian Lum",
                "Suresh Venkatasubramanian"
            ],
            "title": "It\u2019s compaslicated: The messy relationship between RAI datasets and algorithmic fairness benchmarks",
            "venue": "In Joaquin Vanschoren and Sai-Kit Yeung (eds.), Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks",
            "year": 2021
        },
        {
            "authors": [
                "Matias Barenstein"
            ],
            "title": "Propublica\u2019s compas data revisited, 2019",
            "year": 2019
        },
        {
            "authors": [
                "Solon Barocas",
                "Moritz Hardt",
                "Arvind Narayanan"
            ],
            "title": "Fairness and Machine Learning: Limitations and Opportunities",
            "venue": "fairmlbook.org,",
            "year": 2019
        },
        {
            "authors": [
                "Sarah Bird",
                "Miro Dud\u00edk",
                "Richard Edgar",
                "Brandon Horn",
                "Roman Lutz",
                "Vanessa Milan",
                "Mehrnoosh Sameki",
                "Hanna Wallach",
                "Kathleen Walker"
            ],
            "title": "Fairlearn: A toolkit for assessing and improving fairness in AI",
            "venue": "Technical Report MSR-TR-2020-32,",
            "year": 2020
        },
        {
            "authors": [
                "Lynn A Blewett",
                "Julia A Rivera Drew",
                "Risa Griffin",
                "Natalie Del Ponte",
                "Pat Convey"
            ],
            "title": "IPUMS health surveys: Medical expenditure panel survey, version 2.1 [dataset",
            "venue": "Minneapolis: IPUMS,",
            "year": 2021
        },
        {
            "authors": [
                "L. Elisa Celis",
                "Lingxiao Huang",
                "Vijay Keswani",
                "Nisheeth K. Vishnoi"
            ],
            "title": "Classification with fairness constraints: A meta-algorithm with provable guarantees",
            "venue": "In Proceedings of the Conference on Fairness, Accountability, and Transparency,",
            "year": 2019
        },
        {
            "authors": [
                "Sam Corbett-Davies",
                "Emma Pierson",
                "Avi Feller",
                "Sharad Goel",
                "Aziz Huq"
            ],
            "title": "Algorithmic decision making and the cost of fairness",
            "venue": "In Proceedings of the 23rd acm sigkdd international conference on knowledge discovery and data mining,",
            "year": 2017
        },
        {
            "authors": [
                "Andrew Cotter",
                "Heinrich Jiang",
                "Serena Wang",
                "Taman Narayan",
                "Seungil You",
                "Karthik Sridharan",
                "Maya R. Gupta"
            ],
            "title": "Optimization with non-differentiable constraints with applications to fairness, recall, churn, and other goals",
            "venue": "Journal of Machine Learning Research,",
            "year": 2019
        },
        {
            "authors": [
                "Elliot Creager",
                "Joern-Henrik Jacobsen",
                "Richard Zemel"
            ],
            "title": "Environment inference for invariant learning",
            "venue": "Proceedings of the 38th International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Andr\u00e9 F. Cruz",
                "Pedro Saleiro",
                "Catarina Bel\u00e9m",
                "Carlos Soares",
                "Pedro Bizarro"
            ],
            "title": "Promoting fairness through hyperparameter optimization",
            "venue": "IEEE International Conference on Data Mining (ICDM),",
            "year": 2021
        },
        {
            "authors": [
                "Andr\u00e9 F. Cruz",
                "Catarina Bel\u00e9m",
                "S\u00e9rgio Jesus",
                "Jo\u00e3o Bravo",
                "Pedro Saleiro",
                "Pedro Bizarro"
            ],
            "title": "FairGBM: Gradient boosting with fairness constraints",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Steven Diamond",
                "Stephen Boyd"
            ],
            "title": "CVXPY: A Python-embedded modeling language for convex optimization",
            "venue": "Journal of Machine Learning Research,",
            "year": 2016
        },
        {
            "authors": [
                "Frances Ding",
                "Moritz Hardt",
                "John Miller",
                "Ludwig Schmidt"
            ],
            "title": "Retiring adult: New datasets for fair machine learning",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Michele Donini",
                "Luca Oneto",
                "Shai Ben-David",
                "John S Shawe-Taylor",
                "Massimiliano Pontil"
            ],
            "title": "Empirical risk minimization under fairness constraints",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Bradley Efron",
                "Robert J. Tibshirani"
            ],
            "title": "An Introduction to the Bootstrap. Number 57 in Monographs on Statistics and Applied Probability",
            "year": 1994
        },
        {
            "authors": [
                "Alessandro Fabris",
                "Stefano Messina",
                "Gianmaria Silvello",
                "Gian Antonio Susto"
            ],
            "title": "Algorithmic fairness datasets: the story so far",
            "venue": "Data Mining and Knowledge Discovery,",
            "year": 2022
        },
        {
            "authors": [
                "Maurizio Ferrari Dacrema",
                "Paolo Cremonesi",
                "Dietmar Jannach"
            ],
            "title": "Are we really making much progress? a worrying analysis of recent neural recommendation approaches",
            "venue": "In Proceedings of the 13th ACM Conference on Recommender Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Ian Goodfellow",
                "Jean Pouget-Abadie",
                "Mehdi Mirza",
                "Bing Xu",
                "David Warde-Farley",
                "Sherjil Ozair",
                "Aaron Courville",
                "Yoshua Bengio"
            ],
            "title": "Generative adversarial nets",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2014
        },
        {
            "authors": [
                "Ulrike Gr\u00f6mping"
            ],
            "title": "South german credit data: Correcting a widely used data set",
            "venue": "In Reports in Mathematics, Physics and Chemistry,",
            "year": 2019
        },
        {
            "authors": [
                "Moritz Hardt",
                "Eric Price",
                "Nathan Srebro"
            ],
            "title": "Equality of opportunity in supervised learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2016
        },
        {
            "authors": [
                "Ben Hutchinson",
                "Margaret Mitchell"
            ],
            "title": "50 years of test (un) fairness: Lessons for machine learning",
            "venue": "In Proceedings of the conference on fairness, accountability, and transparency,",
            "year": 2019
        },
        {
            "authors": [
                "Maximilian Kasy",
                "Rediet Abebe"
            ],
            "title": "Fairness, equality, and power in algorithmic decision-making",
            "venue": "In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency,",
            "year": 2021
        },
        {
            "authors": [
                "Sadegh Kharazmi",
                "Falk Scholer",
                "David Vallet",
                "Mark Sanderson"
            ],
            "title": "Examining additivity and weak baselines",
            "venue": "ACM Trans. Inf. Syst.,",
            "year": 2016
        },
        {
            "authors": [
                "Lydia T. Liu",
                "Max Simchowitz",
                "Moritz Hardt"
            ],
            "title": "The implicit fairness criterion of unconstrained learning",
            "venue": "Proceedings of the 36th International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Mario Lucic",
                "Karol Kurach",
                "Marcin Michalski",
                "Sylvain Gelly",
                "Olivier Bousquet"
            ],
            "title": "Are gans created equal? a large-scale study",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Aditya Krishna Menon",
                "Robert C Williamson"
            ],
            "title": "The cost of fairness in binary classification",
            "venue": "In Conference on Fairness, accountability and transparency,",
            "year": 2018
        },
        {
            "authors": [
                "Kevin Musgrave",
                "Serge Belongie",
                "Ser-Nam Lim"
            ],
            "title": "A metric learning reality check",
            "venue": "In Computer Vision \u2013 ECCV 2020: 16th European Conference,",
            "year": 2020
        },
        {
            "authors": [
                "Vilfredo Pareto"
            ],
            "title": "Manuale di economia politica: con una introduzione alla scienza sociale, volume 13",
            "venue": "Societa\u0300 editrice libraria,",
            "year": 1919
        },
        {
            "authors": [
                "Valerio Perrone",
                "Michele Donini",
                "Muhammad Bilal Zafar",
                "Robin Schmucker",
                "Krishnaram Kenthapadi",
                "C\u00e9dric Archambeau"
            ],
            "title": "Fair bayesian optimization",
            "venue": "In Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society,",
            "year": 2021
        },
        {
            "authors": [
                "Ravid Shwartz-Ziv",
                "Amitai Armon"
            ],
            "title": "Tabular data: Deep learning is not all you need",
            "venue": "Information Fusion,",
            "year": 2022
        },
        {
            "authors": [
                "Hilde Weerts",
                "Florian Pfisterer",
                "Matthias Feurer",
                "Katharina Eggensperger",
                "Edward Bergman",
                "Noor Awad",
                "Joaquin Vanschoren",
                "Mykola Pechenizkiy",
                "Bernd Bischl",
                "Frank Hutter"
            ],
            "title": "Can fairness be automated? guidelines and opportunities for fairness-aware automl",
            "venue": "arXiv preprint arXiv:2303.08485,",
            "year": 2023
        },
        {
            "authors": [
                "Jiaheng Wei",
                "Zhaowei Zhu",
                "Gang Niu",
                "Tongliang Liu",
                "Sijia Liu",
                "Masashi Sugiyama",
                "Yang Liu"
            ],
            "title": "Fairness improves learning from noisily labeled long-tailed data, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Blake Woodworth",
                "Suriya Gunasekar",
                "Mesrob I Ohannessian",
                "Nathan Srebro"
            ],
            "title": "Learning nondiscriminatory predictors",
            "venue": "In Conference on Learning Theory,",
            "year": 2017
        },
        {
            "authors": [
                "Muhammad Bilal Zafar",
                "Isabel Valera",
                "Manuel Gomez Rodriguez",
                "Krishna P. Gummadi"
            ],
            "title": "Fairness beyond disparate treatment & disparate impact: Learning classification without disparate mistreatment",
            "venue": "In Proceedings of the 26th International Conference on World Wide Web, WWW",
            "year": 2017
        },
        {
            "authors": [
                "Muhammad Bilal Zafar",
                "Isabel Valera",
                "Manuel Gomez-Rodriguez",
                "Krishna P Gummadi"
            ],
            "title": "Fairness constraints: A flexible approach for fair classification",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2019
        },
        {
            "authors": [
                "Rich Zemel",
                "Yu Wu",
                "Kevin Swersky",
                "Toni Pitassi",
                "Cynthia Dwork"
            ],
            "title": "Learning fair representations",
            "venue": "In International Conference on Machine Learning,",
            "year": 2013
        },
        {
            "authors": [
                "Graff"
            ],
            "title": "samples), the age of its data (dates back to 1973\u20131975), and encoding issues that make it impossible to retrieve accurate sensitive information such as the individual\u2019s sex (Gr\u00f6mping, 2019). Overall, a total of 11 different evaluation scenarios were studied, pertaining to 6 datasets, with sizes ranging from 49K to 2.3M samples. Confidence intervals and metric results are computed using",
            "year": 2019
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Risk minimizing predictors generally have different error rates in different groups of a population. When errors are costly, some groups therefore seem to bear the brunt of uncertainty, while others enjoy the benefits of optimal prediction. This fact has been the basis of intense debate in the field of algorithmic fairness, dating back to the 1950s (Hutchinson & Mitchell, 2019). A difference in error rates between groups, equally deserving of a resource, strikes many as a moral wrong (Angwin et al., 2016; Barocas et al., 2019).\nResearchers have therefore proposed numerous algorithmic interventions to mitigate a disparity in error rates. The most basic such method is known as postprocessing. Postprocessing sets groupspecific acceptance thresholds so as to minimize risk while achieving an equality in error rates across a desired set of groups. Postprocessing is both simple and computationally efficient.\nPerhaps because of its simplicity, postprocessing has been widely assumed to be sub-optimal. Troves of academic contributions seek to improve over postprocessing by more sophisticated algorithmic means. These efforts generally fall into two categories. Preprocessing methods aim to adjust the source data in such a manner that predictors trained on the data satisfy certain properties. Socalled \u201cinprocessing\u201d methods, in contrast, modify the training algorithms itself to achieve a desired constraint during the optimization step."
        },
        {
            "heading": "1.1 OUR CONTRIBUTIONS",
            "text": "Through a large, computationally intensive meta study we empirically establish that postprocessing is Pareto-dominant among all methods we were feasibly able to evaluate. Whatever level of accuracy can be achieved by any method at a specific level of error rate disparity, can also be achieved by setting group-specific acceptance thresholds on an unconstrained risk score.\nWe performed more than ten thousand model training and evaluation runs across five different prediction tasks from the folktables package (Ding et al., 2021), with two or four sensitive groups, based on tabular data from the US Census American Community Survey. We also include additional experiments on the Medical Expenditure Panel Survey (MEPS) (Blewett et al., 2021) dataset\nin the appendix. The methods include recent state-of-the-art algorithms, as well as standard baselines. While postprocessing is hyperparameter-free, we did extensive search for the best hyperparameters of all competing methods.\nOur work addresses two common methodological errors that have confounded previous comparisons with postprocessing.\nFirst, many preprocessing and inprocessing methods naturally do not achieve exact error rate equality, but rather some relaxation of the constraint. In contrast, postprocessing is typically applied so as to achieve exact equality. The primary reason for this seems to be that there is a simple and efficient method based on tri-search to achieve exact equality (Hardt et al., 2016). However, an efficient relaxation of error rate parity is more subtle and is therefore lacking from popular software packages. We contribute a linear programming formulation to achieve approximate error rate parity for postprocessing, and open-source our implementation in an easy-to-use Python package called error-parity.1 This allows us to compare methods to postprocessing at the same level of slack.\nSecond, different methods use base models of varying performance. Observed improvements may therefore be due to a better unconstrained base model rather than a better way of achieving error rate parity. How can we put different methods on a level playing field? We introduce a simple idea we call unprocessing that roughly corresponds to the inverse of postprocessing. Here, we take a model that satisfies error rate parity (approximately) and optimize group-specific thresholds so as to yield the best unconstrained model possible. Unprocessing maps any fairness-constrained model to a corresponding unconstrained counterpart. Both models have the same underlying risk-score estimates, to which we can then apply postprocessing. When comparing postprocessing to any given method we therefore do not have to come up with our own unconstrained model. We can simply steal, so to say, the unconstrained model implicit in any method.\nThese findings should not come as a surprise. Theory, perhaps overlooked, had long contributed an important fact: If an unconstrained predictor is close to Bayes optimal (in squared loss), then postprocessing this predictor is close to optimal among all predictors satisfying error rate parity (Hardt et al., 2016, Theorem 4.5). To be sure, this theorem applies to the squared loss and there are clever counterexamples in some other cases (Woodworth et al., 2017). However, our empirical evaluation suggests that these counterexamples don\u2019t arise in the real datasets we considered. This may be the case because, on the tabular datasets we consider, methods such as gradient boosting produce scores that are likely close to Bayes optimal under the squared loss. We focus on tabular data case-studies, an important basis for public policy decisions. High-dimensional datasets containing raw features (e.g., images, text) are not explored in the current paper, making for an interesting future work direction.\nLimitations and broader impacts. We are narrowly concerned with evaluating algorithms that achieve error rate parity approximately. We do not contribute any new substantive insights about fairness in different domains. Nor do we escape the many valid criticisms that have been brought\n1https://github.com/socialfoundations/error-parity\nforward against algorithmic fairness narrowly construed (Bao et al., 2021; Barocas et al., 2019; Kasy & Abebe, 2021). In particular, our work says nothing new about the question whether we should equalize error rates in the first place. Some argue that error rates should be a diagnostic, not a locus of intervention (Barocas et al., 2019). Others reject the idea altogether (Corbett-Davies et al., 2017). If, however, the goal is to equalize error rates exactly or approximately, the simplest way of doing so is optimal: Take the best available unconstrained model and optimize over group-specific thresholds."
        },
        {
            "heading": "1.2 RELATED WORK",
            "text": "Hardt et al. (2016) introduced error rate parity under the name of equalized odds in the context of machine learning and gave an analysis of postprocessing, including the aforementioned tri-search algorithm and theoretical fact. Woodworth et al. (2017) proposed a second-moment relaxation of equalized odds and an algorithm to achieve the relaxation on linear predictors, as well as examples of specific loss functions and predictors for which postprocessing is not optimal.\nNumerous works have considered constrained empirical risk formulations to achieve fairness criteria, see, e.g., Agarwal et al. (2018); Celis et al. (2019); Cotter et al. (2019); Cruz et al. (2023); Donini et al. (2018); Menon & Williamson (2018); Zafar et al. (2017; 2019) for a starting point. The work on learning fair representations (Zemel et al., 2013) spawned much follow-up work on various preprocessing methods. See Section 2 for an extended discussion of the related work that we draw on in our experiments. We are unable to survey the vast space of algorithmic fairness methods here.\nOur findings mirror several studies in ML and related fields that advocate for increased empirical rigor when proposing complex developments over simple baselines. Armstrong et al. (2009), and later Kharazmi et al. (2016), present evidence of the widespread use of weak baselines in information retrieval, leading to over-stated advancements in the field. Lucic et al. (2018) study state-of-the-art generative adversarial networks (GAN), and find no clear-cut improvements over the original GAN introduced in Goodfellow et al. (2014). Ferrari Dacrema et al. (2019) reach similar conclusions for the field of recommenders systems, and Musgrave et al. (2020) for the field of metric learning. We contribute to this growing body of work by showing that a simple postprocessing baseline matches or dominates all evaluated fairness interventions over a variety of datasets and evaluation scenarios."
        },
        {
            "heading": "2 EXPERIMENTAL SETUP",
            "text": "We conduct experiments on four standard machine learning models, paired with five popular algorithmic fairness methods. The standard unconstrained models in the comparison are: gradient boosting machine (GBM), random forest (RF), neural network (NN), and logistic regression (LR).\nRegarding fairness interventions, we include both pre- and inprocessing methods in our experiments. We use the learned fair representations (LFR) (Zemel et al., 2013) and the correlation remover (CR) (Bird et al., 2020) preprocessing fairness methods, respectively implemented in the aif360\nand fairlearn Python libraries. Additionally, we use the exponentiated gradient reduction (EG) and the grid search reduction (GS) inprocessing fairness methods (Agarwal et al., 2018) (implemented in fairlearn), as well as the FairGBM (Cruz et al., 2023) inprocessing method (implemented in fairgbm). The preprocessing methods (CR, LFR) can be paired with any other ML model (2 \u00b74 = 8 pairs), EG as well (4 pairs), and GS is compatible with GBM, RF, and LR models (3 pairs). FairGBM is naturally only compatible with GBM. Together with the four standard unconstrained models, there is a total of 20 different methods (or pairings) in the comparison.\nAlthough there have been numerous proposed fairness methods over the years, far fewer have available and ready-to-use open-source implementations. This is somewhat inevitable, as each new method would have to maintain a usable up-to-date implementation, as well as custom implementations for compatibility with different fairness criteria and different underlying base learners. Postprocessing approaches have a practical advantage: a single implementation is compatible with any underlying learner that can produce scores of predicted probabilities, and any fairness criterion that can be expressed as a constraint over the joint distribution of (Y , Y\u0302 , S), where Y is the true target, Y\u0302 the predictions, and S the protected group membership.\nOn each dataset, we train 50 instances of each ML algorithm in the study. For clarity, we will refer to different pairs of \u27e8unconstrained, fairness-aware\u27e9 algorithms as different algorithms (e.g., \u27e8GBM, EG\u27e9 and \u27e8NN, EG\u27e9 are two different algorithms). As we study 20 different ML algorithms, a total of 50 \u00b7 20 = 1000 ML models is trained on each dataset. Each model is trained with a different randomly-sampled selection of hyperparameters (e.g., learning rate of a GBM, number of trees of an RF, weight regularization of an LR). This fulfills two goals: first, to accurately explore the best outcomes of competing fair ML methods, as related work has shown that a wide range of fairness values can be obtained for the same ML algorithm by simply varying its hyperparameters; and, second, to indirectly benchmark against fairness-aware AutoML approaches, which attempt to train fair models by tuning the hyperparameters of unconstrained models (Cruz et al., 2021; Perrone et al., 2021; Weerts et al., 2023).\nUnprocessing. We define \u03c0r(f) as the process of postprocessing a predictor f to minimize some classification loss function \u2113 over the group-specific decision thresholds ts \u2208 R, subject to an r-relaxed equalized odds constraint (Equation 1),\nmax y\u2208{0,1}\n( P[Y\u0302 = 1|S = a, Y = y]\u2212 P[Y\u0302 = 1|S = b, Y = y] ) \u2264 r, \u2200a, b \u2208 S, (1)\nwhere the prediction for a sample of group s \u2208 S is given by Y\u0302 = 1{R\u0302 \u2265 ts}, and R\u0302 is its real-valued risk score. Thereby, unprocessing is defined as the unconstrained minimization of the loss \u2113; i.e., \u03c0\u221e(f), an \u221e-relaxed solution to equalized odds. As classifiers with different values of constraint violation are potentially incomparable between themselves, unprocessing emerges as a means to fairer comparisons between classifiers, unearthing the achievable unconstrained accuracy underlying a constrained predictor. For example, while classifiers A, B, and A\u0303 of Figure 2 are all Pareto-efficient (Pareto, 1919) (i.e., incomparable), we can fairly compare the accuracy of A with that of \u03c0\u221e(B) = B\u2217 (both are unconstrained classifiers), B with \u03c0v(B) = B\u0303, and A\u0303 with \u03c0r(B).\nThe following subsections will detail the datasets we use (Section 2.1), and the experimental procedure we employ to test our hypothesis (Section 2.2)."
        },
        {
            "heading": "2.1 DATASETS",
            "text": "We evaluate all methods on five large public benchmark datasets from the folktables Python package (Ding et al., 2021). These datasets are derived from the American Community Survey (ACS) public use microdata sample from 2018, containing a variety of demographic features (e.g., age, race, education). We also conduct a similar experiment on the MEPS dataset, shown in Appendix A.5.\nEach of the five ACS datasets is named after a specific prediction task: ACSIncome (1.6M rows) relates to household income prediction, ACSTravelTime (1.4M rows) relates to daily commute time prediction, ACSPublicCoverage (1.1M rows) relates to health insurance coverage prediction, ACSMobility (0.6M rows) relates to the prediction of address changes, and ACSEmployment (2.3M rows) relates to employment status prediction. ACSIncome arguably carries particular weight in the fair ML community, as it is a larger modern-day version of the popular UCI Adult dataset (49K\nrows) (Dua & Graff, 2017), which has been widely used for benchmarking algorithmic fairness methods over the years. We use race group membership as the protected attribute on all five datasets (RAC1P column); specifically, we use samples from the four largest groups: White, Black, Asian, and Other (some other race alone). Additional experiments using only samples from the two largest groups are presented as well, although not the focus of the paper results\u2019 analysis (see Appendix A.4).\nIn total, 11 000 models were trained and evaluated over a range of 11 different evaluation scenarios, pertaining to 6 datasets, with sizes ranging from 49K to 2.3M samples."
        },
        {
            "heading": "2.2 EXPERIMENTAL PROCEDURE",
            "text": "We conduct the following procedure for each dataset, with a 60%/20%/20% train/test/validation data split. First, we fit 1000 different ML models on the training data (50 per algorithm type). Second, to enable comparison of all models on an equal footing, we unprocess all 1000 trained models (on validation), and compute accuracy and equalized odds violation of the resulting classifiers. For any given classifier, its equalized odds violation is given by the left-hand side of the inequality in Equation 1 (or the smallest slack r that fulfills the inequality). Then, we select the model with highest unprocessed accuracy, m\u2217 = \u03c0\u221e(m\u2032), obtained by the unconstrained postprocessing of the model m\u2032. We defer the formal definition and procedure for solving the relaxed problem to Section 4.\nWe solve the r-relaxed equalized odds postprocessing on validation, \u03c0r(m\u2032), for all values of constraint violation, r \u2208 [0, c(m\u2217)] (with discrete intervals of 0.01), where c(m\u2217) is the constraint violation of m\u2217. Finally, we compute accuracy and equalized odds violation on the withheld test dataset for all original 1000 models, and all post-processed versions of m\u2032, \u03c0r(m\u2032). All in all, even though the selection process for m\u2217 and m\u2032 entirely disregarded fairness, we expect \u03c0r(m\u2032) to be the classifier with highest accuracy at all levels of fairness r \u2208 [0.0, 1.0] \u2014 as illustrated in Figure 2. It may happen that the classifier m\u2217 with highest unprocessed accuracy is based on a pre- or inprocessing fairness method m\u2032. Crucially, this implies that the means by which this fairness method resulted in a fairer classifier was by finding more accurate risk scores in the first place. Otherwise, unprocessing fairness-constrained models would not result in accurate unconstrained predictions.\nFigure 3 shows an example of the general effect of unprocessing on the ACSIncome dataset (test results). Example models A and B are chosen respectively to maximize accuracy (A) and to maximize an average of accuracy and fairness (B) on validation data. On the left plot (original, unaltered models), unconstrained (\u25cf markers) and constrained models (\u271a markers) are incomparable in terms of Pareto dominance: the first is generally more accurate than the latter, but the ranking is reversed for constraint violation. However, after unprocessing (middle plot), we can see that the unprocessed unconstrained model A achieves higher accuracy than the unprocessed constrained model B, indicating that postprocessing the first would be Pareto dominant or match the latter. Finally,\nthe right plot confirms this hypothesis as evident by comparing postprocessing curves for both models. More extensive experimental results are shown in Section 3."
        },
        {
            "heading": "3 RESULTS ON AMERICAN COMMUNITY SURVEY DATA",
            "text": "In this section we will present and discuss the results of experiments on all five ACS datasets. These experiments entail a total of 1 000 models trained per dataset. Due to space constraints, plots are shown only for the ACSIncome and ACSPublicCoverage datasets. Corresponding plots for the remaining datasets are shown in Appendix A. Results for a counterpart experiment using only two sensitive groups are also explored in this section, and further detailed in Appendix A.4."
        },
        {
            "heading": "3.1 COMPARISON BETWEEN FAIRNESS METHODS",
            "text": "We first analyze how each pre- or inprocessing fairness method compares with each other, without the effects of postprocessing. Figure 4 shows the Pareto frontiers achieved by each method when using GBM base models (see also Figure A1 of the appendix). Overall, preprocessing methods (LFR and CR) achieved lacklustre fairness-accuracy trade-offs across all datasets, while the EG and FairGBM inprocessing methods performed best (highest area above Pareto frontiers). Specifically, LFR is indeed able to achieve high fairness fulfillment, but at a steep accuracy cost. To clarify: the plotted colored Pareto frontiers correspond to multiple (up to 50) different underlying base models, while the black dashed line corresponds to the postprocessings of the single GBM-based model with highest accuracy. The following subsection contains a more detailed analysis of postprocessing.\nInterestingly, some fairness methods were able to achieve higher test accuracy than unconstrained GBM models, suggesting improved generalization performance. Figure 5 shows one potential reason: fairness methods can take notoriously high compute resources to train, potentially giving them a compute advantage with respect to their unconstrained counterparts. Recent related work has put forth other explanations for why fairness-constraining can improve learning. Wei et al. (2023) find that fairness can improve overall performance under label noise by improving learning on tail sub-populations. On the other hand, Creager et al. (2021) establish ties between common fairness constraints and goals from the robustness literature, suggesting that fairness-aware learning can improve generalization under distribution shifts."
        },
        {
            "heading": "3.2 POSTPROCESSING VS OTHER METHODS",
            "text": "Figure 6 shows test-set results for the experimental procedure detailed in Section 2.2, zoomed on the region of interest (high accuracy and low constraint violation). The model with highest unprocessed validation accuracy, m\u2217, is shown with a larger marker, while all other markers correspond to unaltered non-postprocessed models. The Pareto frontier achieved by postprocessing m\u2217 is shown with a black dash-dot line, as well as corresponding 95% confidence intervals computed using bootstrapping (Efron & Tibshirani, 1994). Figures A2\u2013A6 show a wider view of the same underlying data, as some algorithms fail to show-up in the zoomed region of interest. Figures A8\u2013A12 show the result of conducting the same experiment but using only GBM-based models, leading to identical trends.\nAll datasets show a wide spread of models throughout the fairness-accuracy space, although to varying levels of maximum accuracy (from 0.713 on ACSTravelTime to 0.831 on ACSEmployment). Unconstrained models (circles) can generally be seen to form a cluster of high accuracy and low fairness (high constraint violation). Neither LFR nor LR-based methods manage to produce any model within the region plotted in Figure 6. On the ACSIncome and ACSPublicCoverage datasets, the m\u2217 model corresponds to an unconstrained GBM (blue circle), on ACSTravelTime and ACSEmployment m\u2217 is of type \u27e8GBM, CR\u27e9 (blue diamond), and on ACSMobility m\u2217 is of type \u27e8GBM, GS\u27e9 (blue cross). Models m\u2217 are GBM-based across all datasets, contributing to a wide body of literature reporting that GBM models are highly performant on tabular datasets (Shwartz-Ziv & Armon, 2022).\nCrucially, postprocessing the single most accurate model resulted in the fair optima for all values of fairness constraint violation on all datasets, either dominating or matching other contender models (within 95% confidence intervals). That is, all optimal trade-offs between fairness and accuracy can be retrieved by applying different group-specific thresholds to the same underlying risk scores.\nFinally, Figure 7 shows results for a similar experiment where fairness constraints were learned only on the two largest sub-groups (White and Black). This leads to an arguably easier problem to solve,\nwhich is reflected on the general compression of models on the vertical axis (reduced constraint violation for all models). While previously the maximum unprocessed accuracy on ACSIncome was achieved at 0.38 constraint violation, on this binary-group setting it is achieved at 0.16 constraint violation. Nonetheless, the same trend is visible on all studied datasets. Unconstrained models \u2014 either trained in an unconstrained manner (circles) or made unconstrained via unprocessing (m\u2217) \u2014 occupy regions of high accuracy and low fairness (high constraint violation). However, the Pareto frontier that results from postprocessing m\u2217 (the best-performing unconstrained model) to different levels of fairness relaxation again dominates or matches the remaining fairness methods.\nAll in all, postprocessing provides a full view of the Pareto frontier derived from a single predictor m\u2217. Regardless of fairness violation, when this predictor is near-optimal \u2014 potentially achievable on tabular data by training a variety of algorithms \u2014 so will its postprocessed Pareto frontier be."
        },
        {
            "heading": "4 ACHIEVING RELAXED ERROR RATE PARITY",
            "text": "Error rate parity, also known as equalized odds, enforces equal false positive rate (FPR) and equal true positive rate (TPR) between different protected groups (Hardt et al., 2016). This can be formalized as a constraint on the joint distribution of (Y , Y\u0302 , S):\nP[Y\u0302 = 1|S = a, Y = y] = P[Y\u0302 = 1|S = b, Y = y], \u2200y \u2208 {0, 1}, \u2200a, b \u2208 S, (2)\nwhere a \u0338= b references two distinct groups in the set of all possible groups S. Fulfilling the strict equalized odds constraint greatly simplifies the optimization problem of finding the optimal classifier through postprocessing, as the constrained optimum must be at the intersection of the convex hulls of each group-specific ROC curve. As such, we\u2019re left with a linear optimization problem on a single 2-dimensional variable, \u03b3 = (\u03b30, \u03b31):\nmin \u03b3\u2208D\n\u03b30 \u00b7 \u2113(1, 0) \u00b7 p0 + (1\u2212 \u03b31) \u00b7 \u2113(0, 1) \u00b7 p1, (3)\nwhere \u03b30 is the global FPR, \u03b31 the global TPR, D \u2282 [0, 1]2 the optimization domain, py = P[Y = y] the prevalence of label Y = y, and \u2113(y\u0302, y) the loss incurred for predicting y\u0302 when the correct class was y (we assume w.l.o.g. \u2113(0, 0) = \u2113(1, 1) = 0). Strict equalized odds fulfillment collapses the optimization domain D into a single convex polygon that results from intersecting all groupspecific ROC hulls; i.e., D = \u22c2 s\u2208S Ds, where Ds is the convex hull of the ROC curve for group s. Specifically, Ds = convexhull {Cs(t) : t \u2208 R}, and Cs defines the ROC curve for group s as:\nCs(t) = ( P [ R\u0302 \u2265 t|S = s, Y = 0 ] ,P [ R\u0302 \u2265 t|S = s, Y = 1 ]) , (4)\nwhere t \u2208 R is a group-specific decision threshold, and R\u0302 is the predictor\u2019s real-valued score.\nIn this section, we detail the solution to the r-relaxed equalized odds constraint defined in Equation 1. In order to relax the equalized odds constraint, we introduce a slack variable, \u03b4(a,b) \u2208 [0, 1]2, for each pair a, b \u2208 S:\nP[Y\u0302 = 1|S = a, Y = y]\u2212 P[Y\u0302 = 1|S = b, Y = y] = \u03b4(a,b)y \u2264 r, \u2200y \u2208 {0, 1} , (5)\nwhere r \u2208 [0, 1] is the maximum allowed constraint violation.\nWe introduce variables \u03b3(s) = (\u03b3(s)0 , \u03b3 (s) 1 ) \u2208 Ds, s \u2208 S, as the points of group-specific FPR, \u03b3 (s) 0 , and group-specific TPR, \u03b3(s)1 . Equation 5 can then be equivalently stated as:\u2225\u2225\u2225\u03b3(a) \u2212 \u03b3(b)\u2225\u2225\u2225 \u221e = \u2225\u2225\u2225\u03b4(a,b)\u2225\u2225\u2225 \u221e \u2264 r. (6)\nThe global ROC point, \u03b3, is defined as: \u03b30 = \u2211 s\u2208S \u03b3 (s) 0 \u00b7 ps|0, \u03b31 = \u2211 s\u2208S \u03b3 (s) 1 \u00b7 ps|1, (7)\nwhere ps|y = P[S = s|Y = y] is the relative size of group s within the set of samples with label Y = y. Importantly, the global point \u03b3 is not limited to the intersection of group-specific ROC hulls. Each group-specific ROC point is naturally limited to be inside its group-specific ROC hull, \u03b3(s) \u2208 Ds, and \u03b3 is only limited by its definition as a function of all \u03b3(s), s \u2208 S, as per Equation 7. Finally, finding the r-relaxed optimum boils down to minimizing the linear objective function defined in Equation 3, with domain D = \u22c3 s\u2208S Ds, subject to affine constraints defined in Equations 6\u20137. This optimization problem amounts to a linear program (LP), for which there is a variety of efficient open-source solvers (Diamond & Boyd, 2016). We contribute a solution in an open-source package.1\nFigure 8 shows an example of optimal strict and 0.05-relaxed solutions for equalized odds. Strict fulfillment of the equalized odds constraint (red star) reduces the feasible space of solutions to the intersection of all group-specific ROC hulls. This fact potentially restricts all but one group to sub-optimal accuracy, achieved by randomizing some portion of the classifier\u2019s predictions. On the other hand, if we allow for some relaxation of the constraint, each group\u2019s ROC point will lie closer to its optimum. In this example, the optimal solution to an r = 0.05 relaxation no longer needs to resort to randomization, placing each group\u2019s ROC point on the frontier of its ROC convex hull."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "We revisit the simple postprocessing method in a comprehensive empirical evaluation spanning 6 distinct datasets, 11 evaluation tasks, and more than 11 000 trained models. We find that, in all cases, any Pareto-optimal trade-off between accuracy and error rate parity can be achieved by postprocessing the model with highest accuracy. Along the way, we address two serious confounding factors that have impaired previous comparisons of fairness methods. We hope that our study helps strengthen evaluation standards in algorithmic fairness."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "We\u2019re indebted to Noam Barda, Noa Dagan, and Guy Rothblum for insightful and stimulating discussions about the project. We thank Florian Dorner, Olawale Salaudeen, and Vivian Nastl for invaluable feedback on an earlier version of this paper. Lastly, we thank the four anonymous reviewers for their fruitful suggestions, and the area chair for important and enriching references to related work on the undue over-complexification of ML methods.\nThe authors thank the International Max Planck Research School for Intelligent Systems (IMPRS-IS) for supporting Andr\u00e9 F. Cruz."
        },
        {
            "heading": "REPRODUCIBILITY STATEMENT",
            "text": "We\u2019ve made significant efforts to ease reproducibility of our experiments. All source code has been open-sourced, including open-sourcing a Python package1 to postprocess any score-based classifier to a given level of fairness-constraint relaxation, code to run experiments using the aforementioned package (folder scripts of the supplementary materials2), and code to generate the paper plots (folder notebooks of the supplementary materials2). Detailed hyperparameter search spaces for each algorithm are included in folder hyperparameters_spaces of the supplementary materials.2 Furthermore, we are releasing detailed experimental results for all trained models in a series of csv files (under folder results of the supplementary materials2), including a variety of performance and fairness metrics, as well as their values at 2.5 and 97.5 bootstrapping percentiles. Appendix B details the infrastructure used to run all jobs, as well as total compute usage."
        },
        {
            "heading": "A ADDITIONAL EXPERIMENTAL RESULTS",
            "text": "The main body of the paper discusses results on all five ACS datasets. However, due to space constraints, plots are only shown for two example datasets: ACSIncome and ACSPublicCoverage. Appendices A.1\u2013A.3 show analogous versions of each previous plot for the remaining three ACS datasets: ACSTravelTime, ACSMobility, and ACSEmployment. Plots are shown in the same order as in the main paper body. Additionally, we run a similar experiment on ACS datasets using only the two largest sensitive groups (White and Black), shown in Appendix A.4. Appendix A.5 presents results on the MEPS dataset (Blewett et al., 2021), an entirely different data source corresponding to real-world surveys of healthcare usage across the United States. Appendix A.6 provides further evidence that model ranking is maintained throughout all levels of constraint violation, and Appendix A.7 compares the results of unconstrained model training to unprocessing constrained models.\nWe consciously refrain from evaluating on the popular COMPAS dataset (Angwin et al., 2016), as related work has surfaced severe data gathering issues, including measurement biases and label leakage (Bao et al., 2021; Barenstein, 2019; Fabris et al., 2022). The German Credit dataset (Dua & Graff, 2017) \u2014 another popular benchmark in the fairness literature \u2014 suffers from its small size (1 000 samples), the age of its data (dates back to 1973\u20131975), and encoding issues that make it impossible to retrieve accurate sensitive information such as the individual\u2019s sex (Gr\u00f6mping, 2019). Overall, a total of 11 different evaluation scenarios were studied, pertaining to 6 datasets, with sizes ranging from 49K to 2.3M samples. Confidence intervals and metric results are computed using bootstrapping on the respective evaluation dataset (Efron & Tibshirani, 1994). We hope the scale of our study suffices to convince the reader of the validity of our claims. Source code is made available to easily reproduce our setup on other datasets.2 All appendix experiments are in accordance with the main findings presented in Section 3."
        },
        {
            "heading": "A.1 COMPARISON BETWEEN FAIRNESS METHODS",
            "text": "Figure A1 shows Pareto frontiers for all studied GBM-based algorithms. We observe a similar trend to that seen in Figure 4: preprocessing fairness methods can increase fairness but at dramatic accuracy costs, while EG and FairGBM inprocessing fairness methods trade Pareto-dominance between each other. Postprocessing Pareto frontier is also shown for reference, but a more detailed comparison between postprocessing and all other contender models is shown in the following section."
        },
        {
            "heading": "A.2 POSTPROCESSING VS OTHER METHODS",
            "text": "Figures A2\u2013A6 show complete views of the Pareto frontiers obtained by postprocessing the model with highest validation accuracy m\u2217 on each dataset (potentially obtained by unprocessing a fairnessaware model), together with a scatter of all other competing preprocessing, inprocessing, or unconstrained models (1 000 in total per dataset). Figure A7 shows detailed postprocessing results on each dataset, zoomed on the region of interest (maximal accuracy and minimal constraint violation, i.e., bottom right portion of the plot). Figures A8\u2013A12 show results using only a subset of models: only GBM-based models. The main paper hypothesis is confirmed on each and every plot: we can obtain optimally fair classifiers at any level of constraint violation by postprocessing the model with highest accuracy, m\u2217, irrespective of its constraint violation.\n2Supplementary materials: https://github.com/socialfoundations/error-parity/tree/supp-materials\n0.60 0.65 0.70 accuracy\n0.0\n0.1\n0.2\n0.3\nco ns\ntra in\nt v io\nla tio\nn\nACSTravelTime (test)\n0.74 0.75 0.76 0.77 accuracy\n0.00\n0.05\n0.10\n0.15\n0.20\nco ns\ntra in\nt v io\nla tio\nn\nACSMobility (test) GBM GBM, CR GBM, EG GBM, GS GBM, LFR FairGBM postprocessing\n0.6 0.7 0.8 accuracy\n0.00\n0.05\n0.10\n0.15\n0.20\nco ns\ntra in\nt v io\nla tio\nn\nACSEmployment (test) GBM GBM, CR GBM, EG GBM, GS GBM, LFR FairGBM postprocessing\nFigure A1: Pareto frontier attainable by each GBM-based ML algorithm, together with the Pareto frontier attained by postprocessing m\u2217, the GBM-based model with highest unprocessed validation accuracy. Plotted Pareto curves are linearly interpolated between Pareto-efficient models.\n0.625 0.650 0.675 0.700 0.725 0.750 0.775 0.800 0.825 accuracy\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nco ns\ntra in\nt v io\nla tio\nn\nACSIncome (test)\nbase model GBM LR NN RF\nfairness method None CR EG GS\nLFR FairGBM m * (accuracy=0.821) postprocessing of m * 95% confidence interv.\nm *\nco ns\nta nt\np re\ndi ct\nor a\ncc .\nFigure A2: Fairness and accuracy test results for all 1 000 trained ML models (50 of each type) on the ACSIncome dataset. Colors portray different underlying unconstrained models and markers portray different fairness methods (or no fairness method for circle markers). The unconstrained model with highest validation accuracy, m\u2217, is shown with a larger marker, and the Pareto frontier attainable by postprocessing m\u2217 is shown as a black dash-dot line, together with its 95% confidence intervals in shade. This is a colored and more granular version of Figure 1.\n0.72 0.74 0.76 0.78 0.80 accuracy\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5 0.6 co ns tra in t v io la tio n\nACSPublicCoverage (test)\nbase model GBM LR NN RF\nfairness method None CR EG GS\nLFR FairGBM m * (accuracy=0.808) postprocessing of m * 95% confidence interv.\nm * co ns ta nt p re di\nct or\na cc\n.\nFigure A3: Fairness and accuracy test results on the ACSPublicCoverage dataset. Model m\u2217 is of type \u27e8GBM\u27e9 and achieves 0.808 accuracy. See legend and caption of Figure A2 for more details.\n0.56 0.58 0.60 0.62 0.64 0.66 0.68 0.70 0.72 accuracy\n0.0\n0.1\n0.2\n0.3\nco ns\ntra in\nt v io\nla tio\nn\nACSTravelTime (test)\nbase model GBM LR NN RF\nfairness method None CR EG GS\nLFR FairGBM m * (accuracy=0.713) postprocessing of m * 95% confidence interv.\nm *\nco ns\nta nt\np re\ndi ct\nor a\ncc .\nFigure A4: Fairness and accuracy test results on the ACSTravelTime dataset. Model m\u2217 is of type \u27e8GBM, CR\u27e9 and achieves 0.713 accuracy. See legend and caption of Figure A2 for more details.\n0.735 0.740 0.745 0.750 0.755 0.760 0.765 0.770 accuracy\n0.00\n0.05\n0.10\n0.15\n0.20\nco ns\ntra in\nt v io\nla tio\nn\nACSMobility (test)\nbase model GBM LR NN RF\nfairness method None CR EG GS\nLFR FairGBM m * (accuracy=0.770) postprocessing of m * 95% confidence interv.\nm *\nco ns\nta nt\np re\ndi ct\nor a\ncc .\nFigure A5: Fairness and accuracy test results on the ACSMobility d taset. Model m\u2217 is of type \u27e8GBM, GS\u27e9 and achieves 0.770 accuracy. See legend and caption of Figure A2 for more details.\n0.55 0.60 0.65 0.70 0.75 0.80 accuracy\n0.00\n0.05\n0.10\n0.15 0.20 co ns tra in t v io la tio n\nACSEmployment (test)\nbase model GBM LR NN RF\nfairness method None CR EG GS\nLFR FairGBM m * (accuracy=0.831) postprocessing of m * 95% confidence interv.\nm * co ns\nta nt\np re\ndi ct\nor a\ncc .\nFigure A6: Fairness and accuracy test results n the ACSEmployment dataset. Model m\u2217 is of type \u27e8GBM, CR\u27e9 and achieves 0.831 accuracy. See legend and caption of Figure A2 for more details.\n0.66 0.68 0.70 0.72 accuracy\n0.00\n0.05\n0.10\n0.15\n0.20\nco ns\ntra in\nt v io\nla tio\nn\nACSTravelTime (test)\n0.760 0.765 0.770 accuracy\n0.00\n0.05\n0.10\n0.15\n0.20\nco ns\ntra in\nt v io\nla tio\nn\nACSMobility (test) base model GBM NN RF fairness method None CR EG GS FairGBM m * (selected on val.) postprocessing of m * 95% confidence interv.\n0.8225 0.8250 0.8275 0.8300 0.8325 accuracy\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\nco ns\ntra in\nt v io\nla tio\nn\nACSEmployment (test) base model GBM NN RF fairness method None CR EG GS LFR FairGBM m * (selected on val.) postprocessing of m * 95% confidence interv.\nFigure A7: Detailed view of the postprocessing Pareto frontier on the ACSTravelTime (left), ACSMobility (right), and ACSEmployment (bottom) datasets. Respectively corresponds to zoomed-in versions of Figures A4 (left), A5 (right), and A6 (bottom).\n0.625 0.650 0.675 0.700 0.725 0.750 0.775 0.800 0.825 accuracy\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nco ns\ntra in\nt v io\nla tio\nn\nACSIncome (test)\nGBM GBM, CR GBM, EG\nGBM, GS GBM, LFR FairGBM\nm * (accuracy=0.821) postprocessing of m * dominated by m *\nm *\nco ns\nta nt\np re\ndi ct\nor a\ncc .\nFigure A8: Fairness and accuracy test results for 300 GBM-based ML models (50 of each algorithm type) on the ACSIncome dataset. The unconstrained model with highest validation accuracy, m\u2217, is shown with a larger marker, and the Pareto frontier attainable by postprocessing m\u2217 is shown as a black dash-dot line, together with its 95% confidence intervals in shade.\n0.72 0.74 0.76 0.78 0.80 accuracy\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nco ns\ntra in\nt v io\nla tio\nn\nACSPublicCoverage (test)\nGBM GBM, CR GBM, EG\nGBM, GS GBM, LFR FairGBM\nm * (accuracy=0.808) postprocessing of m * dominated by m *\nm *\nco ns\nta nt\np re\ndi ct\nor a\ncc .\nFigure A9: Fairness and accuracy test results for GBM-based ML models on the ACSPublicCoverage dataset. Model m\u2217 is of type \u27e8GBM\u27e9 and achieves 0.808 accuracy. See legend and caption of Figure A8 for more details.\n0.56 0.58 0.60 0.62 0.64 0.66 0.68 0.70 0.72 accuracy\n0.0\n0.1\n0.2\n0.3\nco ns\ntra in\nt v io\nla tio\nn\nACSTravelTime (test)\nGBM GBM, CR GBM, EG\nGBM, GS GBM, LFR FairGBM\nm * (accuracy=0.713) postprocessing of m * dominated by m *\nm *\nco ns\nta nt\np re\ndi ct\nor a\ncc .\nFigure A10: Fairness and accuracy test results for GBM-based ML models on the ACSTravelTime dataset. Model m\u2217 is of type \u27e8 BM,CR\u27e9 and achieves 0.713 accuracy. See legend and caption of Figure A8 for more details.\nGBM, CR\nGBM, EG\nGBM, LFR\nFairGBM\npostprocessing of m *\ndominated by m *\nGBM, CR\nGBM, EG"
        },
        {
            "heading": "GBM, GS GBM, LFR",
            "text": "FairGBM\npostprocessing of m *\ndominated by m *"
        },
        {
            "heading": "A.3 TIME TO FIT EACH METHOD",
            "text": "Figure A13 shows the mean time to fit each GBM-based model on three separate datasets. The trend is clear on all studied datasets: postprocessing is a small increment to the time taken to fit the base model, preprocessing methods take longer but are still within the same order of magnitude, the FairGBM inprocessing method also incurs a relatively small increment to the base model time, while EG and GS take one to two orders of magnitude longer to fit.\nFor clarification, all times listed are end-to-end process times for fitting and evaluating a given model. For example, postprocessing times include the time taken to fit the base GBM model plus the time taken to solve the LP. We note that most time consumed for postprocessing simply corresponds to computing the model scores for the respective dataset where postprocessing will be fitted, while solving the LP usually takes only a few seconds. Likewise, preprocessing fairness methods include the time taken to fit the preprocessing method, the time taken to transform the input data, and the time to fit the base model. Finally, inprocessing fairness methods include only the time taken to fit the inprocessing method, as no preprocessing or postprocessing steps are required. Nonetheless, the GS and EG inprocessing methods take significantly longer than any other competing method."
        },
        {
            "heading": "A.4 EXPERIMENTS WITH BINARY SENSITIVE GROUPS",
            "text": "While compatibility with more than two sensitive groups is arguably essential for real-world applicability of a fairness intervention, it is common among the fair ML literature to propose and evaluate methods considering only two groups (Zemel et al., 2013; Agarwal et al., 2018; Cruz et al., 2023). In this binary-group setting, constrained optimization methods only have to consider two constraints:\u2223\u2223\u2223P [Y\u0302 = 1|S = 0, Y = 0]\u2212 P [Y\u0302 = 1|S = 1, Y = 0]\u2223\u2223\u2223 \u2264 \u03f5, \u25b7 FPR constraint\u2223\u2223\u2223P [Y\u0302 = 1|S = 0, Y = 1]\u2212 P [Y\u0302 = 1|S = 1, Y = 1]\u2223\u2223\u2223 \u2264 \u03f5, \u25b7 TPR constraint respectively, a constraint on group-specific FPR, and another on group-specific TPR, with some small \u03f5 slack. By relaxing the equalized odds problem to only two constraints we expect to provide fairness-constrained methods with the best chance at disproving the paper hypothesis.\nFigure A14 (as Figure 7) shows results of applying the experimental procedure detailed in Section 2.2 to a sub-sample of the ACS datasets: only samples from the two largest sensitive groups are used (White and Black). We observe substantially lower constraint violation across the board, both for unconstrained and fairness-aware models. In fact, even unconstrained unprocessed models (m\u2217 on each plot) achieve below 0.1 constraint violation on 4 datasets when using binary groups (all but ACSIncome), and below 0.01 on 2 datasets (ACSMobility and ACSEmployment, see Figure A14). These results arguably discourage the use of binary sensitive groups on the ACSMobility and ACSEmployment datasets for fairness benchmarking, as very low disparities are effortlessly achieved."
        },
        {
            "heading": "A.5 RESULTS ON THE MEPS DATASET",
            "text": "The Medical Expenditure Panel Survey (MEPS) (Blewett et al., 2021) dataset consists of large-scale surveys of families and individuals across the United States, together with their medical providers and employees. MEPS collects data on the health services used, costs and frequency of services, as well as demographic information of the respondents. The goal is to predict low (< 10) or high (\u2265 10) medical services utilization. Utilization is defined as the yearly sum total of office-based visits, hospital outpatient visits, hospital emergency room visits, hospital inpatient stays, or home health care visits. Exact data pre-processing is made available in the supplementary materials.2 We use survey panels 19 and 20 for training and validation (data is shuffled and split 70%/30%) \u2014 collected in 2015 and beginning of 2016 \u2014 and survey panel 21 for testing \u2014 collected in 2016. In total, the MEPS dataset consists of 49075 samples, 23380 of which are used for training, 10020 for validation, and 15675 for testing, making it over one order of magnitude smaller than the smallest ACS dataset in our study. We use race as the sensitive attribute, with 3 non-overlapping groups as determined by the panel data: Hispanic, Non-Hispanic White, and Non-White.\nFigure A15 shows results of conducting the experiment detailed in Section 2.2 on the MEPS dataset. We note that the variance of results is the largest among all studied datasets, as evidenced by the wide confidence intervals. This is most likely due to the small dataset size. It is also possible that the m\u2217 model on smaller datasets (such as MEPS) could produce scores that are farther from Bayes optimality than those of m\u2217 on larger datasets (such as ACS). We hope that our study motivates additional empirical work on when exactly the optimality of postprocessing breaks in practice. We recall that, although no counter-example was observed among 11 000 trained models, there are known edge-cases where postprocessing is sub-optimal (Woodworth et al., 2017). Overall, empirical results on the MEPS dataset are in accordance with those observed on the ACS datasets: the most accurate unconstrained model can be postprocessed to match or dominate any other fairness-aware model."
        },
        {
            "heading": "A.6 RANKING PRESERVATION BETWEEN UNPROCESSED AND POSTPROCESSED VERSIONS",
            "text": "Figure A16 \u2014 akin to Figure 3 \u2014 shows real-data examples of the unprocessing-postprocessing experimental setup described in Section 2. The three plot panels show: (left) original results, (middle) results after unprocessing all models, and (right) original results with postprocessing curves overlaid.\nWe recall that the main experimental results (in Section 3) show that postprocessing the model with highest accuracy Pareto-dominates all other models (both fairness-aware and standard models). In this section, we present another perspective on the same empirical insight: given two specific incomparable models (A and B), the postprocessing curve of the model with highest unprocessed accuracy will Pareto-dominate the postprocessing curve of the model with lower unprocessed accuracy. That is, while Figure 6 compares postprocessing to all other fairness interventions, Figure A16 compares postprocessing to postprocessing. In this scenario, the same empirical insight is confirmed: taking the model with highest accuracy is superior at all levels of fairness constraint violation.\nIn summary, when near Bayes optimality,3 model rankings are maintained across all postprocessing relaxations, i.e., if A\u2217 \u2ab0 B\u2217, then \u03c0r(A) \u2ab0 \u03c0r(B),\u2200r \u2208 [0, 1]. We know this to be true on both extremes (r = 0\u2228r = 1) for a Bayes optimal model (Hardt et al., 2016): it achieves optimal accuracy, and its postprocessing achieves optimal fairness-constrained accuracy. At the same time, we know this to be false on some carefully constructed counter-examples (Woodworth et al., 2017). The focus of the present work is to study whether this ranking is generally maintained in practice, on real-world data. This hypothesis is confirmed on all experiments conducted throughout the paper.\n3We only compare models that are Pareto-dominant among their algorithm cohort.\nOne final noteworthy point is that unconstrained models are not significantly affected by unprocessing, occupying approximately the same fairness-accuracy region before and after optimization over groupspecific thresholds (e.g., compare A with A\u2217 in Figure A16). This is expected, as unconstrained learning optimizes for calibration by group (Liu et al., 2019), P [Y = 1|R = r, S = s] = r, \u2200s \u2208 S, which leads to the same loss-minimizing threshold for all groups (further details in Appendix C)."
        },
        {
            "heading": "A.7 UNPROCESSING VS UNCONSTRAINED LEARNING",
            "text": "As per Section 3, the best performing inprocessing fairness interventions are EG and FairGBM (i.e., highest Pareto-dominated area). In this section, we assess how unconstrained learning compares to unprocessing a model that was trained using either of these fairness interventions. Ideally, if enforcing the fairness constraint in-training did not hinder the learning process, we\u2019d expect unprocessed models to approximately occupy the same fairness-accuracy region as unconstrained models.\nFigure A17 shows results before and after unprocessing fairness-constrained models on the ACSIncome dataset. Unprocessing is done on validation data, and results are shown on withheld test data. The plots show that, after unprocessing, fairness-constrained models are naturally brought to similar levels of constraint violation as unconstrained models. While overlap between unconstrained and fairness-constrained models was previously minimal or non-existent (left plots), these models form clearly overlapping clusters after unprocessing (right plots). Figure A18 shows similar results before and after unprocessing fairness-constrained models, as well as results after postprocessing unconstrained models. As evident in the plots, unprocessing brings fairness-constrained models to the high-accuracy and high-disparity region that was previously occupied solely by unconstrained models; while postprocessing brings unconstrained models to the low-disparity region previously occupied solely by fairness-constrained models. This motivates the naming of unprocessing, as it can be seen as the inverse mapping of postprocessing. With these plots we aim to bring attention to the interchangeability of the underlying scores produced by both unconstrained and constrained models. Whether we want to deploy a fairness-constrained or an unconstrained classifier can be chosen after model training, by postprocessing a high-performing model to the appropriate value of fairness-constraint fulfillment. Finally, postprocessing has the added advantage of better-tuned fairness-constraint fulfillment, as models that were trained in a fairness-constrained manner suffer from a wide variability of constraint fulfillment (orange markers of left-most plots)."
        },
        {
            "heading": "B EXPERIMENT RUN DETAILS",
            "text": "All experiments were ran as jobs submitted to a centralized cluster, running the open-source HTCondor scheduler. Each job was given the same computing resources: 1 CPU. Compute nodes use AMD EPYC 7662 64-core CPUs. No GPUs were used. Memory was allocated as required for each algorithm: all jobs were allocated at least 16GB of RAM; GS and EG jobs were allocated 64GB of RAM as these ensembling algorithms have increased memory requirements.\nAn experiment job accounts for training and evaluating a single model on a given dataset. That is, 1 000 models were trained on each dataset (50 per algorithm type), totaling 11 000 models trained: 5 000 for the main ACS experiment using 4 sensitive groups, 5 000 for the ACS experiment using 2 sensitive groups, and 1 000 for the MEPS dataset experiment. Overall, the median job finished in 10.3 minutes, while the average job lasted for 112.0 minutes (most models are fast, but some fairnessaware models such as EG take a long time to fit, as seen in Figures 5 and A13). Compute usage was: 10 528 CPU hours for the main 4-group ACS experiment, 9 967 CPU hours for the binary group ACS experiment (Appendix A.4), and 31 CPU hours for the MEPS dataset experiment (Appendix A.5). Total compute usage was 20 526 CPU hours, which amounts to 14 days on a 64-core node. Detailed per-job CPU usage is available under folder results of the supplementary materials.2\nComplete code base required to replicate experiments is provided as part of the supplementary materials, together with exact evaluation results for each trained model.2"
        },
        {
            "heading": "C THRESHOLDING GROUP-CALIBRATED PREDICTORS",
            "text": "In this section we provide a proof for the following statement: for any classifier with group-calibrated scores (Equations 8\u20139), the group-specific decision thresholds that minimize the classification loss among each group all take the same value, ta = tb,\u2200a, b \u2208 S, which is fully determined by the loss function, ts = \u2113(1,0) \u2113(1,0)+\u2113(0,1) ,\u2200s \u2208 S.\nProof. Given a joint distribution over features, labels, and sensitive attributes (X,Y, S), a binary classification loss function \u2113 : {0, 1}2 \u2192 R+, predictive scores R = f(X), and binary predictions Y\u0302 = 1 {R \u2265 t} , t \u2208 T \u2286 R. Assume the scores R are group-calibrated (Barocas et al., 2019), i.e.:\nP [Y = 1|R = r, S = s] = r, \u2200r \u2208 [0, 1], \u2200s \u2208 S, (8) P [Y = 0|R = r, S = s] = 1\u2212 r, \u2200r \u2208 [0, 1], \u2200s \u2208 S. (9)\nWe want to minimize the expected loss among samples of group s, Ls(t) = E [ \u2113(Y\u0302 , Y )|S = s ] :\nLs(t) = \u2113(1, 0) \u00b7 P [ Y\u0302 = 1, Y = 0|S = s ] + \u2113(0, 1) \u00b7 P [ Y\u0302 = 0, Y = 1|S = s ] , (10)\nassuming w.l.o.g. no cost for correct predictions \u2113(0, 0) = \u2113(1, 1) = 0.\nWe have: P [ Y\u0302 = 1, Y = 0|S = s ] = P [ Y\u0302 = 1|Y = 0, S = s ] \u00b7 P [Y = 0|S = s] = hFPs (t) \u00b7 P [Y = 0|S = s] ,\nP [ Y\u0302 = 0, Y = 1|S = s ] = P [ Y\u0302 = 0|Y = 1, S = s ] \u00b7 P [Y = 1|S = s] = hFNs (t) \u00b7 P [Y = 1|S = s] ,\nwhere hFPs (t) and h FN s (t) are, respectively, the False Positive Rate (FPR) and the False Negative Rate (FNR) among samples of group s, as functions of the chosen group-specific threshold t. We can trade-off FPR and FNR by varying the threshold, leading to a 2-dimensional curve known as the Receiver Operating Characteristic (ROC) curve.\nFurthermore, given the conditional density function of R given S = s, pR|s(r), we have: hFPs (t) = P [ Y\u0302 = 1|Y = 0, S = s ] = P [R \u2265 t|Y = 0, S = s]\n= P [Y = 0|R \u2265 t, S = s] \u00b7 P [R \u2265 t|S = s]\nP [Y = 0|S = s]\n= \u222b 1 t (1\u2212 r) \u00b7 pR|s(r) P [Y = 0|S = s] dr, \u25b7 using calibration (Eq. 9)\n\u2202hFPs \u2202t = (t\u2212 1) \u00b7 pR|s(t) P [Y = 0|S = s] ,\nand, hFNs (t) = P [ Y\u0302 = 0|Y = 1, S = s ] = P [R < t|Y = 1, S = s]\n= P [Y = 1|R < t, S = s] \u00b7 P [R < t|S = s]\nP [Y = 1|S = s]\n= \u222b t 0 r \u00b7 pR|s(r) P [Y = 1|S = s] dr, \u25b7 using calibration (Eq. 8)\n\u2202hFNs \u2202t = t \u00b7 pR|s(t) P [Y = 1|S = s] .\nThe threshold ts that minimizes the group-specific loss Ls(t) is a solution to \u2202Ls\u2202t = 0, where:\nLs(t) = \u2113(1, 0) \u00b7 hFPs (t) \u00b7 P [Y = 0|S = s] + \u2113(0, 1) \u00b7 hFNs (t) \u00b7 P [Y = 1|S = s] , \u2202Ls \u2202t = \u2113(1, 0) \u00b7 (t\u2212 1) \u00b7 pR|s(t) + \u2113(0, 1) \u00b7 t \u00b7 pR|s(t).\nHence, for a group-calibrated predictor (fulfilling Equations 8\u20139), for any group s \u2208 S, the optimal group-specific decision threshold ts does not depend on any group quantities, and is given by:\nts = \u2113(1, 0)\n\u2113(1, 0) + \u2113(0, 1) ."
        }
    ],
    "year": 2024
}