{
    "abstractText": "When a machine learning (ML) model exhibits poor quality (e.g., poor accuracy or fairness), the problem can often be traced back to errors in the training data. Being able to discover the data examples that are the most likely culprits is a fundamental concern that has received a lot of attention recently. One prominent way to measure \"data importance\" with respect to model quality is the Shapley value. Unfortunately, existing methods only focus on the ML model in isolation, without considering the broader ML pipeline for data preparation and feature extraction, which appears in the majority of real-world ML code. This presents a major limitation to applying existing methods in practical settings. In this paper, we propose Canonpipe, a method for efficiently computing Shapley-based data importance over ML pipelines. We introduce several approximations that lead to dramatic improvements in terms of computational speed. Finally, our experimental evaluation demonstrates that our methods are capable of data error discovery that is as effective as existing Monte Carlo baselines, and in some cases even outperform them.",
    "authors": [],
    "id": "SP:ab40bb74564137f3f74190d2149a8c797d9fa854",
    "references": [
        {
            "authors": [
                "Sanjeev Arora",
                "Boaz Barak"
            ],
            "title": "Computational complexity: a modern approach",
            "year": 2009
        },
        {
            "authors": [
                "R Iris Bahar",
                "Erica A Frohm",
                "Charles M Gaona",
                "Gary D Hachtel",
                "Enrico Macii",
                "Abelardo Pardo",
                "Fabio Somenzi"
            ],
            "title": "Algebric decision diagrams and their applications",
            "venue": "Formal methods in system design,",
            "year": 1997
        },
        {
            "authors": [
                "Solon Barocas",
                "Moritz Hardt",
                "Arvind Narayanan"
            ],
            "title": "Fairness and Machine Learning",
            "venue": "fairmlbook.org,",
            "year": 2019
        },
        {
            "authors": [
                "Randal E Bryant"
            ],
            "title": "Graph-based algorithms for boolean function manipulation",
            "venue": "Computers, IEEE Transactions on,",
            "year": 1986
        },
        {
            "authors": [
                "Marco Cadoli",
                "Francesco M Donini"
            ],
            "title": "A survey on knowledge compilation",
            "venue": "AI Communications,",
            "year": 1997
        },
        {
            "authors": [
                "James Cheney",
                "Laura Chiticariu",
                "Wang-Chiew Tan"
            ],
            "title": "Provenance in databases: Why, how, and where",
            "venue": "Now Publishers Inc,",
            "year": 2009
        },
        {
            "authors": [
                "Frances Ding",
                "Moritz Hardt",
                "John Miller",
                "Ludwig Schmidt"
            ],
            "title": "Retiring adult: New datasets for fair machine learning",
            "venue": "arXiv preprint arXiv:2108.04884,",
            "year": 2021
        },
        {
            "authors": [
                "Amirata Ghorbani",
                "James Zou"
            ],
            "title": "Data shapley: Equitable valuation of data for machine learning",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Todd J Green",
                "Grigoris Karvounarakis",
                "Val Tannen"
            ],
            "title": "Provenance semirings",
            "venue": "In Proceedings of the twenty-sixth ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems,",
            "year": 2007
        },
        {
            "authors": [
                "Moritz Hardt",
                "Eric Price",
                "Nati Srebro"
            ],
            "title": "Equality of opportunity in supervised learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2016
        },
        {
            "authors": [
                "Ihab F. Ilyas",
                "Xu Chu"
            ],
            "title": "Data Cleaning",
            "venue": "Association for Computing Machinery,",
            "year": 2019
        },
        {
            "authors": [
                "Abhay Jha",
                "Dan Suciu"
            ],
            "title": "Knowledge compilation meets database theory: Compiling queries to decision diagrams",
            "venue": "In ACM International Conference Proceeding Series, pp",
            "year": 2011
        },
        {
            "authors": [
                "Ruoxi Jia",
                "David Dao",
                "Boxin Wang",
                "Frances Ann Hubis",
                "Nezihe Merve Gurel",
                "Bo Li",
                "Ce Zhang",
                "Costas J Spanos",
                "Dawn Song"
            ],
            "title": "Efficient Task-Specific data valuation for nearest neighbor algorithms",
            "venue": "In VLDB,",
            "year": 2019
        },
        {
            "authors": [
                "Ruoxi Jia",
                "David Dao",
                "Boxin Wang",
                "Frances Ann Hubis",
                "Nick Hynes",
                "Nezihe Merve G\u00fcrel",
                "Bo Li",
                "Ce Zhang",
                "Dawn Song",
                "Costas J Spanos"
            ],
            "title": "Towards efficient data valuation based on the shapley value",
            "venue": "In The 22nd International Conference on Artificial Intelligence and Statistics,",
            "year": 2019
        },
        {
            "authors": [
                "Ruoxi Jia",
                "Xuehui Sun",
                "Jiacen Xu",
                "Ce Zhang",
                "Bo Li",
                "Dawn Song"
            ],
            "title": "Scalability vs. utility: Do we have to sacrifice one for the other in data importance quantification",
            "year": 2021
        },
        {
            "authors": [
                "Thorsten Joachims"
            ],
            "title": "A probabilistic analysis of the rocchio algorithm with tfidf for text categorization",
            "venue": "Technical report, Carnegie-mellon univ pittsburgh pa dept of computer science,",
            "year": 1996
        },
        {
            "authors": [
                "Pang Wei Koh",
                "Percy Liang"
            ],
            "title": "Understanding black-box predictions via influence functions",
            "venue": "International conference on machine learning,",
            "year": 2017
        },
        {
            "authors": [
                "Ron Kohavi"
            ],
            "title": "Scaling up the accuracy of naive-bayes classifiers: A decision-tree hybrid",
            "venue": "In Kdd,",
            "year": 1996
        },
        {
            "authors": [
                "Sanjay Krishnan",
                "Michael J Franklin",
                "Ken Goldberg",
                "Eugene Wu"
            ],
            "title": "Boostclean: Automated error detection and repair for machine learning",
            "venue": "arXiv preprint arXiv:1711.01299,",
            "year": 2017
        },
        {
            "authors": [
                "Yung-Te Lai",
                "Massoud Pedram",
                "Sarma B.K. Vrudhula"
            ],
            "title": "Formal verification using edge-valued binary decision diagrams",
            "venue": "IEEE Transactions on Computers,",
            "year": 1996
        },
        {
            "authors": [
                "C.Y. Lee"
            ],
            "title": "Representation of switching circuits by binary-decision programs",
            "venue": "The Bell System Technical Journal,",
            "year": 1959
        },
        {
            "authors": [
                "Peng Li",
                "Xi Rao",
                "Jennifer Blase",
                "Yue Zhang",
                "Xu Chu",
                "Ce Zhang"
            ],
            "title": "CleanML: A study for evaluating the impact of data cleaning on ML classification tasks",
            "venue": "IEEE International Conference on Data Engineering (ICDE 2020)(virtual),",
            "year": 2021
        },
        {
            "authors": [
                "Weixin Liang",
                "Girmaw Abebe Tadesse",
                "Daniel Ho",
                "L Fei-Fei",
                "Matei Zaharia",
                "Ce Zhang",
                "James Zou"
            ],
            "title": "Advances, challenges and opportunities in creating data for trustworthy ai",
            "venue": "Nature Machine Intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "Scott M Lundberg",
                "Su-In Lee"
            ],
            "title": "A unified approach to interpreting model predictions",
            "venue": "Advances in Neural Information Processing Systems",
            "year": 2017
        },
        {
            "authors": [
                "Mark Mazumder",
                "Colby Banbury",
                "Xiaozhe Yao",
                "Bojan Karla\u0161",
                "William Gaviria Rojas",
                "Sudnya Diamos",
                "Greg Diamos",
                "Lynn He",
                "Douwe Kiela",
                "David Jurado"
            ],
            "title": "Dataperf: Benchmarks for data-centric ai development",
            "venue": "arXiv preprint arXiv:2207.10062,",
            "year": 2022
        },
        {
            "authors": [
                "Sachin Mehta",
                "Mohammad Rastegari"
            ],
            "title": "Mobilevit: Light-weight, general-purpose, and mobilefriendly vision transformer. 2022",
            "venue": "URL https://arxiv.org/abs/2110.02178",
            "year": 2022
        },
        {
            "authors": [
                "Curtis G. Northcutt",
                "Lu Jiang",
                "Isaac L. Chuang"
            ],
            "title": "Confident learning: Estimating uncertainty in dataset labels",
            "venue": "Journal of Artificial Intelligence Research (JAIR),",
            "year": 2021
        },
        {
            "authors": [
                "Fotis Psallidas",
                "Yiwen Zhu",
                "Bojan Karla\u0161",
                "Matteo Interlandi",
                "Avrilia Floratou",
                "Konstantinos Karanasos",
                "Wentao Wu",
                "Ce Zhang",
                "Subru Krishnan",
                "Carlo Curino"
            ],
            "title": "Data science through the looking glass and what we found there",
            "venue": "arXiv preprint arXiv:1912.09536,",
            "year": 2019
        },
        {
            "authors": [
                "Nils Reimers",
                "Iryna Gurevych"
            ],
            "title": "Sentence-bert: Sentence embeddings using siamese bert-networks",
            "venue": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics,",
            "year": 2019
        },
        {
            "authors": [
                "Theodoros Rekatsinas",
                "Xu Chu",
                "Ihab F. Ilyas",
                "Christopher R\u00e9"
            ],
            "title": "Holoclean: Holistic data repairs with probabilistic inference",
            "venue": "Proc. VLDB Endow.,",
            "year": 2017
        },
        {
            "authors": [
                "Peter Ross"
            ],
            "title": "Generalized hockey stick identities and n-dimensional blockwalking",
            "venue": "The College Mathematics Journal,",
            "year": 1997
        },
        {
            "authors": [
                "Scott Sanner",
                "David McAllester"
            ],
            "title": "Affine algebraic decision diagrams (aadds) and their application to structured probabilistic inference",
            "venue": "In IJCAI,",
            "year": 2005
        },
        {
            "authors": [
                "Lloyd S. Shapley"
            ],
            "title": "Notes on the N-Person Game II: The Value of an N-Person Game",
            "venue": "RAND Corporation, Santa Monica,",
            "year": 1951
        },
        {
            "authors": [
                "L G Valiant"
            ],
            "title": "The complexity of computing the permanent",
            "venue": "Theor. Comput. Sci.,",
            "year": 1979
        },
        {
            "authors": [
                "Oriol Vinyals",
                "Charles Blundell",
                "Timothy Lillicrap",
                "Daan Wierstra"
            ],
            "title": "Matching networks for one shot learning",
            "venue": "Advances in neural information processing systems,",
            "year": 2016
        },
        {
            "authors": [
                "Wenhui Wang",
                "Furu Wei",
                "Li Dong",
                "Hangbo Bao",
                "Nan Yang",
                "Ming Zhou"
            ],
            "title": "Minilm: Deep selfattention distillation for task-agnostic compression of pre-trained transformers",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Xiaolan Wang",
                "Xin Luna Dong",
                "Alexandra Meliou"
            ],
            "title": "Data x-ray: A diagnostic tool for data errors",
            "venue": "In Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data,",
            "year": 2015
        },
        {
            "authors": [
                "Jiaheng Wei",
                "Zhaowei Zhu",
                "Hao Cheng",
                "Tongliang Liu",
                "Gang Niu",
                "Yang Liu"
            ],
            "title": "Learning with noisy labels revisited: A study using real-world human annotations",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Weiyuan Wu",
                "Lampros Flokas",
                "Eugene Wu",
                "Jiannan Wang"
            ],
            "title": "Complaint-driven training data debugging for query 2.0",
            "venue": "In Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data,",
            "year": 2020
        },
        {
            "authors": [
                "Han Xiao",
                "Kashif Rasul",
                "Roland Vollgraf"
            ],
            "title": "Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms, 2017",
            "year": 2017
        },
        {
            "authors": [
                "Jia"
            ],
            "title": "2019a) relies on several assumptions that do not hold in the context of fork/join pipelines. The prediction of the KNN model (and by extension its accuracy) for any training data (sub)set is strictly dependent on the labels of the top-K data examples that are most similar to some validation example tval for which the KNN model is supposed to predict the label (and by extension result in a measurement of the accuracy of this prediction)",
            "year": 2019
        },
        {
            "authors": [
                "Li"
            ],
            "title": "2021) and artificially inject 50% of label noise. Label noise injection is performed by selecting a random subset representing 50% of training data",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Data quality issues have been widely recognized to be among the main culprits for underperforming machine learning (ML) models, especially when it comes to tasks that are otherwise considered solved by ML (Liang et al., 2022; Ilyas & Chu, 2019). A common type of data errors are wrong labels. For example, biomedical images can be misdiagnosed due to human error which results in label errors. Many systematic methods have been developed to repair data errors (Rekatsinas et al., 2017; Krishnan et al., 2017). Unfortunately, in many practical scenarios, repairing data in a reliable manner requires human labor, especially if humans have been involved in producing the original data. The high cost of this data debugging process has led to a natural question \u2013 Can we prioritize data repairs based on some notion of importance which leads to the highest quality improvements for the downstream model? In recent years, several approaches have emerged to answer these questions. One line of work suggests expressing importance using influence functions (Koh & Liang, 2017) which is essentially a gradientbased approximation of the leave-one-out (LOO) method. Here, the importance of a training data example is expressed as the difference in the model quality score observed after removing that data example from the training set. This quality difference is referred to as the marginal contribution of that data example. Another line of work proposes Shapley value as a measure of importance (Ghorbani & Zou, 2019; Jia et al., 2019b; 2021) that has a long history in game theory (Shapley, 1951). In the context of data importance, it can be seen as a generalization of LOO. Namely, instead of measuring the marginal contribution over the entire training set, we measure it over every subset of the training set and then compute a weighted average. Apart from having many useful theoretical properties, the Shapley value was shown to be very effective in many data debugging scenarios (Jia et al., 2021). On the flip side, because the Shapley value requires enumerating exponentially many subsets, computing it is intractable in practical settings. There have been different ways to approximate this computation. This includes Monte Carlo (MC) sampling (Ghorbani & Zou, 2019) or group testing (Jia et al., 2019b) to sample subsets of training data, train models as black boxes on those subsets, compute marginal contributions of training data examples, and aggregate the results to compute the final approximated result. Unfortunately, re-training the model can be quite costly, especially for large models. Some methods try to overcome this by leveraging proxy models such as K-nearest neighbors (KNN) (Jia et al., 2019a) and exploiting its simple structure to derive dynamic programming (DP) algorithms for computing the Shapley value.\nOne major trait of the existing work in this space is that it primarily focuses on computing the importance of data examples in the prepared training dataset. This poses a challenge in practical settings where data errors typically occur earlier in the data preparation process. In most realistic scenarios, this process involves taking one or more source training datasets, joining them together if needed, and applying a composition of data preprocessing operators (Figure 1). The simplest operators may represent a 1-1 map of input dataset elements to output dataset elements (referred to as tuples in the data management literature). Some operators, such as the data augmentation operator, might fork the data by converting a single input data tuple into multiple output data tuples. On the other hand, an output tuple of a join operator can be the product of multiple input tuples. Finally, some operators involve a reduce operation which involves computing some intermediate result based on the entire input dataset (e.g. the mean and standard deviation) and then applying that result to output tuples. This new setting impacts existing approximation methods in several ways. Firstly, given that it is a black box approach, Monte Carlo sampling (Ghorbani & Zou, 2019) can directly be applied to this setting. However, this comes with the computational cost of re-running the data preprocessing operators for every subset of the training data that we sample. Depending on the complexity of the preprocessing pipeline, this cost can be quite significant. Secondly, the existing KNN-based Shapley approximation method (Jia et al., 2019a) strictly relies on the ability to independently remove tuples from the prepared training dataset in order to compute their marginal contributions. Given the aforementioned complexity induced by preprocessing operators, the tractability result of the previous KNN-based method does not hold directly in this new setting. Therefore, a novel analysis is needed to see whether the Shapley value computation can be made tractable depending on the structure of the data preprocessing pipeline. Contributions. In this paper, we are primarily interested in studying the relationship between the structure of ML pipelines and our ability to efficiently compute Shapley values of source data examples. We make use of data provenance (Green et al., 2007; Cheney et al., 2009), a simple yet powerful theoretical toolkit for tracing individual data examples as they pass through a data processing pipeline. We propose Canonpipe, a framework for modeling the interdependence between tuples induced by data preprocessing operators. Our contributions can be summarized as follows: \u2022 We apply the notion of data provenance to ML pipelines in order to relate the input and output\ndatasets as a function of the pipeline structure. We introduce the notion of a \u201ccanonical pipeline\u201d which we simply define as a distinct pipeline structure that lends itself to efficiently relating pipeline inputs and outputs, as well as efficiently computing Shapley values. We identify three classes of canonical pipelines: map, fork and one-to-many join. (section 3) \u2022 We show how approximating pipelines as canonical leads to significant speed-ups of Monte Carlo methods for Shapley computation. We also demonstrate how the majority of real-world ML pipelines can be approximated as canonical. (section 3) \u2022 We combine canonical pipelines with the K-nearest neighbor as a proxy model. We show how canonical pipelines can be compiled into efficient counting oracles and used to derive PTIME Shapley computation algorithms. Under this framework, the KNN Shapley method from prior work represents a special case applicable to map pipelines. (section 4) \u2022 We conduct an extensive experimental evaluation by applying all considered Shapley computation methods to the task of repairing noisy labels in various real-world datasets. We conclude that in most cases our method is able to achieve solid performance in terms of reducing the cost of label repair while demonstrating significant improvements in computational runtime. (section 5)"
        },
        {
            "heading": "2 PROBLEM: COMPUTING THE SHAPLEY VALUE OVER ML PIPELINES",
            "text": "Shapley Value. Let Dtr be a training dataset and u some utility function used to express the value of any subset of Dtr by mapping it to a real number. Then, the Shapley value, denoting the importance of a tuple ti \u2208 Dtr, is defined as\n\u03c6(ti) = 1 |Dtr| \u2211 D\u2286Dtr\\{ti} (|Dtr|\u22121 |D| )\u22121\n(u(D \u222a {ti})\u2212 u(D)) . (1) Intuitively, the importance of ti for a subset D \u2286 Dtr\\{ti} is measured as the difference between the utility u(D \u222a {ti}) with ti and the utility u(D) without ti. The Shapley value takes a weighted average of all of the 2|Dtr|\u22121 possible subsets D \u2286 Dtr\\{ti}, which enables it to have a range of desired properties that significantly benefit data debugging tasks, often leading to more effective data debugging mechanisms compared to other leave-one-out methods. Quality of ML Pipelines. As mentioned, the utility function u is defined to measure the value of any subset of Dtr, which in our context corresponds to the source training dataset. We assume that this dataset can be made up of multiple sets of tuples (e.g. a multi-modal dataset involving a set of images and a table with metadata). The validation dataset Dval is defined in a similar manner. Then, let f be a data preprocessing pipeline that transforms any training data subset D \u2286 Dtr into a set of tuples {ti = (xi, yi)}i\u2208[M ] made up of M feature and label pairs that the ML training algorithm A takes as input. Finally, we obtain a trained ML modelA\u25e6 f(D) which we can evaluate using some model quality metric. Based on this, we can define the utility function u used to express the value of a training data subset D as a measure of the quality of an ML pipeline A \u25e6 f(D) when scored using Dval. Formally, we write this as\nu(D) := m(A \u25e6 f(D), f(Dval)). (2) Here, m can be any model quality metric such as accuracy or a fairness metric such as equalized odds difference. Note that, for simplicity, we assume that we are applying the same pipeline to both the training data subset D and the validation dataset Dval. In general, these two pipelines can differ as long as the data format of f(Dval) is readable by the trained ML model. For example, a data augmentation operation is typically applied to training data only (as is the case in our experiments). Core Technical Problem. In this work, we focus on the ML pipeline utility u defined in Equation 2 and we ask the following question: How can we approximate the structure of u in order to obtain Shapley-based data importance that is (1) computationally fast; and (2) effective at downstream data debugging tasks?"
        },
        {
            "heading": "3 CANONICAL ML PIPELINES",
            "text": "In this section, we take a closer look at a data preprocessing pipeline f that can, in principle, contain an arbitrarily complex set of data processing operators. This complexity can result in a heavy overhead on the cost of computing the Shapley value. This overhead comes from having to re-evaluate the pipeline many times for different training data subsets. In this section, we describe a framework for minimizing that overhead by solving a concrete technical problem.\nProblem 1. We are given a training dataset Dtr, a data preprocessing pipeline f , and the output set f(Dtr). For an arbitrary subset D \u2286 Dtr and some tuple t\u2032 \u2208 f(Dtr), decide whether t\u2032 \u2208 f(D) in time O(1) w.r.t. |Dtr|. It is easy to see how solving this problem virtually removes the cost of computing the pipeline output of an arbitrary training data subset. Next, we describe a reduced version of the data provenance framework (Green et al., 2007; Cheney et al., 2009) which we will apply to solve this problem."
        },
        {
            "heading": "3.1 DATA PROVENANCE FOR ML PIPELINES",
            "text": "We define a set of binary variables A and associate a variable at \u2208 A with every training data tuple t \u2208 Dtr. Each subset D \u2286 Dtr can be defined using a value assignment v(a) 7\u2192 {0, 1}, where v(at) = 1 means that t \u2208 D. We can use Dtr[v] to denote D. We write VA to denote the set of all the 2|A| possible value assignments. Next, with every tuple t\u2032 \u2208 f(Dtr) we associate a \u201cprovenance polynomial\u201d pt\u2032 which is a logical formula with variables in A (e.g. a1 + a2 \u00b7 a3). For a given\nvalue assignment v, we define an evaluation function evalv(pt\u2032) 7\u2192 {0, 1} which simply follows the standard logical reduction rules to determine the truthiness of pt\u2032 given v. For a tuple t\u2032 \u2208 f(Dtr) and a value assignment v, we define t\u2032 \u2208 f(Dtr[v]) iff evalv(pt\u2032) = 1. It is easy to see that we can directly apply this framework to solve Problem 1. However, to respect the O(1) time complexity, |pt| must be O(1) w.r.t. |Dtr|. In subsection 3.2, we explore when this condition is met. Redefining the Shapley value. Using this framework, we can rewrite the Shapley value as:\n\u03c6(ti) = 1 |A| \u2211 v\u2208VA\\{ai} ( |A|\u22121 |supp(v)| )\u22121 u(Dtr[v; ai \u2190 1])\u2212 u(Dtr[v; ai \u2190 0]) (3)\nThe notation [v; ai \u2190 X] for X \u2208 {0, 1} means that we augment v with v(ai) = X . Also, we define the support of v as supp(v) := {a \u2208 A : v(a) = 1}."
        },
        {
            "heading": "3.2 APPROXIMATION: ML PIPELINES ARE CANONICAL",
            "text": "As mentioned above, solving Problem 1 in O(1) time depends on |pt| being O(1) w.r.t. |Dtr|. This does not necessarily hold true for an arbitrary pipeline f . However, it does hold true for some classes of pipelines, which we refer to as canonical pipelines. Hence, if we approximate the pipeline f as canonical, then we can solve Problem 1. The three classes of canonical pipelines that we identified to be useful in the context of this work are: map, fork, and one-to-many join pipelines (Figure 2). Map pipelines. This is the simplest form of pipeline where each input tuple t \u2208 Dtr corresponds to at most one output tuple t\u2032 \u2208 f(Dtr), after passing through an optional per-tuple mapping function \u00b5(t) 7\u2192 t\u2032 (Figure 2a). Examples of such pipelines include missing value indicators, polynomial feature generators, pre-trained embeddings, etc. Fork pipelines. In this pipeline, each input tuple t \u2208 Dtr can be associated with multiple output tuples t\u2032 \u2208 f(Dtr), but a single output tuple is associated with a single input tuple (Figure 2b). A prominent example is a data augmentation pipeline that outputs several slightly altered versions of every input tuple. One-to-many Join pipelines. This pipeline contains table join operators like the one in Figure 1. Here, the training dataset Dtr = {Dt,Da1 , . . . ,Dak} is made up of multiple tuple sets that form a \"star schema\". This means that any training example tuple t \u2208 Dt can be joined with no more than one tuple from each of the auxiliary tables Da1 , . . . ,Dak . Note that, for this pipeline, the provenance polynomial of each output tuple is a Boolean product of variables associated with all tuples that were joined to produce that output tuple (Figure 2c)."
        },
        {
            "heading": "3.3 APPROXIMATING REAL ML PIPELINES",
            "text": "Even though many real-world pipelines can be directly represented as our canonical pipelines, there is still a solid amount that cannot be represented. Nevertheless, upon taking a closer look, we can identify a class of pipelines that we might be able to approximately represent. These are the pipelines that exhibit an estimator-transformer pattern f(D) = map(reduce(D),D). Specifically, they are made up of some reduce operation performed on the entire dataset which produces some intermediate data, which is used to parameterize a map operation which is performed on individual tuples. An example of such a pipeline is a min-max scaler, where the reduce step computes min and max statistics for each feature, which are then used to re-scale individual tuples. The reduce step of this pipeline causes every output tuple to depend on every input tuple, which does not fit into our canonical pipeline framework. However, we can still approximate such pipelines by isolating the intermediate data produced by reduce(Dtr). Then, conditioned on that intermediate data, we can re-define our pipeline f to be a conditional map pipeline f\u2217 as follows:\nf(D) = map(reduce(D),D) 7\u2192 f\u2217(D) = map(reduce(Dtr),D).\nEvaluation of Effectiveness. We evaluate our method of approximating pipelines as canonical and apply it directly to compute the Shapley value using the Truncated Monte Carlo (TMC) sampling method (Ghorbani & Zou, 2019). We run the evaluation for 10 and 100 Monte Carlo iterations (x10/x100). We can see that our approach exhibits comparable performance with significant gains in computational runtime (Figure 3). See section 5 for more details about the experimental protocol. Statistics of Real-world Pipelines. A natural question is how common these families of pipelines are in practice. Figure 2d illustrates a case study that we conducted using 500K real-world pipelines provided by Microsoft (Psallidas et al., 2019). We divide pipelines into three categories: (1) \u201cpure\u201d map/fork pipelines, based on our definition of canonical pipelines; (2) \u201cconditional\u201d map/fork pipelines, which are comprised of a reduce operator that can be effectively approximated using the scheme we just described; and (3) other pipelines, which contain complex operators that cannot be approximated. We observe that a vast majority of pipelines we encountered in our case study fall into the first two categories that we can effectively approximate using our canonical pipelines framework."
        },
        {
            "heading": "4 SHAPLEY VALUE OVER CANONICAL PIPELINES",
            "text": "In section 3 we described an approach for treating the data preprocessing pipeline f as a white box which led us to directly attainable performance improvements of Monte Carlo Shapley methods. However, these methods still rely on treating the modelA as a black box and retraining it for different training data subsets, which often results in slow runtime. In this section, we are interested in PTIME algorithms that give orders of magnitude faster runtime and thus open the door for interactive data debugging. Specifically, we focus on the following technical problem: Problem 2. We are given a training dataset Dtr, a data preprocessing pipeline f and a model quality metric m computed over a given validation dataset Dval. Compute the Shapley value (as defined in Equation 1) of a given tuple ti \u2208 Dtr for the ML pipeline utility (as defined in Equation 2) in time polynomial w.r.t. |Dtr| and |Dval|. We will now explore additional approximations we can make on the model A as well as the model quality metric m. Specifically, we replace the model with a KNN classifier, and we assume that the quality metric has a specific additive structure. We then sketch the outline of a solution to the given problem that leverages these approximations. It should be noted that although prior work has explored the idea of using the KNN proxy model for PTIME algorithms (Jia et al., 2019a), to the best of our knowledge, the work presented in this paper is the first to analyze the relationship between the structure of different types of ML pipelines and the computational complexity of the Shapley value computation. A brief discussion about the limitations of prior work is presented in Appendix A."
        },
        {
            "heading": "4.1 APPROXIMATION: THE MODEL IS KNN AND THE MODEL QUALITY METRIC IS ADDITIVE",
            "text": "Here we define two structures which we will use as building blocks for approximating ML pipelines: the KNN model and additive model quality metrics. In the following section we will show how these building blocks can be leveraged to provide PTIME algorithms for computing Shapley values. K-Nearest Neighbor (KNN) Model. We provide a specific definition of the KNN model in order to facilitate our further analysis. Given some set of training tuples D and a validation tuple tval, the KNN model AKNN (D) can be defined as follows:\nAKNN (D)(tval) := argmaxy\u2208Y ( tally ( D \u2223\u2223\u2223 topK(D \u2223\u2223 tval), tval)(y)). (4)\nHere, topK(D | tval) returns a tuple tK \u2208 D that takes the K-th position when ranked by similarity with the validation tuple tval. Furthermore, tally(D | tK , tval) tallies up the class labels of all tuples in D that have similarity with tval higher or equal to tK . It returns \u03b3, a label tally vector that is indexed by class labels (i.e. \u03b3 : Y \u2192 N). Note that the sum of all elements in \u03b3 must be K. Given a set of classes Y , we define \u0393Y,K to be the set of all possible label tally vectors. Finally, assuming a standard majority voting scheme, argmaxy\u2208Y returns the predicted class label with the highest tally. Additive Model Quality Metric. We say that a model quality metric is additive if there exists a tuple-wise metric mT such that m can be written as:\nm(A \u25e6 f(D), f(Dval)) = w \u00b7 \u2211\ntval\u2208f(Dval) mT\n(( A \u25e6 f(D) ) (tval), tval ) (5)\nHere, w is a scaling factor that can depend only onDval. The tuple-wise metric mT : (ypred, tval) 7\u2192 [0, 1] takes a validation tuple tval \u2208 Dval as well as a class label ypred \u2208 Y predicted by the model for tval. It is easy to see that some popular utilities, such as validation accuracy, are additive, e.g., the accuracy utility is simply defined by plugging mT (ypred, (xval, yval)) := 1{ypred = yval} and w := 1/|Dval| into Equation 5. In subsection E.3, we show even more examples of such metrics."
        },
        {
            "heading": "4.2 COMPUTING THE SHAPLEY VALUE",
            "text": "We now outline our approach to computing the Shapley value of a training data tuple ti \u2208 Dtr using our approximation described in subsection 3.2 and subsection 4.1. We start off from Equation 3 and plug in u as defined in Equation 2. Next, since we assume that our model quality metric is additive, we plug in m as defined in Equation 5. By rearranging the sums, we can write the Shapley formula as \u03c6(ti) = w \u00b7 \u2211 tval\u2208f(Dval) \u03c6(ti, tval), where \u03c6(ti, tval) is a validation tuple-wise Shapley value. Under the assumption that our model is KNN, we can plug in A as defined in Equation 4, rearrange the sums, and arrive at the following definition of \u03c6(ti, tval):\n\u03c6(ti, tval) = 1 |A| \u2211 t\u2032,t\u2032\u2032\u2208f(Dtr) \u2211|A| \u03b1=1 (|A|\u22121 \u03b1 )\u22121 \u2211 \u03b3\u2032,\u03b3\u2032\u2032\u2208\u0393Y,K m\u2206(\u03b3 \u2032, \u03b3\u2032\u2032 | tval) \u00b7 \u03c9(\u03b1, \u03b3\u2032, \u03b3\u2032\u2032 | ti, tval, t\u2032, t\u2032\u2032). (6) We define m\u2206(\u03b3\u2032, \u03b3\u2032\u2032 | tval) := mT (argmaxy\u2208Y\u03b3\u2032\u2032(y), tval) \u2212mT (argmaxy\u2208Y\u03b3\u2032(y), tval) as the differential metric. Counting Oracles. The function \u03c9 in Equation 6 is a counting oracle which we introduce to help us isolate and analyze the exponential sum from Equation 3. We define it as:\n\u03c9(\u03b1, \u03b3\u2032, \u03b3\u2032\u2032 | ti, tval, t\u2032, t\u2032\u2032) := \u2211\nv\u2208VA\\{ai}\n\u00b71 { \u03b1 = |supp(v)| } \u00b71 { t\u2032 = topK ( f(Dtr[v; ai \u2190 0]) | tval )} \u00b7 1 { t\u2032\u2032 = topK ( f(Dtr[v; ai \u2190 1]) | tval\n)} \u00b71 { \u03b3\u2032 = tally ( f(Dtr[v; ai \u2190 0]) | t\u2032, tval )} \u00b7 1 { \u03b3\u2032\u2032 = tally ( f(Dtr[v; ai \u2190 1]) | t\u2032\u2032, tval )} . (7)\nIntuitively, the counting oracle is a function that returns the number of value assignments with exactly \u03b1 variables set to 1, and the label tally of the top-K tuples will be exactly \u03b3\u2032\u2032 when ti is included in the training dataset, and \u03b3\u2032 when it is excluded. By looking at Equation 6 we can observe that all the sums are polynomial w.r.t. the size of data. Thus, we arrive at the following theorem (which we prove in Appendix E): Theorem 4.1. If we can compute the counting oracle \u03c9 as defined in Equation 7 in time polynomial w.r.t. |Dtr| and |Dval|, then we can compute the Shapley value of a tuple ti \u2208 Dtr in time polynomial w.r.t. |Dtr| and |Dval|. The above theorem outlines a solution of Problem 2, given that we can find a PTIME solution for computing the counting oracle. Next, we cover a solution that models the problem as a model counting problem by leveraging a data structure which we call Additive Decision Diagrams (ADD\u2019s). Counting Oracle as Model Counting over ADD\u2019s. We use Additive Decision Diagram (ADD) to compute the counting oracle \u03c9t,t\u2032 (Equation 7). An ADD represents a Boolean function \u03d5 : VA \u2192 E \u222a {\u221e} that maps value assignments v \u2208 VA to elements of some set E or a special invalid element \u221e (see Appendix C for more details). For our purpose, we define E := {1, ..., |A|} \u00d7 \u0393Y,K \u00d7 \u0393Y,K . Then, we define a function over Boolean inputs \u03d5(v | ti, , tval, t\u2032, t\u2032\u2032) as follows:\n\u03d5(v | ti, tval, t\u2032, t\u2032\u2032) :=  \u221e, if t\u2032 \u0338\u2208 Dtr[v; ai \u2190 0], \u221e, if t\u2032\u2032 \u0338\u2208 Dtr[v; ai \u2190 1], (\u03b1, \u03b3\u2032, \u03b3\u2032\u2032), otherwise,\n\u03b1 := |supp(v)|, \u03b3\u2032 := tally ( Dtr[v; ai \u2190 0] | t\u2032, tval ) , \u03b3\u2032\u2032 := tally ( Dtr[v; ai \u2190 1] | t\u2032\u2032, tval ) .\n(8)\nIf we can construct an ADD that computes \u03d5(v | ti, tval, t\u2032, t\u2032\u2032), then the model counting operation on that ADD exactly computes \u03c9(\u03b1, \u03b3\u2032, \u03b3\u2032\u2032 | ti, tval, t\u2032, t\u2032\u2032). As the complexity of model counting is O(|N | \u00b7 |E|) (see Equation 12) and |E| is polynomial in the data size, we have the following result: Theorem 4.2. If we can represent the \u03d5t,t\u2032(v) in Equation 8 with an ADD of size polynomial in |A| and |Dftr|, we can compute the counting oracle \u03c9t,t\u2032 in time polynomial of |A| and |Dftr|. A proof is provided in Appendix E. For specific canonical pipelines, we have the following corollaries. Corollary 4.1. (One-to-Many Join Pipelines) For the K-NN accuracy utility and a one-to-many join pipeline, which takes as input two datasets, DF and DD, of total size |DF | + |DD| = N and outputs a joined dataset of size O(N), the Shapley value can be computed in O(N4) time. Corollary 4.2. (Fork Pipelines) For the K-NN accuracy utility and a fork pipeline, which takes as input a dataset of size N and outputs a dataset of size M , the Shapley value can be computed in O(M2N2) time. Corollary 4.3. (Map Pipelines) For the K-NN accuracy utility and a map pipeline, which takes as input a dataset of size N , the Shapley value can be computed in O(N2) time.\nEvaluation of Effectiveness. We evaluate our method of computing the Shapley value by using KNN as a proxy model (Figure 4). We can see that its effectiveness is comparable even when applied to the task of label repair over pipelines that have different models. On the other hand, we can see that the computational cost is orders of magnitude lower when compared to MC methods."
        },
        {
            "heading": "5 EXPERIMENTAL EVALUATION",
            "text": "We evaluate the performance of our method by applying it to a common data debugging scenario \u2013 label repair. The goal of this empirical study was to validate that: (1) our approximations enable significantly faster computation of Shapley values, and (2) in spite of any inherent biases, these approximations still manage to enable effective data debugging."
        },
        {
            "heading": "5.1 EXPERIMENTAL SETUP",
            "text": "Protocol. We conduct a series of experimental runs that simulate a real-world importance-driven data debugging workflow. In each experimental run, we select a dataset, pipeline, model, and data repair method. If a dataset does not already have human-generated label errors, we follow the protocol of Li et al. (2021) and Jia et al. (2021) and artificially inject 50% of label noise. We compute the importance using a validation dataset and use it to prioritize our label repairs. We divide the range between 0% data examined and 100% data examined into 100 checkpoints. At each checkpoint, we measure the quality of the given model on a separate test dataset using some metric (e.g. accuracy). We also measure the time spent on computing importance scores for the entire training dataset. We repeat each experiment 10 times and report the median as well as the 90-th percentile range (either shaded or with error bars). Data Debugging Methods. We apply various methods of computing data importance: \u2022 Random \u2014 Importance is a random number and thus we apply data repairs in random order. \u2022 TMC x10 / x100 \u2014 Shapley values computed using the Truncated Monte-Carlo (TMC)\nmethod (Ghorbani & Zou, 2019), with 10 and 100 Monte-Carlo iterations, respectively. \u2022 Canonpipe TMC x10 / x100 \u2014 This applies our method of approximating pipelines using data\nprovenance over canonical pipelines to the TMC method of computing the Shapley value.\n\u2022 Canonpipe KNN \u2014 This is our method for efficiently computing the Shapley value over ML pipelines by using the KNN as a proxy model. \u2022 Canonpipe KNN Interactive \u2014 While the above methods compute importance only once at the beginning of the repair process, the speed of our method allows us to recompute the importance after each data repair checkpoint.\nData Preprocessing Pipelines. We obtained a dataset with about 500K machine learning workflow instances from internal Microsoft users (Psallidas et al., 2019). Each workflow consists of a dataset, a data preprocessing pipeline, and an ML model. We identified a handful of the most representative pipelines and translated them to sklearn pipelines. All pipelines used in our experiments are listed in Table 1 along with the operators they\nare made up of. Some pipelines are purely canonical, while some involve a reduce operation."
        },
        {
            "heading": "5.2 RESULTS",
            "text": "In this section, we highlight some of the most interesting results of our empirical analysis and point out some key insights that we can draw. A more extensive experimental analysis is presented in Appendix G. We start off with three general scenarios: (1) accuracy-driven label repair; (2) fairnessdriven label repair to demonstrate usage of different model quality metrics; and (3) label repair in deep learning scenarios. In each one, we study the tradeoff between computational cost of any data repair approach, and the labor cost, which is measured as the amount of data repairs that need to be conducted to deliver the biggest improvement of model quality. Finally, we conduct a scalability analysis of our algorithm to showcase its potential for handling large datasets. Improving Accuracy. In this set of experiments, our goal is to improve model accuracy with targeted label repairs. In Figure 4 we show one example workflow for the FolkUCI Adult dataset and the pipeline from Figure 1 without the join operator. We evaluate our KNN-based method over pipelines that contain two different ML models: LogisticRegression and XGBoost. We can draw two key conclusions about our KNN-based algorithm. Firstly, given that our KNN-based method is able to achieve comparable performance to Monte Carlo-based methods, we can conclude that KNN can indeed serve as a good proxy model for computing the Shapley value. Secondly, it is able to achieve this performance at only a fraction of the computational cost which makes it even more compelling. Improving Accuracy and Fairness. Next, we explore the relationship between accuracy and fairness when performing label repairs. In these experiments, we use tabular datasets that have a \u2018sex\u2019 feature that we use to compute group fairness using equalized odds difference (Hardt et al., 2016). In Figure 5 we explore the tradeoff between two data debugging goals \u2013 the left panel illustrates the behavior of optimizing for accuracy whereas the right panel illustrates the behavior of optimizing for fairness. We first notice that being able to debug specifically for fairness is important because for some datasets improving accuracy does not necessarily improve the fairness of the trained model. Secondly, we can see that even when we do optimize for fairness, not all methods will end up being successful. The best-performing method is Canonpipe KNN Interactive which is the only one that recomputes the Shapley value at each of the 100 checkpoints (due to the speed of our KNN-based method). It\nis likely that the complexity of the equalized odds difference as a metric makes it challenging to compute the Shapley value. Especially since some interventions on the dataset might end up shifting the optimal path, and only by recomputing are we able to detect this shift. Deep learning pipelines. We also measured the effectiveness of our approximation methods in several scenarios that involve deep learning models. In Figure 6a we use a pre-trained ResNet-18 model as the target model. We fine-tune it for 5 epochs on a noisy label dataset and see that Canonpipe KNN fares favorably compared to random label repair. Figure 6b shows the result of applying a pre-trained embedding model and evaluating both the Canonpipe KNN and the Canonpipe TMC approximations, where the KNN proxy again shows good performance. Finally, in Figure 6c we show how our method can be used to repair labels of a dataset used as a support set for a one-shot learning neural network. We use the matching networks model (Vinyals et al., 2016) which employs a learned \u201cdistance metric\u201d between examples in the test set and those in the support set. This allows us to replace the standard Euclidean distance metric in our KNN proxy model with a custom one and achieve effective label repairs with efficiently computed Shapley values.\nScalability. We evaluate the speed of our algorithm for larger training datasets. We test the runtime for various sizes of the training set (10k-1M ), the validation set (100-10k), and the number of features (100-1k). As expected, the impact of the training set size and validation set size is roughly linear (Figure 7). Even for large datasets, our method can compute Shapley scores in minutes."
        },
        {
            "heading": "6 RELATED WORK",
            "text": "Targeted data repairs have been studied for some time now. Apart from the work mentioned in section 1, a notable piece of work is CleanLab which leverages confident learning to make targeted repairs of noisy labels (Northcutt et al., 2021). Our work focuses on the Shapley value given it was shown to be applicable to many scenarios (Jia et al., 2021). Apart from the data valuation scenario, the Shapley value has also been used for computing feature importance (Lundberg & Lee, 2017). On the other hand, the scope of our work is data importance over ML pipelines. Debugging data pipelines has started receiving some attention recently. Systems such as Data X-Ray can debug data processing pipelines by finding groups of data errors that might have the same cause (Wang et al., 2015). A notable piece of work leverages influence functions as a method for analyzing pipelines comprising of a model and a post-processing query (Wu et al., 2020). This work also leverages data provenance as a key ingredient. However, their focus is only on queries that take as input predictions of a model that has been trained directly on the source data."
        },
        {
            "heading": "7 CONCLUSION AND OUTLOOK",
            "text": "In this paper, we propose Canonpipe, a framework for representing a wide range of ML pipelines that appear in real-world scenarios with the end goal of efficiently computing the Shapley value of source data examples. We show how this framework can be leveraged to provide significant speed-ups to Monte Carlo-based methods for Shapley value computation. Furthermore, we provide PTIME algorithms for computing the Shapley value using the KNN proxy model for several classes of ML pipelines. Finally, we empirically demonstrate that our methods achieve significant speed-ups over previously developed baselines while demonstrating competitive performance in a downstream data debugging task."
        },
        {
            "heading": "A DISCUSSION ABOUT THE LIMITATIONS OF PRIOR WORK",
            "text": "In this section, we provide a brief outline of the existing KNN approximation method for computing the Shapley value (Jia et al., 2019a) which was instrumental in laying the foundation for applying the KNN proxy model to Shapley computation. However, as we argue in this paper, this work is not directly applicable to ML pipelines as defined in this paper. Note that our goal here is to offer only intuition as to why it is the case, and thus we are leaving out many technical details. In Appendix F we present how the results in (Jia et al., 2019a) can be seen as a special case for computing Shapley values using the 1-NN proxy model. The polynomial time approximation to computing Shapley values using the KNN proxy model established by Jia et al. (2019a) relies on several assumptions that do not hold in the context of fork/join pipelines. The prediction of the KNN model (and by extension its accuracy) for any training data (sub)set is strictly dependent on the labels of the top-K data examples that are most similar to some validation example tval for which the KNN model is supposed to predict the label (and by extension result in a measurement of the accuracy of this prediction). To compute the Shapley value of a training data example ti \u2208 Dtr, we need to know the accuracy difference (i.e. the marginal contribution) that occurs when adding that data example to every possible subset D \u2286 Dtr. In simple terms, the method in (Jia et al., 2019a) computes the Shapley value of an input data example by first sorting all data examples according to their similarity with tval. After that, it relies on the observation that in order for ti to end up in the top-K (and thus have a chance of impacting the prediction accuracy of some subset D \u2286 Dtr), of all data examples that are higher than ti in the sorting order, at most (K \u2212 1) can be present in D. It then computes how many subsets D \u2286 Dtr of size \u03b1 satisfy this condition. Specifically, if ti takes the j-th position in the sorting order, then the number of such subsets is ( j\u22121 K\u22121 )(|Dtr|\u2212j \u03b1\u2212K ) . Finally, it includes the Shapley weighing factor along with some combinatorial tricks to combine all this into a simple formula:\n\u03c6(ti, tval) = |Dtr|\u2211 j=1 ( mT (y(ti), tval)\u2212mT (y(tj), tval) )(|Dtr| \u2212 j j + 1 )\nAs we can see, this method strictly expects that adding ti to any subset of Dtr will always result in either 0 or 1 data examples being added to the top-K and that the choice between 0 and 1 strictly depends on the number of data examples that come before ti in the sorting order. Two core assumptions lie behind this expectation: (1) adding ti to a subset of Dtr will always result in exactly one additional data example being passed to KNN, and (2) the presence of any data example in the KNN training set is caused by the presence of exactly one data example in Dtr. The first assumption allows us to separate data examples into those that come before ti in the sorting order and those that come after. The second assumption allows us to count subsets using binomial coefficients. If any of the two assumptions do not hold, then the simple combinatorial formula is no longer applicable because the data examples passed to KNN are no longer independent from each other. Map pipelines do not break these assumptions. On the other hand, fork pipelines break the first assumption, and join pipelines break both the first and the second assumption. In this work, we examine the broader setting of ML pipelines which comes with several open questions. If any single training data examples ti \u2208 Dtr is associated with e.g. 10 data examples that are passed to KNN, and they are all intertwined in the sorting order, how do we efficiently compute the number of subsets D \u2286 Dtr where adding a specific data example ti will result in altering the accuracy of the KNN prediction? If a data example that gets passed to KNN is the result of joining two data examples t1,1 and t2,1 from separate source datasets D1 and D2, but t1,1 is also joined with other examples from D2 that make up even more output data examples, so removing t2,1 from the training dataset will result in one data example not being passed to KNN but removing t1,1 will result in more than one not being passed, how do we efficiently compute the number of subsets where adding t1,1 will alter the KNN prediction? Do things change in the case of multi-class classification? Can we use model quality metrics other than accuracy? To answer these open questions, we employed all the theoretical components described in this paper, including provenance polynomials, ADD\u2019s, and model counting oracles. The theoretical insight we would like to convey is that all these components are fundamental to solving this problem and that this is the correct level of abstraction for analyzing ML pipelines and developing PTIME algorithms."
        },
        {
            "heading": "B DISCUSSION ABOUT TYPES OF ML PIPELINE OPERATORS",
            "text": "Here we provide an overview of types of pipeline operators that can be found in ML workflows. We base this discussion on operatos that can be found in the scikit-learn and ML.NET frameworks, as well as commonly used operators that can be found in real-world ML code. Unary Map: These are functions that map single value inputs to single value outputs. Examples include:\n\u2022 Log - Computes a logarithm of the input. \u2022 Missing Value Indicator - returns a Boolean that indicates if the input is a missing value or not\n(e.g. MissingIndicator in scikit-learn). \u2022 Stopword Remover - takes an input list of string tokens and removes the ones that correspond to\nstop-words (e.g. \"the\", \"and\", etc); the list of stop words is specified as an additional argument (e.g. StopWordsRemovingTransformer in ML.NET)\nBinary Numerical and Logical Map: These are common mathematical operators such as addition, subtraction, multiplication, division, logical and, logical or, equality test, etc. Multi-Value Map: Values containing multiple elements are taken as inputs and produced as outputs. A key example is a vector normalizing operator which maps a vector input to a vector output. Tuple Filter Map: These operators remove tuples from the dataset based on the result of some unary map operation. Since these operators map a single tuple to either a single output tuple or to nothing, they are categorized as map filters. Examples include:\n\u2022 Missing Value Filter - Removes tuples that contain missing values. \u2022 Range Filter - Removes tuples where values of a specified column are outside a given range.\nNumerical Aggregate Reduce: This operator takes an entire column and reduces it into a single numerical value. Examples include summation, counting, mean value, standard deviation, as well as minimal and maximal element selector operators. Unary Map with Reduce Elements: These operator function similarly to regular unary map operators. However, their mapping operation is dependent on performing some numerical aggregate reduce operation beforehand. Examples include:\n\u2022 Min-Max Scaler - Scales column values to a 0-1 range based on minimal and maximal element values which represent the pre-computed reduce element (e.g. MinMaxScaler in scikit-learn). \u2022 Standardization Scaler - Same as the min-max scaler but transforms elements based on the pre-computed mean and standard deviation values (e.g. StandardScaler in scikit-learn). \u2022 One-Hot Encoder - Encodes numerical features as a one-hot numerical array. Depends on a pre-computed list of unique column element values. \u2022 TD-IDF Encoder - Converts textual values into their Term Frequency - Inverse Document Frequency encodings. This operator depends on a pre-computed dictionary of token frequencies.\nData Augmentation Fork: This can be any data augmentation operator that maps input tuples to some specified number of output tuples. Examples include: random noise injection, randomly shifting or rotating images, removing or replacing characters in text to simulate misspelling, etc. One-to-Many Join: Join operators compute a matching between two sets of tuples DA and DB , and for each pair of matched input tuples produce a single output tuple. In general there are no constraints on the kinds of matchings that can be performed. However, the specific type of join we describe here, referred to as one-to-many type join requires that tuples from one of the two sets (e.g. DA) can be matched with at most one tuple from the other set (e.g. DB). At the same time, tuples from DB can be matched with multuple tuples from DA."
        },
        {
            "heading": "C PRELIMINARY: ADDITIVE DECISION DIAGRAMS (ADD\u2019S)",
            "text": "In this section, we describe a type of decision diagram that we use as a tool for compact representation of functions over Boolean inputs. The process of translating functions into data structures for easier analysis is referred to as knowledge compilation. We briefly describe this in the context of our work, and then go over the data structure we use in our methods \u2013 Additive Decision Diagrams.\nKnowledge Compilation. Our approach to computing the Shapley value will rely upon being able to construct functions over Boolean inputs \u03d5 : VA \u2192 E , where E is some finite value set. We require an elementary algebra with +, \u2212, \u00b7 and / operations to be defined for this value set. Furthermore, we require this value set to contain a zero element 0, as well as an invalid element\u221e representing an undefined result (e.g. a result that is out of bounds). We then need to count the number of value assignments v \u2208 VA such that \u03d5(v) = e, for some specific value e \u2208 E . This is referred to as the model counting problem, which is #P complete for arbitrary logical formulas Valiant (1979); Arora & Barak (2009). For example, if A = {a1, a2, a3}, we can define E = {0, 1, 2, 3,\u221e} to be a value set and a function \u03d5(v) := v(a1) + v(a2) + v(a3) corresponding to the number of variables in A that are set to 1 under some value assignment v \u2208 VA. Knowledge compilation Cadoli & Donini (1997) has been developed as a well-known approach to tackle this model counting problem. It was also successfully applied to various problems in data management Jha & Suciu (2011). One key result from this line of work is that, if we can construct certain polynomial-size data structures to represent our logical formula, then we can perform model counting in polynomial time. Among the most notable of such data structures are decision diagrams, specifically binary decision diagrams Lee (1959); Bryant (1986) and their various derivatives Bahar et al. (1997); Sanner & McAllester (2005); Lai et al. (1996). For our purpose in this paper, we use the additive decision diagrams (ADD), as detailed below. Additive Decision Diagrams (ADD). We define a simplified version of the affine algebraic decision diagrams Sanner & McAllester (2005). An ADD is a directed acyclic graph defined over a set of nodes N and a special sink node denoted as \u22a1. Each node n \u2208 N is associated with a variable a(n) \u2208 A. Each node has two outgoing edges, cL(n) and cH(n), that point to its low and high child nodes, respectively. For some value assignment v, the low/high edge corresponds to v(a) = 0/v(a) = 1. Furthermore, each low/high edge is associated with an increment wL/wH that maps edges to elements of E . Note that each node n \u2208 N represents the root of a subgraph and defines a Boolean function. Given some value assignment v \u2208 VA we can evaluate this function by constructing a path starting from n and at each step moving towards the low or high child depending on whether the corresponding variable is assigned 0 or 1. The value of the function is the result of adding all the edge increments together. Figure 8a presents an example ADD with one path highlighted in red. Formally, we can define the evaluation of the function defined by the node n as follows:\nevalv(n) :=  0, if n = \u22a1, wL(n) + evalv(cL(n)) if v(x(n)) = 0,\nwH(n) + evalv(cH(n)) if v(x(n)) = 1.\n(9)\nIn our work, we focus specifically on ADD\u2019s that are full and ordered. A diagram is full if every path from root to sink encounters every variable in A exactly once. For example, in Figure 8a we see a full diagram over the set of variables A = {a1,1, a1,2, a2,1, a2,2, a2,3}. If any of the variables in A has no node associated with it, then the diagram is not considered full. On the other hand, an ADD is ordered when on each path from root to sink variables always appear in the same order. For this purpose, we define \u03c0 : A \u2192 {1, ..., |A|} to be a permutation of variables that assigns each variable a \u2208 A an index. For example, in Figure 8a, the variable order is \u03c0 = {a1,1 7\u2192 1, a1,2 7\u2192 4, a2,1 7\u2192 2, a2,2 7\u2192 3, a2,3 7\u2192 5}. It is possible, for example, to swap the two nodes on the left side that correspond to a2,1 and a2,2. This, however, makes the diagram unordered, which dramatically complicates certain operations (e.g. the diagram summation operation that we will describe shortly). Diagram Diameter. We define the diameter of an ADD as the maximum number of nodes associated with any single variable. Formally we can write:\ndiam(N ) := max ai\u2208A \u2223\u2223{n \u2208 N : a(n) = ai}\u2223\u2223 (10) We can immediately notice that the size of any ADD with a set of nodes N and variables A is bounded by O(|A| \u00b7 diam(N )). Model Counting. We define a model counting operator\ncounte(n) := \u2223\u2223\u2223{v \u2208 VA[\u2264\u03c0(a(n))] | evalv(n) = e}\u2223\u2223\u2223, (11)\n(a) ADD\na1,1\na2,1\n+1 a2,2\n+1 a1,2\na2,3\n\u00b7 +1\n(b) Uniform ADD\na1\n+5 a2\n+5 a3\n\u00b7 +5\nFigure 8: (a) An ordered and full ADD for computing \u03d5(v) := v(a1,1) \u00b7\n( v(a2,1) + v(a2,2) ) +\nv(a1,2) \u00b7 v(a2,3). (b) A uniform ADD for computing \u03d5(v) := 5 \u00b7 (v(a1) + v(a2) + v(a3)).\nwhere A[\u2264 \u03c0(a(n))] is the subset of variables in A that include a(n) and all variables that come before it in the permutation \u03c0. For an ordered and full ADD, counte(n) satisfies the following recursion:\ncounte(n) :=  1, if e = 0 and n = \u22a1, 0, if e =\u221e or n = \u22a1, counte\u2212wL(n)(cL(n)) + counte\u2212wH(n)(cH(n)), otherwise. (12)\nThe above recursion can be implemented as a dynamic program with computational complexity O(|N | \u00b7 |E|). Figure 8b shows a special case of a full and ordered ADD, which we call a uniform ADD. It is structured as a chain with one node per variable, where all low increments equal zero and all high increments equal some constant E \u2208 E . For this type of ADD, we can perform model counting in constant time, assuming that we have a precomputed table of factorials of size O(|N |) that allows us to compute binomial coefficients in constant time. The counte operator for a uniform ADD can be defined as\ncounte(n) :=\n{( \u03c0(a(n)) e/E ) , if e mod E = 0,\n0 otherwise. (13)\nIntuitively, if we observe the uniform ADD shown in Figure 8b, we see that the result of an evaluation must be a multiple of 5. For example, to evaluate to 10, the evaluation path must pass a high edge exactly twice. Therefore, in a 3-node ADD with root node nR, the result of count10(nR) will be exactly ( 3 2 ) . Special Operations on ADD\u2019s. Given an ADD with node set N , we define two operations that will become useful later on when constructing diagrams for our specific scenario:\n1. Variable restriction, denoted as N [ai \u2190 V ], which restricts the domain of variables A by forcing the variable ai to be assigned the value V . This operation removes every node n \u2208 N where a(n) = ai and rewires all incoming edges to point to the node\u2019s high or low child, depending on whether V = 1 or V = 0. The resulting diagram will have between 1 and diam(N ) nodes less than the original diagram, depending on the number of nodes associated with variable ai. 2. Diagram summation, denoted as N1 + N2, where N1 and N2 are two ADD\u2019s over the same (ordered) set of variables A. ordered in the same way. It starts from the respective root nodes n1 and n2, and produces a new node n := n1 + n2. We then apply the same operation to child nodes. Therefore, cL(n1 + n2) := cL(n1) + cL(n2) and cH(n1 + n2) := cH(n1) + cH(n2). Also, for the increments, we can define wL(n1 +n2) := wL(n1)+wL(n2) and wH(n1 +n2) := wH(n1)+wH(n2). The size of the resulting diagram is bounded by O(|A|\u00b7diam(N1)\u00b7diam(N2)). A proof of this claim is presented in subsection E.2.\nAlgorithm 1 Compiling a provenance-tracked dataset into ADD. 1: function COMPILEADD 2: inputs 3: D, provenance-tracked dataset; 4: A, set of variables; 5: ti, boundary tuple; 6: tval, validation tuple; 7: outputs 8: N , nodes of the compiled ADD; 9: begin 10: N \u2190 {} 11: P \u2190 {(a1, a2) \u2208 A : \u2203ti \u2208 D, a1 \u2208 p(ti) \u2227 a2 \u2208 p(ti)} 12: AL \u2190 GETLEAFVARIABLES(P) 13: for AC \u2208 GETCONNECTEDCOMPONENTS(P) do 14: A\u2032 \u2190 AC \\AL 15: N \u2032 \u2190 CONSTRUCTADDTREE(A\u2032) 16: D\u2032 \u2190 {t\u2032 \u2208 D : p(t\u2032) \u222aAC \u0338= \u2205} 17: for v \u2208 VA\u2032 do 18: NC \u2190 CONSTRUCTADDCHAIN(AC \u2229AL) 19: for n \u2208 NC do 20: v\u2032 \u2190 v \u222a {a(n)\u2192 1} 21: wH(n)\u2190 |{t\u2032 \u2208 D\u2032 : evalv\u2032p(t\u2032) = 1 \u2227 \u03c3(t\u2032, tval) \u2265 \u03c3(ti, tval)}| 22: end for 23: N \u2032 \u2190 APPENDTOADDPATH(N \u2032, NC , v) 24: end for 25: N \u2190 APPENDTOADDROOT(N , N \u2032) 26: end for 27: for a\u2032 \u2208 p(t) do 28: for n \u2208 N where a(n) = a\u2032 do 29: wL(n)\u2190\u221e 30: end for 31: end for 32: return N 33: end function"
        },
        {
            "heading": "D CONSTRUCTING POLYNOMIAL-SIZE ADD\u2019S FOR ML PIPELINES",
            "text": "Algorithm 1 presents our main procedure COMPILEADD that constructs an ADD for a given dataset D made up of tuples annotated with provenance polynomials. This is achieved by invoking the procedure COMPILEADD(D, A, ti, tval) constructs an ADD with node set N \u2032 that computes\n\u03d5(v | ti, tval, t\u2032) := {\u221e, if t\u2032 \u0338\u2208 D[v]), tally(D[v] | t\u2032, tval), otherwise.\n(14)\nWe provide a more detailed description of Algorithm 1 in subsection D.1. To construct the function defined in Equation 8, we need to invoke COMPILEADD once more by passing t\u2032\u2032 instead of t\u2032 in order to obtain another diagram N \u2032\u2032. The final diagram is obtained as a result of N \u2032[ai \u2190 0] +N \u2032\u2032[ai \u2190 1]. In other words, by performing a diagram summation operation over diagrams N \u2032 (with variable restriction ai \u2190 0) and N \u2032\u2032 (with variable restriction ai \u2190 1). The size of the resulting diagram will still be bounded by O(|D|). We can now examine different types of canonical pipelines and see how their structures are reflected onto the structure of ADD\u2019s. In summary, we can construct an ADD with polynomial-size for canonical pipelines, and therefore, by Theorem 4.2, the computation of the corresponding counting oracles is in PTIME. One-to-Many Join Pipeline. In a star database schema, this corresponds to a join between a fact table and a dimension table, where each tuple from the dimension table can be joined with multiple tuples from the fact table. It can be represented by an ADD similar to the one in Figure 8a.\nCorollary D.1. For the K-NN accuracy utility and a one-to-many join pipeline, which takes as input two datasets, DF and DD, of total size |DF |+ |DD| = N and outputs a joined dataset of size O(N), the Shapley value can be computed in O(N4) time.\nProof. This follows from the observation that in Algorithm 1, each connected component AC will be made up of one variable corresponding to the dimension table and one or more variables corresponding to the fact table. Since the fact table variables will be categorized as \"leaf variables\", the expression AC \\AL in Line 14 will contain only a single element \u2013 the dimension table variable. Consequently, the ADD tree in N \u2032 will contain a single node. On the other side, the AC \u2229 AL expression will contain all fact table variables associated with that single dimension table variable. That chain will be added to the ADD tree two times for two outgoing branches of the single tree node. Hence, the ADD segment will be made up of two fact table variable chains stemming from a single dimension table variable node. There will be O(|DD|) partitions in total. Given that the fact table variables are partitioned, the cumulative size of their chains will be O(|DF |). Therefore, the total size of the ADD with all partitions joined together is bounded by O(|DD|+ |DF |) = O(N). Given fact and combining it with Theorem 4.2 we know that the counting oracle can be computed in time O(N) time. Finally, given Theorem 4.1 and the structure of Equation 6 we can observe that the counting oracle is invoked O(N3) times. As a result, we can conclude that the total complexity of computing the Shapley value is O(N4). Here, we assume that we have a precomputed table of factorials from 1 to N that allows us to compute the binomial coefficient in constant time.\nFork Pipeline. The key characteristic of a pipeline f that contains only fork or map operators is that the resulting dataset f(D) has provenance polynomials with only a single variable. This is due to the absence of joins, which are the only operator that results in provenance polynomials with a combination of variables.\nCorollary D.2. For the K-NN accuracy utility and a fork pipeline, which takes as input a dataset of size N and outputs a dataset of size M , the Shapley value can be computed in O(M2N2) time.\nProof. The key observation here is that, since all provenance polynomials contain only a single variable, there is no interdependency between them, which means that the connected components returned in Line 13 of Algorithm 1 will each contain a single variable. Therefore, the size of the resulting ADD will be O(N). Consequently, similar to the proof of the previous corollary, the counting oracle can be computed in time O(N) time. In this case, the size of the output dataset is O(M) which means that Equation 6 will invoke the oracle O(M2N) times. Therefore, the total time complexity of computing the Shapley value will be O(M2N2). Here, we assume that we have a precomputed table of factorials from 1 to N that allows us to compute the binomial coefficient in constant time.\nMap Pipeline. A map pipeline is similar to fork pipeline in the sense that every provenance polynomial contains only a single variable. However, each variable now can appear in a provenance polynomial of at most one tuple, in contrast to fork pipeline where a single variable can be associated with multiple tuples. This additional restriction results in the following corollary:\nCorollary D.3. For the K-NN accuracy utility and a map pipeline, which takes as input a dataset of size N , the Shapley value can be computed in O(N2) time.\nProof. There are two arguments we need to make which will result in the reduction of complexity compared to fork pipelines. The first argument is that given that each variable can appear in the provenance polynomial of at most one tuple, having its value set to 1 can result in either zero or one tuple contributing to the top-K tally. It will be one if that tuple is more similar than the boundary tuple t and it will be zero if it is less similar. Consequently, our ADD will have a chain structure with high-child increments being either 0 or 1. If we partition the ADD into two chains, one with all increments 1 and another with all increments 0, then we end up with two uniform ADD\u2019s. As shown in Equation 13, model counting of uniform ADD\u2019s can be achieved in constant time. The only difference here is that, since we have to account for the support size of each model, computing the oracle \u03c9(\u03b1, \u03b3\u2032, \u03b3\u2032\u2032|ti, tval, t\u2032, t\u2032\u2032) for a given \u03b1 will require us to account for different possible ways to split \u03b1 across the two ADD\u2019s. However, since the tuple t needs to be the boundary tuple, which means it is the K-th most similar, there need to be exactly K \u2212 1 variables from the ADD\nwith increments 1 that can be set to 1. This gives us a single possible distribution of \u03b1 across two ADD\u2019s. Hence, the oracle can be computed in constant time. As for the second argument, we need to make a simple observation. For map pipelines, given a boundary tuple t\u2032 and a tally vector \u03b3\u2032 corresponding to the variable ai being assigned the value 0, we know that setting this variable to 1 can introduce at most one tuple to the top-K. That could only be the single tuple associated with ai. If this tuple has a lower similarity score than t\u2032, there will be no change in the top-K. On the other side, if it has a higher similarity, then it will become part of the top-K and it will evict exactly t\u2032 from it. Hence, there is a unique tally vector \u03b3\u2032\u2032 resulting from ai being assigned the value 1. This means that instead of computing the counting oracle \u03c9(\u03b1, \u03b3\u2032, \u03b3\u2032\u2032|ti, tval, t\u2032, t\u2032\u2032), we can compute the oracle \u03c9(\u03b1, \u03b3\u2032|ti, tval, t\u2032). This means that, in Equation 6 we can eliminate the iteration over t\u2032\u2032 which saves us an order of O(N) in complexity. As a result, Equation 6 will make O(N2) invocations to the oracle which can be computed in constant time. Here, we assume that we have a precomputed table of factorials from 1 to N that allows us to compute the binomial coefficient in constant time. Hence, the final complexity of computing the Shapley value will be O(N2).\nD.1 DETAILS OF ALGORITHM 1\nIn this section, we examine the method of compiling a provenance-tracked dataset f(Dtr) that results from a pipeline f . The crux of the method is defined in Algorithm 1 which is an algorithm that takes a dataset f(Dtr) with provenance tracked over a set of variables A, a boundary tuple t\u2032 \u2208 f(Dtr) and a validation tuple tval \u2208 f(Dval). The result is an ADD that computes the following function:\n\u03d5(v | ti, tval, t\u2032) := {\u221e, if t\u2032 \u0338\u2208 f(Dtr[v]), tally(f(Dtr[v]) | t\u2032, tval), otherwise.\n(15)\nAssuming that all provenance polynomials are actually a single conjunction of variables and that the tally is always a sum over those polynomials, it tries to perform factoring by determining if there are any variables that can be isolated. This is achieved by first constructing the set of \"leaf variables\" AL (Line 12). No pair of variables in AL ever appears in the same provenance polynomial. In graph theory, this set is also known as the \"independent set\". We use a heuristic approach to construct this set that prioritizes the least frequently occurring variables and completes them in O(N) time. We then iterate over each \"connected component\" AC (Line 13) where any two variables are \"connected\" if they are ever in the same provenance polynomial. Then we get the set A\u2032 = AC \\ AL which contains variables that cannot be isolated (because they appear in polynomials in multiple tuples with multiple different variables). We form a group that will be treated as one binary vector and based on the value of that vector we would take a specific path in the tree. We thus take the group of variables and call the CONSTRUCTADDTREE function to construct an ADD tree (Line 15). Every path in this tree corresponds to one value assignment to the variables in that tree. Then, for every path we call the CONSTRUCTADDCHAIN to build a chain made up of the isolated variables and call APPENDTOADDPATH to append them to the leaf of that path (Line 23). For each variable in the chain, we also define an increment that is defined by the number of tuples that will be more similar than the boundary tuple t\u2032 and also have their provenance polynomial \"supported\" by the path. We thus construct a segment of the final ADD made up of different components. We append this segment to the final ADD using the APPENDTOADDROOT function. We don\u2019t explicitly define these functions but we illustrate their functionality in Figure 9."
        },
        {
            "heading": "E ADDITIONAL PROOFS AND DETAILS",
            "text": "E.1 PROOF OF THEOREM 4.1\nProof. This theorem can easily be proven by observing the structure of the Shapley value of a tuple ti for a single validation tuple tval, as we defined it in Equation 6:\n\u03c6(ti, tval) = 1 |A| \u2211 t\u2032,t\u2032\u2032\u2208f(Dtr) \u2211|A| \u03b1=1 (|A|\u22121 \u03b1 )\u22121 \u2211 \u03b3\u2032,\u03b3\u2032\u2032\u2208\u0393Y,K m\u2206(\u03b3 \u2032, \u03b3\u2032\u2032 | tval) \u00b7 \u03c9(\u03b1, \u03b3\u2032, \u03b3\u2032\u2032 | ti, tval, t\u2032, t\u2032\u2032).\nWe can notice that it is made up of several sums: (1) the left-most one is a sum over t\u2032, t\u2032\u2032 \u2208 f(Dtr) which for a canonical pipeline f is a set of cardinality in O(|Dtr|); (2) the next one is a sum over |A| elements which is O(|Dtr|) according to the definition of A given in subsection 3.1; and finally (3)\nN1 = CONSTRUCTADDTREE({a1, a2, a3})\nN2 = CONSTRUCTADDCHAIN({a4, a5, a6})\nthe right-most sum is over \u03b3\u2032, \u03b3\u2032\u2032 \u2208 \u0393Y,K where \u0393Y,K is the set of all |Y|-dimensional label tally vectors which can be defined as \u0393Y,K := {\u03b3 \u2208 N|Y| : K \u2265 \u2211 i \u03b3i} and can be treated as constant since it does not depend on |Dtr|. As we can see, given that all sums in \u03c6(ti, tval) are O(|Dtr|), then it is safe to conclude that if we can compute \u03c9(\u03b1, \u03b3\u2032, \u03b3\u2032\u2032 | ti, tval, t\u2032, t\u2032\u2032) in time polynomial w.r.t |Dtr|, then we can also compute \u03c6(ti, tval) in time polynomial in |Dtr|. Finally, as mentioned in , the Shapley value for a tuple ti can be computed as \u03c6(ti) = w \u00b7 \u2211 tval\u2208f(Dval) \u03c6(ti, tval) which contains a sum over O(|Dval|) elements (given that the pipeline f is canonical). Hence, we can see that the Shapley value can be computed in time polynomial in |Dtr| and |Dval|, which concludes our proof.\nE.2 PROOF OF THEOREM 4.2\nModel Counting for ADD\u2019s. We start off by proving that Equation 12 correctly performs model counting. Lemma E.1. For a given node n \u2208 N of an ADD and a given value e \u2208 E , Equation 12 correctly computes counte(n) which returns the number of assignments v \u2208 VA such that evalv(n) = e. Furthermore, when computing counte(n) for any n \u2208 N , the number of computational steps is bounded by O(|N | \u00b7 |E|).\nProof. We will prove this by induction on the structure of the recursion. (Base case.) Based on Equation 9, when n = \u22a1 we get evalv(n) = 0 for all v. Furthermore, when n = \u22a1, the set VA[a>\u03c0(a(n)) = 0] contains only one value assignment with all variables set to zero. Hence, the model count will equal to 1 only for e = 0 and it will be 0 otherwise, which is reflected in the base cases of Equation 12. (Inductive step.) Because our ADD is ordered and full, both cL(n) and cH(n) are associated with the same variable, which is the predecessor of a(n) in the permutation \u03c0. Based on this and the induction\nhypothesis, we can assume that counte\u2212wL(n)(cL(n)) = \u2223\u2223\u2223{v \u2208 VA[\u2264a(cL(n))] | evalv(cL(n)) = e\u2212 wL(n)}\u2223\u2223\u2223\ncounte\u2212wH(n)(cH(n)) = \u2223\u2223\u2223{v \u2208 VA[\u2264a(cH(n))] | evalv(cH(n)) = e\u2212 wH(n)}\u2223\u2223\u2223 (16)\nWe would like to compute counte(n) as defined in Equation 11. It computes the size of a set defined over possible value assignments to variables in A[\u2264 a(n)]. The set of value assignments can be partitioned into two distinct sets: one where a(n)\u2190 0 and one where a(n)\u2190 1. We thus obtain the following expression:\ncounte(n) := \u2223\u2223\u2223{v \u2208 VA[\u2264a(n)][a(n)\u2190 0] | evalv(n) = e}\u2223\u2223\u2223\n+ \u2223\u2223\u2223{v \u2208 VA[\u2264a(n)][a(n)\u2190 1] | evalv(n) = e}\u2223\u2223\u2223 (17)\nBased on Equation 9, we can transform the evalv(n) expressions as such: counte(n) := \u2223\u2223\u2223{v \u2208 VA[\u2264a(cL(n))] | wL(n) + evalv(cL(n)) = e}\u2223\u2223\u2223\n+ \u2223\u2223\u2223{v \u2208 VA[\u2264a(cL(n))] | wH(n) + evalv(cH(n)) = e}\u2223\u2223\u2223 (18)\nFinally, we can notice that the set size expressions are equivalent to those in Equation 16. Therefore, we can obtain the following expression:\ncounte(n) := counte\u2212wL(n)(cL(n)) + counte\u2212wH(n)(cH(n)) (19)\nwhich is exactly the recursive step in Equation 12. This concludes our inductive proof and we move onto proving the complexity bound. (Complexity.) This is trivially proven by observing that since count has two arguments, we can maintain a table of results obtained for each n \u2208 N and e \u2208 E . Therefore, we know that we will never need to perform more than O(|N | \u00b7 |E|) invocations of counte(n).\nADD Construction. Next, we prove that the size of an ADD resulting from diagram summation as defined in Appendix C is linear in the number of variables. The size of the diagram resulting from a sum of two diagrams with node sets N1 and N2 can be loosely bounded by O(|N1| \u00b7 |N2|) assuming that its nodes come from a combination of every possible pair of operand nodes. However, given the much more narrow assumptions we made in the definition of the node sum operator, we can make this bound considerably tighter. As mentioned in Appendix C, the size of any ADD with set of nodes N and variables A is bounded by O(|A| \u00b7 diam(N )). We can use this fact to prove a tighter bound on the size of an ADD resulting from a sum operation: Lemma E.2. Given two full ordered ADD\u2019s with nodes N1 and N2, noth defined over the set of variables A, the number of nodes in N1 +N2 is bounded by O(|A| \u00b7 diam(N1) \u00b7 diam(N2)).\nProof. It is sufficient to show that diam(N1 +N2) = O(diam(N1) \u00b7 diam(N2)). This is a direct consequence of the fact that for full ordered ADD\u2019s the node sum operator is defined only for nodes associated with the same variable. Since the only way to produce new nodes is by merging one node inN1 with one node inN2, and given that we can merge nodes associated with the same variable, the number of nodes associated with the same variable in the resulting ADD equals the product of the corresponding number of nodes in the constituent ADD\u2019s. Since the diameter is simply the upper bound of the number of nodes associated with any single variable, the same upper bound in the resulting ADD cannot be larger than the product of the upper bounds of constituent nodes.\nComputing the Oracle using ADD\u2019s. Finally, we prove the correctness of Theorem 4.2. Lemma E.3. Given an Additive Decision diagram with root node n(ti, tval, t\u2032, t\u2032\u2032) that computes the Boolean function \u03d5(v|ti, tval, t\u2032, t\u2032\u2032) as defined in Equation 8, the counting oracle \u03c9(\u03b1, \u03b3\u2032, \u03b3\u2032\u2032|ti, tval, t\u2032, t\u2032\u2032) defined in Equation 7 can be computed as:\n\u03c9(\u03b1, \u03b3\u2032, \u03b3\u2032\u2032|ti, tval, t\u2032, t\u2032\u2032) := count(\u03b1,\u03b3\u2032,\u03b3\u2032\u2032)(n(ti, tval, t\u2032, t\u2032\u2032)) (20)\nProof. Given a training dataset Dtr and a data preprocessing pipeline f , we have f(Dtr) as the output of that pipeline and input to an ML model. Let us define f(Dtr)[\u2265\u03c3(\u00b7,tval) t\u2032] \u2286 f(Dtr) as a set of tuples in f(Dtr) with similarity to a validation tuple tval higher or equal than that of t\u2032, formally f(Dtr)[\u2265\u03c3(\u00b7,tval) t\u2032] := {t\u2032\u2032 \u2208 f(Dtr) : \u03c3(t\u2032\u2032, tval) \u2265 \u03c3(t\u2032, tval)}. Similarly to f(Dtr), the semantics of f(Dtr)[\u2265\u03c3(\u00b7,tval) t\u2032] is also that of a set of possible candidate sets. Given a value assignment v, we can obtain f(Dtr[v])[\u2265\u03c3(\u00b7,tval) t\u2032] from f(Dtr[v]). For convenience, we also define f(Dtr)[\u2265\u03c3(\u00b7,tval) t\u2032][=\u2113 y] as a subset of f(Dtr)[\u2265\u03c3(\u00b7,tval) t\u2032] with only tuples that have label y. Given these definitions, we can define several equivalences. First, for topK we have:(\nt\u2032 = topK ( f(Dtr[v]) | tval )) \u21d0\u21d2 ( t\u2032 \u2208 f(Dtr[v]) \u2227 \u2223\u2223f(Dtr[v])[\u2265\u03c3(\u00b7,tval) t\u2032]\u2223\u2223 = K) (21) In other words, for t\u2032 to be the tuple with the K-th highest similarity in f(Dtr[v]), it needs to be a member of f(Dtr[v]) and the number of tuples with similarity greater or equal to t\u2032 has to be exactly K. Similarly, we can define the equivalence for tally(\u00b7|t\u2032, tval):(\n\u03b3\u2032 = tally ( f(Dtr[v]) | t\u2032, tval )) \u21d0\u21d2 ( \u2200y \u2208 Y, \u03b3\u2032y = \u2223\u2223f(Dtr[v])[\u2265\u03c3(\u00b7,tval) t\u2032][=\u2113 y]\u2223\u2223) (22) This is simply an expression that partitions the set f(Dtr[v])[\u2265\u03c3(\u00b7,tval) t\u2032] based on y and tallies them up. The next step is to define an equivalence for (t\u2032 = topK(f(Dtr[v]) | tval)) \u2227 (\u03b3\u2032 = tally(f(Dtr[v]) | t\u2032, tval)). We can notice that since |\u03b3\u2032| = K, if we have (\u2200y \u2208 Y, \u03b3\u2032y = |f(Dtr[v])[\u2265\u03c3(\u00b7,tval) t\u2032][=\u2113 y]|) then we can conclude that (|f(Dtr[v])[\u2265\u03c3(\u00b7,tval) t\u2032]| = K) is redundant. Hence, we can obtain:( t\u2032 = topK ( f(Dtr[v]) | tval )) \u2227 ( \u03b3\u2032 = tally ( f(Dtr[v]) | t\u2032, tval )) \u21d0\u21d2 ( t\u2032 \u2208 f(Dtr[v]) ) \u2227 ( \u2200y \u2208 Y, \u03b3y =\n\u2223\u2223f(Dtr[v])[\u2265\u03c3 t][=\u2113 y]\u2223\u2223) (23)\nAccording to Equation 22, we can reformulate the right-hand side of the above equivalence as:( t\u2032 = topK ( f(Dtr[v]) | tval )) \u2227 ( \u03b3\u2032 = tally ( f(Dtr[v]) | t\u2032, tval )) \u21d0\u21d2 ( t\u2032 \u2208 f(Dtr)[v] ) \u2227 ( \u03b3\u2032 = tally ( f(Dtr[v]) | t\u2032, tval )) (24)\nWe can construct a similar expression for t\u2032 and v[ai = 1] so we cover four out of five predicates in Equation 7. The remaining one is simply the support of the value assignment v which we will leave intact. This leaves us with the following equation for the counting oracle: \u03c9(\u03b1, \u03b3\u2032, \u03b3\u2032\u2032 | ti, tval, t\u2032, t\u2032\u2032) := \u2211\nv\u2208VA\\{ai}\n\u00b71 { \u03b1 = |supp(v)| } \u00b71 { t\u2032 = topK ( f(Dtr[v; ai \u2190 0]) | tval )} \u00b7 1 { t\u2032\u2032 = topK ( f(Dtr[v; ai \u2190 1]) | tval\n)} \u00b71 { \u03b3\u2032 = tally ( f(Dtr[v; ai \u2190 0]) | t\u2032, tval )} \u00b7 1 { \u03b3\u2032\u2032 = tally ( f(Dtr[v; ai \u2190 1]) | t\u2032\u2032, tval )} .\n(25)\nWe can use the Boolean function \u03d5(v|ti, tval, t\u2032, t\u2032\u2032) in Equation 8 to simplify the above equation. Notice that the conditions t\u2032 \u2208 f(Dtr[v; ai \u2190 0]) and t\u2032\u2032 \u2208 f(Dtr[v; ai \u2190 1]) are embedded in the definition of \u03d5(v|ti, tval, t\u2032, t\u2032\u2032) which will return\u221e if those conditions are not met. When the conditions are met, \u03d5(v|ti, tval, t\u2032, t\u2032\u2032) returns exactly the same triple (\u03b1, \u03b3\u2032, \u03b3\u2032\u2032). Therefore it is safe to replace the five indicator functions in the above formula with a single one as such:\n\u03c9(\u03b1, \u03b3\u2032, \u03b3\u2032\u2032 | ti, tval, t\u2032, t\u2032\u2032) := \u2211\nv\u2208VA\\{ai}\n1{(\u03b1, \u03b3\u2032, \u03b3\u2032\u2032) = \u03d5(v|ti, tval, t\u2032, t\u2032\u2032)} (26)\nGiven our assumption that \u03d5(v|ti, tval, t\u2032, t\u2032\u2032) can be represented by an ADD with a root node n(ti, tval, t \u2032, t\u2032\u2032), the above formula is exactly the model counting operation:\n\u03c9(\u03b1, \u03b3\u2032, \u03b3\u2032\u2032 | ti, tval, t\u2032, t\u2032\u2032) := count(\u03b1,\u03b3\u2032,\u03b3\u2032\u2032)(n(ti, tval, t\u2032, t\u2032\u2032)) (27)\nTheorem E.1. If we can represent the Boolean function \u03d5(v|ti, tval, t\u2032, t\u2032\u2032) defined in Equation 8 with an Additive Decision Diagram of size polynomial in |Dtr| and |f(Dtr)|, then we can compute the counting oracle \u03c9(\u00b7 | ti, tval, t\u2032, t\u2032\u2032) in time polynomial in |Dtr| and |f(Dtr)|.\nProof. This theorem follows from the two previously proved lemmas: Lemma E.1 and Lemma E.3. Namely, as a result of Lemma E.3 we claim that model counting of the Boolean function \u03d5(v|ti, tval, t\u2032, t\u2032\u2032) is equivalent to computing the oracle result. On top of that, as a result of Lemma E.1 we know that we can perform model counting in time linear in the size of the decision diagram. Hence, if our function \u03d5(v|ti, tval, t\u2032, t\u2032\u2032) can be represented with a decision diagram of size polynomial in the size of data, then we can conclude that computing the oracle result can be done in time polynomial in the size of data.\nE.3 DETAILS ON ADDITIVE MODEL QUALITY METRICS\nFalse Negative Rate Apart from accuracy which represents a trivial example of an additive utility, we can show how some more complex utilities happen to be additive and can therefore be decomposed according to Equation 5. As an example, we use false negative rate (FNR) which can be defined as such:\nm(Dtr,Dval) := \u2211\ntval\u2208f(Dval) 1{(A \u25e6 f(Dtr))(tval) = 0}1{y(tval) = 1} |{tval \u2208 Dval : y(tval) = 1}| . (28)\nIn the above expression we can see that the denominator only depends on Dval which means it can be interpreted as the scaling factor w. We can easily see that the expression in the numerator neatly fits the structure of Equation 5 as long as we we define mT as mT (ypred, (xval, yval)) := 1{ypred = 0}1{yval = 1}. Similarly, we are able to easily represent various other utilities, including: false positive rate, true positive rate (i.e. recall), true negative rate (i.e. specificity), etc. We describe an additional example in subsection 3.3.\nEqualized Odds Difference We show how slightly more complex utilities can also be represented as additive, with a little approximation, similar to the one described above. We will demonstrate this using the \u201cequalized odds difference\u201d utility, a measure of (un)fairness commonly used in research Hardt et al. (2016); Barocas et al. (2019) that we also use in our experiments. It can be defined as such:\nm(Dtr,Dval) := max{TPR\u2206(Dtr,Dval), FPR\u2206(Dtr,Dval)}. (29) Here, TPR\u2206 and FPR\u2206 are true positive rate difference and false positive rate difference respectively. We assume that each tuple ttr \u2208 f(Dtr) and tval \u2208 f(Dval) have some sensitive feature g (e.g. ethnicity) with values taken from some finite set {G1, G2, ...}, that allows us to partition the dataset into sensitive groups. We can define TPR\u2206 and FPR\u2206 respectively as\nTPR\u2206(Dtr,Dval) := max Gi\u2208G TPRGi(Dtr,Dval)\u2212 min Gj\u2208G TPRGj (Dtr,Dval), and\nFPR\u2206(Dtr,Dval) := max Gi\u2208G FPRGi(Dtr,Dval)\u2212 min Gj\u2208G\nFPRGj (Dtr,Dval). (30)\nFor some sensitive group Gi, we define TPRGi and FPRGi respectively as: TPRGi(Dtr,Dval) := \u2211\ntval\u2208f(Dval) 1{(A \u25e6 f(Dtr))(tval) = 1}1{y(tval) = 1}1{g(tval) = Gi} |{tval \u2208 Dval : y(tval) = 1 \u2227 g(tval) = Gi}| , and\nFPRGi(Dtr,Dval) := \u2211\ntval\u2208f(Dval) 1{(A \u25e6 f(Dtr))(tval) = 1}1{y(tval) = 0}1{g(tval) = Gi} |{tval \u2208 Dval : y(tval) = 0 \u2227 g(tval) = Gi}|\nFor a given training dataset Dtr, we can determine Equation 29 whether TPR\u2206 or FPR\u2206 is going to be the dominant metric. Similarly, given that choice, we can determine a pair of sensitive groups (Gmax, Gmin) that would end up be selected as minimal and maximal in Equation 30. Similarly to the conversion shown in subsection 3.3, we can treat these two steps as a reduce operation over the whole dataset. Then, if we assume that this intermediate result will remain stable over subsets of Dtr, we can approximatly represent the equalized odds difference utility as an additive utility. As an example, let us assume that we have determined that TPR\u2206 dominates over FPR\u2206, and similarly that the pair of sensitive groups (Gmax, Gmin) will end up being selected in Equation 30. Then, our tuple-wise utility uT and the scaling factor w become\nmT (ypred, tval) := TPRGmax,T (ypred, tval)\u2212 TPRGmin,T (ypred, tval), w := 1/|{tval \u2208 Dval : y(tval) = 1 \u2227 g(tval) = Gi}|,\nwhere\nTPRGi,T (ypred, tval) := 1{ypred = 1}1{y(tval) = 1}1{g(tval) = Gi}.\nA similar approach can be taken to define mT and w for the case when FPR\u2206 dominates over TPR\u2206. Then, if we plug them into Equation 5, we obtain an approximate version of the equalized odds difference utility as defined in Equation 29. This approximation relies on the stability of the choices of min and max in Equation 30 and on the choice between TPR and FPR in Equation 29 (both of which can be precomputed)."
        },
        {
            "heading": "F SPECIAL CASE: COMPUTING SHAPLEY FOR 1-NEAREST-NEIGHBOR CLASSIFIERS",
            "text": "We can significantly reduce the time complexity for 1-NN classifiers, an important special case of K-NN classifiers that is commonly used in practice. For each validation tuple tval, there is always exactly one tuple that is most similar to tval. Below we illustrate how to leverage this observation to construct the counting oracle. In the following, we assume that ai is the variable corresponding to the tuple for which we hope to compute the Shapley value. Let \u03d5t represent the event when t is the top-1 tuple:\n\u03d5t := p(t) \u2227 \u2227\nt\u2032\u2208f(Dtr) \u03c3(t\u2032)>\u03c3(t)\n\u00acp(t\u2032). (31)\nFor Equation 31 to be true (i.e. for tuple t to be the top-1), all tuples t\u2032 where \u03c3(t\u2032) > \u03c3(t) need to be absent from the pipeline output. Hence, for a given value assignment v, all provenance polynomials that control those tuples, i.e., p(t\u2032), need to evaluate to false. We now construct the event\n\u03d5t,t\u2032 := \u03d5t[ai/false] \u2227 \u03d5t\u2032 [ai/true],\nwhere \u03d5t[ai/false] means to substitute all appearances of ai in \u03d5t to false. This event happens only if if t is the top-1 tuple when ai is false and t\u2032 is the top-1 tuple when ai is true. This corresponds to the condition that our counting oracle counts models for. Expanding \u03d5t,t\u2032 , we obtain\n\u03d5t,t\u2032 := ( p(t) \u2227 \u2227 t\u2032\u2032\u2208f(Dtr) \u03c3(t\u2032\u2032)>\u03c3(t) \u00acp(t\u2032\u2032) ) [ai/false] \u2227 ( p(t\u2032) \u2227 \u2227 t\u2032\u2032\u2208f(Dtr) \u03c3(t\u2032\u2032)>\u03c3(t\u2032) \u00acp(t\u2032\u2032) ) [ai/true]. (32)\nNote that \u03d5t,t\u2032 can only be true if p(t\u2032) is true when ai is true and \u03c3(t) < \u03c3(t\u2032). As a result, all provenance polynomials corresponding to tuples with a higher similarity score than that of t need to evaluate to false. Therefore, the only polynomials that can be allowed to evaluate to true are those corresponding to tuples with lower similarity score than t. Based on these observations, we can express the counting oracle for different types of ML pipelines. Map Pipeline. In a map pipeline, the provenance polynomial for each tuple t\u2032i \u2208 f(Dtr) is defined by a single distinct variable ai \u2208 A. Furthermore, from the definition of the counting oracle (Equation 7), we can see that \u03c9(\u00b7|ti, tval, t\u2032, t\u2032\u2032) counts the value assignments that result in support size \u03b1 and label tally vectors \u03b3\u2032 and \u03b3\u2032\u2032. Given our observation about the provenance polynomials that are allowed to be set to true, we can easily construct an expression for counting valid value assignments. Namely, we have to choose exactly \u03b1 variables out of the set {t\u2032 \u2208 f(Dtr) : \u03c3(t\u2032, tval) < \u03c3(ti, tval)}, which corresponds to tuples with a lower similarity score than that of ti (measured by the similarity function \u03c3). This can be constructed using a binomial coefficient. Furthermore, when K = 1, the label tally \u03b3\u2032 is entirely determined by the top-1 tuple t\u2032. The same observation goes for \u03b3\u2032\u2032 and t\u2032\u2032. To denote this, we define a constant \u0393L parameterized by some label L. It represents a tally vector with all values 0 and only the value corresponding to label L being set to 1. We thus need to fix \u03b3\u2032 to be equal to \u0393y(t) (and the same for \u03b3\u2032\u2032). Finally, as we observed earlier, when computing \u03c9(\u00b7|ti, tval, t\u2032, t\u2032\u2032) for K = 1, the provenance polynomial of the tuple t\u2032\u2032 must equal ai. With these notions, we can define\nthe counting oracle as \u03c9(\u03b1, \u03b3\u2032, \u03b3\u2032\u2032|ti, tval, t\u2032, t\u2032\u2032) = (|{t\u2032\u2032\u2032 \u2208 f(Dtr) : \u03c3(t\u2032\u2032\u2032, tval) < \u03c3(ti, tval)}|\n\u03b1\n) 1{p(t\u2032\u2032) = ai}\n1{\u03b3\u2032 = \u0393y(t\u2032)} 1{\u03b3\u2032\u2032 = \u0393y(t\u2032\u2032)}.\n(33) Note that we always assume ( a b ) = 0 for all a < b. Given this, we can prove the following corollary about map pipelines: Corollary F.1. For the 1-NN accuracy utility and a map pipeline, which takes as input a dataset of size N , the Shapley value can be computed in O(N logN) time.\nProof. We start off by plugging in the oracle definition from Equation 33 into the Shapley value computation Equation 6:\n\u03c6(ti, tval) = 1\nN \u2211 t\u2032,t\u2032\u2032\u2208f(Dtr) N\u2211 \u03b1=1 ( N \u2212 1 \u03b1 )\u22121 \u2211 \u03b3\u2032,\u03b3\u2032\u2032\u2208\u0393Y,K\nm\u2206(\u03b3 \u2032, \u03b3\u2032\u2032|tval)(|{t\u2032\u2032\u2032 \u2208 f(Dtr) : \u03c3(t\u2032\u2032\u2032, tval) < \u03c3(ti, tval)}|\n\u03b1 ) 1{p(t\u2032\u2032) = ai} 1{\u03b3\u2032 = \u0393y(t\u2032)} 1{\u03b3\u2032\u2032 = \u0393y(t\u2032\u2032)}\n(34)\nAs we can see, the oracle imposes hard constraints on the tuple t\u2032\u2032 and tally vectors \u03b3\u2032 and \u03b3\u2032\u2032. We will replace the tally vectors with their respective constants and the tuple t\u2032\u2032 we will denote as ti because it is the only tuple associated with ai. Because of this, we can remove the sums that iterate over them:\n\u03c6(ti, tval) = 1\nN \u2211 t\u2032\u2208f(Dtr) N\u2211 \u03b1=1 ( N \u2212 1 \u03b1 )\u22121 m\u2206(\u0393y(t\u2032),\u0393y(ti) | tval) (|{t\u2032\u2032\u2032 \u2208 f(Dtr) : \u03c3(t\u2032\u2032\u2032, tval) < \u03c3(ti, tval)}| \u03b1 ) (35)\nWe could significantly simplify this equation by assuming the tuples in f(D) are sorted by decreasing similarity. We then obtain:\n\u03c6(ti, tval) = 1\nN N\u2211 j=i N\u2211 \u03b1=1 ( N \u2212 1 \u03b1 )\u22121 m\u2206(\u0393y(tj),\u0393y(ti) | tval) ( N \u2212 j \u03b1 ) (36)\nWe shuffle the sums a little by multiplying 1N with ( N\u22121 \u03b1 )\u22121 and we expand m\u2206 based on its definition in subsection E.1. We also alter the limit of the innermost sum because \u03b1 \u2264 N \u2212 j. Thus, we obtain:\n\u03c6(ti, tval) = N\u2211 j=i ( mT (y(ti), tval)\u2212mT (y(tj), tval) )N\u2212j\u2211 \u03b1=1 ( N \u03b1 )\u22121( N \u2212 j \u03b1 ) (37)\nThe innermost sum in the above equation can be simplified by applying the so-called Hockey-stick identity Ross (1997). Specifically, ( N \u03b1 )\u22121(N\u2212j \u03b1 ) becomes ( N j )\u22121(N\u2212\u03b1 j ) . Then, \u2211N\u2212j \u03b1=1 ( N j )\u22121(N\u2212\u03b1 j ) becomes ( N j )\u22121( N j+1 ) . Finally, we obtain the following formula:\n\u03c6(ti, tval) = N\u2211 j=i ( mT (y(ti), tval)\u2212mT (y(tj), tval) )(N \u2212 j j + 1 ) (38)\nAs we can see, the above formula can be computed in O(N) iterations. Therefore, given that we still need to sort the dataset beforehand, the overall complexity of the entire Shapley value amounts to O(N logN). Here, we assume that we have a precomputed table of factorials from 1 to N that allows us to compute the binomial coefficient in constant time.\nComputing the Shapley Value for the Entire Training Dataset. Equation 38 represents a method for computing the Shapley value for a single data example ti. When computing the Shapley value for every tuple in a training dataset, given that the tuples are sorted according to similarity to tval, we can notice that the sum in Equation 38 exhibits the following recursive structure:\n\u03c6(ti, tval) = \u03c6(ti+1, tval) + ( mT (y(ti), tval)\u2212mT (y(tj), tval) )(N \u2212 i i+ 1 ) If we take advantage of the above recursive structure, we can see that it is possible to compute the Shapley value for all data examples in a single pass that takes O(N) time. Hence, since the overall computation will still be dominated by the sorting procedure, the time to compute the Shapley value for all training tuples with respect to a single validation tuple tval is O(N logN). Fork Pipeline. As we noted, both map and fork pipelines result in polynomials made up of only one variable. The difference is that in map pipeline each variable is associated with at most one polynomial, whereas in fork pipelines it can be associated with multiple polynomials. However, for 1-NN classifiers, this difference vanishes when it comes to Shapley value computation: Corollary F.2. For the 1-NN accuracy utility and a fork pipeline, which takes as input a dataset of size N , the Shapley value can be computed in O(N logN) time.\nProof. We will prove this by reducing the problem of Shapley value computation in fork pipelines to the one of computing it for map pipelines. Let us have two tuples t\u2032j,1, t \u2032 j,2 \u2208 f(D), both associated with some variable aj \u2208 A. That means that p(t\u2032j,1) = p(t\u2032j,2). If we examine Equation 31, we notice that it will surely evaluate to false if either \u03c3(t\u2032j,1) > \u03c3(t) or \u03c3(t \u2032 j,2) > \u03c3(t). The same observation holds for Equation 32. Without loss of generality, assume \u03c3(tj,1) > \u03c3(tj,2). Then, \u03c3(tj,1) > \u03c3(t) implies \u03c3(tj,2) > \u03c3(t). As a result, we only ever need to check the former condition without paying attention to the latter. The outcome of this is that for all sets of tuples associated with the same variable, it is safe to ignore all of them except the one with the highest similarity score, and we will nevertheless obtain the same oracle result. Since we transformed the problem to one where for each variable we have to consider only a single associated tuple, we have effectively reduced the problem to the one of computing Shapley value for map pipelines. Consequently, we can apply the same algorithm and will end up with the same time complexity."
        },
        {
            "heading": "G DETAILS ABOUT THE EXPERIMENTAL PROTOCOL AND ADDITIONAL EVALUATION RESULTS",
            "text": "Hardware and Platform. All experiments were conducted on an AMD EPYC 7742 2.25GHz CPU. We ran each experiment in single-thread mode. All deep learning models were running on an NVIDIA A100 GPU. Datasets. We assemble a collection of widely used datasets with diverse modalities (i.e. tabular, textual, and image datasets). Table 2 summarizes the datasets that we used. In each experiment, we subsample the dataset to 1K training data examples by using different random seeds.\nModels. We use three downstream ML models following the previous feature extraction pipelines: XGBoost, LogisticRegression, and KNearestNeighbor. We use the LogisticRegression and KNeighborsClassifier provided by the sklearn package. We set max_iter to 5,000 for LogisticRegression and set n_neighbors to 1 for KNearestNeighbor.\n0% 50% 100% Portion of Labels Examined\n0.4\n0.6\n0.8\nA cc\nur ac\ny\n100\n102\nC om\npu te\nT im\ne [s\n]\n0.05\n4.582.88\n38.93\n2.70\n37.95\nDataset: UCI Adult; Pipeline: Identity; Model: Logistic Regression\n0% 50% 100% Portion of Labels Examined\n0.4\n0.6\n0.8\nA cc\nur ac\ny\n10\u22121\n100\n101\nC om\npu te\nT im\ne [s\n]\n0.05\n4.92 1.70\n15.91\n1.17\n13.29\nDataset: UCI Adult; Pipeline: Standard Scaler; Model: Logistic Regression\n0% 50% 100% Portion of Labels Examined\n0.4\n0.6\n0.8\nA cc\nur ac\ny\n10\u22121\n100\n101\nC om\npu te\nT im\ne [s\n]\n0.05\n5.26 2.24\n20.34\n1.18\n12.87\nDataset: UCI Adult; Pipeline: Logarithmic Scaler; Model: Logistic Regression\n0% 50% 100% Portion of Labels Examined\n0.4\n0.6\n0.8\nA cc\nur ac\ny\n100\n102\nC om\npu te\nT im\ne [s ] 0.05 5.02 2.34 25.02 1.62 18.01\nDataset: UCI Adult; Pipeline: PCA; Model: Logistic Regression\n0% 50% 100% Portion of Labels Examined\n0.5\n0.6\n0.7\n0.8\nA cc\nur ac\ny\n100\n102\nC om\npu te\nT im\ne [s\n]\n0.14\n8.347.68\n182.10\n1.11\n17.86\nDataset: UCI Adult; Pipeline: Missing Indicator + K-Means; Model: Logistic Regression\n0% 50% 100% Portion of Labels Examined\n0.4\n0.6\n0.8\nA cc\nur ac\ny\n101\n103\nC om\npu te\nT im\ne [s\n]\n5.36\n538.67 221.21\n1947.04\n7.55\n63.80\nDataset: 20NewsGroups; Pipeline: TF-IDF; Model: Logistic Regression\n0% 50% 100% Portion of Labels Examined\n0.4\n0.6\n0.8\nA cc\nur ac\ny\n101\n103\nC om\npu te\nT im\ne [s\n]\n5.37\n576.43 284.51\n3061.53\n6.76\n53.48\nDataset: 20NewsGroups; Pipeline: ToLower + URLRemove + TF-IDF; Model: Logistic Regression\n0% 50% 100% Portion of Labels Examined\n0.6\n0.8\nA cc\nur ac\ny\n101\n103\nC om\npu te\nT im\ne [s\n]\n0.45\n46.31 20.19\n94.01\n19.35\n77.01\nDataset: FashionMNIST; Pipeline: Gaussian Blur; Model: Logistic Regression\n0% 50% 100% Portion of Labels Examined\n0.4\n0.6\n0.8\nA cc\nur ac\ny\n101\n103\nC om\npu te\nT im\ne [s\n]\n0.40\n39.98 90.29\n1165.40\n1.82\n21.87\nDataset: FashionMNIST; Pipeline: Histogram of Oriented Gradients; Model: Logistic Regression\n0% 50% 100% Portion of Labels Examined\n0.65\n0.70\n0.75\nA cc\nur ac\ny\n100\n101\n102\nC om\npu te\nT im\ne [s\n]\n1.10\n105.48\n3.05\n18.71\n2.01\n14.06\nDataset: DataPerf Vision; Pipeline: PCA; Model: Logistic Regression\n0% 50% 100% Portion of Labels Examined\n0.6\n0.7 0.8 A cc ur ac y\n100\n101\n102\nC om\npu te\nT im\ne [s\n]\n1.13\n103.66\n24.68\n255.41\n1.35\n4.63\nDataset: DataPerf Vision; Pipeline: Missing Indicator + K-Means; Model: Logistic Regression\n0% 50% 100% Portion of Labels Examined\n0.30\n0.35\n0.40\n0.45\nA cc\nur ac\ny\n101\n103\nC om\npu te\nT im\ne [s\n]\n1.09\n108.23\n2546.86\n23353.28\n323.10\n2854.83\nDataset: CifarN; Pipeline: Histogram of Oriented Gradients; Model: Logistic Regression\nRandom TMC x10 TMC x100 Canonpipe TMC x10 Canonpipe TMC x100 Canonpipe KNN Canonpipe KNN Interactive\nFigure 10: Label Repair experiment results over various combinations of datasets (1k samples) and map pipelines. We optimize for accuracy. The model is logistic regression.\nProtocol. We conduct a series of experimental runs that simulate a real-world importance-driven data debugging workflow. In each experimental run, we select a dataset, pipeline, target model, and data repair method. If a dataset does not already have human-generated label errors, we follow the protocol of Li et al. (2021) and Jia et al. (2021) and artificially inject 50% of label noise. Label noise injection is performed by selecting a random subset representing 50% of training data examples, and replace the original label with some other valid label in a given dataset. Given the selected data repair method, we compute the importance using a validation dataset. We use this computed iportance to sort the training dataset. Data repairs will be conducted using this sorting order. If the repair method is random, the data is sorted randomly. We divide the range between 0% data examined and 100% data examined into 100 checkpoints. Specifically, at each checkpoint, we select the next batch out of a 100 batches of data examples ordered based on the importance-based sorting order. We repair the labels in the given batch and we measure the quality of the given target model on a separate test dataset using some metric (e.g. accuracy). We also measure the time spent on computing importance scores. At any given checkpoint, the label effort represents the portion of data that was covered in all batches that were processed up to that checkpoint. We repeat each experiment 10 times with different random seeds and report the median as well as the 90-th percentile range (either shaded or with error bars).\nG.1 ADDITIONAL LABEL REPAIR EXPERIMENTS\nWe present the results of an array of experiments that were conducted for the label repair scenario. See section 5 for details on the experimental protocol. See Figure 10 to Figure 14 for experiments where we focus on improving accuracy. See Figure 15 to Figure 19 for experiments that explore the tradeoff between accuracy and fairness. Finally, in Figure 20 we show more results for the label repair experiments over deep learning embedding models for image and text data. Note about Fork Variants: We create a \u201cfork\u201d version of the above pipelines, by prepending each with a DataProvider operator. It simulates distinct data providers, each providing a portion of the data. The original dataset is split into a given number of groups (100 in our experiments). We compute the importance of each group, and we conduct data repairs on entire groups all at once.\nDataset: UCI Adult; Pipeline: Identity; Model: K-Nearest Neighbor\nDataset: UCI Adult; Pipeline: Standard Scaler; Model: K-Nearest Neighbor\nDataset: UCI Adult; Pipeline: Logarithmic Scaler; Model: K-Nearest Neighbor\nDataset: UCI Adult; Pipeline: Identity; Model: Logistic Regression\nDataset: UCI Adult; Pipeline: Standard Scaler; Model: Logistic Regression\nDataset: UCI Adult; Pipeline: Logarithmic Scaler; Model: Logistic Regression\nDataset: UCI Adult; Pipeline: Identity; Model: K-Nearest Neighbor\nDataset: UCI Adult; Pipeline: Standard Scaler; Model: K-Nearest Neighbor\nDataset: UCI Adult; Pipeline: Logarithmic Scaler; Model: K-Nearest Neighbor\nDataset: UCI Adult; Pipeline: Identity; Model: XGBoost\nDataset: UCI Adult; Pipeline: Standard Scaler; Model: XGBoost\nDataset: UCI Adult; Pipeline: Logarithmic Scaler; Model: XGBoost\nG.2 ADDITIONAL SCALABILITY EXPERIMENTS\nWe provide results of additional experiments where we attempt to measure the trends of both the label repair efficiency and compute time, as a function of dataset size. To achieve this, instead of evaluating on synthetic data, we evaluate on CIFAR-N, a real-world dataset with human-generated label noise (Figure 23). We use logistic regression as a target model and the HOG transform pipeline for feature extraction. We keep the training and test set size to 5K data exaples and we vary the training set size from 1K to 32K. We can notice that for training set of size 32K, the TMC method requires around 1 day to complete with 10 Monte Carlo iterations and around 10 days with 100 iterations. At the same time we can notice that the KNN approximation is able to complete in a matter of minutes.\n0% 50% 100% Portion of Labels Examined\n0.6\n0.8\nA cc\nur ac\ny\n102\n1.2\u00d7 102\n1.4\u00d7 102\n1.6\u00d7 102\n1.8\u00d7 102 2\u00d7 102\n2.2\u00d7 102\nC om\npu te\nT im\ne [s\n]\n107.24\n127.53\n188.46\nDataset: FashionMNIST; Pipeline: ResNet-18 Embedding; Model: K-Nearest Neighbor\n0% 50% 100% Portion of Labels Examined\n0.6\n0.8\nA cc\nur ac\ny\n102\n2\u00d7 102\n3\u00d7 102\nC om\npu te\nT im\ne [s\n]\n105.86\n142.04\n214.86\nDataset: FashionMNIST; Pipeline: ResNet-18 Embedding; Model: Logistic Regression\n0% 50% 100% Portion of Labels Examined\n0.6\n0.8\nA cc\nur ac\ny\n102\n103\n104\nC om\npu te\nT im\ne [s\n]\n108.99 150.89 159.04\nDataset: FashionMNIST; Pipeline: ResNet-18 Embedding; Model: XGBoost\nDataset: 20NewsGroups; Pipeline: MiniLM Embedding; Model: K-Nearest Neighbor\nDataset: 20NewsGroups; Pipeline: MiniLM Embedding; Model: Logistic Regression\nDataset: 20NewsGroups; Pipeline: MiniLM Embedding; Model: XGBoost\n1k 3k 10k 32k Training Set Size\n20%\n40%\nDa ta\nR ep\nai re\nd to\nG et\n5 0%\nof M\nax im\num A\ncc ur\nac y\n1k 3k 10k 32k Training Set Size\n103\n106\nCo m\npu te\nT im\ne [s\n]\nDataset: CIFAR-N; Pipeline: Histogram of Oriented Gradients Target Model: Logistic Regression; Validation / Test Set Size: 5K\nRandom Canonpipe TMC x10 Canonpipe TMC x100 Canonpipe KNN\nFigure 23: Evaluating how the label repair efficiency and compute time of Canonpipe scale as a function of dataset size. On the left-hand side we show how many data examples need to be repaired in order to recover 1/2 of the maximum possible accuracy on the given dataset. We can notice that the KNN approximation is able to consistently achieve comparative label repair efficiency with orders of magnitude less compute time."
        }
    ],
    "year": 2023
}