{
    "abstractText": "Manifold-valued measurements exist in numerous applications within computer vision and machine learning. Recent studies have extended Deep Neural Networks (DNNs) to manifolds, and concomitantly, normalization techniques have also been adapted to several manifolds, referred to as Riemannian normalization. Nonetheless, most of the existing Riemannian normalization methods have been derived in an ad hoc manner and only apply to specific manifolds. This paper establishes a unified framework for Riemannian Batch Normalization (RBN) techniques on Lie groups. Our framework offers the theoretical guarantee of controlling both the Riemannian mean and variance. Empirically, we focus on Symmetric Positive Definite (SPD) manifolds, which possess three distinct types of Lie group structures. Using the deformation concept, we generalize the existing Lie groups on SPD manifolds into three families of parameterized Lie groups. Specific normalization layers induced by these Lie groups are then proposed for SPD neural networks. We demonstrate the effectiveness of our approach through three sets of experiments: radar recognition, human action recognition, and electroencephalography (EEG) classification. The code is available at https://github.com/GitZH-Chen/LieBN.git.",
    "authors": [
        {
            "affiliations": [],
            "name": "Ziheng Chen"
        },
        {
            "affiliations": [],
            "name": "Yue Song"
        },
        {
            "affiliations": [],
            "name": "Yunmei Liu"
        },
        {
            "affiliations": [],
            "name": "Nicu Sebe"
        }
    ],
    "id": "SP:a05b2d1eaa1f4849e5cd17123af7775a13d432c8",
    "references": [
        {
            "authors": [
                "Vincent Arsigny",
                "Pierre Fillard",
                "Xavier Pennec",
                "Nicholas Ayache"
            ],
            "title": "Fast and simple computations on tensors with log-Euclidean metrics",
            "venue": "PhD thesis,",
            "year": 2005
        },
        {
            "authors": [
                "Fr\u00e9d\u00e9ric Barbaresco"
            ],
            "title": "Gaussian distributions on the space of symmetric positive definite matrices from souriau\u2019s gibbs state for siegel domains by coadjoint orbit and moment map",
            "venue": "In Geometric Science of Information: 5th International Conference,",
            "year": 2021
        },
        {
            "authors": [
                "Gary B\u00e9cigneul",
                "Octavian-Eugen Ganea"
            ],
            "title": "Riemannian adaptive optimization methods",
            "venue": "arXiv preprint arXiv:1810.00760,",
            "year": 2018
        },
        {
            "authors": [
                "Marcel Berger"
            ],
            "title": "A panoramic view of Riemannian geometry",
            "venue": "URL https://doi",
            "year": 2003
        },
        {
            "authors": [
                "Rajendra Bhatia"
            ],
            "title": "Matrix analysis, volume 169",
            "venue": "Springer Science & Business Media,",
            "year": 2013
        },
        {
            "authors": [
                "Victoria Bloom",
                "Dimitrios Makris",
                "Vasileios Argyriou"
            ],
            "title": "G3D: A gaming action dataset and real time action recognition evaluation framework",
            "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops,",
            "year": 2012
        },
        {
            "authors": [
                "Michael M Bronstein",
                "Joan Bruna",
                "Yann LeCun",
                "Arthur Szlam",
                "Pierre Vandergheynst"
            ],
            "title": "Geometric deep learning: going beyond Euclidean data",
            "venue": "IEEE Signal Processing Magazine,",
            "year": 2017
        },
        {
            "authors": [
                "Daniel Brooks"
            ],
            "title": "Deep Learning and Information Geometry for Time-Series Classification",
            "venue": "PhD thesis, Sorbonne universite\u0301,",
            "year": 2020
        },
        {
            "authors": [
                "Daniel Brooks",
                "Olivier Schwander",
                "Fr\u00e9d\u00e9ric Barbaresco",
                "Jean-Yves Schneider",
                "Matthieu Cord"
            ],
            "title": "A hermitian positive definite neural network for micro-doppler complex covariance processing",
            "venue": "In 2019 International Radar Conference (RADAR),",
            "year": 2019
        },
        {
            "authors": [
                "Daniel Brooks",
                "Olivier Schwander",
                "Fr\u00e9d\u00e9ric Barbaresco",
                "Jean-Yves Schneider",
                "Matthieu Cord"
            ],
            "title": "Riemannian batch normalization for SPD neural networks",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Daniel Brooks",
                "Olivier Schwander",
                "Fr\u00e9d\u00e9ric Barbaresco",
                "Jean-Yves Schneider",
                "Matthieu Cord"
            ],
            "title": "Second-order networks in pytorch",
            "venue": "In Geometric Science of Information: 4th International Conference,",
            "year": 2019
        },
        {
            "authors": [
                "Daniel Brooks",
                "Olivier Schwander",
                "Fr\u00e9d\u00e9ric Barbaresco",
                "Jean-Yves Schneider",
                "Matthieu Cord"
            ],
            "title": "Deep learning and information geometry for drone micro-doppler radar classification",
            "venue": "IEEE Radar Conference",
            "year": 2020
        },
        {
            "authors": [
                "Daniel A Brooks",
                "Olivier Schwander",
                "Fr\u00e9d\u00e9ric Barbaresco",
                "Jean-Yves Schneider",
                "Matthieu Cord"
            ],
            "title": "Complex-valued neural networks for fully-temporal micro-doppler classification",
            "venue": "In 2019 20th International Radar Symposium (IRS), pp. 1\u201310",
            "year": 2019
        },
        {
            "authors": [
                "Daniel A Brooks",
                "Olivier Schwander",
                "Fr\u00e9d\u00e9ric Barbaresco",
                "Jean-Yves Schneider",
                "Matthieu Cord"
            ],
            "title": "Exploring complex time-series representations for riemannian machine learning of radar data",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2019
        },
        {
            "authors": [
                "Rudrasis Chakraborty"
            ],
            "title": "ManifoldNorm: Extending normalizations on Riemannian manifolds",
            "venue": "arXiv preprint arXiv:2003.13869,",
            "year": 2020
        },
        {
            "authors": [
                "Rudrasis Chakraborty",
                "Baba C Vemuri"
            ],
            "title": "Statistics on the Stiefel manifold: theory and applications",
            "venue": "The Annals of Statistics,",
            "year": 2019
        },
        {
            "authors": [
                "Rudrasis Chakraborty",
                "Chun-Hao Yang",
                "Xingjian Zhen",
                "Monami Banerjee",
                "Derek Archer",
                "David Vaillancourt",
                "Vikas Singh",
                "Baba Vemuri"
            ],
            "title": "A statistical recurrent model on the manifold of symmetric positive definite matrices",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Rudrasis Chakraborty",
                "Jose Bouza",
                "Jonathan Manton",
                "Baba C Vemuri"
            ],
            "title": "Manifoldnet: A deep neural network for manifold-valued data with applications",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Ziheng Chen",
                "Yue Song",
                "Gaowen Liu",
                "Ramana Rao Kompella",
                "Xiaojun Wu",
                "Nicu Sebe"
            ],
            "title": "Riemannian multiclass logistics regression for SPD neural networks",
            "venue": "arXiv preprint arXiv:2305.11288,",
            "year": 2023
        },
        {
            "authors": [
                "Ziheng Chen",
                "Tianyang Xu",
                "Zhiwu Huang",
                "Yue Song",
                "Xiao-Jun Wu",
                "Nicu Sebe"
            ],
            "title": "Adaptive Riemannian metrics on SPD manifolds",
            "venue": "arXiv preprint arXiv:2303.15477,",
            "year": 2023
        },
        {
            "authors": [
                "Ziheng Chen",
                "Tianyang Xu",
                "Xiao-Jun Wu",
                "Rui Wang",
                "Zhiwu Huang",
                "Josef Kittler"
            ],
            "title": "Riemannian local mechanism for SPD neural networks",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2023
        },
        {
            "authors": [
                "Octavian Ganea",
                "Gary B\u00e9cigneul",
                "Thomas Hofmann"
            ],
            "title": "Hyperbolic neural networks",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Guillermo Garcia-Hernando",
                "Shanxin Yuan",
                "Seungryul Baek",
                "Tae-Kyun Kim"
            ],
            "title": "First-person hand action benchmark with RGB-D videos and 3D hand pose annotations",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Alexandre Gramfort"
            ],
            "title": "MEG and EEG data analysis with MNE-Python",
            "venue": "Frontiers in Neuroscience,",
            "year": 2013
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Marcel F. Hinss",
                "Ludovic Darmet",
                "Bertille Somon",
                "Emilie Jahanpour",
                "Fabien Lotte",
                "Simon Ladouce",
                "Rapha\u00eblle N. Roy"
            ],
            "title": "An EEG dataset for cross-session mental workload estimation: Passive BCI competition of the Neuroergonomics Conference",
            "year": 2021
        },
        {
            "authors": [
                "Sepp Hochreiter",
                "J\u00fcrgen Schmidhuber"
            ],
            "title": "Long short-term memory",
            "venue": "Neural Computation,",
            "year": 1997
        },
        {
            "authors": [
                "Zhiwu Huang",
                "Luc Van Gool"
            ],
            "title": "A Riemannian network for SPD matrix learning",
            "venue": "In Thirty-first AAAI Conference on Artificial Intelligence,",
            "year": 2017
        },
        {
            "authors": [
                "Zhiwu Huang",
                "Chengde Wan",
                "Thomas Probst",
                "Luc Van Gool"
            ],
            "title": "Deep learning on Lie groups for skeleton-based action recognition",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Zhiwu Huang",
                "Jiqing Wu",
                "Luc Van Gool"
            ],
            "title": "Building deep networks on Grassmann manifolds",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2018
        },
        {
            "authors": [
                "Sergey Ioffe",
                "Christian Szegedy"
            ],
            "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
            "venue": "In International conference on machine learning,",
            "year": 2015
        },
        {
            "authors": [
                "Catalin Ionescu",
                "Orestis Vantzos",
                "Cristian Sminchisescu"
            ],
            "title": "Matrix backpropagation for deep networks with structured layers",
            "venue": "In Proceedings of the IEEE International Conference on Computer Vision, pp",
            "year": 2015
        },
        {
            "authors": [
                "Vinay Jayaram",
                "Alexandre Barachant"
            ],
            "title": "MOABB: trustworthy algorithm benchmarking for BCIs",
            "venue": "Journal of Neural Engineering,",
            "year": 2018
        },
        {
            "authors": [
                "Hermann Karcher"
            ],
            "title": "Riemannian center of mass and mollifier smoothing",
            "venue": "Communications on Pure and Applied Mathematics,",
            "year": 1977
        },
        {
            "authors": [
                "Reinmar Kobler",
                "Jun-ichiro Hirayama",
                "Qibin Zhao",
                "Motoaki Kawanabe"
            ],
            "title": "SPD domain-specific batch normalization to crack interpretable unsupervised domain adaptation in EEG",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Reinmar J Kobler",
                "Jun-ichiro Hirayama",
                "Motoaki Kawanabe"
            ],
            "title": "Controlling the Fr\u00e9chet variance improves batch normalization on the symmetric positive definite manifold",
            "venue": "In ICASSP 20222022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2022
        },
        {
            "authors": [
                "Alex Krizhevsky",
                "Ilya Sutskever",
                "Geoffrey E Hinton"
            ],
            "title": "Imagenet classification with deep convolutional neural networks",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2012
        },
        {
            "authors": [
                "Zhenhua Lin"
            ],
            "title": "Riemannian geometry of symmetric positive definite matrices via Cholesky decomposition",
            "venue": "SIAM Journal on Matrix Analysis and Applications,",
            "year": 2019
        },
        {
            "authors": [
                "Aaron Lou",
                "Isay Katsman",
                "Qingxuan Jiang",
                "Serge Belongie",
                "Ser-Nam Lim",
                "Christopher De Sa"
            ],
            "title": "Differentiating through the Fr\u00e9chet mean",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Jonathan H Manton"
            ],
            "title": "A globally convergent numerical algorithm for computing the centre of mass on compact lie groups",
            "venue": "ICARCV",
            "year": 2004
        },
        {
            "authors": [
                "Meinard M\u00fcller",
                "Tido R\u00f6der",
                "Michael Clausen",
                "Bernhard Eberhardt",
                "Bj\u00f6rn Kr\u00fcger",
                "Andreas Weber"
            ],
            "title": "Documentation mocap database HDM05",
            "venue": "Technical report, Universita\u0308t Bonn,",
            "year": 2007
        },
        {
            "authors": [
                "Iain Murray"
            ],
            "title": "Differentiation of the Cholesky decomposition",
            "venue": "arXiv preprint arXiv:1602.07527,",
            "year": 2016
        },
        {
            "authors": [
                "Richard M Murray",
                "Zexiang Li",
                "S Shankar Sastry"
            ],
            "title": "A mathematical introduction to robotic manipulation",
            "venue": "CRC press,",
            "year": 2017
        },
        {
            "authors": [
                "Xuan Son Nguyen"
            ],
            "title": "The Gyro-structure of some matrix manifolds",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Xuan Son Nguyen"
            ],
            "title": "A Gyrovector space approach for symmetric positive semi-definite matrix learning",
            "venue": "In Proceedings of the European Conference on Computer Vision, pp",
            "year": 2022
        },
        {
            "authors": [
                "Xuan Son Nguyen",
                "Shuo Yang"
            ],
            "title": "Building neural networks on matrix manifolds: A Gyrovector space approach",
            "venue": "arXiv preprint arXiv:2305.04560,",
            "year": 2023
        },
        {
            "authors": [
                "Adam Paszke",
                "Sam Gross",
                "Francisco Massa",
                "Adam Lerer",
                "James Bradbury",
                "Gregory Chanan",
                "Trevor Killeen",
                "Zeming Lin",
                "Natalia Gimelshein",
                "Luca Antiga"
            ],
            "title": "Pytorch: An imperative style, high-performance deep learning library",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Xavier Pennec"
            ],
            "title": "Probabilities and statistics on Riemannian manifolds: A geometric approach",
            "venue": "PhD thesis,",
            "year": 2004
        },
        {
            "authors": [
                "Xavier Pennec",
                "Nicholas Ayache"
            ],
            "title": "Uniform distribution, distance and expectation problems for geometric features processing",
            "venue": "Journal of Mathematical Imaging and Vision,",
            "year": 1998
        },
        {
            "authors": [
                "Xavier Pennec",
                "Pierre Fillard",
                "Nicholas Ayache"
            ],
            "title": "A Riemannian framework for tensor computing",
            "venue": "International Journal of Computer Vision,",
            "year": 2006
        },
        {
            "authors": [
                "Salem Said",
                "Lionel Bombrun",
                "Yannick Berthoumieu",
                "Jonathan H Manton"
            ],
            "title": "Riemannian gaussian distributions on the space of symmetric positive definite matrices",
            "venue": "IEEE Transactions on Information Theory,",
            "year": 2017
        },
        {
            "authors": [
                "Yann Thanwerdas",
                "Xavier Pennec"
            ],
            "title": "Is affine-invariance well defined on SPD matrices? a principled continuum of metrics",
            "venue": "In Geometric Science of Information: 4th International Conference,",
            "year": 2019
        },
        {
            "authors": [
                "Yann Thanwerdas",
                "Xavier Pennec"
            ],
            "title": "Exploration of balanced metrics on symmetric positive definite matrices",
            "venue": "In Geometric Science of Information: 4th International Conference,",
            "year": 2019
        },
        {
            "authors": [
                "Yann Thanwerdas",
                "Xavier Pennec"
            ],
            "title": "The geometry of mixed-Euclidean metrics on symmetric positive definite matrices",
            "venue": "Differential Geometry and its Applications,",
            "year": 2022
        },
        {
            "authors": [
                "Yann Thanwerdas",
                "Xavier Pennec"
            ],
            "title": "Theoretically and computationally convenient geometries on full-rank correlation matrices",
            "venue": "SIAM Journal on Matrix Analysis and Applications,",
            "year": 2022
        },
        {
            "authors": [
                "Yann Thanwerdas",
                "Xavier Pennec"
            ],
            "title": "O (n)-invariant Riemannian metrics on SPD matrices",
            "venue": "Linear Algebra and its Applications,",
            "year": 2023
        },
        {
            "authors": [
                "Loring W.. Tu"
            ],
            "title": "An introduction to manifolds",
            "venue": "URL https://doi.org/10",
            "year": 2011
        },
        {
            "authors": [
                "Dmitry Ulyanov",
                "Andrea Vedaldi",
                "Victor Lempitsky"
            ],
            "title": "Instance normalization: The missing ingredient for fast stylization",
            "venue": "arXiv preprint arXiv:1607.08022,",
            "year": 2016
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Raviteja Vemulapalli",
                "Felipe Arrate",
                "Rama Chellappa"
            ],
            "title": "Human action recognition by representing 3D skeletons as points in a Lie group",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2014
        },
        {
            "authors": [
                "Rui Wang",
                "Xiao-Jun Wu",
                "Josef Kittler"
            ],
            "title": "SymNet: A simple symmetric positive definite manifold deep learning method for image set classification",
            "venue": "IEEE Transactions on Neural Networks and Learning Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Rui Wang",
                "Xiao-Jun Wu",
                "Ziheng Chen",
                "Tianyang Xu",
                "Josef Kittler"
            ],
            "title": "DreamNet: A deep Riemannian manifold network for SPD matrix learning",
            "venue": "In Proceedings of the Asian Conference on Computer Vision, pp",
            "year": 2022
        },
        {
            "authors": [
                "Rui Wang",
                "Xiao-Jun Wu",
                "Ziheng Chen",
                "Tianyang Xu",
                "Josef Kittler"
            ],
            "title": "Learning a discriminative SPD manifold neural network for image set classification",
            "venue": "Neural Networks,",
            "year": 2022
        },
        {
            "authors": [
                "Or Yair",
                "Mirela Ben-Chen",
                "Ronen Talmon"
            ],
            "title": "Parallel transport on the cone manifold of SPD matrices for domain adaptation",
            "venue": "IEEE Transactions on Signal Processing,",
            "year": 2019
        },
        {
            "authors": [
                "Hongwei Yong",
                "Jianqiang Huang",
                "Deyu Meng",
                "Xiansheng Hua",
                "Lei Zhang"
            ],
            "title": "Momentum batch normalization for deep learning with small batch size",
            "venue": "In Computer Vision\u2013ECCV 2020: 16th European Conference,",
            "year": 2020
        },
        {
            "authors": [
                "Chen"
            ],
            "title": "2023a) and the proof of Prop. 5.2, (\u03b1, \u03b2)-LEM and \u03b8-LCM are pullback metrics from the Euclidean metric. Therefore, for the Lie groups of SPD manifolds w.r.t. (\u03b1, \u03b2)-LEM and \u03b8-LCM, Eq. (14) can transform the Gaussian distribution. Specifically, given a random point X",
            "year": 2023
        },
        {
            "authors": [
                "Lou"
            ],
            "title": "2020, Prop. G.1 and Cor. G.2), from the view of the product manifold, the element-wise Fr\u00e9chet mean and variance on R are equivalent to the vector-valued Euclidean",
            "year": 2020
        },
        {
            "authors": [
                "Lou"
            ],
            "title": "2020, Prop. G.1), the weighted Fr\u00e9chet mean on R is simplified as the weighted arithmetic average. Therefore, on R, the calculation of running statistics",
            "year": 2020
        },
        {
            "authors": [
                "2024 Kobler"
            ],
            "title": "2022a) proposed SPD domain-specific momentum batch normalization (SPDDSMBN) as a domain adaptation approach for EEG classification. SPDDSMBN, based on Eq. (11), performed normalization of mean and variance on SPD manifolds under the specific AIM. Additionally, SPDDSMBN utilized separate momentums for updating training and testing running statistics",
            "year": 2022
        },
        {
            "authors": [
                "Yong"
            ],
            "title": "Following Kobler et al. (2022a, Alg. 1), we also present a momentum LieBN (MLieBN) in Alg",
            "year": 2020
        },
        {
            "authors": [
                "Kobler"
            ],
            "title": "2022a), we adopt multi-channel mechanisms for domainspecific MLieBN (DSMLieBN), where each domain has its own MLieBN layer. Similar to Kobler et al. (2022a), we set the biasing parameter equal to the neutral element, and the scaling factor",
            "year": 2022
        },
        {
            "authors": [
                "Wang"
            ],
            "title": "2021), we represent each sequence as a 63\u00d7 63 covariance matrix. Hinss2021 dataset (Hinss et al., 2021) is a recently released competition dataset containing EEG signals for mental workload estimation. The dataset is employed for two tasks, namely inter-session and inter-subject, which are treated as domain adaptation problems",
            "venue": "Geometry-aware methods (Yair et al.,",
            "year": 2019
        },
        {
            "authors": [
                "Kobler"
            ],
            "title": "2022a) for data preprocessing. In detail, the python package MOABB (Jayaram & Barachant, 2018) and MNE (Gramfort, 2013) are used to preprocess the datasets. The applied steps include resampling the EEG signals to 250/256 Hz, applying temporal filters to extract oscillatory EEG activity in the 4 to 36 Hz range, extracting short segments",
            "year": 2013
        },
        {
            "authors": [
                "Kobler"
            ],
            "title": "2022a), we use accuracy as the scoring metric for the Radar, HDM05, and FPHA datasets, and balanced accuracy (i.e.,the average recall across classes) for the Hinss2021 dataset. Ten-fold experiments on the Radar, HDM05, and FPHA datasets are carried out with randomized initialization and split (split is officially fixed for the FPHA dataset), while on the Hinss2021 dataset, models are fit and evaluated with a randomized",
            "year": 2021
        },
        {
            "authors": [
                "Murray et al",
                "Exs. A"
            ],
            "title": "H.2 DATASETS AND PREPROCESSING Following LieNet, we validate our LieBN on the G3D dataset (Bloom et al., 2012). This dataset (Bloom et al., 2012) consists of 663 sequences of 20 different gaming actions. Each sequence is recorded by 3D locations of 20 joints (i.e., 19 bones)",
            "year": 2017
        },
        {
            "authors": [
                "Vemulapalli"
            ],
            "title": "skeleton sequence as a point on the Lie group SON\u00d7T (3), where N and T denote spatial and temporal dimensions. As preprocessed in Huang & Van Gool (2017), we set T as 100 for each sequence on the G3D. H.3 IMPLEMENTATION DETAILS LieNet: The LieNet consists of three basic layers: RotMap, RotPooling, and LogMap layers",
            "year": 2017
        },
        {
            "authors": [
                "Huang"
            ],
            "title": "2017), we focus on the suggested 3Blocks architecture for the G3D dataset. The learning rate is 1e\u22122 with a weight decay of 1e\u22125. Following LieNet, we adopt a 10-fold cross-subject test setting, where half of the subjects are used for training and the other half are employed for testing",
            "year": 2017
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Over the past decade or so, Deep Neural Networks (DNNs) have achieved remarkable progress across various scientific fields (Hochreiter & Schmidhuber, 1997; Krizhevsky et al., 2012; He et al., 2016; Vaswani et al., 2017). Conventionally, DNNs have been developed with the underlying assumption of the Euclidean geometry inherent to input data. Nonetheless, there exists a plethora of applications wherein the latent spaces are defined by non-Euclidean structures such as manifolds (Bronstein et al., 2017). To address this issue, researchers have attempted to extend various types of DNNs to manifolds based on the theories of Riemannian geometry (Huang & Van Gool, 2017; Huang et al., 2017; 2018; Ganea et al., 2018; Chakraborty et al., 2018; 2020; Chakraborty, 2020; Wang et al., 2022b;a; Nguyen, 2022a;b; Nguyen & Yang, 2023; Chen et al., 2023a;c; Brooks et al., 2019a;e;c;d; Brooks, 2020; Brooks et al., 2020).\nMotivated by the great success of normalization techniques within DNNs (Ioffe & Szegedy, 2015; Ba et al., 2016; Ulyanov et al., 2016; Wu & He, 2018), researchers have sought to devise normalization layers tailored for manifold-valued data. Brooks et al. (2019b) introduced Riemannian Batch Normalization (RBN) specifically designed for SPD manifolds, with the ability to regulate the Riemannian mean. This approach was further refined in Kobler et al. (2022b) to extend the control over the Riemannian variance. However, the above methods are constrained within the affine-invariant metric (AIM) on SPD manifolds, limiting their applicability and generality. On the other hand, Chakraborty (2020) proposed two distinct Riemannian normalization frameworks, one tailored for Riemannian homogeneous spaces and the other catering to matrix Lie groups. Nonetheless, the normalization designed for Riemannian homogeneous spaces cannot regulate mean nor variance, while the normalization approach for matrix Lie groups is confined to a specific type of distance (Chakraborty, 2020, Sec. 3.2). A principled Riemannian normalization framework capable of controlling both Riemannian mean and variance remains unexplored.\n\u2217Corresponding author\nGiven that Batch Normalization (BN) (Ioffe & Szegedy, 2015) serves as the foundational prototype for various types of normalization, our paper only concentrates on RBN currently and can be readily extended to other normalization techniques. As several manifold-valued measurements form Lie groups, including SPD manifolds, Special Orthogonal (SO) groups, and Special Euclidean (SE) groups, we further direct our attention towards Lie groups. We propose a general framework for RBN over Lie groups, referred to as LieBN, and validate our approach in normalizing both the Riemannian mean and variance. On the empirical side, we focus on SPD manifolds, where three distinct types of Lie groups have been recognized in the literature. We generalize these existing Lie groups into parameterized forms through the deformation concept. Then, we showcase our LieBN framework on SPD manifolds under these Lie groups and propose specific normalization layers. Extensive experiments conducted on widely-used SPD benchmarks demonstrate the effectiveness of our framework. We highlight that our work is entirely theoretically different from Brooks et al. (2019b); Kobler et al. (2022a); Lou et al. (2020), and more general than Chakraborty (2020). The previous RBN methods are either designed for a specific manifold or metric (Brooks et al., 2019b; Kobler et al., 2022a; Chakraborty, 2020), or fail to control mean and variance (Lou et al., 2020), while our LieBN guarantees the normalization of mean and variance on general Lie groups. In summary, our main contributions are as follows: (a) a general Lie group batch normalization framework with controllable first- and second-order statistical moments, and (b) the specific construction of our LieBN layers on SPD manifolds based on three deformed Lie groups and their application on SPD neural networks. Due to page limites, all the proofs are placed in App. I."
        },
        {
            "heading": "2 PRELIMINARIES",
            "text": "This section provides a brief review of the Lie group and the geometries of SPD manifolds. For more in-depth discussions, please refer to Tu (2011); Do Carmo & Flaherty Francis (1992).\nDefinition 2.1 (Lie Groups). A manifold is a Lie group, if it forms a group with a group operation \u2299 such that m(x, y) 7\u2192 x\u2299 y and i(x) 7\u2192 x\u22121\u2299 are both smooth, where x\u22121\u2299 is the group inverse.\nDefinition 2.2 (Left-invariance). A Riemannian metric g over a Lie group {G,\u2299} is left-invariant, if for any x, y \u2208 G and V1, V2 \u2208 TxM,\ngy(V1, V2) = gLx(y)(Lx\u2217,y(V1), Lx\u2217,y(V2)), (1)\nwhere Lx(y) = x\u2299 y is the left translation by x, and Lx\u2217,y is the differential map of Lx at y.\nA Lie group is a group and also a manifold. The most natural Riemannian metric on a Lie group is the left-invariant metric1. Similarly, one can define the right-invariant metric as Def. 2.2. A biinvariant Riemannian metric is the one with both left and right invariance. Given the analogous properties of left and right-invariant metrics, this paper focuses on left-invariant metrics.\nThe idea of pullback is ubiquitous in differential geometry and can be considered as a natural counterpart of bijection in the set theory.\nDefinition 2.3 (Pullback Metrics). Suppose M1,M2 are smooth manifolds, g is a Riemannian metric onM2, and f :M1 \u2192M2 is smooth. Then the pullback of g by f is defined point-wisely,\n(f\u2217g)p(V,W ) = gf(p)(f\u2217,p(V ), f\u2217,p(W )), (2)\nwhere p \u2208M, f\u2217,p(\u00b7) is the differential map of f at p, and V,W \u2208 TpM. If f\u2217g is positive definite, it is a Riemannian metric onM1, called the pullback metric defined by f .\nThe most common pullback metric is obtained through the pullback of a diffeomorphism f . Additionally, if {M2,\u22992} constitutes a Lie group, the diffeomorphism f can pull back the group operation \u22992 to \u22991 on M1, which is defined as \u2200P,Q \u2208 M1, P \u22991 Q = f\u22121(f(P ) \u22992 f(Q)). Henceforth, {M,\u2299, g}, abbreviated asM, always signify a Lie group with left-invariant metric.\nNow, we briefly review the geometry of SPD manifolds. We denote n \u00d7 n SPD matrices as Sn++ and n \u00d7 n real symmetric matrices as Sn. As shown in Arsigny et al. (2005), Sn++ forms a manifold, known as the SPD manifold, and Sn is a Euclidean space. SPD manifolds exhibit three Lie group structures, each associated with an invariant metric. These metrics include the Log-Euclidean\n1Left invariant metric always exists for every Lie group (Do Carmo & Flaherty Francis, 1992).\nMetric (LEM) (Arsigny et al., 2005), Affine-Invariant Metric (AIM) (Pennec et al., 2006), and LogCholesky Metric (LCM) (Lin, 2019). In Thanwerdas & Pennec (2023), LEM and AIM are generalized into two-parameter families of metrics, denoted as (\u03b1, \u03b2)-LEM and (\u03b1, \u03b2)-AIM, respectively. Both (\u03b1, \u03b2)-LEM and (\u03b1, \u03b2)-AIM are defined by the O(n)-invariant inner product on the tangent space at the identity matrix, expressed as:\n\u27e8V,W \u27e9(\u03b1,\u03b2) = \u03b1\u27e8V,W \u27e9+ \u03b2 tr(V ) tr(W ), (3)\nwhere \u27e8\u00b7, \u00b7\u27e9 is the Frobenius inner product, V,W \u2208 TISn++ \u223c= Sn, (\u03b1, \u03b2) \u2208 ST = {(\u03b1, \u03b2) \u2208 R2 | min(\u03b1, \u03b1+ n\u03b2) > 0}. In fact, (\u03b1, \u03b2)-LEM, (\u03b1, \u03b2)-AIM, and LCM are all pullback metrics. Specifically, (\u03b1, \u03b2)-LEM is the pullback metric from Sn (Chen et al., 2023a), while (\u03b1, \u03b2)-AIM is the pullback metric from a left-invariant metric on the Cholesky manifold (Thanwerdas & Pennec, 2022b). Additionally, as shown in Chen et al. (2023b), LCM is the pullback metric from the Euclidean space Ln of lower triangular matrices by the diffeomorphism defined as\n\u03c8LC(P ) = \u230aL\u230b+Dlog(L), (4)\nwhere P = LL\u22a4 represents the Cholesky decomposition, \u230a\u00b7\u230b is the strictly lower part of a square matrix, and Dlog(L) is a diagonal matrix consisting of the logarithm of the diagonal element of L.\nLet {w1...N} be weights satisfying a convexity constraint, i.e.,\u2200i, wi > 0 and \u2211 i wi = 1. The weighted Fre\u0301chet mean (WFM) of a set of SPD matrices {Pi...N} is defined as\nWFM({wi}, {Pi}) = argmin S\u2208Sn++\n\u2211N i=1 wi d 2 (Pi, S) , (5)\nwhere d(\u00b7, \u00b7) denotes the geodesic distance. When wi = 1/N for all i, then Eq. (5) is reduced to the Fre\u0301chet mean, denoted as FM({Pi}). The Fre\u0301chet variance v2 is the attained value at the minimizer of the Fre\u0301chet mean. In this paper, we will interchangeably use the terms Riemannian mean with Fre\u0301chet mean, and Riemannian variance with Fre\u0301chet variance. For (\u03b1, \u03b2)-LEM and LCM, the Fre\u0301chet mean has a closed-form expression. Moreover, since (\u03b1, \u03b2)-AIM has non-positive sectional curvature (Thanwerdas & Pennec, 2023, Tab. 5), the Fre\u0301chet mean exists uniquely (Berger, 2003, 6.1.5) and can be computed by the Karcher flow algorithm (Karcher, 1977).\nGiven SPD matrices P,Q \u2208 Sn++ along with tangent vectors V,W \u2208 TPSn++, we introduce the following notations. Specifically, we denote gP (\u00b7, \u00b7) as the Riemannian metric at P , LogP (\u00b7) as the Riemannian logarithm at P , mexp(\u00b7) and mlog(\u00b7) as the matrix exponentiation and logarithm, and d(\u00b7, \u00b7) as the geodesic distance, respectively. Additionally, Chol(\u00b7) signifies the Cholesky decomposition. We use L = Chol(P ) andK = Chol(Q) to denote the Cholesky factor of P andQ. K and L are diagonal matrices with diagonal elements from K and L. mlog\u2217,P and (Chol) \u22121 \u2217,L represent the differentials of mlog and Chol\u22121 at P and L. We denote \u2225 \u00b7 \u2225(\u03b1,\u03b2) and \u2225 \u00b7 \u2225F as the norm induced by \u27e8\u00b7, \u00b7\u27e9 and the standard Frobenius norm. We summarize all the necessary ingredients in Tab. 1."
        },
        {
            "heading": "3 REVISITING NORMALIZATION",
            "text": ""
        },
        {
            "heading": "3.1 REVISITING EUCLIDEAN NORMALIZATION",
            "text": "In Euclidean DNNs, normalization stands as a pivotal technique for accelerating network training by mitigating the issue of internal covariate shift (Ioffe & Szegedy, 2015). While various normalization\nmethods have been introduced (Ioffe & Szegedy, 2015; Ba et al., 2016; Ulyanov et al., 2016; Wu & He, 2018), they all share a common fundamental concept: the regulation of the first and second statistical moments. In this paper, we focus on batch normalization only.\nGiven a batch of activations {xi...N}, the core operation of batch normalization can be expressed as:\n\u2200i \u2264 N, xi \u2190 \u03b3 xi \u2212 \u00b5b\u221a v2b + \u03f5 + \u03b2 (6)\nwhere \u00b5b is the batch mean, v2b is the batch variance, \u03b3 is the scaling parameter, and \u03b2 is the biasing parameter."
        },
        {
            "heading": "3.2 REVISITING EXISTING RBN",
            "text": "Inspired by the remarkable success of normalization techniques in traditional DNNs, endeavors have been made to develop Riemannian normalization approaches tailored for manifolds. Here we recap some representative methods. However, we note that none of the existing methods effectively handle the first and second moments in a principled manner.\nBrooks et al. (2019b) introduced RBN over SPD manifolds under AIM, with operations defined as:\nCentering from geometric mean M : \u2200i \u2264 N, P\u0304i \u2190M\u2212 1 2PiM \u2212 12 , (7)\nBiasing towards SPD parameter B : \u2200i \u2264 N, P\u0302i \u2190 B 1 2 P\u0304iB 1 2 , (8)\nwhere {Pi...N} are SPD matrices, and M are their Fre\u0301chet mean under AIM. Define a map as\n\u0393P\u2192Q(S) = ExpQ [PTP\u2192Q (LogP (S))] , (9)\nwhere P,Q, S \u2208 Sn++, and Exp,Log,PT are Riemannian exponential map, Riemannian logarithmic map, and parallel transportation along the geodesic, respectively. Then under AIM, Eqs. (7) and (8) can be more generally expressed as\n\u0393I\u2192B [\u0393M\u2192I(Pi)]. (10)\nHowever, Eqs. (7) and (8) only consider the Riemannian mean2 and does not consider the Riemannian variance. To remedy this limitation, Kobler et al. (2022b) further improved the RBN to enable control over the Riemannian variance. The key operation is formulated as\n\u2200i \u2264 N, P\u0304i \u2190 \u0393I\u2192B [(\u0393M\u2192I(Pi)) s v ], (11)\nwhere v2 is the Fre\u0301chet variance, s \u2208 R is a scaling factor. However, this method is still limited to SPD manifolds under AIM. In parallel, Chakraborty (2020, Algs. 1-2) proposed a general framework for Riemannian homogeneous spaces based on Eq. (10) by considering both first and second moments. However, as stated in Chakraborty (2020, Sec. 3.1), Eq. (10) does not generally guarantee the control over Riemannian mean, resulting in agnostic Riemannian statistics. To mitigate this limitation, Chakraborty (2020, Algs. 3-4) further proposed normalization over the matrix Lie group. However, the discussion is limited to a certain distance, limiting the applicability of their method. On the other hand, Lou et al. (2020) proposed an RBN based on a variance of Eq. (10). Although their framework encompasses the standard Euclidean BN when the latent geometry is the standard Euclidean geometry, their approach suffers from the same problem of agnostic Riemannian statistics on general manifolds.\nIn summary, prevailing Riemannian normalization approaches lack a principled guarantee for controlling the first and second-order statistics. In contrast, as will be elucidated, our method can govern first and second-order statistics for all Lie groups. We summarize the above RBN methods in Tab. 2."
        },
        {
            "heading": "4 RIEMANNIAN NORMALIZATION ON LIE GROUPS",
            "text": "In this section, the neutral element in the Lie groupM is denoted as E. Notably, the neutral element E is not necessarily the identity matrix. We first clarify the essential properties of Euclidean BN and then present our normalization method, tailored for Lie groups.\n2Although not mentioned in the original paper, Eqs. (7) and (8) can guarantee the mean of the resulting samples under AIM, as Eqs. (7) and (8) are actions of GL(n).\nTwo key points regarding Euclidean BN, as expressed by Eq. (6), are worth highlighting: (a) The standard BN (Ioffe & Szegedy, 2015) implicitly assumes a Gaussian distribution and can effectively normalize and transform the latent Gaussian distribution; (b) The centering and biasing operations control the mean, while the scaling controls the variance. Therefore, extending BN into Lie groups requires defining Gaussian distribution, centering, biasing, and scaling on Lie groups.\nOn manifolds, several notions of Gaussian distribution have been proposed (Pennec, 2004; Chakraborty & Vemuri, 2019; Barbaresco, 2021). In this work, we adopt the definition from Chakraborty & Vemuri (2019), which characterizes a Gaussian distribution on the Lie group M with a mean parameter M \u2208M and variance \u03c32. This distribution is denoted as N (M,\u03c32), and its Probability Density Function (P.D.F.) is defined as3:\np ( X |M,\u03c32 ) = k(\u03c3) exp ( \u2212d(X,M) 2\n2\u03c32\n) , (12)\nwhere k(\u03c3) is the normalizing constant, exp(\u00b7) is the scalar exponentiation, and d(\u00b7, \u00b7) is the geodesic distance. On Lie groups, the natural counterparts of addition and subtraction in Eq. (6) are group operations. Therefore, centering and biasing on Lie groups can be defined by Lie group left translation. Additionally, scaling can be defined by scaling on the tangent space at the Riemannian mean.\nNow, we can extend Eq. (6) into Lie groupsM. For a batch of activation {Pi...N \u2208 M}, we define the key operations of Lie group BN (LieBN) as:\nCentering to the neutral element E: \u2200i \u2264 N, P\u0304i \u2190 LM\u22121\u2299 (Pi), (13) Scaling the dispersion: \u2200i \u2264 N, P\u0302i \u2190 ExpE [\ns\u221a v2 + \u03f5 LogE(P\u0304i)\n] , (14)\nBiasing towards parameter B \u2208M: \u2200i \u2264 N, P\u0303i \u2190 LB(P\u0302i), (15)\nwhere M is the Fre\u0301chet mean, v2 is the Fre\u0301chet variance, M\u22121\u2299 \u2208 M is the group inverse of M , LM\u22121\u2299\nand LG are left translations (LG(Pi) = G \u2299 Pi), and s \u2208 R/{0} is a scaling parameter. To clarify the effect of our method in controlling mean and variance, we first present the following two propositions, one related to population statistics and the other related to sample statistics.\nProposition 4.1 (Population). [\u2193] Given a random point X overM, and the Gaussian distribution N (M, v2) defined in Eq. (12), we have the following properties for the population statistics:\n1. (MLE of M ) Given {Pi...N \u2208 M} i.i.d. sampled from N (M,v2), the maximum likelihood estimator (MLE) of M is the sample Fre\u0301chet mean.\n2. (Homogeneity) Given X \u223c N (M, v2) and B \u2208M, LB(X) \u223c N (LB(M), v2)\nProposition 4.2 (Sample). [\u2193] Given N samples {Pi...N \u2208 M}, denoting \u03d5s(Pi) = ExpE [sLogE(Pi)], we have the following properties for the sample statistics:\nHomogeneity of the sample mean: FM{LB(Pi)} = LB(FM{Pi}),\u2200B \u2208M, (16) Controllable dispersion from E: \u2211N\ni=1 wi d\n2(\u03d5s(Pi), E) = s 2 \u2211N\ni=1 wi d\n2(Pi, E), (17)\nwhere {w1...N} are weights satisfying a convexity constraint, i.e.,\u2200i, wi > 0 and \u2211 i wi = 1.\n3When M corresponds to R equipped with the standard Euclidean metric, Eq. (12) reduces to the Euclidean Gaussian distribution.\nProp. 4.1 and Eq. (16) imply that our centering and biasing in Eqs. (13) and (15) can control the sample and population mean. As the post-centering mean is E, Eq. (17) implies that Eq. (14) can control the dispersion. Although the population variance after Eq. (14) is generally agnostic, in some cases such as SPD manifolds under LEM and LCM, Eq. (14) can normalize the population variance. Please refer to App. C for technical details.\nSimilar to Ioffe & Szegedy (2015), we use moving averages to update running statistics. Now, we present our general framework of LieBN in Alg. 1. Importantly, whenM = Rn, Alg. 1 is reduced to the standard Euclidean BN. More details are exposed in App. D.\nRemark 4.3. The MLE of the mean of the Gaussian distribution in Eq. (12) have been examined in several previous works (Said et al., 2017; Chakraborty & Vemuri, 2019; Chakraborty, 2020). However, these studies primarily focus on particular manifolds or specific metrics. In contrast, our contribution lies in presenting a universally applicable result for all Lie groups with left-invariant metrics. While Eq. (12) briefly appeared in Kobler et al. (2022b), the authors only focus on SPD manifolds under AIM. The transformation of the population under their proposed RBN remains unexplored as well. Besides, while Chakraborty (2020) analyzed the population properties for their RBN over matrix Lie groups, their results were confined within a specific distance. In contrast, our work provides a more extensive examination, encompassing both population and sample properties of our LieBN in a general manner. Besides, all the discussion about our LieBN can be readily transferred to right-invariant metrics. This paper focuses on LieBN based on left-invariant metrics.\nAlgorithm 1: Lie Group Batch Normalization (LieBN) Algorithm Input : A batch of activations {P1...N \u2208M}, a small positive constant \u03f5, and\nmomentum \u03b3 \u2208 [0, 1] running mean Mr = E, running variance v2r = 1, biasing parameter B \u2208M, scaling parameter s \u2208 R/{0},\nOutput : Normalized activations {P\u03031...N} if training then\nCompute batch mean Mb and variance v2b of {P1...N}; Update running statistics Mr \u2190WFM({1\u2212 \u03b3, \u03b3}, {Mr,Mb}), v2r \u2190 (1\u2212 \u03b3)v2r + \u03b3v2b ;\nend if training then M \u2190Mb, v2 \u2190 v2b ; else M \u2190Mr, v2 \u2190 v2r ; for i\u2190 1 to N do\nCentering to the neutral element E: P\u0304i \u2190 LM\u22121\u2299 (Pi) Scaling the dispersion: P\u0302i \u2190 ExpE [ s\u221a v2+\u03f5 LogE(P\u0304i) ]\nBiasing towards parameter B: P\u0303i \u2190 LB(P\u0302i) end"
        },
        {
            "heading": "5 LIEBN ON THE LIE GROUPS OF SPD MANIFOLDS",
            "text": "This section showcases our Alg. 1 on SPD manifolds. Firstly, we extend the current Lie groups on SPD manifolds by the concept of deformation, resulting in three families of parameterized Lie groups. Subsequently, we construct LieBN layers based on these generalized Lie groups."
        },
        {
            "heading": "5.1 DEFORMED LIE GROUPS OF SPD MANIFOLDS",
            "text": "As shown in Tab. 1, there are three types of Lie groups on SPD manifolds, each equipped with a leftinvariant metric. These metrics include (\u03b1, \u03b2)-AIM, (\u03b1, \u03b2)-LEM, and LCM. For clarity, we denote the group operations w.r.t. (\u03b1, \u03b2)-AIM, (\u03b1, \u03b2)-LEM and LCM as \u2299AI, \u2299LE and \u2299LC, respectively. In Thanwerdas & Pennec (2019b), (\u03b1, \u03b2)-AIM is further extended into three-parameters families of metrics by the pullback of matrix power function P\u03b8(\u00b7) and scaled by 1\u03b82 , denoted as (\u03b8, \u03b1, \u03b2)-AIM. The power function serves as a deformation wherein (\u03b8, \u03b1, \u03b2)-AIM encompasses (\u03b1, \u03b2)-AIM when \u03b8 = 1, and includes (\u03b1, \u03b2)-LEM as \u03b8 approaches 0 (Thanwerdas & Pennec, 2019a). Inspired by the deforming utility of the power function, we define the power-deformed metrics of (\u03b1, \u03b2)-LEM and LCM as the pullback metrics by P\u03b8 and scaled by 1\u03b82 . We denote these two metrics as (\u03b8, \u03b1, \u03b2)-LEM and \u03b8-LCM, respectively. We have the following results w.r.t. the deformation.\nProposition 5.1 (Deformation). [\u2193] (\u03b8, \u03b1, \u03b2)-LEM is equal to (\u03b1, \u03b2)-LEM. \u03b8-LCM interpolates between g\u0303-LEM (\u03b8 = 0) and LCM (\u03b8 = 1), with g\u0303-LEM defined as\n\u27e8V,W \u27e9P = g\u0303(mlog\u2217,P (V ),mlog\u2217,P (W )),\u2200P \u2208 Sn++,\u2200V,W \u2208 TPSn++, (18)\nwhere g\u0303(V1, V2) = 12 \u27e8V1, V2\u27e9 \u2212 1 4 \u27e8D(V1),D(V2)\u27e9, D(Vi) is a diagonal matrix consisting of the diagonal elements of Vi, and mlog\u2217,P is the differential map at P .\nAs (\u03b8, \u03b1, \u03b2)-LEM is equal to (\u03b1, \u03b2)-LEM, in the following, we focus on (\u03b1, \u03b2)-LEM, (\u03b8, \u03b1, \u03b2)-AIM, and \u03b8-LCM. As a diffeomorphism, P\u03b8 also can pull back the group operation \u2299AI and \u2299LC, denoted as \u2299\u03b8-AI and \u2299\u03b8-LC. We have the following proposition on the invariance. Proposition 5.2 (Invariance). [\u2193] (\u03b8, \u03b1, \u03b2)-AIM is left-invariant w.r.t. \u2299\u03b8-AI, while \u03b8-LCM is biinvariant w.r.t. \u2299\u03b8-LC."
        },
        {
            "heading": "5.2 LIEBN ON SPD MANIFOLDS",
            "text": "Now, we showcase our LieBN framework illustrated in Alg. 1 on SPD manifolds. As discussed in Sec. 5.1, there are three families of left-invariant metrics, namely (\u03b8, \u03b1, \u03b2)-AIM, (\u03b1, \u03b2)-LEM, and \u03b8-LCM. Since all three metric families are pullback metrics, the LieBN based on these metrics can be simplified and calculated in the co-domain. We denote Alg. 1 as\nLieBN(Pi;B, s, \u03f5, \u03b3),\u2200pi \u2208 {P1...N \u2208M}. (19)\nThen we can obtain the following theorem.\nTheorem 5.3. [\u2193] Given a Lie group M1, a Lie group M2 with a left-invariant metric g2, and a diffeomorphism f : M1 \u2192 M2, then f induces a left-invariant metric g1 on M1, denoted as g1 = f\u2217g2. For a batch of activation {P1...N} in M1, LieBN(Pi;B, s, \u03f5, \u03b3) in M1 can be calculated inM2 by the following process:\nMapping data intoM2 : P\u0304i = f(Pi), B\u0304 = f(B), (20) Calculating LieBN in {M2, g2} : P\u0302i = LieBN(P\u0304i; B\u0304, s, \u03f5, \u03b3), (21)\nMapping the normalized data back toM1 : P\u0303i = f\u22121(P\u0302i), (22)\nGiven a metric g on Sn++, the power-deformed metric g\u0303 = 1\u03b82 P \u2217 \u03b8 g is equal to P \u2217 \u03b8( 1 \u03b82 g). Therefore, the LieBN under g\u0303 can be calculated by the LieBN under 1\u03b82 g. Besides, as the Christoffel symbols remain the same under constant scaling, the LieBNs under 1\u03b82 g and g only differ in the variance. We denote g(\u03b1,\u03b2)-AI and g(\u03b8,\u03b1,\u03b2)-AI as the metric tensors of (\u03b1, \u03b2)-AIM and (\u03b8, \u03b1, \u03b2)-AIM, respectively Based on the above discussions, the computations of the LieBN under g(\u03b8,\u03b1,\u03b2)-AI are reduced to the LieBN under 1\u03b82 g\n(\u03b1,\u03b2)-AI. Furthermore, as shown in Chen et al. (2023a), (\u03b1, \u03b2)-LEM is a pullback metric from the Euclidean space Sn of symmetric matrices, while the proof of Prop. 5.2 indicates that \u03b8-LCM is a pullback metric from the Euclidean space Ln of lower triangular matrices. Note that in the Euclidean space Sn or Ln, as shown in App. D, Eq. (21) simplifies to the standard Euclidean BN. We denote P,Q, P1 and P2 as points in the codomain (Sn++ with (\u03b1, \u03b2)-AIM for (\u03b8, \u03b1, \u03b2)-AIM, Sn for (\u03b1, \u03b2)-LEM, and Ln for \u03b8-LCM, respectively). We summarize all the necessary ingredients in Tab. 3 for calculating LieBN on SPD manifolds. Note that for (\u03b8, \u03b1, \u03b2)-AIM, our scaling operation defined in Eq. (14) encompasses the scaling operation in Kobler et al. (2022b, Eq. (9)) as a special case, when (\u03b8, \u03b1, \u03b2) = (1, 1, 0)."
        },
        {
            "heading": "6 EXPERIMENTS",
            "text": "In this section, we implement our three families of LieBN to SPD neural networks. Following the previous work (Huang & Van Gool, 2017; Brooks et al., 2019b; Kobler et al., 2022a), we adopt three different applications: radar recognition on the Radar dataset (Brooks et al., 2019b), human action recognition on the HDM05 (Mu\u0308ller et al., 2007) and FPHA (Garcia-Hernando et al., 2018) datasets, and EEG classification on the Hinss2021 dataset (Hinss et al., 2021). More details on datasets and hyper-parameters are exposed in App. G. Besides SPD neural networks, we also implement LieBN on special orthogonal groups and present some preliminary experiments (see App. H).\nImplementation details: Note that our LieBN layers are architecture-agnostic and can be applied to any existing SPD neural network. In this paper, we focus on two network architectures: SPDNet\n(Huang & Van Gool, 2017) for the Radar, HDM05, and FPHA datasets; and TSMNet (Kobler et al., 2022a) for the Hinss2021 dataset. For the SPDNet architecture, we compare our LieBN with SPDNetBN (Brooks et al., 2019b), which applies the SPDBN (Eqs. (7) and (8)) to SPDNet. Consistent with SPDNetBN, we apply our LieBN after each transformation layer (BiMap layer in App. B). In the EEG application, the state-of-the-art Riemannian method is TSMNet with SPD domain-specific momentum batch normalization (TSMNet+SPDDSMBN) (Kobler et al., 2022a), which is a domain adaptation version of Kobler et al. (2022b). For a fair comparison, we also implement a domain-specific momentum LieBN, referred to as DSMLieBN (detailed in App. E). Following Kobler et al. (2022a), we apply our DSMLieBN before the LogEig layer (detailed in Appendix B) in TSMNet. We use the standard cross-entropy loss as the training objective and optimize the parameters with the Riemannian AMSGrad optimizer (Be\u0301cigneul & Ganea, 2018). The network architectures are represented as {d0, d1, . . . , dL}, where the dimension of the parameter in the i-th BiMap layer is di\u00d7 di\u22121. The experiments are conducted with a learning rate of 5e\u22123, batch size of 30, and training epoch of 200 on the Radar, HDM05, and FPHA datasets. For the Hinss2021 dataset, following Kobler et al. (2022a), we use a learning rate of 1e\u22123 with a weight decay of 1e\u22124, a batch size of 50, and a training epoch of 50. All experiments use an Intel Core i9-7960X CPU with 32GB RAM and an NVIDIA GeForce RTX 2080 Ti GPU. Evaluation methods are explained in App. G.3."
        },
        {
            "heading": "6.1 EXPERIMENTAL RESULTS",
            "text": "For each family of LieBN or DSMLieBN, we report two representatives: the standard one induced from the standard metric (\u03b8 = 1), and the one induced from the deformed metric with selected \u03b8. If the standard one is already saturated, we only report the results of the standard ones.\nApplication to SPDNet: As SPDNet is the most classic SPD network, we apply our LieBN to SPDNet on the Radar, HDM05, and FPHA datasets. Additionally, we compare our method with SPDNetBN, which applies the SPDBN in Eqs. (7) and (8) to SPDNet. Following Brooks et al.\n(2019b); Chen et al. (2023b), we use the architectures of {20, 16, 8}, {93, 30}, and {63, 33} for the Radar, HDM05 and FPHA datasets, respectively. The 10-fold average results, including the average training time (s/epcoh), are summarized in Tab. 4. We have three key observations regarding the choice of metrics, deformation, and training efficiency. The choice of metrics: The metric that yields the most effective LieBN layer differs for each dataset. Specifically, the optimal LieBN layers on these three datasets are the ones induced by AIM-(1), LCM-(0.5), and AIM-(1.5), respectively, which improves the performance of SPDNet by 2.22%, 11.71%, and 4.8%. Additionally, although the LCM-based LieBN performs worse than other LieBN variants on the Radar and FPHA datasets, it exhibits the best performance on the HDM05 dataset. These observations highlight the advantage of the generality of our LieBN approach. The effect of deformation: Deformation patterns also vary across datasets. Firstly, the standard AIM is already saturated on the Radar dataset. Secondly, as indicated in Tab. 4, an appropriate deformation factor \u03b8 can further enhance the performance of LieBN. Notably, even though the LieBN induced by LCM-(1) impedes the learning of SPDNet on the FPHA datasets, it can improve performance when an appropriate deformation factor \u03b8 is applied. These findings highlight the utility of the deforming geometry of the SPD manifold. Efficiency: Our LieBN achieves comparable or even better efficiency than SPDNetBN, although compared with SPDNetBN, our LieBN places additional consideration on variance. Particularly, the LieBN induced by standard LEM or LCM exhibits better efficiency than SPDNetBN. Even with deformation, the LCM-based LieBN is still comparable with SPDNetBN in terms of efficiency. This phenomenon could be attributed to the fast and simple computation of LCM and LEM.\nApplication to EEG classification: We evaluate our method on the architecture of TSMNet for two tasks, inter-session and inter-subject EEG classification. Following Kobler et al. (2022a), we adopt the architecture of {40, 20}. Compared to the SPDDSMBN, TSMNet+DSMLieBN-AIM obtains the highest average scores of 55.10% and 53.97% in inter-session and -subject transfer learning, improving the SPDDSMBN by 0.98% and 3.87%, respectively. In the inter-subject scenario, the advantage of the efficiency of our LieBN over SPDDSMBN is more obvious. Specifically, both the LEM- and LCM-based DSMLieBN achieve similar or better performance compared to SPDDSMBN, while requiring considerably less training time. For example, DSMLieBN-LCM-(1) achieves better results with only half the training time of SPDDSMBN on inter-subject tasks. Interestingly, under the standard AIM, the sole difference between SPDDSMBN and our DSMLieBN is the way of centering and biasing. SPDDSMBN applies the inverse square root and square root to fulfill centering and biasing, while AIM-induced LieBN uses more efficient Cholesky decomposition. As such, the DSMLieBN induced by the standard AIM is more efficient than SPDDSMBN, particularly on the inter-subject task."
        },
        {
            "heading": "7 CONCLUSIONS",
            "text": "This paper proposes a novel framework called LieBN, enabling batch normalization over Lie groups. Our LieBN can effectively normalize both the sample and population statistics. Besides, we generalize the existing Lie groups on SPD manifolds and showcase our framework on the parameterized Lie groups of SPD manifolds. Extensive experiments demonstrate the advantage of our LieBN.\nThere are several other types of Lie groups in machine learning, such as special Euclidean groups. As a future avenue, we shall extend our LieBN to other Lie groups."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "This work was partly supported by the MUR PNRR project FAIR (PE00000013) funded by the NextGenerationEU and by the EU Horizon project ELIAS (No. 101120237). The authors also gratefully acknowledge financial support from the China Scholarship Council."
        },
        {
            "heading": "APPENDIX CONTENTS",
            "text": ""
        },
        {
            "heading": "A Notations 16",
            "text": ""
        },
        {
            "heading": "B Basic Layers in SPDNet and TSMNet 16",
            "text": ""
        },
        {
            "heading": "C Statistical Results of Scaling in the LieBN 16",
            "text": ""
        },
        {
            "heading": "D LieBN as a Natural Generalization of Euclidean BN 19",
            "text": ""
        },
        {
            "heading": "E Domain-specific Momentum LieBN for EEG Classification 19",
            "text": ""
        },
        {
            "heading": "F Backpropagation of Matrix Functions 20",
            "text": ""
        },
        {
            "heading": "G Additional Details and Experiments of LieBN on SPD manifolds 20",
            "text": "G.1 Datasets and Preprocessing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\nG.2 Hyper-Parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\nG.3 Evaluation Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\nG.4 Empirical Insights on the Hyper-parameters in LieBN on SPD Manifolds . . . . . 21\nG.4.1 The Effect of \u03b2 in SPD LieBN . . . . . . . . . . . . . . . . . . . . . . . . 22"
        },
        {
            "heading": "H Preliminary Experiments on Rotation Matrices 22",
            "text": "H.1 Geometry on Rotation Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\nH.2 Datasets and Preprocessing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\nH.3 Implementation Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\nH.4 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\nI Proofs of the Lemmas and Theories in the Main Paper 23"
        },
        {
            "heading": "A NOTATIONS",
            "text": "For better clarity, we summarize all the notations used in this paper in Tab. 6."
        },
        {
            "heading": "B BASIC LAYERS IN SPDNET AND TSMNET",
            "text": "SPDNet (Huang & Van Gool, 2017) is the most classic SPD neural network. SPDNet mimics the conventional densely connected feedforward network, consisting of three basic building blocks\nBiMap layer: Sk =W kSk\u22121W k\u22a4, with W k semi-orthogonal, (23)\nReEig layer: Sk = Uk\u22121 max(\u03a3k\u22121, \u03f5In)Uk\u22121\u22a4, with Sk\u22121 = Uk\u22121\u03a3k\u22121Uk\u22121\u22a4, (24)\nLogEig layer: Sk = log(Sk\u22121). (25)\nwhere max(\u00b7) is element-wise maximization. BiMap and ReEig mimic transformation and nonlinear activation, while LogEig maps SPD matrices into the tangent space at the identity matrix for classification.\nTSMNet (Kobler et al., 2022a) can be illustrate as ftc \u2192 fsc \u2192 fBiMap \u2192 fReEig \u2192 fLogEig , where ftc and fsc denote temporal and spatial convolution, respectively."
        },
        {
            "heading": "C STATISTICAL RESULTS OF SCALING IN THE LIEBN",
            "text": "In this section, we will show the effect of our scaling (Eq. (14)) on the population. We will see that while the resulting population variance is generally agnostic, it becomes analytic under certain circumstances, such as SPD manifolds under LEM or LCM. As a result, Eq. (14) can normalize and transform the latent Gaussian distribution.\nTo simplify, let \u03d5s(P ) = ExpE [sLogE(P )]. Similar to the main paper, M denotes a Lie group with a left-invariant metric. First, we present a lemma on the resulting P.D.F. of a random point transformed by \u03d5s.\nLemma C.1. Given a random pointX distributed overM with P.D.F. pX , the P.D.F of Y = \u03d5s(X) is given by:\npY (Q) = \u2206pX(\u03d5 (\u22121) s (Q)). (26)\nwhere \u2206 = |\u03d5 \u22121 s\u2217 |\u2223\u2223\u2223L\n\u03d5 \u22121 s (Q)\u2299Q\u22121\u2217 \u2223\u2223\u2223 . Here | \u00b7 | denotes the determinant, and \u03d5\u22121s\u2217 and L\u03d5\u22121s (Q)\u2299Q\u22121\u2217 are the differentials.\nProof. For the sake of simplicity, we will denote \u03d5s as \u03d5 throughout this proof. The volume element w.r.t. a left-invariant metric is the Haar measure (Pennec & Ayache, 1998, Sec. 3.2):\ndLM(P ) = dP\n|LP\u2217,E | , (27)\nwhere |LP\u2217,E | is the determinant4 of the differential of LP at the neutral element E. Then we have\ndLM(\u03d5\u22121(Q)) = d\u03d5\u22121(Q)\n|L\u03d5\u22121(Q)\u2217,E | = |(L\u03d5\u22121(Q)\u2299Q\u22121 \u25e6 LQ)\u2217,E |\u22121|\u03d5\u22121\u2217 |dQ = |\u03d5\u22121\u2217 |\n|L\u03d5\u22121(Q)\u2299Q\u22121\u2217| dLM(Q)\n= \u2206dLM(Q).\n(28)\nThe probability of Q = \u03d5(P ) in a set Y \u2282M is F (\u03d5(P ) \u2208 Y) = F (P \u2208 \u03d5\u22121(Y))\n= \u222b \u03d5\u22121(Y) pX(P ) \u00b7 dLM(P )\n= \u222b Y pX(\u03d5 (\u22121)(Q))dLM(\u03d5(\u22121)(Q))\n= \u222b Y \u2206pX(\u03d5 (\u22121)(Q))dLM(Q).\n(29)\nTherefore, the density of Y = \u03d5(X) is\npY (Q) = \u2206pX(\u03d5 (\u22121)(Q)). (30)\nThe above lemma implies that when \u2206 is a constant, Y also follows a Gaussian distribution.\nCorollary C.2. Following the notations in Lem. C.1, if \u2206 = c is a constant and X \u223c N (E, \u03c32), then Y also follows a Gaussian distribution, i.e.,Y \u223c N (E, s2\u03c32)\nProof.\npY (Q) = ck(\u03c3) exp\n( \u2212d(\u03d5 \u22121 s (Q), E) 2\n2\u03c32 ) = k\u2032(\u03b4) exp ( \u2212d(ExpE 1/sLogE(Q), E) 2\n2\u03c32 ) = k\u2032(\u03b4) exp ( \u2212\u2225LogE(Q)\u2225 2 E\n2s2\u03c32 ) = k\u2032(\u03b4) exp ( \u2212d(Q,E)\n2s2\u03c32\n) ,\n(31)\n4This should be more precisely understood as the determinant of the matrix representation of LP\u2217,E under a local coordinate\nwhere \u2225 \u00b7 \u2225E is the norm of the tangent space at the neutral element E.\nCor. C.2 implies that when \u2206 = c, \u03d5s can scale the population variance and further transform the Gaussian distribution. Simple computations show that in the standard Euclidean space Rn, \u2206 = 1/s. Therefore, it is natural to expect that the pullback of Rn also enjoys constant \u2206.\nProposition C.3. Consider an n-dimensional Lie groupM pulled back from the standard Euclidean space Rn by the diffeomorphism \u03c8 :M\u2192 Rn. In other words, the group operations and Riemannian metric onM are defined by \u03c8 from Rn. Then \u2206 remains constant onM.\nProof. To simplify notation, we denote \u03d5s as \u03d5. Under the given assumption, the group addition and Riemannian metric onM are defined as follows:\n\u2200P,Q \u2208M, P \u2299Q = \u03c8\u22121(\u03c8(P ) + \u03c8(Q)) g = \u03c8\u2217gE,\n(32)\nwhere gE is the standard Euclidean metric. Therefore, \u03d5 can be simplified as\n\u03d5(P ) = ExpE [sLogE(P )] = \u03c8\u22121 (\n\u02dcExp0 [ \u03c8\u2217,E ( s\u03c8\u22121\u2217,0 \u02dcLog0\u03c8(P ) )])\n= \u03c8\u22121(s\u03c8(P )),\n(33)\nwhere \u02dcExp and \u02dcLog are the Riemannian exponential and logarithmic maps in Rn, which are reduced to vector addition and subtraction, respectively. Therefore, the inverse of \u03d5 is\n\u03d5\u22121(P ) = \u03c8\u22121 (1/s\u03c8(P )) . (34)\nBesides, L\u03d5\u22121(Q)\u2299Q\u22121 can also be further simplified:\nL\u03d5\u22121(Q)\u2299Q\u22121(P ) = \u03c8 \u22121 (1/s\u03c8(Q)\u2212 \u03c8(Q) + \u03c8(P )) (35)\nThe differentials of Eqs. (34) and (35) at Q are\n\u03d5\u22121\u2217,Q = 1 s \u03c8\u22121\u2217,1/s\u03c8(Q) \u25e6 \u03c8\u2217,Q, (36)\nL\u03d5\u22121(Q)\u2299Q\u22121\u2217,Q = \u03c8 \u22121 \u2217,1/s\u03c8(Q) \u25e6 \u03c8\u2217,Q. (37)\nTherefore, \u2206 = 1/s for all Q \u2208M.\nBy Prop. C.3, we can directly obtain the following corollary.\nCorollary C.4. Given a Lie groupM pulled back from the Euclidean space, and a random point X \u223c N (E, \u03c32) overM, Y = \u03d5s(X) \u223c N (E, s2\u03c32)\nIn machine learning, several Lie groups are derived by the pullback from the standard Euclidean space. As shown in Chen et al. (2023a) and the proof of Prop. 5.2, (\u03b1, \u03b2)-LEM and \u03b8-LCM are pullback metrics from the Euclidean metric. Therefore, for the Lie groups of SPD manifolds w.r.t. (\u03b1, \u03b2)-LEM and \u03b8-LCM, Eq. (14) can transform the Gaussian distribution. Specifically, given a random point X \u223c N (M,\u03c32), Eqs. (13) to (15) transform the Gaussian distribution as:\nN (M,\u03c32)\u2192 N (E, \u03c32)\u2192 N (E, s2)\u2192 N (B, s2), (38)\nwhere M and \u03c3 are employed to normalize X , and \u03f5 in Eq. (14) is omitted. The above process exactly mirrors the transformation of Gaussian distributions within the framework of standard BN (Ioffe & Szegedy, 2015).\nRemark C.5. A similar result to our Cor. C.4 was also presented in Chakraborty (2020, Prop. 3). However, in his proof, the author did not account for the Haar measure and only considered the P.D.F., casting doubt on the validity of their results. Additionally, their discussion is limited to matrix Lie groups, specifically under the distance d(P,Q) = \u2225mlog(P\u22121Q)\u2225F. In contrast, we rectify their proof and consider general Lie groups."
        },
        {
            "heading": "D LIEBN AS A NATURAL GENERALIZATION OF EUCLIDEAN BN",
            "text": "The centering and biasing in Euclidean BN correspond to the group action of R. From a geometric perspective, the standard Euclidean metric is invariant under this group operation. Consequently, it is not surprising that our LieBN algorithm formulated in Alg. 1 serves as a natural generalization of standard Euclidean batch normalization. We formalize this fact in the following proposition.\nProposition D.1. The LieBN algorithm presented in Alg. 1 is equivalent to the standard Euclidean BN whenM = Rn, both during the training and testing phases.\nProof. The core of this proof lies in the fact that on Rn, (1) the Fre\u0301chet mean and variance are reduced to the familiar Euclidean statistics. (2) the calculation of the running mean becomes the weighted arithmetic mean. (3) Eqs. (13) to (15) become Eq. (6); We prove these three points one by one.\nAs stated in Lou et al. (2020, Prop. G.1 and Cor. G.2), from the view of the product manifold, the element-wise Fre\u0301chet mean and variance on Rn are equivalent to the vector-valued Euclidean variance and mean.\nBesides, by similar proof as in Lou et al. (2020, Prop. G.1), the weighted Fre\u0301chet mean on Rn is simplified as the weighted arithmetic average. Therefore, on Rn, the calculation of running statistics in our Alg. 1 becomes the familiar moving average.\nThirdly, on Rn, we know that Lx(y) = x + y, Expx v = x + v, Logx y = y \u2212 x, and the neutral element is 0. Since statistics, as well as the Euclidean BN, are calculated element-wisely, we can safely consider a single element, i.e.,Rn = R. For a batch of activations {xi...N \u2208 R}, where the batch mean and batch variance are denoted as \u00b5b and v2b , then Eqs. (13) to (15) are rewrote as:\nL\u03b2 ( Exp0 [ \u03b3\u221a v2b + \u03f5 Log0(L\u2212\u00b5b(xi)) ]) = \u03b3 xi \u2212 \u00b5b\u221a v2b + \u03f5 + \u03b2. (39)\nThe above equation is the exact core computation of the standard Euclidean BN."
        },
        {
            "heading": "E DOMAIN-SPECIFIC MOMENTUM LIEBN FOR EEG CLASSIFICATION",
            "text": "Algorithm 2: Momentum LieBN (MLieBN) Algorithm Input : A batch of activations {P1...N \u2208M}, and a small positive constant \u03f5\nrunning mean M\u0304r = E, running variance v\u03042r = 1 for training running mean M\u0303r = E, running variance v\u03032r = 1 for testing biasing parameter B \u2208M, scaling parameter s \u2208 R/{0}, momentum for training and testing \u03b3train, \u03b3 \u2208 [0, 1]\nOutput : Normalized activations {P\u03031...N} if training then\nCompute batch mean Mb and variance v2b of {P1...N}; M\u0304r \u2190WFM({1\u2212 \u03b3train, \u03b3train}, {M\u0304r,Mb}); v\u03042r \u2190 (1\u2212 \u03b3train)v\u03042r + \u03b3trainv2b ; M\u0303r \u2190WFM({1\u2212 \u03b3, \u03b3}, {M\u0303r,Mb}); v\u03032r \u2190 (1\u2212 \u03b3)v\u03032r + \u03b3v2b ;\nend if training then M \u2190 M\u0304r, v2 \u2190 v\u03042r ; else M \u2190 M\u0303r, v2 \u2190 v\u03032r ; for i\u2190 1 to N do\nCentering to the neutral element E: P\u0304i \u2190 LM\u22121\u2299 (Pi) Scaling the dispersion: P\u0302i \u2190 ExpE [ s\u221a v2+\u03f5 LogE(P\u0304i) ]\nBiasing towards parameter B: P\u0303i \u2190 LB(P\u0302i) end\nKobler et al. (2022a) proposed SPD domain-specific momentum batch normalization (SPDDSMBN) as a domain adaptation approach for EEG classification. SPDDSMBN, based on Eq. (11), performed normalization of mean and variance on SPD manifolds under the specific AIM. Additionally, SPDDSMBN utilized separate momentums for updating training and testing running statistics, inspired by the work of Yong et al. (2020). Following Kobler et al. (2022a, Alg. 1), we also present a momentum LieBN (MLieBN) in Alg. 2. Here \u03b3 is fixed and \u03b3train is defined as\n\u03b3train = 1\u2212 \u03c1 1 K\u22121 max(K\u2212k,0) + \u03c1, where \u03c1 = 1\ndomains per batch (40)\nFurthermore, in line with Kobler et al. (2022a), we adopt multi-channel mechanisms for domainspecific MLieBN (DSMLieBN), where each domain has its own MLieBN layer. Similar to Kobler et al. (2022a), we set the biasing parameter equal to the neutral element, and the scaling factor is shared across all domains. We denote Alg. 2 as MLieBN(Pj |M, s, \u03f5, \u03b3, \u03b3train). Then our DSMLieBN follows\nDSMLieBN(Pj , i) = MLieBNi(Pj |E, s, \u03f5, \u03b3, \u03b3train),\u2200Pj \u2208 {P1...N}, (41) where i is the index of the domain. We follow the official code of SPDDSMBN5 to implement our DSMLieBN. In a word, the only difference between DSMLieBN and SPDDSMBN is the different way of normalization.\nAnalogous to Thm. 5.3, computations for DSMLieBN under pullback metrics can also be performed by mapping, calculating, and then remapping."
        },
        {
            "heading": "F BACKPROPAGATION OF MATRIX FUNCTIONS",
            "text": "Our implementation of LieBN on SPD manifolds involves several matrix functions. Thus, we employ matrix backpropagation (BP) (Ionescu et al., 2015) for gradient computation. These matrix operations can be divided into Cholesky decomposition and the functions based on Eigendecomposition.\nThe differentiation of the Cholesky decomposition can be found in Murray (2016, Eq. 8) or Lin (2019, Props. 4). Besides, our homemade BP of the Cholesky decomposition yields a similar gradient to the one generated by autograd of torch.linalg.cholesky. Therefore, during the experiments, we use torch.linalg.cholesky.\nThe second type of matrix functions is based on Eigendecomposition, such as matrix exponential, logarithm, and power. Although torch (Paszke et al., 2019) supports autograd of Eigendecomposition, it requires the computation of 1\u03b4i\u2212\u03b4j (Ionescu et al., 2015, Props. 1), where \u03b4i and \u03b4j denote eigenvalues. This might trigger numerical instability when \u03b4i approximates \u03b4j . Following Brooks et al. (2019b), we use the Dalecki\u0306i-Krei\u0306n formula (Bhatia, 2013, Thm. V.3.3) to calculate the BP of Eigen-based matrix functions. In detail, for a matrix function defined as X = f(S) = Uf(\u03a3)U\u22a4, with S = U\u03a3U\u22a4 as the eigendecomposition of an SPD matrix, its BP is expressed as\n\u2207SL = U [K \u2299 (UT (\u2207XL)U)]UT . (42) where\u2207XL is the Euclidean gradient of the loss function L w.r.t. X . Matrix K is defined as\nKij =\n{ f(\u03c3i)\u2212f(\u03c3j)\n\u03c3i\u2212\u03c3j if \u03c3i \u0338= \u03c3j f \u2032 (\u03c3i) otherwise\n(43)\nwhere \u03a3 = diag(\u03c31, \u03c32, \u00b7 \u00b7 \u00b7 , \u03c3d). Eq. (43) demonstrates the numerical stability of Dalecki\u0306i-Krei\u0306n formula."
        },
        {
            "heading": "G ADDITIONAL DETAILS AND EXPERIMENTS OF LIEBN ON SPD MANIFOLDS",
            "text": "We use the official code of SPDNetBN6 (Brooks et al., 2019b) and TSMNet7 (Kobler et al., 2022a) to implement our experiments on the SPDNet and TSMNet backbones.\n5https://github.com/rkobler/TSMNet 6https://proceedings.neurips.cc/paper files/paper/2019/file/6e69ebbfad976d4637bb4b39de261bf7-\nSupplemental.zip 7https://github.com/rkobler/TSMNet\nG.1 DATASETS AND PREPROCESSING Radar dataset (Brooks et al., 2019b) contains 3,000 synthetic radar signals. Following the protocol in Brooks et al. (2019b), each signal is split into windows of length 20, resulting in 3,000 covariance matrices of the size 20 \u00d7 20 equally distributed in 3 classes. HDM05 dataset (Mu\u0308ller et al., 2007) consists of 2,273 skeleton-based motion capture sequences executed by different actors. Each frame consists of 3D coordinates of 31 joints, allowing the representation of each sequence as a 93 \u00d7 93 covariance matrix. In line with Brooks et al. (2019b), we trim the dataset down to 2086 instances scattered throughout 117 classes by removing some under-represented clips. FPHA (Garcia-Hernando et al., 2018) includes 1,175 skeleton-based first-person hand gesture videos of 45 different categories with 600 clips for training and 575 for testing. Following Wang et al. (2021), we represent each sequence as a 63\u00d7 63 covariance matrix. Hinss2021 dataset (Hinss et al., 2021) is a recently released competition dataset containing EEG signals for mental workload estimation. The dataset is employed for two tasks, namely inter-session and inter-subject, which are treated as domain adaptation problems. Geometry-aware methods (Yair et al., 2019; Kobler et al., 2022a) have demonstrated promising performance in EEG classification. We follow Kobler et al. (2022a) for data preprocessing. In detail, the python package MOABB (Jayaram & Barachant, 2018) and MNE (Gramfort, 2013) are used to preprocess the datasets. The applied steps include resampling the EEG signals to 250/256 Hz, applying temporal filters to extract oscillatory EEG activity in the 4 to 36 Hz range, extracting short segments ( \u2264 3s) associated with a class label, and finally obtaining 40\u00d7 40 SPD covariance matrices.\nG.2 HYPER-PARAMETERS We implement the SPD LieBN and DSMLieBN induced by three standard left-invariant metrics, namely AIM, LEM, and LCM, along with their parameterized metrics. Therefore, our method has a maximum of three hyper-parameters, i.e.,(\u03b8, \u03b1, \u03b2), where \u03b8 controls deformation. In our LieBN, (\u03b1, \u03b2) only affects variance calculation. Therefore, we set (\u03b1, \u03b2) = (1, 0) and only tune the deformation factor \u03b8 from the candidate values of \u00b10.5, \u00b11, and \u00b11.5. We denote [Baseline]+[BN Type]+[Metric]-[\u03b8] as the baseline endowed with a specific LieBN, such as SPDNet+LieBN-AIM-(1) and TSMNet+DSMLieBN-LCM-(1).\nG.3 EVALUATION METHODS In line with the previous work (Brooks et al., 2019b; Kobler et al., 2022a), we use accuracy as the scoring metric for the Radar, HDM05, and FPHA datasets, and balanced accuracy (i.e.,the average recall across classes) for the Hinss2021 dataset. Ten-fold experiments on the Radar, HDM05, and FPHA datasets are carried out with randomized initialization and split (split is officially fixed for the FPHA dataset), while on the Hinss2021 dataset, models are fit and evaluated with a randomized leave 5% of the sessions (inter-session) or subjects (inter-subject) out cross-validation scheme.\nG.4 EMPIRICAL INSIGHTS ON THE HYPER-PARAMETERS IN LIEBN ON SPD MANIFOLDS Our SPD LieBN has at most three types of hyper-parameters: Riemannian metric, deformation factor \u03b8, and O(n)-invariance parameters (\u03b1, \u03b2). The general order of importance should be Riemannian metric > \u03b8 > (\u03b1, \u03b2).\nThe most significant parameter is the choice of Riemannian metric, as all the geometric properties are sourced from a metric. A safe choice would start with AIM, and then decide whether to explore other metrics further. The most important reason is the property of affine invariance of AIM, which is a natural characteristic of covariance matrices. In our experiments, the LieBN-AIM generally achieves the best performance. However, AIM is not always the best metric. As shown in Tab. 4b, the best result on the HDM05 dataset is achieved by LCM-based LieBN, which improves the vanilla SPDNet by 11.71%. Therefore, when choosing Riemannian metrics on SPD manifolds, a safe choice would start with AIM and extend to other metrics. Besides, if efficiency is an important factor, one should first consider LCM, as it is the most efficient one.\nThe second one is the deformation factor \u03b8. As we discussed in Sec. 5.1, \u03b8 interpolates between different types of metrics (\u03b8 = 1 and \u03b8 \u2192 0). Inspired by this, we select \u03b8 around its deformation boundaries (1 and 0). In this paper we roughly select \u03b8 from {\u00b10.5,\u00b11,\u00b11.5}\nThe less important parameters are (\u03b1, \u03b2). Recalling Alg. 1. and Tab. 1, (\u03b1, \u03b2) only affects the calculation of variance, which should have less effects compared with the above two parameters. Therefore, we simply set (\u03b1, \u03b2) = (1, 0) during experiments.\nG.4.1 THE EFFECT OF \u03b2 IN SPD LIEBN Recalling Eq. (3), \u03b2 controls the relative importance of the trace part against the inner product. Therefore, we set the candidate values of \u03b2 as {1, 1/n, 1/n2, 0,\u22121/n+ \u03f5,\u22121/n2}, where n is the input dimension of LieBN, and \u03f5 is a small positive scalar to ensure O(n)-invariance, i.e.,(\u03b1, \u03b2) \u2208 ST. 1/n2 and 1/n means averaging the trace in Eq. (3), while the sign of \u03b2 denotes suppressing (-), enhancing (+), or neutralizing (0) the trace.\nWe focus on AIM-based LieBN on the HDM05 dataset. We set \u03b8 = 1.5, as it is the best deformation factor under this scenario. Other network settings remain the same as the main paper. The 10-fold average results are presented in Tab. 7. Note that on this setting, n = 30. As expected, \u03b2 has minor effects on our LieBN."
        },
        {
            "heading": "H PRELIMINARY EXPERIMENTS ON ROTATION MATRICES",
            "text": "This section implements our LieBN in Alg. 1 on the special orthogonal groups, i.e.,SO(n), also known as rotation matrices. We apply our LieBN to the classic LieNet (Huang & Van Gool, 2017), where the latent space is the special orthogonal group.\nH.1 GEOMETRY ON ROTATION MATRICES\nWe denote R,S \u2208 SO(n), and \u03b3(R,S)(t) as the geodesic connecting R and S. The neutral elements of rotation matrices is the identity matrix. Tab. 8 summarizes all the necessary Riemannian ingredients of the invariant metric on SO(n).\nFor the specific SO(3), the matrix logarithm and exponentiation can be calculated without decomposition (Murray et al., 2017, Exs. A. 11 and A.14).\nH.2 DATASETS AND PREPROCESSING Following LieNet, we validate our LieBN on the G3D dataset (Bloom et al., 2012). This dataset (Bloom et al., 2012) consists of 663 sequences of 20 different gaming actions. Each sequence is recorded by 3D locations of 20 joints (i.e., 19 bones). Following Huang & Van Gool (2017), we use the code of Vemulapalli et al. (2014) to represent each skeleton sequence as a point on the Lie group SON\u00d7T (3), where N and T denote spatial and temporal dimensions. As preprocessed in Huang & Van Gool (2017), we set T as 100 for each sequence on the G3D.\nH.3 IMPLEMENTATION DETAILS LieNet: The LieNet consists of three basic layers: RotMap, RotPooling, and LogMap layers. The RotMap mimics the convolutional layer, while the RotPooling extends the pooling layers to rotation matrices. The logMap layer maps the rotation matrix into the tangent space at the identity for classification. Note that the official code of LieNet8 is developed by Matlab. We follow the opensourced Pytorch code9 to implement our experiments. To reproduce LieNet more faithfully, we made the following modifications to this Pytorch code. We re-code the LogMap and RotPooling layers to make them consistent with the official Matlab implementation. In addition, we also extend the existing Riemannian optimization package geoopt Be\u0301cigneul & Ganea (2018) into SO(3) to allow for Riemannian version of SGD, ADAM, and AMSGrad on SO(3), which is missing in the current\n8https://github.com/zhiwu-huang/LieNet 9https://github.com/hjf1997/LieNet\npackage. However, we find that SGD is the best optimizer for LieNet. Therefore, we adopt SGD during the experiments. We apply our LieBN before the LogMap layer and refer to this network as LieNetLieBN. Note that the dimension of features in LieNet is B\u00d7N \u00d7T \u00d73\u00d73, we calculate Lie group statistics along the batch and spatial dimensions (B \u00d7 T ), resulting in an N \u00d7 3\u00d7 3 running mean.\nTraining Details: Following Huang et al. (2017), we focus on the suggested 3Blocks architecture for the G3D dataset. The learning rate is 1e\u22122 with a weight decay of 1e\u22125. Following LieNet, we adopt a 10-fold cross-subject test setting, where half of the subjects are used for training and the other half are employed for testing.\nH.4 RESULTS The 10-fold results are shown in Tab. 9. Due to different software, our reimplemented LieNet is slightly worse than the performance reported in Huang et al. (2017). However, we still can observe a clear improvement of LieNetLieBN over LieNet."
        },
        {
            "heading": "I PROOFS OF THE LEMMAS AND THEORIES IN THE MAIN PAPER",
            "text": "Proof of Prop. 4.1 . Property 1:\nThe MLE of M is\nMMLE = argmax log(K(v))\u2212 N\u2211 i=1 d(Pi,M) 2 2v2\n= argmin N\u2211 i=1 d(Pi,M) 2.\n(44)\nProperty 2:\nWe denote Y = LB(X), and pX and pY as the density of X and Y , respectively. The density of Y is\npY (Q) = pX(LB\u22121\u2299 (Q))\n= k(\u03c3) exp ( \u2212 d(LB\u22121\u2299 (Q),M)2\n2\u03c32\n)\n= k(\u03c3) exp ( \u2212d(Q,LB(M)) 2\n2\u03c32\n) .\n(45)\nThe first equation is obtained by (Pennec, 2004, Thm. 7), while the last equation is obtained by the isometry of the left translation.\nProof of Prop. 4.2 . The isometry of LB can directly obtain the homogeneity of the sample mean. Now let us focus on Eq. (17). We have the following:\u2211N\ni=1 wi d\n2(\u03d5s(Pi), E) = \u2211N\ni=1 wi\u2225sLogE Pi\u2225E = s2 \u2211N\ni=1 wi\u2225LogE Pi\u22252E = s2 \u2211N\ni=1 wi d\n2(Pi, E),\n(46)\nwhere \u2225 \u00b7 \u2225E is the norm on TEM.\nProof of Prop. 5.1 . We first prove the case of (\u03b8, \u03b1, \u03b2)-LEM, and then proceed to the case of \u03b8-LCM.\n(\u03b8, \u03b1, \u03b2)-LEM: For clarity, we denote the metric tensor of (\u03b8, \u03b1, \u03b2)-LEM as\ng(\u03b8,\u03b1,\u03b2)-LE = 1\n\u03b82 P\u2217\u03b8 g (\u03b1,\u03b2)-LE, (47)\nwhere g(\u03b1,\u03b2)-LE is the metric tensor of (\u03b1, \u03b2)-LEM. Let P \u2208 Sn++ and V,W \u2208 TPSn++, then we have\ng (\u03b8,\u03b1,\u03b2)-LE P (V,W ) =\n1\n\u03b82 g (\u03b1,\u03b2)-LE P\u03b8(P ) (P\u03b8\u2217,P (V ),P\u03b8\u2217,P (W ))\n= 1\n\u03b82 \u27e8(mlog \u25e6P\u03b8)\u2217,P (V ), (mlog \u25e6P\u03b8)\u2217,P (W )\u27e9\n(\u03b1,\u03b2)\n= \u27e8mlog\u2217,P (V ),mlog\u2217,P (W )\u27e9(\u03b1,\u03b2)\n= g (\u03b1,\u03b2)-LE P (V,W ).\n(48)\n\u03b8-LCM: Let us first review a well-known fact of deformed metrics (Thanwerdas & Pennec, 2022a). Let g\u0303 = 1\u03b82 P \u2217 \u03b8 g be the power-deformed metric on SPD Then when \u03b8 tends to 0, for all P \u2208 Sn++ and all V \u2208 TPSn++, we have g\u0303P (V, V )\u2192 gI(log\u2217,P (V ), log\u2217,P (V )). (49)\nBy Eq. (49), we can readily obtain the results.\nProof of Prop. 5.2 . (\u03b1, \u03b2)-AIM is left-invariant (Thanwerdas & Pennec, 2022b). As the pullback of (\u03b1, \u03b2)-AIM, (\u03b8, \u03b1, \u03b2)-AIM is left-invariant as well. Besides, Chen et al. (2023b) shows that LCM is the pullback metric from the Euclidean space of Ln. Therefore, \u03b8-LCM is bi-invariant.\nProof of Thm. 5.3 . We denote Eqs. (13) to (15) onMi, i = 1, 2 as the mapping \u03bei(\u00b7|M,v2, B, s). Let B = {P1...N} and f(B) = {f(P1...N )}. The core of this proof lies in three points:\n1. The Fre\u0301chet mean and variance of B inM1 correspond to the counterparts of f(B) inM2. 2. \u03be1(Pi|M, v2, B, s) inM1 is equal to f\u22121(\u03be2(f(Pi)|f(M), v2, f(B), s)). 3. The updates of running statistics inM1 correspond to the counterparts inM2.\nWe denote M as the Fre\u0301chet mean of B, and v2 as the Fre\u0301chet variance of B. Then, by the isometry of f , the Fre\u0301chet mean and variance of f(B) are f(M) and v2, respectively.\nOnMi, i = 1, 2, we denote Li,\u2299i,Expi,Logi as the Lie group and Riemannian operators, Ei as the neutral element, and Eq. (14) as \u03d5is(\u00b7). With the isometry and Lie group isomorphism of f , we have the following equations:\nL1 M\u22121 \u22991 = f\u22121 \u25e6 L2 f(M)\u22121 \u22992 \u25e6 f, (50)\n\u03d51s = Exp 1 E1 [ sLog1E1(\u00b7) ] = f\u22121 ( Exp1E2 [ sLog2E2(f(\u00b7))\n]) = f\u22121 \u25e6 \u03d52s \u25e6 f, (51) L1B = f \u22121 \u25e6 L2f(B) \u25e6 f. (52)\nThen we have \u03be1(Pi|M,v2, B, s) = f\u22121(\u03be2(f(Pi)|f(M), v2, f(B), s)) (53)\nLastly, we show the correspondence between running statistics. Since the Fre\u0301chet variance is the same for both B and f(B), we focus on the running mean. Let Mr and f(Mr) denote the initial values of the running means inM1 andM2 respectively, and WFMi represent the weighted Fre\u0301chet mean inMi. Then the updated running mean inM1 is\nWFM1({1\u2212 \u03b3, \u03b3}, {Mr,M}) = f\u22121(WFM2({1\u2212 \u03b3, \u03b3}, {f(Mr), f(M)})) (54)\nwe can further simply the above equation as\nWFM1 = f\u22121 \u25e6WFM2 \u25e6f (55)\nDenoting LieBNi as the LieBN algorithm onMi, Eq. (53) and Eq. (55) imply that: LieBN1(Pi|B, s, \u03f5, \u03b3) = f\u22121 [ LieBN2(f(Pi)|f(B), s, \u03f5, \u03b3) ] . (56)"
        }
    ],
    "title": "A LIE GROUP APPROACH TO RIEMANNIAN BATCH NORMALIZATION",
    "year": 2024
}