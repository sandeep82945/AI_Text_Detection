{
    "abstractText": "The transferability of deep neural networks (DNNs) has made significant progress in image and language processing. However, due to the heterogeneity among tables, such DNN bonus is still far from being well exploited on tabular data prediction (e.g., regression or classification tasks). Condensing knowledge from diverse domains, language models (LMs) possess the capability to comprehend feature names from various tables, potentially serving as versatile learners in transferring knowledge across distinct tables and diverse prediction tasks, but their discrete text representation space is inherently incompatible with numerical feature values in tables. In this paper, we present TP-BERTa, a specifically pre-trained LM for tabular data prediction. Concretely, a novel relative magnitude tokenization converts scalar numerical feature values to finely discrete, high-dimensional tokens, and an intra-feature attention approach integrates feature values with the corresponding feature names. Comprehensive experiments demonstrate that our pre-trained TP-BERTa leads the performance among tabular DNNs and is competitive with Gradient Boosted Decision Tree models in typical tabular data regime.",
    "authors": [
        {
            "affiliations": [],
            "name": "TABULAR PREDICTION"
        },
        {
            "affiliations": [],
            "name": "Jiahuan Yan"
        },
        {
            "affiliations": [],
            "name": "Bo Zheng"
        },
        {
            "affiliations": [],
            "name": "Hongxia Xu"
        },
        {
            "affiliations": [],
            "name": "Yiheng Zhu"
        },
        {
            "affiliations": [],
            "name": "Danny Z. Chen"
        },
        {
            "affiliations": [],
            "name": "Jimeng Sun"
        },
        {
            "affiliations": [],
            "name": "Jian Wu"
        },
        {
            "affiliations": [],
            "name": "Jintai Chen"
        }
    ],
    "id": "SP:3f27d2ebf5318ba2e04771d7444dce4f18a900bb",
    "references": [
        {
            "authors": [
                "Takuya Akiba",
                "Shotaro Sano"
            ],
            "title": "Optuna: A next-generation hyperparameter optimization framework",
            "venue": "In KDD,",
            "year": 2019
        },
        {
            "authors": [
                "Sercan \u00d6 Arik",
                "Tomas Pfister"
            ],
            "title": "TabNet: Attentive interpretable tabular learning",
            "venue": "In AAAI,",
            "year": 2021
        },
        {
            "authors": [
                "Saqib Aziz",
                "Michael Dowling",
                "Helmi Hammami",
                "Anke Piepenbrink"
            ],
            "title": "Machine learning in finance: A topic modeling approach",
            "venue": "European Financial Management,",
            "year": 2022
        },
        {
            "authors": [
                "Vadim Borisov",
                "Tobias Leemann",
                "Kathrin Sessler",
                "Johannes Haug",
                "Martin Pawelczyk",
                "Gjergji Kasneci"
            ],
            "title": "Deep neural networks and tabular data: A survey",
            "venue": "TNNLS,",
            "year": 2022
        },
        {
            "authors": [
                "Vadim Borisov",
                "Kathrin Sessler",
                "Tobias Leemann",
                "Martin Pawelczyk",
                "Gjergji Kasneci"
            ],
            "title": "Language models are realistic tabular data generators",
            "venue": "In ICLR,",
            "year": 2023
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners",
            "venue": "NeurIPS,",
            "year": 2020
        },
        {
            "authors": [
                "Jintai Chen",
                "Kuanlun Liao"
            ],
            "title": "DANets: Deep abstract networks for tabular data classification and regression",
            "venue": "In AAAI,",
            "year": 2022
        },
        {
            "authors": [
                "Jintai Chen",
                "KuanLun Liao",
                "Yanwen Fang",
                "Danny Z. Chen",
                "Jian Wu"
            ],
            "title": "TabCaps: A capsule neural network for tabular data classification with BoW routing",
            "venue": "In ICLR,",
            "year": 2023
        },
        {
            "authors": [
                "Jintai Chen",
                "Jiahuan Yan",
                "Danny Ziyi Chen",
                "Jian Wu"
            ],
            "title": "Excelformer: A neural network surpassing GBDTs on tabular data",
            "venue": "arXiv preprint arXiv:2301.02819,",
            "year": 2023
        },
        {
            "authors": [
                "Tianqi Chen",
                "Carlos Guestrin"
            ],
            "title": "XGBoost: A scalable tree boosting system",
            "venue": "In KDD,",
            "year": 2016
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova"
            ],
            "title": "BERT: Pre-training of deep bidirectional Transformers for language understanding",
            "venue": "In NAACL,",
            "year": 2018
        },
        {
            "authors": [
                "James Dougherty",
                "Ron Kohavi",
                "Mehran Sahami"
            ],
            "title": "Supervised and unsupervised discretization of continuous features",
            "venue": "In ICML,",
            "year": 1995
        },
        {
            "authors": [
                "Tianyu Gao",
                "Adam Fisch",
                "Danqi Chen"
            ],
            "title": "Making pre-trained language models better few-shot learners",
            "venue": "In ACL,",
            "year": 2021
        },
        {
            "authors": [
                "Yury Gorishniy",
                "Ivan Rubachev"
            ],
            "title": "Revisiting deep learning models for tabular data",
            "venue": "In NeurIPS,",
            "year": 2021
        },
        {
            "authors": [
                "Yury Gorishniy",
                "Ivan Rubachev",
                "Artem Babenko"
            ],
            "title": "On embeddings for numerical features in tabular deep learning",
            "venue": "In NeurIPS,",
            "year": 2022
        },
        {
            "authors": [
                "Leo Grinsztajn",
                "Edouard Oyallon",
                "Gael Varoquaux"
            ],
            "title": "Why do tree-based models still outperform deep learning on typical tabular data",
            "venue": "NeurIPS,",
            "year": 2022
        },
        {
            "authors": [
                "Md Rafiul Hassan",
                "Sadiq Al-Insaif"
            ],
            "title": "A machine learning approach for prediction of pregnancy outcome following IVF treatment",
            "venue": "Neural Computing and Applications,",
            "year": 2020
        },
        {
            "authors": [
                "Kaiming He",
                "Haoqi Fan",
                "Yuxin Wu",
                "Saining Xie",
                "Ross Girshick"
            ],
            "title": "Momentum contrast for unsupervised visual representation learning",
            "venue": "In CVPR,",
            "year": 2020
        },
        {
            "authors": [
                "Noah Hollmann",
                "Samuel M\u00fcller",
                "Katharina Eggensperger",
                "Frank Hutter"
            ],
            "title": "Tabpfn: A transformer that solves small tabular classification problems in a second",
            "venue": "In ICLR,",
            "year": 2023
        },
        {
            "authors": [
                "Zhengbao Jiang",
                "Frank F Xu",
                "Jun Araki",
                "Graham Neubig"
            ],
            "title": "How can we know what language models know",
            "venue": "TACL, 8:423\u2013438,",
            "year": 2020
        },
        {
            "authors": [
                "Guolin Ke",
                "Qi Meng",
                "Thomas Finley",
                "Taifeng Wang",
                "Wei Chen",
                "Weidong Ma",
                "Qiwei Ye",
                "TieYan Liu"
            ],
            "title": "Lightgbm: A highly efficient gradient boosting decision tree",
            "venue": "NeurIPS,",
            "year": 2017
        },
        {
            "authors": [
                "Ron Kohavi",
                "Mehran Sahami"
            ],
            "title": "Error-based and entropy-based discretization of continuous features",
            "venue": "In KDD,",
            "year": 1996
        },
        {
            "authors": [
                "Liyao Li",
                "Haobo Wang",
                "Liangyu Zha",
                "Qingyi Huang",
                "Sai Wu",
                "Gang Chen",
                "Junbo Zhao"
            ],
            "title": "Learning a data-driven policy network for pre-training automated feature engineering",
            "venue": "In ICLR,",
            "year": 2023
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov"
            ],
            "title": "RoBERTa: A robustly optimized BERT pretraining approach",
            "year": 1907
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter"
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "In ICLR,",
            "year": 2019
        },
        {
            "authors": [
                "Fabio Petroni",
                "Tim Rockt\u00e4schel",
                "Sebastian Riedel",
                "Patrick Lewis",
                "Anton Bakhtin",
                "Yuxiang Wu",
                "Alexander Miller"
            ],
            "title": "Language models as knowledge bases",
            "venue": "In EMNLP,",
            "year": 2019
        },
        {
            "authors": [
                "Sergei Popov",
                "Stanislav Morozov",
                "Artem Babenko"
            ],
            "title": "Neural oblivious decision ensembles for deep learning on tabular data",
            "venue": "In ICLR,",
            "year": 2020
        },
        {
            "authors": [
                "Liudmila Prokhorenkova",
                "Gleb Gusev"
            ],
            "title": "CatBoost: Unbiased boosting with categorical features",
            "venue": "In NeurIPS,",
            "year": 2018
        },
        {
            "authors": [
                "Jing Qian",
                "Hong Wang",
                "Zekun Li",
                "Shiyang Li",
                "Xifeng Yan"
            ],
            "title": "Limitations of language models in arithmetic and symbolic induction",
            "venue": "In ACL,",
            "year": 2023
        },
        {
            "authors": [
                "Alec Radford",
                "Jeffrey Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "venue": "OpenAI Blog,",
            "year": 2019
        },
        {
            "authors": [
                "Gowthami Somepalli",
                "Avi Schwarzschild",
                "Micah Goldblum",
                "C Bayan Bruss",
                "Tom Goldstein"
            ],
            "title": "SAINT: Improved neural networks for tabular data via row attention and contrastive pre-training",
            "venue": "NeurIPS,",
            "year": 2022
        },
        {
            "authors": [
                "Weiping Song"
            ],
            "title": "AutoInt: Automatic feature interaction learning via self-attentive neural networks",
            "venue": "In CIKM,",
            "year": 2019
        },
        {
            "authors": [
                "Ruoxi Wang",
                "Rakesh Shivanna"
            ],
            "title": "DCN V2: Improved deep & cross network and practical lessons for web-scale learning to rank systems",
            "venue": "In WWW,",
            "year": 2021
        },
        {
            "authors": [
                "Zifeng Wang",
                "Jimeng Sun"
            ],
            "title": "TransTab: Learning transferable tabular Transformers across tables",
            "venue": "In NeurIPS,",
            "year": 2022
        },
        {
            "authors": [
                "Qizhe Xie",
                "Minh-Thang Luong",
                "Eduard Hovy",
                "Quoc V Le"
            ],
            "title": "Self-training with noisy student improves ImageNet classification",
            "venue": "In CVPR,",
            "year": 2020
        },
        {
            "authors": [
                "Jiahuan Yan",
                "Jintai Chen",
                "Yixuan Wu",
                "Danny Z Chen",
                "Jian Wu"
            ],
            "title": "T2G-Former: Organizing tabular features into relation graphs promotes heterogeneous feature interaction",
            "venue": "In AAAI,",
            "year": 2023
        },
        {
            "authors": [
                "Chao Ye",
                "Guoshan Lu",
                "Haobo Wang",
                "Liyao Li",
                "Sai Wu",
                "Gang Chen",
                "Junbo Zhao"
            ],
            "title": "Towards cross-table masked pretraining for web data mining",
            "venue": "In WWW,",
            "year": 2024
        },
        {
            "authors": [
                "Liangyu Zha",
                "Junlin Zhou",
                "Liyao Li"
            ],
            "title": "Tablegpt: Towards unifying tables, nature language and commands into one gpt",
            "venue": "arXiv preprint arXiv:2307.08674,",
            "year": 2023
        },
        {
            "authors": [
                "Tianping Zhang",
                "Shaowen Wang",
                "Shuicheng Yan",
                "Jian Li",
                "Qian Liu"
            ],
            "title": "Generative table pretraining empowers models for tabular prediction",
            "venue": "arXiv preprint arXiv:2305.09696,",
            "year": 2023
        },
        {
            "authors": [
                "Bingzhao Zhu",
                "Xingjian Shi",
                "Nick Erickson",
                "Mu Li",
                "George Karypis",
                "Mahsa Shoaran"
            ],
            "title": "XTab: Cross-table pretraining for tabular Transformers",
            "venue": "In ICML,",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Tabular data, a common data form, is pivotal in various fields such as medical trial predictions (Hassan et al., 2020) and financial risk detection (Aziz et al., 2022). The remarkable successes of deep neural networks (DNNs) in computer vision (CV) and natural language processing (NLP) have spurred interest in applying DNNs to tabular data for tasks like classification or regression (Popov et al., 2020; Song et al., 2019; Wang et al., 2021; Chen et al., 2023b), which also pave the road for cross-modality processing. However, most current research on tabular data relies on fully supervised paradigms (Arik & Pfister, 2021; Gorishniy et al., 2021; Somepalli et al., 2022; Li et al., 2023; Hollmann et al., 2023), and with typically limited data available for DNNs in this regime, Gradient Boosted Decision Trees (GBDTs) (Chen & Guestrin, 2016; Ke et al., 2017; Prokhorenkova et al., 2018) continue to outperform these paradigms (Grinsztajn et al., 2022).\nAs widely evidenced in the CV and NLP fields, the transferability of DNNs consistently brought about substantial performance boosts and decreased data demands in downstream tasks (Devlin et al., 2018; Xie et al., 2020; He et al., 2020). However, how to utilize the transferability of DNNs on tabular data is still much under-explored. One major obstacle is the feature heterogeneity among tables (Borisov et al., 2022; Yan et al., 2023; Chen et al., 2022). Unlike images, which often exhibit similar feature distributions (e.g., consistent pixel intensity ranges and color distributions) (Chen et al., 2023a), structured tables inherently contain diverse columns and feature spaces, leading to considerable heterogeneity and feature space shifts between pre-training and downstream datasets.\nRelated Work. Recent studies highlight the importance of tabular transfer learning, with initial efforts like TransTab (Wang & Sun, 2022) and XTab (Zhu et al., 2023) utilizing shared Transformer\n\u2217Work under partial support from the Second Affiliated Hospital Zhejiang University School of Medicine. \u2020Corresponding authors. Codes are available at https://github.com/jyansir/tp-berta.\nblocks in the FT-Transformer architecture (Gorishniy et al., 2021) for cross-table learning. TransTab focused on clinical trial tables with common feature names, facilitating partially overlapped feature embeddings, whereas XTab explored a broader domain with dataset-specific encoders. However, neither achieved comprehensive knowledge transfer, resulting in moderate pre-training performance. The advancements in language models (LMs) have demonstrated their capability to act as commonsense knowledge bases (Petroni et al., 2019; Jiang et al., 2020; Gao et al., 2021; Zha et al., 2023). Through self-supervised pre-training on extensive domain-agnostic corpora, LMs can implicitly capture associations among different words or phrases, showcasing potential as tabular transfer agents with their inherent support for feature name processing within a unified language space. Despite this potential, early attempts of applying LMs to tabular prediction were limited to synthetic table generation (e.g., missing value imputation) and faced challenges. GReaT (Borisov et al., 2023) and TapTap (Zhang et al., 2023) fine-tuned GPT-2 (Radford et al., 2019) on simply templated table texts, treating numerical values as strings, which led to insensitivity to such values (Qian et al., 2023). A contemporary work (Ye et al., 2024) developed a BERT-based model (CT-BERT) using a large tabular database and similar techniques to TransTab. However, these studies overlooked the customization of LMs for understanding continuous numerical values, which is a critical aspect of tables and presents challenges to LMs due to their inherent complexity and rarity (Qian et al., 2023).\nTo unlock LMs\u2019 power and take a pioneering step on LM-based tabular transfer learning, in this paper, we propose a tailored pre-trained LM for tabular prediction based on RoBERTa (Liu et al., 2019), called the Tabular Prediction adapted BERT approach (TP-BERTa). TP-BERTa maintains the strengths of LMs as well as possessing the sensitivity to numeric features. Specifically, TPBERTa discretizes numerical feature values as relative magnitude tokens (RMT) in order to treat them as some meaningful words in the LM\u2019s vocabulary. The design of relative magnitude tokens enables the LM to perceive relative value magnitudes in the language space. In this way, we decouple the representations of feature names and numerical values (compared to FT-Transformer, TransTab, and CT-BERT), preserving the semantic signal of feature names. Further, we develop a shared intrafeature attention (IFA) module to attentively fuse the embeddings of a feature\u2019s name and value into a single vector. IFA retains the text order in a feature name, and outputs a vector for each feature name-value pair to the subsequent LM process to achieve feature order-agnostic prediction.\nWe pre-train TP-BERTa on numerous large tabular datasets (101 binary classification and 101 regression datasets), and provide three versions (i.e., pre-trained on only classification tasks, or only regression tasks, or both). We conduct evaluations on extensive downstream datasets: (1) performance comparison with classical GBDTs, advanced deep tabular models, and cross-table models shows that our TP-BERTa (the pre-trained versions on a single task type, with default hyperparameters) outperforms the other tabular DNNs and is competitive with GBDTs in the overall rank on 145 downstream datasets; (2) comparison with two existing numerical encoding strategies (Borisov et al., 2023; Ye et al., 2024) shows that our RMT adaption achieves average AUC improvements of 12.45% and 3.44% on significantly changed (i.e., with AUC variation over 0.5%) downstream binary classification datasets, respectively; (3) ablation on table-specific designs for LM adaption.\nContributions. In a nutshell, our work offers: (1) A pre-trained LM for tabular data: dealing with fundamental difficulties in LM adaption to tabular data (i.e., numeric feature handling and tabular feature organization), we develop LM-based tabular DNNs and pre-train a tabular-data-tailored LM called TP-BERTa; (2) superior performances: comparisons with various existing methods on 145 downstream datasets demonstrate that pre-trained LMs can outperform common tabular DNNs and are competitive with GBDTs in typical tabular regime; (3) in-depth analysis: multi-facet comparison implies that TP-BERTa has a data appetite of informative discrete features, and key ablation experiments show that our RMT and IFA adaptions are successful.\n2 TP-BERTA: TABULAR PREDICTION ADAPTED BERT APPROACH\nOur proposed TP-BERTa is built on the basis of RoBERTa (Liu et al., 2019) as default. Its model architecture and key components (the relative magnitude tokenization approach and intra-feature attention module) are shown in Fig. 1. Below we introduce our novel (i) relative magnitude tokenization (RMT) for numerical value representation, (ii) intra-feature attention (IFA) module for feature name-value matching before the LM processing, and (iii) the overall pre-training paradigm."
        },
        {
            "heading": "2.1 RELATIVE MAGNITUDE TOKENIZATION",
            "text": "Tabular features can be roughly categorized into continuous type (i.e., numerical features) and discrete type (categorical, binary, or string features). Although discrete feature values with clear semantics (e.g., \u201cmale\u201d and \u201cfemale\u201d are values of a discrete feature \u201cgender\u201d) can be naturally understood by LMs, it is still difficult to make numerical features fully understandable to LMs due to their wide range of values and counter-intuitive meanings of exact numerical values. In this section, we present a novel Relative Magnitude Tokenization (RMT) approach to boost numerical value understanding.\nNumerical Discretization. Our RMT process is inspired by classical works on feature binning (Dougherty et al., 1995; Gorishniy et al., 2022) that utilized discretization techniques for numerical features. To deal with diverse labeled tabular datasets, we adopt a target-aware binning method similar to (Gorishniy et al., 2022). Specifically, the \u201cC4.5 Discretization\u201d algorithm (Kohavi & Sahami, 1996) is applied to each numerical feature by recursively splitting its value range guided by its label. This process is equivalent to building a decision tree, and continuous values are grouped into corresponding tree leaves. The boundary values of all the leaves are used to split the value range into multiple bins. Each numerical value is converted to its bin index after discretization, as:\ne(i) = C4.5(x(i),train,y(i),train), (1)\nBinIndex(x(i)j ) \u2261 k, (2)\nwhere x(i),train is the vector of the i-th numerical feature values in the training set, y(i),train is the corresponding labels, e(i) denotes the vector of leaf node boundary values (in ascending order), x(i)j is the i-th feature value of sample j, and k is its bin index if e(i)k \u2264 x (i) j < e (i) k+1. In TP-BERTa, we set the maximum numerical bin (magnitude token) number (denoted as nbin) to 256 (i.e., 0 \u2264 k < 256), unless otherwise specified. A bin index represents a relative magnitude in the value range.\nMagnitude Tokenization. To transform numerical values into the language space, we treat the numerical bins as new words. Specifically, nbin additional tokens are added to the RoBERTa vocabulary with randomly initialized token embeddings. Each numerical value is discretized with a feature-specific C4.5 process and mapped to these shared magnitude tokens. Since there may be a large number of values in a single numerical bin, the final token embedding of a numerical value is its corresponding bin token embedding multiplied with the value itself, i.e., RMT(x(i)j ) \u2261 Eextra:,k \u00d7x (i) j , where Eextra:,k denotes the k-th embedding of the RoBERTa additional vocabulary for the numerical magnitude. These embeddings are shared across any numerical features or datasets that purely represent relative magnitudes with word vectors. Just as LMs show general capability of language modeling based on reasonable pair-wise word similarity, we seek to make the designed \u201cmagnitude embeddings\u201d follow a similar relationship. Hence, we devise a magnitude-aware triplet loss to regularize the learning process of the magnitude embeddings. We formulate the regularization as:\nLreg = max(d(f(k1), f(k2))\u2212 d(f(k1), f(k3)) +m(k1, k2, k3), 0), s.t. | k1 \u2212 k2 | < | k1 \u2212 k3 |, (3)\nf(k) = LayerNorm(Linear(Eextra:,k )), (4)\nm(k1, k2, k3) = | k1 \u2212 k3 | \u2212 | k1 \u2212 k2 |\nnbin , (5)\nwhere k1, k2, and k3 are three bin indices, and d(x,y) is the L2 distance between vectors x and y. In a nutshell, this regularization process assists to pull the embedding of a bin close to the embedding of a nearby one, while pushing away from the embedding of a bin far away from it, serving as an auxiliary loss to help embedding learning for magnitude tokens.\nTabular Feature Pre-processing. A tabular sample may contain features of different types. We process each feature i by simply concatenating the embeddings of its feature name (Enamei \u2208 Rl1\u00d7d) and value (Evaluei \u2208 Rl2\u00d7d), i.e., Ei = Enamei \u2297 Evaluei , where d is the hidden dimension of the RoBERTa embeddings, l1 is the token length of the feature name, and l2 is the length of the feature value. Notably, l2 \u2261 1 for numerical features. As for categorical features, we convert their values into structured texts (e.g., value \u201c0\u201d of the feature \u201cgender\u201d is mapped to \u201cmale\u201d). Note that we do not distinguish binary and categorical ones in this paper since they are both converted to meaningful\ntexts. Some datasets contain string features, such as a feature \u201cmovie comment\u201d with unstructured texts. We process the values of these feature types in the same way as for feature names."
        },
        {
            "heading": "2.2 INTRA-FEATURE ATTENTION MODULE",
            "text": "Previous attempts of using LMs to process tables still face three lingering issues. (1) Targets in tabular predictions are independent of feature permutations, while LMs inherently process texts with positional encoding since positions of linguistic units matter. (2) When we simply feed all feature values with names into a vanilla LM (e.g., \u201c[Gender] is female, [Blood Pressure] is 123.8\u201d), it likely increases the training difficulty of LMs since they have to understand the correctly matched name-value pairs of features and learn to alleviate interference from other features. However, fully connected attention mechanism (commonly adopted in auto-encoder LMs) makes it inevitable to generate mismatched name-value signal. (3) Feeding the whole templated text can incur computation burden caused by excessively long sequences when the feature amount is large. Recently, a solution was given for issue (1) by augmenting a sample with copies of different feature permutations (Borisov et al., 2023), and position encoding was directly dropped and text order of feature names was ignored (Ye et al., 2024). But, they all neglected issues (2) and (3). Hence, we develop the intra-feature attention (IFA) module for feature refinement before feeding features to the LM.\nIFA is essentially a single multi-head self-attention (MHSA) module shared across all features and datasets. It accepts embeddings of a feature name-value pair and fuses them into a single vector. We formulate the process of IFA fusion on a single feature i as:\nH(i) = eCLS \u2297E(i), (6) Q(i) = W Tq (H (i) + P (i)), K(i) = W Tk (H (i) + P (i)), V (i) = W Tv H (i), (7)\nH\u0302(i) = MHSA(Q(i),K(i),V (i)), h\u0302(i) \u2261 H\u0302(i):,Index(CLS), (8)\nwhere E(i) \u2208 R(l1+l2)\u00d7d is concatenation of name-value embeddings, eCLS \u2208 R1\u00d7d is the [CLS] embedding, Wq,Wk, and Wv \u2208 Rd\u00d7d are transformations for query, key, and value vectors, and P (i) \u2208 R(1+l1+l2)\u00d7d is position embeddings. IFA uses the output vector at the [CLS] position h\u0302(i) as refined feature information and feeds it to the subsequent RoBERTa. It can be clearly seen that information from both the name and value is included in h\u0302(i), and information from other feature names or values cannot corrupt feature i\u2019s representation. As shown in Fig. 1(IFA process), the positions of the [CLS] token and magnitude token are assigned to id 0, and those of feature names are from 1 to l1. This design aims to make the [CLS] token pay more attention to values (which are probably more important for prediction) as well as keeping the text order of feature names. Notably, we remove position encoding on value vectors (see Eq. (7)); the key reason for this is to protect magnitude token embeddings from the impact of embeddings at a constant id position (e.g., position id 0). Since magnitude embeddings are randomly initialized and intentionally regularized to represent the meaning of the relative magnitude carefully, a constant signal may distort the representations and thus make the embedding learning process more difficult."
        },
        {
            "heading": "2.3 OVERALL TRAINING PARADIGM",
            "text": "After features are processed by the IFA module, an n-feature sample is organized as the concatenation of feature vectors and a [CLS] embedding to be the RoBERTa input, i.e., X \u2261 eCLS\u2297h\u03021\u2297h\u03022\u2297 \u00b7 \u00b7 \u00b7\u2297 h\u0302n \u2208 R(1+n)\u00d7d, which is computation-friendly. Since the text order of feature names has been considered in h\u0302i, we can avoid position encoding in this step, and achieve feature order-agnostic prediction. The prediction is based on the [CLS] output of the RoBERTa-Encoder, as:\ny\u0302m = PredictionHead(m)(RoBERTa-Encoder(X(m)):,Index(CLS)), (9) PredictionHead(x) = Dropout(Linear1(Tanh(Linear2(x)))), (10)\nwhere X(m) represents the input from the m-th task (dataset), and we use task-specific prediction heads PredictionHead(m), the shared RoBERTa, and the IFA module (constituting TP-BERTa) to perform supervised pre-training on extensively large tabular datasets. The final pre-training loss consists of supervised loss and regularization loss (see Eq. (3)), as:\nL = Lsup + \u03bbLreg, (11)\nwhere for the supervised loss Lsup, we use binary cross entropy loss for binary classification tasks and mean squared error loss for regression tasks. We keep a constant weight \u03bb \u2261 0.1 in pretraining. For downstream tasks, ordinary finetune is adopted only with Lsup. We exclude multi-class datasets in this work as in (Grinsztajn et al., 2022), for the reasons: (1) they can be decomposed into multiple binary classification tasks, (2) the trends on binary classification can essentially reflect the classification ability, and (3) multi-class datasets are not very common in tabular dataset collections."
        },
        {
            "heading": "3 EXPERIMENTS",
            "text": "We first compare our TP-BERTa with classical and advanced tabular prediction models, including (1) the dominating GBDTs, (2) advanced deep tabular models, and (3) recent open-source crosstable models or pre-trained tabular models. We utilize extensive downstream datasets, and analyze the huge potential of our pre-trained LM, TP-BERTa, as a powerful tabular prediction learner from the data perspective (Sec. 3.2). Based on that, we further demonstrate how the encoding strategy of numerical values impacts the LMs\u2019 performances, and discuss why they were neglected in previous tabular prediction research (Sec. 3.3). Transferability evaluations (Sec. 3.4) and design ablations (Sec. 3.5) are conducted to reflect the generalization capability and rational adaption of TP-BERTa."
        },
        {
            "heading": "3.1 EXPERIMENTAL DETAILS",
            "text": "Datasets. We leverage the high-quality large semantic tabular database TabPertNet (Ye et al., 2024). Datasets with at least 10,000 samples and no more than 32 features are taken for pre-training, and datasets with fewer than 10,000 samples are collected as downstream tasks (following the same \u201ctypical tabular data\u201d settings of \u201cmedium-sized dataset regime\u201d and \u201cnot high dimensional\u201d in (Grinsztajn et al., 2022)). We strictly remove the same datasets in the database and make sure that no subset of pre-training datasets (e.g., a small version of a large dataset) appears in the downstream ones. Since LMs are fueled by meaningful texts, we manually exclude datasets with uninformative feature names (e.g., feature names like \u201cv1, v2, x1, x2\u201d) or unmappable categorical features (e.g., a feature \u201cjob\u201d with values \u201c0, 1, 2\u201d). Note that those excluded datasets can still benefit from our model with simple feature preprocessing with their corresponding data dictionaries. In total, our pre-training datasets consist of 101 binary classification datasets and 101 regression datasets with about 10 million samples, and our downstream datasets consist of 80 binary classification datasets and 65 regression datasets. Detailed dataset statistics are provided in Appendix B.\nPre-training Details. Since our work does not focus on the curriculum learning issue, we warp all the datasets into a large data-loader, which provides a data batch from a randomly selected dataset per training step. Each dataset is learned with a dataset-specific prediction head and the shared TPBERTa (see Sec. 2.3). Because the massive LM is likely to overfit a single dataset, we use 5% of the training data as the validation set. For binary classification, we keep the same label distributions for the training set and validation set. Pre-training is conducted on four NVIDIA A100 Tensor Core GPUs, with a total batch size of 512 per step. We reuse the weights of the RoBERTa-base as the\nstarting point, and follow similar pre-training settings of RoBERTa (Liu et al., 2019): We use a total of 30 training epochs, with a linear warm-up for the first 6% of steps, followed by a linear decay to 0. The best checkpoint is saved by the average validation loss over all the datasets. We provide three TP-BERTa versions: pre-trained on only binary classification tasks, or only regression tasks, or both types. More detailed pre-training information and analysis are given in Appendix D.\nCompared Methods. We compare our TP-BERTa with (1) the representative non-deep learning models XGBoost (Chen & Guestrin, 2016) and CatBoost (Prokhorenkova et al., 2018); (2) known DNNs including MLP, TabNet (Arik & Pfister, 2021), AutoInt (Song et al., 2019), DCNv2 (Wang et al., 2021), FT-Transformer (FTT) (Gorishniy et al., 2021), and SAINT (Somepalli et al., 2022); (3) the recent open-source cross-table model TransTab (Wang & Sun, 2022) and pre-trained model XTab (Zhu et al., 2023). We split each finetune dataset ((64%, 16%, 20%) for training, validation, and testing separately), and keep the same label distribution in each split on binary classification.\nHyperparameter Tuning & Finetune. We implement our TP-BERTa with PyTorch and the HuggingFace Transformers package on Python 3.8. All the models are finetuned on NVIDIA RTX 3090. In training, we uniformly use a training batch size of 64 for all the DNNs. Since the LM takes an increased training time, we directly set fixed hyperparameters on the pre-trained TP-BERTa across all the downstream datasets without tuning. For the other DNNs, the optimizer is AdamW (Loshchilov & Hutter, 2019) with the default configuration except for the learning rate and weight decay rate. We follow the hyperparameter spaces from the original work for SAINT. For TransTab, we use its default hyperparameters without cross-table pre-training because it originally required partially overlapped medical tables. For XTab, we follow its settings that report the score of the best pre-trained checkpoint on the validation set, with other hyperparameters kept fixed. For XGBoost, CatBoost, and the other DNNs, we follow the default (for GBDTs and FT-Transformer) and tuning settings provided in (Gorishniy et al., 2021). Hyperparameter search is performed with the Optuna library (Akiba et al., 2019). More detailed information of hyperparameters is provided in Appendix E."
        },
        {
            "heading": "3.2 ARE PRE-TRAINED TP-BERTA GREAT TABULAR PREDICTION LEARNERS?",
            "text": "Overall Comparison. Table 1 reports the means and standard deviations of model ranks on two dataset collections. As expected, a similar trend as shown in (Grinsztajn et al., 2022) is attained: GBDTs (i.e., XGBoost and CatBoost) still outperform classical and advanced DNNs in typical tabu-\nlar regime (specified in \u201cDatasets\u201dof Sec. 3.1). Yet, it is worth noting that the pre-trained TP-BERTa exhibits a significantly different progress and competitive performances. This notable improvement may be attributed to the generalization ability of the pre-trained LMs (e.g., GPT-3 (Brown et al., 2020)). A medium-sized dataset (with < 10K points and low-dimensional features) may not have sufficient information for non-pre-trained DNNs, while LMs are able to leverage semantic information from feature names and structured values. Besides, our RMT approach further enables the LMs to handle numerical values in the language space (Sec. 3.3 discusses the necessity of RMT). Few previous deep tabular models were evaluated in such data settings, and this is the first time an extensive comparison on typical tabular data is brought to the forefront. As for cross-table models, TransTab was inspired by overlapped columns between pre-training datasets and downstream ones, which can benefit on domain datasets (e.g., medical tables), but general tables can contain many features from various domains, thus constraining its application. XTab adopted dataset-specific featurizers, though a Transformer backbone is shared; it misses the inherent relationship between features of different datasets and learns the feature embeddings from scratch, which may be trapped in insufficiently generalized data patterns. TP-BERTa is able to exploit feature semantics, e.g., the patterns learned on feature values \u201cmale & female\u201d in pre-training can be inherently transferred to \u201cboy & girl\u201d by LMs without compulsory need for overlapped features or dataset-specific encoders.\nComparison from the Feature Perspectives. Since LMs typically operate on discrete texts, we further investigate from the perspective of feature type distributions. We report ranks among the datasets with various feature type distributions in Table 1. One can see that TP-BERTa achieves stably better performances (both \u201cOursj\u201d and \u201cOurss\u2019) when the categorical feature type gradually becomes dominating (a larger \u03b1 or \u03b2). This can be intuitively explained by LMs\u2019 ability to understand meaningful structured values (as discrete strings). Even among the datasets with at least one categorical feature (\u03b1 > 0), TP-BERTa still leads the performances on both task types (also shown in Fig. 2). However, if all features are numerical (\u03b1 = 0), TP-BERTa performs inferiorly. This may be due to its LM nature that precision loss in numerical representation is inevitable. For a more detailed illustration, we show in Fig. 2 rank variation curve plots across datasets grouped in different ranges of feature type distributions. Overall, TP-BERTa is stably promising when discrete features begin to dominate in the datasets, while for purely numerical datasets, GBDTs or FTT are still better choices (especially for classification tasks). Since there can exist useless tabular features, we introduce the ratio of Shapley value sums between categorical and numerical features (i.e., \u03b2, the right column of Fig. 2); an expected smoother trend that TP-BERTa performs better on datasets with larger \u03b2 is observed. Additionally, we empirically observe: (1) XGBoost highly relies on hyperparameter tuning and thus performs unstably (shown in its standard deviations and Fig. 2); (2) in contrast, CatBoost, just using default hyperparameters, is often a good choice, especially on datasets in which categorical features dominate (the same was suggested in (Prokhorenkova et al., 2018)).\nComparison from the Data Volume Perspective. We show rank variations on data volumes in the Appendices (Fig. 5). Similar to studying the feature type distribution effects, we examine the trend in two dataset groups (\u03b2 < 0.1 and \u03b2 \u2265 0.1). In the typical tabular regime, the choices are mostly influenced by the distributions, while from the data scale dimension, no special trend is observed.\nJoint Task Type Pre-training. A more expensive TP-BERTa version is conducted on \u201cOursj\u201d by pre-training on binary classification and regression tasks jointly. A similar trend as \u201cOurss\u201d is observed, with a stably inferior performance. This may be due to: (1) incompatible natures of classification and regression tasks, (2) the dataset-specific head pre-training strategy is unsuitable for the combined patterns of classification and regression, or (3) a more powerful base LM is needed for such complicated data configuration. This will become a part of our future study.\nTakeaway. We empirically show a strong potential for well-adapted LMs to fill the void of previous DNN-based tabular learners under typical tabular data settings, and demonstrate the capability of TP-BERTa to handle tabular prediction tasks, especially those with informative categorical features, which can help architecture selection based on feature type characteristics in future studies."
        },
        {
            "heading": "3.3 WHY WERE LMS NEGLECTED ON TABULAR PREDICTION?",
            "text": "Only a few previous studies directly employed LM-based learners for tabular prediction tasks. Their numerical encoding strategies can be categorized into: (1) value string methods (directly treating numerical values as strings, e.g., GReaT (Borisov et al., 2023) and TapTap (Zhang et al., 2023)); (2) value-multiplied feature name embeddings (e.g., CT-BERT (Ye et al., 2024), FTT (Gorishniy et al., 2021), and TransTab (Wang & Sun, 2022)). Besides, they all fed a longer templated table text to LMs, which may further incur a heavy training burden. In this section, we compare our RMT approach with two other strategies, and conduct ablation study on the intra-feature attention (IFA) module to demonstrate that refining single-feature information before the LM processing is a better and computation-friendly adaption. Since each pre-training round is costly and is equivalent to evaluation under the same condition, we use the non-pre-trained TP-BERTa (initialized with the RoBERTa weights) for the subsequent comparison and ablation studies (marked in the Appendix tables). To show a more clear and quantifiable comparison, we conduct our analysis on the binary classification datasets (Table 2). To alleviate the impact of performance fluctuations caused by random seeds, we exclude the datasets on which the performance changes are insignificant (the column \u201c|\u2206| \u2264 0.5%\u201d) in average difference calculation (the column \u201cAvg. diff.\u201d). The following sections use a similar analysis method. We present the detailed results in the Appendices (Table 11).\nNumerical Encoding Strategy Comparison. After directly substituting RMT with \u201cvalue string\u201d (Value2Str) or \u201cvalue-multiplied feature name embeddings\u201d (VMFE), changes in performance are observed (the upper half of Table 2). Both the previous strategies hurt AUC scores on most significantly changed datasets with average declines of 12.45% and 3.44%, respectively. There are still 10 datasets with better performances on both substitutions. This is probably due to insufficient embedding learning: Since in non-pre-trained TP-BERTa, magnitude embeddings are randomly initialized, direct finetune on downstream tasks faces a risk of data inadequacy to learn precise representations.\nIFA Module Ablation. Performances and training time changes are reported in the lower half of Table 2, by removing IFA and directly feeding all feature names and values to the LM as done in previous works. A noticeable performance degradation occurs on 52 datasets (\u2206 < \u22120.5%) with an average AUC decline of 4.17% (on 52 + 14 = 66 datasets). This indicates that LMs are likely to be confused when they process a pile of unmatched feature name-value texts, giving\nthem additional burden to learn correct matchings while fully connected attention in Transformerbased LMs interacts a name with values from other features. The IFA module explicitly fuses the name-value pair of a single feature into a vector before passing it to the LM, guarding against noisy interactions from other features. Besides, a shorter input sequence length (equal to the feature amount) accelerates learning (e.g., see the 1.32 average training time ratio without IFA in Table 2).\nSince TP-BERTa adopts both magnitude-aware numerical encoding and intra-feature pre-processing before the LM process, it acquires a significantly better ability as well as friendly computation cost, becoming competitive with GBDTs and other deep models. Our comparison shows that simply treating tables as normal texts can pose difficulties for LMs to understand structured tabular features, thus decreasing their potential on high-precision demanding tasks such as tabular predictions."
        },
        {
            "heading": "3.4 TP-BERTA TRANSFERABILITY ON TABULAR DATA",
            "text": "Table 3 reports performance changes by comparing the non-pre-trained TP-BERTa (initialized by random weights or RoBERTa weights) with the pre-trained one (\u201cOurss\u201d in Table 1). Overall, over 3% AUC increase is attained on 26 (comparing to random weights) and 21 (comparing to RoBERTa weights) datasets using pre-training, and the average improvement on significantly changed datasets is 3.16% and 2.79%, respectively. It seems that using the RoBERTa weights is better than random weights, as LM weights have inherently entailed meaningful semantic knowledge. A more significant leap can be achieved by further pre-training on extensive tabular data. This indicates that LMs are also effective in transferring tabular data knowledge and suitable for cross-table pre-training."
        },
        {
            "heading": "3.5 THE NECESSITY OF OTHER DESIGN DETAILS",
            "text": "Since there are several key differences in our TP-BERTa design compared to the common Transformer-based LMs, we further examine their necessity by ablation studies. By evaluating different magnitude token numbers (nbin = 256 as default), we find that using 128 and 32 tokens yields an average AUC decline of 2.06% and 3.59%, respectively. This indicates that a larger nbin for numerical value representation benefits performances on most datasets, while a few tables favor a smaller nbin, which may be due to over-representation of excessive magnitude tokens. The detailed results with analysis are presented in Appendix C, which further discusses the regularization efforts on magnitude embeddings of the magnitude-aware triplet loss function (Eq. (3)) and the function of removing position encoding for value vectors (Eq. (7)). We find that the magnitude-aware triplet loss function potentially facilitates fast convergence and over-fitting reduction."
        },
        {
            "heading": "4 CONCLUSIONS AND FUTURE WORK",
            "text": "This paper undertook the first study of the substantial difficulties of continuous value representation and tabular feature organization in building LM-based tabular DNNs. We designed and deployed two bespoke adaptions, relative magnitude tokenization and intra-feature attention, to explore the possibilities of using pre-trained LMs on tabular prediction tasks. Our proposed TP-BERTa exhibits unprecedented progress over various non-LM DNNs, and is competitive with GBDTs under the typical tabular prediction regime, contributing a powerful DNN alternative for typical tabular data.\nWhile our approach has significantly improved the performance of language models in handling numerical features in tables, TP-BERTa currently excels more on tables dominated by categorical features. Besides, it was witnessed that some tables prefer a small magnitude token number. We will conduct more endeavors on better numerical representation in future tabular prediction studies."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "The research of Jiahuan Yan, Hongxia Xu and Jian Wu was supported in part by the National Natural Science Foundation of China under grants No. 62176231 and No. 82202984, and the Zhejiang Key R&D Program of China under grant No. 2023C03053. The research of Jintai Chen and Jimeng Sun was supported by NSF award SCH-2205289, SCH-2014438, and IIS-2034479."
        },
        {
            "heading": "A LIMITATIONS",
            "text": "Since our TP-BERTa relies on the semantic knowledge of LMs to transfer feature patterns from pre-training feature names to downstream ones, it implicitly requires meaningful and clear feature semantics. However, in the real world, there always exist tables with unclear feature names or values, such as quantum physics experiment tables containing lots of uncommon quantum descriptor feature names and medical domain substituting specific feature values with meaningless encoding to protect patient privacy. This suggests that LM-based tabular DNNs cannot take their inherent advantages of feature semantic understanding in privacy-sensitive (e.g., federated learning) or semantic-incomplete (missing original meanings in data collection) tables. For an uncommon domain, LMs pre-trained on domain corpora may be utilized as base models. Besides, LMs own a larger space of parameters and hyperparameters, making them more time-consuming in hyperparameter tuning with a potentially higher performance ceiling compared to non-LM tabular DNNs. Hence, we directly finetune TPBERTa with default hyperparameters for fairness of time, in which case it can achieve adequate results with less time than other tuned methods."
        },
        {
            "heading": "B DATASET INFORMATION AND MAIN EXPERIMENTAL DETAILS",
            "text": "We provide detailed information of the pre-training datasets in Table 6 and detailed information of the downstream datasets in Table 7 and Table 9. Detailed baseline performances are given in Table 8 and Table 10. To represent distributions of feature types in a dataset, we define \u03b1 as the feature amount ratio between the categorical type and numerical type. Since there exist datasets with usefulness features, we further define \u03b2 as the Shapley value (calculated with default XGBoost in Appendix E) sum ratio between categorical features and numerical features. These are used as references of feature type characteristics. Specifically, \u03b1 and \u03b2 of the i-th dataset are formulated as:\n\u03b1i = #Cat.(i)\n#Num.(i) , (12)\n\u03b2i = \u2211 f\u2208Cat.(i) Shapley value(f)\u2211 f\u2208Num.(i) Shapley value(f) . (13)\nWe exclude datasets with pure categorical features to avoid zero division, while as an LM, TPBERTa can inherently handle discrete features. We provide the dataset frequency in different feature type distribution ranges in Table 4."
        },
        {
            "heading": "C RESULTS AND ANALYSIS OF OTHER DESIGN COMPARISONS",
            "text": "The Number of Magnitude Tokens. In Sec. 2.1, we set the maximum number of magnitude tokens, nbin = 256, for TP-BERTa. In fact, the C4.5 decision tree splits the value range in a greedy fashion, and the actual number of leaves can be less than 256 (e.g., a small dataset with less than 256 points). Hence, we choose a conservative method to balance between \u201cnot too many new words to learn\u201d and \u201cenough precision for relative magnitude representation\u201d. In Table 5, we present the performance changes by setting nbin to 32 and 128 separately. As expected, the overall performance gradually drops when using a smaller token number to represent the numerical value magnitude. We find that some datasets favor a smaller nbin, which may be attributed to over-representation of too many magnitude tokens. Thus, a better solution is to set a large nbin for pre-training in order to enhance the upper limit of magnitude representation capability, and search for a reasonable dataset-specific nbin on downstream tasks.\nRegularization on Magnitude Embeddings. Since all the magnitude embeddings are randomly initialized, in Sec. 2.1, we propose a magnitude-aware triplet loss (see Eq. (3)) to assist the learning process. Fig. 3 presents the validation AUC curves of several datasets on using our regularization or not using it during finetuning, which shows that the designed regularization provides a potential of fast convergence and overfitting reduction. We use the triplet loss only in pre-training for a smoother and accelerated learning, and exclude it in actual finetune because the loss has converged in pre-training.\nNo Position Encoding for Value Vectors. In Eq. (7), we explicitly remove position encoding in value vectors of self-attention since position embeddings may distort randomly initialized magnitude embedding learning. Table 5 shows a major performance decline when adding position encoding. The reason for this probably lies in semantic corruption of magnitude tokens, since numerical values need precise representations to convey magnitude information."
        },
        {
            "heading": "D PRE-TRAINING DETAILS",
            "text": "Starting Point. We reuse the weights of the RoBERTa-base with the HuggingFace Transformers API. The additional nbin magnitude embeddings and the IFA module are randomly initialized.\nRuntime Environment. Pre-training is conducted with PyTorch version 1.9.0, CUDA version 11.3, and HuggingFace Transformers package version 4.18.0, using 4 NVIDIA A100 PCIe 40GB and 2 Intel 6248R 48C@3.0GHz.\nPre-training Process. We pre-train three versions of TP-BERTa: pre-training only on binary classification datasets, or only on regression datasets (these two versions constitute \u201cOurss\u201d in Table 1), or jointly pre-training on both types (\u201cOursj\u201d in Table 1). These three versions share the same maximum epoch number of 30 and the total batch size of 512, using the best average validation loss across all the datasets to save the checkpoint. The same pre-training learning rate and linear decay in (Liu et al., 2019) are used. The pre-training on binary classification tasks and regression tasks took 72 hours and 98 hours respectively, and the one on both types of tasks took 143 hours. We provide several loss curves during pre-training in Fig. 6 and validation metric curves on several pre-training datastes in Fig. 7.\nAnalysis. In most cases, the TP-BERTa version pre-trained jointly on both task types yields sightly lower validation scores on the pre-training datasets compared to the two TP-BERTa versions pretrained on a single task type (see Fig. 7), and the gap is more noticeable on binary classification datasets. This probably leads to a smaller overall rank difference between Oursj and Ourss on regression tasks (see Table 1)."
        },
        {
            "heading": "E HYPERPARAMETER TUNING",
            "text": "For the baselines of XGBoost, CatBoost, MLP, AutoInt, DCNv2, TabNet, and FT-Transformer, we reuse the implementations, default settings, and hyperparameter search spaces in (Gorishniy et al., 2021). For SAINT, the hyperparameter space in (Somepalli et al., 2022) is used. For TransTab and XTab, we follow the same settings as in (Zhu et al., 2023), using the default hyperparameters in (Wang & Sun, 2022) for TransTab and the best checkpoint on the validation set for XTab. As for TP-BERTa (including the joint pre-trained version \u201cOursj\u201d and the single pre-trained one \u201cOurs\u201d), we keep the default hyperparameters of 1e-5 learning rate without weight decay. All the baselines use AdamW (Loshchilov & Hutter, 2019) as the optimizer and the Optuna-driven tunning.\nF INTERPRETABILITY OF RMT\nIn Fig. 4, we visualize the TP-BERTa\u2019s 256 magnitude tokens by directly applying t-SNE algorithm using scikit-learn package (with default function parameters). Interestingly, even if we have randomly placed them in the language space at first, after pre-training a clear inherent distribution is captured, all tokens lie on a highly regular manifold and successfully maintain an intuitive assumption that the embedding of a numerical value should close to the embedding of a nearby one. This empirically demonstrates the TP-BERTa is sensitive to the numerical value magnitudes and benefits from the captured regular relationship among the relative magnitudes, which interprets its significant progress on the existing LM-based tabular models (e.g., directly treating values as raw strings)."
        },
        {
            "heading": "G EVALUATIONS ON OTHER DATA SCENARIOS",
            "text": "G.1 IMBALANCED LABEL DISTRIBUTION\nTo inspect the performances on imbalanced datasets, we create pivot tables by filtering datasets whose minor-class proportions, i.e., p = min(#positive,#negative)/#sample, are less than 1/3 (32 datasets), 1/5 (18 datasets), 1/8 (12 datasets), 1/20 (4 datasets) from 80 binary classification datasets. The results are reported in the upper part of Table 14. It is obvious that TP-BERTa outperforms baselines in moderate class-imbalance situations. In the extremely imbalanced situations (4 datasets, with p < 0.05), GBDTs showcase dominating performances, but TP-BERTa still outperforms most DNN approaches. Perhaps this is attributed to TP-BERTa leveraging the transferability and semantic understanding capabilities of the language model, thus resulting in consistent performance across various levels of data imbalance.\nG.2 MULTI-CLASS CLASSIFICATION\nWe additionally experiment on 32 downstream multi-class datasets from the database, the data statistics and results are reported in Table 12 and the middle part of Table 14 respectively, and the TPBERTa used here is the version pre-trained on 101 binary classification datasets.\nG.3 MEDICAL APPLICATIONS\nMedical data and labels hold inherently greater value than those from other domains. Therefore, we further inspect whether our pre-trained model can achieve notable performance on medical domain tasks, such as patient risk prediction and clinical trial outcome prediction, by leveraging knowledge learned and transferred from diverse domains, which typically offer more cost-effective data sources. We identified and filtered out all 25 medical tasks from 145 main experiment downstream datasets (statistics are given in Table 13). Refer to the bottom row of Table 14, the results indicate that the pre-trained TP-BERTa model significantly outperforms Gradient Boosting Decision Trees (GBDTs) and other Deep Neural Networks (DNNs) that are trained in the supervision manner and undergo meticulous hyperparameter tuning. However, our TP-BERTa achieved this without the need for hyperparameter tuning.\nThe noteworthy performance suggests that we can harness more affordable data sources to alleviate reliance on costly healthcare data in clinical practice. This illuminates a promising pathway towards reducing the need for extensive medical data and expediting the development of algorithms for healthcare applications."
        }
    ],
    "title": "MAKING PRE-TRAINED LANGUAGE MODELS GREAT",
    "year": 2024
}