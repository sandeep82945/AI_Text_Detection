{
    "abstractText": "We present a novel usage of Transformers to make image classification interpretable. Unlike mainstream classifiers that wait until the last fully-connected layer to incorporate class information to make predictions, we investigate a proactive approach, asking each class to search for itself in an image. We realize this idea via a Transformer encoder-decoder inspired by DEtection TRansformer (DETR). We learn \u201cclass-specific\u201d queries (one for each class) as input to the decoder, enabling each class to localize its patterns in an image via cross-attention. We name our approach INterpretable TRansformer (INTR), which is fairly easy to implement and exhibits several compelling properties. We show that INTR intrinsically encourages each class to attend distinctively; the cross-attention weights thus provide a faithful interpretation of the prediction. Interestingly, via \u201cmulti-head\u201d cross-attention, INTR could identify different \u201cattributes\u201d of a class, making it particularly suitable for fine-grained classification and analysis, which we demonstrate on eight datasets.",
    "authors": [],
    "id": "SP:6bd5fbbbb455c65d457e6e397bd5a17256989b6a",
    "references": [
        {
            "authors": [
                "David Bau",
                "Bolei Zhou",
                "Aditya Khosla",
                "Aude Oliva",
                "Antonio Torralba"
            ],
            "title": "Network dissection: Quantifying interpretability of deep visual representations",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Adrien Bibal",
                "R\u00e9mi Cardon",
                "David Alfter",
                "Rodrigo Wilkens",
                "Xiaoou Wang",
                "Thomas Fran\u00e7ois",
                "Patrick Watrin"
            ],
            "title": "Is attention explanation? an introduction to the debate",
            "venue": "In Proceedings of the 60th annual meeting of the association for computational linguistics,",
            "year": 2022
        },
        {
            "authors": [
                "Vanessa Buhrmester",
                "David M\u00fcnch",
                "Michael Arens"
            ],
            "title": "Analysis of explainers of black box deep neural networks for computer vision: A survey",
            "venue": "Machine Learning and Knowledge Extraction,",
            "year": 2021
        },
        {
            "authors": [
                "Nadia Burkart",
                "Marco F Huber"
            ],
            "title": "A survey on the explainability of supervised machine learning",
            "venue": "Journal of Artificial Intelligence Research,",
            "year": 2021
        },
        {
            "authors": [
                "Nicolas Carion",
                "Francisco Massa",
                "Gabriel Synnaeve",
                "Nicolas Usunier",
                "Alexander Kirillov",
                "Sergey Zagoruyko"
            ],
            "title": "End-to-end object detection with transformers",
            "venue": "In European conference on computer vision,",
            "year": 2020
        },
        {
            "authors": [
                "Mathilde Caron",
                "Hugo Touvron",
                "Ishan Misra",
                "Herv\u00e9 J\u00e9gou",
                "Julien Mairal",
                "Piotr Bojanowski",
                "Armand Joulin"
            ],
            "title": "Emerging properties in self-supervised vision transformers",
            "venue": "In Proceedings of the IEEE/CVF international conference on computer vision,",
            "year": 2021
        },
        {
            "authors": [
                "Diogo V Carvalho",
                "Eduardo M Pereira",
                "Jaime S Cardoso"
            ],
            "title": "Machine learning interpretability",
            "venue": "A survey on methods and metrics. Electronics,",
            "year": 2019
        },
        {
            "authors": [
                "Chaofan Chen",
                "Oscar Li",
                "Daniel Tao",
                "Alina Barnett",
                "Cynthia Rudin",
                "Jonathan K Su"
            ],
            "title": "This looks like that: deep learning for interpretable image recognition",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Arun Das",
                "Paul Rad"
            ],
            "title": "Opportunities and challenges in explainable artificial intelligence (xai): A survey",
            "venue": "arXiv preprint arXiv:2006.11371,",
            "year": 2020
        },
        {
            "authors": [
                "Jia Deng",
                "Wei Dong",
                "Richard Socher",
                "Li-Jia Li",
                "Kai Li",
                "Li Fei-Fei"
            ],
            "title": "Imagenet: A large-scale hierarchical image database",
            "venue": "IEEE conference on computer vision and pattern recognition,",
            "year": 2009
        },
        {
            "authors": [
                "Jon Donnelly",
                "Alina Jade Barnett",
                "Chaofan Chen"
            ],
            "title": "Deformable protopnet: An interpretable image classifier using deformable prototypes",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "Lucas Beyer",
                "Alexander Kolesnikov",
                "Dirk Weissenborn",
                "Xiaohua Zhai",
                "Thomas Unterthiner",
                "Mostafa Dehghani",
                "Matthias Minderer",
                "Georg Heigold",
                "Sylvain Gelly"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
            "venue": "In International conference on learning representations,",
            "year": 2021
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Gao Huang",
                "Zhuang Liu",
                "Geoff Pleiss",
                "Laurens Van Der Maaten",
                "Kilian Q Weinberger"
            ],
            "title": "Convolutional networks with dense connectivity",
            "venue": "IEEE transactions on pattern analysis and machine intelligence,",
            "year": 2019
        },
        {
            "authors": [
                "Sarthak Jain",
                "Byron C Wallace"
            ],
            "title": "Attention is not explanation. In Proceedings of the 2019 annual conference of the north american chapter of the association for computational linguistics, 2019",
            "year": 2019
        },
        {
            "authors": [
                "Aditya Khosla",
                "Nityananda Jayadevaprakash",
                "Bangpeng Yao",
                "Fei-Fei Li"
            ],
            "title": "Novel dataset for fine-grained image categorization: Stanford dogs",
            "venue": "In Proceedings CVPR workshop on fine-grained visual categorization (FGVC),",
            "year": 2011
        },
        {
            "authors": [
                "Sangwon Kim",
                "Jaeyeal Nam",
                "Byoung Chul Ko"
            ],
            "title": "Vit-net: Interpretable vision transformers with neural tree decoder",
            "venue": "In International conference on machine learning,",
            "year": 2022
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "arXiv preprint arXiv:1412.6980,",
            "year": 2014
        },
        {
            "authors": [
                "Pang Wei Koh",
                "Percy Liang"
            ],
            "title": "Understanding black-box predictions via influence functions",
            "venue": "In International conference on machine learning,",
            "year": 2017
        },
        {
            "authors": [
                "Jonathan Krause",
                "Michael Stark",
                "Jia Deng",
                "Li Fei-Fei"
            ],
            "title": "3d object representations for fine-grained categorization",
            "venue": "In Proceedings of the IEEE international conference on computer vision workshops,",
            "year": 2013
        },
        {
            "authors": [
                "Alex Krizhevsky",
                "Ilya Sutskever",
                "Geoffrey E Hinton"
            ],
            "title": "Imagenet classification with deep convolutional neural networks",
            "venue": "Communications of the ACM,",
            "year": 2017
        },
        {
            "authors": [
                "Tsung-Yi Lin",
                "Michael Maire",
                "Serge Belongie",
                "James Hays",
                "Pietro Perona",
                "Deva Ramanan",
                "Piotr Doll\u00e1r",
                "C Lawrence Zitnick"
            ],
            "title": "Microsoft coco: Common objects in context",
            "venue": "In European Conference on Computer Vision, pp",
            "year": 2014
        },
        {
            "authors": [
                "Pantelis Linardatos",
                "Vasilis Papastefanopoulos",
                "Sotiris Kotsiantis"
            ],
            "title": "Explainable ai: A review of machine learning interpretability methods",
            "year": 2020
        },
        {
            "authors": [
                "Ze Liu",
                "Yutong Lin",
                "Yue Cao",
                "Han Hu",
                "Yixuan Wei",
                "Zheng Zhang",
                "Stephen Lin",
                "Baining Guo"
            ],
            "title": "Swin transformer: Hierarchical vision transformer using shifted windows",
            "venue": "In Proceedings of the IEEE/CVF international conference on computer vision,",
            "year": 2021
        },
        {
            "authors": [
                "Subhransu Maji",
                "Esa Rahtu",
                "Juho Kannala",
                "Matthew Blaschko",
                "Andrea Vedaldi"
            ],
            "title": "Fine-grained visual classification of aircraft",
            "venue": "arXiv preprint arXiv:1306.5151,",
            "year": 2013
        },
        {
            "authors": [
                "Gabriela Montejo-Kovacevich",
                "Eva van der Heijden",
                "Nicola Nadeau",
                "Chris Jiggins"
            ],
            "title": "Cambridge butterfly wing collection batch 10, 2020",
            "venue": "URL https://doi.org/10.5281/zenodo.4289223",
            "year": 2020
        },
        {
            "authors": [
                "Meike Nauta",
                "Ron Van Bree",
                "Christin Seifert"
            ],
            "title": "Neural prototype trees for interpretable fine-grained image recognition",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Omkar M Parkhi",
                "Andrea Vedaldi",
                "Andrew Zisserman",
                "CV Jawahar"
            ],
            "title": "Cats and dogs",
            "venue": "In 2012 IEEE conference on computer vision and pattern recognition,",
            "year": 2012
        },
        {
            "authors": [
                "Roger Tory Peterson"
            ],
            "title": "A field guide to the birds: eastern and central North America",
            "year": 1999
        },
        {
            "authors": [
                "Vitali Petsiuk",
                "Abir Das",
                "Kate Saenko"
            ],
            "title": "Rise: Randomized input sampling for explanation of black-box models",
            "venue": "arXiv preprint arXiv:1806.07421,",
            "year": 2018
        },
        {
            "authors": [
                "Gerald Piosenka"
            ],
            "title": "Birds 525 species - image classification",
            "year": 2023
        },
        {
            "authors": [
                "Yao Qiang",
                "Deng Pan",
                "Chengyin Li",
                "Xin Li",
                "Rhongho Jang",
                "Dongxiao Zhu"
            ],
            "title": "Attcat: Explaining transformers via attentive class activation tokens",
            "venue": "In Advances in neural information processing systems,",
            "year": 2022
        },
        {
            "authors": [
                "Marco Tulio Ribeiro",
                "Sameer Singh",
                "Carlos Guestrin"
            ],
            "title": " why should i trust you?\" explaining the predictions of any classifier",
            "venue": "In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining,",
            "year": 2016
        },
        {
            "authors": [
                "Mattia Rigotti",
                "Christoph Miksovic",
                "Ioana Giurgiu",
                "Thomas Gschwind",
                "Paolo Scotton"
            ],
            "title": "Attention-based interpretability with concept transformers",
            "venue": "In International conference on learning representations,",
            "year": 2021
        },
        {
            "authors": [
                "Olga Russakovsky",
                "Jia Deng",
                "Hao Su",
                "Jonathan Krause",
                "Sanjeev Satheesh",
                "Sean Ma",
                "Zhiheng Huang",
                "Andrej Karpathy",
                "Aditya Khosla",
                "Michael Bernstein"
            ],
            "title": "Imagenet large scale visual recognition challenge",
            "venue": "International journal of computer vision,",
            "year": 2015
        },
        {
            "authors": [
                "Ramprasaath R Selvaraju",
                "Michael Cogswell",
                "Abhishek Das",
                "Ramakrishna Vedantam",
                "Devi Parikh",
                "Dhruv Batra"
            ],
            "title": "Grad-cam: Visual explanations from deep networks via gradient-based localization",
            "venue": "In Proceedings of the IEEE international conference on computer vision,",
            "year": 2017
        },
        {
            "authors": [
                "Sofia Serrano",
                "Noah A Smith"
            ],
            "title": "Is attention interpretable? In Proceedings of the annual meeting of the association for computational linguistics, 2019",
            "year": 2019
        },
        {
            "authors": [
                "Karen Simonyan",
                "Andrew Zisserman"
            ],
            "title": "Very deep convolutional networks for large-scale image recognition",
            "venue": "In International conference on learning representations,",
            "year": 2015
        },
        {
            "authors": [
                "Christian Szegedy",
                "Wei Liu",
                "Yangqing Jia",
                "Pierre Sermanet",
                "Scott Reed",
                "Dragomir Anguelov",
                "Dumitru Erhan",
                "Vincent Vanhoucke",
                "Andrew Rabinovich"
            ],
            "title": "Going deeper with convolutions",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2015
        },
        {
            "authors": [
                "Hugo Touvron",
                "Matthieu Cord",
                "Matthijs Douze",
                "Francisco Massa",
                "Alexandre Sablayrolles",
                "Herv\u00e9 J\u00e9gou"
            ],
            "title": "Training data-efficient image transformers & distillation through attention",
            "venue": "In International conference on machine learning,",
            "year": 2021
        },
        {
            "authors": [
                "Grant Van Horn",
                "Elijah Cole",
                "Sara Beery",
                "Kimberly Wilber",
                "Serge Belongie",
                "Oisin Mac Aodha"
            ],
            "title": "Benchmarking representation learning for natural world image collections",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Catherine Wah",
                "Steve Branson",
                "Peter Welinder",
                "Pietro Perona",
                "Serge Belongie"
            ],
            "title": "The caltech-ucsd birds-2002011",
            "year": 2011
        },
        {
            "authors": [
                "Jiaqi Wang",
                "Huafeng Liu",
                "Xinyue Wang",
                "Liping Jing"
            ],
            "title": "Interpretable image recognition by constructing transparent embedding space",
            "venue": "In Proceedings of the IEEE/CVF international conference on computer vision,",
            "year": 2021
        },
        {
            "authors": [
                "Sarah Wiegreffe",
                "Yuval Pinter"
            ],
            "title": "Attention is not not explanation",
            "venue": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),",
            "year": 2019
        },
        {
            "authors": [
                "Thomas Wolf",
                "Lysandre Debut",
                "Victor Sanh",
                "Julien Chaumond",
                "Clement Delangue",
                "Anthony Moi",
                "Pierric Cistac",
                "Tim Rault",
                "R\u00e9mi Louf",
                "Morgan Funtowicz"
            ],
            "title": "Transformers: State-of-the-art natural language processing",
            "venue": "In Proceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations,",
            "year": 2020
        },
        {
            "authors": [
                "Mengqi Xue",
                "Qihan Huang",
                "Haofei Zhang",
                "Lechao Cheng",
                "Jie Song",
                "Minghui Wu",
                "Mingli Song"
            ],
            "title": "Protopformer: Concentrating on prototypical parts in vision transformers for interpretable image recognition",
            "venue": "arXiv preprint arXiv:2208.10431,",
            "year": 2022
        },
        {
            "authors": [
                "Tingyi Yuan",
                "Xuhong Li",
                "Haoyi Xiong",
                "Hui Cao",
                "Dejing Dou"
            ],
            "title": "Explaining information flow inside vision transformers using markov chain. In eXplainable AI approaches for debugging and diagnosis",
            "year": 2021
        },
        {
            "authors": [
                "Quan-shi Zhang",
                "Song-Chun Zhu"
            ],
            "title": "Visual interpretability for deep learning: a survey",
            "venue": "Frontiers of Information Technology & Electronic Engineering,",
            "year": 2018
        },
        {
            "authors": [
                "Bolei Zhou",
                "Aditya Khosla",
                "Agata Lapedriza",
                "Aude Oliva",
                "Antonio Torralba"
            ],
            "title": "Object detectors emerge in deep scene cnns",
            "venue": "In International conference on learning representations,",
            "year": 2015
        },
        {
            "authors": [
                "Bolei Zhou",
                "Aditya Khosla",
                "Agata Lapedriza",
                "Aude Oliva",
                "Antonio Torralba"
            ],
            "title": "Learning deep features for discriminative localization",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Bolei Zhou",
                "David Bau",
                "Aude Oliva",
                "Antonio Torralba"
            ],
            "title": "Interpreting deep visual representations via network dissection",
            "venue": "IEEE transactions on pattern analysis and machine intelligence,",
            "year": 2018
        },
        {
            "authors": [
                "Xizhou Zhu",
                "Weijie Su",
                "Lewei Lu",
                "Bin Li",
                "Xiaogang Wang",
                "Jifeng Dai"
            ],
            "title": "Deformable detr: Deformable transformers for end-to-end object detection",
            "venue": "In International conference on learning representations,",
            "year": 2021
        },
        {
            "authors": [
                "Rigotti"
            ],
            "title": "2021), utilizes patch embeddings of an image as queries and attributes from the dataset as keys and values within a transformer. This approach allows the model to obtain multi-head attention weights, which are then used to interpret the model\u2019s predictions. However, a drawback of this method is that it relies on human-defined attribute annotations",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Mainstream neural networks for image classification (He et al., 2016; Simonyan & Zisserman, 2015; Krizhevsky et al., 2017; Huang et al., 2019; Szegedy et al., 2015; Dosovitskiy et al., 2021; Liu et al., 2021) typically allocate most of their model capacity to extract \u201cclass-agnostic\u201d feature vectors from images, followed by a fully-connected layer that compares image feature vectors with \u201cclass-specific\u201d vectors to make predictions. While these models have achieved groundbreaking accuracy, their model design cannot directly explain where a model looks for predicting a particular class.\nIn this paper, we investigate a proactive approach to classification, asking each class to look for itself in an image. We hypothesize that this \u201cclass-specific\u201d search process would reveal where the model looks, offering a built-in interpretation of the prediction.\nAt first glance, implementing this idea may need a significant model architecture design and a complex training process. However, we show that a novel usage of the Transformer encoder-decoder (Vaswani et al., 2017) inspired by DEtection TRansformer (DETR) (Carion et al., 2020) can essentially realize this idea, making our model fairly easy to reproduce and extend.\nConcretely, the DETR encoder extracts patch-wise features from the image, and the decoder attends to them based on learnable queries. We propose to learn \u201cclass-specific\u201d queries (one for each class) as input to the decoder, enabling the model to obtain \u201cclass-specific\u201d image features via self-attention and cross-attention \u2014 self-attention encodes the contextual information among candidate classes, determining the patterns necessary to distinguish between classes; cross-attention then allows each class to look for the distinctive patterns in the image. The resulting \u201cclass-specific\u201d image feature vectors (one for each class) are then compared with a shared \u201cclass-agnostic\u201d vector to predict the label of the image. We name our model INterpretable TRansformer (INTR). Figure 2 illustrates the model architecture. In the training phase, we learn INTR by minimizing the cross-entropy loss. In the inference phase, INTR allows us to visualize the cross-attention maps triggered by different \u201cclass-specific\u201d queries to understand why the model predicts or does not predict a particular class.\nOn the surface, INTR may fall into the debate of whether attention is interpretable (Jain & Wallace, 2019; Wiegreffe & Pinter, 2019; Bibal et al., 2022). However, we mathematically show that INTR offers faithful attention to distinguish between classes. In short, INTR computes logits by performing inner products between class-specific feature vectors and the shared class-agnostic vector. To classify\nan image correctly, the ground-truth class must obtain distinctive class-specific image features to claim the highest logit against other classes, which is possible only through distinct cross-attention weights. Minimizing the training loss thus encourages each class-specific query to produce distinct cross-attention weights. Manipulating the cross-attention weights in inference, as done in adversarial attacks to attention-based interpretation (Serrano & Smith, 2019), would alter the prediction notably.\nWe extensively analyze INTR, especially in cross-attention. We find that the \u201cmultiple heads\u201d in cross-attention could learn to identify different \u201cattributes\u201d of a class and consistently localize them in images, making INTR particularly well-suited for fine-grained classification. We validate this on multiple datasets, including CUB-200-2011 (Wah et al., 2011), Birds-525 (Piosenka, 2023), Oxford Pet (Parkhi et al., 2012), Stanford Dogs (Khosla et al., 2011), Stanford Cars (Krause et al., 2013), FGVC-Aircraft (Maji et al., 2013), iNaturalist-2021 (Van Horn et al., 2021), and Cambridge butterfly (Montejo-Kovacevich et al., 2020). Interestingly, by concentrating the decoder\u2019s input on visually similar classes (e.g., the mimicry in butterflies), INTR could attend to the nuances of patterns, even matching those found by biologists, suggesting its potential benefits to scientific discovery.\nIt is worth reiterating that INTR is built upon a widely-used Transformer encoder-decoder architecture and can be easily trained end-to-end. What makes it interpretable is the novel usage \u2014 incorporating class-specific information at the decoder\u2019s input rather than output. We view these as key strengths and contributions. They make INTR easily applicable, reproducible, and extendable."
        },
        {
            "heading": "2 BACKGROUND AND RELATED WORK",
            "text": ""
        },
        {
            "heading": "2.1 WHAT KIND OF INTERPRETATIONS ARE WE LOOKING FOR?",
            "text": "As surveyed in (Zhang & Zhu, 2018; Burkart & Huber, 2021; Carvalho et al., 2019; Das & Rad, 2020; Buhrmester et al., 2021; Linardatos et al., 2020), various ways exist to explain or interpret a model\u2019s prediction (see Appendix A for more details). Among them, the most popular is localizing where the model looks for predicting a particular class. We follow this notion and focus on fine-grained classification (e.g., bird and butterfly species). That is, not only do we want to localize the coarsegrained objects (e.g., birds and butterflies), but we also want to identify the \u201cattributes\u201d (e.g., wing patterns) that are useful to distinguish between fine-grained classes. We note that an attribute can be decomposed into \u201cobject part\u201d (e.g., head, tail, wing, etc.) and \u201cproperty\u201d (e.g., patterns on the wings), in which the former is commonly shared across all classes (Wah et al., 2011). We thus expect that our approach could identify the differences within a part between classes, not just localize parts."
        },
        {
            "heading": "2.2 BACKGROUND AND NOTATION",
            "text": "We denote an image and its ground-truth label by I and y, respectively. To perform classification over C classes, mainstream neural networks learn a feature extractor f\u03b8 to obtain a feature map\nX = f\u03b8(I) \u2208 RD\u00d7H\u00d7W . Here, \u03b8 denotes the parameters; D denotes the number of channels; H and W denote the number of grids in the height and width dimensions. For instance, ResNet (He et al., 2016) realizes f\u03b8 by a convolutional neural network (ConvNet) with residual links; Vision Transformer (ViT) (Dosovitskiy et al., 2021) realizes it by a Transformer encoder. Normally, this feature map is reshaped and/or pooled into a feature vector denoted by x = Vect(X), which then undergoes inner products with C class-specific vectors {wc}Cc=1. The class with the largest inner product is outputted as the predicted label,\ny\u0302 = argmaxc\u2208[C] w \u22a4 c x. (1)"
        },
        {
            "heading": "2.3 RELATED WORK ON POST-HOC EXPLANATION AND SELF-INTERPRETABLE METHODS",
            "text": "Since this classification process does not explicitly localize where the model looks to make predictions, the model is often considered a black box. To explain the prediction, a post-hoc mechanism is needed (Ribeiro et al., 2016; Koh & Liang, 2017; Yuan et al., 2021; Qiang et al., 2022; Zhou et al., 2015). For instance, CAM (Zhou et al., 2016) and Grad-CAM (Selvaraju et al., 2017) obtain class activation maps (CAM) by back-propagating class-specific gradients to the feature map. RISE (Petsiuk et al., 2018) iteratively masks out image contents to identify essential regions for classification. These methods have been widely used. However, they are often low-resolution (e.g., blurred or indistinguishable across classes), computation-heavy, and not necessarily aligned with how models make predictions.\nTo address these drawbacks, another branch of work designs models with interpretable prediction processes, incorporating explicit mechanisms that allow for a direct understanding of the predictions (Wang et al., 2021; Donnelly et al., 2022; Rigotti et al., 2021; Kim et al., 2022; Bau et al., 2017; Zhou et al., 2018). For example, ProtoPNet (Chen et al., 2019) compares the feature map X to \u201clearnable prototypes\u201d of each class, resulting in a feature vector x whose elements are semantically meaningful: the d-th dimension corresponds to a prototypical part of a certain class and x[d] indicates its activation in the image. By reading x and visualizing the activated prototypes, one could better understand the model\u2019s decision. Inspired by ProtoPNet, ProtoTree (Nauta et al., 2021) arranges the comparison to prototypes in a tree structure to mimic human reasoning; ProtoPFormer (Xue et al., 2022) presents a Transformer-based realization of ProtoPNet, which was originally based on ConvNets. Along with these interpretable decision processes, however, come specifically tailored architecture designs and increased complexity of the training process, often making them hard to reproduce, adapt, or extend. For instance, ProtoPNet requires a multi-stage training strategy, each stage taking care of a portion of the learnable parameters including the prototypes."
        },
        {
            "heading": "3 INTERPRETABLE TRANSFORMER (INTR)",
            "text": ""
        },
        {
            "heading": "3.1 MOTIVATION AND BIG PICTURE",
            "text": "Taking into account the pros and cons of the above two paradigms, we ask, Can we obtain interpretability via standard neural network architectures and standard learning algorithms?\nTo respond to \u201cinterpretability\u201d, we investigate a proactive approach to classification, asking each class to search for its presence and distinctive patterns in an image. Denote by S the set of candidate classes; we propose a new classification rule,\ny\u0302 = argmaxc\u2208[C] w \u22a4g\u03d5(f\u03b8(I), c,S), (2)\nwhere g\u03d5(f\u03b8(I), c,S) represents the image feature vector extracted specifically for class c in the context of S, and w denotes a binary classifier determining whether class c is present in the image I . Compared to Equation 1, the new classification rule in Equation 2 incorporates class-specific information in the feature extraction stage, not in the final fully-connected layer. As will be shown in subsection 3.4, this design is the key to generating faithful attention for interpretation.\nTo respond to \u201cstandard neural network architectures\u201d, we find that the Transformer encoder-decoder (Vaswani et al., 2017), which is widely used in object detection (Carion et al., 2020; Zhu et al., 2021) and natural language processing (Wolf et al., 2020), could essentially realize Equation 2. Specifically, the encoder extracts the image feature map X = f\u03b8(I). For the decoder, we propose to learn C class-specific queries {z(c)in }Cc=1 as input, enabling it to extract the feature vector g\u03d5(f\u03b8(I), z (c) in ,S) for class c via cross-attention.\nTo ease the description, let us first focus on cross-attention, the key building block in Transformer decoders in subsection 3.2. We then introduce our full model in subsection 3.3."
        },
        {
            "heading": "3.2 INTERPRETABLE CLASSIFICATION VIA CROSS-ATTENTION",
            "text": "Cross-attention. Cross-attention can be seen as a (soft) retrieval process. Given an input query vector zin \u2208 RD, it finds similar vectors from a vector pool and combines them via weighted average. In our application, this pool corresponds to the feature map X . Without loss of generality, let us reshape the feature map X \u2208 RD\u00d7H\u00d7W to X = [x1, \u00b7 \u00b7 \u00b7 ,xN ] \u2208 RD\u00d7N . That is, X contains N = H \u00d7W feature vectors representing each spatial grid in an image; each vector xn is D-dimensional.\nWith zin and X , cross-attention performs the following sequence of operations. First, it projects zin and X to a common embedding space such that they can be compared, and separately projects X to another space to emphasize the information to be combined, q = Wqzin \u2208 RD, K = WkX \u2208 RD\u00d7N , V = WvX \u2208 RD\u00d7N . (3) Then, it performs an inner product between q and K, followed by Softmax, to compute the similarities between zin and vectors in X , and uses the similarities as weights to combine vectors in V linearly,\nzout = V \u00d7 Softmax( K\u22a4q\u221a\nD ) \u2208 RD, (4)\nwhere \u221a D is a scaling factor based on the dimensionality of features. In other words, the output of cross-attention is a vector zout that aggregates information in X according to the input query zin.\nClass-specific queries. Inspired by the inner workings of cross-attention, we propose to learn C \u201cclass-specific\u201d query vectors Zin = [z (1) in , \u00b7 \u00b7 \u00b7 , z (C) in ] \u2208 RD\u00d7C , one for each class. We expect each of these queries to look for the \u201cclass-specific\u201d distinctive patterns in X . The output vectors Zout = [z (1) out , \u00b7 \u00b7 \u00b7 , z (C) out ] \u2208 RD\u00d7C thus should encode whether each class finds itself in the image,\nZout = V \u00d7 Softmax( K\u22a4Q\u221a\nD ) \u2208 RD\u00d7C , where Q = WqZin \u2208 RD\u00d7C . (5)\nWe note that the Softmax is taken over elements of each column; i.e., in Equation 5, each column in Zin attends to X independently. We use superscript/subscript to index columns in Z/X .\nClassification rule. We compare each vector in Zout to a learnable \u201cpresence\u201d vector w \u2208 RD to determine whether each class is found in the image. The predicted class is thus\ny\u0302 = argmaxc\u2208[C] w \u22a4z (c) out . (6)\nTraining. As each class obtains a logit w\u22a4z(c)out , we employ the cross-entropy loss,\n\u2113(I, y) = \u2212 log exp(w \u22a4z (y) out )\u2211\nc\u2032 exp (w \u22a4z (c\u2032) out )\n, (7)\ncoupled with stochastic gradient descent (SGD) to optimize the learnable parameters, including Zin, w, and the projection matrices Wq, Wk, and Wv in Equation 3. This design responds to the final piece of question in subsection 3.1, \u201cstandard learning algorithms\u201d.\nInference and interpretation. We follow Equation 6 to make predictions. Meanwhile, each column of the cross-attention weights Softmax(K\n\u22a4Q\u221a D\n) in Equation 5 reveals where each class looks to find itself, enabling us to understand why the model predicts or does not predict a class. We note that this built-in interpretation does not incur additional computation costs like post-hoc explanation.\nMulti-head attention. It is worth noting that a standard cross-attention block has multiple heads. It learns multiple sets of matrices (Wq, r,Wk, r,Wv, r) in Equation 3, r \u2208 {1, \u00b7 \u00b7 \u00b7 , R}, to look for different patterns in X , resulting in multiple Softmax(K\n\u22a4 r Qr\u221a D\n) and Zout, r in Equation 5. This enables the model to identify different \u201cattributes\u201d of a class and allows us to visualize them.\nIn training and inference, {Zout, r}Rr=1 are concatenated row-wise, followed by another learnable matrix Wo to obtain a single Zout as in Equation 5,\nZout = Wo[Z \u22a4 out, 1, \u00b7 \u00b7 \u00b7 ,Z\u22a4out, R]\u22a4 \u2208 RD. (8)\nAs such, Equation 7 and Equation 6 are still applicable to optimize the model and make predictions."
        },
        {
            "heading": "3.3 OVERALL MODEL ARCHITECTURE (SEE FIGURE 2 FOR AN ILLUSTRATION)",
            "text": "We implement our full INterpretable TRansformer (INTR) model (cf. Equation 2) using a Transformer decoder (Vaswani et al., 2017) on top of a feature extractor f\u03b8 that produces a feature map X . Without loss of generality, we use the DEtection TRansformer (DETR) (Carion et al., 2020) as the backbone. DETR uses a Transformer decoder of multiple layers; each contains a cross-attention block. The output vectors of one layer become the input vectors of the next layer. In DETR, the input to the decoder (at its first layer) is a set of object proposal queries, and we replace it with our learnable \u201cclass-specific\u201d query vectors Zin = [z (1) in , \u00b7 \u00b7 \u00b7 , z (C) in ] \u2208 RD\u00d7C . The Transformer decoder then outputs the \u201cclass-specific\u201d feature vectors Zout that will be fed into Equation 6.\nFigure 2: Model architecture of INTR. See subsection 3.2 for details.\nUsing a Transformer decoder rather than a single cross-attention block has several advantages. First, with multiple decoder layers, the learned queries Zin can improve over layers by grounding themselves on the image. Second, the self-attention block in each decoder layer allows class-specific queries to exchange information to encode the context. (See Appendix C for details.) As shown in Figure 11, the cross-attention blocks in later layers can attend to more distinctive patterns.\nTraining. INTR has three sets of learnable parameters: a) the parameters in the DETR backbone, including f\u03b8; b) the class-specific input queries Zin \u2208 RD\u00d7C to the decoder; and c) the class-agnostic vector w. We train all these parameters end-to-end via SGD, using the loss in Equation 7."
        },
        {
            "heading": "3.4 HOW DOES INTR LEARN TO PRODUCE INTERPRETABLE CROSS-ATTENTION WEIGHTS?",
            "text": "We analyze how INTR offers interpretability. For brevity, we focus on the model in subsection 3.2.\nAttention vs. interpretation. There has been an ongoing debate on whether attention offers faithful interpretation (Wiegreffe & Pinter, 2019; Jain & Wallace, 2019; Serrano & Smith, 2019; Bibal et al., 2022). Specifically, Serrano & Smith (2019) showed that significantly manipulating the attention weights at inference time does not necessarily change the model\u2019s prediction. Here, we provide a mathematical explanation for why INTR may not suffer from the same problem. The key is in our classification rule. In Equation 6, we obtain the logit for class c by w\u22a4z(c)out . If c is the ground-truth label, it must obtain a logit larger than other classes c\u2032 \u0338= c to make a correct prediction. This implies z (c) out \u0338= z (c\u2032) out , which is possible only if the cross-attention weights triggered by z (c) in are different from those triggered by other class-specific queries z(c \u2032)\nin (cf. Equation 4 and Equation 5 for how z (c) out is\nconstructed). Minimizing the training loss in Equation 7 thus would force each learnable query vector z (c) in ,\u2200c \u2208 [C], to be distinctive and able to attend to class-specific patterns in the input image.\nUnveiling the inner workings. We dig deeper to understand what INTR learns. For class c to obtain a high logit in Equation 6, z(c)out must have a large inner product with the class-agnostic w. We note that z(c)out is a linear combination of V = [v1, \u00b7 \u00b7 \u00b7 ,vN ] \u2208 RD\u00d7N (cf. Equation 4), and V is obtained by applying a projection matrix Wv to the feature map X = [x1, \u00b7 \u00b7 \u00b7 ,xN ] \u2208 RD\u00d7N (cf. Equation 3). Let q(c) = Wqz (c) in and let \u03b1 (c) = Softmax(K \u22a4q(c)\u221a D ) \u2208 RN , the logit w\u22a4z(c)out can be rewritten as\nw\u22a4z (c) out = w \u22a4V \u03b1(c) \u221d[w\u22a4v1, \u00b7 \u00b7 \u00b7 ,w\u22a4vN ]exp(K\u22a4q(c)) \u221d[w\u22a4Wvx1, \u00b7 \u00b7 \u00b7 ,w\u22a4WvxN ]exp ( (WkX) \u22a4q(c) )\n(9)\n\u221d[s1, \u00b7 \u00b7 \u00b7 , sN ]exp([q(c) \u22a4 Wkx1, \u00b7 \u00b7 \u00b7 , q(c) \u22a4 WkxN ] \u22a4) = \u2211 n sn \u00d7 \u03b1(c)[n],\nwhere sn = w\u22a4Wvxn and \u03b1(c)[n] \u221d exp(q(c) \u22a4 Wkxn). We note that sn does not depend on the class-specific query z(c)in . It only depends on the input image I , or more specifically, the feature map X and how it aligns with the vector w. In other words, we can view sn as an \u201cimage-specific\u201d salient score for patch n. In contrast, \u03b1(c)[n] depends on the class-specific query z(c)in ; its value will be high if class c finds the distinctive patterns in patch n.\nBuilding on this insight and Equation 9, if class c is the ground-truth class, what its query z(c)in needs to do is putting its attention weights \u03b1(c) on those high-score patches. Namely, class c must find its distinctive patterns in the salient image regions. Putting things together, we can view the roles of Wv and Wk as \u201cdisentanglement\u201d. They disentangle the information in xn into \u201cimage-specific\u201d and \u201cclassification-specific\u201d components \u2014 the former highlights \u201cwhether a patch should be looked at\u201d; the latter highlights \u201cwhat distinctive patterns it contains\u201d. When multi-head cross-attention is used, each pair of (Wv, Wk) can learn to highlight an object \u201cpart\u201d and the distinctive \u201cproperty\u201d in that part. These together offer the opportunity to localize the \u201cattributes\u201d of a class. Please see Appendix B for more details and discussions."
        },
        {
            "heading": "3.5 COMPARISON TO CLOSELY RELATED WORK",
            "text": "ProtoPNet (Chen et al., 2019) and Concept Transformers (CT) (Rigotti et al., 2021). INTR is fundamentally different in two aspects. First, both methods aim to represent image patches by a set of learnable vectors (e.g., prototypes in ProtoPNet; concepts in CT *). The resulting features for image patches are then pooled into a vector x and undergo a fully-connected layer for classification. In other words, their classification rules still follow Equation 1. In contrast, INTR extracts classspecific features from the image (one per class) and uses a new classification rule to make predictions (cf. Equation 6). Second, both methods require specifically designed training strategies or signals. For example, CT needs human annotations to learn the concepts. In contrast, INTR is based on a standard model architecture and training algorithm and requires no additional human supervision.\nDINO-v1 (Caron et al., 2021). DINO-v1 shows that the \u201c[CLS]\u201d token of a pre-trained ViT (Dosovitskiy et al., 2021) can attend to different \u201cparts\u201d of objects via multi-head attention. While this shares some similarities with our findings in INTR, what INTR attends to are \u201cattributes\u201d that can be used to distinguish between fine-grained classes, not just \u201cparts\u201d that are shared among classes.\n4 EXPERIMENTS Table 1: Dataset statistics. (# images are rounded.) Bird CUB BF Fish Dog Pet Car Craft\n# Train Img. 85K 6K 5K 45K 12K 4K 8K 3K # Test Img. 3K 6K 1K 2K 9K 4K 8K 3K # Classes 525 200 65 183 120 37 196 100 Dataset. We consider eight fine-grained datasets from various domains, including Birds-525 (Bird) (Piosenka, 2023), CUB200-2011 Birds (CUB) (Wah et al., 2011), Cambridge butterfly (BF) (Montejo-Kovacevich et al., 2020), iNaturalist-2021-Fish (Fish) (Van Horn et al., 2021), Stanford Dogs (Dog) (Khosla et al., 2011), Stanford Cars (Car) (Krause et al., 2013), Oxford Pet (Pet) (Parkhi et al., 2012), and FGVC Aircraft (Craft) (Maji et al., 2013). We create the BF dataset by considering the species-level labels from (Montejo-Kovacevich et al., 2020). For the Fish dataset, we extract species from the taxonomical Class named Animalia Chordata Actinopterygii in iNaturalist-2021. Table 1 provides the dataset statistics. See Appendix D for additional details.\nModel. We implement INTR on top of the DETR backbone (Carion et al., 2020). DETR stacks a Transformer encoder on top of a ResNet as the feature extractor. We use its DETR-ResNet-50 version, in which the ResNet-50 (He et al., 2016) was pre-trained on ImageNet-1K (Russakovsky et al., 2015; Deng et al., 2009) and the whole model including the Transformer encoder-decoder (Vaswani et al., 2017) was further trained on MSCOCO (Lin et al., 2014).\u2020 We remove its prediction heads located on top of the decoder and add our class-agnostic vector w; we remove its object proposal queries and\n*Even though CT applies cross-attention, it uses image patches as queries to attend to the concept embeddings; the outputs of cross-attention are thus features for image patches.\n\u2020We understand this may cause a concern of data leakage and unfair comparison. We note that MSCOCO only offers bounding boxes for objects, not for parts, and it does not contain fine-grained labels. Regarding fair comparisons, our work is not to claim higher accuracy but to offer a new perspective. We use DETR to demonstrate that our idea can be easily compatible with pre-trained encoder-decoder (foundation) models.\nadd our C learnable class-specific queries (e.g., for CUB, C = 200). See Figure 2 for an illustration and subsection 3.3 for more details. We further remove the positional encoding that was injected into the cross-attention keys in the DETR decoder: we find this information adversely restricts our queries to look at particular grid locations and leads to artifacts. We note that DETR sets its feature map size D \u00d7H \u00d7W (at the encoder output) as 256\u00d7 H032 \u00d7 W0 32 , where H0 and W0 are the height and width resolutions of the input image. For example, a typical CUB image is of a resolution roughly 800\u00d7 1200; thus, the resolution of the feature map and cross-attention map is roughly 25\u00d7 38. We investigate other encoders and the number of attention heads and decoder layers in Appendix F.\nVisualization. We visualize the last (i.e., sixth) decoder layer, whose cross-attention block has eight heads. We superimpose the cross-attention weight (maps) on the input images.\nTraining detail. The hyper-parameter details such as epochs, learning rate, and batch size for training INTR are reported in Appendix E. We use the Adam optimizer (Kingma & Ba, 2014) with its default hyper-parameters. We train INTR using the StepLR scheduler with a learning rate drop at 80 epochs. The rest of the hyper-parameters follow DETR.\nBaseline. We consider two sets of baseline methods. First, we use a ResNet-50 (He et al., 2016) pre-trained on ImageNet-1K and fine-tune it on each dataset. We then use Grad-CAM (Selvaraju et al., 2017) and RISE Petsiuk et al. (2018) to construct post-hoc saliency maps: the results are kept in Appendix F. Second, we compare to models designed for interpretability, such as ProtoPNet (Chen et al., 2019), ProtoTree (Nauta et al., 2021), and ProtoPFormer (Xue et al., 2022). We understand that these are by no means a comprehensive set of existing works. Our purpose in including them is to treat them as references for what kind of interpretability INTR can offer with its simple design.\nEvaluation. We reiterate that achieving a high classification accuracy is not the goal of this paper. The goal is to demonstrate the interpretability. We thus focus our evaluation on qualitative results.\n4.1 EXPERIMENTAL RESULTS Table 2: Accuracy(%) comparison.\nModel Bird CUB BF Fish Dog Pet Car Craft ResNet 98.5 83.8 95.6 71.9 77.1 89.5 89.3 80.9 INTR 97.4 71.8 95.0 81.1 72.5 90.4 86.8 76.1 Accuracy comparison. It is crucial to emphasize that the primary objective of INTR is to promote interpretability, not to claim high accuracy. Nevertheless, we report in\nTable 2 the classification accuracy of INTR and ResNet-50 on all eight datasets. INTR obtains comparable accuracy on most of the datasets except for CUB (12% worse) and Fish (9.2% better). We note that both CUB and Bird datasets focus on fine-grained bird species. The main difference is that the Bird dataset offers higher-quality images (e.g., cropped to focus on objects). INTR\u2019s accuracy drop on CUB thus more likely results from its inability to handle images with complex backgrounds or small objects, not its inability to recognize bird species.\nComparison to interpretable models. We compare INTR to ProtoPNet, ProtoTree, and ProtoPFormer (Figure 3). For the compared methods, we show the responses of the top three prototypes (sorted by their activations in the image) of the ground-truth class. For INTR, we show the top three cross-attention maps (sorted by the peak un-normalized attention weight in the map) triggered by the ground-true class. INTR can identify distinctive attributes similarly to the other methods. In particular, INTR is capable of localizing tiny attributes (like patterns of beaks and eyes): unlike the other methods, INTR does not need to pre-define the patch size of a prototype or attribute."
        },
        {
            "heading": "4.2 FURTHER ANALYSIS OF INTR",
            "text": "INTR can consistently identify attributes. We first analyze whether different cross-attention heads identify different attributes of a class and if those attributes are consistent across images of the same\nclass. Figure 1 shows a result (please see the caption for details). Different columns correspond to different heads, and we see that each captures a distinct attribute that is consistent across images. Some of them are very fine-grained, such as Head-4 (tail pattern) and Head-5 (breast color). The reader may notice the less concentrated attention in the last row. Indeed, it is a misclassified case: the query of the ground-truth class (i.e., Painted Bunting) cannot find itself in the image. This showcases how INTR interprets incorrect predictions. We show more results in Appendix H.\nINTR is applicable to a variety of domains. Figure 4 shows the cross-attention results on all eight datasets. (See the caption for details.) INTR can identify the attributes well in all of them, demonstrating its remarkable generalizability and applicability.\nINTR offers meaningful interpretation about attribute modulation. We investigate INTR\u2019s response to image modulation by deleting (the first block of Figure 5) and adding (the second block of Figure 5) important attributes. We obtain human-identified attributes of Red-winged Blackbird (the first block) and Orchard Oriole (the second block) from (Cor) and manipulate them accordingly. As shown in Figure 5, INTR is sensitive to the attribute changes; the cross-attention maps change drastically at the manipulated parts. These results suggest that INTR\u2019s inner working is heavily dependent on attributes to make correct classifications.\nINTR can attend differently based on the context. As mentioned in subsection 3.3, the selfattention block in INTR\u2019s decoder could encode the context of candidate classes to determine the patterns necessary to distinguish between them. When all the class-specific queries (e.g., 65 classes in the BF dataset) are inputted to the decoder, INTR needs to identify sufficient patterns (e.g., both coarse-grained and fine-grained) to distinguish between all of them. Here, we investigate whether limiting the input queries to visually similar ones would encourage the model to attend to finer-grained attributes. We focus on the BF dataset and compare two species, Heliconius melpomene (blue box in Figure 6) and Heliconius elevatus (green box in Figure 6), whose visual difference is very subtle. We limit the input queries by setting other queries as zero vectors. As shown in Figure 6, this modification does allow INTR to localize nuances of patterns between the two classes.\nLimitations. INTR learns C class-specific queries that must be inputted to the Transformer decoder jointly. This could increase the training and inference time if C is huge, e.g., larger than the number of grids N in the feature map. Fortunately, fine-grained classification usually focuses on a small set of visually similar classes; C is usually not large."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "We present Interpretable Transformer (INTR), a simple yet effective interpretable classifier building upon standard Transformer encoder-decoder architectures. INTR makes merely two changes: learning class-specific queries (one for each class) as input to the decoder and learning a class-agnostic vector on top of the decoder output to determine whether a class is present in the image. As such, INTR can be easily trained end-to-end. During inference, the cross-attention weights triggered by the winning class-specific query indicate where the model looks to make the prediction. We conduct extensive experiments and analyses to demonstrate the effectiveness of INTR in interpretation. Specifically, we show that INTR can localize not only object parts like bird heads but also attributes (like patterns around eyes) that distinguish one bird species from others. In addition, we present a mathematical explanation of why INTR can learn to produce interpretable cross-attention for each class without ad-hoc model design, complex training strategies, and auxiliary supervision. We hope that our study can offer a new way of thinking about interpretable machine learning."
        },
        {
            "heading": "A RELATED WORK",
            "text": "In recent years, there has been a significant increase in the size and complexity of models, prompting a surge in research and development efforts focused on enhancing model interpretability. The need for interpretability arises not only from the goal of instilling trust in a model\u2019s predictions but also from the desire to comprehend the reasoning behind a model\u2019s predictions, gain insight into its internal mechanisms, and identify the specific input features it relies on to make accurate predictions. Numerous research directions have emerged to facilitate model interpretation for human understanding. One notable research direction involves extracting and visualizing the salient regions in an input image that contribute to the model\u2019s prediction. By identifying these regions, researchers aim to provide meaningful explanations that highlight the relevant aspects of the input that influenced the model\u2019s decision. Existing efforts in this domain can be broadly categorized into post hoc methods and self-interpretable models.\nPost hoc methods involve applying interpretation techniques after a model has been trained. These methods focus on analyzing the model\u2019s behavior without modifying its architecture or training process. Most CNN-based classification processes lack explicit information on where the model focuses its attention during prediction. Post hoc methods address this limitation by providing interpretability and explanations for pre-trained black box models without modifying the model itself. For instance, CAM (Zhou et al., 2016) computes a weighted sum of feature maps from the last convolutional layer based on learned fully connected layer weights, generating a single heat map highlighting relevant regions for the predicted class. GRAD-CAM (Selvaraju et al., 2017) employs gradient information flowing into the last convolutional layer to produce a heatmap, with the gradients serving as importance weights for feature maps, emphasizing regions with the greatest impact on the prediction. Koh & Liang (2017) introduce influence functions, which analyze gradients of the model\u2019s loss function with respect to training data points, providing a measure of their influence on predictions. Another approach in post hoc methods involves perturbing or sampling the input image. For example, LIME (Ribeiro et al., 2016) utilizes superpixels to generate perturbations of the input image and explain predictions of a black box model. RISE (Petsiuk et al., 2018) iteratively blocks out parts of the input image, classifies the perturbed image using a pre-trained model, and reveals the blocked regions that lead to misclassification. However, post hoc methods for model interpretation can be computationally expensive, making them less scalable for real-world applications. Moreover, these methods may not provide precise explanations or a comprehensive understanding of how the model makes decisions, affecting the reliability and robustness of the interpretation results obtained.\nSelf-interpretable models are designed with interpretability as a core principle. These models incorporate explicit mechanisms or structures that allow for a direct understanding of their decisionmaking process. One direction is prototype-based models. Prototypes are visual representations of concepts that can be used to explain how a model works. The first work of using prototypes to describe the DNN model\u2019s prediction is ProtoPNet (Chen et al., 2019), which learns a predetermined number of prototypical parts (prototypes) per class. To classify an image, the model calculates the\nsimilarity between a prototype and a patch in the image. This similarity is measured by the distance between the two patches in latent space. Inspired by ProtoPNet, ProtoTree (Nauta et al., 2021) is a hierarchical neural network architecture that learns class-agnostic prototypes approximated by a decision tree. This significantly decreases the required number of prototypes for interpreting a prediction than ProtoPNet.\nProtoPNet and its variants were originally designed to work with CNN-based backbones. However, they can also be used with ViTs (Vision Transformer) by removing the class token. This approach, however, has several limitations. First, prototypes are more likely to activate in the background than in the foreground. When activated in the foreground, their activation is often scattered and fragmented. Second, prototype-based methods are computationally heavy and require domain knowledge to fix the parameters. With the widespread use of transformers in computer vision, many approaches have been proposed to interpret their classification predictions. These methods often rely on attention weights to visualize the important regions in the image that contribute to the prediction. ProtoPformer addresses this problem by applying the prototype-based method to ViTs. However, these prototypebased methods are computationally expensive and require domain knowledge to set the parameters. ProtoPformer (Xue et al., 2022) works on solving the problem by applying the prototype-based method with ViTs. However, these prototype-based works are computationally heavy and require domain knowledge to fix the parameters. ViT-Net (Kim et al., 2022) integrates ViTs and trainable neural trees based on ProtoTree, which only uses ViTs as feature extractors without fully exploiting their architectural characteristics. Another recent work, Concept Transformer (Rigotti et al., 2021), utilizes patch embeddings of an image as queries and attributes from the dataset as keys and values within a transformer. This approach allows the model to obtain multi-head attention weights, which are then used to interpret the model\u2019s predictions. However, a drawback of this method is that it relies on human-defined attribute annotations for the dataset, which can be prone to errors and is costly as it necessitates domain expert involvement."
        },
        {
            "heading": "B ADDITIONAL DETAILS OF INNER WORKINGS AND VISUALIZATION",
            "text": "Interpretability vs. model capacity. We investigate whether the conventional classification rule in Equation 1 induces the same property discussed in subsection 3.4. We replace w\u22a4z(c)out with w\u22a4c z (c) out ; i.e., we learn for each class a class-specific wc. This can be thought of as increasing the model capacity by introducing additional learnable parameters. The resulting classification rule is\ny\u0302 = argmaxc\u2208[C] w \u22a4 c z (c) out . (10)\nHere, even if z(c)out = z (c\u2032) out , class c can still claim the highest logit as long as z (c) out has a larger inner product with wc than other w\u22a4c\u2032z (c\u2032) out . Namely, even if the cross-attention weights triggered by different class-specific queries are identical,\u2021 as long as the extracted features in X are correlated strongly enough with class c, the model can still predict correctly. Thus, the learnable queries z (c) in ,\u2200c \u2208 [C], need not necessarily learn to produce distinct and meaningful cross-attention weights.\nIndeed, as shown in Figure 7, we implement a variant of our approach INTR-FC with its classification rule replaced by Equation 10. INTR produces more distinctive (column-wise) and consistent (rowwise) attention.\nVisualization. In subsection 3.4 of the main paper, we show how the logit of class c can be decomposed into\nw\u22a4z (c) out = \u2211 n sn \u00d7 \u03b1(c)[n],\nwhere sn = w\u22a4Wvxn; (11)\n\u03b1(c)[n] \u221d exp(q(c) \u22a4 Wkxn) = exp(z (c) in \u22a4 W\u22a4q Wkxn).\nThe index n corresponds to a grid location (or column) in the feature map X \u2208 RD\u00d7N .\n\u2021In the extreme case, one may consider the weights to be uniform, i.e., 1 N , at all spatial grids for all classes.\nBased on Equation 11, to predict an input image as class c, the cross-attention map \u03b1(c) triggered by the class-specific query z(c)in should align with the image-specific scores [s1, \u00b7 \u00b7 \u00b7 , sN ]. In other words, for an image that is predicted as class c, the cross-attention map \u03b1(c) very much implies which grids in an image have higher scores. Hence, in the qualitative visualizations, we only show the cross-attention map \u03b1(c) rather than the image-specific scores.\nWe note that throughout the whole paper, INTR learns to identify attributes that are useful to distinguish classes without relying on the knowledge of human experts."
        },
        {
            "heading": "C ADDITIONAL DETAILS OF MODEL ARCHITECTURES",
            "text": "Our idea in subsection 3.2 can be realized by the standard Transformer decoder (Vaswani et al., 2017) on top of any \u201cclass-agnostic\u201d feature extractors that produce a feature map X (e.g., ResNet (He et al., 2016) or ViT (Dosovitskiy et al., 2021)). A Transformer decoder often stacks M layers of the same decoder architecture denoted by {Lm}Mm=1. Each layer Lm takes a set of C vector tokens as input and produces another set of C vector tokens as output, which can then be used as the input to the subsequent layer Lm+1. In our application, the learnable \u201cclass-specific\u201d query vectors Zin = [z (1) in , \u00b7 \u00b7 \u00b7 , z (C) in ] \u2208 RD\u00d7C are the input tokens to L1.\nWithin each decoder layer is a sequence of building blocks. Without loss of generality, let us omit the layer normalization, residual connection, and Multi-Layer Perceptron (MLP) operating on each token independently, but focus on the Self-Attention (SA) and the subsequent Cross-Attention (CA) blocks.\nAn SA block is very similar to the CA block introduced in subsection 3.1. The only difference is the pool of vectors to be retrieved \u2014 while a CA block attends to the feature map extracted from the image, the SA block attends to its input tokens. That is, in an SA block, the X \u2208 RD\u00d7N matrix in Equation 3 is replaced by the input matrix Zin \u2208 RD\u00d7C . This allows each query token z(c)in \u2208 RD to combine information from other query tokens, resulting in a new set of C query tokens. This new set of query tokens is then fed into a CA block that attends to the image features in X to generate the \u201cclass-specific\u201d feature tokens.\nAs a Transformer decoder stacks multiple layers, the input tokens to the second layers and beyond possess not only the \u201clearnable\u201d class-specific information in Zin but also the class-specific feature information from X . We note that an SA block can aggregate information not only from similar tokens\u00a7 but also from dissimilar tokens. For example, when Wq is an identity matrix and Wk = \u2212Wq, a pair of similar tokens in Zin will receive smaller weights than a pair of dissimilar tokens. This allows similar query tokens to be differentiated if their relationships to other tokens are different, enabling the model to distinguish between semantically or visually similar fine-grained classes.\n\u00a7In this paragraph, this refers to the similarity in the inner product space."
        },
        {
            "heading": "D DETAILS OF DATASETS",
            "text": "We present the detailed dataset statistics in Table 3. We download the butterfly (BF) dataset from the Heliconiine Butterfly Collection Records\u00b6 at the University of Cambridge. These downloaded datasets exhibit class imbalances. To address this, we performed a selection process on the downloaded data as follows: First, we consider classes with a minimum of B images, where B is set to 20. Subsequently, for each class, we retained at least K images for testing, with K set to 3. Throughout this process, we also ensured that we had no more than M training images, where M is defined as 5 times the quantity (B \u2212K). The resulting dataset statistics are presented in Table 3."
        },
        {
            "heading": "E DETAILS OF EXPERIMENTAL SETUP",
            "text": "During our experiment, for all datasets, except for Bird, we set the learning rate to 1\u00d7 e\u22124, while for Bird, we use a learning rate of 5 \u00d7 e\u22125. Additionally, we utilize a batch size of 16 for Bird, Dog, and Fish datasets, and a batch size of 12 for the other datasets. Furthermore, the number of epochs required for training is 100 for BF and Pet datasets, 170 for Dog, and 140 for remaining datasets."
        },
        {
            "heading": "F ADDITIONAL EXPERIMENTAL RESULTS",
            "text": "Performance with different encoder backbones. In subsection 4.1, we demonstrate that INTR yields consistent results when compared to ResNet50. We further delve into an analysis of INTR\u2019s performance using various architectural configurations, specifically by employing different encoder backbones. Our objective is to ascertain whether INTR can potentially achieve superior performance with an alter-\nnative encoder backbone. To investigate this, we employ DeiT (Touvron et al., 2021) and ViT (Dosovitskiy et al., 2021) models pre-trained on ImageNet-1K and ImageNet-21K dataset respectively. Specifically, we utilize DeiT-Small (INTR-DeiT-S-1K) and ViT-Huge (INTR-ViT-H-21K). The results of our investigation are presented in Table 4: we see that using different encoders can impact the accuracy. Specifically, on CUB where we see a huge drop of INTR in Table 2, using a ViT-Huge backbone can achieve a 11.4% improvement.\nWe further perform ablation studies on different numbers of attention heads and decoder layers. The results are reported in Table 5. We find that the setup by DETR (i.e., 8 heads and 6 decoder layers) performs the best.\n\u00b6https://zenodo.org/record/3477412\nComparisons to post-hoc explanation methods. We use Grad-CAM (Selvaraju et al., 2017) and RISE (Petsiuk et al., 2018) to construct post-hoc saliency maps on the ResNet-50 (He et al., 2016) classifiers. We also report the insertion and deletion metric scores (Petsiuk et al., 2018) to quantify the results. It is worth mentioning that insertion and deletion metrics were designed to quantify post-hoc explanation methods. However, here we show a comparison between INTR, RISE and Grad-CAM. We examine the CUB dataset images that are accurately classified by both ResNet50 and INTR, resulting in\na reduced count of 3, 582 validation images. We generate saliency maps using Grad-CAM, RISE and INTR and then rank the patches to assess the insertion and deletion metrics. For a fair comparison, we employ ResNet-50 as a shared classifier for evaluation. The results are reported in Table 6.\nWe further compare the attention map of INTR (averaged over eight heads) to the saliency map of Grad-CAM and RISE (using ResNet-50). All methods can visualize the model\u2019s responses to different classes. For INTR, this is through the cross-attention triggered by different queries. In Figure 8, we show a correctly (left) and a wrongly (right) classified example. The two test images on the top are from the same class (i.e., Rufous Hummingbird), and we visualize the model\u2019s responses to four candidate classes (row-wise; exemplar images are provided for reference); the first row is the groundtruth class. Resolution-wise, both INTR and Grad-CAM show sharper saliency. Discrimination-wise, INTR clearly identifies where each class sees itself \u2014 each attention map highlights where the candidate class and the test image look alike. Such a message is not clear from Grad-CAM and RISE. Interestingly, in the case where both INTR and ResNet-50 predict wrongly (into the fourth class: Ruby-Throated Hummingbird), INTR is able to interpret the decision: the similarity between the test image and the fourth class seems more pronounced compared to the true class. Indeed, the notable attribute of the true class (i.e., the bright orange head) is not clearly shown in the test image in Figure 8 (b)."
        },
        {
            "heading": "G ADDITIONAL DISCUSSIONS",
            "text": "In this section, we discuss how biologists recognize organisms, how ornithologists (or birders) recognize bird species, and how our algorithm INTR approaches fine-grained classification.\nBiologists use traits \u2014 the characteristics of an organism describing its physiology, morphology, health, life history, demographic status, and behavior \u2014 as the basic units for understanding ecology and evolution. Traits are determined by genes, the environment, and the interactions among them. Peterson (1999) || created the modern bird field guide where he identified key markings to help the birder identify the traits that are distinctive to a species and separate it from other closely related or\n||This field guide presents the idea of highlighting key features from images to identify species and distinguish them from closely related species. In the field guide, Peterson used expert knowledge to draw a synthetic representation of the species and pointed arrows to the key features that would focus a birder\u2019s attention when in the field to a few defining traits that would help the observer to correctly identify it to species.\naligned species. These traits are grouped into four categories: 1) habitat and context, 2) size and morphology, 3) color and pattern, and 4) behavior. Figure 1 shows that INTR can extract the first three categories from the static images, while behavior typically requires videos.\nMore specifically, bird field marks that are used by ornithologists often center around two main bird parts: the head and the wing**. Patterns of contrasting colors in these regions are often key to distinguishing closely related and therefore similar-looking species. Painted bunting males are nearly impossible to confuse with almost any other species of bunting \u2014 the stunning patches of color are highlighted in the results presented in Figure 1. For more dissimilar species, aspects of the shape and overall color pattern are generally more important \u2014 as well as habitat and behavior. The characteristic traits of the three species in Figure 9 correspond broadly to these features \u2014 the long pointed wings of the albatross; plump body of the finch; diminutive feet of the hummingbird. Overall, INTR is able to capture features at both of these scales, mimicking the process used by humans guided by experts."
        },
        {
            "heading": "H ADDITIONAL QUALITATIVE RESULTS AND ANALYSIS",
            "text": "Figure 10 offers additional results of Figure 3. In Figure 3 of the main paper, we visualize the top three cross-attention heads or prototypes. Figure 10 further shows all the prototypes or attention heads for the same test image featuring the Painted Bunting species.\nClass-specific queries are improved over decoder layers. As mentioned in section 4 and Appendix B, our implementation of INTR has six decoder layers; each contains one cross-attention block. In qualitative results, we only show the cross-attention maps from the sixth layer, which produces the class-specific features that will be compared with the class-agnostic vector for prediction (cf. Equation 6). For the cross-attention blocks in other decoder layers, their output feature tokens become the input (query) tokens to the subsequent decoder layers. That is, the class-specific queries will change (and perhaps, improve) over layers.\nTo illustrate this, we visualize the cross-attention maps produced by each decoder layer. The results are in Figure 11. The attention maps improve over layers in terms of the attributes they identify so as to differentiate different classes.\nFigure 12, Figure 13, Figure 14, Figure 15, Figure 16, and Figure 17 showcase the cross-attention maps associated with the dataset Bird, BF, Dog, Pet, Craft and Car respectively, following the same format of Figure 1. Remarkably, different heads of INTR for each dataset successfully identify different attributes of classes, and these attributes remain consistent across images of the same class. INTR provides the interpretation of its prediction on different datasets across domains.\n**Please be referred to (Cor) and (Bir)."
        }
    ],
    "year": 2023
}