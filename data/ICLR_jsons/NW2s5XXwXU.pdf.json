{
    "abstractText": "Diffusion models are acclaimed for generating high-quality and diverse images. However, their performance notably degrades when trained on data with a longtailed label distribution. Traditional methods typically tackle this problem by adjusting the proportion of samples from head and tail classes, yet overlook connections between samples. In this paper, we introduce a novel approach in diffusion models that facilitates the transfer of knowledge between head and tail classes based on their score similarities when diffused in the noisy space. Utilizing a Bayesian framework, we develop a weighted denoising score-matching technique for knowledge transfer between head and tail classes. On one hand, this weighting approach encourages the model to prioritize score similarity during the knowledge transfer process. Additionally, we incorporate a gating mechanism in the knowledge transfer process. We provide statistical analysis to validate this methodology, revealing that the effectiveness of such knowledge transfer depends on both label distribution and sample similarity, providing the insight to consider sample similarity when re-balancing the label proportion in training. Extensive experiments conducted on various benchmark datasets demonstrate that our method outperforms existing techniques, consistently yielding better quality and diversity in generated samples.",
    "authors": [],
    "id": "SP:325e5f7ee2badb8583dffc3fb75ad9e0a5ca51e0",
    "references": [
        {
            "authors": [
                "REFERENCES Brian DO Anderson"
            ],
            "title": "Reverse-time diffusion equation models",
            "venue": "Stochastic Processes and their Applications,",
            "year": 1982
        },
        {
            "authors": [
                "Kaidi Cao",
                "Colin Wei",
                "Adrien Gaidon",
                "Nikos Arechiga",
                "Tengyu Ma"
            ],
            "title": "Learning imbalanced datasets with label-distribution-aware margin loss",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Yin Cui",
                "Menglin Jia",
                "Tsung-Yi Lin",
                "Yang Song",
                "Serge Belongie"
            ],
            "title": "Class-balanced loss based on effective number of samples",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Martin Heusel",
                "Hubert Ramsauer",
                "Thomas Unterthiner",
                "Bernhard Nessler",
                "Sepp Hochreiter"
            ],
            "title": "Gans trained by a two time-scale update rule converge to a local nash equilibrium",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Jonathan Ho",
                "Ajay Jain",
                "Pieter Abbeel"
            ],
            "title": "Denoising diffusion probabilistic models",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Chen Huang",
                "Yining Li",
                "Chen Change Loy",
                "Xiaoou Tang"
            ],
            "title": "Learning deep representation for imbalanced classification",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Tero Karras",
                "Miika Aittala",
                "Janne Hellsten",
                "Samuli Laine",
                "Jaakko Lehtinen",
                "Timo Aila"
            ],
            "title": "Training generative adversarial networks with limited data",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Tero Karras",
                "Miika Aittala",
                "Timo Aila",
                "Samuli Laine"
            ],
            "title": "Elucidating the design space of diffusionbased generative models",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Aditya Krishna Menon",
                "Sadeep Jayasumana",
                "Ankit Singh Rawat",
                "Himanshu Jain",
                "Andreas Veit",
                "Sanjiv Kumar"
            ],
            "title": "Long-tail learning via logit adjustment",
            "venue": "arXiv preprint arXiv:2007.07314,",
            "year": 2020
        },
        {
            "authors": [
                "Yiming Qin",
                "Huangjie Zheng",
                "Jiangchao Yao",
                "Mingyuan Zhou",
                "Ya Zhang"
            ],
            "title": "Class-balancing diffusion models",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Harsh Rangwani",
                "Konda Reddy Mopuri",
                "R Venkatesh Babu"
            ],
            "title": "Class balancing gan with a classifier in the loop",
            "venue": "In Uncertainty in Artificial Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Harsh Rangwani",
                "Naman Jaswani",
                "Tejan Karmali",
                "Varun Jampani",
                "R Venkatesh Babu"
            ],
            "title": "Improving gans for long-tailed data through group spectral regularization",
            "venue": "In European Conference on Computer Vision,",
            "year": 2022
        },
        {
            "authors": [
                "Harsh Rangwani",
                "Lavish Bansal",
                "Kartik Sharma",
                "Tejan Karmali",
                "Varun Jampani",
                "R. Venkatesh Babu"
            ],
            "title": "Noisytwins: Class-consistent and diverse image generation through stylegans",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Gabriel Raya",
                "Luca Ambrogioni"
            ],
            "title": "Spontaneous symmetry breaking in generative diffusion models",
            "venue": "arXiv preprint arXiv:2305.19693,",
            "year": 2023
        },
        {
            "authors": [
                "Tim Salimans",
                "Ian Goodfellow",
                "Wojciech Zaremba",
                "Vicki Cheung",
                "Alec Radford",
                "Xi Chen"
            ],
            "title": "Improved techniques for training gans",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2016
        },
        {
            "authors": [
                "Jascha Sohl-Dickstein",
                "Eric Weiss",
                "Niru Maheswaranathan",
                "Surya Ganguli"
            ],
            "title": "Deep unsupervised learning using nonequilibrium thermodynamics",
            "venue": "In International Conference on Machine Learning,",
            "year": 2015
        },
        {
            "authors": [
                "Jiaming Song",
                "Chenlin Meng",
                "Stefano Ermon"
            ],
            "title": "Denoising diffusion implicit models",
            "venue": "arXiv preprint arXiv:2010.02502,",
            "year": 2020
        },
        {
            "authors": [
                "Yang Song",
                "Stefano Ermon"
            ],
            "title": "Improved techniques for training score-based generative models",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Yang Song",
                "Jascha Sohl-Dickstein",
                "Diederik P Kingma",
                "Abhishek Kumar",
                "Stefano Ermon",
                "Ben Poole"
            ],
            "title": "Score-based generative modeling through stochastic differential equations",
            "venue": "arXiv preprint arXiv:2011.13456,",
            "year": 2020
        },
        {
            "authors": [
                "Amirhossein Tavanaei"
            ],
            "title": "Embedded encoder-decoder in convolutional networks towards explainable ai",
            "venue": "arXiv preprint arXiv:2007.06712,",
            "year": 2020
        },
        {
            "authors": [
                "Pascal Vincent"
            ],
            "title": "A connection between score matching and denoising autoencoders",
            "venue": "Neural Computation,",
            "year": 2011
        },
        {
            "authors": [
                "Yu-Xiong Wang",
                "Deva Ramanan",
                "Martial Hebert"
            ],
            "title": "Learning to model the tail",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Yilun Xu",
                "Shangyuan Tong",
                "Tommi Jaakkola"
            ],
            "title": "Stable target field for reduced variance score estimation in diffusion models",
            "venue": "arXiv preprint arXiv:2302.00670,",
            "year": 2023
        },
        {
            "authors": [
                "Lu Yang",
                "He Jiang",
                "Qing Song",
                "Jun Guo"
            ],
            "title": "A survey on long-tailed visual recognition",
            "venue": "International Journal of Computer Vision,",
            "year": 2022
        },
        {
            "authors": [
                "Jaejun Yoo",
                "Namhyuk Ahn",
                "Kyung-Ah Sohn"
            ],
            "title": "Rethinking data augmentation for image superresolution: A comprehensive analysis and a new strategy",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Kun Zhang",
                "Bernhard Sch\u00f6lkopf",
                "Krikamol Muandet",
                "Zhikun Wang"
            ],
            "title": "Domain adaptation under target and conditional shift",
            "venue": "In International Conference on Machine Learning,",
            "year": 2013
        },
        {
            "authors": [
                "Zijian Zhang",
                "Zhou Zhao",
                "Jun Yu",
                "Qi Tian"
            ],
            "title": "Shiftddpms: Exploring conditional diffusion models by shifting diffusion trajectories",
            "venue": "arXiv preprint arXiv:2302.02373,",
            "year": 2023
        }
    ],
    "sections": [
        {
            "text": "Diffusion models are acclaimed for generating high-quality and diverse images. However, their performance notably degrades when trained on data with a longtailed label distribution. Traditional methods typically tackle this problem by adjusting the proportion of samples from head and tail classes, yet overlook connections between samples. In this paper, we introduce a novel approach in diffusion models that facilitates the transfer of knowledge between head and tail classes based on their score similarities when diffused in the noisy space. Utilizing a Bayesian framework, we develop a weighted denoising score-matching technique for knowledge transfer between head and tail classes. On one hand, this weighting approach encourages the model to prioritize score similarity during the knowledge transfer process. Additionally, we incorporate a gating mechanism in the knowledge transfer process. We provide statistical analysis to validate this methodology, revealing that the effectiveness of such knowledge transfer depends on both label distribution and sample similarity, providing the insight to consider sample similarity when re-balancing the label proportion in training. Extensive experiments conducted on various benchmark datasets demonstrate that our method outperforms existing techniques, consistently yielding better quality and diversity in generated samples."
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "Diffusion models have emerged as a powerful class of deep probabilistic models. These models leverage techniques from statistical physics and probabilistic modeling to generate high-quality, realistic samples from complex data distributions (Sohl-Dickstein et al., 2015). The effective implementation of a diffusion model necessitates extensive training on a diverse and sizable collection of image data. In general, there is a prevalent occurrence of a long-tail distribution (Yang et al., 2022), wherein a vast majority of images belong to a few dominant categories, while a significant portion of the dataset comprises less frequently occurring categories. As a consequence, the training of diffusion models with long-tail data continues to pose a formidable challenge owing to the distortion in the entire dataset.\nIn current works, many attempts have been made to address the lack of diversity and mode collapse issues in the generation of tail classes. A series of methods based on Generative Adversarial Networks (GANs) has been proposed. One of the common approaches is to adopt strategies that refine the general model\u2019s generation ability by improving its conditional modeling (Rangwani et al., 2022) on tail categories, which heavily depend on GANs structure. Another line of research focuses on alleviating the scarcity of samples in tail classes through appropriate data augmentation techniques (Karras et al., 2020; Rangwani et al., 2023). Nevertheless, such methods may not effectively capture the underlying data distribution or introduce meaningful variations (Yoo et al., 2020). For long tail diffusion models, Class Balancing Diffusion Models (CBDM) (Qin et al., 2023) has proposed a distribution adjustment regularizer enhancing tail generation based on the model prediction on the head class. However, the augmentation relying on the condition and prediction of the model might cause bias during training resulting in generated outcomes that do not meet expectations and leading to a lack of robustness.\nIn order to alleviate the issue, a direct knowledge transfer from head to tail categories should be established. Let\u2019s review a recent study on the diffusion process, Xu et al. (2023) observed that the score function exhibits the highest variance during the intermediate steps, which is a critical period for semantic formation (Zhang et al., 2023) with the undetermined target. Therefore, based on this evidence, we can utilize the score information from head classes to calibrate and enhance the generation of tail classes in this period and finally improve the overall generation performance. In this study, leveraging\nthe evidence that the score of the diffusion model could be estimated via referencing multiple targets in the dataset (Xu et al., 2023), we propose a calibration strategy for the scores of the tail class directly making use of the head samples as reference, as shown in Figure. 1. By employing the strategy that leverages the similarity of underlying data distribution, the reliance on the conditional capacity of the model is mitigated and the generation performance is improved.\nTo realize the augmentation for the tail scores, we begin with modeling the score as the weighted scores averaging towards different targets. For conditional generation, the score estimation of noisy tail samples is augmented via properly improving the contribution of the score from the head samples in the reference batch, denoted as a T2H (noisy Tail to clean Head) operation to enhance the diversity of the tail generation. Simultaneously, a Batch Re-sample approach is utilized to alleviate the limitation on overall head-to-tail transfer strength in T2H mode. Besides that, in unconditional generation, the score function is predominantly influenced by samples from the head classes. Batch re-sampling is also employed to address this issue. While the method H2T (noisy Head to clean Tail), the contrary to T2H, has shown its effectiveness in unconditional generation.\nOur contributions can be summarized as follows: (1) We employ a \u201dBatch Re-sample\u201d strategy that constructs a balanced reference batch to address the extreme dominance issue of scores from the head class and promotes head-to-tail transfer under T2H mode. (2) We have developed a method denoted as T2H based on the multi-target nature of score estimation to effectively calibrate and enhance the generation of tail classes in the semantic formation period, thereby significantly improving the overall generation performance. (3) We conduct extensive testing on three different long tail datasets (CIFAR10, CIFAR100, TinyImageNet) to validate the effectiveness of our proposed method. The results consistently demonstrated the superiority of our approach."
        },
        {
            "heading": "2 PRELIMINARY",
            "text": "The diffusion model involves slowly adding noise to the existing training data in the forward process, and then utilizing a deep learning network to gradually recover the original data from the noise in the reverse process. In the forward process, a clean image is progressively transformed by adding carefully calibrated Gaussian noise (Ho et al., 2020) at each diffusion step t, q(xt|x) = N (\u03b1tx, \u03c32t I), 0 \u2264 t \u2264 T where the coefficients \u03b1t and \u03c3t are chosen so that q(xt) is close to initial data density at t \u2248 0 and close to Gaussian at t \u2248 T . For the reverse process, the diffusion model firstly samples from a Gaussian noise distribution p(xT ) \u223c N (0, I), and then gradually incorporates various structures and semantic information at each step. Then transition p(xt\u22121|xt) is estimated by training an image-to-image Unet parameterized by \u03b8, p(xt\u22121|xt) = N (xt\u22121;\u00b5\u03b8(xt, t),\u03a3t). Instead of directly predicting the mean value, the Unet is trained to predict the noise content of xt, which is optimized by the following loss:\nLDiff (\u03b8, x) = Et\u223cU(0,T ),\u03f5\u223cN (0,I)[||\u03f5\u2212 \u03f5\u03b8(xt; t)||22] (1)\nAnother perspective on the diffusion model is to start from a parameter-free designed stochastic differential equation (SDE) dx = f(x, t)dt+ g(t)dw, which transfers the initial data distribution to\na prior distribution as time goes from 0 to T . The w is the standard Wigner process. Sampling the diffusion model via a reverse time SDE (Anderson, 1982): dx = [f(x, t)\u2212 g(t)2\u2207xt log p(xt)]dt\u0304+ g(t)dw\u0304. The \u00b7\u0304 means time reverse. The score function namely s(xt, t) = \u2207xt log p(xt) should be estimated in the training stage. The predicted noise in Eq. (1) could be related to the score function via denoising score matching (Vincent, 2011):\n\u03f5\u03b8(xt, t) = \u2212\u03c3ts\u03b8(xt, t) (2)\nFrom Eq. (2) we could learn that the score is proportional to the noise predicted by the model. In this study, we start from the optimal score \u2207xt log p(xt) to formulate our methodology for long tail distribution."
        },
        {
            "heading": "3 METHOD",
            "text": "Our problem involves training data that exhibits a long-tail distribution (xi, yi) M i=1 sampled from p(x, y) and a diffusion model parametrized via a denoising Unet \u03f5\u03b8(xt, t). The yi \u2208 {C1, C2, ..., CL} is the label of xi , assuming that the classes are ordered in descending probability of label occurrence, i.e., if i < j then ni > nj , where the ni is the number training samples belonging to class Ci. With proper training methodology using long-tailed training data and diffusion model, we want to generate a more balanced and diversified data distribution p\u22c6(x0) in the inference time.\nDuring the inference stage, a diffusion model utilizes a step-by-step reverse operation from a prior distribution p(xT ) to data distribution p(x0) with reverse-SDE discussed in Section. 2. The reverseSDE uses the score function s(xt, t) at time step t obtained from the training stage. The optimal score s\u22c6(xt, t) = \u2207xt log qt(xt) could be expressed with the expectation:\n\u2207xt log qt(xt) = Eq(x0|xt)\u2207xt log q(xt|x0), (3)\nwhere the proof is provided in Appendix B. As the equation illustrated, the score for a given xt could be calculated by the expectation under the distribution q(x0|xt). Since this distribution is intractable, we utilize importance sampling to sample from initial data distribution q(x0). Using the Bayesian rule, we have q(x0|xt) = q(x0)q(xt|x0)/q(xt). Given q(xt) =\u222b q(xt|x0)q(x0)dx0, we can derive\nEq(x0|xt)\u2207xt log q(xt|x0) = Eq(x0) q(xt|x0)\nEx\u20320\u223cq(x0)q(xt|x \u2032 0) \u2207xt log q(xt|x0). (4)\nThus, with a mini-batch of samples from q(x0), i.e., x (1:N) 0 iid\u223c q(x0), we have\nEq(x0|xt)\u2207xt log q(xt|x0) \u2248 N\u2211 i=1 q(xt|x(i)0 )\u2211N j=1 q(xt|x (j) 0 ) \u2207xt log q(xt|x (i) 0 ). (5)\nSpecifically, we have expanded upon the original one-to-one optimal noise estimator in denoising score matching and transformed it into the one-to-many distributional matching technique. The expectation in the equation we approximate with a weighted average of scores with respect to different samples {x(i)0 } denoted as a reference batch.\nT2H. In conditional generation, the labels are involved and conditioned. The unconditional score \u2207xt log q(xt) and conditional score \u2207xt log q(xt|y) are both estimated in the training stage. With label participated, the sampling distribution should be x(i)0 \u223c q(x0|y) instead of x (i) 0 \u223c q(x0) in Eq. (5) as:\n\u2207xt log q(xt|y) \u2248 N\u2211 i=1 q(xt|x(i)0 , y(i))\u2211 j q(xt|x (j) 0 , y\n(i) 0 )\ufe38 \ufe37\ufe37 \ufe38\nScore Weight\n\u2207xt log q(xt|x (i) 0 , y) {(x (k) 0 , y (k) 0 )}Nk=1 \u223c q(x0, y0),\n(6) where the proof is provided in Appendix. C. In the denominator, we sample from q(x0, y0) instead of q(x0) in the self-normalizing technique with label. The distribution of q(xt|x(i)0 ) follows a Gaussian distribution and could be calculated by B exp(\u2212 ||x (i) 0 \u2212xt|| 2\n2\u03c32t ) where B is a normalizing constant and\nthere is no label involved. The mixing weight q(xt|x (i) 0 ,y)\u2211\nj q(xt|x (j) 0 ,y (j) 0 )\nfrom (x(j)0 , y (j) 0 ) is dominant by the\nL2 distance respect to xt. In addition to the L2 distance in the Gaussian kernel, here we make a further assumption that the distribution q(xt|x0, y0) is adjusted by q(y0)\u03b2 :\nq(xt|x0, y0) \u221d B q(y0)\u03b2 exp(\u2212 ||xt \u2212 x0||22\n2\u03c32t ), (7)\nwhere \u03b2 is a pre-defined parameter, that controls the overall distribution density with respect to y0. When \u03b2 = 0 then the xt\u2019s distribution is only dependent on x0.\nFor the score estimation of xt obtained from noisy tail sample (xT0 , y T 0 ), we employ a method of score-oriented calibration. This approach enhances the contribution of head class samples (xH0 , y H 0 ) by reasonably increasing the mixing score weight in Eq. 6. Consequently, it leverages the rich diversity of the head class to improve the generative performance of the tail class.\nTo improve the mixing score weight for the head samples, we could assign \u03b2 = 1 in Eq. 7, since the q(yH0 ) in the equation has a relative larger value. As shown in Figure. 3, the mixing weights towards head samples are improved compared with directly calculating the mixing weight depending only on exp(\u2212 ||x (i) 0 \u2212xt|| 2\n2\u03c32t ).\nFurthermore, for faster and easier implementation, we firstly sample a mini-batch {x(i)0 , y (i) 0 }Ki=1. Then as the typical training strategy of the diffusion model, for each sample x(i)0 , we sample a random t \u223c U(0, T ) and random Gaussian noise \u03f5i \u223c N (0, I) and obtain the perturbed noisy xt. The training objective is to predict the noise \u03f5\u0302i by our parametrized Unet \u03f5\u03b8(xt, t; y) = \u2212\u03c3ts\u03b8(xt, t; y) .\nSince the score for each target xt, the \u2207xt log q(xt|x (i) 0 , y) \u221d \u2212 1\u03c3t \u03f5i. As a consequence, we transfer the Eq. 6 from a score-weighted mixing problem to a score selection problem as:\n\u03f5\u0302i = \u03f5z; z \u223c psel(z) = q(xt|x(z)0 , y (z) 0 )\u2211\nj q(xt|x (j) 0 , y (j) 0 )\n; 0 \u2264 z \u2264 K, (8)\nwhere psel(z) denotes the probability of selecting score with index z in the batch. \u03f5z is calculated with \u03f5z = xt\u221a\u03b1\u0304 \u2212 xz , where the estimated noise is obtained by a new clean target and current noisy sample. So we firstly calculate the multi-nomial distribution in Eq. (8) with Gaussian kernel:\nq(xt|x(z)0 , y (z) 0 ) \u221d B exp(\u2212\n||x(z)0 \u2212xt|| 2 2\n2\u03c32t ), and then we sample z from this distribution. If q(yz) \u2265\nq(yi), the transferring is allowed and the score is substituted with \u03f5z , which means the noisy tail sample is mapped to a clean head sample. If not, the transferring is forbidden then the score is sent back to \u03f5i, which means that the noisy head sample is forbidden to map to the clean tail sample. Since the noisy sample is obtained from the head class, we denote such kind of transferring mode as T2H (noisy Tail sample to clean Head sample). Here, we amplify the contribution of head samples by selectively transferring the target solely to the clean head samples in the reference batch. The equivalence between T2H and directly enhancing the contribution of head samples via setting \u03b2 = 1 in score mixing has also been validated in the experiments, as shown in Figure. 5. We summarize the Algorithm in Alg. 1. We note for H2T mode, we just need to change the boundary condition from q(yz) \u2265 q(yi) in the algorithm to q(yz) \u2264 q(yi).\nAlgorithm 1 T2H algorithm for conditional long tail generation\nSample mini-batch {(x(i)0 , y (i) 0 )}Ki=1 with balanced distribution q\u22c6(x, y) for each sample (x(i)0 , y (i) 0 ) in the mini-batch do\nSample xt with random t and Gaussian noise \u03f5i \u223c N (0, \u03c32t I) Calculate psel(z) according to Eq. 8 with Gaussian kernel C exp(\u2212 ||xz\u2212xt|| 2 2\n2\u03c32t ) 0 \u2264 z \u2264 K\nSample z \u223c psel(z) if q(yz) \u2265 q(yi) then \u03f5\u0302i = \u03f5z else \u03f5\u0302i = \u03f5i end if Compute denoising loss if Conditional then LDiff = ||\u03f5\u0302i \u2212 \u03f5\u03b8(xt, t; y)||22 else LDiff = ||\u03f5\u0302i \u2212 \u03f5\u03b8(xt, t)||22\nend if end for\nFor a specific sample of tail class CT , we could augment the score via encouraging the model to predict the score towards the head class. Indeed, by leveraging the rich semantics of the head class and increasing the diversity of scores from the tail categories along the generation path, our approach enhances the overall diversity of generated samples. The increased variety ensures a more comprehensive generation of different classes and ultimately enhances the overall quality and diversity of the generated samples.\nBatch Re-sample. T2H achieves enhancement in generating tail categories through head-to-tail transfer for conditional generation. Here we evaluate the strength of the transfer quantitatively. Consider two samples (xH0 , y H 0 ) and (x T 0 , y T 0 ) from different head class CH and tail class CT . Let\u2019s discuss the distribution with same perturbed noise level N (0, \u03c32t I) with q(xt|xH0 , yH0 ) and q(xt|xT0 , yT0 ). For the purpose of measuring the strength of transition from CT to CH based on Langevin dynamics, then the Eq(xt|xH ,yH) q(xt|xT0 ,y T 0 )\u2211\nk q(xt|x (k) 0 ,y (k) 0 )\nshould be calculated (Song & Ermon,\n2020). Then we have the following proposition:\nProposition 3.1 Let q(xt|x0, y0) \u221d B q(y0)\u03b2 exp(\u2212 ||xt\u2212x0|| 2 2\n2\u03c32t ), then:\nEq(xt|xH0 ,yH0 ) q(xt|xT0 , yT0 )\u2211 k q(xt|x (k) 0 , y (k) 0 ) \u2264 B 2 (q(yH) q(yT ))\u03b2 exp(\u2212||x H 0 \u2212 xT0 ||22 8\u03c32t ). (9)\nAs you can see from the proposition, the transition strength is determined by the L2 distance represented by a Gaussian kernel between two samples and the product of label probability powered by \u03b2. In the perspective of Eq. (7) with algorithm T2H, when \u03b2 = 1, The transfer strength is constrained by the product of the q(y) of the head and tail classes, which is relatively a small value.\nTherefore, employing a Batch Re-sampling strategy during training, which equalizes the appearance probability of each category, can significantly enhance the head-to-tail transfer.\nBesides, in unconditional generation, the training data q(x0) exhibits a long-tail distribution during the training process. As a result, If we directly use the results from the score estimated in Eq. (5), then the generated p(x0) will also follow a long-tail distribution. The frequent occurrence of head samples will lead to unconditional scores being dominated by the score of head samples. To intuitively illustrate the necessity of using the Batch Re-sample, we employed a toy example to illustrate the phenomenon of head class dominance in long-tail unconditional generation, as shown in Figure. 4. The BatchRe-sample could also alleviate this issue. More details could be found in Appendix. K.\nHere, we follow the common assumption in the long tail recognition: the balanced distribution and initial long tail distribution are related by sharing the same conditional probability q(x|y) = q\u22c6(x|y) (Zhang et al., 2013). H2T, Full. In the former analysis, in order to augment the tail class, we propose a method denoted as noisy Tail to clean Head (T2H). Conversely, there would exist a method of H2T denoting noisy Head to clean Tail. In H2T, the weight of score towards target tail samples is improved corresponding to smaller \u03b2 in Eq. (7), eg. \u03b2 = \u22121, as the inverse value q(y) for the tail class is larger than the head class. Consequently, for a noisy head sample, the contribution of the tail sample targets are enhanced. In addition to H2T and T2H, if we do not assess the probabilities of transferred labels, this mode is denoted as \u2019Full\u2019 (means allowing both transfer directions).\nConnection with CBDM (Qin et al., 2023). The CBDM has employed a score aligning loss in the training stage: 1|Y| \u2211 y\u2032\u2208Y t||\u03f5\u03b8(xt, y) \u2212 \u03f5\u03b8(xt, y\u2032)||2. The conditional score with label y of xt is regularized with an another conditional score with label y\u2032. The aligned strength is weighted with the diffusion time t. We could further prove that the optimal score of CBDM could be written as a weighted sum of the initial denoising score and an adjustment term, which is similar to Eq. (6). When the diffusion model converges, the optimal minimizer \u03f5\u2217(xt, y) for the CBDM loss: LCBDM (xt, y, t, \u03f5) = ||\u03f5\u03b8(xt, y)\u2212 \u03f5||2+ \u03c4t|Y| \u2211 y\u2032\u2208Y ||\u03f5\u03b8(xt, y)\u2212 \u03f5\u03b8(xt, y\u2032)||2 could be denoted as: \u03f5\u2217(xt, y) = 1 1+t\u03c4 \u03f5+ t \u03c4 (1+t \u03c4) |Y| \u2211 y\u2032 \u03f5\u03b8(xt, y \u2032). where the proof is provided in Appendix. E. The operation also can be considered as augmenting the tail class with the head with labels. The difference is that the reference of our method is obtained from data, whereas CBDM\u2019s reference is based on the predictions and condition capacity of the model that may cause biased scores, as we have discussed in Section. 1. Our method possesses greater robustness besides the state-the-of-the-art performance."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": "Experimental Setup. We started by selecting two widely utilized datasets in the field of image synthesis, namely CIFAR10/CIFAR100, with their long-tailed versions CIFAR10LT and CIFAR100LT. The construction of CIFAR10LT and CIFAR100LT follows the methodology proposed in Cao et al. (2019) , wherein the size of each category exponentially decreases with its category index, adhering to an imbalance factor of imb = 0.01. For the CIFAR10LT dataset, we also implement a more skewed version with the imbalance factor of imb = 0.001. Two commonly used metrics for image generation are adopted namely Frechet Inception Distance (FID) (Heusel et al., 2017) and Inception Scores (IS) (Salimans et al., 2016). During the inference time, we generate 50k images for the eval-\nuation of the metrics. A DDIM sampler (Song et al., 2020a) is utilized with 100 steps of 10 steps skip comparison with initial DDPM (Ho et al., 2020) 1000 steps.\nOur training schedules are strictly follows the implementation of CBDM (Qin et al., 2023), which follows the DDPM settings. The experiments are conducted with two settings corresponding to the methods, unconditional generation and conditional generation. For the unconditional generation, we only adjust the training strategy with no label information injected into the diffusion model for optimization. At the inference stage, the unconditional diffusion model is asked to generate 50k images freely. For the conditional generation, there is label information injected into the diffusion model in the training stage. While at the inference stage, the diffusion model is asked to generate 50k/L images for each class where L is the number of classes.\nAblation study. We conduct both conditional and unconditional generation experiments on CIFAR10LT, the strategies that have been discussed are applied to a base DDPM unconditional/conditional generation model. As shown in the Table 1, in an unconditional generation, the Batch Re-sample together with H2T and T2H could improve the performance, where H2T is more effective as we discussed in the former section. In a conditional generation, T2H is more efficient than H2T and \u2019Full\u2019 since improving the contribution of head samples for the noisy tail sample could enhance the diversity of tail classes and promoting the overall generation performance.\nComparison with other methods. We conduct a conditional generation, on CIFAR10LT, CIFAR100LT dataset. And we do a comparison with two GANbased long-tail generation methods CBGAN (Rangwani et al., 2021) and gSR-GAN (Rangwani et al., 2022) and one diffusion-based methods CBDM. The results are shown in Table 2.\nAs illustrated in the Table 2, in the comparative analysis of conditional generation, the T2H method achieved the best performance of 6.89 FID score, surpassing base DDPM models with 3.83 and\nCBDM with 0.38. In a more skewed version of CIFAR10LT with an imbalanced factor of 0.001, T2H has achieved more than 1.0 FID improvements compared with CBDM. Under the CIFAR100LT benchmark, our method has also achieved more than 1.0 FID than the CBDM. The results have shown that our method has obtained a further improvement in the diffusion model on the long-tailed distribution, illustrating a more effective calibration and augmentation from data distribution than from the model prediction.\nWe also conduct experiments on a dataset TinyImageNet200LT with more classes and higher resolution, which is the long tail version of TinyImageNet200 (Tavanaei, 2020). The metric is based on 10k generated images referenced with its validation set, as shown in Table. 3\nT2H and H2T relation with different \u03b2 value. For the purpose of validating the assumption with Eq. (7), we directly utilize the formula to calculate psel(z) instead of H2T and T2H. As shown in Figure. 5, in the case of unconditional generation, the generation performance improves as the \u03b2 increases. Conversely, in the case of conditional generation, the generation performance deteriorates as the \u03b2 increases.\nTraining Robustness of our method. In order to achieve better performance for image generation, a diffusion model is required to train for a long time. Due to various forms of data imbalance, which is essentially the long-tail nature of the data, the model tends to overfit and consequently leads to a decrease in performance, as observed in Figure 6. This phenomenon is evident in both conditional and unconditional diffusion model training without any additional processing. However, when employing CBDM in conditional generation, this situation can be mitigated\nto some extent. However, as training progresses, CBDM augmentation heavily relies on the model\u2019s prediction for other labels, so this phenomenon still exists to some degree. Nevertheless, our method effectively suppresses this issue and achieves stable performance in the long-time training process.\nTransfer Probability with diffusion time. We make a study on selecting probability psel(z) in Eq. (8) with diffusion time. The transfer probability \u2211 z \u0338=i psel(z), which denotes shifting the denoise target in the training stage, is counted and calculated with 100k samples perturbed with levels of noise corresponding to different diffusion times within a mini-batch. As shown in Figure. 7, when the diffusion step size is between 500 and 800, the probability of transferring the denoise target increases from 0 to 1 as the step size increases. If the step size exceeds 800, then the transfer will occur with an approximately 1.0 probability. We also restrict the transfer cutting step, denoting that the target transfer (T2H, H2T) is only allowed below this diffusion step. The FID scores decrease\ndramatically for both conditional and unconditional generation with a cutting time range from 500- 800, which is consistent with the observation of high variance phase in Xu et al. (2023)."
        },
        {
            "heading": "5 RELATED WORK",
            "text": "Diffusion Models. For the diffusion generation model, Ho et al. (2020) firstly announced that the training of the model is accomplished by utilizing a weighted variational bound. Song et al. (2020b) proposed an alternative approach to constructing a diffusion model, which involves utilizing a stochastic differential equation (SDE) that gradually injects noise to smoothly transform a complex data distribution into a known prior distribution. Karras et al. (2022) presents a design space that distinctly delineates the concrete design choices for former works. As for the diffusion process, Xu et al. (2023) has observed three phases with distinct behaviors affecting the generation diffusion model. Raya & Ambrogioni (2023) demonstrate that the diffusion process could be modeled in a manner analogous to symmetry breaking in physics.\nLong Tail Recognition. Long tail recognition refers to the task of accurately recognizing and classifying rare or infrequently occurring classes in a given dataset together with frequently occurring classes. There are several approaches to address the problem, including re-weighting (Huang et al., 2016), logit adjustment (Menon et al., 2020), and knowledge transfer (Wang et al., 2017). Cui et al. (2019) declare that as the number of samples increases, the diminishing phenomenon suggests that there is a decreasing marginal benefit for a model to extract additional information from the data due to the presence of information overlap.\nLong Tail Generation. The objective of long tail generation is to generate a more balancing and diverse dataset training with a long tail dataset. CB-GAN (Rangwani et al., 2021) has used a regularizer that makes use of a pretrained classifier in the training stage to ascertain the balance learning of all classes in the dataset. gSR-GAN (Rangwani et al., 2022) observes that the performance decline in long tail generation primarily occurs because of class-specific mode collapse in tail classes which correlated with the spectral explosion of the conditioning parameter matrix and proposes a corresponding group spectral regularizer. CBDM (Qin et al., 2023) makes use of a distribution adjustment regularizer in the training stage for the purpose of augmenting the tail classes."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "The main challenge for the long-tail diffusion generation is the lack of diversity for the tail class generation. To tackle the challenge, based on the multi-target characteristic of denoising score, a T2H augmentation for the estimation of noisy tail samples is achieved improving the score contribution of head samples in the reference batch. At the same time, the Batch Re-sample operation helps alleviate the dominant effect of head samples on the scores and promote head-to-tail transfer. The experiments are conducted to validate our approach on multiple benchmark datasets, demonstrating effectiveness compared with baseline methods and robustness versus training time."
        },
        {
            "heading": "A DECLARATION",
            "text": "For the simplification of the proof, we use subscripts with t to denote noisy samples, while subscripts such as 0, i, and j represent specific data samples.8\nB THE EXPECTATION FORMULATION OF OPTIMAL SCORE FOR xt.\nThe optimal score for xt could be calculated as : \u2207xt log q(xt) = Eq(x0|xt)\u2207xt log q(xt|x0)\nProof B.1 Based on Bayes theorem and definition of expectation:\n\u2207xt log q(xt) = 1\nq(xt) \u2207xtq(xt)\n= 1\nq(xt) \u2207xt\n\u222b q(xt|x0)q(x0)dx0\n=\n\u222b q(x0)\nq(xt) \u2207xtq(xt|x0)dx0\n=\n\u222b q(xt|x0) q(x0)\nq(xt) \u2207xt log q(xt|x0)dx0\n=\n\u222b q(xt, x0)\nq(xt) \u2207xt log q(xt|x0)dx0\n= \u222b q(x0|xt)\u2207xt log q(xt|x0)dx0\n= Eq(x0|xt)\u2207xt log q(xt|x0)"
        },
        {
            "heading": "C THE SCORE FOR THE CONDITIONAL GENERATION",
            "text": "The optimal score for xt given y could be calculated as:\n\u2207xt log q(xt|y) \u2248 N\u2211 i=1 q(xt|xi, yi)\u2211 j q(xt|xj , yj) \u2207xt log q(xt|xi, yi); (xi, yi) \u223c q(x0, y0) (10)\nProof C.1 \u2207xt log q(xt|y) = Eq(x0|xt,y)\u2207xt log q(xt|x0, y) We use importance sampling:\nEq(x0|xt,y)\u2207xt log q(xt|x0, y) = \u2211\nx0\u223cq(x0|y)\nq(x0|xt, y) q(x0|y) \u2207xt log q(xt|x0, y) (11)\nsince conditional probability formula: q(x0|xt, y) q(x0|y) = q(xt|x0, y)q(x0|y) q(xt|y)q(x0|y) = q(xt|x0, y) q(xt|y)\n(12)\nCompared with unconditional generation, we here calculate q(xt|y) as q(xt|y) =\u222b q(xt|x0, y)q(x0|y)dx0, we utilize Monte-Carlo sample as:\nq(xt|y) = \u2211 j q(xt|xj , y) xj \u223c q(x0|y) (13)\nWe substitute the Equation into the Eq. (12): 1\nN \u2211 xi\u223cq(x0|y) q(xt|xi, y)\u2211 j q(xt|xj , y) \u2207xt log q(xt|xi, y) (14)\nWe broad the distribution of q(x0|y) to entire distribution q(x0, y0) as reference: 1\nN \u2211 xi\u223cq(x0) q(xt|xi, y)\u2211 j q(xt|xj , yj) \u2207xt log q(xt|xi, y) (xj , yj) \u223c q(x0, y0) (15)"
        },
        {
            "heading": "D THE PROOF FOR THE PROPOSITION 3.1",
            "text": "Proposition D.1 Let q(xt|xi, yi) \u221d B q(yi)\u03b2 exp(\u2212 ||xt\u2212xi|| 2 2\n2\u03c32t ), then:\nEq(xt|xi,yi) q(xt|xj , yj)\u2211 k q(xt|xj , yj) \u2264 B 2 (q(yi) q(yj)) \u03b2 exp(\u2212||xi \u2212 xj || 2 2 8\u03c32t )\nProof D.1 Eq(xt|xi,yi) q(xt|xj , yj)\u2211 k q(xt|xj , yj) =\n\u222b q(xt|xi, yi)q(xt|xj , yj)\u2211\nk q(xt|xj , yj) dxt\nwhile the term in the integral of the right side could be transformed into:\nq(xt|xi, yi)q(xt|xj , yj)\u2211 k q(xt|xj , yj)\n\u2264 \u222b\nq(xt|xi, yi)q(xt|xj , yj) q(xt|xi, yi) + q(xt|xj , yj) = 1 2\n\u222b 2\n1 q(xt|xi,yi) + 1 q(xt|xj ,yj)\n\u2264 1 2\n\u221a q(xt|xi, yi)p(xt|xj , yj)\nHere we substitute the assumption q(xt|xi, yi) \u221d B q(yi)\u03b2 exp(\u2212 ||xt\u2212xi|| 2 2\n2\u03c32t ) into the formula:\n1\n2\n\u221a q(xt|xi, yi)q(xt|xj , yj) = B\n2\n\u221a q(yi)\u03b2 exp(\u2212\n||xt \u2212 xi||22 2\u03c32t q(yj)\u03b2 exp(\u2212 ||xt \u2212 xj ||22 2\u03c32t )\n= B\n2 (q(yi)q(yj))\n\u03b2 \u221a exp(\u2212||xt \u2212 xi||\n2 2 + ||xt \u2212 xj ||22 2\u03c32t )\nHere we take a twice look at the integral and exclude terms unrelated to xt:\u222b B\n2 (q(yi)q(yj))\n\u03b2 \u221a exp(\u2212||xt \u2212 xi||\n2 2 + ||xt \u2212 xj ||22 2\u03c32t )dxt = B 2 (q(yi)q(yj)) \u03b2\n\u222b exp(\u2212||xt \u2212 xi||\n2 2 + ||xt \u2212 xj ||22 4\u03c32t )dxt\nWe will extract the numerator term from the exponential:\n||xt \u2212 xi||22 + ||xt \u2212 xj ||22 = (xt \u2212 xi)T (xt \u2212 xi) + (xt \u2212 xj)T (xt \u2212 xj) = 2xtx T t \u2212 2xTt (xi + xj) + xTi xi + xTj xj\n= 2||xt \u2212 xi + xj\n2 ||22 + xTi xi + xTj xj \u2212\n(xi + xj) T (xi + xj)\n2\nSince the first term related to xt could be integral to a constant K which do not include xi and xj .\u222b exp(\u2212 ||xt \u2212 xi+xj2 || 2 2\n2\u03c32t )dxt = K\nowing to the Gaussian distribution normalization. Then the left term:\nxTi xi + x T j xj \u2212\n(xi + xj) T (xi + xj)\n2 =\n1 2 (xTi xi + x T j xj \u2212 2 xTi xj)\n= 1\n2 ||xi \u2212 xj ||22\nSubstitute in the initial formula, we obtain: Eq(xt|xi,yi) q(xt|xj , yj)\u2211 k q(xt|xj , yj) \u2264 B 2 (q(yi)q(yj)) \u03b2 \u222b exp(\u2212||xt \u2212 xi|| 2 2 + ||xt \u2212 xj ||22 4\u03c32t )dxt\n= B\n2 (q(yi)q(yj))\n\u03b2 \u222b exp(\u2212 ||xt \u2212 xi+xj2 || 2 2\n2\u03c32t )dxt \u00d7 exp(\u2212 ||xi \u2212 xj ||22 8\u03c32t )\n= BK\n2 (q(yi) q(yj))\n\u03b2 exp(\u2212||xi \u2212 xj || 2 2\n8\u03c32t )\nThe K is absorbed in constant B, we finish the proof."
        },
        {
            "heading": "E THE RELATION WITH CBDM LOSS AND ANALYSIS",
            "text": "Proposition E.1 When the diffusion model converges, the optimal minimizer \u03f5\u2217(xt, y) for the CBDM loss:\nLCBDM (xt, y, t, \u03f5) = ||\u03f5\u03b8(xt, y)\u2212 \u03f5||2 + \u03c4t |Y| \u2211 y\u2032\u2208Y ||\u03f5\u03b8(xt, y)\u2212 \u03f5\u03b8(xt, y\u2032)||2\ncould be denoted as:\n\u03f5\u2217(xt, y) = 1\n1 + t\u03c4 \u03f5+\nt \u03c4 (1 + t \u03c4) |Y| \u2211 y\u2032 \u03f5\u03b8(xt, y \u2032)\nProof E.1 When the diffusion model converges, for specific label y, the score \u03f5\u03b8(xt, y\u2032) for other label y\u2032 is fixed. When the loss is minimized:\nLCBDM (xt, y, t, \u03f5) = ||\u03f5\u03b8(xt, y)\u2212 \u03f5||2 + \u03c4t |Y| \u2211 y\u2032\u2208Y ||\u03f5\u03b8(xt, y)\u2212 \u03f5\u03b8(xt, y\u2032)||2\n= \u03f5\u03b8(xt, y) T \u03f5\u03b8(xt, y) + \u03f5 T \u03f5\u2212 2\u03f5\u03b8(xt, y)T \u03f5+ \u03c4t |Y| \u2211 y\u2032\u2208Y (\u03f5\u03b8(xt, y) T \u03f5\u03b8(xt, y) + \u03f5\u03b8(xt, y \u2032)T \u03f5\u03b8(xt, y \u2032)\u2212 2\u03f5\u03b8(xt, y)T \u03f5\u03b8(xt, y\u2032))\n= (1 + \u03c4t)\u03f5\u03b8(xt, y) T \u03f5\u03b8(xt, y)\u2212 2\u03f5\u03b8(xt, y)T (\u03f5+\n\u03c4t |Y| \u2211 y\u2032\u2208Y \u03f5\u03b8(xt, y \u2032)) + Const\nwhere the Const has no relation with \u03f5\u03b8(xt, y). The term related with \u03f5\u03b8(xt, y) could be expressed as a quadratic form:\nLCBDM (xt, y, t, \u03f5) = (1 + \u03c4t)\u03f5\u03b8(xt, y)T \u03f5\u03b8(xt, y)\u2212 2\u03f5\u03b8(xt, y)T (\u03f5+ \u03c4t |Y| \u2211 y\u2032\u2208Y \u03f5\u03b8(xt, y \u2032))\n= (1 + \u03c4t)[\u03f5\u03b8(xt, y) T \u03f5\u03b8(xt, y)\u2212 2 \u03f5\u03b8(xt, y)T\n\u03f5+ \u03c4t|Y| \u2211 y\u2032\u2208Y \u03f5\u03b8(xt, y \u2032)\n(1 + \u03c4t) ]\n= (1 + \u03c4t)[||\u03f5\u03b8(xt, y)\u2212 \u03f5+ \u03c4t|Y|\n\u2211 y\u2032\u2208Y \u03f5\u03b8(xt, y \u2032)\n(1 + \u03c4t) ||22] + const\u2032\nWhen the LCBDM converges to the minimum, the L2 norm should approximately near zero, so that we have:\n\u03f5\u2217(xt, y) \u2248 1\n1 + t\u03c4 \u03f5+\nt \u03c4 (1 + t \u03c4) |Y| \u2211 y\u2032 \u03f5\u03b8(xt, y \u2032)\nAnalysis This approach has two limitations. Firstly, referring to scores from other labels requires training the entire model within a conditional generation framework, thus restricting its applicability. Secondly, relying on scores from other labels for the same input (xt) introduces potential biases, particularly when there is a substantial semantic difference between the two classes. This can lead to some degree of offset. In contrast, our method addresses these limitations. First, it can be used in both conditional and unconditional generation scenarios. Second, we utilize the inherent similarity of data in the L2 space where the diffusion model operates. For example, in Figure. 2, we consider the similarity between the red airplane and the red car, ensuring that our enhancement is more logically grounded."
        },
        {
            "heading": "F CLASS-WISE FID SCORES OF T2H COMPARISON TO BASE DDPM.",
            "text": "We calculate the FID score using 5k images of T2H and base DDPM for each class on cifar10lt datasets. The final column of the table is overall performance utilizing 50k images. We have also modified the asssignment of head and tail classes based on whether they are animals or vehicles. As shown in the table below, By altering the assignment of classes, our method demonstrates a greater improvement compared to the baseline."
        },
        {
            "heading": "G THE IMPACT OF DATASET IMBALANCE ON PERFORMANCE.",
            "text": "We have investigated the impact of dataset balance on performance. As illustrated in the following table, we computed the FID scores using different reference sets: one balanced and the other longtailed sampled. It is observable that, despite the long-tail dataset consisting of real images, its performance metrics are inferior to those of the balanced generated dataset, due to its significant imbalance."
        },
        {
            "heading": "H THE VALIDATION OF TRANSFERRING TARGETS WITH T2H AND H2T.",
            "text": "In this section, we aim to verify the reliability and correctness of the transfer target. Suppose we have a clean sample x0, perturbed with noise \u03f5 \u223c N (0, \u03c32t ) obtaining the noisy sample xt with probability pt = B exp[\u2212 ||x0\u2212xt|| 2 2\n2\u03c32t ] where B \u221d 1\u03c3t is some normalizing constant. Then xt is involving the\nT2H or H2T algorithm and transfer the target as xz with probability psel(z) = q(xt|xz,yz)\u2211 j q(xt|xj ,yj) where q(xt|xz, yz) = B exp[\u2212 ||xz\u2212xt|| 2 2\n2\u03c32t ] also calculated with Gaussian kernel. Firstly, let us discuss the\nsimilarity of x0 and xz measured with ||x0 \u2212 xz||22 with following proposition.\nProposition H.1 The L2 similarity with x0 and xz bounded by the pt and psel(z):\n||x0 \u2212 xz||22 \u2264 \u22122\u03c32t (log(pt psel(z)) + log(pt + psel(z)) + 2 log \u03c3t). (16)\nProof H.1\n||x0 \u2212 xz||22 1 \u2264 \u22122\u03c32t (\u2212 ||xz \u2212 xt||22 2\u03c32t \u2212 ||x0 \u2212 xt|| 2 2 2\u03c32t )\n= \u22122\u03c32t (log exp[\u2212 ||xz \u2212 xt||22 2\u03c32t ] + log exp[\u2212||x0 \u2212 xt|| 2 2 2\u03c32t ])\n= \u22122\u03c32t (log pt psel(z)\n\u2211 j q(xt|xj , yj)\nB2 )\n2 \u2264 \u22122\u03c32t (log(pt psel(z)) + log(pt + psel(z)) + 2 log \u03c3t)\nwhere 1 involves utilizing the triangle inequality. And 2 comes from \u2211\nj q(xt|xj , yj) \u2265 q(xt|x0, y0) + q(xt|xz, yz).\nAs you can observe from the proposition above, the similarity x0 between and xz is bounded by an upper limit. Moreover, in the sampling process, the larger the values pt and psel(z) the tighter this bound becomes.\nFurthermore, from an alternative viewpoint, we evaluate the probability of obtaining xt using a clean xz within the framework of a one-to-one clean-to-noisy mapping.\nProposition H.2 The probability pzt of obtaining xt from xz under single denoising target scenario could be:\npzt \u2265 psel(z) pt 1\u2212 psel(z) , (17)\nwhere psel(z) comes from Equation.( 8) and q(xt|x0, y0) \u221d B exp[\u2212 ||xz\u2212xt|| 2 2\n2\u03c32t ]\nProof H.2 psel(z) = q(xt|xz, yz)\u2211 j q(xt|xj , yj)\n\u2264 q(xt|xz, yz) q(xt|x0, y0) + q(xt|xj , yj) ,\nbecause pt = q(xt|x0, y0) and pt = q(xt|xz, yz) under the assumption of Equation.( 8) so that:\npsel(z) \u2264 pzt\npzt + pt\nso we obtain:\npzt \u2265 psel(z) pt 1\u2212 psel(z) .\nWe have obtained a lower bound for pzt . If we assume pt \u2248 psel(z) \u2248 0.5 then the pzt \u2265 0.5 where is also valid noise-clean pair for xt \u2192 xz under the single target scenario."
        },
        {
            "heading": "I EXPERIMENTS ON IMAGENETLT DATASET",
            "text": "We conduct our method on large scale datasets ImagenetLT, the performance is shown in the following Table. 7. We generate 20k images with 1000 classes and use the balanced validation set with 20k images as the reference set for the calculation of FID scores. As shown in the Table, Our results on large-scale are consistent to the observations on small-scale data, which validates the effectiveness of our method at scale."
        },
        {
            "heading": "J FINETUNING EXPERIMENTS FROM NORMAL TRAINED MODEL",
            "text": "We also conduct the experiments of finetuning with the model pre-trained with normal denoising loss function. The finetuning starting step is ranging from 100k to 500k, and the results with no pretriaining and no finetuning are also provided. The conditional model is finetuned with T2H strategy, while the unconditional model is finetuned with H2T strategy.\nAs shown in the table, finetuning is capable of further improving model performance based on pretraining. However, as the number of pretraining steps increases, the extent of improvement gradually diminishes and eventually becomes stable.\nK IMPLEMENTATION DETAIL OF TOY GAUSSIAN EXAMPLES IN FIGURE. 4\nWe random sample 10k samples from distribution N ((0, 4), 0.2I) as head samples {xHi }, and 0.1k samples from N ((0,\u22124), 0.2I) simulating tail samples {xTj }. The empirical distribution of the overall dataset could be denoted as:\npdata(x) = \u2211 i \u03b4(x\u2212 xHi ) + \u2211 j \u03b4(x\u2212 xTj )\nSimulating the forward diffusion process, we convolute the empirical distribution with Gaussian noise N (0, \u03c32t I):\np\u03c3t(xt) = pdata(x) \u2217 N (0, \u03c32t I) = \u2211 i N (xt;xHi , \u03c32t I) + \u2211 j N (xt;xHj , \u03c32t I) (18)\nThe score estimation is calculated as \u2207xt log p\u03c3t(xt), we substitute the result in Eq. (18): \u2207xt log p\u03c3t(xt) = \u2207xt log( \u2211 i N (xt;xHi , \u03c32t I) + \u2211 j N (xt;xHj , \u03c32t I))\n= 1\u2211 i N (xt;xHi , \u03c32t I) + \u2211 j N (xt;xHj , \u03c32t I) \u2207xt( \u2211 i N (xt;xHi , \u03c32t I) + \u2211 j N (xt;xHj , \u03c32t I)\n= \u2212 1 \u03c32t\n\u2211 i(xt \u2212 xHi )N (xt;xHi , \u03c32t I) + \u2211 j(xt \u2212 xTj )N (xt;xHj , \u03c32t I)\u2211\ni N (xt;xHi , \u03c32t I) + \u2211 j N (xt;xHj , \u03c32t I)\nThe last step is because of:\n\u2207xtN (xt;x, \u03c32t I) = \u2207xtC exp(\u2212 ||xt \u2212 x||22\n2\u03c32t )\n= \u2212C exp(\u2212||xt \u2212 x|| 2 2\n2\u03c32t )\u2207xt ||xt \u2212 x||22 2\u03c32t\n= \u2212 1 \u03c32t N (xt;x, \u03c32t I)(xt \u2212 x)\nwe used two 2-D Gaussian distributions to simulate examples from head and tail classes. The number of points in the head class is 100 times greater than the tail class, resulting in an imbalance factor of 0.01. It can be observed that the score distribution across the entire space is dominated by the head class, leading to a highly imbalanced dataset generation.\nL VISUALIZATION RESULTS"
        }
    ],
    "year": 2023
}