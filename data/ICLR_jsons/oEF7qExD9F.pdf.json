{
    "abstractText": "Transformer models have demonstrated high accuracy in numerous applications but have high complexity and lack sequential processing capability making them ill-suited for many streaming applications at the edge where devices are heavily resource-constrained. Thus motivated, many researchers have proposed reformulating the transformer models as RNN modules which modify the self-attention computation with explicit states. However, these approaches often incur significant performance degradation. The ultimate goal is to develop a model that has the following properties: parallel training, streaming and low-cost inference, and state-of-the-art (SOTA) performance. In this paper, we propose a new direction to achieve this goal. We show how architectural modifications to a fully-sequential recurrent model can help push its performance toward Transformer models while retaining its sequential processing capability. Specifically, inspired by the recent success of Legendre Memory Units (LMU) in sequence learning tasks, we propose LMUFormer, which augments the LMU with convolutional patch embedding and convolutional channel mixer. Moreover, we present a spiking version of this architecture, which introduces the benefit of states within the patch embedding and channel mixer modules while simultaneously reducing the computing complexity. We evaluated our architectures on multiple sequence datasets. Of particular note is our performance on the Speech Commands V2 dataset (35 classes). In comparison to SOTA transformer-based models within the ANN domain, our LMUFormer demonstrates comparable performance while necessitating a remarkable 53\u00d7 reduction in parameters and a substantial 65\u00d7 decrement in FLOPs. Furthermore, when benchmarked against extant low-complexity SNN variants, our model establishes a new SOTA with an accuracy of 96.12%. Additionally, owing to our model\u2019s proficiency in real-time data processing, we are able to achieve a 32.03% reduction in sequence length, all while incurring an inconsequential decline in performance. Our code is publicly available here.",
    "authors": [
        {
            "affiliations": [],
            "name": "Zeyu Liu"
        },
        {
            "affiliations": [],
            "name": "Gourav Datta"
        },
        {
            "affiliations": [],
            "name": "Anni Li"
        },
        {
            "affiliations": [],
            "name": "Peter A. Beerel"
        }
    ],
    "id": "SP:951960a16849c1fe187ecc11148b0cb90e704116",
    "references": [
        {
            "authors": [
                "Sami Barchid",
                "Jos\u00e9 Mennesson",
                "Jason Eshraghian",
                "Chaabane Dj\u00e9raba",
                "Mohammed Bennamoun"
            ],
            "title": "Spiking neural networks for frame-based and event-based single object localization",
            "year": 2023
        },
        {
            "authors": [
                "Axel Berg",
                "Mark O\u2019Connor",
                "Miguel Tairum Cruz"
            ],
            "title": "Keyword transformer: A self-attention model for keyword spotting",
            "venue": "arXiv preprint arXiv:2104.00769,",
            "year": 2021
        },
        {
            "authors": [
                "Alexandre Bittar",
                "Philip N Garner"
            ],
            "title": "A surrogate gradient spiking baseline for speech command recognition",
            "venue": "Frontiers in Neuroscience,",
            "year": 2022
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Narsimha Reddy Chilkuri",
                "Chris Eliasmith"
            ],
            "title": "Parallelizing legendre memory unit training",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Kyunghyun Cho",
                "Bart Van Merri\u00ebnboer",
                "Dzmitry Bahdanau",
                "Yoshua Bengio"
            ],
            "title": "On the properties of neural machine translation: Encoder-decoder approaches",
            "venue": "arXiv preprint arXiv:1409.1259,",
            "year": 2014
        },
        {
            "authors": [
                "Gourav Datta",
                "Haoqin Deng",
                "Robert Aviles",
                "Zeyu Liu",
                "Peter A. Beerel"
            ],
            "title": "Bridging the gap between spiking neural networks & lstms for latency & energy efficiency",
            "venue": "IEEE/ACM International Symposium on Low Power Electronics and Design (ISLPED),",
            "year": 2023
        },
        {
            "authors": [
                "Douglas Coimbra De Andrade",
                "Sabato Leo",
                "Martin Loesener Da Silva Viana",
                "Christoph Bernkopf"
            ],
            "title": "A neural attention model for speech command recognition",
            "venue": "arXiv preprint arXiv:1808.08929,",
            "year": 2018
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "Lucas Beyer",
                "Alexander Kolesnikov",
                "Dirk Weissenborn",
                "Xiaohua Zhai",
                "Thomas Unterthiner",
                "Mostafa Dehghani",
                "Matthias Minderer",
                "Georg Heigold",
                "Sylvain Gelly"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
            "venue": "arXiv preprint arXiv:2010.11929,",
            "year": 2020
        },
        {
            "authors": [
                "Wei Fang",
                "Zhaofei Yu",
                "Yanqi Chen",
                "Tiejun Huang",
                "Timoth\u00e9e Masquelier",
                "Yonghong Tian"
            ],
            "title": "Deep residual learning in spiking neural networks",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Yuan Gong",
                "Yu-An Chung",
                "James Glass"
            ],
            "title": "Ast: Audio spectrogram transformer",
            "venue": "arXiv preprint arXiv:2104.01778,",
            "year": 2021
        },
        {
            "authors": [
                "Albert Gu",
                "Tri Dao",
                "Stefano Ermon",
                "Atri Rudra",
                "Christopher R\u00e9"
            ],
            "title": "Hippo: Recurrent memory with optimal polynomial projections",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Albert Gu",
                "Karan Goel",
                "Christopher R\u00e9"
            ],
            "title": "Efficiently modeling long sequences with structured state spaces",
            "venue": "arXiv preprint arXiv:2111.00396,",
            "year": 2021
        },
        {
            "authors": [
                "Anmol Gulati",
                "James Qin",
                "Chung-Cheng Chiu",
                "Niki Parmar",
                "Yu Zhang",
                "Jiahui Yu",
                "Wei Han",
                "Shibo Wang",
                "Zhengdong Zhang",
                "Yonghui Wu"
            ],
            "title": "Conformer: Convolution-augmented transformer for speech recognition",
            "year": 2005
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Kevin Gimpel"
            ],
            "title": "Gaussian error linear units (gelus)",
            "venue": "arXiv preprint arXiv:1606.08415,",
            "year": 2016
        },
        {
            "authors": [
                "Sepp Hochreiter",
                "J\u00fcrgen Schmidhuber"
            ],
            "title": "Long short-term memory",
            "venue": "Neural computation,",
            "year": 1997
        },
        {
            "authors": [
                "Angelos Katharopoulos",
                "Apoorv Vyas",
                "Nikolaos Pappas",
                "Fran\u00e7ois Fleuret"
            ],
            "title": "Transformers are rnns: Fast autoregressive transformers with linear attention",
            "venue": "In International conference on machine learning,",
            "year": 2020
        },
        {
            "authors": [
                "Seijoon Kim",
                "Seongsik Park",
                "Byunggook Na",
                "Sungroh Yoon"
            ],
            "title": "Spiking-yolo: spiking neural network for energy-efficient object detection",
            "venue": "In Proceedings of the AAAI conference on artificial intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "arXiv preprint arXiv:1412.6980,",
            "year": 2014
        },
        {
            "authors": [
                "Alex Krizhevsky",
                "Ilya Sutskever",
                "Geoffrey E Hinton"
            ],
            "title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing",
            "year": 2012
        },
        {
            "authors": [
                "Quoc V. Le",
                "Navdeep Jaitly",
                "Geoffrey E. Hinton"
            ],
            "title": "A simple way to initialize recurrent networks of rectified linear units",
            "venue": "ArXiv, abs/1504.00941,",
            "year": 2015
        },
        {
            "authors": [
                "Yann LeCun",
                "Corinna Cortes",
                "Christopher J.C. Burges"
            ],
            "title": "The mnist database of handwritten digits",
            "year": 1998
        },
        {
            "authors": [
                "Chankyu Lee",
                "Syed Shakib Sarwar",
                "Priyadarshini Panda",
                "Gopalakrishnan Srinivasan",
                "Kaushik Roy"
            ],
            "title": "Enabling spike-based backpropagation for training deep neural network architectures",
            "venue": "Frontiers in neuroscience,",
            "year": 2020
        },
        {
            "authors": [
                "Jun Haeng Lee",
                "Tobi Delbruck",
                "Michael Pfeiffer"
            ],
            "title": "Training deep spiking neural networks using backpropagation",
            "venue": "Frontiers in neuroscience,",
            "year": 2016
        },
        {
            "authors": [
                "Ze Liu",
                "Yutong Lin",
                "Yue Cao",
                "Han Hu",
                "Yixuan Wei",
                "Zheng Zhang",
                "Stephen Lin",
                "Baining Guo"
            ],
            "title": "Swin transformer: Hierarchical vision transformer using shifted windows",
            "venue": "In Proceedings of the IEEE/CVF international conference on computer vision,",
            "year": 2021
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter"
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "arXiv preprint arXiv:1711.05101,",
            "year": 2017
        },
        {
            "authors": [
                "Ali Lotfi Rezaabad",
                "Sriram Vishwanath"
            ],
            "title": "Long short-term memory spiking networks and their applications",
            "venue": "In International Conference on Neuromorphic Systems 2020,",
            "year": 2020
        },
        {
            "authors": [
                "Changze Lv",
                "Jianhan Xu",
                "Xiaoqing Zheng"
            ],
            "title": "Spiking convolutional neural networks for text classification",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Wolfgang Maass"
            ],
            "title": "Networks of spiking neurons: the third generation of neural network models",
            "venue": "Neural networks,",
            "year": 1997
        },
        {
            "authors": [
                "Qingyan Meng",
                "Mingqing Xiao",
                "Shen Yan",
                "Yisen Wang",
                "Zhouchen Lin",
                "Zhi-Quan Luo"
            ],
            "title": "Training high-performance low-latency spiking neural networks by differentiation on spike representation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Paul A Merolla",
                "John V Arthur",
                "Rodrigo Alvarez-Icaza",
                "Andrew S Cassidy",
                "Jun Sawada",
                "Filipp Akopyan",
                "Bryan L Jackson",
                "Nabil Imam",
                "Chen Guo",
                "Yutaka Nakamura"
            ],
            "title": "A million spikingneuron integrated circuit with a scalable communication network and interface",
            "year": 2014
        },
        {
            "authors": [
                "Bo Peng",
                "Eric Alcaide",
                "Quentin Anthony",
                "Alon Albalak",
                "Samuel Arcadinho",
                "Huanqi Cao",
                "Xin Cheng",
                "Michael Chung",
                "Matteo Grella",
                "Kranthi Kiran GV"
            ],
            "title": "Rwkv: Reinventing rnns for the transformer era",
            "venue": "arXiv preprint arXiv:2305.13048,",
            "year": 2023
        },
        {
            "authors": [
                "Darjan Salaj",
                "Anand Subramoney",
                "Ceca Kraisnikovic",
                "Guillaume Bellec",
                "Robert Legenstein",
                "Wolfgang Maass"
            ],
            "title": "Spike frequency adaptation supports network computations on temporally dispersed information",
            "venue": "Elife, 10:e65459,",
            "year": 2021
        },
        {
            "authors": [
                "Leslie N Smith",
                "Nicholay Topin"
            ],
            "title": "Super-convergence: Very fast training of neural networks using large learning rates. In Artificial intelligence and machine learning for multi-domain operations applications, volume 11006",
            "year": 2019
        },
        {
            "authors": [
                "Yi Tay",
                "Mostafa Dehghani",
                "Samira Abnar",
                "Yikang Shen",
                "Dara Bahri",
                "Philip Pham",
                "Jinfeng Rao",
                "Liu Yang",
                "Sebastian Ruder",
                "Donald Metzler"
            ],
            "title": "Long range arena: A benchmark for efficient transformers",
            "year": 2011
        },
        {
            "authors": [
                "Ilya O Tolstikhin",
                "Neil Houlsby",
                "Alexander Kolesnikov",
                "Lucas Beyer",
                "Xiaohua Zhai",
                "Thomas Unterthiner",
                "Jessica Yung",
                "Andreas Steiner",
                "Daniel Keysers",
                "Jakob Uszkoreit"
            ],
            "title": "Mlp-mixer: An all-mlp architecture for vision",
            "venue": "Advances in neural information processing systems,",
            "year": 2021
        },
        {
            "authors": [
                "Hugo Touvron",
                "Thibaut Lavril",
                "Gautier Izacard",
                "Xavier Martinet",
                "Marie-Anne Lachaux",
                "Timoth\u00e9e Lacroix",
                "Baptiste Rozi\u00e8re",
                "Naman Goyal",
                "Eric Hambro",
                "Faisal Azhar"
            ],
            "title": "Llama: Open and efficient foundation language models",
            "venue": "arXiv preprint arXiv:2302.13971,",
            "year": 2023
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Aaron Voelker",
                "Ivana Kaji\u0107",
                "Chris Eliasmith"
            ],
            "title": "Legendre memory units: Continuous-time representation in recurrent neural networks",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Sinong Wang",
                "Belinda Z Li",
                "Madian Khabsa",
                "Han Fang",
                "Hao Ma"
            ],
            "title": "Linformer: Self-attention with linear complexity",
            "venue": "arXiv preprint arXiv:2006.04768,",
            "year": 2020
        },
        {
            "authors": [
                "Pete Warden"
            ],
            "title": "Speech commands: A dataset for limited-vocabulary speech recognition",
            "venue": "arXiv preprint arXiv:1804.03209,",
            "year": 2018
        },
        {
            "authors": [
                "Yujie Wu",
                "Lei Deng",
                "Guoqi Li",
                "Jun Zhu",
                "Luping Shi"
            ],
            "title": "Spatio-temporal backpropagation for training high-performance spiking neural networks",
            "venue": "Frontiers in neuroscience,",
            "year": 2018
        },
        {
            "authors": [
                "Yujie Wu",
                "Lei Deng",
                "Guoqi Li",
                "Jun Zhu",
                "Yuan Xie",
                "Luping Shi"
            ],
            "title": "Direct training for spiking neural networks: Faster, larger, better",
            "venue": "In Proceedings of the AAAI conference on artificial intelligence,",
            "year": 2019
        },
        {
            "authors": [
                "Tete Xiao",
                "Mannat Singh",
                "Eric Mintun",
                "Trevor Darrell",
                "Piotr Doll\u00e1r",
                "Ross Girshick"
            ],
            "title": "Early convolutions help transformers see better",
            "venue": "Advances in neural information processing systems,",
            "year": 2021
        },
        {
            "authors": [
                "Yunyang Xiong",
                "Zhanpeng Zeng",
                "Rudrasis Chakraborty",
                "Mingxing Tan",
                "Glenn Fung",
                "Yin Li",
                "Vikas Singh"
            ],
            "title": "Nystr\u00f6mformer: A nystr\u00f6m-based algorithm for approximating self-attention",
            "year": 2021
        },
        {
            "authors": [
                "Manzil Zaheer",
                "Guru Prashanth Guruganesh",
                "Avinava Dubey",
                "Joshua Ainslie",
                "Chris Alberti",
                "Santiago Ontanon",
                "Philip Minh Pham",
                "Anirudh Ravula",
                "Qifan Wang",
                "Li Yang",
                "Amr Mahmoud El Houssieny Ahmed"
            ],
            "title": "URL https://proceedings.neurips.cc/paper/2020/file/ c8512d142a2d849725f31a9a7a361ab9-Paper.pdf",
            "venue": "Big bird: Transformers for longer sequences",
            "year": 2020
        },
        {
            "authors": [
                "Zhaokun Zhou",
                "Yuesheng Zhu",
                "Chao He",
                "Yaowei Wang",
                "Shuicheng Yan",
                "Yonghong Tian",
                "Li Yuan"
            ],
            "title": "Spikformer: When spiking neural network meets transformer",
            "venue": "arXiv preprint arXiv:2209.15425,",
            "year": 2022
        },
        {
            "authors": [
                "Rui-Jie Zhu",
                "Qihang Zhao",
                "Jason K. Eshraghian"
            ],
            "title": "Spikegpt: Generative pre-trained language model with spiking neural networks",
            "venue": "arXiv preprint arXiv:2302.13939,",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "In recent years, Transformers (Vaswani et al., 2017) have become the most prevalent deep learning models for many applications. The global attention mechanism enables Transformers to capture long-distance dependencies in input data. Moreover, their state-less structure makes their training easily parallelizable, helping them perform well with large-scale datasets, leading to SOTA accuracy in a wide range of applications, including Natural Language Processing (NLP) (Brown et al., 2020; Touvron et al., 2023), Computer Vision (CV) (Dosovitskiy et al., 2020; Liu et al., 2021), and spoken term classification (STC) (Gulati et al., 2020; Gong et al., 2021). However, their selfattention mechanism imposes compute and memory complexity that is quadratic in the sequence length N . Moreover, the nature of the self-attention computation prevents the underlying hardware from performing much of the computation until after the entire input sequence is available.\nIn contrast, Recurrent Neural Network (RNN) models are designed to process data sequentially and their complexity is only O(N) making them an attractive low-complexity alternative to transformers. However, RNN models traditionally have higher training time as the training procedure must\naccommodate the long sequence of dependencies within the model, making parallelization more difficult. Moreover, unlike transformers, RNN models like LSTMs (Hochreiter & Schmidhuber, 1997) and GRUs (Cho et al., 2014) can only leverage past and current information and traditionally suffer from forgetting due to having a limited memory horizon. Recently, however, Legendre Memory Units(Voelker et al., 2019) have been proposed that have the ability to remember information over a theoretically infinite time horizon. Benefiting from this property, LMU-based models outperform many RNN models while still underperforming compared to Transformer alternatives. We hypothesize that the remaining performance gap between LMUs and Transformers is not only because Transformers benefit from future information, but also because they possess more complex network structures, such as the self-attention computation, and thus have higher representational capacity.\nIn an attempt to explore this hypothesis, we propose a novel sequential network architecture, dubbed LMUFormer, that augments the LMU module with convolutional patch embedding and convolutional channel mixers. Importantly, our convolutional patch embedding only interacts with neighboring input samples and our convolutional channel mixers and the final classifier only operate on the current state of the LMU. Therefore, our LMUFormer model can process data sequentially. Moreover, we present a spiking version of this architecture which extends the benefits of state that is explicit in the LMU implicitly to the patch embedding and channel mixing structures to improve accuracy while simultaneously further enabling complexity reduction. In summary, this paper makes the following contributions:\n\u2022 We propose a novel architecture LMUFormer which outperforms most existing RNN models with similar complexity on a wide range of sequence learning tasks. On evaluating the Speech Commands dataset, our LMUFormer model, when benchmarked against transformer-based models with competitive performance levels, manifests a significant reduction \u2014 accounting for 53\u00d7 fewer parameters and 65\u00d7 diminished FLOPs.\n\u2022 We further devised a spiking variant of the LMUFormer that not only achieves state-ofthe-art (SOTA) performance within the realm of SNN models on the Speech Commands dataset but also presents comparable results on the Long Range Arena benchmark.\n\u2022 Attributable to the real-time data processing capabilities inherent to our models, we evaluate the performance of the model when different proportions of sequences are received. We demonstrate that our model can achieve 99% of the original performance while reducing 32.03% sequence length."
        },
        {
            "heading": "2 PRELIMINARIES",
            "text": "Legendre Memory Unit & Parallel Training: The Legendre Memory Unit (LMU) is a memory cell proposed by Voelker et al. (2019) designed to efficiently capture and represent temporal dependencies in sequential data rooted in the mathematical properties of Legendre polynomials. Mathematically, the LMU is based on two state-space matrices (A,B) that approximate a linear transfer function in continuous time as follows\nm\u0307(t) = Am(t) +Bu(t), (1)\nwhere u(t) represents the input signal and m(t) represents the memory state vector. This continuoustime system is mapped onto discrete time with time step t as follows\nm[t] = A\u0304m[t\u2212 1] + B\u0304u[t], (2)\nwhere A\u0304 and B\u0304 are the discretized version of A\u0304 and B\u0304.\nTo better support parallel training, we adopt the same module design as Chilkuri & Eliasmith (2021) to get u[t] from x[t], which is the input signal of the LMU cell in t time step, as follows\nu[t] = Actu(Wux[t] + bu) (3)\nand obtain the output of the LMU module as described as follows\no[t] = Acto(Wmm[t] +Wxx[t] + bo), (4)\nwhere Wu, Wm, and Wx are the learnable parameter matrices. Note that Actu and Acto represent the activation functions. Therefore, the module only has one recurrent connection and can be\nregarded as a linear time-invariant (LTI) system which can be solved in a non-iterative way which is the key for parallel training. We also adopt fast Fourier transform (FFT) as Chilkuri & Eliasmith (2021) to further reduce the training complexity to O(N log2(N) \u00b7 dc), where N is the length of the sequence and dc is the feature dimension of the input x.\nSpiking Neural Network: As the third-generation neural network (Maass, 1997), SNNs have gained a lot of attention for their potential for higher energy efficiency than traditional ANNs. Mimicking the behavior of biological neurons which communicate using brief electrical pulses, SNNs use binary \u201dspikes\u201d to process and transmit information. However, to convert sequential multi-bit input data such as audio or text into spikes, a coding scheme is required. Rate coding (Lee et al., 2016) and direct coding (Wu et al., 2019) are two of the most commonly used methods. Rate coding translates the input sequence into a spike train across T time steps, with the spike count correlating to input magnitude and spikes following a Poisson distribution (Lee et al., 2020). In direct coding, in contrast, the multi-bit inputs are fed to the first convolution layer in the models and spikes are used only in subsequent portions of the network. If the dataset does not contain temporal information, the outputs of the 1st convolution layer need to be repeated T time steps, and converted to binary or multi-bit spikes through spiking neurons.\nIn this paper, we use direct coding as well as the Leaky Integrate-and-Fire (LIF) (Maass, 1997) neuron model. The behavior of the LIF neuron is described as follows:\nutl = \u03bbu t\u22121 l +wlo t l\u22121 \u2212 vthl ot\u22121l\u22121 o t\u22121 l = { 1, if ut\u22121l \u2265 vthl ; 0, otherwise\n(5)\nutl represents the membrane potential tensor of the l th layer at the tth time step, \u03bb is a leak factor that varies between 0 and 1, wl is the weight connecting layers (l\u22121) and l, otl\u22121 is the spike output of the (l\u22121)th layer at the tth time step, vthl is the threshold that is kept constant for layer l."
        },
        {
            "heading": "3 RELATED WORK",
            "text": "SNN for sequential learning: In the domain of computer vision, various SNN models have been proposed, serving purposes ranging from image recognition (Fang et al., 2021; Meng et al., 2022) to object detection (Kim et al., 2020; Barchid et al., 2023), and these models have demonstrably achieved competitive performance relative to their ANN counterparts. Notwithstanding, the exploration of SNNs in sequential tasks, such as text classification and spoken term classification, remains notably limited, with scant literature (Lotfi Rezaabad & Vishwanath, 2020; Datta et al., 2023; Lv et al., 2022) addressing these applications.\nRecurrent Transformers: Since the inception of the work Katharopoulos et al. (2020) that proposed linear transformers, many researchers focused on modifying the self-attention mechanism to make transformer-based model have lower costs during inference. Linformer (Wang et al., 2020) incorporates fixed-size linear projections, facilitating a streamlined approximation of attention across longer sequences. Similarly, Nystromformer (Xiong et al., 2021)leverages the Nystro\u0308m method to realize an approximation of the standard self-attention mechanism, achieving linear complexity. In a distinct approach, Peng et al. (2023) proposed a novel model architecture, Receptance Weighted Key Value (RWKV). Notably, the time-mixing block within RWKV can arguably be interpreted as computing the product of the matrices K and V, subsequently multiplied by R, suggesting its foundational roots in the Transformer paradigm. Building upon this foundation, Zhu et al. (2023) design the SpikeGPT based on RWKV which has demonstrated competitive performance across both Natural Language Generation (NLG) and Natural Language Understanding (NLU) tasks.\nMLP-Mixer: The MLP-Mixer (Tolstikhin et al., 2021) has emerged as a novel paradigm in the field of computer vision, diverging from the conventional approaches of Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs). The MLP-Mixer leverages multilayer perceptrons (MLPs) for both spatial and channel-wise data processing. By sequentially employing independent MLPs, it avoids convolution and attention mechanisms, resulting in a streamlined architecture. Inspired by this, we followed a similar framework to design our LMUFormer but further improved it to handle data with temporal information."
        },
        {
            "heading": "4 PROPOSED SPIKING LMUFORMER",
            "text": "We propose spiking LMUFormer as shown in Fig. 1, which can be trained in parallel and process data sequentially during inference. Our model is primarily based on LMU and augmented with several techniques from Transformers. Specifically, our model consists of a convolutional patch embedding, LMU block, convolutional channel mixers, and a final fully-connected layer as the classification head. This section first introduces the structure of the patch embedding and channel mixers and then elaborates on how to convert the model to an efficient spiking version."
        },
        {
            "heading": "4.1 CONVOLUTIONAL PATCH EMBEDDING",
            "text": "Embedding is the first component of Transformer models and is responsible for converting words/tokens into continuous vectors. For NLP tasks, we simply use the built-in PyTorch embedding module to model the embedding functionality. For STC tasks, we design two types of Convolutional patch embedding. The first is inspired by Gong et al. (2021) and Xiao et al. (2021) which used a series of 2D convolutional layers to split the spectrogram into a sequence of N 16\u00d716 patches. Specifically, we create a new dimension for the embedding features and regard the spectrogram as an image to perform convolution. This structure mixes temporal and frequency information to extract superior feature representations. However, it also yields significant algorithmic delays when processing sequential input data, hindering real-time processing.\nTo mitigate this concern, we propose to use several 1D convolutional layers with a kernel size of 3 for patch embedding, as we illustrate in Fig. 2(a). When the kernel size is 1, the patch embedding layer is ideal for processing sequential data with no algorithmic latency but also suffers from poor performance. We empirically observed that a convolution kernel of size 3 can dramatically improve the performance since it can capture information about nearby samples and, as analyzed in Fig. 2(b), the added latency is negligible. As an example, the blue lines in Fig. 2(b) represent that to obtain the first output in layer 5, i.e., the output of the patch embedding, we need to wait until the 9th input sample arrives. After that, for each input sample fed into the model, the patch embedding can calculate an output, as shown by the red lines. Thus, our network can process the input sequence in real-time after a delay of only 9 samples, which is negligible compared to the total number of samples (typically at least a few hundreds) present in any sequence learning task."
        },
        {
            "heading": "4.2 CONVOLUTIONAL CHANNEL MIXER",
            "text": "Inspired by Tolstikhin et al. (2021), we expect the LMU module to be primarily responsible for mixing information across tokens due to the presence of the state variables. To effectively capture the information across the different feature channels, we augment the LMU module with a channel mixer module, that consists of BN and non-linear activation layers followed by 1\u00d71 convolutional layers as described in Eq. 6. The first activation layer employs Gaussian Error Linear Units (GELU) as introduced by Hendrycks & Gimpel (2016), while the subsequent activation layer utilizes the Rectified Linear Units (ReLU) (Krizhevsky et al., 2012). Since the convolutional layer, BN layer, and non-linear activation layer do not interact with temporal information, we omit the time notation T in Eq. 6 for simplicity. We also add a residual connection (He et al., 2016) between the input Xi and the output Xo of the convolutional channel mixer and refer to this enhanced structure as the Conv Channel Mixer Block.\nX = Conv1(GELU(BN1(Xi))), Xo = Conv2(ReLU(BN2(X))) (6)"
        },
        {
            "heading": "4.3 SPIKING LMUFORMER NETWORK STRUCTURE",
            "text": "To further improve the energy efficiency, we propose a spiking version of the LMUFormer, named spiking LMUFormer.\nFor NLP tasks, we use the batch normalization and the spiking LIF (SN) layer in the first LMU block to convert the floating point values to spikes, that results in sparsity and accumulate-only operations. For STC tasks, we use the first convolution layer together with its followed BN and SN layer in the patch embedding as the direct encoder. Inspired by (Yao et al., 2023), we also adjust the positions of residual shortcuts to make sure the output is the addition between two floating point numbers rather than the spikes.\nAs depicted in Fig. 3, the spiking LMU block encompasses several layers, structured sequentially as: a BN layer, followed by a SN layer, succeeded by the core spiking LMU cell, which is then coupled with a Conv layer, and concluded with another BN layer. Notably, the integration strategy we employed capitalizes on the inherent temporal dynamics of both the LMU and the SNN. Considering the concurrent updates of both the memory and hidden states in the LMU at every discrete time step, in conjunction with the analogous updates of the membrane potentials and spikes within the SNN, we have devised a merged process to optimize the overall operational efficiency of their integration.\nIn particular, we feed the input spikes XS [t] into a convolutional layer and a BN layer to get the input signal of the memory cell.\nU [t] = BN(Conv1d(XS [t])) (7)\nMeanwhile, they are also regarded as the input of spike neurons. As we adopt the Leaky Integrateand-Fire (LIF) neuron (Maass, 1997) model, the update of the membrane potential UH [t] at time step t is described below (Wu et al., 2018)\nUH [t] = UV [t\u2212 1] + 1\n\u03c4 (U [t]\u2212 (UV [t\u2212 1]\u2212 UresetV )), (8)\nwhere UV [t\u2212 1] means the membrane potential after emitting spikes at time step t\u2212 1, \u03c4 is the time constant, and UresetV represents the reset potential. The firing of spikes is then described as follows\nUS [t] = F (UH [t]\u2212 U thV ) (9) where US [t] means the spikes of the input signal of the memory cell and F denotes the firing function, which outputs 1 when the input is greater than 0, and 0 otherwise. Finally, to reset the membrane potential UV at time step t we use the following equation\nUV [t] = UH(t) \u00b7 (1\u2212 US [t]) + UresetV \u00b7 US [t]. (10)\nAfter obtaining US [t] and the memory vector M [t \u2212 1] in the last time step, we can formulate the update of M [t] as shown below\nM [t] = A\u0304 \u00b7MS [t\u2212 1] + B\u0304 \u00b7 US [t], (11)\nwhere A\u0304 and B\u0304 are the discretized space metric from Voelker et al. (2019). To simplify the notation, we use the function Spiking Neuron SN(\u00b7) as the abbreviation of Eqs. (8) to (10). Therefore, as shown below\nMS [t] = SN(M [t]), O[t] = SN(BN(Conv1d([MS [t], XS [t]]))) (12)\nwe first feed the M [t] into the spiking neurons to get the spikes of the memory state at time step t. Then we concatenate MS [t] and XS [t] along the feature dimension and feed them into the 1D Conv layer, BN layer, and SN to get the output spikes of the LMU block at time step t."
        },
        {
            "heading": "5 EXPERIMENTS",
            "text": "5.1 PERMUTED SEQUENTIAL MNIST\nTo evaluate the performance of our models on vision task, we use the permuted sequential MNIST (psMNIST) dataset (Le et al., 2015). Unlike the original MNIST dataset (LeCun et al., 1998) showing each 28\u00d7 28 grayscale image as a whole, psMNIST presents the pixels of each image as a sequence, typically in a permuted order. It results in this task being much more challenging than the original task since the models need to remember and integrate in-\nformation over time, i.e., have the ability to handle long-range dependencies. psMNIST contains 10 classes, involving handwritten digits from 0 to 9 and we split 50k images as training dataset, 10k images as validation dataset, and the rest 10k images as test images.\nAs shown in Table .3, our models not only outperform all the RNN models but also surpass the existing LMU-based models. Since the psMNIST dataset is relatively small, directly applying our LMUFormer model could easily lead to overfitting, so we used a simplified version. We only use one Conv layer as well as a BN layer as the patch embedding and remove the channel mixer."
        },
        {
            "heading": "5.2 SPEECH RECOGNITION",
            "text": "We use the Speech Commands V2 dataset (Warden, 2018) contains 105,829 audio files of 35 different words. Each recording lasts up to 1 second and the sampling rate is 16KHz. We use the official dataset splits, where the training, validation, and test dataset contains 84,843, 9,981, and 11,005 samples, respectively. We chose the hardest classification task with 35 classes to test our models.\nAs shown in Table 2, our LMUFormer model outperforms the existing RNN models. Although there is still about a 1.5% drop in test accuracy between our model with the SOTA transformer-based model, our model has fewer parameters (1.622 Million vs. 86.933 Million, 86.933/1.622 \u2248 53.66), fewer FLOPs (1.89 \u00d7 108 vs. 1.24 \u00d7 1010, 124/1.89 \u2248 65.61) and can handle data sequentially. Additionally, our evaluations indicate that the LMUFormer (with states) achieves a noteworthy performance of 96.92%. This empirical outcome substantiates our initial hypothesis that the integration of states within patch embedding and the channel mixer block can enhance model performance. In pursuit of unparalleled energy efficiency, we convert this model to the 1-bit SNN model, i.e., the spiking LMUFormer, which surpasses all the contemporary SNN models with mere 3.09\u00d7 107 theoretical synaptic operations (SOPs) (Merolla et al., 2014).\nIn addition, we also added the result of two experiments for two transformerbased SNN models, Spikeformer (Zhou et al., 2022) and Spike-driven ViT (Yao et al., 2023), that achieve the SOTA performance in the vision tasks. Because they do not provide the official version of their models for the SC dataset, we tested new models in which we use their proposed structure of self-attention as a substitute for the LMU module in our model. It is important to note that the\nSpikformer (Zhou et al., 2022) model has a dramatic drop compared with the non-spiking transformer-based AST (Gong et al., 2021). This suggests that if we restrict the patch embedding to only mix the information from neighboring samples, the power of the original global attention in transformer models will be degraded. On the contrary, the Spike-driven ViT (Yao et al., 2023) can preserve a higher accuracy which indicates its linear attention is more robust to the degradation of the features extracted by the patch embedding. Moreover, our spiking LMUFormer achieves\na further 1.27% increase in accuracy, demonstrating that our model is superior to the traditional transformer-based spiking models when handling information with temporal information."
        },
        {
            "heading": "5.3 LONG RANGE ARENA BENCHMARK",
            "text": "To showcase the capability of our LMUFormer with longer tokens, we utilize the Long Range Arena (LRA) benchmark introduced by Tay et al. (2020). The LRA benchmark assesses machine learning models on long-context understanding through subtasks including text classification, document retrieval, image classification, pathfinder, and listops, highlighting their performance across various domains. Adhering to the evaluation protocol from Tay et al. (2020), which establishes specific train/test splits, we report the classification accuracy for each task and present an aggregated performance measure across all tasks.\nWe conducted a comparative study involving five transformer-based models: the vanilla transformer (Vaswani et al., 2017), Linear Trans. (Katharopoulos et al., 2020), Linformer (Wang et al., 2020), BigBird (Zaheer et al., 2020) and Nystromformer (Xiong et al., 2021). And the results of the first four models are from Tay et al. (2020). As detailed in Table 4, there is still a significant gap in the performance between our models with the recent S4-based models (Gu et al., 2021), but our LMUFormer excels against these transformer-based models in almost all tasks except Pathfinder. Furthermore, our spiking variant of LMUFormer surpasses them in the ListOps and Retrieval tasks.\nNotably, our spiking LMUFormer is the inaugural SNN model to not only demonstrate comparable performance on the LRA dataset but also to outshine the majority of its transformer-based counterparts. Intriguingly, the spiking LMUFormer outperforms the regular LMUFormer by an average margin of 0.53%, suggesting the potential of SNN models to harness their inherent temporal dynamics for superior performance with long sequences."
        },
        {
            "heading": "5.4 ABLATION STUDY",
            "text": "We first conducted extensive ablation studies to show the impact of the different patch embeddings and channel mixers in our proposed LMUFormer model on the final performance. All the models have one final linear layer that acts as the classification head and are trained and tested on the Speech Command V2 dataset with 35 classes. As shown in Table 5, a naive LMU module with a classification head, same as a pLMU, can only achieve 76.17% accuracy while simply adding a convolution layer can boost the result to 89.28%. Although the performance of the non-spiking model with our LMU block is only slightly better than the model with a naive LMU, our LMU block significantly improves the performance of the spiking model by 4.1% (an increase from 83.78% to 87.88%). Moreover, the results show our convolutional patch embedding can further improve the accuracy of the models compared to models with a simple convolution layer. Finally, using a convolutional channel mixer, our models achieve a final accuracy of 96.53% for the non-spiking model and 96.12% for the spiking model.\n5.5 RECURRENT PROCESSING\nTo more clearly showcase our model\u2019s proficiency with sequential data, we evaluated the trained spiking LMUFormer on the Speech Command V2 test dataset, gradually increasing the sequence length from 0 to its full length of 128 samples. As shown in Fig. 4, the increase in the model\u2019s accuracy with the number of samples first accelerates continuously and then levels off. This indicates that after a certain number of samples have been obtained, the model has been able to predict the results almost correctly. Specifically, our model achieves 99% of its original performance while getting a 32.03% (1 - 87/128) reduction in the sequence length, yielding results of 95.17% compared to 96.12% which is even higher than the spikformer model (93.38%) which utilizes all the information from the whole sequence."
        },
        {
            "heading": "6 CONCLUSIONS",
            "text": "The transformer model excels across various domains, particularly with large-scale datasets, owing to its exceptional performance and straightforward parallel training. However, its O(N2) complexity during inference, coupled with the necessity to acquire the complete sequence before calculating the self-attention, limits its utility in resource-constrained environments demanding low latency. Hence, we introduce the LMUFormer and Spiking LMUFormer models, uniquely designed to switch between parallel and recurrent forwarding when training and testing. Through extensive experiments, we demonstrate that our non-spiking model achieves close to the performance of state-of-the-art (SOTA) models while markedly reducing the model size by approximately 53\u00d7 and computational complexity by roughly 65\u00d7. Furthermore, our Spiking LMUFormer achieves SOTA performance, registering a notable accuracy of 96.12% among prevailing SNN models evaluated on the Speech Commands V2 dataset, without compromising on efficiency. It is our aspiration that this contribution serves as a catalyst for further exploration and advancements in model design for SNNs, particularly in the domain of sequence learning tasks."
        },
        {
            "heading": "7 ACKNOWLEDGEMENTS",
            "text": "This work is supported by a gift funding from Intel Labs. We would also like to thank Sumit Bam Shrestha and Timothy Shea from Intel for insightful discussions on LMUFormer."
        },
        {
            "heading": "A APPENDIX",
            "text": "A.1 LRA BENCHMARK\nThe benchmark comprises:\n\u2022 ListOps: A synthetic task where models operate on nested lists, gauging their understanding of hierarchical structures.\n\u2022 Text: Byte-level classification of IMDb reviews, pushing the boundaries on sequence lengths.\n\u2022 Retrieval: Document retrieval tasks centered around byte-level tokens. \u2022 Image: Image classification, but uniquely handled as pixel sequences, sidestepping tradi-\ntional 2D processing.\n\u2022 Pathfinder: An image-based task assessing if an unobstructed path connects two highlighted points, challenging models on long-range spatial dependencies.\nA.2 TRAINING HYPERPARAMETERS\nFor psMNIST dataset, we trained the models 50 epochs with Adam (Kingma & Ba, 2014) optimizer and the initial learning rate is 0.0001 and the batch size is 100. As for the Speech Commands V2 dataset, the batch size is set to 128 during 100 (200) epochs with an initial learning rate of 0.00025 (0.0025) and weight decay of 5e\u22127. After the very first 5 (10) epochs, we multiply the learning rate by 0.85 every 5 (10) epochs. The numbers in brackets are the settings for spiking models. For the LRA dataset, we generally used the setting as Gong et al. (2021). For all subtasks, we adopted AdamW (Loshchilov & Hutter, 2017) optimizer and the 1cycle learning rate policy (Smith & Topin, 2019). The weight decay for the \u2019Image\u2019 task is 0.01 and 0 for other tasks. More hyperparameters are shown in Table. 6.\nA.3 MODEL BIAS & LIMITATIONS\nThere may be a few potential limitations or scenarios where LMUFormer may not perform very well. For example, LMUFormer may lack the pre-training capabilities of a Transformer, that can enable high performance when fine-tuned on a range of downstream tasks. Rather, it needs to be trained on the streaming use-case directly by the user. That said, it is not very clear whether pre-training on a large text corpus that empower Transformers, can improve the performance of our models on streaming use-cases. Moreover, to our best knowledge, there is no large-scale streaming dataset, such as the BookCorpus, that we can use to pre-train our models. Given a large-scale pre-trained\ndataset (and enough compute power), it may be possible for LMUFormer to scale up, and achieve high performance when fine-tuned on downstream tasks. We hypothesize this may not be possible by simply stacking LMUFormer blocks, and may require network architectural optimizations, which is an important research direction.\nMoreover, as shown in Table 4, LMUFormer, though yields higher performance compared to all transformer variants, can not surpass the S4 models on LRA dataset, that evaluates model quality under long-context scenarios. Improving the LMUFormer performance for such datasets is also an important and interesting research direction.\nA.4 COMPARISON BETWEEN DIFFERENT MODELS ON LRA\nFor the LRA dataset, we use the same word and position embedding for all the models, replacing the patch embedding block we use for LMUFormer. This narrows the gap between LMUFormer and pLMU models on LRA compared to Google Speech Commands as shown in Table 7. Overall, the LMUFormer significantly outperforms pLMU, with comparable results only in Task \u2019Image\u2019 and Task \u2019Pathfinder\u2019.\nWe also show a comparison of performance, model size, peak memory, training time, and inference time for the pLMU model, LMUFormer, Spiking LMUFormer and other Transformer variants, using the \u201dText\u201d subtask as an example. Our results are shown in the Table 8. As we can see, our LMUFormer achieves the best performance while maintaining relatively decent training and inference speeds, and requires less memory during training.\nA.5 HARDWARE-SOFTWARE CO-DESIGN\nWe develop a hardware simulation framework for SNNs to estimate the energy, latency, and throughput of our non-spiking and spiking LMUFormer models. We have also incorporated the overhead due to the spike sparsity in our framework, which is minimal with high sparsity as obtained in this work.\nThe total compute energy (CE) of the spiking LMUFormer (SpLMUCE) can be estimated as\nSpLMUCE= T\u2211 t=1 ( DNNop1 Emac+ L\u2211 l=2 (StlDNN op l Eac + EspDNN op l )+ L\u2211 l=1 DNN coml Ecom ) (13) because the SNN receives full-precision input in the first layer (l=1) without any sparsity. Note that DNN coml denotes the total number of comparison (thresholding) operations in the layer l with each operation consuming 1.64pJ energy in our 28nm Kintex-7 FPGA platform for floating point (FP) reperesentation. Also, note that DNNopl denote the total number of floating point (MAC or AC) operations in layer l. Lastly, Stl denotes the activation sparsity of layer l at time step t, and Esp = 0.05pJ denotes the energy overhead due to sparsity, that is incurred in checking whether the binary activation is zero.\nThe CE of the full-precision LMUFormer (LMUCE) is estimated as DNNCE =\u2211L l=1 DNN op l Emac, where we ignore the energy consumed by the non-linear activation operation (significantly lower compared to thresholding operation).\nThe compute-efficiency of the spiking LMUFormer stems from two factors: 1) high activation sparsity, where \u2211T t=1 S t l=0.15 on average across all the layers, and 2) Use of only AC (1.8pJ) operations that consume 7.4\u00d7 lower compared to each MAC (13.32pJ) operation in our FPGA setup for floating point (FP) representation. Note that the binary activations can replace the FP multiplications with logical operations, i.e., conditional assignment to 0 with a bank of AND gates. These replacements may be realized using existing hardware depending on the compiler and the details of their data paths. Based on these two factors, we observe that our spiking LMUFormer incurs 27.2\u00d7 lower compute energy compared to LMUFormer on the Google speech commands at iso-parameter.\nIn contrast, the energy incurred in the memory access of the weights for both non-spiking and spiking LMUFormer depend on the data re-use scheme and the underlying hardware. However, since both the models have almost the same number of trainable parameters, they are expected to incur identical memory energy. We also do not expect any additional latency or throughput improvement in SNN, since we need to process the identical sequential input, and activation sparsity favors compute energy (and not latency). That said, unlike existing SNNs that incur an additional temporal overhead and suffers from high latency, our SNN re-uses the hidden memory dimension of the LMUformer to incur the temporal dimension. Thus, our spiking LMUFormer can yield higher compute efficiency with no overhead on latency."
        }
    ],
    "title": "LMUFORMER: LOW COMPLEXITY YET POWERFUL SPIKING MODEL WITH LEGENDRE MEMORY UNITS",
    "year": 2024
}