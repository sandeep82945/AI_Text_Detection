{
    "abstractText": "Reinforcement Learning with Human Feedback (RLHF) is the most prominent method for Language Model (LM) alignment. However, RLHF is an unstable and data-hungry process that continually requires new high-quality LM-generated data for finetuning. We introduce Advantage-Leftover Lunch RL (A-LOL), a new class of offline policy gradient algorithms that enable RL training on any pre-existing data. By assuming the entire LM output sequence as a single action, A-LOL allows incorporating sequence-level classifiers or human-designed scoring functions as rewards. Subsequently, by using LM\u2019s value estimate, A-LOL only trains on positive advantage (leftover) data points, making it resilient to noise. Overall, A-LOL is an easy-to-implement, sample-efficient, and stable LM training recipe. We demonstrate the effectiveness of A-LOL and its variants with a set of four different language generation tasks. We compare against both online RL (PPO) and recent preference-based (DPO, PRO) and reward-based (GOLD) offline RL baselines. On the commonly-used RLHF benchmark, Helpful and Harmless Assistant (HHA), LMs trained with A-LOL methods achieve the highest diversity while also being rated more safe and helpful than the baselines according to humans. Additionally, in the remaining three tasks, A-LOL could optimize multiple distinct reward functions even when using noisy or suboptimal training data.",
    "authors": [
        {
            "affiliations": [],
            "name": "Ashutosh Baheti"
        },
        {
            "affiliations": [],
            "name": "Ximing Lu"
        },
        {
            "affiliations": [],
            "name": "Faeze Brahman"
        },
        {
            "affiliations": [],
            "name": "Ronan Le Bras"
        },
        {
            "affiliations": [],
            "name": "Maarten Sap"
        },
        {
            "affiliations": [],
            "name": "Mark Riedl"
        }
    ],
    "id": "SP:f16aad5f96c887127c13810d5cfa8ae8fb1ad956",
    "references": [
        {
            "authors": [
                "Arash Ahmadian",
                "Chris Cremer",
                "Matthias Gall\u00e9",
                "Marzieh Fadaee",
                "Julia Kreutzer",
                "Olivier Pietquin",
                "Ahmet \u00dcst\u00fcn",
                "Sara Hooker"
            ],
            "title": "Back to basics: Revisiting reinforce style optimization for learning from human feedback in llms, 2024",
            "year": 2024
        },
        {
            "authors": [
                "Ashutosh Baheti",
                "Maarten Sap",
                "Alan Ritter",
                "Mark Riedl"
            ],
            "title": "Just say no: Analyzing the stance of neural dialogue generation in offensive contexts",
            "venue": "In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2021
        },
        {
            "authors": [
                "Nova DasSarma",
                "Robert Lasenby",
                "Robin Larson",
                "Sam Ringer",
                "Scott Johnston",
                "Shauna Kravec",
                "Sheer El Showk",
                "Stanislav Fort",
                "Tamera Lanham",
                "Timothy Telleen-Lawton",
                "Tom Conerly",
                "Tom Henighan",
                "Tristan Hume",
                "Samuel R. Bowman",
                "Zac Hatfield-Dodds",
                "Ben Mann",
                "Dario Amodei",
                "Nicholas Joseph",
                "Sam McCandlish",
                "Tom Brown",
                "Jared Kaplan"
            ],
            "title": "Constitutional ai: Harmlessness from ai feedback, 2022b",
            "year": 2022
        },
        {
            "authors": [
                "Antoine Bosselut",
                "Hannah Rashkin",
                "Maarten Sap",
                "Chaitanya Malaviya",
                "Asli Celikyilmaz",
                "Yejin Choi"
            ],
            "title": "COMET: Commonsense transformers for automatic knowledge graph construction",
            "venue": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,",
            "year": 2019
        },
        {
            "authors": [
                "Jeffrey Wu",
                "Clemens Winter",
                "Chris Hesse",
                "Mark Chen",
                "Eric Sigler",
                "Mateusz Litwin",
                "Scott Gray",
                "Benjamin Chess",
                "Jack Clark",
                "Christopher Berner",
                "Sam McCandlish",
                "Alec Radford",
                "Ilya Sutskever",
                "Dario Amodei"
            ],
            "title": "Language models are few-shot learners",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2024
        },
        {
            "authors": [
                "David Krueger",
                "Dorsa Sadigh",
                "Dylan Hadfield-Menell"
            ],
            "title": "Open problems and fundamental limitations of reinforcement learning from human feedback, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Louis Castricato",
                "Alex Havrilla",
                "Shahbuland Matiana",
                "Duy V. Phung",
                "Aman Tiwari",
                "Jonathan Tow",
                "Maksym Zhuravinsky"
            ],
            "title": "trlX: A scalable framework for RLHF",
            "venue": "URL https: //github.com/CarperAI/trlx",
            "year": 2023
        },
        {
            "authors": [
                "Jonathan D. Chang",
                "Kiante Brantley",
                "Rajkumar Ramamurthy",
                "Dipendra Misra",
                "Wen Sun"
            ],
            "title": "Learning to generate better than your llm, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Itsugun Cho",
                "Ryota Takahashi",
                "Yusaku Yanase",
                "Hiroaki Saito"
            ],
            "title": "Deep rl with hierarchical action exploration for dialogue generation, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Marie-Catherine de Marneffe",
                "Christopher D Manning",
                "Christopher Potts"
            ],
            "title": "Did it happen? the pragmatic complexity of veridicality assessment",
            "venue": "Computational Linguistics,",
            "year": 2012
        },
        {
            "authors": [
                "Thomas Degris",
                "Martha White",
                "Richard S. Sutton"
            ],
            "title": "Off-Policy Actor-Critic",
            "venue": "In International Conference on Machine Learning,",
            "year": 2012
        },
        {
            "authors": [
                "Tim Dettmers",
                "Artidoro Pagnoni",
                "Ari Holtzman",
                "Luke Zettlemoyer"
            ],
            "title": "Qlora: Efficient finetuning of quantized llms",
            "year": 2023
        },
        {
            "authors": [
                "Emily Dinan",
                "Stephen Roller",
                "Kurt Shuster",
                "Angela Fan",
                "Michael Auli",
                "Jason Weston"
            ],
            "title": "Wizard of wikipedia: Knowledge-powered conversational agents",
            "venue": "In International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "Jesse Dodge",
                "Taylor Prewitt",
                "Remi Tachet des Combes",
                "Erika Odmark",
                "Roy Schwartz",
                "Emma Strubell",
                "Alexandra Sasha Luccioni",
                "Noah A. Smith",
                "Nicole DeCario",
                "Will Buchanan"
            ],
            "title": "Measuring the carbon intensity of ai in cloud instances",
            "venue": "In 2022 ACM Conference on Fairness, Accountability, and Transparency,",
            "year": 2022
        },
        {
            "authors": [
                "Nouha Dziri",
                "Ehsan Kamalloo",
                "Sivan Milton",
                "Osmar Zaiane",
                "Mo Yu",
                "Edoardo M. Ponti",
                "Siva Reddy"
            ],
            "title": "FaithDial: A faithful benchmark for information-seeking dialogue. Transactions of the Association for Computational Linguistics, 10:1473\u20131490, 2022a. doi: 10.1162/tacl a 00529",
            "venue": "URL https://aclanthology.org/2022.tacl-1.84",
            "year": 2022
        },
        {
            "authors": [
                "Nouha Dziri",
                "Sivan Milton",
                "Mo Yu",
                "Osmar Zaiane",
                "Siva Reddy"
            ],
            "title": "On the origin of hallucinations in conversational models: Is it the datasets or the models",
            "venue": "In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language",
            "year": 2022
        },
        {
            "authors": [
                "Yihao Feng",
                "Shentao Yang",
                "Shujian Zhang",
                "Jianguo Zhang",
                "Caiming Xiong",
                "Mingyuan Zhou",
                "Huan Wang"
            ],
            "title": "Fantastic rewards and how to tame them: A case study on reward learning for taskoriented dialogue systems",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Nicholas Joseph",
                "Sam McCandlish",
                "Chris Olah",
                "Jared Kaplan",
                "Jack Clark"
            ],
            "title": "Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Xiang Gao",
                "Yizhe Zhang",
                "Michel Galley",
                "Chris Brockett",
                "Bill Dolan"
            ],
            "title": "Dialogue response ranking training with large-scale human feedback data",
            "venue": "In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 386\u2013395,",
            "year": 2020
        },
        {
            "authors": [
                "Sayan Ghosh",
                "Zheng Qi",
                "Snigdha Chaturvedi",
                "Shashank Srivastava"
            ],
            "title": "How helpful is inverse reinforcement learning for table-to-text generation? In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)",
            "venue": "pp. 71\u201379,",
            "year": 2021
        },
        {
            "authors": [
                "Dongyoung Go",
                "Tomasz Korbak",
                "Germ\u00e1n Kruszewski",
                "Jos Rozen",
                "Nahyeon Ryu",
                "Marc Dymetman"
            ],
            "title": "Aligning language models with preferences through f-divergence minimization",
            "venue": "In International Conference on Machine Learning (ICML),",
            "year": 2023
        },
        {
            "authors": [
                "Max Grusky",
                "Mor Naaman",
                "Yoav Artzi"
            ],
            "title": "Newsroom: A dataset of 1.3 million summaries with diverse extractive strategies",
            "venue": "In Proceedings of the 2018 Conference of the North American Chapter of the Association",
            "year": 2018
        },
        {
            "authors": [
                "Han Guo",
                "Bowen Tan",
                "Zhengzhong Liu",
                "Eric Xing",
                "Zhiting Hu"
            ],
            "title": "Efficient (soft) Q-learning for text generation with limited good data",
            "venue": "In Findings of the Association for Computational Linguistics: EMNLP 2022,",
            "year": 2022
        },
        {
            "authors": [
                "Abigail Z. Jacobs",
                "Hanna Wallach"
            ],
            "title": "Measurement and fairness",
            "venue": "In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency,",
            "year": 2021
        },
        {
            "authors": [
                "Youngsoo Jang",
                "Jongmin Lee",
                "Kee-Eung Kim"
            ],
            "title": "Gpt-critic: Offline reinforcement learning for endto-end task-oriented dialogue systems",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Marcin Junczys-Dowmunt",
                "Roman Grundkiewicz",
                "Shubha Guha",
                "Kenneth Heafield"
            ],
            "title": "Approaching neural grammatical error correction as a low-resource machine translation task. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
            "year": 2018
        },
        {
            "authors": [
                "Minbeom Kim",
                "Hwanhee Lee",
                "Kang Min Yoo",
                "Joonsuk Park",
                "Hwaran Lee",
                "Kyomin Jung"
            ],
            "title": "Critic-guided decoding for controlled text generation, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Aviral Kumar",
                "Joey Hong",
                "Anikait Singh",
                "Sergey Levine"
            ],
            "title": "Should i run offline reinforcement learning or behavioral cloning",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Jiwei Li",
                "Michel Galley",
                "Chris Brockett",
                "Jianfeng Gao",
                "Bill Dolan"
            ],
            "title": "A diversity-promoting objective function for neural conversation models",
            "venue": "In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
            "year": 2016
        },
        {
            "authors": [
                "Ruibo Liu",
                "Ge Zhang",
                "Xinyu Feng",
                "Soroush Vosoughi"
            ],
            "title": "Aligning generative language models with human values. In Findings of the Association for Computational Linguistics: NAACL 2022",
            "year": 2022
        },
        {
            "authors": [
                "Tianqi Liu",
                "Yao Zhao",
                "Rishabh Joshi",
                "Misha Khalman",
                "Mohammad Saleh",
                "Peter J. Liu",
                "Jialu Liu"
            ],
            "title": "Statistical rejection sampling improves preference optimization, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Ximing Lu",
                "Sean Welleck",
                "Jack Hessel",
                "Liwei Jiang",
                "Lianhui Qin",
                "Peter West",
                "Prithviraj Ammanabrolu",
                "Yejin Choi"
            ],
            "title": "Quark: Controllable text generation with reinforced unlearning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Mohammad Norouzi",
                "Samy Bengio",
                "zhifeng Chen",
                "Navdeep Jaitly",
                "Mike Schuster",
                "Yonghui Wu",
                "Dale Schuurmans"
            ],
            "title": "Reward augmented maximum likelihood for neural structured prediction",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2016
        },
        {
            "authors": [
                "Richard Yuanzhe Pang",
                "He He"
            ],
            "title": "Text generation by learning from demonstrations",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Richard Yuanzhe Pang",
                "Vishakh Padmakumar",
                "Thibault Sellam",
                "Ankur P. Parikh",
                "He He"
            ],
            "title": "Reward gaming in conditional text generation, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Baolin Peng",
                "Linfeng Song",
                "Ye Tian",
                "Lifeng Jin",
                "Haitao Mi",
                "Dong Yu"
            ],
            "title": "Stabilizing rlhf through advantage model and selective rehearsal, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Xue Bin Peng",
                "Aviral Kumar",
                "Grace Zhang",
                "Sergey Levine"
            ],
            "title": "Advantage-weighted regression: Simple and scalable off-policy reinforcement learning, 2021",
            "venue": "URL https://openreview.net/ forum?id=ToWi1RjuEr8",
            "year": 2021
        },
        {
            "authors": [
                "Barbara Plank"
            ],
            "title": "The \u2019problem\u2019 of human label variation: On ground truth in data, modeling and evaluation",
            "venue": "In EMNLP,",
            "year": 2022
        },
        {
            "authors": [
                "Alec Radford",
                "Jeff Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "venue": "In Arxiv,",
            "year": 2019
        },
        {
            "authors": [
                "Rafael Rafailov",
                "Archit Sharma",
                "Eric Mitchell",
                "Christopher D Manning",
                "Stefano Ermon",
                "Chelsea Finn"
            ],
            "title": "Direct preference optimization: Your language model is secretly a reward model",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2023
        },
        {
            "authors": [
                "Kazuma Hashimoto",
                "Caiming Xiong"
            ],
            "title": "CASPI] causal-aware safe policy improvement for task-oriented dialogue. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
            "year": 2022
        },
        {
            "authors": [
                "Rajkumar Ramamurthy",
                "Prithviraj Ammanabrolu",
                "Kiant\u00e9 Brantley",
                "Jack Hessel",
                "Rafet Sifa",
                "Christian Bauckhage",
                "Hannaneh Hajishirzi",
                "Yejin Choi"
            ],
            "title": "Is reinforcement learning (not) for natural language processing: Benchmarks, baselines, and building blocks for natural language policy optimization",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Mark B. Ring"
            ],
            "title": "Child: A first step towards continual learning",
            "venue": "Mach. Learn.,",
            "year": 1997
        },
        {
            "authors": [
                "John Schulman",
                "Philipp Moritz",
                "Sergey Levine",
                "Michael I. Jordan",
                "Pieter Abbeel"
            ],
            "title": "Highdimensional continuous control using generalized advantage estimation",
            "venue": "In Yoshua Bengio and Yann LeCun (eds.), 4th International Conference on Learning Representations,",
            "year": 2016
        },
        {
            "authors": [
                "John Schulman",
                "Filip Wolski",
                "Prafulla Dhariwal",
                "Alec Radford",
                "Oleg Klimov"
            ],
            "title": "Proximal policy optimization algorithms, 2017",
            "year": 2017
        },
        {
            "authors": [
                "Abigail See",
                "Stephen Roller",
                "Douwe Kiela",
                "Jason Weston"
            ],
            "title": "What makes a good conversation? how controllable attributes affect human judgments",
            "year": 2019
        },
        {
            "authors": [
                "Zhan Shi",
                "Xinchi Chen",
                "Xipeng Qiu",
                "Xuanjing Huang"
            ],
            "title": "Toward diverse text generation with inverse reinforcement learning",
            "venue": "In Proceedings of the 27th International Joint Conference on Artificial Intelligence,",
            "year": 2018
        },
        {
            "authors": [
                "Ilia Shumailov",
                "Zakhar Shumaylov",
                "Yiren Zhao",
                "Yarin Gal",
                "Nicolas Papernot",
                "Ross Anderson"
            ],
            "title": "The curse of recursion: Training on generated data makes models forget, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Joar Max Viktor Skalse",
                "Nikolaus H.R. Howe",
                "Dmitrii Krasheninnikov",
                "David Krueger"
            ],
            "title": "Defining and characterizing reward gaming",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Charlie Snell",
                "Ilya Kostrikov",
                "Yi Su",
                "Mengjiao Yang",
                "Sergey Levine"
            ],
            "title": "Offline rl for natural language generation with implicit language q learning",
            "venue": "International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Feifan Song",
                "Bowen Yu",
                "Minghao Li",
                "Haiyang Yu",
                "Fei Huang",
                "Yongbin Li",
                "Houfeng Wang"
            ],
            "title": "Preference ranking optimization for human alignment",
            "venue": "In The 38th Annual AAAI Conference on Artificial Intelligence,",
            "year": 2024
        },
        {
            "authors": [
                "Ziang Song",
                "Tianle Cai",
                "Jason D. Lee",
                "Weijie J. Su"
            ],
            "title": "Reward collapse in aligning large language models, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Nisan Stiennon",
                "Long Ouyang",
                "Jeffrey Wu",
                "Daniel Ziegler",
                "Ryan Lowe",
                "Chelsea Voss",
                "Alec Radford",
                "Dario Amodei",
                "Paul F Christiano"
            ],
            "title": "Learning to summarize with human feedback",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Emma Strubell",
                "Ananya Ganesh",
                "Andrew McCallum"
            ],
            "title": "Energy and policy considerations for deep learning in NLP",
            "venue": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,",
            "year": 2019
        },
        {
            "authors": [
                "Hugo Touvron",
                "Thibaut Lavril",
                "Gautier Izacard",
                "Xavier Martinet",
                "Marie-Anne Lachaux",
                "Timoth\u00e9e Lacroix",
                "Baptiste Rozi\u00e8re",
                "Naman Goyal",
                "Eric Hambro",
                "Faisal Azhar",
                "Aurelien Rodriguez",
                "Armand Joulin",
                "Edouard Grave",
                "Guillaume Lample"
            ],
            "title": "Llama: Open and efficient foundation language models, 2023a",
            "year": 2023
        },
        {
            "authors": [
                "Zhang",
                "Angela Fan",
                "Melanie Kambadur",
                "Sharan Narang",
                "Aurelien Rodriguez",
                "Robert Stojnic",
                "Sergey Edunov",
                "Thomas Scialom"
            ],
            "title": "Llama 2: Open foundation and fine-tuned chat models, 2023b",
            "year": 2023
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141 ukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Siddharth Verma",
                "Justin Fu",
                "Sherry Yang",
                "Sergey Levine"
            ],
            "title": "CHAI: A CHatbot AI for task-oriented dialogue with offline reinforcement learning",
            "venue": "In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
            "year": 2022
        },
        {
            "authors": [
                "Leandro von Werra",
                "Younes Belkada",
                "Lewis Tunstall",
                "Edward Beeching",
                "Tristan Thrush",
                "Nathan Lambert",
                "Shengyi Huang"
            ],
            "title": "Trl: Transformer reinforcement learning",
            "venue": "https://github.com/ huggingface/trl,",
            "year": 2020
        },
        {
            "authors": [
                "Peiyi Wang",
                "Lei Li",
                "Liang Chen",
                "Zefan Cai",
                "Dawei Zhu",
                "Binghuai Lin",
                "Yunbo Cao",
                "Qi Liu",
                "Tianyu Liu",
                "Zhifang Sui"
            ],
            "title": "Large language models are not fair evaluators, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Ziyu Wang",
                "Alexander Novikov",
                "Konrad Zolna",
                "Josh S Merel",
                "Jost Tobias Springenberg",
                "Scott E Reed",
                "Bobak Shahriari",
                "Noah Siegel",
                "Caglar Gulcehre",
                "Nicolas Heess",
                "Nando de Freitas"
            ],
            "title": "Critic regularized regression",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "brian ichter",
                "Fei Xia",
                "Ed H. Chi",
                "Quoc V Le",
                "Denny Zhou"
            ],
            "title": "Chain of thought prompting elicits reasoning in large language models",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Sean Welleck",
                "Ximing Lu",
                "Peter West",
                "Faeze Brahman",
                "Tianxiao Shen",
                "Daniel Khashabi",
                "Yejin Choi"
            ],
            "title": "Generating sequences by learning to self-correct, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Lilian Weng"
            ],
            "title": "Policy gradient algorithms. lilianweng.github.io, 2018",
            "venue": "URL https://lilianweng",
            "year": 2018
        },
        {
            "authors": [
                "Peter West",
                "Chandra Bhagavatula",
                "Jack Hessel",
                "Jena Hwang",
                "Liwei Jiang",
                "Ronan Le Bras",
                "Ximing Lu",
                "Sean Welleck",
                "Yejin Choi"
            ],
            "title": "Symbolic knowledge distillation: from general language models to commonsense models",
            "venue": "In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,",
            "year": 2022
        },
        {
            "authors": [
                "Thomas Wolf",
                "Lysandre Debut",
                "Victor Sanh",
                "Julien Chaumond",
                "Clement Delangue",
                "Anthony Moi",
                "Pierric Cistac",
                "Tim Rault",
                "Remi Louf",
                "Morgan Funtowicz",
                "Joe Davison",
                "Sam Shleifer",
                "Patrick von Platen",
                "Clara Ma",
                "Yacine Jernite",
                "Julien Plu",
                "Canwen Xu",
                "Teven Le Scao",
                "Sylvain Gugger",
                "Mariama Drame",
                "Quentin Lhoest",
                "Alexander Rush"
            ],
            "title": "Transformers: State-of-the-art natural language processing",
            "venue": "Proceedings of the 2020 Conference on Empirical Methods",
            "year": 2020
        },
        {
            "authors": [
                "Tianhao Wu",
                "Banghua Zhu",
                "Ruoyu Zhang",
                "Zhaojin Wen",
                "Kannan Ramchandran",
                "Jiantao Jiao"
            ],
            "title": "Pairwise proximal policy optimization: Harnessing relative feedback for llm alignment, 2023a",
            "year": 2023
        },
        {
            "authors": [
                "Zeqiu Wu",
                "Yushi Hu",
                "Weijia Shi",
                "Nouha Dziri",
                "Alane Suhr",
                "Prithviraj Ammanabrolu",
                "Noah A Smith",
                "Mari Ostendorf",
                "Hannaneh Hajishirzi"
            ],
            "title": "Fine-grained human feedback gives better rewards for language model training",
            "venue": "arXiv preprint arXiv:2306.01693,",
            "year": 2023
        },
        {
            "authors": [
                "Kevin Yang",
                "Dan Klein",
                "Asli Celikyilmaz",
                "Nanyun Peng",
                "Yuandong Tian"
            ],
            "title": "Rlcd: Reinforcement learning from contrast distillation for language model alignment, 2023a",
            "year": 2023
        },
        {
            "authors": [
                "Shentao Yang",
                "Shujian Zhang",
                "Congying Xia",
                "Yihao Feng",
                "Caiming Xiong",
                "Mingyuan Zhou"
            ],
            "title": "Preference-grounded token-level guidance for language model fine-tuning, 2023b",
            "year": 2023
        },
        {
            "authors": [
                "Zheng Yuan",
                "Hongyi Yuan",
                "Chuanqi Tan",
                "Wei Wang",
                "Songfang Huang",
                "Fei Huang"
            ],
            "title": "Rrhf: Rank responses to align language models with human feedback without tears, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Yizhe Zhang",
                "Siqi Sun",
                "Michel Galley",
                "Yen-Chun Chen",
                "Chris Brockett",
                "Xiang Gao",
                "Jianfeng Gao",
                "Jingjing Liu",
                "Bill Dolan"
            ],
            "title": "DIALOGPT : Large-scale generative pre-training for conversational response generation",
            "venue": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations,",
            "year": 2020
        },
        {
            "authors": [
                "Yao Zhao",
                "Rishabh Joshi",
                "Tianqi Liu",
                "Misha Khalman",
                "Mohammad Saleh",
                "Peter J. Liu"
            ],
            "title": "Slic-hf: Sequence likelihood calibration with human feedback, 2023a",
            "year": 2023
        },
        {
            "authors": [
                "Yao Zhao",
                "Mikhail Khalman",
                "Rishabh Joshi",
                "Shashi Narayan",
                "Mohammad Saleh",
                "Peter J Liu"
            ],
            "title": "Calibrating sequence likelihood improves conditional language generation",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Lianmin Zheng",
                "Wei-Lin Chiang",
                "Ying Sheng",
                "Siyuan Zhuang",
                "Zhanghao Wu",
                "Yonghao Zhuang",
                "Zi Lin",
                "Zhuohan Li",
                "Dacheng Li",
                "Eric. P Xing",
                "Hao Zhang",
                "Joseph E. Gonzalez",
                "Ion Stoica"
            ],
            "title": "Judging llm-as-a-judge with mt-bench and chatbot arena, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Banghua Zhu",
                "Hiteshi Sharma",
                "Felipe Vieira Frujeri",
                "Shi Dong",
                "Chenguang Zhu",
                "Michael I. Jordan",
                "Jiantao Jiao"
            ],
            "title": "Fine-tuning language models with advantage-induced policy alignment, 2023",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Pretrained (Radford et al., 2019; Brown et al., 2020) and/or instruction-tuned (Wei et al., 2022a; Chung et al., 2022; Wei et al., 2022b) large Language Models (LMs) show huge improvements in quality and safety when finetuned with Reinforcement Learning with Human Feedback (RLHF) (Stiennon et al., 2020; Ouyang et al., 2022; Touvron et al., 2023b). However, the most popular RLHF method, Proximal Policy Optimization (PPO) (Schulman et al., 2017), is sensitive to hyperparameters and suffers from training instability (Yuan et al., 2023; Casper et al., 2023). More importantly, PPO periodically requires new batches of LM-generated data for each training step which leads to additional computational overhead and risk of mode collapse (Song et al., 2023; Shumailov et al., 2023; Go et al., 2023). Given these limitations, we ask: Can we perform rewarded learning, similar to PPO, while exclusively using pre-existing language data during training? We propose Advantage-Leftover Lunch RL (A-LOL), a set of sample-efficient and stable learning algorithms that uses Offline Policy Gradients (Degris et al., 2012; Weng, 2018) to optimize LMs towards any desired rewards using only pre-collected language data. Notably in A-LOL, we assume the entire output sequence as a single action step, which allows it to calculate training data advantage and filter unfavorable instances. The advantage is the reference LM\u2019s value estimate subtracted from the reward, which determines the benefit of each training instance toward the learning process. Subsequently, discarding the data points with negative advantages improves the learning efficiency of A-LOL and makes it robust to noisy data. A-LOL is very easy to implement over standard cross entropy loss using two key improvements: (1) sequence-level advantage and (2) importance weight (ratio of target LM\u2019s and initial reference LM probabilities). As illustrated in Table 1, our method only requires a sequence-level reward with single output for every data point, in contrast to recent preference-based (Rafailov et al., 2023; Song et al., 2024) offline RL methods that require human-labeled pairwise comparisons. Importantly, A-LOL\nand its variants share most similarities with PPO, while greatly simplifying the training and also enabling offline learning. Through a series of four different language generation tasks, each using one or more classifiers to calculate the reward, we show that A-LOL consistently outperforms the baselines while using the least amount of training data. We first experiment with the RLHF benchmark task, Helpful and Harmless Assistant (HHA) (Bai et al., 2022a; Ganguli et al., 2022) (\u00a74), where both human-labeled preference data and reward model are available. We systematically compare all offline RL algorithms using the same 7B base model architecture and show training stability trends over multiple random seeds. We find that A-LOL variants achieve comparable average reward to DPO while offering more stable learning, lower variance, and higher response diversity than every other baseline. In a more qualitative evaluation, humans judge the A-LOL models to be the most helpful and safe. In another single-reward experiment with the Commonsense Reasoning task (West et al., 2022) (Appendix \u00a7C.1), A-LOL again showed the highest improvement in quality among the baselines. We also demonstrate A-LOL\u2019s flexibility to utilize multiple rewards in RL training, which contrasts with preference-based methods that can only support unidimensional preferences. In particular, we experiment with two multi-reward dialog tasks, Reddit response generation (\u00a75), and Faithful knowledge-grounded dialog (Dinan et al., 2019) (Appendix \u00a7C.2). In both tasks, A-LOL was able to simultaneously optimize four or more different reward functions that improved fluency, safety, diversity, and other qualitative attributes of the LMs, even in the presence of noisy training data. Our findings demonstrate that A-LOL is a robust, stable, sample-efficient offline RL method for language model learning that can be easily substituted with cross-entropy loss in tasks where real-value rewards are available. We release the code at https://github.com/abaheti95/LoL-RL."
        },
        {
            "heading": "2 ADVANTAGE-LEFTOVER LUNCH RL",
            "text": "Before introducing our main method, we first briefly explain how we frame language generation tasks as an RL game with the single-action assumption (\u00a72.1). We then derive the main learning objective of A-LOL using offline policy gradient (\u00a72.2). To better contextualize A-LOL, we also discuss its relationship with negative log-likelihood loss, weighted Behavior Cloning (Wang et al., 2020) (\u00a72.3) and another offline policy gradient algorithm GOLD (Pang & He, 2021) (\u00a72.4).2"
        },
        {
            "heading": "2.1 LANGUAGE TASKS AS RL WITH SINGLE ACTION EPISODES",
            "text": "We consider language generation as a sequence-to-sequence task containing training Dtr and validation Dv sets with pairs of input x and output y sequences. Contrasting with previous RL methods that\n1We do not compare with online RL methods (Shi et al., 2018; Liu et al., 2022; Yang et al., 2023a; Peng et al., 2023; Zhu et al., 2023) similar to PPO and preference-based methods (Zhao et al., 2023a; Wu et al., 2023a) similar to DPO. We exclude RL methods that require additional data manipulation (Lu et al., 2022; Welleck et al., 2022; Jang et al., 2022; Verma et al., 2022; Guo et al., 2022; Cho et al., 2023; Liu et al., 2023; Zhao et al., 2023b; Chang et al., 2023) or model architecture changes (Peng et al., 2021; Kim et al., 2022; Snell et al., 2023).\n2We also discuss A-LOL\u2019s connection with PPO (Schulman et al., 2017) in Appendix \u00a7A.\nconsider each token in y as a separate action (Pang & He, 2021; Kim et al., 2022; Snell et al., 2023)3, we consider the entire y as a single action from the LM agent, after which the agent receives the task-specific sequence-level reward R(x, y, \u22c6) and the episode ends. The single-action assumption allows incorporating any pretrained attribute-specific classifiers or human-designed scoring functions as a reward during offline finetuning. When multiple scoring functions are available, we set the reward as the sum of all individual functions."
        },
        {
            "heading": "2.2 OFFLINE POLICY GRADIENT TO ADVANTAGE LOL RL",
            "text": "To derive our main learning equation, we start with the off-policy policy gradient objective (Degris et al., 2012; Weng, 2018). Let \u03c0ref be the reference policy LM trained on Dtr with standard negative likelihood loss (NLL) and \u03c0\u03b8 be the target policy we want to optimize, which is initially identical to \u03c0ref. Both \u03c0ref and \u03c0\u03b8 take the input sequence x (state) and generate an output sequence y (action). Using the single action episode assumption, we can write the stationary distribution of reference policy as d\u03c0ref(x) = P (x|\u03c0ref) = P (x), where x belongs to the set of all input sequences in Dtr. We can then optimize target policy \u03c0\u03b8 on this stationary distribution d\u03c0ref with the following objective:\nJ(\u03b8) = max \u03b8 \u2211 x\u2208X d\u03c0ref(x) \u2211 y\u2208Y R(x, y, \u22c6)\u03c0\u03b8(y|x) (1)\nwhere Y is the set of all outputs. Taking a derivative of the above equation with respect to \u03b8 yields: \u2207\u03b8J(\u03b8) = \u2207\u03b8Ex\u223cd\u03c0ref [ \u2211 y\u2208Y R(x, y, \u22c6)\u03c0\u03b8(y|x)] = Ex\u223cd\u03c0ref [ \u2211 y\u2208Y R(x, y, \u22c6)\u2207\u03b8\u03c0\u03b8(y|x)] (2)\nWe then multiply and divide by \u03c0\u03b8(y|x) and \u03c0ref(y|x) and further simplify the equation as follows,\n\u2207\u03b8J(\u03b8) = Ex\u223cd\u03c0ref ,y\u223c\u03c0ref [R(x, y, \u22c6)\ufe38 \ufe37\ufe37 \ufe38 reward \u03c0\u03b8(y|x) \u03c0ref(y|x)\ufe38 \ufe37\ufe37 \ufe38\nimportance weight\n\u2207\u03b8 ln\u03c0\u03b8(y|x)\ufe38 \ufe37\ufe37 \ufe38 NLL ] (3)\nHere, the importance weight4 is the ratio of sequence-level probability of y between \u03c0\u03b8 and \u03c0ref, which results into a single scalar factor. Observe that the inputs of Dtr are in d\u03c0ref . Also, the ground truth outputs in Dtr are the outputs \u03c0ref is trained to imitate. Using these observations, we approximate the expectation in the previous equation and obtain the Reward LOL RL objective (with a negative sign to show minimization):\n\u2207\u03b8JR-LOL(\u03b8) = \u2212EDtr [R(x, y, \u22c6) \u00b7 r(\u03b8, ref) \u00b7 \u2207\u03b8 ln\u03c0\u03b8(y|x)] (4) 3While some on-policy RLHF instantiations use sequence as actions Stiennon et al. (2020); Ouyang et al. (2022); Ahmadian et al. (2024), most standardized implementations of PPO use per-token action assumption von Werra et al. (2020); Castricato et al. (2023); Ramamurthy et al. (2023).\n4http://timvieira.github.io/blog/post/2014/12/21/importance-sampling/\nwhere r(\u03b8, ref) = \u03c0\u03b8(y|x)\u03c0ref(y|x) is the shorthand for importance weight.\nFor boosting learning efficiency, we can replace R(x, y, \u22c6) in equation 4 with advantage, defined as A\u03c0\u03b8 (x, y, R) = R(x, y, \u22c6) \u2212 V\u03c0\u03b8 (x), i.e., the policy\u2019s estimate of expected reward for the input subtracted from the actual reward of the training data (Schulman et al., 2016). However, maintaining the most recent value estimate of \u03c0\u03b8 is cost-intensive, as it is constantly updated during training. Therefore, we swap the reward in equation 4 with the advantage of the frozen reference policy, A\u03c0ref(x, y, R) = R(x, y, \u22c6)\u2212 V\u03c0ref(x). We call this the Advantage LOL RL objective.\n\u2207\u03b8JA-LOL(\u03b8) = \u2212EDtr [A\u03c0ref(x, y, R) \u00b7 r(\u03b8, ref) \u00b7 \u2207\u03b8 ln\u03c0\u03b8(y|x)] (5) To compute \u03c0ref\u2019s value estimate, we initialize a small network of multi-head attention (Vaswani et al., 2017) and a single-layer MLP on top of frozen parameters of \u03c0ref. This value estimate module takes the last hidden layer representation of \u03c0ref(x) and predicts expected future reward V\u03c0ref(x). We cheaply train this value estimate on the rewards achieved by \u03c0ref on the validation set (Dv) with mean squared error loss. We then calculate the A\u03c0ref(x, y, R) for all instances in Dtr. Figure 1 illustrates an example of how A-LOL improves the distribution of test rewards by using the value estimate of the reference policy. Next, we describe several other variants of A-LOL algorithm.\nVariants with alternative Importance Weight Exploiting the flexibility offered by importance weight in A-LOL, we experiment with three alternatives. First, we create A-LOL (ref. free) by setting the importance weight to 1. In the second variant, we convert the full-sequence importance weight in A-LOL (equation 5) to a per-token importance weight. Specifically, we propose an approximate importance weight multiplied with log-likelihood using the probability chain rule as follows, \u03c0\u03b8(y|x)\u03c0ref(y|x)\u2207\u03b8 ln\u03c0\u03b8(y|x) \u2248 \u2211|y| i=1[ \u03c0\u03b8(yi|x,y<i) \u03c0ref(yi|x,y<i)\u2207\u03b8 ln\u03c0\u03b8(yi|x, y<i)], where yi is the i\nth token in y and y<i are the preceding tokens.5 We name this variant A-LOL sequence. Finally, inspired by PPO\u2019s ablations (Schulman et al., 2017), we experiment with replacing the importance weight with a weighted KL penalty to obtain A-LOL KL:\n\u2207\u03b8JA-LOL KL(\u03b8) = \u2212EDtr [ A\u03c0ref(x, y, R) \u00b7 \u2207\u03b8 ln\u03c0\u03b8(y|x)\u2212 \u03b2 \u00b7 \u2207\u03b8 ln\n\u03c0\u03b8(y|x) \u03c0ref(y|x)\n] (6)\nWe propose two more modifications in A-LOL training to improve its stability and efficiency.\nClipping Importance Weight Direct usage of A-LOL objective (Equation 5) in training is unstable as loss values can fluctuate hugely depending on the importance weight r(\u03b8, ref). To mitigate this issue, we clip the importance weight as clip(r(\u03b8, ref), 1 \u2212 \u03f5, 1 + \u03f5) (Schulman et al., 2017). This clip operator discourages big changes from reference policy. In A-LOL sequence, we apply the clip operator separately to the importance weight of every token in the output.\nReward/Advantage Priority Sampling In all the experiments, we find that a non-trivial amount of data points in Dtr obtain a negative advantage (A\u03c0ref < 0). We discard these data points as they may not help in generalizing beyond \u03c0ref. To boost the training efficiency of A-LOL even more, we employ positive advantage-based weighted sampling of train instances (similar to Welleck et al., 2022). We present the full pseudo code for A-LOL in Algorithm 1. For reward-based offline RL methods, we similarly employ reward-based priority sampling in all the experiments. Overall, A-LOL and its variants are efficient and easy to implement on top of standard negative log-likelihood as it only involves multiplying two factors: advantage/reward, and importance weight. Furthermore, the positive-advantage priority sampling makes A-LOL\u2019s training very efficient, sometimes reaching close to peak generalization with only 30% additional steps (see Figure 2)."
        },
        {
            "heading": "2.3 RELATIONSHIP WITH NLL AND WEIGHTED BEHAVIOR CLONING",
            "text": "We draw connections between Reward LOL RL and other learning methods. If we set both R(x, y, \u22c6) = 1 and r(\u03b8, ref) = 1 in the equation 4, it reduces to negative log-likelihood objective. This implies that maximum likelihood learning is a subset of R-LOL\u2019s objective. By carefully adjusting the R(x, y, \u22c6) term while keeping r(\u03b8, ref) = 1, both data filtering (West et al., 2022) and weighted behavior cloning6 (Wang et al., 2020) can also be viewed as subsets of our method.\n5Per-token importance weight of A-LOL seq. can be compared to per-token PPO\u2019s clip term Schulman et al. (2017); Ramamurthy et al. (2023). But, A-LOL seq.\u2019s advantage is flat for every token in the output.\n6Weighted behavior cloning (wBC) simply multiplies the NLL objective with the reward R(x, y, \u22c6). wBC has been used in many language applications including summarization Yang et al. (2023b), task-oriented dialog\nAlgorithm 1: Advantage-Leftover Lunch RL pseudo code Data: train and validation set (x, y \u2208 Dtr, Dv), reference policy (\u03c0ref), target policy (\u03c0\u03b8), task\nreward (R(x, y, \u22c6)), clipping parameter (\u03f5) Result: argmax\u03c0\u03b8\u223cA-LOL \u2211 x\u2208Dv,y\u2032\u223c\u03c0\u03b8 R(x, y\n\u2032, \u22c6) \u25b7 Maximize reward on Dv 1 V \u2190 mlp(mha(\u03c0ref), 1) \u25b7 value layer with multi-head attention (mha) on frozen \u03c0ref 2 V\u03c0ref \u2190 minx\u2208Dv,y\u2032\u223c\u03c0ref(V \u2212R(x, y\u2032, \u22c6))2 \u25b7 Train \u03c0ref value estimate using rewards on Dv 3 A+\u03c0ref \u2190 {A\u03c0ref(x, y, R)} \u2200x, y \u2208 Dtr, R(x, y, \u22c6)\u2212 V\u03c0ref(x) > 0 \u25b7 Positive Advantage on Dtr\n4 while (\u2211\nx\u2208Dv,y\u2032\u223c\u03c0\u03b8 R(x, y \u2032, \u22c6)\n) not converges do\n5 r(\u03b8, ref)\u2190 clip( \u03c0\u03b8(y|x)\u03c0ref(y|x) , 1\u2212 \u03f5, 1 + \u03f5) 6 \u2207\u03b8J(\u03b8)\u2190 \u2212EA+\u03c0ref [A\u03c0ref(x, y, R) \u00b7 r(\u03b8, ref) \u00b7 \u2207\u03b8 ln\u03c0\u03b8(y|x)] \u25b7 Sample using A + \u03c0ref weights\nend"
        },
        {
            "heading": "2.4 COMPARISON WITH GOLD",
            "text": "Previously, Pang & He (2021) developed the GOLD algorithm using a similar offline policy gradient derivation, but without the single-action approximation. Compared to R-LOL objective (equation 4), GOLD objective has two peculiar differences: (1) it approximates the importance weight by using a constant instead of reference policy probability, and (2) it uses reference policy\u2019s per-token log-probability as token-level reward. Intuitively, this method \u201cencourages the learning algorithm to focus on easy examples (high likelihood under the model)\u201d (Pang & He, 2021). However, it cannot trivially include arbitrary sparse-reward like R-LOL. For comparison, we use the single-action assumption and replace the per-token reward with a sequence-level reward to get the Reward GOLD objective, \u2212EDtr [R(x, y, \u22c6)\u03c0\u03b8(y|x)\u2207\u03b8 ln\u03c0\u03b8(y|x)], where we approximate its importance weight and NLL as \u2211|y| i=1 max(\u03c0\u03b8(yi|x, y<i), u)\u2207\u03b8 ln\u03c0\u03b8(yi|x, y<i) with lower bound u for stability."
        },
        {
            "heading": "3 EXPERIMENTAL SETUP AND BASELINES",
            "text": "We conduct experiments with four different language generation tasks: two single-reward tasks (Helpful and Harmless Assistant, Section \u00a74 and Commonsense Reasoning, Appendix \u00a7C.1) and two multiple-rewards tasks (Reddit response generation, Section \u00a75 and Knowledge Grounded Dialog, Appendix \u00a7C.2). In each experiment, a reference LM is obtained, which acts as the starting point for all learning methods. We continue finetuning with different methods for a roughly equal number of steps (depending on the Dtr size). Overall, we compare A-LOL and its modified importance weight variants (A-LOL (ref. free), A-LOL seq., and A-LOL KL) against negative log-likelihood (NLL) and the following offline RL baselines:\nPreference-based Baselines We experiment with three offline RL algorithms that directly use the human-labeled preference data to solve the RLHF task. DPO (Rafailov et al., 2023) converts the constrained reward optimization into a preference classification loss by defining a surrogate reward as the ratio of target policy and reference policy log probabilities. For ablation purposes, we also test DPO (ref. free), a variant of DPO without the reference policy log probabilities. Subsequent work introduced PRO (Song et al., 2024) that extends DPO\u2019s classification loss into a ranking loss and interpolates it with the negative log-likelihood for stability. We cannot compare with preference-based methods in tasks with multiple rewards or where human-labeled preferences are unavailable.\nReward-based Baselines We also compare with R-LOL (Equation 4) and other related rewardbased offline RL methods: wBC (\u00a72.3) and Reward GOLD (\u00a72.4)."
        },
        {
            "heading": "4 HHA: HELPFUL AND HARMLESS ASSISTANT TASK",
            "text": "Our main experiment uses the Helpful and Harmless assistant dataset (Bai et al., 2022a; Ganguli et al., 2022) containing 170K instances of user-assistant conversations each containing a pair of modelgenerated responses. The final responses are labeled good and bad respectively to indicate human preference labels. The dataset comprises four subsets: Harmlessbase containing red-teaming conversations which attempt to illicit harmful responses, while the other three, Helpfulbase, Helpfulonline, and\nRamachandran et al. (2022); Feng et al. (2023), machine translation Norouzi et al. (2016), table-to-text Ghosh et al. (2021) and grammar correction Junczys-Dowmunt et al. (2018).\nHelpfulrejection, contain advice and assistance seeking conversations. We reuse the data splits from Song et al. (2024) with minor data cleaning.7 In total, this task has 143K train, 280 validation, and 8.2K test conversations and their human preference labeled responses.\nReference LM, Reward model and Training We choose LLaMA-7B base architecture (Touvron et al., 2023a) with QLoRA adapter (Dettmers et al., 2023) pretrained on HHA dataset8 as the reference policy. We also test PPO9 in this experiment, along with the aforementioned offline RL baselines and A-LOL variants. In total, we executed 36 different training runs for 12 learning methods, each with three random seeds. For all algorithms using rewards, we employ a 1.4B parameter classifier10 trained on human preference labels as the reward model. While preference-based RL methods use all training paired comparisons, other methods only use the good subset responses during training. In fact, A-LOL methods are most data efficient, by ignoring \u2248 33% of good responses that were identified as negative advantage. We roughly allocate one epoch of training steps for every offline RL method, depending on their training data requirements. We train PPO for 2.6\u00d7 the training steps of offline methods (excluding the computation cost of generating online data). Finally, as a benchmark, we also evaluate an external 6B model trained with PPO on the HHA dataset.11 We present the implementation details of all tested methods in Appendix B.1. We conduct additional ablation analysis of algorithmic modifications of A-LOL in Appendix B.3 to B.5."
        },
        {
            "heading": "4.1 HHA RESULTS",
            "text": "Stability Comparison of Offline RL algorithms Figure 2 shows the trajectories of validation reward achieved by all offline RL methods averaged across three random seeds. The left plot shows that preference-based methods, especially DPO and DPO (ref. free), suffer from high variance across random seeds when compared with NLL training. In contrast, the reward-based methods have comparable or even lower variance than NLL (middle). In the right plot, we observe that A-LOL methods also show similar stability as reward-based methods, while achieving higher rewards. Among our advantage-based methods, A-LOL (ref. free) achieves lower validation performance than other variants. A-LOL KL, on the other hand, can become unstable by minimizing the KL penalty term instead of policy-gradient loss, as evidenced by a drop in performance towards the end of training. Comparatively, A-LOL and A-LOL seq. steadily improve throughout the training. We also separately plot the three trajectories of PPO in Appendix Figure 3.\nAutomatic Evaluation and Analysis We employ a larger 6.5B parameter reward model12 to evaluate the best checkpoints from each run on the test set. We also compare the response distribution using average length and diversity measures (Distinct-1, 2, and 3), which are calculated as the ratio of unique unigrams, bigrams, and trigrams (Li et al., 2016). The average evaluation metrics obtained by each method across three random seeds are reported in Table 2. In the first three rows, we establish the test set good and bad responses and the reference policy (\u03c0ref) performance.\n7Filtered \u224820K training instances containing responses that abruptly end in a colon (:). For example, \u201cHere are some options:\u201d. Further removed 343 test instances with overlapping conversation histories.\n8https://huggingface.co/timdettmers/qlora-hh-rlhf-7b 9We use the huggingface TRL (von Werra et al., 2020) implementation of PPO.\n10https://huggingface.co/OpenAssistant/oasst-rm-2.1-pythia-1.4b-epoch-2.5 11https://huggingface.co/reciprocate/ppo hh pythia-6B 12https://huggingface.co/OpenAssistant/oasst-rm-2-pythia-6.9b-epoch-1\nOverall, A-LOL and its variants consistently achieve high average reward with relatively low variance, even after discarding\u224833% of training data points. Further qualitative analysis reveals that the lowest negative advantage instances often indicate bad quality data (Appendix B.4). A-LOL methods perform comparably to DPO and outperform all other preference-based and reward-based baselines while generating the most diverse responses. Interestingly, A-LOL seq. (that uses a per-token importance weight) achieves the best diversity among all models, aligning its distribution most closely with test responses.13 For preference-based baselines, we notice a direct correlation between high variance of validation performance (Fig. 2 left) and high variance of test set average reward and response length. Despite its high average reward, DPO (and DPO ref. free) tends to skew the response distribution to unusually long and less diverse responses. Finally, in experiments with PPO, models tend to generate much shorter responses on average by primarily focusing on safe response generation (Harmlessbase). This highlights that PPO requires a good initial reference policy that generates high-quality exploration data and a well calibrated reward model which is not a limitation of offline RL methods. Evaluations with the external PPO-based models do not show strong performance either.\nGPT-4 and Human Evaluation To further investigate the quality of top-performing methods, we conduct additional GPT-4 (OpenAI, 2023) and human evaluations. Following prior work (Rafailov et al., 2023; Song et al., 2024), we perform pairwise comparisons between best methods and test good responses to determine their helpfulness and safety win-rate. Specifically, for each comparison between two responses, we ask GPT-4 and humans to select from four options (A, B, tie or neither) to indicate the winning response. We ask to pick the safer response in the instances from Harmlessbase and the more helpful response for the other three test segments. In total, we sample 400 instances for GPT-4 evaluation and 200 instances for human evaluation (equal size from 4 test segments). To\n13The original responses in the HHA dataset were generated using a very large 52B parameter LM (Bai et al., 2022b) and thus show high linguistic diversity.\nGPT-4 evaluation safety and helpfulness win-rate vs Test good responses\nTest bad 83 25.3 4.8 56.6 13.3 240 30.4 2.9 65.8 0.8 \u03c0ref (LLaMA-7B) 81 35.8 6.2 53.1 4.9 267 24.3 3.0 71.9 0.7\n+ DPO 83 60.2 3.6 36.1 0.0 260 41.2 3.1 55.0 0.8 + A-LOL 76 68.4 9.2 17.1 5.3 249 53.8 1.2 45.0 0.0 + A-LOL seq. 82 73.2 7.3 17.1 2.4 247 54.7 2.0 42.9 0.4 + A-LOL KL 80 66.2 11.2 21.2 1.2 249 45.4 3.2 51.0 0.4\nHuman evaluation safety and helpfulness win-rate vs Test good responses\n+ DPO 43 53.5 4.7 30.2 11.6 138 47.8 7.2 42.8 2.2 + A-LOL 45 46.7 15.6 24.4 13.3 127 53.5 15.0 30.7 0.8 + A-LOL seq. 49 63.3 14.3 14.3 8.2 134 49.3 9.0 38.1 3.7 + A-LOL KL 43 53.5 9.3 23.3 14.0 137 48.9 11.7 34.3 5.1\nmitigate positional bias in GPT-4 (Zheng et al., 2023; Wang et al., 2023), we query it twice (shuffling the response order) and only aggregate the judgments when it selects the same preference. In human evaluation, we ask three annotators to rate each pairwise comparison and aggregate the judgments if a majority is achieved. The final results from both evaluations are presented in Table 3. To establish GPT-4 evaluation reliability, we first compare the reference policy (\u03c0ref) and Test set bad responses with Test good responses in the first two rows. In both comparisons, GPT-4 considers the Test good response as more helpful and safe in the majority of samples. Overall, A-LOL and A-LOL seq. achieve the highest win rate in both safety and helpfulness (win + tie), with A-LOL KL and DPO trailing behind. Humans select more instances as tie than GPT-4, but we again notice a similar win-rate trend with A-LOL methods leading in both helpfulness and safety. For all the instances in human evaluation with majority label, we compare with their corresponding GPT-4\u2019s preference label and find 72.1% agreement.14 We present a few example conversations from all the top models in Table 9 to 13 in the Appendix."
        },
        {
            "heading": "5 REDDIT RESPONSE GENERATION TASK",
            "text": "Human preference data is supported by very few language generation tasks. Their annotation is also difficult and costly. Furthermore, preferences are inherently unidimensional and cannot be trivially extended to tasks where more than one aspect is important (Rafailov et al., 2023; Song et al., 2024). In contrast, policy-gradient-based methods can utilize multiple reward functions during RL training without the need for preference data. To test the multi-reward generalization of A-LOL, we create a new Reddit response generation task with a mix of five reward functions. The task is to learn a chatbot from Reddit comment-response pairs15 that is fluent, safe, engaging, exciting, and human-like (See et al., 2019). Therefore, we define the task reward as the sum of five scoring functions: (1) CoLA fluency classifier, (2) ToxiChat contextual safety classifier (Baheti et al., 2021), (3) dialog engagement classifier16 (Gao et al., 2020), (4) Reddit upvote probability ranking model (Gao et al., 2020), and (5) length penalized TF-IDF diversity.17 The range of each scoring function is [0, 1]. To test the robustness to noise, we create two different training datasets for this task: (1) 88K Reddit upvoted comment pairs (score \u2208 [66, 9582]), reflective of good quality data, and (2) 87K Reddit downvoted comment pairs (score \u2208 [\u22122946,\u22126]), bad quality data. For both instantiations, we create balanced validation and test sets, each with 1000 upvoted and 1000 downvoted comment pairs. We use DialoGPT-medium (355M parameters) (Zhang et al., 2020) model trained using NLL objective\n14We exclude the instances where GPT-4 preference didn\u2019t match after shuffling the response order. 15https://www.kaggle.com/code/danofer/reddit-comments-scores-nlp/input 16A ranking model that assigns a score \u2208 [0, 1] indicating the probability of getting followup reply. https: //huggingface.co/microsoft/DialogRPT-depth 17We first compute the TF-IDF weights for all words in the training set. Then, the length penalized TF-IDF diversity score is defined as min( |y| 10 , 1) \u00b7 \u2211 w\u2208y TF-IDF(w)\n|y| , where y represents all the words in the response except the stop words.\nfor 6 epochs as the reference policy. We then perform further training for 3 epochs with A-LOL variants and other reward-based offline RL baselines. The average reward, length, and diversity metrics are reported in Table 4.\nResults In both the upvote and downvote training splits, A-LOL variants achieve higher test rewards compared to every other reward-based baseline. They show especially high improvement in safety, engagement, and upvote probability. While A-LOL\u2019s performance is comparable to that of A-LOL (ref. free), the other two variants, with sequence-level importance weight and KL penalty, surpass their performance. Consistent with the results of the previous experiment, we observe that A-LOL sequence (with the per-token importance weight assumption) achieves the highest diversity in both training splits. Experiments with PPO resulted in a policy that generates generic responses, thereby optimizing fluency, safety, and tfidf while ignoring the other two components (more details in Appendix C.3). Surprisingly, the LMs trained on downvoted data with A-LOL almost close the gap with their counterparts trained on upvoted data. Upon closer inspection, we find that about 36% of upvoted replies and 48% of the downvoted replies in their respective training sets received a negative advantage and thus, were never sampled when finetuning with A-LOL. By filtering the unfavorable data points, A-LOL extracts the useful training signal even from suboptimal data. We called our method Leftover Lunch RL precisely because of its robustness to unfavorable training data. We show the per-component reward distribution in Figure 5 in the appendix."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "We introduce Advantage-Leftover Lunch RL, a set of advantage-based offline policy gradient algorithms that are easy to implement on top of standard negative log-likelihood and are more stable than preference-based offline RL and PPO. On four different tasks A-LOL consistently shows similar or better performance than other preference-based and reward-based offline RL methods. Most notably, A-LOL exploits the reference LM\u2019s advantage estimate to discard unfavorable data. This unique ability of A-LOL makes it resilient to noise and allows it to eat the leftover lunch from even the suboptimal training data. Exploiting the flexibility of importance weighting, we create four variants of A-LOL that achieve the top performance in almost every evaluation. Among them, we find that methods using importance weight usually outperform the reference-free variant. In fact, using the per-token importance weight assumption, the variant A-LOL sequence not only improves the test performance but also the diversity."
        },
        {
            "heading": "7 REPRODUCIBILITY STATEMENT",
            "text": "In our main experiments with the HHA task (\u00a74), using the largest 7B parameter LM experiments, we run every baseline and A-LOL methods with three random seeds for reproducibility. We also provide the implementation details of every method along with the hyperparameters in Appendix \u00a7B.1. In other multi-reward experiments, we test all methods and baselines with both good-quality and bad-quality data settings. We also present the generalized pseudocode of A-LOL in Algorithm 1. Finally, we share the code on GitHub at https://github.com/abaheti95/LoL-RL."
        },
        {
            "heading": "8 ACKNOWLEDGEMENTS",
            "text": "We would like to thank Xuhui Zhou and Akhila Yerukola for their suggestions on an earlier draft of the paper. We also thank the anonymous reviewers for providing valuable feedback to improve presentation quality."
        },
        {
            "heading": "A A-LOL\u2019S RELATIONSHIP WITH PPO",
            "text": "Proximal Policy Optimization (Schulman et al., 2017) with clipped surrogate loss has led to huge success over a wide range of RL tasks. The clipped surrogate loss optimizes the objective JPPO(\u03b8) = Et[min(r(\u03b8, \u03b8old)A\u0302t, clip(r(\u03b8, \u03b8old), 1\u2212 \u03f5, 1+ \u03f5)A\u0302t)], where r(\u03b8, \u03b8old) = \u03c0\u03b8(at|st)\u03c0\u03b8old (at|st) is the ratio of current policy probability and old policy probability and A\u0302t is the advantage estimate of old policy for taking an action at given the state st. During training, PPO collects a rollout of actions using \u03c0\u03b8old on the environment and updates \u03c0\u03b8 on PPO\u2019s objective using stochastic gradient descent. Applying the single-action language generation assumption, as done in A-LOL, and using the (full sequence or per-token) importance weight, we notice that the PPO\u2019s objective has similarities with A-LOL\u2019s objective (\u03c0old is replaced with \u03c0ref and the A\u0302t is swapped with A\u03c0ref ). Intuitively, A-LOL can be seen as a form of one-step PPO on fixed rollout (training set Dtr) while never updating the \u03c0ref during training."
        },
        {
            "heading": "B HHA IMPLEMENTATION, ABLATIONS AND ANALYSIS",
            "text": "B.1 IMPLEMENTATION DETAILS\nWe implement all A-LOL methods along with the baselines using huggingface Transformers library (Wolf et al., 2020). For each training run, we use two NVIDIA RTX A6000 GPUs (48GB memory each). In total, we have 143K training instances and 280 validation instances in the Helpful and Harmless Assistant task. We keep the batch size =16 for all methods with 9,000 steps (i.e. in total 144K individual instances). For A-LOL and its variants, we first compute the value estimate of the frozen reference LM using its validation performance. Specifically, we sample one output for each input in validation, compute its reward, and train the value layer using mean squared loss on the rewards for 10 epochs. Since there are only 280 instances, training the value estimate barely takes 10 mins of training time. We then calculated the value estimate of all train instances and found that around 46K instances were negative advantages and were discarded from training. Although this step is slightly costly (4 hrs of non-gradient computations), it reduces the number of steps for A-LOL methods to 6,093 steps (i.e. \u224897K instances) and thus the overall training process of A-LOL including the value estimate computation ends faster than NLL (which roughly takes 1 day for full 9000 steps). Consequently, among all offline RL methods, A-LOL methods use the least training time whereas preference-based methods use the most training time (due to their usage of both good and bad responses during training). We select 0.0002 as the learning rate for all offline RL experiments and use the paged adamw 32bit optimizer. The remaining method-specific hyperparameters include \u03b2 = 0.1 as the KL weight in DPO, \u03b3 = 0.05 as the SFT weight in PRO, and \u03f5 = 0.9 as the clipping parameter in A-LOL. For both advantage and reward priority sampling, we simply divide the score of each instance with the L1 norm to obtain the sampling probability. For each method, we keep the same input formatting with 640 tokens for context and generate a maximum of 128 tokens with greedy decoding. In PPO, we execute 6,000 PPO update steps, where each step consists of a rollout of batch size 1618 that is trained for 4 internal PPO epochs (\u2248384K internal gradient calculations). To encourage diversity during online exploration, we use top-p sampling with p = 0.95 but reuse greedy decoding for evaluation (similar to the offline RL methods). We tested multiple learning rates {2e-4, 2e-5, 5e-6, 5e-7} and found 2e-5 to work best. We also include adaptive KL control with an initial coefficient = 0.2. Including the cost of generating the responses, PPO takes almost 6\u00d7 the time taken for offline RL training (approximately 6 days). We tune the hyperparameters on a single seed and test those hyperparameters with three random seeds while performing validation of the intermediate checkpoints at every 100 PPO steps. The validation reward plots of each seed are shown in Figure 3.\nB.2 GPT-4 AND HUMAN PREFERENCE EVALUATION SETUP\nFor a pair of responses from model A and model B, we prompt GPT-4 twice while shuffling the response order to indicate helpfulness and safety preference. The entire prompt is given below: For a conversation with a chatbot, which response is more helpful and safe? A good response should not have unimportant, repetitive, or irrelevant details.\nConversation: <the user query>\n18Experimented with rollout batch size 128 showed less overall performance than the batch size 16\nResponse A: <A model response>\nResponse B: <B model response>\nFIRST, provide a one-sentence comparison of the two responses and explain which you feel is more helpful and safe. SECOND, on a new line, state \"A\", \"B\", \"equal\" or \"neither\" to indicate which response is more helpful and safer. Your response should use the format:\nComparison: <one-sentence comparison and explanation> More helpful: <\"A\" or \"B\" or \"equal\" or \"neither\"> More safe: <\"A\" or \"B\" or \"equal\" or \"neither\"> ####################\nExamples of Safety and Helpful human annotation setup are given in Figure 8.\nB.3 A-LOL ABLATION EXPERIMENTS\nWe study the effects of priority sampling in A-LOL. In our main experiments, we prioritize highadvantage data points more when sampling from the training set. Precomputing advantages allow A-LOL to remove unfavorable data, save training compute, and enable priority sampling. However, it also incurs the extra computational cost of performing a forward pass through the full training set using the reference LM and its trained value estimate layer (\u00a72.2). To test the effectiveness of priority\nsampling, we compare it against random sampling from the entire dataset. We test two versions of random sampling - one with both positive and negative advantage and another with the negative advantage instances clamped to 0. The evaluation trajectory of priority vs. random sampling in A-LOL is presented in Figure 6. We notice a clear benefit of using priority sampling, as LM trained with it reaches higher performance than random sampling. Also, the removal of negative advantage data points is good for the performance. We also measure the differences resulting from changes in clipping parameter (\u03f5 = 0.9, 0.2 and no clipping). We see that \u03f5 = 0.9 wins by a slight margin, whereas the version without clipping leads to full degeneration due to high fluctuation in importance weight.19\nB.4 QUALITATIVE ANALYSIS OF NEGATIVE ADVANTAGE INSTANCES\nWe manually analyze the training instances that obtained the lowest reference policy advantage in the HHA dataset to check for safety and helpfulness. Out of the 50 analyzed conversation history and good preference-labeled responses, 26 were unsafe due to offensive statements or malicious advice, and 2 more contained inappropriate opinions. Even the remaining 22 instances were not high quality. We also use an off-the-shelf conversational offensive language classifier (Baheti et al., 2021) to test\n19A-LOL without clipping quickly started receiving nan values in loss.\nthe last two turns of the bottom 1000 negative advantage instances. The classifier identified 118 good responses to be outright offensive (with the probability of being offensive \u2265 0.5). We present a few example training instances and their reference LM advantage values in Table 14. By discarding the model-identified bad-quality instances, A-LOL improves both the training efficiency and the output quality of fine-tuned LMs.\nB.5 ANALYZING IMPORTANCE WEIGHT FOR A-LOL VARIANTS\nWe compare the importance weight behavior for A-LOL (ratio of entire output probabilities) with the per-token importance weight of A-LOL sequence. We find that importance weight in A-LOL is 66% > 1 + \u03f5 and 23% < 1 \u2212 \u03f5 where (\u03f5 = 0.9). In comparison, A-LOL seq. uses per-token importance weight < 1 \u2212 \u03f5 for only 5% of the tokens and > 1 + \u03f5 for 19% of the tokens. Thus, it shows that the A-LoL sequence variant is better able to make use of the importance weight as full output probability muddles the per-token differences. We show a qualitative example instance with clamped per-token importance weight in Figure 7. Subsequently, in our experiments A-LOL sequence shows better output diversity than A-LOL."
        },
        {
            "heading": "C ADDITIONAL EXPERIMENTS AND RESULTS",
            "text": "C.1 COMMONSENSE REASONING TASK\nCommonsense Transformer (COMET) (Bosselut et al., 2019) is an LM trained to predict the cause/effect of various social situations. To improve beyond the original COMET model, West et al. (2022) proposed symbolic knowledge distillation (SKD). They first construct ATOMIC10x containing 6.5M GPT-3-generated (Brown et al., 2020) commonsense knowledge pairs. The authors further condense it by filtering the bottom 62% of the data according to a critic classifier trained on 10K human-annotated labels. COMET model trained on this smaller high-quality subset improved the performance, however, this aggressive filtering may lose valuable training signal in return. In this experiment, we investigate whether A-LOL can improve upon the COMETDISTIL model from SKD (West et al., 2022), a 1.5B parameter GPT2-XL model trained on the entire ATOMIC10x data. Thus, COMETDISTIL is set as the reference policy, while COMET critic classifier is used as the task reward. The train and validation split from SKD is used as Dtr and Dv, whereas for testing, we use 17K unique prompts from human-written ATOMIC2020 test split. Due to the large training set, we only finetune the COMETDISTIL model further with all learning algorithms for 1 epoch. A-LOL identified 32% of ATOMIC10x data as negative advantage. In this task, we cannot compare with preference-based offline RL methods as human-labeled preferences are not available in the dataset.\nResults Table 5 shows that COMETDISTIL finetuned with A-LOL variants obtains the highest COMET critic score by improving an absolute \u22488% on top of its reference policy and reaching the closest to human quality. Second to this, weighted behavior cloning, Reward GOLD, and Reward LOL RL all utilize the rewards to improve average critic scores, but not as much as A-LOL variants. Interestingly, in this task A-LOL KL variant went into degeneration due to over-optimization of KL penalty, thus highlighting its instability. Also, further finetuning with NLL did not yield any improvement upon the reference policy. Compared to humans, all model generations are much less diverse indicating there is still progress to be made.\nC.2 KNOWLEDGE-GROUNDED DIALOG TASK\nLMs trained on knowledge-grounded dialog task fail to maintain faithfulness to the given knowledge and hallucinate incorrect information or opinions (Dziri et al., 2022b). In one of the commonly used knowledge-grounded dialog corpus, Wizard of Wikipedia (WoW) (Dinan et al., 2019), previous studies have shown that only 24% of the responses were truly faithful to the given knowledge and also contained huge lexical overlap with the knowledge sentences (Dziri et al., 2022a). To mitigate this issue, researchers identified and rewrote the hallucinated responses, to construct a smaller and more faithful training set called FaithDial (Dziri et al., 2022a). They also trained a FaithCritic classifier to automatically predict the faithfulness probability of a knowledge and response pair. Subsequently, dialog models trained on the FaithDial corpus were found to be more faithful and engaging. However, such data collection is costly due to the required domain expertise and careful human annotations. In this experiment, we test whether A-LOL methods can improve LMs faithfulness even from suboptimal WoW data. Consequently, we select Dtr, Dv from WoW, containing 69K and 3.7K instances respectively, while Dtest is chosen from the FaithDial corpus test split with 3.6K high-quality faithful gold responses. While keeping Dtest fixed, we also create two additional Dtr with 18.3K instances from FaithDial and 87.3K instances from merged FaithDial and WoW. Similar to our previous dialog experiment, we finetune the DialoGPT-medium (DGPT) (Zhang et al., 2020) model on the respective train sets using NLL objective for 6 epochs and use it as the reference policy. Subsequently, we continue further finetuning for 3 epochs with NLL, reward-based offline RL, and A-LOL variants. In knowledge-grounded dialogs, responses should not only be faithful but also fluent, engaging, and diverse. Therefore, we use the final reward as a sum of four different scoring functions: probability estimates from the FaithCritic classifier, CoLA fluency classifier, and dialog engagement classifier (Gao et al., 2020) along with the TF-IDF diversity score. We evaluate all LMs using the rewards obtained on Dtest. Knowledge-grounded dialog models can occasionally copy the provided knowledge verbatim in their outputs. To evaluate this behavior, we also report the coverage and density automatic metrics from summarization research (Grusky et al., 2018), that capture the lexical overlap between knowledge and response strings.20 Similar to our previous experiments, we also calculate the average response length and corpus-level distinct-n-gram diversity metrics (Li et al., 2016). We present the metrics achieved by all methods for all three datasets in Table 6.\nResults We again observe A-LOL models outperform reference LM and all other LMs trained with NLL and reward-based baselines in all three data settings. In the LMs trained with the WoW dataset, high coverage and density metrics indicate more copying of knowledge compared to the other two datasets. Interestingly, A-LOL models decrease the average density compared to models trained with NLL and reward-based objectives. This indicates that our method not only improves overall performance according to rewards but also reduces the knowledge-copying behavior. Even when mixed with good and bad quality data (WoW and FaithDial merged), A-LOL is able to maintain very similar performance to the counterpart with only good quality data (FaithDial). We find that A-LOL identified a negative advantage in 39% of WoW\u2019s training data, 10% of FaithDial\u2019s\n20Coverage is the average lexical overlap between knowledge and response, whereas, Density is the average length of extractive spans in response that are copied from the knowledge.\nModels trained on FaithDial training set\n\u03c0ref (DialoGPT) 2.89 .99 .90 .75 .25 .34 2.31 15.8 .147/.408/.565 + NLL 2.89 .99 .91 .75 .25 .32 2.01 15.6 .146/.408/.568 + wBC 2.90 .98 .91 .75 .25 .34 2.36 15.4 .154/.435/.605 + R GOLD 2.90 .99 .91 .74 .26 .40 3.00 15.5 .150/.411/.562 + R-LOL 2.91 .98 .91 .76 .26 .36 2.35 14.4 .159/.440/.608 + A-LOL (ref. free) 2.93 .98 .92 .76 .26 .33 2.21 15.0 .155/.437/.606 + A-LOL 2.94 .98 .93 .77 .26 .33 2.15 14.1 .159/.430/.592 + A-LOL seq 2.94 .97 .93 .78 .26 .32 1.82 14.5 .160/.450/.630 + A-LOL KL 2.92 .98 .92 .77 .26 .33 2.02 14.8 .159/.447/.623\nModels trained on both WoW and FaithDial training set\n\u03c0ref (DialoGPT) 2.80 .90 .91 .73 .25 .43 3.78 15.7 .160/.449/.622 + wBC 2.86 .95 .91 .75 .25 .41 3.48 15.7 .157/.447/.618 + R GOLD 2.87 .97 .91 .73 .25 .50 5.27 16.1 .158/.422/.569 + R-LOL 2.88 .94 .92 .75 .26 .43 3.66 15.0 .168/.465/.639 + A-LOL (ref. free) 2.92 .98 .92 .76 .26 .43 3.51 14.8 .164/.453/.624 + A-LOL 2.92 .97 .93 .75 .27 .40 2.92 14.1 .164/.452/.625 + A-LOL seq 2.93 .97 .93 .77 .27 .38 2.53 14.1 .164/.462/.650 + A-LOL KL 2.91 .97 .92 .76 .26 .41 3.39 15.0 .164/.454/.626\nFaithDial Dtest 2.76 .95 .81 .76 .24 .23 .97 17.6 .166/.555/.792\ntraining data, and 55% of merged training instances. Thus, A-LOL automatically filters the badquality instances again showing its resilience to noise.\nC.3 PPO EXPERIMENTS ON REDDIT RESPONSE GENERATION TASK\nWe reuse the best hyperparameters from the HHA task and run PPO on the Reddit upvoted and downvoted data for two epochs and present the results in Table 7 alongside the best A-LOL seq. models. Since on-policy updates are not affected by upvoted and downvoted responses, in both datasets PPO achieved roughly similar total reward. However, both models learned to optimize fluency and safety while generating generic responses regardless of the input, thereby reducing the corpus diversity drastically. We share some example responses from PPO trained on upvoted data\nin Table 8. This highlights the weakness of PPO when the rewards are not well designed for online exploration and thus, offline RL methods, including A-LOL, are more robust."
        },
        {
            "heading": "D LIMITATIONS AND SOCIETAL AND ETHICAL CONSIDERATIONS",
            "text": "We discuss some of the limitations of Advantage-Leftover Lunch RL. First, A-LOL requires some good data coverage to get a good initial policy for success. In the presence of exclusively bad data, most of the training instances will be negative advantage, and thus, A-LOL won\u2019t be of benefit. Secondly, A-LOL requires that the evaluation metric aligns with the provided rewards. In our preliminary experiments with machine translation task (Bojar et al., 2016), we found that A-LOL could not improve lexical matching-based metrics when we used multilingual embedding similarity as the reward. The single-action assumption may not hold in the case of multi-turn dialogs where rewards may be at the utterance level. A single sequence-level reward for each instance will obscure disagreement in how humans would label sequences (i.e., average out value pluralism). For example, people differ in what they consider a high-quality text, what is commonsense vs. domain-specific knowledge, etc. (de Marneffe et al., 2012; Plank, 2022). One can also design rewards to elicit nefarious behavior and optimize LMs on it. Future research using A-LOL or any offline RL method should not only include access to the training data sources but also the reward models, while also describing how they were acquired. Although less than other RL methods, A-LOL is also susceptible to reward hacking (Skalse et al., 2022; Pang et al., 2023) by learning bad sequences as \u201ccritical\u201d actions (Kumar et al., 2022). To avoid this, reward models and training data should be carefully inspected and cleaned before training with the A-LOL algorithms. Researchers should also conduct human evaluations to gauge how well the reward models and LMs trained on them actually align with human-desired behavior (Jacobs & Wallach, 2021). On the positive side, A-LOL allows for both stable and sample-efficient training of models on existing language data. Our method has potential benefits in reducing the carbon footprint of training large language models by avoiding expensive online RL exploration and only training on positive advantage data points (Strubell et al., 2019; Dodge et al., 2022). Furthermore, A-LOL can leverage feedback from multiple readily available pretrained classifiers and tune language models to satisfy multiple desirable attributes such as fluency, non-toxicity, and engagement.21\n21Our experiment of \u201chuman-like\u201d Reddit response generation task (\u00a75) is not intended towards making bots that post comments on Reddit or other social media. It was only to demonstrate a proof-of-concept of using multiple rewards with A-LOL."
        },
        {
            "heading": "E FUTURE WORK",
            "text": "Exploiting the single-action assumption, we can use A-LOL with both fine-grained and sequencelevel rewards (Wu et al., 2023b). We also plan to investigate continual learning (Ring, 1997) in A-LOL by adding new (advantageous) data points to the training set, that are either human-written or LM-generated."
        }
    ],
    "title": "LEFTOVER-LUNCH: ADVANTAGE-BASED OFFLINE REINFORCEMENT LEARNING FOR LANGUAGE MODELS",
    "year": 2024
}