{
    "abstractText": "We present a novel approach named OmniControl for incorporating flexible spatial control signals into a text-conditioned human motion generation model based on the diffusion process. Unlike previous methods that can only control the pelvis trajectory, OmniControl can incorporate flexible spatial control signals over different joints at different times with only one model. Specifically, we propose analytic spatial guidance that ensures the generated motion can tightly conform to the input control signals. At the same time, realism guidance is introduced to refine all the joints to generate more coherent motion. Both the spatial and realism guidance are essential and they are highly complementary for balancing control accuracy and motion realism. By combining them, OmniControl generates motions that are realistic, coherent, and consistent with the spatial constraints. Experiments on HumanML3D and KIT-ML datasets show that OmniControl not only achieves significant improvement over state-of-the-art methods on pelvis control but also shows promising results when incorporating the constraints over other joints. Our code and model weights will be publicly released.",
    "authors": [],
    "id": "SP:c65d30d5375cebe63cc065087edff7621b5f98b7",
    "references": [
        {
            "authors": [
                "Chaitanya Ahuja",
                "Louis-Philippe Morency"
            ],
            "title": "Language2pose: Natural language grounded pose forecasting",
            "venue": "In 3DV,",
            "year": 2019
        },
        {
            "authors": [
                "Xin Chen",
                "Zhuo Su",
                "Lingbo Yang",
                "Pei Cheng",
                "Lan Xu",
                "Bin Fu",
                "Gang Yu"
            ],
            "title": "Learning variational motion prior for video-based motion capture",
            "year": 2022
        },
        {
            "authors": [
                "Xin Chen",
                "Biao Jiang",
                "Wen Liu",
                "Zilong Huang",
                "Bin Fu",
                "Tao Chen",
                "Gang Yu"
            ],
            "title": "Executing your commands via motion diffusion in latent space",
            "year": 2023
        },
        {
            "authors": [
                "Jooyoung Choi",
                "Sungwon Kim",
                "Yonghyun Jeong",
                "Youngjune Gwon",
                "Sungroh Yoon"
            ],
            "title": "Ilvr: Conditioning method for denoising diffusion probabilistic models",
            "venue": "ArXiv,",
            "year": 2021
        },
        {
            "authors": [
                "Hyungjin Chung",
                "Byeongsu Sim",
                "Dohoon Ryu",
                "Jong Chul Ye"
            ],
            "title": "Improving diffusion models for inverse problems using manifold constraints",
            "venue": "NeurIPS,",
            "year": 2022
        },
        {
            "authors": [
                "Prafulla Dhariwal",
                "Alexander Nichol"
            ],
            "title": "Diffusion models beat gans on image synthesis",
            "venue": "In NeurIPS,",
            "year": 2021
        },
        {
            "authors": [
                "Anindita Ghosh",
                "Rishabh Dabral",
                "Vladislav Golyanik",
                "Christian Theobalt",
                "Philipp Slusallek"
            ],
            "title": "Imos: Intent-driven full-body motion synthesis for human-object interactions",
            "venue": "In CGF,",
            "year": 2023
        },
        {
            "authors": [
                "Chuan Guo",
                "Xinxin Zuo",
                "Sen Wang",
                "Shihao Zou",
                "Qingyao Sun",
                "Annan Deng",
                "Minglun Gong",
                "Li Cheng"
            ],
            "title": "Action2motion: Conditioned generation of 3d human motions",
            "venue": "In ACM MM,",
            "year": 2020
        },
        {
            "authors": [
                "Chuan Guo",
                "Shihao Zou",
                "Xinxin Zuo",
                "Sen Wang",
                "Wei Ji",
                "Xingyu Li",
                "Li Cheng"
            ],
            "title": "Generating diverse and natural 3d human motions from text",
            "venue": "In CVPR,",
            "year": 2022
        },
        {
            "authors": [
                "Chuan Guo",
                "Xinxin Zuo",
                "Sen Wang",
                "Li Cheng"
            ],
            "title": "Tm2t: Stochastic and tokenized modeling for the reciprocal generation of 3d human motions and texts",
            "venue": "In ECCV,",
            "year": 2022
        },
        {
            "authors": [
                "Ikhsanul Habibie",
                "Daniel Holden",
                "Jonathan Schwarz",
                "Joe Yearsley",
                "Taku Komura"
            ],
            "title": "A recurrent variational autoencoder for human motion synthesis",
            "year": 2017
        },
        {
            "authors": [
                "Mohamed Hassan",
                "Duygu Ceylan",
                "Ruben Villegas",
                "Jun Saito",
                "Jimei Yang",
                "Yi Zhou",
                "Michael Black"
            ],
            "title": "Stochastic scene-aware motion prediction",
            "venue": "In ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "Jonathan Ho",
                "Tim Salimans"
            ],
            "title": "Classifier-free diffusion guidance",
            "venue": "In NeurIPS Workshop,",
            "year": 2021
        },
        {
            "authors": [
                "Jonathan Ho",
                "Ajay Jain",
                "Pieter Abbeel"
            ],
            "title": "Denoising diffusion probabilistic models",
            "venue": "In NeurIPS,",
            "year": 2020
        },
        {
            "authors": [
                "Siyuan Huang",
                "Zan Wang",
                "Puhao Li",
                "Baoxiong Jia",
                "Tengyu Liu",
                "Yixin Zhu",
                "Wei Liang",
                "SongChun Zhu"
            ],
            "title": "Diffusion-based generation, optimization, and planning in 3d scenes",
            "year": 2023
        },
        {
            "authors": [
                "Biao Jiang",
                "Xin Chen",
                "Wen Liu",
                "Jingyi Yu",
                "Gang Yu",
                "Tao Chen"
            ],
            "title": "Motiongpt: Human motion as a foreign language",
            "venue": "NeurIPS,",
            "year": 2023
        },
        {
            "authors": [
                "Nan Jiang",
                "Tengyu Liu",
                "Zhexuan Cao",
                "Jieming Cui",
                "Yixin Chen",
                "He Wang",
                "Yixin Zhu",
                "Siyuan Huang"
            ],
            "title": "Chairs: Towards full-body articulated human-object interaction",
            "year": 2022
        },
        {
            "authors": [
                "Jordan Juravsky",
                "Yunrong Guo",
                "Sanja Fidler",
                "Xue Bin Peng"
            ],
            "title": "Padl: Language-directed physicsbased character control",
            "venue": "In SIGGRAPH Asia,",
            "year": 2022
        },
        {
            "authors": [
                "Korrawe Karunratanakul",
                "Konpat Preechakul",
                "Supasorn Suwajanakorn",
                "Siyu Tang"
            ],
            "title": "Gmd: Controllable human motion synthesis via guided diffusion models",
            "venue": "In ICCV,",
            "year": 2023
        },
        {
            "authors": [
                "Manuel Kaufmann",
                "Emre Aksan",
                "Jie Song",
                "Fabrizio Pece",
                "Remo Ziegler",
                "Otmar Hilliges"
            ],
            "title": "Convolutional autoencoders for human motion infilling",
            "venue": "In 3DV,",
            "year": 2020
        },
        {
            "authors": [
                "Jihoon Kim",
                "Jiseob Kim",
                "Sungjoon Choi"
            ],
            "title": "Flame: Free-form language-based motion synthesis & editing",
            "venue": "In AAAI,",
            "year": 2023
        },
        {
            "authors": [
                "Nilesh Kulkarni",
                "Davis Rempe",
                "Kyle Genova",
                "Abhijit Kundu",
                "Justin Johnson",
                "David Fouhey",
                "Leonidas Guibas"
            ],
            "title": "Nifty: Neural object interaction fields for guided human motion synthesis",
            "year": 2023
        },
        {
            "authors": [
                "Buyu Li",
                "Yongchi Zhao",
                "Shi Zhelun",
                "Lu Sheng"
            ],
            "title": "Danceformer: Music conditioned 3d dance generation with parametric motion transformer",
            "venue": "In AAAI,",
            "year": 2022
        },
        {
            "authors": [
                "Ruilong Li",
                "Shan Yang",
                "David A. Ross",
                "Angjoo Kanazawa"
            ],
            "title": "Ai choreographer: Music conditioned 3d dance generation with aist++",
            "year": 2021
        },
        {
            "authors": [
                "Yuheng Li",
                "Haotian Liu",
                "Qingyang Wu",
                "Fangzhou Mu",
                "Jianwei Yang",
                "Jianfeng Gao",
                "Chunyuan Li",
                "Yong Jae Lee"
            ],
            "title": "Gligen: Open-set grounded text-to-image generation",
            "year": 2023
        },
        {
            "authors": [
                "Han Liang",
                "Wenqian Zhang",
                "Wenxuan Li",
                "Jingyi Yu",
                "Lan Xu"
            ],
            "title": "Intergen: Diffusion-based multihuman motion generation under complex interactions",
            "year": 2023
        },
        {
            "authors": [
                "Hung Yu Ling",
                "Fabio Zinno",
                "George Cheng",
                "Michiel Van De Panne"
            ],
            "title": "Character controllers using motion vaes",
            "year": 2020
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter"
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "In ICLR,",
            "year": 2017
        },
        {
            "authors": [
                "Cheng Lu",
                "Yuhao Zhou",
                "Fan Bao",
                "Jianfei Chen",
                "Chongxuan Li",
                "Jun Zhu"
            ],
            "title": "Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Thomas Lucas",
                "Fabien Baradel",
                "Philippe Weinzaepfel",
                "Gr\u00e9gory Rogez"
            ],
            "title": "Posegpt: Quantizationbased 3d human motion generation and forecasting",
            "venue": "In ECCV,",
            "year": 2022
        },
        {
            "authors": [
                "Zhaoyang Lyu",
                "Xudong Xu",
                "Ceyuan Yang",
                "Dahua Lin",
                "Bo Dai"
            ],
            "title": "Accelerating diffusion models via early stop of the diffusion process",
            "venue": "arXiv preprint arXiv:2205.12524,",
            "year": 2022
        },
        {
            "authors": [
                "Naureen Mahmood",
                "Nima Ghorbani",
                "Nikolaus F. Troje",
                "Gerard Pons-Moll",
                "Michael J. Black"
            ],
            "title": "AMASS: Archive of motion capture as surface shapes",
            "year": 2019
        },
        {
            "authors": [
                "Alexander Quinn Nichol",
                "Prafulla Dhariwal"
            ],
            "title": "Improved denoising diffusion probabilistic models",
            "venue": "In ICML,",
            "year": 2021
        },
        {
            "authors": [
                "Xue Bin Peng",
                "Ze Ma",
                "Pieter Abbeel",
                "Sergey Levine",
                "Angjoo Kanazawa"
            ],
            "title": "Amp: Adversarial motion priors for stylized physics-based character control",
            "year": 2021
        },
        {
            "authors": [
                "Mathis Petrovich",
                "Michael J Black",
                "G\u00fcl Varol"
            ],
            "title": "Action-conditioned 3d human motion synthesis with transformer vae",
            "venue": "In CVPR,",
            "year": 2021
        },
        {
            "authors": [
                "Mathis Petrovich",
                "Michael J Black",
                "G\u00fcl Varol"
            ],
            "title": "Temos: Generating diverse human motions from textual descriptions",
            "venue": "In ECCV,",
            "year": 2022
        },
        {
            "authors": [
                "Mathis Petrovich",
                "Michael J Black",
                "G\u00fcl Varol"
            ],
            "title": "Tmr: Text-to-motion retrieval using contrastive 3d human motion synthesis",
            "venue": "ArXiv,",
            "year": 2023
        },
        {
            "authors": [
                "Huaijin Pi",
                "Sida Peng",
                "Minghui Yang",
                "Xiaowei Zhou",
                "Hujun Bao"
            ],
            "title": "Hierarchical generation of human-object interactions with diffusion probabilistic models",
            "venue": "In ICCV,",
            "year": 2023
        },
        {
            "authors": [
                "Matthias Plappert",
                "Christian Mandery",
                "Tamim Asfour"
            ],
            "title": "The kit motion-language dataset",
            "venue": "Big Data,",
            "year": 2016
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "year": 2021
        },
        {
            "authors": [
                "Aditya Ramesh",
                "Mikhail Pavlov",
                "Gabriel Goh",
                "Scott Gray",
                "Chelsea Voss",
                "Alec Radford",
                "Mark Chen",
                "Ilya Sutskever"
            ],
            "title": "Zero-shot text-to-image generation",
            "venue": "In ICML,",
            "year": 2021
        },
        {
            "authors": [
                "Davis Rempe",
                "Tolga Birdal",
                "Aaron Hertzmann",
                "Jimei Yang",
                "Srinath Sridhar",
                "Leonidas J Guibas"
            ],
            "title": "Humor: 3d human motion model for robust pose estimation",
            "year": 2021
        },
        {
            "authors": [
                "Davis Rempe",
                "Zhengyi Luo",
                "Xue Bin Peng",
                "Ye Yuan",
                "Kris Kitani",
                "Karsten Kreis",
                "Sanja Fidler",
                "Or Litany"
            ],
            "title": "Trace and pace: Controllable pedestrian animation via guided trajectory diffusion",
            "year": 2023
        },
        {
            "authors": [
                "Robin Rombach",
                "Andreas Blattmann",
                "Dominik Lorenz",
                "Patrick Esser",
                "Bj\u00f6rn Ommer"
            ],
            "title": "Highresolution image synthesis with latent diffusion models",
            "year": 2022
        },
        {
            "authors": [
                "Chitwan Saharia",
                "William Chan",
                "Saurabh Saxena",
                "Lala Li",
                "Jay Whang",
                "Emily L Denton",
                "Kamyar Ghasemipour",
                "Raphael Gontijo Lopes",
                "Burcu Karagol Ayan",
                "Tim Salimans"
            ],
            "title": "Photorealistic text-to-image diffusion models with deep language understanding",
            "venue": "NeurIPS,",
            "year": 2022
        },
        {
            "authors": [
                "Tim Salimans",
                "Jonathan Ho"
            ],
            "title": "Progressive distillation for fast sampling of diffusion models",
            "venue": "arXiv preprint arXiv:2202.00512,",
            "year": 2022
        },
        {
            "authors": [
                "Yonatan Shafir",
                "Guy Tevet",
                "Roy Kapon",
                "Amit H. Bermano"
            ],
            "title": "Human motion diffusion as a generative prior",
            "venue": "ArXiv,",
            "year": 2023
        },
        {
            "authors": [
                "Yi Shi",
                "Jingbo Wang",
                "Xuekun Jiang",
                "Bo Dai"
            ],
            "title": "Controllable motion diffusion model",
            "venue": "ArXiv,",
            "year": 2023
        },
        {
            "authors": [
                "Sebastian Starke",
                "He Zhang",
                "Taku Komura",
                "Jun Saito"
            ],
            "title": "Neural state machine for character-scene interactions",
            "year": 2019
        },
        {
            "authors": [
                "Sebastian Starke",
                "Ian Mason",
                "Taku Komura"
            ],
            "title": "Deepphase: Periodic autoencoders for learning motion phase manifolds",
            "year": 2022
        },
        {
            "authors": [
                "Guy Tevet",
                "Brian Gordon",
                "Amir Hertz",
                "Amit H Bermano",
                "Daniel Cohen-Or"
            ],
            "title": "Motionclip: Exposing human motion generation to clip space",
            "year": 2022
        },
        {
            "authors": [
                "Guy Tevet",
                "Sigal Raab",
                "Brian Gordon",
                "Yonatan Shafir",
                "Daniel Cohen-or",
                "Amit Haim Bermano"
            ],
            "title": "Human motion diffusion model",
            "venue": "In ICLR,",
            "year": 2023
        },
        {
            "authors": [
                "Jonathan Tseng",
                "Rodrigo Castellon",
                "Karen Liu"
            ],
            "title": "Edge: Editable dance generation from music",
            "venue": "In CVPR,",
            "year": 2023
        },
        {
            "authors": [
                "Jingbo Wang",
                "Yu Rong",
                "Jingyuan Liu",
                "Sijie Yan",
                "Dahua Lin",
                "Bo Dai"
            ],
            "title": "Towards diverse and natural scene-aware 3d human motion synthesis",
            "venue": "In CVPR,",
            "year": 2022
        },
        {
            "authors": [
                "Zan Wang",
                "Yixin Chen",
                "Tengyu Liu",
                "Yixin Zhu",
                "Wei Liang",
                "Siyuan Huang"
            ],
            "title": "Humanise: Language-conditioned human motion generation in 3d scenes. NeurIPS, 2022b",
            "year": 2022
        },
        {
            "authors": [
                "Sirui Xu",
                "Zhengyuan Li",
                "Yu-Xiong Wang",
                "Liang-Yan Gui"
            ],
            "title": "Interdiff: Generating 3d humanobject interactions with physics-informed diffusion",
            "venue": "In ICCV,",
            "year": 2023
        },
        {
            "authors": [
                "Sijie Yan",
                "Zhizhong Li",
                "Yuanjun Xiong",
                "Huahan Yan",
                "Dahua Lin"
            ],
            "title": "Convolutional sequence generation for skeleton-based action synthesis",
            "year": 2019
        },
        {
            "authors": [
                "Ye Yuan",
                "Jiaming Song",
                "Umar Iqbal",
                "Arash Vahdat",
                "Jan Kautz"
            ],
            "title": "Physdiff: Physics-guided human motion diffusion model",
            "venue": "In ICCV,",
            "year": 2023
        },
        {
            "authors": [
                "Jianrong Zhang",
                "Yangsong Zhang",
                "Xiaodong Cun",
                "Shaoli Huang",
                "Yong Zhang",
                "Hongwei Zhao",
                "Hongtao Lu",
                "Xi Shen"
            ],
            "title": "T2m-gpt: Generating human motion from textual descriptions with discrete representations",
            "venue": "In CVPR,",
            "year": 2023
        },
        {
            "authors": [
                "Lvmin Zhang",
                "Anyi Rao",
                "Maneesh Agrawala"
            ],
            "title": "Adding conditional control to text-to-image diffusion models",
            "venue": "In ICCV,",
            "year": 2023
        },
        {
            "authors": [
                "Mingyuan Zhang",
                "Zhongang Cai",
                "Liang Pan",
                "Fangzhou Hong",
                "Xinying Guo",
                "Lei Yang",
                "Ziwei Liu"
            ],
            "title": "Motiondiffuse: Text-driven human motion generation with diffusion model. ArXiv, 2022a",
            "year": 2022
        },
        {
            "authors": [
                "Xiaohan Zhang",
                "Bharat Lal Bhatnagar",
                "Sebastian Starke",
                "Vladimir Guzov",
                "Gerard Pons-Moll. Couch"
            ],
            "title": "Towards controllable human-chair interactions",
            "venue": "In ECCV,",
            "year": 2022
        },
        {
            "authors": [
                "Yaqi Zhang",
                "Di Huang",
                "Bin Liu",
                "Shixiang Tang",
                "Yan Lu",
                "Lu Chen",
                "Lei Bai",
                "Qi Chu",
                "Nenghai Yu",
                "Wanli Ouyang"
            ],
            "title": "Motiongpt: Finetuned llms are general-purpose motion generators. arXiv, 2023c",
            "year": 2023
        },
        {
            "authors": [
                "Kaifeng Zhao",
                "Yan Zhang",
                "Shaofei Wang",
                "Thabo Beeler",
                "Siyu Tang"
            ],
            "title": "Synthesizing diverse human motions in 3d indoor scenes",
            "venue": "ArXiv,",
            "year": 2023
        },
        {
            "authors": [
                "Shafir"
            ],
            "title": "V is the number of frames we want to control (density) and \u03a3\u0302t = min(\u03a3t, 0.01)",
            "venue": "Experiment details. Tevet et al",
            "year": 2023
        },
        {
            "authors": [
                "Liang"
            ],
            "title": "2023) propose a non-canonicalization representation for multi-person interaction",
            "venue": "WHY DIDN\u2019T WE USE THE GLOBAL POSE REPRESENTATION PROPOSED IN LIANG ET AL",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "We address the problem of incorporating spatial control signals over any joint at any given time into text-conditioned human motion generation, as shown in Fig. 1. While recent diffusion-based methods can generate diverse and realistic human motion, they cannot easily integrate flexible spatial control signals that are crucial for many applications. For example, to synthesize the motion for picking up a cup, a model should not only semantically understand \u201cpick up\u201d but also control the hand position to touch the cup at a specific position and time. Similarly, for navigating through a low-ceiling space, a model needs to carefully control the height of the head during a specific period to prevent collisions.\nThese control signals are usually provided as global locations of joints of interest in keyframes as they are hard to convey in the textual prompt. The relative human pose representations adopted by existing inpainting-based methods (Karunratanakul et al., 2023; Shafir et al., 2023; Tevet et al., 2023), however, prevent them from incorporating flexible control signals. The limitations mainly stem from the relative positions of the pelvis w.r.t. the previous frame and other joints w.r.t. the pelvis. As a result, to input the global pelvis position specified in the control signal to the keyframe, it needs to be converted to a relative location w.r.t the preceding frame. Similarly, to input positions of other joints, a conversion of the global position w.r.t. the pelvis is also required. But in both cases, the relative positions of the pelvis are non-existent or inaccurate in-between the diffusion generation process. Therefore, both Tevet et al. (2023) and Shafir et al. (2023) struggle to handle sparse constraints on the pelvis and incorporate any spatial control signal on joints other than the pelvis. Although Karunratanakul et al. (2023) introduces a two-stage model to handle the sparse control signals over the pelvis, it still faces challenges in controlling other joints.\nIn this work, we propose OmniControl, a novel diffusion-based human generation model that can incorporate flexible spatial control signals over any joint at any given time. Building on top of Tevet et al. (2023), OmniControl introduces spatial and realism guidance to control human motion generation. We adopt the same relative human pose representations as the model\u2019s input and output for its effectiveness. But in the spatial guidance module, unlike existing methods, we propose to convert the generated motion to global coordinates to directly compare with the input control sig-\nnals, where the gradients of the error are used to refine the generated motion. It eliminates the ambiguity related to the relative positions of the pelvis and thereby addresses the limitations of the previous inpainting-based approaches. Moreover, it allows dynamic iterative refinement of the generated motion compared with other methods, leading to better control accuracy. While effective at enforcing spatial constraints, spatial guidance alone usually leads to unnatural human motion and drifting problems. To address these issues, taking inspirations from the controllable image generation (Zhang et al., 2023b), we introduce the realism guidance that outputs the residuals w.r.t. the features in each attention layer of the motion diffusion model. These residuals can directly perturb the whole-body motion densely and implicitly. Both the spatial and realism guidance are essential and they are highly complimentary in balancing control accuracy and motion realism, yielding motions that are realistic, coherent, and consistent with the spatial constraints.\nExperiments on HumanML3D (Guo et al., 2022a) and KIT-ML (Plappert et al., 2016) show that OmniControl outperforms the state-of-the-art text-based motion generation methods on pelvis control by large margins in terms of both motion realism and control accuracy. More importantly, OmniControl achieves impressive results in incorporating the spatial constraints over any joint at any time. In addition, we can train a single model for controlling multiple joints together instead of having an individual model for each joint, as shown in Fig. 1 (e.g., both left wrist and right wrist). These proprieties of OmniControl enable many downstream applications, e.g., connecting generated human motion with the surrounding objects and scenes, as demonstrated in Fig. 1 (last column).\nTo summarize, our contributions are: (1) To our best knowledge, OmniControl is the first approach capable of incorporating spatial control signals over any joint at any time. (2) We propose a novel control module that uses both spatial and realism guidance to effectively balance the control accuracy and motion realism in the generated motion. (3) Experiments show that OmniControl not only sets a new state of the art in controlling the pelvis but also can control any other joints using a single model in text-based motion generation, thereby enabling a set of applications in human motion generation."
        },
        {
            "heading": "2 RELATED WORK",
            "text": ""
        },
        {
            "heading": "2.1 HUMAN MOTION GENERATION",
            "text": "Human motion synthesis can be broadly categorized into two groups: auto-regressive methods (Rempe et al., 2021; Starke et al., 2019; 2022; Shi et al., 2023; Ling et al., 2020; Peng et al., 2021; Juravsky et al., 2022) and sequence-level methods (Tevet et al., 2023; Yan et al., 2019). Autoregressive methods use the information from past motion to recursively generate the current motion frame by frame. These methods are primarily tailored for real-time scenarios. In contrast, sequence-\nlevel methods are designed to generate entire fixed-length motion sequences. Owing to this inherent feature, they can seamlessly integrate with existing generative models, such as VAE (Habibie et al., 2017; Petrovich et al., 2021; Lucas et al., 2022) and diffusion models (Zhang et al., 2022a; Chen et al., 2023), enabling various prompts. These prompts can originate from various external sources, such as text (Petrovich et al., 2023; Guo et al., 2022b; Petrovich et al., 2022; Tevet et al., 2023; Chen et al., 2023; Zhang et al., 2022a; Jiang et al., 2023; Zhang et al., 2023c;a; Tevet et al., 2022; Ahuja & Morency, 2019; Guo et al., 2022a; Kim et al., 2023; Yuan et al., 2023), action (Guo et al., 2020; Petrovich et al., 2021), music (Li et al., 2022; Tseng et al., 2023; Li et al., 2021), images (Chen et al., 2022), trajectories (Kaufmann et al., 2020; Karunratanakul et al., 2023; Rempe et al., 2023), 3D scenes (Huang et al., 2023; Zhao et al., 2023; Wang et al., 2022a;b) and objects (Ghosh et al., 2023; Kulkarni et al., 2023; Jiang et al., 2022; Xu et al., 2023; Hassan et al., 2021; Starke et al., 2019; Zhang et al., 2022b; Pi et al., 2023).\nAlthough incorporating spatial constraints is a fundamental feature, it remains a challenge for textbased human motion synthesis methods. An ideal method should guarantee that the produced motion closely follows the global spatial control signals, aligns with the textual semantics, and maintains fidelity. Such an approach should also be capable of controlling any joint and their combinations, as well as handling sparse control signals. PriorMDM (Shafir et al., 2023) and MDM (Tevet et al., 2023) use inpainting-based methods to input the spatial constraints into the generated motions. However, limited by their relative human pose representations where locations of other joints are defined w.r.t. to the pelvis, these methods struggle to incorporate the global constraints for other joints except for the pelvis and handle sparse spatial constraints. Although the inpainting-based method GMD (Karunratanakul et al., 2023) introduces a two-stage guided motion diffusion model to handle sparse control signals. it still faces challenges in incorporating spatial constraints into any other joint. In this paper, we focus on sequence-level motion generation and propose a novel method that enables control over any joint, even with sparse control signals, using a single model."
        },
        {
            "heading": "2.2 CONTROLLABLE DIFFUSION-BASED GENERATIVE MODEL IN IMAGE GENERATION",
            "text": "Recently, the diffusion-based generative model has gained significant attention due to their impressive performance in image generation (Rombach et al., 2022). Diffusion models are well-suited for controlling and conditioning. Typically, there are several methods for conditional generation. Imputation and inpainting (Choi et al., 2021; Chung et al., 2022) fill in missing parts of data with observed data such that the filled-in content is visually consistent with the surrounding area. However, it is difficult when the observed data is in a different space compared to the filling part, e.g., generating images from semantic maps. Classifier guidance (Chung et al., 2022; Dhariwal & Nichol, 2021) exploits training a separate classifier to improve the conditional diffusion generation model. Classifierfree guidance (Ho & Salimans, 2021) jointly trains conditional and unconditional diffusion models and combines them to attain a trade-off between sample quality and diversity. GLIGEN (Li et al., 2023) adds a trainable gated self-attention layer at each transformer block to absorb new grounding input. ControlNet (Zhang et al., 2023b) introduces a neural network designed to control large image diffusion models, enabling rapid adaptation to task-specific control signals with minimal data and training. These controlling methods are not mutually exclusive, and solely adopting one may not achieve the desired goals. Inspired by classifier guidance and ControlNet, we design hybrid guidance, consisting of spatial and realism guidance, to incorporate spatial control signals into human motion generation. The spatial guidance applies an analytic function to approximate a classifier, enabling multiple efficient perturbations of the generated motion. At the same time, the realism guidance uses a neural network similar to ControlNet to adjust the output to generate coherent and realistic motion. Both of these two guidance modules are essential and they are highly complimentary in balancing motion realism and control accuracy."
        },
        {
            "heading": "3 OMNICONTROL",
            "text": "In this section, we introduce our proposed OmniControl for incorporating spatial constraints into a human motion generation process. Fig. 2 shows an overview of OmniControl. Given a prompt p, such as text, and an additional spatial control signal c \u2208 RN\u00d7J\u00d73, our goal is to generate a human motion sequence x \u2208 RN\u00d7D. N is the length of the motion sequence, J is number of joints, and D is the dimension of human pose representations, e.g., D = 263 in the HumanML3D (Guo et al.,"
        },
        {
            "heading": "Diffuse T times",
            "text": "2022a) dataset. The spatial constraints c consist of the xyz positions of each joint at each frame. In practice, only a subset of joints\u2019 locations are provided as spatial constraints for human motion generation, where the positions of the rest of the joints are simply set to zero. This form of definition enables us to flexibly specify the controlling joints and thus control the motion generation for any joint at any time (keyframe). We first provide a brief overview of the text-prompt human motion generation model based on the diffusion process in Sec. 3.1. We then introduce our proposed approach to incorporate the spatial control signal c into the generation model in Sec. 3.2."
        },
        {
            "heading": "3.1 BACKGROUND: HUMAN MOTION GENERATION WITH DIFFUSION MODELS",
            "text": "Diffusion process for human motion generation. Diffusion models have demonstrated excellent results in text-to-image generation (Rombach et al., 2022; Saharia et al., 2022; Ramesh et al., 2021). Tevet et al. (2023) extend it to the human generation to simultaneously synthesize all human poses in a motion sequence. The model learns the reversed diffusion process of gradually denoising xt starting from the pure Gaussian noise xT\nP\u03b8(xt\u22121|xt,p) = N (\u00b5t(\u03b8), (1\u2212 \u03b1t)I), (1)\nwhere xt \u2208 RN\u00d7D denotes the motion at the tth noising step and there are T diffusion denoising steps in total. \u03b1t \u2208 (0, 1) are hyper-parameters, which should gradually decrease to 0 at later steps. Following Tevet et al. (2023), instead of predicting the noise at each diffusion step, our model directly predicts the final clean motion x0(\u03b8) = M(xt, t,p; \u03b8)1 where M is the motion generation model with parameters \u03b8. The mean \u00b5t(\u03b8) can be computed following Nichol & Dhariwal (2021) \u00b5t(\u03b8) = \u221a \u03b1\u0304t\u22121\u03b2t 1\u2212\u03b1\u0304t x0(\u03b8) + \u221a \u03b1t(1\u2212\u03b1\u0304t\u22121) 1\u2212\u03b1\u0304t xt, where \u03b2t = 1 \u2212 \u03b1t and \u03b1\u0304t = \u220ft\ns=0 \u03b1s. We omit \u03b8 for brevity and simply use x0 and \u00b5t in the rest of the paper. The model parameters \u03b8 are optimized to minimize the objective \u2225x0 \u2212 x\u22170\u222522, where x\u22170 is the ground-truth human motion sequence.\nHuman pose representations. In human motion generation, the redundant data representations suggested by Guo et al. (2022a) are widely adopted (Tevet et al., 2023) which include pelvis velocity, local joint positions, velocities and rotations of other joints in the pelvis space, as well as the foot contact binary labels. Generally, the pelvis locations are represented as relative positions w.r.t. the previous frame, and the locations of other joints are defined as relative positions w.r.t. the pelvis. Such representations are easier to learn and can produce realistic human motions. However, their relative nature makes inpainting-based controlling methods (Tevet et al., 2023) struggle to handle sparse constraints on the pelvis and incorporate any spatial control signal on joints other than the pelvis. For instance, to input the global pelvis position specified in the control signal to the keyframe, the global pelvis position needs to be converted to a relative location w.r.t the preceding frame. Similarly, to input positions of other joints, such as the left hand, a conversion of the global hand position w.r.t. the relative location of pelvis is required. However, in both cases, the relative positions of the pelvis do not exist and are yet to be generated by the model. Some approaches use the generated motion in-between the diffusion process to perform the conversion to enforce the spatial constraints. But relying on the generated pelvis positions for these conversions can sometimes\n1Strictly speaking, it should be written as x0(xt, t,p; \u03b8) = M(xt, t,p; \u03b8). So should \u00b5t(\u03b8). We slightly abuse the notations here for brevity, highlighting their dependence on the model parameters \u03b8.\nyield unreasonable velocities or leg lengths, culminating along the generation process, which lead to unnatural generated motions. We still use the relative representations as input. To address the aforementioned limitation, we convert the relative representations to global ones in our proposed spatial guidance, allowing flexible control of any joints at any time, which will be introduced in detail in the next section."
        },
        {
            "heading": "3.2 MOTION GENERATION WITH FLEXIBLE SPATIAL CONTROL SIGNAL",
            "text": "When the text prompt p and spatial control signal c are given together, how to ensure the generated motions adhere to both of them while remaining realistic is key to producing plausible motions. In this section, we will introduce our spatial and realism guidance to fulfill such an objective.\nSpatial guidance. The architecture of spatial guidance is shown in Fig. 3. The core of our spatial guidance is an analytic function G(\u00b5t, c) that assesses how closely the joint of the generated motion aligns with a desired spatial location c. Following Dhariwal & Nichol (2021), the gradient of the analytic function is utilized to guide the generated motions in the desired direction. We employ the spatial guidance to perturb the predicted mean at every denoising step t2\n\u00b5t = \u00b5t \u2212 \u03c4\u2207\u00b5tG(\u00b5t, c), (2)\nwhere \u03c4 controls the strength of the guidance. G measures the L2 distance between the joint location of the generated motion and the spatial constraints:\nG(\u00b5, c) =\n\u2211 n \u2211 j \u03c3nj \u2225\u2225cnj \u2212 \u00b5gnj\u2225\u22252\u2211 n \u2211 j \u03c3nj , \u00b5g = R(\u00b5), (3)\nwhere \u03c3nj is a binary value indicating whether the spatial control signal c contains a valid value at frame n for joint j. R(\u00b7) converts the joint\u2019s local positions to global absolute locations. For simplicity, we omit the diffusion denoising step t here. In this context, the global location of the pelvis at a specific frame can be determined through cumulative aggregation of rotations and translations from all the preceding frames. The locations of the other joints can also be ascertained through the aggregation of the pelvis position and the relative positions of the other joints.\nUnlike existing approaches that convert global control signals to the relative locations w.r.t. the pelvis, which are non-existent or not accurate in-between the diffusion process, we propose to convert the generated motion to global coordinates. It eliminates ambiguity and thus empowers the model to incorporate flexible control signals over any joint at any time. Note that we still use the local human pose representations as the model\u2019s input and output. Consequently, the control signal is effective for all previous frames beyond the keyframe of the control signal as the gradients can be backpropagated to them, enabling the spatial guidance to densely perturb the motions even when the spatial constraints are extremely sparse. Moreover, as the positions of the remaining joints are relative to the pelvis position, spatial constraints applied to other joints can also influence the gradients on the pelvis position of previous frames. This property is desired. For instance, when one intends to reach for an object with a hand, adjustment of the pelvis position is usually needed, which would otherwise lead to unreasonable arm lengths in the generated motion. Note, however, that spatial control signals applied to the pelvis will not affect other joints. We address this problem using the realism guidance introduced below.\nOur proposed spatial guidance is more effective than the classifier guidance used in other motion generation works (Rempe et al., 2023; Kulkarni et al., 2023; Karunratanakul et al., 2023) in terms of control accuracy. The key advantage lies in the fact that the gradient is calculated w.r.t the predicted mean \u00b5t, which only needs backpropagation through the lightweight function in Eq.(3). In contrast, previous works train a classifier or reward function and need gradient backpropagation through\n2We should note that the denoising step t should be distinguished from the frame number n.\na heavier model (e.g., the entire motion diffusion model or a classifier), which is notably timeintensive. Thus, they guide the generated motion only once at each denoising diffusion step to maintain efficiency, which typically falls short of achieving the desired objective. Instead, we can afford to perturb the generated motion sequence for multiple times, largely improving the control accuracy. Specifically, we perturb \u00b5t by applying Eq.(2) iteratively for K times at the denoising step t, which is set dynamically to balance the control accuracy and inference speed:\nK = { Ke if Ts \u2264 t \u2264 T, Kl if t \u2264 Ts. (4)\nWe use Ke = 10, Kl = 500, and Ts = 10 in our experiments. In the early diffusion steps when t \u2264 Ts, the generated motion is of low quality. We enforce the spatial guidance for a small number of iterations. Later, as the quality of the motion improves when the diffusion step t is large, intensive perturbations will be performed. The ablation study in Sec. 4.2 validates our design.\nRealism guidance. Even though the spatial guidance can effectively enforce the controlled joints to adhere to the input control signals, it may leave other joints unchanged. For example, if we only control the pelvis position, the gradients of spatial guidance cannot be backpropagated to other joints due to the nature of the relative human pose representations and thus have no effect on other joints, as we mentioned earlier. It will lead to unrealistic motions. Moreover, since the perturbed position is only a small part of the whole motion, the motion diffusion model may ignore the change from the spatial guidance and fail to make appropriate modifications for the rest of the human joints, leading to incoherent human motion and foot sliding, as shown in Fig. 5 (b).\nTo address this issue, inspired by Zhang et al. (2023b), we propose realism guidance. Specifically, it is a trainable copy of the Transformer encoder in the motion diffusion model to learn to enforce the spatial constraints. The architecture of realism guidance is shown in Fig 4. The realism guidance takes in the same textual prompt p as the motion diffusion model, as well as the spatial control signal c. Each of the Transformer layers in the original model and the new trainable copy are connected by a linear layer with both weight and bias initialized with zeros, so they have no effect of controlling at the beginning. As the training goes on, the realism guidance model learns the spatial constraints and adds the learned feature corrections to the corresponding layers in the motion diffusion model to amend the generated motions implicitly.\nWe use a spatial encoder F to encode the spatial control signals c at each frame independently, as shown in Fig. 4. To effectively handle the sparse control signals in time, we mask out the features at frames where there are no valid control signals, fn = onF (cn). on is a binary label that is an aggregation of \u03c3nj in Eq.(3) such that on is 1 (valid) if any of {\u03c3nj}Jj=1 is 1. Otherwise, it is 0 (invalid). fn are the features of spatial control signals at frame n, which are fed into the trainable copy of the Transformer. This helps the following attention layers know where the valid spatial control signals are and thus amend the corresponding features.\nCombination of spatial and realism guidance. These two guidance are complementary in design, and both of them are necessary. The spatial guidance can change the position of corresponding control joints as well as the pelvis position to make the generated motion fulfill the spatial constraints. But it usually fails to amend the position of other joints that cannot receive the gradients, producing unreal and physically implausible motions. At the same time, although the realism guidance alone cannot ensure the generated motion tightly follows the spatial control signals, it amends the whole-body motion well, making up for the critical problem of spatial guidance. The combination of spatial guidance and realism guidance can effectively balance realistic human motion generation and the accuracy of incorporating spatial constraints. We ablate these two guidance in Sec. 4.2."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": "Datasets. We experiment on the popular HumanML3D (Guo et al., 2022a) dataset which contains 14,646 text-annotate human motion sequences from AMASS (Mahmood et al., 2019) and HumanAct12 (Guo et al., 2020) datasets. We also evaluate our method on the KIT-ML (Plappert et al., 2016) dataset with 3,911 sequences.\nEvaluation methods. We adopt the evaluation protocol from Guo et al. (2022a). Fre\u0301chet Inception Distance (FID) reports the naturalness of the generated motion. R-Precision evaluates the relevancy of the generated motion to its text prompt, while Diversity measures the variability within the generated motion. In order to evaluate the controlling performance, following Karunratanakul et al. (2023), we report foot skating ratio as a proxy for the incoherence between trajectory and human motion and physical plausibility. We also report Trajectory error, Location error, and Average error of the locations of the controlled joints in the keyframes to measure the control accuracy. We provide detailed information of other metrics in the Appendix A.10. All the models are trained to generate 196 frames in our evaluation, where we use 5 sparsity levels in the controlling signal, including 1, 2, 5, 49 (25% density), and 196 keyframes (100% density). The time steps of keyframes are randomly sampled. We report the average performance over all density levels. In both training and evaluation, all models are provided with ground-truth trajectories as the spatial control signals."
        },
        {
            "heading": "4.1 COMPARISON TO OTHER METHODS",
            "text": "Since all previous methods, MDM (Tevet et al., 2023), PriorMDM (Shafir et al., 2023), and GMD (Karunratanakul et al., 2023) focus on controlling the pelvis only, we report the pelvis controlling performance for fair comparisons (Joint: Pelvis). All of these existing methods use the same pose representations and thus inherit the limitations detailed in 3.1. As a result, they only accept spatial constraints that are dense and over the pelvis alone. GMD changes the pelvis location from relative representation to absolute global ones so it can handle sparse control signals over the pelvis via a two-stage design. However, GMD only takes care of the pelvis location of the human body on the ground plane (xz positions). We retrain GMD to handle the full position of the pelvis (xyz position) to fairly compare with our method.\nThe top part in Table 1 reports the comparisons of different methods on the HumanML3D dataset. Our method consistently outperforms all existing methods in the pelvis control over all metrics in terms of both realism and control accuracy. In particular, our approach has a significant reduction of 54.1% in terms of FID compared to PriorMDM, proving that our proposed hybrid guidance generates much more realistic motions. Our method also surpasses the previous state-of-the-art method GMD by reducing Avg. err. of 79.2%. In addition, our foot skating ratio is the lowest compared to all other methods. We provide the complete table in the appendix.\nMore importantly, unlike previous approaches that can control the pelvis alone, our model can control over all joints using a single model. In the second part of Table 1, we report the performance in controlling each joint, where we consider pelvis, left foot, right foot, head, left wrist, and right wrist, given their common usage in the interactions with objects and the surrounding scene. We can see our model can achieve comparable performance in controlling the pelvis with only one model compared to the model specifically trained for controlling the pelvis only. The third part of Table 1 also show that the average controlling performance of each joint (Joint: Average) are comparable to the pelvis on both the HumanML3D and KIT-ML datasets. This largely simplifies the model training and usage, and provides more flexibility in controlling human motion generation. In the last row (Joint: Cross), we report the performance over the cross combination of joints. We randomly sample one possible combination for each sample during the evaluation. The results on KIT-ML dataset are reported in Table 2."
        },
        {
            "heading": "4.2 ABLATION STUDIES",
            "text": "We conduct several ablation experiments on HumanML3D to validate the effectiveness of our model\u2019s design choices. We summarize key findings below.\nSpatial guidance largely improves the controlling performance. In Table 3, we compare our model (1st row) to a variant without any spatial guidance, w/o spatial guidance (2nd row) to show its effectiveness. The model with spatial guidance performs much better across all metrics of control accuracy (Traj. err., Loc. err., and Avg. err.) and shows 90% decrease in Avg. err.. Fig. 5(a) validates this observation, where we can see the generated motion cannot tightly follow the spatial constraints without spatial guidance. These results show that the spatial guidance is effective.\nComputing gradients w.r.t \u00b5t is effective. In spatial guidance, we calculate the gradient w.r.t the predicted \u00b5t. To show the effectiveness of this design, we report the performance of a variant which computes the gradient w.r.t the input noised motion xt, in Table 3 Gradient w.r.t xt (last row). Following Karunratanakul et al. (2023), we only perturb the controlled joints once at each diffusion step, partially due to the long-running time of 99s. In our spatial guidance, it takes 121s to perturb the joints multiple times. Our spatial guidance produces 83.8% lower Avg. err., validating that our design is much more effective compared to the similar operations used in Karunratanakul et al. (2023). Fig. 5 (c) validates this observation.\nRealism guidance is critical for generating coherent motions. As shown in Table 3, compared to a variant without realism guidance, w/o realism guidance (3rd row), our proposed model leads to 50%\ndecrease in FID. Fig. 5(b) visualizes the generated motions when removing the realism guidance. In this case, the model cannot amend the rest of the joints by correctly fusing the information in both the input textual prompt and spatial control signals, yielding unreal and incoherent motions."
        },
        {
            "heading": "4.3 DEEPER DIVE INTO OMNICONTROL",
            "text": "Balancing the inference time and Average Error. The spatial guidance in OmniControl adopts an iterative strategy to perturb the predicted mean \u00b5t at each diffusion step. We explore the effect of varying the dynamic number iterations (Ke and Kl in Eq.(4)) in Fig. 6. We see that more iterations in the early stage of the diffusion process do not necessarily lead to better performance. So we use Ke << Kl. We vary Ts in Fig. 6 (a). When setting Ts smaller than 10, the inference time slightly drops but the Average Error increases. On the contrary, a large Ts causes a much larger inference time is much larger (121s vs 143s). So we set Ts = 10 for a trade-off. We vary Ke in Fig. 6 (b), in which large Ke (> 10) reports steady performance but much more time in inference. We vary Kl in Fig. 6 (c), where Kl = 500 is an appropriate setting to balance inference time and Average Error.\nVarying the density of the spatial signal. We report the performance of different models in different density levels in Fig. 7. Under all density levels, GMD\u2019s performance is consistently worse than ours. Regarding MDM and PriorMDM, their FID and Foot skating ratio metrics significantly increase as the density increases while ours remain stable. When the spatial control signal is dense, the Avg. error of MDM and PriorMDM are 0 because of the properties of the inpainting method. However, substantially high FID and Foot skating ratio indicate that both of them cannot generate realistic motions and fail to ensure the coherence between the controlling joints and the rest, resulting in physically implausible motions. It can be clearly seen that our method is significantly more robust to different density levels than existing approaches.\nControlling multiple joints together enables downstream applications. We demonstrate the OmniControl can employ a single model to support controlling multiple joints together. This new capability enables a set of downstream applications, as shown in Fig. 1 (last column), which supports correctly connecting isolated human motion with the surrounding objects and scenes."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "We presented OmniControl, an effective method that controls any joint of humans at any time for text-based human motion generation. OmniControl works by combining the spatial and realism guidance that are highly complementary, enabling realistic human motion generation while conforming to the input spatial control signals. Extensive experimental results and ablation studies on HumanML3D and KIT-ML datasets are provided to validate the effectiveness of OmniControl."
        },
        {
            "heading": "A APPENDIX",
            "text": ""
        },
        {
            "heading": "A.1 PSEUDO CODE",
            "text": "Algorithm 1 OmniControl\u2019s inference Require: A motion diffusion model M , a realism guidance model S, spatial control signals c (if\nany), text prompts p (if any). 1: xT \u223c N (0, I) 2: for all t from T to 1 do 3: {f} \u2190 S(xt, t,p, c;\u03d5) # Realism guidance model 4: x0 \u2190M(xt, t,p, {f}; \u03b8) # Model diffusion model 5: \u00b5t,\u03a3t \u2190 \u00b5(x0,xt),\u03a3t 6: for all k from 1 to K do # Spatial guidance 7: \u00b5t = \u00b5t \u2212 \u03c4\u2207\u00b5tG(\u00b5t, c) 8: end for 9: xt\u22121 \u223c N (\u00b5t,\u03a3t)\n10: end for 11: return x0"
        },
        {
            "heading": "A.2 MORE IMPLEMENTATION DETAILS",
            "text": "Training details. We implemented our model using Pytorch with training on 1 NVIDIA A5000 GPU. Batch size b = 64. We use AdamW optimizer (Loshchilov & Hutter, 2017), and the learning rate is 1e\u2212 5. It takes 29 hours to train on a single A5000 GPU with 250,000 iterations in total. We compare our method to the baselines in terms of training time in Table 4.\nModel details. Our baseline motion diffusion model is based on MDM (Tevet et al., 2023). Both the motion diffusion model and realism guidance model resume the pretrain weights from Tevet et al. (2023) and are fine-tuned together. Similar to MDM, we use the CLIP (Radford et al., 2021) model to encode text prompts and the generation process is in a classifier-free (Ho & Salimans, 2021) manner. The input process in Fig. 4 mainly consists of a CLIP-based (Radford et al., 2021) textual embedding to encode the text prompt and linear layers to encode the noisy motion. Then, the encoded text and encoded noisy motion will be concatenated as the input to the self-attention layers. The spatial encoder in Fig. 4 consists of four linear layers to encode the spatial control signals. The dimension of encoded spatial control signals fn is (L, B, C), where L = 196 is the sequence length, B is the batch size, and C = 512 is the feature dimension. The spatial guidance is also used in training time. In the training stage, the prompt p is randomly masked for classifier-free learning (Ho & Salimans, 2021). We utilize DDPM (Ho et al., 2020) with T = 1000 denoising steps. The control strength \u03c4 = 20\u03a3\u0302tV , where V is the number of frames we want to control (density) and \u03a3\u0302t = min(\u03a3t, 0.01).\nExperiment details. Tevet et al. (2023); Shafir et al. (2023) naturally cannot handle the sparse control signals due to the relative pelvis representation detailed in 3.1 in the main paper. To conduct these two methods with sparse control signals, we insert the ground truth velocity at specific times.\nA.3 INFERENCE TIME\nWe report the inference time of our submodules, our full pipeline, and baseline methods in Table 5. The inference time is measured on an NVIDIA A5000 GPU. When comparing to GMD, we use the inference time reported in its paper."
        },
        {
            "heading": "A.4 MORE DISCUSSIONS ABOUT CONTROLLING MULTIPLE JOINTS",
            "text": "We report the quantitative results of controlling multiple joints in the last row of Table 1. There are 57 possible combinations for six types of joints. Since running an evaluation for each of them is costly, it\u2019s impractical to evaluate all the combinations. Instead, we randomly sample one possible combination for each motion sequence for evaluation. The performance is lower compared to the single-joint control (not an apple-to-apple comparison as the ground-truths are different). Nevertheless, the results show that controlling multiple joints is harder than a single one due to the increased degrees of freedom."
        },
        {
            "heading": "A.5 LIMITATIONS AND FUTURE PLAN",
            "text": "The problem with our approach is that there are still a lot of foot skating cases. We show the failure cases in the supplementary video. The realism guidance is not a perfect module when used to amend the whole-body motion from the input spatial control signals. We are interested in exploring more effective designs to improve the realism and physical plausibility of human motion generation. In addition, some physical constraints (Yuan et al., 2023) can be used to reduce the foot skating ratio.\nAnother significant limitation of the diffusion approach arises from its extended inference time, necessitating roughly 1,000 forward passes to produce a single result. As diffusion models persist in their development (Lu et al., 2022; Lyu et al., 2022; Salimans & Ho, 2022), we are inclined to explore, in future work, strategies to expedite computational speed.\nAlthough OmniControl can be used to control multiple joints without other special designs or finetuning, in some cases when the spatial control signals for two joints are conflicted, our method usually produces unnatural motions. We will explore more to improve this problem in future work."
        },
        {
            "heading": "A.6 WHY DIDN\u2019T WE USE GLOBAL POSE REPRESENTATION FOR HUMAN MOTION GENERATION",
            "text": "The human pose representation suggested by Guo et al. (2022a) is easier to learn and produce realistic human motions because it leverages the human skeleton prior. However, this representation is not friendly for inpainting-based methods, detailed in 3.1. One question is whether we can use the global representation for all joints of humans. We try to train the MDM (Tevet et al., 2023) using the global representation, in which the human pose is represented with the global position of joints. In this case, D = 66 (22 joints) on HumanML3D dataset or D = 63 (21 joints) on KIT-ML dataset. We found the model cannot converge, and produce unreasonable human poses, as shown in Fig. 8.\nA.7 WHY DIDN\u2019T WE USE THE GLOBAL POSE REPRESENTATION PROPOSED IN LIANG ET AL. (2023)\nLiang et al. (2023) propose a non-canonicalization representation for multi-person interaction motion. Instead of transforming joint positions and velocities to the root frame, they keep them in the\nworld frame. To avoid the unreasonable human poses shown in A.6, they introduce bone length loss to constrain the global joint positions to satisfy skeleton consistency, which implicitly encodes the human body\u2019s kinematic structure. We train MDM (Tevet et al., 2023) with this global representation and bone length loss, and report the performance in Tab. 6. There is a significant drop in performance when converted to global representation for text-based human motion generation. Therefore, global coordinates are not an optimal choice for our task."
        },
        {
            "heading": "A.8 WHY DIDN\u2019T WE REPORT THE FOOT SKATING RATIO ON KIT-ML DATASET",
            "text": "The data quality of KIT-ML is relatively low. The foot height on KIT-ML is not necessarily close to 0, even when the foot is on the ground, and there are a lot of foot skating cases in the ground truth motions. We, therefore do not evaluate the skating ratio on KIT-ML because it cannot be evaluated accurately."
        },
        {
            "heading": "A.9 MORE DISCUSSION ABOUT FIG. 7",
            "text": "In Fig. 7 (a), the higher density leads to lower FID and Foot skating ratio for ours and GMD, while it\u2019s the opposite for the other two methods (MDM and PriorMDM). Ideally, higher density should lead to better performance if the generated motion could accurately follow the control signal, and make corresponding adjustments to the other joints to make the motion realistic and natural. This is not the case for MDM and PriorMDM since they cannot effectively modify whole-body motion according to the input control signal. When the density is higher (the constraint is stricter) and other joints are NOT adjusted effectively to compensate for the more rigidity in the control signal, they will produce unnatural results, thus leading to higher FID and higher foot skating ratio. On the contrary, GMD and ours, which are specially designed for both text and spatial control signal conditions, can efficiently adjust the whole-body motion and better leverage the context information in the input signal, yielding better performance when the density is higher."
        },
        {
            "heading": "In Fig. 7 (b), the Avg. error of MDM and PriorMDM is zero when using dense control signals.",
            "text": "The Avg. error of MDM and PriorMDM is zero due to the inpainting property. Inpainting-based methods aim to reconstruct the rest of joint motions based on the given control signals over one or more control joints. The input control signals won\u2019t be changed during this process, i.e., the output motion over the control joints remains the same as the input control signal. As a result, the Avg. error is zero."
        },
        {
            "heading": "A.10 DETAILED INFORMATION OF THE METRICS",
            "text": "Fre\u0301chet Inception Distance (FID) reports the naturalness of the generated motion. R-Precision evaluates the relevancy of the generated motion to its text prompt, while Diversity measures the\nvariability within the generated motion. In order to evaluate the controlling performance, following Karunratanakul et al. (2023), we report foot skating ratio as a proxy for the incoherence between trajectory and human motion and physical plausibility. It measures the proportion of frames in which either foot skids more than a certain distance (2.5 cm) while maintaining contact with the ground (foot height < 5 cm). We also report Trajectory error, Location error, and Average error of the locations of the controlled joints in the keyframes to measure the control accuracy. Trajectory error is the ratio of unsuccessful trajectories, defined as those with any keyframe location error exceeding a threshold. Location error is the ratio of keyframe locations that are not reached within a threshold distance. Average error measures the mean distance between the generated motion locations and the keyframe locations measured at the keyframe motion steps."
        },
        {
            "heading": "A.11 THE SOURCE OF THE SPATIAL CONTROL SIGNAL",
            "text": "In both training and evaluation, all models are provided with ground-truth trajectories as the spatial control signals. In the visualizations or video demos, the spatial control signals are manually designed. In the downstream applications, we envision there may be two ways to collect the spatial guidance trajectories. First, for practical applications in industries such as 3D animation, the user (designer) may provide such guidance trajectories as part of the application development. Second, the spatial guidance may be from the interactions and constraints of the scene/object. For example, the height of the ceiling or the position of the chair (and thus the position of the controlled joint), as we show in the last column of Fig. 1 in the paper. In both cases, the spatial guidance trajectories can be efficiently collected."
        },
        {
            "heading": "A.12 THE SOURCE OF OBJECT MODELS USED IN THE LAST COLUMN OF FIG. 1",
            "text": "All these object models are licensed under Creative Commons Attribution 4.0 International License. Specifically, Chair: external link; Ceiling fan: external link. Handrail: external link."
        },
        {
            "heading": "A.13 ALL EVALUATION RESULTS",
            "text": "In Table 7 and Table 8, we first present the detailed performance of OmniControl across five sparsity levels, which is trained for pelvis control on the HumanML3D and KIT-ML test set. Subsequently, in Table 10, and Table 9, we showcase the comprehensive results of OmniControl in controlling various joints (pelvis, left foot, right foot, head, left wrist, and right wrist). BUN in Table 9 means body upper neck. As shown in table 10, the Traj. err. and Loc. err. are sometimes zeros when the density is low since the definitions of these two metrics are not strict. As discussed in A.10, Traj. err. (50 cm) is the ratio of unsuccessful trajectories. The unsuccessful trajectories are defined as the trajectories with any keyframe whose location error exceeds a threshold (50 cm). And Loc. err. (50 cm) is the ratio of unsuccessful keyframes whose location error exceeds a threshold (50 cm). When the density is low (e.g., only have spatial control signal in one keyframe), it is easier for all samples to meet this threshold and thus achieve zero errors."
        }
    ],
    "year": 2023
}