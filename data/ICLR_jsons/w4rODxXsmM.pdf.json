{
    "abstractText": "Reinforcement learning (RL) presents a promising framework to learn policies through environment interaction, but often requires an infeasible amount of interaction data to solve complex tasks from sparse rewards. One direction includes augmenting RL with offline data demonstrating desired tasks, but past work often require a lot of high-quality demonstration data that is difficult to obtain, especially for domains such as robotics. Our approach consists of a reverse curriculum followed by a forward curriculum. Unique to our approach compared to past work is the ability to efficiently leverage more than one demonstration via a per-demonstration reverse curriculum generated via state resets. The result of our reverse curriculum is an initial policy that performs well on a narrow initial state distribution and helps overcome difficult exploration problems. A forward curriculum is then used to accelerate the training of the initial policy to perform well on the full initial state distribution of the task and improve demonstration and sample efficiency. We show how the combination of a reverse curriculum and forward curriculum in our method, RFCL, enables significant improvements in demonstration and sample efficiency compared against various state-of-the-art learning-from-demonstration baselines, even solving previously unsolvable tasks that require high precision and control. Website with code and visualizations are here: https://reverseforward-cl.github.io/",
    "authors": [],
    "id": "SP:93b161c982ed537065504ca5cf4fd8ae0fceff11",
    "references": [
        {
            "authors": [
                "Dario Amodei",
                "Chris Olah",
                "Jacob Steinhardt",
                "Paul F. Christiano",
                "John Schulman",
                "Dan Man\u00e9"
            ],
            "title": "Concrete problems in AI safety",
            "venue": "CoRR, abs/1606.06565,",
            "year": 2016
        },
        {
            "authors": [
                "Christopher G. Atkeson",
                "Stefan Schaal"
            ],
            "title": "Robot learning from demonstration",
            "venue": "Proceedings of the Fourteenth International Conference on Machine Learning (ICML",
            "year": 1997
        },
        {
            "authors": [
                "Philip J. Ball",
                "Laura M. Smith",
                "Ilya Kostrikov",
                "Sergey Levine"
            ],
            "title": "Efficient online reinforcement learning with offline data",
            "venue": "International Conference on Machine Learning,",
            "year": 2023
        },
        {
            "authors": [
                "Xinyue Chen",
                "Che Wang",
                "Zijian Zhou",
                "Keith W. Ross"
            ],
            "title": "Randomized ensembled double qlearning: Learning fast without a model",
            "venue": "In 9th International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Yuanpei Chen",
                "Chen Wang",
                "Li Fei-Fei",
                "C. Karen Liu"
            ],
            "title": "Sequential dexterity: Chaining dexterous policies for long-horizon manipulation",
            "venue": "In Conference on Robot Learning,",
            "year": 2023
        },
        {
            "authors": [
                "Adrien Ecoffet",
                "Joost Huizinga",
                "Joel Lehman",
                "Kenneth O. Stanley",
                "Jeff Clune"
            ],
            "title": "Go-explore: a new approach for hard-exploration problems",
            "venue": "CoRR, abs/1901.10995,",
            "year": 2019
        },
        {
            "authors": [
                "Carlos Florensa",
                "David Held",
                "Markus Wulfmeier",
                "Michael Zhang",
                "Pieter Abbeel"
            ],
            "title": "Reverse curriculum generation for reinforcement learning",
            "venue": "In 1st Annual Conference on Robot Learning, CoRL 2017,",
            "year": 2017
        },
        {
            "authors": [
                "C. Daniel Freeman",
                "Erik Frey",
                "Anton Raichuk",
                "Sertan Girgin",
                "Igor Mordatch",
                "Olivier Bachem"
            ],
            "title": "Brax - A differentiable physics engine for large scale rigid body simulation",
            "venue": "Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks",
            "year": 2021
        },
        {
            "authors": [
                "Jiayuan Gu",
                "Fanbo Xiang",
                "Xuanlin Li",
                "Zhan Ling",
                "Xiqiang Liu",
                "Tongzhou Mu",
                "Yihe Tang",
                "Stone Tao",
                "Xinyue Wei",
                "Yunchao Yao",
                "Xiaodi Yuan",
                "Pengwei Xie",
                "Zhiao Huang",
                "Rui Chen",
                "Hao Su"
            ],
            "title": "Maniskill2: A unified benchmark for generalizable manipulation skills",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Tuomas Haarnoja",
                "Aurick Zhou",
                "Pieter Abbeel",
                "Sergey Levine"
            ],
            "title": "Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor",
            "venue": "Proceedings of the 35th International Conference on Machine Learning,",
            "year": 2018
        },
        {
            "authors": [
                "Nicklas Hansen",
                "Yixin Lin",
                "Hao Su",
                "Xiaolong Wang",
                "Vikash Kumar",
                "Aravind Rajeswaran"
            ],
            "title": "Modem: Accelerating visual model-based reinforcement learning with demonstrations",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Ionel-Alexandru Hosu",
                "Traian Rebedea"
            ],
            "title": "Playing atari games with deep reinforcement learning and human checkpoint",
            "venue": "replay. CoRR,",
            "year": 2016
        },
        {
            "authors": [
                "Boris Ivanovic",
                "James Harrison",
                "Apoorva Sharma",
                "Mo Chen",
                "Marco Pavone"
            ],
            "title": "Barc: Backward reachability curriculum for robotic reinforcement learning",
            "venue": "In International Conference on Robotics and Automation,",
            "year": 2019
        },
        {
            "authors": [
                "Minqi Jiang",
                "Edward Grefenstette",
                "Tim Rockt\u00e4schel"
            ],
            "title": "Prioritized level replay",
            "venue": "Proceedings of the 38th International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Ilya Kostrikov",
                "Ashvin Nair",
                "Sergey Levine"
            ],
            "title": "Offline reinforcement learning with implicit qlearning",
            "venue": "In The Tenth International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Richard Li",
                "Allan Jabri",
                "Trevor Darrell",
                "Pulkit Agrawal"
            ],
            "title": "Towards practical multi-object manipulation using relational reinforcement learning",
            "venue": "IEEE International Conference on Robotics and Automation,",
            "year": 2020
        },
        {
            "authors": [
                "Viktor Makoviychuk",
                "Lukasz Wawrzyniak",
                "Yunrong Guo",
                "Michelle Lu",
                "Kier Storey",
                "Miles Macklin",
                "David Hoeller",
                "Nikita Rudin",
                "Arthur Allshire",
                "Ankur Handa",
                "Gavriel State"
            ],
            "title": "Isaac gym: High performance gpu-based physics simulation for robot learning, 2021",
            "year": 2021
        },
        {
            "authors": [
                "Ajay Mandlekar",
                "Danfei Xu",
                "Josiah Wong",
                "Soroush Nasiriany",
                "Chen Wang",
                "Rohun Kulkarni",
                "Li FeiFei",
                "Silvio Savarese",
                "Yuke Zhu",
                "Roberto Mart\u0131\u0301n-Mart\u0131\u0301n"
            ],
            "title": "What matters in learning from offline human demonstrations for robot manipulation",
            "venue": "Conference on Robot Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Ashvin Nair",
                "Bob McGrew",
                "Marcin Andrychowicz",
                "Wojciech Zaremba",
                "Pieter Abbeel"
            ],
            "title": "Overcoming exploration in reinforcement learning with demonstrations",
            "venue": "IEEE International Conference on Robotics and Automation,",
            "year": 2018
        },
        {
            "authors": [
                "Mitsuhiko Nakamoto",
                "Yuexiang Zhai",
                "Anikait Singh",
                "Max Sobol Mark",
                "Yi Ma",
                "Chelsea Finn",
                "Aviral Kumar",
                "Sergey Levine"
            ],
            "title": "Cal-ql: Calibrated offline RL pre-training for efficient online finetuning",
            "venue": "CoRR, abs/2303.05479,",
            "year": 2023
        },
        {
            "authors": [
                "Sanmit Narvekar",
                "Bei Peng",
                "Matteo Leonetti",
                "Jivko Sinapov",
                "Matthew E. Taylor",
                "Peter Stone"
            ],
            "title": "Curriculum learning for reinforcement learning domains: A framework and survey",
            "venue": "J. Mach. Learn. Res.,",
            "year": 2020
        },
        {
            "authors": [
                "Xue Bin Peng",
                "Pieter Abbeel",
                "Sergey Levine",
                "Michiel van de Panne"
            ],
            "title": "Deepmimic: exampleguided deep reinforcement learning of physics-based character skills",
            "venue": "ACM Trans. Graph.,",
            "year": 1975
        },
        {
            "authors": [
                "Xue Bin Peng",
                "Aviral Kumar",
                "Grace Zhang",
                "Sergey Levine"
            ],
            "title": "Advantage-weighted regression: Simple and scalable off-policy reinforcement learning",
            "venue": "CoRR, abs/1910.00177,",
            "year": 2019
        },
        {
            "authors": [
                "Ivaylo Popov",
                "Nicolas Heess",
                "Timothy P. Lillicrap",
                "Roland Hafner",
                "Gabriel Barth-Maron",
                "Matej Vecer\u0131\u0301k",
                "Thomas Lampe",
                "Yuval Tassa",
                "Tom Erez",
                "Martin A. Riedmiller"
            ],
            "title": "Data-efficient deep reinforcement learning for dexterous manipulation",
            "venue": "CoRR, abs/1704.03073,",
            "year": 2017
        },
        {
            "authors": [
                "Aravind Rajeswaran",
                "Vikash Kumar",
                "Abhishek Gupta",
                "Giulia Vezzani",
                "John Schulman",
                "Emanuel Todorov",
                "Sergey Levine"
            ],
            "title": "Learning Complex Dexterous Manipulation with Deep Reinforcement Learning and Demonstrations",
            "venue": "In Proceedings of Robotics: Science and Systems (RSS),",
            "year": 2018
        },
        {
            "authors": [
                "Cinjon Resnick",
                "Roberta Raileanu",
                "Sanyam Kapoor",
                "Alex Peysakhovich",
                "Kyunghyun Cho",
                "Joan Bruna"
            ],
            "title": "Backplay: \u201dman muss immer umkehren",
            "venue": "CoRR, abs/1807.06919,",
            "year": 2018
        },
        {
            "authors": [
                "Tim Salimans",
                "Richard Chen"
            ],
            "title": "Learning montezuma\u2019s revenge from a single demonstration",
            "venue": "CoRR, abs/1812.03381,",
            "year": 2018
        },
        {
            "authors": [
                "John Schulman",
                "Filip Wolski",
                "Prafulla Dhariwal",
                "Alec Radford",
                "Oleg Klimov"
            ],
            "title": "Proximal policy optimization algorithms. CoRR, abs/1707.06347, 2017",
            "year": 2017
        },
        {
            "authors": [
                "Younggyo Seo",
                "Danijar Hafner",
                "Hao Liu",
                "Fangchen Liu",
                "Stephen James",
                "Kimin Lee",
                "Pieter Abbeel"
            ],
            "title": "Masked world models for visual control",
            "venue": "Conference on Robot Learning, CoRL 2022,",
            "year": 2022
        },
        {
            "authors": [
                "Avi Singh",
                "Larry Yang",
                "Kristian Hartikainen",
                "Chelsea Finn",
                "Sergey Levine"
            ],
            "title": "End-to-end robotic reinforcement learning without reward engineering",
            "venue": "Robotics: Science and Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Ikechukwu Uchendu",
                "Ted Xiao",
                "Yao Lu",
                "Banghua Zhu",
                "Mengyuan Yan",
                "Jos\u00e9phine Simon",
                "Matthew Bennice",
                "Chuyuan Fu",
                "Cong Ma",
                "Jiantao Jiao",
                "Sergey Levine",
                "Karol Hausman"
            ],
            "title": "Jump-start reinforcement learning",
            "venue": "International Conference on Machine Learning,",
            "year": 2023
        },
        {
            "authors": [
                "Tianhe Yu",
                "Deirdre Quillen",
                "Zhanpeng He",
                "Ryan Julian",
                "Karol Hausman",
                "Chelsea Finn",
                "Sergey Levine"
            ],
            "title": "Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning",
            "venue": "In Conference on Robot Learning (CoRL),",
            "year": 2019
        },
        {
            "authors": [
                "Kevin Zakka",
                "Laura M. Smith",
                "Nimrod Gileadi",
                "Taylor A. Howell",
                "Xue Bin Peng",
                "Sumeet Singh",
                "Yuval Tassa",
                "Pete Florence",
                "Andy Zeng",
                "Pieter Abbeel"
            ],
            "title": "Robopianist: A benchmark for highdimensional robot control",
            "venue": "CoRR, abs/2304.04150,",
            "year": 2023
        },
        {
            "authors": [
                "Qinqing Zheng",
                "Amy Zhang",
                "Aditya Grover"
            ],
            "title": "Online decision transformer",
            "venue": "International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Yuke Zhu",
                "Ziyu Wang",
                "Josh Merel",
                "Andrei A. Rusu",
                "Tom Erez",
                "Serkan Cabi",
                "Saran Tunyasuvunakool",
                "J\u00e1nos Kram\u00e1r",
                "Raia Hadsell",
                "Nando de Freitas",
                "Nicolas Heess"
            ],
            "title": "Reinforcement and imitation learning for diverse visuomotor skills",
            "venue": "Robotics: Science and Systems XIV,",
            "year": 2018
        }
    ],
    "sections": [
        {
            "text": "1 INTRODUCTION\nThe RL paradigm enables a trial and error approach to learn behaviors from rewards. However, RL from scratch is sample-inefficient, especially in high-dimensional tasks with complex dynamics and/or long horizons. Even when training in fast simulators or highly GPU parallelized simulators (Makoviychuk et al., 2021; Freeman et al., 2021) a lack of priors means infeasible amounts of exploration and human programmed priors such as shaped reward functions or environment simplifications are required, none of which are remotely scalable.\nLearning from demonstrations has become a popular paradigm for learning complex control skills without relying on engineered dense reward signals, complex motion planning, or infeasible amounts of compute to brute force learn solutions. However, as often is the case in robotics, demonstrations are infeasible to collect at scale, making it difficult to scale learning from demonstration methods. The classical approach to lever-\naging demonstrations is through behavior cloning (Atkeson & Schaal, 1997), with more recent methods performing behavior cloning via sequence modelling (Chen et al., 2021a; Zheng et al., 2022). A recent class of methods in offline RL learn value functions from demonstrations and finetune online\n(Kostrikov et al., 2022; Nakamoto et al., 2023), allowing them to learn from more demonstrations including suboptimal ones.\nHowever, many of these methods require many demonstrations even with finetuning to solve hard tasks, which raises a critical issue in domains such as robotics since robotics demonstration data is scarce. Even human demonstrations, while flexible to collect, are not only difficult to collect at scale but also exhibit undesirable properties like non-Markovianess and sub-optimality making them complicated to learn from (Mandlekar et al., 2021). Furthermore, these methods still cannot overcome the problem of exploration when it comes to sparse reward tasks, where achieving the reward in online training is infrequent, or when the task has a very wide state distribution. Such difficult tasks include long-horizon tasks (Zakka et al., 2023), highly randomized and high-precision robot manipulation tasks (Gu et al., 2023), or complex dynamics such as dexterous hand tasks (Chen et al., 2023; Rajeswaran et al., 2018).\nThe main motivating question then for our work is how can we effectively and practically overcome the exploration problem with access to only a few demonstrations? To address these challenges we leverage state reset from demonstrations and apply reverse and forward curriculums to online RL for sample and demonstration-efficient learning. We show how our per-demonstration reverse curriculum can overcome the exploration problems that prevent prior works from ever solving some tasks by first training an initial weak policy capable of only solving a task from a narrow initial state distribution. We then demonstrate the sample-efficiency improvements a forward curriculum brings when finetuning the reverse curriculum-trained policy to gradually learn to solve the task under the full initial state distribution.\nOur primary contributions consist of our novel reverse forward curriculum learning (RFCL) algorithm and several key methods that accelerate RFCL, making it the most demonstration and sampleefficient model-free RL algorithm. We rigorously evaluate RFCL against several state-of-the-art baselines across 21 fully-observable manipulation tasks from 3 benchmarks: Adroit, ManiSkill2, and MetaWorld (Rajeswaran et al., 2018; Gu et al., 2023; Yu et al., 2019). Critically, we show that RFCL is the only method that can solve every main task from just 5 demonstrations or less with strong sample-efficiency in addition to fast wall-time results. Through ablations, we show how reverse and forward curriculums enable sample and demonstration-efficient learning."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Learning from Demonstrations: Offline collected demonstrations provide an avenue for accelerating policy learning without excessive online interactions. The classical behavior cloning approach is one method that tries to clone the exact behavior of the agent that generated the demonstrations via supervised learning (Atkeson & Schaal, 1997) and recently via sequence modelling (Chen et al., 2021a). Offline RL is another approach to imitate demonstrations without any online interactions. However, both BC and offline RL tend to be incapable of achieving high success rates/returns due to being limited to the quality and diversity of the demonstrations. Some offline RL methods address this via online finetuning (Kostrikov et al., 2022; Nakamoto et al., 2023).\nFinally, there are online RL solutions that incorporate demonstration data into their objective (Rajeswaran et al., 2018; Ball et al., 2023). However, when demonstration data is extremely limited, prior approaches cannot solve complex tasks that are long-horizon, precise, and/or highly randomized due to exploration difficulty.\nCurriculum Learning: Prior work has investigated how to incorporate curriculum learning approaches to gradually increase the difficulty of a task to facilitate training. Typically, these curriculums are hand-designed in order to solve complex long-horizon problems with sparse rewards and/or little guidance from other sources of data like demonstrations (Li et al., 2020).\nAlternatively, reverse curriculums have been leveraged by initializing the agent near states that are easier to achieve meaningful rewards from, and the curriculum progresses by initializing from gradually more difficult states. Reverse curriculums can be generated via reversible dynamics (Florensa et al., 2017) or a model of backward reachability (Ivanovic et al., 2019). Some methods rollout a separate policy or demonstrator and initialize the learning agent at the end of the rollout, with some rolling out a random number of steps (Popov et al., 2017), and recently JSRL (Uchendu et al., 2023) rolling out fewer steps over time to form a reverse curriculum.\nState Reset: A number of approaches leverage state reset in order to overcome the problems of exploration by initializing the agent directly in more often difficult-to-reach states instead of relying on sampled actions to reach there. Some methods leverage hardcoded states or previously encountered states during interaction (Florensa et al., 2017; Ecoffet et al., 2019; Chen et al., 2023). Other methods use demonstrations as their source of initial states to initialize to, with some using a uniform curriculum over these initial states (Nair et al., 2018; Peng et al., 2018; Hosu & Rebedea, 2016), a hand-defined curriculum (Zhu et al., 2018), or a reverse curriculum (Resnick et al., 2018; Salimans & Chen, 2018).\nOur method is similar to some prior work in that we leverage a reverse curriculum and reset to states in demonstrations. Unique to our approach, however, is the ability to efficiently leverage more than one demonstration via a per-demonstration reverse curriculum, different to Resnick et al. (2018) which only uses one demonstration and tackles tasks with small initial state distrbutions, and different to Nair et al. (2018) which uses multiple demonstrations but applies a uniform curriculum and struggles with more difficult tasks due to exploration inefficiency. Furthermore, we propose a novel approach of coupling reverse curriculum with a forward curriculum, which enables greater demo efficiency and drastically improves sample efficiency compared to state-of-the-art baselines. Moreover, we introduce key methods that further accelerate learning in our curriculum designs not done by past work. These contributions enable our approach to even solve some previously unsolved environments from sparse rewards that even human designed dense rewards are insufficient for."
        },
        {
            "heading": "3 PRELIMINARIES",
            "text": ""
        },
        {
            "heading": "3.1 PROBLEM SETTING",
            "text": "We consider the standard Markov Decision Process (MDP) which can be described as a tuple M = (S,A,R, T , \u03c1, \u03b3) where S is the continuous state space, A is the continuous action space, R : S \u00d7 A \u2192 R is the scalar reward function, T : S \u00d7 A \u2192 S is the environment dynamics function, \u03c1 is the initial state distribution, and \u03b3 \u2208 [0, 1] is the discount factor. The goal is to learn a policy \u03c0\u03b8 : S \u2192 A parameterized by \u03b8 that maximizes the expected discounted return, namely solving max\u03b8 E\u03c0\u03b8 [ \u2211\u221e t=0 \u03b3\ntrt] where rt is the reward at timestep t. One goal in RL is sample efficiency, learning a \u03c0\u03b8 that maximizes the discounted return with as few environment interactions as possible.\nIn this work, we consider complex MDPs with sparse reward functions where +1 is given for being in a success state and 0 otherwise. This setting is motivated by how many environments in embodied AI research such as robotics are high-dimensional with difficult dynamics. Moreover, dense reward functions are non-trivial to construct and may be sub-optimal (Singh et al., 2019; Amodei et al., 2016). While sparse reward functions are desirable as they reflect directly our desired goal (e.g. high success rates), they are more difficult to learn from due to exploration. A common approach is to leverage pre-collected demonstration data to aid in overcoming the problems of exploration in sample-efficient learning under sparse rewards. We define a dataset of demonstrations D = {\u03c40, \u03c41, ..., \u03c4N} to be a set of N demonstrations where \u03c4i = (si,0, ai,0, ..., si,Ti\u22121, ai,Ti\u22121, si,Ti) is a trajectory of length Ti composed of a sequence of states and actions. In practice, the observations used by the policy \u03c0\u03b8 may be different from the actual environment state but for simplicity in this paper state also refers to observation."
        },
        {
            "heading": "3.2 CURRICULUM LEARNING",
            "text": "In RL, curriculum learning is an approach where during training, the agent is presented with gradually harder tasks, each leveraging skills from the previous easier tasks in order to accelerate learning, mimicing how humans learn (Narvekar et al., 2020). In this paper, the MDP M is changed throughout training by modifying the initial state distribution \u03c1 to facilitate a curriculum. Define Sinit to be the original initial state distribution of the MDP we wish to maximize discounted return. We can then define a curriculum as a sequence of initial state distributions (\u03c10, \u03c11, ...); each of the elements are also referred to as stages of the curriculum. Each curriculum also comes with a criterion for whether to advance to the next stage of the curriculum from \u03c1i to \u03c1i+1.\nA key novelty of our work is leveraging both reverse curriculums and forward curriculums. The reverse curriculum initial state distributions are denoted with \u03c1r and the forward curriculum with \u03c1f . In the reverse curriculum, we reset to states in the demonstrations similar to Nair et al.\n(2018). Different to prior work we auto generate a per-demonstration reverse curriculum around these demonstration states in order to accelerate learning under sparse rewards. The reverse curriculum enables sample-efficient learning of solving the MDP from the narrow set of initial states in the demonstrations {si,0}, detailed in Sec. 4.1. In the forward curriculum, we adopt a similar approach to Prioritized Level Replay (PLR) (Jiang et al., 2021) which assumes no priors about the parameterization of the state. Different to the reverse curriculum, we reset to states sampled from Sinit instead of states in demonstrations. The curriculum here is constructed in a way that prioritizes initial states that are at the edge of the ability of the policy \u03c0\u03b8. By sampling these initial states more frequently, the training focuses on learning to solve from initial states just within reach of the agent, and as the agent improves, the curriculum advances to provide more difficult initial states, enabling the agent to solve the majority of initial states in Sinit. This is detailed in Sec. 4.2, and results in our demonstration ablations in Sec. 5.2 demonstrate how the forward curriculum coupled with the reverse curriculum enables our algorithm to use fewer demonstrations than just using a reverse curriculum and significantly fewer demonstrations than prior work.\n4 REVERSE FORWARD CURRICULUM LEARNING (RFCL)\nThe goal of our method is to provide a practical and flexible algorithm that accelerates learning in complex environments with a limited number of demonstrations without shaped rewards. A visual overview of the two stage training process of RFCL is in Fig. 2. In stage 1, a reverse curriculum is generated via state resets to states in demonstrations, learning a policy to initially solve a narrow initial state distribution. The reverse curriculum and state resets help overcome difficult exploration problems by initializing the agent near success states and gradually moving it further away. The forward curriculum then generalizes the initial policy to\na larger initial state distribution without using more demonstration data by gradually sampling more difficult initial states to train on over time, enabling additional demonstration and sample efficiency. In both stages, we use the off-policy algorithm Soft Actor Critic (Haarnoja et al., 2018) with a Q-ensemble (Chen et al., 2021b), see hyperparameters in Appendix C."
        },
        {
            "heading": "4.1 STAGE 1: REVERSE CURRICULUM",
            "text": "In this stage, we start with randomly initialized actor and critic networks and train using standard RL with SAC. We further aggressively oversample a separate offline buffer consisting of all the given demonstration data, optimal and sub-optimal, as done in Hansen et al. (2023); Ball et al. (2023) during online RL by sampling 50% of data from the online buffer and the rest from the offline buffer during optimization. We further adopt a per-demonstration reverse curriculum that improves sampleefficiency compared to prior work that use other kinds of reverse curriculums or demonstration state resets. Finally, we introduce a few key methods that further accelerate reverse curriculum learning.\nPer-demonstration Reverse Curriculum Construction: We observe that not all demonstrations are similar due to different initial states and are often multi-modal, as is the case of human or motion-planned demonstrations. As a result, a curriculum for each demonstration is necessary as opposed to a curriculum constructed from all demonstrations as done in prior work. A per-demo curriculum ensures noisy information arising from the multi-modality of demonstrations do not impact the reverse curriculum of each demonstration as much.\nFor each successful demonstration \u03c4i we assign a start step ti. For each new episode during training, we first sample a demonstration \u03c4i from our dataset D with probability ti/Ti, sampling demonstra-\ntions progressing slower more frequently. Then, we sample a discrete offset value k \u223c K and reset the environment to the state si,t+k from \u03c4i, the state at the t + k timestep of demonstration \u03c4i. We make the design choice of using a geometric distribution as K.\nInitializing all ti = Ti, the horizon of \u03c4i, at the start of training, we train mostly on states that are success states as these demonstrations are successful. As a result, it is highly likely to achieve positive rewards during training and thus easy to learn. We also define a small reverse step size \u03b4 constituting the distance between stages in the curriculum. Thus, the reverse curriculum for each demonstration \u03c4i is defined as (\u03c1rTi , \u03c1 r Ti\u2212\u03b4, ..., \u03c1 r 0) where \u03c1 r t samples si,t+k with probability pK(k).\nWe now define the stage transition criterion. During training and at curriculum stage \u03c1rt , whenever we sample si,t (when k = 0), we are sampling an initial state that is at the frontier of the agent\u2019s abilities. If the last m times the agent achieved success at the end of episodes initialized at state si,t, then we progress the curriculum of demonstration \u03c4i to the next stage \u03c1rt\u2212\u03b4 . This ensures the agent only begins to tackle the task from the slightly more difficult demonstration state si,t\u2212\u03b4 provided it is already succesful enough on si,t, forming the curriculum.\nOnce all demonstration curriculums are at stage \u03c1r0, the reverse curriculum is considered complete, as now the agent can achieve a high return on all demonstration initial states si,0. We visualize how the reverse curriculum progresses in Fig. 2 and via videos on the project page. The red area demonstrates how the sampled initial states slowly reverse their way back to the first states of the two demonstrations, training the policy in reverse to perform well on the narrow initial state distribution visualized by the green region. An empirical investigation via a simple pointmaze is done as well in Sec. 5.2. We benchmark alternative curriculum choices in addition and show the sample efficiency gains of the per-demonstration reverse curriculum in Table 1.\nDynamic Episode Timelimits: Since we are often resetting to states not from the true initial state distribution Sinit but from various points in demonstrations, the time it takes to achieve success from these states as quickly as possible varies significantly. Observe that if we sample a state near a success state that yields positive reward, we need far less environment interactions to then get the positive reward compared to sampling a state farther away. This motivates the use of a dynamic timelimit depending on which state is sampled. Suppose we sample state si,t, then a simple choice is to set the episode timelimit to 1+(Ti\u2212 t)\u03d5\u22121. \u03d5 is hyperparameter for the ratio of demonstration length to episode horizon. We fix the value of \u03d5 for each environment suite and generally for better sample-efficiency \u03d5 can be set larger if the source demonstration is fairly slow (e.g. human demonstrations) and smaller if the source demonstration is fast or optimal (e.g. scripted policies). We show an ablation on dynamic epsiode timelimits on stage 1 training and demonstrate the improved sample efficiency in Table 1."
        },
        {
            "heading": "4.2 STAGE 2: FORWARD CURRICULUM",
            "text": "In this stage, we continue the training of the actor and critics from stage 1 without resetting any networks. We instead reset the online buffer and initialize a new offline buffer. Like stage 1, we continue to aggressively sample from a separate offline buffer. Different this time is that we fill the offline buffer with the online buffer of the stage 1 training, with the motivation being to ensure the value functions do not suddenly have to learn from completely unseen data and avoid unlearning phenomenon encountered by past work investigating finetuning of pretrained policies (Peng et al., 2019; Kostrikov et al., 2022). Moreover, our choice of RL algorithm is capable of learning from sub-optimal data so it is helpful to add in the data that was collected from stage 1 training instead of just using successful demonstrations. Crucial to this stage is the forward curriculum described below, which enables more demonstration-efficient learning.\nForward Curriculum Construction: Following the reverse curriculum stage, we will have trained a policy \u03c0\u03b8 that achieves a high return on the MDP given the initial state is one of the initial states of a demonstration si,0. It is critical to have the reverse curriculum as only applying a forward curriculum cannot work well in sparse reward settings due to exploration difficulty. If there do not exist initial states s0 \u2208 Sinit where it is easy to obtain any reward, the problem becomes very difficult and sample-inefficient, in addition to making it difficult to automatically generate a useful forward curriculum without relying on prior heuristics. Even when learning from demonstrations, if there are few demonstrations it still remains difficult to get the sparse reward due to the exploration problem and lack of state coverage in the few demonstrations, especially in complex tasks.\nThanks to a few demonstrations and reverse curriculum learning, the problem of \u2018no easy initial states\u2019 is alleviated, as we know that \u03c0\u03b8 can at least perform well when starting from the initial states in the demonstrations. We only make a weak assumption that the initial state in the demonstrations we reverse-solved are close to states in Sinit, which is often the case in collected demonstrations. Then, using a forward curriculum, with RL we initially prioritize training on initial states where there is the most learning potential. These are often close to the initial states of demonstrations because the policy can still get some nonzero reward if the initial state is similar enough to one seen during reverse curriculum learning. Eventually, the forward curriculum will train the policy on a sufficient number of initial states in Sinit and learn a policy that achieves high return on initial states sampled from Sinit, not just the demonstration initial states. In order to maximize the outcome of each environment interaction, following PLR (Jiang et al., 2021) we prioritize resetting to initial states that are at the edge of the agent\u2019s abilities before resetting to states that are too difficult to solve from or states that are already easy to solve from. As PLR was originally designed for PPO, an on-policy algorithm, we make a simpler adaptation of PLR that works well and robustly for off-policy SAC and our environments.\nFirst, we uniformally sample a set of n initial states \u039btrain = {si,init} from Sinit. As policy \u03c0 after reverse curriculum learning has high return on initial states si,0 we add these to \u039btrain. Our method uses a simpler score function to PLR that assigns three scores with higher scores leading to higher priority, and it does not have a separate seen vs unseen set of initial states. Define q to be the fraction of episodes out of the last k episodes that receive nonzero return starting from a sampled initial state si,init. If q is 0, then assign a score of 2 to si,init. If 0 < q < \u03c9 for a threshold 0 < \u03c9 < 1, assign a score of 3. If q \u2265 \u03c9, assign a score of 1. In order of decreasing priority, we sample initial states that sometimes receive return, then initial states that receive no return, then initial states that consistently receive return. Following PLR, we adopt the same rank-based prioritization scheme which enables prioritization to be invariant to the scale of the score, and we scale the importance of rank via a temperature value \u03b2, resulting in the score-prioritized distribution PS . We further adopt the same staleness-aware prioritization PC .\nPS(si,init|\u039btrain, S) = rank(Si)\u22121/\u03b2\u2211 j rank(Sj) \u22121/\u03b2 , PC(si,init|\u039btrain, C, c) = c\u2212 Ci\u2211 Cj\u2208C c\u2212 Cj (1)\nwhere Si are the scores assigned to initial state si,init, c is the total number of episodes rolled out in training, and Ci is the episode count at which initial state si,init was last sampled. As q is based on whether an episode received nonzero return or not, our score function is independent of scale of return of the environment under a optimal policy, making it a more generalizable scoring scheme. We follow the same intuition as PLR in that staleness prioritization ensures initial states that have not been sampled in a while get re-sampled in order to update their scores and ensure scores do not drift too far off their true value. The forward curriculum follows stages \u03c1f0 , \u03c1 f 1 , ..., advancing each time a new score is assigned. The linear combination of the score and staleness prioritzation distributions form the initial state distribution at each stage\nP\u03c1fi (si,init) = PS(si,init|\u039btrain, S) + PC(si,init|\u039btrain, C, c) (2)\nFor all experiments we use the same hyperparameters that construct the forward curriculum. We visually showcase how the forward curriculum progresses over the course of training in Fig. 2 via a toy example. While the initial policy starts off successful only on a narrow initial state distribution represented by the narrow green region, via the forward curriculum the policy prioritizes starting from initial states at the edge of what it is capable of and expands the green region until the majority of initial states are covered."
        },
        {
            "heading": "5 RESULTS",
            "text": "Through our experiments we aim to answer the following questions (1) How sample and demonstration efficient is RFCL compared to other model-free baselines? (2) What methods accelerate reverse\ncurriculum learning? (3) How does the curriculums in RFCL enable success from significantly less demonstrations and robustness to the demonstration source?\nFor our experiments, we rigorously evaluate and compare our algorithm against several baselines across 3 robot environment suites for a total of 21 fully-observable environments with sparse rewards. The 3 environments suites are MetaWorld (Yu et al., 2019), Adroit (Rajeswaran et al., 2018), and ManiSkill2 (Gu et al., 2023), see B.1 for details on each task. We use an absolute sparse reward where +1 is rewarded only on success states and 0 is given otherwise. The environments consist of a range of easy to challenging tasks and we benchmark our algorithm on the low demonstration regime, showcasing how RFCL is far more sample/demo efficient than prior approaches. We benchmark against 3 model-free baselines that also leverage demonstrations: (1) RLPD (Ball et al., 2023) is a well-tuned variation of SAC + demonstration data that at the time of writing achieves the state-of-the-art results on Adroit; (2) Jump Start RL (JSRL) (Uchendu et al., 2023) like RFCL leverages reverse curriculums, although they generate the curriculum with a offline trained guide policy. JSRL code is currently not available so we can only report on Adroit environments with numbers from their paper; (3) DAPG (Rajeswaran et al., 2018) uses a demo-augmented policy gradient for RL. (4) Cal-QL is a state-of-the-art offline-to-online RL method. For each training run, we select a random seed. The seed also determines which uniformly sampled demonstrations are used for training. All baselines use the same seeds and thus the same demonstration data."
        },
        {
            "heading": "5.1 MAIN BENCHMARK RESULTS",
            "text": "The main results are summarized in Fig. 3, showing how RFCL outperforms all baselines that leverage demonstrations across all benchmarks and tasks. There are significant improvements in ManiSkill2 and MetaWorld and minor improvements in Adroit. RFCL is the only method that is capable of achieving nonzero success on every environment within a reasonable compute budget. We argue that the critical reason why prior methods are unable to solve these tasks is because of the exploration bottleneck, which is more easily overcome via our method\u2019s state reset reverse curriculum. Fig. 4 shows a selection of the most difficult tasks and demonstrate that RFCL is significantly better if the task is complex and difficult to explore. To visually see why these are more difficult see Appendix B.2. For a disambiguation of each environment\u2019s results, see Appendix A.1."
        },
        {
            "heading": "5.2 ABLATIONS",
            "text": "For all ablations, we benchmark on the ManiSkill2 environments as it is the most difficult set of environments due to high initial state randomization and precise manipulation requirements.\n# of Demonstrations: RFCL provides a flexible approach to leveraging a wider range of demonstration dataset sizes and still being able to perform well as shown in Fig. 5. We observe that when there are very few demonstrations, the forward curriculum is important for improving performance. We see significant improvements on success rate for PickCube for 1 demonstration, for Stackcube for 5 demonstrations or less, and for PegInsertionSide for 10 demonstrations or less. The same conclusions can be drawn from the same demonstration ablation on Metaworld which we show in Appendix A.3. We can attribute this effect to the narrowness of the distribution of initial states the reverse curriculum trained policy performs well on. When given very few demonstrations covering a small set of initial states, the policy is successful on a narrow distribution of initial states. Without a forward curriculum, we are sampling initial states uniformly from Sinit, meaning a large majority of initial states are usually completely out of distribution and lead to 0 return and mostly useless exploration. With a forward curriculum, initial states that lead to nonzero return are prioritized, leading to sample-efficiency gains. Critically, we observe that RFCL still can solve difficult tasks that have high initial state randomization like PegInsertionSide from just a few demonstrations whereas the strongest baseline fails completely and can only solve PickCube when given 20\u00d7 the demonstrations RFCL needs to solve PickCube.\nDemonstration Source: Adroit, Metaworld, and ManiSkill2 demonstrations are tele-operated, generated via scripts, and generated via motion planning respectively. For all these different kinds of demonstrations, RFCL can still solve the task showing that it is robust to demonstration source. This is exemplified by how human and motion planned demonstrations are often sub-optimal\n(with respect to a sparse reward maximization objective), exhibiting multiple modes in actions, and possessing non-Markovian characteristics (Mandlekar et al., 2021). The robustness can be attributed to the fact that we are learning from a purely sparse reward objective via online interaction, and not perfectly imitating past demonstrations. The demonstrations primarily provide a means of overcoming exploration bottlenecks instead of leading to suboptima during training.\nReverse Curriculum: For stage 1 of training, alternative curriculums utilizing demonstration states for state resets are uniform and global. In a uniform \u201ccurriculum\u201d we benchmark the uniform state reset method used in Nair et al. (2018) where they uniformly sample a random demonstration and state in the demonstration. In a global curriculum, instead of assigning each demonstration a start step value ti, all demonstrations are assigned the start step value Ti\u2212u and we progress the curriculum of all demonstrations at the same time if the mean success rate of the past v episodes exceeds a threshold. We perform a hyperparameter sweep over v and reported the best results. We further ablate on the use of a dynamic timelimit. From the results in Table 1, we find that the uniform state reset approach when given just 5 demonstrations cannot get any reasonable results in 1M interactions. The per-demonstration reverse curriculum and dynamic timelimit both significantly improve the sample efficiency of the first training stage and learn a weak policy that can solve from {si,0}.\nImpact of Reverse and Forward Curriculums: The toy experiment on a continuous state/action space pointmaze environment in Fig. 6 demonstrates the agent\u2019s performance throughout training. The agent can reset to any square not covered by the demonstration and the default initial state distribution is heavily biased to the squares far away from the goal, making exploration difficult. In order to succeed, a policy would need to explore around states along the demonstration as the state space is continuous. Notably, under the reverse for-\nward curriculum the agent quickly learns to first perform well around states along the demonstration, after which it relies on the forward curriculum to quickly learn to solve from states all around the maze in a ever-growing frontier. Comparatively, without a curriculum under the same time budget the agent is unable to improve due to no initial state prioritization and difficult exploration. The forward curriculum works eventually thanks to prioritizing initial states closer to the demonstration and goal, but is less sample efficient compared to reverse forward due to exploration difficulty."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "In this work, we introduce the RFCL algorithm that can solve complex tasks with significantly fewer demonstrations than before, including previously never-before solved sparse reward manipulation tasks. We show how the reverse curriculum trains an initial policy that solves the task from a narrow initial state distribution, and how the forward curriculum generalizes the policy to the full initial state distribution, enabling extreme demonstration and sample efficiency as shown in Fig. 3 and 5. We further motivate why the combination of a reverse and forward curriculum works, and introduce key methods that accelerate the reverse curriculum compared to alternatives.\nA limitation of RFCL is the use of state reset and is thus generally restricted to training in simulation, relying on sim2real to be deployed in the real-world for robotics tasks. Despite this limitation, we argue sim2real is orthogonal research that can augment our research, and point to past work such as Chen et al. (2023) that succesfully perform sim2real on dexterous tasks and use state reset. Moreover, while RFCL can flexibly solve all tasks with fewer demonstrations relative to past work, it remains an open question of how to handle many demonstrations as the per-demonstration reverse curriculum slows down when given more demonstrations. A simple option is to add all demonstrations to the offline buffer but only reverse curriculum a subset of demonstrations.\nWe hope our work provides a strong starting point for how far the capabilities of robot learning from demonstration methods can go if we fully leverage the advantages of simulation vs real world training. With the demo efficiency of RFCL, one can prioritize scaling up the diversity of tasks as opposed to the quantity of demonstrations. We will open source all code and are excited with how far the community can push when leveraging more properties of simulation."
        },
        {
            "heading": "A FULL ENVIRONMENT RESULTS",
            "text": "We show the training curves of our method compared to the primary baseline, RLPD, on all environments here without averaging across environments in Appendix A.1.\nBecause we are working with an extremely small number of demonstrations relative to the total number of demonstrations usually used with the tasks we tackle, there can be a high amount of variance in the result curves caused by the choice of demonstrations, not the algorithm itself. To disambiguate the source of variance, in Appendix A.2 we show results on ManiSkill2 on one of the harder environments PegInsertion where we have 2 sets of demonstrations and for each set we run 5 training runs. The 2 sets of demonstrations are the same ones sampled for the main results shown in Figure 3.\nFinally we show some additional experiments with a demo ablation on MetaWorld in Appendix A.3 and compare against a baseline that uses state reset by Nair et al. (2018) on one of their tasks in Appendix A.4 (their open sourced code does not use state reset so we can only test on their environment instead of benchmarking their algorithm on the more common robotics benchmarks like Metaworld)."
        },
        {
            "heading": "A.1 RESULTS WITH RANDOM DEMONSTRATIONS",
            "text": "All algorithms are given 5 randomly sampled demonstrations. We further mark the average step at which the reverse curriculum stage completes training with a vertical, dashed gray line. The bolded lines represent the mean values, and the shaded areas represent 95% CIs over 5 seeds. Note that each seed uses the same demonstrations. Figures 7 and 8 show results on each environment in MetaWorld and Adroit respectively. The demo ablation curve in Fig. 5 in the main paper shows results for ManiSkill2. We further explored just how variable demonstations are by running 30 seeds on the PegInsertion task with each seed using different sets of 5 demonstrations in Fig. 9. The figures show that we are the only method to succeed on all environments, significantly beating baselines. Furthermore, empirically our method is generally quite robust to choice of demonstrations as shown in in Fig. 9. Only 1 seed out of 30 failed to get above 80% success rate after 6M interactions. Note that the robustness may vary between environments, and our results in the main figures do include the seeds where nearly no success was obtained that occured. We believe there could be interesting future work in investigating what kind of demonstrations are easier to learn from using RFCL."
        },
        {
            "heading": "A.2 RESULTS WITH FIXED DEMONSTRATIONS",
            "text": "We pick 2 of the 5 sets of demonstrations used for training shown in Figures 3 and 4, one of which was succesful and solved PegInsertionSide rather fast, and the other demonstration set being the one where there was no success after training for 2M interactions. We then run 5 seeds on both sets of demonstrations and show the results in Fig. 10. We observe that the failed set of demonstrations is indeed harder to learn from, although not impossible as 2/5 seeds can solve the task using that set of\ndemonstrations while 3/5 seeds still has around 0 success after 2M interactions. For the successful set of demonstrations, all seeds solved the task relatively quickly."
        },
        {
            "heading": "A.3 METAWORLD DEMO ABLATION",
            "text": "Fig. 11 is another set of demo ablations on the Metaworld suite comparing RFCL with reverse curriculum only and shows that when there are fewer demonstrations, the forward curriculum enables more sample efficiency. The conclusions drawn from the main demo ablation on ManiSkill2 in Fig. 5 can also be drawn here."
        },
        {
            "heading": "A.4 COMPARISON AGAINST PREVIOUS STATE RESET METHODS",
            "text": "Fig. 12 compares our RFCL method with the state reset method proposed by Nair et al. (2018) on the difficult Stack 3 task, which tasks the agent to stack 3 blocks into a tower. We label their method as Nair et. al (2018). Our approach vastly outperforms their method by being able to solve the task in less than 10 million samples with just 20 demonstrations whereas Nair et al. (2018) requires more than 100 million samples using 100 demonstrations to just get a 39% success rate and has 0 success even after 50 million samples (see their paper for their exact success rate curves). We note that while RFCL performs better, this comparison is a little unfair as the core RL algorithms used are different between papers (hence why this figure is in the appendix). For a more fair comparison we benchmarked the state reset strategy Nair et al. (2018) uses in Table 1 while keeping the rest of RFCL the same, which still shows that our per-demo reverse curriculum is far more sample efficient compared to alternatives"
        },
        {
            "heading": "B TASK DETAILS AND VISUALIZATIONS",
            "text": ""
        },
        {
            "heading": "B.1 TASK DETAILS",
            "text": "Below is the observation dimensions, action dimensions, and task horizons of all benchmarked tasks. For ManiSkill2 and Metaworld, we use the more robust and difficult metric of measuring success at the end of episodes as opposed to allowing success and then failure (e.g. stopping an episode upon success being reached). Following recent past work on Adroit environments, we measure a normalized score which is equal to the percent of timesteps in an episode where the agent is in a success state. For all tasks we do not use any action repeat."
        },
        {
            "heading": "B.2 TASK INITIAL START STATES AND CATEGORIZING DIFFICULTY",
            "text": "In Figures 13 to 18 we show visually what randomly sampled start states from the true start state distribution Sinit look like. We further note which of the environments have large amounts of randomization, which environments do not, and where the randomization comes from, in addition to briefly detailing where the complexity of the task comes from that makes it difficult to solve. Our conclusion with regards to which environments are difficult in MetaWorld is corroborated by the categories used by Hansen et al. (2023); Seo et al. (2022).\nNotably we observe that environments that are complex and have a large amount of initial state randomization tend to need more compute or demonstrations to solve while those with less randomization can get by with much less demonstrations. For example, Adroit Door has little to no\nrandomization and is easily solved by RFCL and the RLPD baseline with just 1 demonstration. On the other end, ManiSkill2 PegInsertionSide and PlugCharger are complex and have a lot of randomization and can only be reliably solved by our method RFCL when given enough demonstrations and/or more compute. The randomization magnitude can stem from higher dimensions (e.g. more objects have randomized shape and positions) and/or stem from a wider range of random sampling."
        },
        {
            "heading": "B.3 TASK REVERSE CURRICULUM",
            "text": "Below are samples of various states of the automatically generated per-demonstration reverse curriculum over the course of training for some of the more difficult tasks. Note that during the reverse curriculum stage of training, the reverse step size is small and the distance between adjacent stages of the curriculum are smaller than shown here."
        },
        {
            "heading": "B.4 TASK SOLVES",
            "text": "For videos of example solves of these environments produced by our method RFCL, see our website at https://reverseforward-cl.github.io/"
        },
        {
            "heading": "B.5 BASELINE SELECTION METHODOLOGY",
            "text": "As we are proposing a learning-from-demonstrations model-free RL algorithm that is more efficient in terms of samples and demonstrations, we select state-of-the-art model-free RL baselines that can use demo data with code.\nWe select RLPD (Ball et al., 2023) as its the current state of the art method on the Adroit suite of tasks for learning from demos. It is the best baseline we have as well. As the RLPD paper shows it out-performs standard SAC + adding demo data to the online buffer, we do not include a SAC baseline.\nWe also select Jump Start RL (Uchendu et al., 2023). Although it does not achieve state of the art results on Adroit, the concept of a reverse curriculum shares similarities with our proposed RFCL method.\nWe further select DAPG (Rajeswaran et al., 2018) and Cal-QL (Nakamoto et al., 2023) as they once achieved state-of-the art results on some tasks for learning from demos algorithms.\nThere are many on-policy algorithms including the popular PPO (Schulman et al., 2017) baseline that could be benchmarked. However, it is well known that off-policy algorithms like SAC are much more sample efficient than on-policy algorithms, and it would be unfair to compare against on-policy methods as they would do poorly. The recent Sequential Dexterity (Chen et al., 2023) method in theory would be good to compare against as they are one of the few papers with code that use state resets, however they use PPO and rely on human engineering to split a task into subtasks which is infeasible to do on the 20+ tasks we test on.\nThere are also model-based methods like MoDem (Hansen et al., 2023) that are very sample efficient thanks to the off-policy setup and model-based approach. We do not compare against MoDem directly as they use world models, but we believe it would be interesting future work to investigate reverse forward curriculums with world models for even better sample efficiency.\nFinally, while we cite a number of past methods that use state resets, we currently are unable to benchmark any of them as the majority do not have code and/or have results on tasks that are not open-sourced. We qualitatively comment on key differences in the related works section Sec. 2 and benchmark the uniform state reset strategy employed by Nair et al. (2018) and show that strategy performs worse compared to our per-demo reverse curriculum state reset approach in 1. We further show results of RFCL on a task that Nair et al. (2018) tests on in Appendix A.4."
        },
        {
            "heading": "C HYPERPARAMETERS",
            "text": "The non-highlighted hyperparameters are standard ones used in Soft-Actor-Critic with a Q-ensemble or Prioritized Level Replay (PLR). Furthermore, while a discount of 0.99 could be used for all tasks, we opt to use 0.9 for some ManiSkill2 tasks as it seems neither our method nor any of the baselines can solve the tasks with a discount of 0.99 easily in 1M interactions when given significant amounts of demonstration data. Moreover, while it is strange only 1 Adroit task (Pen) has a smaller horizon of 100, recent past work (Uchendu et al., 2023; Ball et al., 2023) use this specific horizon so we copy them for fair comparison.\nFinally, for just the PlugCharger environment, we make a few modifications to make it work relatively more efficiently. We use a discount of 0.95, batch size of 512 to improve sample efficiency a little. Importantly, we rescale the action space to be 125% the magnitude of the actions in the set of demonstrations used during training as the demonstration actions of PlugCharger on average are very small in magnitude, suggesting the full action space of the environment is unrealistic and pose great difficulty to exploration. This is a fairly reasonable method to make the problem easier without relying on heavy heuristics, just introducing some bias about the magnitude of actions. We observe this rescaling not only accelerates learning but produces more reasonable policies that take smaller actions and are likely easier to transfer to real. We only use the action rescaling for PlugCharger tests, leaving all other environments as they were, although we believe this would likely accelerate learning for other environments requiring high precision like PegInsertionSide."
        },
        {
            "heading": "D WALL TIME EFFICIENT RESULTS",
            "text": "We note that while it is standard in the field of robot learning and reinforcement learning to compare performance against the number of environment samples, for practical purposes it is also important to have fast wall-time performance. This is made more critical if one\u2019s direction of research is towards sim2real of which then environment sample time is generally fast. With fast environment sample times, it is more wall-time optimal to increase the number of environment samples and lower the relative number of gradient updates, which is done with our wall time efficient set of hyperparameters. The only changes are to use a much smaller update to data ratio of 0.5 and increase actor update frequency to 1, essentially for e.g. every 32 environment steps we update the critic and actor 16 times each. Moreover, the fast hyperparameters run 8 parallel environments with each taking 4 steps at a time, after which then 16 gradient updates are performed. Training can go even faster if 32 parallel environments are used.\nWe did not heavily tune these hyperparameters since they were already quite fast, but we note that depending on environment speed, it may be optimal to tune the update to data ratio accordingly.\nTime in Table 6 is reported in GPU minutes representing the time until the running average success rate of the last 5 evaluations reaches 90%. Experiments all ran on a RTX 2080 GPU. This is a very loose evaluation as it includes the non-trivial amount of time spent running test evaluation in addition to actual training, but it\u2019s clear that simply tuning a few hyperparameters can easily improve training time."
        }
    ],
    "year": 2023
}