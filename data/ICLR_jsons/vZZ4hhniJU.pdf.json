{
    "abstractText": "Communication is a powerful tool for coordination in multi-agent RL. But inducing an effective, common language is a difficult challenge, particularly in the decentralized setting. In this work, we introduce an alternative perspective where communicative messages sent between agents are considered as different incomplete views of the environment state. By examining the relationship between messages sent and received, we propose to learn to communicate using contrastive learning to maximize the mutual information between messages of a given trajectory. In communication-essential environments, our method outperforms previous work in both performance and learning speed. Using qualitative metrics and representation probing, we show that our method induces more symmetric communication and captures global state information from the environment. Overall, we show the power of contrastive learning and the importance of leveraging messages as encodings for effective communication. Figure 1: In multi-view learning, augmentations of the original image or \u201cviews\u201d are used as positive samples for contrastive learning. In our proposed method, CACL, different agents\u2019 views of the same environment state are considered positive samples and messages are contrastively learned as encodings of that state.",
    "authors": [],
    "id": "SP:e1261854589ba841bd38fac125752b997582c360",
    "references": [
        {
            "authors": [
                "Ankesh Anand",
                "Evan Racah",
                "Sherjil Ozair",
                "Yoshua Bengio",
                "Marc-Alexandre C\u00f4t\u00e9",
                "R Devon Hjelm"
            ],
            "title": "Unsupervised State Representation Learning in Atari",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Philip Bachman",
                "R Devon Hjelm",
                "William Buchwalter"
            ],
            "title": "Learning representations by maximizing mutual information across views",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Samuel Barrett",
                "Peter Stone",
                "Sarit Kraus"
            ],
            "title": "Empirical Evaluation of Ad Hoc Teamwork in the Pursuit Domain",
            "venue": "In AAMAS, pp",
            "year": 2011
        },
        {
            "authors": [
                "Miroslav Benda",
                "V Jagannathan",
                "R Dodhiawala"
            ],
            "title": "On optimal cooperation of knowledge sources an empirical investigation",
            "venue": "Technical report, Boeing Advanced Technology Center, Boeing Computing Services,",
            "year": 1986
        },
        {
            "authors": [
                "Micah Carroll",
                "Rohin Shah",
                "Mark K. Ho",
                "Thomas L. Griffiths",
                "Sanjit A. Seshia",
                "Pieter Abbeel",
                "Anca Dragan"
            ],
            "title": "On the Utility of Learning about Humans for Human-AI Coordination",
            "venue": "In Neural Information Processing Systems. arXiv,",
            "year": 2019
        },
        {
            "authors": [
                "Rahma Chaabouni",
                "Eugene Kharitonov",
                "Alessandro Lazaric",
                "Emmanuel Dupoux",
                "Marco Baroni"
            ],
            "title": "Word-order biases in deep-agent emergent communication",
            "venue": "In ACL,",
            "year": 2019
        },
        {
            "authors": [
                "Ting Chen",
                "Simon Kornblith",
                "Mohammad Norouzi",
                "Geoffrey Hinton"
            ],
            "title": "A Simple Framework for Contrastive Learning of Visual Representations",
            "venue": "In arXiv:2002.05709 [cs, stat], June 2020a. URL http://arxiv.org/abs/2002.05709",
            "year": 2002
        },
        {
            "authors": [
                "Ting Chen",
                "Simon Kornblith",
                "Kevin Swersky",
                "Mohammad Norouzi",
                "Geoffrey Hinton"
            ],
            "title": "Big Self-Supervised Models are Strong Semi-Supervised Learners",
            "venue": "In arXiv:2006.10029 [cs, stat], October 2020b. URL http://arxiv.org/abs/2006.10029",
            "year": 2006
        },
        {
            "authors": [
                "Kyunghyun Cho",
                "Bart van Merrienboer",
                "Dzmitry Bahdanau",
                "Yoshua Bengio"
            ],
            "title": "On the Properties of Neural Machine Translation: Encoder-Decoder Approaches",
            "venue": "In 8th Workshop on Syntax, Semantics and Structure in Statistical Translation. arXiv,",
            "year": 2014
        },
        {
            "authors": [
                "Sumit Chopra",
                "Raia Hadsell",
                "Yann LeCun"
            ],
            "title": "Learning a similarity metric discriminatively, with application to face verification",
            "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR\u201905),",
            "year": 2005
        },
        {
            "authors": [
                "Filippos Christianos",
                "Lukas Sch\u00e4fer",
                "Stefano Albrecht"
            ],
            "title": "Shared experience actor-critic for multi-agent reinforcement learning",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Allan Dafoe",
                "Edward Hughes",
                "Yoram Bachrach",
                "Tantum Collins",
                "Kevin R McKee",
                "Joel Z Leibo",
                "Kate Larson",
                "Thore Graepel"
            ],
            "title": "Open problems in cooperative ai",
            "year": 2012
        },
        {
            "authors": [
                "Abhishek Das",
                "Th\u00e9ophile Gervet",
                "Joshua Romoff",
                "Dhruv Batra",
                "Devi Parikh",
                "Michael Rabbat",
                "Joelle Pineau"
            ],
            "title": "TarMAC: Targeted Multi-Agent Communication",
            "venue": "In ICML,",
            "year": 2019
        },
        {
            "authors": [
                "Roberto Dess\u00ec",
                "Eugene Kharitonov",
                "Marco Baroni"
            ],
            "title": "Interpretable agent communication from scratch(with a generic visual processor emerging on the side)",
            "venue": "NeurIPS,",
            "year": 2021
        },
        {
            "authors": [
                "Carl Doersch",
                "Abhinav Gupta",
                "Alexei A. Efros"
            ],
            "title": "Unsupervised Visual Representation Learning by Context Prediction",
            "venue": "In ICCV,",
            "year": 2015
        },
        {
            "authors": [
                "Tom Eccles",
                "Yoram Bachrach",
                "Guy Lever",
                "Angeliki Lazaridou",
                "Thore Graepel"
            ],
            "title": "Biases for emergent communication in multi-agent reinforcement learning",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Martin Ester",
                "Hans-Peter Kriegel",
                "J\u00f6rg Sander",
                "Xiaowei Xu"
            ],
            "title": "A density-based algorithm for discovering clusters in large spatial databases with noise",
            "venue": "In kdd,",
            "year": 1996
        },
        {
            "authors": [
                "Jakob Foerster",
                "Ioannis Alexandros Assael",
                "Nando De Freitas",
                "Shimon Whiteson"
            ],
            "title": "Learning to communicate with deep multi-agent reinforcement learning",
            "venue": "Advances in neural information processing systems,",
            "year": 2016
        },
        {
            "authors": [
                "Florin Gogianu",
                "Tudor Berariu",
                "Mihaela C Rosca",
                "Claudia Clopath",
                "Lucian Busoniu",
                "Razvan Pascanu"
            ],
            "title": "Spectral normalisation for deep reinforcement learning: an optimisation perspective",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Laura Graesser",
                "Kyunghyun Cho",
                "Douwe Kiela"
            ],
            "title": "Emergent linguistic phenomena in multi-agent communication games",
            "venue": "In EMNLP,",
            "year": 2019
        },
        {
            "authors": [
                "Niko A. Grupen",
                "Daniel D. Lee",
                "Bart Selman"
            ],
            "title": "Low-Bandwidth Communication Emerges Naturally in Multi-Agent Learning Systems, December 2020",
            "venue": "URL http://arxiv.org/ abs/2011.14890",
            "year": 2011
        },
        {
            "authors": [
                "Abhinav Gupta",
                "Marc Lanctot",
                "Angeliki Lazaridou"
            ],
            "title": "Dynamic population-based meta-learning for multi-agent communication with natural language",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Kaiming He",
                "Haoqi Fan",
                "Yuxin Wu",
                "Saining Xie",
                "Ross Girshick"
            ],
            "title": "Momentum contrast for unsupervised visual representation learning",
            "venue": "In CVPR,",
            "year": 2020
        },
        {
            "authors": [
                "Hengyuan Hu",
                "Adam Lerer",
                "Alex Peysakhovich",
                "Jakob Foerster"
            ],
            "title": "Other-Play\" for Zero-Shot Coordination",
            "venue": "In ICML. arXiv,",
            "year": 2020
        },
        {
            "authors": [
                "Natasha Jaques",
                "Angeliki Lazaridou",
                "Edward Hughes",
                "Caglar Gulcehre",
                "Pedro A. Ortega",
                "D.J. Strouse",
                "Joel Z. Leibo",
                "Nando de Freitas"
            ],
            "title": "Social Influence as Intrinsic Motivation for Multi-Agent Deep Reinforcement Learning",
            "venue": "In ICML. arXiv,",
            "year": 2018
        },
        {
            "authors": [
                "Jiechuan Jiang",
                "Zongqing Lu"
            ],
            "title": "Learning Attentional Communication for Multi-Agent Cooperation",
            "venue": "In Neural Information Processing Systems. arXiv,",
            "year": 2018
        },
        {
            "authors": [
                "Chi Jin",
                "Qinghua Liu",
                "Yuanhao Wang",
                "Tiancheng Yu"
            ],
            "title": "V-Learning \u2013 A Simple, Efficient, Decentralized Algorithm for Multiagent RL, October 2021",
            "venue": "URL http://arxiv.org/abs/ 2110.14555",
            "year": 2021
        },
        {
            "authors": [
                "Soyi Jung",
                "Won Joon Yun",
                "Joongheon Kim",
                "Jae-Hyun Kim"
            ],
            "title": "Coordinated Multi-Agent Deep Reinforcement Learning for Energy-Aware UAV-Based Big-Data",
            "venue": "Platforms. Electronics,",
            "year": 2021
        },
        {
            "authors": [
                "Prannay Khosla",
                "Piotr Teterwak",
                "Chen Wang",
                "Aaron Sarna",
                "Yonglong Tian",
                "Phillip Isola",
                "Aaron Maschinot",
                "Ce Liu",
                "Dilip Krishnan"
            ],
            "title": "Supervised contrastive learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "arXiv preprint arXiv:1412.6980,",
            "year": 2014
        },
        {
            "authors": [
                "Anurag Koul"
            ],
            "title": "ma-gym: Collection of multi-agent environments based on openai gym",
            "venue": "https: //github.com/koulanurag/ma-gym,",
            "year": 2019
        },
        {
            "authors": [
                "Angeliki Lazaridou",
                "Marco Baroni"
            ],
            "title": "Emergent Multi-Agent Communication in the Deep Learning Era",
            "year": 2020
        },
        {
            "authors": [
                "Angeliki Lazaridou",
                "Karl Moritz Hermann",
                "Karl Tuyls",
                "Stephen Clark"
            ],
            "title": "Emergence of Linguistic Communication from Referential Games with Symbolic and Pixel Input",
            "venue": "In ICLR,",
            "year": 2018
        },
        {
            "authors": [
                "Wenhao Li",
                "Bo Jin",
                "Xiangfeng Wang",
                "Junchi Yan",
                "Hongyuan Zha"
            ],
            "title": "F2a2: Flexible fullydecentralized approximate actor-critic for cooperative multi-agent reinforcement learning",
            "venue": "arXiv preprint arXiv:2004.11145,",
            "year": 2020
        },
        {
            "authors": [
                "Toru Lin",
                "Jacob Huh",
                "Christopher Stauffer",
                "Ser Nam Lim",
                "Phillip Isola"
            ],
            "title": "Learning to ground multiagent communication with autoencoders",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Ryan Lowe",
                "Yi I Wu",
                "Aviv Tamar",
                "Jean Harb",
                "Pieter Abbeel",
                "Igor Mordatch"
            ],
            "title": "Multi-agent actor-critic for mixed cooperative-competitive environments",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Andrei Lupu",
                "Brandon Cui",
                "Hengyuan Hu",
                "Jakob Foerster"
            ],
            "title": "Trajectory diversity for zero-shot coordination",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Xueguang Lyu",
                "Yuchen Xiao",
                "Brett Daley",
                "Christopher Amato"
            ],
            "title": "Contrasting Centralized and Decentralized Critics in Multi-Agent Reinforcement Learning",
            "venue": "In AAMAS,",
            "year": 2021
        },
        {
            "authors": [
                "Vincent Mai",
                "Tianyu Zhang",
                "Antoine Lesage-Landry"
            ],
            "title": "Multi-agent reinforcement learning for renewable integration in the electric power grid. In Tackling Climate Change with Machine Learning Workshop at NeurIPS, 2021",
            "year": 2021
        },
        {
            "authors": [
                "Volodymyr Mnih",
                "Adria Puigdomenech Badia",
                "Mehdi Mirza",
                "Alex Graves",
                "Timothy Lillicrap",
                "Tim Harley",
                "David Silver",
                "Koray Kavukcuoglu"
            ],
            "title": "Asynchronous methods for deep reinforcement learning",
            "venue": "In International conference on machine learning,",
            "year": 2016
        },
        {
            "authors": [
                "Michael Noukhovitch",
                "Travis LaCroix",
                "Angeliki Lazaridou",
                "Aaron Courville"
            ],
            "title": "Emergent Communication under Competition",
            "venue": "In Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Frans A Oliehoek",
                "Christopher Amato"
            ],
            "title": "A concise introduction to decentralized POMDPs",
            "year": 2016
        },
        {
            "authors": [
                "Mikayel Samvelyan",
                "Tabish Rashid",
                "Christian Schroeder de Witt",
                "Gregory Farquhar",
                "Nantas Nardelli",
                "Tim G.J. Rudner",
                "Chia-Man Hung",
                "Philip H.S. Torr",
                "Jakob Foerster",
                "Shimon Whiteson"
            ],
            "title": "The StarCraft Multi-Agent Challenge, December 2019",
            "year": 1902
        },
        {
            "authors": [
                "Florian Schroff",
                "Dmitry Kalenichenko",
                "James Philbin"
            ],
            "title": "FaceNet: A Unified Embedding for Face Recognition and Clustering",
            "venue": "In Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2015
        },
        {
            "authors": [
                "Amanpreet Singh",
                "Tushar Jain",
                "Sainbayar Sukhbaatar"
            ],
            "title": "Learning when to communicate at scale in multiagent cooperative and competitive tasks",
            "venue": "arXiv preprint arXiv:1812.09755,",
            "year": 2018
        },
        {
            "authors": [
                "Brian Skyrms"
            ],
            "title": "Signals: Evolution, Learning, & Information",
            "year": 2010
        },
        {
            "authors": [
                "Sainbayar Sukhbaatar",
                "Rob Fergus"
            ],
            "title": "Learning multiagent communication with backpropagation",
            "venue": "Advances in neural information processing systems,",
            "year": 2016
        },
        {
            "authors": [
                "Richard Sutton",
                "Andrew G Barto"
            ],
            "title": "Reinforcement Learning: An Introduction",
            "venue": "URL http://incompleteideas.net/book/the-book-2nd.html",
            "year": 2018
        },
        {
            "authors": [
                "Gerald Tesauro"
            ],
            "title": "Td-gammon, a self-teaching backgammon program, achieves master-level play",
            "venue": "Neural computation,",
            "year": 1994
        },
        {
            "authors": [
                "Laurens Van der Maaten",
                "Geoffrey Hinton"
            ],
            "title": "Visualizing data using t-sne",
            "venue": "Journal of machine learning research,",
            "year": 2008
        },
        {
            "authors": [
                "Kyle Wagner",
                "James A Reggia",
                "Juan Uriagereka",
                "Gerald S Wilkinson"
            ],
            "title": "Progress in the simulation of emergent communication and language",
            "venue": "Adaptive Behavior,",
            "year": 2003
        },
        {
            "authors": [
                "Singh"
            ],
            "title": "Visual illustration of the environments used. Left: Predator-Prey, taken from Koul (2019). Middle: Find-Goal, taken from Lin et al. (2021)",
            "venue": "Right: Traffic-Junction,",
            "year": 2019
        },
        {
            "authors": [
                "Lin"
            ],
            "title": "DIAL, gradients would flow from another agent to the agent that sends a message through the message head and message encoder. Unless specified otherwise, we fix all hidden layers to be a size of 32. We experimented with using the output of the GRU, or hidden state, to condition the message",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Communication is a key capability necessary for effective coordination among agents in partially observable environments. In multi-agent reinforcement learning (MARL) (Sutton & Barto, 2018), agents can use their actions to transmit information (Grupen et al., 2020) but continuous or discrete messages on a communication channel (Foerster et al., 2016), i.e., linguistic communication (Lazaridou & Baroni, 2020), are more flexible and powerful because they can convey more complex concepts. To successfully communicate, a speaker and a listener must share a common language with a shared understanding of the symbols being used (Skyrms, 2010; Dafoe et al., 2020). Emergent communication or learning a common protocol (Wagner et al., 2003; Lazaridou & Baroni, 2020), is a thriving research direction but most works focus on simple, single-turn, sender-receiver games (Lazaridou et al., 2018; Chaabouni et al., 2019). In more visually and structurally complex MARL environments\n(Samvelyan et al., 2019), existing approaches often rely on centralized learning mechanisms by sharing models (Lowe et al., 2017) or gradients (Sukhbaatar et al., 2016).\nHowever, a centralized controller is impractical in many real-world environments (Mai et al., 2021; Jung et al., 2021) where agents cannot easily synchronize and must act independently i.e. decentralized. Centralized training with decentralized execution (CTDE) (Lowe et al., 2017) is a middle-ground between purely centralized and decentralized methods but may not perform better than purely decentralized training (Lyu et al., 2021). A centralized controller suffers from the curse of dimensionality: as the number of agents it must control increases, the amount of communication between agents to process increases exponentially (Jin et al., 2021). Furthermore, the fully decentralized setting is more flexible and requires fewer assumptions about other agents, making it more realistic in many real-world scenarios (Li et al., 2020). Hence, this work explores learning to communicate to coordinate agents in the decentralized setting. In MARL, this means each agent will have its own model to decide how to act and communicate, and no agents share parameters or gradients.\nTypical RL approaches to decentralized communication are known to perform poorly even in simple tasks (Foerster et al., 2016) due to the large space of communication to explore, the high variance of RL, and a lack of common grounding on which to base communication (Lin et al., 2021). Earlier work leveraged how communication influences other agents (Jaques et al., 2018; Eccles et al., 2019) to learn the protocol. Most recently, Lin et al. (2021) proposed agents that autoencode their observations and use the encodings as communication, using the shared environment as the common grounding. We build on this work in using both the shared environment and the relationship between sent and received messages to ground a protocol. We extend the Lin et al. (2021) perspective that agents\u2019 messages are encodings and propose that agents in similar states should produce similar messages. This perspective leads to a simple method based on contrastive learning to ground communication.\nInspired by the literature in representation learning that uses different \u201cviews\u201d of a data sample (Bachman et al., 2019), for a given trajectory, we frame an agent\u2019s observation as a \u201cview\u201d of the environment state. Thus, different agents\u2019 messages are encodings of different incomplete \u201cviews\u201d of the same underlying state. From this perspective, messages from the same state should, generally, be more similar to each other than to those from distant states or other trajectories, as shown in Figure 1. As with image augmentations, two agent observations may not necessarily overlap, but a contrastive objective can generally lead to effective encodings. We propose Communication Alignment Contrastive Learning (CACL), where each agent separately uses contrastive learning between their own sent and received messages to learn a communication encoding.\nWe experimentally validate CACL in three communication-essential environments and show how CACL leads to improved performance and speed, outperforming state-of-the-art decentralized MARL communication algorithms. To understand CACL\u2019s success, we propose a suite of qualitative and quantitative metrics. We demonstrate that CACL leads to more symmetric communication (i.e., different agents communicate similarly when faced with the same observations), allowing agents to be more mutually intelligible. By treating our messages as representations, we show that CACL\u2019s messages capture global semantic information about the environment better than baselines. Overall, we argue that contrastive learning is a powerful direction for multi-agent communication and has fundamental benefits over previous approaches."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Learning to coordinate multiple RL agents is a challenging and unsolved task where naively applying single-agent RL algorithms often fails (Foerster et al., 2016). Recent approaches focus on neural network-based agents (Goodfellow et al., 2016) with a message channel to develop a common communication protocol (Lazaridou & Baroni, 2020). To handle issues of non-stationarity, some work focuses on centralized learning approaches that globally share models (Foerster et al., 2016), training procedures (Lowe et al., 2017), or gradients (Sukhbaatar et al., 2016) among agents. This improves coordination and can reduce optimization issues but results are often still sub-optimal in practise (Foerster et al., 2016; Lin et al., 2021) and may violate independence assumptions, effectively modelling the multi-agent scenario as a single agent (Eccles et al., 2019).\nThis work focuses on independent, decentralized agents and non-differentiable communication. In previous work, Jaques et al. (2018) propose a loss to influence other agents but require explicit\nand complex models of other agents and their experiments focus on mixed cooperative-competitive scenarios. Eccles et al. (2019) add biases to each agent\u2019s loss function that separately encourage positive listening (i.e., the listener to act differently for different messages) and positive signaling (i.e., the speaker to produce diverse messages in different situations). Their method is simpler but requires task-specific hyperparameter tuning to achieve reasonable performance and underperforms in sensory-rich environments (Lin et al., 2021). Our work is closest to Lin et al. (2021), who leverage autoencoding as their method to learn a message protocol in cooperative 2D MARL games. Agents learn to reconstruct their observations and communicate their autoencoding. It outperforms previous works while being algorithmically and conceptually simpler. Our method builds on this encoding perspective by considering other agents\u2019 messages to ground communication. Whereas agents in Lin et al. (2021) can only learn to encode the observation, our approach leverages the relationship between different agents\u2019 messages to encode global state information. Empirically, our method is also more efficient as it requires no extra learning parameters whereas Lin et al. (2021) learn and discard their decoder network. Note that our setup uses continuous messages instead of discrete (Eccles et al., 2019; Lin et al., 2021), a standard choice in contrastive learning (Chopra et al., 2005; He et al., 2020; Chen et al., 2020a) and embodied multi-agent communication (Sukhbaatar et al., 2016; Jiang & Lu, 2018; Das et al., 2019).\nAutoencoding is a form of generative self-supervised learning (SSL) (Doersch et al., 2015). We propose to use another form of SSL, contrastive learning (Chen et al., 2020a), as the basis for learning communication. We are motivated by recent work that achieves state-of-the-art representation learning on images using contrastive learning methods (Chen et al., 2020b) and leverages multiple \"views\" of the data. Whereas negative samples are simply different images, positive samples are image data augmentations or \u201cviews\u201d of the original image (Bachman et al., 2019). We treat agents\u2019 messages of the same state in a trajectory as positives of each other, so we base our method on SupCon (Supervised Contrastive Learning) (Khosla et al., 2020) which modifies the classic contrastive objective to account for multiple positive samples. Relatedly, Dess\u00ec et al. (2021) use a two-agent discrete communication setup to do contrastive learning on images, we do the opposite and leverage contrastive learning to learn multi-agent communication in an RL environment."
        },
        {
            "heading": "3 PRELIMINARIES",
            "text": "We base our investigations on decentralized partially observable Markov decision processes (DecPOMDPs) with N agents to describe a fully cooperative multi-agent task (Oliehoek & Amato, 2016). A Dec-POMDP consists of a tuple G = \u27e8S,A, P,R,Z,\u2126, n, \u03b3\u27e9. s \u2208 S is the true state of the environment. At each time step, each agent i \u2208 N chooses an action ai \u2208 Ai to form a joint action a \u2208 A \u2261 A1 \u00d7A2...\u00d7AN . It leads to an environment transition according to the transition function P (s\u2032|s, a1, ...aN ) : S \u00d7 A \u00d7 S \u2192 [0, 1]. All agents share the same reward function R(s, a) : S\u00d7A \u2192 R. \u03b3 \u2208 [0, 1) is a discount factor. As the environment is partially observable, each agent i receives individual observations z \u2208 Z based on the observation function \u2126i(s) : S \u2192 Z. We denote the environment trajectory and the action-observation history (AOH) of an agent i as \u03c4t = s0, a0, ....st, at and \u03c4 it = \u2126 i(s0), a i 0, ....\u2126 i(st), a i t \u2208 T \u2261 (Z \u00d7A)\u2217 respectively. A stochastic policy \u03c0(ai|\u03c4 i) : T \u00d7A \u2192 [0, 1] conditions on AOH. The joint policy \u03c0 has a corresponding actionvalue function Q\u03c0(st, at) = Est+1:\u221e,at+1:\u221e [Rt|st, at], where Rt = \u2211\u221e i=0 \u03b3\nirt+i is the discounted return. rt+i is the reward obtained at time t+ i from the reward function R.\nTo account for communication, similar to Lin et al. (2021), at each time step t, an agent i takes an action ait and produces a message m i t = \u03a8\ni(\u2126i(st)) after receiving its observation \u2126i(st) including messages from the previous time step m\u2212it\u22121, where \u03a8\ni is agent i\u2019s function to produce a message given its observation and m\u2212it\u22121 refers to messages sent by agents other than agent i. The messages are continuous vectors of dimensionality D."
        },
        {
            "heading": "4 METHODOLOGY",
            "text": "We propose a different perspective on the message space used for communication. At each time step t for a given trajectory \u03c4 , a message mit of an agent i can be viewed as an incomplete view of the environment state st because mit is a function of st as formulated in section 3. Naturally, messages of all the N agents are different incomplete perspectives of st. To ground decentralized communication,\nwe hypothesize that we could leverage this relationship between messages from similar states to encourage consistency and similarity of the messages space across agents. Specifically, we propose maximizing the mutual information using contrastive learning which aligns the message space by pushing messages from similar states closer together and messages of different states further apart. Note that agents see a partial view of the state from their observation, so they will inherently communicate different messages to reflect their partial knowledge. However, aligning their message spaces enables communicating these partial views of the state in a more mutually-intelligible way.\nAs a heuristic for state similarity, we consider a window of timesteps within a trajectory to be all similar states i.e. positive samples of each other. To guarantee dissimilar negative samples (Schroff et al., 2015), we use states from other trajectories as negatives. Since each underlying state has multiple positive views (w steps, N agent messages each), we leverage the recent contrastive learning method SupCon (Khosla et al., 2020). We refer to the contrastive SupCon objective across multiple MARL trajectories as Communication Alignment Contrastive Learning (CACL).\nLet M be all the messages in a batch of trajectories and M\u03c4 be the messages in trajectory \u03c4 . Let mit \u2208 M\u03c4 the message of agent i at time t. Thus, positives H for a message mit given a timestep window w are all other messages from the same trajectory \u03c4 sent within that timestep window H(mit) \u2261 {m j t\u2032 \u2208 M\u03c4 \\ {mit} : t\u2032 \u2208 [t\u2212 w, t+ w]}. Let all other messages K from all trajectories in the batch be K(mit) \u2261 M \\ {mit}. Formally, the contrastive loss LCACL:\n\u2211 mit\u2208M\u03c4 \u22121 |H(mit)| \u2211 mh\u2208H(mit) log exp(mit \u00b7mh/\u03b7)\u2211 mk\u2208K(mit) exp(mit \u00b7mk/\u03b7)\n(1)\nWhere \u03b7 \u2208 R+ is a scalar temperature and |H(mit)| is the cardinality. Practically, each agent has a replay buffer that maintains a batch of trajectory data collected from multiple environment instances. It contains messages received during training to compute the CACL loss. We use a timestep window of size 5 for all the environments based on hyperparameter tuning of different window sizes. Following Khosla et al. (2020), messages are normalized before the loss computation and a low temperature (i.e. \u03b7 = 0.1) is used as it empirically benefits performance and training stability. The total loss for each agent is a reinforcement learning loss LRL using the reward to learn a policy (but not message head) and a separate contrastive loss LCACL to learn just the message head, formulated as follows:\nL = LRL + \u03baLCACL (2)\nwhere \u03ba \u2208 R+ is a hyperparameter to scale the CACL loss. See Appendix A.1 for LRL details."
        },
        {
            "heading": "5 EXPERIMENTS AND RESULTS",
            "text": ""
        },
        {
            "heading": "5.1 EXPERIMENTAL SETUP",
            "text": "We evaluate our method on three multi-agent environments with communication channels. Given the limited information each agent observes, agents must meaningfully communicate in order to improve task performance. All results are averaged over 12 evaluation episodes and over 6 random seeds. More details of the environments and parameters can be found in appendix A.2.\nTraffic-Junction: Proposed by Sukhbaatar et al. (2016), it consists of a 4-way traffic junction with cars entering and leaving the grid. The goal is to avoid collision when crossing the junction. We use 5 agents with a vision of 1. Although not necessary, given the limited vision in agents, communication could help in solving the task. We evaluate algorithms using their success rate in avoiding all collisions during evaluation episodes.\nPredator-Prey: A variant of the classic game (Benda et al., 1986; Barrett et al., 2011) based on Koul (2019) where 4 agents (i.e. predators) have the cooperative goal to capture 2 randomly-moving prey by surrounding each prey with more than one predator. We devise a more difficult variation where agents have to entirely surround a prey on all 4 sides to successfully capture it and they cannot see each other in their observations. Thus, agents must communicate their positions and actions in order to coordinate their attacks. We evaluate each algorithm with episodic rewards in evaluation episodes.\nFind-Goal: Proposed by Lin et al. (2021), agents\u2019 goal is to reach the green goal location as fast as possible in a grid environment with obstacles. We use 3 agents, each observes a partial view of the environment centered at its current position. Unlike in Lin et al. (2021), we use a field of view of 3 \u00d7 3 instead of 7 \u00d7 7 to make the problem harder. Each agent receives an individual reward of 1 for reaching the goal and an additional reward of 5 when all of them reach the goal. Hence, it is beneficial for an agent to communicate the goal location once it observes the goal. As in Lin et al. (2021), we measure performance using episode length. An episode ends quicker if agents can communicate goal locations to each other more efficiently. Hence, a method performs better if it has shorter episode lengths."
        },
        {
            "heading": "5.2 TRAINING DETAILS",
            "text": "We compare CACL to the state-of-the-art independent, decentralized method, autoencoded communication (AEComm; Lin et al., 2021), which grounds communication by reconstructing encoded observations. We also compare to baselines from previous work: independent actor critic without communication (IAC) and positive listening loss (PL; Eccles et al., 2019) (See Appendix A.6). We exclude the positive signalling loss (Eccles et al., 2019) as extending it to continuous messages is non-trivial but note that AEComm outperforms it in the discrete case (Lin et al., 2021). We also include DIAL (Foerster et al., 2016) which learns to communicate through differentiable messages to share gradients so is decentralized but not independent.\nAll methods use the same architecture based on the IAC algorithm with n-step returns and asynchronous environments (Mnih et al., 2016). Each agent has an encoder for observations and received messages. For methods with communication, each agent has a communication head to produce messages based on encoded observations. For policy learning, a GRU (Cho et al., 2014) is used to generate a hidden representation from a history of observations and messages. Agents use the hidden state for their the policy and value heads, which are 3-layer fully-connected neural networks. We perform spectral normalization (Gogianu et al., 2021) in the penultimate layer for each head to improve training stability. The architecture is shown in Figure 8 and hyperparameters are further described, both in Appendix A.3."
        },
        {
            "heading": "5.3 TASK PERFORMANCE",
            "text": "We run all methods on the three selected environments and plot results in Figure 2. Our proposed method CACL outperforms all baseline methods in both final performance and learning speed and, consistent with previous results (Lin et al., 2021), AEComm is the strongest baseline. The largest performance increase from CACL is in FindGoal where partial observability is most prominent due to agents\u2019 small field-of-view which makes communication more necessary (hence why IAC performs worst). These results show the effectiveness of self-supervised methods for learning communication in the fully-decentralized setting, as they both outperform DIAL which, notably, backpropogates\ngradients through other agents. Furthermore, it demonstrates CACL\u2019s contrastive learning as a more powerful alternative to AEComm\u2019s autoencoding for coordinating agents with communication.\nImprovement on Traffic-Junction is not as significant as others because communication is less essential for task completion, as shown by the strong performance of IAC. For Predator-Prey, results are clearly better than baselines but have high variance due to the difficulty of the task. The goal of Predatory-Prey is to capture two moving prey and requires coordinating precisely to surround and attack a prey at the same time. Any slight miscoordination leads to sharp drop in rewards. For another metric of success, we compute the percentages of evaluation episodes that capture no, one, or two preys. Averaging over 6 random seeds, we show results in Figure 3. CACL does significantly better on the task, outperforms all baselines, and solving the complete task more robustly while failing less frequently. Find-Goal requires the most communication among the environments because the gridworld is the largest and agents must clearly communicate the location of goal. Here, CACL significantly outperforms the baselines, demonstrating that as the communicative task gets harder, CACL outperforms more.\nWe confirm the effectiveness of CACL with an ablation study of the key design decisions: sliding window and SupCon. CACL leverages the temporal nature of RL to treat a sliding window of timesteps as positive views of each other. We plot results for a range of window sizes run on PredatorPrey in Figure 4. No sliding window (size 1) performs poorly, demonstrating its necessity and that the choice of sliding window size is an important hyperparameter. Through the use of SupCon (Khosla et al., 2020) we treat all sent and received messages in the sliding window as all positive views of each other, with many positives per batch. Creating a batch with just one positive view per message corresponds to SimCLR (Chen et al., 2020a) and results in much worse performance (1.36\u00b1 9.46). We also run Predator-Prey and search across values of the CACL loss coefficient \u03ba in Figure 4. We used the best values (5-step window, \u03ba=0.5) across all the environments, demonstrating that the choice of CACL hyperparameters is robust. Overall, we show the issues in naively implementing contrastive learning for communication, and the clear, important design decisions behind CACL."
        },
        {
            "heading": "5.4 AUGMENTING CACL WITH RL",
            "text": "The contrastive loss in the communication head of CACL is very performant without optimizing for reward, so a natural question is whether we can achieve even better results if we learn the message\nusing reward as well. To answer this, we add DIAL to both CACL and the next best method, AEComm, and evaluate in the three environments. This is equivalent to backpropogating LRL from Equation 2 through agents to learn the message head. In this way, both RL and SSL (contrastive or autoencoding) signals are used to learn the message head.\nFigure 5 compares the performance of CACL and AEComm with their DIAL-augmented variants. Our findings are consistent with Lin et al. (2021), who find that mixing AEComm and RL objectives are detrimental to performance. We observe that augmenting either AEComm or CACL with DIAL performs generally worse, except in Find-Goal, where performances is similar but not better. We hypothesize that decentralized DIAL is a complex, and high-variance optimization that is difficult to stabilize. DIAL\u2019s gradient updates may clash with CACL and result in neither a useful contrastive representation, nor a strong reward-oriented one. It is also possible that CACL\u2019s messages would not be improved with reward-oriented gradients. As we show in Section 5.6, CACL already captures useful semantic information that other agents can effectively extract."
        },
        {
            "heading": "5.5 PROTOCOL SYMMETRY",
            "text": "We hypothesize that CACL\u2019s improved performance over the baselines is because it induces a more consistent, mutually-intelligible communication protocol that is shared among agents. More specifically, we consider consistency to be how similarly agents communicate (i.e., sending similar messages) when faced with the same observations. A consistent protocol can reduce the optimization complexity since agents only need to learn one protocol for the whole group and it also makes agents more mutually intelligible.\nTo evaluate consistency, we measure protocol symmetry (Graesser et al., 2019) so if an agent swaps observations and trajectory with another agent, it should produce a similar message as what the other agent produced. We extend this metric from previous work to the continuous, embodied case. We feed the same trajectory to all agents and measure the pairwise cosine similarities of the messages that they produce. Given a trajectory \u03c4 and {t \u2208 T} as a set of time steps of \u03c4 , protocol symmetry (protocol_sym) is written as:\n1 |T | \u2211 t\u2208T 1 |N | \u2211 i\u2208N\n1 |N | \u2212 1 \u2211\nj\u2208N\\i\n\u03a8i(\u2126j(st)) \u00b7\u03a8j(\u2126j(st)) \u2225\u03a8i(\u2126j(st))\u2225\u2225\u03a8j(\u2126j(st))\u2225\n(3)\nTherefore, a more consistent protocol has higher symmetry. We swap agent trajectory and observations and compute this metric over 10 evaluation episodes for 6 random seeds, and show results in Table 1. The self-supervised methods (CACL and AEComm) clearly outperform the others (DIAL and PL) implying that SSL is better for learning consistent representations in decentralized MARL. Furthermore, CACL\u2019s protocol is very highly symmetric, clearly outperforming all others. Each AEComm agent autoencodes their own observation without considering the other agents\u2019 messages, leading to the formation of multiple protocols between agents. In contrast, CACL induces a common protocol by casting the problem in the multi-view perspective and implicitly aligning agents\u2019 messages. The possible correlation between protocol symmetry and overall performance and speed further indicates the benefits of learning a common protocol in the decentralized setting."
        },
        {
            "heading": "5.6 PROTOCOL REPRESENTATION PROBING",
            "text": "To further investigate how informative our protocols are, we propose a suite of qualitative and quantitative representation probing tests based on message clustering and classification, respectively. We perform these tests on the protocols learned in the Find-Goal environment.\nSimilar to Lin et al. (2021), we cluster messages generated from 10 evaluation episodes to qualitatively assess how informative CACL\u2019s protocol is. The messages are first compressed to a dimension of 2 using t-SNE (Van der Maaten & Hinton, 2008) and then clustered using DBSCAN (Ester et al., 1996). We look at each cluster\u2019s messages and their corresponding observations to extract any patterns and semantics captured. As shown in Figure 6, we observe a cluster of messages for observations when the goal is visible and another one when another agent is visible. Two clusters correspond to agents seeing neither the goal nor another agent. Notably, the messages in these clusters can come from different agents in different episodes, demonstrating that agents can indeed communicate symmetrically. The clusters indicate that CACL learns to compress meaningful, global state information in messages, allowing other agents to reasonably learn this semantic information.\nTo quantitatively evaluate the informativeness of learned protocols, we propose to treat messages as representations and learn a classifier on top of the messages, following work in RL representation learning (Lazaridou et al., 2018; Anand et al., 2019). Since FindGoal is focused on reaching a\ngoal, intuitively, agents should communicate whether they have found the goal and, if so, where other agents should go to reach the goal. Thus, we propose to probe the goal visibility and goal location. The former uses the messages to classify whether the goal is visible in observations or not (i.e. a binary classification). The latter uses messages where the goal is visible in the observations to classify the general location of the goal (i.e., a 5-class classification: Top-Left, Top-Right, BottomLeft, Bottom-Right and Middle). Whereas goal visibility is easy for egocentric communication, goal location requires detailed spatial information and communicating the absolute location from their relative position. This tests whether the communication protocol can consider other agents\u2019 perspectives and give global information from an egocentric observation. We use 30 evaluation episodes per method to generate messages for our experiments but different methods may have different numbers of acceptable messages for our probing task (e.g. a limited number of messages where the goal is visible for predicting goal location). To ensure fair comparison, we choose an equal number of samples per class (i.e., positive/negative, 5-class location) for all methods and use a 70%/30% random split for training and testing. We use a 2-layer fully-connected neural network to test each method, as this corresponds to the same network that agents use to encode each others\u2019 messages as part of their observations.\nTable 2 shows the classification results for the two probing tests. Goal visibility is an easier task and all methods\u2019 messages can be effectively used to determine it. In the more difficult goal location task, all methods perform above chance (20%) but CACL\u2019s protocol significantly outperforms baselines. Contrastive learning across different agents\u2019 messages can enable CACL to learn a more global understanding of location from their egocentric viewpoint. We further compare the similarity between messages sent when the goal is at location [1, 1] and when the goal is at other coordinates along the diagonal in Appendix A.7, which corroborates with the Goal Location Test in CACL\u2019s capability in encoding global information better. By encoding the goal\u2019s spatial information, CACL agents are more likely able to move directly towards it, and reduce episode length. If other methods simply communicate that a goal is found, agents know to alter their search but are not as precise in direction. This explains why AEComm, PL, and DIAL perform better than IAC but worse than CACL, which also learns much quicker as shown in Figure 2. For completeness, we also provide similar classification results with a one-layer (linear) probe in Appendix A.8."
        },
        {
            "heading": "6 LIMITATIONS",
            "text": "Our work investigates fully-cooperative environments but learning to communicate in less cooperative settings, such as those with adversaries (Noukhovitch et al., 2021), is a harder optimization problem. CACL would likely need stronger regularization to be effective. Furthermore, our empirical testing has revealed that SSL objectives are ineffective with reward-oriented gradients, as demonstrated in section 5.4. Although this phenomenon is well known (Lin et al., 2021), it is still not fully understood and future work should aim to combine the two objectives. Finally, this work evaluates agents that were trained together. A more challenging frontier is zero-shot communication, an extension of zero-shot cooperation (Hu et al., 2020), in which agents must communicate effectively with novel partners, unseen during training. In Appendix A.9, we show how existing methods perform poorly in this settings and leave this challenging setup to future work."
        },
        {
            "heading": "7 CONCLUSION AND FUTURE WORK",
            "text": "This work introduces an alternative perspective for learning to communicate in decentralized MARL based on the relationship between sent and received messages within a trajectory. Drawing inspiration from multi-view learning, we ground communication using contrastive learning by considering agents\u2019 messages to be encoded views of the same state. We empirically show that our method leads to better performance through a more consistent, common protocol and learns to communicate more global state information. We believe this work solidifies contrastive learning as an effective perspective for learning to communicate and hope it invigorates research into contrastive methods for communication with a focus on consistency. Furthermore, by establishing the connection between multi-view SSL, which has traditionally focused on images, and communication in MARL, we hope to encourage more cross-domain research. Finally, we see contrastive learning as a potential method for simulating human language evolution, and hope to inspire research in this direction."
        },
        {
            "heading": "A APPENDIX",
            "text": ""
        },
        {
            "heading": "A.1 REINFORCEMENT LEARNING LOSS",
            "text": "In Equation 2, we use LRL to denote the RL loss. As our investigation is orthogonal to the choice of RL algorithm, this term can be any RL algorithm dependent on which algorithm is being used. In this work, we use Independent Asynchronous Actor Critic (IA2C) as our base algorithm, denoted as IAC in our experiments (Christianos et al., 2020). Given an agent i \u2208 N , it has a policy \u03c0i\u03d5 and value function V i\u03b8 , parameterized by parameters \u03d5 and \u03b8 respectively. The policy loss for agent i is defined as:\nL(\u03d5) = \u2212 log(\u03c0(ait|\u2126i(st);\u03d5)(rt + \u03b3V (\u2126i(st+1); \u03b8)\u2212 V (\u2126i(st); \u03b8) (4)\nwith the value function minimizes:\nL(\u03b8) = \u2225V (\u2126i(st); \u03b8)\u2212 (rt + \u03b3V (\u2126i(st+1); \u03b8))\u22252 (5)\nThese two losses are denoted as LRL due to space limit. In the case with communication, the local observation of an agent is simply augmented with received messages."
        },
        {
            "heading": "A.2 ENVIRONMENT DETAILS",
            "text": "Figure A.2 provides a visual illustration of the environments used.\nFigure 7: Visual illustration of the environments used. Left: Predator-Prey, taken from Koul (2019). Middle: Find-Goal, taken from Lin et al. (2021). Right: Traffic-Junction, taken from Singh et al. (2018)"
        },
        {
            "heading": "A.2.1 PREDATOR-PREY",
            "text": "We modify the Predator-Prey implementation by Koul (2019). Our Predator-Prey has a higher communication and coordination requirement than the original Predator-Prey environment. Specifically, for a prey to be captured, it has to be entirely surrounded (i.e. the prey cannot move to another grid position in any actions).\nHere, we use an 7x7 gridworld. In each agent\u2019s observation, it can only see the prey if it is within the field of view (3x3) and cannot see where other agents are. A shared reward of 10 is given for a successful capture and a penalty of -0.5 is given for a failed attempt. A -0.01 step penalty is also applied per step. Each agent has the actions of LEFT, RIGHT, UP. DOWN and NO-OP. The prey has the movement probability vector of [0.175, 0.175, 0.175, 0.175, 0.3] with each value corresponding to the probability of each action taken.\nAll algorithms are trained for 30 million environment steps with a maximum of 200 steps per episode."
        },
        {
            "heading": "A.2.2 FIND-GOAL",
            "text": "We use the Find-Goal environment implementation provided by Lin et al. (2021). The agents have the goal to find where the goal is in a 15x15 grid world with obstacles.\nUnlike in Lin et al. (2021), each agent has a 3x3 field of view (instead of 7x7) to make the task more difficult. Each agent receives a reward of 1 for reaching a goal and an additional reward of 5 if all agents reach the goal. We use a step penalty of -0.01 and an obstacle density of 0.15.\nAll algorithms are trained for 40 million environment steps with a maximum of 512 steps per episode."
        },
        {
            "heading": "A.2.3 TRAFFIC-JUNCTION",
            "text": "We use the Traffic-Junction environment implementation provided by Singh et al. (2018). The gridworld is 7x7 with 1 traffic junction. The rate of cars being added has a minimum and maximum of 0.1 and 0.3. We use the easy version with two arrival points and 5 agents. Agents are heavily penalized if a collision happens and have only two actions, namely gas and brake.\nAll algorithms are trained for 20 million environment steps with a maximum of 20 steps per episode."
        },
        {
            "heading": "A.3 ARCHITECTURE AND HYPERPARAMETERS",
            "text": "Figure 8 illustrates the components of the architecture used in this work, similar to (Lin et al., 2021). A message head is only used for algorithms with communication, namely CACL, AEComm, PL and DIAL. The Grounding Module refers to mechanisms to ground the messages produced by the message head, used in CACL and AEComm. Colored boxes denote components where gradients are applied to for a particular loss function. In the case of DIAL, gradients would flow from another agent to the agent that sends a message through the message head and message encoder. Unless specified otherwise, we fix all hidden layers to be a size of 32.\nWe experimented with using the output of the GRU, or hidden state, to condition the message head. Empirically we found that directly conditioning on the observation encoding, as in Lin et al. (2021), led to more stable learning dynamics.\nThe observation encoder output values of size 32. For Predator-Prey and Traffic Junction, a one-layer fully-connected neural network is used as observation encoder. For Find-Goal, same as Lin et al.\n(2021), we use a two-layer convolutional neural network followed by a 3-layer fully-connected neural network.\nFor the message encoder, it outputs values of size 8 in Predator-Prey and Find-Goal with one hidden layer. It outputs values of size 16 in Traffic-Junction with two hidden layers. These configurations are selected based on the best performance of the baseline communication learning algorithm used - DIAL. Messages received are concatenated before passing to message encoders. For all the methods with communication, they produce messages of length 4 (D = 4) with a sigmoid function as activation. All models are trained with the Adam optimizer (Kingma & Ba, 2014).\nTable 4 lists out the hyperparameters used for all the methods."
        },
        {
            "heading": "A.4 CACL CONTRASTIVE LOSS CURVE",
            "text": "Figure 9: CACL\u2019s contrastive loss over time during training for Predator-Prey\nFigure 9 shows the CACL\u2019s contrastive loss over time during training for Predator-Prey. This illustrates training convergence of the loss and improved separation of positive and negative samples between the start and end of training."
        },
        {
            "heading": "A.5 PREDATOR-PREY CAPTURE RATE",
            "text": "Table 3 shows each method\u2019s success rate in capturing preys for Predator-Prey. CACL outperforms the baselines by capturing the most preys out of the evaluation episodes."
        },
        {
            "heading": "A.6 POSITIVE LISTENING",
            "text": "This section describes the loss function we implemented for positive listening, based on Eccles et al. (2019). Given two policies \u03c0i and \u03c0i of agent i where the latter is the policy with messages zeroed out in the observations, and a trajectory \u03c4 of length T , the positive listening loss is written as:\nLPL = \u2212 1\n|T | T\u2211 j [\u2211 a\u2208Ai (|\u03c0i(a|\u03c4 ij)\u2212 \u03c0i(a|\u03c4 ij)|) + (\u03c0i(a|\u03c4 ij) log(\u03c0i(a|\u03c4 ij)) ] (6)\nwhere in inner summation, the first term is the L1 Norm and the second term is the cross entropy loss."
        },
        {
            "heading": "A.7 MESSAGE SIMILARITY FOR DIFFERENT GOAL LOCATIONS IN FIND-GOAL",
            "text": "To further support that CACL can encode global information better, we measure the message similarity between messages sent when the goal is in [1, 1] and when the goal is in other coordinates along the diagonal of the gridworld. As shown in Figure 10, CACL shows a clear downward trend in similarity as the distance from [1, 1] increases unlike AEComm. This means CACL learns to encode global locations differently despite learning only with egocentric views. This cooroborates with the Goal Location Test in Table 2 that CACL is able to encode global information better."
        },
        {
            "heading": "A.8 PROTOCOL REPRESENTATION PROBING: 1-LAYER",
            "text": "Table 5 shows the same results for the two probing tests in section 5.6 except here we use a 1-layer neural network instead of 2 layers. We observe significant dips in performance across all methods. Particularly, CACL becomes worse than the baselines in the easier Goal Visibility Test. However, CACL remains superior in the more difficult Goal Location test by an even bigger margin than the results in table 2."
        },
        {
            "heading": "A.9 ZERO-SHOT CROSS-PLAY",
            "text": "An advanced form of coordination is working with partners you have not seen during training (Hu et al., 2020). Previous work has focused on coordination through actions (Carroll et al., 2019; Lupu et al., 2021) or pre-test grounding with a common dataset (Gupta et al., 2021) but to our knowledge, no previous work has succeeded in learning a linguistic communication protocol that is robust to zero-shot partners. To assess this advanced robustness, we take trained agents from different methods and random seeds and evaluate them with each other (i.e., zero-shot cross-play) in Predator-Prey and Find-Goal. Given two communication learning methods, m1 and m2, we sample two agents from each method for Predator-Prey and for Find-Goal, we average over sampling two agents from one method and one agent from the other and vice-versa. For intra-method cross-play, m1 = m2, we evaluate agents that were trained with the same method but from different random seeds, so they have not been trained with each other. For inter-method cross-play, m1 \u0338= m2, we sample agents from two different methods and pair them with each other. Each pairing is evaluated for 10 random seeds each with 10 evaluation episodes. Given that agents are trained in self-play (Tesauro, 1994) without regard for cross-play, we expect severe performance dips.\nWe show mean and standard deviation across random seeds for Predator-Prey and Find-Goal in Tables 6 and 7, respectively. As expected, all pairings take a significant dip in performance when compared with the main results. Inter-method cross-play performance is particularly bad across all algorithms. However, notably, CACL outperforms other methods in intra-method cross-play, indicating that the protocols learned by CACL are generally more robust even across random seeds. In general, zero-shot linguistic communication is incredibly difficult and our results are far from optimal. Still, CACL shows promise and demonstrates that contrastive SSL methods can lead to better zero-shot communication and coordination."
        },
        {
            "heading": "A.10 BROADER IMPACT",
            "text": "More multi-agent learning systems will be deployed in the real-world as further progress is made in fields of multi-agent learning like MARL. We expect communication to play an essential role in these systems given how real-world problems are inherently complex and partially observable in most cases. Our focus on the decentralized communication setting contributes to the capability of learning more effective and consistent communication protocols. Having more consistent protocols improve mutual intelligibility and pave the way to multi-agent systems in which agents can communicate with unseen agents or even humans.\nOn the other hand, increasing adoption of such multi-agent learning systems could exacerbate certain risks. For instance, this could increase unemployment in a significant scale if systems operated by multiple humans like warehouses are replaced with multi-robot learning systems. It could also contribute to more advanced automated weaponry. In particular, given that our method explicitly considers messages sent from other agents in our protocol learning algorithm, it could encourage adversarial attacks which would lead to harmful behaviors and miscommunication, especially in mission-critical systems."
        }
    ],
    "year": 2023
}