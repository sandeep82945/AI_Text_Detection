{
    "abstractText": "Neural networks efficiently encode learned information within their parameters. Consequently, many tasks can be unified by treating neural networks themselves as input data. When doing so, recent studies demonstrated the importance of accounting for the symmetries and geometry of parameter spaces. However, those works developed architectures tailored to specific networks such as MLPs and CNNs without normalization layers, and generalizing such architectures to other types of networks can be challenging. In this work, we overcome these challenges by building new metanetworks \u2014 neural networks that take weights from other neural networks as input. Put simply, we carefully build graphs representing the input neural networks and process the graphs using graph neural networks. Our approach, Graph Metanetworks (GMNs), generalizes to neural architectures where competing methods struggle, such as multi-head attention layers, normalization layers, convolutional layers, ResNet blocks, and group-equivariant linear layers. We prove that GMNs are expressive and equivariant to parameter permutation symmetries that leave the input neural network functions unchanged. We validate the effectiveness of our method on several metanetwork tasks over diverse neural network architectures.",
    "authors": [
        {
            "affiliations": [],
            "name": "Derek Lim"
        },
        {
            "affiliations": [],
            "name": "Haggai Maron"
        }
    ],
    "id": "SP:8ef7ce3ce15fcf848defc54899029a115a28cf05",
    "references": [
        {
            "authors": [
                "Samuel Ainsworth",
                "Jonathan Hayase",
                "Siddhartha Srinivasa"
            ],
            "title": "Git re-basin: Merging models modulo permutation symmetries",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Bruno Andreis",
                "Soro Bedionita",
                "Sung Ju Hwang"
            ],
            "title": "Set-based neural network encoding",
            "venue": "arXiv preprint arXiv:2305.16625,",
            "year": 2023
        },
        {
            "authors": [
                "Peter W Battaglia",
                "Jessica B Hamrick",
                "Victor Bapst",
                "Alvaro Sanchez-Gonzalez",
                "Vinicius Zambaldi",
                "Mateusz Malinowski",
                "Andrea Tacchetti",
                "David Raposo",
                "Adam Santoro",
                "Ryan Faulkner"
            ],
            "title": "Relational inductive biases, deep learning, and graph networks",
            "venue": "arXiv preprint arXiv:1806.01261,",
            "year": 2018
        },
        {
            "authors": [
                "Matthias Bauer",
                "Emilien Dupont",
                "Andy Brock",
                "Dan Rosenbaum",
                "Jonathan Schwarz",
                "Hyunjik Kim"
            ],
            "title": "Spatial functa: Scaling functa to imagenet classification and generation",
            "venue": "arXiv preprint arXiv:2302.03130,",
            "year": 2023
        },
        {
            "authors": [
                "L\u00e9on Bottou",
                "Patrick Gallinari"
            ],
            "title": "A framework for the cooperation of learning algorithms",
            "venue": "Advances in neural information processing systems,",
            "year": 1990
        },
        {
            "authors": [
                "Adriano Cardace",
                "Pierluigi Zama Ramirez",
                "Francesco Ballerini",
                "Allan Zhou",
                "Samuele Salti",
                "Luigi Di Stefano"
            ],
            "title": "Neural processing of tri-plane hybrid neural fields",
            "venue": "arXiv preprint arXiv:2310.01140,",
            "year": 2023
        },
        {
            "authors": [
                "Eric R Chan",
                "Connor Z Lin",
                "Matthew A Chan",
                "Koki Nagano",
                "Boxiao Pan",
                "Shalini De Mello",
                "Orazio Gallo",
                "Leonidas J Guibas",
                "Jonathan Tremblay",
                "Sameh Khamis"
            ],
            "title": "Efficient geometry-aware 3d generative adversarial networks",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Ting Chen",
                "Simon Kornblith",
                "Mohammad Norouzi",
                "Geoffrey Hinton"
            ],
            "title": "A simple framework for contrastive learning of visual representations",
            "venue": "In International conference on machine learning,",
            "year": 2020
        },
        {
            "authors": [
                "Terrance DeVries",
                "Graham W Taylor"
            ],
            "title": "Improved regularization of convolutional neural networks with cutout",
            "venue": "arXiv preprint arXiv:1708.04552,",
            "year": 2017
        },
        {
            "authors": [
                "Cameron Diao",
                "Ricky Loynd"
            ],
            "title": "Relational attention: Generalizing transformers for graphstructured tasks",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Laurent Dinh",
                "Razvan Pascanu",
                "Samy Bengio",
                "Yoshua Bengio"
            ],
            "title": "Sharp minima can generalize for deep nets",
            "venue": "In International Conference on Machine Learning,",
            "year": 2017
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "Lucas Beyer",
                "Alexander Kolesnikov",
                "Dirk Weissenborn",
                "Xiaohua Zhai",
                "Thomas Unterthiner",
                "Mostafa Dehghani",
                "Matthias Minderer",
                "Georg Heigold",
                "Sylvain Gelly",
                "Jakob Uszkoreit",
                "Neil Houlsby"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Emilien Dupont",
                "Hyunjik Kim",
                "SM Eslami",
                "Danilo Rezende",
                "Dan Rosenbaum"
            ],
            "title": "From data to functa: Your data point is a function and you can treat it like one",
            "venue": "International Conference on Machine Learning (ICML),",
            "year": 2022
        },
        {
            "authors": [
                "Gabriel Eilertsen",
                "Daniel J\u00f6nsson",
                "Timo Ropinski",
                "Jonas Unger",
                "Anders Ynnerman"
            ],
            "title": "Classifying the classifier: dissecting the weight space of neural networks",
            "venue": "Proceedings of the European Conference on Artificial Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Rahim Entezari",
                "Hanie Sedghi",
                "Olga Saukh",
                "Behnam Neyshabur"
            ],
            "title": "The role of permutation invariance in linear mode connectivity of neural networks",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Ziya Erko\u00e7",
                "Fangchang Ma",
                "Qi Shan",
                "Matthias Nie\u00dfner",
                "Angela Dai"
            ],
            "title": "Hyperdiffusion: Generating implicit neural fields with weight-space diffusion",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV),",
            "year": 2023
        },
        {
            "authors": [
                "Ji\u0159\u0131\u0301 Fiala",
                "Jan Kratochv\u0131\u0301l"
            ],
            "title": "Locally constrained graph homomorphisms\u2014structure, complexity, and applications",
            "venue": "Computer Science Review,",
            "year": 2008
        },
        {
            "authors": [
                "Marc Finzi",
                "Max Welling",
                "Andrew Gordon Wilson"
            ],
            "title": "A practical method for constructing equivariant multilayer perceptrons for arbitrary matrix groups",
            "venue": "In International conference on machine learning,",
            "year": 2021
        },
        {
            "authors": [
                "Cedric Gegout",
                "Bernard Girau",
                "Fabrice Rossi"
            ],
            "title": "A mathematical model for feed-forward neural networks : theoretical description and parallel applications",
            "venue": "Research Report LIP RR-1995-23, Laboratoire de l\u2019informatique du paralle\u0301lisme,",
            "year": 1995
        },
        {
            "authors": [
                "Justin Gilmer",
                "Samuel S Schoenholz",
                "Patrick F Riley",
                "Oriol Vinyals",
                "George E Dahl"
            ],
            "title": "Neural message passing for quantum chemistry",
            "venue": "In International conference on machine learning,",
            "year": 2017
        },
        {
            "authors": [
                "Charles Godfrey",
                "Davis Brown",
                "Tegan Emerson",
                "Henry Kvinge"
            ],
            "title": "On the symmetries of deep learning models and their internal representations",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "William L Hamilton"
            ],
            "title": "Graph representation learning",
            "year": 2020
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Robert Hecht-Nielsen"
            ],
            "title": "On the algebraic structure of feedforward network weight spaces",
            "venue": "In Advanced Neural Computers,",
            "year": 1990
        },
        {
            "authors": [
                "Kurt Hornik"
            ],
            "title": "Approximation capabilities of multilayer feedforward networks",
            "venue": "Neural networks,",
            "year": 1991
        },
        {
            "authors": [
                "Weihua Hu",
                "Matthias Fey",
                "Hongyu Ren",
                "Maho Nakata",
                "Yuxiao Dong",
                "Jure Leskovec"
            ],
            "title": "Ogb-lsc: A large-scale challenge for machine learning on graphs",
            "venue": "arXiv preprint arXiv:2103.09430,",
            "year": 2021
        },
        {
            "authors": [
                "Sergey Ioffe",
                "Christian Szegedy"
            ],
            "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
            "venue": "In International conference on machine learning,",
            "year": 2015
        },
        {
            "authors": [
                "Jinwoo Kim",
                "Saeyoon Oh",
                "Seunghoon Hong"
            ],
            "title": "Transformers generalize deepsets and can be extended to graphs & hypergraphs",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Jinwoo Kim",
                "Dat Nguyen",
                "Seonwoo Min",
                "Sungjun Cho",
                "Moontae Lee",
                "Honglak Lee",
                "Seunghoon Hong"
            ],
            "title": "Pure transformers are powerful graph learners",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Jinwoo Kim",
                "Saeyoon Oh",
                "Sungjun Cho",
                "Seunghoon Hong"
            ],
            "title": "Equivariant hypergraph neural networks",
            "venue": "In European Conference on Computer Vision,",
            "year": 2022
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "arXiv preprint arXiv:1412.6980,",
            "year": 2014
        },
        {
            "authors": [
                "Boris Knyazev",
                "Michal Drozdzal",
                "Graham W Taylor",
                "Adriana Romero Soriano"
            ],
            "title": "Parameter prediction for unseen deep architectures",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Alex Krizhevsky"
            ],
            "title": "Learning multiple layers of features from tiny images",
            "venue": "URL https: //www.cs.toronto.edu/",
            "year": 2009
        },
        {
            "authors": [
                "Guillaume Leclerc",
                "Andrew Ilyas",
                "Logan Engstrom",
                "Sung Min Park",
                "Hadi Salman",
                "Aleksander Madry"
            ],
            "title": "Ffcv: Accelerating training by removing data bottlenecks",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Yann LeCun",
                "Bernhard Boser",
                "John Denker",
                "Donnie Henderson",
                "R. Howard",
                "Wayne Hubbard",
                "Lawrence Jackel"
            ],
            "title": "Handwritten digit recognition with a back-propagation network",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 1989
        },
        {
            "authors": [
                "Derek Lim",
                "Joshua David Robinson",
                "Lingxiao Zhao",
                "Tess Smidt",
                "Suvrit Sra",
                "Haggai Maron",
                "Stefanie Jegelka"
            ],
            "title": "Sign and basis invariant networks for spectral graph representation learning",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Or Litany",
                "Haggai Maron",
                "David Acuna",
                "Jan Kautz",
                "Gal Chechik",
                "Sanja Fidler"
            ],
            "title": "Federated learning with heterogeneous architectures using graph hypernetworks",
            "venue": "arXiv preprint arXiv:2201.08459,",
            "year": 2022
        },
        {
            "authors": [
                "Hanxiao Liu",
                "Karen Simonyan",
                "Yiming Yang"
            ],
            "title": "DARTS: Differentiable architecture search",
            "venue": "In International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "Luca De Luigi",
                "Adriano Cardace",
                "Riccardo Spezialetti",
                "Pierluigi Zama Ramirez",
                "Samuele Salti",
                "Luigi di Stefano"
            ],
            "title": "Deep learning on implicit neural representations of shapes",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Liheng Ma",
                "Chen Lin",
                "Derek Lim",
                "Adriana Romero-Soriano",
                "Puneet K Dokania",
                "Mark Coates",
                "Philip Torr",
                "Ser-Nam Lim"
            ],
            "title": "Graph inductive biases in transformers without message passing",
            "venue": "In ICML,",
            "year": 2023
        },
        {
            "authors": [
                "Haggai Maron",
                "Heli Ben-Hamu",
                "Nadav Shamir",
                "Yaron Lipman"
            ],
            "title": "Invariant and equivariant graph networks",
            "venue": "In International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "Luke Metz",
                "James Harrison",
                "C Daniel Freeman",
                "Amil Merchant",
                "Lucas Beyer",
                "James Bradbury",
                "Naman Agrawal",
                "Ben Poole",
                "Igor Mordatch",
                "Adam Roberts"
            ],
            "title": "Velo: Training versatile learned optimizers by scaling up",
            "venue": "arXiv preprint arXiv:2211.09760,",
            "year": 2022
        },
        {
            "authors": [
                "Thomas M\u00fcller",
                "Alex Evans",
                "Christoph Schied",
                "Alexander Keller"
            ],
            "title": "Instant neural graphics primitives with a multiresolution hash encoding",
            "venue": "ACM Transactions on Graphics (ToG),",
            "year": 2022
        },
        {
            "authors": [
                "Aviv Navon",
                "Aviv Shamsian",
                "Idan Achituve",
                "Ethan Fetaya",
                "Gal Chechik",
                "Haggai Maron"
            ],
            "title": "Equivariant architectures for learning in deep weight spaces",
            "venue": "International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Adam Paszke",
                "Sam Gross",
                "Soumith Chintala",
                "Gregory Chanan",
                "Edward Yang",
                "Zachary DeVito",
                "Zeming Lin",
                "Alban Desmaison",
                "Luca Antiga",
                "Adam Lerer"
            ],
            "title": "Automatic differentiation in pytorch",
            "year": 2017
        },
        {
            "authors": [
                "William Peebles",
                "Ilija Radosavovic",
                "Tim Brooks",
                "Alexei A Efros",
                "Jitendra Malik"
            ],
            "title": "Learning to learn with generative models of neural network checkpoints",
            "venue": "arXiv preprint arXiv:2209.12892,",
            "year": 2022
        },
        {
            "authors": [
                "Ladislav Ramp\u00e1\u0161ek",
                "Michael Galkin",
                "Vijay Prakash Dwivedi",
                "Anh Tuan Luu",
                "Guy Wolf",
                "Dominique Beaini"
            ],
            "title": "Recipe for a general, powerful, scalable graph transformer",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Siamak Ravanbakhsh",
                "Jeff Schneider",
                "Barnabas Poczos"
            ],
            "title": "Equivariance through parametersharing",
            "venue": "In International conference on machine learning,",
            "year": 2017
        },
        {
            "authors": [
                "Konstantin Sch\u00fcrholt",
                "Diyar Taskiran",
                "Boris Knyazev",
                "Xavier Gir\u00f3-i Nieto",
                "Damian Borth"
            ],
            "title": "Model zoos: A dataset of diverse populations of neural network models",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Towaki Takikawa",
                "Joey Litalien",
                "Kangxue Yin",
                "Karsten Kreis",
                "Charles Loop",
                "Derek Nowrouzezahrai",
                "Alec Jacobson",
                "Morgan McGuire",
                "Sanja Fidler"
            ],
            "title": "Neural geometric level of detail: Real-time rendering with implicit 3D shapes",
            "year": 2021
        },
        {
            "authors": [
                "Veronika Thost",
                "Jie Chen"
            ],
            "title": "Directed acyclic graph neural networks",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Thomas Unterthiner",
                "Daniel Keysers",
                "Sylvain Gelly",
                "Olivier Bousquet",
                "Ilya Tolstikhin"
            ],
            "title": "Predicting neural network accuracy from weights",
            "venue": "arXiv preprint arXiv:2002.11448,",
            "year": 2020
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Clement Vignac",
                "Igor Krawczuk",
                "Antoine Siraudin",
                "Bohan Wang",
                "Volkan Cevher",
                "Pascal Frossard"
            ],
            "title": "Digress: Discrete denoising diffusion for graph generation",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Saining Xie",
                "Alexander Kirillov",
                "Ross Girshick",
                "Kaiming He"
            ],
            "title": "Exploring randomly wired neural networks for image recognition",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2019
        },
        {
            "authors": [
                "Jiaxuan You",
                "Jure Leskovec",
                "Kaiming He",
                "Saining Xie"
            ],
            "title": "Graph structure of neural networks",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "David W. Zhang",
                "Miltiadis Kofinas",
                "Yan Zhang",
                "Yunlu Chen",
                "Gertjan J. Burghouts",
                "Cees G.M. Snoek"
            ],
            "title": "Neural networks are graphs! graph neural networks for equivariant processing of neural networks",
            "year": 2023
        },
        {
            "authors": [
                "Muhan Zhang",
                "Shali Jiang",
                "Zhicheng Cui",
                "Roman Garnett",
                "Yixin Chen"
            ],
            "title": "D-vae: A variational autoencoder for directed acyclic graphs",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Allan Zhou",
                "Kaien Yang",
                "Kaylee Burns",
                "Yiding Jiang",
                "Samuel Sokota",
                "J Zico Kolter",
                "Chelsea Finn"
            ],
            "title": "Permutation equivariant neural functionals",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2023
        },
        {
            "authors": [
                "Allan Zhou",
                "Kaien Yang",
                "Yiding Jiang",
                "Kaylee Burns",
                "Winnie Xu",
                "Samuel Sokota",
                "J Zico Kolter",
                "Chelsea Finn"
            ],
            "title": "Neural functional transformers",
            "venue": "arXiv preprint arXiv:2305.13546,",
            "year": 2023
        },
        {
            "authors": [
                "Zhou et al",
                "2023a). Finally",
                "Zhang"
            ],
            "title": "2023) evaluate their method empirically on simple networks, but they have limited theoretical guarantees and empirical results on other networks. NNs as Graphs. Many works consider neural networks as graphs, including early works that aimed at developing unified frameworks for neural networks (Bottou",
            "year": 2024
        },
        {
            "authors": [
                "G. group be"
            ],
            "title": "Suppose the group acts via representations \u03c11(g) \u2208 GL(n1) on the input and \u03c12(g) \u2208 GL(n2) on the output. Then the set of all G-equivariant linear maps, i.e. the set of linear maps T : R1 \u2192 R2 such that T \u25e6 \u03c11(g) = \u03c12(g) \u25e6 T for all g \u2208 G, forms a vector space of dimension that we denote as db (Maron et al., 2019",
            "venue": "Finzi et al.,",
            "year": 2021
        },
        {
            "authors": [
                "f\u0303L"
            ],
            "title": "F\u0303i} universally approximates F",
            "year": 2020
        },
        {
            "authors": [
                "Unterthiner"
            ],
            "title": "2020) are the mean, variance, and qth percentiles for q \u2208",
            "year": 2024
        },
        {
            "authors": [
                "Zhou"
            ],
            "title": "cross entropy loss between the predicted and true accuracy; each metanet has a sigmoid nonlinearity at the end to ensure that its input is within [0, 1]. Small CNN Experiments To compare against previously proposed metanetworks, we also run experiments on predicting accuracy of the small CNNs trained by Unterthiner et al. (2020), following the experimental setup",
            "year": 2023
        },
        {
            "authors": [
                "Zhou"
            ],
            "title": "2023a) for the experiments in Section 4.2. As in their work, we train the metanets for 50 000 iterations with a batch size of 32, using the Adam optimizer with .001 learning",
            "year": 2023
        },
        {
            "authors": [
                "Navon"
            ],
            "title": "2023) for the self-supervised learning task. In particular, as they do in their experiments, we choose hyperparameters for our graph metanet such that the number of trainable metanetwork parameters is about 100 000. Thus, we use a graph metanetwork with 3 GNN layers and a hidden dimension of 76",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Neural networks are well-established for predicting, generating, and transforming data. A newer paradigm is to treat the parameters of neural networks themselves as data. This insight inspired researchers to suggest neural architectures that can predict properties of trained neural networks (Eilertsen et al., 2020), generate new networks (Erkoc\u0327 et al., 2023), optimize networks (Metz et al., 2022), or otherwise transform them (Navon et al., 2023; Zhou et al., 2023a). We refer to these neural networks that process other neural networks as metanetworks, or metanets for short.\nMetanets enable new applications, but designing them is nontrivial. A common approach is to flatten the network parameters into a vector representation, neglecting the input network structure. More generally, a prominent challenge in metanet design is that the space of neural network parameters exhibits symmetries. For example, permuting the neurons in the hidden layers of a Multilayer Perceptron (MLP) leaves the network output unchanged (Hecht-Nielsen, 1990). Ignoring these symmetries greatly degrades the metanet performance (Peebles et al., 2022; Navon et al., 2023). Instead, equivariant metanets respect these symmetries, so that if the input network is permuted then the metanet output is permuted in the same way.\nRecently, several works have proposed equivariant metanets that have shown significantly improved performance (Navon et al., 2023; Zhou et al., 2023a;b). However, these networks typically require highly specialized, hand-designed layers that can be difficult to devise. A careful analysis of its symmetries is necessary for any input architecture, followed by the design of corresponding equivariant metanet layers. Generalizing this procedure to more complicated network architectures is\n\u2217Work done while interning at NVIDIA.\ntime-consuming and nontrivial, so existing methods can only process simple input networks with linear and convolutional layers, and cannot process standard modules such as normalization layers or residual connections \u2014 let alone more complicated modules such as attention blocks. Moreover, these architectures cannot directly process input neural networks with varying architectures, such as those with different numbers of layers or hidden units.\nThis work offers a simple and elegant solution to metanet design that respects neural network parameter symmetries. As in the concurrent work of Zhang et al. (2023), our technique\u2019s crux is representing an input neural network as a graph (see Figure 1). We show how to efficiently transform a neural network into a graph such that standard techniques for learning on graphs \u2013 e.g., Message Passing Neural Networks Gilmer et al. (2017); Battaglia et al. (2018) or Graph Transformers (Rampa\u0301s\u030cek et al., 2022) \u2013 will be equivariant to the parameter symmetries. One of our key contributions is in developing a compact parameter graph representation, which in contrast to established computation graphs allows us to handle parameter-sharing layers like convolutions and attention layers without scaling with the activation count. While past work is typically restricted to processing MLPs and simple Convolutional Neural Networks (CNNs) (LeCun et al., 1989), we validate experimentally that our graph metanets (GMNs) generalize to more complicated networks such as Transformers (Vaswani et al., 2017), residual networks (He et al., 2016), normalization layers (Ioffe & Szegedy, 2015; Ba et al., 2016; Wu & He, 2018), general group-equivariant architectures (Ravanbakhsh et al., 2017) like Deep Sets (Zaheer et al., 2017), and more.\nWe prove theoretically that our metanets are equivariant to permutation symmetries in the input network, which we formulate via neural graph automorphisms (Section 2.2). This generalizes the hidden neuron permutations in MLPs and channel permutations of CNNs covered in prior work to arbitrary feedforward neural architectures. We further prove that our metanets operating on computation graphs are at least as expressive as prior methods \u2014 meaning they can approximate them to arbitrary accuracy \u2014 and can express the forward pass of any input feedforward neural network, generalizing a result of Navon et al. (2023) (Section 3).\nEmpirical evaluations show that our approach solves a variety of metanet tasks with diverse neural architectures. As part of this effort, we trained new datasets of diverse image classifiers, including 2D CNNs, 1D CNNs, DeepSets, ResNets, and Vision Transformers. Our method is easier to implement than past equivariant metanets while being at least as expressive, and it is applicable to more general input architectures. Crucially, our GMNs achieve strong quantitative performance across all tasks we explored."
        },
        {
            "heading": "2 GRAPH AUTOMORPHISM-BASED METANETS",
            "text": "We first explain how neural networks can be encoded as Directed Acyclic Graphs (DAGs). There are many choices in representing neural networks as DAGs, perhaps the most common being a\ncomputation graph (see Appendix C). This work introduces a more compact representation, referred to as parameter graphs.\nWe then introduce one of the paper\u2019s main concepts \u2014 Neural DAG Automorphisms. This concept generalizes previously studied symmetry groups for MLPs and CNNs to arbitrary feedforward architectures represented as DAGs. To conclude this section, we describe our GNN-based metanet that operates over these graphs and is equivariant to Neural DAG Automorphisms. A glossary of our notation is provided in Appendix Table 4.\nMotivation. Certain permutations of parameters in neural networks do not change the function they parameterize. For example, consider a simple MLP defined such that f\u03b8(x) := W2\u03c3(W1x) with one hidden layer, where \u03b8 := (W2,W1) are the parameters of the network, and \u03c3 is a nonlinear element-wise activation function. For any permutation matrix P , if we define \u03b8\u0303 := (W2P\n\u22a4,PW1), then for all inputs x, we have f\u03b8(x) = f\u03b8\u0303(x). This P corresponds to a permutation of the order of the hidden neurons, which is well-known not to affect the network function (Hecht-Nielsen, 1990). Likewise, permuting the hidden channels of a CNN does not affect the network function (Navon et al., 2023; Entezari et al., 2022; Ainsworth et al., 2023).\nWhile these permutation symmetries for MLPs and simple CNNs are easy to determine by manual inspection, it is more difficult to determine the symmetries of general architectures. For example, simple residual connections introduce additional neuron dependencies across layers. Instead of manual inspection, we show that graph automorphisms (i.e. graph isomorphisms from a graph to itself) on DAGs representing feedforward networks correspond to permutation parameter symmetries. From this observation, it can be shown that GNNs acting on these DAGs are equivariant to their permutation symmetries.\nOverview of our approach. See Figure 1 for an illustration. Given a general input feedforward neural network, we first encode it as a graph in which each parameter is associated with an edge and then process this graph with a GNN. The GNN outputs a single fixed-length vector or predictions for each node or edge depending on the learning task. For instance, one graph-level task is to predict the scalar accuracy of an input neural network on some task. An edge-level task is to predict new weights for an input neural network to change its functionality somehow.\nWe now discuss the graph construction, the symmetries of these graphs, and the GNN we use.\n2.1 GRAPH CONSTRUCTION FOR GENERAL FEEDFORWARD ARCHITECTURES\n2.1.1 COMPUTATION GRAPHS\nEvery feedforward neural network defines a computation graph as a DAG (Zhang et al., 2023), where nodes are neurons and edges hold neural network parameter weight values (see Fig. 1 and Fig. 2). Thus, this gives a method to construct a weighted graph. However, the computation graph approach has some downsides. For one, it may be expensive due to weightsharing: for instance, a 1-input-and-outputchannel 2D-convolution layer with a kernel size of 2 has 4 parameters, but the 4 parameters are used in the computation of many activations (e.g. 1024 activations for a 32\u00d7 32 input grid). Further, we may want to add input node and edge features \u2013 such as layer number \u2013 to help performance and expressive power1. Figure 2 illustrates an example of a (small) computation\ngraph for convolutions (for visual clarity, we exclude bias terms). More details, including the exact formal correspondence between feedforward neural network functions and computation graphs, are given in Appendix C.1.\n1In Section 3, our proofs rely on these features to show graph metanets can express existing metanets."
        },
        {
            "heading": "2.1.2 PARAMETER GRAPHS",
            "text": "To deal with the challenges of computation graphs, we propose alternate neural network graph constructions \u2014 some examples of which are shown in Figure 3 \u2014 that are (a) efficient and (b) allow expressive metanets. We call these graphs parameter graphs because we design the graphs so that each parameter is associated with a single edge of the graph (whereas a parameter may be associated to many edges in a computation graph).\nWe design modular subgraphs that can be created for each layer and then stitched together. Our goal is to design parameter graphs with at most one edge for each parameter in the neural network. Additionally, they should capture the correct set of parameter permutation symmetries. Full details are in Appendix B, but we discuss the major points behind the design of a selection of parameter graphs here.\nLinear layers. Figure 1 depicts three linear layers in sequence. Each linear layer\u2019s parameter subgraph coincides with its computation graph, but even so, there are important design choices to be made. Bias parameters could be encoded as node features as done by Zhang et al. (2023) or as self-loop edges on each neuron. Instead, we include a bias node for each layer and encode the bias parameters as edges from that node to the corresponding neurons in the layer.\nThe bias node approach is preferable because the self-loop or node feature approaches to encoding biases can hinder the expressive power of the metanet. In particular, the results of Navon et al. (2023) and Zhou et al. (2023a) show that the permutation equivariant linear metanet maps for MLP inputs include message-passing-like operations where the representation of a bias parameter is updated via the representations of other bias parameters in the same layer. Using a message passing GNN on a graph with bias nodes allows us to naturally express these operations in a single graph metanet layer, as explained in Appendix D.1.3.\nConvolution layers. Convolutions and other group equivariant linear layers leverage parameter sharing, where the same parameter is used to compute many activations (Ravanbakhsh et al., 2017). Therefore, representing convolutions as a computation graph introduces scaling issues and binds the network graph to a choice of input size. To avoid this, we develop a succinct and expressive parameter graph representation of convolutions. This is depicted in the left of Figure 3 for a convolution layer with a 2\u00d7 2 filter, one input channel, and two output channels. Note that we have two output channels here, unlike the computation graph in Figure 2, where there is only one.\nOur subgraph construction allocates a node for each input and output channel. We then have parallel edges between each input and output node for each spatial location in the filter kernel \u2014 making this a multigraph. Bias nodes are added as in the linear layers. This subgraph contains exactly one edge for each parameter in the convolution layer while capturing the parameter permutation symmetries as graph automorphisms. The spatial position of each weight within the kernel is included by a positional encoding in the corresponding edge feature.\nIn Section 4.1, we use our graph metanets to process 1D and 2D convolutional networks, as well as DeepSets networks that consist of permutation equivariant linear layers (Zaheer et al., 2017).\nMulti-head attention layers. Attention layers (Vaswani et al., 2017) also exhibit parameter sharing across sequence length. As with convolutions, we design an efficient subgraph representation where each parameter appears as a single edge. There is one node for each feature dimension of the input,\nvectors used in attention computation, and output. There are two sets of edges: a set that corresponds to the query, key, and value maps, and a second set corresponding to the final output mapping.\nIn the middle of Figure 3 we show a single-headed self attention layer, with bias nodes excluded for visual clarity. Generalizing this to multi-head attention simply requires adding additional node features to the middle layer that indicate which head each node belongs to.\nResidual layers. A residual connection does not introduce any additional parameters, but it does affect the permutation symmetries in the parameter space of the network. Therefore, it is crucial to represent residual connections as additional parameter-free edges within the parameter graph. The top-right of Figure 3 shows a residual connection bypassing a linear layer. The edges are drawn as dashed lines to emphasize that there are no associated parameters. As is natural in the computation graph, we fix the weight of the residual edge to be 1."
        },
        {
            "heading": "2.2 NEURAL DAG AUTOMORPHISMS",
            "text": "The prior section describes how to represent (feedforward) neural networks as DAGs. A natural question from an equivariant learning perspective is: what are the symmetries of this DAG representation? Specifically, we consider graph automorphisms, which are structure-preserving transformations of a graph unto itself. A neural DAG automorphism of a DAG (V,E) associated with a neural network is a permutation of nodes \u03d5 : V \u2192 V that preserves adjacency, preserves types of nodes (e.g. \u03d5 cannot map hidden neurons to input neurons), and preserves weight-sharing constraints (i.e. tied weights must still be tied after permuting the endpoints with the automorphism); see Appendix C.2 for more details. Every automorphism \u03d5 also induces a permutation of edges \u03d5e : E \u2192 E, where edge (i, j) is mapped to \u03d5e((i, j)) = (\u03d5(i), \u03d5(j)). Intuitively, a neural DAG automorphism represents a permutation of the neural network parameters via the induced edge permutation, \u03d5e. We write \u03a6(\u03b8) to represent this permutation on the parameters themselves, meaning \u03a6(\u03b8)(\u03d5(i),\u03d5(j)) = \u03b8(i,j). Hidden node permutations in MLPs and hidden channel permutations in CNNs are special cases of neural DAG automorphisms, which we explain in Appendix C.3. To formalize these notions, we show that every neural DAG automorphism \u03d5 of a computation graph is a permutation parameter symmetry, in the sense that the induced parameter permutation \u03a6 does not change the neural network function. Figure 4 illustrates several neural DAG automorphisms.\nProposition 1. For any neural DAG automorphism \u03d5 of a computation graph, the neural network function is left unchanged: \u2200x \u2208 X , f\u03b8(x) = f\u03a6(\u03b8)(x).\nA proof is given in Appendix C.4. Recall that our goal is to design metanets equivariant to parameter permutation symmetries. Proposition 1 shows to achieve this it is necessary to design metanets that are equivariant to neural DAG automorphisms. Graph metanets achieve this exactly since GNNs are equivariant to permutation symmetries of graphs (Maron et al., 2019).\nProposition 2. Graph metanets are equivariant to parameter permutations induced by neural DAG automorphisms.\nThese results formally justify using graph metanets on computation graphs for equivariance to parameter permutation symmetries. Now, noting the parameters are stored as edge features in our computation and parameter graphs, we design graph neural networks that operate on these DAGs."
        },
        {
            "heading": "2.3 FORMULATING METANETS AS GNNS",
            "text": "After constructing the input graphs, we use a GNN as our metanet to learn representations and perform downstream tasks. We desire GNNs that learn edge representations since the input neural network parameters are placed on the edges of the constructed graph. While countless GNN variants have been developed in the last several years (Hamilton, 2020), most learn node representations.\nFor simplicity, we mostly loosely follow the general framework of Battaglia et al. (2018), which defines general message-passing GNNs that update node, edge, and global features. For a graph, let vi \u2208 Rdx be the feature of node i, e(i,j) \u2208 Rde a feature of the directed edge (i, j), u \u2208 Rdu be a global feature associated to the entire graph, and let E be the set of edges in the graph. The directed edge (i, j) represents an edge starting from j and ending at i. We allow multigraphs, where there can be several edges (and hence several edge features) between a pair of nodes (i, j); thus, we let E(i,j) denote the set of edge features associated with (i, j). Then, a general GNN layer updating these features can be written as:\nvi \u2190 MLPv2 vi, \u2211 j,e(i,j)\u2208E(i,j) MLPv1 (vi,vj , e(i,j),u),u  (1) e(i,j) \u2190 MLPe(vi,vj , e(i,j),u) (2)\nu\u2190 MLPu (\u2211\ni vi, \u2211 e\u2208E e,u\n) (3)\nIntuitively, node features are updated by message passing along neighbors (Equation 1), the features of adjacent nodes update the features of the edges connecting them (Equation 2), and the global feature u is updated with aggregations of all features (Equation 3). While our graphs are DAGs, we are free to use undirected edges by ensuring that (i, j) \u2208 E implies (j, i) \u2208 E with e(i,j) = e(j,i). We often choose to allow message passing between layers in both directions.\nFor parameter-level metanet tasks with per-parameter predictions, we let the prediction be the final feature e(i,j) of the parameter\u2019s corresponding edge. We pool edge features for network-level metanet tasks where a fixed-length vector is the final prediction for each graph. We can pool node features but found pooling edge features sufficient. We do not use global features for empirical results, but we find them crucial for the expressive power results of Proposition 3."
        },
        {
            "heading": "3 EXPRESSIVE POWER OF GRAPH METANETS (GMNS)",
            "text": "Ideally, one does not sacrifice expressive power when restricting a neural architecture to satisfy group equivariance constraints. We want our metanet architecture to be powerful enough to learn useful functions of network parameters. To this end, we first show that GMNs can express two existing equivariant metanets on MLP inputs. Consequently, GMNs are at least as expressive as these approaches. Proposition 3. On MLP inputs (where parameter graphs and computation graphs coincide), graph metanets can express StatNN (Unterthiner et al., 2020) and NP-NFN (Zhou et al., 2023a).\nThe proof is given in Appendix D.1. StatNN is based on permutation-invariant statistics of each weight or bias, which graph metanetworks can easily compute. The linear layers of NP-NFN consist of local message-passing-like operations and global operations that the global feature can capture.\nSecond, we show that GMNs can simulate the forward pass of an input neural network represented by the DAG of its computation graph (Appendix C.1). This substantially generalizes a result of Navon et al. (2023), who show that their DWSNets can simulate the forward pass of MLPs. Proposition 4. On computation graph inputs, graph metanets can express the forward pass of any input feedforward neural network as defined in Section C.1.\nThe above result applies only to GMNs operating on computation graphs. This makes it directly applicable to the parameter graphs for MLPs as they coincide with the computation graph. However, we expect that a similar result is possible for more general parameter graphs. We leave formal proof of this to future work."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "4.1 PREDICTING ACCURACY FOR VARYING ARCHITECTURES",
            "text": "Task. As in prior works (Unterthiner et al., 2020; Zhou et al., 2023a), we train metanets to predict the test accuracy of input neural networks. We consider image classification neural networks trained on the CIFAR-10 dataset (Krizhevsky, 2009), and the metanet task is to take the parameters of an input network and predict the network\u2019s image classification accuracy on the CIFAR-10 test set.\nDatasets. To demonstrate the flexibility of our graph metanets, we train image classifiers that significantly vary in size and architecture. For our \u201cVarying CNNs\u201d dataset, we train about 30 000 basic 2D CNNs varying in common architectural design choices like hidden dimension, number of convolution layers, number of fully connected layers, and type of normalization layers (BatchNorm (Ioffe & Szegedy, 2015) or GroupNorm (Wu & He, 2018)).\nFor our \u201cDiverse Architectures\u201d dataset, we also train four other diverse image classifiers to test our model\u2019s generalization with multiple architectures: (i) basic 1D CNNs treating images as raster ordered sequences of pixels, (ii) DeepSets treating images as pixel sets, which maintains pixel position information with positional encodings (Zaheer et al., 2017), (iii) ResNets (He et al., 2016), and (iv) Vision Transformers (Dosovitskiy et al., 2021). This dataset also totals about 30 000 networks. The Vision Transformers\u2019 patch embedding module can be viewed as a convolution, which is how we encode this module as a graph (the Transformer layer graph encodings are described in Figure 7).\nFigure 5 shows these trained networks span a wide range of accuracies, between 10% and 77.5% for Varying CNNs and between 7.5% and 87.8% for Diverse Architectures. Also, the number of parameters in these networks ranges from 970 to 21 165 in Varying CNNs and from 970 to 87 658 for Diverse Architectures. These networks are significantly more diverse and achieve higher accuracy than the dataset of Unterthiner et al. (2020), who train small CNNs of a fixed architecture that obtain at most 56% test accuracy on CIFAR-10. Also, our Diverse Architectures dataset contains many more network types and modules than the datasets of Eilertsen et al. (2020) and Schu\u0308rholt et al. (2022), which are limited to CNNs.\nMetanetworks. For our graph metanet, we consider a simple message passing GNN as in Section 2.3 that does not use a global graph feature. To obtain an invariant prediction, we mean-pool over edge representations.\nWe cannot apply competing permutation equivariant methods like DWSNet (Navon et al., 2023) or NFN (Zhou et al., 2023a), because they cannot process normalization layers, input neural networks of different sizes, or modules like self-attention. Instead, as a baseline, we consider the Deep Meta Classifier (DMC) from Eilertsen et al. (2020), which vectorizes an input network\u2019s parameters and processes it with a 1D CNN, allowing the use of differently sized networks. We also consider a baseline that treats the parameters as a set and applies a DeepSets network (Zaheer et al., 2017) to output a scalar prediction. Note that the DeepSets baseline is invariant to permutation parameter symmetries, but it is also invariant to permutations that do not correspond to parameter symmetries (which significantly outnumber permutation parameter symmetries), so it has low expressive power.\nWe evaluate our method and the two baselines across six different data settings. Using both the Varying CNN dataset and the Diverse Architectures dataset. We explore training on about half of the input networks, training on only 10% of this previous split, and an out-of-distribution (OOD) generalization setting where we train on a reduced set of architectures that have smaller hidden dimension than the held-out architectures.\nResults. See Table 1 for quantitative results and Figure 8 in the Appendix for scatterplots in the OOD setting on Diverse Architectures. We report the R-Squared value and the Kendall \u03c4 coefficient of the predicted generalization against the true generalization. Our GMNs outperform both baselines in predicting accuracy for input networks across all six data settings. When we restrict to 10% of the full training set size, we see that GMNs generalize substantially better than the baselines. This performance gap is maintained in the more challenging OOD generalization setting, where the nonequivariant DMC performs very poorly in R2. The improved GMN performance could be from high expressive power (which the DeepSets baseline lacks), with better generalization due to equivariance to parameter permutation symmetries (which DMC lacks)."
        },
        {
            "heading": "4.2 EDITING 2D INRS",
            "text": "Table 2: Test MSE (lower is better) for editing 2D INRs, following the methodology of (Zhou et al., 2023a). Results of baselines are from (Zhou et al., 2023a;b).\nMetanetwork Contrast Dilate\nMLP .031 .306 MLP-Aug .029 .307 NFN-PT .029 .197 NFN-HNP .0204 \u00b1.0000 .0706 \u00b1.0005 NFN-NP .0203 \u00b1.0000 .0693 \u00b1.0009 NFT .0200 \u00b1.0002 .0510 \u00b1.0004 GMN (ours) .0197 \u00b1.0000 .0603 \u00b1.0010\nTable 3: Results for self-supervised learning of neural net representations, in test MSE of a linear regressor on the learned representations. Numbers besides GMN from Navon et al. (2023).\nMetanetwork Test MSE\nMLP 7.39 \u00b1.19 MLP + Perm. aug 5.65 \u00b1.01 MLP + Alignment 4.47 \u00b1.15 INR2Vec (Arch.) 3.86 \u00b1.32 Transformer 5.11 \u00b1.12 DWSNets 1.39 \u00b1.06 GMN (ours) 1.13 \u00b1.08\nNext, we empirically test the ability of GMNs to process simple MLP inputs to compare against less-flexible permutation equivariant metanets such as NFN (Zhou et al., 2023a) and NFT (Zhou et al., 2023b). For this, we train metanetworks on the 2D INR editing tasks of (Zhou et al., 2023a), where the inputs are weights of an INR representing an image, and the outputs are weights of an INR representing the image with some transformation applied to it.\nTable 2 shows our results. We see that our GMNs outperform most metanetworks on these simple input networks. In particular, GMN outperforms all methods on the Contrast task, and is only beat by NFT on the Dilate task."
        },
        {
            "heading": "4.3 SELF-SUPERVISED LEARNING WITH INRS",
            "text": "We also compare GMNs against another less-flexible permutation equivariant metanet, DWSNets (Navon et al., 2023), in the self-supervised learning experiments of Navon et al. (2023). Here, the input data are MLPs fit to sinusoidal functions of the form x 7\u2192 a sin(bx), where a, b \u2208 R are varying parameters. The goal is to learn a metanet encoder that gives strong representations of input networks, using a contrastive-learning framework similar to SimCLR (Chen et al., 2020). Our GMNs learn edge representations, which are then mean-pooled to get a vector representation of each input network. The downstream task for evaluating these representations is fitting a linear model on the metanet representations to predict the coefficients a and b of the sinusoid that the input MLP represents.\nTable 3 shows the results. GMNs outperform all baselines in this task. Thus, even in the most advantageous setting for competitors, which they are restricted to, GMNs are empirically successful at metanet tasks."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "In this work, we proposed Graph Metanetworks, an approach to processing neural networks with theoretical and empirical benefits. Theoretically, our approach satisfies permutation parameter symmetry equivariance while having provably high expressive power. Empirically, we can process diverse neural architectures, including layers that appear in state-of-the-art models, and we outperform existing metanetwork baselines across all tasks that we evaluated.\nLimitations We make substantial progress towards improving the scalability of GMNs by introducing parameter graphs. However, large neural networks can have billions of parameters and processing them may be more difficult. We believe that standard scalable GNN methods can be used to scale to the billion parameter regime (as existing GNNs are capable of processing graphs with billions of edges (Hu et al., 2021) using modest computing resources), but we have not yet tried to use Graph Metanets at such a scale. Additionally, we argue that parameter graphs are easier to design than specialized architectures of prior work, but we do not give formal constraints on their design. Further work investigating parameter graphs is promising; for instance, our theory of neural DAG automorphisms depends on the more expensive computation graphs, but it could possibly be extended to parameter graphs. Moreover, our approach only accounts for permutation-based parameter symmetries and does not account for e.g. symmetries induced by scaling weights in ReLU networks (Dinh et al., 2017; Godfrey et al., 2022).\nFuture work Our graph-based approach to metanets is promising for future development. This paper mostly uses a basic message-passing GNN architecture, which can be further improved using the many GNN improvements in the literature. Furthermore, our theoretical developments largely apply to computation graphs and we expect future work can extend these results to the more practical parameter graphs. Since graph metanets can process modern neural network layers like self-attention and spatial INR feature grids, they can be used to process and analyze state-of-the-art neural networks. Further, future work could try applying graph metanets to difficult yet impactful metanets tasks such as pruning, learned optimization, and finetuning pretrained models."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "We thank Matan Atzmon, Jiahui Huang, Karsten Kreis, Francis Williams, and Xiaohui Zeng for helpful comments. We would also like to thank Jun Gao, Or Perel, and Frank Shen for helpful input on some of our early INR experiments. DL is funded by an NSF Graduate Fellowship. HM is the Robert J. Shillman Fellow, and is supported by the Israel Science Foundation through a personal grant (ISF 264/23) and an equipment grant (ISF 532/23)."
        },
        {
            "heading": "A RELATED WORK",
            "text": "Metanetworks. Recently, several works developed metanets equivariant to permutation parameter symmetries of simple networks. DWSNets (Navon et al., 2023) and NFN (Zhou et al., 2023a) derive the form of linear layers that are group-equivariant to permutation parameter symmetries of simple MLPs (and NFN also handles CNNs). NFTs (Zhou et al., 2023b) use special parameter-permutationequivariant attention layers and layer-number features to build Transformer-like metanets.\nOther types of metanets have been developed, including those based on statistics such as weight mean and standard deviation (Unterthiner et al., 2020), 1D convolutional models on the flattened weight vector (Eilertsen et al., 2020), hierarchical LSTMs and per-parameter MLPs (Metz et al., 2022), and set functions applied to chunked parameter sets (Andreis et al., 2023).\nSpecial metanets have also been developed to process implicit neural representations (INRs) representing shapes, images, or other types of continuous data (Dupont et al., 2022; Luigi et al., 2023; Bauer et al., 2023). However, these approaches require special training procedures for the input neural networks and/or access to forward passes of the input network. In contrast, graph metanets can process datasets of independently trained neural networks of diverse architectures without modifying the inputs or requiring forward passes through the inputs, as we demonstrate in our experiments.\nThe recent work of Zhang et al. (2023) also proposes graph neural networks as metanets. They use the computation graph construction of MLPs \u2013 not our general parameter graphs \u2013 which allows us to scale efficiently for models with parameter sharing (like CNNs). Moreover, they adopt a different graph construction for bias nodes, where our approach is inspired by expressive power considerations from the form of the parameter-permutation-equivariant linear maps (Navon et al.,\n2023; Zhou et al., 2023a). Finally, Zhang et al. (2023) evaluate their method empirically on simple networks, but they have limited theoretical guarantees and empirical results on other networks.\nNNs as Graphs. Many works consider neural networks as graphs, including early works that aimed at developing unified frameworks for neural networks (Bottou & Gallinari, 1990; Gegout et al., 1995). The graph perspective has been used in several application areas, including: neural architecture search (Liu et al., 2019; Xie et al., 2019), analyzing relationships between graph structure and performance (You et al., 2020), and federated learning with differing neural architectures (Litany et al., 2022). Several works process neural networks in some graph representation (Zhang et al., 2019; Thost & Chen, 2021; Knyazev et al., 2021; Litany et al., 2022), but these works generally operate on a much coarser network representation (e.g. where a node can correspond to a whole convolutional layer), whereas in our approach we desire each parameter to correspond to an edge.\nGNNs and Edge Representations. There has been much work on graph neural networks (GNNs) in recent years, leading to many variants that can be used in our framework (Hamilton, 2020). GNNs learning edge representations instead of just node or whole-graph representations are particularly relevant. These include message-passing-like architectures, architectures based on graph-permutationequivariant linear layers, and Transformers for graph data (Battaglia et al., 2018; Maron et al., 2019; Kim et al., 2021; 2022a;b; Diao & Loynd, 2023; Vignac et al., 2023; Ma et al., 2023)."
        },
        {
            "heading": "B MORE GRAPH CONSTRUCTION DETAILS",
            "text": "We implemented a procedure to automatically build the parameter graph given a PyTorch (Paszke et al., 2017) definition of a model that uses supported layers. Our approach converts all layers in the network into their parameter subgraph representation and iteratively stitches the subgraphs together to form the overall parameter graph.\nThis section details how we build the per-layer subgraphs we partially described in Section 2.1. We also include additional examples in Figure 7, which we explain below.\nNode and edge features For all subgraph constructions, we use node and edge features to improve the expressive power of our GMNs. Some examples include layer index, neuron type (input neuron index, output neuron index, or layer type), edge direction (when message-passing in an undirected manner), and positional encodings (for parameters with associated spatial features, such as convolutions or the spatial feature grids detailed below). In all cases except for the residual layers, edge features always include the parameter value. All nodes and edges always include integer features for the layer index and neuron type. Importantly, these added features are invariant under neural DAG automorphisms, so adding them does not break the equivariance properties of our Graph Metanets.\nLinear layers For each linear layer, we use node and edge features corresponding to the layer index and the node/edge type. The latter is an integer value identifying that each node/edge is either a linear weight or bias type.\nConvolution layers For convolutions, we include additional edge features to identify each parameter\u2019s spatial position in the filter kernel. We use integer coordinates appended to the other edge features that indicate layer index and edge type. The node features include only the layer index and node type.\nEquivariant linear layers We can also handle general group-equivariant linear layers, of which convolutions are a special case. The high-level idea is the same: each layer has a node for each \u201cchannel\u201d, and each node in the input layer is connected with db edges to each node in the output layer, where db is the dimension of the space of equivariant linear maps.\nMore specifically, let the input space of the layer be Rn1 , the output space be Rn2 , and the symmetry group be G. Suppose the group acts via representations \u03c11(g) \u2208 GL(n1) on the input and \u03c12(g) \u2208 GL(n2) on the output. Then the set of all G-equivariant linear maps, i.e. the set of linear maps T : Rn1 \u2192 Rn2 such that T \u25e6 \u03c11(g) = \u03c12(g) \u25e6 T for all g \u2208 G, forms a vector space of dimension that we denote as db (Maron et al., 2019; Finzi et al., 2021). Let B1, . . . , Bdb be a basis for the space\nof equivariant linear maps. Then a group equivariant linear layer takes the form\nx 7\u2192 db\u2211 i=1 wiBix, (4)\nfor some learnable weights wi \u2208 R. These weights wi are the parameters of the layer.\nAs is often done in practice, this can be extended to multiple input or output feature dimensions (also called channels) as follows. We now let the input space be Rn1\u00d7d1 and the output space be Rn2\u00d7d2 , where d1 is the number of input channels and d2 the number of output channels. The group now acts independently on each channel: via representations \u03c1\u03031(g) = \u03c11(g) \u2297 Id1\u00d7d1 on the input, and \u03c1\u03032(g) = \u03c12(g) \u2297 Id2\u00d7d2 on the output; e.g. for an input matrix X \u2208 Rn1\u00d7d1 , the action is given by g \u00b7X = \u03c11(g)X . The basis of equivariant linear maps for this space is now has dbd1d2 elements, and they take the form Bi \u2297Ejl, where i \u2208 [db], j \u2208 [d2], and l \u2208 [d1]. An equivariant linear layer then takes the form\nX 7\u2192 db\u2211 i=1 BiXW (i), (5)\nwhere Wi \u2208 Rd1\u00d7d2 are learnable weights. To construct a graph for this layer, we let the input layer have d1 nodes, and the output layer have d2 nodes. For j \u2208 [d2] and l \u2208 [d1], we connect input node l to input node j with dB\nedges. The ith edge has as weight W (i)j,l . Moreover, the ith edge has a feature denoting that it is part of Bi \u2014 in convolutions this is an encoding of the spatial position of the parameter in the kernel.\nAs an example, consider the permutation equivariant DeepSets linear layer (Zaheer et al., 2017). Here, the group G of permutations on n elements acts on Rn by permuting coordinates. There are two G-equivariant linear maps from Rn \u2192 Rn, I and 11\u22a4 (so dB = 2). Thus, the equivariant linear layer from Rn\u00d7d1 \u2192 Rn\u00d7d2 takes the form\nX 7\u2192 XW (1) + 11\u22a4XW (2). (6)\nSee Figure 6 for an illustration of how we encode this layer, as is used for our experiments in Section 4.1.\nMulti-head attention layers For multi-head attention (Vaswani et al., 2017), we can include the head index as an additional node/edge feature for the relevant nodes and edges (though we do not currently do this in our experiments of Section 4.1, where we only use 2-head attention layers). Otherwise, we treat the edges as standard linear parameters.\nResidual layers No parameters are associated with the edges for residual layers. Consequently, the edge features do not include the parameter values (we replace them with a constant value of 1). Otherwise, we include the layer index (for the starting layer) and an integer value identifying the edge as a residual connection.\nNormalization layers Similar to bias nodes, normalization layers such as BatchNorm (Ioffe & Szegedy, 2015), LayerNorm (Ba et al., 2016), and GroupNorm (Wu & He, 2018) can be represented by two additional nodes corresponding to the learned mean and variance. Edges are drawn between these nodes and the neurons to be normalized. The middle of Figure 7 depicts a normalization layer with 3 neurons. We include the layer index and an integer value for both the node and edge features, identifying them as either the mean/variance and which normalization layer they correspond to. The different types of normalization layers are distinguished via node and edge features: for instance, LayerNorm mean parameter nodes are given a different node label from BatchNorm mean parameter nodes.\nSpatial parameter grids An increasingly common design pattern in Implicit Neural Representations (INRs) introduces a grid of parameters that are accessed according to input coordinates, such as: triplanar grids (Chan et al., 2022), octrees (Takikawa et al., 2021), and hash grids (Mu\u0308ller et al., 2022). A typical implementation of these parameter grids involves finding a set of features according to the input coordinates and then using linear interpolation to combine the features. The resulting feature vector is then fed into an MLP to decode the features of a target object.\nWe show a subgraph construction for a simple 2D parameter grid in the right of Figure 7. The 2D grid contains only four entries at the corners of the domain, with two feature channels. We assume the two feature vectors are bilinearly interpolated based on the input coordinates (X,Y ). The resulting vector is concatenated with the input coordinates to produce a four-dimensional vector. In practice, we can add positional encoding of the grid position to the edge features for each element of the spatial parameter grid.\nNote that the graph construction for the triplanar grid can largely be reused for dense voxel feature grids, or hash grids (Mu\u0308ller et al., 2022). The same principles also apply to sparse feature grids, for example, those using octrees (Takikawa et al., 2021). A recent work by Cardace et al. (2023) develops a Transformer-based metanet for processing INRs with triplanar grid features."
        },
        {
            "heading": "B.1 CODE FOR CONSTRUCTING PARAMETER GRAPHS",
            "text": "We plan to release the code for constructing parameter graphs at a later date. Here, we outline some basics about how the code works. The code takes as input a PyTorch sequential module, which consists of several modules applied in sequence to some data. We iteratively build a graph, starting from the first layer in the input. For each layer in the input (e.g. a linear layer), we convert the layer to a subgraph, using the constructions outlined in Section 2.1 and Appendix B. We connect each of each of these subgraphs to the graph that we are iteratively building, until we have done this for all of the layers. We also have a way to invert this process, i.e. to go from parameter graph to neural network module. This is useful for metanet applications where we wish to modify the weights of an input neural network. To do this, we take an architecture specification and the edge weights of a parameter graph, and we then iteratively build a PyTorch module of the correct architecture and parameter values."
        },
        {
            "heading": "C THEORY: EQUIVARIANCE AND NEURAL GRAPH AUTOMORPHISMS",
            "text": ""
        },
        {
            "heading": "C.1 DAGS AND COMPUTATION GRAPHS",
            "text": "Consider a DAG (V,E), with weights \u03b8 \u2208 R|E| for each edge. We assume throughout that this DAG is connected. We may also have parameter-sharing constraints (e.g. in the case of convolution layers), which is a set S that partitions the edge set E, so that if edges (i1, j1) and (i2, j2) are in the same equivalence class, then \u03b8(i1,j1) = \u03b8(i2,j2). Further choose a nonlinearity \u03c3 : R\u2192 R. Let there be din input nodes in a set Vin, dout output nodes in Vout, some number of bias nodes in Vbias, and every other node in a set Vhidden. This defines a neural network function f\u03b8 : Rdin \u2192 Rdout with computation graph given by the DAG. To define the function, for an input x \u2208 Rdin we first define\nthe activation of node i \u2208 V to be a real value h\u03b8i (x) \u2208 R, which is defined recursively as follows:\nh\u03b8i (x) =  xi i \u2208 Vin 1 i \u2208 Vbias \u03c3 (\u2211 (i,j)\u2208E \u03b8(i,j)h \u03b8 j (x) ) i \u2208 Vhidden\u2211\n(i,j)\u2208E \u03b8(i,j)h \u03b8 j (x) i \u2208 Vout\n(7)\nNote that the activations are well-defined because the graph is a DAG. Here, we slightly abuse notation and assume that nodes 1, . . . , din are the input nodes. Finally, the neural network function is given by the values of the activations at the output nodes:\nf\u03b8(x) = [h \u03b8 j1(x), . . . ,h \u03b8 jdout (x)] \u2208 Rdout . (8)\nWhere j1, . . . , jdout \u2208 Vout are the output nodes. In the main paper, we generally consider our more efficient parameter graphs instead of computation graphs, since computation graphs may be much larger in general due to parameter sharing, in which one parameter is associated with many edges. Nonetheless, we study the theory of computation graphs here, as they are more closely related to the neural network function f\u03b8, and they can be defined unambiguously for feedforward neural networks (whereas we make some choices in our definitions of parameter graphs). We discuss extensions of our theoretical results to parameter networks in Appendix C.5."
        },
        {
            "heading": "C.2 NEURAL DAG AUTOMORPHISMS",
            "text": "We define an automorphism (of Neural DAGs) to be a bijection \u03d5 : V \u2192 V with the following three properties:\n1. Edge preservation: (\u03d5(i), \u03d5(j)) \u2208 E if and only if (i, j) \u2208 E. 2. Node label preservation: input nodes, output nodes, and bias nodes are fixed points, mean-\ning they are mapped to themselves. 3. Weight sharing preservation: if \u03b8(i1,j1) and \u03b8(i2,j2) are constrained to be equal, then\n\u03b8(\u03d5(i1),\u03d5(j1)) and \u03b8(\u03d5(i2),\u03d5(j2)) are also constrained to be equal.\nIn particular, node label preservation means that if i and j are two distinct input, output, or bias nodes, then \u03d5(i) = i and \u03d5(j) = j, and we cannot have \u03d5(i) = j."
        },
        {
            "heading": "C.3 NEURAL DAG AUTOMORPHISMS GENERALIZE KNOWN PARAMETER SYMMETRIES",
            "text": "This result shows that every hidden neuron permutation in an MLP is an automorphism, and also that every automorphism of an MLP DAG takes this form. Proposition 5. A permutation \u03d5 : V \u2192 V of an MLP DAG is a Neural DAG Automorphism if and only if \u03d5 only permutes hidden neurons, and \u03d5(i) is in the same layer as i for each hidden neuron i.\nProof. ( =\u21d2 ) Suppose \u03d5 is a Neural DAG Automorphism. Then because of the node label preservation property, we know that \u03d5 maps each input neuron, output neuron, and bias neuron to itself, meaning that \u03d5 only permutes hidden neurons. Moreover, since \u03d5 is a graph isomorphism, it does not change path lengths: the maximum path length between i and j is the same as that between \u03d5(i) and \u03d5(j). Hence, \u03d5 preserves layer number of each hidden neuron.\n(\u21d0= ) Suppose \u03d5 only permutes hidden neurons and preserves the layer of each hidden neuron. We need only show edge preservation to show that \u03d5 is a Neural DAG Automorphism. If (i, j) \u2208 E, then we know that layer(j) = layer(i) \u2212 1, so layer(\u03d5(j)) = layer(\u03d5(i)) \u2212 1. As MLPs are fully connected between adjacent layers, this means that (\u03d5(i), \u03d5(j)) \u2208 E. Analogously, if (\u03d5(i), \u03d5(j)) \u2208 E, then layer(j) = layer(i) \u2212 1, so (i, j) \u2208 E. We have thus showed edge preservation and are done.\nFor this next proposition, we will analyze the symmetries of simple CNN parameter spaces. In particular, we will assume the CNN only consists of convolutional layers (and no pooling, or fully connected layers). Every neuron has a spatial position and a channel index. For instance,\ninput neurons in a 32 \u00d7 32 input with 3 channels are indexed by a spatial position (x, y) \u2208 {1, . . . , 32} \u00d7 {1, . . . , 32}, and a channel c \u2208 {1, 2, 3}. We say that a permutation \u03d5 : V \u2192 V permutes hidden channels if it only permutes hidden neurons, \u03d5(i) has the same spatial position as i, and channel(i) = channel(j) if and only if channel(\u03d5(i)) = channel(\u03d5(j)).\nHere, we can show that any permutation of hidden channels that preserves neuron layers is a Neural DAG Automorphism, though we do not show the converse. Nonetheless, this direction shows that Neural DAG Automorphisms capture the known permutation parameter symmetries of CNNs, and thus our Graph Metanetwork framework can handle these symmetries.\nProposition 6. Let \u03d5 : V \u2192 V be a permutation of a CNN computation graph that permutes hidden channels and preserves neuron layers. Then \u03d5 is a Neural DAG Automorphism.\nProof. Suppose \u03d5 only permutes hidden channels and preserves the layer of each hidden neuron. Then \u03d5 satisfies node label preservation, and also weight sharing preservation. Now, consider an edge (i, j) \u2208 E, so layer(j) = layer(i) \u2212 1, and also the spatial positions of i and j fit within the convolution filter of this layer. Since \u03d5 preserves layers, we have layer(j) = layer(i)\u2212 1. Further, since \u03d5 only permutes hidden channels, the spatial position of \u03d5(i) is the same as i, and likewise the spatial position of \u03d5(j) is the same as j. Thus, (\u03d5(i), \u03d5(j)) \u2208 E. A similar argument show that (\u03d5(i), \u03d5(j)) being in E implies that (i, j) \u2208 E, so we are done."
        },
        {
            "heading": "C.4 NEURAL DAG AUTOMORPHISMS PRESERVE NETWORK FUNCTIONS",
            "text": "Proposition 7. For any neural DAG automorphism \u03d5 of a computation graph, the neural network function is left unchanged: \u2200x \u2208 X , f\u03b8(x) = f\u03a6(\u03b8)(x).\nProof. We will show a stronger statement: namely that the activations of the network with permuted parameters are permuted version of the activations of the original network. In other words, we will show that\nh\u03b8v(x) = h \u03a6(\u03b8) \u03d5(v) (x) (9)\nfor all nodes v \u2208 V . This is sufficient to show the proposition for the following reason. Let the output nodes be j1, . . . , jdout . Since a neural DAG automorphism \u03d5 maps each output node to itself, if equation 9 holds, then\nf\u03a6(\u03b8)(x) = [h \u03a6(\u03b8) j1 (x), . . . ,h \u03a6(\u03b8) jdout (x)] (10)\n= [h \u03a6(\u03b8) \u03d5(j1) (x), . . . ,h \u03a6(\u03b8) \u03d5(jdout ) (x)] (11) = [h\u03b8j1(x), . . . ,h \u03b8 jdout (x)] (12)\n= f\u03b8(x). (13)\nThe second line follows as \u03d5 leaves the output nodes unchanged. The third line follows from equation 9. Thus, it suffices to show equation 9. We proceed by induction on the layer number of v, i.e. the maximum path distance from an input node to v.\nFor the base case, if v is an input node, then \u03d5(v) = v, so\nh\u03b8v(x) = xv = h \u03a6(\u03b8) v (x) = h \u03a6(\u03b8) \u03d5(v) (x). (14)\nBias nodes always have activation 1, so this combined with the fact that automorphisms map bias nodes to themselves show equation 9 for bias nodes.\nNow, consider a hidden node i at layer l > 0, and suppose that equation 9 holds for all nodes of lower layers. If i is a hidden node, then we have\nh \u03a6(\u03b8) \u03d5(i) (x) = \u03c3  \u2211 (\u03d5(i),\u03d5(j))\u2208E \u03a6(\u03b8)(\u03d5(i),\u03d5(j))h \u03a6(\u03b8) \u03d5(j) (x)  (15) = \u03c3\n \u2211 (\u03d5(i),\u03d5(j))\u2208E \u03b8(i,j)h \u03a6(\u03b8) \u03d5(j) (x)  definition of \u03a6(\u03b8) (16) = \u03c3\n \u2211 (\u03d5(i),\u03d5(j))\u2208E \u03b8(i,j)h \u03b8 j (x)  induction (17) = \u03c3\n \u2211 (i,j)\u2208E \u03b8(i,j)h \u03b8 j (x)  edge preservation property of \u03d5 (18) = h\u03b8i (x). (19)\nIf i is an output node, then the same derivation holds without the nonlinearity \u03c3. Thus, we have shown equation 9, and we are done."
        },
        {
            "heading": "C.5 FROM COMPUTATION GRAPHS TO PARAMETER GRAPHS",
            "text": "We conjecture that Proposition 1 can be extended to automorphisms over parameter graphs instead of computation graphs. Here we discuss the formal intuition behind this conjecture.\nTo prove that the network function remains unchanged after an automorphism of the parameter graph, we could show that any automorphism of the parameter graph corresponds to an automorphism of the computation graph. One way to do this is through a surjective homomorphism from the computation graph to the parameter graph that is locally-injective (Fiala & Kratochv\u0131\u0301l, 2008). This requires that each 1-neighbourhood in the computation graph is mapped injectively to a 1- neighbourhood in the parameter graph.\nIntuitively, this allows us to define an automorphism on the parameter graph and lift the automorphism to the computation graph through the preimage of the homomorphism. Concretely, we apply the parameter graph automorphism to the computation graph by permuting the preimage sets for each node in the parameter graph under the homomorphism.\nUsing this machinery, we expect that the proof of Proposition 1 can be extended to parameter graphs. One point of difficulty is that our parameter graph construction allows for multigraphs \u2014 which are valuable for succinct representations and also make it easier to stitch together subgraphs."
        },
        {
            "heading": "C.6 EQUIVARIANCE OF GRAPH METANETWORKS TO NEURAL DAG AUTOMORPHISMS",
            "text": "Here, we show that graph metanetworks are equivariant to architecture graph symmetries. In particular, consider an architecture graph G = (V,E). We also allow modifications to the graph, so that G does not need to be the exact DAG of the computation graph; for instance, G can be the undirected version of the computation graph. Consider a fixed ordering of the n nodes, and let X \u2208 Rn\u00d7dx be the input node features, and A \u2208 Rn\u00d7n\u00d7de be the input edge features. For any permutation \u03b7 : V \u2192 V , we define the standard group action: \u03b7(X) \u2208 Rn\u00d7dx is defined by \u03b7(X)\u03b7(i),k = Xi,k, and \u03b7(A) \u2208 Rn\u00d7n\u00d7de is defined by \u03b7(A)\u03b7(i),\u03b7(j),k = Ai,j,k.\nConsider a graph function that updates node features and edge features, say GNN : Rn\u00d7dx \u00d7 Rn\u00d7n\u00d7de \u2192 Rn\u00d7dx \u00d7 Rn\u00d7n\u00d7de . For inputs X and A, let GNN(X,A)X \u2208 Rn\u00d7dx denote the updated node features, and GNN(X,A)A \u2208 Rn\u00d7n\u00d7de denote the updated edge features. We say such a function is permutation equivariant if for all permutations \u03b7 : [n]\u2192 [n], it holds that\nGNN(\u03b7(X), \u03b7(A))X\u03b7(i),k = GNN(X,A) X i,k (20)\nGNN(\u03b7(X), \u03b7(A))A\u03b7(i),\u03b7(j),k = GNN(X,A) A i,j,k. (21)\nGraph neural networks have exactly this permutation equivariance (Maron et al., 2019). Thus, since any architecture graph symmetry \u03c4 is a permutation [n]\u2192 [n], it holds that graph metanetworks are equivariant to architecture graph symmetries. Thus, we restate our result: Proposition 8. Graph metanets are equivariant to parameter permutations induced by neural DAG automorphisms.\nHowever, this would also seem to imply that graph metanetworks are equivariant to more permutations than just architecture graph symmetries, which would be undesirable because this would mean reduced expressivity. The key to fixing this problem is via adding the additional node and edge features described in Appendix B. Recall that we choose additional node and edge features, say X\u0303 and A\u0303, that are invariant to neural DAG automorphisms. That is, \u03d5(X\u0303) = X\u0303 and \u03d5(A\u0303) = A\u0303 for any neural DAG automorphism \u03d5. Also, we add these features in a fixed manner, so they do not depend on the choice of node indices. Thus, the output node features of the Graph Metanet on the original graph are GNN([X, X\u0303], [A, A\u0303])X , whereas after applying a permutation they are GNN([\u03b7(X), X\u0303], [\u03b7(A), A\u0303])X . These outputs are in general not equal up to a permutation. However, when \u03b7 is a neural DAG automorphism, they are equal, because\nGNN([\u03d5(X), X\u0303], [\u03d5(A), A\u0303])X\u03b7(i),k (22)\n= GNN([\u03d5(X), \u03d5(X\u0303)], [\u03d5(A), \u03d5(A\u0303)])X\u03b7(i),k X\u0303, A\u0303 invariant to \u03d5 (23)\n= GNN([X, X\u0303], [A, A\u0303])Xi,k GNN equivariance. (24) The same argument holds for output edge features."
        },
        {
            "heading": "D THEORY: EXPRESSIVE POWER OF GRAPH METANETWORKS",
            "text": "Representation vs. Approximation. In this Appendix section, we prove results on the expressive power of Graph Metanets. For these results, there is some continuous target function ftarget that we wish to approximate (either StatNN, NP-NFN, or forward passes through a neural network). In our proofs, for ease of exposition we show that if we assume that the MLPs of our Graph Metanets can be arbitrary continuous functions, then there is some Graph Metanet that is exactly ftarget.\nHowever, MLPs cannot generally express every continuous function. Instead, MLPs are known to be universal approximators of continuous functions (Hornik, 1991), meaning that for any continuous ftarget with inputs from a compact domain, for every precision \u03f5 > 0 there is an MLP that is within \u03f5 (in some function-space metric) to ftarget. We will call any function class with this property a universal function class. If our metanets were just a single MLP, then it would be trivial to go from an exact representation statement to an approximation statement. However, in general our metanets are compositions of several MLPs and other operations.\nDespite this discrepancy, we can appeal to standard techniques that have been used in recent works in the study of expressive power of equivariant neural networks to show approximation results (Lim et al., 2023; Navon et al., 2023). In particular, if a neural network architecture can be written as a composition of functions fL \u25e6 \u00b7 \u00b7 \u00b7 \u25e6 f1, and each of the functions comes from a universal function class, then the composition also forms a universal function class. Thus, one need only show that each component can be approximated to show that an entire target function can be approximated. This is captured in the following lemma, which is proved in Lim et al. (2023). Lemma 1. Let X \u2286 Rd0 be compact, and let F1, . . . ,FL be families of continuous functions, where Fi consists of functions from Rdi\u22121 \u2192 Rdi . Let F be the set of compositions of these functions, so F = {fL \u25e6 \u00b7 \u00b7 \u00b7 \u25e6 f1 : X \u2192 RdL , fi \u2208 Fi}.\nFor each i, let F\u0303i be a family of continuous functions that universally approximates Fi. Then the family of compositions F\u0303 = {f\u0303L \u25e6 \u00b7 \u00b7 \u00b7 \u25e6 f\u03031 : f\u0303i \u2208 F\u0303i} universally approximates F ."
        },
        {
            "heading": "D.1 PROOF: SIMULATING EXISTING METANETWORKS",
            "text": ""
        },
        {
            "heading": "D.1.1 SIMULATING STATNN",
            "text": "In its most expressive form, StatNN (Unterthiner et al., 2020) computes statistics from each weight and bias of an MLP or CNN input, and then applies an MLP on top of these statistics. The\nstatistics considered by Unterthiner et al. (2020) are the mean, variance, and qth percentiles for q \u2208 {0, 25, 50, 75, 100}. These are all continuous permutation invariant functions of the input, i.e. for any of these statistics s and flattened weight vector w (e.g. w = vec(W (l)) for a weight matrix W (l) or w = b(l) for a bias vector b(l)), it holds that s(w) = s(Pw) for any permutation matrix P . This is the key to showing that Graph Metanets can express StatNN, since the global graph feature update can compute these permutation invariant functions, and then the final invariant layer can apply an MLP on the global graph feature. Proposition 9. Graph Metanetworks can express StatNN (Unterthiner et al., 2020) on MLP input networks.\nProof. Let W (1), . . . ,W (L) be the weights and b(1), . . . , b(L) the biases of an input MLP. Let s be a continuous permutation invariant function with output in Rds , and let MLPStatNN : R2Lds \u2192 R be the MLP of the StatNN, so\nStatNN(W (1), . . . ,W (L), b(1), . . . , b(L)) = MLPStatNN(s(W (1)), . . . , s(b(L))). (25)\nWe will show that a Graph Metanetwork can express this function. Let the edge features e(i,j) \u2208 R3 be three dimensional, with the first dimension holding the parameter value, the second dimension denoting the layer index, and the third dimension denoting whether the edge is associated to a weight or bias parameter. We will not require node features for this proof. Thus, our Graph Metanetwork will take the form\ne(i,j) \u2190 MLPe(e(i,j)) (26) u\u2190 MLPu (\u2211\ne\u2208E e\n) , (27)\nand the result will be that u \u2208 R will be the desired StatNN output. Since s is a continuous permutation invariant function, it can be expressed in the form s(w) = \u03c1( \u2211\ni \u03d5(wi)) for continuous functions \u03c1 : Rd\u03d5 \u2192 Rds and \u03d5 : R \u2192 Rd\u03d5 (Zaheer et al., 2017). Let MLPe : R3 \u2192 R2L\u00d7d\u03d5 be given by\n[MLPe(e(i,j))] l,: = \u03d5(e(i,j)) \u00b7 1 [ [e(i,j)] 2 = l, [e(i,j)] 3 = weight ] (28)\n[MLPe(e(i,j))] l+L,: = \u03d5(e(i,j)) \u00b7 1 [ [e(i,j)] 2 = l, [e(i,j)] 3 = bias ] (29)\nfor l = 1, . . . , L. Here, [a]i is the ith entry of the vector a, [A]i,: denotes the ith entry of the matrix A, and 1 is one if the condition is true and zero otherwise. Note that we write the output of MLPe as a matrix to make indexing simpler, but it functionally is just a vector.\nThen note that the input to MLPu is \u2211\ne\u2208E e, which is a matrix A \u2208 R2L\u00d7d\u03d5 that satisfies: [A]l,: = \u2211 i \u03d5(vec(W (l))i) (30)\n[A]l+L,: = \u2211 i \u03d5(vec(b(l))i). (31)\nHence, we have that\n\u03c1([A]2l,:) = s(W (l)) (32)\n\u03c1([A]2l+1,:) = s(b(l)). (33)\nThus, we can let MLPu : R2L\u00d7d\u03d5 \u2192 R be given by MLPu(A) = MLPStatNN ( \u03c1([A]1,:), . . . , \u03c1([A]2L,:) ) (34)\nand we are done."
        },
        {
            "heading": "D.1.2 SIMULATING NP-NFN",
            "text": "Proposition 10. Graph Metanetworks can express NP-NFN (Zhou et al., 2023a) on MLP input networks.\nProof. GNN Form. Here is the form of our GNN, where for simplicity of the proof, we move the global graph feature update before the other updates and remove one of the node-update MLPs. Note that the MLPu is inside the sum \u2014 this is important for our construction.\nu\u2190 \u2211\n(i,j)\u2208E\nMLPu ( e(i,j), u ) (35)\nvi \u2190 \u2211\nj:(i,j)\u2208E\nMLPv(vi,vj , e(i,j),u) (36)\ne(i,j) \u2190 MLPe(vi,vj , e(i,j),u). (37)\nGraph Form. We make the graph undirected by including the reverse edge for each edge in the DAG and labelling these as backward edges. We assume the nodes and edges are endowed with certain features at the start (the global graph feature u is initialized to 0). Each node has the features: layer number and node type (input neuron number, output neuron number, hidden neuron, or bias neuron number). Each edge has the features: weight value, layer number, weight type, whether it is backward or not, and whether it is a weight or bias edge. Notably, each of these features is invariant to neural graph automorphisms. The node and edge features belong to the following sets:\nvi \u2208 [\n{0, . . . , L} {in1, . . . , indin , out1, . . . , outdout , bias1, . . . , biasL, hidden}\n] (38)\ne(i,j) \u2208  R{1, . . . , L}{forward, backward} {weight, bias}  (39) Without biases. For illustration, we will first prove this result for the case when we are only processing weights and no biases. First, we recall the form of the NP-NFN linear layer (Zhou et al., 2023a). It takes MLP weights W \u2208 W = RdL\u00d7dL\u22121 \u00d7 . . . \u00d7 Rd1\u00d7d0 , and outputs new weight representations H(W ) \u2208 W . We let W (l) denote the weight matrix of the lth layer, and let W (l)i,j be the (i, j)th entry of this matrix. Then, the NP-NFN linear layer takes the form:\nH(W ) (l) i,j =\n( L\u2211\ns=1\nal,s1 W (s) \u22c6,\u22c6 ) + al,l2 W (l) \u22c6,j + a l,l\u22121 3 W (l\u22121) j,\u22c6 (40)\n+ al,l4 W (l) i,\u22c6 + a l,l+1 5 W (l+1) \u22c6,i + a l 6W (l) i,j , (41)\nwhere we define W (0) = 0 and W (L+1) = 0 for the boundary cases of l = 1 and l = L. Here, the a \u2208 R are learnable scalars, and the \u22c6 denotes a summation over all relevant indices. For instance, for W (1) \u2208 Rn1\u00d7n0 , we have W (1)\u22c6,k = \u2211n1 i=1 W (1) i,k and W (1) \u22c6,\u22c6 = \u2211n1 i=1 \u2211n0 j=1 W (1) i,j .\nFirst, we use the global graph feature to compute the first summands. Let MLPu have an output space of RL, and let\n[MLPu(e,u)]l = [e]1 \u00b7 1[[e]2 = l], (42)\nwhere [y]i \u2208 R is the ith entry of the vector y, and 1 is one if the condition inside is true, and zero otherwise. Since the first column of e holds the weight and the second column holds the layer number, the global graph feature is updated as\nu\u2190 [ W (1) \u22c6,\u22c6 . . . W (L) \u22c6,\u22c6 ]\u22a4 \u2208 RL. (43)\nNext, we consider the node feature updates. We define MLPv to have an output space of R2, such that\n[MLPv(vi,vj , e(i,j),u)] 1 = [e(i,j)] 1 \u00b7 1[[e(i,j)]3 = backward] (44) [MLPv(vi,vj , e(i,j),u)] 2 = [e(i,j)] 1 \u00b7 1[[e(i,j)]3 = forward]. (45)\nFor a node v that is the kth node in layer l (where the input nodes are at layer 0), the node update is then given by:\nv \u2190 [ W\n(l+1) \u22c6,k W (l) k,\u22c6\n] \u2208 R2. (46)\nFinally, we define the edge update. We let MLPe have an output space of R, such that for any forward edge (i, j) the update is (writing l = [e(i,j)]2 for brevity):\nMLPe(vi,vj , e(i,j),u) = (47) L\u2211\ns=1\nal,s1 [u] s + al,l2 [vj ] 1 + al,l\u221213 [vj ] 2 + al,l4 [vi] 2 + al,l+15 [vi] 1 + al6[e(i,j)] 1. (48)\nBy plugging in the above forms of u, vi, and vj (and noting that [e(i,j)]1 = W (l) i,j ), we see that this new edge representation is exactly the NP-NFN update, so we are done.\nWith biases. Recall the form of the NP-NFN linear layer where the input MLP also has biases:\nH(W, b) = (W\u0303 , b\u0303) (49)\nW\u0303 (l) i,j =\n( L\u2211\ns=1\nal,s1 W (s) \u22c6,\u22c6 ) + al,l2 W (l) \u22c6,j + a l,l\u22121 3 W (l\u22121) j,\u22c6 (50)\n+ al,l4 W (l) i,\u22c6 + a l,l+1 5 W (l+1) \u22c6,i + a l 6W (l) i,j (51)\n+\n( L\u2211\ns=1\nal,s7 b (s) \u22c6 ) + al8b (l) i + a l 9b (l\u22121) j (52)\nb\u0303 (l) j =\n( L\u2211\ns=1\ncl,s1 W (s) \u22c6,\u22c6 ) + cl,l2 W (l) j,\u22c6 + c l,l+1 3 W (l+1) \u22c6,j (53)\n+\n( L\u2211\ns=1\ncl,s4 b (s) \u22c6 ) + cl5b (l) j (54)\nNote that we define b(0) = 0, and once again define W (0) = 0 and W (L+1) = 0 for the boundary cases.\nWe define the global model MLPu to have output in R2L, such that for l \u2208 {1, . . . , L},\n[MLPu(v, e,u)]l = [e]1 \u00b7 1 [ [e]2 = l, [e]4 = weight ] (55)\n[MLPu(e,u)]l+L = [e]1 \u00b7 1 [ [e]2 = l, [e]4 = bias ] , (56)\nThe global graph feature then takes the form\nu\u2190 [ W (1) \u22c6,\u22c6 . . . W (L) \u22c6,\u22c6 b (1) \u22c6 . . . b (L) \u22c6 ]\u22a4 \u2208 R2L. (57)\nNext, we consider the node feature updates. We define MLPv to have an output space of R3, such that\n[MLPv(vi,vj , e(i,j),u)] 1 = [e(i,j)] 1 \u00b7 1 [ [e(i,j)] 3 = backward, [e(i,j)] 4 = weight ] (58)\n[MLPv(vi,vj , e(i,j),u)] 2 = [e(i,j)] 1 \u00b7 1 [ [e(i,j)] 3 = forward, [e(i,j)] 4 = weight ] (59)\n[MLPv(vi,vj , e(i,j),u)] 3 = [e(i,j)] 1 \u00b7 1 [ [e(i,j)] 3 = forward, [e(i,j)] 4 = bias ] (60)\nFor a non-bias node v that is the kth node in layer l, the node update is then given by: v \u2190 [ W\n(l+1) \u22c6,k W (l) k,\u22c6 b (l) k\n] \u2208 R3. (61)\nFinally, we define the edge update. We let MLPe have an output space of R, such that for any forward edge (i, j) the update is (writing l = [e(i,j)]2 for brevity):\nMLPe(vi,vj , e(i,j),u) = (62)\n1 [ [e4(i,j)] = weight ]( L\u2211 s=1 al,s1 [u] s + al,l2 [vj ] 1 + al,l\u221213 [vj ] 2 (63)\n+ al,l4 [vi] 2 + al,l+15 [vi] 1 + al6[e(i,j)] 1 (64)\n+ L\u2211 s=1 al,s7 [u] s+L + al8[vi] 3 + al9[vj ] 3\n) (65)\n+1 [ [e(i,j)] 4 = bias ]( L\u2211\ns=1\ncl,s1 [u] s + cl,l2 [vj ] 2 + cl,l+13 [vj ] 1 (66)\n+ L\u2211 s=1 cl,s4 [u] s+L + cl5[e(i,j)] 1\n) (67)\nBy plugging in the above forms of u, vi, and vj , we see that this new edge representation is exactly the NP-NFN update, so we are done."
        },
        {
            "heading": "D.1.3 EXPRESSIVE POWER WITH BIAS NODES",
            "text": "Here, we explain how encoding biases in linear layers as bias nodes can aid expressive power (in the simple case of MLPs). Intuitively, this is because the bias node allows for different bias parameters of the same layer to communicate with each other in each layer of a message passing graph metanet, whereas this is not as simple and requires at least two layers if biases are encoded as self-loops or node features.\nFor a specific example, when the graph metanet does not use a global feature (as in our experiments), then the bias node can allow message passing between biases within the same layer. In the NP-NFN case covered in Appendix D.1.2, we see that the global feature allows message passing between biases of any layer. When there is no global feature, letting vb(l) denote the bias node of the lth layer, then our graph metanet can nonetheless update\nvb(l) \u2190 dl\u2211 i=1 b (l) i = b (l) \u22c6 (68)\ne(b(l),i) \u2190 cvb(l) = cb (l) \u22c6 (69)\nfor some scalar c \u2208 R. This is one of the equivariant linear maps included in NP-NFN. On the other hand, a graph metanet with no global feature cannot compute this in one layer if the biases are encoded as self-loops or node features \u2014 this would require at least two layers."
        },
        {
            "heading": "D.2 PROOF: SIMULATING FORWARD PASSES",
            "text": "Proposition 11. On computation graph inputs, graph metanets can express the forward pass of any input feedforward neural network as defined in Section 2.2.\nProof. We will prove this statement by showing that a graph metanet can compute the activations h\u03b8v(x) for an input network f\u03b8 and input x. In particular, we will show that l layers of a graph metanet can compute the activation h\u03b8v(x) for any node v with layer number at most l, where the layer number is defined as the maximum path distance from any input node to v. We do this by induction on l. For the base case, h\u03b8v(x) = xv for any input node v, so at depth zero of the graph metanet the induction hypothesis is satisfied. Also, the activation for a bias node is always 1, which a graph metanet can compute using a node-update MLP that outputs 1 for any node with the bias-node label.\nFor a hidden node i with layer number l > 0, recall that we have (output nodes are handled similarly):\nh\u03b8i (x) = \u03c3  \u2211 (i,j)\u2208E \u03b8(i,j)h \u03b8 j (x)  . (70) Let v(l\u22121)i denote the node feature computed for node i at depth l \u2212 1 of the graph metanet. By induction, we have v(l\u22121)j = h \u03b8 j (x) for any node j in layer l \u2212 1 or earlier. Recall the form of the node update in a graph metanet layer:\nvi \u2190 MLPv2 vi, \u2211 (i,j)\u2208E MLPv1 (vi,vj , e(i,j),u),u  (71) Let MLPv1 for depth l of the GNN be given by\nMLPv1 (vi,vj , e(i,j), u) = [e(i,j)] 1vj , (72)\nand let MLPv2 for depth l be given by\nMLPv2 (vi, a,u) = \u03c3(a). (73)\nThen the node update equation for the GNN is\nv (l) i = \u03c3  \u2211 (i,j)\u2208E \u03b8(i,j)v (l\u22121) j  (74) = \u03c3\n \u2211 (i,j)\u2208E \u03b8(i,j)h \u03b8 j (x)  (75) = h\u03b8i (x). (76)\nThe same derivation holds for an output node i, replacing \u03c3 with the identity map. So, we are done by induction."
        },
        {
            "heading": "E ADDITIONAL EXPERIMENTAL DETAILS",
            "text": ""
        },
        {
            "heading": "E.1 PREDICTING ACCURACY",
            "text": ""
        },
        {
            "heading": "E.1.1 CLASSIFIER TRAINING DETAILS",
            "text": "For the predicting accuracy experiments, we train two datasets of about 30 000 CIFAR-10 image classification neural networks each. We use the Fast Forward Computer Vision (FFCV) library (Leclerc et al., 2023) for fast training and build off their template training setup for CIFAR-10. We use random horizontal flips, random translations, and Cutout (DeVries & Taylor, 2017) as data augmentation. The learning rate schedule has a linear warmup and then a linear decay to zero. All the other hyperparameters are given in Table 5; we sample many of the hyperparameters for each neural network. For the Diverse Architectures dataset, we train about 6000 of each of the 5 types of networks, while for the Varying CNNs dataset we train about 30 000 of the 2D-CNNs."
        },
        {
            "heading": "E.1.2 METANETWORK TRAINING DETAILS",
            "text": "Data splits. For the 50% training data experiment, we take a random split of 15 000 training networks, and for the 5% training data experiment we take a random split of 1500 training networks. Then 2000 random networks are selected for validation, and the rest are for testing.\nFor the OOD experiments, we only train and validate on networks of the lowest hidden dimension of each architecture. This means that we only train and validate on CNNs (both 2D and 1D) of hidden dimension 24, DeepSets of hidden dimension 32, ResNets of hidden dimension 16, and ViTs of hidden dimension 32. 2000 of these shallower networks are chosen for validation, and the rest for training. Then we test on all networks with higher hidden dimension.\nMetanetworks. For each metanet, we first choose hyperparameters such that the total number of trainable parameters is around 750 000. Then we search learning rates \u03b1 \u2208 {.0001, .0005, .001, .005, .01}, and choose the learning rate that achieves the best validation R2. Finally, we train the model using this best learning rate with 5 random seeds and report the mean and standard deviation of the test set R2 and Kendall \u03c4 values.\nWe train all metanets with the Adam optimizer (Kingma & Ba, 2014). The training loss is a binary cross entropy loss between the predicted and true accuracy; each metanet has a sigmoid nonlinearity at the end to ensure that its input is within [0, 1].\nSmall CNN Experiments To compare against previously proposed metanetworks, we also run experiments on predicting accuracy of the small CNNs trained by Unterthiner et al. (2020), following the experimental setup of Zhou et al. (2023a). Results are given in Table E.1.2. We see that our GMNs outperform all other metanetworks. We use GRIT (Ma et al., 2023) as our graph learning model, which is a type of Graph Transformer. Our GRIT model has 3 652 865 parameters, uses a hidden dimension of 256, has 4 layers, is trained with a learning rate of 3e-4, and uses 16 heads in its attention modules."
        },
        {
            "heading": "E.2 EDITING 2D INRS DETAILS",
            "text": "We closely follow the experimental setup of Zhou et al. (2023a) for the experiments in Section 4.2. As in their work, we train the metanets for 50 000 iterations with a batch size of 32, using the Adam optimizer with .001 learning rate. We follow their setup to parameterize the updated weights \u03b8\u0303 as\n\u03b8\u0303 = \u03b8 + \u03b3 \u00b7Metanet(\u03b8), where \u03b8 are the parameters of the INR representing the unedited image, and \u03b3 are learned scalars for each parameter that are initialized to .01. Our message passing GNN metanetworks have 8 945 671 parameters, hidden dimension of 512, and 4 message passing layers."
        },
        {
            "heading": "E.3 SELF-SUPERVISED LEARNING WITH INRS DETAILS",
            "text": "We closely follow the experimental setup of Navon et al. (2023) for the self-supervised learning task. In particular, as they do in their experiments, we choose hyperparameters for our graph metanet such that the number of trainable metanetwork parameters is about 100 000. Thus, we use a graph metanetwork with 3 GNN layers and a hidden dimension of 76.\nWe follow the same training procedure as Navon et al. (2023), which uses a contrastive objective with data augmentations given by adding Gaussian noise to the weights and masking weights to zero. As in Navon et al. (2023), we evaluate our metanet on three random seeds, and report the mean and standard deviation across these three runs."
        }
    ],
    "title": "GRAPH METANETWORKS FOR PROCESSING DIVERSE NEURAL ARCHITECTURES",
    "year": 2024
}