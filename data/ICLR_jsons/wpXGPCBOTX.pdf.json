{
    "abstractText": "Optimal Transport is a useful metric to compare probability distributions and to compute a pairing given a ground cost. Its entropic regularization variant (eOT) is crucial to have fast algorithms and reflect fuzzy/noisy matchings. This work focuses on Inverse Optimal Transport (iOT), the problem of inferring the ground cost from samples drawn from a coupling that solves an eOT problem. It is a relevant problem that can be used to infer unobserved/missing links, and to obtain meaningful information about the structure of the ground cost yielding the pairing. On one side, iOT benefits from convexity, but on the other side, being ill-posed, it requires regularization to handle the sampling noise. This work presents an in-depth theoretical study of the l1 regularization to model for instance Euclidean costs with sparse interactions between features. Specifically, we derive a sufficient condition for the robust recovery of the sparsity of the ground cost that can be seen as a far reaching generalization of the Lasso\u2019s celebrated \u201cIrrepresentability Condition\u201d. To provide additional insight into this condition, we work out in detail the Gaussian case. We show that as the entropic penalty varies, the iOT problem interpolates between a graphical Lasso and a classical Lasso, thereby establishing a connection between iOT and graph estimation, an important problem in ML.",
    "authors": [
        {
            "affiliations": [],
            "name": "Clarice Poon"
        }
    ],
    "id": "SP:3087b7bc08b9b9a322ca5d58a216f39d063cedfa",
    "references": [
        {
            "authors": [
                "Jason Altschuler",
                "Jonathan Niles-Weed",
                "Philippe Rigollet"
            ],
            "title": "Near-linear time approximation algorithms for optimal transport via sinkhorn iteration",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Martin Arjovsky",
                "Soumith Chintala",
                "L\u00e9on Bottou"
            ],
            "title": "Wasserstein generative adversarial networks",
            "venue": "In International conference on machine learning,",
            "year": 2017
        },
        {
            "authors": [
                "Aur\u00e9lien Bellet",
                "Amaury Habrard",
                "Marc Sebban"
            ],
            "title": "A survey on metric learning for feature vectors and structured data",
            "venue": "arXiv preprint arXiv:1306.6709,",
            "year": 2013
        },
        {
            "authors": [
                "Mathieu Blondel",
                "Andr\u00e9 FT Martins",
                "Vlad Niculae"
            ],
            "title": "Learning with fenchel-young losses",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2020
        },
        {
            "authors": [
                "Raicho Bojilov",
                "Alfred Galichon"
            ],
            "title": "Matching in closed-form: equilibrium, identification, and comparative statics",
            "venue": "Economic Theory,",
            "year": 2016
        },
        {
            "authors": [
                "Emmanuel J Cand\u00e8s",
                "Justin Romberg",
                "Terence Tao"
            ],
            "title": "Robust uncertainty principles: Exact signal reconstruction from highly incomplete frequency information",
            "venue": "IEEE Transactions on information theory,",
            "year": 2006
        },
        {
            "authors": [
                "Guillaume Carlier",
                "Arnaud Dupuy",
                "Alfred Galichon",
                "Yifei Sun"
            ],
            "title": "Sista: learning optimal transport costs under sparsity constraints",
            "venue": "Communications on Pure and Applied Mathematics,",
            "year": 2023
        },
        {
            "authors": [
                "Wei-Ting Chiu",
                "Pei Wang",
                "Patrick Shafto"
            ],
            "title": "Discrete probabilistic inverse optimal transport",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Beno\u00eet Colson",
                "Patrice Marcotte",
                "Gilles Savard"
            ],
            "title": "Bilevel programming: A survey",
            "venue": "4or, 3:87\u2013107,",
            "year": 2005
        },
        {
            "authors": [
                "Marco Cuturi"
            ],
            "title": "Sinkhorn distances: Lightspeed computation of optimal transport",
            "venue": "In Adv. in Neural Information Processing Systems,",
            "year": 2013
        },
        {
            "authors": [
                "Marco Cuturi",
                "David Avis"
            ],
            "title": "Ground metric learning",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2014
        },
        {
            "authors": [
                "Marco Cuturi",
                "Gabriel Peyr\u00e9"
            ],
            "title": "A smoothed dual approach for variational wasserstein problems",
            "venue": "SIAM Journal on Imaging Sciences,",
            "year": 2016
        },
        {
            "authors": [
                "Jason V Davis",
                "Inderjit S Dhillon"
            ],
            "title": "Structured metric learning for high dimensional problems",
            "venue": "In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining,",
            "year": 2008
        },
        {
            "authors": [
                "Valentin De Bortoli",
                "James Thornton",
                "Jeremy Heng",
                "Arnaud Doucet"
            ],
            "title": "Diffusion schr\u00f6dinger bridge with applications to score-based generative modeling",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Klaus Deimling"
            ],
            "title": "Nonlinear functional analysis",
            "venue": "Courier Corporation,",
            "year": 2010
        },
        {
            "authors": [
                "Arnaud Dupuy",
                "Alfred Galichon"
            ],
            "title": "Personality traits and the marriage market",
            "venue": "Journal of Political Economy,",
            "year": 2014
        },
        {
            "authors": [
                "Arnaud Dupuy",
                "Alfred Galichon",
                "Yifei Sun"
            ],
            "title": "Estimating matching affinity matrices under low-rank constraints",
            "venue": "Information and Inference: A Journal of the IMA,",
            "year": 2019
        },
        {
            "authors": [
                "Marvin Eisenberger",
                "Aysim Toker",
                "Laura Leal-Taix\u00e9",
                "Florian Bernard",
                "Daniel Cremers"
            ],
            "title": "A unified framework for implicit sinkhorn differentiation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Alfred Galichon"
            ],
            "title": "Optimal transport methods in economics",
            "year": 2018
        },
        {
            "authors": [
                "Alfred Galichon",
                "Bernard Salani\u00e9"
            ],
            "title": "Matching with trade-offs: Revealed preferences over competing characteristics",
            "venue": "Preprint hal-00473173,",
            "year": 2010
        },
        {
            "authors": [
                "Alfred Galichon",
                "Bernard Salani\u00e9"
            ],
            "title": "Cupid\u2019s invisible hand: Social surplus and identification in matching models",
            "venue": "The Review of Economic Studies,",
            "year": 2022
        },
        {
            "authors": [
                "Aude Genevay",
                "Gabriel Peyr\u00e9",
                "Marco Cuturi"
            ],
            "title": "Learning generative models with sinkhorn divergences",
            "venue": "In International Conference on Artificial Intelligence and Statistics,",
            "year": 2018
        },
        {
            "authors": [
                "Aude Genevay",
                "L\u00e9naic Chizat",
                "Francis Bach",
                "Marco Cuturi",
                "Gabriel Peyr\u00e9"
            ],
            "title": "Sample complexity of Sinkhorn divergences",
            "venue": "In The 22nd international conference on artificial intelligence and statistics,",
            "year": 2019
        },
        {
            "authors": [
                "Trevor Hastie",
                "Robert Tibshirani",
                "Martin Wainwright"
            ],
            "title": "Statistical learning with sparsity: the lasso and generalizations",
            "venue": "CRC press,",
            "year": 2015
        },
        {
            "authors": [
                "Matthieu Heitz",
                "Nicolas Bonneel",
                "David Coeurjolly",
                "Marco Cuturi",
                "Gabriel Peyr\u00e9"
            ],
            "title": "Ground metric learning on graphs",
            "venue": "Journal of Mathematical Imaging and Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Jean-Baptiste Hiriart-Urruty",
                "Claude Lemar\u00e9chal"
            ],
            "title": "Conjugacy in convex analysis",
            "venue": "Convex Analysis and Minimization Algorithms II: Advanced Theory and Bundle Methods,",
            "year": 1993
        },
        {
            "authors": [
                "Gao Huang",
                "Chuan Guo",
                "Matt J Kusner",
                "Yu Sun",
                "Fei Sha",
                "Kilian Q Weinberger"
            ],
            "title": "Supervised word mover\u2019s distance",
            "venue": "Advances in neural information processing systems,",
            "year": 2016
        },
        {
            "authors": [
                "Tanguy Kerdoncuff",
                "R\u00e9mi Emonet",
                "Marc Sebban"
            ],
            "title": "Metric Learning in Optimal Transport for Domain Adaptation",
            "venue": "In International Joint Conference on Artificial Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Matt Kusner",
                "Yu Sun",
                "Nicholas Kolkin",
                "Kilian Q Weinberger"
            ],
            "title": "From word embeddings to document distances",
            "venue": "In Proc. of the 32nd Intern. Conf. on Machine Learning,",
            "year": 2015
        },
        {
            "authors": [
                "Jason D Lee",
                "Yuekai Sun",
                "Jonathan E Taylor"
            ],
            "title": "On model selection consistency of regularized m-estimators",
            "venue": "Electronic Journal of Statistics,",
            "year": 2015
        },
        {
            "authors": [
                "Christian L\u00e9onard"
            ],
            "title": "From the Schr\u00f6dinger problem to the Monge\u2013Kantorovich problem",
            "venue": "Journal of Functional Analysis,",
            "year": 2012
        },
        {
            "authors": [
                "Ruilin Li",
                "Xiaojing Ye",
                "Haomin Zhou",
                "Hongyuan Zha"
            ],
            "title": "Learning to match via inverse optimal transport",
            "venue": "Journal of machine learning research,",
            "year": 2019
        },
        {
            "authors": [
                "Shaojun Ma",
                "Haodong Sun",
                "Xiaojing Ye",
                "Hongyuan Zha",
                "Haomin Zhou"
            ],
            "title": "Learning cost functions for optimal transport",
            "venue": "arXiv preprint arXiv:2002.09650,",
            "year": 2020
        },
        {
            "authors": [
                "Gonzalo Mena",
                "Jonathan Niles-Weed"
            ],
            "title": "Statistical bounds for entropic optimal transport: sample complexity and the central limit theorem",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Marcel Nutz"
            ],
            "title": "Introduction to entropic optimal transport",
            "venue": "Lecture notes, Columbia University,",
            "year": 2021
        },
        {
            "authors": [
                "Fran\u00e7ois-Pierre Paty",
                "Marco Cuturi"
            ],
            "title": "Regularized optimal transport is ground cost adversarial",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Gabriel Peyr\u00e9",
                "Marco Cuturi"
            ],
            "title": "Computational optimal transport: With applications to data science",
            "venue": "Foundations and Trends\u00ae in Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Clarice Poon",
                "Gabriel Peyr\u00e9"
            ],
            "title": "Smooth bilevel programming for sparse regularization",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Philippe Rigollet",
                "Austin J Stromme"
            ],
            "title": "On the sample complexity of entropic optimal transport",
            "venue": "arXiv preprint arXiv:2206.13472,",
            "year": 2022
        },
        {
            "authors": [
                "Yossi Rubner",
                "Carlo Tomasi",
                "Leonidas J. Guibas"
            ],
            "title": "The earth mover\u2019s distance as a metric for image retrieval",
            "venue": "International Journal of Computer Vision,",
            "year": 2000
        },
        {
            "authors": [
                "Filippo Santambrogio"
            ],
            "title": "Optimal transport for applied mathematicians",
            "venue": "Birka\u0308user, NY,",
            "year": 2015
        },
        {
            "authors": [
                "Geoffrey Schiebinger",
                "Jian Shu",
                "Marcin Tabaka",
                "Brian Cleary",
                "Vidya Subramanian",
                "Aryeh Solomon",
                "Joshua Gould",
                "Siyan Liu",
                "Stacie Lin",
                "Peter Berube"
            ],
            "title": "Optimal-transport analysis of single-cell gene expression identifies developmental trajectories in reprogramming",
            "year": 2019
        },
        {
            "authors": [
                "Liangliang Shi",
                "Gu Zhang",
                "Haoyu Zhen",
                "Jintao Fan",
                "Junchi Yan"
            ],
            "title": "Understanding and generalizing contrastive learning from the inverse optimal transport perspective",
            "year": 2023
        },
        {
            "authors": [
                "Richard Sinkhorn"
            ],
            "title": "A relationship between arbitrary positive matrices and doubly stochastic matrices",
            "venue": "Ann. Math. Statist.,",
            "year": 1964
        },
        {
            "authors": [
                "Andrew M Stuart",
                "Marie-Therese Wolfram"
            ],
            "title": "Inverse optimal transport",
            "venue": "SIAM Journal on Applied Mathematics,",
            "year": 2020
        },
        {
            "authors": [
                "Robert Tibshirani"
            ],
            "title": "Regression shrinkage and selection via the lasso",
            "venue": "Journal of the Royal Statistical Society Series B: Statistical Methodology,",
            "year": 1996
        },
        {
            "authors": [
                "Joel A Tropp"
            ],
            "title": "An introduction to matrix concentration inequalities",
            "venue": "Foundations and Trends\u00ae in Machine Learning,",
            "year": 2015
        },
        {
            "authors": [
                "Samuel Vaiter",
                "Mohammad Golbabaee",
                "Jalal Fadili",
                "Gabriel Peyr\u00e9"
            ],
            "title": "Model selection with low complexity priors",
            "venue": "Information and Inference: A Journal of the IMA,",
            "year": 2015
        },
        {
            "authors": [
                "Fan Wang",
                "Leonidas J Guibas"
            ],
            "title": "Supervised earth mover\u2019s distance learning and its computer vision applications",
            "venue": "In European Conference on Computer Vision,",
            "year": 2012
        },
        {
            "authors": [
                "Kilian Q Weinberger",
                "John Blitzer",
                "Lawrence K Saul"
            ],
            "title": "Distance metric learning for large margin nearest neighbor classification",
            "venue": "In Advances in neural information processing systems,",
            "year": 2006
        },
        {
            "authors": [
                "Eric Xing",
                "Michael Jordan",
                "Stuart J Russell",
                "Andrew Ng"
            ],
            "title": "Distance metric learning with application to clustering with side-information",
            "venue": "Advances in neural information processing systems,",
            "year": 2002
        },
        {
            "authors": [
                "Jie Xu",
                "Lei Luo",
                "Cheng Deng",
                "Heng Huang"
            ],
            "title": "Multi-level metric learning via smoothed wasserstein distance",
            "venue": "In IJCAI,",
            "year": 2018
        },
        {
            "authors": [
                "Gloria Zen",
                "Elisa Ricci",
                "Nicu Sebe"
            ],
            "title": "Simultaneous ground metric learning and matrix factoriza",
            "year": 2024
        },
        {
            "authors": [
                "\u3008cA",
                "\u03c0\u3009\u2212KL(\u03c0|\u03b1\u0302\u2297 \u03b2\u0302). A"
            ],
            "title": "FENCHEL-YOUNG LOSS INTERPRETATION It is argued in Blondel et al. (2020) that, associated to a prediction rule of the form \u0177(\u03b8) = argmax \u03bc\u2208dom(\u03a9",
            "year": 2020
        },
        {
            "authors": [
                "G j"
            ],
            "title": "PROOFS FOR SECTION 3 C.1 PROOF OF PROPOSITION 2 (DIFFERENTIABILITY OF W ) For the strict convexity of W (A) see Lemma 3 in Dupuy",
            "venue": "Galichon",
            "year": 2014
        },
        {
            "authors": [
                "\u2202u",
                "v F (uA",
                "A v A"
            ],
            "title": "\u22121 is a bounded operator (see e.g. Deimling (2010) for the statement of IFT in Banach spaces) and to show this it is enough to show that, for some constant C",
            "year": 2010
        },
        {
            "authors": [
                "Poon",
                "Peyr\u00e9"
            ],
            "title": "2021), which proposes a state-of-the-art Lasso solver, the last step is to use the following Hadamard product over-parameterization of the l1 norm \u2225A\u22251 = min",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Optimal transport has emerged as a key theoretical and numerical ingredient in machine learning for performing learning over probability distributions. It enables the comparison of probability distributions in a \u201cgeometrically faithful\u201d manner by lifting a ground cost (or \u201cmetric\u201d in a loose sense) between pairs of points to a distance between probability distributions, metrizing the convergence in law. However, the success of this OT approach to ML is inherently tied to the hypothesis that the ground cost is adapted to the problem under study. This necessitates the exploration of ground metric learning. However, it is exceptionally challenging due to its a priori highly non-convex nature when framed as an optimization problem, thereby inheriting complications in its mathematical analysis. As we illustrate in this theoretical article, these problems become tractable \u2013 numerically and theoretically \u2013 if one assumes access to samples from the OT coupling (i.e., having access to some partial matching driven by the ground cost). Admittedly, this is a restrictive setup, but it arises in practice (refer to subsequent sections for illustrative applications) and can also be construed as a step in a more sophisticated learning pipeline. The purpose of this paper is to propose some theoretical understanding of the possibility of stably learning a ground cost from partial matching observations."
        },
        {
            "heading": "1.1 PREVIOUS WORKS",
            "text": "Entropic Optimal Transport. OT has been instrumental in defining and studying various procedures at the core of many ML pipelines, such as bag-of-features matching Rubner et al. (2000), distances in NLP Kusner et al. (2015), generative modeling Arjovsky et al. (2017), flow evolution for sampling De Bortoli et al. (2021), and even single-cell trajectory inference Schiebinger et al. (2019). We refer to the monographs Santambrogio (2015) for detailed accounts on the theory of OT, and Peyr\u00e9 et al. (2019) for its computational aspects. Of primary importance to our work, entropic regularization of OT is the workhorse of many ML applications. It enables a fast and highly parallelizable estimation of the OT coupling using the Sinkhorn algorithm Sinkhorn (1964).\nMore importantly, it defines a smooth distance that incorporates the understanding that matching procedures should be modeled as a noisy process (i.e., should not be assumed to be 1:1). These advantages were first introduced in ML by the seminal paper of Cuturi Cuturi (2013), and this approach finds its roots in Schr\u00f6dinger\u2019s work in statistical physics L\u00e9onard (2012). The role of noise in matching (with applications in economics) and its relation to entropic OT were advanced in a series of papers by Galichon and collaborators Galichon & Salani\u00e9 (2010); Dupuy & Galichon (2014); Galichon & Salani\u00e9 (2022); see the book Galichon (2018). These works are key inspirations for the present paper, which aims at providing more theoretical understanding in the case of inverse OT (as detailed next).\nMetric Learning. The estimation of some metrics from pairwise interactions (either positive or negative) falls into the classical field of metric learning in ML, and we refer to the monograph Bellet et al. (2013) for more details. In contrast to the inverse OT (iOT) problem considered in this paper, classical metric learning is more straightforward, as no global matching between sets of points is involved. This allows the metric to be directly optimized, while the iOT problem necessitates some form of bilevel optimization. Similarly to our approach, since the state space is typically continuous, it is necessary to restrict the class of distances to render the problem tractable. The common option, which we also adopt in our paper to exemplify our findings, is to consider the class of Mahalanobis distances. These distances generalize the Euclidean distance and are equivalent to computing a vectorial embedding of the data points. See, for instance, Xing et al. (2002); Weinberger et al. (2006); Davis & Dhillon (2008).\nOT Ground Metric Learning. The problem of estimating the ground cost driving OT in a supervised manner was first addressed by Cuturi & Avis (2014). Unlike methods that have access to pairs of samples, the ground metric learning problem requires pairs of probability distributions and then evolves into a classical metric learning problem, but within the OT space. The class of ground metrics can be constrained, for example, by utilizing Mahalanobis Wang & Guibas (2012); Xu et al. (2018); Kerdoncuff et al. (2021) or geodesic distances Heitz et al. (2021), to devise more efficient learning schemes. The study Zen et al. (2014) conducts ground metric learning and matrix factorization simultaneously, finding applications in NLP Huang et al. (2016). It is noteworthy that ground metric learning can also be linked to generative models through adversarial training Genevay et al. (2018) and to robust learning Paty & Cuturi (2020) by maximizing the cost to render the OT distance as discriminative as possible.\nInverse Optimal Transport. The inverse optimal transport problem (iOT) can be viewed as a specific instance of ground metric learning, where one aims to infer the ground cost from partial observations of the (typically entropically regularized) optimal transport coupling. This problem was first formulated and examined by Dupuy and Galichon Dupuy & Galichon (2014) over a discrete space (also see Galichon & Salani\u00e9 (2022) for a more detailed analysis), making the fundamental remark that the maximum likelihood estimator amounts to solving a convex problem. The mathematical properties of the iOT problem for discrete space (i.e., direct computation of the cost between all pairs of points) are explored in depth in Chiu et al. (2022), studying uniqueness (up to trivial ambiguities) and stability to pointwise noise. Note that our theoretical study differs fundamentally as we focus on continuous state spaces. This \u201ccontinuous\u201d setup assumes access only to a set of couples, corresponding to matches (or links) presumed to be drawn from an OT coupling. In this scenario, the iOT is typically an ill-posed problem, and Dupuy et al. (2019); Carlier et al. (2023) propose regularizing the maximum likelihood estimator with either a lowrank (using a nuclear norm penalty) or a sparse prior (using an \u21131 Lasso-type penalty). In our work, we concretely focus on the sparse case, but our theoretical treatment of the iOT could be extended to general structured convex regularization, along the lines of Vaiter et al. (2015). While not the focus of our paper, it is noteworthy that these works also propose efficient large-scale, non-smooth proximal solvers to optimize the penalized maximum likelihood functional, and we refer to Ma et al. (2020) for an efficient solver without inner loop calls to Sinkhorn\u2019s algorithm. This approach was further refined in Stuart & Wolfram (2020), deriving it from a Bayesian interpretation, enabling the use of MCMC methods to sample the posterior instead of optimizing a pointwise estimate (as we consider here). They also propose parameterizing cost functions as geodesic distances on graphs (while we consider only linear models to maintain convexity). An important application of iOT to ML, explored by Li et al. (2019), is to perform link prediction by solving new OT problems once the cost has been estimated from the observed couplings. Another category\nof ML application of iOT is representation learning (learning embeddings of data into, e.g., an Euclidean space) from pairwise interactions, as demonstrated by Shi et al. (2023), which recasts contrastive learning as a specific instance of iOT.\nInverse problems and model selection. The iOT problem is formally a bilevel problem, as the observation model necessitates solving an OT problem as an inner-level program Colson et al. (2005) \u2013 we refer to Eisenberger et al. (2022) for a recent numerical treatment of bilevel programming with entropic OT. The iOT can thus be conceptualized as an \u201cinverse optimization\u201d problem Zhang & Liu (1996); Ahuja & Orlin (2001), but with a particularly favorable structure, allowing it to be recast as a convex optimization. This provides the foundation for a rigorous mathematical analysis of performance, as we propose in this paper. The essence of our contributions is a theoretical examination of the recoverability of the OT cost from noisy observations, and particularly, the robustness to noise of the sparse support of the cost (for instance, viewed as a symmetric matrix for Mahalanobis norms). There exists a rich tradition of similar studies in the fields of inverse problem regularization and model selection in statistics. The most prominent examples are the sparsistency theory of the Lasso Tibshirani (1996) (least square regularized by \u21131), which culminated in the theory of compressed sensing Cand\u00e8s et al. (2006). These theoretical results are predicated on a so-called \u201cirrepresentability condition\u201d Zhao & Yu (2006), which ensures the stability of the support. While our analysis is grounded in similar concepts (in particular, we identify the corresponding irrepresentability condition for the iOT), the iOT inverse problem is fundamentally distinct due to the differing observation model (it corresponds to a sampling process rather than the observation of a vector) and the estimation process necessitates solving a linear program of potentially infinite dimension (in the limit of a large number of samples). This mandates a novel proof strategy, which forms the core of our mathematical analysis."
        },
        {
            "heading": "1.2 CONTRIBUTIONS",
            "text": "This paper proposes the first mathematical analysis of the performance of regularized iOT estimation, focusing on the special case of sparse \u21131 regularization (the \u21131-iOT method). We begin by deriving the customary \u201cirrepresentability condition\u201d of the iOT problem, rigorously proving that it is well-defined. This condition interweaves the properties of the Hessian of the maximum likelihood functional with the sparse support of the sought-after cost. The main contribution of this paper is Theorem 5, which leverages this abstract irrepresentability condition to ensure sparsistency of the \u21131-iOT method. This relates to the robust estimation of the cost and its support in some linear model, assuming the number n of samples is large enough. Specifically, we demonstrate a sample complexity of n\u22121/2. Our subsequent sets of contributions are centered on the case of matching between samples of Gaussian distributions. Herein, we illustrate in Lemma 7 how to compute the irrepresentability condition in closed form. This facilitates the examination of how the parameters of the problem, particularly regularization strength and the covariance of the distributions, influence the success and stability of iOT. We further explore the limiting cases of small and large entropic regularization, revealing in Proposition 8 and Proposition 9 that iOT interpolates between the graphical lasso (to estimate the graph structure of the precision matrix) and a classical lasso. This sheds light on the connection between iOT and graph estimation procedures. Simple synthetic numerical explorations in Section 5.2 further provide intuition about how \u03b5 and the geometry of the graph associated with a sparse cost impact sparsistency. As a minor numerical contribution, we present in Appendix F a large-scale \u21131-iOT solver, implemented in JAX and distributed as open-source software."
        },
        {
            "heading": "2 INVERSE OPTIMAL TRANSPORT",
            "text": "The forward problem Given probability distributions \u03b1 \u2208P (X ), \u03b2 \u2208P (Y ) and cost function c : X \u00d7Y \u2192R, the entropic optimal transport problem seeks to compute a coupling density\nSink(c,\u03b5)\u225c argmax \u03c0\u2208U (\u03b1,\u03b2) \u2329c, \u03c0\u232a\u2212 \u03b5 2\nKL(\u03c0|\u03b1\u2297\u03b2) where \u2329c, \u03c0\u232a\u225c \u222b c(x, y)d\u03c0(x, y), (1)\nwhere KL(\u03c0|\u03be) \u225c \u222b log(d\u03c0/d\u03be)d\u03c0\u2212 \u222b d\u03c0 and U (\u03b1,\u03b2) is the space of all probability measures \u03c0 \u2208P (X \u00d7Y ) with marginals \u03b1,\u03b2.\nThe inverse problem The inverse optimal transport problem seeks to recover the cost function c given an approximation \u03c0\u0302n of the probability coupling \u03c0\u0302\u225c Sink(c,\u03b5). A typical setting is an\nempirical probability coupling \u03c0\u0302n = 1n \u2211n i=1\u03b4(xi ,yi ) where (xi , yi ) n i=1 i i d\u223c \u03c0\u0302. See Section 2.2.\nThe loss function The iOT problem has been proposed and studied in a series of papers, see Section 1.2. The approach is typically to consider some linear parameterization 1 of the cost cA(x, y) by some parameter A \u2208Rs . The key observation of Dupuy et al. (2019) is that the negative log-likelihood of \u03c0\u0302 at parameter value A is given by\nL (A, \u03c0\u0302)\u225c\u2212\u2329cA , \u03c0\u0302\u232a+W\u03c0\u0302(A) where W\u03c0\u0302(A)\u225c sup \u03c0\u2208U (\u03b1\u0302,\u03b2\u0302) \u2329cA , \u03c0\u232a\u2212 \u03b5 2 KL(\u03c0|\u03b1\u0302\u2297 \u03b2\u0302),\nand \u03b1\u0302, \u03b2\u0302 are the marginals of \u03c0\u0302. For ease of notation, unless stated otherwise, we write W =W\u03c0\u0302. So, computing the maximum likelihood estimator A for the cost corresponds to minimizing the convex \u2018loss function\u2019 A 7\u2192L (A, \u03c0\u0302), which, by regarding W\u03c0\u0302 as a convex conjugate, can be seen as an instance of a Fenchel-Young loss, a family of losses proposed in Blondel et al. (2020) . We write the parameterization as\n\u03a6 : A \u2208Rs 7\u2192 cA = s\u2211\nj=1 A j C j , where C j \u2208C (X \u00d7Y ).\nA relevant example are quadratic loss functions, so that for X \u2282 Rd1 , Y \u2282 Rd2 , given A \u2208 Rd1\u00d7d2 , cA(x, y) = x\u22a4Ay . In this case, s = d1d2 and for k = (i , j ) \u2208 [d1]\u00d7 [d2], Ck (x, y) = xi y j .\n\u21131-iOT To handle the presence of noisy data (typically coming from the sampling process), various regularization approaches have been proposed. In this work, we focus on the use of \u21131-regularization Carlier et al. (2023) to recover sparse parametrizations:\nargmin A\nF (A), where F (A)\u225c\u03bb\u2225A\u22251 +L (A, \u03c0\u0302). (iOT\u2212\u21131(\u03c0\u0302))\nKantorovich formulation Note that W (A) is defined via a concave optimization problem and by Fenchel duality, one can show that (iOT\u2212\u21131(\u03c0\u0302)) has the following equivalent Kantorovich formulation Carlier et al. (2023):\nargmin A, f ,g\nK (A, f , g ), where K (A, f , g )\u225cJ (A, f , g )+\u03bb\u2225A\u22251 , and (K\u221e)\nJ (A, f , g )\u225c\u2212 \u222b ( f (x)+ g (y)+\u03a6A(x, y))d \u03c0\u0302(x, y)+\u03b5 2 \u222b exp ( 2( f (x)+ g (y)+\u03a6A(x, y)) \u03b5 ) d\u03b1(x)d\u03b2(y).\nBased on this formulation, various algorithms have been proposed, including alternating minimization with proximal updates Carlier et al. (2023). Section F details a new large scale solver that we use for the numerical simulations."
        },
        {
            "heading": "2.1 INVARIANCES AND ASSUMPTIONS",
            "text": "Assumption 1. We first assume that X and Y are compact.\nNote that J has the translation invariance property that for any constant function u, J ( f +u, g \u2212 u, A) =J ( f , g , A), so to remove this invariance, throughout, we restrict the optimization of (K\u221e) to the set\nS \u225c { (A, f , g ) \u2208Rs \u00d7L2(\u03b1)\u00d7L2(\u03b2) ; \u222b g (y)d\u03b2(y) = 0 } . (2)\nNext, we make some assumptions on the cost to remove invariances in the iOT problem. Assumption 2 (Assumption on the cost). (i) E(x,y)\u223c\u03b1\u2297\u03b2[C(x, y)C(x, y)\u22a4] \u2ab0 Id is invertible.\n(ii) \u2225\u2225C(x, y)\u2225\u2225\u00c9 1 for \u03b1 almost every x and \u03b2-almost every y .\n1In this work, we restrict to linear parameterizations, although the same loss function can also be applied to learn costs with nonlinear parameterization, e.g. via neural networks Ma et al. (2020).\n(iii) for all k, \u222b Ck (x, y)d\u03b1(x) = 0 for \u03b2-a.e. y and \u222b\nCk (x, y)d\u03b2(y) = 0 for \u03b1-a.e. x. Under these assumptions, it can be shown that iOT has a unique solution (see remark after Proposition 2). Assumption 2 (i) is to ensure that cA =\u03a6A is uniquely determined by A. Assumption 2 (ii) is without loss of generality, since we assume that \u03b1,\u03b2 are compactly supported, so this holds up to a rescaling of the space. Assumption 2 (iii) is to handle the invariances pointed out in Carlier et al. (2023) and Ma et al. (2020):\nJ (A, f , g ) =J (A\u2032, f \u2032, g \u2032) \u21d0\u21d2 cA + ( f \u2295 g ) = cA\u2032 + ( f \u2032\u2295 g \u2032). (3)\nAs observed in Carlier et al. (2023), any cost can be adjusted to fit this assumption: one can define\nC\u0303k (x, y) = Ck (x, y)\u2212uk (x)\u2212 vk (y) where uk (x) = \u222b Ck (x, y)d\u03b2(y) and vk (y) = \u222b Ck (x, y)d\u03b1(x) \u2212 \u222b Ck (x, y)d\u03b1(x)d\u03b2(y). Letting\n\u03a6\u0303A = \u2211k Ak C\u0303k , we have (( f \u2212\u2211k Ak uk)\u2295 (g \u2212\u2211k Ak vk))+ \u03a6\u0303A = ( f \u2295 g )+\u03a6A. So optimization with the parametrization\u03a6 is equivalent to optimization with \u03a6\u0303. NB: For the quadratic cost ck (x, y) = xi y j for k = (i , j ), condition (iii) corresponds to recentering the data points, and taking x 7\u2192 x\u2212\u222b xd\u03b1(x) and y 7\u2192 y\u2212\u222b yd\u03b2(y). Condition (ii) holds if \u2225x\u2225\u2228\u2225\u2225y\u2225\u2225\u00c9 1 for \u03b1-a.e. x and \u03b2-a.e. y . Condition (i) corresponds to invertibility of E\u03b1[xx\u22a4]\u2297E\u03b2[y y\u22a4]."
        },
        {
            "heading": "2.2 THE FINITE SAMPLE PROBLEM",
            "text": "In practice, we do not observe \u03c0\u0302 but n data-points (xi , yi ) i i d\u223c \u03c0\u0302 for i = 1, . . . ,n, where \u03c0\u0302= Sink(c A\u0302 ,\u03b5). To recover A\u0302, we plug into iOT\u2212\u21131 the empirical measure \u03c0\u0302n = 1n \u2211n\ni=1\u03b4xi ,yi and consider the estimator\nAn \u2208 argmin A \u03bb\u2225A\u22251 +L (A, \u03c0\u0302n), (iOT\u2212\u21131(\u03c0\u0302n)) As in section 2.1, to account for the invariance (3), when solving (iOT\u2212\u21131(\u03c0\u0302n)), we again centre the cost parameterization such that for all i , \u2211n i=1 Ck (xi , y j ) = 0 and for all j , \u2211n j=1 Ck (xi , y j ) = 0. Note also that iOT\u2212\u21131(\u03c0\u0302n) can be formulated entirely in finite dimensions. See Appendix B for details."
        },
        {
            "heading": "3 THE CERTIFICATE FOR SPARSISTENCY",
            "text": "In this section, we consider the problem (iOT\u2212\u21131(\u03c0\u0302)) with full data \u03c0\u0302 and present a sufficient condition for support recovery, that we term non-degeneracy of the certificate. For simplicity of notation, throughout this section denote W \u225cW\u03c0\u0302. Under non-degeneracy of the certificate we obtain support recovery as stated in Theorem 3, a known result whose proof can be found e.g. in Lee et al. (2015). This condition can be seen as a generalization of the celebrated Lasso\u2019s Irrepresentability condition (see e.g. Hastie et al. (2015)) \u2013 Lasso corresponds to having a quadratic loss instead of L (A, \u03c0\u0302), thus \u22072AW (A) in the definition below reduces to a matrix. In what follows, we denote uI \u225c (ui )i\u2208I and UI ,J \u225c (Ui , j )i\u2208I , j\u2208J the restriction operators. Definition 1. The certificate with respect to A and support I = {i : Ai \u0338= 0} is z\u2217A \u225c\u22072W (A)(:,I )(\u22072W (A)(I ,I ))\u22121 sign(A)I . (C) We say that it is non-degenerate if \u2225(z\u2217A)I c \u2225\u221e < 1. The next proposition, whose proof can be found in Appendix C.1, shows that the function W (A) is twice differentiable, thus ensuring that C is well defined. Proposition 2. A 7\u2192W (A) is twice differentiable, strictly convex, with gradient and Hessian\n\u2207AW (A) =\u03a6\u2217\u03c0A , \u22072AW (A) =\u03a6\u2217 \u2202\u03c0A\n\u2202A (x, y)\nwhere \u03c0A is the unique solution to (1) with cost cA =\u03a6A. We remark that strict convexity of W implies that any solution of (iOT\u2212\u21131(\u03c0\u0302)) must be unique, and by \u0393-convergence, solutions A\u03bb to (iOT\u2212\u21131(\u03c0\u0302)) converge to A\u0302 as \u03bb\u2192 0. The next theorem, a well-known result (see (Lee et al., 2015, Theorem 3.4) for a proof), shows that support recovery can be characterized via z\u2217\nA\u0302 .\nTheorem 3. Let \u03c0\u0302= Sink(c A\u0302 ,\u03b5). If z\u2217A\u0302 is non-degenerate, then for all \u03bb sufficiently small, the solution A\u03bb to (iOT\u2212\u21131(\u03c0\u0302)) is sparsistent with \u2225\u2225A\u03bb\u2212 A\u0302\u2225\u2225=O (\u03bb) and Supp(A\u03bb)= Supp(A\u0302)."
        },
        {
            "heading": "3.1 INTUITION BEHIND THE CERTIFICATE (C)",
            "text": "Implication of the non-degeneracy condition The non-degeneracy condition is widely studied for the Lasso problem Hastie et al. (2015). Just as for the Lasso, the closer \u2223\u2223\u2223(z\u2217\nA\u0302 )i \u2223\u2223\u2223 is to 1, the more unstable is this coefficient, and if \u2223\u2223\u2223(z\u2217 A\u0302 )i \u2223\u2223\u2223> 1 then one cannot expect to recover this coefficients when there is noise. Note also that in the Lasso, the pre-certificate formula in (C) roughly the correlation between coefficient inside and outside the support I . In our case, the loss is more complex and this correlation is measured according to the hessian of W , which integrates the curvature of the loss. This curvature formula is however involved, so to gain intuition, we perform a detailed analysis in the \u03b5\u2192 0 and \u03b5\u2212 > \u221e for the Gaussian setting where it becomes much simpler (see Section 5.1).\nLink to optimality conditions The certificate z\u2217 A\u0302\ncan be seen as the limit optimality condition for the optimization problem (iOT\u2212\u21131(\u03c0\u0302)) as \u03bb\u2192 0: by the first order optimality condition to (iOT\u2212\u21131(\u03c0\u0302)), A\u03bb is a solution if and only if z\u03bb\u225c\u2212 1\u03bb\u2207L(A\u03bb) \u2208 \u2202\u2225A\u03bb\u22251 , where the subdifferential for the \u21131 norm has the explicit form \u2202\u2225A\u22251 = { z ; \u2225z\u2225\u221e \u00c9 1, \u2200i \u2208 Supp(A), zi = sign(Ai ) } . It follows\nz\u03bb can be seen as a certificate for the support of A\u03bb since Supp(A\u03bb) \u2286 {i ; \u2223\u2223z\u03bbi \u2223\u2223= 1}. To study the support behavior of A\u03bb for small \u03bb, it is therefore interesting to consider the limit of z\u03bb as \u03bb\u2192 0. Its limit is precisely the subdifferential element with the minimal norm and coincides with (C) under the nondegeneracy condition:\nProposition 4. Let z\u03bb\u225c\u2212 1\u03bb\u2207L(A\u03bb) where A\u03bb solves (iOT\u2212\u21131(\u03c0\u0302)). Then, lim \u03bb\u21920 z\u03bb = zmin A\u0302 \u225c argmin z { \u2329z, (\u22072W (A\u0302)\u22121 z\u232aF ; z \u2208 \u2202\u2225A\u0302\u22251} (MNC) Moreover, if z\u2217 A\u0302 is non-degenerate, then zmin A\u0302 = z\u2217 A\u0302 ."
        },
        {
            "heading": "4 SAMPLE COMPLEXITY BOUNDS",
            "text": "Our main contribution shows that (C) is a certificate for sparsistency under sampling noise: Theorem 5. Let \u03c0\u0302= Sink(c A\u0302 ,\u03b5). Suppose that the certificate z\u2217A\u0302 is non-degenerate. Let \u03b4> 0. Then, for all sufficiently small regularization parameters \u03bb and sufficiently many number of samples n,\n\u03bb\u2272 1 and max ( exp(C \u2225\u2225A\u0302\u2225\u22251 /\u03b5)\u221alog(1/\u03b4)\u03bb\u22121,\u221alog(2s))\u2272pn, for some constant C > 0, with probability at least 1\u2212\u03b4, the minimizer An to (Pn) is sparsistent with A\u0302 with Supp(An) = Supp(A\u0302) and\n\u2225\u2225An \u2212 A\u0302\u2225\u22252 \u2272\u03bb+\u221aexp(C \u2225\u2225A\u0302\u2225\u22251 /\u03b5) log(1/\u03b4)n\u22121. Main idea behind Theorem 5 We know from Theorem 3 that there is some \u03bb0 > 0 such that for all \u03bb \u00c9 \u03bb0, the solution to (K\u221e) has the same support as A\u0302. To show that the finite sample problem also recovers the support of A\u0302 when n is sufficiently large, we fix \u03bb \u2208 (0,\u03bb0] and consider the setting where the observations are iid samples from the coupling measure \u03c0\u0302. We will derive convergence bounds for the primal and dual solutions as the number of samples n increases. Let (A\u221e, f\u221e, g\u221e) minimise (K\u221e). Denote F\u221e = ( f\u221e(xi ))i\u2208[n], G\u221e = (g\u221e(y j )) j\u2208[n] and\nP\u221e = 1 n2\n(p\u221e(xi , y j ))i , j\u2208[n], where p\u221e(x, y) = exp ( 2\n\u03b5\n( \u03a6A\u221e(x, y)+ f\u221e(x)+ g\u221e(y) )) .\nLet An minimize iOT\u2212\u21131(\u03c0\u0302n). Then, there exists a probability matrix Pn \u2208 Rn\u00d7n+ and vectors Fn ,Gn \u2208 Rn such that Pn = 1n2 exp ( 2 \u03b5 (\u03a6n An +Fn \u2295Gn) ) . In fact, (An ,Fn ,Gn) minimize the finite dimensional dual problem (Kn) given in the appendix. Now consider the certificates\nz\u221e = 1 \u03bb \u03a6\u2217\n( p\u221e\u03b1\u2297\u03b2\u2212 \u03c0\u0302 ) and zn = 1\n\u03bb \u03a6\u2217n\n( Pn \u2212 P\u0302n ) .\nNote that z\u221e and zn both depend on \u03bb; the superscript was dropped since \u03bb is fixed. Moreover, z\u221e is precisely z\u03bb from Propostion 4. By exploiting strong convexity properties of Jn , one can show the following sample complexity bound on the convergence of zn to z\u221e (the proof can be found in the appendix):\nProposition 6. Let n \u2273 max ( log(1/\u03b4)\u03bb\u22122, log(2s) ) for some \u03b4> 0. For some constant C > 0, with\nprobability at least 1\u2212\u03b4, \u2225z\u221e\u2212 zn\u2225\u221e \u2272 exp(C \u2225\u2225A\u0302\u2225\u22251 /\u03b5) log(1/\u03b4)\u03bb\u22121n\u2212 12 and\n\u2225An \u2212 A\u221e\u222522 + 1\nn \u2211 i (Fn \u2212F\u221e)2i + 1 n \u2211 j (Gn \u2212G\u221e)2j \u2272 \u03b52 exp(C \u2225\u2225A\u0302\u2225\u22251 /\u03b5) log(1/\u03b4)n\u22121.\nFrom Proposition 4, for all \u03bb\u00c9\u03bb0 for some \u03bb0 sufficiently small, the magnitude of z\u221e outside the support of A\u0302 is less than one. Moreover, the convergence result in Proposition 6 above implies that, for n sufficiently large, the magnitude of zn outside the support of A\u0302 is also less than one. Hence, since the set {i ; (zn)i =\u00b11} determines the support of An , we have sparsistency."
        },
        {
            "heading": "5 GAUSSIAN DISTRIBUTIONS",
            "text": "To get further insight about the sparsistency property ot iOT, we consider the special case where the source and target distributions are Gaussians, and the cost parametrization cA(x, y) = x\u22a4Ay . To this end, we first derive closed form expressions for the Hessian \u22022AL (A) = \u22072AW (A). Given \u03b1=N (m\u03b1,\u03a3\u03b1) and \u03b2=N (m\u03b2,\u03a3\u03b2), it is known (see Bojilov & Galichon (2016)) that the coupling density is also a Gaussian of the form \u03c0=N ((m\u03b1 m\u03b2 ) , ( \u03a3\u03b1 \u03a3 \u03a3\u22a4 \u03a3\u03b2 )) for some \u03a3 \u2208Rd1\u00d7d2 . In this case, W can be written as an optimization problem over the cross-covariance \u03a3 Bojilov & Galichon (2016).\nW (A) = sup \u03a3\u2208Rd1\u00d7d2 \u2329A, \u03a3\u232a+ \u03b5 2\nlogdet ( \u03a3\u03b2\u2212\u03a3\u22a4\u03a3\u22121\u03b1 \u03a3 ) , (4)\nwith the optimal solution being precisely the cross-covariance of the optimal coupling \u03c0. In Bojilov & Galichon (2016), the authors provide an explicit formula for the minimizer \u03a3 to (4), and, consequently, for \u2207W (A):\n\u03a3=\u03a3\u03b1A\u2206 ( \u2206A\u22a4\u03a3\u03b1A\u2206 )\u2212 12 \u2206\u2212 12\u03b5A\u2020,\u22a4 where \u2206\u225c (\u03a3\u03b2+ \u03b524 A\u2020\u03a3\u22121\u03b1 A\u2020,\u22a4 ) 1 2 . (5)\nBy differentiating the first order condition for W , that is A\u2020 = \u03b5\u22121(\u03a3\u03b2\u2212\u03a3\u22a4\u03a3\u22121\u03b1 \u03a3)\u03a3\u2020\u03a3\u03b1, Galichon derives the expression for the Hessian in terms of \u03a3 in (5):\n\u22072W (A) = \u2202A\u03a3= \u03b5 ( \u03a3\u03b1\u03a3 \u22121,\u22a4\u2297\u03a3\u03b2\u03a3\u22121 +T )\u22121 ( A\u22121,\u22a4\u2297 A\u22121) , (6) where T is such that Tvec(A) = vec(A\u22a4). This formula does not hold in when A is rectangular or rank deficient, since A\u2020 is not differentiable. In the following, we derive, via the implicit function theorem, a general formula for \u2202A\u03a3 that agrees with that of Galichon in the square invertible case.\nLemma 7. Denoting \u03a3 as in (5), one has \u22072W (A) = \u03b5 ( \u03b52(\u03a3\u03b2\u2212\u03a3\u22a4\u03a3\u03b1\u03a3)\u22121 \u2297 (\u03a3\u03b1\u2212\u03a3\u03a3\u22121\u03b2 \u03a3\u22a4)\u22121 + (A\u22a4\u2297 A)T )\u22121 (7)\nThis formula given in Lemma 7 provides an explicit expression for the certificate (C).\n5.1 LIMIT CASES FOR LARGE AND SMALL \u03b5\nThis section explores the behaviour of the certificate in the large/small \u03b5 limits: Proposition 8 reveals that the large \u03b5 limit coincides with the classical Lasso while Proposition 9 reveals that the small epsilon limit (for symmetric A \u227b 0 and \u03a3\u03b1 =\u03a3\u03b2 = Id) coincides with the Graphical Lasso. In the following results, we denote the functional in (iOT\u2212\u21131(\u03c0\u0302)) with parameters \u03bb and \u03b5 by F\u03b5,\u03bb(A). Proposition 8 (\u03b5\u2192\u221e). Let A\u0302 be invertible and let \u03c0\u0302= Sink(c A\u0302 ,\u03b5) be the observed coupling between \u03b1=N (m\u03b1,\u03a3\u03b1) and \u03b2=N (m\u03b2,\u03a3\u03b2). Then,\nlim \u03b5\u2192\u221ez\u03b5 = (\u03a3\u03b2\u2297\u03a3\u03b1)(:,I )\n( (\u03a3\u03b2\u2297\u03a3\u03b1)(I ,I ) )\u22121 sign(A\u0302)I . (8)\nMoreover, for \u03bb0 > 0, given any sequence (\u03b5 j ) j and A j \u2208 argminA F\u03b5,\u03bb0/\u03b5 j (A) with lim j\u2192\u221e \u03b5 j =\u221e, any cluster point of (A j ) j is in\nargmin A\u2208Rd\u00d7d\n\u03bb0 \u2225A\u22251 + 1\n2 \u2225(\u03a31/2\u03b2 \u2297\u03a31/2\u03b1 )(A\u2212 A\u0302)\u22252F (9)\nInterpretation As \u03b5\u2192\u221e, the KL term forces \u03c0\u0302 to be close to the independent coupling \u03b1\u2297\u03b2 and in the limit, iOT is simply a Lasso problem and the limit in (8) is precisely the Lasso certificate Hastie et al. (2015). Here, the cross covariance of \u03c0\u0302 satisfies \u03b5\u03a3= A+O (\u03b5) ((53) in the appendix), so for large \u03b5, sparsity in A indicates the independence in the coupling between \u03b1 and \u03b2.\nProposition 9 (\u03b5\u2192 0). Let A\u0302 be symmetric positive-definite and let \u03c0\u0302= Sink(c A\u0302 ,\u03b5) be the observed coupling between \u03b1=N (m\u03b1, Id) and \u03b2=N (m\u03b2, Id). Then,\nlim \u03b5\u21920 z\u03b5 = (A\u0302\n\u22121 \u2297 A\u0302\u22121)(:,I ) ( (A\u0302\u22121 \u2297 A\u0302\u22121)(I ,I ) )\u22121 sign(A\u0302)I . (10) Let \u03bb0 > 0. Then, optimizing over symmetric positive semi-definite matrices, given any sequence (\u03b5 j ) j and A j \u2208 argminA\u2ab00 F\u03b5,\u03bb0\u03b5 j (A) with lim j\u2192\u221e \u03b5 j = 0, any cluster point of (A j ) j is in\nargmin A\u2ab00\n\u03bb0 \u2225A\u22251 \u2212 1 2 logdet(A)+ 1 2 \u2329A, A\u0302\u22121\u232a. (11)\nInterpretation In contrast, \u03b5 \u2192 0, the KL term disappears and the coupling \u03c0\u0302 becomes dependent. Naturally, the limit problem is the graphical lasso, typically used to infer conditional independence in graphs (but where covariates can be highly dependent). Note also that the limit (10) is precisely the graphical Lasso certificate Hastie et al. (2015). Here, (Remark 25 in the appendix) one show that for (x, y) \u223c\u03c0, the conditional covariance of x conditional on y (and also vice versa) is \u03b5A\u22121 +O (\u03b52). Sparsity in A can therefore be viewed as information on conditional independence."
        },
        {
            "heading": "5.2 NUMERICAL ILLUSTRATIONS",
            "text": "In order to gain some insight into the impact of \u03b5 and the covariance structure on the efficiency of iOT, we present numerical computations of certificates here. We fix the covariances of the input measures as \u03a3\u03b1 = \u03a3\u03b2 = Idn , similar results are obtained with different covariance as long as they are not rank-deficient. We consider that the support of the sought-after cost matrix A = \u03b4Idn +diag(G1n)\u2212G \u2208Rn\u00d7n is defined as a shifted Laplacian matrix of some graph adjacency matrix G , for a graph of size n = 80 (similar conclusions hold for larger graphs). We set the shift \u03b4 to be 10% of the largest eigenvalue of the Laplacian, ensuring that C is symmetric and definite. This setup corresponds to graphs defining positive interactions at vertices and negative interactions along edges. For small \u03b5, adopting the graphical lasso interpretation (as exposed in Section 5.1) and interpreting C as a precision matrix, this setup corresponds (for instance, for a planar graph) to imposing a spatially smoothly varying covariance C\u22121. Figure 1 illustrates how the value of the certificates zi , j evolves depending on the indexes (i , j ) for three types of graphs (circular, planar, and Erdo\u030bs\u2013R\u00e9nyi with a probability of edges equal to 0.1), for several values of \u03b5. By construction, zi ,i = 1 and zi , j =\u22121 for (i , j ) connected by the graph. For z to be non-degenerate, it is required that |zi , j | < 1, as i moves away from j on the graph. For the circular and planar graphs, the horizontal axis represents the geodesic distance dgeod(i , j ), demonstrating how the certificates become well-behaved as the distance increases. The planar graph displays envelope curves showing the range of values of z for a fixed value of dgeod(i , j ), while this is a single-valued curve for the circular graph due to periodicity. For the Erdo\u030bs\u2013R\u00e9nyi graph, to better account for the randomness of the certificates along the graph edges, we display the histogram of the distribution of |zi , j | for dgeod(i , j ) = 2 (which represents the most critical set of edges, as they are the most likely to be large). All these examples show the same behavior, namely, that increasing \u03b5 improves the behavior of the certificates (which is in line with the stability analysis of Section 5.1, since (8) implies that for large \u03b5, the certificate is trivially non-degenerate whenever \u03a3\u03b1,\u03a3\u03b2 are diagonal), and that pairs of vertices (i , j ) connected by a small distance dgeod(i , j ) are the most likely to be degenerate. This suggests that they will be inaccurately estimated by iOT for small \u03b5.\nFigure 3 in the appendix displays the certificate values in the case of a non-symmetric planar graph. The graph is obtained by deleting all edges on a planar graph with i < j . We plot the certificate values as a function of geodesic distances dgeod(i , j ) of the symmetrized graph. The middle plot shows the certificate values on i \u00ca j (where the actual edges are constrained to be and nondegeneracy requires values smaller than 1 in absolute value for dgeod(i , j ) \u00ca 2). The right plot shows the certificate values on i \u00c9 j where there are no edges and for nondegeneracy, one expects values smaller than 1 in absolute value for dgeod(i , j ) \u00ca 1. Observe that here, the certificate is degenerate for small values of dgeod(i , j ) when \u03b5= 0, meaning that the problem is unstable at the \u201cghost\u201d symmetric edges. As \u03b5\u2192\u221e, the certificate becomes non-degenerate."
        },
        {
            "heading": "CONCLUSION",
            "text": "In this paper, we have proposed the first theoretical analysis of the recovery performance of \u21131-iOT. Much of this analysis can be extended to more general convex regularizers, such as the nuclear norm to promote low-rank Euclidean costs, for instance. Our analysis and numerical exploration support the conclusion that iOT becomes ill-posed and fails to maintain sparsity for overly small \u03b5. When approached from the perspective of graph estimations, unstable indices occur at smaller geodesic distances, highlighting the geometric regularity of iOT along the graph geometry."
        },
        {
            "heading": "ACKNOWLEDGEMENTS",
            "text": "The work of G. Peyr\u00e9 was supported by the European Research Council (ERC project NORIA) and the French government under management of Agence Nationale de la Recherche as part of the \u201cInvestissements d\u2019avenir\u201d program, reference ANR19-P3IA-0001 (PRAIRIE 3IA Institute)."
        },
        {
            "heading": "A.1 MAXIMUM LIKELIHOOD INTERPRETATION",
            "text": "The map A \u2192 Sink(cA ,\u03b5), where Sink(\u00b7, \u00b7) is defined in Section 2, can be seen as parameterizing a set of measures which are absolutely continuous with respect to \u03b1\u2297\u03b2. By the standard duality result (see Nutz (2021)):\n1) The density of Sink(cA ,\u03b5) is given by dSink(cA ,\u03b5)/d(\u03b1\u2297\u03b2) = exp(cA + f A + g A), where f A and g A solve the dual problem, i.e., sup f ,g \u222b f d\u03b1+\u222b g d\u03b2\u2212\u222b exp(cA + f +g )d(\u03b1\u2297\u03b2)+1;\n2) The values of the primal and dual problems agree and are equal to \u2212\u222b f A d\u03b1\u2212\u222b g A d\u03b2. Combining these two facts we obtain that\n\u2212 log (dSink(cA ,\u03b5) d(\u03b1\u2297\u03b2) ) =\u2212cA \u2212 f A \u2212 g A =\u2212cA + sup \u03c0\u2208U (\u03b1,\u03b2) \u2329cA ,\u03c0\u232a\u2212 \u03b5 2 KL(\u03c0|\u03b1\u2297\u03b2) =\u2212cA +W\u03c0\u0302(A),\nwhere W\u03c0\u0302 is defined in as in Section 2. Finally, taking expectation with respect to \u03c0\u0302 yields E(x,y)\u223c\u03c0\u0302 [ \u2212 log (dSink(cA ,\u03b5) d(\u03b1\u2297\u03b2) )] = E(x,y)\u223c\u03c0\u0302 [\u2212 cA]+E(x,y)\u223c\u03c0\u0302[W\u03c0\u0302(A)]=\u2212\u2329cA , \u03c0\u0302\u232a+W\u03c0\u0302(A) =L (A, \u03c0\u0302), thus establishing the connection with ML estimation.\nBilevel interpretation Note also that\nargmin A L (A, \u03c0\u0302) = argmin A \u2212\u2329cA , \u03c0\u0302\u2212\u03c0A\u232a\u2212 \u03b5 2 KL(\u03c0A |\u03b1\u0302\u2297 \u03b2\u0302)\n= argmin A \u2212\u2329cA , \u03c0\u0302\u2212\u03c0A\u232a\u2212 \u03b5 2 KL(\u03c0A |\u03b1\u0302\u2297 \u03b2\u0302)+ \u03b5 2 KL(\u03c0\u0302|\u03b1\u0302\u2297 \u03b2\u0302)\nwhere \u03c0A = argmin\u03c0\u2208U (\u03b1\u0302,\u03b2\u0302)\u2329cA , \u03c0\u232a\u2212KL(\u03c0|\u03b1\u0302\u2297 \u03b2\u0302). Since \u03c0\u0302,\u03c0A have the same marginals,\n\u2329cA , \u03c0\u0302\u2212\u03c0A\u232a = \u2329cA + f A + g A , \u03c0\u0302\u2212\u03c0A\u232a = \u03b5 2 \u2329log(\u03c0A/(\u03b1\u0302\u2297 \u03b2\u0302)), \u03c0\u0302\u2212 \u03c0\u0302A\u232a.\nWe therefore have\nargmin A L (A, \u03c0\u0302) = argmin A KL(\u03c0\u0302|\u03c0A), where \u03c0A = argmin \u03c0\u2208U (\u03b1\u0302,\u03b2\u0302) \u2329cA , \u03c0\u232a\u2212KL(\u03c0|\u03b1\u0302\u2297 \u03b2\u0302)."
        },
        {
            "heading": "A.2 FENCHEL-YOUNG LOSS INTERPRETATION",
            "text": "It is argued in Blondel et al. (2020) that, associated to a prediction rule of the form\ny\u0302(\u03b8) = argmax \u00b5\u2208dom(\u2126) \u2329\u03b8,\u00b5\u232a\u2212\u2126(\u00b5),\nthere is a natural loss function that the authors term Fenchel-Young loss and that is given by\nL\u2126(\u03b8; y)\u225c\u2126 \u2217(\u03b8)+\u2126(y)\u2212\u2329\u03b8, y\u232a.\nWith this in mind, let\u2126(\u03c0) = KL(\u03c0|\u03b1\u2297\u03b2)+ \u03b9U (\u03b1,\u03b2)(\u03c0) where \u03b9U (\u03b1,\u03b2)(\u00b7) is the indicator function of U (\u03b1,\u03b2), and note that\nL (A, \u03c0\u0302) = L\u2126(cA ; \u03c0\u0302)\u2212\u2126(\u03c0\u0302). Since, for the purposes of minimization with respect to A, the term \u2126(\u03c0\u0302) is irrelevant, we see that finding a minimizer of (iOT\u2212\u21131(\u03c0\u0302)) amounts to finding a minimizer of the l1 regularized Fenchel-Young loss associated with KL(\u00b7|\u03b1\u2297\u03b2)+ \u03b9U (\u03b1,\u03b2)(\u00b7)."
        },
        {
            "heading": "B THE FINITE SAMPLE PROBLEM",
            "text": "As mention in Section 2.2, we do not observe the full coupling \u03c0\u0302 = Sink(c A\u0302 ,\u03b5), but only the the empirical measure \u03c0\u0302n = 1n \u2211n i=1\u03b4xi ,yi with (xi , yi )\ni i d\u223c \u03c0\u0302. We show here that the problem iOT\u2212\u21131(\u03c0\u0302n) can be formulated entirely in finite dimensions as follows. Let\nH(P |Q)\u225c\u2211 i , j Pi , j (log(Pi , j /Qi , j )\u22121).\nNote that \u03c0\u0302n has marginals a\u0302n = 1n \u2211n i=1\u03b4xi and b\u0302n = 1n \u2211n\ni=1\u03b4yi . We can interpret \u03c0\u0302n as the matrix P\u0302n = 1n Idn\u00d7n and the \u201cnoisy\u201d primal problem can be equivalently written as\nmin A\u2208Rs sup P\u2208Rn\u00d7n+\n\u03bb\u2225A\u22251 +\u2329\u03a6n A, P \u2212 P\u0302n\u232a\u2212 \u03b5 2 H(P ) s.t . P1= 1 n 1 and P\u22a41= 1 n 1, (Pn)\nwhere we write H(P ) \u225c H(P | 1 n2 1\u22971) and \u03a6n A = \u2211k AkCk , where Ck = (Ck (xi , y j ))i , j\u2208[n] \u2208 Rn\u00d7n . Note that the finite-dimensional problem has the same invariances as (iOT\u2212\u21131(\u03c0\u0302)), so, we will take Ck to be centred so that for all i , \u2211 i (Ck )i , j = 0 and for all j , \u2211 j (Ck )i , j = 0. The finite sample Kantorovich formulation is\ninf A,F,G\u2208Sn\nKn(A,F,G) where Kn(A,F,G)\u225cJn(A,F,G)+\u03bb\u2225A\u22251 and (Kn)\nJn(A,F,G)\u225c\u2212 \u2211 i , j ( Fi \u2295G j + (\u03a6n A)i , j )( P\u0302n ) i , j + \u03b5 2n2 \u2211 i , j exp\n( 2\n\u03b5 (Fi +G j + (\u03a6n A)i , j\n) ,\nand we restrict the optimization of (Kn) over Sn \u225c { (A,F,G) \u2208Rs \u00d7Rn \u00d7Rn ; \u2211 j G j = 0} ."
        },
        {
            "heading": "C PROOFS FOR SECTION 3",
            "text": ""
        },
        {
            "heading": "C.1 PROOF OF PROPOSITION 2 (DIFFERENTIABILITY OF W )",
            "text": "For the strict convexity of W (A) see Lemma 3 in Dupuy & Galichon (2014). The gradient formula can also be found in Dupuy & Galichon (2014) and trivially follows from the envelope theorem\nbecause the optimization problem W (A) has a unique solution. We will only give a proof of the Hessian formula.\nThe formula for the Hessian follows from the formula for the gradient provided we show that the density \u03c0A is continuously differentiable with respect to A. The fact that we can swap the order of the operator\u03a6\u2217 and partial differentiation follows from the conditions for differentiability under the integral sign which holds since the measures are compactly supported. Without loss of generality, let \u03b5= 1. Then, the optimizer in W (A) is of the form Santambrogio (2015):\n\u03c0A(x, y) = exp ( uA(x)+ v A(y)+ cA(x, y) ) ,\nwhere uA(\u00b7) and v A(\u00b7) satisfy uA(x) =\u2212 log \u222b Y exp ( v A(y)+ cA(x, y) ) d\u03b2(y), \u03b1-a.s.\nv A(y) =\u2212 log \u222b X exp ( uA(x)+ cA(x, y) ) d\u03b1(x), \u03b2-a.s.\nIt is known (see e.g. Nutz (2021)) that the functions uA(\u00b7) and v A(\u00b7) inherit the modulus of continuity of the cost (in this case the map (x, y) \u2192 cA(x, y))) and, hence, since we are assuming the measures to be compactly supported, it follows that uA \u2208 L2(\u03b1) and v A \u2208 L2(\u03b2). Moreover, if (uA , v A) solve these two equations then, for any constant c, the pair (uA + c, v A \u2212 c) is also a solution and, hence, to eliminate this ambiguity we consider solutions in H = L2(\u03b1)\u00d7L20(\u03b2), where L20(\u03b2) = { g \u2208 L2(\u03b2) ; \u222b g d\u03b2(y) = 0}. To show that W is twice differentiable, since \u2207W (A) =\u03a6\u2217\u03c0A , it is sufficient to show that A 7\u2192 uA and A 7\u2192 v A are differentiable. To this end, we will apply the Implicit Function Theorem (in Banach spaces) to the map\nF : H \u00d7Rs \u2192 L2(\u03b1)\u00d7L2(\u03b2)uv A \u2192 u + log\u222bY exp(v(y)+ cA(x, y))d\u03b2(y) v + log\u222bX exp(u(x)+ cA(x, y))d\u03b1(x)  ,\nsince we have F (uA , v A , A) = 0. The partial derivative of F at (uA , v A , A) , denoted by \u2202u,v F (uA , v A , A), is the linear map defined by(\n\u2202u,v F (uA , v A , A) ) ( f , g ) = ( f + \u222b Y p A(\u00b7, y)g (y)d\u03b2(y), g + \u222b X qA(x, \u00b7) f (x)d\u03b1(x) ) , (12)\nwhere\np A(x, y) = exp\n( v A(y)+ cA(x, y) ) \u222b Y exp ( v A(y)+ cA(x, y) ) d\u03b2(y) and qA(x, y) = exp ( uA(x)+ cA(x, y) ) \u222b X exp ( uA(y)+ cA(x, y) ) d\u03b2(x) .\nNote that, since F (uA , v A , A) = 0, p A(x, y) = qA(x, y) =\u03c0A(x, y). Moreover, since \u03c0A has marginals \u03b1 and \u03b2, it follows that\u2329\n( f , g ), ( \u2202u,v F (uA , v A , A) ) ( f , g ) \u232a =\n\u222b X f (x)2 d\u03b1(x)+ \u222b X\u00d7Y 2 f (x)g (y)\u03c0A(x, y)d(\u03b1\u2297\u03b2)(x, y)+ \u222b Y g (y)2 d\u03b2(y)\n= \u222b X\u00d7Y ( f (x)+ g (y) )2 \u03c0A(x, y)d(\u03b1\u2297\u03b2)(x, y)\n. (13)\nThis shows that \u2202u,v F (uA , v A , A) is invertible \u2013 the last line of 13 is zero if and only if f \u2295 g \u2261 0 and since g \u2208 L20(\u03b2) it follows that g = 0 and f = 0. To conclude the proof we need to show that ( \u2202u,v F (uA , v A , A)\n)\u22121 is a bounded operator (see e.g. Deimling (2010) for the statement of IFT in Banach spaces) and to show this it is enough to show that, for some constant C , \u2225\u2225\u2225(\u2202u,v F (uA , v A , A))( f , g )\u2225\u2225\u2225\u00caC\u2225( f , g )\u2225.\nThis follows from 13 and the fact that there exists a constant C such that \u03c0A(x, y) \u00ca C for all x and y (see e.g. Nutz (2021) for the existence of C ). In fact, from g \u2208 L20(\u03b2), we obtain \u222b ( f (x)+ g (y)))2 d(\u03b1\u2297\u03b2)(x, y) = \u2225( f , g )\u22252 and, hence, 13 implies that\u2329 ( f , g ), ( \u2202u,v F (uA , v A , A) ) ( f , g ) \u232a \u00caC\u2225( f , g )\u22252\nand from Cauchy-Schwarz applied to the left-hand-side we obtain\u2225\u2225\u2225(\u2202u,v F (uA , v A , A))( f , g )\u2225\u2225\u2225\u00caC\u2225( f , g )\u2225, thus concluding the proof.\nC.2 CONNECTION OF THE CERTIFICATE (C) WITH OPTIMIZATION PROBLEM (iOT\u2212\u21131(\u03c0\u0302)) As mentioned in Section 3.1, the vector z\u03bb provides insight into support recovery since Supp(A\u03bb) \u2286{\ni ; z\u03bbi =\u00b11 } . In this section, we provide the proof to Proposition 4, which shows that as \u03bb converges to 0, z\u03bb converges to the solution of a quadratic optimization problem (MNC) that we term the minimal norm certificate.\nNote that the connection with (C) can now be established by noting that if the minimal norm certificate is non-degenerate then the inequality constraints (which correspond to the complement of the support of A\u0302) in (MNC) can be dropped since they are inactive; in this case (MNC) reduces to a quadratic optimization problem with only equality constraints and whose solution can be seen to be (C) which will thus be non-degenerate as well. The converse is clear. So, under nondegeneracy, (C) can be seen as the limit optimality vector and determines the support of A\u03bb when \u03bb is small.\nTo prove Proposition 4, we first show that the vector z\u03bb coincides with the solution to a dual problem of (iOT\u2212\u21131(\u03c0\u0302)). Proposition 10. Let W \u2217 be the convex conjugate of W . Problem (iOT\u2212\u21131(\u03c0\u0302)) admits a dual given by\nargmin z\nW \u2217(\u03a3\u0302x y \u2212\u03bbz) subject to \u2225z\u2225\u221e \u00c9 1, (14)\nwhere \u03a3\u0302x y =\u03a6\u2217\u03c0\u0302 . Moreover, a pair of primal-dual solutions (A\u03bb, z\u03bb) is related by z\u03bb =\u2212 1\u03bb\u2207AL (A\u03bb, \u03c0\u0302) and z\u03bb \u2208 \u2202\u2225A\u03bb\u22251. (15)\nProof. Observe that we can write L (A, \u03c0\u0302) as L (A, \u03c0\u0302) =W (A)\u2212 \u222b X\u00d7Y (\u03a6A)(x, y)d \u03c0\u0302=W (A)\u2212\u2329\u03a6\u2217\u03c0\u0302, A\u232aF =W (A)\u2212\u2329A, \u03a3\u0302x y \u232a.\nThe Fenchel Duality Theorem (see e.g. Borwein & Lewis (2006)) yields a dual of (iOT\u2212\u21131(\u03c0\u0302)) given by\nargmin w\nW \u2217(\u03a3\u0302x y \u2212w)+ (\u03bb\u2225 \u00b7\u22251)\u2217(w).\nTo conclude the proof just note that the Fenchel conjugate of \u03bb\u2225 \u00b7 \u22251 is the indicator of the set {v : \u2225v\u2225\u221e \u00c9 \u03bb} and make a change of variable z \u225c 1/\u03bbw to obtain 14. The relationship between any primal-dual pair in Fenchel Duality can also be found in Borwein & Lewis (2006).\nProof of Proposition 4. We begin by noting that W \u2217 is of class C 2 in a neighborhood of \u03a3x y and that\n\u22072W \u2217(\u03a3\u0302x y ) = ( \u22072W (A\u0302) )\u22121 . (16)\nTo see this, note that Proposition 2 together with the assumption on \u03c0\u0302 implies that\n\u03a3\u0302x y =\u2207W (A\u0302).\nMoreover, since W (A) is twice continuously differentiable and strictly convex (see Proposition 2), it follows (see e.g. Corollary 4.2.9 in Hiriart-Urruty & Lemar\u00e9chal (1993)) that W \u2217(\u00b7) is C 2 and strictly convex in a neighborhood of \u03a3\u0302x y and that (16) holds.\nNow observe that, since \u2207W \u2217(\u03a3\u0302x y ) =\u2207W \u2217 (\u2207W (A\u0302))= A\u0302, we can rewrite \u2202\u2225\u2225A\u0302\u2225\u22251 as\n\u2202 \u2225\u2225A\u0302\u2225\u22251 = argmin\nz \u2329\u2212 z,\u2207W \u2217(\u03a3\u0302x y )\u232a subject to \u2225z\u2225\u221e \u00c9 1 (17) Observe that since zambd a are uniformly bounded vectors due to the constraint set in 14, there is a convergent subsequence converging to some z\u2217. We later deduce that all limit points are the same and hence, the full sequence z\u03bb converges to z\u2217. Let \u03bbn be such that lim\u03bbn\u21920 z\n\u03bbn = z\u2217, and let z0 be any element in \u2202 \u2225\u2225A\u0302\u2225\u22251. We have that\u2329\u2212 z0,\u2207W \u2217(\u03a3\u0302x y )\u232a\u00c9 \u2329\u2212 z\u03bbn ,\u2207W \u2217(\u03a3\u0302x y )\u232a\u00c9 1 \u03bbn ( W \u2217 ( \u03a3\u0302x y \u2212\u03bbn z\u03bbn\n)\u2212W (\u03a3\u0302x y )) \u00c9 1 \u03bbn ( W \u2217 ( \u03a3\u0302x y \u2212\u03bbn z0\n)\u2212W (\u03a3\u0302x y )), where the first inequality is the optimality of z0, the second inequality is the gradient inequality of convex functions and the last inequality follows from the optimality of z\u03bb. Taking the limit as \u03bbn \u2192 0 we obtain that \u2329\u2212 z0,\u2207W \u2217(\u03a3\u0302x y )\u232a= \u2329\u2212 z\u2217,\u2207W \u2217(\u03a3\u0302x y )\u232a, showing that z\u2217 \u2208 \u2202\u2225\u2225A\u0302\u2225\u22251. We now finish the proof by showing that\u2329\nz\u2217,\u22072W \u2217(\u03a3\u0302x y )z\u2217 \u232a \u00c9 \u2329 z0,\u22072W \u2217(\u03a3\u0302x y )z0 \u232a . (18)\nSince W \u2217(\u00b7) is C 2 in a neighborhood of \u03a3\u0302x y , Taylor\u2019s theorem ensures that there exists a remainder function R(x) with limx\u2192\u03a3\u0302x y R(x) = 0 such that\n\u2329\u2212z\u03bbn ,\u2207W \u2217(\u03a3\u0302x y )\u232a+ \u03bbn 2\n\u2329 z\u03bbn ,\u22072W \u2217(\u03a3\u0302x y )z\u03bbn \u232a+R(\u03a3\u0302x y +\u03bbn z\u03bbn )\u03bb2n = 1 \u03bbn ( W \u2217 ( \u03a3\u0302x y \u2212\u03bbn z\u03bbn )\u2212W \u2217(\u03a3\u0302x y ))\u00c9 1 \u03bbn ( W \u2217 ( \u03a3\u0302x y \u2212\u03bbn z0\n)\u2212W \u2217(\u03a3\u0302x y )) =\u2329\u2212z0,\u2207W \u2217(\u03a3\u0302x y )\u232a+ \u03bbn\n2\n\u2329 z0,\u22072W \u2217(\u03a3\u0302x y )z0 \u232a+R(\u03a3\u0302x y +\u03bbn z0)\u03bb2n \u00c9\u2329\u2212z\u03bbn ,\u2207W \u2217(\u03a3\u0302x y )\u232a+ \u03bbn\n2\n\u2329 z0,\u22072W \u2217(\u03a3\u0302x y )z0 \u232a+R(\u03a3\u0302x y +\u03bbn z0)\u03bb2n , where we used the optimality of z0 and of z\u03bbn . We conclude that\n1\n2\n\u2329 z\u03bbn ,\u22072W \u2217(\u03a3\u0302x y )z\u03bbn \u232a+R(\u03a3\u0302x y +\u03bbn z\u03bbn )\u03bbn \u00c9 1 2 \u2329 z0,\u22072W \u2217(\u03a3\u0302x y )z0) \u232a+R(\u03a3\u0302x y +\u03bbn z0)\u03bbn . Taking the limit establishes 18. Since z0 was an arbitrary element in \u2202\n\u2225\u2225A\u0302\u2225\u22251, we obtain that the limit of z\u03bbn is\nz\u2217 = argmin z\n\u2329 z, ( \u22072W (A\u0302) )\u22121 z \u232a subject to z \u2208 \u2202\u2225\u2225A\u0302\u2225\u22251 where we used 16. Finally, observe that z\u2217 was an arbitrary limit point of z\u03bb and we showed that all limit points are the same; this is enough to conclude the result."
        },
        {
            "heading": "D PROOF OF PROPOSITION 6",
            "text": "The proof of this statement relies on strong convexity of Jn . Similar results have been proven in the context of entropic optimal transport (e.g. Genevay et al. (2019); Mena & Niles-Weed (2019)). Our proof is similar to the approach taken in Rigollet & Stromme (2022).\nLet (A\u221e, f\u221e, g\u221e) minimize (K\u221e). Note that p\u221e\u03b1\u2297\u03b2 with p\u221e(x, y) = exp ( 2\n\u03b5\n( \u03a6A\u221e(x, y)+ f\u221e(x)+ g\u221e(y) )) minimizes (iOT\u2212\u21131(\u03c0\u0302)). Let\nP\u221e = 1 n2 (p\u221e(xi , y j ))i , j , F\u221e = ( f\u221e(xi ))i and G\u221e = (g\u221e(y j )) j .\nNote that by optimality of A\u221e, \u2225A\u221e\u22251 \u00c9 \u2225\u2225A\u0302\u2225\u22251 \u2013 this can be seen by comparing the objective (iOT\u2212\u21131(\u03c0\u0302)) at A\u221e and A\u0302. Moreover, due to the uniform bounds on f\u221e, g\u221e from Lemma 12, p\u221e is uniformly bounded away from 0 by exp(\u2212C /\u03bb) for some constant C that depends on \u03c0\u0302. Let Pn minimise (Pn), we know it is of the form\nPn = 1 n2 exp\n( 2\n\u03b5 (\u03a6n An +Fn \u2295Gn)\n) .\nfor vectors An ,Fn ,Gn . The \u2018certificates\u2019 are\u03a6\u2217\u03d5\u221e and zn \u225c\u03a6\u2217n\u03d5n where\n\u03d5\u221e = 1 \u03bb\n( p\u221e\u03b1\u2297\u03b2\u2212 \u03c0\u0302 ) and \u03d5n = 1\n\u03bb\n( Pn \u2212 P\u0302n ) .\nNote that zn =\u03a6\u2217\u03d5\u221e =\u2212 1\u03bb\u2207AL (A\u221e, \u03c0\u0302). The goal is to bound \u2225\u2225\u03a6\u2217\u03d5\u221e\u2212\u03a6\u2217n\u03d5n\u2225\u2225\u221e so that nondegeneracy of z\u221e would imply nondegeneracy of\u03a6\u2217n\u03d5n . Note that\u03a6\u2217nPn =\u03a6\u2217\u03c0\u0302n . So, by the triangle inequality,\u2225\u2225\u03a6\u2217\u03d5\u221e\u2212\u03a6\u2217n\u03d5n\u2225\u2225\u221e \u00c9 1\u03bb \u2225\u2225\u03a6\u2217 (p\u221e\u03b1\u2297\u03b2)\u2212\u03a6\u2217nPn\u2225\u2225\u221e+ 1\u03bb \u2225\u2225\u03a6\u2217 (\u03c0\u0302n \u2212 \u03c0\u0302)\u2225\u2225\u221e\n\u00c9 1 \u03bb \u2225\u2225\u03a6\u2217nP\u221e\u2212\u03a6\u2217nPn\u2225\u2225\u221e+ 1\u03bb \u2225\u2225\u03a6\u2217 (p\u221e\u03b1\u2297\u03b2)\u2212\u03a6\u2217nP\u221e\u2225\u2225\u221e+ 1\u03bb \u2225\u2225\u03a6\u2217 (\u03c0\u0302n \u2212 \u03c0\u0302)\u2225\u2225\u221e The last two terms on the RHS can be controlled using Proposition 20, and are bounded by O (tn\u22121/2) with probability at least 1\u2212O (exp(\u2212t 2)) for t > 0. For the first term on the RHS, letting Z = P\u221e\u2212Pn ,\u2225\u2225\u03a6\u2217n Z\u2225\u2225\u221e = \u2225\u2225\u2225\u2225\u2225 n\u2211 i , j=1 C(xi , y j )Zi , j \u2225\u2225\u2225\u2225\u2225 \u221e\n\u00c9 \u2225C\u2225\u221e \u221a\u221a\u221a\u221a 1\nn2 \u2211 i , j ( exp ( 2 \u03b5 (\u03a6n An +Fn \u2295Gn) ) \u2212exp ( 2 \u03b5 (\u03a6n A\u221e+F\u221e\u2295G\u221e) ))2 i , j\nLet L \u225c 2(\u2225\u03a6n An +Fn \u2295Gn)\u2225\u221e\u2228\u2225\u03a6n A\u221e+F\u221e\u2295G\u221e)\u2225\u221e). According to the Lipschitz continuity of the exponential\n1 n2 \u2211 i , j ( exp ( 2 \u03b5 (\u03a6n An +Fn \u2295Gn) ) \u2212exp ( 2 \u03b5 (\u03a6n A\u221e+F\u221e\u2295G\u221e) ))2 i , j\n(19)\n\u00c9 4exp(L/\u03b5) \u03b52n2 \u2211 i , j ((\u03a6n An +Fn \u2295Gn)\u2212 (\u03a6n A\u221e+F\u221e\u2295G\u221e))2i , j (20)\n\u00c9 12exp(L/\u03b5) \u03b52\n( 1\nn2 \u2211 i , j (\u03a6n An \u2212\u03a6n A\u221e)2i , j + 1 n \u2211 i (Fn \u2212F\u221e)2i + 1 n \u2211 j (Gn \u2212G\u221e)2j )\n(21)\nBy strong convexity properties of Jn and hence Kn , it can be shown (see Prop 14) that (21) is upper bounded up to a constant by\n\u03b5\u22121 exp(L/\u03b5) (Kn(F\u221e,G\u221e, A\u221e)\u2212Kn(Fn ,Gn , An))\n\u00c9exp(L/\u03b5) 4\n(\u2225\u2225n\u22122(\u03a6\u2217n\u03a6n)\u22121\u2225\u2225\u2225\u2225(\u2202AJn(A\u221e,F\u221e,G\u221e)+\u03bb\u03be\u221e)\u2225\u22252\n+n \u2225\u2225\u2202F Jn(A\u221e,F\u221e,G\u221e)\u2225\u22252 +n \u2225\u2225\u2202GJn(A\u221e,F\u221e,G\u221e)\u2225\u22252 ) where \u03be\u221e = 1\u03bb (\u03a6\u2217\u03c0\u0302\u2212\u03a6\u2217(p\u221e\u03b1\u2297\u03b2)) \u2208 \u2202\u2225A\u221e\u22251. By Lemma 11 and Lemma 12, L = O (\n\u2225\u2225A\u0302\u2225\u22251) with probability at least 1\u2212O (exp(\u2212t 2)) if \u03bb\u2273 tn\u2212 12 . Finally,\u2225\u2225(\u2202AJn(A\u221e,F\u221e,G\u221e)+\u03bb\u03be\u221e)\u2225\u22252 = \u2225\u2225\u03a6\u2217n P\u0302 +\u03a6\u2217nP\u221e+\u03bb\u03be\u221e\u2225\u22252\n\u00c9 2 (\u2225\u2225\u03a6\u2217n P\u0302 \u2212\u03a6\u2217\u03c0\u0302\u2225\u22252 +\u2225\u2225\u03a6\u2217nP\u221e\u2212\u03a6\u2217(p\u221e\u03b1\u2297\u03b2)\u2225\u22252)\nn \u2225\u2225\u2202F Jn(A\u221e,F\u221e,G\u221e)\u2225\u222522 = 1n n\u2211i=1\n( 1\u2212 1\nn n\u2211 j=1 p\u221e(xi , y j )\n)2\nn \u2225\u2225\u2202GJn(A\u221e,F\u221e,G\u221e)\u2225\u222522 )= 1n n\u2211j=1 ( 1\u2212 1 n n\u2211 i=1 p\u221e(xi , y j ) )2 ) .\nWe show in Propositions 16, 19 and 20 that these are bounded by O (t 2n\u22121) with probability at least 1\u2212O (exp(\u2212t 2)) and from Proposition 23, assuming n \u2273 log(2s), \u2225\u2225(n\u22122\u03a6\u2217n\u03a6n)\u22121\u2225\u2225 \u2272 1 with probability at least 1\u2212O (exp(\u2212n)). So, for some constant C > 0, \u03bb\u2225\u2225\u03a6\u2217\u03d5\u221e\u2212\u03a6\u2217n\u03d5n\u2225\u2225\u221e \u2272 exp(C \u2225\u2225A\u0302\u2225\u22251 /\u03b5) tpn with probability at least 1\u2212exp(\u2212t 2). The second statement follows by combining our bound for (21) with the fact that\u2225\u2225(n\u22122\u03a6\u2217n\u03a6n)\u22121\u2225\u2225\u2272 1. In the following subsections, we complete the proof by establishing the required strong convexity properties of Jn and bound \u2207Jn at (A\u221e,F\u221e,G\u221e) using concentration inequalities. The proofs to some of these results are verbatim to the results of Rigollet & Stromme (2022) for deriving sampling complexity bounds in eOT, although they are included due to the difference in our setup.\nD.1 STRONG CONVEXITY PROPERTIES OF Jn\nIn this section, we present some of the key properties of Jn . Recall that since we assume that \u03b1,\u03b2 are compactly supported, up to a rescaling of the space, we assume without loss of generality that for all k, \u2223\u2223Ck (x, y)\u2223\u2223\u00c9 1. Lemma 11. Let An minimize (iOT\u2212\u21131(\u03c0\u0302n)). Assume that for some constant C1 > 0,\nt exp(C1/\u03b5)n \u2212 12 \u2272\u03bbmin ( 1, \u2225\u2225A\u0302\u2225\u22251) , then with probability at least 1\u2212O (exp(\u2212t 2)),\n\u2225An\u22251 \u00c9 2 \u2225\u2225A\u0302\u2225\u2225\nProof. Let A be a minimizer to (iOT\u2212\u21131(\u03c0\u0302n)). By optimality of A, \u03bb\u2225A\u2225+L (A, \u03c0\u0302n) \u00c9 \u03bb \u2225\u2225A\u0302\u2225\u22251 +\nL (A\u0302, \u03c0\u0302n). Writing P\u0302n = 1n Id and\u03a6n A = (\u2211s k=1 Ak Ck (xi , y j ) )n i , j=1,\n\u03bb\u2225A\u22251 \u2212\u2329P\u0302n ,\u03a6n A\u232a+\u2329P,\u03a6n A\u232a\u2212 \u03b5 2 H(P ) \u00c9\u03bb\u2225\u2225A\u0302\u2225\u22251 +L (A\u0302, \u03c0\u0302n) (22)\nfor all P satisfying the marginal constraints P1= 1n1 and P\u22a41= 1n1. Note that since L (A\u0302, \u03c0\u0302)+KL(\u03c0\u0302|\u03b1\u2297\u03b2) = 0,\nL (A\u0302, \u03c0\u0302n) =L (A\u0302, \u03c0\u0302n)\u2212L (A\u0302, \u03c0\u0302)\u2212KL(\u03c0\u0302|\u03b1\u2297\u03b2) (23) =\u2212\u2329\u03a6A\u0302, \u03c0\u0302\u2212 \u03c0\u0302n\u232a+W\u03c0\u0302n (A\u0302)\u2212W\u03c0\u0302(A\u0302)\u2212KL(\u03c0\u0302|\u03b1\u2297\u03b2). (24)\nThe first two terms can be shown to be O (n\u2212 1 2 ): Indeed,\n\u2329\u03a6A\u0302, \u03c0\u0302\u2212 \u03c0\u0302n\u232a = 1 n n\u2211 i=1 Zi , where Zi \u225c (\u2329A\u0302, C(xi , yi )\u232a\u2212E[\u2329A\u0302, C(xi , yi )]\u232a)\nis the sum of n terms with mean zero. Moreover, |Zi | \u00c9 2 \u2225\u2225A\u0302\u2225\u2225. By Lemma 18, \u2329\u03a6A\u0302, \u03c0\u0302\u2212 \u03c0\u0302n\u232a \u00c9 8\u2225A\u0302\u22252tpn with probability at least 1\u22122exp(\u2212t 2).\nThe bound W\u03c0\u0302n (A\u0302)\u2212W\u03c0\u0302(A\u0302) =O (exp(\u2212C /\u03b5)tn\u2212 1 2 ) with probability 1\u2212O (exp(t 2)) is a due to the sample complexity of eOT Rigollet & Stromme (2022). Plugging this back into (22), we obtain\n\u03bb\u2225A\u22251 \u00c9\u03bb \u2225\u2225A\u0302\u2225\u22251 +O (n\u2212 12 )+ \u03b52 (H(P )\u2212KL(\u03c0\u0302|\u03b1\u2297\u03b2))\ufe38 \ufe37\ufe37 \ufe38\nT1\n+(\u2329P\u0302n ,\u03a6n A\u232a+\u2329P,\u03a6n A\u232a)\ufe38 \ufe37\ufe37 \ufe38 T2\n(25)\nIt remains to bound the terms T1 and T2. Intuitively, if \u03c0\u0302= p\u0302\u03b1\u2297\u03b2 has density p\u0302, then we can show that T1,T2 =O (n\u2212 12 ) by choosing P =Qn \u225c 1n2 (p\u0302(xi , y j ))ni , j=1. However, Qn will only approximately satisfy the marginal constraints Qn1\u2248 1n1 and Q\u22a4n 1\u2248 1n1 (this approximation can be made precise using Proposition 16). So, we insert into the above inequality P = Q\u0303n with Q\u0303n being the projection of Qn onto the constraint set U ( 1 n1n , 1 n1n).\nBy (Altschuler et al., 2017, Lemma 7), the projection Q\u0303n satisfies\u2225\u2225Q\u0303n \u2212Qn\u2225\u22251 \u00c9 2\u2225\u2225\u2225\u2225Qn1\u2212 1n1 \u2225\u2225\u2225\u2225 1 +2 \u2225\u2225\u2225\u2225Q\u22a4n 1\u2212 1n1 \u2225\u2225\u2225\u2225 1 . (26)\nBy Proposition 16, with probability at least 1\u2212O (exp(\u2212t 2)), \u2225\u2225\u2225\u2225Qn1\u2212 1n1 \u2225\u2225\u2225\u2225 1 = 1 n n\u2211 i=1 \u2223\u2223\u2223\u2223\u2223 1n n\u2211j=1 p\u0302(xi , y j )\u22121 \u2223\u2223\u2223\u2223\u2223\u00c9 \u221a\u221a\u221a\u221a 1 n n\u2211 i=1 \u2223\u2223\u2223\u2223\u2223 1n n\u2211j=1 p\u0302(xi , y j )\u22121 \u2223\u2223\u2223\u2223\u2223 2 =O ( tp n ) .\nSimilarly, \u2225\u2225Qn1\u2212 1n1\u2225\u22251 =O (n\u2212 12 ) with high probability. Note that \u2223\u2223\u2329Q\u0303n \u2212Qn ,\u03a6n A\u232a\u2223\u2223= \u2223\u2223\u2223\u2223\u2223 s\u2211 k=1 n\u2211 i , j=1 Ak (Ck )i , j (Q\u0303n \u2212Qn)i , j \u2223\u2223\u2223\u2223\u2223 (27)\n\u00c9 max k\n\u2225Ck\u2225\u221e \u2225A\u22251 \u2225\u2225Qn \u2212Q\u0303n\u2225\u22251 (28)\nMoreover,\n\u2329P\u0302n \u2212Qn ,\u03a6n A\u232a \u00c9 \u2225A\u2225 \u2225\u2225\u03a6\u2217n(P\u0302n \u2212Qn)\u2225\u2225 (29)\nBy Proposition 19 and 20, with probability at least 1\u2212O (exp(\u2212t 2)),\nE \u2225\u2225\u03a6\u2217n P\u0302n \u2212\u03a6\u2217\u03c0\u0302\u2225\u2225=O (tn\u2212 12 ) and E\u2225\u2225\u03a6\u2217nQn \u2212\u03a6\u2217\u03c0\u0302\u2225\u2225=O (tn\u2212 12 ).\nSo, we have T2 \u00c9C n\u2212 12 \u2225A\u22251. We now consider the term T1 in (25). Since p\u0302 is uniformly bounded from above by exp(C /\u03b5) and from below with constant exp(\u2212C /\u03b5) for some C > 0, one can check from the projection procedure of Altschuler et al. (2017) that Q\u0303n is also uniformly bounded from above by\n1 n2 exp(C /\u03b5) and away\nfrom zero by 1 n2 exp(\u2212C /\u03b5) for some C > 0. So, |T1| \u00c9 \u2223\u2223H(Q\u0303n)\u2212H(Qn)\u2223\u2223+ \u2223\u2223H(Qn)\u2212KL(\u03c0\u0302|\u03b1\u2297\u03b2)\u2223\u2223 (30) \u2272 eC /\u03b5\n\u2225\u2225Q\u0303n \u2212Qn\u2225\u22251 + \u2223\u2223H(Qn)\u2212KL(\u03c0\u0302|\u03b1\u2297\u03b2)\u2223\u2223 (31) We can use the bound (26) to see that\n\u2225\u2225Q\u0303n \u2212Qn\u2225\u22251 \u2272 t/pn with probability at least 1\u2212exp(\u2212t 2). To bound T3 \u225c H(Qn)\u2212KL(\u03c0\u0302|\u03b1\u2297\u03b2), note that Moreover,\nE[T3] = E ( 1\nn2 n\u2211 i=1 n\u2211 j=1 log(p\u0302(xi , y j ))p\u0302(xi , y j )\n) \u2212 \u222b log(p\u0302(x, y))p\u0302(x, y)d\u03b1(x)d\u03b2(y)\n= (\nn(n \u22121) n2\n\u22121 )\u222b\nlog(p\u0302(x, y))p\u0302(x, y)d\u03b1(x)d\u03b2(y)+ 1 n2 n\u2211 i=1 E ( log(p\u0302(xi , yi ))p\u0302(xi , yi ) )\n\u00c9 \u22121 n2\n\u222b log(p\u0302(x, y))p\u0302(x, y)d\u03b1(x)d\u03b2(y)+ 1\nn2\n\u222b log(p\u0302(x, y))p\u0302(x, y)2d\u03b1(x)d\u03b2(y)\n\u00c9 1 n2 (\u2225\u2225p\u0302\u2225\u2225\u221e\u22121)\u222b log(p\u0302(x, y))d \u03c0\u0302(x, y) Therefore, by the bounded differences lemma 15 with\nf (z1, . . . , zn) = 1 n2 n\u2211 i=1 n\u2211 j=1 log(p\u0302(xi , y j ))p\u0302(xi , y j )\u2212 \u222b log(p\u0302(x, y))p\u0302(x, y)d\u03b1(x)d\u03b2(y)\nwhere zi = (xi , yi ). Then, letting z j = z \u2032j for all j \u0338= i , the bounded differences property is satisfied with\nf (z1, . . . , zn)\u2212 f (z \u20321, . . . , z \u2032n)\n= 1 n2  n\u2211 j=1 j \u0338=i + n\u2211 j=1 (log(p\u0302(xi , y j ))p\u0302(xi , y j )\u2212 log(p\u0302(x \u2032i , y j ))p\u0302(x \u2032i , y j ))\n\u00c9 4( \u2225\u2225p\u0302\u2225\u2225\u221e+1)2)\nn \u225c c.\nSo, with probability at least 1\u22122exp(\u2212t 2), |T3| \u00c9 1n2 + 2t (\u2225p\u0302\u2225\u221e+1)pn . In summary, with probability 1\u2212O (exp(\u2212t 2)),(\n\u03bb\u2212C2t exp(C1/\u03b5)n\u2212 1 2 ) \u2225A\u22251 \u00c9\u03bb \u2225\u2225A\u0302\u2225\u22251 + C2(exp(C1/\u03b5)tpn for some constants C1,C2 > 0. So, choosing\nC2t exp(C1/\u03b5)n \u2212 12 \u00c9 min(\u03bb/4,\u03bb\u2225\u2225A\u0302\u2225\u22251 /2) ,\nwe have \u2225A\u22251 \u00c9 2 \u2225\u2225A\u0302\u2225\u2225 with probability at least 1\u2212exp(\u2212t 2).\nLemma 12. Let (A,F,G) minimize (Kn). Let C A =\u03a6n A. Then, Fi \u2208 [\u22123\u2225C A\u2225\u221e ,\u2225C A\u2225\u221e] and Gi \u2208 [\u22122\u2225C A\u2225\u221e ,2\u2225C A\u2225\u221e].\nMoreover, exp(4\u2225C A\u2225\u221e \u03b5\u22121) \u00ca exp ( 2 \u03b5 ( Fi +G j + (\u03a6n A)i , j )) \u00ca exp(\u22126\u2225C A\u2225\u221e \u03b5\u22121). Note that \u2225C A\u2225\u221e \u00c9 \u2225A\u22251. If (A, f , g ) minimize (K\u221e). Let cA =\u03a6A. Then, for all x, y\nf (x) \u2208 [\u22123\u2225cA\u2225\u221e ,\u2225cA\u2225\u221e] and g (y) \u2208 [\u22122\u2225cA\u2225\u221e ,2\u2225cA\u2225\u221e]. Moreover, exp(4\u2225cA\u2225\u221e \u03b5\u22121) \u00ca exp ( 2 \u03b5 ( f \u2295+g + (\u03a6A)i , j ))\u00ca exp(\u22126\u2225cA\u2225\u221e \u03b5\u22121). Proof. This proof is nearly identical to (Rigollet & Stromme, 2022, Prop. 10): Let A,F,G minimize (Kn).\nBy the marginal constraints for Pn \u225c 1n2 ( exp ( 2 \u03b5 (F \u2295G +\u03a6n A) )) i , j given in (Pn),\n1 = 1 n \u2211 j exp\n( 2\n\u03b5 (\u03a6A(xi , y j )+Fi +G j ) ) \u00ca exp ( 2\n\u03b5 (\u2212\u2225C A\u2225\u221e+Fi ) )\u2211 j 1 n exp ( 2G j /\u03b5 )\u00ca exp(2 \u03b5 (\u2212\u2225C A\u2225\u221e+Fi ) ) (32)\nwhere we use Jensen\u2019s inequality and the assumption that \u2211\nj G j = 0 for the second. So, Fi \u00c9 \u2225C A\u2225\u221e for all i \u2208 [n]. Using the other marginal constraint for Pn along with this bound on Fi implies that\n1 = 1 n \u2211 i exp\n( 2\n\u03b5 (\u03a6A(xi , y j )+Fi +G j )\n) \u00c9 exp ( 2\n\u03b5\n( 2\u2225C A\u2225\u221e+G j ) )) So, Gn, j \u00ca\u22122\u2225C A\u2225\u221e. To prove the reverse bounds, we now show that \u2211 i Fi is lower bounded: Note that since H (P ) \u00ca\u22121, by duality between (Pn) and (Kn),\n\u2225C A\u2225\u221e+ \u03b5 2 \u00ca \u2329\u03a6n A, P\u232a\u2212 \u03b5 2 H(P ) =\u2212 1 n \u2211 i Fi \u2212 1 n \u2211 j G j + \u03b5 2n2 \u2211 i , j exp\n( 2\n\u03b5 (Fi +G j + (\u03a6n A)i j ) By assumption, \u2211 j G j = 0 and \u2211 i , j exp ( 2 \u03b5 (Fi +G j + (\u03a6n A)i j\n)= n2. So, 1\nn \u2211 i Fi \u00ca\u2212\u2225C A\u2225\u221e .\nBy repeating the argument in (32), we see that 1 \u00ca exp(2/\u03b5(\u2212\u2225C A\u2225\u221e+G j ))exp ( 2\nn\u03b5 \u2211 i Fi\n) \u00ca exp(2/\u03b5(\u22122\u2225C A\u2225\u221e+G j )).\nSo, G j \u00c9 2\u2225C A\u2225\u221e and Fi \u00ca\u22123\u2225C A\u2225\u221e. The proof for (K\u221e) is nearly identical and hence omitted.\nSimilarly to (Rigollet & Stromme, 2022, Lemma 11), we derive the following strong convexity bound for Jn :\nLemma 13. The functional Jn is strongly convex with\nJn(A \u2032,F \u2032,G \u2032) \u00caJn(A,F,G)+\u2329\u2207Jn(A,F,G), (A\u2032,F \u2032,G \u2032)\u2212 (A,F,G)\u232a\n+ exp(\u2212L/\u03b5) \u03b5\n( 1\nn2 \u2225\u2225\u03a6n(A\u2212 A\u2032)\u2225\u222522 + 1n \u2225\u2225F \u2212F \u2032\u2225\u222522 + 1n \u2225\u2225G \u2212G \u2032\u2225\u222522 ) , (33)\nfor some L =O (\u2225A\u22251 \u2228 \u2225\u2225A\u2032\u2225\u22251).\nProof. To establish the strong convexity inequality, let\nh(t )\u225cJ ((1\u2212 t )A+ t A\u2032, (1\u2212 t ) f + t f \u2032, (1\u2212 t )g + t g \u2032). It suffices to find \u03b4> 0 such that for all t \u2208 [0,1],\nh\u2032\u2032(t ) \u00ca \u03b4 ( 1 n2 \u2225\u2225\u03a6n(A\u2212 A\u2032)\u2225\u222522 + 1n \u2225\u2225F \u2212F \u2032\u2225\u222522 + 1n \u2225\u2225G \u2212G \u2032\u2225\u222522 ) . (34)\nLet Zt \u225c ((1\u2212 t )F + tF \u2032)\u2295 ((1\u2212 t )G + tG \u2032)+ ((1\u2212 t )\u03a6A+ t\u03a6A\u2032). Note that\nh\u2032\u2032(t ) = 2 \u03b5n2 \u2211 i , j exp\n( 2\n\u03b5 (Zt )i , j\n) (F \u2032i \u2212Fi +G \u2032i \u2212Gi + (\u03a6A\u2032)i , j \u2212 (\u03a6A)i , j )2 (35)\nSince \u2225cA\u2225\u221e\u2228\u2225cA\u2032\u2225\u221e \u00c9 L, by Lemma 12, \u2225Zt\u2225\u221e \u2272 \u2225cA\u2225\u221e\u2228\u2225cA\u2032\u2225\u221e. So,\nh\u2032\u2032(t ) \u00ca 2 \u03b5n2 exp(\u2212L/\u03b5)\u2211 i , j (F \u2032i \u2212Fi +G \u2032i \u2212Gi + (\u03a6A\u2032)i , j \u2212 (\u03a6A)i , j )2.\nBy expanding out the brackets and using \u2211 i Gi = 0 and since Ck are centred ( \u2211\ni (Ck )i , j = 0 and\u2211 j (Ck )i , j = 0), (34) holds with \u03b4= 2\u03b5 exp(\u2212L/\u03b5).\nBased on this strong convexity result, we have the following bound.\nProposition 14. Let (An ,Fn ,Gn) minimise Fn , then for all (A,F,G) \u2208S such that n\u22122 exp(F \u2295G + \u03a6n A) satisfy the marginal constraints of (Pn),\nKn(A,F,G)\u2212Kn(An ,Fn ,Gn) \u00ca exp(\u2212L/\u03b5) \u03b5\n( 1\nn2 \u2225\u03a6n(A\u2212 An)\u222522 +\n1 n \u2225F \u2212Fn\u222522 + 1 n \u2225\u2225G \u2212G \u2032n\u2225\u222522) . and\nKn(A,F,G)\u2212Kn(An ,Fn ,Gn) \u00c9\u03b5exp(L/\u03b5) 4 (\u2225\u2225n\u22122(\u03a6\u2217n\u03a6n)\u22121\u2225\u2225\u2225\u2225(\u2202AJn(A,F,G)+\u03bb\u03be)\u2225\u22252 +n \u2225\u2225\u2202F Jn(A,F,G)\u2225\u222522 +n \u2225\u2225\u2202GJn(A,F,G)\u2225\u222522 ).\nwhere L =O (\u2225A\u22251 \u2228\u2225An\u22251).\nProof. By strong convexity of Jn , we can show that for any (A, f , g ), (A\u2032, f \u2032, g \u2032) \u2208 S and any \u03be \u2208 \u2202\u2225A\u22251,\nKn(A \u2032,F \u2032,G \u2032) \u00caKn(A,F,G)+\u2329\u2207Jn(A,F,G), (A\u2032,F \u2032,G \u2032)\u2212 (A,F,G)\u232a+\u03bb\u2329\u03be, A\u2032\u2212 A\u232a\n+ \u03b4 2\n( 1\nn2 \u2225\u2225\u03a6n(A\u2212 A\u2032)\u2225\u222522 + 1n \u2225\u2225F \u2212F \u2032\u2225\u222522 + 1n \u2225\u2225G \u2212G \u2032\u2225\u222522 ) ,\n(36)\nwhere \u03b4 = 2exp(\u2212L/\u03b5)\u03b5 with L = O (\u2225A\u22251 \u2228 \u2225\u2225A\u2032\u2225\u22251). The first statement follows by letting (A,F,G) = (An ,Fn ,Gn) in the above inequality.\nTo prove the second statement, let\nM \u225c\u2329\u2207Jn(A,F,G), (A\u2032,F \u2032,G \u2032)\u2212 (A,F,G)\u232a+\u03bb\u2329\u03be, A\u2032\u2212 A\u232a\n+ \u03b4 2\n( 1\nn2 \u2225\u2225\u03a6n(A\u2212 A\u2032)\u2225\u222522 + 1n \u2225\u2225F \u2212F \u2032\u2225\u222522 + 1n \u2225\u2225G \u2212G \u2032\u2225\u222522 ) .\nBy minimising over (A\u2032,F \u2032,G \u2032), note that\nM \u00ca\u2212 1 2\u03b4\n( n2 \u2225\u2225(\u2202AJn(A,F,G)+\u03bb\u03be)\u2225\u22252(\u03a6\u2217n\u03a6n )\u22121 +n \u2225\u2225\u2202F Jn(A,F,G)\u2225\u222522 +n \u2225\u2225\u2202GJn(A,F,G)\u2225\u222522) , So,\n\u2212M \u00c9 1 2\u03b4 (\u2225\u2225n\u22122(\u03a6\u2217n\u03a6n)\u22121\u2225\u2225\u2225\u2225(\u2202AJn(A,F,G)+\u03bb\u03be)\u2225\u22252 +n \u2225\u2225\u2202F Jn(A,F,G)\u2225\u222522 +n \u2225\u2225\u2202GJn(A,F,G)\u2225\u222522) Finally, note that Kn(A,F,G) \u2212Kn(A\u2032,F \u2032,G \u2032) \u00ca Kn(A,F,G) \u2212Kn(An ,Fn ,Gn) by optimality of An ,Fn ,Gn ."
        },
        {
            "heading": "D.2 CONCENTRATION BOUNDS",
            "text": "Lemma 15 (McDiarmid\u2019s inequality). Let f : X n \u2192R satisfy the bounded differences property: sup\nx\u2032i\u2208X \u2223\u2223 f (x1, . . . , xi\u22121, xi , xi+1, . . . , xn)\u2212 f (x1, . . . , xi\u22121, x \u2032i , xi+1, . . . , xn)\u2223\u2223\u00c9 c. Then, given X1, . . . , Xn random variables with Xi \u2208X , for any t > 0,\nP( \u2223\u2223 f (X1, . . . , Xn)\u2212E[ f (X1, . . . , Xn)]\u2223\u2223\u00ca t ) \u00c9 2exp(\u22122t 2/(nc2)).\nGiven random vectors X and Y , denote Cov(X ,Y ) = E\u2329Y \u2212E[Y ], X \u2212E[X ]\u232a and Var(X ) = Cov(X , X ). Proposition 16. Let \u03c0 have marginals \u03b1 and \u03b2 and let p = d\u03c0d(\u03b1\u2297\u03b2) . Let (xi , yi ) \u223c \u03c0\u0302 where \u03c0\u0302 has marginals \u03b1,\u03b2. Assume that\n\u2225\u2225p\u2225\u2225\u221e \u00c9 b. Then, E [ 1\nn n\u2211 j=1\n( 1\u2212 1\nn n\u2211 i=1 p(xi , y j )\n)2] \u00c9 (b +1) 2\nn\nand 1\nn n\u2211 j=1\n( 1\u2212 1\nn n\u2211 i=1 p(xi , y j )\n)2 \u00c9 (t +b +1) 2\nn .\nwith probability at least 1\u2212exp(\u2212t 2/(4b2)).\nRemark 17. The bounds also translate to an \u21131 norm on the marginal errors, since by CauchySchwarz,\n1\nn n\u2211 j=1 \u2223\u2223\u2223\u2223\u22231\u2212 1n n\u2211i=1 p(xi , y j ) \u2223\u2223\u2223\u2223\u2223\u00c9 \u221a\u221a\u221a\u221a n\u2211 j=1 1 n ( 1\u2212 1 n n\u2211 i=1 p(xi , y j ) )2 .\nMoreover, by Jensen\u2019s inequality E 1n \u2211n j=1 \u2223\u22231\u2212 1n \u2211ni=1 p(xi , y j )\u2223\u2223\u00c9\u221aE\u2211nj=1 1n (1\u2212 1n \u2211ni=1 p(xi , y j ))2.\nProof.\nE\n[ 1\nn n\u2211 j=1\n( 1\u2212 1\nn n\u2211 i=1 p(xi , y j )\n)2] = 1\nn3 n\u2211 i j ,k=1 E ( (1\u2212p(xi , y j ))(1\u2212p(xk , y j )) ) .\nFor each j \u2208 [n], we have the following cases for u j \u225c E ( (1\u2212p(xi , y j ))(1\u2212p(xk , y j )) ) :\n1. i = k = j , then u j = E(x,y)\u223c\u03c0\u0302(p\u221e(x, y)\u22121)2. There is 1 such term.\n2. i = j and k \u0338= j , then u j = E(x,y)\u223c\u03c0\u0302,z\u223c\u03b1 ( (1\u2212p(x, y))(1\u2212p(z, y)))= 0. There are n \u22121 such\nterms. 3. i \u0338= j and k = j , then u j = E(z,y)\u223c\u03c0\u0302,x\u223c\u03b1 ( (1\u2212p(x, y))(1\u2212p(z, y)))= 0. There are n \u22121 such\nterms.\n4. i = k and i \u0338= j , then u j = Ex\u223c\u03b1,y\u223c\u03b2(1\u2212p(x, y))2 and there are (n \u22121) such terms. 5. i , j ,k all distinct. Then, u j = 0 and there are (n \u22121)(n \u22122) such terms.\nTherefore,\n1\nn3 n\u2211 i j ,k=1 E ( (1\u2212p(xi , y j ))(1\u2212p(xk , y j )) )= 1 n2 E(x,y)\u223c\u03c0\u0302(p(x, y)\u22121)2 + n \u22121 n2 Ex\u223c\u03b1,y\u223c\u03b2(1\u2212p(x, y))2\nUsing \u2223\u22231\u2212p(x, y)\u2223\u2223\u00c9 b +1 gives the first inequality. Note also that letting V = 1n \u2211n i=1 ( (1\u2212p(xi , y j )) ) j , \u2225V \u22252 = \u2211n j=1 ( 1 n \u2211n i=1(1\u2212p(xi , y j )) )2 . We will\napply Lemma 15 to f (z1, . . . , zn) = \u2225V \u2225. Let V \u2032 = 1n \u2211n i=1 ( (1\u2212p(x \u2032i , y \u2032j )) ) j\nwhere xi , y j = x \u2032j , y \u2032j for i , j \u00ca 2. Then, for all vectors u of norm 1,\n\u2329u, V \u2032\u2212V \u232a = n\u2211\nj=1 u j\n1\nn n\u2211 i=1 (p(x \u2032i , y \u2032 j )\u2212p(xi , y j ))\n= u1 1 n n\u2211 i=1 (p(x \u2032i , y \u2032 1)\u2212p(xi , y1))+ n\u2211 j=2 u j 1 n (p(x \u20321, y \u2032 j )\u2212p(x1, y j ))\n\u00c9 2b n n\u2211 j=1 u j \u00c9 2bp n .\nSo, by the reverse triangle inequality, \u2223\u2223\u2225V \u2225\u2212\u2225\u2225V \u2032\u2225\u2225\u2223\u2223\u00c9 2bp\nn . It follows that\nn\u22121/2 \u2225V \u2225 \u00c9 n\u22121/2t +n\u22121/2E\u2225V \u2225 \u00c9 n\u22121/2t + \u221a n\u22121E\u2225V \u22252 \u00c9 n\u22121/2(t +b +1).\nwith probability at least 1\u2212exp(\u2212t 2/(4b2)) as required."
        },
        {
            "heading": "D.2.1 BOUNDS FOR THE COST PARAMETRIZATION",
            "text": "In the following two propositions (Prop 20 and 19), we assume that \u03a6n is defined with Ck = (Ck (xi , y j ))i , j and discuss in the remark afterward how to account for the fact that our cost Ck in (Pn) are centered.\nNote that the following classical result is a direct consequence of Lemma 15\nLemma 18. Suppose Z1, . . . , Zm are independent mean zero random variables taking values in Hilbert space H . Suppose there is some C > 0 such that for all k, \u2225Zk\u2225 \u00c9C . Then, for all t > 0, with probability at least 1\u22122exp(\u2212t ), \u2225\u2225\u2225\u2225\u2225 1m m\u2211k=1 Zk \u2225\u2225\u2225\u2225\u2225 2 \u00c9 8C 2t m .\nProposition 19. Let C = maxx,y \u2225\u2225C(x, y)\u2225\u2225. Let (xi , yi )ni=1 be iid drawn from \u03c0\u0302. Then,\u2225\u2225\u03a6\u2217n P\u0302 \u2212\u03a6\u2217\u03c0\u0302\u2225\u2225\u00c9 p8C tn\nwith probability at least 1\u22122exp(\u2212t 2).\nProof. Direct consequence of Lemma 18.\nProposition 20. Let t > 0. Let (xi , yi )ni=1 be i.i.d. drawn from \u03c0\u0302, which has marginals \u03b1,\u03b2. Let P = 1\nn2 (p(xi , y j ))ni , j=1 where \u03c0 has marginals \u03b1,\u03b2 and p = d\u03c0d(\u03b1\u2297\u03b2) . Then, E \u2225\u2225\u03a6\u2217nP \u2212\u03a6\u2217(p\u03b1\u2297\u03b2)\u2225\u22252 = O (n\u22121) and \u2225\u2225\u03a6\u2217nP \u2212\u03a6\u2217p\u03b1\u2297\u03b2\u2225\u2225\u2272 1+ tpn with probability at least 1\u22122exp(\u22122t 2/(64\u2225\u2225p\u2225\u22252\u221e)) Proof. Let h(x, y)\u225c p(x, y)C(x, y), then\n\u03a6\u2217nP = 1\nn2 n\u2211 j=1 h(x j , y j )+ n\u2211 i=1 i \u0338= j h(xi , y j )  and\nE[\u03a6\u2217nP \u2212\u03a6\u2217p\u03b1\u2297\u03b2] = 1 n A\u2217\n( p\u03c0\u0302\u2212p\u03b1\u2297\u03b2)\nIt follows that\nE \u2225\u2225\u03a6\u2217nP \u2212\u03a6\u2217p\u03b1\u2297\u03b2\u2225\u22252 = 1n2 \u2225\u2225\u03a6\u2217 (p\u03c0\u0302\u2212p\u03b1\u2297\u03b2)\u2225\u22252 +Var(\u03a6\u2217nP )\nand\nVar(\u03a6\u2217nP ) = 1\nn4 n\u2211 i , j ,k,\u2113=1 Cov(h(xi , yk ),h(x\u2113, yk ))\nNote that Cov(h(xi , yk ),h(x\u2113, yk )) = 0 whenever i , j ,k,\u2113 are distinct and there are n(n \u22121)(n \u2212 2)(n \u22123) = n4 \u22126n3 +12n2 \u22126n such terms, i.e. there are 6n3 \u221212n2 +6n nonzero terms in the sum. It follows that Var(\u03a6\u2217nP ) =O (n\u22121) if \u2225C\u2225 and p are uniformly bounded. To conclude, we apply Lemma 15 with f (z1, . . . , zn) =\n\u2225\u2225\u03a6\u2217nP \u2212\u03a6\u2217p\u03b1\u2297\u03b2\u2225\u2225 and zi = (xi , yi ). Let X (z1, . . . , zn) =\u03a6\u2217nP \u2212\u03a6\u2217p\u03b1\u2297\u03b2. Then, Note that for an arbitrary vector u,\n\u2329u, X (z1, z2, . . . , zn)\u2212X (z \u20321, z2, . . . , zn)\u232a = \u2211 i , j\n( s\u2211\nk=1 uk (h(xi , y j )\u2212h(x \u2032i , y \u2032j ))\n)\n= n\u2211\ni=2\n( s\u2211\nk=1 uk (h(xi , y1)\u2212h(x \u2032i , y1))\n) +\nn\u2211 j=1\n( s\u2211\nk=1 uk (h(x1, y j )\u2212h(x \u20321, y \u2032j ))\n)\n\u00c9 8n\u22121 \u2225u\u2225sup x,y \u2225\u2225C(x, y)\u2225\u2225\u2225\u2225p\u2225\u2225\u221e So, f (z1, z2, . . . , zn)\u2212 f (z \u20321, z2, . . . , zn) \u00c9 8n\u22121\n\u2225\u2225p\u2225\u2225\u221e and\u2225\u2225\u03a6\u2217nP \u2212\u03a6\u2217p\u03b1\u2297\u03b2\u2225\u2225\u2272 1+ tpn with probability at least 1\u22122exp(\u22122t 2/(64\u2225\u2225p\u2225\u22252\u221e)) Remark 21 (Adjusting for the centred cost parametrization). In the above two propositions, we compare\u03a6\u2217nP = \u2211 i , j Ck (xi , y j )Pi , j with\u03a6\n\u2217\u03c0= \u222b Ck (x, y)d\u03c0(x, y) for P being a discretized version of \u03c0. The delicate issue is that in (Pn), Ck is a centralized version of C\u0302k \u225c (Ck (xi , y j ))i , j . In particular,\nCk = C\u0302k \u2212 1\nn n\u2211 i=1 (C\u0302k )i , j \u2212 1 n n\u2211 i=1 (C\u0302k )i , j + 1 n2 n\u2211 j=1 n\u2211 i=1 (C\u0302k )i , j .\nNote that if P1= 1n1 and P\u22a41= 1n1, then\u2211 i , j (Ck )i , j Pi , j = \u2211 i , j (C\u0302k )i , j Pi , j \u2212 1 n2 n\u2211 j=1 n\u2211 k=1 (C\u0302k )k, j\nThis last term on the RHS is negligible because\u03a6\u2217(\u03b1\u2297\u03b2) = 0 by assumption of Ck being centred: Note that\n1\nn2 n\u2211 j=1 n\u2211 k=1 (C\u0302k )k, j = \u03a6\u0302\u2217n( 1 n2 1n\u00d7n).\nApplying the above proposition,\u2225\u2225\u2225\u2225\u2225 1n2 n\u2211j=1 n\u2211 k=1 (C\u0302k )k, j \u2212\u03a6\u2217(\u03b1\u2297\u03b2) \u2225\u2225\u2225\u2225\u2225\u2272 m + log(n)pn with probability at least 1\u2212 exp(\u2212m). So, up to constants, the above two propositions can be applied even for our centralized cost C .\nD.2.2 INVERTIBILITY BOUND\nRecall our assumption that M \u225c E(x,y)\u223c\u03b1\u2297\u03b2[C(x, y)C(x, y)\u22a4] is invertible. In Proposition 23, we bound the deviation of\u03a6\u2217n\u03a6n from M in the spectral norm, and hence establish that it is invertible with high probability.\nWe will make use of the following matrix Bernstein inequality. Theorem 22 (Matrix Bernstein). Tropp et al. (2015) Let Z1, . . . , Zn \u2208Rd\u00d7d be independent symmetric mean-zero random matrices such that \u2225Zi\u2225 \u00c9 L for all i \u2208 [n]. Then, for all t \u00ca 0,\nE \u2225\u2225\u2225\u2225\u2225\u2211 i Zi \u2225\u2225\u2225\u2225\u2225\u00c9\u221a2\u03c3 log(2d)+ 13 L log(2d) where \u03c32 = \u2225\u2225\u2211ni=1E[Z 2i ]\u2225\u2225. Proposition 23. Assume that log(2s)+1 \u00c9 n. Let t > 0. Then,\u2225\u2225\u2225\u2225 1n2\u03a6\u2217n\u03a6n \u2212M \u2225\u2225\u2225\u2225\u2272 \u221a log(2s) n \u22121 + tp n\nwith probability at least 1\u2212O (exp(\u2212t 2)). Proof. Recall that \u03a6n A = \u2211k AkCk , where Ck is the centred version of the matrix C\u0302k = (Ck (xi , y j ))i , j . That is,\n(Ck )i , j = (C\u0302k )i , j \u2212 1\nn \u2211 \u2113 (C\u0302k )\u2113, j \u2212 1 n \u2211 m (C\u0302k )i ,m + 1 n2 \u2211 \u2113 \u2211 m (C\u0302k )\u2113,m . (37)\nFor simplicity, we first do the proof for (\u03a6n A)i , j =\u2211k Ak Ck (xi , y j ), and explain at the end how to modify the proof to account for\u03a6n using the centered Ck .\nFirst,\n1\nn2 \u03a6\u2217n\u03a6n \u2212M =\n1\nn n\u2211 i=1\n( 1\nn n\u2211 j=1 C(xi , y j )C(xi , y j ) \u22a4\u2212 \u222b C(xi , y)C(xi , y) \u22a4d\u03b2(y) ) (38)\n+ 1 n n\u2211 i=1 \u222b C(xi , y)C(xi , y) \u22a4d\u03b2(y)\u2212 \u222b C(x, y)C(x, y)\u22a4d\u03b1(x)d\u03b2(y) (39)\nTo bound the two terms in (39), let Zi \u225c \u222b C(xi , y)C(xi , y)\u22a4d\u03b2(y)\u2212 \u222b\nC(x, y)C(x, y)\u22a4d\u03b1(x)d\u03b2(y) and observe that these are i.i.d. matrices with zero mean. By matrix Bernstein with the bounds \u2225Zi\u2225 \u00c9 2 and \u2225\u2225Z 2i \u2225\u2225\u00c9 4, E\n\u2225\u2225\u2225\u2225\u2225 1n n\u2211i=1 Zi \u2225\u2225\u2225\u2225\u2225\u00c9 \u221a 8log(2s)p n + 2log(2s) 3n \u00c9 4 \u221a log(2s) n\nassuming that log(2s) \u00c9 n. For the two terms in (38),\nE \u2225\u2225\u2225\u2225\u2225 1n n\u2211i=1 ( 1 n n\u2211 j=1 C(xi , y j )C(xi , y j ) \u22a4\u2212 \u222b C(xi , y)C(xi , y) \u22a4d\u03b2(y) )\u2225\u2225\u2225\u2225\u2225 (40) \u00c9 E\n\u2225\u2225\u2225\u2225\u2225 1n2 n\u2211i=1 C(xi , yi )C(xi , yi )\u22a4 \u2225\u2225\u2225\u2225\u2225+E \u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225 1 n n\u2211 i=1  1n n\u2211j=1 j \u0338=i C(xi , y j )C(xi , y j ) \u22a4\u2212 \u222b C(xi , y)C(xi , y) \u22a4d\u03b2(y)  \u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225\n(41)\n\u00c9 2 n + n \u22121 n2 n\u2211 i=1 E \u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225 1 n \u22121 n\u2211\nj=1 j \u0338=i\n( C(xi , y j )C(xi , y j ) \u22a4\u2212 \u222b C(xi , y)C(xi , y) \u22a4d\u03b2(y) )\u2225\u2225\u2225\u2225\u2225\u2225\u2225\u2225 (42) For each i = 1, . . .n, let Y j = C(xi , y j )C(xi , y j )\u2212 \u222b C(xi , y)C(xi , y)\u22a4d\u03b2(y) and observe that condi-\ntional on xi , { Y j } j \u0338=i are iid matrices with zero mean. The matrix Bernstein inequality applied to\n1 n\u22121 \u2211 j\u2208[n]\\{i } Y j implies that\nE \u2225\u2225\u2225\u2225\u2225 1n \u22121 \u2211j\u2208[n]\\{i } Y j \u2225\u2225\u2225\u2225\u2225\u00c9 \u221a 8log(2s)p n \u22121 + 2log(2s) 3n \u22123 \u00c9 4 \u221a log(s) n\nassuming that log(2s) \u00c9 n \u22121. Finally, we apply Lemma 15 to f (z1, . . . , zn) = \u2225\u2225\u2225 1n2\u03a6\u2217n\u03a6n \u2212M\u2225\u2225\u2225. Let \u03a6\u2032nu = \u2211k uk C(x \u2032i , y \u2032i ) with x \u2032i = xi and y \u2032i = yi for i \u00ca 2. For each vector u of unit norm,\n1\nn2 \u2329(\u03a6\u2217n\u03a6n \u2212 (\u03a6\u2032n)\u2217\u03a6\u2032n)u, u\u232a =\n1\nn2 n\u2211 i=1 n\u2211 j=1 \u2223\u2223C(xi , y j )\u22a4u\u2223\u22232 \u2212 \u2223\u2223\u2223C(x \u2032i , y \u2032j )\u22a4u\u2223\u2223\u22232 (43) = 1\nn2 n\u2211 i=1 \u2223\u2223C(xi , y1)\u22a4u\u2223\u22232 \u2212 \u2223\u2223C(x \u2032i , y \u20321)\u22a4u\u2223\u22232 + 1n2 n\u2211j=2 \u2223\u2223C(x1, y j )\u22a4u\u2223\u22232 \u2212 \u2223\u2223\u2223C(x \u20321, y \u2032j )\u22a4u\u2223\u2223\u22232 \u00c9 4n\u22121. (44)\nSo, \u2225\u2225\u2225\u2225 1n2\u03a6\u2217n\u03a6n \u2212M \u2225\u2225\u2225\u2225\u00c9 8 \u221a log(2s) n \u22121 + tp n\nwith probability at least 1\u2212exp(\u22122t 2/16).\nTo conclude this proof, we now discuss how to modify the above proof in the case of the centered cost Ck , that is\u03a6n A = \u2211 k AkCk where Ck is as defined in (37). Note that in this case,\n1\nn2 (\u03a6\u2217n\u03a6n)k,\u2113 =\n1\nn2 n\u2211 i , j=1 Ck (xi , y j )C\u2113(xi , y j )+ 1 n4\n( n\u2211\np,q=1 C\u2113(xp , yq )\n)( n\u2211\ni , j=1 Ck (xi , y j )\n) (45)\n\u2212 1 n3 n\u2211 j=1\n( n\u2211\np=1 C\u2113(xp , y j )\n)( n\u2211\ni=1 Ck (xi , y j )\n) \u2212 1\nn3 n\u2211 i=1\n( n\u2211\nq=1 C\u2113(xi , yq )\n)( n\u2211\nj=1 Ck (xi , y j ) ) (46)\nWe already know from the previous arguments that\nE \u2225\u2225\u2225\u2225\u2225 1n2 n\u2211i , j=1 Ck (xi , y j )C\u2113(xi , y j )\u2212M \u2225\u2225\u2225\u2225\u2225=O (\u221alog(s)/n).\nFor the last term in (45), let \u039b = {(i , j , p, q) ; i \u2208 [n], j \u2208 [n] \\ {i } , p \u2208 [n] \\{i , j} , q \u2208 [n] \\{i , j , p}}. Note that\u039b has n(n\u22121)(n\u22122)(n\u22123) terms, and\u039bc = [n]\u00d7[n]\u00d7[n]\u00d7[n] has O (n3) terms. Therefore, we can write\nE \u2225\u2225\u2225\u2225\u2225 1n4 ( n\u2211 p,q=1 C(xp , yq ) )( n\u2211 i , j=1 C(xi , y j ) \u22a4 )\u2225\u2225\u2225\u2225\u2225\n\u00c9 1 n4 E \u2225\u2225\u2225\u2225\u2225 \u2211 (i , j ,p,q)\u2208\u039b C(xp , yq )C(xi , y j ) \u22a4 \u2225\u2225\u2225\u2225\u2225+E \u2225\u2225\u2225\u2225\u2225 1n4 \u2211(i , j ,p,q)\u2208\u039bc C(xp , yq )C(xi , y j )\u22a4 \u2225\u2225\u2225\u2225\u2225\n\u00c9 1 n4 E \u2225\u2225\u2225\u2225\u2225 \u2211 (i , j ,p,q)\u2208\u039b C(xp , yq )C(xi , y j ) \u22a4 \u2225\u2225\u2225\u2225\u2225+O (n\u22121)\nwhere the second inequality is because there are O (n3) terms in\u039bc and we used the bound that\u2225\u2225C(x, y)\u2225\u2225= 1. Moreover, 1\nn4 E \u2225\u2225\u2225\u2225\u2225 \u2211 (i , j ,p,q)\u2208\u039b C(xp , yq )C(xi , y j ) \u22a4 \u2225\u2225\u2225\u2225\u2225\u00c9 n \u22123n4 \u2211i \u2211 j \u0338\u2208{i } \u2211 p \u0338\u2208{i , j} E \u2225\u2225\u2225\u2225\u2225 1n \u22123 \u2211q \u0338\u2208{i , j ,p}C(xp , yq )C(xi , y j )\u22a4 \u2225\u2225\u2225\u2225\u2225 .\nNote that conditional on i , j , p,\nE[ 1 n \u22123 \u2211\nq \u0338\u2208{i , j ,p} C(xp , yq )C(xi , y j )\n\u22a4] = \u222b C(xp , y)d\u03b2(y)C(xi , y j ) \u22a4 = 0\nby assumption that C(x, y)d\u03b2(y) = 0. So, we can apply Matrix Bernstein as before to show that\nE[ 1\nn4 E \u2225\u2225\u2225\u2225\u2225 \u2211 (i , j ,p,q)\u2208\u039b C(xp , yq )C(xi , y j ) \u22a4 \u2225\u2225\u2225\u2225\u2225]\u2272\u221alog(s)n\u22121.\nA similar argument can be applied to handle the two terms in (46), and so,\nE \u2225\u2225\u2225\u2225 1n2\u03a6\u2217n\u03a6n \u2212M \u2225\u2225\u2225\u2225\u2272\u221alog(s)/n.\nThe high probability bound can now be derived using Lemma 15 as before."
        },
        {
            "heading": "E PROOFS FOR THE GAUSSIAN SETTING",
            "text": "Simplified problem To ease the computations, we will compute the Hessian of the following function (corresponding to the special case where \u03a3\u03b1 = Id and \u03a3\u03b2 = Id):\nW\u0303 (A)\u225c sup \u03a3 \u2329A, \u03a3\u232a+ \u03b5 2 logdet(Id\u2212\u03a3\u22a4\u03a3). (47)\nTo retrieve the Hessian of the original function note that, since logdet(\u03a3\u03b2\u2212\u03a3\u22a4\u03a3\u22121\u03b1 \u03a3) = logdet(\u03a3\u03b2)+ logdet(Id\u2212\u03a3\u2212 1 2\n\u03b2 \u03a3\u22a4\u03a3\u22121\u03b1 \u03a3\u03a3 \u2212 12 \u03b2 ), a change of variable \u03a3\u0303\u225c\u03a3 \u2212 12 \u03b1 \u03a3\u03a3 \u2212 12 \u03b2 , shows that W (A) = W\u0303 (\u03a3 1 2 \u03b1 A\u03a3 1 2 \u03b2 )\nand, hence,\n\u22072W (A) = (\u03a3 1 2 \u03b2 \u2297\u03a3 1 2 \u03b1)\u22072W\u0303 (\u03a3 1 2 \u03b1 A\u03a3 1 2 \u03b2 )(\u03a3 1 2 \u03b2 \u2297\u03a3 1 2 \u03b1). (48)\nBy the envelope theorem, \u2207W\u0303 (A) =\u03a3, where \u03a3 is the maximizer of (47), thus reducing the computation of \u22072W\u0303 (A) to differentiating the optimality condition of \u03a3, i.e.,\nA = \u03b5\u22121\u03a3(Id\u2212\u03a3\u22a4\u03a3)\u22121. (49) Recall also that such a \u03a3 has an explicit formula given in (5).\nLemma 24. \u22072W\u0303 (A) = \u03b5(\u03b52(Id\u2212\u03a3\u22a4\u03a3)\u22121 \u2297 (Id\u2212\u03a3\u03a3\u22a4)\u22121 + (A\u22a4\u2297 A)T)\u22121 (50) and \u03a3 is as in (49) and T is the linear map defined by Tvec(X ) = vec(X \u22a4).\nProof of Lemma 24. Define G(\u03a3, A) \u225c \u03a3(Id \u2212\u03a3\u22a4\u03a3)\u22121 \u2212 \u03b5\u22121 A. Note that a maximizer \u03a3 of (47) satisfies G(\u03a3, A) = 0. Moreover, \u2202\u03a3G is invertible at such (\u03a3, A) because this is the Hessian of logdet(Id\u2212\u03a3\u22a4\u03a3), a strictly concave function. By the implicit function theorem, there exists a function f : A 7\u2192\u03a3 such that G( f (A), A) = 0 and\n\u2207 f = (\u2202\u03a3G)\u22121\u2202AG = \u03b5\u22121(\u2202\u03a3G)\u22121. It remains to compute \u2202\u03a3G at ( f (A), A):\nAt an arbitrary point (\u03a3, A) we have\n\u2202\u03a3G = (Id\u2212\u03a3\u22a4\u03a3)\u22121 \u2297 Id+ (Id\u2297\u03a3)\u2202\u03a3(Id\u2212\u03a3\u22a4\u03a3)\u22121 = (Id\u2212\u03a3\u22a4\u03a3)\u22121 \u2297 Id+ (Id\u2297\u03a3)((Id\u2212\u03a3\u22a4\u03a3)\u22121 \u2297 (Id\u2212\u03a3\u22a4\u03a3)\u22121)((\u03a3\u22a4\u2297 Id)T+ (Id\u2297\u03a3\u22a4))\nand at ( f (A), A), i.e, with \u03a3(Id\u2212\u03a3\u22a4\u03a3)\u22121 = \u03b5\u22121 A, we can further simplify to\n\u2202\u03a3G = (Id\u2212\u03a3\u22a4\u03a3)\u22121 \u2297 (Id+\u03a3(Id\u2212\u03a3\u22a4\u03a3)\u22121\u03a3\u22a4))+\u03b5\u22122(A\u22a4\u2297 A)T By the Woodbury matrix formula,\n(Id+\u03a3(Id\u2212\u03a3\u22a4\u03a3)\u22121\u03a3\u22a4) = (Id\u2212\u03a3\u03a3\u22a4)\u22121 = Id+ A\u03a3\u22a4. So, \u2202\u03a3G = (Id\u2212\u03a3\u22a4\u03a3)\u22121 \u2297 (Id\u2212\u03a3\u03a3\u22a4)\u22121 +\u03b5\u22122(A\u22a4\u2297 A)T, thus concluding the proof.\nWe remark that from the connection between W (A) and W\u0303 (A), i.e., 48, we obtain Lemma 7 as a corollary."
        },
        {
            "heading": "E.1 LIMIT CASES",
            "text": "SVD representation of the covariance To derive the limiting expressions, we make an observation on the singular value decomposition of \u03a3: Let the singular value decomposition of A be A = U DV \u22a4, where D is the diagonal matrix with positive entries di . Note that \u2206 = (Id+ \u03b524 (A\u22a4A)\u2020) 1 2 =V (Id+ \u03b524 D\u22122) 1 2 V \u22a4. Moreover, \u2206 and A\u22a4A commute, so\n\u03a3= A\u2206((\u22062 A\u22a4A)\u2020) 12\u2206\u2212 \u03b5 2\nAT,\u2020 =U (Id+ \u03b52\n4 D\u2020,2\n) 1 2\n\u2212 \u03b5 2 D\u2020 V \u22a4 =U D\u0303V \u22a4, (51) where D\u0303 is the diagonal matrix with diagonal entries\nd\u0303i = \u221a\u221a\u221a\u221a(1+ \u03b52\n4d 2i\n) \u2212 \u03b5\n2\n1\ndi . (52)\nE.1.1 LINK WITH LASSO: LIMIT AS \u03b5\u2192\u221e Note that\nd\u0303i = \u03b5 2di\n\u221a 1+ 4d 2 i\n\u03b52 \u2212 \u03b5 2di = di \u03b5 \u2212 d\n3 i\n\u03b53 +O (\u03b5\u22125) \u2192 0, \u03b5\u2192\u221e. (53)\nIt follows that lim\u03b5\u2192\u221e\u03a3= 0 and hence, \u03b5\u22072W\u0303 (A) \u2192 Id. So, the certificate converges to (\u03a3\u03b2\u2297\u03a3\u03b1)(:,I ) ( (\u03a3\u03b2\u2297\u03a3\u03b1)(I ,I ) )\u22121 sign(A\u0302)I . (54) Proof of Proposition 8. The iOT problem approaches a Lasso problem as \u03b5\u2192\u221e. Recall that in the Gaussian setting, the iOT problem is of the form\nargmin A\nF (A) =\u03bb\u2225A\u22251 +\u2329A, \u03a3\u03b5,A \u2212\u03a3\u03b5,A\u0302\u232a+ \u03b5\n2 logdet\n( \u03a3\u03b2\u2212\u03a3\u22a4\u03b5,A\u03a3\u22121\u03b1 \u03a3\u03b5,A ) (55)\nwhere \u03a3\u03b5,A satisfies \u03a3\u03b2\u2212\u03a3\u22a4\u03b5,A\u03a3\u22121\u03b1 \u03a3\u03b5,A = \u03b5\u03a3\u22121\u03b1 \u03a3\u03b5,A A\u22121. So, we can write\nargmin A\nF (A)\u225c\u03bb\u2225A\u22251 +\u2329A, \u03a3\u03b5,A \u2212\u03a3\u03b5,A\u0302\u232a+ \u03b5\n2 logdet\n( \u03b5\u03a3\u22121\u03b1 \u03a3\u03b5,A A \u22121) (56) Let\nX \u225c\u03a3 1 2 \u03b1 A\u03a3 1 2 \u03b2 and \u03a3\u0303\u03b5,A =\u03a3\u2212 1 2 \u03b1 \u03a3\u03b5,A\u03a3 \u2212 12 \u03b2 .\nFrom (52), if X has SVD decomposition W =U diag(di )V \u22a4, then \u03a3\u0303\u03b5,A =U D\u0303V \u22a4, where D\u0303 = diag(d\u0303i )\nand\nd\u0303i = di \u03b5\n\u2212 d 3 i\n\u03b53 +O (\u03b5\u22125).\nSo,\nlogdet(\u03b5\u03a3\u22121\u03b1 \u03a3\u03b5,A A \u22121) = logdet ( \u03b5\u03a3 \u2212 12 \u03b1 \u03a3\u0303\u03b5,A X \u22121\u03a3 1 2 \u03b1 ) = logdet(U diag(1\u2212d 2i /\u03b52 +O (\u03b5\u22124))U\u22a4) = logdet(diag(1\u2212d 2i /\u03b52 +O (\u03b5\u22124)))=\u2212 1\u03b52 \u2225X \u22252F +O (\u03b5\u22124).\nAlso, \u03b5\u2329\u03a3\u03b5,A \u2212\u03a3\u03b5,A\u0302 , A\u232a = \u03b5\u2329\u03a3\u0303\u03b5,A \u2212 \u03a3\u0303\u03b5,A\u0302 , X \u232a = \u2329X \u2212X0, X \u232a+O (\u03b5\u22122). So, assuming that \u03bb=\u03bb0/\u03b5, \u03b5F (A) =\u03bb0 \u2225A\u22251 +\u2329X \u2212X0, X \u232a\u2212\u2225X \u22252F +O (\u03b5\u22122) (57)\n=\u03bb0 \u2225A\u22251 + \u2225\u2225\u2225\u2225(\u03a3 12\u03b2 \u2297\u03a3 12\u03b1)(A\u2212 A\u0302)\u2225\u2225\u2225\u22252\nF \u2212 1 2 \u2225\u2225\u2225\u2225(\u03a3 12\u03b2 \u2297\u03a3 12\u03b1)A\u0302\u2225\u2225\u2225\u22252 +O (\u03b5\u22122) (58) The final statement on the convergence of minimizers follows by Gamma-convergence."
        },
        {
            "heading": "E.1.2 LINK WITH GRAPHICAL LASSO",
            "text": "In the special case where the covariances are the identity (\u03a3\u03b1 = Id and \u03a3\u03b2 = Id) and A is symmetric positive definite we have that \u03a3 is also positive definite and Galichon\u2019s formula (5) holds (since A is invertible) and hence the Hessian reduces to(\n1 \u03b5 \u22072W\u0303 (A)\n)\u22121 = (A\u2297 A)(\u03a3\u22121 \u2297\u03a3\u22121 +T) . (59)\nMoreover, if A admits an eigenvalue decomposition A = U DU\u22a4, then \u03a3 admits an eigenvalue decomposition \u03a3=U D\u0303U\u22a4 with entries of D\u0303 given by (52). Note that it follows that lim\u03b5\u21920\u03a3= Id and, hence, lim\u03b5\u21920 ( \u03a3\u22121 \u2297\u03a3\u22121 +T) = Id+T, a singular matrix with the kernel being the set of\nasymmetric matrices. So, the limit does not necessarily exist as \u03b5\u2192 0. However, in the special case where A is symmetric positive definite, one can show that the certificates remain well defined as \u03b5\u2192 0: Let S+ be the set of matrices \u03c8 such that \u03c8 is symmetric and S\u2212 be the set of matrices \u03c8 such that \u03c8 is anti-symmetric. Then, (\u22072W\u0303 (A))\u22121(S+) \u2282 S+ and (\u22072W\u0303 (A))\u22121(S\u2212) \u2282 S\u2212. Moreover,(\n1 \u03b5 \u22072W\u0303 (A)\n)\u22121 \u21beS+ = (A\u2297 A) ( \u03a3\u22121 \u2297\u03a3\u22121 + Id) ,(\n1 \u03b5 \u22072W\u0303 (A)\n)\u22121 \u21beS\u2212 = (A\u2297 A) ( \u03a3\u22121 \u2297\u03a3\u22121 \u2212 Id) .\nSince the symmetry of A implies the symmetry of sign(A), we can replace the Hessian given in (59) by (A\u2297 A)(\u03a3\u22121 \u2297\u03a3\u22121 + Id) and, hence, since lim\u03b5\u21920\u03a3= Id, the limit as \u03b5\u2192 0 is\nlim \u03b5\u21920 z\u03b5 = (A\n\u22121 \u2297 A\u22121)(:,I ) ( (A\u22121 \u2297 A\u22121)(I ,I ) )\u22121 sign(A)I . (60) This coincides precisely with the certificate of the graphical lasso:\nargmin \u0398\u2ab00\n\u2329S,\u0398\u232a\u2212 logdet(\u0398)+\u03bb\u2225\u0398\u22251 .\nProof of Proposition 9. The iOT problem with identity covariances restricted to the set of positive semi-definite matrices has the form\nargmin A\u2ab00\nF\u03b5,\u03bb(A), where F\u03b5,\u03bb(A)\u225c\u03bb\u2225A\u22251 +\u2329A, \u03a3\u03b5,A \u2212 \u03a3\u0302\u232a+ \u03b5 2 logdet(Id\u2212\u03a3\u22a4\u03b5,A\u03a3\u03b5,A), (61)\nwhere I \u2212\u03a3\u22a4\u03b5,A\u03a3\u03b5,A = \u03b5\u03a3\u03b5,A A\u22121. From the singular value decomposition of \u03a3\u03b5,A ,i.e., (51), we see that if A is symmetric positive definite, then so is \u03a3\u03b5,A . Plugging the optimality condition, i.e., I \u2212\u03a3\u22a4\u03b5,A\u03a3\u03b5,A = \u03b5\u03a3\u03b5,A A\u22121, into (61), we obtain\nargmin A\u2ab00\n\u03bb\u2225A\u22251 +\u2329A, \u03a3\u03b5,A \u2212 \u03a3\u0302\u232a+ \u03b5 2 logdet(\u03b5A\u22121\u03a3\u03b5,A)\n=argmin A\u2ab00\n\u03bb\u2225A\u22251 \u2212 \u03b5 2 logdet(A/\u03b5)+\u03b5\u2329A, (\u03a3\u03b5,A \u2212 \u03a3\u0302)/\u03b5\u232a+ \u03b5 2 logdet(\u03a3\u03b5,A)\nLet \u03bb= \u03b5\u03bb0 for some \u03bb0 > 0. Then, removing the constant \u03b52 logdet(\u03b5Id) term and factoring out \u03b5, the problem is equivalent to\nargmin A\u2ab00\n\u03bb0 \u2225A\u22251 \u2212 1 2 logdet(A)+\u2329A, \u03b5\u22121(\u03a3\u03b5,A \u2212 \u03a3\u0302)\u232a+ 1 2 logdet(\u03a3\u03b5,A)\nAssume that \u03a3\u0302=\u03a3\u03b5,A\u0302 . From the expression for the singular values of \u03a3\u03b5,A in (52), note that\n\u03a3\u03b5,A = Id\u2212 \u03b5 2 A\u22121 +O (\u03b52).\nSo, lim\u03b5\u21920(\u03a3\u03b5,A \u2212\u03a3\u03b5,A\u0302)/\u03b5=\u2212 12 ( A\u22121 \u2212 A\u0302\u22121). The objective converges pointwise to \u03bb0 \u2225A\u22251 \u2212 1\n2 logdet(A)+ 1 2 \u2329A, A\u0302\u22121 \u2212 A\u22121\u232a,\nand the statement is then a direct consequence of Gamma-convergence.\nRemark 25. Note that from (52), we have \u03a3= Id\u2212 \u03b52 A\u22121 +O (\u03b52). So, in this case, the covariance of \u03c0\u0302 is (\nId Id\u2212 \u03b52 A\u22121 +O (\u03b52) Id\u2212 \u03b52 A\u22121 +O (\u03b52) Id\n) .\nFor (X ,Y ) \u223c \u03c0, The Schur complement of this is the covariance of X conditional on Y , which is \u03b5A\u22121 +O (\u03b52). So, up the \u03b5, one can see A\u22121 as the \u201cprecision\" matrix of the covariance of X conditional on Y .\nF LARGE SCALE \u21131-IOT SOLVER\nRecall that the iOT optimization problem, recast over the dual potentials for empirical measures, reads\ninf A,F,G\n1\nn \u2211 i Fi +Gi + (\u03a6n A)i ,i + \u03b5 2n2 \u2211 i , j exp\n( 2\n\u03b5 (Fi +G j + (\u03a6n A)i , j\n) +\u03bb\u2225A\u22251 .\nTo obtain a better-conditioned optimization problem, in line with Cuturi & Peyr\u00e9 (2016), we instead consider the semi-dual problem, which is derived by leveraging the closed-form expression for the optimal G , given F .\ninf A,F\n1\nn \u2211 i Fi + (\u03a6n A)i ,i + \u03b5 n \u2211 i log 1 n \u2211 j exp\n( 2\n\u03b5 (Fi + (\u03a6n A)i , j\n) +\u03bb\u2225A\u22251 .\nFollowing Poon & Peyr\u00e9 (2021), which proposes a state-of-the-art Lasso solver, the last step is to use the following Hadamard product over-parameterization of the \u21131 norm\n\u2225A\u22251 = min U\u2299V \u2225U\u222522 2 + \u2225V \u2225 2 2 2 .\nwhere the Hadamard product is U \u2299V \u225c (Ui Vi )i , to obtain the final optimization problem\ninf A,U ,V\n1\nn \u2211 i Fi + (\u03a6n(U \u2299V ))i ,i + \u03b5 n \u2211 i log 1 n \u2211 j exp\n( 2\n\u03b5 (Fi + (\u03a6n(U \u2299V ))i , j\n) + \u03bb\n2 \u2225U\u222522 +\n\u03bb 2 \u2225V \u222522 .\nThis is a smooth optimization problem, for which we employ a quasi-Newton solver (L-BFGS). Although it is non-convex, as demonstrated in Poon & Peyr\u00e9 (2021), the non-convexity is benign, ensuring the solver always converges to a global minimizer, (F\u22c6,U\u22c6,V \u22c6), of the functional. From this, one can reconstruct the cost parameter, A\u22c6\u225cU\u22c6\u2299V \u22c6."
        }
    ],
    "year": 2024
}