{
    "abstractText": "A remarkable ability of human beings resides in compositional reasoning, i.e., the capacity to make \u201cinfinite use of finite means\u201d. However, current large visionlanguage foundation models (VLMs) fall short of such compositional abilities due to their \u201cbag-of-words\u201d behaviors and inability to construct words that correctly represent visual entities and the relations among the entities. To this end, we propose Compositional VLM, which can guide the LLM to explicitly compose visual entities and relationships among the text and dynamically communicate with the vision encoder and detection network to achieve vision-language communicative decoding. Specifically, we first devise a set of novel communication tokens for the LLM, for dynamic communication between the visual detection system and the language system. A communication token is generated by the LLM following a visual entity or a relation, to inform the detection network to propose regions that are relevant to the sentence generated so far. The proposed regions-of-interests (ROIs) are then fed back into the LLM for better language generation contingent on the relevant regions. The LLM is thus able to compose the visual entities and relationships through the communication tokens. The vision-to-language and language-to-vision communication are iteratively performed until the entire sentence is generated. Our framework seamlessly bridges the gap between visual perception and LLMs and outperforms previous VLMs by a large margin on compositional reasoning benchmarks (e.g., \u223c 20% in HICO-DET mAP, \u223c 14% in Cola top-1 accuracy, and \u223c 3% on ARO top-1 accuracy). We also achieve competitive performances on traditional vision-language tasks such as referring expression comprehension and visual question answering 1.",
    "authors": [],
    "id": "SP:c7925ba2d7c82877fc56fe2f017eac480a02b7a2",
    "references": [
        {
            "authors": [
                "Aishwarya Agrawal",
                "Aniruddha Kembhavi",
                "Dhruv Batra",
                "Devi Parikh"
            ],
            "title": "C-vqa: A compositional split of the visual question answering (vqa",
            "year": 2017
        },
        {
            "authors": [
                "Anurag Ajay",
                "Seungwook Han",
                "Yilun Du",
                "Shuang Li",
                "Abhi Gupta",
                "Tommi Jaakkola",
                "Josh Tenenbaum",
                "Leslie Kaelbling",
                "Akash Srivastava",
                "Pulkit Agrawal"
            ],
            "title": "Compositional foundation models for hierarchical planning, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Anas Awadalla",
                "Irena Gao",
                "Josh Gardner",
                "Jack Hessel",
                "Yusuf Hanafy",
                "Wanrong Zhu",
                "Kalyani Marathe",
                "Yonatan Bitton",
                "Samir Gadre",
                "Shiori Sagawa",
                "Jenia Jitsev",
                "Simon Kornblith",
                "Pang Wei Koh",
                "Gabriel Ilharco",
                "Mitchell Wortsman",
                "Ludwig Schmidt"
            ],
            "title": "Openflamingo: An opensource framework for training large autoregressive vision-language models",
            "venue": "arXiv preprint arXiv:2308.01390,",
            "year": 2023
        },
        {
            "authors": [
                "Stella Biderman",
                "Hailey Schoelkopf",
                "Quentin Anthony",
                "Herbie Bradley",
                "Kyle O\u2019Brien",
                "Eric Hallahan",
                "Mohammad Aflah Khan",
                "Shivanshu Purohit",
                "USVSN Sai Prashanth",
                "Edward Raff",
                "Aviya Skowron",
                "Lintang Sutawika",
                "Oskar van der Wal"
            ],
            "title": "Pythia: A suite for analyzing large language models across training and scaling, 2023",
            "year": 2023
        },
        {
            "authors": [
                "T.J. Buschman",
                "Earl K. Miller"
            ],
            "title": "Top-down versus bottom-up control of attention in the prefrontal and posterior parietal cortices",
            "year": 2007
        },
        {
            "authors": [
                "Paola Cascante-Bonilla",
                "Khaled Shehada",
                "James Seale Smith",
                "Sivan Doveh",
                "Donghyun Kim",
                "Rameswar Panda",
                "G\u00fcl Varol",
                "Aude Oliva",
                "Vicente Ordonez",
                "Rogerio Feris",
                "Leonid Karlinsky"
            ],
            "title": "Going beyond nouns with vision & language models using synthetic data, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Soravit Changpinyo",
                "Piyush Sharma",
                "Nan Ding",
                "Radu Soricut"
            ],
            "title": "Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Yu-Wei Chao",
                "Zhan Wang",
                "Yugeng He",
                "Jiaxuan Wang",
                "Jia Deng"
            ],
            "title": "Hico: A benchmark for recognizing human-object interactions in images",
            "venue": "In Proceedings of the IEEE International Conference on Computer Vision (ICCV),",
            "year": 2015
        },
        {
            "authors": [
                "Colin Conwell",
                "Tomer David Ullman"
            ],
            "title": "Testing relational understanding in text-guided image generation",
            "venue": "ArXiv, abs/2208.00005,",
            "year": 2022
        },
        {
            "authors": [
                "Wenliang Dai",
                "Lu Hou",
                "Lifeng Shang",
                "Xin Jiang",
                "Qun Liu",
                "Pascale Fung"
            ],
            "title": "Enabling multimodal generation on clip via vision-language knowledge distillation",
            "venue": "arXiv preprint arXiv:2203.06386,",
            "year": 2022
        },
        {
            "authors": [
                "Sivan Doveh",
                "Assaf Arbelle",
                "Sivan Harary",
                "Rameswar Panda",
                "Roei Herzig",
                "Eli Schwartz",
                "Donghyun Kim",
                "Raja Giryes",
                "Rog\u00e9rio Schmidt Feris",
                "Shimon Ullman",
                "Leonid Karlinsky"
            ],
            "title": "Teaching structured vision & language concepts to vision & language models",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2023
        },
        {
            "authors": [
                "Sivan Doveh",
                "Assaf Arbelle",
                "Sivan Harary",
                "Roei Herzig",
                "Donghyun Kim",
                "Paola Cascante-Bonilla",
                "Amit Alfassy",
                "Rameswar Panda",
                "Raja Giryes",
                "Rog\u00e9rio Schmidt Feris",
                "Shimon Ullman",
                "Leonid Karlinsky"
            ],
            "title": "Dense and aligned captions (dac) promote compositional reasoning in vl models",
            "venue": "ArXiv, abs/2305.19595,",
            "year": 2023
        },
        {
            "authors": [
                "Leo Gao",
                "Stella Biderman",
                "Sid Black",
                "Laurence Golding",
                "Travis Hoppe",
                "Charles Foster",
                "Jason Phang",
                "Horace He",
                "Anish Thite",
                "Noa Nabeshima"
            ],
            "title": "The pile: An 800gb dataset of diverse text for language modeling",
            "venue": "arXiv preprint arXiv:2101.00027,",
            "year": 2020
        },
        {
            "authors": [
                "Zheng Ge",
                "Songtao Liu",
                "Feng Wang",
                "Zeming Li",
                "Jian Sun"
            ],
            "title": "Yolox: Exceeding yolo series",
            "year": 2021
        },
        {
            "authors": [
                "Georgia Gkioxari",
                "Ross Girshick",
                "Piotr Doll\u00e1r",
                "Kaiming He"
            ],
            "title": "Detecting and recognizing humanobject interactions",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Yash Goyal",
                "Tejas Khot",
                "Douglas Summers-Stay",
                "Dhruv Batra",
                "Devi Parikh"
            ],
            "title": "Making the v in vqa matter: Elevating the role of image understanding in visual question answering",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Yaru Hao",
                "Haoyu Song",
                "Li Dong",
                "Shaohan Huang",
                "Zewen Chi",
                "Wenhui Wang",
                "Shuming Ma",
                "Furu Wei"
            ],
            "title": "Language models are general-purpose interfaces",
            "venue": "arXiv preprint arXiv:2206.06336,",
            "year": 2022
        },
        {
            "authors": [
                "Yining Hong",
                "Haoyu Zhen",
                "Peihao Chen",
                "Shuhong Zheng",
                "Yilun Du",
                "Zhenfang Chen",
                "Chuang Gan"
            ],
            "title": "3d-llm: Injecting the 3d world into large language models, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Matthew Honnibal",
                "Ines Montani",
                "Sofie Van Landeghem",
                "Adriane Boyd"
            ],
            "title": "spacy: Industrialstrength natural language processing in python",
            "year": 2020
        },
        {
            "authors": [
                "Drew A. Hudson",
                "Christopher D. Manning"
            ],
            "title": "Gqa: A new dataset for real-world visual reasoning and compositional question answering, 2019",
            "year": 2019
        },
        {
            "authors": [
                "Theo M.V. Janssen",
                "Barbara H"
            ],
            "title": "Partee. Chapter 7 - compositionality",
            "venue": "Handbook of Logic and Language,",
            "year": 1997
        },
        {
            "authors": [
                "Woojeong Jin",
                "Subhabrata Mukherjee",
                "Yu Cheng",
                "Yelong Shen",
                "Weizhu Chen",
                "Ahmed Hassan Awadallah",
                "Damien Jose",
                "Xiang Ren"
            ],
            "title": "Grill: Grounded vision-language pre-training via aligning text and image regions",
            "venue": "arXiv preprint arXiv:2305.14676,",
            "year": 2023
        },
        {
            "authors": [
                "Justin Johnson",
                "Bharath Hariharan",
                "Laurens van der Maaten",
                "Li Fei-Fei",
                "C. Lawrence Zitnick",
                "Ross Girshick"
            ],
            "title": "Clevr: A diagnostic dataset for compositional language and elementary visual reasoning, 2016",
            "year": 2016
        },
        {
            "authors": [
                "Ranjay Krishna",
                "Yuke Zhu",
                "Oliver Groth",
                "Justin Johnson",
                "Kenji Hata",
                "Joshua Kravitz",
                "Stephanie Chen",
                "Yannis Kalantidis",
                "Li-Jia Li",
                "David A Shamma"
            ],
            "title": "Visual genome: Connecting language and vision using crowdsourced dense image annotations",
            "venue": "International journal of computer vision,",
            "year": 2017
        },
        {
            "authors": [
                "Xin Lai",
                "Zhuotao Tian",
                "Yukang Chen",
                "Yanwei Li",
                "Yuhui Yuan",
                "Shu Liu",
                "Jiaya Jia"
            ],
            "title": "Lisa: Reasoning segmentation via large language model, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Junnan Li",
                "Dongxu Li",
                "Caiming Xiong",
                "Steven Hoi"
            ],
            "title": "Blip: Bootstrapping language-image pretraining for unified vision-language understanding and generation",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Junnan Li",
                "Dongxu Li",
                "Silvio Savarese",
                "Steven Hoi"
            ],
            "title": "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Yue Liao",
                "Aixi Zhang",
                "Miao Lu",
                "Yongliang Wang",
                "Xiaobo Li",
                "Si Liu"
            ],
            "title": "Gen-vlkt: Simplify association and enhance interaction understanding for hoi detection",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Tsung-Yi Lin",
                "Michael Maire",
                "Serge Belongie",
                "James Hays",
                "Pietro Perona",
                "Deva Ramanan",
                "Piotr Doll\u00e1r",
                "C Lawrence Zitnick"
            ],
            "title": "Microsoft coco: Common objects in context",
            "venue": "In Computer Vision\u2013ECCV",
            "year": 2014
        },
        {
            "authors": [
                "Haotian Liu",
                "Chunyuan Li",
                "Qingyang Wu",
                "Yong Jae Lee"
            ],
            "title": "Visual instruction tuning, 2023a",
            "year": 2023
        },
        {
            "authors": [
                "Nan Liu",
                "Yilun Du",
                "Shuang Li",
                "Joshua B. Tenenbaum",
                "Antonio Torralba"
            ],
            "title": "Unsupervised compositional concepts discovery with text-to-image generative models, 2023b",
            "year": 2023
        },
        {
            "authors": [
                "Shilong Liu",
                "Zhaoyang Zeng",
                "Tianhe Ren",
                "Feng Li",
                "Hao Zhang",
                "Jie Yang",
                "Chunyuan Li",
                "Jianwei Yang",
                "Hang Su",
                "Jun Zhu"
            ],
            "title": "Grounding dino: Marrying dino with grounded pre-training for open-set object detection",
            "venue": "arXiv preprint arXiv:2303.05499,",
            "year": 2023
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter"
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "arXiv preprint arXiv:1711.05101,",
            "year": 2017
        },
        {
            "authors": [
                "Zixian Ma",
                "Jerry Hong",
                "Mustafa Omer Gul",
                "Mona Gandhi",
                "Irena Gao",
                "Ranjay Krishna"
            ],
            "title": "Crepe: Can vision-language foundation models reason compositionally",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Junhua Mao",
                "Jonathan Huang",
                "Alexander Toshev",
                "Oana Camburu",
                "Alan L Yuille",
                "Kevin Murphy"
            ],
            "title": "Generation and comprehension of unambiguous object descriptions",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Vicente Ordonez",
                "Girish Kulkarni",
                "Tamara Berg"
            ],
            "title": "Im2text: Describing images using 1 million captioned photographs",
            "venue": "Advances in neural information processing systems,",
            "year": 2011
        },
        {
            "authors": [
                "Zhiliang Peng",
                "Wenhui Wang",
                "Li Dong",
                "Yaru Hao",
                "Shaohan Huang",
                "Shuming Ma",
                "Furu Wei"
            ],
            "title": "Kosmos-2: Grounding multimodal large language models to the world, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Renjie Pi",
                "Jiahui Gao",
                "Shizhe Diao",
                "Rui Pan",
                "Hanze Dong",
                "Jipeng Zhang",
                "Lewei Yao",
                "Jianhua Han",
                "Hang Xu",
                "Lingpeng Kong",
                "Tong Zhang"
            ],
            "title": "Detgpt: Detect what you need via reasoning, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark",
                "Gretchen Krueger",
                "Ilya Sutskever"
            ],
            "title": "Learning transferable visual models from natural language supervision, 2021",
            "year": 2021
        },
        {
            "authors": [
                "Arijit Ray",
                "Filip Radenovic",
                "Abhimanyu Dubey",
                "Bryan A. Plummer",
                "Ranjay Krishna",
                "Kate Saenko"
            ],
            "title": "Cola: A benchmark for compositional text-to-image retrieval, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Shaoqing Ren",
                "Kaiming He",
                "Ross Girshick",
                "Jian Sun"
            ],
            "title": "Faster r-cnn: Towards real-time object detection with region proposal networks, 2016",
            "year": 2016
        },
        {
            "authors": [
                "Kuniaki Saito",
                "Kihyuk Sohn",
                "Xiang Zhang",
                "Chun-Liang Li",
                "Chen-Yu Lee",
                "Kate Saenko",
                "Tomas Pfister"
            ],
            "title": "Pic2word: Mapping pictures to words for zero-shot composed image retrieval, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Christoph Schuhmann",
                "Richard Vencu",
                "Romain Beaumont",
                "Robert Kaczmarczyk",
                "Clayton Mullis",
                "Aarush Katta",
                "Theo Coombes",
                "Jenia Jitsev",
                "Aran Komatsuzaki"
            ],
            "title": "Laion-400m: Open dataset of clip-filtered 400 million image-text pairs",
            "venue": "arXiv preprint arXiv:2111.02114,",
            "year": 2021
        },
        {
            "authors": [
                "Piyush Sharma",
                "Nan Ding",
                "Sebastian Goodman",
                "Radu Soricut"
            ],
            "title": "Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning",
            "venue": "In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
            "year": 2018
        },
        {
            "authors": [
                "Amanpreet Singh",
                "Ronghang Hu",
                "Vedanuj Goswami",
                "Guillaume Couairon",
                "Wojciech Galuba",
                "Marcus Rohrbach",
                "Douwe Kiela"
            ],
            "title": "Flava: A foundational language and vision alignment model",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Sanjay Subramanian",
                "William Merrill",
                "Trevor Darrell",
                "Matt Gardner",
                "Sameer Singh",
                "Anna Rohrbach"
            ],
            "title": "Reclip: A strong zero-shot baseline for referring expression comprehension",
            "venue": "arXiv preprint arXiv:2204.05991,",
            "year": 2022
        },
        {
            "authors": [
                "Wenhai Wang",
                "Zhe Chen",
                "Xiaokang Chen",
                "Jiannan Wu",
                "Xizhou Zhu",
                "Gang Zeng",
                "Ping Luo",
                "Tong Lu",
                "Jie Zhou",
                "Yu Qiao",
                "Jifeng Dai"
            ],
            "title": "Visionllm: Large language model is also an open-ended decoder for vision-centric tasks, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Qinghao Ye",
                "Haiyang Xu",
                "Guohai Xu",
                "Jiabo Ye",
                "Ming Yan",
                "Yiyang Zhou",
                "Junyang Wang",
                "Anwen Hu",
                "Pengcheng Shi",
                "Yaya Shi",
                "Chenliang Li",
                "Yuanhong Xu",
                "Hehong Chen",
                "Junfeng Tian",
                "Qian Qi",
                "Ji Zhang",
                "Fei Huang"
            ],
            "title": "mplug-owl: Modularization empowers large language models with multimodality, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Licheng Yu",
                "Patrick Poirson",
                "Shan Yang",
                "Alexander C Berg",
                "Tamara L Berg"
            ],
            "title": "Modeling context in referring expressions",
            "venue": "In Computer Vision\u2013ECCV 2016: 14th European Conference,",
            "year": 2016
        },
        {
            "authors": [
                "Hangjie Yuan",
                "Jianwen Jiang",
                "Samuel Albanie",
                "Tao Feng",
                "Ziyuan Huang",
                "Dong Ni",
                "Mingqian Tang"
            ],
            "title": "Rlip: Relational language-image pre-training for human-object interaction detection",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Hangjie Yuan",
                "Shiwei Zhang",
                "Xiang Wang",
                "Samuel Albanie",
                "Yining Pan",
                "Tao Feng",
                "Jianwen Jiang",
                "Dong Ni",
                "Yingya Zhang",
                "Deli Zhao"
            ],
            "title": "Rlipv2: Fast scaling of relational language-image pretraining",
            "venue": "arXiv preprint arXiv:2308.09351,",
            "year": 2023
        },
        {
            "authors": [
                "Mert Yuksekgonul",
                "Federico Bianchi",
                "Pratyusha Kalluri",
                "Dan Jurafsky",
                "James Zou"
            ],
            "title": "When and why vision-language models behave like bags-of-words, and what to do about it",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Aixi Zhang",
                "Yue Liao",
                "Si Liu",
                "Miao Lu",
                "Yongliang Wang",
                "Chen Gao",
                "Xiaobo Li"
            ],
            "title": "Mining the benefits of two-stage and one-stage hoi detection",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Shilong Zhang",
                "Peize Sun",
                "Shoufa Chen",
                "Min Xiao",
                "Wenqi Shao",
                "Wenwei Zhang",
                "Kai Chen",
                "Ping Luo"
            ],
            "title": "Gpt4roi: Instruction tuning large language model on region-of-interest, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Tiancheng Zhao",
                "Tianqi Zhang",
                "Mingwei Zhu",
                "Haozhan Shen",
                "Kyusong Lee",
                "Xiaopeng Lu",
                "Jianwei Yin"
            ],
            "title": "Vl-checklist: Evaluating pre-trained vision-language models with objects, attributes and relations, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Deyao Zhu",
                "Jun Chen",
                "Xiaoqian Shen",
                "Xiang Li",
                "Mohamed Elhoseiny"
            ],
            "title": "Minigpt-4: Enhancing vision-language understanding with advanced large language models, 2023",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "A remarkable ability of human beings resides in compositional reasoning: the capacity to construct an endless number of novel combinations from a finite set of known components, i.e., \u201cinfinite use of finite means\u201d (Chomsky, 1965; 1957; Montague, 1970). As depicted in Figure 1, for someone who has never witnessed a scene where a person sits on a sulky, it\u2019s not hard to render this conclusion by combining the known components - \u201cman\u201d, \u201cis sitting on\u201d and \u201csulky\u201d. Compositionality is omnipresent in the language such that a sentence is made up of words like nouns (\u201cman\u201d) and verbs (\u201csit\u201d). It also exists ubiquitously in vision so that we could easily detect visual entities such as the person and the sulky, composed with relationships like \u201csit on\u201d. In fact, it\u2019s believed by cognitive scientists that the meaning of a sentence lies in the interaction between an utterance and external situations that can be perceived - the meaning of a noun phrase is linked to a visual entity, and the meaning of a verb phrase is linked to a relational property (Janssen & Partee, 1997). From the meanings of the subject, verb phrase and object, the sentence is built in a systematic and compositional way.\nCurrent Vision-Language Models (VLMs), however, tend to fall short of such compositional abilities (Ma et al., 2023; Cascante-Bonilla et al., 2023; Doveh et al., 2022; Zhao et al., 2023). As noted by recent works, deficiency of compositionality in these VLMs is likely due to the hypothesis that\n1Project page: https://compositionalvlm.github.io\nthey behave like \u201cbag-of-words\u201d (Yuksekgonul et al., 2022) - that they merely memorize by rote the frequent co-occurrences of words, but fail to construct words that could correctly represent objects and the relations between objects. In fact, previous works (Zhao et al., 2023; Cascante-Bonilla et al., 2023) have shown that VLMs struggle a lot when relationships are involved. We can also come to this conclusion from Figure 1, in which the models utilize the shortcut learned from pre-training that \u201ca man sits on a horse\u201d appears frequently and there\u2019s a man and a horse in the image, utterly overlooking the real object, sulky, that the person is sitting on.\nDelving into the architectures of these VLMs and how they infuse images into LLMs, we find that these VLMs deviate from the way human beings perform compositional reasoning from several perspectives. First, they feed one single image as a whole into LLMs and generate language descriptions based on the holistic image embedding. This is inconsistent with object-centric representations in vision, through which the whole image can be constituted by visual entities and more importantly, relationships between the entities. Second, these methods disregard the interaction between the sentence parts and the ingredients in the images. The generation of a new word by the LLM is not linked to a specific visual entity or relationship but is contingent on previous words and holistic image feature instead. Although a series of works have been proposed to strengthen VLMs\u2019 compositional abilities (Doveh et al., 2023), they mainly probe the problem by proposing additional datasets. However, as stated by recent analysis on compositionality (Doveh et al., 2022), collecting specialized large-scale data to teach VL models the missing compositionality is impractical, as finding specialized text-image pairs for each kind and possible value of the visual entities and their relations is rather expensive. In this paper, we approach the essence of this problem from the perspective of model architecture, unveiling a compositional structure of LLM that can conduct step-by-step communication with visual components and relationships.\nWe propose Compositional VLM: Composing Visual Entities and Relationships in Large Language Models, which guides the LLM to explicitly compose visual entities and relationships among the text, and dynamically communicate with the detection network to achieve vision-language communicative decoding. Specifically, we devise a novel set of communication tokens for dynamic interaction and communication between the detection network and the LLM. Communication tokens are generated by the LLM, after the language tokens that denote visual entities or relationships. Upon the generation of communication tokens, a detection network is utilized to decode the regions relevant to the generated language sequence so far, and propose several bounding box proposals. The features of relevant regions are then fed back to LLM by communication tokens, conditioned on which the LLM decodes the subsequent tokens. The bottom-up vision-to-language and top-down language-to-vision communicative decoding are iteratively performed until all words and tokens are generated. The paradigm is shown on the right part of Figure 1.\nWe first evaluate our Compositional VLM on compositional reasoning tasks, including predicting the object entity given the subject and the relationship (ARO (Yuksekgonul et al., 2022)), matching the correct captions describing the relation between two images with similar entities (Cola, (Ray et al., 2023)), and human-object interaction detection (HICO-DET, (Chao et al., 2015)). We outperform baseline VLMs by a large margin (e.g., \u223c 20% in HICO-DET mAP, \u223c 14% in Cola top-1 accuracy,\nand \u223c 3% on ARO top-1 accuracy). We also achieve competitive results on vision-language tasks such as referring expression comprehension and visual question answering."
        },
        {
            "heading": "2 RELATED WORKS",
            "text": ""
        },
        {
            "heading": "2.1 VISION-LANGUAGE MODEL (VLM)",
            "text": "A proliferation of VLMs with remarkable commonsense reasoning abilities have been proposed recently. Among them, Flamingo (Alayrac et al., 2022) employs cross-attention and perceiver sampler to attend to visual contexts and enables visual context learning. BLIP2 (Li et al., 2023) uses a QFormer to attend to salient visual context for better language generation based on the visual context. LLaVA (Liu et al., 2023a) performs image-text alignment first and then conducts instruction finetuning. MiniGPT-4 (Zhu et al., 2023) aligns a frozen visual encoder with LLM using just one projection layer. mPLUG-Owl (Ye et al., 2023) also involves a two-stage method for aligning image and text. There are also recent papers that push VLMs to the 3D domain (Hong et al., 2023).\nRecently, there has been a series of works that utilize LLMs for visual segmentation tasks. Specifically, VisionLLM (Wang et al., 2023) uses an LLM-based decoder which makes predictions about bounding boxes and polygons given language instructions. DetGPT (Pi et al., 2023) is able to interpret human instruction, reason about the visual scene with common sense knowledge, and finally output the objects of interest. GPT4RoI (Zhang et al., 2023) is capable of processing the user instructions that contain interleaved sequences of language and spatial information. LISA (Lai et al., 2023) proposes the embedding-as-mask paradigm to unlock the segmentation capability. However, the vision-language communication of these VLMs is one-way and one-time, merely using language instructions to generate segmentations, or input segmented regions into the LLMs. KOSMOS-2 (Peng et al., 2023) infuses location tokens after visual entities into the language generation process. However, the communication is purely from the language system to the image for segmentation, while the grounded visual regions are not fed back to the language system. Furthermore, none of these VLMs tackle the relations or compositionality in the language inputs. In this paper, we propose Compositional VLM with a set of communication tokens for composing visual entities and relations and communicating between visual and language systems at each step."
        },
        {
            "heading": "2.2 COMPOSITIONALITY IN VISION AND LANGUAGE",
            "text": "Compositionality is a hallmark of human intelligence and plays an indispensable role in vision and language. Previous works exploring the compositionality in vision and language cover a variety of tasks such as visual question answering (Agrawal et al., 2017), generation (Liu et al., 2023b), retrieval (Saito et al., 2023), planning (Ajay et al., 2023) and so on . A set of datasets have been proposed for examining the compositionality of vision-language models (Hudson & Manning, 2019; Johnson et al., 2016; Agrawal et al., 2017; Krishna et al., 2017; Ma et al., 2023). Specifically, the Attribution, Relation, and Order (ARO) benchmark (Yuksekgonul et al., 2022) is a benchmark to systematically evaluate the ability of VLMs to understand different types of relationships, attributes, and order. Recently, VL-Checklist (Zhao et al., 2023) is a framework to evaluate VLM\u2019s abilities of recognizing objects, attributes, and relations. Cola (Ray et al., 2023) analyzes VLMs\u2019 compositional ability in detail and propose a text-to-image retrieval benchmark to compose objects with their relations. Evaluation of VLMs on these benchmarks and metrics show current VLMs struggle with compositionality. Furthermore, a set of works find it particularly frustrating for VLMs when relationships are involved (Conwell & Ullman, 2022; Zhao et al., 2023). In this paper, we specially focus on relational compositionality, with the help of the aforementioned datasets and metrics."
        },
        {
            "heading": "3 COMPOSITONAL VLM",
            "text": "Most state-of-the-art Vision Language Models (VLMs) (e.g., LLaVA (Liu et al., 2023a), Flamingo (Alayrac et al., 2022), BLIP-2 (Li et al., 2023)) take an image and text prompt as inputs, and output a text sequence. Several recent VLMs (e.g., VisionLLM (Wang et al., 2023), LISA (Lai et al., 2023), KOSMOS-2 (Peng et al., 2023)) enable a new ability to output segmentation masks based on the text input. Specifically, KOSMOS-2 generates a location token denoting a discretized bounding box for each visual entity to ground the visual entity to the image. However, the communi-\ncation is purely from the LLM to the image for segmentation, while the grounded visual regions are not fed back to the LLM. In addition, the location tokens are generated after the visual entities, thus failing to assist in the process of generating word tokens based on grounded visual entities and relations. In short, the cut-off between the vision module and the LLM deprives previous VLMs of the crucial visual compositional ability. On the other hand, detection networks like Faster RCNN (Ren et al., 2016) are able to generate region proposals and classify the proposals, but can not interact with the language models.\nIn stark contrast to previous VLMs, our Compositional VLM stands out with its pioneering integration of detection networks into LLM to enable the seamless interaction between the vision module and the LLM, and compositionality over visual entities and relations. As shown in Figure 2, we first devise a set of special communication tokens for flexibly switch between the visual detection module and the LLM. For LLM, we use a pre-trained Pythia model (Biderman et al., 2023) that can handle language tokens as inputs and outputs, as well as visual embeddings and special tokens which are mapped to the same embedding space as the language tokens, to constitute the LLM representations. The vision module consists of an image encoder that produces features to feed into the LLM, and a detection network that proposes region proposals that are relevant to previous language inputs. Topdown language-to-vision communication is achieved by concatenating the last hidden state of the LLM-encoded features to the image embeddings, and input them into the detection network, which proposes relevant regions conditioned on the LLM representations. Bottom-up vision-to-language communication extracts the features of the relevant regions, and concatenates the features back into LLM for further language generation. In this way, the LLM is equipped with visual composionality. We give the details below."
        },
        {
            "heading": "3.1 VISION MODULE",
            "text": "Our vision module consists of two parts: image encoder and detection network.\nImage Encoder. In this paper, we use the CLIP ViT-L model (Radford et al., 2021) for encoding the image. We use a linear mapping layer to map the image embeddings to the same embedding space as the Pythia language embedding space. We then append the image embeddings to the beginning of the language sequence.\nDetection Network. Our detection network is similar to the YOLOX (Ge et al., 2021). The detection network takes as inputs two things: 1) the image embeddings of the whole image (N \u00d7 N \u00d7 D, where N is the patch size and D is the embedding dim); 2) the last hidden state of the LLM so\nfar (1 \u00d7 D). The LLM embedding is expanded and concatenated to the same dim as the image embedding, yielding a final multi-modal embedding of sizeN \u00d7N \u00d7 2D, and send to the detection network. The detection network outputsN \u00d7N \u00d74 bounding boxes andN \u00d7N confidence scores. After non-maximum suppression, we keep a set of bounding boxes as regions of interest (ROIs). To extract the embeddings of one ROI, we extract the features of all patches that are covered by the ROI, and average pool to yield a box embedding of size D. We choose the cropped image features of m bounding boxes with top scores."
        },
        {
            "heading": "3.2 LANGUAGE MODELS",
            "text": "We utilize the pre-trained Pythia model (Biderman et al., 2023) as the backbone of our LLM. In addition to language tokens, we also devise a set of special communication tokens to facilitate compositional vision-language modeling and communication, as is shown in Figure 2. We list the set of tokens below:\n\u2022 <obj>, </obj>: these two tokens enclose a set of language tokens referring to a visual entity \u2022 <visual>: this token is for switching to the vision module after a visual entity token v1 is\ncaptured by LLM, so the vision module could attend to the visual entity\n\u2022 <box>: this token receives the feedback from the vision module, concatenating the image features of detected v1 back into the LLM\n\u2022 <previsual>: this token is for switching to the vision module after a relation r to a previous visual entity v1 is detected (and before the visual entity v2 that is in relation r to v1 is generated).\n\u2022 <prebox>: this token switches back from the vision module after potential regions of v2 are detected, and concatenating the features to better generate the language description of v2.\nThe generation of communication tokens for visual entities and relations enable us to decompose the language sequences into smaller components, where each component connects to the vision module, thus improving compositionality."
        },
        {
            "heading": "3.3 VISION-LANGUAGE COMMUNICATION",
            "text": "The dynamic interaction and communication between the vision module and the language model can be iteratively performed through the special communication tokens introduced above.\nTop-Down Language-to-Vision Communication. Top-down communication is achieved by first generating the <visual> token or <previsual> token. After the token is generated, we summarize the language information generated so far by taking the last hidden state of the LLM. This information gives the vision module a goal or task to attend to, just like the human visual system (Buschman & Miller, 2007). Contingent on the information so far, the vision module then uses the detection network to propose several ROIs, and extracts the features of these ROIs.\nBottom-Up Vision-to-Language Communication. Bottom-up communication is achieved by generating the <box> token or <prebox> token. The ROIs generated by the vision module are then fed back into the LLM to assist further language generation. For instance, if the prebox contains regions relevant to \u201ca bread is on the left of\u201d, the LLM is then capable of absorbing this information and generating \u201csalad\u201d."
        },
        {
            "heading": "3.4 MODEL PRE-TRAINING",
            "text": "Pre-training data. We create a large-scale grounded image-text dataset which consists of over 97M image-text pairs from the pre-training data of BLIP-2 (Li et al., 2023). The images are from a variety of datasets including COCO (Lin et al., 2014), CC3M (Sharma et al., 2018), CC12M (Changpinyo et al., 2021), Visual Genome (Krishna et al., 2017), SBU (Ordonez et al., 2011) and a subset of LAION400M (Schuhmann et al., 2021). Similar to KOSMOS-2 (Peng et al., 2023), we apply a grounding pipeline to the image-text pair to associate the text spans in the caption to their corresponding visual entities in the image. The pipeline consists of three steps: First, we use GroundingDINO (Liu et al., 2023c) to detect objects and its corresponding textual description. Inspired by KOSMOS-2 (Peng et al., 2023), we then apply spaCy (Honnibal et al., 2020) to expand the grounded\nwords to grounded expressions to enrich its linguistic meaning. Finally, we insert communication tokens around the textual description to finalize the grounded data. More details about how we create the pre-training dataset can be found in Appendix.\nPre-training settings. We trained two models: Compositional VLM 1.4B and 2.8B, which uses Pythia-1.4B and Pythia-2.8B as the LLM respectively. Both of them use CLIP ViT-L/14 (Radford et al., 2021) as the image encoder. We load the huggingface checkpoint for these models, and fully fine-tune the whole model during pre-training. More details can be found in Appendix."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "4.1 EVALUATION ON COMPOSITIONAL REASONING TASKS",
            "text": "We aim to probe the model\u2019s ability to reason about entities with detailed attributes in an image and also the relationships between two entities. Descriptions of the three datasets we are evaluating on below, the metric we use for evaluation, and the baselines we used are in Appendix. All experiments are conducted in a zero-shot manner."
        },
        {
            "heading": "4.1.1 ARO",
            "text": "Setup. For our model and other vision-language generative models, we feed the model with the image and text prompt \u201centity A relation\u201d, considering the model output as predicted entity B. For our model, we further insert a <visual> token after entity A and a <previsual> token after relation to encourage the language model to better communicate with the visual branch. For visionlanguage alignment models, we use all 890 candidates entity B to build 890 possible captions in the form of \u201centity A relation entity B\u201d and pick the top-1/top-5 captions that have the highest similarity score with the image as the top-1/top-5 predictions.\nResults. In Table 1, our model achieves superior performance, outperforming all other visionlanguage generative models and alignment models. It indicates that our model has a better ability to understand relations among visual entities in an image, and can better infer one visual entity using information of the presence of other visual entities and their relations. We also notice that the alignment models perform worse in this task. We hypothesize this is because the alignment models are trained using contrastive learning, which makes them behave like bag-of-words (Yuksekgonul et al., 2022). This makes them more easily to be misdirected by other objects in the image and produce the wrong prediction, instead of using the relationship to infer the correct one."
        },
        {
            "heading": "4.1.2 COLA",
            "text": "Setup. For our model and other vision-language generative models, we calculate the perplexity score between a caption and all candidate images and choose the image with lower perplexity as the prediction. Specifically, we feed the model with one image and a caption in the form of \u201centity a relation entity b\u201d. We calculate the average perplexity over all text output. Notably, for our model,\nwe will insert a <visual> and <previsual> tokens after entity a and relation, respectively to encourage vision-language communication. For vision-language alignment models, we directly report the results from Cola (Ray et al., 2023).\nResults. In Table 1, our Compositional VLM significantly outperforms both alignment and generative methods by a large margin. We attribute the performance to the <previsual> token which helps to retrieve the visual information of the entity b for better describing its detailed attributes in text form, thus leading to lower complexity for the ground-truth caption. Also, the <visual> token helps to better localize entity a, allowing the model to better localize the area of entity b according to relation. In Table 2, we also compare our zero-shot re-\nsults with task-specific methods proposed in Ray et al. (2023) which fine-tunes CLIP (Radford et al., 2021) and FLAVA (Singh et al., 2022) on the training data of Cola. Our method still achieves the best performance, demonstrating the superiority of generalization and robustness of our model."
        },
        {
            "heading": "4.1.3 HICO-DET",
            "text": "Setup. For our method and other generative models, we predict HOI (subject, verb, object) in two steps: a) recognizing the interaction categories represented by verb, and b) localizing the subject and object. To determine the existence of interaction, we manually build positive and negative phases with verb, i.e., \u201cthe person is verb\u201d and \u201cthe person is not verb\u201d and calculate their perplexities using the generative model. If the perplexity of the positive phase is lower than the negative one, we consider this verb exists in the image. For all detected verb, we feed the model with the image and text prompt \u201cthe person is verb\u201d to predict object and the location of the person and object. Notably, for our Compositional VLM, we use the inserted <visual> and <previsual> token after the person and verb to predict the locations. Since the output of alignment methods does not contain the object location, we ignore these methods on the HICO-DET dataset.\nResults. Table 1 presents the zero-shot results on HICO-DET test set. Our model significantly outperforms KOSMOS-2 (Peng et al., 2023). We attribute the performance improvement to the <previsual> token that forces the model to communicate with the input image to localize the area of object and predict the text of object. In comparison, KOSMOS-2 only feeds the image information at the beginning of the input, and thus the model may suffer from language prior for predicting a wrong object. In Table 3, we also compare our zero-shot results with the task-specific supervised learning methods. Our model achieves comparable results on the Non-Rare and Full sets. Notably, our zero-shot result exceeds all supervised learning methods in the Rare set, demonstrating the generalization ability of our model."
        },
        {
            "heading": "4.2 EVALUATION ON VISION-LANGUAGE TASKS",
            "text": "In addition to compositional reasoning tasks, we also evaluate object localization and complex vision-language understanding abilities of our model. All experiments are conducted in a zero-shot manner."
        },
        {
            "heading": "4.2.1 REFERRING EXPRESSION COMPREHENSION",
            "text": "Setup. This task requires a model to locate the bounding box of an object specified by language description. We follow KOSMOS-2 (Peng et al., 2023) to use three well-established benchmarks namely RefCOCOg (Mao et al., 2016), RefCOCO+ (Yu et al., 2016) and RefCOCO (Yu et al., 2016). All these datasets use images from COCO (Lin et al., 2014). Both RefCOCO and RefCOCO+ are annotated by a two-player game, where the spatial relations are excluded in RefCOCO+. The RefCOCOg contains longer referring expressions and spatial relations.\nFor our Compositional VLM, we feed the model with <obj>expression</obj><visual> and the <visual> will generate multiple bounding boxes with their confidence scores. Instead of choosing the bounding box with the highest score as a prediction, we use <previsual> to further measure the alignment between the box and expression. Specifically, we select the bounding box with the highest confidence score and subsequently choose additional bounding boxes from the remaining set whose confidence scores exceed a predefined threshold (0.5 times the highest score in our case) as candidates. For each candidate, we feed the model with <previsual><prebox><obj>expression</obj> and put the image feature of the bounding box region into <prebox>. Then we calculate the perplexity of the expression for this bounding box candidate and choose the one with the lowest perplexity as our final prediction.\nResults. In Table 4, our Compositional VLM 2.8B variant performs the best on both val and test sets of RefCOCOg and RefCOCO+, demonstrating its superior localization ability. On RefCOCO, we achieve comparable performance with KOSMOS-2. Note that KOSMOS-2 has been instruction fine-tuned on the data with the same form as the referring expression task, while our Compositional VLM is directly transferred to this new task without any form of instruction fine-tuning.\n4.2.2 VISUAL QUESTION ANSWERING\nSetup. This task requires a model to answer questions about an image. Following BLIP-2 (Li et al., 2023), we evaluate on VQAv2 (Goyal et al., 2017) dataset and report the zeroshot accuracy on test-dev set. We use the prompt \u201cQuestion: {question} Short Answer:\u201d. Results. In Figure 3, our Compositional VLM 2.8B model achieves better performance compared with MetaLM (Hao et al., 2022), VLKD (Dai et al., 2022), OpenFlamingo3B (Awadalla et al., 2023), and KOSMOS2 (Peng et al., 2023), and has a small margin compared with Flamingo3B (Alayrac et al., 2022) and BLIP-2 ViT-L OPT2.7B Li et al. (2023). We hypothesize the accuracy margin may stem from the generative model generating diverse answers that align conceptually with ground truth, yet\nmay not exactly match the annotation, affecting the evaluation. To get a better insight into how well our model performs on VQAv2, we conduct a round of human evaluation for our Compositional VLM 2.8B model and BLIP-2 ViT-L OPT2.7B model on a randomly selected subset with 1000 samples. The human evaluation accuracy for our model is 57.11 while the accuracy for BLIP-2 is 56.62, suggesting that the performance gap between our model and BLIP-2 is negligible."
        },
        {
            "heading": "4.3 ABLATION STUDY",
            "text": "Compared to other VLMs, our Compositional VLM introduces a novel set of communication tokens to improve the compositional reasoning ability. To evaluate the effectiveness of each type of communication token and the effectiveness of the design of bidirectional communication of visual module and language module, we create the following four settings: 1) No communication token at all. 2) No <previsual> and <prebox>, which indicates that there is no communication tokens to compose relationship. 3) No <prebox> and <box>, which indicates that the communication is unidirectional and there is no or very restricted information from vision module to language module. 4) All communication tokens are presented. For each setting, we pre-train our Compositional VLM 1.4B model from scratch under the same hyper-parameters, and is tested under ARO and Cola benchmarks to evaluate the compositional reasoning ability.\nTable 5 shows the evaluation results for these four settings. We summarize the insights as followings:\n1. With no communication tokens, our model\u2019s performance on compositional reasoning tasks is very similar to BLIP-2. This is reasonable because we share the same pre-training data with BLIP2, thus also inheriting a similar performance on downstream tasks.\n2. Generate <visual>/<box> after the object does not help compositional reasoning. It is reasonable because add extra information after the object description won\u2019t help the generation of that object description itself as this is an auto-regressive generation process.\n3. Not put <prebox>/<box> back into the generated sequence will hurt the compositional reasoning ability for complex object description. We can find this insight from the last two rows of the ablation study table. ARO Top-1 accuracy does not hurt much if we do not put <prebox>/<box> back, while the performance for Cola will drop significantly. This is due to the fact that in ARO, the predicted object is usually a simple phrase without any attribute, such as \u201chorse\u201d and \u201ccar\u201d. During training, model can learn to bind this kind of simple concept with the <previsual>/<visual> tokens, so merely generating <previsual>/<visual> is adequate for enhancing the model\u2019s compositional reasoning ability on ARO benchmark. However, the object description in Cola is much more complex, such as \u201cyellow vehicle\u201d and \u201cstanding man\u201d, which requires a close inspection of the visual feature of the object. In this case, <prebox>/<box> tokens which contains more fine-grained information of the visual entity can play an important role to assist LLM to focus on these complex objects and generate more faithful and related tokens thereafter."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "In this paper we propose Compositional VLM: Composing Visual Entities and Relationships in Large Language Models, which guides the LLM to explicitly compose visual entities and relationships among the text, and dynamically communicate with the detection networks to achieve visionlanguage communicative decoding. We outperform previous VLMs by a large margin on compositional reasoning benchmarks (e.g., \u223c 20% in HICO-DET mAP, \u223c 14% in Cola top-1 accuracy, and \u223c 3% in ARO top-1 accuracy). We also achieve competitive performances on vision-language tasks such as referring expression comprehension and VQA. However, we do not cope much with objectattribute compositionality and spatial event compositionality, which are crucial future directions."
        },
        {
            "heading": "A APPENDIX",
            "text": "A.1 PSEUDO ALGORITHM FOR COMMUNICATIVE DECODING\nAlgorithm 1 Communicative Decoding Input: 224x224 RGB image Iraw and text prompt Praw Output: VLM generated tokens S 1: I \u2190 vision encoder(Iraw) 2: P \u2190 text embedding(Praw) 3: S \u2190 empty list 4: S \u2190 Append(S, I) 5: S \u2190 Append(S, P ) 6: repeat 7: X \u2190 Auto-regressively predicted next token of S 8: S \u2190 Append(S,X) 9: if X is <previsual> then 10: H \u2190 last hidden state of <previsual> 11: ROIs\u2190 detector(I,H) 12: for each ROI \u2208 ROIs do 13: IROI \u2190 Crop(I,ROI) 14: <prebox>\u2190MeanPooling(IROI ) 15: S \u2190 Append(S,<prebox>) 16: end for 17: else if X is <visual> then 18: S \u2190 Delete the latest <previsual> and all its corresponding <prebox> in S 19: H \u2190 last hidden state of <visual> 20: ROIs\u2190 detector(I,H) 21: for each ROI \u2208 ROIs do 22: IROI \u2190 Crop(I,ROI) 23: <box>\u2190MeanPooling(IROI ) 24: S \u2190 Append(S,<box>) 25: end for 26: end if 27: until X is EOS token\nAlgorithm 1 describes how our communicative decoding works. Our model takes an RGB image with a resolution of 224x224 and a text prompt as input. The RGB image will first be encoded to an image embedding by the vision encoder and the text prompt will be encoded by the LLM\u2019s text embedding. Then, we put the image embedding in front of the encoded prompt to form a multimodal prompt, and we feed this multimodal prompt into LLM for auto-regressive generation.\nThe generation process are mostly the same as the LLM\u2019s original auto-regressive next token prediction process, except that we include an iterative and automatic insertion of communication tokens to facilitate the compositional reasoning ability. <previsual> is automatically generated by LLM via next token prediction, and arbitrary number of ROIs can be detected through the object detector module. We set the confidence score threshold to be 0.05 and retain all ROIs that has a confidence score higher than 0.05. For each ROI, we crop that region from image embedding, and perform a mean pooling to get a token, namely <prebox>, that contains the visual information of the ROI. All <prebox>s are automatically inserted into the token sequences so that the future generation process can use the information of these tokens. The generation of <visual> and <box> are similar to <previsual> and <prebox>. In order to prevent the localization information leak induced by the latest <previsual> and <prebox>, we delete the latest <previsual> and <prebox> before we use the last hidden state of the new <visual> to detect objects.\nA.2 PRE-TRAINING DATASET\nSimilar to KOSMOS-2 (Peng et al., 2023), we adopt a pipeline making use of out-of-the-box open vocabulary detector to create our grounded pre-training dataset. It conssts of three steps:\nStep-1: Generating bounding-box-word pairs. We use GroundingDINO (Liu et al., 2023c) to detect objects in the image and link the bounding box of the object to words in the text. We keep bounding boxes whose highest similarities are higher than 0.35 and extract the words whose similarities are higher than 0.25 as the words that correspond to a bounding box. Non-maximum suppression algorithm is applied to eliminate bounding boxes that have a high overlap with other bounding boxes linked to the same word.\nStep-2: Expanding grounded words to grounded expressions. In practice, we observe that GroundingDINO often fail to link the whole referring expressions to an object in the image. For example, for the expression \u201cman with a hat on his head\u201d, GroundingDINO will only link \u201cman\u201d to the person in the image, but not the whole expression. This will limit the model\u2019s ability to understand complicated expressions. Inspired by KOSMOS-2 (Peng et al., 2023), we apply spaCy (Honnibal et al., 2020) to obtain each word\u2019s dependency relation in the sentence, and expand a grounded word to a grounded expression by recursively traversing the dependency tree of that word and concatenate eligible children words based on the linguistic rules.\nStep-3: Assigning bounding boxes to the special communication tokens. Given the expressions and their associated bounding boxes in a grounded image-text pair, we can now insert the special communication tokens into the text and assign the bounding boxes to them. We follow KOSMOS2 (Peng et al., 2023) to enclose the expression in <obj>/</obj> pair, and then add communication tokens around them. For a given expression with a single bounding box, the resulted input sequence for that expression is either in the form of \u201c<obj>expression</obj><visual><box>\u201d or \u201c<previsual><prebox><obj>expression</obj>\u201d depending on the position of the expression in the sentence. If an expression is associated with multiple bounding boxes, we add multiple <prebox> or <box>. If it is the first expression in the sentence, we use the form with a trailing <visual> token. Otherwise, we randomly select one from these two available forms.\nA.3 PRE-TRAINING DETAILS\nApart from the grounded image-text pair dataset we created, we also use The Pile (Gao et al., 2020) as part of our pre-training dataset. The total pre-training loss consists of the language modeling loss and the detection loss, with a loss weight of 0.025 for the detection loss. We pre-train for 20k steps and use a batch size of 2,304 for grounded image-text data and a batch size of 2,304 for The Pile data. AdamW (Loshchilov & Hutter, 2017) optimizer is employed with a learning rate of 1.0e\u22124 and \u03b2 = (0.9, 0.999). We do not apply weight decay to the weights of LLM and CLIP, but apply a weight decay of 0.05 to the detection network.\nA.4 COMPOSITIONAL EVALUATION DATASET\nDatasets and Metrics. We conduct experiments on ARO (Yuksekgonul et al., 2022), Cola (Ray et al., 2023), and HICO-DET (Chao et al., 2015) datasets.\n\u2022 ARO contains 23,937 testing images. One entity pair in an image is annotated as a tuple (entity A, relation, entity B). Given entity A and relation, the model is required to predict entity B out of all candidate objects. We use top-1 and top-5 accuracy as evaluation metrics.\n\u2022 Cola contains 420 testing images, where each image is paired with a caption describing the relation between two entities similar to ARO. The entity in Cola is described with more attribute details (e.g., texture, color, and size) which require the model to perform more fine-grained object recognition. We conduct experiments on the multi-object part of this dataset, where two images with similar entities and their corresponding captions are paired as a testing sample. The model is required to correctly match the corresponding caption for both two images. We report the accuracy following Cola (Ray et al., 2023).\n\u2022 HICO-DET contains 9,658 testing images, with 600 HOI categories constructed by 80 object categories and 117 verb classes. Each instance of human-object interaction is represented as a triplet, denoted as (subject, verb, object), accompanied by their respective bounding boxes. The model is required to not only recognize the HOI categories but also localize the subject and object. Following Chao et al. (2015), we report the mean AP (mAP) on 3 splits, namely a) Rare: 138 HOI categories with less than 10 training instances, b) Non-Rare: the remaining 462 HOI categories, and c) Full: all 600 HOI categories.\nBaselines. We mainly compare our Compositional VLM with two types of methods, namely the vision-language alignment model (i.e., CLIP (Radford et al., 2021), FLAVA (Singh et al., 2022)) and the vision-language generative model (i.e., OpenFlamingo (Awadalla et al., 2023), BLIP-2 (Li et al., 2023) and KOSMOS-2 (Peng et al., 2023)). The vision-language alignment models learn a vision encoder and a language encoder, which pull close the features of paired image and text while pushing away the unpaired one. The vision-language generative model takes as input the image and text and auto-regressively generates a sequence of text.\nA.5 VISULIZATION OF COMPOSITIONAL REASONING RESULTS\nWe show the visualization results for our model, BLIP-2 and KOSMOS-2 on the three compositional reasoning tasks.\nFigure 5 shows the quantitative results for ARO. Compared to BLIP-2 and KOSMOS-2, we can rank the ground truth object higher thanks to the <previsual> token to localize the object.\nFigure 6 shows the quantitative results for Cola. The presence of the <prebox> token can encode the visual feature of the ROI, helping the model better understand the attribute of the objects due to its zoom-in effect. Also, relationship can be explicitly used by <previsual> token to better infer the visual entity.\nFigure 7 shows the comparison of our model and KOSMOS-2 on the HICO-DET task. In general our model gives more accurate bounding box for object localization. Also, when there are two identical objects in the image, i.e., the bottom-right example in Figure 7, our model can take advantage of the <previsual> token which can guide our model to pay attention to the region of the correct object (the bench that the person is lying on), instead of the wrong object (the bench that behind the person).\nFigure 8 illustrates the performance consistency of our model across diverse and ambiguous scenarios. Specifically, in instances where the expression inputs exhibit ambiguity \u2014 such as having multiple boxes corresponding to the same text \u2014 our model generates all potential bounding boxes with scores surpassing the threshold.\nFigure 9 depicts the functioning of the model in the image captioning task. Notably, when the model produces nouns with modifiers, such as \u201dan old man\u201d \u201dA red fire hydrant\u201d and \u201da glass of champagne\u201d it considers them as integral units and automatically generates \u201d<obj>an old man</obj>,\u201d \u201d<obj>A red fire hydrant</obj>,\u201d \u201d<obj>a glass of champagne</obj>,\u201d instead of parsing the objects individually, for instance, \u201dan old <obj>man</obj>,\u201d \u201dA red <obj>fire hydrant</obj>.\u201d\nFigure 10 delineates instances of model failures. Various texts were input under the same picture to obtain the output. As depicted in (a), our model proficiently recognizes normal-sized objects. However, challenges arise when dealing with minuscule or blurred objects, such as the \u201dspoon\u201d depicted in the picture, making it difficult for the model to yield the desired results. Moreover, as illustrated in (b), the model successfully identifies aligned objects of the same kind in the picture. While our model adeptly produces corresponding bounding boxes when multiple objects of the same kind are neatly arranged, the scenario becomes more challenging in crowded scenes or when the object arrangement is irregular, often resulting in the model\u2019s inability to output all the bounding boxes."
        }
    ],
    "year": 2023
}