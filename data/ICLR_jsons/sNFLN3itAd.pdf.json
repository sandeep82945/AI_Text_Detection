{
    "abstractText": "In this work, we propose a novel link prediction model and further boost it by studying graph incompleteness. First, We introduce MPNN-then-SF, an innovative architecture leveraging structural feature (SF) to guide MPNN\u2019s representation pooling, with its implementation, namely Neural Common Neighbor (NCN). NCN exhibits superior expressiveness and scalability compared with existing models, which can be classified into two categories: SF-then-MPNN, augmenting MPNN\u2019s input with SF, and SF-and-MPNN, decoupling SF and MPNN. Second, we investigate the impact of graph incompleteness\u2014the phenomenon that some links are unobserved in the input graph\u2014on SF, like the common neighbor. Through dataset visualization, we observe that incompleteness reduces common neighbors and induces distribution shifts, significantly affecting model performance. To address this issue, we propose to use a link prediction model to complete the common neighbor structure. Combining this method with NCN, we propose Neural Common Neighbor with Completion (NCNC). NCN and NCNC outperform recent strong baselines by large margins, and NCNC further surpasses state-of-the-art models in standard link prediction benchmarks.",
    "authors": [],
    "id": "SP:712d0feadca572c3382c2290146363a5236ba5b6",
    "references": [
        {
            "authors": [
                "Lada A Adamic",
                "Eytan Adar"
            ],
            "title": "Friends and neighbors on the web",
            "venue": "Social networks,",
            "year": 2003
        },
        {
            "authors": [
                "Takuya Akiba",
                "Shotaro Sano",
                "Toshihiko Yanase",
                "Takeru Ohta",
                "Masanori Koyama"
            ],
            "title": "Optuna: A next-generation hyperparameter optimization framework",
            "venue": "In KDD,",
            "year": 2019
        },
        {
            "authors": [
                "Albert-L\u00e1szl\u00f3 Barab\u00e1si",
                "R\u00e9ka Albert"
            ],
            "title": "Emergence of scaling in random",
            "venue": "networks. science,",
            "year": 1999
        },
        {
            "authors": [
                "Benjamin Paul Chamberlain",
                "Sergey Shirobokov",
                "Emanuele Rossi",
                "Fabrizio Frasca",
                "Thomas Markovich",
                "Nils Hammerla",
                "Michael M. Bronstein",
                "Max Hansmire"
            ],
            "title": "Graph neural networks for link prediction with subgraph sketching",
            "year": 2023
        },
        {
            "authors": [
                "Kaiwen Dong",
                "Yijun Tian",
                "Zhichun Guo",
                "Yang Yang",
                "Nitesh V. Chawla"
            ],
            "title": "Fakeedge: Alleviate dataset shift in link prediction",
            "venue": "In LoG,",
            "year": 2022
        },
        {
            "authors": [
                "Matthias Fey",
                "Jan Eric Lenssen"
            ],
            "title": "Fast graph representation learning with pytorch geometric",
            "venue": "arXiv preprint arXiv:1903.02428,",
            "year": 2019
        },
        {
            "authors": [
                "Justin Gilmer",
                "Samuel S Schoenholz",
                "Patrick F Riley",
                "Oriol Vinyals",
                "George E Dahl"
            ],
            "title": "Neural message passing for quantum chemistry",
            "venue": "In ICML,",
            "year": 2017
        },
        {
            "authors": [
                "Aditya Grover",
                "Jure Leskovec"
            ],
            "title": "node2vec: Scalable feature learning for networks",
            "venue": "In KDD, pp. 855\u2013864",
            "year": 2016
        },
        {
            "authors": [
                "William L Hamilton",
                "Rex Ying",
                "Jure Leskovec"
            ],
            "title": "Inductive representation learning on large graphs",
            "venue": "NeurIPS, pp",
            "year": 2017
        },
        {
            "authors": [
                "Weihua Hu",
                "Matthias Fey",
                "Marinka Zitnik",
                "Yuxiao Dong",
                "Hongyu Ren",
                "Bowen Liu",
                "Michele Catasta",
                "Jure Leskovec"
            ],
            "title": "Open graph benchmark: Datasets for machine learning on graphs",
            "venue": "NeurIPS,",
            "year": 2020
        },
        {
            "authors": [
                "Thomas N. Kipf",
                "Max Welling"
            ],
            "title": "Variational graph auto-encoders",
            "venue": "CoRR, abs/1611.07308,",
            "year": 2016
        },
        {
            "authors": [
                "Thomas N. Kipf",
                "Max Welling"
            ],
            "title": "Semi-supervised classification with graph convolutional networks",
            "venue": "In ICLR,",
            "year": 2017
        },
        {
            "authors": [
                "David Liben-Nowell",
                "Jon Kleinberg"
            ],
            "title": "The link prediction problem for social networks",
            "venue": "In International conference on Information and knowledge management,",
            "year": 2003
        },
        {
            "authors": [
                "Adam Paszke",
                "Sam Gross",
                "Francisco Massa",
                "Adam Lerer",
                "James Bradbury",
                "Gregory Chanan",
                "Trevor Killeen",
                "Zeming Lin",
                "Natalia Gimelshein",
                "Luca Antiga"
            ],
            "title": "Pytorch: An imperative style, highperformance deep learning",
            "venue": "library. NeurIPS,",
            "year": 2019
        },
        {
            "authors": [
                "Bryan Perozzi",
                "Rami Al-Rfou",
                "Steven Skiena"
            ],
            "title": "Deepwalk: online learning of social representations",
            "venue": "In KDD,",
            "year": 2014
        },
        {
            "authors": [
                "E. Amiri Souri",
                "Roman Laddach",
                "S.N. Karagiannis",
                "Lazaros G. Papageorgiou",
                "Sophia Tsoka"
            ],
            "title": "Novel drug-target interactions via link prediction and network embedding",
            "venue": "BMC Bioinform.,",
            "year": 2022
        },
        {
            "authors": [
                "Jian Tang",
                "Meng Qu",
                "Mingzhe Wang",
                "Ming Zhang",
                "Jun Yan",
                "Qiaozhu Mei"
            ],
            "title": "LINE: large-scale information network embedding",
            "year": 2015
        },
        {
            "authors": [
                "Petar Velickovic",
                "Guillem Cucurull",
                "Arantxa Casanova",
                "Adriana Romero",
                "Pietro Li\u00f2",
                "Yoshua Bengio"
            ],
            "title": "Graph attention networks",
            "venue": "In ICLR,",
            "year": 2018
        },
        {
            "authors": [
                "Zhitao Wang",
                "Yong Zhou",
                "Litao Hong",
                "Yuanhang Zou",
                "Hanjing Su",
                "Shouzhi Chen"
            ],
            "title": "Pairwise learning for neural link prediction",
            "year": 2021
        },
        {
            "authors": [
                "Zixiao Wang",
                "Yuluo Guo",
                "Jin Zhao",
                "Yu Zhang",
                "Hui Yu",
                "Xiaofei Liao",
                "Hai Jin",
                "Biao Wang",
                "Ting Yu"
            ],
            "title": "GIDN: A lightweight graph inception diffusion network for high-efficient link prediction",
            "year": 2022
        },
        {
            "authors": [
                "Keyulu Xu",
                "Weihua Hu",
                "Jure Leskovec",
                "Stefanie Jegelka"
            ],
            "title": "How powerful are graph neural networks",
            "venue": "In ICLR,",
            "year": 2019
        },
        {
            "authors": [
                "Haotong Yang",
                "Zhouchen Lin",
                "Muhan Zhang"
            ],
            "title": "Rethinking knowledge graph evaluation under the open-world assumption",
            "year": 2022
        },
        {
            "authors": [
                "Zhilin Yang",
                "William W. Cohen",
                "Ruslan Salakhutdinov"
            ],
            "title": "Revisiting semi-supervised learning with graph embeddings",
            "venue": "In Proceedings of the 33nd International Conference on Machine Learning,",
            "year": 2016
        },
        {
            "authors": [
                "Seongjun Yun",
                "Seoyoon Kim",
                "Junhyun Lee",
                "Jaewoo Kang",
                "Hyunwoo J. Kim"
            ],
            "title": "Neo-gnns: Neighborhood overlap-aware graph neural networks for link prediction",
            "venue": "In NeurIPS,",
            "year": 2021
        },
        {
            "authors": [
                "Muhan Zhang",
                "Yixin Chen"
            ],
            "title": "Link prediction based on graph neural networks. NeurIPS",
            "year": 2018
        },
        {
            "authors": [
                "Muhan Zhang",
                "Yixin Chen"
            ],
            "title": "Inductive matrix completion based on graph neural networks",
            "venue": "In ICLR,",
            "year": 2020
        },
        {
            "authors": [
                "Muhan Zhang",
                "Pan Li",
                "Yinglong Xia",
                "Kai Wang",
                "Long Jin"
            ],
            "title": "Labeling trick: A theory of using graph neural networks for multi-node representation learning",
            "venue": "NeurIPS,",
            "year": 2021
        },
        {
            "authors": [
                "Jianan Zhao",
                "Qianlong Wen",
                "Mingxuan Ju",
                "Chuxu Zhang",
                "Yanfang Ye"
            ],
            "title": "Self-supervised graph structure refinement for graph neural networks",
            "venue": "In WSDM,",
            "year": 2023
        },
        {
            "authors": [
                "Tong Zhao",
                "Yozen Liu",
                "Leonardo Neves",
                "Oliver J. Woodford",
                "Meng Jiang",
                "Neil Shah"
            ],
            "title": "Data augmentation for graph neural networks",
            "venue": "In AAAI,",
            "year": 2021
        },
        {
            "authors": [
                "Tao Zhou",
                "Linyuan L\u00fc",
                "Yi-Cheng Zhang"
            ],
            "title": "Predicting missing links via local information",
            "venue": "The European Physical Journal B,",
            "year": 2009
        },
        {
            "authors": [
                "Zhaocheng Zhu",
                "Zuobai Zhang",
                "Louis-Pascal A.C. Xhonneux",
                "Jian Tang"
            ],
            "title": "Neural bellmanford networks: A general graph neural network framework for link prediction",
            "venue": "In NeurIPS,",
            "year": 2021
        },
        {
            "authors": [
                "B. Specifically",
                "BUDDY (Chamberlain"
            ],
            "title": "2023) uses a simplified MPNN with ndF in B. Moreover, Neo-GNN needs to precompute high order graph A, which takes O(nd) time and space",
            "year": 2023
        }
    ],
    "sections": [
        {
            "text": "1 INTRODUCTION\nLink prediction is a crucial task in graph machine learning, finding applications in various domains, such as recommender systems (Zhang & Chen, 2020), knowledge graph completion (Zhu et al., 2021), and drug interaction prediction (Souri et al., 2022). Graph Neural Networks (GNNs) have gained prominence in link prediction tasks, with Graph Autoencoder (GAE) (Kipf & Welling, 2016) being a notable representation. GAE utilizes Message Passing Neural Network (MPNN) (Gilmer et al., 2017) representations of two individual target nodes to predict link existence. However, Zhang et al. (2021) point out a limitation in GAE: it overlooks pairwise relations between target nodes. For example, in Figure 1, GAE always produces the same prediction for two links (v1, v2) and (v1, v3) despite their differing pairwise relationships, because MPNN generates identical representations for nodes v2, v3 due to graph symmetry. Nevertheless, the two links have different structural features. For example, v1 and v2 have a common neighbor v4,\nwhile v1 and v3 do not have any. Therefore, various methods combine structural feature (SF) and MPNN for better expressivity and have dominated the link prediction task (Zhang et al., 2021; Yun et al., 2021; Chamberlain et al., 2023).\nHowever, models combining SF and MPNN still have much room for improvement. They can generally be concluded into two architectures: SF-then-MPNN and SF-and-MPNN, as shown in Figure 2. SEAL (Zhang & Chen, 2018) adds target-link-specific hand-crafted features to the node features of the input graphs of MPNN, whose output node representations are then pooled to produce link representations. SEAL belongs to the SF-then-MPNN architecture, which leverages SF to augment the input graph of MPNN. Though SF-then-MPNN models achieve provably high expressivity (Zhang et al., 2021), they require running MPNN on a different graph for each target link, resulting in significant computational overhead.\nIn contrast, Neo-GNN (Yun et al., 2021) and BUDDY (Chamberlain et al., 2023) decouple the structural feature from MPNN. They directly incorporate manually created pairwise features with individual node representations produced by MPNN as link representations, necessitating only a single run of MPNN on the original graph. These models fall under the SF-and-MPNN category, where structural features and MPNN are independent. Such methods have limited expressivity. For example, SEAL can capture the representations of common neighbor nodes, while BUDDY can only count the number of common neighbors.\nTo solve the drawback of the two architectures above, we propose MPNN-then-SF architecture, which initially applies MPNN to the original graph and then uses structural features to guide the pooling of node representations. This approach offers strong expressivity and scalability: similar to SF-then-MPNN models, MPNN-then-SF can capture the node features of common neighbors, and similar to SF-and-MPNN models, it runs MPNN only once for all target links. We introduce the Neural Common Neighbor (NCN) as an instantiation of the MPNN-then-SF architecture. In experiments, NCN outperforms existing models in both scalability and performance.\nFurthermore, since NCN heavily relies on common neighbor structure, which is significantly affected by graph incompleteness, we also investigate the impact of incompleteness. Graph incompleteness is ubiquitous in link prediction tasks because the goal is to predict unobserved edges not present in the input graph. We empirically observe that incompleteness reduces the number of common neighbor and leads to a shift in the distribution of common neighbor between the training and test sets. These phenomena collectively lead to performance degradation. To mitigate this issue, we first employ NCN to complete the common neighbor structure and then apply NCN to the completed graph. In experiments, this method significantly improves the performance of NCN.\nIn conclusion, our contributions are as follows: \u2022 We introduce the Neural Common Neighbor (NCN) for link prediction using the MPNN-then-SF\narchitecture, demonstrating superior performance and scalability compared to existing models. \u2022 We analyze the impact of graph incompleteness and propose Neural Common Neighbor with\nCompletion (NCNC), which completes the input common neighbor structure and applies NCN to the completed graph. NCNC outperforms state-of-the-art models."
        },
        {
            "heading": "2 PRELIMINARY",
            "text": "We consider an undirected graph G = (V,E,A,X), where V = {1, 2, . . . , n} represents a set of n nodes, E \u2286 V \u00d7 V denotes the set of edges, X \u2208 Rn\u00d7F is a node feature matrix whose v-th row Xv is the feature of node v, and adjacency matrix A \u2208 Rn\u00d7n is a symmetric matrix, whose (u, v) element is 1 if (u, v) \u2208 E and 0 otherwise. The degree of node u is d(u,A) := \u2211n v=1 Auv . Node u\u2019s neighbors are nodes connected to u, N(u,A) := {v|v \u2208 V,Auv > 0}. For simplicity of notations, we use N(u) to denote N(u,A) when A is fixed. Common neighbor means nodes connected to both i and j: N(i) \u2229N(j).\nHigh Order Neighbor. We define Al as a high-order adjacency matrix, where Aluv represents the number of walks of length l between nodes u and v in graph A. N(u,Al) = {v|v \u2208 V,Aluv > 0} denotes the set of nodes connected to u by a walk of length l in graph A, equivalent to the neighbors in higher-order adjacency. Nl(u,A) denotes the set of nodes whose shortest path distance to u in graph A is l. Existing works define high-order neighbors as either N(u,Al) or Nl(u,A). More generally, the neighborhood of u can be expressed as Nl1(u,A\nl2), returning all nodes with a shortest path distance of l1 to u in the high-order graph Al2 . We use N l2l1 (u) to denote Nl1(u,A l2) when A is fixed. Given a target link (i, j), their general neighborhood overlap is given by N l2l1 (i) \u2229N l\u20322 l\u20321 (j), and their neighborhood difference is given by N l2l1 (i)\u2212N l\u20322 l\u20321 (j).\nMessage Passing Neural Network (MPNN). Comprising message passing layers, MPNN (Gilmer et al., 2017) is a common GNN framework. The kth layer is as follows.\nh(k)v = U (k)(h(k\u22121)v ,AGG({M (k)(h(k\u22121)v ,h(k\u22121)u )|u \u2208 N(v)})), (1)\nwhere h(k)v is the representation of node v at the kth layer, U (k),M (k) are functions like multi-layer perceptron (MLP), and AGG denotes an aggregation function like sum or max. The initial node representation h(0)v is node feature Xv . In each message passing layer, information is aggregated from neighbors to update the node representation. The final node representations produced by MPNN are the output of the last message passing layer, denoted as MPNN(v,A,X) = h(K)v ."
        },
        {
            "heading": "3 RELATED WORK",
            "text": ""
        },
        {
            "heading": "3.1 LINK PREDICTION MODEL",
            "text": "There are three primary categories of link prediction models. Node embedding methods (Perozzi et al., 2014; Grover & Leskovec, 2016; Tang et al., 2015) map each node to an embedding vector and combine the embeddings of target nodes to predict link. Link prediction heuristics (LibenNowell & Kleinberg, 2003; Baraba\u0301si & Albert, 1999; Zhou et al., 2009; Adamic & Adar, 2003) develop hand-crafted structural features. GNNs utilize Graph Neural Networks to predict link existence. Among these GNNs, Graph Autoencoder (GAE) (Kipf & Welling, 2016) uses the inner product of the MPNN representations of two target nodes, \u27e8MPNN(i, A,X),MPNN(j, A,X)\u27e9, as the representations of link (i, j). It uses MPNN only and thus fails to capture pairwise relations between nodes. In contrast, various GNNs combining MPNN and structural features (Zhang & Chen, 2018; Yun et al., 2021; Chamberlain et al., 2023) have achieved state-of-the-art performance. Take SEAL (Zhang & Chen, 2018) as an example. For a target link (i, j), SEAL initially augments node feature X by concatenating each node\u2019s shortest path distance to (i, j) and extracts a k-hop subgraph from the whole graph, generating augmented node feature X \u2032 and adjacency A\u2032, respectively. Subsequently, SEAL applies MPNN to this subgraph and use the sum of node representations within it, \u2211 u MPNN(u,A\n\u2032, X \u2032), as the target link representation. Other models employ a distinct approach to incorporate structural features. Neo-GNN (Yun et al., 2021) and BUDDY (Chamberlain et al., 2023), for instance, directly apply MPNN to the original graph and concatenate structural features, such as the count of common neighbors, with the Hadamard product of target node MPNN representations, MPNN(i, A,X)\u2299 MPNN(j, A,X)||structural features."
        },
        {
            "heading": "3.2 STRUCTURAL FEATURE",
            "text": "Structural features for link prediction vary among models but are generally based on neighborhood overlap. Notable heuristics, such as Common Neighbor (CN), Resource Allocation (RA), and Adamic Adar (AA), use first-order common neighbors to compute scores for a target link (i, j):\nCN(i, j) = \u2211\nu\u2208N(i)\u2229N(j)\n1, RA(i, j) = \u2211\nu\u2208N(i)\u2229N(j)\n1\nd(u) , AA(i, j) = \u2211 u\u2208N(i)\u2229N(j)\n1\nlog d(u) . (2)\nGNN Neo-GNN HO \u2229 x MLPBUDDY HO \u2229&\u2212 1 1\nNeo-GNN (Yun et al., 2021) and BUDDY (Chamberlain et al., 2023) extend these heuristics by utilizing higher-order neighbors. NeoGNN computes features for high-order neighborhood overlap N l1(i), N l2(j) as follows,\u2211\nu\u2208N l11 (i)\u2229N l2 1 (j)\nAl1iuA l2 juf(d(u)), (3)\nwhere f is a learnable function of node degree d(u). BUDDY (Chamberlain et al., 2023) further utilize high-order neighborhood difference. It computes overlap features {al1,l2(i, j)|l1, l2 = 1, 2, ..., k} and difference features {bl(i, j), bl(j, i)|l = 1, 2, ..., k} as follows:\nal1,l2(i, j) = \u2211\nu\u2208N1 l1 (i)\u2229N1 l2 (j)\n1, bl(i, j) = \u2211\nu\u2208N1 l (i)\u2212 \u22c3k l\u2032=1 N 1 l\u2032 (j)\n1. (4)\nAll these pairwise features can be summarized into the following framework.\u2211 u\u2208Nl2\nl1 (i)\u2295N l\u20322 l\u20321 (j)\ng(Al2iu)g(A l\u20322 ju)f(d(u)), (5)\nwhere N l2l1 (i) and N l\u20322 l\u20321 (j) denote the general neighborhood of i and j, \u2295 is a set operator like intersection or difference, and f, g are node degree and high-order adjacency weight functions, respectively. Details on how this framework unify existing structure features are shown in Table 1."
        },
        {
            "heading": "3.3 INCOMPLETENESS OF GRAPH",
            "text": "The primary aim of the link prediction task is to forecast unobserved edges, inherently making the input graph incomplete. Nevertheless, graph incompleteness can significantly impact structural features, such as common neighbors, and models based on them. This issue has drawn attention in some prior works. Yang et al. (2022) examined how unobserved links could distort evaluation scores, with a specific focus on metrics and benchmark design, while our research concentrates on model design. Dong et al. (2022) explored the consequences of the presence of target links, whereas our emphasis lies in understanding how incompleteness affects common neighbor-based features. Outside the domain of link prediction, Zhao et al. (2023) and Zhao et al. (2021) add edges predicted by GAE to the input graph of GNNs. However, their primary objective was node classification tasks, aiming to enhance edges between nodes of the same class while diminishing others. In contrast, our research addresses distribution shifts and information loss stemming from graph incompleteness, offering unique completion methods and insights tailored for link prediction."
        },
        {
            "heading": "4 NEURAL COMMON NEIGHBOR",
            "text": "Structural features (SF), such as common neighbors, are commonly employed in link prediction models. Existing approaches combine SF with Message Passing Neural Networks (MPNN) in two manners (illustrated in Figure 2): SF-then-MPNN and SF-and-MPNN. However, these approaches exhibit limitations in terms of either scalability or expressivity. To address these issues comprehensively, we introduce a novel architecture, MPNN-then-SF, which offers a unique blend of high expressivity and scalability. Subsequently, we present a concrete instantiation of this architecture, Neural Common Neighbor (NCN). All proofs for theorems in this section are in Appendix A."
        },
        {
            "heading": "4.1 NEW ARCHITECTURE COMBINING MPNN AND SF",
            "text": "In Figure 2, we categorize existing methods into two architectures:\n\u2022 SF-then-MPNN: This category includes SEAL (Zhang & Chen, 2018) and NBFNet (Zhu et al., 2021). In this approach, the input graph is initially enriched with structural features and then fed into the MPNN, which allows MPNN to leverage SF and have provable expressivity (Zhang & Chen, 2018). However, the drawback is that structural features change with target link, necessitating MPNN to be re-run for each link, resulting in lower scalability.\n\u2022 SF-and-MPNN: This category encompasses models like NeoGNN (Yun et al., 2021) and BUDDY (Chamberlain et al., 2023). Here, MPNN takes the original graph as input and runs only once for all target links, leading to high scalability. However, SF are directly concatenated to the final representations and thus detached from MPNN, leading to reduced expressivity.\nFrom these two architectural paradigms, it becomes apparent that feeding the original graph to MPNN is essential for achieving high scalability. Moreover, the coupling between SF and MPNN remains\na crucial factor for expressivity. Thus, we introduce a new architecture: MPNN-then-SF. This approach initially runs MPNN on the original graph and then employs structural features to guide the\npooling of MPNN features, requiring only one MPNN run and enhancing expressivity. The specific representation of the target link (i, j) is as follows:\nPool({MPNN(u,A,X)|u \u2208 S}), (6) where Pool is a pooling function mapping a multiset of node representations to a single set representation, and S is a node set related to the target link. Multiple node sets can be used in conjunction to produce concatenated representations. This flexible framework can express various models. When using target nodes i and j as S and Hadamard product as Pool, it can express GAE:\nMPNN(i, A,X)\u2299 MPNN(j, A,X). (7) Alternatively, we can choose S as combinations of high-order neighbor sets of i and j, leading to the following form (see Appendix A.1 for the detailed derivation.):\u2211\nu\u2208Nl2 l1 (i)\u2295N l\u20322 l\u20321 (j)\ng(Al2iu)g(A l\u20322 ju)MPNN(u,A,X), (8)\nwhere g is a function transforming the edge weight in high-order adjacency matrix. This framework exhibits stronger expressivity than existing SF-and-MPNN models. Theorem 1. Combination of Equation 7 and Equation 8 are strictly more expressive than MPNNonly model: GAE, SF-only models: CN, RA, AA, and MPNN-and-SF models: Neo-GNN, BUDDY.\nA key factor contributing to its higher expressivity is the coupling of MPNN and SF. While SFand-MPNN typically only counts the number of common neighbors, MPNN-then-SF, similar to SFthen-MPNN, can capture node properties of these common neighbors. As shown in Figure 3, node tuples (v1, v2) and (v1, v3) have the same number of common neighbors. However, their common neighbors have different node features, allowing MPNN-then-SF and SF-then-MPNN to distinguish them, a capability that SF-and-MPNN lacks."
        },
        {
            "heading": "4.2 NEURAL COMMON NEIGHBOR",
            "text": "We will now present an implementation for the MPNN-then-SF framework. Notably, the previous models NeoGNN (Yun et al., 2021) and BUDDY (Chamberlain et al., 2023) all incorporate higher-order neighbors into their architectures, resulting in significant performance improvements. Surprisingly, in our experiments, we observed that the gains achieved by explicitly considering higher-order neighbors were marginal once we introduced MPNN into the framework (as discussed in Section 6.3). We speculate that this marginal improvement arises because MPNN implicitly learns information related to higher-order neighbors. Therefore, considering scalability, we opt to utilize only the target nodes and their first-order common neighbors as the node set, leading to the development of our NCN model:\nNCN(i, j, A,X) = MPNN(i, A,X)\u2299 MPNN(j, A,X)|| \u2211\nu\u2208N(i)\u2229N(j)\nMPNN(u,A,X) (9)\nwhere g(Aiu) and g(Aju) are constants and ignored, and || denotes concatenation. It has high expressivity. Theorem 2. NCN is strictly more expressive than GAE, CN, RA, AA. Moreover, Neo-GNN and BUDDY are not more expressive than NCN.\nTo elaborate, in certain scenarios where the properties of common neighbors hold significant importance, NCN outperforms both BUDDY and Neo-GNN in expressiveness.\nAs our first major contribution, NCN represents a straightforward yet potent model for combining structural features and MPNNs. It operates as an implicit high-order model by aggregating firstorder common neighbors, each of which implicitly learns higher-order information through MPNN. A comprehensive analysis of time complexity is in Appendix E."
        },
        {
            "heading": "5 NEURAL COMMON NEIGHBOR WITH COMPLETION",
            "text": "While NCN outperforms existing models, it relies heavily on the common neighbor structure, which the incompleteness of the graph can notably influence. For instance, in cases where node pairs lack\ncommon neighbors, NCN essentially degenerates to GAE, rendering it unable to leverage structural features. Although the absence of common neighbors can suggest that a link is unlikely to exist, certain node pairs may possess common neighbors in the ground truth that remain unobserved in the input graph due to graph incompleteness. Graph incompleteness is ubiquitous in link prediction tasks, given that the objective is to predict unobserved edges. However, few studies have delved into this issue. In this section, we initially demonstrate that incompleteness can result in the loss of common neighbor information, distribution shifts between the training and test sets, and the consequent deterioration of model performance. We propose a straightforward yet effective method to tackle these challenges: common neighbor completion (CNC). CNC completes unobserved common neighbors using a link prediction model. With the introduction of CNC, we enhance NCN and introduce Neural Common Neighbor with Completion (NCNC)."
        },
        {
            "heading": "5.1 INCOMPLETENESS VISUALIZATION",
            "text": "To illustrate the challenges posed by incompleteness, we analyze two common datasets: ogblcollab (Hu et al., 2020) and Cora (Yang et al., 2016). We refer to the graph containing only the edges from the training set as the incomplete graph, while the one encompassing edges from the training, validation, and test sets is termed the complete graph.\nGiven the pivotal role of common neighbor information in our NCN model and other link prediction models, we visualize the distribution of the number of common neighbors for training/test edges in both complete and incomplete graphs separately in Figure 4 (a)(c). To assess how incompleteness impacts model performance, we present the performance of CN model (as shown in Section 3.2) in four distinct scenarios in Figure 4 (b) (d). We observe the following effects of incompleteness:\nLoss of Common Neighbors. Figure 4(c) illustrates that in the incomplete graph, there are fewer common neighbors for both training and test sets, as indicated by the comparison between the blue (green) and red (orange) lines. When comparing the incomplete and complete graphs, it becomes evident that the incomplete graph suffers from a loss of common neighbor information due to incompleteness. Additionally, more links have no common neighbors at all.\nCommon Neighbor Distribution Shift. A noticeable distribution shift between the training and test sets is evident in the incomplete graph of the ogbl-collab dataset, as seen in the comparison between the blue and green lines in Figure 4(a). This shift disappears when the graph is complete (the red and orange lines), indicating that incompleteness is the cause. Such a substantial distribution shift between training and test links could pose challenges in model generalization. This distribution shift is related to the dataset split method. Ogbl-collab is splitted based on the timestamp of edges, and the test edges all belong to the same year. Consequently, compared to the training edges, test edges exhibit stronger correlations with other test edges, resulting in a greater loss of common neighbor when these test edges are absent from the incomplete graph. Conversely, the Cora dataset is randomly splitted, so training and test edges lose a similar ratio of common neighbors and does not exhibit distribution shifts (Figure 4(c)).\nPerformance Degradation. The performance of CN aligns with the common neighbor distribution. In the ogbl-collab dataset, the common neighbor distribution is nearly identical for the training\nset in both the complete and incomplete graphs, as is the performance (See Figure 4 (b)). However, test performance on the incomplete graph decreases significantly as the test distribution changes. Similar trends are observed in the Cora dataset, with test and training scores declining on incomplete graphs when the common neighbor distribution changes compared to the complete graph.\nRemark. Note that while common neighbor distribution changes may not fully account for the differences between complete and incomplete graphs, they offer valuable insights into how incompleteness alters the input graph structure for other learnable models. Despite CN is non-learnable and non-generalizable, its calculation for a target edge doesn\u2019t involve the edge itself, thereby avoiding data leakage concerns. These findings suggest that having a more complete input graph could yield superior link prediction models. However, in practice, we can only work with the incomplete input graph, necessitating exploring other mitigation methods for these issues."
        },
        {
            "heading": "5.2 COMMON NEIGHBOR COMPLETION",
            "text": "Motivated by the analysis above, we address graph incompleteness issues with a two-step method:\nSoft Completion of Common Neighbors. We start by softly completing the input graph with a link prediction model, such as NCN. However, instead of completing all edges in the entire graph, which can be impractical for large graphs, we focus specifically on common neighbor links. We compute the probability that a node u serves as a common neighbor for a node tuple (i, j) as follows:\nPuij =  1 if u\u2208N(i,A) \u2229N(j,A) A\u0302iu if u\u2208 N(j,A)\u2212N(i,A) A\u0302ju if u\u2208N(i,A)\u2212N(j,A) 0 otherwise\n(10)\nwhere A\u0302iu represents the predicted existence probability of link (i, u) by the model. The idea is that u is a common neighbor of (i, j) iff both edges (i, u) and (j, u) exist. If one of these edges is unobserved, we use NCN to predict its link existence probability, which we also use as the probability of u being a common neighbor. In the rare case where both (i, u) and (j, u) are unobserved, we set the probability to 0. This technique is called \u201dCommon Neighbor Completion\u201d (CNC).\nReapplication of NCN on the Completed Graph. Following CNC, we apply the NCN model again on the graph that has been completed using the soft common neighbor weights Puij . This final model is named Neural Common Neighbor with Completion (NCNC) and is defined as:\nNCNC(i, j, A,X) = MPNN(i, A,X)\u2299 MPNN(j, A,X)|| \u2211\nu\u2208N(i)\u222aN(j)\nPuijMPNN(u,A,X). (11)\nNotably, the input graph of MPNN still takes the original graph as input, allowing it to run only once for all target links, thus maintaining high scalability. While Puij can be predicted using any link prediction model, weak models may not accurately recover the unobserved common neighbor structure. Therefore, in practice, we employ NCN to complete it.\nIn addition to addressing distribution shifts and common neighbor loss, NCNC also solves the problem that NCN can degrade to GAE when node pairs lack common neighbors. With NCNC, common neighbors are always completed, and the model only degenerates to GAE when both target nodes are isolated nodes. In such cases, where no structural features can be utilized, relying solely on the target node representations is reasonable. For a visual demonstration of CNC\u2019s effect, please refer to Appendix H, which illustrates how NCN can make more precise predictions by completing common neighbors for node pairs with no observed common neighbors."
        },
        {
            "heading": "6 EXPERIMENT",
            "text": "In this section, we extensively evaluate the performance of both NCN and NCNC. Our code is available in the supplementary material. Detailed experimental settings are included in Appendix D.\nWe use seven popular real-world link prediction benchmarks. Among these, three are Planetoid citation networks: Cora, Citeseer, and Pubmed (Yang et al., 2016). Others are from Open Graph\nBenchmark (Hu et al., 2020): ogbl-collab, ogbl-ppa, ogbl-citation2, and ogbl-ddi. Their statistics and splits are shown in Appendix B."
        },
        {
            "heading": "6.1 EVALUATION ON REAL-WORLD DATASETS",
            "text": "In our evaluation on real-world datasets, we employ a range of baseline methods, encompassing traditional heuristics like CN (Baraba\u0301si & Albert, 1999), RA (Zhou et al., 2009), and AA (Adamic & Adar, 2003), as well as GAE models, such as GCN (Kipf & Welling, 2017) and SAGE (Hamilton et al., 2017). Additionally, we consider SF-then-MPNN models, including SEAL (Zhang & Chen, 2018) and NBFNet (Zhu et al., 2021), as well as SF-and-MPNN models like Neo-GNN (Yun et al., 2021) and BUDDY (Chamberlain et al., 2023). The baseline results are sourced from (Chamberlain et al., 2023). Our models consist of NCN and NCNC. Their architectures are detailed in Appendix C.\nThe experimental results are presented in Table 2. NCN surpasses all baselines on 5/7 datasets and exhibits an average score improvement of 5% compared to BUDDY, the most competitive baseline. Even on the remaining two datasets, NCN outperforms all baselines except BUDDY. These impressive results underscore the outstanding expressivity of our MPNN-then-SF architecture. Furthermore, NCNC enhances performance by an additional 2%, emerging as the top-performing method on all datasets. Notably, on ogbl-ppa, NCNC achieves an HR@100 score of 61.42%, surpassing the strongest baseline BUDDY by a substantial margin of over 10%. It\u2019s worth mentioning that our models outperform node embedding techniques (Perozzi et al., 2014; Grover & Leskovec, 2016; Tang et al., 2015) and other GNNs lacking pairwise features (Wang et al., 2021; 2022) significantly (see Appendix G)."
        },
        {
            "heading": "6.2 SCALABILITY",
            "text": "We compare the inference time and GPU memory on ogbl-collab in Figure 5. NCN and NCNC have a similar computation overhead to GAE, as they both need to run MPNN only once.\nIn contrast, SEAL, which reruns MPNN for each target link, takes 86 times more time compared with NCN with a small batch size 2048, and the disadvantage will be more significant with a larger batch size. Surprisingly, BUDDY and Neo-GNN are slower than NCN. The reason is that it uses pairwise features depending on high order neighbors that are much more time-consuming than common neighbor. NCN\nand NCNC also achieve low GPU memory consumption. We also conduct scalability comparisons on other datasets and observe the same results (see Appendix F)."
        },
        {
            "heading": "6.3 ABLATION ANALYSIS",
            "text": "To assess the effectiveness of the NCNC design, we conducted a comprehensive ablation analysis, as presented in Table 3.\nStarting with GAE, which relies solely on node representations, we introduced GAE+CN, which incorporates Common Neighbor (CN) as pairwise features. Remarkably, GAE+CN outperforms GAE by 70% on Open Graph Benchmark (OGB) datasets, illustrating the importance of structural features. Furthermore, NCN exhibits a 5.5% score increase over GAE+CN, highlighting the advantages of the MPNN-then-SF architecture over the MPNN-and-SF architecture.\nWe also explore variants of NCN, namely NCN-diff and NCN2. In NCN-diff, we include neighborhood difference information by summing the representations of nodes in N(i, A)\u2212N(j, A) and N(j, A) \u2212 N(i, A), while NCN2 incorporates high-order neighborhood overlap using N(i, A2) \u2229 N(j, A) and N(i, A)\u2229N(j, A2). Notably, NCN, NCN-diff, and NCN2 exhibit similar performances across most datasets, suggesting that first-order neighborhood overlap might be sufficient. However, NCN-diff achieves a lower score on the DDI dataset, possibly because the high node degree in DDI introduces noisy and uninformative neighborhood difference information."
        },
        {
            "heading": "7 CONCLUSION",
            "text": "In this work, we introduce Neural Common Neighbor (NCN), a scalable and robust model for link prediction that harnesses the power of learnable pairwise features. Additionally, we address the challenge of graph incompleteness by identifying and visualizing common neighbor loss and distribution shifts stemming from this issue. To mitigate these problems, we introduce the Common Neighbor Completion (CNC) technique. Combining CNC with NCN, our final model, Neural Common Neighbor with Completion (NCNC), outperforms state-of-the-art baselines across various datasets in terms of both speed and prediction performance."
        },
        {
            "heading": "8 LIMITATIONS",
            "text": "Though we propose MPNN-then-SF framework, we do not exhaust the design space and only propose one implementation, NCN, and its variants NCN2 and NCN-diff in ablation study. Moreover, while we only analyze the impact of incompleteness on common neighbor structures, graph incompleteness can also affect other structural features. Additionally, the proposed completion method has the potential to be generalized to address other structural features. Our future research will explore the design space of MPNN-then-SF and the broader implications of incompleteness on various structural features."
        },
        {
            "heading": "9 REPRODUCIBILITY STATEMENT",
            "text": "Our code is in the supplementary material. Proofs of all theorems in the maintext are in Appendix A."
        },
        {
            "heading": "A PROOF",
            "text": "A.1 DERIVATION OF EQUATION 8\nThe MPNN-then-SF architecture is as follows, Pool({{MPNN(u,A,X)|u \u2208 S}}) (12)\nLet Sab be the following set,\n(N l2l1 (i)\u2295N l\u20322 l\u20321 (j)) \u2229 {u \u2208 V |Al2iu = a} \u2229 {u \u2208 V |A l\u20322 uj = b}. (13)\nThen, for Sab, we set the pooling function to sum and multiplied with g(a)g(b), where g is a function with high-order adjacency edge weight as input. Then the MPNN-then-SF architecture can express,\u2211\nu\u2208Sab\ng(a)g(b)MPNN(u,A,X) (14)\nSimply sums the feature of all Sab leads to,\u2211 u\u2208N l2l1 (i)\u2295N l\u20322 l\u20321 (j) g(Al2iu)g(A l\u20322 ju)MPNN(u,A,X). (15)\nA.2 PROOF OF THEOREM 1 AND 2\nHere, we present the theoretical proof of MPNN-then-SF\u2019s higher expressivity. We say algorithm A is strictly more expressive than algorithm B when A can differentiate all pairs of links that B can differentiate, while there exists a pair of links that A can distinguish while B cannot. We first prove the more expressive results by simulating other models with SF-then-MPNN and NCN then prove the strictness by constructing an example. Lemma 1. Equation 7 and NCN are more expressive than Graph Autoencoder (GAE)\nProof. Graph Autoencoder\u2019s prediction for link (i, j) is \u27e8MPNN(i, A,X),MPNN(j, A,X)\u27e9. So directly sum Equation 7 leads to GAE. Equation 7 is a part of NCN, so NCN can also express GAE.\nLemma 2. NCN is more expressive than CN,RA, and AA. Combination of Equation 7 and Equation 8 is more expressive than CN,RA,AA, BUDDY and Neo-GNN\nProof. As MPNN can learn arbitrary functions of node degrees, NCN can express Equation 2, and Equation 8 can express the general form of structure feature 5.\nFurthermore, we construct an example in Figure 3. In that graph, v2 and v3 are symmetric and thus have the same MPNN representation, so GAE cannot distinguish (v1, v2) and (v1, v3). Moreover, (v1, v2) and (v1, v3) are symmetric if the node feature is ignored, so CN, RA, AA, Neo-GNN, BUDDY cannot distinguish them. However, (v1, v2) have a common neighbor with feature 2, and (v1, v3) have a common neighbor with feature 1, so NCN can distinguish them."
        },
        {
            "heading": "B DATASET STATISTICS",
            "text": "The statistics of each dataset are shown in Table 4.\nRandom splits use 70%/10%/20% edges for training/validation/test set respectively. Different from others, the collab dataset allows using validation edges as input on test set."
        },
        {
            "heading": "C MODEL ARCHITECTURE",
            "text": "This section concludes our methods in Section 4 and Section 5.\nGiven an input graph A, a node feature matrix X , and target links {(i1, j1), (i2, j2), ..., (it, jt)}, our models consist of three steps: target link removal, MPNN, and predictor. NCN and NCNC only differ in the last step. The model architecture is visualized in Figure 6\nmodels first set \ud835\udc34 = 0 (Target link removal, TLR). Then, ?\u0305?, \ud835\udc4b are fed to a vanilla MPNN for node representations \u210e. With (\ud835\udc56, \ud835\udc57),\n\u210e and \ud835\udc34 as input, the predictor produces \ud835\udc34 , the probability that edge \ud835\udc56, \ud835\udc57 exists. (b) The NCN predictor. It uses node\nrepresentations of target nodes \ud835\udc56, \ud835\udc57 and their common neighbors to produce edge representations. Then, it feeds the edge\nrepresentation to an MLP to produce the final prediction. (c) The NCNC predictor. It first uses NCN to predict unobserved links\n\ud835\udc34 , \ud835\udc34 , which is then used to complete unobserved common neighbors.\nTarget link removal. We make no changes to the input graph in the validation and test set where the target links are unobserved. In the training set, we remove target links from A. Let A\u0304 denote the processed graph. This method is detailed in Section 5.\nMPNN. We use MPNN to produce node representations h. For each node i,\nhi = MPNN(i, A\u0304,X). (16)\nFor all target links, MPNN needs to run only once.\nPredictor. Predictors use the node representations and graph structure to produce link prediction. Link representations of NCN are as follows,\nzij = (hi \u2299 hj || \u2211\nu\u2208N(i,A\u0304)\u2229 N(j,A\u0304)\nhu), (17)\nwhere || means concatenation, zij is the representation of link (i, j). zij composed of two components: two nodes\u2019 presentation hi \u2299 hj and representations of nodes within the common neighbor set. The former component is often used in link prediction models (Kipf & Welling, 2016; Yun et al., 2021; Chamberlain et al., 2023), while we propose the latter one for the first time. Link representations are then used to produce link existence probability.\nA\u0302ij = sigmoid(MLP(zij)), (18)\nwhere A\u0302ij is the probability that link (i, j) exists. NCNC has a similar form. The only difference is that \u2211\nu\u2208N(i,A\u0304)\u2229N(j,A\u0304) hu in Equation (17) is replaced with the follow form:\u2211\nu\u2208N(i,A\u0304)\u2229 N(j,A\u0304)\nhu + \u2211\nu\u2208N(j,A\u0304)\u2212 N(i,A\u0304)\nA\u0302iuhu + \u2211\nu\u2208N(i,A\u0304)\u2212 N(j,A\u0304)\nA\u0302juhu. (19)\nwhere A\u0302ab is the link existence probability produced by NCNC."
        },
        {
            "heading": "D EXPERIMENTAL SETTINGS",
            "text": "Computing infrastructure. We leverage Pytorch Geometric (Fey & Lenssen, 2019) and Pytorch (Paszke et al., 2019) for model development. All experiments are conducted on an Nvidia 4090 GPU on a Linux server.\nBaselines. We directly use the results reported in (Chamberlain et al., 2023).\nModel hyperparameter. We use optuna (Akiba et al., 2019) to perform random searches. Hyperparameters were selected to maximize validation score. The best hyperparameters selected for each model can be found in our code.\nTraining process. We utilize Adam optimizer to optimize models and set an epoch upper bound 100. All results of our models are provided from runs with 10 random seeds.\nComputation cost The total time of each main experiment is shown in Table 5. Reproducing all main results takes 280 GPU hours."
        },
        {
            "heading": "E TIME AND SPACE COMPLEXITY",
            "text": "Let t denote the number of target links, n denote the number of nodes in the graph, and d denote the maximum node degree. Existing models\u2019 time and space complexity can be expressed in O(B + Ct) and O(D + Et) respectively, where B,C,D,E are irrelevant to t. B,C,D,E of models are summarized in Table 6. The derivation of the complexity is as follows. As NCN, GAE, and GNN with separated structural features run MPNN on the original graph, they share similar ndF + nF 2 in B. Specifically, BUDDY (Chamberlain et al., 2023) uses a simplified MPNN with ndF in B. Moreover, Neo-GNN needs to precompute high order graph Al, which takes O(ndl) time and space.\nBUDDY needs to hash each node and takes O(nh) time and O(nh\u2032) space. In contrast, B of SEAL is 0 as it does not run MPNN on the original graph. For each target link, vanilla GNN only needs to feed the feature vector to MLP for each link, so C = F 2. Besides GAE\u2019s operation, BUDDY further needs to hash the structure for structural features, whose complexity is complex but higher than d per edge, and Neo-GNN computes pairwise feature with O(dl) complexity, where l is the number of hop Neo-GNN consider. NCN needs to compute common neighbor: O(d), pool node embeddings: O(dF ), and feed to MLP: O(F 2). NCNC-1 runs NCN for each potential common neighbor: O(F 2 + d(dF + F 2)) = O(d2F + dF 2). Similarly, NCNC-K runs O(d) times NCNC(K\u22121), so its time complexity is O(dK+1F + dKF 2). For each target link, SEAL segregates a subgraph of size O(dl \u2032 ) and runs MPNN on it, so C = dl \u2032 F 2 + dl\n\u2032+1F , where l\u2032 is the number of hops of the subgraph."
        },
        {
            "heading": "F SCALABILITY COMPARISON ON DATASETS",
            "text": "The time and memory consumption of models on different datasets are shown in Figure 7. On these datasets, we observe results similar to those on the ogbl-collab dataset in Section 6.2: NCN achieves similar computation overhead to GAE; NCNC usually scales better than Neo-GNN; SEAL\u2019s scalabilty is the worst. However, on the ogbl-citation2 dataset, SEAL has the lowest GPU memory consumption with small batch sizes, because the whole graph in ogbl-citation2 is large, on which MPNN is expensive, while SEAL only runs MPNN on small subgraphs sampled from the whole graph, leading to lower overhead."
        },
        {
            "heading": "G COMPARISON WITH OTHER LINK PREDICTION MODELS",
            "text": "Node embedding methods The main advantage of GNN methods is that they keep permutation equivariance. In other words, these methods can give isomorphic links (links with the same structure) the same prediction. In contrast, node embedding methods, such as Node2Vec (Grover &\nFigur 8: Visualization of how NCNC works. The example is from the Cora dataset, a citation graph.\nThe target link is (0, 1). Models should produce high link existence probability. However, in the\nobserved graph, link (0, 5) is missing, and (0,1) thus has no common neighbor. So NCN predicts that\nthe link is not likely to exist. However, for NCNC, it first completes common eighbors (see gr en\nlines). Therefore, NCNC predicts that (0, 1) is more likely to exist. Note that NCNC completes\ncommon neighbors by probability, and we only plot completion with probability \u00bf 0.5 here. And the two completions are with about 0.95 probability. Though the common neighbor 2 completed by the model does not exist in the full graph, the full graph here only means a graph with all training, validation, and test edges, and the citation relation in the graph may still need to be completed.\nLeskovec, 2016), LINE (Tang et al., 2015), and DeepWalk (Perozzi et al., 2014), will produce different results for isomorphic links, leading to potentially bad generalization.\nWe also compare our method with representative node embedding methods on ogb datasets in Table 7. NCN and NCNC outperform node embedding methods significantly on all datasets, indicating the advantages of MPNNs considering pairwise features for link prediction.\nOther GNNs Instead of representations of pairwise relations, PLNLP (Wang et al., 2021) and GIDN (Wang et al., 2022) boost GNNs on link prediction tasks by training tricks like loss function and data augmentation. These tricks are orthogonal to our model design. In experiments (Table 7), compared with PLNLP, NCN achieves 89% performance gain on ogbl-ppa and 20% gain on average. As GIDN only conducts experiment on one dataset ogbl-collab, the comparison is not complete. Moreover, tricks of PLNLP can also boost our models."
        },
        {
            "heading": "H CNC EXAMPLE",
            "text": "Figure 8 provides an example from Cora dataset on how CNC works."
        },
        {
            "heading": "I ABLATION OF MPNN",
            "text": "Here we provide an ablation study on the MPNN used in NCN. The results are shown in Table 8. The MPNN model includes GIN (Xu et al., 2019), GraphSage (Hamilton et al., 2017), MPNN with max aggregation, GCN (Kipf & Welling, 2017), and GAT (Velickovic et al., 2018). Though the performance of NCN is sensitive to the MPNN model, NCN achieves performance gain with all GNNs compared with GraphAutoencoder (GAE)."
        },
        {
            "heading": "J CHOICE OF METRICS",
            "text": "We test our model in different metrics. The results are shown in Table 9. In total, NCN achieves 11 best score (in bold), NCNC achieves 22 best score, and our strongest baseline achieves 9 best score. Therefore, our NCN and NCNC still outperforms baselines in different metrics."
        }
    ],
    "year": 2023
}