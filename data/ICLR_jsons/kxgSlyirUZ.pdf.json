{
    "abstractText": "Text generation under constraints have seen increasing interests in natural language processing, especially with the rapidly improving capabilities of large language models. However, existing benchmarks for constrained generation usually focus on fixed constraint types (e.g. generate a sentence containing certain words) that have proved to be easy for state-of-the-art models like GPT-4. We present COLLIE, a grammar-based framework that allows the specification of rich, compositional constraints with diverse generation levels (word, sentence, paragraph, passage) and modeling challenges (e.g. language understanding, logical reasoning, counting, semantic planning). We also develop tools for automatic extraction of task instances given a constraint structure and a raw text corpus. Using COLLIE, we compile the COLLIE-v1 dataset with 2,080 instances comprising 13 constraint structures. We perform systematic experiments across five state-of-the-art instruction-tuned language models and analyze their performances to reveal shortcomings. COLLIE is designed to be extensible and lightweight, and we hope the community finds it useful to develop more complex constraints and evaluations in the future.",
    "authors": [],
    "id": "SP:085c5465a819f6d1e4e68da2e9b7b23d384e65ae",
    "references": [
        {
            "authors": [
                "R. Anil",
                "A.M. Dai",
                "O. Firat",
                "M. Johnson",
                "D. Lepikhin",
                "A. Passos",
                "S. Shakeri",
                "E. Taropa",
                "P. Bailey",
                "Z. Chen"
            ],
            "title": "Palm 2 technical report",
            "venue": "arXiv preprint arXiv:2305.10403,",
            "year": 2023
        },
        {
            "authors": [
                "S. Bird"
            ],
            "title": "Nltk: the natural language toolkit",
            "venue": "In Proceedings of the COLING/ACL",
            "year": 2006
        },
        {
            "authors": [
                "J. Brooke",
                "A. Hammond",
                "G. Hirst"
            ],
            "title": "Gutentag: an nlp-driven tool for digital humanities research in the project gutenberg corpus",
            "venue": "In Proceedings of the Fourth Workshop on Computational Linguistics for Literature,",
            "year": 2015
        },
        {
            "authors": [
                "H. Chen",
                "H. Li",
                "D. Chen",
                "K. Narasimhan"
            ],
            "title": "Controllable text generation with language constraints",
            "venue": "In preprint,",
            "year": 2022
        },
        {
            "authors": [
                "A.N. Carr",
                "J. Leike",
                "J. Achiam",
                "V. Misra",
                "E. Morikawa",
                "A. Radford",
                "M. Knight",
                "M. Brundage",
                "M. Murati",
                "K. Mayer",
                "P. Welinder",
                "B. McGrew",
                "D. Amodei",
                "S. McCandlish",
                "I. Sutskever",
                "W. Zaremba"
            ],
            "title": "Evaluating large language models trained on code",
            "venue": "In preprint, 2022b. URL https://arxiv.org/abs/2107.03374",
            "year": 2022
        },
        {
            "authors": [
                "M. Chevalier-Boisvert",
                "D. Bahdanau",
                "S. Lahlou",
                "L. Willems",
                "C. Saharia",
                "T.H. Nguyen",
                "Y. Bengio"
            ],
            "title": "Babyai: A platform to study the sample efficiency of grounded language learning",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2019
        },
        {
            "authors": [
                "W.-L. Chiang",
                "Z. Li",
                "Z. Lin",
                "Y. Sheng",
                "Z. Wu",
                "H. Zhang",
                "L. Zheng",
                "S. Zhuang",
                "Y. Zhuang",
                "J.E. Gonzalez",
                "I. Stoica",
                "E.P. Xing"
            ],
            "title": "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023",
            "venue": "URL https://lmsys.org/blog/2023-03-30-vicuna/",
            "year": 2023
        },
        {
            "authors": [
                "S. Dathathri",
                "A. Madotto",
                "J. Lan",
                "J. Hung",
                "E. Frank",
                "P. Molino",
                "J. Yosinski",
                "R. Liu"
            ],
            "title": "Plug and play language models: A simple approach to controlled text generation",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2020
        },
        {
            "authors": [
                "G. Dinu",
                "P. Mathur",
                "M. Federico",
                "Y. Al-Onaizan"
            ],
            "title": "Training neural machine translation to apply terminology constraints. In Association for Computational Linguistics (ACL), 2019",
            "year": 2019
        },
        {
            "authors": [
                "W. Foundation"
            ],
            "title": "Wikimedia downloads, 2022",
            "venue": "URL https://dumps.wikimedia.org",
            "year": 2022
        },
        {
            "authors": [
                "F. Hamborg",
                "N. Meuschke",
                "C. Breitinger",
                "B. Gipp"
            ],
            "title": "news-please: A generic news crawler and extractor",
            "venue": "In Proceedings of the 15th International Symposium of Information Science,",
            "year": 2017
        },
        {
            "authors": [
                "E. Hasler",
                "A. de Gispert",
                "G. Iglesias",
                "B. Byrne"
            ],
            "title": "Neural machine translation decoding with terminology constraints",
            "venue": "In North American Association for Computational Linguistics (NAACL),",
            "year": 2018
        },
        {
            "authors": [
                "D. Hendrycks",
                "C. Burns",
                "S. Basart",
                "A. Zou",
                "M. Mazeika",
                "D. Song",
                "J. Steinhardt"
            ],
            "title": "Measuring massive multitask language understanding",
            "venue": "arXiv preprint arXiv:2009.03300,",
            "year": 2020
        },
        {
            "authors": [
                "C. Hokamp",
                "Q. Liu"
            ],
            "title": "Lexically constrained decoding for sequence generation using grid beam search",
            "venue": "In Association for Computational Linguistics (ACL),",
            "year": 2017
        },
        {
            "authors": [
                "J.E. Hu",
                "H. Khayrallah",
                "R. Culkin",
                "P. Xia",
                "T. Chen",
                "M. Post",
                "B. Van Durme"
            ],
            "title": "Improved lexically constrained decoding for translation and monolingual rewriting",
            "venue": "In North American Association for Computational Linguistics (NAACL),",
            "year": 2019
        },
        {
            "authors": [
                "Z. Hu",
                "Z. Yang",
                "X. Liang",
                "R. Salakhutdinov",
                "E.P. Xing"
            ],
            "title": "Toward controlled generation of text",
            "venue": "In International Conference on Machine Learning (ICML),",
            "year": 2017
        },
        {
            "authors": [
                "J. Johnson",
                "B. Hariharan",
                "L. van der Maaten",
                "L. Fei-Fei",
                "C.L. Zitnick",
                "R. Girshick"
            ],
            "title": "Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. In Conference on computer vision and pattern recognition",
            "venue": "URL https://ieeexplore.ieee. org/document/8099698",
            "year": 2017
        },
        {
            "authors": [
                "N.S. Keskar",
                "B. McCann",
                "L.R. Varshney",
                "C. Xiong",
                "R. Socher"
            ],
            "title": "CTRL: A conditional transformer language model for controllable generation",
            "venue": "In preprint,",
            "year": 2019
        },
        {
            "authors": [
                "B. Krause",
                "A.D. Gotmare",
                "B. McCann",
                "N.S. Keskar",
                "S. Joty",
                "R. Socher",
                "N.F. Rajani"
            ],
            "title": "GeDi: Generative discriminator guided sequence generation",
            "venue": "In Findings of the Empirical Methods in Natural Language Processing (EMNLP Findings),",
            "year": 2021
        },
        {
            "authors": [
                "Q. Lhoest",
                "A. Villanova del Moral",
                "Y. Jernite",
                "A. Thakur",
                "P. von Platen",
                "S. Patil",
                "J. Chaumond",
                "M. Drame",
                "J. Plu",
                "L. Tunstall",
                "J. Davison",
                "M. \u0160a\u0161ko",
                "G. Chhablani",
                "B. Malik",
                "S. Brandeis",
                "T. Le Scao",
                "V. Sanh",
                "C. Xu",
                "N. Patry",
                "A. McMillan-Major",
                "P. Schmid",
                "S. Gugger",
                "C. Delangue",
                "T. Matussi\u00e8re",
                "L. Debut",
                "S. Bekman",
                "P. Cistac",
                "T. Goehringer",
                "V. Mustar",
                "F. Lagunas",
                "A. Rush",
                "T. Wolf"
            ],
            "title": "Datasets: A community library for natural language processing",
            "year": 2021
        },
        {
            "authors": [
                "B.Z. Li",
                "J. Yu",
                "M. Khabsa",
                "L. Zettlemoyer",
                "A. Halevy",
                "J. Andreas"
            ],
            "title": "Quantifying adaptability in pre-trained language models with 500 tasks. In North American Association for Computational Linguistics (NAACL), 2022a. URL https://arxiv.org/abs/2112.03204",
            "year": 2022
        },
        {
            "authors": [
                "X.L. Li",
                "J. Thickstun",
                "I. Gulrajani",
                "P. Liang",
                "T.B. Hashimoto"
            ],
            "title": "Diffusion-lm improves controllable text generation",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2022
        },
        {
            "authors": [
                "P. Liang",
                "R. Bommasani",
                "T. Lee",
                "D. Tsipras",
                "D. Soylu",
                "M. Yasunaga",
                "Y. Zhang",
                "D. Narayanan",
                "Y. Wu",
                "A. Kumar"
            ],
            "title": "Holistic evaluation of language models",
            "venue": "arXiv preprint arXiv:2211.09110,",
            "year": 2022
        },
        {
            "authors": [
                "B.Y. Lin",
                "W. Zhou",
                "M. Shen",
                "P. Zhou",
                "C. Bhagavatula",
                "Y. Choi",
                "X. Ren"
            ],
            "title": "Commongen: A constrained text generation challenge for generative commonsense reasoning",
            "venue": "In Findings of the Empirical Methods in Natural Language Processing (EMNLP Findings),",
            "year": 2020
        },
        {
            "authors": [
                "X. Lu",
                "P. West",
                "R. Zellers",
                "R.L. Bras",
                "C. Bhagavatula",
                "Y. Choi"
            ],
            "title": "Neurologic decoding: (un)supervised neural text generation with predicate logic constraints",
            "venue": "In North American Association for Computational Linguistics (NAACL),",
            "year": 2021
        },
        {
            "authors": [
                "X. Lu",
                "S. Welleck",
                "P. West",
                "L. Jiang",
                "J. Kasai",
                "D. Khashabi",
                "R.L. Bras",
                "L. Qin",
                "Y. Yu",
                "R. Zellers",
                "N.A. Smith",
                "Y. Choi"
            ],
            "title": "Neurologic a*esque decoding: Constrained text generation with lookahead heuristics",
            "venue": "In North American Association for Computational Linguistics (NAACL),",
            "year": 2022
        },
        {
            "authors": [
                "L. Ouyang",
                "J. Wu",
                "X. Jiang",
                "D. Almeida",
                "C.L. Wainwright",
                "P. Mishkin",
                "C. Zhang",
                "S. Agarwal",
                "K. Slama",
                "A. Ray",
                "J. Schulman",
                "J. Hilton",
                "F. Kelton",
                "L. Miller",
                "M. Simens",
                "A. Askell",
                "P. Welinder",
                "P. Christiano",
                "J. Leike",
                "R. Lowe"
            ],
            "title": "Training language models to follow instructions with human feedback",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2022
        },
        {
            "authors": [
                "C. Raffel",
                "N. Shazeer",
                "A. Roberts",
                "K. Lee",
                "S. Narang",
                "M. Matena",
                "Y. Zhou",
                "W. Li",
                "P.J. Liu"
            ],
            "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2020
        },
        {
            "authors": [
                "L. Ruis",
                "J. Andreas",
                "M. Baroni",
                "D. Bouchacourt",
                "B.M. Lake"
            ],
            "title": "A benchmark for systematic generalization in grounded language understanding",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2020
        },
        {
            "authors": [
                "T. Schick",
                "J. Dwivedi-Yu",
                "R. Dess\u00ec",
                "R. Raileanu",
                "M. Lomeli",
                "L. Zettlemoyer",
                "N. Cancedda"
            ],
            "title": "Scialom. Toolformer: Language models can teach themselves to use tools",
            "venue": "In preprint,",
            "year": 2023
        },
        {
            "authors": [
                "A. Srivastava",
                "A. Rastogi",
                "A. Rao",
                "A.A.M. Shoeb",
                "A. Abid",
                "A. Fisch",
                "A.R. Brown",
                "A. Santoro",
                "A. Gupta",
                "A. Garriga-Alonso"
            ],
            "title": "Beyond the imitation game: Quantifying and extrapolating the capabilities of language models",
            "venue": "arXiv preprint arXiv:2206.04615,",
            "year": 2022
        },
        {
            "authors": [
                "R. Taori",
                "I. Gulrajani",
                "T. Zhang",
                "Y. Dubois",
                "X. Li",
                "C. Guestrin",
                "P. Liang",
                "T.B. Hashimoto"
            ],
            "title": "Stanford alpaca: An instruction-following llama model",
            "venue": "https://github.com/tatsu-lab/ stanford_alpaca,",
            "year": 2023
        },
        {
            "authors": [
                "Y. Wang",
                "S. Mishra",
                "P. Alipoormolabashi",
                "Y. Kordi",
                "A. Mirzaei",
                "A. Naik",
                "A. Ashok",
                "A.S. Dhanasekaran",
                "A. Arunkumar",
                "D. Stap"
            ],
            "title": "Super-naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks",
            "venue": "In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2022
        },
        {
            "authors": [
                "J. Weston",
                "A. Bordes",
                "S. Chopra",
                "A.M. Rush",
                "B. Van Merri\u00ebnboer",
                "A. Joulin",
                "T. Mikolov"
            ],
            "title": "Towards ai-complete question answering: A set of prerequisite toy tasks",
            "venue": "arXiv preprint arXiv:1502.05698,",
            "year": 2015
        },
        {
            "authors": [
                "S. Yao",
                "J. Zhao",
                "D. Yu",
                "N. Du",
                "I. Shafran",
                "K. Narasimhan",
                "Y. Cao"
            ],
            "title": "React: Synergizing reasoning and acting in language models",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2022
        },
        {
            "authors": [
                "Y. Zhang",
                "Y. Liu",
                "Z. Yang",
                "Y. Fang",
                "Y. Chen",
                "D. Radev",
                "C. Zhu",
                "M. Zeng",
                "R. Zhang"
            ],
            "title": "Macsum: Controllable summarization with mixed attributes",
            "venue": "Transactions of the Association for Computational Linguistics,",
            "year": 2023
        },
        {
            "authors": [
                "W. Zhou",
                "Y.E. Jiang",
                "E. Wilcox",
                "R. Cotterell",
                "M. Sachan"
            ],
            "title": "Controlled text generation with natural language instructions",
            "venue": "arXiv preprint arXiv:2304.14293,",
            "year": 2023
        },
        {
            "authors": [
                "Huggingface Lhoest"
            ],
            "title": "Wiki We use the 20220301.en train split of the dataset",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Large language models (LLMs) are increasingly capable of generating coherent and fluent text when provided with high-level prompts (OpenAI, 2023a). Such capabilities have raised the bar for automated text generation, allowing us to explore more nuanced ways of utilizing LMs. One such line of inquiry is constrained text generation, whereby the LM is asked to adhere to a particular topic (Keskar et al., 2019; Dathathri et al., 2020), or avoid using certain words (Lu et al., 2021; 2022). However, these works scratch the surface of a broader phenomenon \u2014 LMs do not just generate text, as evidenced by their use in more structured tasks like problem solving (Yao et al., 2022), code generation (Chen et al., 2022b) and even tool use through API calls (Schick et al., 2023).\nThis raises a natural question \u2014 \u2018what is the next iteration of text generation benchmarks that can evaluate these advanced capabilities in LLMs\u2019? We posit that one direction is incorporating logical and compositional challenges via constrained text generation. Existing benchmarks for constrained generation, however, focus only on a particular constraint types, require tailored pipelines to collect data and annotations, and/or can only evaluate a specific aspect of LM strengths (Lin et al., 2020; Chen et al., 2022a). They also suffer from challenges in scalable dataset construction.\nIn this paper, we propose COLLIE, a grammar-based framework that enables systematic construction of compositional constraints over diverse generation levels (e.g., words, sentences, paragraphs) and semantic requirements (e.g., language understanding, logical reasoning, counting). Operationally, COLLIE allows researchers to 1) easily specify constraint templates, and then automatically 2) extract constraint values from language corpora, 3) render them into natural language instructions, and 4) evaluate model generations against the constraint instructions.\nExisting benchmarks for constrained generation focus only on particular constraint types and formats (e.g., \u201cgenerate a sentence with words...\u201d). These limitations mean that benchmarks become quickly obsolete as LLMs progress. In contrast, the modular and extensible design of COLLIE allows the broader NLP community to contribute additional constraints that can co-evolve with LLM capabilities over time, while also providing a convenient endpoint for users that only want to evaluate their model without developing their own constraints. The flexibility of such a grammar-based framework may not only be useful for evaluation, but also in practice (e.g. word constraints, words blacklist, etc.).\nWe construct the dataset COLLIE-v1 with 2,080 constraint instances across 13 different types, using three different corpora: Wikipedia (Foundation, 2022), CC-News (Hamborg et al., 2017), and Project Gutenberg (Brooke et al., 2015). We perform zero-shot evaluations of five state-of-the-art LLMs of varying sizes including GPT-4 (OpenAI, 2023a) and PaLM (Anil et al., 2023). While GPT-4 comparatively performs the best, it still achieves an average constraint satisfaction rate of only 50.9%. We find that challenges correlate with position \u2013 for instance, instructing models to begin a sentence with a specific word leads to a 100% success rate for GPT-4, while asking models to end a sentence with a particular word results in a success rate of 40%-60%. These insights can help us diagnose LLMs, which in turn can improve LLM capabilities, and further advance the benchmark itself.\nTo summarize, we make the following contributions: (1) We introduce COLLIE, a framework for systematic generation of compositional constraints, that is flexible and extensible. (2) We use COLLIE to curate a new dataset COLLIE-v1 comprising of 13 constraint structures. (3) We perform a comprehensive evaluation of five state-of-the-art LLMs of varying sizes and provide useful insights for both model and benchmark development in the future."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Constrained text generation (CTG). Early work in controllable text generation used control codes to steer the generation towards desired topics or to reduce undesirable content, by controlling for broad attributes such as sentiment or toxicity (Hu et al., 2017; Keskar et al., 2019; Dathathri et al., 2020; Krause et al., 2021). Other work on constrained decoding provides to the language model a collection of lexical items as constraints to be included or excluded in the final generated text (Hokamp and Liu, 2017; Hasler et al., 2018; Dinu et al., 2019; Hu et al., 2019; Lin et al., 2020; Lu et al., 2021; 2022; Li et al., 2022b). Recent advances in instruction tuning LLMs (Ouyang et al., 2022) have brought major improvements to controllability. These advancements have made it challenging to use existing controllable generation datasets to fully assess the capabilities of modern LLMs. InstructCTG (Zhou et al., 2023) is a concurrent work that also constructed a dataset with text constraints. However, it mainly focuses on synthesizing 5 types of simple CTG instructions for tuning small language models such as T5-11B (Raffel et al., 2020), whereas COLLIE serves to construct much more challenging and open-ended CTG tasks to evaluate and diagnose start-of-the-art LLMs like GPT-4 (OpenAI, 2023b). Lastly, there is a line of work that sets up CTG in more practical downstream applications, such as controllable summarization (Zhang et al., 2023). The flexibility of COLLIE allows these \u201cfunctional constraints\u201d to be incorporated for more usefulness, which we leave for future work.\nGrammar-based compositional tests. Building benchmarks with data synthesized from grammars has been explored previously in the context of question answering (Weston et al., 2015), instruction following (Chevalier-Boisvert et al., 2019; Ruis et al., 2020)) and visual reasoning (Johnson et al.,\n2017). These benchmarks showcased the utility of grammars to systematically generate a comprehensive set of test cases or to specify some fixed constraints. In contrast, COLLIE aims to enable flexible, and dynamic constraint construction that can co-evolve with models. Furthermore, previous datasets were synthetic with limited linguistic diversity and practical applicability to real-world scenarios. In contrast, since our COLLIE framework extracts values and examples from natural language corpora to construct the constraints, it represents a more realistic challenge for modern LLMs.\nSystematic and scalable language benchmarks. The emergence of increasingly powerful generalpurpose language models has created a need for scalable benchmarks that can systematically and comprehensively evaluate them. A few recent examples include HELM (Liang et al., 2022), BIGBench (Srivastava et al., 2022), MMLU (Hendrycks et al., 2020), TaskBench500 (Li et al., 2022a), and Natural Instructions (Wang et al., 2022). However, building such benchmarks require considerable human effort, and may become obsolete when stronger models enter the arena. We provide a new perspective in this race between model capabilities and challenging benchmarks: leverage compositionality to construct automatic and scalable benchmarks with minimal human effort that can co-evolve with model capabilities to remain challenging and relevant."
        },
        {
            "heading": "3 COLLIE FRAMEWORK TO CONSTRUCT CONSTRAINED TEXT GENERATION",
            "text": "COLLIE allows researchers to easily 1) specify textual constraint structures via a grammar, then automatically 2) extract constraint values from text corpora, 3) render constraints into natural language instructions, and 4) evaluate generations with respect to constraints.\nGrammar. Two observations about text constraints motivate a grammar characterization: 1) they involve different levels of text, e.g. character, word, sentence, or paragraph; and 2) many of them specify either the count or position at a certain text level (existence is equivalent to count > 0).\nLet capitalized letters (S,M,C, T ) denote non-terminal variables, and other symbols (\u2113, \u25e6,\u2295, v) denote terminals. A full constraint specification within our grammar S (Eq. 1) consists of two parts: a generation level (level(\u03be) = \u2113) specifying whether the generated text \u03be should be a word, a sentence, a paragraph, or a document, and a multi-constraint M (Eq. 2), which is a logical composition of one or more base-constraints C. A text T (Eq. 4) within these constraints can either be the full generated text \u03be, or a part of it when qualified with a pos(\u00b7). For example, pos(pos(\u03be, paragraph, 3), sentence,\u22121) means \u201cthe last sentence of the 3rd paragraph of the generated text\u201d. For terminal variables, we define a level \u2113 of a text (Eq. 5), a string or number relation \u25e6 or \u2295 (Eq. 6), and a string or number value vstr or vnum (Eq. 7). \u2227 represents the logical \u2018and\u2019 operator, and \u2228 represents the logical \u2018or\u2019. With these definitions, we construct the following grammar:\nS \u2192 (level(\u03be) = \u2113) \u2227M (constraint specification) (1) M \u2192 C | C \u2227M | C \u2228M (multi-constraint) (2) C \u2192 count(T, \u2113, vstr | \u2113\u2032)\u2295 vnum | pos(T, \u2113, vnum) \u25e6 vstr (base-constraint) (3) T \u2192 \u03be | pos(T, \u2113, vnum) (text) (4) \u2113 \u2192 char | word | sentence | paragraph | passage (level) (5) \u25e6 \u2192=|=\u0338 \u2295 \u2192=|=\u0338|>|<|\u2264|\u2265 (relation) (6)\nvstr \u2208 \u03a3\u2217 vnum \u2208 Z (value) (7) At the core of our grammar, we consider two (symmetrical) types of base-constraints C (Eq. 3):\n1. Count constraints. count(T, \u2113, vstr) \u2295 vnum constrains the occurrences of a particular level-\u2113 string vstr. For example, count(T,word, \u2018happy\u2019) \u2264 3 means \u201cT should contain the word \u2018happy\u2019 no more than 3 times\u201d. In contrast, count(T, \u2113, \u2113\u2032) \u2295 vnum constrains the occurrences of level-\u2113 strings in each level-\u2113\u2032 unit of text T. For example, count(T, char, sentence) = 50 means \u201ceach sentence of text T should have exactly 50 characters\u201d.\n2. Position constraints. pos(T, \u2113, vnum) \u25e6 vstr specifies that a particular part of the text T should equal (or not equal) the given string vstr. For example, pos(T,word, 3) = \u2018happy\u2019 means \u201cthe 3rd word should be \u2018happy\u2019 in text T\u201d. We also allow negative indices for reverse counting, e.g.pos(T, char,\u22121) \u0338= x means \u201cthe last letter should not be \u2018x\u2019 in text T\u201d. Note that the grammar above can easily be extended to accommodate more types of base-constraints (e.g. part of speech, sentiment) by implementing the corresponding semantic checks \u2014 we leave this\nto future work. Also for convenience, we use constraint structure to refer to a family of constraint specifications that only differ in their values (e.g. generate a sentence with exactly x words, x \u2208 N), and constraint to refer to a particular constraint specification with concrete values (e.g. generate a sentence with exactly 5 words).\nExamples and conceptual challenges. Our grammar can express a wide range of constraints through logical compositions of base-constraints across different text levels. Table 1 illustrates some structures across generation levels, identified by names such as para01 for paragraph generation, etc.\nIn addition to the generation levels, count and pos across different levels introduce a variety of challenges. For example, word01 and sent01 challenge token-based language models to count characters; pass01 requires high-level semantic planning for models to generate a coherent passage under constraints; sent04 and para02 challenge models to generate text with presence or absence of particular words; sent03, para03, and para04 require counting at multiple levels; and word02, word03, sent02, para05, and pass01 combine counting and positional challenges at different levels, which can be considered most demanding conceptually. We empirically assess the difficulty of constraint structures in Section 5. Example COLLIE usage is presented in Figure 2.\nIn conjunction with the grammar, we develop a set of compiling tools to help construct datasets with minimal human efforts. Concretely, the pipeline of dataset construction involves 4 stages (Figure 1):\n1. Specify constraint structures. Researchers can specify constraint structures (e.g. Table 1), and optionally with a value range (e.g. \u201cgenerate a sentence with x words\u201d, and 5 \u2264 x \u2264 10). This is the only stage that involves manual effort.\n2. Extract constraint values from corpora. We design an automatic extraction algorithm that runs through a given text corpus to find strings that fit a constraint structure with some value ranges. For example, given the constraint structure count(\u03be,word, \u2205) = x with value range 5 \u2264 x \u2264 10, the extraction algorithm returns sentences in the corpus that have 5-10 words, with associated word counts. This ensures each constraint has at least one natural solution. More details are in Section 4.1.\n3. Render natural language instructions. Each constraint can be rendered into a natural language instruction (Table 1) via ruled-based translation, thanks to the compositionality grammar of COLLIE. For example, a constraint count(\u03be, char, \u2018v\u2032) = 2 \u2227 count(\u03be, char, \u2018i\u2032) = 3 can be synthetically rendered into the instruction \u201cPlease generate a word with exactly 2 character \u2018v\u2019 and exactly 3 character \u2018i\u2019.\u201d. It is also possible to improve the instruction fluency or naturalness by adding additional rules to the synthetic translation, or use LLMs to polish instructions. More details are in Section A.1.\n4. Evaluate generations. Given text \u03be generated by a model, we use a parser to evaluate it against a constraint specification S and derive a True/False value, indicating if \u03be satisfies S. We use an average Success Rate as the main metric to evaluate constraint conformance. We can also compare the fluency of \u03be against the corpus-extracted \u201cgroundtruth\u201d text, and render more fine-grained natural language feedback indicating which base-constraints are met and which not (see Section A.2)."
        },
        {
            "heading": "4 COLLIE-V1 DATASET",
            "text": "We construct COLLIE-v1 using constraints structures from Table 1, which contains 2,080 constraint instances from 13 constraint types, with 1,435 unique constraint prompts. The broader NLP community can contribute to future dataset releases by adding additional constraints, metrics, data sources. The curated constraint set can co-evolve with models to become more challenging and comprehensive as model capabilities improve."
        },
        {
            "heading": "4.1 CONSTRAINT SPECIFICATION AND EXTRACTION",
            "text": "Constraint specification. We begin by defining 13 constraint structures. We chose these 13 structures to span various generation levels (word, sentence, paragraph and passage generation) and challenges (counting, position). In total, we have 3 word-level, 4 sentence-level, 5 paragraph-level, and 1 passage-level constraint structures. Of these 13 constraint structures, 5 are single-level and the remaining 8 are multi-level constraints. See Table 1 for the exact constraint structures we use.\nConstraint extraction. While constructing constraint structures is straightforward using our grammar, choosing constraint targets is challenging for two reasons: (1) Not all targets will admit a conforming natural language string. For instance, the constraint, \u201cGenerate a two word sentence beginning with the word The.\u201d has no grammatically acceptable answer. (2) Even if a constraint admits a possible answer, it may not admit a plausible answer. For instance, \u201cGenerate a sentence with 1928 words\u201d is possible, but any such sentence is very unlikely to appear in regular discourse.\nTo address both challenges, we sample constraint target values from natural language corpora, which we denote as the data source. Given a constraint structure C and documents D = {d1, ..., dn}, we chunk each document into a series of strings di = {s1, ..., sm}, where each si can be a sentence, paragraph, or passage as required by C. Each string si undergoes source-specific automated filtering and post-processing to remove artifacts, which we detail in Section B.4. Given C and si, we extract target values such that C is satisfied. In most cases, the satisfying target values can be directly extracted using our provided utilities. For example, for constraints with structure \u201csentence with x words\u201d, we can directly apply word tokenization and counting to the example string si. In cases in which direct extraction is not possible, (e.g. \u201cdo not include word w\u201d), we specify a range of possible targets (e.g. {the, and, of}) to sweep over. All in all, our approach ensures that (1) there exists a natural language string that can satisfy each constraint and target pair, and (2) the targets follow a plausible distribution induced by natural language corpora. Our extraction system is extensible, and can operate on new constraints and data sources with minimal modifications.\nExtensibility Adding additional data sources to the extraction pipeline is similarly easy, requiring a text delimiter, and optional string filtering and post-processing functions. As a case-study on the extensibility of COLLIE, we demonstrate how to extend constraints to include POS-tags such as \u201cGenerate a sentence with verbs\u201d. Details are in section B.5."
        },
        {
            "heading": "4.2 DATA SOURCES",
            "text": "To adequately cover diverse styles and content, we extract constraint targets from three distinct data sources: Wikipedia (Wiki) (Foundation, 2022), Common Crawl News (CC-News) (Hamborg et al., 2017), and the Project Gutenberg Corpus (Guten) (Brooke et al., 2015). We provide an overview of these data sources below and leave source-specific filtering and post-processing details to Section B.4.\nWiki. Wikipedia (Wiki) (Foundation, 2022) consists of over 6 million English Wikipedia articles. We included this data source for the diverse subject matter present in the corpus.\nCC-News. The Common Crawl News corpus (CC-News) (Hamborg et al., 2017) consists of 708,241 English language news articles published between Jan 2017 and December 2019. We include CC-News to include interview dialogues, as well as popular culture and current events.\nGuten. The Project Gutenberg corpus (Guten) (Brooke et al., 2015) consists of over 50,000 documents that include fiction, histories, biographies, and other works that are in the public domain in the United States. We include this corpus for its variety in genres (e.g. non-fiction, fiction, plays, etc.) and style from different time periods."
        },
        {
            "heading": "4.3 DATA VALIDATION AND STATISTICS",
            "text": "We extract constraints from 300 randomly sampled documents from each source. After extracting the target values, we sample up to 100 targets for each constraint structure on each data source. We remove any string targets by that begins or ends with any character that is not a letter or number. We randomly sample 5 out of these 100 targets and their supporting examples to qualitatively verify their validity. Since the extraction process is relatively fast, we modify filters and post-processors if there are systemic issues and re-run the extraction phase. We provide statistics of the final number of constraints from each constraint structure in Figure 3(a). Some constraints (e.g. number of sentences per paragraph) are tightly clustered around the mean, and thus does not induce many valid constraint targets. The fraction of strings filtered for each data source and level is presented in Figure 3(b). The automated filtering removes a large fraction of the strings in most cases, as high recall is important to ensure the quality. The high fraction of omitted passages is due to the removal of passages < 2 paragraphs in length. Mean lengths for each level and data source is presented in Figure 3(c)."
        },
        {
            "heading": "5 RESULTS",
            "text": "Our main experiments in this paper focus on a zero-shot prompting setup with the following language models (LMs): 1) larger and closed-source LMs such as OpenAI GPT (Brown et al., 2020; OpenAI, 2023b) (gpt-3.5-turbo, gpt-4) and Google PaLM-2 (Anil et al., 2023) (text-bison-001);\n2) smaller and open-source LMs such as Alpaca-7B (Taori et al., 2023), Vicuna-7B (Chiang et al., 2023). We performed additional one-shot prompting and find GPT performances similar to zero-shot performance, see Section C.1. By default, we use a sampling temperature of 0.7, and sample multiple trials (20 for GPT/PaLM, 5 for Alpaca/Vicuna). All experiments were run in July, 2023.\nZero-shot performance comparison. As evidenced in Figures 4(a), GPT-4 consistently surpassed other models in zero-shot constrained text generation performances, achieving more than twice the constraint satisfaction rate than other non-GPT models. The overarching performance trend observed shows GPT-4 leading the pack, followed by GPT-3.5 and PaLM with a large gap, and then followed closely by the smaller models, Vicuna-7B and Alpaca-7B.\nConstraints all models can follow. Certain tasks, specifically word01 (generating a word with at least a letters), sent04 (generating a sentence containing words X, Y, Z), and para01 (generating a paragraph with each sentence starting with the word X), posed minimal challenge to the majority of contemporary language models. These tasks demonstrate the proficiency of current models at simple constraints ensuring existence, as depicted in Figure 9(f).\nConstraints partially solved by GPT-4 only. However, a notable distinction arose when tasks incorporated more counting/position constraints and requested longer generations. Tasks such as word03, para04, para05, and pass01 were only partially addressed by GPT-4, with constraint satisfaction rates ranging between 40% and 70%. Despite GPT-4\u2019s partial success in these tasks, other models failed to deliver any satisfactory performance.\nConstraints remaining very challenging. Furthermore, some tasks proved challenging across all models. Tasks word02, sent01, sent02, and para03 present challenges in terms of arbitrary position constraints and mixed counting levels (see Section 5.1 for detailed analysis), indicating areas that necessitate further advancements in language model technology. Moreover, the average pass@20 rate of GPT-4 was above 63% across all constraints, significantly higher than the 32% achieved by GPT-3.5, as depicted in Figure 5. Although GPT-4 demonstrated a significant performance advantage, its constraint satisfaction rate of 63% is far from perfect. This suggests considerable scope for improvement in controllable text generation with language models. These findings underscore the opportunities and challenges in the continued evolution of language models."
        },
        {
            "heading": "5.1 ANALYSIS",
            "text": "Performance consistency across data sources. We observe a high degree of consistency in the performance of models on a given constraint structure, regardless of the data source. This uniformity is evident across all models, as highlighted in Figure 9 (g). This indicates that the ability of a language\nmodel to adhere to the logic of constraints takes precedence over the specific target values or the distribution of the data.\nPosition effect. As depicted in Figure 6, the pos(\u03be, level, i) function, constraining the i-th sub-string (letter, word, or sentence), exhibits varying levels of difficulty depending on the value of i. Models generally perform well when the positional constraint is applied to the first sub-string (i = 1, task para01). However, only GPT-4 displays partial success with the last positional constraints (i = \u22121, tasks word03, para05, pass01). Notably, all models encounter difficulties when generating text that satisfies positional constraints at arbitrary positions i. Additionally, we find that the position effect exhibits a lower sensitivity to constraint levels.\nCounting level effect. The task of counting characters within a word is comparatively easier for models than counting characters within a sentence, as illustrated in Figure 7. Furthermore, tasks demanding exact equality (task sent01) prove more challenging than those requiring a range (task para03), and are considerably more difficult than tasks specifying just an upper or lower bound (tasks word01, sent03, para04).\nIncreased difficulty with logical composition. The incorporation of logical compositions into constraints considerably increases their difficulty. Task sent03 serves as an example of this, adding\nan extra constraint at the sentence level compared to task word01. Despite the assumption that the added constraint should be manageable for all models, performance on task sent03 uniformly trails behind that on task word01, as shown in Figure 7. This highlights the intricacy and challenge introduced by logical compositions within constraints.\nPerformance enhancement through feedback and interaction. We utilize COLLIE to generate automated natural language feedback (e.g., \u201cYour task is to generate a word with exactly 2 character \u2018v\u2019 and exactly 3 character \u2018i\u2019. However, you generate a word with 3 character \u2018v\u2019 and 4 character \u2018i\u2019.\u201d), and engage LLMs in a generation-feedback dialogue. In Figure 8, we observe a significant 20% improvement in GPT-4 performance after the second round of feedback. However, the model\u2019s performance plateaus at 66% even after three additional rounds of feedback, comparable to pass@5 using i.i.d. sampling. The extent of performance improvement varies across tasks, with word03\u2019s constraint satisfaction rate increasing from 62.1% to 10%. Conversely, word02, sent01, and sent02 tasks remain challenging for the model. These findings suggest that there is still room for improvement, highlighting the difficulty of our dataset, and emphasizing the need for further research on better ways to incorporate natural language feedback."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "In this work, we present COLLIE, a grammar-based framework for specifying textual constraints. COLLIE simplifies the process of creating constrained-generation datasets by enabling researchers to focus on specifying high level constraint structures, while COLLIE automatically extracts constraint values, renders natural language instructions, and assesses model performance. To demonstrate the utility of the COLLIE framework, we construct COLLIE-v1 with 1,132 constraints from 13 different types, extracted from 3 different data sources. We evaluate five state-of-the-art LLMs of various sizes on COLLIE-v1, and find that it provides fine-grained insights into model capabilities and shortcomings. We hope that model developers can use COLLIE-v1 to develop more capable models, while future releases of COLLIE can continue to adapt to the capabilities and needs of future models and users.\nLimitations and societal impacts. Although care was taken to design the filtering and processing functions, such automated approaches are never perfect and remaining artifacts in corpora might lead to unnatural reference texts or constraints. Further filtering (e.g., by grammar checkers, parsers, or humans) could improve the dataset quality. Our representative constraint structures were selected to encompass diverse constrained generation challenges, but as with all generation benchmarks, they cannot capture all dimensions and nuances of model capabilities. Benchmarks are highly influential in shaping model development, the capabilities and limitations of which may disproportionately impact different communities. By providing an extensible, easy-to-use framework for constraint development, we hope COLLIE will enable diverse stakeholders to engage with dataset building, helping ensure that future model capabilities serve diverse interests and needs.\nReproducibility. We provide all important implementation details in the main text and Appendix. Our code is available at https://anonymous.4open.science/r/collie-7193/."
        },
        {
            "heading": "A NATURAL LANGUAGE RENDERING OF CONSTRAINTS",
            "text": "A.1 INSTRUCTION RENDERING\nCOLLIE provides a rule-based constraint renderer that converts constraints into natural language instructions (see examples in Table 1).\nLeveraging the compositionality of the context-free grammar, the renderer first parses the constraint as a tree. In the case of multi-constraints, it generates prompts for each base-constraint individually and then concatenates the generated prompts together at the end. For a base-constraint, it follows a pre-order traversal of the subtree to modify the initial template \"Please generate a {generation_level} with @... {tagert_level},\" where \"@...\" serves as a placeholder.\nAlthough the rule-based instruction prompts are natural enough for all examples in COLLIE-v1, there might be some edge cases where the rule-based instructions are not fluent enough for newly specified constraints. To address this, we offer an option to utilize language models to enhance the rule-based instructions. We employ the following prompt for the language model to refine the instructions: \"Please rewrite the following paragraph to improve fluency without altering the original meaning. You should provide the revised paragraph directly. Original paragraph: {prompt}.\"\nThe renderer is independent of the constraint construction and can be easily extended with new rules for parsing and mapping to instruction templates."
        },
        {
            "heading": "A.2 FEEDBACK RENDERING",
            "text": "We further extend our framework to provide natural language feedback when the extracted value of the generated text differs from the target value. Similar to instruction rendering, we first employ a rule-based renderer to compose the feedback by modifying a template. We also provide an option to use language models to polish the generated feedback.\nFor instance, consider a constraint count(\u03be, char, \u2018v\u2032) = 2 \u2227 count(\u03be, char, \u2018i\u2032) = 3, while the generated word includes three \u2018v\u2019 and four \u2018i\u2019. Our framework can generate the following instructions and feedback:\n\u2022 INSTRUCTION: Please generate a word with exactly 2 character \u2018v\u2019 and exactly 3 character \u2018i\u2019.\n\u2022 GPT-POLISHED INSTRUCTION: Please generate a word that contains exactly 2 instances of the letter \u2018v\u2019 and exactly 3 instances of the letter \u2018i\u2019.\n\u2022 FEEDBACK: Your task is to generate a word with exactly 2 character \u2018v\u2019 and exactly 3 character \u2018i\u2019. However, you generate a word with 3 character \u2018v\u2019 and 4 character \u2018i\u2019.\n\u2022 GPT-POLISHED FEEDBACK: Your task was to generate a word with precisely 2 \u2018v\u2019 characters and precisely 3 \u2018i\u2019 characters. However, you generated a word with precisely 3 \u2018v\u2019 characters and precisely 4 \u2018i\u2019 characters.\nBy incorporating this feedback mechanism, our framework can provide explicit guidance for the language models to improve the generation quality and adhere to the specified constraints."
        },
        {
            "heading": "B DATASET",
            "text": ""
        },
        {
            "heading": "B.1 EXTRACTION OVERVIEW",
            "text": "The extraction phase is split into six steps.:\n1. Document loading. The document d, usually consisting of multiple paragraphs is loaded. 2. Text chunking. Each document is divided into paragraphs using a source-specific delimiter (e.g.\n\\n). To obtain sentences, we use the nltk (Bird, 2006) sentence tokenizer on each paragraph.\nTo obtain passages, we string together multiple consecutive paragraphs that survive filtering. To obtain word-level constraint targets, we iterate each si from an English language word list.\n3. Text filtering. The paragraph or sentence passes a source-specific filtering function that attempts to remove all strings that are not natural language, for instance copyright statements.\n4. Text post-processing. The paragraphs or sentences that survive filtering are post-processed to remove source-specific artifacts, such as Markdown formatting.\n5. Passage construction (passage-level only). Paragraphs and sentences pass through to the next step. Passages are constructed by appending as many consecutive paragraphs that survive filtering as possible. For instance, if a document contains paragraphs p1, ..., p9, and p4 is the only paragraph that is removed due to filtering, then we return two passages: (p1, p2, p3) and (p5, ..., p9). Each paragraph is joined by two newline characters within each passage.\n6. Constraint extraction. The sentence, paragraph, or passage-level string is passed to the constraint extractor that pulls out constraint targets from the string. This can either be done directly, such as directly extracting the total word count, or sweeping over a set of possible target values.\nFor each data source, we randomly sample 300 documents. For each document, we randomly sample up to 100 text sequences of the specified level (sentence, paragraph, or passage) for constraint extraction to prevent over-representation from very long documents. We then randomly sample up to 100 constraint targets for each constraint structure and data source. We now discuss source-specific details below:"
        },
        {
            "heading": "B.2 TEXT FILTERS",
            "text": "In this section, we describe the text filter heuristics in detail. Note that which filters to use are source-specific.\n\u2022 URL. This filter removes any string that contains a pattern that appears to be a URL. The pattern we find is expressed using the following regex: r\"(http(s)?://)?(www\\.)?[a-zA-Z0-9\\-]+\\.[a-zA-Z]{2,6} (\\.[a-zA-Z]{2,6})?(/[a-zA-Z0-9\\-]*)*(\\?[a-zA-Z0-9\\-=&]*)?\"\n\u2022 All caps. This filter removes any text that only contains capitalized letters, which may be indicative of a section heading.\n\u2022 No sentences. This is a filter that tries to detect strings without any valid sentences in the text. We first sentence tokenize the string. If no \u201csentence\u201d contains a period and has length greater than 2, then we remove the string. Otherwise, we keep it. Future improvements could use a parser or trained classifier.\n\u2022 Copyright. This filter removes copyright statements typically found at the end of articles. It removes any string that contains the copyright symbol \u201c\u00a9\u201d or where the uncased first word is \u201ccopyright\u201d.\n\u2022 Caption. This filter attempts to remove captions, such as those under diagrams or images. These strings typically follow the format: \u201cPhoto: a green car.\u201d. We heuristically detect such strings by rejecting any string where the number of characters to the left of the first \u201c:\u201d is less than six characters."
        },
        {
            "heading": "B.3 TEXT POST-PROCESSING",
            "text": "In this section, we describe the post-processing functions used on the strings. Note that which post-processing functions to use depends on the data-source.\n\u2022 Markdown removal. We remove markdown artifacts using the following substitution rule: (r\u2019(\\*\\*|__|\\*|_|backslash~\\~)(.*?)\\1\u2019, r\u2019\\2\u2019)\n\u2022 Consecutive whitespace. Consecutive whitespace is removed with the following substitution rule: (r\u2019\\s{2,}\u2019, \u2019 \u2019)\n\u2022 Single newline to space. Single newlines are replaced with a single space using the following substitution rule: (r\"(?<!\\n)\\n(?!\\n)\", \" \")\n\u2022 Bracket removal. We remove brackets from the text using the following substitution rule: (r\u2019\\[[\u02c6\\]]*\\]\u2019, \"\"). This is useful for removing references inside the text."
        },
        {
            "heading": "B.4 DATA SOURCES",
            "text": "Detailed statistics on the number of constraints extracted for each constraint structure for the grouped, and individual data sources are found in Figures 11 and 10 respectively. None of the datasets used contain PII, as far as authors are aware.\nWiki We use the 20220301.en train split of the dataset from Huggingface Lhoest et al. (2021). We split each document into paragraphs using two newlines as the delimiter (\\n\\n). We use three filters for Wiki: URL, caption, and no sentences. For our passage level constraint, we also omit any text that contains the vertical line character \"|\", as these were identified to often be tables. We use a Wiki specific post-processing function that removes any text before the first newline character, for any text that contains a newline character. We found that these are almost always section headings. Wiki is licensed under a CC BY-SA 3.0 license and GNU Free Documentation License1.\nCC-News We load from the train split of the cc_news dataset on Huggingface Datasets for convenience. We split each document into paragraphs using a single newline as the delimiter (\\n. We use four filters for CC-News: copyright, URL, cpation, and no sentences. We do not use any post-processing function. The TOS for this data can be found at https://commoncrawl.org/ terms-of-use/full/.\nGuten We use the processed dataset from Gutenberg, dammit: https://github.com/ aparrish/gutenberg-dammit. We split each document using two newlines as the delimiter (\\n\\n). We two filters for Guten: all caps, and no sentences. We post-process the text using four processors, applied in the following order: markdown removal, bracket removal, single newline to space, consecutive whitespace to single whitespace. All documents in Guten are in the public domain in the U.S.\nWords For word-level constraints, we iterate over the the words present in the following newlineseparated word list: http://www.gwicks.net/textlists/english3.zip. We conduct no filtering or post-processing on the words from the list.\nOur entire code, including those used for data extraction will be released under an MIT license.\n1See https://dumps.wikimedia.org/legal.html"
        },
        {
            "heading": "B.5 EXTENDING COLLIE FOR POS-TAGS",
            "text": "To showcase the flexibility of Collie to incorporate new concepts for constraints, we updated our repo to allow constraints on POS tags. For example, \u201cGenerate a sentence with verbs\u201d can be written as:\nc = Constraint( target_level=TargetLevel(\u2019word\u2019), transformation=Count(\u2019VERB\u2019), # specify target to be VERB relation=Relation(\u2019==\u2019), attribute=\u2019pos\u2019, # invoke the POS tagging model ) # s has 2 verbs s= \u2019Apple is looking at buying U.K. startup for \\$1 billion.\u2019 # True, because s has 2 verbs print(c.check(s, 2)) # False, because s does not have only 1 verb print(c.check(s, 1))\n\u201cGenerate a sentence with the third word being verb\u201d can be written as:\nc = Constraint( target_level=TargetLevel(\u2019word\u2019), transformation=Position(2), # specify the position relation=Relation(\u2019==\u2019), attribute=\u2019pos\u2019, # invoke the POS tagging model ) # 3rd word is \"looking\", verb s= \u2019Apple is looking at buying U.K. startup for $1 billion.\u2019 # True, because the third word of s is verb print(c.check(s, \u2019VERB\u2019)) # 3rd word is \"at\", not verb s= \u2019Apple looked at buying U.K. startup for $1 billion.\u2019 # False, because the third word of s is not verb print(c.check(s, \u2019VERB\u2019))\nOnce the attribute argument is specified as pos, it invokes the POS tagging model in the constraint class to first tag the text at the target level. In a similar pattern, other attributes such as sentiment of the sentences in a passage can also be incorporated in a similar fashion by specifying the attribute argument to sentiment and invoke a sentiment classifier on the target level (e.g., sentence)."
        },
        {
            "heading": "C ADDITIONAL EXPERIMENTAL RESULTS",
            "text": "We note that out of 1132 constraints, 2 constraint prompts are blocked by PaLM-2 API for the guardrailing reason:\n1. In ccnews_c07: Please generate a sentence containing the word \u2019charged\u2019, \u2019been\u2019, \u2019Father\u2019.\n2. In ccnews_c14: Please generate a passage with all paragraphs having the last sentence to be \u2019Gramercy\u2019s portfolio looks attractive relative to peers\u2019, \u2019I am going to add Gramercy Property Trust to my income portfolio this week\u2019, \u2019An investment in GPT yields 6.6 percent\u2019, \u2019The REIT\u2019s shares have slumped a whopping \u223c 15 percent in 2018, but are no longer oversold\u2019 respectively. Only generate the passage, without extra things like \u201cParagraph 1\" or \u201cAnswer:\"."
        },
        {
            "heading": "C.1 ONE-SHOT EXPERIMENTS",
            "text": "To understand if LLM performances are bottlenecked by the zero-shot instruction format and if example input-output pairs could boost performances, we did a preliminary one-shot prompting experiment using an internal version of the dataset before the current COLLIE-v1, where for each constraint structure, we use a fixed constraint and its corpus example as an example input-output pair attached before the constraint prompt. As shown below in Table 2, results are very similar for both GPT-3.5 and GPT-4, which suggests the task difficulty is mainly about generation under the constraint instead of understanding the constraint (by similar examples)."
        },
        {
            "heading": "C.2 CONSTRAINT SATISFACTION RATES",
            "text": "Figure 9 provides a comparison of constraint satisfaction rates for various models across all tasks. The performance of the models remains consistently high for a specific constraint structure, regardless of the data source. The satisfaction rates are summarized in heatmap Figures 9(f)-(g).\nFigures 10 and 11 provide detailed information about the dataset size and sample size for each model in the study. Specifically, we conducted 20 trials for each instruction prompt in the case of GPT-4 and GPT-3.5. For PaLM, a total of 30 trials were conducted for each instruction prompt. However, due to a certain failure rate, the number of generated texts may not be a multiple of the number of instruction prompts. Vicuna-7B and Alpaca-7B were each run for 10 trials, and they also experienced some low failure rates during the experiments."
        },
        {
            "heading": "C.3 ADDITIONAL EVALUATIONS",
            "text": "In addition to evaluating binary constraint satisfaction in general, it is also possible to evaluate particular aspects of text generation with respect to the constraint and extracted corpus text.\nWord validity evaluation. To determine the validity of word-level generations as English words, we cross-reference the generated words with the word list available at http://www.gwicks. net/textlists/english3.zip. Since this word list is not complete, we supplement it by including eight additional uncommon but valid English words: \u2019supercalifragilisticexpialidocious\u2019, \u2019pneumonoultramicroscopicsilicovolcanoconiosis\u2019, \u2019antidisestablishmentarianism\u2019, \u2019pseudopseudohypoparathyroidism\u2019, \u2019extraterrestrializationism\u2019, \u2019acceleratrix\u2019, \u2019circumlocutrix\u2019, and \u2019procrastinatrix\u2019.\nFigure 12 illustrates the performance comparison of different models in generating long words (word01). Notably, GPT-4 demonstrates superior performance compared to other models in generating long words. However, when faced with more challenging constraints, such as the requirement for the i-th letter to be \u2019r\u2019, all models fail to generate a word that satisfies the constraint (see Figure 9). In\nthis case, GPT-3.5 manages to generate valid words, while GPT-4 resorts to fabricating words like \"coordinasor\" to better conform to the constraints.\nRegarding task word03, only GPT-4 is capable of generating words that satisfy the constraint on the last character. However, it still frequently generates made-up words. None of the other models are able to generate valid words or strings that satisfy the given constraint.\nPassage coherence evaluation. In order to assess the coherence and flow of content within the generated paragraphs, we utilize GPT-4 as a third-party judge to provide coherence scores. For this evaluation, we employ the following prompt: \"Analyze the following passage, then conclude with the statement \u2019Thus, the coherency score is s,\u2019 where s is an integer ranging from 1 to 10.\" We conduct three separate samplings of coherence scores for each generated text and calculate the average score. This methodology allows us to quantitatively measure the overall coherence of the generated paragraphs and gauge their coherence in a relatively consistent and reliable manner.\nFigure 13 presents the coherence scores of generated passages for task pass01. Notably, both GPT-4 and GPT-3.5 consistently outperform the other models in terms of coherence. Furthermore, GPT-4 achieves a level of coherence that is comparable to the ground truth passages in the dataset."
        }
    ],
    "title": "COLLIE: SYSTEMATIC CONSTRUCTION OF CONSTRAINED TEXT GENERATION TASKS",
    "year": 2023
}