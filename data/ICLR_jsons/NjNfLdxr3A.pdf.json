{
    "abstractText": "Low-rank adapation (LoRA) is a popular method that reduces the number of trainable parameters when finetuning large language models, but still faces acute storage challenges when scaling to even larger models or deploying numerous peruser or per-task adapted models. In this work, we present Vector-based Random Matrix Adaptation (VeRA)1, which significantly reduces the number of trainable parameters compared to LoRA, yet maintains the same performance. It achieves this by using a single pair of low-rank matrices shared across all layers and learning small scaling vectors instead. We demonstrate its effectiveness on the GLUE and E2E benchmarks, image classification tasks, and show its application in instruction-tuning of 7B and 13B language models.",
    "authors": [
        {
            "affiliations": [],
            "name": "Dawid J. Kopiczko"
        },
        {
            "affiliations": [],
            "name": "Tijmen Blankevoort"
        },
        {
            "affiliations": [],
            "name": "Yuki M. Asano"
        }
    ],
    "id": "SP:a9a1fdede228f4c1ea9f006cac9f0450a47b67fe",
    "references": [
        {
            "authors": [
                "Armen Aghajanyan",
                "Sonal Gupta",
                "Luke Zettlemoyer"
            ],
            "title": "Intrinsic dimensionality explains the effectiveness of language model fine-tuning",
            "venue": "In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),",
            "year": 2021
        },
        {
            "authors": [
                "H. Bal",
                "D. Epema",
                "C. de Laat",
                "R. van Nieuwpoort",
                "J. Romein",
                "F. Seinstra",
                "C. Snoek",
                "H. Wijshoff"
            ],
            "title": "A medium-scale distributed system for computer science research: Infrastructure for the long term",
            "year": 2016
        },
        {
            "authors": [
                "Lukas Bossard",
                "Matthieu Guillaumin",
                "Luc Van Gool"
            ],
            "title": "Food-101 \u2013 mining discriminative components with random forests",
            "venue": "In European Conference on Computer Vision,",
            "year": 2014
        },
        {
            "authors": [
                "Radford",
                "Ilya Sutskever",
                "Dario Amodei"
            ],
            "title": "Language models are few-shot learners",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Gong Cheng",
                "Junwei Han",
                "Xiaoqiang Lu"
            ],
            "title": "Remote sensing image scene classification: Benchmark and state of the art",
            "venue": "Proceedings of the IEEE,",
            "year": 2017
        },
        {
            "authors": [
                "Wei-Lin Chiang",
                "Zhuohan Li",
                "Zi Lin",
                "Ying Sheng",
                "Zhanghao Wu",
                "Hao Zhang",
                "Lianmin Zheng",
                "Siyuan Zhuang",
                "Yonghao Zhuang",
                "Joseph E. Gonzalez",
                "Ion Stoica",
                "Eric P. Xing"
            ],
            "title": "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023",
            "venue": "URL https: //lmsys.org/blog/2023-03-30-vicuna/",
            "year": 2023
        },
        {
            "authors": [
                "Jia Deng",
                "Wei Dong",
                "Richard Socher",
                "Li-Jia Li",
                "Kai Li",
                "Li Fei-Fei"
            ],
            "title": "Imagenet: A large-scale hierarchical image database",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2009
        },
        {
            "authors": [
                "Tim Dettmers",
                "Artidoro Pagnoni",
                "Ari Holtzman",
                "Luke Zettlemoyer"
            ],
            "title": "Qlora: Efficient finetuning of quantized llms",
            "venue": "arXiv preprint arXiv:2305.14314,",
            "year": 2023
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "Lucas Beyer",
                "Alexander Kolesnikov",
                "Dirk Weissenborn",
                "Xiaohua Zhai",
                "Thomas Unterthiner",
                "Mostafa Dehghani",
                "Matthias Minderer",
                "Georg Heigold",
                "Sylvain Gelly",
                "Jakob Uszkoreit",
                "Neil Houlsby"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Jonathan Frankle",
                "Michael Carbin"
            ],
            "title": "The lottery ticket hypothesis: Finding sparse, trainable neural networks",
            "venue": "In ICLR,",
            "year": 2019
        },
        {
            "authors": [
                "Jonathan Frankle",
                "David J. Schwab",
                "Ari S. Morcos"
            ],
            "title": "Training batchnorm and only batchnorm: On the expressive power of random features in {cnn}s",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification",
            "venue": "In 2015 IEEE International Conference on Computer Vision (ICCV),",
            "year": 2015
        },
        {
            "authors": [
                "Neil Houlsby",
                "Andrei Giurgiu",
                "Stanislaw Jastrzebski",
                "Bruna Morrone",
                "Quentin de Laroussilhe",
                "Andrea Gesmundo",
                "Mona Attariyan",
                "Sylvain Gelly"
            ],
            "title": "Parameter-efficient transfer learning for nlp, 2019",
            "year": 2019
        },
        {
            "authors": [
                "Edward J Hu",
                "yelong shen",
                "Phillip Wallis",
                "Zeyuan Allen-Zhu",
                "Yuanzhi Li",
                "Shean Wang",
                "Lu Wang",
                "Weizhu Chen"
            ],
            "title": "LoRA: Low-rank adaptation of large language models",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Alex Krizhevsky"
            ],
            "title": "Learning multiple layers of features from tiny images",
            "venue": "Technical report,",
            "year": 2009
        },
        {
            "authors": [
                "Chunyuan Li",
                "Heerad Farkhoor",
                "Rosanne Liu",
                "Jason Yosinski"
            ],
            "title": "Measuring the intrinsic dimension of objective landscapes",
            "venue": "In International Conference on Learning Representations,",
            "year": 2018
        },
        {
            "authors": [
                "Zhaojiang Lin",
                "Andrea Madotto",
                "Pascale Fung"
            ],
            "title": "Exploring versatile generative language model via parameter-efficient transfer learning. In Findings of the Association for Computational Linguistics: EMNLP 2020",
            "venue": "pp. 441\u2013459,",
            "year": 2020
        },
        {
            "authors": [
                "Xiao Liu",
                "Kaixuan Ji",
                "Yicheng Fu",
                "Weng Lam Tam",
                "Zhengxiao Du",
                "Zhilin Yang",
                "Jie Tang"
            ],
            "title": "Ptuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks",
            "venue": "arXiv preprint arXiv:2110.07602,",
            "year": 2021
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov"
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach, 2019",
            "year": 2019
        },
        {
            "authors": [
                "Kevin Lu",
                "Aditya Grover",
                "Pieter Abbeel",
                "Igor Mordatch"
            ],
            "title": "Frozen pretrained transformers as universal computation engines",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "Sourab Mangrulkar",
                "Sylvain Gugger",
                "Lysandre Debut",
                "Younes Belkada",
                "Sayak Paul"
            ],
            "title": "Peft: State-of-the-art parameter-efficient fine-tuning methods",
            "venue": "https://github.com/ huggingface/peft,",
            "year": 2022
        },
        {
            "authors": [
                "M-E. Nilsback",
                "A. Zisserman"
            ],
            "title": "Automated flower classification over a large number of classes",
            "venue": "In Proceedings of the Indian Conference on Computer Vision, Graphics and Image Processing,",
            "year": 2008
        },
        {
            "authors": [
                "Jekaterina Novikova",
                "Ond\u0159ej Du\u0161ek",
                "Verena Rieser"
            ],
            "title": "The E2E dataset: New challenges for end-to-end generation",
            "venue": "In Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue,",
            "year": 2017
        },
        {
            "authors": [
                "Hao Peng",
                "Nikolaos Pappas",
                "Dani Yogatama",
                "Roy Schwartz",
                "Noah Smith",
                "Lingpeng Kong"
            ],
            "title": "Random feature attention",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Jonas Pfeiffer",
                "Aishwarya Kamath",
                "Andreas R\u00fcckl\u00e9",
                "Kyunghyun Cho",
                "Iryna Gurevych"
            ],
            "title": "AdapterFusion: Non-destructive task composition for transfer learning",
            "venue": "In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,",
            "year": 2021
        },
        {
            "authors": [
                "Alec Radford",
                "Jeff Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "year": 2019
        },
        {
            "authors": [
                "V. Ramanujan",
                "M. Wortsman",
                "A. Kembhavi",
                "A. Farhadi",
                "M. Rastegari"
            ],
            "title": "What\u2019s hidden in a randomly weighted neural network",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2020
        },
        {
            "authors": [
                "Andreas R\u00fcckl\u00e9",
                "Gregor Geigle",
                "Max Glockner",
                "Tilman Beck",
                "Jonas Pfeiffer",
                "Nils Reimers",
                "Iryna Gurevych"
            ],
            "title": "AdapterDrop: On the efficiency of adapters in transformers",
            "venue": "In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,",
            "year": 2021
        },
        {
            "authors": [
                "Nataniel Ruiz",
                "Yuanzhen Li",
                "Varun Jampani",
                "Wei Wei",
                "Tingbo Hou",
                "Yael Pritch",
                "Neal Wadhwa",
                "Michael Rubinstein",
                "Kfir Aberman"
            ],
            "title": "Hyperdreambooth: Hypernetworks for fast personalization of text-to-image models, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Martin Schrimpf",
                "Idan Asher Blank",
                "Greta Tuckute",
                "Carina Kauf",
                "Eghbal A. Hosseini",
                "Nancy Kanwisher",
                "Joshua B. Tenenbaum",
                "Evelina Fedorenko"
            ],
            "title": "The neural architecture of language: Integrative modeling converges on predictive processing",
            "venue": "Proceedings of the National Academy of Sciences,",
            "year": 2021
        },
        {
            "authors": [
                "Rohan Taori",
                "Ishaan Gulrajani",
                "Tianyi Zhang",
                "Yann Dubois",
                "Xuechen Li",
                "Carlos Guestrin",
                "Percy Liang",
                "Tatsunori B. Hashimoto"
            ],
            "title": "Stanford alpaca: An instruction-following llama model",
            "venue": "https://github.com/tatsu-lab/stanford_alpaca,",
            "year": 2023
        },
        {
            "authors": [
                "Hugo Touvron",
                "Thibaut Lavril",
                "Gautier Izacard",
                "Xavier Martinet",
                "Marie-Anne Lachaux",
                "Timoth\u00e9e Lacroix",
                "Baptiste Rozi\u00e8re",
                "Naman Goyal",
                "Eric Hambro",
                "Faisal Azhar",
                "Aurelien Rodriguez",
                "Armand Joulin",
                "Edouard Grave",
                "Guillaume Lample"
            ],
            "title": "Llama: Open and efficient foundation language models, 2023a",
            "year": 2023
        },
        {
            "authors": [
                "Zhang",
                "Angela Fan",
                "Melanie Kambadur",
                "Sharan Narang",
                "Aurelien Rodriguez",
                "Robert Stojnic",
                "Sergey Edunov",
                "Thomas Scialom"
            ],
            "title": "Llama 2: Open foundation and fine-tuned chat models, 2023b",
            "year": 2023
        },
        {
            "authors": [
                "Mojtaba Valipour",
                "Mehdi Rezagholizadeh",
                "Ivan Kobyzev",
                "Ali Ghodsi"
            ],
            "title": "Dylora: Parameter efficient tuning of pre-trained models using dynamic search-free low-rank adaptation",
            "venue": "arXiv preprint arXiv:2210.07558,",
            "year": 2022
        },
        {
            "authors": [
                "Alex Wang",
                "Amanpreet Singh",
                "Julian Michael",
                "Felix Hill",
                "Omer Levy",
                "Samuel R. Bowman"
            ],
            "title": "GLUE: A multi-task benchmark and analysis platform for natural language understanding",
            "venue": "In International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "Elad Ben Zaken",
                "Shauli Ravfogel",
                "Yoav Goldberg"
            ],
            "title": "Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Longteng Zhang",
                "Lin Zhang",
                "Shaohuai Shi",
                "Xiaowen Chu",
                "Bo Li"
            ],
            "title": "Lora-fa: Memory-efficient low-rank adaptation for large language models fine-tuning",
            "venue": "arXiv preprint arXiv:2308.03303,",
            "year": 2023
        },
        {
            "authors": [
                "Qingru Zhang",
                "Minshuo Chen",
                "Alexander Bukharin",
                "Pengcheng He",
                "Yu Cheng",
                "Weizhu Chen",
                "Tuo Zhao"
            ],
            "title": "Adaptive budget allocation for parameter-efficient fine-tuning",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Lianmin Zheng",
                "Wei-Lin Chiang",
                "Ying Sheng",
                "Siyuan Zhuang",
                "Zhanghao Wu",
                "Yonghao Zhuang",
                "Zi Lin",
                "Zhuohan Li",
                "Dacheng Li",
                "Eric. P Xing",
                "Hao Zhang",
                "Joseph E. Gonzalez",
                "Ion Stoica"
            ],
            "title": "Judging llm-as-a-judge with mt-bench and chatbot arena, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Bojia Zi",
                "Xianbiao Qi",
                "Lingzhi Wang",
                "Jianan Wang",
                "Kam-Fai Wong",
                "Lei Zhang"
            ],
            "title": "Deltalora: Fine-tuning high-rank parameters with the delta of low-rank matrices",
            "venue": "arXiv preprint arXiv:2309.02411,",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "In the era of increasingly large and complex language models, the challenge of efficient adaptation for specific tasks has become more important than ever. While these models provide powerful capabilities, their extensive memory requirements pose a significant bottleneck, particularly when adapting them for personalized use. Consider, for example, a cloud-based operating system assistant that continuously learns from and adapts to individual user behaviors and feedback. The need to store multiple checkpoints of finetuned models for each user rapidly escalates the required storage, even more so when multiple tasks come into play.\nThe situation is further exacerbated when we look at the state-of-the-art models like GPT-4 (OpenAI, 2023). Finetuning techniques like LoRA (Hu et al., 2022), while effective, still introduce considerable memory overhead. As an illustrative example, applying LoRA with a rank of 16 to the query and value layers of GPT-3 (Brown et al., 2020) would demand at least 288MB of memory, if stored in singe-precision \u2013 at a million finetuned weights, e.g., one per user, that would amount to 275TB.\nGiven the recent proliferation of language models and their deployment in personalized assistants, edge devices, and similar applications, efficient adaptation methods are paramount. We believe there is untapped potential for even more efficient approaches. Previous work (Aghajanyan et al., 2021) pointed out the low intrinsic dimensionality of pretrained models\u2019 features. These studies reported numbers much lower than the trainable parameters used in LoRA, suggesting there is room for improvement.\nIn parallel to this, recent research has shown the surprising effectiveness of models utilizing random weights and projections (Peng et al., 2021; Ramanujan et al., 2020; Lu et al., 2022; Schrimpf et al., 2021; Frankle et al., 2021). Such models serve as the basis of our proposed solution, Vector-based Random Matrix Adaptation (VeRA), which minimizes the number of trainable parameters introduced during finetuning by reparametrizing the weights matrices. Specifically, we employ \u201cscaling vectors\u201d to adapt a pair of frozen random matrices shared between layers. With this approach, many more versions of the model can reside in the limited memory of a single GPU.\n\u2217dj.kopiczko@gmail.com \u2020Datasets were solely downloaded and evaluated by the University of Amsterdam. \u2021Qualcomm AI Research is an initiative of Qualcomm Technologies, Inc. 1Website: https://dkopi.github.io/vera/\nIn summary, our main contributions are as follows:\n\u2022 We introduce a novel finetuning method with no additional inference time cost. Our method further reduces the number of trainable parameters compared to the state-of-the-art LoRA method, while yielding comparable results.\n\u2022 We compare our approach with LoRA and other parameter-efficient adaptation methods on the natural language understanding (GLUE) and natural language generation (E2E) benchmarks, and compare against LoRA on instruction-following and image classification tasks.\n\u2022 We perform an ablation study to better understand the individual components of our method and their effects on performance."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Low-Rank Adaptation (LoRA). LoRA offers an innovative solution to the computational challenges posed by the finetuning of large pretrained language models. Introduced by Hu et al. (2022), the method employs low-rank matrices to approximate the weight changes during finetuning, effectively reducing the number of parameters that need to be trained. Among its advantages, LoRA significantly lowers the hardware barrier for finetuning by reducing the need for gradient calculation and optimizer state maintenance for most parameters. It can also work with quantized model weights (Dettmers et al., 2023), reducing the requirements even further. Furthermore, LoRA modules are easily swappable, making task-switching efficient and less resource-intensive. Importantly, and different to adapter-based finetuning approaches (Houlsby et al., 2019; Lin et al., 2020; Pfeiffer et al., 2021; Ru\u0308ckle\u0301 et al., 2021), LoRA incurs no additional inference time cost when deployed, as the trainable matrices can be merged with the frozen weights.\nBased on this, AdaLoRA (Zhang et al., 2023b) extends the LoRA method, introducing dynamic rank adjustment for the low-rank matrices during finetuning. The core idea is to optimally distribute the parameter budget by selectively pruning less important components of the matrices based on an importance metric.\nParameter Efficiency in Existing Methods While methods such as LoRA have shown significant improvements in finetuning performance, they still require a considerable amount of trainable parameters. According to Aghajanyan et al. (2021), the upper bound for intrinsic dimensions is much smaller than what is typically utilized in such methods. For instance, the d902 for RoBERTabase is reported to be 896, whereas authors of the LoRA paper reported using 0.3M trainable parameters for this model, suggesting that the parameter count could be reduced further.\nAlthough AdaLoRA takes steps in this direction by dynamically allocating parameters to more critical layers, we posit that a different approach could achieve substantial parameter reduction, while tolerating a marginal performance degradation. This sets the stage for the method we introduce in the following section.\nRandom Models and Projections. The concept of using random matrices and projections for model efficiency is supported by multiple strands of research. Frankle & Carbin (2019) identified that randomly-initialized neural networks contain subnetworks that are capable of reaching high performance when trained. Meanwhile, Ramanujan et al. (2020) revealed that there exist subnetworks that can achieve impressive results even in the absence of training. Aghajanyan et al. (2021) showed that training only a small number of parameters, randomly projected back into the full space, could achieve 90% of the full-parameter model performance. Ruiz et al. (2023) introduced a parameterefficient finetuning method for personalization of text-to-image models, utilising random frozen matrices inside LoRA. Other works (Lu et al., 2022; Schrimpf et al., 2021; Frankle et al., 2021) have shown that frozen, randomly initialized models, with small sections finetuned, can perform surprisingly well.\n2The smallest dimension d that provides a satisfactory solution, which is 90% of the full training metric, as defined by Li et al. (2018).\nCollectively, these works create a compelling case for the utilization of frozen random matrices in finetuning methods, providing both a theoretical and an empirical foundation for the approach taken in this paper."
        },
        {
            "heading": "3 METHOD",
            "text": "In this section, we introduce Vector-based Random Matrix Adaptation, a novel parameter-efficient finetuning method that builds upon and extends the state-of-the-art method, LoRA. The central innovation in VeRA lies in the reparameterization of the low-rank matrices. Specifically, we freeze a single pair of randomly initialized matrices, shared across all adapted layers, and introduce trainable scaling vectors that allow for layer-wise adaptation, as shown in Figure 1. Similarly to LoRA, trained scaling vectors along with low-rank matrices can be merged into original weights, eliminating additional inference latency."
        },
        {
            "heading": "3.1 METHOD FORMULATION",
            "text": "LoRA (Hu et al., 2022) finetunes a matrix product of two low-rank matrices to adapt large-language models for a new task. Formally, for a pretrained weight matrix W0 \u2208 Rm\u00d7n, the weight update \u2206W is constrained to a low-rank decomposition, as expressed in Equation 1\nh = W0x+\u2206Wx = W0x+BAx, (1)\nwhere we undeline the parameters updated via gradient descent. This approximation enables the model to keep the original weight W0 frozen while optimizing only the new low-rank matrices A and B. These matrices are much smaller in size than the original matrix due to their rank-reduced nature. A has shape m\u00d7 r and B has shape r \u00d7 n, where r \u226a min(m,n) serves as the bottleneck dimension. In contrast, our VeRA method is expressed as:\nh = W0x+\u2206Wx = W0x+ \u039bbB\u039bdAx (2)\nIn this approach, B and A are frozen, random, and shared across layers, while the scaling vectors b and d are trainable, and formally denoted by diagonal matrices \u039bb and \u039bd. This approach can effectively scale and disable rows and columns of both A and B, allowing for layer-wise adaptation with a minimal number of trainable parameters. Note that in this setup, B \u2208 Rm\u00d7r and A \u2208 Rr\u00d7n\nare not required to be low-rank. This is because they remain static and we do not need to store their values. Instead, varying r leads to a linear increase in the number of trainable parameters via d \u2208 R1\u00d7r."
        },
        {
            "heading": "3.2 PARAMETER COUNT",
            "text": ""
        },
        {
            "heading": "3 1 4.7M 18MB 2.4M 9.1MB",
            "text": ""
        },
        {
            "heading": "16 75.5M 288MB 2.8M 10.5MB",
            "text": "We use Ltuned to denote the number of finetuned layers and dmodel to represent the dimension of these layers. The number of trainable parameters in VeRA is then governed by |\u0398| = Ltuned\u00d7(dmodel+r), contrasting with LoRA\u2019s |\u0398| = 2 \u00d7 Ltuned \u00d7 dmodel \u00d7 r. Specifically, for the lowest rank (i.e., r = 1), VeRA requires approximately half the trainable parameters of LoRA. Moreover, as the rank increases, VeRA\u2019s parameter count increases by Ltuned for each increment, a substantial saving compared to LoRA\u2019s 2Ltuneddmodel. This parameter efficiency becomes notably significant in the context of extremely deep and wide models, such as GPT-3 (Brown et al., 2020), which has 96 attention layers and a hidden size of 12288.\nBuilding on this efficiency, the main advantage of VeRA is its minimal memory footprint for storing the trained weight adjustments. Because the random frozen matrices can be regenerated from a random number generator (RNG) seed, these do not need to be stored in memory. This substantially reduces the memory requirement, which is now limited to the bytes needed for the trained b and d vectors and a single RNG seed. The memory efficiency in comparison to LoRA is shown in Table 1."
        },
        {
            "heading": "3.3 INITIALIZATION STRATEGIES",
            "text": "\u2022 Shared Matrices: In our method, we employ Kaiming initialization (He et al., 2015) for the frozen low-rank matrices A and B. By scaling the values based on matrix dimensions, it ensures that a matrix product of A and B maintains a consistent variance for all ranks, eliminating the need to finetune the learning rate for each rank.\n\u2022 Scaling Vectors: The scaling vector b is initialized to zeros, which aligns with the initialization of matrix B in LoRA and ensures that the weight matrix is unaffected during the first forward pass. The scaling vector d is initialized with a single non-zero value across all its elements, thereby introducing a new hyperparameter that may be tuned for better performance.\nFigure 1 illustrates example initializations for the low-rank matrices and scaling vectors in VeRA. Specifically, the low-rank matrices are initialized using a normal distribution, and the d vector is initialized with ones. Note that alternative initializations, such as uniform distribution for A and B, and other non-zero constants for d, are also explored in our experiments."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": "In this section, we conduct a series of experiments to evaluate our finetuning method. We start by comparing our approach to LoRA and other baselines on the GLUE and E2E benchmarks. Following\nthis, we turn our attention to instruction-tuning of Llama models, and image classification with Vision Transformers. Next, we select one task and vary the rank for both methods, LoRA and VeRA, to examine how performance scales with the number of trainable parameters. Lastly, an ablation study sheds light on the importance of each component in our method, including the influence of different initializations.\nBaselines. We compare VeRA to the following baselines:\n\u2022 Full finetuning - the model is initialized with pretrained weights and all parameters are being trained.\n\u2022 Bitfit - this baseline involves the sole finetuning of bias vectors, keeping all other parameters fixed. This technique has been investigated in depth by Zaken et al. (2022).\n\u2022 Adapter tuning - initially introduced by Houlsby et al. (2019), involves the integration of adapter layers between the self-attention and MLP modules, followed by a residual connection. This setup includes two fully connected layers and a nonlinearity and is denoted as AdapterH. A variation by Lin et al. (2020), AdapterL, employs the adapter layer solely after the MLP module and subsequent to a LayerNorm. This closely resembles an alternative design suggested by Pfeiffer et al. (2021), referred to as AdapterP. Another baseline, termed AdapterDrop by Ru\u0308ckle\u0301 et al. (2021), enhances efficiency by omitting certain adapter layers and is represented as AdapterD.\n\u2022 LoRA (Hu et al., 2022) - as introduced in the earlier section."
        },
        {
            "heading": "4.1 GLUE BENCHMARK",
            "text": "We evaluate our approach on the General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2019), employing the RoBERTabase and RoBERTalarge models (Liu et al., 2019). For RoBERTabase we use a rank of 1024, and for RoBERTalarge a rank of 256. The shared matrices are initialized using the uniform version of Kaiming initialization as implemented in PyTorch (Paszke et al., 2019), with an initial value of 0.1 for the d vector.\nOur experimental setup generally aligns with that of Hu et al. (2022), applying our method to the query and value projection matrices in each self-attention module and fully training the classification head. Unlike Hu et al. (2022), who used an additional hyperparameter \u03b1 to adjust gradients for the adapted layers, we introduce separate learning rates for the classification head and the adapted layers. We determine the learning rates and the number of training epochs through hyperparameter tuning; for detailed settings, refer to the Table 8 in Appendix A. The batch size is set to 64 for RoBERTabase and 32 for RoBERTalarge, with maximum sequence lengths of 512 and 128 respectively.\nDue to time constraints and budget limitations, we omit the time-intensive MNLI and QQP tasks, thus forgoing the use of the MNLI trick3 for tasks MRPC, RTE, and STS-B. In line with Hu et al. (2022), we report the number of trainable parameters attributable to the finetuned layers, explicitly excluding the classification head, which is trained in a standard way. We perform 5 runs with different random seeds, recording the best epoch\u2019s outcome for each run, and report the median of these results.\nResults. Table 2 reveals that VeRA performs competitively with LoRA across both models, yet achieves these results with an order of magnitude fewer parameters."
        },
        {
            "heading": "4.2 E2E BENCHMARK",
            "text": "For the E2E benchmark (Novikova et al., 2017), we follow the experimental setup from Hu et al. (2022) and finetune the GPT-2 (Radford et al., 2019) Medium and Large models. For LoRA we use the implementation and set of hyperparameters provided in Hu et al. (2022), while for VeRA we change the rank and learning rate, both of which are tuned. Table with all hyperparameters used can be found in Appendix A.\n3For the RoBERTabase model and MRPC, RTE and STS-B tasks, Hu et al. (2022) initialized the model with the best weights finetuned on the MNLI task.\nResults. We report results from the last epoch. Table 3 shows that VeRA outperforms LoRA with 3 and 4 times less trainable parameters, for GPT2 Medium and Large respectively."
        },
        {
            "heading": "4.3 INSTRUCTION TUNING",
            "text": "Instruction tuning is a process by which language models are finetuned to follow specific instructions more effectively (Ouyang et al., 2022). We demonstrate the efficacy of VeRA in enabling Llama (Touvron et al., 2023a) and Llama2 (Touvron et al., 2023b) models to follow instructions using only 1.6M and 2.4M trainable parameters, for 7B and 13B variants respectively, in contrast to 159.9M and 250.3M trainable parameters when employing LoRA with a rank of 64 as proposed by Dettmers et al. (2023).\nWe perform finetuning using both LoRA and VeRA, by applying both methods on all linear layers except the top one, similarly to Dettmers et al. (2023). Additionally, we leverage the quantization techniques from Dettmers et al. (2023) to train the model on a single GPU.\nFor our experiment, we employ the Alpaca dataset (Taori et al., 2023), specifically its cleaned version4. This dataset comprises 51K instructions and demonstrations and is suitable for instructiontuning. The cleaned version corrects multiple issues such as hallucinations, merged instructions, and empty outputs. We train for one epoch, preceded by a learning rate sweep.\nWe evaluate finetuned models on MT-Bench (Zheng et al., 2023), by generating model responses to a pre-defined set of 80 multi-turn questions and subsequently evaluating these using GPT-4 (OpenAI, 2023). GPT-4 reviews the answers and assigns a quantitative score on a scale of 10 to each response. We present the average scores alongside the number of trainable parameters in Table 4.\nWe find that despite the 100x reduction in the number of trainable parameters, our method closely matches the performance of LoRA-based finetuning."
        },
        {
            "heading": "4.4 IMAGE CLASSIFICATION",
            "text": "To evaluate the method on the image classification task, we adapt Vision Transformer (ViT) (Dosovitskiy et al., 2021), Base and Large variants, on datasets - CIFAR100 (Krizhevsky, 2009), Food101 (Bossard et al., 2014), Flowers102 (Nilsback & Zisserman, 2008), and RESISC45 (Cheng et al., 2017). For each dataset we train on a subset of 10 samples per class, and evaluate on the full test set (CIFAR100, Food101, Flowers102) or on all the remaining samples (RESISC45). We use weights of ViT models pretrained on the ImageNet-21k (Deng et al., 2009) dataset.\nWe evaluated LoRA and VeRA methods applied on the query and value layers of ViT, along with two baselines - fully-finetuned model (referred to as Full), and training the classification head only (referred to as Head). Similarly to the GLUE benchmark, we use rank 8 for LoRA, and rank 256 for VeRA. We tuned learning rates for all methods and reported results after 10 epochs in Table 5. The reported parameter count excludes the classification head, which has to be trained in all methods.\nWe find that VeRA approaches performance of LoRA on the Base model for three datasets and outperforms it for Flowers102, despite using over 10x fewer trainable parameters. For ViT-Large, it outperforms LoRA for three datasets: CIFAR100, Flowers102 and RESISC45."
        },
        {
            "heading": "4.5 SCALING THE NUMBER OF TRAINABLE PARAMETERS",
            "text": "Finally, we investigate the trade-offs involved in parameter scalability for both LoRA and our method using the RoBERTalarge model on the RTE task from the GLUE benchmark. We use a set of ranks r = {1, 4, 16, 64, 256, 1024} for VeRA and r = {1, 2, 4, 8, 16, 32, 64} for LoRA, and observe the trade-off between trainable parameters and the accuracy. We replicate each configuration five times for different random seeds, and report the median of results. For LoRA, we employ the HuggingFace PEFT (Mangrulkar et al., 2022) implementation, adhering to the hyperparameters specified in Hu et al. (2022). Our own method uses the same hyperparameters as employed in the\n4https://huggingface.co/datasets/yahma/alpaca-cleaned\n10 5\n10 6\n# Trainable Parameters\n83\n84\n85\n86\nA cc\nur ac\ny [%\n]\nVeRA LoRA\nFigure 2: Performance of LoRA and VeRA methods for varying ranks on RTE task.\n0 5 10 15 20 Layer\n4\n6\n8\n10\n12\n||d || 2\nquery value\nFigure 3: Magnitude of the adapted d vector for query and value matrices across layers for RoBERTa-L on the RTE task.\nRTE experiments from the previous subsection. The results, depicted in Figure 2, reveal that our method is significantly more parameter-efficient. Notably, when the higher-rank VeRA has the same number of parameters as standard LoRA, it outperforms LoRA by 4 accuracy percentage points."
        },
        {
            "heading": "4.6 ABLATION STUDY",
            "text": "In this section, we conduct an ablation study to examine the impact of individual components of our method. All subsequent experiments focus on the MRPC and RTE tasks and utilize the RoBERTalarge model. We adhere to the hyperparameters used in previous experiments, modifying only the component under investigation for each test. Each experiment is run with 5 random seeds, and we report the mean and standard deviation of the results.\nSingle Scaling Vector We first investigate the necessity of both the d and b scaling vectors in our method. We create two ablation setups: one that excludes d (termed as only b) and another that omits b (termed as only d). In the only d setup, d is initialized with zeros. As shown in Table 6, omitting either scaling vector compromises performance. The only d configuration performs slightly better than its only b counterpart. This disparity in performance underscores the higher expressiveness of\nthe d scaling vector over the b vector. Specifically, d modulates the rows of both low-rank matrices, thereby influencing a broader aspect of the final constructed matrix. In contrast, b only scales the rows of the final matrix resulting from the product of the low-rank matrices.\nInitialization of Shared Matrices We examine three different initialization schemes for the shared matrices: Kaiming normal, Kaiming uniform, and uniform initialization within the range [0, 0.1]. As per the results in Table 6, both Kaiming initializations outperform the uniform range initialization, with uniform variant having slightly better results than the normal one.\nInitialization of Scaling Vector We further explore the impact of the initialization values for the d vector. Experiments are conducted with dinit set at 1.0, 10\u22121, and 10\u22127. The results in Table 6 show that the choice of dinit significantly influences the method\u2019s performance; in the settings we examined, values 10\u22121 and 10\u22127 outperformed 1.0, potentially offering more flexibility in the optimization process through early sign changes in selected rows of the frozen matrices.\nMagnitude of Adaptation In Figure 3 we provide a visualisation of the magnitude of the changes of the d vectors after finetuning on RTE task. Because the low-rank frozen matrices remain the same for each layer, we can directly compare the length of the d vector across layers to account for its relative adaptation. Overall, we find that the largest adaptation happens for query matrices compared to the value ones, indicating a larger need or ease for finetuning a model there. Furthermore, similar to previous efficient adaptation methods\u2019 findings (Zhang et al., 2023b; Liu et al., 2021) we also observe a higher adaptation for the later layers compared to earlier ones.\nSharing Random Matrices We conduct experiments on RTE, MRPC, CoLA, and STS-B tasks to assess the impact of sharing random matrices on the performance. We evaluate two setups - one with random matrices shared across all adapted layers, and another with uniquely generated ones. Results in Table 7 show that the mean performance is identical in case of tasks RTE and STS-B, and there is a slight improvement for MRPC and CoLA when using unique matrices."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "In this work, we introduce a finetuning method that significantly reduces the number of trainable parameters compared to LoRA, yielding similar or better results on downstream tasks. Specifically, it achieved ten-fold reduction in parameters yielding the same performance on the GLUE benchmark for RoBERTalarge, ten-fold reduction on image classification tasks, and three-fold reduction on the E2E benchmark. This method is particularly well-suited for scenarios that require frequent swapping of numerous finetuned models, such as cloud-based AI services personalized for individual users. Due to the minimal size of the scaling vectors, many versions can reside in the limited memory of a single GPU, thus substantially improving serving efficiency and removing the bottleneck of loading specific models into memory. While the current study focuses on language and vision models with Transformer architecture, the applicability of the method across different architectures and domains remains an area for future research. Moreover, the performance of the method may benefit from additional refinements, such as dynamic parameter budget allocation, or different initialization and regularization techniques."
        },
        {
            "heading": "ACKNOWLEDGEMENTS",
            "text": "This work is financially supported by Qualcomm Technologies Inc., the University of Amsterdam and the allowance Top consortia for Knowledge and Innovation (TKIs) from the Netherlands Ministry of Economic Affairs and Climate Policy. We also acknowledge the use of the National Supercomputer Snellius and Distributed ASCI Supercomputer 6 (Bal et al., 2016) for essential computational tasks."
        },
        {
            "heading": "A HYPERPARAMETERS",
            "text": "In Table 8, we provide the hyperparameters used for the GLUE benchmark in the main paper. Note that due to our academic compute we were not able to run full grid searches on any hyperparameters. We only evaluated different learning rates and number of epochs and even relied on existing configurations of LoRA (Optimizer, Warmup ratio, LR schedule)."
        },
        {
            "heading": "B RELATIVE PERFORMANCE GAIN.",
            "text": "Figure 4 quantifies the efficiency of each method in terms of performance gains per 1K trainable parameters. For a focused comparison, we select the RTE task and RoBERTalarge model.\nTo establish a baseline, we conduct auxiliary experiments where only the classification head is trained while the remainder of the model is frozen. This baseline is constructed using the same hyperparameters as in our VeRA method. We then evaluate the performance gain attributable to each method, normalized by the additional trainable parameters introduced, relative to the baseline. The results clearly show that VeRA yields the highest performance gain per 1K trainable parameters.\nC IMPACT ON TRAINING TIME AND MEMORY USAGE\nTo evaluate the training time and GPU memory benefits of our method, we conducted a comparison between LoRA and VeRA while fine-tuning LLaMA 7B with the same rank (64) on instruction tuning dataset, introduced earlier in this work. The results are summarized in Table 12:\nWhile VeRA includes more operations than LoRA because of the additional vector multiplies in the forward pass, we find that it only results in a modest 1.8% increase in training time. For the GPU memory, we observe a 7.4% reduction in memory usage with VeRA, as it does not require storing optimizer states and gradients for shared random matrices."
        },
        {
            "heading": "D SIMILARITIES OF TRAINED WEIGHTS",
            "text": "We compared the weights trained with LoRA and VeRA at a single rank of 64 across all query layers. For each method and adapted layer, we constructed a weight difference. In LoRA\u2019s case, this involved the multiplication of two low-rank matrices, while for VeRA, it also included multiplication by scaling vectors. We then calculated the cosine similarity of these flattened weights. Additionally, we compared the similarity between trained LoRA weights and randomly initialized matrices as a baseline: We find that similarities of VeRA to LoRA are on average 2e-3 while LoRA to random matrices is -8e-5.\nIn Figure 5 we can see a notable increase in similarity between the trained weights, particularly in the latter layers. This observation aligns with our earlier findings (Figure 3) that the highest adaptation occurs in these layers. These results support the notion that VeRA can approximate the weights trained with LoRA."
        },
        {
            "heading": "E EXPRESSIVITY OF VERA",
            "text": "We conducted an experiment on the expressivity of LoRA and VeRA on the task of fitting random square 10x10 matrices, with results seen in Figure 6. For given number of trainable parameters, both methods perform equally well, with VeRA providing more flexibility, e.g. by allowing for much lower parametrization - below LoRA\u2019s rank 1.\nF INSTRUCTION-TUNING WITH VICUNA EVAL\nResults and samples from evaluation of instruction tuned Llama 7B model with Vicuna Eval (Chiang et al., 2023), predecessor of MT-Bench. The model has been finetuned on a 10K subset of cleaned Alpaca dataset.\nLoRA VeRA\nWhat if Isaac Newton had focused on biology instead of physics?\nA N\nS W\nE R Isaac Newton was a brilliant scientist who made\nsignificant contributions to both physics and biology. While he is best known for his work in physics, he also made important discoveries in biology, including the discovery of the laws of motion and the theory of gravity.\nIf Newton had focused on biology instead of physics, he may have made even greater contributions to the field. He was a skilled observer and experimenter, and his keen intellect would have allowed him to make important discoveries in biology. He may have studied the anatomy of plants and animals, or the behavior of living organisms, or the interactions between different species.\nNewton\u2019s work in physics laid the foundation for many of the discoveries made in biology, including the understanding of the role of gravity in the movement of objects. His work in biology may have built upon this foundation, leading to even greater advances in the field.\nIn summary, if Newton had focused on biology instead of physics, he may have made even greater contributions to the field, and his work in biology would have been just as significant as his work in physics.\nIf Isaac Newton had focused on biology instead of physics, he may have made significant contributions to the field of biology. Newton was a brilliant scientist and his work in physics revolutionized the field. However, he also had a keen interest in biology and made several observations and insights about the natural world.\nFor example, Newton was the first to describe the process of photosynthesis, the process by which plants convert sunlight into energy. He also made observations about the behavior of animals, including the way they move and the way they interact with their environment.\nNewton\u2019s work in biology was not as extensive as his work in physics, but it shows that he had a deep interest in the natural world and a desire to understand it. If he had focused on biology instead of physics, he may have made even greater contributions to the field of biology.\nR E\nV IE W Assistant 1\u2019s response is somewhat helpful and relevant, but it contains a major inaccuracy. The assistant claims that Isaac Newton made significant contributions to biology, including the discovery of the laws of motion and the theory of gravity. However, these are actually contributions to physics, not biology. The assistant also speculates on what Newton might have studied if he had focused on biology, but these are purely hypothetical and not based on any historical evidence. The assistant\u2019s response is detailed and well-structured, but the inaccuracy undermines its overall quality. Assistant 2\u2019s response is less helpful and relevant, and it contains a major inaccuracy. The assistant claims that Isaac Newton was the first to describe the process of photosynthesis. This is incorrect; the process of photosynthesis was not understood until the 19th century, long after Newton\u2019s time. The assistant also makes some general observations about Newton\u2019s interest in the natural world, but these are not specific to biology and do not provide a clear answer to the user\u2019s question. The assistant\u2019s response is less detailed and less well-structured than Assistant 1\u2019s response, and the inaccuracy further reduces its quality.\nS C\nO R\nE\n4 2\nLoRA VeRA\nHow many times has the Earth orbited the Sun since the beginning of life? Try to explain your answer. Your explanation should take the reader through your reasoning step-by-step.\nA N\nS W\nE R The Earth has orbited the Sun approximately\n5.5 billion times since the beginning of life. This is based on the assumption that life on Earth began approximately 4.5 billion years ago, and that the Earth has been orbiting the Sun for approximately 4.5 billion years.\nTo calculate the number of times the Earth has orbited the Sun, we can use the formula for the circumference of a circle, which is 2 * pi * r, where pi is approximately 3.14 and r is the radius of the circle. The radius of the Earth is approximately 6,371 kilometers, so the circumference of the Earth is approximately 2 * pi * 6,371 = 12,742,200 kilometers.\nThe Earth orbits the Sun at an average distance of approximately 149,600,000 kilometers, so the Earth travels approximately 149,600,000 kilometers per orbit. Therefore, the Earth has orbited the Sun approximately 149,600,000 / 12,742,200 = 1.17 times.\nThis means that the Earth has orbited the Sun approximately 5.5 billion times since the beginning of life.\nIt is difficult to determine exactly how many times the Earth has orbited the Sun since the beginning of life, as the exact age of the Earth is still a matter of debate. However, we can make an estimate based on the current scientific consensus. The age of the Earth is estimated to be between 4.54 and 4.58 billion years old, with the most recent estimates placing it at 4.54 billion years. This means that the Earth has been orbiting the Sun for approximately 4.54 billion years. To calculate the number of times the Earth has orbited the Sun since the beginning of life, we can use the formula: [N = 1\nT \u00d7 T 365.25 ]\nwhere N is the number of orbits, T is the age of the Earth, and 365.25 is the number of days in a year. Substituting the values into the formula, we get: [N = 1\n4.54\u00d7109 \u00d7 4.54\u00d7109 365.25\n] which simplifies to: [N = 1.2\u00d7 1010] This means that the Earth has orbited the Sun approximately 1.2 trillion times since the beginning of life. It is important to note that this estimate is based on the current scientific consensus, and that the actual number of orbits may vary slightly due to uncertainties in the age of the Earth. Additionally, the age of the Earth is constantly being revised as new evidence is discovered, so this number may change in the future.\nR E\nV IE W Assistant 1\u2019s response is confusing and incorrect. The assistant tries to calculate the number of Earth\u2019s orbits around the Sun by using the formula for the circumference of a circle and the Earth\u2019s radius, which is irrelevant to the question. The assistant then incorrectly states that the Earth has orbited the Sun approximately 5.5 billion times, which contradicts the initial assumption that life on Earth began approximately 4.5 billion years ago. Assistant 2\u2019s response, on the other hand, is more accurate and detailed. The assistant correctly states that the exact number of Earth\u2019s orbits around the Sun since the beginning of life is difficult to determine due to uncertainties in the age of the Earth. However, the assistant provides a reasonable estimate based on the current scientific consensus on the age of the Earth. The assistant also correctly uses the formula for calculating the number of orbits and provides a clear, step-by-step explanation of the calculation. However, the assistant made a mistake in the final calculation, stating that the Earth has orbited the Sun approximately 1.2 trillion times, which is significantly higher than the actual estimate. Despite this error, Assistant 2\u2019s response is more helpful, relevant, and accurate than Assistant 1\u2019s response.\nS C\nO R\nE\n4 7\nLoRA VeRA\nImplement a program to find the common elements in two arrays without using any extra data structures.\nA N\nS W\nE R Here is a Python program that can find the\ncommon elements between two arrays without using any extra data structures: python def common elements(arr1, arr2): # Sort both arrays arr1.sort() arr2.sort()\n# Find the length of the shorter array length = min(len(arr1), len(arr2))\n# Create a set to store the common elements common elements = set() # Loop through the shorter array for i in range(length): # Check if the current element is present in the shorter array if arr1[i] in arr2: # Add the element to the common elements set common elements.add(arr1[i])\n# Return the common elements return common elements\nThis program takes two arrays as input and returns a set containing the common elements between the two arrays. The program first sorts both arrays to ensure that the elements are in the same order. Then, it finds the length of the shorter array and creates a set to store the common elements. It then loops through the shorter array and checks if the current element is present in the shorter array. If it is, the element is added to the common elements set. Finally, the common elements are returned as a set.\nHere is a Python program that can find the common elements in two arrays without using any extra data structures:\npython def common elements(arr1, arr2): # Initialize a set to store the common elements common set = set()\n# Iterate over the first array for element in arr1: # If the element is already in the set, skip it if element in common set: continue\n# Otherwise, add it to the set common set.add(element)\n# Iterate over the second array for element in arr2: # If the element is already in the set, skip it if element in common set: continue\n# Otherwise, add it to the set common set.add(element)\n# Return the set of common elements return common set\nThis program takes two arrays as input and returns a set containing the common elements between the two arrays.\nR E\nV IE W Assistant 1: The assistant has provided a solution, but it does not adhere to the problem statement. The problem specifically states not to use any extra data structures, but the assistant\u2019s solution uses a set to store the common elements. The assistant\u2019s comments are clear and explain the logic of the code well. However, the code itself is not efficient as it uses the \u2019in\u2019 operator to check if an element is in the array, which has a time complexity of O(n). This makes the overall time complexity of the solution O(n \u02c6 2), which is not efficient. Assistant 2: The assistant\u2019s solution also does not adhere to the problem statement as it uses a set to store the common elements. The assistant\u2019s comments are clear and explain the logic of the code well. However, the code itself is not correct. The assistant\u2019s solution adds all elements from both arrays to the set, which means it will not find the common elements but rather all unique elements from both arrays. This is a fundamental misunderstanding of the problem.\nS C\nO R\nE\n4 2"
        }
    ],
    "title": "VERA: VECTOR-BASED RANDOM MATRIX ADAPTATION",
    "year": 2024
}