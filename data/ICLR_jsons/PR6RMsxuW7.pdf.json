{
    "abstractText": "Despite recent advancements, deep reinforcement learning (DRL) still struggles at learning sparse-reward goal-directed tasks. Classical planning excels at addressing hierarchical tasks by employing symbolic knowledge, yet most of the methods rely on assumptions about pre-defined subtasks. To bridge the best of both worlds, we propose a framework that integrates DRL with classical planning by automatically inducing task structures and substructures from a few demonstrations. Specifically, genetic programming is used for substructure induction where the program model reflects prior domain knowledge of effect rules. We compare the proposed framework to state-of-the-art DRL algorithms, imitation learning methods, and an exploration approach in various domains. Experimental results show that our proposed framework outperforms all the abovementioned algorithms in terms of sample efficiency and task performance. Moreover, our framework achieves strong generalization performance by effectively inducing new rules and composing task structures. Ablation studies justify the design of our induction module and the proposed genetic programming procedure.",
    "authors": [
        {
            "affiliations": [],
            "name": "TASK SUBSTRUCTURES"
        },
        {
            "affiliations": [],
            "name": "Jung-Chun Liu"
        },
        {
            "affiliations": [],
            "name": "Chi-Hsien Chang"
        },
        {
            "affiliations": [],
            "name": "Shao-Hua Sun"
        },
        {
            "affiliations": [],
            "name": "Tian-Li Yu"
        }
    ],
    "id": "SP:729d72de4616d285c8a49f790216bcafbbd785b9",
    "references": [
        {
            "authors": [
                "David Abel",
                "Dilip Arumugam",
                "Lucas Lehnert",
                "Michael Littman"
            ],
            "title": "State abstractions for lifelong reinforcement learning",
            "venue": "In International Conference on Machine Learning,",
            "year": 2018
        },
        {
            "authors": [
                "Jacob Andreas",
                "Dan Klein",
                "Sergey Levine"
            ],
            "title": "Modular multitask reinforcement learning with policy sketches",
            "venue": "In International Conference on Machine Learning,",
            "year": 2017
        },
        {
            "authors": [
                "Ankuj Arora",
                "Humbert Fiorino",
                "Damien Pellier",
                "Marc M\u00e9tivier",
                "Sylvie Pesty"
            ],
            "title": "A review of learning planning action models",
            "venue": "The Knowledge Engineering",
            "year": 2018
        },
        {
            "authors": [
                "Kai Arulkumaran",
                "Marc Peter Deisenroth",
                "Miles Brundage",
                "Anil Anthony Bharath"
            ],
            "title": "Deep reinforcement learning: A brief survey",
            "venue": "IEEE Signal Processing Magazine,",
            "year": 2017
        },
        {
            "authors": [
                "Bowen Baker",
                "Ilge Akkaya",
                "Peter Zhokov",
                "Joost Huizinga",
                "Jie Tang",
                "Adrien Ecoffet",
                "Brandon Houghton",
                "Raul Sampedro",
                "Jeff Clune"
            ],
            "title": "Video pretraining (VPT): Learning to act by watching unlabeled online videos",
            "venue": "In Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Ethan Brooks",
                "Janarthanan Rajendran",
                "Richard Lewis",
                "Satinder Singh"
            ],
            "title": "Reinforcement learning of implicit and explicit control flow in instructions",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Richard Byrne",
                "Anne Russon"
            ],
            "title": "Learning by imitation: A hierarchical approach",
            "venue": "Behavioral and Brain Sciences,",
            "year": 1998
        },
        {
            "authors": [
                "Ethan Callanan",
                "Rebecca De Venezia",
                "Victoria Armstrong",
                "Alison Paredes",
                "Tathagata Chakraborti",
                "Christian Muise"
            ],
            "title": "MACQ: A holistic view of model acquisition techniques",
            "venue": "In The ICAPS Workshop on Knowledge Engineering for Planning and Scheduling,",
            "year": 2022
        },
        {
            "authors": [
                "Maxime Chevalier-Boisvert",
                "Bolun Dai",
                "Mark Towers",
                "Rodrigo de Lazcano",
                "Lucas Willems",
                "Salem Lahlou",
                "Suman Pal",
                "Pablo Samuel Castro",
                "Jordan Terry"
            ],
            "title": "Minigrid & miniworld: Modular & customizable reinforcement learning environments for goal-oriented",
            "venue": "tasks. CoRR,",
            "year": 2023
        },
        {
            "authors": [
                "Luis Cobo",
                "Peng Zang",
                "Charles Isbell",
                "Andrea Thomaz"
            ],
            "title": "Automatic state abstraction from demonstration",
            "venue": "In International Joint Conference on Artificial Intelligence,",
            "year": 2011
        },
        {
            "authors": [
                "Mohamad Hosein Danesh",
                "Panpan Cai",
                "David Hsu"
            ],
            "title": "Leader: Learning attention over driving behaviors for planning under uncertainty",
            "venue": "In Conference on Robot Learning,",
            "year": 2023
        },
        {
            "authors": [
                "Carlos Florensa",
                "Yan Duan",
                "Pieter Abbeel"
            ],
            "title": "Stochastic neural networks for hierarchical reinforcement learning",
            "venue": "In International Conference on Learning Representations,",
            "year": 2017
        },
        {
            "authors": [
                "Maria Fox",
                "Derek Long"
            ],
            "title": "PDDL2.1: An extension to PDDL for expressing temporal planning domains",
            "venue": "Journal of Artificial Intelligence Research,",
            "year": 2003
        },
        {
            "authors": [
                "Daniel Furelos-Blanco",
                "Mark Law",
                "Anders Jonsson",
                "Krysia Broda",
                "Alessandra Russo"
            ],
            "title": "Induction and exploitation of subgoal automata for reinforcement learning",
            "venue": "Journal of Artificial Intelligence Research,",
            "year": 2021
        },
        {
            "authors": [
                "Malik Ghallab",
                "Craig Knoblock",
                "David Wilkins",
                "Anthony Barrett",
                "Dave Christianson",
                "Marc Friedman",
                "Chung Kwok",
                "Keith Golden",
                "Scott Penberthy",
                "David Smith",
                "Ying Sun",
                "Daniel Weld"
            ],
            "title": "PDDL - the planning domain definition language",
            "year": 1998
        },
        {
            "authors": [
                "Lin Guan",
                "Sarath Sreedharan",
                "Subbarao Kambhampati"
            ],
            "title": "Leveraging approximate symbolic models for reinforcement learning via skill diversity",
            "venue": "arXiv preprint arXiv:2202.02886,",
            "year": 2022
        },
        {
            "authors": [
                "Bradley Hayes",
                "Brian Scassellati"
            ],
            "title": "Autonomously constructing hierarchical task networks for planning and human-robot collaboration",
            "venue": "In IEEE International Conference on Robotics and Automation,",
            "year": 2016
        },
        {
            "authors": [
                "Jonathan Ho",
                "Stefano Ermon"
            ],
            "title": "Generative adversarial imitation learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2016
        },
        {
            "authors": [
                "Rodrigo Toro Icarte",
                "Ethan Waldie",
                "Toryn Klassen",
                "Rick Valenzano",
                "Margarita Castro",
                "Sheila McIlraith"
            ],
            "title": "Learning reward machines for partially observable reinforcement learning",
            "venue": "Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Rodrigo Toro Icarte",
                "Toryn Klassen",
                "Richard Valenzano",
                "Sheila A. McIlraith"
            ],
            "title": "Reward machines: exploiting reward function structure in reinforcement learning",
            "venue": "Journal of Artificial Intelligence Research,",
            "year": 2022
        },
        {
            "authors": [
                "Mu Jin",
                "Zhihao Ma",
                "Kebing Jin",
                "Hankz Hankui Zhuo",
                "Chen Chen",
                "Chao Yu"
            ],
            "title": "Creativity of ai: Automatic symbolic option discovery for facilitating deep reinforcement learning",
            "venue": "In AAAI Conference on Artificial Intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "B Ravi Kiran",
                "Ibrahim Sobh",
                "Victor Talpaert",
                "Patrick Mannion",
                "Ahmad A. Al Sallab",
                "Senthil Yogamani",
                "Patrick P\u00e9rez"
            ],
            "title": "Deep reinforcement learning for autonomous driving: A survey",
            "venue": "IEEE Transactions on Intelligent Transportation Systems,",
            "year": 2022
        },
        {
            "authors": [
                "George Konidaris",
                "Leslie Pack Kaelbling",
                "Tomas Lozano-Perez"
            ],
            "title": "From skills to symbols: Learning symbolic representations for abstract high-level planning",
            "venue": "Journal of Artificial Intelligence Research,",
            "year": 2018
        },
        {
            "authors": [
                "John Koza"
            ],
            "title": "Genetic programming: On the programming of computers by means of natural selection",
            "venue": "Statistics and computing,",
            "year": 1994
        },
        {
            "authors": [
                "Tejas Kulkarni",
                "Karthik Narasimhan",
                "Ardavan Saeedi",
                "Joshua B. Tenenbaum"
            ],
            "title": "Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2016
        },
        {
            "authors": [
                "Junkyu Lee",
                "Michael Katz",
                "Don Joven Agravante",
                "Miao Liu",
                "Tim Klinger",
                "Murray Campbell",
                "Shirin Sohrabi",
                "Gerald Tesauro"
            ],
            "title": "AI planning annotation in reinforcement learning: Options and beyond",
            "venue": "In Planning and Reinforcement Learning Workshop at International Conference on Automated Planning and Scheduling,",
            "year": 2021
        },
        {
            "authors": [
                "Youngwoon Lee",
                "Shao-Hua Sun",
                "Sriram Somasundaram",
                "Edward Hu",
                "Joseph J. Lim"
            ],
            "title": "Composing complex skills by learning transition policies",
            "venue": "In International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "Yu-An Lin",
                "Chen-Tao Lee",
                "Guan-Ting Liu",
                "Pu-Jen Cheng",
                "Shao-Hua Sun"
            ],
            "title": "Addressing longhorizon tasks by integrating program synthesis and state machines",
            "year": 2023
        },
        {
            "authors": [
                "Zachary C Lipton",
                "Jianfeng Gao",
                "Lihong Li",
                "Xiujun Li",
                "Faisal Ahmed",
                "Li Deng"
            ],
            "title": "Efficient exploration for dialogue policy learning with bbq networks & replay buffer spiking",
            "venue": "arXiv preprint arXiv:1608.05081,",
            "year": 2016
        },
        {
            "authors": [
                "Anthony Liu",
                "Sungryull Sohn",
                "Mahdi Qazwini",
                "Honglak Lee"
            ],
            "title": "Learning parameterized task structure for generalization to unseen entities",
            "venue": "In AAAI Conference on Artificial Intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "Guan-Ting Liu",
                "En-Pei Hu",
                "Pu-Jen Cheng",
                "Hung-Yi Lee",
                "Shao-Hua Sun"
            ],
            "title": "Hierarchical programmatic reinforcement learning via learning to compose programs",
            "venue": "In International Conference on Machine Learning,",
            "year": 2023
        },
        {
            "authors": [
                "Wei-Yin Loh"
            ],
            "title": "Classification and regression trees. Wiley interdisciplinary reviews: data mining and knowledge discovery",
            "year": 2011
        },
        {
            "authors": [
                "Daoming Lyu",
                "Fangkai Yang",
                "Bo Liu",
                "Steven Gustafson"
            ],
            "title": "Sdrl: Interpretable and data-efficient deep reinforcement learning leveraging symbolic planning",
            "venue": "In AAAI Conference on Artificial Intelligence,",
            "year": 2019
        },
        {
            "authors": [
                "Jiayuan Mao",
                "Tom\u00e1s Lozano-P\u00e9rez",
                "Joshua B. Tenenbaum",
                "Leslie Pack Kaelbling"
            ],
            "title": "PDSketch: Integrated domain programming, learning, and planning",
            "venue": "In Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Volodymyr Mnih",
                "Koray Kavukcuoglu",
                "David Silver",
                "Alex Graves",
                "Ioannis Antonoglou",
                "Daan Wierstra",
                "Martin Riedmiller"
            ],
            "title": "Playing atari with deep reinforcement learning",
            "venue": "arXiv preprint arXiv:1312.5602,",
            "year": 2013
        },
        {
            "authors": [
                "Volodymyr Mnih",
                "Koray Kavukcuoglu",
                "David Silver",
                "Andrei A Rusu",
                "Joel Veness",
                "Marc G Bellemare",
                "Alex Graves",
                "Martin Riedmiller",
                "Andreas K Fidjeland",
                "Georg Ostrovski",
                "Stig Petersen",
                "Charles Beattie",
                "Amir Sadik",
                "Ioannis Antonoglou",
                "Helen King",
                "Dharshan Kumaran",
                "Daan Wierstra",
                "Shane Legg",
                "Demis Hassabis"
            ],
            "title": "Human-level control through deep reinforcement learning",
            "venue": "Nature,",
            "year": 2015
        },
        {
            "authors": [
                "Volodymyr Mnih",
                "Adria Puigdomenech Badia",
                "Mehdi Mirza",
                "Alex Graves",
                "Timothy Lillicrap",
                "Tim Harley",
                "David Silver",
                "Koray Kavukcuoglu"
            ],
            "title": "Asynchronous methods for deep reinforcement learning",
            "venue": "In International Conference on Machine Learning,",
            "year": 2016
        },
        {
            "authors": [
                "Thanh Thi Nguyen",
                "Ngoc Duy Nguyen",
                "Saeid Nahavandi"
            ],
            "title": "Deep reinforcement learning for multiagent systems: A review of challenges, solutions, and applications",
            "venue": "IEEE Transactions on Cybernetics,",
            "year": 2020
        },
        {
            "authors": [
                "Hanna Pasula",
                "Luke Zettlemoyer",
                "Leslie Kaelbling"
            ],
            "title": "Learning symbolic models of stochastic domains",
            "venue": "Journal of Artificial Intelligence Research,",
            "year": 2007
        },
        {
            "authors": [
                "Shubham Pateria",
                "Budhitama Subagdja",
                "Ah-hwee Tan",
                "Chai Quek"
            ],
            "title": "Hierarchical reinforcement learning: A comprehensive survey",
            "venue": "ACM Computing Surveys,",
            "year": 2022
        },
        {
            "authors": [
                "Deepak Pathak",
                "Pulkit Agrawal",
                "Alexei A. Efros",
                "Trevor Darrell"
            ],
            "title": "Curiosity-driven exploration by self-supervised prediction",
            "venue": "In International Conference on Machine Learning,",
            "year": 2017
        },
        {
            "authors": [
                "Antonin Raffin",
                "Ashley Hill",
                "Adam Gleave",
                "Anssi Kanervisto",
                "Maximilian Ernestus",
                "Noah Dormann"
            ],
            "title": "Stable-baselines3: Reliable reinforcement learning implementations",
            "venue": "Journal of Machine Learning Research,",
            "year": 2021
        },
        {
            "authors": [
                "Roberta Raileanu",
                "Tim Rockt\u00e4schel"
            ],
            "title": "RIDE: Rewarding impact-driven exploration for procedurally-generated environments",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "St\u00e9phane Ross",
                "Geoffrey Gordon",
                "Drew Bagnell"
            ],
            "title": "A reduction of imitation learning and structured prediction to no-regret online learning",
            "venue": "In International Conference on Artificial Intelligence and Statistics,",
            "year": 2011
        },
        {
            "authors": [
                "John Schulman",
                "Filip Wolski",
                "Prafulla Dhariwal",
                "Alec Radford",
                "Oleg Klimov"
            ],
            "title": "Proximal policy optimization algorithms",
            "venue": "arXiv preprint arXiv:1707.06347,",
            "year": 2017
        },
        {
            "authors": [
                "Claude Elwood Shannon"
            ],
            "title": "A mathematical theory of communication",
            "venue": "The Bell system technical journal,",
            "year": 1948
        },
        {
            "authors": [
                "Tom Silver",
                "Rohan Chitnis"
            ],
            "title": "PDDLGym: Gym environments from PDDL problems",
            "venue": "In Planning and Reinforcement Learning Workshop at International Conference on Automated Planning and Scheduling,",
            "year": 2020
        },
        {
            "authors": [
                "Tom Silver",
                "Ashay Athalye",
                "Joshua B. Tenenbaum",
                "Tom\u00e1s Lozano-P\u00e9rez",
                "Leslie Pack Kaelbling"
            ],
            "title": "Learning neuro-symbolic skills for bilevel planning",
            "venue": "In Conference on Robot Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Sungryull Sohn",
                "Junhyuk Oh",
                "Honglak Lee"
            ],
            "title": "Hierarchical reinforcement learning for zero-shot generalization with subtask dependencies",
            "venue": "Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Sungryull Sohn",
                "Hyunjae Woo",
                "Jongwook Choi",
                "Honglak Lee"
            ],
            "title": "Meta Reinforcement Learning with Autonomous Inference of Subtask Dependencies",
            "venue": "arXiv preprint arXiv:2001.00248,",
            "year": 2020
        },
        {
            "authors": [
                "Sungryull Sohn",
                "Hyunjae Woo",
                "Jongwook Choi",
                "lyubing qiang",
                "Izzeddin Gur",
                "Aleksandra Faust",
                "Honglak Lee"
            ],
            "title": "Fast inference and transfer of compositional task structures for few-shot task generalization",
            "venue": "In Uncertainty in Artificial Intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "Roni Stern",
                "Brendan Juba"
            ],
            "title": "Efficient, safe, and probably approximately complete learning of action models",
            "venue": "In International Joint Conference on Artificial Intelligence,",
            "year": 2017
        },
        {
            "authors": [
                "Shao-Hua Sun",
                "Hyeonwoo Noh",
                "Sriram Somasundaram",
                "Joseph Lim"
            ],
            "title": "Neural program synthesis from diverse demonstration videos",
            "venue": "In International Conference on Machine Learning,",
            "year": 2018
        },
        {
            "authors": [
                "Shao-Hua Sun",
                "Te-Lin Wu",
                "Joseph J Lim"
            ],
            "title": "Program guided agent",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Maxwell Svetlik",
                "Matteo Leonetti",
                "Jivko Sinapov",
                "Rishi Shah",
                "Nick Walker",
                "Peter Stone"
            ],
            "title": "Automatic curriculum graph generation for reinforcement learning agents",
            "venue": "AAAI Conference on Artificial Intelligence,",
            "year": 2017
        },
        {
            "authors": [
                "Dweep Trivedi",
                "Jesse Zhang",
                "Shao-Hua Sun",
                "Joseph J Lim"
            ],
            "title": "Learning to synthesize programs as interpretable and generalizable policies",
            "venue": "In Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Mauro Vallati",
                "Lukas Chrpa",
                "Marek Grze\u015b",
                "Thomas Leo McCluskey",
                "Mark Roberts",
                "Scott Sanner"
            ],
            "title": "The 2014 international planning competition: Progress and trends",
            "venue": "AI Magazine,",
            "year": 2015
        },
        {
            "authors": [
                "Manuela Veloso",
                "Jaime Carbonell",
                "Alicia Perez",
                "Daniel Borrajo",
                "Eugene Fink",
                "Jim Blythe"
            ],
            "title": "Integrating planning and learning: The prodigy architecture",
            "venue": "Journal of Experimental & Theoretical Artificial Intelligence,",
            "year": 1995
        },
        {
            "authors": [
                "Marco Virgolin",
                "Tanja Alderliesten",
                "Cees Witteveen",
                "Peter A.N. Bosman"
            ],
            "title": "Scalable genetic programming by gene-pool optimal mixing and input-space entropy-based building-block learning",
            "venue": "In Genetic and Evolutionary Computation Conference,",
            "year": 2017
        },
        {
            "authors": [
                "Marco Virgolin",
                "Tanja Alderliesten",
                "Cees Witteveen",
                "Peter AN Bosman"
            ],
            "title": "Improving model-based genetic programming for symbolic regression of small expressions",
            "venue": "Evolutionary computation,",
            "year": 2021
        },
        {
            "authors": [
                "Lucas Willems"
            ],
            "title": "PyTorch Actor-Critic deep reinforcement learning algorithms: A2C and PPO",
            "venue": "https://github.com/lcswillems/torch-ac,",
            "year": 2022
        },
        {
            "authors": [
                "Zhe Xu",
                "Bo Wu",
                "Aditya Ojha",
                "Daniel Neider",
                "Ufuk Topcu"
            ],
            "title": "Active finite reward automaton inference and reinforcement learning using queries and counterexamples",
            "venue": "In Machine Learning and Knowledge Extraction,",
            "year": 2021
        },
        {
            "authors": [
                "Qiang Yang",
                "Kangheng Wu",
                "Yunfei Jiang"
            ],
            "title": "Learning action models from plan examples using weighted max-sat",
            "venue": "Artificial Intelligence,",
            "year": 2007
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Deep reinforcement learning (DRL) as an inductive learning method allows agents to deal with high-dimensional decision-making problems considered intractable in the past (Arulkumaran et al., 2017). DRL has applied to various fields, including robotics (Nguyen et al., 2020), autonomous driving (Kiran et al., 2022), and video games (Mnih et al., 2013). However, exploring complex tasks with sparse and delayed rewards still remains challenging, leading to inapplicability on many real-world problems comprising multiple subtasks, e.g., cooking and furniture assembly.\nIn contrast, classical planning is a deductive learning method which aims to solve planning and scheduling problems. Particularly, classical planning is adept at finding the sequence of actions in deterministic and known environments. Researchers in classical planning have developed effective planners that can handle large-scale problems (Vallati et al., 2015). Yet, classical planning agents face difficulties exploring environments due to limitations in model and domain-specific representation in unknown environments where action models are undiscovered.\nSeveral methods work on combining planning and DRL to address hierarchical tasks with high-level abstraction. Konidaris et al. (2018) develop a skill-up approach to build a planning representation from skill to abstraction, while they do not encompass skill acquisition from low-level execution. Mao et al. (2022) introduce an extension of planning domain definition language (Ghallab et al., 1998) to model the skill, and Silver et al. (2022) propose a method for learning parameterized policies integrated with symbolic operators and neural samplers. However, they consider object-centric representations, which require fully observable environments and carefully designed predicates.\nIn this paper, we combine classical planning and DRL to augment agents effectively to adapt to environments by inducing underlying prior knowledge from expert demonstrations. Specifically, we devise a method that induces symbolic knowledge using genetic programming (Koza, 1994), an evolutionary computation approach, to discover task substructures that accurately capture the underlying patterns within the data. The compositional property of the programs enables generalizability that adapts to new environments by discovering new substructures from known ones.\nTo evaluate the proposed framework, we design three gridworld environments where agents can move on and interact with objects. The result shows the improvement of DRL agents and outperformance compared to other imitation learning and exploration-based methods. Also, our framework demonstrates generalizability by inducing variant substructures and recomposing task structures. Finally, we show the ablation studies about the accuracy of induction."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Learning abstraction from demonstrations. State abstraction facilitates the agent\u2019s reasoning capabilities in high-level planning by extracting symbolic representations from low-level states (Abel et al., 2018; Guan et al., 2022). Agents can learn the abstraction from demonstrations since demonstrations encompass valuable information regarding task composition and relevant features (Byrne & Russon, 1998). Some methods were developed to extract task decomposition and abstraction from demonstrations (Hayes & Scassellati, 2016; Cobo et al., 2011). Our work extends these approaches to infer knowledge from demonstrations.\nLearning planning action models. Some works have focused on integrating learning and planning to enhance capabilities in complex environments (Danesh et al., 2023; Veloso et al., 1995). To leverage the strategies of classical planning, many works have developed building planning action models, including skill acquisition and action schema learning (Arora et al., 2018; Stern & Juba, 2017; Pasula et al., 2007; Callanan et al., 2022; Silver et al., 2022; Mao et al., 2022; Yang et al., 2007). However, these works mainly focus on the existing planning benchmark and less focus on general Markov decision process (MDP) problems. On the other hand, to address the issue of sample efficiency in DRL, several techniques explored the integration of symbolic knowledge with DRL by learning planning action models (Jin et al., 2022; Lyu et al., 2019). In this work, we aim to bridge this gap by extending these approaches to incorporate inferred knowledge into DRL, enhancing its applicability in complex decision-making scenarios.\nHierarchical task learning. A proper hierarchical structure is crucial for task decomposition and abstraction. Various methods have been proposed for constructing hierarchical task representation (Pateria et al., 2022), including graphs (Svetlik et al., 2017), automata (Furelos-Blanco et al., 2021; Icarte et al., 2019; 2022; Xu et al., 2021), programs (Sun et al., 2018; 2020; Trivedi et al., 2021; Liu et al., 2023; Lin et al., 2023), and hierarchical task networks (Hayes & Scassellati, 2016; Sohn et al., 2020; Lee et al., 2019). Some approaches utilize the capabilities of deep learning with intrinsic rewards (Kulkarni et al., 2016; Florensa et al., 2017). In addition, some of these works specifically address leveraging knowledge to deal with multiple compositional tasks via task decomposition (Andreas et al., 2017; Sohn et al., 2022; Liu et al., 2022; Sun et al., 2020; Sohn et al., 2018; FurelosBlanco et al., 2021). Despite the success in building hierarchical models shown in previous works, these works put less emphasis on inducing subtask rules and substructure. Therefore, we develop a method to induce symbolic knowledge and leverage it for hierarchical task representation."
        },
        {
            "heading": "3 PROBLEM FORMULATION",
            "text": "We address the sparse-reward goal-directed problems which can be formulated as MDPs denoted as \u27e8S,A, T,R, \u03b3\u27e9, where S denotes state space, A denotes action space, T : S \u00d7 A \u2192 S denotes a transition function, R : S \u00d7 A \u2192 R denotes a reward function, and \u03b3 \u2208 (0, 1] denotes a discounting factor. DRL agents often struggle at solving sparse-reward, hierarchical tasks, while classical planning techniques excel in such scenarios. On the other hand, unlike classical planning, DRL, as a generic model-free framework, does not require pre-defined models. This motivates us to bridge the best of both worlds by integrating these two paradigms.\nHowever, while DRL directly learns from interacting with MDPs, classical planning operates on literal conjunctions. To address this gap, we integrate planning and DRL methods by annotating the specific MDP actions in the form of action schemata. Specifically, we consider the problem of inducing the action schemata from demonstrations. The objective is to induce the action schemata which can be leveraged for task structure deduction. After the action schemata are discovered, the framework deduces task structures from the action model and aims to offer guidance for the training of DRL agents based on the task structures."
        },
        {
            "heading": "4 INTEGRATION OF MDP IN DRL AND PLANNING WITH CRITICAL ACTION",
            "text": "To bridge the gap between DRL and planning, we introduce the concept of critical actions in this section. Specifically, we formulate our problems as mapping tasks described by MDPs to planning domain definition language (PDDL) (Ghallab et al., 1998; Silver & Chitnis, 2020) and SAS+ (Ba\u0308ckstro\u0308m & Nebel, 1995), where the preliminary notation is elaborated in Appendix A. To express numeric variables, we adopt the configuration of PDDL 2.1 (Fox & Long, 2003), which includes arithmetic operators for specification.\nMapping between MDP and classical planning. Lee et al. (2021) has developed the abstraction mapping between planning and MDP problems. A planning task \u03a0 = \u27e8V,O, s\u2032g\u27e9, where V is a set of variables, O is a set of operators in the domain, and s\u2032g is a partial state describing the goal. The transition graph of the planning task is a tuple T \u2032 = \u27e8S \u2032, T \u2032,S \u2032goal\u27e9, where T \u2032 is a set of transitions \u27e8s\u2032, o, T \u2032(s\u2032)\u27e9 for all s\u2032 in S \u2032, and S \u2032goal is a set of goal states. Let L : S \u2192 S \u2032 be a mapping from the MDP state space S to high-level planning state space S \u2032. Given an MDP problem, the abstraction \u27e8L,\u03a0\u27e9 is proper iff there exists a mapping L to \u03a0 such that \u27e8L(s), \u03c8,L(T (s, a\u03c8))\u27e9 \u2208 T \u2032 if some \u03c8 is admissible in the MDP state s \u2208 S or L(s) = L(T (s, a\u03c8)), where T \u2032 is a set of all possible transitions in \u03a0. In this work, we focus on the MDP problems with proper abstraction in which a mapping to a planning domain exists, and action models can be induced by the proposed framework.\nCritical action. Critical actions are the actions that lead to the transitions in T \u2032. That is, these actions are critical for progress at the planning level and must be executed in a specific order. A state s\u2032 \u2208 S \u2032 in the planning domain is an assignment to V , and s\u2032v \u2208 R is the value assigned to the variable v \u2208 V . We map each si in MDP problems with distinct s\u2032v in planning, considering a planning task as an MDP-like tuple \u27e8S \u2032,O, T \u2032\u27e9, a transition \u27e8s, a\u03c8, T (s, a\u03c8)\u27e9 in an MDP problem can be directly transferred to the transition \u27e8L(s), \u03c8,L(T (s, a\u03c8))\u27e9 in planning domain.\n\u2022 MDP action a\u03c8 \u2208 A denotes the MDP action mapping to \u03c8. \u2022 Precondition pre(\u03c8) is a set of conditions requires satisfaction before executing \u03c8. \u2022 Effect eff (\u03c8) is a set of functions which indicates the state change after executing \u03c8.\nA state s\u2032 \u2208 S \u2032, where S \u2032 denotes planning state space, is an assignment to V , where V(p) denotes the variables of the assignment p. Given an effect eff (\u03c8) and one of its variables v \u2208 V(eff (\u03c8)),\nan effect rule eff (\u03c8)v : R \u2192 R is a function which transfers the specific feature value s\u2032v in s\u2032 to another value eff (\u03c8)v[s\u2032v] in the transition, and a precondition rule pre(\u03c8)v is a logical formula pre(\u03c8)v : R\u2192 {0, 1} that determines whether the variable v is satisfied to execute \u03c8. Given a state s\u2032, two critical action \u03c8 and \u03d5, eff (\u03c8)v satisfy pre(\u03d5)v in state s\u2032 iff a variable v in both V(eff (\u03c8)) and V(pre(\u03d5)), and pre(\u03d5)v[s\u2032v] is false while pre(\u03d5)v[eff (\u03c8)v[s\u2032v]] is true in a transition with \u03c8. That is, executing \u03c8 makes \u03d5 become admissible.\nTo efficiently induce the model, we assume that the properties of the features in a state are known. Effect variable space E = {v | v \u2208 V(eff (\u03c8)) \u2200 \u03c8 \u2208 O} contains the variables that will change in transitions and related to the progress of the tasks.\nCritical action network. This work represents symbolic knowledge structures as critical action networks illustrated in Figure 1c. Given a set of critical actions O and a desired goal specification pgoal, a critical action network G = (V,E) is an in-tree structure where the root is the critical action that can satisfy the goal specification directly. For each edge (\u03c8, \u03d5) \u2208 E, there exists eff (\u03c8)v for some v that satisfy pre(\u03d5)v . Once the action schemata are known, we can construct the network using planners or backward chaining."
        },
        {
            "heading": "5 METHOD",
            "text": "We introduce the induction module that determines the critical actions from demonstrations and extracts symbolic rules in Section 5.1. Then, Section 5.2 describes the training module that deduces task structures to build critical action networks online from the given goal. The network contains subtask dependencies, providing guidance through intrinsic rewards and augmenting the training efficiency of DRL agents. An overview of our proposed framework is illustrated in Figure 2a."
        },
        {
            "heading": "5.1 INDUCTION MODULE",
            "text": "The procedure of the induction module is illustrated in Figure 2b. The module first extracts actioneffect linkages (a,V(eff (\u03c8))) from demonstrations. Second, the module induces effect rules eff (\u03c8) given (a,V(eff (\u03c8))). Finally, the module leverages the rules to determine the precondition rules\npre(\u03c8) for each (a, eff (\u03c8)). After these steps, the components of critical action schemata are all determined. Note that we name the critical action \u03c8 for the convenience of reference, which is not known when inducing action schemata. We use \u201c\u00b7\u201d to represent an undefined critical action. The following paragraphs will elaborate on the details of the induction methods.\nAction-effect linkages. Based on the outcome assumption that one action only impacts specific state features, we can detect co-occurrence of what effects often occur after executing a by calculating mutual information (Shannon, 1948) between actions and effect variables. Let E be a set of possible effect variable combinations V(eff (\u00b7)) in the transitions of demonstrations. The mutual information M(a,V(eff (\u00b7))) is defined as follows:\nM(a,V(eff (\u00b7))) = \u2211 a\u2208A \u2211 V(eff (\u00b7))\u2208E PAE(a,V(eff (\u00b7))) log PAE(a,V(eff (\u00b7))) PA(a)PE(V(eff (\u00b7))) , (1)\nwhere PA(a) is the count of transitions with action a; PE(V(eff (\u00b7))) is the count of transitions that include variables in V(eff (\u00b7)); PAE(a,V(eff (\u00b7))) is the count of transitions that include changed variables in V(eff (\u00b7)) with action a. To determine the linkage, the pairs are divided into two clusters with the threshold of a maximum gap, and the cluster with higher values are selected. The detailed algorithm is shown in Appendix B.1.\nEffect symbolic rules. Given an action-effect pair (a,V(eff (\u00b7))), the induction module proceeds to search for the effect eff (\u00b7), which can be formulated as a symbolic regression. To accomplish this, we employ genetic programming for symbolic regression to discover each effect rule eff (\u00b7)v for all v in V(eff (\u00b7)), aiming to discover programs that can accurately predict the effects. In genetic programming, each program is represented as an expression tree, taking sv and a\u03c8 in each transition as input and yielding the predicted value of v after the transition as output. The algorithm consists of three key steps: initialization, evaluation, selection, and reproduction. Initially, a population of programs is randomly generated. The fitness of each program is evaluated based on its prediction accuracy, and the programs with the highest fitness values are selected, serving as parents to reproduce offspring through crossover, mutation, and duplication mechanisms. The procedures and the example of genetic programming are illustrated in Figure 3.\nThe model of symbolic rules is regarded as the substructures of the subtasks, and selecting the proper operators for the symbolic model compatible with the effects plays a crucial role in facilitating effective inference. For instance, in the context of general DRL task with numerical variable representation configuration, arithmetic operation set F = {+,\u2212,\u00d7,\u00f7, inc,dec} is used as the function set in genetic programming, where inc denotes an increment operator and dec denotes a decrement oper-\nator. This choice of function set is consistent with the numerical variable representation commonly employed in DRL tasks. The underlying assumption guiding our approach is that the effects can be expressed through these programs, serving as prior knowledge of the problem. This allows our method to induce task substructures and generalize the knowledge across domains that share identical operation configurations. This distinguishing feature sets our approach apart from alternative model-free methodologies. Additional implementation details can be found in Appendix B.2.\nPrecondition rules. After the relation between a and eff (\u00b7) are found, determining precondition rules pre(\u00b7) can be formulated as a classification problem, as the objective is to identify whether eff (\u00b7) occurs given the action and the state. The process involves minimal consistent determination (MCD) and the decision tree method. The model of pre(\u00b7) decides what preconditions leading to desired effects after executing a. Additional details can be found in Appendix B.3."
        },
        {
            "heading": "5.2 TRAINING MODULE",
            "text": "After the induction process, the critical action schemata serve as the components of knowledge base that guides the agent in the training stage. During the training stage, the training module deduces the critical action network given the initial state and goal specification and provides intrinsic reward if the agent successfully performs an action that meets the critical effects in the network.\nInferring critical action network. Once the critical actions schemata are defined, we can infer task structures from the model. Given a goal and an initial state, the proposed framework deduces the critical action networks by backward chaining. Starting from the goal, the module searches for the critical action to find the desired effect for unconnected precondition rules pre(\u00b7)v where v \u2208 E. Maximum operation steps are set to terminate the search. Once the critical action is found, the critical action will be considered as the predecessor of previous critical actions.\nDRL agent. In the training stage, we aim to train a DRL agent that can learn the subtask by leveraging the feature-extracting power of neural networks. The induction module only specifies the coarse-grained critical action to express temporal order. Therefore, the framework deploys DRL to complete the fine-grained decision-making tasks, which utilizes deep learning to approximate the optimal policy with neuron networks. DRL uses the policy gradient method to update the policy. In the proposed method, we use the action-critic method (Mnih et al., 2016; Raffin et al., 2021) as the DRL agent. The implementation details are described in Appendix B.4.\nIntrinsic rewards. During the training stage, if the agent successfully executes the critical effects, it will receive an intrinsic reward when the preconditions of a critical action \u03c8 are satisfied. Conversely, if the agent takes an action that leads to undesired effects, such as violating effect rules, it will receive a penalty. However, note that our model only specifies positive critical actions and does not explicitly identify actions that have possible negative consequences. Therefore, the implementation of a penalty depends on the specific domain."
        },
        {
            "heading": "6 EXPERIMENTS",
            "text": "We evaluate our framework and provide ablation studies in this section. Section 6.1 lists the algorithms we use for comparison. Section 6.2 provides the description of the environments and tasks. Section 6.3 presents the results of training efficiency and performance. Section 6.4 demonstrates the generalizability in different levels."
        },
        {
            "heading": "6.1 BASELINES",
            "text": "We extensively compare our framework to various DRL algorithms (DQN and PPO) learning from rewards, imitation learning methods (BC and GAIL) learning from demonstrations, advanced approaches (DQN-RBS and BC-PPO) that leverage both rewards and demonstrations, and an exploration method (RIDE) that maximizes intrinsic and extrinsic rewards.\n\u2022 DQN (Mnih et al., 2015) is an off-policy deep Q-learning algorithm. \u2022 PPO (Schulman et al., 2017) is a state-of-the-art on-policy DRL algorithm.\n\u2022 Behavior cloning (BC; Ross et al., 2011) imitates an expert by learning from demonstrations in a supervised manner.\n\u2022 Generative adversarial imitation learning (GAIL; Ho & Ermon, 2016) mimics expert behaviors via learning a generative adversarial network whose generator is a policy.\n\u2022 DQN-RBS initializes the replay buffer of DQN with demonstrations, allowing for a performance boost. This is inspired by the replay buffer spiking technique (Lipton et al., 2016).\n\u2022 BC-PPO pre-trains a BC policy using demonstrations and then fine-tunes the policy with PPO using rewards, similar to Video PreTraining (VPT; Baker et al., 2022).\n\u2022 Rewarding impact-driven exploration (RIDE; Raileanu & Rockta\u0308schel, 2020) is an RL exploration method inspired by the intrinsic curiosity module (Pathak et al., 2017)."
        },
        {
            "heading": "6.2 ENVIRONMENTS & TASKS",
            "text": "To evaluate the proposed framework and the baselines, we design three groups of tasks in a 8 \u00d7 8 gridworld environment, where an agent can move along four directions {up, down, left, right} and interact with objects. The tasks are described as follows. See Appendix C for more details.\nSWITCH requires an agent to turn on the switches in sequential order. If it toggles the wrong switch, the progress will regress, which makes it challenging for DRL agents to solve the task through solely exploration. We design four tasks 4-SWITCH, 8-SWITCH, 4-DISTRACTORS and 4-ROOMS, where 4-SWITCH and 8-SWITCH evaluate the performance between different difficulties of tasks. 4- DISTRACTORS consist of four target switches and four distractor switches, and 4-ROOMS combines the configuration Minigrid four-rooms tasks (Chevalier-Boisvert et al., 2023).\nDOORKEY features a hierarchical task similar to the Minigrid door-key tasks, where the agent needs to open a door with a key and turn on a switch behind the door.\nMINECRAFT is inspired by the computer game Minecraft and is similar to the environment in previous works (Sohn et al., 2018; Andreas et al., 2017; Sun et al., 2020; Brooks et al., 2021). The environment is designed for evaluation using multiple-task demonstrations. We select a simple task IRON, a difficult task ENHANCETABLE, and a multiple-goal task MULTIPLE.\nFor the methods that require demonstrations, we collect 20 demonstrations from corresponding tasks in SWITCH and DOORKEY and collect 64 multiple-task demonstrations from MULTIPLE for all tasks in MINECRAFT."
        },
        {
            "heading": "6.3 RESULTS",
            "text": "The experimental results in Figure 4 show that our framework outperforms all the baselines on challenging tasks (e.g., 8-SWITCH, 4-DISTRACITORS, ENHANCETABLE, MULTIPLE) and performs competitively on simpler tasks (e.g., DOORKEY, 4-SWITCHES, IRON). The imitation learning approaches, BC and GAIL, fail to learn all the tasks due to insufficient demonstrations and lack of exploration, while RIDE, BC-PPO, and DQN-RBS, which consider rewards online, fail on advanced tasks that require long-term planning. In contrast, our framework can leverage the knowledge from the same number of demonstrations and efficiently explore the environment, especially on the tasks 8-SWITCHES where all the baselines completely fail, as shown in Figure 4b. Moreover, our proposed framework is the most sample-efficient method in learning the DOORKEY task."
        },
        {
            "heading": "6.4 GENERALIZABILITY",
            "text": "Our framework employs the critical action model to achieve task-level generalizability, enabling the construction of novel task structures based on familiar critical actions. Additionally, we introduce genetic programming, renowned for its adaptability in reasoning symbolic rules as task substructures, thereby enhancing generalizability at the rule level. To define the domain gap, we denote the original domain as the domain where demonstrations are collected and the variant domain as the domain where agents learn. For rule-level generalizability, we define a variant critical action \u03d5 from \u03c8 where V(eff (\u03d5)) = V(eff (\u03c8)) and V(pre(\u03d5)) = V(pre(\u03c8)) while eff (\u03d5) \u0338= eff (\u03c8) or pre(\u03d5) \u0338= pre(\u03c8). If a critical action \u03c8 varies, the induced symbolic programs and the population can evolve and adapt to new substructures. Since V(pre(\u03d5)) and V(eff (\u03d5)) are known, the procedure starts from inducing effect rules eff (\u03d5) \u0338= eff (\u03c8). Thus, the proposed framework can potentially achieve task generalization.\nSetup. To evaluate the generalizability of our framework and baselines, we consider 4-SWITCHES(N+1) as the original domain and its variant domains, 8-SWITCHES-(N+1), 4-DISTRACTORS(N+1), and 4-DISTRACTORS-(2N+1). In 8-SWITCHES-(N+1), we extend the number of switches from 4 to 8 to evaluate the generalization of task structures. In 4-DISTRACTORS-(N+1), 4 distractor switches are added to 4-SWITCHES-(N+1), and in 4-DISTRACTORS-(2N+1), the order of switches changes to 2n+ 1 (e.g., 1\u2192 3\u2192 5\u2192 7), while the order of switches in N-DISTRACTORS-(N+1) is n + 1 (e.g., 1 \u2192 2 \u2192 3 \u2192 4). This series of settings evaluates if a method can generalize to different effect rules. We collect 200 demonstrations in 4-SWITCHES-(N+1) and and run 5M steps for all methods. For 4-DISTRACTORS-(2N+1), we collect only 4 additional demonstrations for all methods, and our framework leverages the previous populations of genetic programming to re-induce the rules, which only require few-shot demonstrations.\nTable 1: Generalization performance in the original domain 4-SWITCHES and its variant domains.\nTask GAIL BC-PPO Ours\n4-SWITCHES-(N+1) 30%\u00b18% 97%\u00b10% 96%\u00b11% 8-SWITCHES-(N+1) 10%\u00b12% 00%\u00b10% 90%\u00b12% 4-DISTRACTORS-(N+1) 10%\u00b17% 41%\u00b14% 95%\u00b12% 4-DISTRACTORS-(2N+1) 11%\u00b16% 33%\u00b12% 95%\u00b11%\nBaselines. We compare our framework with the best-performing baselines (BCPPO) and the most widely used baseline (GAIL) for the generalization experiments.\nResults. The results in Table 1 demonstrate that the performance of GAIL and BC-PPO drops in the variant domains, whereas our framework is able to generalize, highlighting its ability to construct novel rules and structures in the variant domains.\n6.5 ABLATION STUDY\nThis section presents the ablation studies of the induction modules. Section 6.5.1 qualitatively examines the mutual information and Section 6.5.2 shows the accuracy of symbolic regression using genetic programming."
        },
        {
            "heading": "6.5.1 ACTION-EFFECT LINKAGE",
            "text": "In Section 5.1, we introduce action-effect linkages to discover the co-occurred effect variables and actions. Figure 5 presents the experimental results in MINECRAFT and shows the relationship between the logarithm of mutual information and action-effect linkages. The\nheat map visualizes the values of all action-effect pairs, with darker colors indicating higher val-\nues and stronger associations, highlighting the linkages. For instance, { wood, stick} is the effect variables of make stick as mentioned in Figure 1a and 1b, discovered by our framework from executing make1.\n6.5.2 SYMBOLIC REGRESSION\nThe proposed framework necessitates a robust symbolic regression module to generate the symbolic rules. In Section 5.1, we introduce genetic programming as symbolic regression for induction. Since genetic programming is a randomized search method, empirical results are shown to discuss the success rate of finding correct rules and how much demonstrations are required to capture the symbolic rules.\nThe experiment setting is described as follows. In MINECRAFT environment, there are 27 ef-\nfect rules listed in Table 3. We sample different numbers of demonstrations from random subtasks, and the number of population are 500, 1000, and 2000. Other parameters of genetic programming are the same as the setting in Table 2. We calculate the number of programs which is equivalent to the ground truth after simplification. The result is the average accuracy out of five runs shown in Figure 6. We claim that the effect rules can be induced via genetic programming when a sufficient number of demonstrations and programs in the population are available. Noting that the results are related to the diversity of the data. In theoretically, each n-polynomial rule requires more than n + 1 points for regression. In addition, critical action networks can still be built when some rules are inequivalent to the ground truth due to the bias of the data, as long as the rules match with the precondition of succeeding critical actions."
        },
        {
            "heading": "7 DISCUSSION",
            "text": "We present a framework to address sparse-reward, goal-directed MDP tasks by integrating DRL and classical planning techniques. Our proposed framework represents symbolic knowledge as critical actions and employs a procedure to automatically extract knowledge from demonstrations. This combination of inductive learning (i.e., DRL) and deductive learning (i.e., classical planning) enables our framework to perform explicit high-level planning and accurate low-level execution, allowing for robust task performance and generalizing to unseen domains. Additionally, evolutionary computation provides adaptability at the rule level by inducing the task substructures.\nSpecifically, by representing knowledge as critical actions and employing critical action networks, we provide a structured and organized mechanism for capturing and utilizing symbolic knowledge within DRL. The proposed procedures of subtask decomposition combine planning and DRL, leading to effective and efficient learning in goal-directed tasks. Furthermore, the compositionality of the critical action model allows for different levels of generalization, highlighting its potential to address a wide range of general problems. In sum, our work offers a holistic perspective to effectively handle general goal-directed decision-making problems with the integration of inductive and deductive learning.\nThis work extensively evaluates our proposed framework on deterministic, fully observable environments within the integer domain. To extend our proposed framework to complex domains, such as continuous input, we can potentially leverage recent studies that have developed genetic programming for different variable types (Virgolin et al., 2017; 2021). When facing stochastic or partially observable environments, the induction module can fail to induce some critical actions because of the uncertainty. In a worst-case scenario, when the induction module produces no rule, the proposed framework simply reduces to the backbone DRL algorithm (i.e., PPO) without any intrinsic reward. One potential direction to address this issue is to extend the induction module to recognize and infer probabilistic rules (Pasula et al., 2007; Arora et al., 2018), which is left for future work."
        },
        {
            "heading": "8 ACKNOWLEDGEMENT",
            "text": "The authors would like to thank the support of the National Science and Technology Council in Taiwan under grant No. NSTC 111-2221-E-002-189 and National Taiwan University under grant No. 112L891103. Shao-Hua Sun was partially supported by the Yushan Fellow Program by the Ministry of Education, Taiwan."
        },
        {
            "heading": "APPENDIX",
            "text": ""
        },
        {
            "heading": "A PRELIMINARY DEFINITION",
            "text": "This section provides the annotations of the Markov decision process (MDP) and planning specification discussed in Sectino 4. Section A.1 gives the formulation of MDP problems and Section A.2 explains the definition and the annotation of planning domain definition language (PDDL)."
        },
        {
            "heading": "A.1 MARKOV DECISION PROCESS",
            "text": "A decision-making problem is formulated as a Markov decision process (MDP). MDP consists of a five-tuple \u27e8S,A, T,R, \u03b3\u27e9, where S denotes state space, A denotes action space, T : S \u00d7 A \u2192 S denotes a transition function, R : S \u00d7 A \u2192 R denotes a reward function, and \u03b3 \u2208 (0, 1] denotes a discounting factor.\nIn contrast with classical planning, problems in reinforcement learning represent the actions and the states with vectors of numeric values instead of literal conjunctions. A state in S is a n-dimension vector s, where each entry si, i \u2208 {1, 2, ..., n}, represents the value of a numeric variable. In this work, we focus on goal-directed sparse-reward problems. That is, given an initial state, the objective is to find a policy to reach a desired goal state, and the agent only receives rewards when reaching the goal state."
        },
        {
            "heading": "A.2 PLANNING DOMAIN DEFINITION LANGUAGE",
            "text": "PDDL is a language in first-order logic to describe the domains and the problems. A domain description includes the specification of objects, variables, and action models with preconditions and effects. A problem description includes an initial state and goal specification. These standard language specifications allow off-the-shelf planners to deduce the optimal action sequence of the goal.\nFor theory formalism, we follow the representation of SAS+. To distinguish with MDP state space S, the state space in the planning domain are denoted as S \u2032. A SAS+ task \u03a0 can be represented as a tuple \u27e8V,O, s\u2032init, p\u2032goal\u27e9, where V is a set of variables, and O is a set of operators in the domain. A state s\u2032 \u2208 S \u2032 in the planning domain is an assignment to V , and s\u2032v \u2208 R is the value assigned to the variable v \u2208 V in s\u2032. p\u2032 \u2282 s\u2032 is a partial state of s\u2032, where p\u2032 is the assignment to V(p\u2032) \u2282 V . Specifically, s\u2032init \u2208 S \u2032 is the initial state, and p\u2032goal is a partial state of the goal specification. A logical condition lv describes the relation between the variable v and an distinct value (e.g., wood = 1, stick \u2265 2). Each operator o \u2208 O can be described as an action schema in a pair \u27e8pre(o), eff (o)\u27e9, where pre(o) is a conjunction of logical conditions denoted the precondition needed to be satisfied before executing o, eff (o) is a set of functions denoted the change toward the state variables after executing o. The prevail condition is a subset of pre(o) which holds during the action and does not affect by the effect, denoted as prv(o) = {lv | lv \u2208 pre(o), v \u0338\u2208 V(eff (o))}. An operator o is admissible in state s\u2032 iff pre(o) \u2282 s\u2032 and prv(o) \u2282 s\u2032.\nB IMPLEMENTATION DETAIL\nIn this section, we elaborate on the implementation detail of the proposed framework that was used in the experiments. We implement several modules using off-the-shelf packages and approaches, including genetic programming as symbolic regression with gplearn, the agents with PPO, and the intrinsic reward function we use in the experiments."
        },
        {
            "heading": "B.1 ALGORITHM IN EXTRACTING ACTION-EFFECT LINKAGES",
            "text": "In Algorithm 1, for each MDP action, we calculate the mutual information between the action and all combinations of effect variables in demonstrations. Then, we apply the two-center clustering method to determine the threshold shown in Algorithm 2. Two-center clustering finds a threshold value that separates a given data into two clusters, which minimizes the sum of distances of data points from their respective cluster centers. We take the logarithm of mutual information as the metric to avoid incorrect thresholds caused by extremely high mutual information.\nAlgorithm 1 Extracting Action-Effect Linkages Input: Demonstrations, action set A, effect set E Output: Action-effect pairs with linkages L L\u2190 \u2205 for a in A do\nNa \u2190 \u2205 for V(eff (\u00b7)) in E do\nNa \u2190 Na \u222aM(a,V(eff (\u00b7))) end for t\u2190 Two-Center-Clustering(Na) for V(eff (\u00b7)) in E do\nif M(a,V(eff (\u00b7))) \u2265 t then L\u2190 L \u222a (a,V(eff (\u00b7)))\nend if end for\nend for\nAlgorithm 2 Two-Center-Clustering Input: Data D with length n Output: Threshold of two clusters cluster1, cluster2 \u2190 \u2205 c1, c2 \u2190 min(D),max(D) if c1 = c2 then\nreturn c1 end if terminated\u2190 False while not terminated do:\nfor i = 1 to n do if |D[i]\u2212 c1| < |D[i]\u2212 c2| then\ncluster1 \u2190 D[i] else\ncluster2 \u2190 D[i] end if\nend for c\u20321, c \u2032 2 \u2190 mean(cluster1),mean(cluster2) if c1 = c\u20321 and c2 = c\u20322 then terminated\u2190 True end if c1, c2 \u2190 c\u20321, c\u20322\nend while return (c1 + c2)/2"
        },
        {
            "heading": "B.2 GENETIC PROGRAMMING",
            "text": "Genetic programming is employed as a symbolic regressor for determining symbolic effect rules in the proposed methods, illustrated in Figure 3a. We use the gplearn package for implementation and the parameter settings of are shown in Table 2. Given the action-effect linkage (a,V(eff (\u00b7))), the transitions with action a are selected as the training data. For each effect variable v in V(eff (\u00b7)), the algorithm\u2019s objective is to find the program eff (\u00b7)v that predicts v after executing the action with the highest accuracy.\nEach program is represented as an expression tree where input is the current state in the transition and output is the predicted value. The algorithm comprises several steps: initialization, evaluation, selection, crossover, and mutation. Initially, the population, which is a set of programs, is randomly generated. Fitness evaluation is then performed on all programs; a subset of programs with the highest fitness values is selected. These programs serve as parents to produce offspring through crossover and mutation mechanisms. Through iterative selection and production, the evolution of\nthe population to discover the programs that best fit the given data. The evaluation metric used in genetic programming is the percentage of correct effect prediction shown below:\nfitness(eff (\u00b7)v) = # of transitions with (a,V(eff (\u00b7))) consistent with eff (\u00b7)v\n# of transitions with (a,V(eff (\u00b7))) , (2)\nwhere a transition consistent with eff (\u00b7)v means that the predicted effect value eff (\u00b7)v(sv) is consistent with the actual one T (s, a) given the transition \u27e8s, a, T (s, a)\u27e9. To prevent bloat issues in which the program grows extremely large to fit the data, the algorithm contains two phases: exploring and pruning. The best programs with the highest accuracy are determined in the exploring phase. Subsequently, in the pruning phase, we set high parsimony to prune the program."
        },
        {
            "heading": "B.3 DECISION TREE METHOD",
            "text": "The proposed framework uses classification and regression tree (CART) (Loh, 2011) to build decision trees. CART is a supervised learning algorithm that generates binary trees by recursive partitioning, where each internal node represents a decision based on a specific variable, and each leaf node represents a prediction. Let data partitioned at the internal node m denoted as Dm with nm samples. The algorithm aims to find a decision with a variable q and a threshold t to partition Dm into two subsets D0m and D 1 m with n 0 m and n 1 m samples. The loss function of the partition is defined as follows:\nG(Dm, q, t) = n0m nm H(D0m) + n1m nm H(D1m), (3)\nwhere H(Dim) is the entropy of D i m. In each partition, the algorithm\u2019s objective is to find the (q, t) that minimizes G(Dm, q, t) at node m. This process is repeated until a stopping criterion is met.\nIn the given transition \u27e8s, a, T (s, a)\u27e9with a in demonstrations, the current states s are taken as inputs to a decision tree, and the outcome of the decision tree is a true value that whether T (s, a) consistent with the rules in eff (\u00b7). After generating a decision tree by CART, the model of this decision tree is then transferred into a conjunction of rules by logical simplification and set as the precondition rules pre(\u00b7), while V(pre(\u00b7)) is the set of variables mentioned in pre(\u00b7). Considering the precondition is the conjunction of the precondition rules while the formula of the decision tree may involve disjunction, the decision tree model is transferred into the disjunctive normal form. Each clause in the disjunctive normal form is considered the precondition for different critical actions."
        },
        {
            "heading": "B.4 DEEP REINFORCEMENT LEARNING MODULE",
            "text": "In our research, we adopt the Proximal Policy Optimization (PPO) as the foundation for our framework and the baseline in this work, utilizing the torch-ac library (Willems, 2022) for implementation. The architecture of our model is designed to encode an 8x8 gridworld and state information.\nThe gridworld is handled by a convolutional neural network (CNN) comprising four layers. The network effectively processes the input images, which are formatted in a three-channel setup, representing the object, color, and status respectively. The first convolutional layer utilizes 32 filters with a kernel size of 3\u00d73 and a stride of 2\u00d72. Subsequent layers employ 2\u00d72 kernels, with channel sizes incrementing through 64, 96, to 128. Each layer integrates the ReLU activation function.\nParallelly, the state representation, converted into PDDL, is encoded through a two-layer fullyconnected network. Each of these layers contains 64 neurons and leverages the ReLU activation function for non-linear transformation.\nTwo types of encoded observation are concatenated and encoded by another two-layer 64\u00d764 fullyconnected network. The output-encoded observation is then used as the input of the actor network and the critic network. Both the encoding networks and the actor-critic networks are trained."
        },
        {
            "heading": "B.5 REWARD FUNCTION",
            "text": "During the training stage, we train an agent with intrinsic rewards generated from the critical action network. The modified rewards function is illustrated as follows:\nRint(s) = { +1 if execute a critical action, 0 otherwise.\n(4)\nGiven the original reward function R as extrinsic rewards, the overall reward of the MDP problem is Rmod(s) = R(s) +Rint(s). In the experiment, the reward function is defined as\nR =\n{ stepmax\u2212step stepmax\nif the episode terminated, 0 otherwise,\n(5)\nwhere stepmax is the maximum number of steps in the environment, and step is where step is the current number of steps the agent has done. The setting of the maximum number of steps in each environment is described in Appendix C."
        },
        {
            "heading": "C TEST ENVIRONMENTS",
            "text": "We use three MDP environments: SWITCH, DOORKEY, and MINECRAFT for evaluation. The first environment SWITCH tests the ability to achieve sequential tasks. The second environment DOORKEY is similar to door-key in Minigrid which is a baseline environment with hierarchical tasks. The third environment MINECRAFT is designed to evaluate the ability to construct various task structures with multiple subtasks for compositional tasks. The following sections provide a description of the environments. The maximum number of steps in DOORKEY is 1600, while in SWITCH and MINECRAFT is 25600."
        },
        {
            "heading": "C.1 SWITCH",
            "text": "The environment SWITCH is designed to evaluate the ability to solve hierarchical tasks. In SWITCH, several switches are placed on the grid. The objective of the agent in SWITCH is to sequentially turn on switches in a pre-determined order.\nWe define the state variables as V = {at switch, next switch, goal switch}. at switch indicates the switch the agent stays at. If the agent does not stay at any switch, this variable is set to zero. next switch indicates which switch should be activated in the following actions. goal switch denotes the last switch and also implies how many switches should be turned on. The action space A contains five actions: {left, right, up, down, toggle}. toggle enables the agent to activate or deactivate a switch.\nThe switches have three states, including available, on and off. The agent can turn the available switch to on. If the agent executes the action toggle at the switch, it will be deactivated and turned to available. The agent can not change the status of the off switch until the predecessor switch is on. If a on switch is turned to available, all subsequent switches will also be deactivated. This makes it challenging for RL agents to solve the task through random walks or exploration alone.\nFor various evaluations, we design several situations, including the number of switches, sequential order, and distractors. In the following sections, the settings of the tasks are listed and the illustrations can be found in Figure 7.\n\u2022 N-SWITCHES. When the number of switches increases, the tasks become more difficult as it has more chance to turn off the switch. This setting evaluates the performance of different difficulties of tasks.\n\u2022 N-DISTRACTORS. Available switches are added as distractors to the environment. The agent can turn on and off the distractor switches, but it does not help to achieve the tasks. In n-switch incremental-order tasks, the switches labeled n + 1 to 2n are set as distractors, while in n-switch incremental-order tasks, the switches labeled 2, 4, ..., 2n are set as distractors. This setting evaluates whether the agent acquires the ability to select correct switches and neglects the incorrect ones.\n\u2022 4-ROOMS. Two lines of walls divide the gridworld into four rooms according to the fourroom configuration in Minigrid. Every two rooms are interconnected by a gap in the walls. In this scenario, the agent must navigate through the rooms considering the walls to activate the switches. This setting evaluates the efficacy of DRL involving navigating obstacles at low-level execution.\n\u2022 Order of the switches. To evaluate generalizability, we define two types of orders including (N+1) and (2N+1). (N+1) indicates that the switches should be turned on in incremental order (i.e., 1 \u2192 2 \u2192 3 \u2192 4...) where (goal switch = n), and (2N+1) indicates that the switches should be turned on in odd order (i.e., 1 \u2192 3 \u2192 5 \u2192 7...) where (goal switch = 2n+ 1). The order are labeled after the task name (e.g., 4- DISTRACTORS-(N+1)), and by default the order is (N+1)."
        },
        {
            "heading": "C.2 DOORKEY",
            "text": "The environment DOORKEY presents a task where an agent must collect a key to unlock a door and turn on the switch behind the door. It is a basic setting which can be used to evaluate the ability to solve hierarchical tasks.\nWe define the state variables as V = {agent dir, has key, door state, at switch, at door, next switch}. agent dir indicates the agent direction, has key indicates whether the agent holds the key, and door state indicates the state of the door, including open, closed, and locked. The action space A contains seven actions: {left, right, up, down, toggle, pickup, drop}. toggle enables the agent to activate a switch or open the door."
        },
        {
            "heading": "C.3 MINECRAFT",
            "text": "MINECRAFT is inspired by the computer game Minecraft and is similar to the environment in previous works (Sohn et al., 2018; Andreas et al., 2017; Sun et al., 2020; Brooks et al., 2021) illustrated in Figure 8a. The agent can pick up the primary materials on the map and make different tools in specific places consuming the materials. The goal of each task is to acquire the desired materials or tools. The state variables include {x, y, at < place >,< inventory >}, where at < place > denotes whether the agent is at the place, and < inventory > denotes the number of materials or tools the agent holds.\nIn our experiments, thirteen types of items are designed in the inventory: wood, stone, stick, iron, gen, stone pickaxe, iron pickaxe, wool, paper, scissors, bed, jukebox, enhance table, and there are seven places on the gird world: at wood, at stone, at iron, at gem, at sheep, at workbench, at toolshed.\nThe action spaceA contains eight actions: {left, right, up, down, make1, make2, make3, make4}. The agent crafts different items when executing different make actions (make1, make2, make3, make4) and at different places (workbench or toolshed). The formulas of the items are listed in Table 3, and the dependency of the subtasks is illustrated in Figure 8b. The agent needs to get the materials to create desired items. We test two single tasks with different difficulties IRON and ENHANCETABLE, and a multiple task MULTIPLE that sample the goal at random."
        },
        {
            "heading": "D CRITICAL ACTION",
            "text": "The critical action and its graph are shown in this section. Tables 4 and 5 show the critical actions in DOORKEY and SWITCH environments. The rules of critical actions in MINECRAFT environment are already listed in Table 3 and are omitted in this section. Figures 9, 10, 11 and 12 illustrate the critical action networks of N-SWITCHES, DOORKEY, IRON and ENHANCETABLE, respectively."
        }
    ],
    "year": 2024
}