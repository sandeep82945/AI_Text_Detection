{
    "abstractText": "Saliency-based representation visualization (SRV) (e.g., Grad-CAM) is one of the most classical and widely adopted explainable artificial intelligence (XAI) methods for its simplicity and efficiency. It can be used to interpret deep neural networks by locating saliency areas contributing the most to their predictions. However, it is difficult to automatically measure and evaluate the performance of SRV methods due to the lack of ground-truth salience areas of samples. In this paper, we revisit the backdoor-based SRV evaluation, which is currently the only feasible method to alleviate the previous problem. We first reveal its implementation limitations and unreliable nature due to the trigger generalization of existing backdoor watermarks. Given these findings, we propose a generalization-limited backdoor watermark (GLBW), based on which we design a more faithful XAI evaluation. Specifically, we formulate the training of watermarked DNNs as a min-max problem, where we find the \u2018worst\u2019 potential trigger (with the highest attack effectiveness and differences from the ground-truth trigger) via inner maximization and minimize its effects and the loss over benign and poisoned samples via outer minimization in each iteration. In particular, we design an adaptive optimization method to find desired potential triggers in each inner maximization. Extensive experiments on benchmark datasets are conducted, verifying the effectiveness of our generalization-limited watermark. Our codes are available at https://github.com/yamengxi/GLBW.",
    "authors": [
        {
            "affiliations": [],
            "name": "BACKDOOR WATERMARK"
        },
        {
            "affiliations": [],
            "name": "Mengxi Ya"
        },
        {
            "affiliations": [],
            "name": "Yiming Li"
        },
        {
            "affiliations": [],
            "name": "Tao Dai"
        },
        {
            "affiliations": [],
            "name": "Bin Wang"
        },
        {
            "affiliations": [],
            "name": "Yong Jiang"
        },
        {
            "affiliations": [],
            "name": "Shu-Tao Xia"
        }
    ],
    "id": "SP:3f297f31e5ca270c4bb3489f8e8f931f501634f8",
    "references": [
        {
            "authors": [
                "Gunilla Borgefors"
            ],
            "title": "Hierarchical chamfer matching: A parametric edge matching algorithm",
            "venue": "IEEE Transactions on pattern analysis and machine intelligence,",
            "year": 1988
        },
        {
            "authors": [
                "L\u00e9on Bottou"
            ],
            "title": "Large-scale machine learning with stochastic gradient descent",
            "venue": "In COMPSTAT,",
            "year": 2010
        },
        {
            "authors": [
                "Zana Bu\u00e7inca",
                "Phoebe Lin",
                "Krzysztof Z Gajos",
                "Elena L Glassman"
            ],
            "title": "Proxy tasks and subjective measures can be misleading in evaluating explainable ai systems",
            "venue": "In IUI,",
            "year": 2020
        },
        {
            "authors": [
                "John Canny"
            ],
            "title": "A computational approach to edge detection",
            "venue": "IEEE Transactions on pattern analysis and machine intelligence,",
            "year": 1986
        },
        {
            "authors": [
                "Ruoyu Chen",
                "Hua Zhang",
                "Siyuan Liang",
                "Jingzhi Li",
                "Xiaochun Cao"
            ],
            "title": "Less is more: Fewer interpretable region via submodular subset selection",
            "venue": "In ICLR,",
            "year": 2024
        },
        {
            "authors": [
                "Khoa Doan",
                "Yingjie Lao",
                "Ping Li"
            ],
            "title": "Backdoor attack with imperceptible input and latent modification",
            "venue": "In NeurIPS,",
            "year": 2021
        },
        {
            "authors": [
                "Rudresh Dwivedi",
                "Devam Dave",
                "Het Naik",
                "Smiti Singhal",
                "Rana Omer",
                "Pankesh Patel",
                "Bin Qian",
                "Zhenyu Wen",
                "Tejal Shah",
                "Graham Morgan"
            ],
            "title": "Explainable ai (xai): Core ideas, techniques, and solutions",
            "venue": "ACM Computing Surveys,",
            "year": 2023
        },
        {
            "authors": [
                "Guanhao Gan",
                "Yiming Li",
                "Dongxian Wu",
                "Shu-Tao Xia"
            ],
            "title": "Towards robust model watermark via reducing parametric vulnerability",
            "venue": "In ICCV,",
            "year": 2023
        },
        {
            "authors": [
                "Kuofeng Gao",
                "Jiawang Bai",
                "Bin Chen",
                "Dongxian Wu",
                "Shu-Tao Xia"
            ],
            "title": "Backdoor attack on hashbased image retrieval via clean-label data poisoning",
            "venue": "In BMVC,",
            "year": 2023
        },
        {
            "authors": [
                "Yinghua Gao",
                "Yiming Li",
                "Linghui Zhu",
                "Dongxian Wu",
                "Yong Jiang",
                "Shu-Tao Xia"
            ],
            "title": "Not all samples are born equal: Towards effective clean-label backdoor attacks",
            "venue": "Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Xueluan Gong",
                "Ziyao Wang",
                "Yanjiao Chen",
                "Meng Xue",
                "Qian Wang",
                "Chao Shen"
            ],
            "title": "Kaleidoscope: Physical backdoor attacks against deep neural networks with rgb filters",
            "venue": "IEEE Transactions on Dependable and Secure Computing,",
            "year": 2023
        },
        {
            "authors": [
                "Tianyu Gu",
                "Kang Liu",
                "Brendan Dolan-Gavitt",
                "Siddharth Garg"
            ],
            "title": "Badnets: Evaluating backdooring attacks on deep neural networks",
            "venue": "IEEE Access,",
            "year": 2019
        },
        {
            "authors": [
                "Junfeng Guo",
                "Yiming Li",
                "Lixu Wang",
                "Shu-Tao Xia",
                "Heng Huang",
                "Cong Liu",
                "Bo Li"
            ],
            "title": "Domain watermark: Effective and harmless dataset copyright protection is closed at hand",
            "venue": "NeurIPS,",
            "year": 2023
        },
        {
            "authors": [
                "Wenbo Guo",
                "Lun Wang",
                "Yan Xu",
                "Xinyu Xing",
                "Min Du",
                "Dawn Song"
            ],
            "title": "Towards inspecting and eliminating trojan backdoors in deep neural networks",
            "venue": "In ICDM,",
            "year": 2020
        },
        {
            "authors": [
                "Wooseok Ha",
                "Chandan Singh",
                "Francois Lanusse",
                "Srigokul Upadhyayula",
                "Bin Yu"
            ],
            "title": "Adaptive wavelet distillation from neural networks through interpretations",
            "venue": "NeurIPS,",
            "year": 2021
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "year": 2016
        },
        {
            "authors": [
                "Sara Hooker",
                "Dumitru Erhan",
                "Pieter-Jan Kindermans",
                "Been Kim"
            ],
            "title": "A benchmark for interpretability methods in deep neural networks",
            "venue": "NeurIPS,",
            "year": 2019
        },
        {
            "authors": [
                "Andrew Ilyas",
                "Shibani Santurkar",
                "Dimitris Tsipras",
                "Logan Engstrom",
                "Brandon Tran",
                "Aleksander Madry"
            ],
            "title": "Adversarial examples are not bugs, they are features",
            "venue": "NeurIPS,",
            "year": 2019
        },
        {
            "authors": [
                "Been Kim",
                "Martin Wattenberg",
                "Justin Gilmer",
                "Carrie Cai",
                "James Wexler",
                "Fernanda Viegas"
            ],
            "title": "Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav)",
            "year": 2018
        },
        {
            "authors": [
                "Alex Krizhevsky"
            ],
            "title": "Learning multiple layers of features from tiny images",
            "venue": "Technical report,",
            "year": 2009
        },
        {
            "authors": [
                "Chao Li",
                "Kelu Yao",
                "Jin Wang",
                "Boyu Diao",
                "Yongjun Xu",
                "Quanshi Zhang"
            ],
            "title": "Interpretable generative adversarial networks",
            "venue": "In AAAI,",
            "year": 2022
        },
        {
            "authors": [
                "Jiawei Li",
                "Yiming Li",
                "Xingchun Xiang",
                "Shu-Tao Xia",
                "Siyi Dong",
                "Yun Cai"
            ],
            "title": "Tnt: An interpretable tree-network-tree learning framework using knowledge distillation",
            "year": 2020
        },
        {
            "authors": [
                "Yiming Li",
                "Jiawang Bai",
                "Jiawei Li",
                "Xue Yang",
                "Yong Jiang",
                "Shu-Tao Xia"
            ],
            "title": "Rectified decision trees: exploring the landscape of interpretable and effective machine learning",
            "venue": "arXiv preprint arXiv:2008.09413,",
            "year": 2020
        },
        {
            "authors": [
                "Yiming Li",
                "Tongqing Zhai",
                "Yong Jiang",
                "Zhifeng Li",
                "Shu-Tao Xia"
            ],
            "title": "Backdoor attack in the physical world",
            "venue": "In ICLR Workshop,",
            "year": 2021
        },
        {
            "authors": [
                "Yiming Li",
                "Yang Bai",
                "Yong Jiang",
                "Yong Yang",
                "Shu-Tao Xia",
                "Bo Li"
            ],
            "title": "Untargeted backdoor watermark: Towards harmless and stealthy dataset copyright protection",
            "venue": "NeurIPS,",
            "year": 2022
        },
        {
            "authors": [
                "Yiming Li",
                "Yong Jiang",
                "Zhifeng Li",
                "Shu-Tao Xia"
            ],
            "title": "Backdoor learning: A survey",
            "venue": "IEEE Transactions on Neural Networks and Learning Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Yiming Li",
                "Baoyuan Wu",
                "Yan Feng",
                "Yanbo Fan",
                "Yong Jiang",
                "Zhifeng Li",
                "Shu-Tao Xia"
            ],
            "title": "Semisupervised robust training with generalized perturbed neighborhood",
            "venue": "Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Yiming Li",
                "Haoxiang Zhong",
                "Xingjun Ma",
                "Yong Jiang",
                "Shu-Tao Xia"
            ],
            "title": "Few-shot backdoor attacks on visual object tracking",
            "venue": "In ICLR,",
            "year": 2022
        },
        {
            "authors": [
                "Yiming Li",
                "Mengxi Ya",
                "Yang Bai",
                "Yong Jiang",
                "Shu-Tao Xia"
            ],
            "title": "BackdoorBox: A python toolbox for backdoor learning",
            "venue": "In ICLR Workshop,",
            "year": 2023
        },
        {
            "authors": [
                "Yiming Li",
                "Mingyan Zhu",
                "Xue Yang",
                "Yong Jiang",
                "Tao Wei",
                "Shu-Tao Xia"
            ],
            "title": "Black-box dataset ownership verification via backdoor watermarking",
            "venue": "IEEE Transactions on Information Forensics and Security,",
            "year": 2023
        },
        {
            "authors": [
                "Yuezun Li",
                "Yiming Li",
                "Baoyuan Wu",
                "Longkang Li",
                "Ran He",
                "Siwei Lyu"
            ],
            "title": "Invisible backdoor attack with sample-specific triggers",
            "venue": "In ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "Yi-Shan Lin",
                "Wen-Chuan Lee",
                "Z Berkay Celik"
            ],
            "title": "What do you see? evaluation of explainable artificial intelligence (xai) interpretability through neural backdoors",
            "venue": "In KDD,",
            "year": 2021
        },
        {
            "authors": [
                "Aleksander Madry",
                "Aleksandar Makelov",
                "Ludwig Schmidt",
                "Dimitris Tsipras",
                "Adrian Vladu"
            ],
            "title": "Towards deep learning models resistant to adversarial attacks",
            "venue": "In ICLR,",
            "year": 2018
        },
        {
            "authors": [
                "Richard Meyes",
                "Melanie Lu",
                "Constantin Waubert de Puiseau",
                "Tobias Meisen"
            ],
            "title": "Ablation studies in artificial neural networks",
            "venue": "arXiv preprint arXiv:1901.08644,",
            "year": 2019
        },
        {
            "authors": [
                "Anh Nguyen",
                "Anh Tran"
            ],
            "title": "Wanet\u2013imperceptible warping-based backdoor attack",
            "venue": "In ICLR,",
            "year": 2021
        },
        {
            "authors": [
                "Nobuyuki Otsu"
            ],
            "title": "A threshold selection method from gray-level histograms",
            "venue": "IEEE transactions on systems, man, and cybernetics,",
            "year": 1979
        },
        {
            "authors": [
                "Xiangyu Qi",
                "Tinghao Xie",
                "Yiming Li",
                "Saeed Mahloujifar",
                "Prateek Mittal"
            ],
            "title": "Revisiting the assumption of latent separability for backdoor defenses",
            "venue": "In ICLR,",
            "year": 2023
        },
        {
            "authors": [
                "Ximing Qiao",
                "Yukun Yang",
                "Hai Li"
            ],
            "title": "Defending neural backdoors via generative distribution modeling",
            "venue": "In NeurIPS,",
            "year": 2019
        },
        {
            "authors": [
                "Haibo Qiu",
                "Baosheng Yu",
                "Dihong Gong",
                "Zhifeng Li",
                "Wei Liu",
                "Dacheng Tao"
            ],
            "title": "Synface: Face recognition with synthetic data",
            "venue": "In ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "Sukrut Rao",
                "Moritz B\u00f6hle",
                "Bernt Schiele"
            ],
            "title": "Towards better understanding attribution methods",
            "venue": "In CVPR,",
            "year": 2022
        },
        {
            "authors": [
                "Marco Tulio Ribeiro",
                "Sameer Singh",
                "Carlos Guestrin"
            ],
            "title": "why should i trust you?\u201d explaining the predictions of any classifier",
            "year": 2016
        },
        {
            "authors": [
                "Wojciech Samek",
                "Alexander Binder",
                "Gr\u00e9goire Montavon",
                "Sebastian Lapuschkin",
                "Klaus-Robert M\u00fcller"
            ],
            "title": "Evaluating the visualization of what a deep neural network has learned",
            "venue": "IEEE transactions on neural networks and learning systems,",
            "year": 2016
        },
        {
            "authors": [
                "Ramprasaath R Selvaraju",
                "Michael Cogswell",
                "Abhishek Das",
                "Ramakrishna Vedantam",
                "Devi Parikh",
                "Dhruv Batra"
            ],
            "title": "Grad-cam: Visual explanations from deep networks via gradient-based localization",
            "year": 2017
        },
        {
            "authors": [
                "Shawn Shan",
                "Emily Wenger",
                "Bolun Wang",
                "Bo Li",
                "Haitao Zheng",
                "Ben Y Zhao"
            ],
            "title": "Gotta catch\u2019em all: Using honeypots to catch adversarial attacks on neural networks",
            "venue": "In CCS,",
            "year": 2020
        },
        {
            "authors": [
                "J Springenberg",
                "Alexey Dosovitskiy",
                "Thomas Brox",
                "M Riedmiller"
            ],
            "title": "Striving for simplicity: The all convolutional net",
            "venue": "In ICLR Workshop,",
            "year": 2015
        },
        {
            "authors": [
                "Johannes Stallkamp",
                "Marc Schlipsing",
                "Jan Salmen",
                "Christian Igel"
            ],
            "title": "Man vs. computer: Benchmarking machine learning algorithms for traffic sign recognition",
            "venue": "Neural networks,",
            "year": 2012
        },
        {
            "authors": [
                "Xiaoou Tang",
                "Zhifeng Li"
            ],
            "title": "Frame synchronization and multi-level subspace analysis for video based face recognition",
            "venue": "In CVPR,",
            "year": 2004
        },
        {
            "authors": [
                "Guanhong Tao",
                "Guangyu Shen",
                "Yingqi Liu",
                "Shengwei An",
                "Qiuling Xu",
                "Shiqing Ma",
                "Pan Li",
                "Xiangyu Zhang"
            ],
            "title": "Better trigger inversion optimization in backdoor scanning",
            "year": 2022
        },
        {
            "authors": [
                "Alexander Turner",
                "Dimitris Tsipras",
                "Aleksander Madry"
            ],
            "title": "Label-consistent backdoor attacks",
            "venue": "arXiv preprint arXiv:1912.02771,",
            "year": 2019
        },
        {
            "authors": [
                "Jasper van der Waa",
                "Elisabeth Nieuwburg",
                "Anita Cremers",
                "Mark Neerincx"
            ],
            "title": "Evaluating xai: A comparison of rule-based and example-based explanations",
            "venue": "Artificial Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Bolun Wang",
                "Yuanshun Yao",
                "Shawn Shan",
                "Huiying Li",
                "Bimal Viswanath",
                "Haitao Zheng",
                "Ben Y Zhao"
            ],
            "title": "Neural cleanse: Identifying and mitigating backdoor attacks in neural networks",
            "venue": "IEEE S&P,",
            "year": 2019
        },
        {
            "authors": [
                "Haofan Wang",
                "Zifan Wang",
                "Mengnan Du",
                "Fan Yang",
                "Zijian Zhang",
                "Sirui Ding",
                "Piotr Mardziel",
                "Xia Hu"
            ],
            "title": "Score-cam: Score-weighted visual explanations for convolutional neural networks",
            "venue": "In CVPR Workshop,",
            "year": 2020
        },
        {
            "authors": [
                "Emily Wenger",
                "Roma Bhattacharjee",
                "Arjun Nitin Bhagoji",
                "Josephine Passananti",
                "Emilio Andere",
                "Heather Zheng",
                "Ben Zhao"
            ],
            "title": "Finding naturally occurring physical backdoors in image datasets",
            "venue": "NeurIPS,",
            "year": 2022
        },
        {
            "authors": [
                "Jiahao Xia",
                "Min Xu",
                "Haimin Zhang",
                "Jianguo Zhang",
                "Wenjian Huang",
                "Hu Cao",
                "Shiping Wen"
            ],
            "title": "Robust face alignment via inherent relation learning and uncertainty estimation",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 2023
        },
        {
            "authors": [
                "Liyao Xiang",
                "Haotian Ma",
                "Hao Zhang",
                "Yifan Zhang",
                "Jie Ren",
                "Quanshi Zhang"
            ],
            "title": "Interpretable complex-valued neural networks for privacy protection",
            "venue": "In ICLR,",
            "year": 2020
        },
        {
            "authors": [
                "Chih-Kuan Yeh",
                "Cheng-Yu Hsieh",
                "Arun Suggala",
                "David I Inouye",
                "Pradeep K Ravikumar"
            ],
            "title": "On the (in) fidelity and sensitivity of explanations",
            "venue": "NeurIPS,",
            "year": 2019
        },
        {
            "authors": [
                "Matthew D Zeiler",
                "Rob Fergus"
            ],
            "title": "Visualizing and understanding convolutional networks",
            "venue": "In ECCV,",
            "year": 2014
        },
        {
            "authors": [
                "Yi Zeng",
                "Minzhou Pan",
                "Hoang Anh Just",
                "Lingjuan Lyu",
                "Meikang Qiu",
                "Ruoxi Jia"
            ],
            "title": "Narcissus: A practical clean-label backdoor attack with limited information",
            "venue": "In CCS,",
            "year": 2023
        },
        {
            "authors": [
                "Hongyang Zhang",
                "Yaodong Yu",
                "Jiantao Jiao",
                "Eric Xing",
                "Laurent El Ghaoui",
                "Michael Jordan"
            ],
            "title": "Theoretically principled trade-off between robustness and accuracy",
            "year": 2019
        },
        {
            "authors": [
                "Quanshi Zhang",
                "Ying Nian Wu",
                "Song-Chun Zhu"
            ],
            "title": "Interpretable convolutional neural networks",
            "venue": "In CVPR,",
            "year": 2018
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Deep neural networks (DNNs) have been widely and successfully adopted in many areas, such as facial recognition (Tang & Li, 2004; Qiu et al., 2021; Xia et al., 2023). Despite their promising performance, DNNs are also criticized for acting as a \u2018black box\u2019 whose predictions are not interpretable. The lack of interpretability of DNNs is mostly due to their nonlinear components (e.g., activation functions) and multi-layer structure, hindering DNNs to be deployed in mission-critical applications (e.g., medical diagnosis) at scale.\nCurrently, there are tremendous efforts to explain their prediction behaviors and mechanisms. These methods are called explainable artificial intelligence (XAI). Current, existing XAI methods can be roughly divided into three main categories, including (1) visualizing DNN representations (Zeiler & Fergus, 2014; Wang et al., 2020; Rao et al., 2022), (2) distilling DNNs to explainable models (Li et al., 2020b;a; Ha et al., 2021), and (3) building explainable DNNs (Zhang et al., 2018; Xiang et al., 2020; Li et al., 2022a). Among all these methods, saliency-based representation visualization (SRV) (Springenberg et al., 2015; Selvaraju et al., 2017), as a sub-category of the first one, is probably the most classical and widely adopted one for its simplicity and efficiency. In general, these methods try to measure the significance of each input region to the final prediction. Accordingly, they can locate and highlight saliency input areas that contribute the most to a model prediction.\n\u2217The first two authors contributed equally to this work. Correspondence to: Yiming Li (li-ym@zju.edu.cn).\nOriginal Trigger\nPotential Trigger Distribution\nTarget Label\nPotential Trigger\nBackdoored DNN\nFigure 1: The generalization of existing backdoor watermarks where all potential triggers can activate backdoors.\nWatermarked Image with Original Trigger\nWatermarked Image with Potential Trigger\nBP\nIOU = 0.47 Rank = 2\nIOU = 0.28 Rank = 2\nLIME\nIOU = 1.00 Rank = 1\nIOU = 0.13 Rank = 3\nFA\nIOU = 0.42 Rank = 3\nIOU = 0.50 Rank = 1\nBackdoor-based SRV Evaluation\nFigure 2: The backdoor-based evaluation of three SRV methods (i.e., BP, FA, and LIME) with original and potential triggers.\nHowever, it is difficult to measure and evaluate the performance of SRV methods due to the lack of the ground-truth salience map of the sample. Currently, the most common evaluation method is to have human experts score or rank saliency maps of different SRV methods for each sample. This method is time-consuming, expensive, and may be subject to significant deviations due to the bias of different people. To alleviate this problem, (Lin et al., 2021) proposed to exploit (patchbased) backdoor attacks (Li et al., 2022c) (e.g., BadNets (Gu et al., 2019)) to design an automatic and efficient XAI evaluation. Specifically, they modified a few training samples by stamping a pre-defined trigger patch on their images and changing their labels to a specific target label. DNNs trained on the modified dataset will learn a latent connection between the trigger patch and the target label (i.e., the backdoor). Accordingly, all samples containing the trigger patch will be predicted to be the target label, no matter what their ground-truth label is. Given this backdoor property, they can calculate the average intersection over union (IOU) between the saliency areas generated by the SRV method of the backdoored model over different backdoored samples and the trigger area as an indicator to evaluate the SRV method. This method is developed based on the understanding that trigger regions can be regarded as areas that contribute the most to the model\u2019s predictions and therefore can serve as the ground-truth salience areas for evaluation.\nIn this paper, we revisit the backdoor-based SRV evaluation. We first reveal that it has three implementation limitations, including (1) failing to take absolute values of gradients in the calculation of some SRV methods, (2) selecting saliency areas based on the threshold instead of the saliency size, and (3) using the minimum bounding box covering all edges of significant regions instead of the regions themselves for calculating IOU of SRV methods, leading to potentially false results. More importantly, this method relies on a latent assumption that existing backdoor attacks have no trigger generalization, i.e., only the trigger used for training (dubbed \u2018original trigger\u2019) can activate backdoors, since they treat trigger regions as the ground-truth salience areas. We show that this assumption does not hold for existing backdoor attacks and therefore may lead to unreliable results. For example, given a SRV method, assume that its generated saliency areas of most poisoned samples are only a small part of that of the original trigger. According to the existing backdoor-based evaluation, this SRV method will be treated as having poor performance since it has a small IOU. However, due to the generalization of backdoor watermarks, the model may only learn this local region rather than the whole trigger. In this case, the evaluated SRV method is in fact highly effective, contradicting to the results of the backdoor-based SRV evaluation. Besides, existing backdoor-based evaluation has different results with potential trigger patterns that can activate the backdoor while significantly different from the original trigger (as shown in Figure 1-2). This phenomenon further verifying that its results are unreliable. Accordingly, an intriguing and important question arises: Is it possible to design a faithful automatic XAI evaluation method?\nIn this paper, we explore how to reduce the position generalization of backdoor triggers, based on which to design a faithful backdoor-based XAI evaluation. Arguably, the most straightforward method is to synthesize the potential trigger pattern and penalize its effects based on its distance to the original trigger in each iteration during the training process. However, as we will show in our main experiments, this method suffers from relatively poor performance (i.e., still with high trigger generalization) in many cases. This failure is mostly because the synthesized triggers are \u2018weak\u2019 with either a large loss value or are similar to the original trigger. Based on these understandings, we formulate the training of watermarked DNNs as a min-max problem, where we find the \u2018worst\u2019 potential trigger with the highest attack effectiveness and differences from the original trigger via inner maximization and minimize its effects and the loss over benign and poisoned samples through outer\nminimization. In particular, we design an adaptive optimization method to find desired (\u2018worst\u2019) potential triggers in each inner maximization.\nIn conclusion, the main contributions of this paper are four-folds: (1) We reveal and address the implementation limitations of the existing backdoor-based XAI evaluation; (2) We analyze the generalization pattern of backdoor triggers, based on which we further reveal the unreliable nature of the existing backdoor-based XAI evaluation; (3) We propose a generalization-limited backdoor watermark (GLBW), based on which we design a more faithful XAI evaluation. To the best of our knowledge, we are the first trying to measure and even manipulate trigger generalization; (4) We conduct extensive experiments on benchmark datasets to verify the effectiveness of our GLBW."
        },
        {
            "heading": "2 BACKGROUND AND RELATED WORK",
            "text": ""
        },
        {
            "heading": "2.1 BACKDOOR ATTACKS AND THEIR POSITIVE APPLICATIONS",
            "text": "Backdoor attack is an emerging threat to deep neural networks (Li et al., 2022c). Currently, existing backdoor attacks can be roughly divided into two main categories, including patch-based attacks and non-patch-based attacks, based on the trigger properties.\nSpecifically, patch-based attacks (Gu et al., 2019; Li et al., 2022e; Gao et al., 2023b) adopted some local image patches as the trigger patterns. For example, BadNets randomly modified a few samples from the original benign dataset by stamping the black-and-white trigger square to their images and changing their labels to the target label. These attacks are highly effective since their trigger patterns are significantly different from the surrounding of the replaced image areas in the poisoned samples; In contrast, non-patch-based attacks exploited some transformations over the whole image, such as additive noises (Li et al., 2021b), image-warping (Nguyen & Tran, 2021), and color shifting (Gong et al., 2023), to generate poisoned images. In particular, a few works (Qiao et al., 2019; Li et al., 2021a; Qi et al., 2023) initially discovered trigger generalization phenomenon, i.e., some potential trigger patterns are (significantly) different from the one used for training but can still activate hidden backdoors in the attacked models, in a few cases of neural cleanse (Wang et al., 2019). However, none of them tried to provided its statistical patterns and measure or control this generalization. How to better understand and control trigger generalization is still an important open-question.\nExcept for using for malicious purposes, there are also a few research studied how to exploit backdoor attacks for positive purposes based on their properties, such as adversarial defense (Shan et al., 2020) and copyright protection (Li et al., 2022b; 2023b; Guo et al., 2023; Gan et al., 2023). In particular, Lin et al. (Lin et al., 2021) used backdoor attacks to watermark DNNs, based on which to design an evaluation method of saliency-based representation visualization. However, almost all these applications relied on a latent assumption that backdoor triggers have mild generalization, which does not holds for existing backdoor attacks."
        },
        {
            "heading": "2.2 SALIENCY-BASED REPRESENTATION VISUALIZATION (SRV) AND ITS EVALUATIONS",
            "text": "How to explain the predictions or mechanisms of DNNs is always an important research direction. Currently, existing XAI methods of DNNs can be divided into three main categories, including (1) visualizing DNN representations (Zeiler & Fergus, 2014; Rao et al., 2022; Chen et al., 2024), (2) distilling DNNs to explainable models (Li et al., 2020a; Ha et al., 2021), and (3) building explainable DNNs (Zhang et al., 2018; Xiang et al., 2020; Li et al., 2022a). In this paper, we focus on the saliency-based representation visualization (SRV), which a sub-type of the first category.\nSaliency-based Representation Visualization (SRV). In general, SRV methods explore visual patterns within a DNN unit (e.g., the logit). Specifically, they generate a saliency map for each sample whose value indicates the contribution of its corresponding part (e.g., pixel) to the unit\u2019s result. In general, we can divide existing SRV methods into two main categories, including white-box SRV and black-box SRV. Their difference lies in whether the SRV method can access the source files of the model. Currently, gradient-based methods are the mainstay of white-box SRV. For example, back-propagation (BP) adopted the gradient of samples concerning their predicted results to render their saliency maps; Guided back-propagation (GBP) (Springenberg et al., 2015) only considered the positive gradients and set the negative parts as zero in ReLU layers; Gradient-weighted class activation mapping (GCAM) (Selvaraju et al., 2017) generated saliency maps via features in the\nlast convolutional layer; GGCAM (Selvaraju et al., 2017) combined GBP and GCAM via elementwise multiplication. Black-box SRV usually obtained the saliency map by perturbing the input and observing prediction changes. For example, occlusion sensitivity (OCC) (Zeiler & Fergus, 2014) occluded different image portions and monitored output differences; LIME (Ribeiro et al., 2016) learned an interpretable model locally varound the prediction; Recently, feature ablation (FA) (Meyes et al., 2019) perturbed a group of features instead of a single one to render saliency maps.\nThe Evaluation Methods of SRV Methods. In general, it is difficult to measure and evaluate the performance of SRV methods due to the lack of the ground-truth salience map of a sample. Currently, the most reliable evaluation of SRV methods is still based on the human inspection with ranking or scoring (Ribeiro et al., 2016; Kim et al., 2018; Dwivedi et al., 2023). However, this evaluation is time-consuming and costly or even leads to biased and inaccurate results (Buc\u0327inca et al., 2020). There are also a few automatic SRV evaluations, mostly by measuring the performance fluctuations when perturbing critical regions (Samek et al., 2016; Yeh et al., 2019; van der Waa et al., 2021). However, these methods usually require heavy computations and are even inaccurate due to distribution shift (Hooker et al., 2019); Recently, Lin et al. (Lin et al., 2021) proposed to exploit backdoor-based model watermarks to design SRV evaluation, based on the principle that trigger locations can be regarded as the most critical regions contributing the most to model\u2019s predictions. Specifically, it first generated the saliency maps of some poisoned samples containing trigger patterns and turned them into 0-1 masks based on pre-defined thresholds. After that, it found a minimum bounding box covering all edges of the trigger mask detected by the Canny algorithm (Canny, 1986). The final score was calculated by the distance, such as average intersection over union (IOU), between generated bounding box and the trigger mask over all samples. However, as we will show in the following sections, this evaluation method is also unreliable."
        },
        {
            "heading": "3 REVISITING BACKDOOR-BASED XAI EVALUATION",
            "text": ""
        },
        {
            "heading": "3.1 STANDARDIZE BACKDOOR-BASED XAI EVALUATION",
            "text": "Currently, the backdoor-based SRV evaluation (Lin et al., 2021) still has some implementation limitations that may lead to inaccurate results. In this section, we will standardize it.\nImplementation Limitations and Their Standardization. Firstly, (Lin et al., 2021) took absolute values of gradients in only parts of (instead of all) evaluated SRV methods before generating their saliency map. Since both positive or negative influnces are all important, to ensure a fair comparison, we propose to conduct it on the gradients of all SRV methods; Secondly, (Lin et al., 2021) adopted a pre-defined threshold to filter out the most critical regions (i.e., the 0-1 mask) from the saliency map for SRV methods. However, the same threshold value has different influences across samples even for the same SRV method. To ensure a fair comparison, we propose to select M pixel\nlocations with the maximum saliency value as significant regions, where M is the size of trigger used for model watermarking; Thirdly, (Lin et al., 2021) adopted the minimum bounding box covering all edges of significant regions instead of themselves for calculating IOU. It may lead to inaccurate results, especially when the original trigger has complex shapes. Accordingly, we calculate the IOU based on the significant regions directly. In general, the second limitation has largest effects.\nWe conduct the following experiments to verify that the vanilla backdoor-based SRV evaluation may lead to inaccurate results and therefore we need to standardize it. Please refer to Appendix C for more details about why Lin et al. (2021) and we only consider patch-based triggers.\nSettings. We adopt ResNet-18 (He et al., 2016) on CIFAR-10 (Krizhevsky, 2009) and GTSRB (Stallkamp et al., 2012) datasets for our discussions. Following the settings in (Lin et al., 2021), we adopt BadNets (Gu et al., 2019) for model watermarking. Specifically, the target label is set to \u20180\u2019 and the watermarking rate is set to 5%. We adopt four triggers with two patterns (as shown in Figure 3) located in the lower-right corner or a random position of the image. We randomly sample 100 testing images for evaluation. We calculate the average rank by averaging the rank of SRV methods (based on the IOU) in each scenario. Please find more details in Appendix A.\nResults. As shown in Table 1-2, the vanilla backdoor-based method and our standardized version have significantly different results. The vanilla method has various evaluations of the performance of the same SRV method on different datasets (especially for BP). In contrast, our standardized evaluation method has consistent rankings. These results verify that our evaluation is more faithful."
        },
        {
            "heading": "3.2 TRIGGER GENERALIZATION AND ITS INFLUENCES TO XAI EVALUATION",
            "text": "A Closer Look to Location Generalization of Trigger Patterns. In this section, we generate 1,000 targeted universal adversarial perturbations (with random initializations) regarding the target label of models watermarked with the square-type trigger as potential trigger patterns. The watermarked DNNs are the same as those obtained in Section 3.1. To analyze the generalization properties of backdoor triggers, we visualize the distance between the potential trigger and the ground-truth one w.r.t. its loss value. Specifically, we exploit Chamfer distance (Borgefors, 1988) to cover the locations of all trigger pixels. As shown in Figure 4, many potential trigger patterns are having a large distance to the original one but with significantly lower loss values. In other words, the trigger pattern of existing watermarks has a high-level position generalization, although only the original trigger pattern is adopted. It is most probably due to the high non-convexity and excessive learning capacities of DNNs. We will further explore its intrinsic mechanism in our future works.\nBackdoor-based XAI Evaluation with Potential Trigger Patterns. In this part, we evaluate whether the existing backdoor-based evaluation method has consistent rankings with potential triggers different from the original one but can still activate model backdoors. Specifically, we adopted\nthree synthesized triggers generated in the previous part on each dataset (as shown in Figure 5) and calculate the IOU of SRV methods with them. As shown in Table 3-4, the backdoor-based method has significantly different results across different triggers even on the same dataset (especially for GGCAM, FA and LIME). This phenomenon also indicates that its results are not reliable."
        },
        {
            "heading": "4 THE PROPOSED METHOD",
            "text": "As demonstrated in Section 3.2, existing backdoor-based SRV evaluation may have unreliable results due to the trigger generalization of existing backdoor attacks. In this section, we discuss how to obtain the generalization-limited backdoor watermark to design a more faithful SRV evaluation."
        },
        {
            "heading": "4.1 PRELIMINARIES",
            "text": "Threat Model. In this paper, we assume that users (i.e., SRV appraisers) can fully control the training process of DNNs. They will try to implant backdoor triggers into the model, based on which to evaluate the performance of SRV methods via poisoned samples.\nThe Main Pipeline of Backdoor-based Model Watermark. LetD = {(xi, yi)}Ni=1 denotes the benign training set, where xi \u2208 X = {0, 1, . . . , 255}C\u00d7H\u00d7W is the image, yi \u2208 Y = {0, . . . ,K \u2212 1} is its label, and K is the number of classes. How to generate the watermarked dataset Dw is the cornerstone of the backdoor-based model watermark. Specifically, Dw consists of two disjoint parts, including the modified version of a selected subset (i.e., Ds) of D and remaining benign samples, i.e., Dw = Dm \u222a Db, where yt is a pre-defined target label, Db = D\\Ds, Dm = {(x\u2032, yt)|x\u2032 = G(x;w), (x, y) \u2208 Ds}, \u03b3 \u225c |Ds||D| is the watermarking rate, and G : X \u2192 X is the poisoned image generator with parameter w. For example, G(x) = (1 \u2212m) \u2297 x +m \u2297 t, where the mask m \u2208 [0, 1]C\u00d7H\u00d7W , t \u2208 X is the trigger pattern, and \u2297 is the element-wise product used in the backdoor-based SRV evaluation (Lin et al., 2021). Once the watermarked dataset Dw is generated, users will train the watermarked DNN f via min\u03b8 \u2211 (x,y)\u2208Dw L(f(x;\u03b8), y)."
        },
        {
            "heading": "4.2 AN INEFFECTIVE BASELINE: BACKDOOR WATERMARK WITH TRIGGER PENALTY",
            "text": "Arguably, the most straightforward method is to synthesize the potential trigger pattern and penalize its effects based on its distance to the original one in each iteration during the training process. In general, if the potential trigger pattern is close to the ground-truth one, the predictions of watermarked DNNs to samples containing this pattern should be similar to the target label; Otherwise,\ntheir predictions should be similar to their ground-truth label. To achieve this goal, we design a penalty loss and call this method backdoor watermark with trigger penalty ( BWTP), as follows.\nmin \u03b8 \u2211 (x,y)\u2208D\nL (f\u03b8 (x) , y)\ufe38 \ufe37\ufe37 \ufe38 + \u03bb1 \u00b7 L (f\u03b8 ((1\u2212m)\u2297 x+m\u2297 t) , yt)\ufe38 \ufe37\ufe37 \ufe38 + Benign Loss Backdoor Loss\n\u03bb2 \u00b7 max m\u2032,t\u2032,|m\u2032|\u2264T\nL\u2032 ( f\u03b8 ( (1\u2212m\u2032)\u2297 x+m\u2032 \u2297 t\u2032 ) , y\u0302t + y\u0302 ) \ufe38 \ufe37\ufe37 \ufe38,\nP enalty Loss\n(1)\nwhere D is the benign dataset, L and L\u2032 are the loss functions (e.g., cross-entropy and mutual information), t is the original trigger pattern with mask m, and T is the threshold of the size of potential trigger mask m\u2032. y\u0302t = (1 \u2212 p(d(m,m\u2032))) \u00b7 yt and y\u0302 = p(d(m,m\u2032)) \u00b7 y, where y is the one-hot representation of y, d(\u00b7, \u00b7) is a distance metric (e.g., chamfer distance), p(\u00b7) is a probability activation function to map the distance to a probability (the larger the distance, the higher the probability). \u03bb1 and \u03bb2 are two trade-off hyper-parameters for backdoor loss and penalty loss.\nSimilar to the optimization strategies used in adversarial training (Madry et al., 2018; Zhang et al., 2019; Li et al., 2022d), we alternately optimize the inner maximization and the outer minimization in each iteration. Please find more optimization details in Appendix D."
        },
        {
            "heading": "4.3 GENERALIZATION-LIMITED BACKDOOR WATERMARK (GLBW)",
            "text": "As we will show in the experiments and appendix, the previous BWTP suffers from relatively poor performance whose watermarked DNNs still have high trigger generalization in many cases. According to the intermediate results, this failure is mostly because the synthesized triggers are \u2018weak\u2019 with either a large loss value or are similar to the original trigger. Besides, the training of watermarked DNNs is unstable even when the synthesized triggers are promising. We argue that these limitations are due to the complexity of its penalty loss. As such, we propose to directly identify the potential trigger patterns that are more different from the ground-truth one and force them to be predicted as the ground-truth label to further simplify the loss term, inspired by neural cleanse (Wang et al., 2019). Its optimization process is as follows.\nmin \u03b8 \u2211 (x,y)\u2208D\nL (f\u03b8 (x) , y)\ufe38 \ufe37\ufe37 \ufe38 + \u03bb3 \u00b7 L (f\u03b8 ((1\u2212m)\u2297 x+m\u2297 t) , yt)\ufe38 \ufe37\ufe37 \ufe38 + Benign Loss Backdoor Loss\n\u03bb4 \u00b7 max m\u2032,t\u2032,|m\u2032\u2229m|\u2264\u03c4 \u00b7|m|\n\u2212 { L ( f\u03b8 ( (1\u2212m\u2032)\u2297 x+m\u2032 \u2297 t\u2032 ) , yt ) + \u00b5 \u00b7 |m\u2032| } \ufe38 \ufe37\ufe37 \ufe38\nGeneralization Loss\n(2)\nwhere \u00b5 is a hyper-parameter trading off its size and effectiveness for synthesized trigger, \u03bb3 & \u03bb4 are two hyper-parameters for losses, \u03c4 is the overlap threshold between synthesized and original triggers, and |m\u2032| is the size of potential trigger mask. Notice that the y and yt contained in GLBW are different from y\u0302 and y\u0302t used in the previous BWTP since they are fundamentally different.\nIn particular, we design an adaptive optimization method to find promising synthesized patterns with the highest attack effectiveness and differences from the original trigger. Specifically, we repeat the trigger generation and adaptively adjust the \u00b5 based on the current trigger candidate untill we find the promising synthesized trigger in each inner maximization. The main pipeline of our method is shown in Figure 6. Please find more optimization details and its pseudocodes in Appendix E."
        },
        {
            "heading": "5 EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "5.1 MAIN SETTINGS",
            "text": "Baseline Selection. In this paper, we compare our generalization-limited backdoor watermark (GLBW) with vanilla backdoor watermark (dubbed \u2018Vanilla\u2019) (Lin et al., 2021) and backdoor watermark with trigger penalty (BWTP) with standardized evaluation process.\nSettings for Model Watermarks. Following settings in (Lin et al., 2021), we adopt the 3\u00d7 3 white square located in the lower-right bottom as the original trigger on both CIFAR-10 and GTSRB. Specifically, we simply set \u03bb1 = \u03bb2 = 1 for BWTP and \u03bb3 = \u03bb4 = 1 for our GLBW. We use the \u21131-norm as the distance measurement d and exploit a modified version of the Sigmoid function as the probability activation function p(\u00b7) used in BWTP. Please refer to Appendix F for more details. Settings for the Generation of Potential Triggers. In Section 3.2, we adopt neural cleanse (Wang et al., 2019) to generate potential triggers. In our GLBW, we also exploit a similar approach during the generation of synthesized triggers. To evaluate whether our method can truly decrease trigger generalization, we hereby adopt two other trigger synthesis methods, including TABOR (Guo et al., 2020) and pixel backdoor (Tao et al., 2022) to generate potential triggers for all watermarked DNNs. We generate 1,000 trigger candidates (with random initializations) in all cases. We only select those whose loss values are less than 1.5 times of that of the original trigger (as shown in the red line in Figure 7) as our potential trigger patterns. Please refer to Appendix F for more details.\nEvaluation Metrics. We adopt benign accuracy (BA) and watermark success rate (WSR) to evaluate the effectiveness of model watermarks. To evaluate their trigger generalization, we exploit chamfer distance (dubbed \u2018Chamfer\u2019) and the percentage of effective potential triggers with low generalization (dubbed \u2018PLG\u2019). The effective triggers whose IOU values are greater than the threshold (0.3 for CIFAR-10 and 0.18 for GTSRB) are regarded as with low generalization. In general, the smaller the chamfer distance and the larger the PLG, the lower the trigger generalization."
        },
        {
            "heading": "5.2 MAIN RESULTS",
            "text": "As shown in Table 5, both our BWTP and GLBW can obtain a sufficiently high WSR (> 85%) on both datasets. In particular, our GLBW can significantly reduce trigger generalization in all cases (as shown in both Table 5 and Figure 7). For example, the chamfer distance (measured via neural cleanse and TABOR) of GLBW is more than 170 times smaller than that of the vanilla backdoor\nTable 7: Effects of the trigger size.\nSize BA (%) WSR (%) Chamfer PLG (%) 3\u00d7 3 78.33 93.68 23.04 100 4\u00d7 4 83.20 91.58 67.74 100 5\u00d7 5 87.81 91.79 90.44 100\nTable 8: Effects of the target label.\nLabel BA (%) WSR (%) Chamfer PLG (%)\n0 78.33 93.68 23.04 100 1 84.27 83.82 25.31 100 2 77.08 93.23 25.09 100\n5 10 15 20 Watermarking Rate (%)\n0 5\n10 15 20 25\nCh am\nfe r\nD is\nta nc\ne\nChamfer Distance PLG 96\n98 100 102 104 PL G ( % )\nFigure 8: Effects of the watermarking rate. ResNet-18 ResNet-34 VGG-13 VGG-16 Model\n0\n10\n20\n30\n40\nCh am\nfe r\nD is\nta nc\ne\n0 20 40 60 80 100 PL G ( % ) Chamfer Distance PLG\nFigure 9: Effects of the Model Structure.\nwatermark on CIFAR-10. However, the BWTP may even increase the generalization on CIFAR-10, although it can reach remarkable performance on the GTSRB dataset. As we will show in Appendix G.3, it is mostly because its synthesized triggers are \u2018weak\u2019 with either a large loss value or are similar to the original trigger. These results demonstrate the effectiveness of our GLBW method.\nWe notice that both BWTP and GLBW may decrease the benign accuracy to some extent, compared to the vanilla backdoor watermark. However, this mild potential limitation will not hinder the usefulness (i.e., more faithful evaluation) of our methods, since the watermarked DNNs are only used for evaluating SRV methods instead of for deployment (more details are in Appendix J). The average rank of SRV methods via our GLBW is shown in Table 6. As shown in this table, the results are consistent across datasets and therefore are more faithful and reliable. Please find the IOU of SRV methods via three discussed model watermarks under different settings in Appendix G."
        },
        {
            "heading": "5.3 THE EFFECTS OF KEY HYPER-PARAMETERS",
            "text": "We hereby verify whether our GLBW is still effective under different settings on CIFAR-10. The chamfer distance and PLG are measured via neural cleanse. More results are in Appendix H\nEffects of the Watermarking Rate \u03b3. As shown in Figure 8, the watermarking rate has minor effects on trigger generalization. In particular, our GLBW can reach high effectiveness (as shown in our appendix) and low generalization under different \u03b3. It verifies the our effectiveness again.\nEffects of the Trigger Size. As shown in Table 7, increasing the trigger size has minor effects to watermark effectiveness, although there are some fluctuations for BA and WSR. We notice that the chamfer distance raises with the increase in trigger size. However, it is due to the properties of this distance metric and does not mean an increase in trigger generalization. The PLG remains 100%.\nEffects of the Target Label yt. As shown in Table 8, our GLBW has promising performance under different yt. The trigger generalization remains low with a small chamfer distance and 100% PLG.\nEffects of the Model Structure. As shown in Figure 9, our GLBW reaches similar generalization ability, especially with similar model architectures."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "In this paper, we revisited the evaluation methods of saliency-based representation visualization (SRV). We revealed the unreliable nature (due to the trigger generalization of backdoor watermarks) and implementation limitations of existing backdoor-based SRV evaluation methods. Based on these findings, we proposed a generalization-limited backdoor watermark, based on which we designed a more faithful XAI evaluation. We hope our work can provide a deeper understanding of XAI evaluation, to facilitate the design of more interpretable deep learning methods."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "This work is supported in part by the National Natural Science Foundation of China under Grants (62302309, 62171248), Shenzhen Science and Technology Program under Grants (JCYJ20220818101014030, JCYJ20220818101012025), and the PCNL KEY Project (PCL2023AS6-1).\nETHICS STATEMENT\nThis paper is the first attempt toward measuring and reducing trigger generalization of backdoor watermarks and its positive application in XAI evaluation. Accordingly, our paper has positive societal impacts in general. However, we notice that the adversaries may adopt our backdoor watermark with trigger penalty (BWTP) or generalization-limited backdoor watermark (GLBW) as backdoor attacks when they can control the training process of DNNs. However, these attacks are less harmful compared to existing backdoor attacks because they will significantly reduce the benign accuracy of attacked models and therefore can be noticed by victim model users. Besides, victim users can also exploit existing backdoor defenses, especially saliency-based methods, to detect or remove these backdoor attacks. Moreover, although we do not design a specified defense against BWTP or GLBW, people can still mitigate or even avoid the threats by only using trusted training resources."
        },
        {
            "heading": "A SETTINGS FOR STANDARDIZE BACKDOOR-BASED XAI EVALUATION",
            "text": "Datasets and Models. We conduct experiments on two classical datasets, including CIFAR-10 (Krizhevsky, 2009) and GTSRB (Stallkamp et al., 2012), with ResNet-18 (He et al., 2016).\nWatermark Setup. We adopt two watermark patterns: the square-like pattern (as shown in Figure 10(a)) and the compass-like pattern (as shown in Figure 10(b)). Based on these patterns, we generate four kinds of triggers: (a), (b), (c), and (d). Specifically, (a) and (c) are the triggers with the squarelike pattern; (b) and (d) are triggers with the compass-like pattern; (a) and (b) are the triggers located in the lower-right corner; (c) and (d) are the triggers with random locations.\nTraining Setup. We adopt BadNets (Gu et al., 2019) to train our backdoored models based on the open-sourced Python toolbox\u2014BackdoorBox (Li et al., 2023a). We train 8 backdoored models using four triggers (i.e., (a), (b), (c), and (d)) on CIFAR-10 and GTSRB. We set the watermarking rate as 5% and the target label yt is set to 0 for all backdoor training. To train our backdoored models, the mini-batch is set to 128 for each iteration. We use the SGD optimizer by setting momentum=0.9 and weight decay=5e-4. For CIFAR-10, the number of the training epochs is 200 and the learning rate is initialized as 0.1 and multiplied by 0.1 at the 150 and 180 epoch, respectively. For GTSRB, the number of the training epochs is 30 and the learning rate is initialized as 0.01 and multiplied by 0.1 at the 20th epoch. In particular, we add watermarks after performing the data augmentation.\nComputational Facilities. We train each model with one NVIDIA RTX3090 GPU. For CIFAR-10, it costs about 40 minutes for training. For GTSRB, it costs about 15 minutes for training."
        },
        {
            "heading": "B SETTINGS FOR TRIGGER GENERALIZATION AND ITS INFLUENCES TO XAI EVALUATION",
            "text": "We select the four models watermarked with the triggers (a) and (c) on CIFAR-10 and GTSRB used in Section A for discussing trigger generalization. We generate 1,000 targeted universal adversarial perturbations (Wang et al., 2019) (with random initializations) regarding the target label. In particular, we used the neural cleanse (Wang et al., 2019) to generate all synthesized triggers, where the weight of the regularization term is set to 8 \u00d7 10\u22125. The training dataset is used as the set of clean images for neural cleanse. We generate each synthesized trigger with 10 epochs."
        },
        {
            "heading": "C WHY DO WE (ONLY) NEED TO CONSIDER PATCH-BASED TRIGGERS?",
            "text": "People may worry about whether our method can truly lead more faithful evaluation since we only focus on patch-based attacks (e.g., BadNets) and there are still many other backdoor attacks with different trigger patterns (Li et al., 2021b; Doan et al., 2021; Zeng et al., 2023), while people may also use complicated features in recognizing real images in practice.\nFirstly, patch-based poisoned-label backdoor watermark is the most suitable and probably the only suitable method for backdoor-based SRV evaluation. Specifically, clean-label attacks where the adversaries only poison samples for the target class (Turner et al., 2019; Gao et al., 2023b;a; Zeng\net al., 2023) are not suitable since attacked DNNs may use both \u2018ground-truth\u2019 features and trigger features instead of just trigger features for predictions. Besides, the triggers of almost all existing non-patch-based attacks are with full image-size. We certainly don\u2019t want saliency ares are the whole image. Accordingly, these attacks are also not suitable for SRV evaluation.\nSecondly, people categorize a given image usually based on its local regions instead of the whole image. For example, we only need to see the image areas of its \u2018head\u2019 to know it is a bird. These local regions are similar to the trigger patch used in our evaluation. Accordingly, results made by our method can be a good reference in real images for practice. This is probably the main reason why our baseline research (Lin et al., 2021) evaluated their method only with trigger patches.\nThirdly, it is impossible to faithfully evaluate the performance of SRV methods for complicated features (instead of simple trigger patches) since there are no ground-truth salience maps. Even a human expert cannot accurately mark the salience map for complicated features in most cases.\nFourthly, simple trigger patches are also features used by DNNs for their predictions. Accordingly, the evaluation of SRV methods on trigger features is the first and the most important step toward evaluating their general performance and is therefore of great significance.\nIn particular, one may argue that there are still two other patch-based triggers, including \u2018Blended\u2019 (i.e., BadNets with trigger transparency) and patch-size additive trigger (dubbed \u2018Additive\u2019), that we did not evaluated in our main experiments. Accordingly, we hereby conduct additional experiments on the CIFAR-10 dataset with them. As shown in Table 9, our GLBW is still highly effective."
        },
        {
            "heading": "D THE OPTIMIZATION PROCESS OF BWTP",
            "text": "Arguably, the most straightforward method is to synthesize the potential trigger pattern and penalize its effects based on its distance to the original one in each iteration during the training process. In general, if the potential trigger pattern is close to the ground-truth one, the predictions of watermarked DNNs to samples containing this pattern should be similar to the target label; Otherwise, their predictions should be similar to their ground-truth label. In this paper, we call this method as backdoor watermark with trigger penalty (BWTP), whose optimization process is as follows:\nmin \u03b8 \u2211 (x,y)\u2208D\nL (f\u03b8 (x) , y)\ufe38 \ufe37\ufe37 \ufe38 + \u03bb1 \u00b7 L (f\u03b8 ((1\u2212m)\u2297 x+m\u2297 t) , yt)\ufe38 \ufe37\ufe37 \ufe38 + Benign Loss Backdoor Loss\n\u03bb2 \u00b7 max m\u2032,t\u2032,|m\u2032|\u2264T L\u2032 (f\u03b8 ((1\u2212m\u2032)\u2297 x+m\u2032 \u2297 t\u2032) , y\u0302t + y\u0302)\ufe38 \ufe37\ufe37 \ufe38, P enalty Loss\n(1)\nwhere D is the benign dataset, L and L\u2032 are the loss functions (e.g., cross-entropy and mutual information), t is the original trigger pattern with mask m, and T is the threshold of the size of potential trigger mask m\u2032. y\u0302t = (1 \u2212 p(d(m,m\u2032))) \u00b7 yt and y\u0302 = p(d(m,m\u2032)) \u00b7 y, where y is the one-hot representation of y, d(\u00b7, \u00b7) is a distance metric (e.g., chamfer distance), p(\u00b7) is a probability activation function to map the distance to a probability (the larger the distance, the higher the probability). \u03bb1 and \u03bb2 are two trade-off hyper-parameters for backdoor loss and penalty loss.\nSimilar to adversarial training (Madry et al., 2018), we alternately optimize the inner maximization and the outer minimization in each iteration. For the inner maximization, we solve the following formula to get a penalized trigger (with mask m\u2032 and pattern t\u2032):\nmax m\u2032,t\u2032,|m\u2032|\u2264T\nL\u2032 (f\u03b8 ((1\u2212m\u2032)\u2297 x+m\u2032 \u2297 t\u2032) , y\u0302t + y\u0302) . (2)\nFor convenience, this formula can be written in an equivalent form:\nmin m\u2032,t\u2032,|m\u2032|\u2264T\n\u2212 L\u2032 (f\u03b8 ((1\u2212m\u2032)\u2297 x+m\u2032 \u2297 t\u2032) , y\u0302t + y\u0302) . (3)\nAfter that, this conditional minimization problem can be approximated as a non-conditional surrogate problem, as follow:\nmin m\u2032,t\u2032\n[\u2212L\u2032 (f\u03b8 ((1\u2212m\u2032)\u2297 x+m\u2032 \u2297 t\u2032) , y\u0302t + y\u0302) + \u03b3 \u00b7max(0, |m\u2032| \u2212 T )] , (4)\nwhere \u03b3 is the weight of the regularization term used for limiting the size of m\u2032.\nWe proposed an algorithm (as shown in Algorithm 1) to solve the aforementioned problem (4): Firstly, we randomly initialize m\u2032o and t \u2032 o by sampling from the standard normal distribution. Secondly, we iteratively optimize the problem (4) on a benign dataset D. In particular, we map the values of m\u2032o and t \u2032 o to [0, 1] by a modification of tanh function in each iteration. After optimizing for several epochs, the final penalized trigger mask m\u2032 and pattern t\u2032 are obtained.\nFor the outer minimization, we adopt the penalized trigger mask and pattern generated from Algorithm 1 as m\u2032 and t\u2032. After that, we solve the following trying to reduce the trigger generalization of the watermarked DNN during its training process:\nmin \u03b8 \u2211 (x,y)\u2208D\nL (f\u03b8 (x) , y)\ufe38 \ufe37\ufe37 \ufe38 + \u03bb1 \u00b7 L (f\u03b8 ((1\u2212m)\u2297 x+m\u2297 t) , yt)\ufe38 \ufe37\ufe37 \ufe38 + Benign Loss Backdoor Loss \u03bb2 \u00b7 L\u2032 (f\u03b8 ((1\u2212m\u2032)\u2297 x+m\u2032 \u2297 t\u2032) , y\u0302t + y\u0302)\ufe38 \ufe37\ufe37 \ufe38 . P enalty Loss\n(5)\nWe solve the aforementioned problem (5) with the standard SGD (Bottou, 2010), as shown in Algorithm 2. After several epochs, the final watermarked model f\u03b8 with BWTP is obtained."
        },
        {
            "heading": "E THE OPTIMIZATION PROCESS OF GLBW",
            "text": "As we show in the experiments, the previous BWTP suffers from relatively poor performance whose watermarked DNNs still have high trigger generalization in some cases. According to the intermediate results, this failure is mostly because the synthesized triggers are \u2018weak\u2019 with either a large loss value or are similar to the original trigger. Besides, the training of watermarked DNNs is unstable even when the synthesized triggers are promising. We argue that these limitations are due to the complexity of its penalty loss. As such, we propose to directly target the generation of synthesized triggers to make this process more controllable, inspired by neural cleanse which is a backdoor defense based on trigger synthesis (Wang et al., 2019). Its optimization is as follows:\nmin \u03b8 \u2211 (x,y)\u2208D\nL (f\u03b8 (x) , y)\ufe38 \ufe37\ufe37 \ufe38 + \u03bb3 \u00b7 L (f\u03b8 ((1\u2212m)\u2297 x+m\u2297 t) , yt)\ufe38 \ufe37\ufe37 \ufe38 + Benign Loss Backdoor Loss\n\u03bb4 \u00b7 max m\u2032,t\u2032,|m\u2032\u2229m|\u2264\u03c4 \u00b7|m| \u2212 {L (f\u03b8 ((1\u2212m\u2032)\u2297 x+m\u2032 \u2297 t\u2032) , yt) + \u00b5 \u00b7 |m\u2032|}\ufe38 \ufe37\ufe37 \ufe38 Generalization Loss\n(6)\nwhere \u00b5 is a hyper-parameter trading off its size and effectiveness for synthesized trigger, \u03bb3 & \u03bb4 are two hyper-parameters for losses, \u03c4 is the overlap threshold between synthesized and original\nAlgorithm 1 Generating the penalized trigger. Input: The benign dataset D; The current model f\u03b8; Target label yt; The mask m and pattern t of the original trigger; Hyper-parameters T and \u03b3; The number of epochs for generating the penalized trigger Ngen; Output: The mask m\u2032 and pattern t\u2032 of the penalized trigger;\n1: function GET PENALIZED TRIGGER(D, f\u03b8, yt, m, t, T , \u03b3, Ngen) 2: m\u2032o \u223c N (0, 1) 3: t\u2032o \u223c N (0, 1) 4: for n = 1 to Ngen do 5: for each mini-batch (x, y) \u2208 D do 6: m\u2032 \u2190 tanh(m \u2032 o)+1\n2 \u25b7 Map the values of m \u2032 o to [0, 1]\n7: t\u2032 \u2190 tanh(t \u2032 o)+1 2 \u25b7 Map the values of t \u2032 o to [0, 1] 8: gm\u2032o \u2190 \u2207m\u2032o [\u2212L \u2032(f\u03b8((1\u2212m\u2032)\u2297 x+m\u2032 \u2297 t\u2032), y\u0302t + y\u0302) + \u03b3 \u00b7max(0, |m\u2032| \u2212 T )] 9: gt\u2032o \u2190 \u2207t\u2032o [\u2212L \u2032(f\u03b8((1\u2212m\u2032)\u2297 x+m\u2032 \u2297 t\u2032), y\u0302t + y\u0302) + \u03b3 \u00b7max(0, |m\u2032| \u2212 T )]\n10: Use Adam optimizer to update m\u2032o and t \u2032 o with gradients gm\u2032o and gt\u2032o 11: end for 12: end for 13: return m\u2032, t\u2032 14: end function\nAlgorithm 2 Watermarking DNNs with BWTP. Input: The benign dataset D; Model f\u03b8; Target label yt; The mask m and pattern t of the original trigger; Hyper-parameters \u03bb1, \u03bb2, T , \u03b3; The number of the training epochs Ntrain and the number of epochs for generating the penalized trigger Ngen; Output: BWTP-watermarked DNN f\u03b8;\n1: function BWTP(D, f\u03b8, yt, m, t, \u03bb1, \u03bb2, T , \u03b3, Ntrain, Ngen) 2: for n\u2190 1 to Ntrain do 3: m\u2032, t\u2032 \u2190 GET PENALTY TRIGGER(D, f\u03b8, yt,m, t, T, \u03b3,Ngen) 4: for each mini-batch (x, y) \u2208 D do 5: Lbenign \u2190 L(f\u03b8(x), y) 6: Lbackdoor \u2190 L(f\u03b8((1\u2212m)\u2297 x+m\u2297 t), yt) 7: Lpenalty \u2190 L\u2032(f\u03b8((1\u2212m\u2032)\u2297 x+m\u2032 \u2297 t\u2032), y\u0302t + y\u0302) 8: g\u03b8 \u2190 \u2207\u03b8 (Lbenign + \u03bb1 \u00b7 Lbackdoor + \u03bb2 \u00b7 Lpenalty) 9: Use SGD optimizer to update \u03b8 with gradient g\u03b8\n10: end for 11: end for 12: return f\u03b8 13: end function\ntriggers. Notice that the y and yt contained in GLBW are different from y\u0302 and y\u0302t used in the previous BWTP since these two methods are fundamentally different.\nTo solve the aforementioned problem (6), we alternately optimize the inner maximization and the outer minimization in each iteration. For the inner maximization, we solve the following maximization to get a promising synthesized trigger:\nmax m\u2032,t\u2032,|m\u2032\u2229m|\u2264\u03c4 \u00b7|m|\n\u2212 [L (f\u03b8 ((1\u2212m\u2032)\u2297 x+m\u2032 \u2297 t\u2032) , yt) + \u00b5 \u00b7 |m\u2032|] . (7)\nFor convenience, this problem can be written in an equivalent form:\nmin m\u2032,t\u2032,|m\u2032\u2229m|\u2264\u03c4 \u00b7|m|\n[L (f\u03b8 ((1\u2212m\u2032)\u2297 x+m\u2032 \u2297 t\u2032) , yt) + \u00b5 \u00b7 |m\u2032|] . (8)\nIn particular, we design an adaptive optimization algorithm (as shown in Algorithm 3) to solve the aforementioned problem (8): At the beginning, we initialize init flag to False and \u00b5 to \u00b50. init flag is flag used to identify whether to force m\u2032 to be different from m by the special initialization of m\u2032o. \u00b5 is an adaptive parameter to limit the size of the synthesized trigger mask m \u2032. After\nAlgorithm 3 Generating the desired synthesized trigger. Input: The benign dataset D; The current model f\u03b8; Target label yt; The mask m and trigger pattern t of the original trigger; Hyper-parameters \u00b50 and \u03c4 ; The number of epochs for generating the potential trigger Ngen; Output: The mask m\u2032 and its pattern t\u2032 of the promising synthesized trigger;\n1: function GET PROMISING TRIGGER(D, f\u03b8, yt, m, t, \u00b50, \u03c4 , Ngen) 2: init flag \u2190 False 3: \u00b5\u2190 \u00b50 4: loop 5: m\u2032o \u223c N (0, 1) 6: t\u2032o \u223c N (0, 1) 7: if init flag = True then 8: m\u2032oi,j \u2190 \u2212\u221e where mi,j = 1 9: end if\n10: for n = 1 to Ngen do 11: for each mini-batch x \u2208 D do 12: m\u2032 \u2190 tanh(m \u2032 o)+1\n2 \u25b7 Map the values of m \u2032 o to [0, 1]\n13: t\u2032 \u2190 tanh(t \u2032 o)+1 2 \u25b7 Map the values of t \u2032 o to [0, 1]"
        },
        {
            "heading": "14: gm\u2032o \u2190 \u2207m\u2032o [L(f\u03b8((1\u2212m",
            "text": "\u2032)\u2297 x+m\u2032 \u2297 t\u2032), yt) + \u00b5 \u00b7 |m\u2032|] 15: gt\u2032o \u2190 \u2207t\u2032o [L(f\u03b8((1\u2212m \u2032)\u2297 x+m\u2032 \u2297 t\u2032), yt) + \u00b5 \u00b7 |m\u2032|] 16: Use Adam optimizer to update m\u2032o and t \u2032 o with gradients gm\u2032o and gt\u2032o 17: end for 18: end for 19: lossm \u2190 1|D| \u2211 x\u2208D L(f\u03b8((1\u2212m)\u2297 x+m\u2297 t), yt)\n20: lossm\u2032 \u2190 1|D| \u2211\nx\u2208D L(f\u03b8((1\u2212m\u2032)\u2297 x+m\u2032 \u2297 t\u2032), yt) 21: if |m\u2032| > |m| \u00b7 2 then 22: \u00b5\u2190 \u00b5 \u00b7 |m\n\u2032| |m| \u25b7 Adjust \u00b5 adaptively\n23: continue 24: else if |m\u2032| < |m| \u00b7 0.6 and lossm\u2032 > lossm then 25: \u00b5\u2190 \u00b5 \u00b7 0.618 \u25b7 Adjust \u00b5 adaptively 26: continue 27: else if lossm\u2032 > 1.8 \u00b7 lossm or lossm\u2032 + \u00b5 \u00b7 |m\u2032| > 1.5 \u00b7 (lossm + \u00b5 \u00b7 |m|) then 28: continue \u25b7 We get a bad trigger candidate and run the algorithm again to find a\nbetter one. 29: else if |m\u2032 \u2229m| > \u03c4 \u00b7 |m| then 30: init flag \u2190 True 31: continue \u25b7 We get a trigger candidate that is too similar to the original trigger m\nand run the algorithm again to find a different one. 32: end if 33: return m\u2032, t\u2032 34: end loop 35: end function\ninitializing init flag and \u00b50, the program enters a loop to generate potential synthesized triggers adaptively. In the loop, the first step is randomly initializing m\u2032o and t \u2032 o by sampling from the standard normal distribution. If init flag is True, we set m\u2032oi,j to \u2212\u221e where mi,j = 1. This is to force m\u2032 to be different from m (that is m\u2032 \u2229m = 0). After that, we iteratively optimize the the problem (8) on the benign dataset D. After optimizing for several epochs, we get a trigger candidate mask m\u2032 and its corresponding pattern t\u2032. Then, we calculate the loss of the original trigger mask m with target label yt (dubbed as lossm) and the loss of the candidate trigger mask m\u2032 with target label yt (dubbed as lossm\u2032 ). After that, we will check the quality of the trigger candidate. The first check is whether the size of candidate trigger mask m\u2032 is too big. If |m\u2032| > 2 \u00b7 |m|, we increase \u00b5 by multiplying |m\n\u2032| |m| to limit the size of m \u2032 and then continue the loop to generate another trigger candidate; The second check is whether the size of candidate trigger mask m\u2032 is too\nAlgorithm 4 Watermarking DNNs with GLBW. Input: The benign dataset D; The model f\u03b8; Target label yt; The mask m and pattern t of the original trigger; Hyper-parameters \u03bb3, \u03bb4, \u00b50, \u03c4 ; The number of the training epochs Ntrain and the number of epochs for generating the penalized trigger Ngen; Output: A model with generalization-limited backdoor watermark f\u03b8;\n1: function GLBW(D, f\u03b8, yt, m, t, \u03bb3, \u03bb4, \u00b50, \u03c4 , Ntrain, Ngen) 2: for n\u2190 1 to Ntrain do 3: m\u2032, t\u2032 \u2190 GET PROMISING TRIGGER(D, f\u03b8, yt,m, t, \u00b50, \u03c4,Ngen) 4: for each mini-batch (x, y) \u2208 D do 5: Lbenign \u2190 L(f\u03b8(x), y) 6: Lbackdoor \u2190 L(f\u03b8((1\u2212m)\u2297 x+m\u2297 t), yt) 7: Lgeneralization \u2190 L(f\u03b8((1\u2212m\u2032)\u2297 x+m\u2032 \u2297 t\u2032), y) 8: g\u03b8 \u2190 \u2207\u03b8 (Lbenign + \u03bb3 \u00b7 Lbackdoor + \u03bb4 \u00b7 Lgeneralization) 9: Use SGD optimizer to update \u03b8 with gradient g\u03b8\n10: end for 11: end for 12: return f\u03b8 13: end function\nsmall. If |m\u2032| < 0.6 \u00b7 |m| and lossm\u2032 > lossm, that is the trigger mask is too small and with a large loss, we decrease \u00b5 by multiplying 0.618 to relax the size of m\u2032 and then continue the loop to generate another trigger candidate; The third check is whether the candidate trigger is effective. If lossm\u2032 > 1.8 \u00b7 lossm or lossm\u2032 +\u00b5 \u00b7 |m\u2032| > 1.5 \u00b7 (lossm+\u00b5 \u00b7 |m|), we think the trigger candidate is too bad and cannot effectively activate the backdoor, then continue the loop to generate another better trigger candidate; The fourth check is whether the candidate trigger is similar to the original trigger. If |m\u2032\u2229m| > \u03c4 \u00b7 |m|, we get a trigger candidate that is too similar to the original trigger m, we set init flag to True and run the algorithm again to find a different one. If the trigger candidate passes all four checks, it is a promising synthesized trigger and we mark its mask m\u2032 and pattern t\u2032.\nFor the outer minimization, we use the promising synthesized trigger mask and pattern generated from Algorithm 3 as m\u2032 and t\u2032. After that, we solve the following problem to train a generalizationlimited watermarked model:\nmin \u03b8 \u2211 (x,y)\u2208D\nL (f\u03b8 (x) , y)\ufe38 \ufe37\ufe37 \ufe38 + \u03bb3 \u00b7 L (f\u03b8 ((1\u2212m)\u2297 x+m\u2297 t) , yt)\ufe38 \ufe37\ufe37 \ufe38 + Benign Loss Backdoor Loss \u03bb4 \u00b7 L (f\u03b8 ((1\u2212m\u2032)\u2297 x+m\u2032 \u2297 t\u2032) , yt)\ufe38 \ufe37\ufe37 \ufe38, Generalization Loss\n(9)\nWe solve the aforementioned problem (9) with the standard SGD (Bottou, 2010), as shown in Algorithm 4. After several epochs, the final GLBW-watermarked model f\u03b8 is obtained."
        },
        {
            "heading": "F DETAILED SETTINGS FOR MAIN EXPERIMENTS",
            "text": "General Settings for Backdoor Watermarks. We adopt the trigger (a) used in Section A as the original trigger pattern for all model watermarks on both CIFAR-10 and GTSRB. The watermarking rate and the target label are also the same as those used in Section A.\nSettings for Vanilla Backdoor Watermark. The settings are the same as those used in Section A.\nSettings for BWTP. We set the the threshold of the size of potential trigger mask T as 18, the number of epochs for generating each potential trigger Ngen as 10, \u03bb1 = \u03bb2 = \u03b3 = 1, and the distance measurement d(m,m\u2032) = |m \u2212m\u2032|. In particular, we use a modified version of the Sigmoid function as the probability activation function p(\u00b7) (as shown in Figure 11). When d(m,m\u2032) is very large, the probability p(d(m,m\u2032)) \u2248 1 and the penalty label y\u0302t + y\u0302 \u2248 y; When d(m,m\u2032) is very small, the probability activation p(d(m,m\u2032)) \u2248 0 and the penalty label y\u0302t + y\u0302 \u2248 yt.\nSettings for GLBW. We set the hyper-parameters \u03bb3 = \u03bb4 = 1, \u00b50 = 0.001, \u03c4 = 0.05 and the number of epochs for generating each potential trigger Ngen as 15.\nGeneral Settings for Trigger Synthesis. We adopt three trigger synthesis methods, including neural cleanse (Wang et al., 2019), TABOR (Guo et al., 2020), and pixel backdoor (Tao et al., 2022) to generate potential triggers of watermarked DNNs. We generate 1,000 trigger candidates (with random initializations) in all cases. We only select those whose loss values are less than 1.5 times of that of the original trigger as our potential triggers.\nEvaluation Metrics. Following the most classical settings, we adopt benign accuracy (BA) and watermark success rate (WSR) to evaluate the effectiveness of model watermarks. To evaluate their trigger generalization, we exploit chamfer distance (dubbed \u2018Chamfer\u2019) and the percentage of effective potential triggers with low generalization (dubbed \u2018PLG\u2019). The effective triggers whose IOU values are greater than the threshold (0.3 for CIFAR-10 and 0.18 for GTSRB) are regarded as with low generalization. In general, the smaller the chamfer distance and the larger the PLG, the lower the position generalization of a watermark.\nThe Visualization of Different Types of Trigger Candidates and The Definition of PLG. We visualize different trigger candidates for better understanding (as shown in Figure 12). Triggers in the first line are effective candidates with a high IOU value (i.e., effective triggers with low generalization). These triggers have high watermark success rate (WSR) and low backdoor loss that can activate the backdoor of watermarked DNNs successfully; Triggers in the second line are effective candidates with a low IOU value (i.e., effective triggers with high generalization). These potential triggers have high WSR and low backdoor loss that can activate the backdoor of watermarked DNNs successfully, whereas they are different from the original trigger pattern; Triggers in the third line are ineffective trigger candidates with low WSR and high backdoor loss that can not activate model backdoors. Accordingly, they should not be regarded as potential triggers but targeted universal adversarial perturbations. We do not include them in the evaluation of trigger generalization. As such, the caculation of PLG can be written as follows:\nPLG = the number of the effective triggers with high IOU\nthe number of all effective triggers .\nSettings for Neurl Cleanse. We set the number of training epochs as 10 for the vanilla watermark and 3 for others. On the CIFAR-10 dataset, we set the weight of the regularization term \u03bb = 8\u00d710\u22125 for the vanilla watermark, \u03bb = 6 \u00d7 10\u22122 for BWTP, and \u03bb = 9.315 \u00d7 10\u22122 for GLBW. On the GTSRB dataset, we set \u03bb = 8 \u00d7 10\u22125 for the vanilla watermark, \u03bb = 9 \u00d7 10\u22122 for BWTP, and \u03bb = 9\u00d7 10\u22122 for GLBW. We adopt different settings to achieve the best synthesis performance. Settings for TABOR. We set the number of training epochs as 2. On the CIFAR-10 dataset, we set the weights of the TABOR regularization terms \u03bb1 = 8 \u00d7 10\u22124, \u03bb2 = 10\u22124, \u03bb = 10\u22125 for the vanilla watermark, \u03bb1 = 5\u00d710\u22122, \u03bb2 = 5\u00d710\u22123, \u03bb = 10\u22125 for BWTP, and \u03bb1 = 7.5\u00d710\u22122, \u03bb2 = 7.5\u00d7 10\u22123, \u03bb = 10\u22125 for GLBW. On the GTSRB dataset, we set \u03bb1 = 8\u00d7 10\u22124, \u03bb2 = 10\u22124, \u03bb = 10\u22125 for the vanilla watermark, \u03bb1 = 8 \u00d7 10\u22122, \u03bb2 = 8 \u00d7 10\u22123, \u03bb = 10\u22125 for BWTP, and \u03bb1 = 7.2 \u00d7 10\u22122, \u03bb2 = 7.2 \u00d7 10\u22123, \u03bb = 10\u22125 for GLBW. We adopt different settings in different cases to achieve the best synthesis performance.\nSettings for Pixel Backdoor. We set the number of training epochs as 10. On the CIFAR-10 dataset, we set the weight of the regularization term \u03b1 = 5 \u00d7 10\u22124 for the vanilla watermark, \u03b1 = 0.5 for BWTP, and \u03b1 = 2 for GLBW. On the GTSRB dataset, we set \u03b1 = 5 \u00d7 10\u22124 for the vanilla watermark, \u03b1 = 0.7 for BWTP, and \u03b1 = 1 for GLBW. We adopt different settings in different cases to achieve the best synthesis performance towards all watermarked models."
        },
        {
            "heading": "G ADDITIONAL RESULTS OF MAIN EXPERIMENTS",
            "text": "In this section, we provide additional results that are omitted in the main manuscript.\nG.1 THE GENERALIZATION OF MODEL WATERMARKS\nIn the main manuscript, we adopt chamfer distance and PLG to measure the trigger generalization. In this section, we provide the results under \u21131-norm. Besides, we provide the number of all synthesized triggers (dubbed \u2018# All\u2019) and potential triggers (dubbed \u2018# P\u2019).\nAs shown in Table 10, our GLBW can significantly reduce the trigger generalization compared to the vanilla backdoor watermark in all cases whereas BWTP may even increase it in some cases.\nG.2 THE IOU OF SRV METHODS WITH OUR GLBW\nThe IOU of SRV methods calculated based on our GLBW is shown in Table 11. The results show that our GLBW can lead to more consistent and stable evaluation and therefore is more faithful.\nG.3 A CLOSER LOOK TO THE FAILURE OF BWTP\nIn this section, we explore why our BWTP method may lead to poor performance on CIFAR-10 by visualizing the training process of BWTP. Specifically, we present the backdoor loss of each synthesized trigger with respect to the training epoch on the CIFAR-10 dataset.\nAs shown in Figure 13, the loss values of synthesized triggers are significantly larger than that of the original trigger. In other words, most of them are ineffective and cannot activate the backdoor in the watermarked model. Even among the few valid synthesized triggers, there is still a trigger pattern that is similar to the original trigger (as shown in Figure 14). Accordingly, penalizing these synthesized triggers has a minor benefit to reduce trigger generalization.\nTable 12: Effects of the hyper-parameter \u03bb3.\n\u03bb3 BA (%) WSR (%) Chamfer PLG (%) 0.6 78.35 93.15 23.13 100 0.8 81.49 92.45 17.38 100 1.0 78.33 93.68 23.04 100 1.2 82.63 93.65 24.28 100 1.4 79.85 89.88 32.99 100\nTable 13: Effects of the hyper-parameter \u03bb4.\n\u03bb4 BA (%) WSR (%) Chamfer PLG (%) 0.6 74.57 92.56 24.68 100 0.8 81.16 87.71 26.37 100 1.0 78.33 93.68 23.04 100 1.2 75.90 88.05 24.03 100 1.4 80.74 87.30 24.02 100"
        },
        {
            "heading": "H ADDITIONAL RESULTS OF THE EFFECTS OF KEY HYPER-PARAMETERS",
            "text": "In this section, similar to the main manuscript, we adopt the CIFAR-10 dataset as an example to discuss the effects of other key hyper-parameters involved in our GLBW.\nH.1 EFFECTS OF THE HYPER-PARAMETER \u03bb3\nAs shown in Table 12, the hyper-parameter \u03bb3 has a mild influence on our GLBW, especially on its trigger generalization. Specifically, our method can reach a 100% PLG under all settings. In other words, users can achieve faithful SRV evaluation based on our GLBW without fine-tuning \u03bb3.\nH.2 EFFECTS OF THE HYPER-PARAMETER \u03bb4\nAs shown in Table 13, the hyper-parameter \u03bb4 also has a mild influence on our GLBW, although it has higher impacts on WSR. This is mostly because \u03bb4 relates to the \u2018suppression\u2019 of potential triggers that may have adverse effects on the WSR. Nevertheless, our method can reach a 100% PLG under all settings. Accordingly, users can achieve faithful SRV evaluation based on our GLBW without fine-tuning this hyper-parameter.\nH.3 EFFECTS OF THE HYPER-PARAMETER \u00b50\nAs shown in Table 14, \u00b50 also has mild effects to trigger the generalization of our GLBW. It is mostly because of the adaptive optimization process of generating synthesized triggers. Users can still achieve faithful SRV evaluation based on our GLBW without fine-tuning this hyper-parameter."
        },
        {
            "heading": "I A CLOSER LOOK TO THE EFFECTIVENESS OF OUR GLBW",
            "text": "In this section, we further explore why our GLBW can reduce trigger generalization by visualizing all synthesized triggers via principal component analysis (PCA).\nSettings. We use the vanilla, BWTP and GLBW models trained in Section 5.2 for experiments. Similar to (Qiao et al., 2019), for each model, we sample 1,000 triggers produced by neural cleanse and embed these triggers into a 2-D space via PCA to visualize trigger distribution. In particular, we train PCA on the triggers generated from the vanilla model, and then apply it to the triggers generated from all the models. That is to say, all the dimension reduction processes are under the same linear transformation.\nResults. As shown in Figure 15, the effective triggers generated from the model watermarked by our GLBW are clustered together in all cases. In other words, they are from a similar distribution.Accordingly, our GLBW has a low trigger generalization. In contrast, the effective triggers generated from the model watermarked by vanilla backdoor attack scatter in the whole space, indicating that they are from different latent distributions. As such, the vanilla backdoor watermark\nhas a high trigger generalization. The effective triggers of BWTP have similar behavior to that of GLBW on GTSRB while having a similar behavior to the vanilla backdoor watermark on CIFAR10. These phenomena are consistent with the trigger generalization of BWTP (low generalization on the GTSRB dataset and high generalization on the CIFAR-10 dataset)."
        },
        {
            "heading": "J WHY WOULD OUR METHODS DECREASE BENIGN ACCURACY?",
            "text": "In our methods, we treat the universal adversarial perturbations (UAPs) and potential triggers the same, whereas we have to admit that they may be different. Accordingly, our methods (especially GLBW) are similar to adversarial training against UAPs in terms of effectiveness. As such, our GLBW has a similar effect as conducting adversarial training on UAPs that will (significantly) decreases benign accuracy especially when the task is relatively complicated.\nHowever, it does not necessarily means that our method is not practical even we treat them the same.\nFirstly, having a relatively low benign accuracy will not reduce the reliability and practicality of our method. Firstly, our evaluation is based on poisoned samples instead of clean samples. As such, we only need to ensure that our methods lead to a high watermark success rate (instead of a high benign accuracy) for faithful results. The watermark success rates are higher than 90% in all cases, which is sufficiently high; Secondly, as we mentioned in our experiments, the watermarked DNNs are only used for evaluating SRV methods instead of for deployment. Accordingly, the decrease in clean accuracy led by our method will not hinder its usefulness.\nSecondly, our method can still reduce trigger generalization since it minimizes both trigger generalization and the risk of UAPs simultaneously, even though we treat them the same.\nThirdly, minimizing the risk of UAPs may have potential benefits in reducing trigger generalization. (Ilyas et al., 2019) revealed that adversarially robust models focus more on \u2018robust features\u2019 instead of non-robust ones (e.g., textures). Accordingly, our method may makes DNNs rely more on the original trigger pattern for poisoned samples and therefore could reduce trigger generalization.\nLastly, UAPs can be regarded as the trigger of the \u2018natural backdoor\u2019 of models learned from samples (Wenger et al., 2022). They have very similar properties to backdoor triggers and therefore it is very difficult (or probably even impossible) to distinguish them from potential triggers."
        },
        {
            "heading": "K ADDITIONAL EXPERIMENTS OF HUMAN INSPECTION",
            "text": "To further verify that our GLBW-based XAI evaluation is faithful, we generate 10 groups of saliency maps based on 10 randomly selected images on the GTSRB dataset. We conduct human inspection experiments by inviting 10 people and asking them to grade all groups of saliency maps independently. As shown in Table 15, the rankings generated by our GLBW method are similar to those made by people. These results verify that an SRV method with a higher order in the ranking list generated by our method is truly better than those with lower orders."
        },
        {
            "heading": "L ADDITIONAL EXPERIMENTS OF STANDARDIZED XAI EVALUATION WITH DIFFERENT TRIGGERS",
            "text": "L.1 RESULTS WITH DIFFERENT TRIGGER SHAPES\nTo explore whether the trigger shape has significant effects to our standardized XAI evaluation, we evaluate the SRV methods with different trigger shapes, including square, pencil, and triangle (as shown in Figure 16), on CIFAR-10 and GTSRB datasets. In this experiment, all evaluated trigger\npatterns have 9 pixels. As shown in Table 16-17, the trigger shape has mild effects to the final ranking, although it may have some influences to IOU values.\nL.2 RESULTS WITH DIFFERENT TRIGGER SIZES\nTo explore whether the trigger size has significant effects to our standardized XAI evaluation, we evaluate SRV methods based on square-type triggers with different sizes (i.e., 3\u00d73, 4\u00d74 and 5\u00d75, as shown in Figure 16), on CIFAR-10 and GTSRB datasets. As shown in Table 18-19, the trigger shape has minor effects to the final ranking, although it may have some influences to IOU values."
        },
        {
            "heading": "M CONNECTIONS AND DIFFERENCES WITH RELATED WORKS",
            "text": "In this section, we discuss the connections and differences between our methods (i.e., BWTP and GLBW) and backdoor attacks, model watermarks, adversarial learning, and neural cleanse.\nM.1 CONNECTIONS AND DIFFERENCES WITH BACKDOOR ATTACKS\nIn general, both our methods and backdoor attacks intend to implant special model behaviors (i.e., backdoor) to DNNs. However, they still have many intrinsic differences.\nFirstly, backdoor attacks aim at maliciously manipulating attacked models during the inference process. In contrast, our watermarks are designed for the evaluation of saliency-based representation visualization (SRV), which is a positive application.\nSecondly, there are different parties (i.e., adversaries, and victim model users) involved in backdoor attacks while only the adjudicator is included in our SRV evaluation methods. Accordingly, backdoor attacks need to ensure that the attacked model has high benign accuracy or its poisoned samples are similar to their benign version to circumvent the detection of victim model users. However, in our methods, we only need to ensure a high watermark/attack success rate (instead of the high benign accuracy) of watermarked DNNs since we don\u2019t need to fulfill watermark stealthiness.\nLastly, backdoor attacks only need to ensure that the original trigger pattern used for training can activate hidden backdoors in attacked DNNs. In other words, the backdoor adversaries don\u2019t need to measure or control the degree of trigger generalization. However, as we mentioned in the main manuscript, our methods need to minimize the generalization to design faithful SRV evaluation.\nM.2 CONNECTIONS AND DIFFERENCES WITH MODEL WATERMARKS\nCurrently, there are different types of model watermarks. Among all watermarking techniques, backdoor-based methods are probably the most classical and widespread ones. In general, our methods share some similarities to backdoor-based model watermarks. For example, both of them assume that the defenders can fully control the training process of models and intend to implant backdoors to watermarked DNNs. However, they still have some essential differences.\nFirstly, similar to backdoor attacks, there are different parties (i.e., the model owner and users/adversaries) involved in model watermarks. Accordingly, the model owner still requires to preserve the benign accuracy of watermarked DNNs otherwise users will not exploit them. In contrast, it is not necessary to have a high benign accuracy in our methods.\nSecondly, model watermarks need to ensure robustness under different watermark-removal attacks (e.g., fine-tuning, and model pruning) since adversaries have the incentive to remove them. However, in our methods, we only require to have a low trigger generalization of backdoor watermarks.\nM.3 CONNECTIONS AND DIFFERENCES WITH ADVERSARIAL LEARNING\nBoth (targeted universal) adversarial attacks and our methods (during the training process) intend to find a pattern that can mislead model predictions to a specific class. Instead of simply having a high attack effectiveness as required by adversarial attacks, our methods also need to ensure that the generated perturbation is (significantly) different from the original trigger pattern. This requirement is necessary for minimizing trigger generalization of backdoor watermarks.\nBoth our methods and adversarial training formulate model training as a min-max problem. However, adversarial training aims to make models robust to all potential perturbations. In contrast, our watermarks intend to preserve the backdoor regarding the original trigger while reducing other backdoors instead of eliminating all backdoors.\nM.4 CONNECTIONS AND DIFFERENCES WITH NEURAL CLEANSE\nIn general, both our methods and neural cleanse (Wang et al., 2019) intend to generate targeted universal adversarial perturbations as potential trigger patterns. However, our methods have different constraints (e.g., |m\u2032 \u2229m| \u2264 \u03c4 \u00b7 |m|) since we have fundamentally different purposes. Firstly, neural cleanse has no information (e.g., trigger size and target label) about the ground-truth trigger pattern. Accordingly, it needs to generate potential trigger towards each class and adopt trigger sparsity to decide the final one. However, its assumption that all trigger patterns should be sparse is not hold for advanced backdoor attacks. As such, this method is not effective (Li et al., 2021b). In contrast, we know the ground-truth trigger patch, its target label, and its sparsity. Our\npurpose is to find potential triggers with the highest attack effectiveness and differences from the ground-truth one instead of finding the ground-truth trigger.\nSecondly, neural cleanse exploited the standard SGD to solve the optimization problem. In contrast, in our GLBW, we design an adaptive optimization method to find desired potential trigger patterns since our task is significantly more difficult than that of the neural cleanse. Please refer to Appendix E for more details about our optimization process."
        },
        {
            "heading": "N ADDITIONAL EXPERIMENTS OF STANDARDIZED EVALUATION METHOD WITH ADAPTIVE BINARIZATION",
            "text": "Regarding the selection of M pixels proposed in our standardized backdoor-based XAI evaluation, people may argue that it is not significantly better than the previous method. In this section, we discuss whether it would be better if we use adaptive binarization methods to obtain regions with high influences to calculate the rank of each SRV method.\nSettings. We hereby exploit a classical adaptive binarization method, i.e., OTSU (Otsu, 1979), to obtain regions with high influences to calculate the rank of each SRV method and compare the results of our standardized method (dubbed \u2019Ours\u2019) and those of the standardized method with adaptive binarization (dubbed \u2019Adaptive\u2019). All other settings are the same as those used in Section 3.1.\nResults. As shown in Table 20, using adaptive binarization method has a mild influence to the final average ranking. These results verify the effectiveness of our method."
        },
        {
            "heading": "O REPRODUCIBILITY STATEMENT",
            "text": "In the appendix, we provide detailed descriptions of the datasets, models, training and evaluation settings, and computational facilities. The codes and model checkpoints for reproducing the main experiments of our evaluation are also provided in the supplementary material. Our codes are available at https://github.com/yamengxi/GLBW."
        },
        {
            "heading": "P DISCUSSIONS ABOUT ADOPTED DATA",
            "text": "In this paper, all adopted samples are from the open-sourced datasets (i.e., CIFAR-10 and GTSRB). These datasets contain no human object. We admit that we modified a few samples for watermarking. However, our research treats all samples the same and all triggers contain no offensive content. Accordingly, our work fulfills the requirements of these datasets and has no privacy violation."
        }
    ],
    "year": 2024
}