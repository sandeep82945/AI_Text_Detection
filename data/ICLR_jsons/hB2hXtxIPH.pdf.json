{
    "abstractText": "Cooperative multi-agent reinforcement learning (MARL) is extensively used for solving complex cooperative tasks, and value decomposition methods are a prevalent approach for this domain. However, these methods have not been successful in addressing both homogeneous and heterogeneous tasks simultaneously which is a crucial aspect for the practical application of cooperative agents. On one hand, value decomposition methods demonstrate superior performance in homogeneous tasks. Nevertheless, they tend to produce agents with similar policies, which is unsuitable for heterogeneous tasks. On the other hand, solutions based on personalized observation or assigned roles are well-suited for heterogeneous tasks. However, they often lead to a trade-off situation where the agent\u2019s performance in homogeneous scenarios is negatively affected due to the aggregation of distinct policies. An alternative approach is to adopt sequential execution policies, which offer a flexible form for learning both types of tasks. However, learning sequential execution policies poses challenges in terms of credit assignment, and the limited information about subsequently executed agents can lead to sub-optimal solutions, which is known as the relative over-generalization problem. To tackle these issues, this paper proposes Greedy Sequential Execution (GSE) as a solution to learn the optimal policy that covers both scenarios. In the proposed GSE framework, we introduce an individual utility function into the framework of value decomposition to consider the complex interactions between agents. This function is capable of representing both the homogeneous and heterogeneous optimal policies. Furthermore, we utilize greedy marginal contribution calculated by the utility function as the credit value of the sequential execution policy to address the credit assignment and relative over-generalization problem. We evaluated GSE in both homogeneous and heterogeneous scenarios. The results demonstrate that GSE achieves significant improvement in performance across multiple domains, especially in scenarios involving both homogeneous and heterogeneous tasks.",
    "authors": [
        {
            "affiliations": [],
            "name": "Shanqi Liu"
        },
        {
            "affiliations": [],
            "name": "Dong Xing"
        },
        {
            "affiliations": [],
            "name": "Pengjie Gu"
        },
        {
            "affiliations": [],
            "name": "Xinrun Wang"
        },
        {
            "affiliations": [],
            "name": "Bo An"
        },
        {
            "affiliations": [],
            "name": "Yong Liu"
        }
    ],
    "id": "SP:a196531af2aa21edfc2f560c92cdd3a31d0cae00",
    "references": [
        {
            "authors": [
                "Lucian Busoniu",
                "Robert Babuska",
                "Bart De Schutter"
            ],
            "title": "A comprehensive survey of multiagent reinforcement learning",
            "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews),",
            "year": 2008
        },
        {
            "authors": [
                "Fabrizio Caccavale",
                "Pasquale Chiacchio",
                "Alessandro Marino",
                "Luigi Villani"
            ],
            "title": "Six-dof impedance control of dual-arm cooperative manipulators",
            "venue": "IEEE/ASME Transactions On Mechatronics,",
            "year": 2008
        },
        {
            "authors": [
                "Dong Chen",
                "Mohammad Hajidavalloo",
                "Zhaojian Li",
                "Kaian Chen",
                "Yongqiang Wang",
                "Longsheng Jiang",
                "Yue Wang"
            ],
            "title": "Deep multi-agent reinforcement learning for highway on-ramp merging in mixed traffic",
            "venue": "arXiv preprint arXiv:2105.05701,",
            "year": 2021
        },
        {
            "authors": [
                "Filippos Christianos",
                "Georgios Papoudakis",
                "Muhammad A Rahman",
                "Stefano V Albrecht"
            ],
            "title": "Scaling multi-agent reinforcement learning with selective parameter sharing",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Jakob Foerster",
                "Gregory Farquhar",
                "Triantafyllos Afouras",
                "Nantas Nardelli",
                "Shimon Whiteson"
            ],
            "title": "Counterfactual multi-agent policy gradients",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2018
        },
        {
            "authors": [
                "Wei Fu",
                "Chao Yu",
                "Zelai Xu",
                "Jiaqi Yang",
                "Yi Wu"
            ],
            "title": "Revisiting some common practices in cooperative multi-agent reinforcement learning",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Jayesh K Gupta",
                "Maxim Egorov",
                "Mykel Kochenderfer"
            ],
            "title": "Cooperative multi-agent control using deep reinforcement learning",
            "venue": "In Autonomous Agents and Multiagent Systems: AAMAS",
            "year": 2017
        },
        {
            "authors": [
                "Tarun Gupta",
                "Anuj Mahajan",
                "Bei Peng",
                "Wendelin B\u00f6hmer",
                "Shimon Whiteson"
            ],
            "title": "Uneven: Universal value exploration for multi-agent reinforcement learning",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Kyunghwan Son",
                "Daewoo Kim",
                "Yung Yi Qtran"
            ],
            "title": "Learning to factorize with transformation for cooperative multi-agent reinforcement learning",
            "venue": "In Proceedings of the 31st International Conference on Machine Learning, Proceedings of Machine Learning Research. PMLR,",
            "year": 2019
        },
        {
            "authors": [
                "Shariq Iqbal",
                "Fei Sha"
            ],
            "title": "Actor-attention-critic for multi-agent reinforcement learning",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Paul Knott",
                "Micah Carroll",
                "Sam Devlin",
                "Kamil Ciosek",
                "Katja Hofmann",
                "Anca D Dragan",
                "Rohin Shah"
            ],
            "title": "Evaluating the robustness of collaborative agents",
            "venue": "arXiv preprint arXiv:2101.05507,",
            "year": 2021
        },
        {
            "authors": [
                "Jakub Grudzien Kuba",
                "Ruiqing Chen",
                "Muning Wen",
                "Ying Wen",
                "Fanglei Sun",
                "Jun Wang",
                "Yaodong Yang"
            ],
            "title": "Trust region policy optimisation in multi-agent reinforcement learning",
            "venue": "arXiv preprint arXiv:2109.11251,",
            "year": 2021
        },
        {
            "authors": [
                "Jinoh Lee",
                "Pyung Hun Chang",
                "Rodrigo S Jamisola"
            ],
            "title": "Relative impedance control for dual-arm robots performing asymmetric bimanual tasks",
            "venue": "IEEE transactions on industrial electronics,",
            "year": 2013
        },
        {
            "authors": [
                "Chenghao Li",
                "Tonghan Wang",
                "Chengjie Wu",
                "Qianchuan Zhao",
                "Jun Yang",
                "Chongjie Zhang"
            ],
            "title": "Celebrating diversity in shared multi-agent reinforcement learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Jiahui Li",
                "Kun Kuang",
                "Baoxiang Wang",
                "Furui Liu",
                "Long Chen",
                "Fei Wu",
                "Jun Xiao"
            ],
            "title": "Shapley counterfactual credits for multi-agent reinforcement learning",
            "venue": "In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining,",
            "year": 2021
        },
        {
            "authors": [
                "Shanqi Liu",
                "Licheng Wen",
                "Jinhao Cui",
                "Xuemeng Yang",
                "Junjie Cao",
                "Yong Liu"
            ],
            "title": "Moving forward in formation: A decentralized hierarchical learning approach to multi-agent moving together",
            "venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),",
            "year": 2021
        },
        {
            "authors": [
                "Ryan Lowe",
                "Yi Wu",
                "Aviv Tamar",
                "Jean Harb",
                "Pieter Abbeel",
                "Igor Mordatch"
            ],
            "title": "Multi-agent actorcritic for mixed cooperative-competitive environments",
            "venue": "arXiv preprint arXiv:1706.02275,",
            "year": 2017
        },
        {
            "authors": [
                "Anuj Mahajan",
                "Tabish Rashid",
                "Mikayel Samvelyan",
                "Shimon Whiteson"
            ],
            "title": "Maven: Multi-agent variational exploration",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Liviu Panait",
                "Sean Luke",
                "R Paul Wiegand"
            ],
            "title": "Biasing coevolutionary search for optimal multiagent behaviors",
            "venue": "IEEE Transactions on Evolutionary Computation,",
            "year": 2006
        },
        {
            "authors": [
                "Peng Peng",
                "Ying Wen",
                "Yaodong Yang",
                "Quan Yuan",
                "Zhenkun Tang",
                "Haitao Long",
                "Jun Wang"
            ],
            "title": "Multiagent bidirectionally-coordinated nets: Emergence of human-level coordination in learning to play starcraft combat games",
            "venue": "arXiv preprint arXiv:1703.10069,",
            "year": 2017
        },
        {
            "authors": [
                "Tabish Rashid",
                "Mikayel Samvelyan",
                "Christian Schroeder",
                "Gregory Farquhar",
                "Jakob Foerster",
                "Shimon Whiteson"
            ],
            "title": "Qmix: Monotonic value function factorisation for deep multi-agent reinforcement learning",
            "venue": "In International Conference on Machine Learning,",
            "year": 2018
        },
        {
            "authors": [
                "Tabish Rashid",
                "Gregory Farquhar",
                "Bei Peng",
                "Shimon Whiteson"
            ],
            "title": "Weighted qmix: Expanding monotonic value function factorisation for deep multi-agent reinforcement learning",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Bidipta Sarkar",
                "Aditi Talati",
                "Andy Shih",
                "Dorsa Sadigh"
            ],
            "title": "Pantheonrl: A marl library for dynamic training interactions",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "John Schulman",
                "Filip Wolski",
                "Prafulla Dhariwal",
                "Alec Radford",
                "Oleg Klimov"
            ],
            "title": "Proximal policy optimization algorithms",
            "venue": "arXiv preprint arXiv:1707.06347,",
            "year": 2017
        },
        {
            "authors": [
                "Kyunghwan Son",
                "Daewoo Kim",
                "Wan Ju Kang",
                "David Earl Hostallero",
                "Yung Yi"
            ],
            "title": "Qtran: Learning to factorize with transformation for cooperative multi-agent reinforcement learning",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Peter Sunehag",
                "Guy Lever",
                "Audrunas Gruslys",
                "Wojciech Marian Czarnecki",
                "Vinicius Zambaldi",
                "Max Jaderberg",
                "Marc Lanctot",
                "Nicolas Sonnerat",
                "Joel Z Leibo",
                "Karl Tuyls"
            ],
            "title": "Value-decomposition networks for cooperative multi-agent learning",
            "venue": "arXiv preprint arXiv:1706.05296,",
            "year": 2017
        },
        {
            "authors": [
                "Justin K Terry",
                "Nathaniel Grammel",
                "Ananth Hari",
                "Luis Santos",
                "Benjamin Black"
            ],
            "title": "Revisiting parameter sharing in multi-agent deep reinforcement learning",
            "venue": "arXiv preprint arXiv:2005.13625,",
            "year": 2020
        },
        {
            "authors": [
                "Jianhao Wang",
                "Zhizhou Ren",
                "Terry Liu",
                "Yang Yu",
                "Chongjie Zhang"
            ],
            "title": "Qplex: Duplex dueling multi-agent q-learning",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Jianhong Wang",
                "Yuan Zhang",
                "Tae-Kyun Kim",
                "Yunjie Gu"
            ],
            "title": "Shapley q-value: a local reward approach to solve global reward games",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Tonghan Wang",
                "Heng Dong",
                "Victor Lesser",
                "Chongjie Zhang"
            ],
            "title": "Roma: Multi-agent reinforcement learning with emergent roles",
            "venue": "In Proceedings of the 37th International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Muning Wen",
                "Jakub Kuba",
                "Runji Lin",
                "Weinan Zhang",
                "Ying Wen",
                "Jun Wang",
                "Yaodong Yang"
            ],
            "title": "Multiagent reinforcement learning is a sequence modeling problem",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Erfu Yang",
                "Dongbing Gu"
            ],
            "title": "Multiagent reinforcement learning for multi-robot systems: A survey",
            "venue": "Technical report, tech. rep,",
            "year": 2004
        },
        {
            "authors": [
                "Chao Yu",
                "Akash Velu",
                "Eugene Vinitsky",
                "Yu Wang",
                "Alexandre Bayen",
                "Yi Wu"
            ],
            "title": "The surprising effectiveness of ppo in cooperative, multi-agent games",
            "venue": "arXiv preprint arXiv:2103.01955,",
            "year": 2021
        },
        {
            "authors": [
                "Yifan Zang",
                "Jinmin He",
                "Kai Li",
                "Haobo Fu",
                "Qiang Fu",
                "Junliang Xing"
            ],
            "title": "Sequential cooperative multi-agent reinforcement learning",
            "venue": "In Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems,",
            "year": 2023
        },
        {
            "authors": [
                "Lianmin Zheng",
                "Jiacheng Yang",
                "Han Cai",
                "Ming Zhou",
                "Weinan Zhang",
                "Jun Wang",
                "Yong Yu"
            ],
            "title": "Magent: A many-agent reinforcement learning platform for artificial collective intelligence",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2018
        },
        {
            "authors": [
                "HAPPO. SeCA (Zang"
            ],
            "title": "2023) constructs a new advantage value to improve upon PG-based methods. Different from focusing on an increment of PG-based methods, our work is proposed to extend the applicability of value decomposition methods to solve the mixing of homogeneous and heterogeneous tasks",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Centralized training with decentralized execution (CTDE) provides a popular paradigm for valuebased cooperative multi-agent reinforcement learning (MARL), which has been extensively employed to learn effective behaviors in many real-world tasks from agents\u2019 experiences (Sunehag et al., 2017; Rashid et al., 2018). These tasks encompass different types of scenarios, including homogeneous scenarios where agents are required to take similar actions, e.g., bimanual manipulation (Lee et al., 2013; Caccavale et al., 2008), and heterogeneous scenarios where agents are required to take distinct actions, e.g., autonomous driving through a crossroad (Chen et al., 2021). Following the Individual Global Maximum (IGM) principle (Hostallero et al., 2019), these value decomposition methods can learn centralized value functions as monotonic factorizations of each agent\u2019s utility function and enable decentralized execution. Meanwhile, as the parameters of the utility network can be shared among all agents (Gupta et al., 2017), the number of parameters to be trained can be significantly\n\u2217Co-corresponding Authors\nreduced. These features together increase the potential of previous CTDE solutions to be applied in large-scale scenarios involving either homogeneous or heterogeneous tasks.\nAssuming that scenarios only involve either homogeneous or heterogeneous tasks is oversimplified, as many real-world scenarios require agents to learn both tasks simultaneously (for example, running a restaurant requires waiters and cooks to cooperate within each group and between these two groups (Knott et al., 2021)). However, existing value decomposition methods have not been successful in addressing both types of tasks simultaneously. In homogeneous scenarios, the monotonic value function restricts the value function to sub-optimal value approximations in environments with nonmonotonic payoffs (Wang et al., 2020a; Son et al., 2019), they cannot represent the policy that an agent\u2019s optimal action depends on actions from other agents. This problem, known as the relative overgeneralization (Panait et al., 2006), prevents the agents from solving all kinds of homogeneous tasks. Recent methods have attempted to address this issue by encouraging agents to simultaneously take the same cooperative actions to find optimal policies in non-monotonic payoffs (Rashid et al., 2020; Mahajan et al., 2019). However, while agents acquiring similar policies can be advantageous for learning in non-monotonic homogeneous scenarios, it impedes the ability of agents to adapt to heterogeneous scenarios. Meanwhile, in heterogeneous scenarios, one of the main challenges is obtaining distinct policies among agents when the utility network is shared among all of them. This shared utility network makes it difficult for agents to learn and exhibit diverse policies that are necessary for such scenarios. Therefore, several methods employ techniques such as incorporating agent ID as input to construct different policies (Li et al., 2021a) or assigning different roles to encourage diverse behaviors (Christianos et al., 2021; Wang et al., 2020c). Nevertheless, these methods still encounter other challenges. They tend to result in fixed policies that can only represent a single solution mode of the task, which precludes cooperation when working with dynamically changing agents (Fu et al., 2022). In addition, simply aggregating distinct policies results in a trade-off scenario, where performance in homogeneous scenarios is negatively impacted (Christianos et al., 2021; Li et al., 2021a).\nTo address these challenges, sequential execution policies have been introduced. These policies allow agents to take actions based on the actions of previous agents, enabling the learning of both homogeneous and heterogeneous tasks (Fu et al., 2022; Liu et al., 2021). In this approach, as latter agents can adjust their actions based on the actions of earlier agents, they can exhibit either homogeneous or heterogeneous policy to cooperate with the previous agents, depending on the specific situation. However, sequential execution methods encounter challenges in credit assignment, as the policy form does not conform to the IGM principle, precluding current value decomposition methods from learning each agent\u2019s individual utility. Additionally, as the former executed agents lack action information about the latter agents, the former executed policies may still converge to a sub-optimal solution and cannot solve the relative overgeneralization problem thoroughly.\nIn this work, we propose Greedy Sequential Execution (GSE) which is capable of addressing these problems and adapting to both homogeneous and heterogeneous tasks. Specifically, we first propose a value decomposition scheme that captures the interactions between agents while adhering to the IGM principle. This value decomposition enables agents to learn individual utilities that take into account interactions with all other cooperative agents. We demonstrate that such individual utilities can accurately represent both homogeneous and heterogeneous payoff matrices. However, since the individual utilities require the actions of all other agents to conduct actions, which is infeasible in sequential execution, they cannot be directly used as the policy\u2019s value function. To address this issue, we further propose an explicit credit assignment method that calculates a greedy marginal contribution as the credit value for each agent\u2019s policy in sequential execution. The insight behind this is that each agent\u2019s optimal cooperative policy is to maximize the marginal contribution of joining the group of previously executed agents while considering that the latter agents will take optimal cooperative actions to cooperate with it. We show that the greedy marginal contribution can overcome the relative over-generalization problem by avoiding taking conservative actions that lead to mis-coordination. Furthermore, using the explicit credit assignment method can address the challenges of learning each agent\u2019s individual utility in the sequential execution policy. It allows for the precise allocation of credit to each agent\u2019s actions, enabling effective learning of the sequential execution policy. We evaluated GSE in comparison to several state-of-the-art baselines in various scenarios, including homogeneous tasks, heterogeneous tasks, and a combination of both tasks. The results demonstrate that our proposed method achieves a significant improvement in performance\nacross all domains and the most prominent advantages in scenarios involving both tasks, while other methods struggle to provide effective solutions."
        },
        {
            "heading": "2 PRELIMINARIES",
            "text": "Dec-POMDP. A fully cooperative multi-agent sequential decision-making task can be described as a decentralized partially observable Markov decision process (Dec-POMDP), which is defined by a set of possible global states S, actions A1, ..., An, and observations \u21261, ...,\u2126n. At each time step, each agent i \u2208 {1, ..., n} chooses an action ai \u2208 Ai, and they together form a joint action u \u2208 U . The next state is determined by a transition function P : S \u00d7 U \u2192 S. The next observation of each agent oi \u2208 \u2126i is updated by an observation function O : S \u2192 \u2126. All agents share the same reward r : S \u00d7 U \u2192 r. The objective is to learn a local policy \u03c0i(ai|st) for each agent such that they can cooperate to maximize the expected cumulative discounted return Rt = \u2211\u221e j=0 \u03b3\njrt+j . The joint value function is Qtot = Est+1:\u221e,at+1:\u221e[Rt|st,ut]. The observation of each agent can also be replaced by the history of actions and observations of each agent to handle partial observability (Sunehag et al., 2017; Rashid et al., 2018). The history of actions and observations of agent i can be viewed as \u03c4i which is (o0i , a 0 i , ..., o t i).\nValue Decomposition Methods. Current value decomposition methods represent the joint action value function Qtot as a mixing of per-agent utility functions to form the CTDE structure, where Individual Global Max (IGM) principle (Hostallero et al., 2019) is wildly used to enable efficient decentralized execution:\nargmax u (Qtot(s, u)) = {argmax a1 (Q1(\u03c41, a1)), ..., argmax an (Qn(\u03c4n, an))}. (1)\nQMIX (Rashid et al., 2018) combines the agent utilities via a continuous monotonic function to satisfy IGM, i.e.,\nQQMIXtot (s, u) = f(Q1(\u03c41, a1), ..., Qn(\u03c4n, an))\n\u2202QQMIXtot \u2202Qi > 0,\u2200i \u2208 n. (2)\nShapley Value and Marginal Contribution. We introduce the concept of the marginal contribution of Shapley Value (Shapley, 2016). The marginal contribution of Shapley Value for agent i is defined as\n\u03d5i = v(C)\u2212 v(C/i) (3) where C is a team of agents that cooperate with one another to achieve a common goal, and C/i represents the set in the absence of agent i. v(C) refers to the value function that estimates the cooperation of a set of agents."
        },
        {
            "heading": "3 MOTIVATING EXAMPLES",
            "text": "In this section, we utilize two example tasks as motivating examples to illustrate the advantages and limitations of various methods. Their payoff matrices are depicted in Figure 1."
        },
        {
            "heading": "3.1 ISSUES OF CURRENT METHODS",
            "text": "We investigate the limitations of current value decomposition methods by analyzing the form of individual utility. Currently, these methods model the individual utility of each agent as a value function Qi(\u03c4i, ai) to learn the decentralized policy. However, since the returns of both the homogeneous and heterogeneous tasks depend on the actions of other agents, such an individual utility is not sufficient to represent the cooperation policy. We propose a more comprehensive individual utility function, Qic(\u03c4i, u \u2212 i , ai), where u \u2212 i represents the joint actions of all other agents who have the potential to cooperate (discussed in detail in Section 4.1). According to this decomposition, the individual utility Qi(\u03c4i, ai) can be viewed as a variable sampled from the distribution Qic(\u03c4i, u \u2212 i , ai) over u\u2212i . This understanding enables us to demonstrate that Qi(\u03c4i, ai) cannot represent homogeneous and heterogeneous policies simultaneously, resulting in the trade-off when learning both types of tasks concurrently. We illustrate this through the two motivating example scenarios.\nFor the two example tasks, the learned policy fails to represent the optimal policy when\nr1 r2 < 2pb \u2212 1 1\u2212 pb . (4)\nIn the homogeneous scenarios, while the learned policy can never represent the optimal policy in the heterogeneous scenarios and the possibility Pc of achieving cooperation is\nPc = 2 \u00b7 pb \u00b7 (1\u2212 pb). (5)\nwhere pb is the probability of each policy taking action B. The detailed derivation and proof are included in Appendix 5. Figure 1 illustrates the result of Eq. (4) and Eq. (5). The result indicates that as the pb grows the r1r2 grows exponentially in the homogeneous scenarios. Therefore, we notice that there is a trade-off as solving the homogeneous non-monotonic matrixes requires the pb to decrease to zero, while solving the heterogeneous matrixes needs to increase pb when pb is below 0.5. As a result, the ability of these methods to effectively learn both homogeneous and heterogeneous tasks simultaneously is limited."
        },
        {
            "heading": "3.2 SEQUENTIAL EXECUTION AS A REMEDY",
            "text": "Another method is proposed that models individual utility as Qis(\u03c4i, a1:i\u22121, ai), which is a sequential execution policy. We illustrate that as pb serves as a known variable for the subsequently executed agent, the utility of the latter agent can choose actions in accordance with the actions of the former agent, thereby achieving the desired similar or distinct policies. As a result, the overall policy can encompass both homogeneous and heterogeneous policies. Although the interactions between agents involve communication, the bandwidth is limited as the actions are one-hot vectors. Therefore, these methods retain the potential to be implemented in complex real-world scenarios. However, the individual utility Qis(\u03c4i, a1:i\u22121, ai) does not satisfy the IGM principle as the former agents\u2019 utilities lack the necessary information about other agents\u2019 actions (detailed in the Appendix 6), which precludes implicit credit assignment. Additionally, the individual utility of the former agent remains Qis(\u03c4i, ai), which encounters the problem of relative over-generalization. Therefore, while the policy form is capable of representing the target policy mode, it is not guaranteed to converge to it."
        },
        {
            "heading": "4 GREEDY SEQUENTIAL EXECUTION",
            "text": "In this section, we propose Greedy Sequential Execution (GSE) to address the problems of credit assignment and relative over-generalization in the sequential execution policy. Specifically, we first propose a value decomposition that can capture the interactions between agents while adhering to the IGM principle. Then, we propose an actor-critic structure based on the value decomposition that trains a critic and calculates a greedy marginal contribution as credit value for sequential execution policy training. This explicit credit value addresses the credit assignment problem in learning\nsequential execution policy. Meanwhile, the greedy value of marginal contribution tackles the relative overgeneralized problem of former executed agents in sequential execution. As a result, GSE achieves optimal cooperation in scenarios involving both homogeneous and heterogeneous tasks by effectively learning the sequential execution policy."
        },
        {
            "heading": "4.1 VALUE DECOMPOSITION",
            "text": "Since the current individual utility Qi(\u03c4i, ai) is insufficient to represent the optimal policy in both homogeneous and heterogeneous scenarios, we illustrate that Qic(\u03c4i, u \u2212 i , ai) is a more comprehensive utility which can capture the interactions between agents.\nTheorem 4.1. For any rtot(s, u), the corresponding Qtot(s, u) = E [ \u2211\u221e t=0 \u03b3 trtot(s, u) | \u03c0] and each agent\u2019s utility Qic(\u03c4i, u \u2212 i , ai) satisfies\nargmax u (Qtot(s, u)) = {argmax a1 (Q1c(\u03c41, u \u2212 1 , a1)), ..., argmax an (Qnc (\u03c4n, u \u2212 n , an))}. (6)\nDetailed proof can be found in Appendix 4. Theorem 4.1 indicates that the value decomposition using utility Qic(\u03c4i, u \u2212 i , ai) can represent the value decomposition given any reward function and satisfies the IGM principle. Therefore, we can use all Qic to calculate Qtot through a monotonic mixing network similar to QMIX, and Theorem 4.1 illustrates the mixing value is unbiased. Specifically, the overall critic value function consists of each agent\u2019s adaptive utility Qic(\u03c4i, u \u2212 i , ai) and a mixing network to produce the global Q-value Qtot(s, u). The critic value function is learned by optimizing\nLTD(\u03b8) = E\u03c0[Qtot(st,ut)\u2212 yt]2\nyt = rt + \u03b3max ut+1\nQtot (st+1,ut+1) . (7)"
        },
        {
            "heading": "4.2 CREDIT ASSIGNMENT VIA GREEDY MARGINAL CONTRIBUTION",
            "text": "The utilization of the critic value function as the agent policy\u2019s value function is not feasible due to its reliance on the actions of all other agents, resulting in a deadlock situation. An alternative approach is the utilization of a sequential execution policy, represented by Qis(\u03c4i, a1:i\u22121, ai), which allows for the consideration of the actions of former agents. However, this approach does not adhere to the principles of IGM principal and encounters the relative over-generalization problem. To overcome these limitations, we propose an explicit credit assignment method utilizing marginal contribution of Shapley values for the learning of a policy capable of addressing both homogeneous and heterogeneous tasks.\nAccording to the sequential execution process, each agent takes actions with the actions of former executed agents, which is equivalent to agent i joining a group consisting of former executed agents. Therefore, the policy represented by Qis(\u03c4i, a1:i\u22121, ai) should aim to maximize the marginal contribution of agent i joining the group of former executed agents. Based on the critic value function, the marginal contribution of agent i can be calculated as\n\u03d5i(\u03c4i, a1:i\u22121, ai) = v(Ti)\u2212 v(Ti/i) = Qic(\u03c4i, a1:i\u22121, ai)\u2212 V ic (\u03c4i, a1:i\u22121), (8)\nwhere \u03d5i is the marginal contribution. The reason behind this is that the uniqueness of the optimal action within the entire action space, where the primary actions often result in mis-coordination. Therefore, we can use V ic (\u03c4i, a1:i\u22121) as the value of agent i not joining the former group to calculate the marginal contribution. However, such a calculation still faces other problems. Since the critic value function Qic is trained by taking u \u2212 i instead of a1:i\u22121 as input, the accuracy of the calculated marginal contribution may be affected. Additionally, such a marginal contribution encounters the relative over-generalization problem, as the value of action ai depends on overall joint actions u\u2212i and the current marginal contribution cannot consider the actions of latter agents ai+1:n, leading the marginal contribution of ai converge to an average value as we discussed in Section 3. To address these problems, we propose a greedy marginal contribution,\n\u03d5\u2217i (\u03c4i, a1:i\u22121, ai) = Q i c(\u03c4i, a1:i\u22121, a \u2217 i+1:n, ai)\u2212 V ic (\u03c4i, a1:i\u22121, a\u2217i+1:n), (9)\nwhere a\u2217i+1:n is the optimal cooperative actions to cooperate with former agents that maximize \u03d5i. This approach ensures that the marginal contribution accurately represents the potential optimal value of action ai, rather than an average value, thus addressing the issue of relative over-generalization. Furthermore, by including full information about u\u2212i as inputs, this approach allows for the accurate calculation of values by \u03d5i. However, a\u2217i+1:n is not directly observable. Intuitively, the greedy marginal contribution means that each agent takes the action under the condition that all the latter agents will take the optimal cooperative action to cooperate with it. Therefore, we use the greedy actions agi+1:n from the behavior policy to represent the a \u2217 i+1:n,\nagi+1:n = {argmaxai+1 (Qi+1s (\u03c41, a1:i)), ..., argmax an (Qns (\u03c4i, a1:n\u22121))}, (10)\nHowever, such an estimation is not guaranteed to be correct when the behavior policy has not converged in the early training period. Therefore, we additionally use the Monte Carlo method to estimate the a\u2217i+1:n to address this problem. Specifically, we sample M random joint actions as aj=1:Mi+1:n and search for the ai+1:n with the maximum value of \u03d5i in the collection of a j=1:M i+1:n and agi+1:n to be the a \u2217 i+1:n.\nIn this way, we have our sequential execution policy\u2019s value function Qis(\u03c4i, a1:i\u22121, ai) and decentralized policy as \u03c0(ai|\u03c4i, a1:i\u22121) = argmaxai(Qis(\u03c4i, a1:i\u22121, ai)). The overall loss is\nLp(\u00b5) = E\u03c0[Qis(\u03c4i, a1:i\u22121, ai)\u2212 \u03d5\u2217i (\u03c4i, a1:i\u22121, ai)]2. (11)\nIn terms of practical implementation, we utilize an attention structure to address the varying dimensions of a1:i\u22121, enabling the implementation of parameter sharing. The overall structure of our method is shown in Figure 2."
        },
        {
            "heading": "5 UNDERSTANDING TASK TYPES THROUGH MULTI-XOR GAMES",
            "text": "To evaluate how the different task types affect the performance of all methods, we devise a simple one-step randomized Multi-XOR game. Agents in the game can either take a cooperative action C or a lazy action L. The game requires two out of four agents to take cooperative actions C simultaneously to achieve successful cooperation. In contrast, if a single agent takes cooperative actions C, a homogeneous penalty is applied, and when more than two agents take cooperative actions C, a heterogeneous penalty is given. To prevent agents from learning a fixed strategy, we randomly selected a dummy agent at the beginning of each episode, which cannot participate in cooperation and can only take the lazy action L. We set three scenarios according to the challenges of tasks, the homogeneous scenario with only the homogeneous penalty, the heterogeneous with only the heterogeneous penalty, and the mixing scenario with both penalties. These methods that we compared include CDS (Li et al., 2021a), which employs mutual information to learn agent ID-specific policies; Shapley (Li et al., 2021b), which utilizes Shapley Value to estimate complex agent interactions; and\nboth MAVEN (Mahajan et al., 2019) and QMIX (Rashid et al., 2018), which are common value decomposition methods. For a detailed introduction to all the methods compared, please refer to Appendix 1. Detailed reward settings are included in Appendix 2.\nThe results displayed in Figure 3 indicate that our method effectively masters both homogeneous and heterogeneous tasks in the randomized Multi-XOR game. Specifically, in the homogeneous scenario, all other methods fail to overcome the non-monotonicity and learn a lazy policy that never engages in cooperative action to avoid penalties, except for MAVEN which is proposed to learn cooperative policy in non-monotonic environments. In the heterogeneous scenario, MAVEN and QMIX fail to learn heterogeneous policies and take cooperative action together, resulting in failure. In the mixing scenario, our method also outperforms other methods, indicating its robustness in handling both homogeneous and heterogeneous tasks simultaneously. Overall, our method demonstrates superior performance in adapting to different types of cooperative tasks compared to other methods."
        },
        {
            "heading": "6 EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "6.1 EXPERIMENTAL SETTINGS",
            "text": "We have developed a series of challenging cooperative scenarios that involve the integration of both homogeneous and heterogeneous tasks to evaluate all methods. The experiments are conducted based on MAgent (Zheng et al., 2018) and Overcooked (Sarkar et al., 2022). We implement five tasks in MAgent: lift, heterogeneous_lift, multi_target_lift, pursuit and bridge. These scenarios can be classified into three categories: homogeneous, heterogeneous, and mixing.\nHomogeneous scenarios: In our implementation, we have included two homogeneous scenarios within the task of lift. Specifically, the lift task requires the coordination of two agents to lift a cargo. Successful completion of the task necessitates the simultaneous execution of the \"lift\" action by both agents, otherwise, the cooperation will fail and the agent who takes the action will incur a penalty of \u2212r2, similar to the homogeneous task discussed in Section 3. To evaluate all methods, we have chosen two scenarios with \u2212r2 = 0 and \u2212r2 = \u22120.3, as these scenarios represent situations without and with the relative over-generalization problem, respectively.\nHeterogeneous scenarios: In heterogeneous_lift, agents must lift cargos cooperatively with changing partners. Rewards are given for two-agent lifts, with penalties for more than two. Each episode randomly excludes one or two agents from cooperating, preventing fixed policy learning. The challenge of this task lies in the fact that agents must adapt to varied partners for successful lifting. This necessitates the learning of heterogeneous policy to succeed.\nHomogeneous & heterogeneous scenarios: We also implement three tasks that necessitate both homogeneous and heterogeneous policies to be solved. The first task, multi_target_lift, is similar to lift, but with different cargos with varying rewards. The challenge of this task is that agents must learn to lift cargos with lower rewards in order to achieve optimal cooperation, instead of all competing to lift cargos with higher rewards. The second task, pursuit, requires agents to catch a prey and rewards are only given when two agents attack together, otherwise, a penalty is imposed. Additionally, the prey will be killed if more than two agents attack together, resulting in a significant penalty, thus requiring agents to perform differently to avoid killing the prey. In the third bridge task, agents start on opposite sides of the map and must navigate through a single tunnel, initially blocked by an\nobstacle. Two agents must cooperate to break this barrier, then learn to retreat, preventing tunnel blockage for others.\nWe also designed three scenarios in Overcooked that require both homogeneous and heterogeneous policies. In the game, the objective is to cook and deliver soups. We designed the picking of onions to be a cooperative action that requires two agents to work together. However, the cooking and delivering process introduce the need for agents to exhibit different behaviors, such as yielding to others. We evaluated our approach on three maps of different difficulties: an easy small map with multiple onions, a medium-difficulty small map with a single onion, and a challenging large map with a single onion. Please refer to Appendix 2 and Appendix 3 for more experimental settings.\nIn the experiments, the methods compared are introduced in Section 5. All the baseline methods use the agent ID as an extra input to enable the learning of heterogeneous policies, while our method does not. All methods use the same basic hyperparameters and network structures with similar parameters to ensure the comparison is fair."
        },
        {
            "heading": "6.2 PERFORMANCE",
            "text": "We evaluate the performance of various methods in three types of scenarios: homogeneous, heterogeneous, and homogeneous & heterogeneous. The results of MAgent scenarios are shown in Figure 4 and Overcooked are in Figure 5. The results for the homogeneous scenarios indicate that the performance of all comparison methods is heavily influenced by the increase in penalty. Most methods are able to learn an optimal policy when the penalty is zero, but they fail to converge to a cooperative policy when the penalty is -0.3. Their policies converge to a sub-optimal policy that never takes cooperative actions in order to avoid the mis-coordination penalty, except for MAVEN which is proposed to solve the relative overgeneralization problem. In contrast, our method can find the optimal cooperative policy in both scenarios regardless of the growing penalty. This result indicates that our method is able to overcome the relative over-generalization problem.\nIn the heterogeneous scenario, we observe that QMIX converges to a sub-optimal policy that all agents lift together, resulting in penalties. Similarly, other methods also struggle to learn the cooperative policy but can learn to take lazy actions to avoid penalties. All of these policies reflect a failure to learn distinct policies that can adapt to dynamically changing partners. In contrast, our method demonstrates the ability to adapt to the actions of other agents and outperforms other methods in terms of both final performance and sample efficiency when learning heterogeneous policy.\nLastly, we evaluate all methods in mixing scenarios involving both homogeneous and heterogeneous tasks. The results show that our method has the most significant advantage in these scenarios. In MAgent scenarios, most methods coverage to the conservative policy since the penalty comes from mis-coordination of both homogeneous and heterogeneous actions which they cannot handle simultaneously. However, CDS can solve the bridge scenario, which is because the homogeneous behavior only involves breaking the obstacle and most required behaviors are heterogeneous actions. In Overcooked scenarios, we compared our method with MAVEN and CDS as they represent methods that can handle complex homogeneous and heterogeneous tasks, respectively. The results are consistent with other mixing scenarios. Since the learning difficulty of these tasks mainly arises from learning two conflicting tasks modes simultaneously, this result indicates that our method can unify the learning of similar and distinct policies. On the contrary, all other methods struggle to learn an efficient policy to solve the tasks due to their narrow policy representation ability."
        },
        {
            "heading": "6.3 ABLATIONS",
            "text": "We conduct several ablation studies to evaluate the effectiveness of our proposed method in homogeneous & heterogeneous scenarios, multi_target_lift and multi_XOR. Specifically, we evaluate our method training without using greedy actions, meaning we relied on marginal contributions instead of greedy marginal contributions. Additionally, we evaluated the method without using marginal contributions, instead directly using the policy value function to fit the Qtot. The results of these evaluations are presented in Figure 6. The results indicate that training without using greedy actions can significantly degrade performance, as using greedy actions helps to overcome the relative overgeneralization problem. Training without using marginal contributions also degrades performance in both scenarios as the sequential execution policy does not satisfy the IGM principle, underscoring the importance of our proposed utility function. Additionally, we evaluated how the sample number M affects performance. The results demonstrate that a small value of M can be problematic as it may not select the greedy value of actions. However, a reasonably large value of M is sufficient as increasing M beyond 5 does not further improve performance. Finally, we evaluated our method with a larger number of agents, specifically by using double or triple the number of agents compared to our standard configurations. The results in Figure 6 demonstrate that our method is capable of handling a larger number of agents without affecting performance."
        },
        {
            "heading": "7 CONCLUSION",
            "text": "This work introduces Greedy Sequential Execution (GSE), a unified solution designed to extend value decomposition methods for tackling complex cooperative tasks in real-world scenarios. Our method employs a utility function that accounts for complex agent interactions and supports both homogeneous and heterogeneous optimal policies. Furthermore, by implementing a greedy marginal contribution, GSE overcomes the relative over-generalization problem. Our experiments show that GSE significantly improves performance across various domains, especially in mixed-task scenarios."
        },
        {
            "heading": "ACKNOWLEDGEMENTS",
            "text": "This work was supported by NSFC 62088101 Autonomous Intelligent Unmanned Systems and China Scholarship Council."
        },
        {
            "heading": "A RELATED WORK",
            "text": "Recent works have extended MARL from small discrete state spaces (Yang & Gu, 2004; Busoniu et al., 2008) to high-dimensional, continuous state spaces (Lowe et al., 2017; Peng et al., 2017). The progresses of deep reinforcement learning give rise to an increasing effort in designing generalpurpose deep MARL methods for complex multi-agent environments, including COMA (Foerster et al., 2018), MADDPG (Lowe et al., 2017), MAPPO (Yu et al., 2021) and etc. Currently, CTDE is considered to be the de facto mainstream paradigm in this field (Lowe et al., 2017; Iqbal & Sha, 2019). In terms of specific methods, the Value-Decomposition Network (VDN) (Sunehag et al., 2017) utilizes the factorization of joint-action Q-values as the sum of each agent\u2019s utility. QMIX (Rashid et al., 2018) is an extension of VDN which allows the joint action Q-value to be a monotonically increasing combination of each agent\u2019s utility, which can vary depending on the global state. There are also other variants proposed to extend the applicability of the value decomposition methods. For instance, QPLEX (Wang et al., 2020a) and QTRAN (Son et al., 2019) aim to learn value functions with complete expressiveness capacity. MAVEN (Mahajan et al., 2019) hybridises value and policybased methods by introducing a latent space for hierarchical control. This allows MAVEN to achieve committed, temporally extended exploration. Weighted QMIX (Rashid et al., 2020) is based on QMIX and rectifies the suboptimality by introducing weights to place more importance on the better joint actions. UneVEn (Gupta et al., 2021) learns a set of related tasks simultaneously with a linear decomposition of universal successor features. Despite the effectiveness of these methods, they are commonly designed to facilitate the learning of similar policies, it can be detrimental to the acquisition of heterogeneous policies.\nTo solve the heterogeneous tasks, previous methods choose to add agent-specific information to the observation or assign different roles to learn the distinct policies. PSHA (Terry et al., 2020) proposes an agent indication to enable agents to represent heterogeneous policies. CDS (Li et al., 2021a) uses mutual information to learn an agent ID-specific policy to deal with the problem of learning diversity policies. ROMA (Wang et al., 2020c) proposes a role-oriented MARL framework to make agents specialized in certain tasks. However, these methods, which solely focus on learning distinct policies, often come at the cost of sacrificing the advantages associated with learning in homogeneous scenarios. Furthermore, these methods tend to learn fixed policies that lack the necessary flexibility.\nOther methods use a sequential execution policy to represent distinct policies. AR (Fu et al., 2022) proposes a centralized sequential execution policy to solve permutation games. MAiF (Liu et al., 2021) uses a sequential execution policy to learn a path-finding and formation policy for a multiagent navigation task. These methods can represent the optimal policy in both homogeneous and heterogeneous scenarios. However, a naive sequential execution policy is not guaranteed to converge to optimal policy and has the problem of credit assignment. Additionally, there are also methods such as HAPPO (Kuba et al., 2021) that use sequential policy updates to guarantee monotonic policy improvement of PPO (Schulman et al., 2017). MAT (Wen et al., 2022) adopts sequential policy updates within the structure of a transformer. This design is aimed at executing updates both monotonically and in parallel, thereby enhancing the time efficiency compared to previous methods like HAPPO. SeCA (Zang et al., 2023) constructs a new advantage value to improve upon PG-based methods. Different from focusing on an increment of PG-based methods, our work is proposed to extend the applicability of value decomposition methods to solve the mixing of homogeneous and heterogeneous tasks.\nOur work is also related to the credit assignment. Previous methods usually use implicit credit assignment methods to learn the policy, such as VDN and QMIX. However, explicit credit assignment methods have also been proposed. For instance, COMA (Foerster et al., 2018) utilizes a counterfactual advantage to learn the value function. Other methods use Shapley Value (Shapley, 2016) as the credit value of each agent. Shapley Value originates from cooperative game theory and is able to distribute benefits reasonably by estimating the contribution of participating agents. In these methods, SQDDPG (Wang et al., 2020b) and Shapley (Li et al., 2021b) use Shapley Value to estimate the complex interactions between agents. However, these methods can only get approximated Shapley value as calculating the Shapley value involves exponential time complexity (Wang et al., 2020b) and they are not designed to learn similar and distinct policies simultaneously. In this work, we introduce an explicit credit assignment method using marginal contribution in Shapley value to learn a sequential execution policy that can represent the optimal policy in scenarios with a mixing of homogeneous and heterogeneous tasks."
        },
        {
            "heading": "B SCENARIOS SETTINGS AND TRAINING DETAILS",
            "text": "In the Multi-XOR games, agents receive two types of rewards, as illustrated in Table 1 and 2. Table 1 displays the homogeneous reward, which exhibits a non-monotonic payoff. This poses a challenge of relative overgeneralization for the learning process. Table 2 presents the heterogeneous reward, where agents are required to take distinct actions. Specifically, if two agents choose the joint actions C&C to solve the task, the other two agents must choose L&L; otherwise, a penalty will be imposed. However, if all agents select L&L, the return will be zero.\nIn MAgent, each agent corresponds to one grid and has a local observation that contains a square view centered at the agent and a feature vector including coordinates, health point (HP) and ID of agents nearby, and the agent\u2019s last action. The discrete actions are moving, staying, and attacking. The global state of MAgent is a mini-map (6\u00d7 6) of the global information. The opponent\u2019s policies used in experiments are randomly escaping policy in pursuit. We choose five different scenarios lift, heterogeneous_lift, multi_target_lift, pursuit and bridge. There are detailed settings of these scenarios, as shown in Table 3. We demonstrate the payoff matrix by showing the R as the reward returned when cooperation is achieved, Pho as are penalty when taking cooperative action but failing to achieve cooperation in homogeneous scenarios, and Phe as the penalty for taking the same action in heterogeneous scenarios."
        },
        {
            "heading": "C +0.5 -0.3",
            "text": "In the Overcooked environment, the objective is to perform a series of tasks involving onions, dishes, and soups. The agents are required to place 2 onions in a pot, let them cook for 5 timesteps, transfer the resulting soup into a dish, and finally serve it, which rewards all players with a score of 20. There are six possible actions available to the agents: up, down, left, right, noop (no operation), and interact. Notably, the action of picking up onions requires two agents to simultaneously take the \"interact\" action, otherwise a penalty of -0.1 is incurred. On the other hand, actions such as putting onions into the pots can be performed by a single agent. To evaluate the difficulty level of different scenarios, we have designed three maps with varying levels of complexity. The easy map consists of more onions and a smaller map size (5\u00d75), making it relatively easier to solve. The medium map, on the other hand, contains a single onion and a smaller map size (5\u00d75). Finally, the hard map poses a greater challenge with its larger map size (7\u00d77) and a single onion, making exploration more demanding for the agents.\nWe set the discount factor as 0.99 and use the RMSprop optimizer with a learning rate of 5e-4 for policy and 1e-3 for the critic. The \u03f5-greedy is used for exploration with \u03f5 annealed linearly from 1.0 to 0.05 in 700k steps. The batch size is 4 and updating the target network every 200 episodes. The length of each episode in MAgent is limited to 100 steps in bridge and 50 for others, except for Multi-XOR which is a single-step game. The sample number M of our method is 5 in all scenarios. We run all the experiments five times with different random seeds and plot the mean/std in all the\nfigures. All experiments are carried out on the same computer, equipped with Intel(R) Xeon(R) Gold 5218R CPU @ 2.10GHz, 64GB RAM and an NVIDIA RTX3090. The system is Ubuntu 18.04 and the framework is PyTorch."
        },
        {
            "heading": "C DETAILS OF MODEL IMPLEMENTATION AND HYPERPARAMETERS",
            "text": "The network of all compared methods uses the same LSTM network, consisting of a recurrent layer comprised of a GRU with a 64-dimensional hidden state, with one fully-connected layer before and two after. All mixing networks use a fully-connected layer with 32-dimensional hidden state. The network of our critic and policy uses two fully-connected layers with 64-dimensional hidden state and one fully-connected layers with 32-dimensional hidden state after."
        },
        {
            "heading": "D PROOF OF THE VALUE DECOMPOSITION OF CRITIC",
            "text": "First of all, according to the decentralized execution setting, there exists a reward decomposition, rtot(s, u) = \u2211n i=1 r i c(oi, u) = \u2211n i=1 r i c(oi, u \u2212 i , ai). (12)\nThis is because if the task can be solved by decentralized execution, the observation of each agent must contain all the necessary information to identify the goals. Otherwise, agents will require density communication to receive information about others\u2019 observations to identify the goals, which is not the setting that we discussed in our works. Then, we define the value decomposition Qic which models each agent\u2019s individual utility. From Eq. (12), we have\nQtot(s, u) = E [\u2211\u221e t=0 \u03b3 trtot (s, u) | \u03c0 ] = E [\u2211\u221e t=0 \u03b3 t \u2211n i=1 r i team(oi, u \u2212 i , ai) | \u03c0 ] = \u2211n i=1 Q i c(s, u).\n(13) In addition, we have\nargmax ai (Qtot(s, u)) = argmax ai (Qic(s, u)) = argmax ai (Qic(\u03c4i, u \u2212 i , ai)). (14)\nThe first part is because the value of ai is represented by item Qic and the reason for the second part is that Qic is only related to agent i as well as the actions of potential cooperative agents and all the necessary information is contained in (\u03c4i, u\u2212i , ai), so we can get the unbiased estimated value of Q i c given (\u03c4i, u\u2212i , ai). Therefore, from Eq. (13) and Eq. (14) we have\nargmax u (Qtot(s, u)) = {argmax a1 (Q1c(\u03c41, u \u2212 1 , a1)), ..., argmax an (Qnc (\u03c4n, u \u2212 n , an))}. (15)\nAn intuitive understanding of Eq. (15) is that each agent takes action based on the perception of other potential cooperative agents\u2019 actions, so they can take the corresponding cooperative action and the joint action is the optimal cooperative joint action."
        },
        {
            "heading": "E LIMITATIONS OF INDIVIDUAL UTILITY",
            "text": "E.1 HOMOGENEOUS SCENARIOS\nFirst, for the homogeneous task, we have the payoff matrix in Table 4. Since, we indicate that the\nindividual utility Qi(\u03c4i, ai), should be viewed as a variable sampled from distribution Qic(\u03c4i, u \u2212 i , ai). Following this conclusion, we have the loss of Qi(\u03c4i, ai) should be\nLi = \u2211Ki k=1 pk \u00b7 (Q\u0302ic(\u03c4i, u k\u2212 i , ai)\u2212Qi(\u03c4i, ai))2. (16)\nwhere Q\u0302ic means the ground true value function, u k\u2212 i means one of the combination of u \u2212 i and pk is the possibility of uk\u2212i occurred. Therefore, Qi(\u03c4i, ai) learns to the converged value by optimizing Li, we have the converged Q\u0302i(\u03c4i, ai) when Li is minimized,\nQ\u0302i(\u03c4i, ai) = \u2211Ki k=1 pk \u00b7 Q\u0302ic(\u03c4i, u k\u2212 i , ai). (17)\nFor a simple demonstration, we take the example payoff into Eq. (17). The value of cooperative action a\u2217i is\nQ\u0302i(\u03c4i, a \u2217 i ) = pa \u00b7 Q\u0302ic(\u03c4i, u \u2212\u2217 i , a \u2217 i ) + pb \u00b7 Q\u0302ic(\u03c4i, u \u2212 i , a \u2217 i ). (18)\nwhere pa means the possibility of other agent taking cooperative actions ua\u2212\u2217i (u a\u2212\u2217 i = A) and pb means the possibility of other agents taking the other actions ub\u2212i (u b\u2212 i = B). Additionally, we have\npa + pb = 1 (19)\nSimilarly, we have the value of lazy action a\u2212i as\nQ\u0302i(\u03c4i, a \u2212 i ) = pa \u00b7 Q\u0302ic(\u03c4i, u \u2212\u2217 i , a \u2212 i ) + pb \u00b7 Q\u0302ic(\u03c4i, u \u2212 i , a \u2212 i ). (20)\nWe know the policy represented by Qi(\u03c4i, ai) fails when Q\u0302i(\u03c4i, a\u2212i ) is larger than Q\u0302i(\u03c4i, a \u2217 i ), which is Q\u0302i(\u03c4i, a \u2212 i )\u2212 Q\u0302i(\u03c4i, a\u2217i ) = pa \u00b7 (Q\u0302ic(\u03c4i, u \u2212\u2217 i , a \u2212 i )\u2212Qic(\u03c4i, u \u2212\u2217 i , a \u2217 i ))\n+pb \u00b7 (Q\u0302ic(\u03c4i, u\u2212i , a \u2212 i )\u2212Qic(\u03c4i, u \u2212 i , a \u2217 i )) > 0\n(21)\nWe take the +r1 and \u2212r2 into Eq. (21),\nQ\u0302i(\u03c4i, a \u2212 i )\u2212 Q\u0302i(\u03c4i, a\u2217i ) = pa \u00b7 (\u2212r2 \u2212 r1) + pb \u00b7 (0\u2212 (\u2212r2)) = (pb \u2212 1) \u00b7 (r2 + r1) + pb \u00b7 r2 > 0\n(22) This means the policy represented by Qi(\u03c4i, ai) will fail when\nr1 \u00b7 (1\u2212 pb) < (2pb \u2212 1) \u00b7 r2. (23)\nwhich equals to r1 r2 < 2pb\u221211\u2212pb . (24)\nE.2 HETEROGENEOUS SCENARIOS\nFirst, for the heterogeneous task, we have the payoff matrix in Table 5. Similarly, agents with the policy represented by Qi(\u03c4i, ai) fails when Q\u0302i(\u03c4i, a\u2212i ) is larger than Q\u0302i(\u03c4i, a \u2217 i ) in the heterogeneous scenario. However, there are multiple optimal joint actions (1=A,2=B), (1=B,2=A), which are different from the homogeneous scenarios. We first consider the (1=A,2=B) situation which is\nQ\u0302i(\u03c4i, a \u2212 i )\u2212 Q\u0302i(\u03c4i, a\u2217i ) = pb \u00b7 (Q\u0302ic(\u03c4i, u \u2212\u2217 i , a \u2212 i )\u2212Qic(\u03c4i, u \u2212\u2217 i , a \u2217 i ))\n+pa \u00b7 (Q\u0302ic(\u03c4i, u\u2212i , a \u2212 i )\u2212Qic(\u03c4i, u \u2212 i , a \u2217 i )) > 0\n(25)\nTaking the the +r1 and \u2212r2 into Eq. (25),\nQ\u0302i(\u03c4i, a \u2212 i )\u2212 Q\u0302i(\u03c4i, a\u2217i ) = pb \u00b7 (\u2212r2 \u2212 r1) + pa \u00b7 (r1 \u2212 (\u2212r2)) = \u2212pb \u00b7 (r2 + r1) + (1\u2212 pb) \u00b7 (r2 + r1) > 0\n(26)\nwhich equals to 1\u2212 2pb > 0 (27) pb < 1 2 . (28)\nFor situation (1=B,2=A), we have a similar conclusion,\npa < 1 2 . (29)\nWe notice that the overall possibility of failure is\nPf = P (pa < 1 2 ) + P (pb < 1 2 ) = P ((1\u2212 pb) < 1 2 ) + P (pb < 1 2 ) = P ( 1 2 < pb) + P (pb < 1 2 ) = 1.\n(30) Therefore, the policy represented by Qi(\u03c4i, ai) can never promise to solve the heterogeneous task. Furthermore, we can calculate the possibility of reaching cooperation,\nPc = P ((1 = B, 2 = A)) + P ((1 = A, 2 = B)) = pb \u00b7 pa + pa \u00b7 pb = 2 \u00b7 pb \u00b7 (1\u2212 pb). (31)\nThe maximization of Eq. (31) is 0.5 when pb = pa = 0.5. The result demonstrated that decreasing pb when pb < 0.5 causes cooperation more difficult to be reached."
        },
        {
            "heading": "F ANALYSIS OF INDIVIDUAL UTILITY OF SEQUENTIAL EXECUTION POLICY",
            "text": "We have the IGM principal as\nargmax u (Qtot(s, u)) = {argmax a1 (Q1(\u03c41, a1)), ..., argmax an (Qn(\u03c4n, an))}. (32)\nFor sequential execution method, the policy is the form of\nu = {argmax a1 (Qis(\u03c41, a1)), ..., argmax an (Qis(\u03c4i, a1:n\u22121, an))}. (33)\nWe take the payoff matrix in Table 5 as an example, there are multiple optimal joint actions (1=A,2=B), (1=B,2=A), we take the optimal actions into Eq. (33),\n(1 = A, 2 = B) = {argmax a1 (Qis(\u03c41, a1)), argmax an (Qis(\u03c4i, A, an))}\n(1 = B, 2 = A) = {argmax a1 (Qis(\u03c41, a1)), argmax an\n(Qis(\u03c4i, B, an))}. (34)\nWe notice that although the latter utility has the correct maximization of the utility, the former one has a conflict maximum result as it lacks the necessary information about other agents\u2019 actions."
        }
    ],
    "year": 2024
}