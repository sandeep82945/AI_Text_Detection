{
    "abstractText": "Recent advances in high-fidelity virtual environments serve as one of the major driving forces for building intelligent embodied agents to perceive, reason and interact with the physical world. Typically, these environments remain unchanged unless agents interact with them. However, in real-world scenarios, agents might also face dynamically changing environments characterized by unexpected events and need to rapidly take action accordingly. To remedy this gap, we propose a new simulated embodied benchmark, called HAZARD, specifically designed to assess the decision-making abilities of embodied agents in dynamic situations. HAZARD consists of three unexpected disaster scenarios, including fire , flood , and wind , and specifically supports the utilization of large language models (LLMs) to assist common sense reasoning and decision-making. This benchmark enables us to evaluate autonomous agents\u2019 decision-making capabilities across various pipelines, including reinforcement learning (RL), rule-based, and search-based methods in dynamically changing environments. As a first step toward addressing this challenge using large language models, we further develop an LLM-based agent and perform an in-depth analysis of its promise and challenge of solving these challenging tasks. HAZARD is available at https://sites.google. com/view/hazard-challenge.",
    "authors": [],
    "id": "SP:f900b31a2e01c4f2a39e13e208be38662c622229",
    "references": [
        {
            "authors": [
                "Michael Ahn",
                "Anthony Brohan",
                "Noah Brown",
                "Yevgen Chebotar",
                "Omar Cortes",
                "Byron David",
                "Chelsea Finn",
                "Keerthana Gopalakrishnan",
                "Karol Hausman",
                "Alex Herzog"
            ],
            "title": "Do as i can, not as i say: Grounding language in robotic affordances",
            "venue": "arXiv preprint arXiv:2204.01691,",
            "year": 2022
        },
        {
            "authors": [
                "Jonathan Binney",
                "Andreas Krause",
                "Gaurav S Sukhatme"
            ],
            "title": "Informative path planning for an autonomous underwater vehicle",
            "venue": "In 2010 IEEE International Conference on Robotics and Automation,",
            "year": 2010
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Kai Chen",
                "Jiaqi Wang",
                "Jiangmiao Pang",
                "Yuhang Cao",
                "Yu Xiong",
                "Xiaoxiao Li",
                "Shuyang Sun",
                "Wansen Feng",
                "Ziwei Liu",
                "Jiarui Xu",
                "Zheng Zhang",
                "Dazhi Cheng",
                "Chenchen Zhu",
                "Tianheng Cheng",
                "Qijie Zhao",
                "Buyu Li",
                "Xin Lu",
                "Rui Zhu",
                "Yue Wu",
                "Jifeng Dai",
                "Jingdong Wang",
                "Jianping Shi",
                "Wanli Ouyang",
                "Chen Change Loy",
                "Dahua Lin"
            ],
            "title": "MMDetection: Open mmlab detection toolbox and benchmark",
            "venue": "arXiv preprint arXiv:1906.07155,",
            "year": 2019
        },
        {
            "authors": [
                "Allan Chikwanha",
                "Sibonelo Motepe",
                "Riaan Stopforth"
            ],
            "title": "Survey and requirements for search and rescue ground and air vehicles for mining applications",
            "venue": "In 2012 19th International Conference on Mechatronics and Machine Vision in Practice",
            "year": 2012
        },
        {
            "authors": [
                "Aakanksha Chowdhery",
                "Sharan Narang",
                "Jacob Devlin",
                "Maarten Bosma",
                "Gaurav Mishra",
                "Adam Roberts",
                "Paul Barham",
                "Hyung Won Chung",
                "Charles Sutton",
                "Sebastian Gehrmann"
            ],
            "title": "Palm: Scaling language modeling with pathways",
            "venue": "arXiv preprint arXiv:2204.02311,",
            "year": 2022
        },
        {
            "authors": [
                "Geert De Cubber",
                "Daniela Doroftei",
                "Konrad Rudin",
                "Karsten Berns",
                "Daniel Serrano",
                "Jose Sanchez",
                "Shashank Govindaraj",
                "Janusz Bedkowski",
                "Rui Roda"
            ],
            "title": "Search and rescue robotics-from theory to practice, 2017",
            "year": 2017
        },
        {
            "authors": [
                "Abhishek Das",
                "Samyak Datta",
                "Georgia Gkioxari",
                "Stefan Lee",
                "Devi Parikh",
                "Dhruv Batra"
            ],
            "title": "Embodied question answering",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Danny Driess",
                "Fei Xia",
                "Mehdi S.M. Sajjadi",
                "Corey Lynch",
                "Aakanksha Chowdhery",
                "Brian Ichter",
                "Ayzaan Wahid",
                "Jonathan Tompson",
                "Quan Vuong",
                "Tianhe Yu",
                "Wenlong Huang",
                "Yevgen Chebotar",
                "Pierre Sermanet",
                "Daniel Duckworth",
                "Sergey Levine",
                "Vincent Vanhoucke",
                "Karol Hausman",
                "Marc Toussaint",
                "Klaus Greff",
                "Andy Zeng",
                "Igor Mordatch",
                "Pete Florence"
            ],
            "title": "Palm-e: An embodied multimodal language model",
            "venue": "In arXiv preprint arXiv:2303.03378,",
            "year": 2023
        },
        {
            "authors": [
                "Samira Hayat",
                "Ev\u015fen Yanmaz",
                "Raheeb Muzaffar"
            ],
            "title": "Survey on unmanned aerial vehicle networks for civil applications: A communications viewpoint",
            "venue": "IEEE Communications Surveys & Tutorials,",
            "year": 2016
        },
        {
            "authors": [
                "Kaiming He",
                "Georgia Gkioxari",
                "Piotr Doll\u00e1r",
                "Ross Girshick"
            ],
            "title": "Mask r-cnn",
            "venue": "In Proceedings of the IEEE international conference on computer vision,",
            "year": 2017
        },
        {
            "authors": [
                "Wenlong Huang",
                "Fei Xia",
                "Ted Xiao",
                "Harris Chan",
                "Jacky Liang",
                "Pete Florence",
                "Andy Zeng",
                "Jonathan Tompson",
                "Igor Mordatch",
                "Yevgen Chebotar"
            ],
            "title": "Inner monologue: Embodied reasoning through planning with language models",
            "venue": "arXiv preprint arXiv:2207.05608,",
            "year": 2022
        },
        {
            "authors": [
                "Yash Kant",
                "Arun Ramachandran",
                "Sriram Yenamandra",
                "Igor Gilitschenski",
                "Dhruv Batra",
                "Andrew Szot",
                "Harsh Agrawal"
            ],
            "title": "Housekeep: Tidying virtual households using commonsense reasoning",
            "venue": "In Computer Vision\u2013ECCV 2022: 17th European Conference,",
            "year": 2022
        },
        {
            "authors": [
                "Levente Kocsis",
                "Csaba Szepesv\u00e1ri"
            ],
            "title": "Bandit based monte-carlo planning",
            "venue": "In Machine Learning: ECML 2006: 17th European Conference on Machine Learning Berlin,",
            "year": 2006
        },
        {
            "authors": [
                "Eric Kolve",
                "Roozbeh Mottaghi",
                "Winson Han",
                "Eli VanderBilt",
                "Luca Weihs",
                "Alvaro Herrasti",
                "Matt Deitke",
                "Kiana Ehsani",
                "Daniel Gordon",
                "Yuke Zhu"
            ],
            "title": "Ai2-thor: An interactive 3d environment for visual ai",
            "venue": "arXiv preprint arXiv:1712.05474,",
            "year": 2017
        },
        {
            "authors": [
                "Federico Landi",
                "Roberto Bigazzi",
                "Marcella Cornia",
                "Silvia Cascianelli",
                "Lorenzo Baraldi",
                "Rita Cucchiara"
            ],
            "title": "Spot the difference: A novel task for embodied agents in changing environments",
            "venue": "26th International Conference on Pattern Recognition (ICPR),",
            "year": 2022
        },
        {
            "authors": [
                "Daoliang Li",
                "Peng Wang",
                "Ling Du"
            ],
            "title": "Path planning technologies for autonomous underwater vehicles-a review",
            "venue": "Ieee Access,",
            "year": 2018
        },
        {
            "authors": [
                "Shuang Li",
                "Xavier Puig",
                "Chris Paxton",
                "Yilun Du",
                "Clinton Wang",
                "Linxi Fan",
                "Tao Chen",
                "De-An Huang",
                "Ekin Aky\u00fcrek",
                "Anima Anandkumar"
            ],
            "title": "Pre-trained language models for interactive decision-making",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Jacky Liang",
                "Wenlong Huang",
                "Fei Xia",
                "Peng Xu",
                "Karol Hausman",
                "Brian Ichter",
                "Pete Florence",
                "Andy Zeng"
            ],
            "title": "Code as policies: Language model programs for embodied control",
            "venue": "arXiv preprint arXiv:2209.07753,",
            "year": 2022
        },
        {
            "authors": [
                "Kevin Lin",
                "Christopher Agia",
                "Toki Migimatsu",
                "Marco Pavone",
                "Jeannette Bohg"
            ],
            "title": "Text2motion: From natural language instructions to feasible plans",
            "venue": "arXiv preprint arXiv:2303.12153,",
            "year": 2023
        },
        {
            "authors": [
                "Bo Liu",
                "Yuqian Jiang",
                "Xiaohan Zhang",
                "Qiang Liu",
                "Shiqi Zhang",
                "Joydeep Biswas",
                "Peter Stone"
            ],
            "title": "Llm+ p: Empowering large language models with optimal planning proficiency",
            "venue": "arXiv preprint arXiv:2304.11477,",
            "year": 2023
        },
        {
            "authors": [
                "Shilong Liu",
                "Zhaoyang Zeng",
                "Tianhe Ren",
                "Feng Li",
                "Hao Zhang",
                "Jie Yang",
                "Chunyuan Li",
                "Jianwei Yang",
                "Hang Su",
                "Jun Zhu"
            ],
            "title": "Grounding dino: Marrying dino with grounded pre-training for open-set object detection",
            "venue": "arXiv preprint arXiv:2303.05499,",
            "year": 2023
        },
        {
            "authors": [
                "Saqib Mehmood",
                "Shakeel Ahmed",
                "Anders Schmidt Kristensen",
                "Dewan Ahsan"
            ],
            "title": "Multi criteria decision analysis (mcda) of unmanned aerial vehicles (uavs) as a part of standard response to emergencies",
            "venue": "In 4th International Conference on Green Computing and Engineering Technologies,",
            "year": 2018
        },
        {
            "authors": [
                "Luis Merino",
                "Fernando Caballero",
                "JR Martinez-de Dios",
                "An\u00edbal Ollero"
            ],
            "title": "Cooperative fire detection using unmanned aerial vehicles",
            "venue": "In Proceedings of the 2005 IEEE international conference on robotics and automation,",
            "year": 2005
        },
        {
            "authors": [
                "Anibal Ollero",
                "Simon Lacroix",
                "Luis Merino",
                "Jeremi Gancet",
                "Johan Wiklund",
                "Volker Remu\u00df",
                "Iker Veiga Perez",
                "Luis G Guti\u00e9rrez",
                "Domingos Xavier Viegas",
                "Miguel Angel Gonz\u00e1lez Benitez"
            ],
            "title": "Multiple eyes in the skies: architecture and perception issues in the comets unmanned air vehicles project",
            "venue": "IEEE robotics & automation magazine,",
            "year": 2005
        },
        {
            "authors": [
                "Long Ouyang",
                "Jeffrey Wu",
                "Xu Jiang",
                "Diogo Almeida",
                "Carroll Wainwright",
                "Pamela Mishkin",
                "Chong Zhang",
                "Sandhini Agarwal",
                "Katarina Slama",
                "Alex Ray"
            ],
            "title": "Training language models to follow instructions with human feedback",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Xavier Puig",
                "Kevin Ra",
                "Marko Boben",
                "Jiaman Li",
                "Tingwu Wang",
                "Sanja Fidler",
                "Antonio Torralba"
            ],
            "title": "Virtualhome: Simulating household activities via programs",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Lu\u00eds C Santos",
                "Filipe N Santos",
                "EJ Solteiro Pires",
                "Ant\u00f3nio Valente",
                "Pedro Costa",
                "Sandro Magalh\u00e3es"
            ],
            "title": "Path planning for ground robots in agriculture: A short review",
            "venue": "IEEE International Conference on Autonomous Robot Systems and Competitions (ICARSC),",
            "year": 2020
        },
        {
            "authors": [
                "Manolis Savva",
                "Angel X Chang",
                "Alexey Dosovitskiy",
                "Thomas Funkhouser",
                "Vladlen Koltun"
            ],
            "title": "Minos: Multimodal indoor simulator for navigation in complex environments",
            "venue": "arXiv preprint arXiv:1712.03931,",
            "year": 2017
        },
        {
            "authors": [
                "Manolis Savva",
                "Abhishek Kadian",
                "Oleksandr Maksymets",
                "Yili Zhao",
                "Erik Wijmans",
                "Bhavana Jain",
                "Julian Straub",
                "Jia Liu",
                "Vladlen Koltun",
                "Jitendra Malik"
            ],
            "title": "Habitat: A platform for embodied ai research",
            "venue": "In Proceedings of the IEEE/CVF international conference on computer vision,",
            "year": 2019
        },
        {
            "authors": [
                "John Schulman",
                "Filip Wolski",
                "Prafulla Dhariwal",
                "Alec Radford",
                "Oleg Klimov"
            ],
            "title": "Proximal policy optimization algorithms. CoRR, abs/1707.06347, 2017",
            "year": 2017
        },
        {
            "authors": [
                "Dhruv Shah",
                "B\u0142a\u017cej Osi\u0144ski",
                "Sergey Levine"
            ],
            "title": "Lm-nav: Robotic navigation with large pre-trained models of language, vision, and action",
            "venue": "In Conference on Robot Learning,",
            "year": 2023
        },
        {
            "authors": [
                "Hazim Shakhatreh",
                "Ahmad H Sawalmeh",
                "Ala Al-Fuqaha",
                "Zuochao Dou",
                "Eyad Almaita",
                "Issa Khalil",
                "Noor Shamsiah Othman",
                "Abdallah Khreishah",
                "Mohsen Guizani"
            ],
            "title": "Unmanned aerial vehicles (uavs): A survey on civil applications and key research challenges",
            "venue": "Ieee Access,",
            "year": 2019
        },
        {
            "authors": [
                "Bokui Shen",
                "Fei Xia",
                "Chengshu Li",
                "Roberto Mart\u00edn-Mart\u00edn",
                "Linxi Fan",
                "Guanzhi Wang",
                "Claudia P\u00e9rezD\u2019Arpino",
                "Shyamal Buch",
                "Sanjana Srivastava",
                "Lyne Tchapmi"
            ],
            "title": "igibson 1.0: A simulation environment for interactive tasks in large realistic scenes",
            "venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),",
            "year": 2021
        },
        {
            "authors": [
                "Chan Hee Song",
                "Jiaman Wu",
                "Clayton Washington",
                "Brian M Sadler",
                "Wei-Lun Chao",
                "Yu Su"
            ],
            "title": "Llm-planner: Few-shot grounded planning for embodied agents with large language models",
            "venue": "arXiv preprint arXiv:2212.04088,",
            "year": 2022
        },
        {
            "authors": [
                "Andrew Szot",
                "Alexander Clegg",
                "Eric Undersander",
                "Erik Wijmans",
                "Yili Zhao",
                "John Turner",
                "Noah Maestre",
                "Mustafa Mukadam",
                "Devendra Singh Chaplot",
                "Oleksandr Maksymets"
            ],
            "title": "Habitat 2.0: Training home assistants to rearrange their habitat",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Jian Tang",
                "Kejun Zhu",
                "Haixiang Guo",
                "Chengzhu Gong",
                "Can Liao",
                "Shuwen Zhang"
            ],
            "title": "Using auctionbased task allocation scheme for simulation optimization of search and rescue in disaster relief",
            "venue": "Simulation Modelling Practice and Theory,",
            "year": 2018
        },
        {
            "authors": [
                "Hugo Touvron",
                "Thibaut Lavril",
                "Gautier Izacard",
                "Xavier Martinet",
                "Marie-Anne Lachaux",
                "Timoth\u00e9e Lacroix",
                "Baptiste Rozi\u00e8re",
                "Naman Goyal",
                "Eric Hambro",
                "Faisal Azhar"
            ],
            "title": "Llama: Open and efficient foundation language models",
            "venue": "arXiv preprint arXiv:2302.13971,",
            "year": 2023
        },
        {
            "authors": [
                "Hugo Touvron",
                "Louis Martin",
                "Kevin Stone",
                "Peter Albert",
                "Amjad Almahairi",
                "Yasmine Babaei",
                "Nikolay Bashlykov",
                "Soumya Batra",
                "Prajjwal Bhargava",
                "Shruti Bhosale"
            ],
            "title": "Llama 2: Open foundation and fine-tuned chat models",
            "venue": "arXiv preprint arXiv:2307.09288,",
            "year": 2023
        },
        {
            "authors": [
                "Sai Vemprala",
                "Rogerio Bonatti",
                "Arthur Bucker",
                "Ashish Kapoor"
            ],
            "title": "Chatgpt for robotics: Design principles and model abilities",
            "venue": "Microsoft Auton. Syst. Robot. Res,",
            "year": 2023
        },
        {
            "authors": [
                "Naoki Wake",
                "Atsushi Kanehira",
                "Kazuhiro Sasabuchi",
                "Jun Takamatsu",
                "Katsushi Ikeuchi"
            ],
            "title": "Chatgpt empowered long-step robot control in various environments: A case application",
            "venue": "arXiv preprint arXiv:2304.03893,",
            "year": 2023
        },
        {
            "authors": [
                "Zihao Wang",
                "Shaofei Cai",
                "Anji Liu",
                "Xiaojian Ma",
                "Yitao Liang"
            ],
            "title": "Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Fanbo Xiang",
                "Yuzhe Qin",
                "Kaichun Mo",
                "Yikuan Xia",
                "Hao Zhu",
                "Fangchen Liu",
                "Minghua Liu",
                "Hanxiao Jiang",
                "Yifu Yuan",
                "He Wang"
            ],
            "title": "Sapien: A simulated part-based interactive environment",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Fanbo Xiang",
                "Yuzhe Qin",
                "Kaichun Mo",
                "Yikuan Xia",
                "Hao Zhu",
                "Fangchen Liu",
                "Minghua Liu",
                "Hanxiao Jiang",
                "Yifu Yuan",
                "He Wang"
            ],
            "title": "Sapien: A simulated part-based interactive environment",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Claudia Yan",
                "Dipendra Misra",
                "Andrew Bennnett",
                "Aaron Walsman",
                "Yonatan Bisk",
                "Yoav Artzi"
            ],
            "title": "Chalet: Cornell house agent learning environment",
            "venue": "arXiv preprint arXiv:1801.07357,",
            "year": 2018
        },
        {
            "authors": [
                "Sherry Yang",
                "Ofir Nachum",
                "Yilun Du",
                "Jason Wei",
                "Pieter Abbeel",
                "Dale Schuurmans"
            ],
            "title": "Foundation models for decision making: Problems, methods, and opportunities",
            "venue": "arXiv preprint arXiv:2303.04129,",
            "year": 2023
        },
        {
            "authors": [
                "SP Yeong",
                "LM King",
                "SS Dol"
            ],
            "title": "A review on marine search and rescue operations using unmanned aerial vehicles",
            "venue": "International Journal of Marine and Environmental Sciences,",
            "year": 2015
        },
        {
            "authors": [
                "Wu Yi",
                "Wu Yuxin",
                "Gkioxari Georgia",
                "Tian Yuandong"
            ],
            "title": "Building generalizable agents with a realistic and rich 3d environment, 2018",
            "venue": "URL https://openreview.net/forum?id= rkaT3zWCZ",
            "year": 2018
        },
        {
            "authors": [
                "Kuo-Hao Zeng",
                "Luca Weihs",
                "Roozbeh Mottaghi",
                "Ali Farhadi"
            ],
            "title": "Moving forward by moving backward: Embedding action impact over action semantics",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Zheng Zeng",
                "Lian Lian",
                "Karl Sammut",
                "Fangpo He",
                "Youhong Tang",
                "Andrew Lammas"
            ],
            "title": "A survey on path planning for persistent autonomy of autonomous underwater vehicles",
            "venue": "Ocean Engineering,",
            "year": 2015
        }
    ],
    "sections": [
        {
            "text": "Recent advances in high-fidelity virtual environments serve as one of the major driving forces for building intelligent embodied agents to perceive, reason and interact with the physical world. Typically, these environments remain unchanged unless agents interact with them. However, in real-world scenarios, agents might also face dynamically changing environments characterized by unexpected events and need to rapidly take action accordingly. To remedy this gap, we propose a new simulated embodied benchmark, called HAZARD, specifically designed to assess the decision-making abilities of embodied agents in dynamic situations. HAZARD consists of three unexpected disaster scenarios, including fire , flood , and wind , and specifically supports the utilization of large language models (LLMs) to assist common sense reasoning and decision-making. This benchmark enables us to evaluate autonomous agents\u2019 decision-making capabilities across various pipelines, including reinforcement learning (RL), rule-based, and search-based methods in dynamically changing environments. As a first step toward addressing this challenge using large language models, we further develop an LLM-based agent and perform an in-depth analysis of its promise and challenge of solving these challenging tasks. HAZARD is available at https://sites.google. com/view/hazard-challenge."
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "Embodied agents operate in a dynamic world that exhibits constant changes. This world experiences various changes at every moment, including the rising and setting of the sun, the flow of rivers, weather variations, and human activities. To successfully navigate and function in such an everchanging environment, robots are required to perceive changes in their surroundings, reason the underlying mechanisms of these changes, and subsequently make decisions in response to them.\nTo simulate a dynamic world, it is necessary to create environments that can spontaneously undergo changes. Currently, various simulation platforms have emerged in the field of embodied AI, including iGibson (Shen et al., 2021), Habitat (Savva et al., 2019), SAPIEN (Xiang et al., 2020b), VirtualHome (Puig et al., 2018), AI2THOR (Kolve et al., 2017), ThreeDWorld (TDW) (Gan et al.), etc. Existing tasks on these simulation platforms involve agent exploration and agent-driven interactions, but they lack support for environment-driven changes, which are rather influential and unpredictable. The iGibson 2.0 (Li et al.) platform partially supports spontaneous environmental changes to a limited extent, but these changes are limited to the propagation of a few variables between individual objects.\nIn this paper, we propose the HAZARD challenge, an innovative exploration of embodied decisionmaking in dynamic environments, by designing and implementing new capabilities for physical simulation and visual effects on top of the ThreeDWorld. HAZARD manifests itself in the form of unexpected disasters, such as fires, floods, and wild winds, and requires agents to rescue valuable items from these continuously evolving and perilous circumstances.\nThe HAZARD challenge places agents within indoor or outdoor environments, compelling them to decipher disaster dynamics and construct an optimal rescue strategy. As illustrated in Figure 1, the scenarios vary in severity and complexity. An indoor fire scenario might involve the rapid spread of flames, threatening flammable target objects. In an indoor flood scenario, an overwhelming volume of water inundates the house, jeopardizing non-waterproof targets. In an outdoor wind scenario, strong winds scatter lightweight objects across roads, making retrieval a challenging task for agents. To\nscenarios: fire , flood , and wind . In the fire scenario, flames continuously spread and burn objects. In the flood scenario, water spreads and rises, washing away objects and causing damage to non-waterproof objects. The wind scenario poses the challenge of objects being blown away, making them hard to reach. These scenarios present embodied agents with complex perception, reasoning, and planning challenges.\nsuccessfully rescue target objects from these disasters, agents must effectively transfer them to safe zones such as backpacks or shopping carts.\nTo facilitate this endeavor, we introduce a comprehensive benchmark comprising these disaster scenarios, complete with quantitative evaluation metrics. We also provide an API to employ large language models (LLMs) for action selection. This API integrates visual observations and historical memories into textual descriptions, thereby providing a semantic understanding of the dynamic environment. To optimize the use of LLMs, we compress a large volume of low-level actions by A* algorithm, significantly reducing the frequency of LLM queries.\nWe evaluate both LLM-based agents and several other decision-making pipelines on our benchmark, including a rule-based pipeline that operates based on a simple set of rules, a search-based pipeline that utilizes the Monte Carlo tree search (MCTS) algorithm for action selection, and a reinforcement learning-based pipeline. Through our experiments, we find while the LLM pipeline is capable of understanding and considering certain basic factors, such as object distance, it may encounter challenges in comprehending and effectively handling more complex factors, such as the dynamic nature of environmental changes.\nThe main contributions of our work are: 1) designing and implementing a new feature that enables the simulation of complex fire, flood, and wind effects for both indoor and outdoor virtual environments in TDW; 2) developing a comprehensive benchmark, HAZARD, for evaluating embodied decisionmaking in dynamically changing environments, as well as incorporating the LLM API into our benchmark; and 3) conducting an in-depth analysis of the challenges posed by perception and reasoning for existing methods, especially LLM-based agents in tackling the proposed benchmark."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Simulators for Embodied AI The recent advance of embodied AI has largely been driven by the development of simulation platforms. While earlier platforms primarily focused on supporting agent exploration (Savva et al., 2017; Beattie et al., 2016; Savva et al., 2019; Yi et al., 2018; Das et al., 2018), recent platforms (Gan et al.; Xiang et al., 2020a; Shen et al., 2021; Szot et al., 2021; Li et al.; Puig et al., 2018; Kolve et al., 2017; Yan et al., 2018) have advanced by enabling physical\nagent-driven interactions. In this paper, we specifically focus on the impact of environment-driven changes on embodied agents, which remains a relatively unexplored area of research. Earlier works either supported spontaneous changes in the environment within limited ranges (Li et al.), different environmental impacts on agent actions (Zeng et al., 2022), or just focused on identifying such changes, which occurred only during each reset (Landi et al., 2022).\nEmbodied AI with Large Language Models Recently, large language models (LLMs) (Brown et al., 2020; Chowdhery et al., 2022; Touvron et al., 2023a; Ouyang et al., 2022) have made remarkable strides in the field of AI, and their potential in embodied AI tasks has also been widely investigated and explored (Kant et al., 2022; Wake et al., 2023; Shah et al., 2023; Vemprala et al., 2023; Lin et al., 2023; Yang et al., 2023; Liu et al., 2023a). LLMs are capable of providing contextual information (Ahn et al., 2022), building inner monologues (Huang et al., 2022), providing model initializing weights (Li et al., 2022), and error correction (Wang et al., 2023) to enhance the planning of embodied agents. More directly, LLMs can generate policy code (Liang et al., 2022) or produce plans for embodied agents (Song et al., 2022; Driess et al., 2023). Different from previous works, we explore the planning and decision-making ability of LLMs in dynamically changing environments. We also implement APIs for LLM-based agents in HAZARD, providing support for future research in this area.\nSearch and Rescue Search and rescue (SAR) is a widely explored area for robotics. Most of the existing projects and programs in robot SAR focus on using unmanned aerial vehicles (Ollero et al., 2005; Mehmood et al., 2018; Merino et al., 2005; Hayat et al., 2016; Shakhatreh et al., 2019; Yeong et al., 2015), unmanned ground vehicles (Chikwanha et al., 2012; Santos et al., 2020; Cubber et al., 2017), and unmanned underwater vehicles (Binney et al., 2010; Zeng et al., 2015; Li et al., 2018) for searching one or more target in mostly static scenes. Some previous works also provide a dynamic simulation of disasters but are restricted to 2D abstract environments (rob, 2023) or limited scenes (Tang et al., 2018). Different from these works, we provide simulation for dynamically changing environments in embodied scenes and focus on decision-making in these environments."
        },
        {
            "heading": "3 THE HAZARD CHALLENGE",
            "text": ""
        },
        {
            "heading": "3.1 OVERVIEW",
            "text": "The HAZARD challenge sets itself apart through its distinctive dynamic scenarios, requiring highquality physical and visual simulation of significant environmental changes such as fire, flood, and wind. To realize our objectives, we have developed an environment change simulation framework, premised on the robust foundation of the ThreeDWorld platform. This framework offers a reliable physical simulation system, with a versatile renderer capable of producing visual effects tailored to each type of environmental change. In this section, we delve into the implementation details of both how we simulate these dynamic environment changes and our new dataset."
        },
        {
            "heading": "3.2 SCENES",
            "text": ""
        },
        {
            "heading": "3.2.1 FIRE",
            "text": "In the fire scenario, we simulate an indoor scene of a room on fire. This dynamic environment presents various changes to the agent, including the spreading visual effects of the fire, changes in\ntemperature, and the transformation of objects as they burn or become burnt. To capture these effects accurately, we implement a temperature system and integrate it with the visual effects using the ThreeDWorld simulator.\nTemperature The temperature system simulates the heat transfer process among all objects in the environment. According to the second law of thermodynamics, in this system, heat is transferred from objects with higher temperatures to those with lower temperatures. However, it is infeasible to simulate real heat transfer to a fine-grained level due to computational limits. Instead, we adopt an expedient approach described below.\nFor each object o in each time frame, we update the temperature T 1poq \u201c T poq \u00a8 p1\u00b4 dq ` d \u00a8Tenvpoq where d is the decay rate. Tenvpoq simulates the heat transfer from the environment and is defined as the weighted average of room temperature Troom and surrounding objects T po1q. The weight for Troom is a pre-defined constant Wroom and the weight for T po1q is Wo1 \u201c minpD\u00b42, distpo, o1q\u00b42q where D is the distance threshold and distpo, o1q is the distance between objects o and o1. In summary, we have pre-defined constants Wr, Troom, decay rate d and distance threshold D to simulate real-life heat transfer between objects.\nNot only the objects can catch on fire. In our setting, we divide the floor into 2-D grids where each grid can burn separately. Since the grid size is set small, it\u2019s infeasible to assign a temperature to each grid and update them every frame. Instead, we use a spreading model where a fire having burnt for time t has probability pptq to spread to a nearby grid. pptq is a linear function so that after sufficient time the fire must spread.\nObject status and visual effect To simulate the visual effects generated by a spreading fire, we define three distinct object statuses similar to (Li et al.): normal, burning, and burnt. Objects in the normal status exhibit no additional visual effects. An object becomes burning once its temperature reaches the ignition point. As illustrated in Figure 2, burning objects are adorned with a fire visual effect on the top, with the scale of this visual effect gradually amplifying as the object combusts over subsequent frames. After a designated burning duration, an object becomes burnt and has a black hue to represent the burnt state."
        },
        {
            "heading": "3.2.2 FLOOD",
            "text": "The flood scenario is designed to simulate the spread and rise of water within an indoor room. This scene introduces the following changes to the agent: (a). The flood gradually submerges the objects within the scene, making them challenging to recognize and causing damage to non-waterproof objects, and (b). objects with low density have the tendency to float on the flood surface and may be swept away by the force of the flood.\nVisual effect As Figure 2 shows, the flood surface is designed to be translucent, allowing the agent to perceive both the flood surface itself and the objects located beneath it. To simulate the spread of the flood, we rotate the surface into a sloped position and make it gradually rise and move forward. This combination of visual effects accurately depicts the dynamic nature of a spreading flood.\nPhysical simulation For each object in the flood, we incorporate two forces to simulate the physical effects of the flood: buoyancy and drag force. The buoyancy force, denoted as FB , acts in the opposite direction to gravity and its magnitude is determined by FB \u201c \u03c1fV g, where V denotes the product of the volume of the submerged portion of the object, g denotes the gravitational acceleration, and \u03c1f denotes the density of the flood. On the other hand, the drag force, denoted as FD, is calculated using the drag equation FD \u201c 1\n2 \u03c1fv 2CDA, where v represents the relative velocity of the object with respect to the fluid, CD denotes the drag coefficient, and A represents the vertical area of the object. The direction of FD is opposite to the relative speed of the object in relation to the fluid."
        },
        {
            "heading": "3.2.3 WIND",
            "text": "In contrast to the fire and flood scenarios, the wind scenario simulates an outdoor scene where objects are affected by intense and turbulent winds. As a result, the primary dynamic feature of this scenario is the movement of objects induced by powerful wind forces. In real life, determining wind forces is a complex topic in aerodynamics, so we take an ideal model. Specifically, we assume the wind has a fixed velocity everywhere and objects have a face vertical to this velocity. With some physics\nderivation, The force is F \u201c \u03c1av2A where \u03c1a is the air density, v is the relative wind velocity and A is the vertical area facing the wind. We modify it by setting F \u201c F1 ` F2 in which F1 \u201c \u03c1av2A and F2 \u201c r \u02c6 F1. r is a random vector of fixed length so that F2 simulates a random turbulence minor compared to F1. We also apply a random torque to each object to account for forces applied not on the center of mass. The magnitude of forces and torques is hand-adjusted to create a dynamically changing scene.\nTo reduce the difficulty of the HAZARD challenge, environmental impacts like temperature, flood forces, and wind forces do not affect agents in the default setting. Additionally, we explore a more challenging scenario where agents are influenced by these environmental effects in Appendix B."
        },
        {
            "heading": "3.2.4 PROCEDURAL SCENE GENERATION",
            "text": "Based on the ProcGenKitchen pipeline from ThreeDWorld simulator, we develop a procedural generation pipeline tailored for the HAZARD challenge. This pipeline enables the generation of diverse scenes with dynamic changes. Initially, a basic scene is generated using ProcGenKitchen. Subsequently, we introduce additional elements to this basic scene in a random manner, including (a). target objects and additional objects on both floor and surfaces of the existing objects, such as tables and chairs, (b). agents\u2019 initial positions and directions, and (c). initial positions of the fire sources. This procedural generation pipeline ensures the creation of various dynamically changing scenes for the HAZARD challenge."
        },
        {
            "heading": "3.3 BENCHMARK DETAILS",
            "text": "Problem definition In the HAZARD challenge, the objective for an embodied agent is to rescue a predetermined set of objects, denoted as target objects, and bring them to a given safe location such as a bag held by the agent or a shopping cart. As Figure 3 shows, at each step, an agent receives an RGB-D observation, semantic segmentations of the observation, and a blurred environment-specific observation (temperature in the fire scenario and water level in the flood scenario). We also provide a perceptional version of HAZARD which excludes semantic segmentation from inputs. Given these observations, agents must make appropriate choices from the available agent action space at each step to accomplish the mission.\nTarget objects The objects that agents are required to rescue are denoted as \u201ctarget objects.\" These objects are randomly placed on the floor or other surfaces within the environment. In each fire or flood scene, we randomly select 4 categories of objects from a universal target category pool, which consists of 22 object categories for fire or flood scenarios, and 11 object categories for wind scenarios. All objects falling within these selected categories are considered target objects. The\ntarget category pool encompasses objects that exhibit diversity across four attributes: object value, waterproof capability, ignition point, and susceptibility to the wind.\nAgent action space In our study, we utilize ThreeDWorld Replicant, a humanoid agent with basic actions such as Move Forward By, Turn By, Reach For, and Reset Arm. To accomplish the proposed task, we introduced compressed actions derived from these foundational actions. Specifically, the Explore action combines several Turn By actions, enabling agents to swiftly perceive their environment. A robust Pick Up action, formed by Reach For and Reset Arm actions, allows the agent to grasp target objects effectively. Additionally, a Drop action helps the agent put down held objects. Therefore, once the agent reaches the object, it can utilize the Pick Up action to hold the object and subsequently use the Drop action to position it in a safe location. HAZARD also supports direct utilization of low-level actions, including \u2018move_by\u2019, \u2018turn_by\u2019, and \u2018turn_to\u2019.\nSupport for LLM-based pipelines The proposed benchmark also supports using LLMs as decisionmakers. However, a significant challenge arises when querying the LLM for actions at each step, as it can lead to frequent queries and subsequently lower inference speed. To solve this problem, we implement agent navigation to objects using the A* algorithm. This navigation algorithm compresses a large number of Move Forward By and Turn By actions into a single Walk To action. As a result, apart from the Pick Up and Drop actions, the LLM is only queried to select which objects to Walk To. Based on this design, we implement efficient APIs for LLM-based pipelines in HAZARD.\nEvaluation metrics For HAZARD challenge, we use rescued value rate (Value), averaged rescue step (Step), and averaged damaged rate (Damage) as evaluation metrics. The rescued value rate is calculated as the ratio of the total value of the rescued objects to the total initial value of all target objects. Objects that are damaged due to environmental changes will lose half of their value. Specifically, in the fire scenario, objects lose their value once they start burning. In the flood scenario, a non-waterproof object loses its value if it becomes submerged by the flood. The averaged rescue step, as a measurement of efficiency, is defined as the averaged step number to rescue an object for an agent. To improve the efficiency of the evaluation process, we set a time limit of 1,500 frames for tasks fire and flood while a longer 3,000 frames is set in the wind scenario. The averaged damaged rate (Damage) is calculated as the proportion of damaged objects among the objects that the agent rescued. Note that objects can only be damaged in a fire or flood scenario."
        },
        {
            "heading": "4 BUILDING LLM-BASED PIPELINE FOR EMBODIED AGENTS",
            "text": "Taking inspiration from recent research (Song et al., 2022; Driess et al., 2023), we employ an LLM as the decision maker to determine the target destination for embodied agents. As illustrated in Figure 4, to enable the perception of LLM, we convert the information from the environment into text descriptions. Then LLM is required to select a proper action, which is subsequently converted into multiple low-level actions for execution. We provide the LLM decision maker in this step with the following information:\n\u2022 Task description: The first part of the prompt consists of a manually designed description of the current task. The description briefly introduces the challenges the disaster agent needs to face, the overall goal of the agent, and the format of the following prompt parts.\n\u2022 Target information: After the task description, we provide target information for LLMs, including the names, values, and properties of target objects.\n\u2022 Current state: To provide LLM with comprehensive information about the current state, we convert visual observations, thermal or water-level signals, and semantic masks into a 2D semantic map. For the perceptional version of HAZARD, input semantic mask is replaced with a segmentation proposal provided by a perception model. Then the LLM is provided with a textual description of the semantic map, including object distance, temperature, water level, and value.\n\u2022 Observation memory: To help LLMs infer the dynamics of the environment and predict future changes, we have designed an observation memory to store the historical states. During inference, LLMs utilize the observation memory by incorporating the description of each historical state into the prompt.\n\u2022 Available actions: The prompt concludes with a list of the currently available actions. Therefore, LLMs make decisions by solving a multi-choice problem.\n\u2022 Other information: The prompt also includes other necessary information, such as the agent\u2019s history of actions taken and its current status.\nThe detailed format of the prompt can be referred to as shown in Figure 7 in the Appendix."
        },
        {
            "heading": "5 EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "5.1 EXPERIMENTAL SETUP",
            "text": "Traning and testing setup To create the dataset for HAZARD, we choose 4 distinct indoor rooms for the fire and flood tasks, and 4 outdoor regions for the wind task. Within each room or region, we generate 25 diverse scenes using our procedural generation pipeline (described in Section 3.2.4). One indoor room and one outdoor region are selected as the test set. As a result, we obtain a total of 100 unique scenes for each task, with a train-set split ratio of 3:1.\nDetails of language model backbone We evaluate the LLM-based agent in Section 4 with 3 different LLM backbones, including Llama-13b-chat model (Touvron et al., 2023b), OpenAI GPT-3.5-turbo (August 3 Version), and OpenAI GPT-4. We use max tokens of 512, temperature of 0.7, top p of 1.0 as hyper-parameters during inference.\nPerception Module To get semantic segmentations in the perceptional version of HAZARD, we use OpenMMLab detection framework (Chen et al., 2019) to implement our perceptional model. We collect 200 images with ground truth segmentation in each training instance and use the collected data to fine-tune a Mask-RCNN (He et al., 2017) model provided by OpenMMLab."
        },
        {
            "heading": "5.2 BASELINES",
            "text": "We implement several baseline agents for evaluations as follows.\nRandom agent An agent randomly selects low-level actions to execute. To emphasize the hardness of our challenge, we provide more informative actions including walking to the nearest target object (container), picking up (dropping) the nearest object and exploring.\nRL model We also trained reinforcement learning models using Proximal Policy Optimization (PPO) (Schulman et al., 2017). The actions are the same as described in the random agent. We design the function that rewards picking up and dropping correctly while penalizing actions that fail or have no effect. To make the reward function smoother, we also add a factor of the distance from the agent to the nearest object.\nRule-based agent An agent randomly chooses a target object to rescue. After selecting the target object, the agent automatically walks to the object, picks it up, and drops it into a safe place.\nMCTS agent Monte Carlo Tree Search (MCTS) (Kocsis & Szepesv\u00e1ri, 2006) is a commonly used algorithm in decision-making problems, which has an effective balance of exploration and\nexploitation. Since it is hard to get the ground truth frame costs of each action, we design several kinds of heuristic costs for MCTS, such as navigation heuristics, grasp heuristics, drop heuristics, and exploration heuristics. After that, we use MCTS to find an action plan with minimal total cost.\nGreedy agent A simple greedy agent that persists in rescuing the nearest target object with the lowest cost. This agent chooses actions randomly when there are no target objects in observation or memory."
        },
        {
            "heading": "5.3 EXPERIMENTAL RESULTS",
            "text": "Quantitative results According to the quantitative results in Table 1, the proposed HAZARD benchmark presents a significant challenge, as all the Random, Rule, and Greedy methods exhibit poor performance across all three scenarios. The MCTS method inherently reasons the environment changes through simulation and therefore performs the best among baseline methods. Surprisingly, although not finetuned on the training data, the LLM pipeline demonstrates superior performance compared to most baseline methods on three scenarios, showing its strong zero-shot decision-making capabilities. Furtherly, the results indicate a clear difference in decision making ability among different LLMs, as the GPT-4 model outperforms both GPT-3.5-turbo model and LLaMa-13b chat model by a large margin. Notably, in the wind scenario, all methods struggle with a remarkably low Value score, highlighting the difficulty of reasoning the movements of objects.\nPerceptional results According to Table 1, all methods show reduced performance in the with perception scenario, highlighting the challenges of perception in dynamic environments. The perception model struggles to detect objects submerged in water or obscured by flames. Interestingly, in wind scenarios, this perceptual difficulty can be beneficial, as agents stop pursuing objects blown far away. Notably, despite these challenges, LLM-based agents still demonstrate competitive and robust decision-making ability.\nQualitative results As illustrated in Figure 5, the LLM pipeline shows the ability to take into account basic attributes during decision-making processes, enabling it to make rational choices in some cases. For instance, in the fire scenario, the LLM pipeline demonstrates comprehensive consideration of object temperature, distance, and value in the reasoning path. Accordingly, the LLM finally selects a valuable target with a low temperature, which is an optimal choice in this situation.\nFailure cases In Figure 6, we provide two failure cases of the LLM pipeline. In the first case on the left, the LLM struggles with effectively considering dynamics during decision-making. The LLM pipeline chooses to walk towards the closest target, the backpack object. However, the object is swiftly carried away by the wind, leading to the failure of the LLM in catching up with the backpack object. In the second case on the right, the LLM pipeline suffers from inconsistency between its reasoning and prediction. Despite the thought process indicating that the optimal choice is to search for other target objects, which leads to hairbrush with id 66, it ultimately fails to select the desired target as its final decision."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "We introduce HAZARD, a novel challenge with dynamically changing environments. To support environment changes, we develop a simulation system on top of the ThreeDWorld platform. This system includes a physical simulator and a visual effect generator, enabling simulations of fire, flood, and wind scenarios. Leveraging this framework, we design an object rescue task for embodied agents and generate a dataset for this task. Subsequently, we evaluate and analyze the performance of large language model (LLM) agents and existing baseline methods using the generated dataset. However, the HAZARD challenge focuses only on object rescue. In the future, we will introduce more actions to the simulator to allow agents to mitigate environmental changes (e.g., using an extinguisher to put out fires).\nREPRODUCIBILITY STATEMENT\nFor readers interested in reproducing the experimental results presented in this paper, we have made our experiments accessible via an anonymous Github repository, available at https:// anonymous.4open.science/r/HAZARD-challenge/. For implementation details, please refer to the documentation within the repository."
        },
        {
            "heading": "A EXPERIMENT DETAILS",
            "text": "Navigation An integrated navigation module based on perception and A\u02da planning is provided to all agents. The module divides the world into grids of size 0.25 and calculates the maximum height of objects inside each grid using RGB-D observation. It then assigns weights to each grid according to the exponential of height. Finally, it runs an A\u02da path planning algorithm on the grids and controls agents to walk to the path midpoints sequentially. The module also provides a \u2018walk one step\u2019 option, in which the agent only walks to the first midpoint.\nPerception model details Since we used RCNN-based perception models, we map each detected instance to a ground truth object index during testing. For this purpose, we find the ground truth object in the same category as the detected instance and has the most overlapping bounding box with that instance, and then assign its index to the instance. Since we need to make sure the same instance is mapped to the same index across different frames, we have to use some of the ground truth information here. However, we keep GT information usage minimal.\nRandom Agent We implemented an agent that does random actions chosen from:\n\u2022 Walk one step closer to the nearest target object.\n\u2022 Walk one step closer to the nearest container (available in \u2018wind\u2019 scene).\n\u2022 Pick up the nearest target object.\n\u2022 Drop the object in hand into the nearest container in \u2018wind\u2019 scene or into the grasped container on another hand in other scenes.\n\u2022 Turn around and explore.\n\u2022 Walk to a random object in sight.\nReinforcement Learning Our RL agent uses the same set of actions as the random agent. We design the cumulative reward as the sum of the following terms\n\u2022 For each object retrieved, add 20 to the reward.\n\u2022 If the agent is holding something, add \u00b410 to the reward. This penalizes the agent for holding an object for too long.\n\u2022 The distance from the agent to the nearest target or container, depending on whether it is grasping a target.\n\u2022 For each action done, add \u00b40.1 to the reward. For invalid actions, add \u00b45 instead.\nThe agent is given a map of 4 \u02c6 W \u02c6 H where W and H are the sizes of the grid map set in advance. One channel contains binary information denoting if each grid is explored, while another contains height information as in navigation. We also provide the object id in each grid (if any) in the third channel and agent information in the last channel. For fire and flood scenes respectively, we additionally provide the temperature or water level in a separate channel.\nWe use the PPO algorithm with learning rate 2.5 \u02c6 10\u00b44 and train for 105 steps. We did not run a typical 106 steps for two reasons: low sample frequency due to physics simulation, and the model collapsing to doing only the explore action after sufficient steps. We use an early-stopped training result of our RL agent to do our evaluation.\nRule-based Agent Another set of high-level motions is used in all agents except Random and RL, which is outlined as follows:\n\u2022 walk to nearest: The agent moves towards the nearest object, with the heuristic cost equating to the distance required to reach the destination;\n\u2022 explore: This action involves the agent exploring its surroundings to discover more objects, and the heuristic cost for this action is a constant;\n\u2022 pick up nearest: When an agent is near an object, he can perform the action to pick the nearest object up;\n\u2022 drop: The agent can drop the object in hand to a container or ground;\nOur rule-based agent first decides all available actions from the objects visible from the observation or in memory. It then randomly selects a target object, walks to it, picks the target up, possibly walks to a random container, and drops it. This process is repeated until the step threshold is reached or no target object is visible. In the latter case, the agent does a \u2018look around\u2019 action and tries to select a target again.\nMCTS Agent The MCTS agent applies the Monte Carlo Tree Search (MCTS) strategy for action planning and decision-making to rescue objects. It can memorize all the objects it has seen and simulate the object status transition in its mind before making a decision. When faced with a decision, the agent identifies a plan that results in the lowest heuristic cost to complete the task at hand. The first action of this plan is then selected as the current action. In detail, in each step, the agent will simulate 2000 times. To consider object properties and historical information, the MCTS agent retains past observations of objects, enabling linear predictions of their status and value in the next\nstep. Subsequently, these object value predictions are incorporated into the heuristic cost calculation employed by MCTS.\nFor each decision in MCTS, the score Qps, aq and policy \u03c0ps, aq for each action a and current state s is:\nQMCT ps, aq \u201c Qps, aq ` cpnpsqq a npsq 1 ` nps, aq ; (1)\n\u03c0MCT psq \u201c argmaxaQUCT ps, aq. (2)\nwhere npsq, nps, aq is the visiting times for state s and action a at state s respectively, Qps, aq is the mean value of action a at state s, cpnpsqq is a function related to npsq:\ncpxq \u201c log \u02c6 1 ` x ` c0 c0 \u02d9 ` c1, (3)\nHere, c0 \u201c 106 and c1 \u201c 0.1.\nGreedy Agent We modify the selection of our rule-based agent to create the greedy agent. Instead of randomly choosing a target, it chooses the target or container that has the lowest heuristic cost calculated in the same way as the MCTS agent, to the best of the agent\u2019s knowledge (memory and observation). If no target object is available, we also let the agent look around.\nLLM-based Agent In Figure 7, we illustrate an examplar input prompt for the LLM-based agent. This prompt begins with an outline of the task description, prompt format, and agent\u2019s objective. The Target objects section enumerates the names, values, and attributes of the target objects. Current state depicts objects the agent holds, which is empty in this example as the agent currently holds nothing. The sections, Target objects currently seen and Target objects previously seen, represent target objects in the present and past semantic maps, respectively. Previous actions records prior agent actions, while Objects states history includes observed object states over time. The prompt concludes with a list of possible actions.\nComputational resources used We run most of our experiments on an Intel i9-9900k CPU and RTX2080-Super GPU Desktop. Each trial takes no more than 15 minutes, except for LLM-based agents where most of the time is spent on API calls. Our RL training took 20 GPU hours for each scene."
        },
        {
            "heading": "B EXPLORING THE IMPACT OF ENVIRONMENTAL EFFECTS ON AGENTS",
            "text": "In the default setting of HAZARD, agents are not affected by environmental hazards, including fire, flood, and wind. To investigate how agents performs when they are impacted by environmental hazards, we provide an additional setting supported by HAZARD as follows:\n\u2022 Fire: In the fire scene, the agent has its own temperature affected by fires. By limiting the agent\u2019s temperature upper bound, we keep the agent away from regions occupied by fire.\n\u2022 Flood and Wind: In flood and wind scenes, agents are affected by flood or wind forces. These forces slow down the agents\u2019 actions when agents are moving against the direction of the flood or wind.\nTable 2 shows the results in wind and flood scenes when baseline agents are influenced by environmental forces. Most of baseline methods have a minor decline in performance. From these results, we can infer that the impact of environmental factors marginally increases the difficulty level in both flood and wind scenes. The change in fire scenes requires all baseline methods to be modified, as fire blocking the way will greatly affect our path planning and decision making components. Therefore, we leave this part of experiments as future work."
        },
        {
            "heading": "C ADDITIONAL TEST SETS",
            "text": "To assess the generalization capabilities of embodied agents more comprehensively, we have introduced the following two additional test sets:\n1. Test Set with New Objects: This test set includes target objects that were not present in the training set.\n2. Test Set with Larger Rooms: The rooms in this test set are larger than those in the training set.\nFor the test set with new objects, the \u2018without perception\u2019 version of HAZARD does not present additional challenges, as agents are provided with ground truth labels. In this section, we propose a method to tackle the test set with new objects in the \u2018with perception\u2019 version of HAZARD. We have replaced the R-CNN perception module with Grounded-SAM (Kirillov et al., 2023; Liu et al., 2023b) to generate segmentations for scenes containing new objects. As illustrated in Table 3, this powerful perception module helps all baseline models to retain most of their effectiveness when encountering new objects."
        },
        {
            "heading": "D TRAINING DEMONSTRATIONS GENERATION",
            "text": "To enable the training of imitation learning methods, we provide demonstrations on training sets using an oracle planner with access to complete ground truth information. After obtaining the ground truth information for all time steps, the oracle planner traverses all possible rescue plans and identifies the one with the highest value. If multiple plans have the same value, the plan with the fewest time steps is selected. However, this planner is imperfect because it assumes successful action execution (e.g., unobstructed navigation)."
        }
    ],
    "title": "HAZARD CHALLENGE: EMBODIED DECISION MAK-",
    "year": 2023
}