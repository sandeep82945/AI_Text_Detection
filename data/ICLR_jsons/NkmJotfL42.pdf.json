{
    "abstractText": "We study the notion of a generalization bound being uniformly tight, meaning that the difference between the bound and the population loss is small for all learning algorithms and all population distributions. Numerous generalization bounds have been proposed in the literature as potential explanations for the ability of neural networks to generalize in the overparameterized setting. However, in their paper \u201cFantastic Generalization Measures and Where to Find Them,\u201d Jiang et al. (2020) examine more than a dozen generalization bounds, and show empirically that none of them are uniformly tight. This raises the question of whether uniformly-tight generalization bounds are at all possible in the overparameterized setting. We consider two types of generalization bounds: (1) bounds that may depend on the training set and the learned hypothesis (e.g., margin bounds). We prove mathematically that no such bound can be uniformly tight in the overparameterized setting; (2) bounds that may in addition also depend on the learning algorithm (e.g., stability bounds). For these bounds, we show a trade-off between the algorithm\u2019s performance and the bound\u2019s tightness. Namely, if the algorithm achieves good accuracy on certain distributions, then no generalization bound can be uniformly tight for it in the overparameterized setting. We explain how these formal results can, in our view, inform research on generalization bounds for neural networks, while stressing that other interpretations of these results are also possible.",
    "authors": [],
    "id": "SP:0856c4005fdadad2ab481daabd1206252d2f874a",
    "references": [
        {
            "authors": [
                "Emmanuel Abbe",
                "Colin Sandon"
            ],
            "title": "On the universality of deep learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Omer Angel",
                "Yinon Spinka"
            ],
            "title": "Pairwise optimal coupling of multiple random variables, 2021",
            "year": 2021
        },
        {
            "authors": [
                "Sanjeev Arora",
                "Rong Ge",
                "Behnam Neyshabur",
                "Yi Zhang"
            ],
            "title": "Stronger generalization bounds for deep nets via a compression approach",
            "venue": "Proceedings of the 35th International Conference on Machine Learning,",
            "year": 2018
        },
        {
            "authors": [
                "Peter L Bartlett",
                "Philip M Long"
            ],
            "title": "Failures of model-dependent generalization bounds for leastnorm interpolation",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2021
        },
        {
            "authors": [
                "Peter L Bartlett",
                "Shahar Mendelson"
            ],
            "title": "Rademacher and gaussian complexities: Risk bounds and structural results",
            "venue": "Journal of Machine Learning Research,",
            "year": 2002
        },
        {
            "authors": [
                "Peter L. Bartlett",
                "Vitaly Maiorov",
                "Ron Meir"
            ],
            "title": "Almost linear vc-dimension bounds for piecewise polynomial networks",
            "venue": "Neural Comput.,",
            "year": 1998
        },
        {
            "authors": [
                "Peter L Bartlett",
                "Dylan J Foster",
                "Matus J Telgarsky"
            ],
            "title": "Spectrally-normalized margin bounds for neural networks",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Peter L. Bartlett",
                "Nick Harvey",
                "Christopher Liaw",
                "Abbas Mehrabian"
            ],
            "title": "Nearly-tight vc-dimension and pseudodimension bounds for piecewise linear neural networks",
            "venue": "Journal of Machine Learning Research,",
            "year": 2019
        },
        {
            "authors": [
                "Raef Bassily",
                "Shay Moran",
                "Ido Nachum",
                "Jonathan Shafer",
                "Amir Yehudayoff"
            ],
            "title": "Learners that use little information",
            "venue": "In Algorithmic Learning Theory,",
            "year": 2018
        },
        {
            "authors": [
                "Ian F Blake",
                "Chris Studholme"
            ],
            "title": "Properties of random matrices and applications",
            "venue": "Unpublished report available at https://www.cs.toronto.edu/ \u0303cvs/coding/random_report. pdf,",
            "year": 2006
        },
        {
            "authors": [
                "Gintare Karolina Dziugaite",
                "Daniel M. Roy"
            ],
            "title": "Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data",
            "venue": "CoRR, abs/1703.11008,",
            "year": 2017
        },
        {
            "authors": [
                "Gintare Karolina Dziugaite",
                "Alexandre Drouin",
                "Brady Neal",
                "Nitarshan Rajkumar",
                "Ethan Caballero",
                "Linbo Wang",
                "Ioannis Mitliagkas",
                "Daniel M Roy"
            ],
            "title": "In search of robust measures of generalization",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Gintare Karolina Dziugaite",
                "Kyle Hsu",
                "Waseem Gharbieh",
                "Gabriel Arpino",
                "Daniel Roy"
            ],
            "title": "On the role of data in pac-bayes bounds",
            "venue": "In International Conference on Artificial Intelligence and Statistics,",
            "year": 2021
        },
        {
            "authors": [
                "Noah Golowich",
                "Alexander Rakhlin",
                "Ohad Shamir"
            ],
            "title": "Size-independent sample complexity of neural networks",
            "venue": "Proceedings of the 31st Conference On Learning Theory,",
            "year": 2018
        },
        {
            "authors": [
                "Mahdi Haghifam",
                "Shay Moran",
                "Daniel M. Roy",
                "Gintare Karolina Dziugiate"
            ],
            "title": "Understanding generalization via leave-one-out conditional mutual information",
            "venue": "IEEE International Symposium on Information Theory (ISIT),",
            "year": 2022
        },
        {
            "authors": [
                "Moritz Hardt",
                "Ben Recht",
                "Yoram Singer"
            ],
            "title": "Train faster, generalize better: Stability of stochastic gradient descent",
            "venue": "Proceedings of the 33nd International Conference on Machine Learning,",
            "year": 2016
        },
        {
            "authors": [
                "Hrayr Harutyunyan",
                "Maxim Raginsky",
                "Greg Ver Steeg",
                "Aram Galstyan"
            ],
            "title": "Information-theoretic generalization bounds for black-box learning algorithms",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Fredrik Hellstr\u00f6m",
                "Giuseppe Durisi"
            ],
            "title": "A new family of generalization bounds using samplewise evaluated cmi",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Yiding Jiang",
                "Behnam Neyshabur",
                "Hossein Mobahi",
                "Dilip Krishnan",
                "Samy Bengio"
            ],
            "title": "Fantastic generalization measures and where to find them",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Nitish Shirish Keskar",
                "Dheevatsa Mudigere",
                "Jorge Nocedal",
                "Mikhail Smelyanskiy",
                "Ping Tak Peter Tang"
            ],
            "title": "On large-batch training for deep learning: Generalization gap and sharp minima",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Tengyuan Liang",
                "Tomaso Poggio",
                "Alexander Rakhlin",
                "James Stokes"
            ],
            "title": "Fisher-rao metric, geometry, and complexity of neural networks",
            "venue": "arXiv preprint arXiv:1711.01530,",
            "year": 2017
        },
        {
            "authors": [
                "Tengyuan Liang",
                "Tomaso A. Poggio",
                "Alexander Rakhlin",
                "James Stokes"
            ],
            "title": "Fisher-rao metric, geometry, and complexity of neural networks",
            "venue": "The 22nd International Conference on Artificial Intelligence and Statistics,",
            "year": 2019
        },
        {
            "authors": [
                "Wolfgang Maass"
            ],
            "title": "Neural nets with superlinear vc-dimension",
            "venue": "Neural Comput.,",
            "year": 1994
        },
        {
            "authors": [
                "David A McAllester"
            ],
            "title": "Pac-bayesian model averaging",
            "venue": "In COLT,",
            "year": 1999
        },
        {
            "authors": [
                "Mehryar Mohri",
                "Afshin Rostamizadeh",
                "Ameet Talwalkar"
            ],
            "title": "Foundations of machine learning. adaptive computation and machine learning",
            "year": 2012
        },
        {
            "authors": [
                "Ido Nachum",
                "Amir Yehudayoff"
            ],
            "title": "On symmetry and initialization for neural networks",
            "venue": "LATIN 2020: Theoretical Informatics 14th Latin American Symposium,",
            "year": 2021
        },
        {
            "authors": [
                "Vaishnavh Nagarajan",
                "J Zico Kolter"
            ],
            "title": "Deterministic pac-bayesian generalization bounds for deep networks via generalizing noise-resilience",
            "venue": "arXiv preprint arXiv:1905.13344,",
            "year": 2019
        },
        {
            "authors": [
                "Vaishnavh Nagarajan",
                "J Zico Kolter"
            ],
            "title": "Generalization in deep networks: The role of distance from initialization",
            "venue": "arXiv preprint arXiv:1901.01672,",
            "year": 2019
        },
        {
            "authors": [
                "Vaishnavh Nagarajan",
                "J Zico Kolter"
            ],
            "title": "Uniform convergence may be unable to explain generalization in deep learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Behnam Neyshabur",
                "Ruslan R Salakhutdinov",
                "Nati Srebro"
            ],
            "title": "Path-sgd: Path-normalized optimization in deep neural networks",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2015
        },
        {
            "authors": [
                "Behnam Neyshabur",
                "Ryota Tomioka",
                "Nathan Srebro"
            ],
            "title": "Norm-based capacity control in neural networks",
            "venue": "Proceedings of The 28th Conference on Learning Theory, COLT 2015,",
            "year": 2015
        },
        {
            "authors": [
                "Behnam Neyshabur",
                "Ryota Tomioka",
                "Nathan Srebro"
            ],
            "title": "Norm-based capacity control in neural networks",
            "venue": "Proceedings of The 28th Conference on Learning Theory,",
            "year": 2015
        },
        {
            "authors": [
                "Behnam Neyshabur",
                "Srinadh Bhojanapalli",
                "David McAllester",
                "Nati Srebro"
            ],
            "title": "Exploring generalization in deep learning",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Behnam Neyshabur",
                "Srinadh Bhojanapalli",
                "Nathan Srebro"
            ],
            "title": "A PAC-bayesian approach to spectrally-normalized margin bounds for neural networks",
            "venue": "In International Conference on Learning Representations,",
            "year": 2018
        },
        {
            "authors": [
                "Behnam Neyshabur",
                "Hanieh Sedghi",
                "Nathan Srebro"
            ],
            "title": "The role of over-parametrization in generalization of neural networks",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Konstantinos Nikolakakis",
                "Farzin Haddadpour",
                "Amin Karbasi",
                "Dionysios Kalogerias"
            ],
            "title": "Beyond lipschitz: Sharp generalization and excess risk bounds for full-batch GD",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Konstantinos Pitas",
                "Mike Davies",
                "Pierre Vandergheynst"
            ],
            "title": "Pac-bayesian margin bounds for convolutional neural networks",
            "venue": "arXiv preprint arXiv:1801.00171,",
            "year": 2017
        },
        {
            "authors": [
                "Akito Sakurai"
            ],
            "title": "Tighter bounds of the vc-dimension of three-layer networks",
            "venue": "In Proceedings of the World Congress on Neural Networks 1993,",
            "year": 1993
        },
        {
            "authors": [
                "Vladimir N Vapnik",
                "A Ya Chervonenkis"
            ],
            "title": "On the uniform convergence of relative frequencies of events to their probabilities",
            "venue": "In Theory of probability and its applications,",
            "year": 1971
        },
        {
            "authors": [
                "Ziqiao Wang",
                "Yongyi Mao"
            ],
            "title": "Tighter information-theoretic generalization bounds from supersamples",
            "venue": "arXiv preprint arXiv:2302.02432,",
            "year": 2023
        },
        {
            "authors": [
                "Aolin Xu",
                "Maxim Raginsky"
            ],
            "title": "Information-theoretic analysis of generalization capability of learning algorithms",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Peiyuan Zhang",
                "Jiaye Teng",
                "Jingzhao Zhang"
            ],
            "title": "Lower generalization bounds for gd and sgd in smooth stochastic convex optimization",
            "venue": "arXiv preprint arXiv:2303.10758,",
            "year": 2023
        },
        {
            "authors": [
                "Norm-based",
                "margin-based bounds: Liang"
            ],
            "title": "Norm-based bounds typically consist of a complexity term",
            "venue": "Neyshabur et al",
            "year": 2002
        },
        {
            "authors": [
                "Dziugaite"
            ],
            "title": "bounds, the explicit expression in the bound depends solely on the hypothesis class, the selected hypothesis, and the training set. They do not explicitly depend on any property of the learning algorithm or the population distribution",
            "year": 2020
        },
        {
            "authors": [
                "Harutyunyan"
            ],
            "title": "to approximate numerically in a tight manner (for example the bound in Theorem 1 in Xu",
            "year": 2017
        },
        {
            "authors": [
                "Dziugaite"
            ],
            "title": "2022), they do not reveal what properties of the pair (distribution, algorithm) allowed for such success of learning and estimation (so the utility they offer is similar to that of a validation set). C EXAMPLE APPLICATIONS Understanding how our formal results relate to generalization bounds in the literature on large neural",
            "year": 2022
        },
        {
            "authors": [],
            "title": "n)-estimable with respect to H and loss l. For that end, we use Theorem 1 in Angel",
            "venue": "Spinka",
            "year": 2021
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "There has been extensive research in recent years aiming to understand generalization in neural networks. Principled mathematical approaches often focus on proving generalization bounds, which bound the population risk from above by quantities depending on the training set and the trained model. Unfortunately, many known bounds of this type are often very weak, or even vacuous1, and they do not imply performance guarantees that could explain the strong real-world generalization of neural networks. Incidentally, there might be a good reason for this: in this paper we show that it is mathematically impossible for certain types of generalization bounds to be tight in a specific sense.\nGeneralization bounds in the literature often take the following form:\nLD(A(S)) < LS(A(S)) +C(A(S), S), (1) where S is the training set, A(S) is the hypothesis selected by the learning algorithm A, LD and LS denote the population and empirical risk respectively, C is some measure of complexity, and the inequality holds with high probability over the choice of S. For example, in their paper \u201cFantastic Generalization Measures and Where to Find Them,\u201d Jiang et al. (2020) examine more than a dozen generalization bounds of this form that have been suggested in the literature.\nFor a generalization bound to be useful, it should ideally be tight, meaning that the difference between the two sides of Eq. (1) is small with high probability. Moreover, we shall call a bound uniformly tight if it is tight over all possible (distribution, algorithm)-pairs.\nIn order to explain generalization in deep neural networks, it is necessary that the bound be tight in the overparameterized setting, which roughly means that the number of parameters in the networks 1A generalization bound is vacuous if it implies a population loss no better than guessing random labels.\nis much larger than the number of examples in the training set (Definition 2; all definitions appear in Sections 4 to 6). Given that essentially all known generalization bounds do not satisfy these two criteria, it is natural to ask:\nQuestion 1. Does there exist a generalization bound of the form of Eq. (1) that is uniformly tight in the overparameterized setting?\nObviously, one can always bound the population loss using a validation set. However, the upper bound in Eq. (1) depends only on the hypothesis A(S) and the training set S, whereas using a validation set does not technically satisfy the requirement of Question 1. Beyond technicalities, using a validation set is conceptually very different from a generalization bound. Using a validation set is a post hoc measurement that provides little insight as to why a certain algorithm does or does not generalize. In contrast, a meaningful generalization bound (like the VC bound2 for example) provides a scientific theory that predicts the behavior of learning algorithms in a wide range of conditions, and can inform the design of novel learning systems.\nOne might imagine that the generalization bounds for neural networks surveyed by Jiang et al. (2020) are not uniformly tight simply because the analyses in the proofs of these bounds are not optimal, and that a more careful proof might establish a tighter bound with better constants. Or perhaps, none of these measures yield uniformly-tight bounds for large neural networks, but in the future researchers might devise better complexity measures of the form of Eq. (1) that do. We show that obtaining a bound of the form of Eq. (1) that is uniformly tight requires more assumptions than are typically found in current literature."
        },
        {
            "heading": "2 OUR CONTRIBUTIONS",
            "text": "For a high-level overview of our contributions, see Table 1. Our results are stated using a notion of estimability, which is presented informally in Eq. (2) below (for formal definitions, see Definitions 3 and 4). All proofs appear in the appendices."
        },
        {
            "heading": "2.1 DISTRIBUTION- AND ALGORITHM-INDEPENDENT GENERALIZATION BOUNDS",
            "text": "One central message of this paper is that the answer to Question 1 is negative. The conclusion we draw from this and further analysis is that generalization bounds can be uniformly tight in the overparameterized setting, but only under suitable assumptions on the population distribution or the learning algorithm. Arguably, many bounds in the literature are presented without assumptions of the type we show are necessary for uniform tightness \u2014 so their tightness for any specific use case is not guaranteed.\nTo reason about Question 1, we introduce the notion of estimability. Informally, a hypothesis class H is estimable with accuracy \u03b5 if there exists an estimator E such that for every algorithm A and everyH-realizable distribution D, the inequality\n\u2223LD(A(S)) \u2212E(A(S), S)\u2223 < \u03b5 (2)\nholds with high probability over the choice of S (see Definition 3). In the realizable setting, a uniformly tight generalization bound like Eq. (1) exists if and only if H is estimable. Furthermore, if H is not estimable then there exists no uniformly tight bound as in Eq. (1) also for learning in the agnostic (non-realizable) setting. Our negative results for the realizable setting are stronger than (i.e., they imply) negative results for the agnostic setting.3\nOur first result shows that no hypothesis classH is estimable in the overparameterized setting:\n2The VC bound does not satisfy Question 1 because it is vacuous in the overparameterized setting. 3If a bound cannot be uniformly tight even just with respect to realizable distributions, then it definitely cannot be uniformly tight with respect to all distributions (both realizable and not) in the more general agnostic setting.\nTheorem (Informal Version of Theorem 2). Let H be a hypothesis class. If H has VC dimension d and the size of the training set is at most d/2, then every estimator E satisfying Eq. (2) has \u03b5 \u2265 1/8 \u2212 o(1). We emphasize that the lower bound \u03b5 \u2265 1/8 \u2212 o(1) in Theorem 2 does not hold merely for a single \u2018pathological\u2019 hard distribution. Rather, it holds for many ERMs over a sizable fraction of all Hrealizable distributions.\nWe believe Theorem 2 is worthy of attention because it precludes uniform tightness in the overparameterized setting for any generalization bound that depends solely on the training set, the learned hypothesis, and the hypothesis class. Determining which bounds in the literature on neural networks fall within this category is a matter of some debate. This category may arguably include some subset of the following bounds: VC bounds (Bartlett et al., 2019), Rademacher bounds (Bartlett & Mendelson, 2002), bounds based on the spectral norm (Pitas et al., 2017), Frobenius norm (Neyshabur et al., 2015b), path-norm (Neyshabur et al., 2015b), Fisher-Rao norm (Liang et al., 2019), as well as PACBayes-flatness and sharpness-flatness measures (e.g., see the appendices of Jiang et al. (2020) and Dziugaite et al. (2020)), and some compression bounds like Arora et al. (2018). For further details on the aforementioned bounds, see Appendix B.1. We take an expansive view, arguing that the abovementioned bounds, when applied to large neural networks, fall within the framework of Theorem 2, and therefore are not uniformly tight; however, we acknowledge that other scholarly views (mentioned in Section 6.2) also have merit, and the reader is encouraged to form an independent opinion on this matter."
        },
        {
            "heading": "2.2 ALGORITHM-DEPENDENT GENERALIZATION BOUNDS",
            "text": "An important facet of generalization not addressed by the formalism of Eq. (1) involves the choice of the training algorithm. The bound in Eq. (1) depends only on the training set S and the selected hypothesis A(S), and therefore it cannot capture certain beneficial aspects of the training algorithm. As a simple example, consider the case of a constant algorithm, that ignores the input S and always outputs a specific fixed hypothesis h0 \u2208 H. For such an algorithm, choosing E such that E(A(S), S) = LS(h0) yields an excellent estimator of the population loss.4\n4This estimator works only for the constant algorithm that outputs h0, so its existence does not contradict Theorem 2.\nSimilarly, in the context of neural networks, it is possible that certain training algorithms like SGD perform \u2018implicit regularization\u2019, or satisfy various stability properties, etc. Therefore there might exist generalization bounds that are tight specifically for these algorithms. The work of Hardt et al. (2016) is a prominent example. We formalize this notion by considering generalization bounds of the form\nLD(A(S)) < LS(A(S)) +C(A, S), (3)\nwhere the complexity C depends also on the algorithm A.5 Eq. (3) leads to the following question:\nQuestion 2. For which algorithms does there exist a generalization bound of the form of Eq. (3) that is tight for all population distributions in every overparameterized setting?\nQuestion 2 prompts us to define algorithm-dependent estimability (Definition 4) which is analogous to estimability (Eq. (2) and Definition 3), but involves an estimator E(A, S) that depends also on the learning algorithm. Our second result establishes a trade-off between learning performance and estimability:\nTheorem (Informal Version of Theorem 3). LetH \u2286 YX be a hypothesis class that is rich enough in a certain technical sense.6 Then, for any learning algorithm A, at least one of the following conditions does not hold:\n1. A learns a subsetH0 \u2286H with certain properties.6\n2. A is algorithm-dependent estimable.\nTheorem 3 states that if an algorithm learns well enough in the sense of Item 1, then the algorithm is not estimable, and this implies that there exists no generalization bound for that algorithm that is tight across all population distributions.\nTheorem 3 hinges on the algorithm satisfying Item 1 for a suitable choice of H and H0. We emphasize that it is known that this assumption can indeed be satisfied for some large neural network architectures when trained with SGD. For instance, the class of parity functions7 is one suitable choice for H. It is well known that even a simple fully-connected neural network is expressive enough to represent this class (e.g., Lemma 2 in Nachum & Yehudayoff (2020)). Furthermore, there exist specific network architectures that can provably learn a suitable subset H0 of the class of parities using SGD (e.g., Theorem 1 in Abbe & Sandon, 2020).\nTo illustrate the utility of Theorem 3, in Section 6.1 we conduct a detailed mathematical study of classes H that are suitable for use in the theorem, and of the quantitative limitations on the tightness of generalization bounds that these classes entail. Specifically, we show that an algorithm that can learn a suitable subset of the class of parity functions is not estimable with \u03b5 = 1/4 (see Theorem 5).\nMore generally, we consider the class of linear functions over finite fields, which is a generalization of parities, and show even stronger results. For example, for a field of size 11 (which corresponds to a multiclass classification task with 11 labels), we show that an algorithm that learns a suitable subclass is not estimable with \u03b5 = 0.45.\n5In Eq. (3) C receives a complete description of the algorithm A, whereas in Eq. (1) it received A(S), which is the algorithm\u2019s output. Specifically, in Eq. (3), if A is deterministic then C can compute the value A(S) using the inputs A and S. 6The precise details are specified in the formal version of Theorem 3. 7Namely, the class of functions that are an XOR of a fixed subset of the input bits."
        },
        {
            "heading": "3 RELATED WORK",
            "text": "Here we address two works that also study cases where generalization bounds are vacuous. For further related works, see Appendix B.\nThe main theorems in Nagarajan & Kolter (2019c) (Theorem 3.1) and in Bartlett & Long (2021) (Theorem 1) preclude the existence of tight algorithm-dependent generalization bounds in the overparameterized setting. They show this only for bounds based on uniform convergence and for a linear classifier (a single neuron). Also, Theorem 3.1 and Theorem 1 in Nagarajan & Kolter (2019c); Bartlett & Long (2021) consider the failure over a specific kind of distribution (Gaussian) and specific type of SGD algorithm, and in the proof of Theorem 1, the authors use a different distribution for every sample.\nCompared to these works, our results are more general and stronger: we show limitations for any kind of algorithm-dependent generalization bound, for many algorithms, and for any architecture in the overparameterized setting while using the same distribution across all sample sizes, with concrete quantitative implications (see Section 6.1)."
        },
        {
            "heading": "4 PRELIMINARIES",
            "text": "For our standard learning theory notation, see Appendix A.\nDefinition 1 (learnability). Let H be a hypothesis class and let D = {Di}Ti=1 a set of realizable distributions. (H,D) is (\u03b1,\u03b2,n)-learnable if there exists an algorithm (possibly randomized) A such that LDI (A(S)) < \u03b1 with probability at least 1 \u2212 \u03b2 over I \u223c U([T ]) and S \u223c DnI . We say that such an algorithm (\u03b1,\u03b2,n)-learns (H,D).\nLearning with neural networks is often considered an example of an overparameterized setting: in most practical scenarios, the number of parameters of the network greatly exceeds the number of data points n in the training set. Hence, for any given dataset of size n, there exist many different sets of weights (or hypotheses) that fit the data. This implies that in the absence of assumptions on the population distribution, the network will overfit in many scenarios. Since we study a general learning setting (the hypothesis class is not necessarily parametrized), we take the above implication as our definition of an overparameterized setting:\nDefinition 2 (overparameterized setting). Let H be a hypothesis class, let n,T \u2208 N, let \u03b1,\u03b2 \u2265 0, and let D = {Di}Ti=1 be a finite collection of H-realizable distributions. We say that (H,D) is an (\u03b1,\u03b2,n)-overparameterized setting if (H,D) is not (\u03b1,\u03b2,n)-learnable.\nFor a detailed discussion of how this definition compares to some common notions of overparamterization, see Appendix D."
        },
        {
            "heading": "5 BOUNDS THAT ARE ALGORITHM- AND DISTRIBUTION-INDEPENDENT CANNOT BE UNIFORMLY TIGHT",
            "text": "To answer Question 1, we introduce the following framework:\nDefinition 3 (estimability). A hypothesis class H \u2282 YX is (\u03f5, \u03b4, n)-estimable with respect to a loss function \u2113 if there exists a function E \u2236 H \u00d7 S \u2192 R such that for all algorithms A \u2236 (X \u00d7Y)n \u2192 H and all realizable distributions D over X \u00d7Y it holds that\n\u2223E(A(S), S) \u2212LD(A(S))\u2223 < \u03f5\nwith probability at least 1 \u2212 \u03b4 over S \u223c Dn. We call such E an estimator ofH.\nNote that given an algorithm-independent and distribution-independent bound C, it is not hard to construct a single (algorithm, distribution) pair that makes it vacuous. To illustrate this, consider the ERM defined as AC(S) \u2236= argminh\u2208H\u2236LS(h)=0C(h,S). Assume for simplicity that C is a generalization bound such that Eq. (1) holds with probability 1 for every H-realizable distribution D with labeling function hD. Then, with probability 1 over S \u223c Dn, LD(AC(S)) < LS(AC(S)) +\nC(AC(S), S) \u2264 C(hD, S), where the first inequality is Eq. (1) and the second follows from the construction of AC . Now assume that the setting is overparameterized (say with large \u03b1 and \u03b2 for a fixed n), i.e., no algorithm can learn (with error \u03b1) the ground-truth labeling with probability at least 1\u2212\u03b2 jointly over the uniform choice of the distributions, and n samples from the chosen distribution. This implies that for every algorithm A, there exists at least one distribution D\u2032 such that with probability at least \u03b2, A fails to learn when S \u223c (D\u2032)n. Hence there exists a realizable distribution D\u2032 with deterministic labeling function hD\u2032 such that, w.h.p., LD\u2032(AC(S)) > \u03b1, which implies by the previous inequality that C(hD\u2032 , S) > \u03b1 w.h.p. But then it holds w.h.p. that C(hD\u2032 , S) > \u03b1\u226b 0 = LD\u2032(hD\u2032)\u2212LS(hD\u2032). Hence the bound is w.h.p. not \u03b1-tight for the pair (AhD\u2032 ,D\u2032), where AhD\u2032 is the constant algorithm that always outputs hD\u2032 .\nThe argument above considers the binary classification setting with the 0 \u2212 1 loss and shows failure over a single (algorithm, distribution) pair. In the next theorem, we consider arbitrary hypotheses classes (not necessarily binary) and show that any estimator fails over a large fraction of possible (algorithm, distribution) pairs.\nTheorem 1. Let H be a hypothesis class, \u2113 be the 0 \u2212 1 loss, D = {Di}Ti=1 be a finite collection of H-realizable distributions each associated with a hypothesis hi, and (H,D) an (\u03b1,\u03b2,n) overparameterized setting. For any h \u2208 H, let Ah be an ERM algorithm that outputs h for any input sample S consistent with h. Then, there exists a distribution DERM over ERMH (the set of all deterministic ERM algorithms over H) such that for any estimator E of H and for any \u03f5, \u03b3 \u2208 [0,1] at least one of the following conditions does not hold:\n1. With probability at least 1 \u2212 \u03b3 over I \u223c U([T ]) and S \u223c DnI , \u2223E (AhI (S), S) \u2212LDI (AhI (S))\u2223 < \u03f5.\n2. With probability at least 1 \u2212 \u03b2 + \u03b3 over I \u223c U([T ]), S \u223c DnI , and AERM \u223c DERM , \u2223E (AERM(S), S) \u2212LDI (AERM(S))\u2223 < \u03b1 \u2212 \u03f5.\nIn particular,H is not (\u03b1/2, \u03b2/2, n)-estimable.\nTheorem 2 below is an application of Theorem 1 for VC classes. Theorem 2 shows that when Theorem 1 is applied, we get substantial numerical values which highlight Theorem 1 prevalence. Theorem 2. Let H be a hypothesis class of VC dimension d \u226b 1, and \u2113 be the 0 \u2212 1 loss. Let X \u2282 X be a set of size d shattered by HX = {hi}2 d i=1 \u2282 H and let {Di} 2d\ni=1 be the set of realizable distributions that correspond to HX , where for all i the marginal of Di on X is uniform over X . Let ERMHX be the set of all deterministic ERM algorithms forHX . For any h \u2208HX , let Ah be an ERM algorithm that outputs h for any input sample S consistent with h. Then, for any estimator E ofH, at least one of the following conditions does not hold:\n1. With probability at least 1/2 over I \u223c U([2d]) and S \u223c DnI ,\n\u2223E (AhI (S), S) \u2212LDI (AhI (S))\u2223 < d \u2212 n 4d .\n2. With probability at least 1/2\u2212o(1) over I \u223c U([2d]), S \u223c DnI , andAERM \u223c U(ERMHX ),\n\u2223E (AERM(S), S) \u2212LDI (AERM(S))\u2223 < d \u2212 n 4d \u2212 o(1),\nwhere U(ERMHX ) denotes the uniform distribution over ERMHX .\nIn particular, H is not (d\u2212n 4d \u2212 o(1),1/2 \u2212 o(1), n)-estimable for any n \u2264 d/2. The notation o(1) denotes quantities that vanish as d goes to infinity.\nTheorem 2 states that any estimator E fails to predict the performance of many ERM algorithms over many scenarios. If Item 1 does not hold, then E fails to estimate the success of algorithms Ah in a situation where they perform very well (since by definition LDI (AhI (S)) = 0), despite the fact that these are very simple algorithms. If Item 2 does not hold, then E fails to estimate the performance of many ERM algorithms across many distributions, namely, on roughly 50% of (ERM, distribution) pairs."
        },
        {
            "heading": "5.1 DISCUSSION OF THEOREM 2",
            "text": "Theorem 2 shows that bounds that depend solely on the training set, the learned hypothesis, and the hypothesis class cannot be uniformly tight in the overparameterized setting. More broadly, Jiang et al. (2020) showed empirically that many published generalization bounds are in fact not tight. This is an empirical fact that calls for an explanation. Theorem 2 implies that algorithmindependent bounds being not-tight must be a very common phenomenon, in the sense that any algorithm-independent bound that is tight on (a specific set of) simple cases, must be far from tight for many natural algorithms and distributions. Thus, Theorem 2 can at least partially explain the empirical findings of Jiang et al. (2020). To clarify, Theorem 2 does not imply that any specific bound is not tight for a specific (algorithm, distribution) pair, but it qualitatively matches the observation that lack of tightness is ubiquitous among algorithm-independent bounds.\nMany published generalization bounds are stated and proved without explicit restrictions on the set of distributions or algorithms. Hence, these bounds are valid upper bounds on the population loss in all scenarios. Due to the limitations on estimability shown in Theorem 2, these bounds cannot fully distinguish between distributions for which the algorithm performs well and distributions for which it does not. Therefore, such bounds have no other option but to predict a large population error (making them loose for some cases in which the population error is not as large).\nWe emphasize that Theorem 2 shows that VC classes are not estimable, and hence do not admit tight generalization bounds, not merely over some pathological distribution, but in a fairly simple setting. Let us elaborate: take a dataset of natural images N (e.g., MNIST, CIFAR, etc.) and consider the uniform distribution over this set with a sample size n = \u2223N \u2223/2. This is a simple setting; we merely consider a small set of natural images. Still, Theorem 2 shows that without any assumption on the relation between the images and their labels, any estimator would not perform better than a random guess. That is, any estimator will fail over many ERMs (Item 1 and Item 2 in the theorem) with probability 1/2 over I \u223c U([2d]) and S \u223c DnI to produce an estimate of the true error with an accuracy of 1/8. Hence, distribution-independent bounds can be loose even in a simple setting."
        },
        {
            "heading": "6 ALGORITHM-DEPENDENT BOUNDS ARE LIMITED BY A LEARNABILITY-ESTIMABILITY TRADE-OFF",
            "text": "To prove Theorem 2, we used constant algorithms, that is, algorithms that output the same hypotheses regardless of the input S. The true error of such algorithms is easy to estimate using the empirical error (by Hoeffding\u2019s inequality). Thus, it might be that adding the algorithm we use as a parameter for the estimator E might help. For example, sharpness-based measures cannot estimate well the accuracy of a neural network for all algorithms, as Theorem 2 shows. Yet, these measures might be a good estimator when just considering SGD from random initialization. Definition 4 (algorithm-dependent estimability). A hypotheses class and a collection of algorithms (H,A) is (\u03f5, \u03b4, n)-estimable with respect to a loss function \u2113 if there exists a function E \u2236 A\u00d7S \u2192 R such that for all algorithms A \u2208 A and all realizable distributions D over X \u00d7Y it holds\n\u2223E(A, S) \u2212LD(A(S))\u2223 < \u03f5 with probability at least 1\u2212 \u03b4 over S \u223c Dn. We call such E an algorithm-dependent estimator ofH. If (H,{A}) is (\u03f5, \u03b4, n)-estimable, we say A is (\u03f5, \u03b4, n)-estimable with respect toH and loss \u2113.\nThe function E is now provided with a complete description of the algorithm used to generate a hypothesis. Yet, the following theorem provides a more subtle negative answer to Question 2 compared to Theorem 2. Learnability and estimability are mutually exclusive. Theorem 3. Let H = H0 \u222aH1 be a hypothesis class, let \u2113 be a loss function, and let T = T0 + T1 be integers. Let D = {Di}Ti=1 be a set of H-realizable distributions such that D0 = {Di} T0 i=1 is realizable overH0, and D1 = {Di}Ti=T0+1 is realizable overH1. Assume that (H,D) is an (\u03b1,\u03b2,n)overparameterized setting, and furthermore assume:\ndTV (S0, S1) \u2264 1 \u2212 \u03b3, where S0 \u223c DnI0 , and S1 \u223c D n I1 , I0 \u223c U([T0]) and I1 \u223c T0 + U([T1]).\nLet \u03b7 = \u03b3 2 \u2212 1\u2212\u03b2 T T1 +\u03b4(1+T0T1 ) 2\n. Then, for any learning algorithmA (possibly randomized), at least one of the following conditions does not hold:\n1. A (\u03f5, \u03b4, n)-learnsH0.\n2. A is (\u03b1\u2212\u03f5 2 , \u03b7, n)-estimable with respect toH and loss \u2113.\nIn particular, for any estimator E it holds \u2223E(A, S) \u2212LDI (A(S))\u2223 > \u03b1\u2212\u03f52 with probability of at least \u03b7 over I \u223c U([T ]) and S \u223c DnI .\nAs a concrete realization of Theorem 3 with a simple set of parameters, we refer the reader to Theorem 4 in Section 6.1. We consider multiclass classification with q labels and the 0 \u2212 1 loss. We show that many algorithms are not (1/2\u2212 o(1),1/2\u2212 o(1), n)-estimable where o(1) is with respect to q, and already for q = 11, we get (0.45,0.4, n). Note that (0.5,0.5, n) is the performance of a random estimator that outputs a random estimation uniformly at random from [0,1]. In the overparameterized setting, no algorithm can achieve low loss for all distributions D. In this scenario, Theorem 3 shows that if a learning algorithm has a bias towards some part of the hypothesis class, that is, it learns the distributions in D0 well (a bias towards H0), then it is necessarily not estimable, even when using complete knowledge of the algorithm being used. So the theorem shows a trade-off between learnability and estimabilty. To see more carefully why, consider the parameters \u03b1,\u03b2, \u03b3, T, T0, T1 to be fixed, as they are not part of the algorithm A in question but parameters of the overparameterized setting at hand. Then, we observe an affine relation between the accuracy parameter \u03f5 and the accuracy parameter for estimating A. An affine relation also holds between the confidence parameter \u03b4 and the confidence parameter for estimatingA. This is depicted in Figure 1. In conclusion, an algorithm cannot perform well and be certain of it when it does.\nIn the following section, we illustrate Theorem 3 and show that it applies to a natural setting that includes neural networks."
        },
        {
            "heading": "6.1 QUANTITATIVE LIMITATIONS FOR ALGORITHM-DEPENDENT BOUNDS",
            "text": "To illustrate our results, we conclude our paper with a case study of the hypothesis class of linear functionals Linq (d) over the vector space Fdq where Fq is the finite field with q elements, with q prime. For example, Lin2(d) consists of all parity functions with input size d.\nLinq (d) \u2261 (Fdq) \u2217 \u2236= {fa \u2236 Fdq \u2192 Fq \u2236 a \u2208 Fdq , fa(x) =\nd\n\u2211 i=1 ai \u22c5 xi mod q}\nAn important property of this class is presented in the following lemma: each two distinct functions in the class differ exactly on a q\u22121\nq fraction of elements in Fdq .\nLemma 1. Each two distinct functions f, h \u2208 Linq(d) agree on a fraction 1/q of the space and the 0 \u2212 1 risk of the function h over samples from Df = f \u25c7 U(Fdq) is given by\nLDf (h) = { 0 h = f 1 \u2212 1/q h \u2260 f .\nFor this class, we consider the following set of algorithms:\nDefinition 5. Let X = Fdq , Y = Fq , andH = Linq (d). We say a learning algorithmA \u2236 (X \u00d7Y)\u2217 \u2192 H (possibly randomized) is an ERM algorithm with a linear bias if for every sample size n \u2264 d, we can associate (A, n) with a linear subspace Hn \u2282 H of dimension n such that if \u2223S\u2223 = n and S is consistent with some function in Hn, then A(S) \u2208 Hn. We denote by Alin the set of all ERM algorithms with a linear bias.\nSuch choice of algorithms is natural with respect to Theorem 3. They perform well on distributions that are associated with their linear bias. Such algorithms are not estimable, as the following theorem shows. For brevity, we denote: F (q, n) \u2236= 1\n2 \u2211 n k=0 [(qk\u2212n\u22122qk\u2212n\u22121\u22121) q k\u22121 qn+1\u22121 +2\u2212q k\u2212n] \u22c5Rq(n,n+ 1, k)\u2212\u2211n\u22121k=0 (1\u2212qk\u2212n) \u22c5Rq(n,n, k)where Rq(n1, n2, r) denotes the probability of an n1\u00d7n2 matrix with i.i.d. entries picked uniformly at random from Fq to have rank r over Fq (more details in the appendix).\nTheorem 4. For every A \u2208 Alin and every n \u2264 d \u2212 1, A is not (1/2 \u2212 1/2q, \u03b7, n)-estimable with respect to Linq (d) and the 0 \u2212 1 loss where \u03b7 = F (q, n). In particular, for q > 10, it holds that \u03b7 > 0.4, and generally, \u03b7 = 1/2 \u2212 1/q + o(1/q).\nTheorem 4 shows that already for multiclass classification with q = 11 labels (comparable to the ten labels of MNIST and CIFAR datasets), with probability at most 0.6 we estimate the performance of an algorithm with an accuracy of 10/22 \u2248 0.45. This is not much better than a random guess that estimates with an accuracy of 0.45 with probability 0.5. This holds since the loss of any algorithm in Alin is either 0 or 1 \u2212 1/q \u2248 0.9, as Lemma 1 shows. So, we can always flip a coin, declare either 0 or 0.9, and be correct with probability 0.5. Finally, as q grows, our estimation can truly only be as good as a random guess.\nWhen we consider the case of q = 2 (parity functions) for Theorem 4, we get that the algorithms we consider are only (1/4,0.025)-not estimable. To stengthen our results, the following theorem provides a separate analysis that includes a different technique from Theorem 3 and that works specifically for q = 2.\nTheorem 5. For everyA \u2208 Alin (possibly randomized) and every n \u2264 d\u22121,A is not (0.25,0.14, n)estimable with respect to Lin2 (d) and the 0\u2212 1 loss. More so, for every deterministicA \u2208 Alin and every 6 \u2264 n \u2264 d, A is not (0.25,0.32, n)-estimable with respect to Lin2 (d) and the 0 \u2212 1 loss.\nIt might appear that Theorems 4 and 5 are unrelated to practical overparameterized models because their setting is too artificial. However, as mentioned above, neural network can represent parities (e.g., Lemma 2 in Nachum & Yehudayoff, 2020) and, in some specific cases, can learn parities using SGD (Abbe & Sandon, 2020). Hence, these theorems are relevant at least to some neural networks."
        },
        {
            "heading": "6.2 CONCLUSIONS",
            "text": "We have proved mathematically that specific types of generalization bounds are subject to specific limitation in a precise formal sense. The interpretation of these formal results, and the extent to which they apply to existing generalization bounds in the literature on large neural networks, is a matter of some debate. It is possible to argue that our results do not apply to various generalization bounds in the literature, e.g., because our definition of an overparameterized setting doesn\u2019t hold in cases of interest, because those bounds do not satisfy our definitions of distribution-independence or algorithm-independence, or for other reasons. Alternatively, one could argue that uniform-tightness is not an important property for a generalization bound, and that for specific practical cases of interest, it is easy to tell empirically whether a bound is tight or not when applying it to a trained model. Below we outline our view, but we also acknowledge that a variety of scholarly positions exist. We encourage the reader to develop an independent opinion on this matter.\nIn our view, Theorems 2 and 3 point to two possibilities for obtaining uniformly tight generalization bounds in overparameterized settings. The first option is that when stating a generalization bound, the statement explicitly specifies a set of \u2018nice\u2019 or \u2018natural\u2019 population distributions for which the bound is tight. Thus, the \u2018bad\u2019 population distributions on which the bound is not tight are clearly excluded from the set of distributions for which the bound is intended to work. The second option for obtaining a tight generalization bound is to make explicit assumptions about the learning algorithm, which in particular imply that for any choice of classes H,H0 suitable for Theorem 3, the algorithm cannot learn H0. We suggest that every proposal of a generalization bound for the overparameterized setting explicitly include one of these two types of assumptions. Otherwise, if the setting is overparameterized, there provably exist pairs of learning algorithms and population distributions for which the bound applies and is valid, but is not tight. See Appendix C for further illustrations.\nExplicitly stating the assumptions underlying generalization bounds is not only necessary for the bounds to be uniformly tight in the overparameterized setting, but can also promote more clarity within the scientific community, and guide future research."
        },
        {
            "heading": "A NOTATION",
            "text": "Following is a summary of the standard learning theory notation used in this paper.\nX the set of possible inputs (the domain) Y the set of possible labels D a distribution over X \u00d7Y DX the marginal distribution of D on X D a set of distributions over X \u00d7Y\nH \u2286 YX a hypotheses class S = ((x1, y1) , ..., (xn, yn)) \u223c Dn an i.i.d. sample or a training set\n(X \u00d7Y)\u2217 = \u222a\u221ek=1(X \u00d7Y)k the set off all possible finite samples A \u2236 (X \u00d7Y)\u2217 \u2192 YX a learning algorithm (possibly randomized)\nA a set of learning algorithms \u2113 \u2236H \u00d7X \u00d7Y \u2192 R a loss function\n\u21130\u22121 (h,x, y) = 1h(x)\u2260y the 0-1 loss function LS (h) \u2236= 1n \u2211 n i=1 \u2113(h,xi, yi) the empirical loss of h on the sample S LD (h) \u2236= E(x,y)\u223cD\u2113(h,x, y) the population loss of h with respect to distribution D f \u25c7DX the distribution of the random variable (X,f(X))\nwhere X \u223c DX U(\u2126) the uniform distribution over a set \u2126 [m] the set {1,2, . . . ,m} Fq the finite field with q elements, where q is prime\nLinq (d) the set of all linear functionals over Fdq Rq(n1, n2, r) the probability of an n1 times n2 random matrix with\nentries drawn i.i.d. uniformly at random from Fq to have rank r (see Lemma 2)\n[nk]q the Gaussian coefficient\u220f k\u22121 i=0 qn\u2212i\u22121 qk\u2212i\u22121\nFor simplicity, all spaces we consider are finite. Definition 6. A distributionD is realizable with respect to a hypothesis classH if there exists h \u2208H such that LD(h) = 0.\nNote: in the case of a random algorithm A, we define LD (A(S)) \u2236= Eh\u223cQ(S)LD (h) where Q(S) is the posterior distribution over YX that A outputs. This fits the PAC-Bayes framework that upper bounds this quantity."
        },
        {
            "heading": "B FURTHER RELATED WORKS",
            "text": ""
        },
        {
            "heading": "B.1 EXAMPLES OF GENERALIZATION BOUNDS THAT ARE DISTRIBUTION-INDEPENDENT AND ALGORITHM-INDEPENDENT",
            "text": "Our definition of an estimator E captures the formalism of generalization bounds that are distribution-independent and algorithm-independent. Following are some bounds from the literature that, in our view, satisfy these independence requirements.\n1. VC bounds: Vapnik & Chervonenkis (1971); Bartlett et al. (1998; 2019). For a classH of VC dimension d, the complexity measure is \u0398 (d+log(1/\u03b4)\nn ). The complexity term depends\nsolely on the hypothesis class (for fixed \u03f5, \u03b4, and n).\n2. Rademacher bounds: Mohri et al. (2012). RadH(S) = 1nE\u03c3supf\u2208H\u2211 n i=1 \u03c3if(xi) where\n\u03c3 \u223c U{\u00b11}n. This yields the complexity measure 2 \u22c5 RadH(S) + 4 \u221a 2 ln(2/\u03b4) n\n. The complexity term measures how rich the hypothesis classH is with respect to S.\n3. Norm-based and margin-based bounds: Liang et al. (2017); Neyshabur et al. (2015a); Nagarajan & Kolter (2019b); Pitas et al. (2017); Bartlett & Mendelson (2002); Bartlett et al. (2017); Neyshabur et al. (2019); Golowich et al. (2018); Neyshabur et al. (2018; 2015c). Norm-based bounds typically consist of a complexity term that depends on the algorithm\u2019s output. For example, \u2211Li=1 \u2223Wi\u2223 2 F is the sum of the Frobenius norms of all\nthe neural network layers. Margin-based bounds quantify a measure of separation for the induced representation of the training set.\n4. PAC-Bayes with a fixed prior: McAllester (1999). This yields the complexity measure\u221a D(Q\u2223\u2223P )+ln(n/\u03b4)\n2(n\u22121) where P is a fixed distribution (prior) over the hypothesis class and Q\nis the output distribution of a randomized algorithm.\n5. Sharpness-based measures: Nagarajan & Kolter (2019a); Neyshabur et al. (2017); Keskar et al. (2017). These measures (inspired by the PAC-Bayes framework) quantify how flat the solution generated by the algorithm is with respect to the empirical loss. For example, 1/\u03c32 where \u03c3 = max{\u03b1 \u2236 EW\u223cN (w,\u03b1I)LS(fW (x)) < 0.05}, w is a weight vector of the algorithm\u2019s output, W is a random perturbation of w, and fW is the hypothesis realized by the network\u2019s architecture with respect to W . C depends both on A(S) and S. Such measures received considerable attention as candidates to explain neural network generalization (see Dziugaite et al. 2020).\nIn all these bounds, the explicit expression in the bound depends solely on the hypothesis class, the selected hypothesis, and the training set. They do not explicitly depend on any property of the learning algorithm or the population distribution. Hence, in our view, they are subject to Theorems 1 and 2."
        },
        {
            "heading": "B.2 EXAMPLES OF GENERALIZATION BOUNDS THAT ARE DISTRIBUTION-INDEPENDENT AND ALGORITHM-DEPENDENT",
            "text": "The following two works present algorithm-dependent bounds.\nZhang et al. (2023): The paper studies convex optimization, so the results can hold only for a single neuron. Nevertheless, although it gives matching lower and upper bounds, the bounds match only asymptotically when n is very large so the scenario is far away from the overparameterized regime (which is the focus of interest for neural networks).\nNikolakakis et al. (2023): The paper suggests a new generalization bound that is an algorithmdependent bound that does not include any distributional assumptions (it is distributionindependent).\nIn our view, Theorem 3 implies that these bounds cannot be tight for all population distributions."
        },
        {
            "heading": "B.3 EXAMPLES OF GENERALIZATION BOUNDS THAT ARE DISTRIBUTION-DEPENDENT AND ALGORITHM-DEPENDENT",
            "text": "The following are information-theoretic generalization bounds that are both algorithm and distribution-dependent (which we advocate for in this paper). However, bounds are sometimes hard to approximate numerically in a tight manner (for example the bound in Theorem 1 in Xu & Raginsky (2017)). On a high level, such bounds are part of the PAC Bayes framework; see proof 4 for Theorem 8 in Bassily et al. (2018), which is equivalent to Theorem 1 in Xu & Raginsky (2017). Unfortunately, when the PAC-Bayes/information-theoretic bounds can be approximated in a tight manner such as in Harutyunyan et al. (2021); Hellstro\u0308m & Durisi (2022); Wang & Mao (2023); Dziugaite et al. (2021); Haghifam et al. (2022), they do not reveal what properties of the pair (distribution, algorithm) allowed for such success of learning and estimation (so the utility they offer is similar to that of a validation set)."
        },
        {
            "heading": "C EXAMPLE APPLICATIONS",
            "text": "Understanding how our formal results relate to generalization bounds in the literature on large neural networks, and to their use in practical settings, is a nontrivial question. In this appendix we illustrate our position via two examples, but we also recognize that other positions are possible."
        },
        {
            "heading": "C.1 MARGIN BOUNDS",
            "text": "There are many margin- and norm-based bounds in the literature (see Appendix B.1). We start with a toy example, illustrating why these bounds are not uniformly tight, and why we believe that Theorem 2 applies to these types of bounds.\nFor simplicity, let the domain X = {x \u2208 Rd \u2236 \u2225x\u22252 = 1} be the unit sphere in Rd, and consider a half-space classifier with hypothesis classH = {hw \u2236 w \u2208 X}, where hw(x) = sign(\u27e8w,x\u27e9). For a fixed sample size n, consider a margin bound of the form f = f(\u03b3), where \u03b3 is the minimum distance of a point in the training set from the learned decision boundary. f(\u03b3) is an upper bound on the difference between the population and empirical losses. f(\u03b3) is small only if \u03b3 is large. We will show that f is not uniformly tight.\nIndeed, consider a population distribution D that has a uniform marginal on the domain X , with labels that are generated by a specific half-space h\u2217 \u2208 H. Consider an algorithm A that has a bias towards h\u2217, in the sense that it will output h\u2217 whenever the training set is consistent with h\u2217.\nIn this case, given samples from D, with probability 1, A will output h\u2217. Hence, the population loss and the empirical loss are the same (they are both exactly 0). Because the marginal of the population distribution on the domain is uniform, the population margin is 0, and therefore the empirical margin \u03b3 will be close to 0 (even for a sample size n that is small compared to the dimension d). Thus, f(\u03b3) is large, but the generalization gap is 0. Hence, the bound f(\u03b3) is not tight for the pair (D,A). Note that f(\u03b3) is a quantity that depends only on the selected hypothesis and on the training set. Namely, this is a distribution- and algorithm-independent bound. Hence, Theorem 2 implies a stronger limitation, in the sense that it shows that there exist many (distribution, algorithm) pairs for which the bound is not tight, not just a single pair. The same holds for bounds of the form f(\u2225W \u2225, \u03b3), where \u2225W \u2225 is some norm-like function of the parameters of the learned classfier."
        },
        {
            "heading": "C.2 PAC-BAYES BOUNDS",
            "text": "The contributions of this paper can be illustrated by applying them, for example, to the landmark work of Dziugaite & Roy (2017). They presented PAC-Bayes bounds for neural networks that were the first generalization bounds to be non-vacuous in some real-world overparameterized settings. However, that paper did not state formal assumptions on the set of population distributions or the set of algorithms to which it is intended to apply. Therefore, our results imply the following for any fixed choice of a countable collection of priors as in the bound of Dziugaite & Roy (2017). First, by Theorem 2, the bounds is not tight for roughly half of all (distribution, algorithm) pairs. Second, by Theorem 3, for any specific learning algorithm, if the algorithm can learn a suitable set of functions H0, then the bound is not tight on a sizable collection of population distributions. One way to understand the forgoing example is that the non-vacuous numerical bound presented for the MNIST dataset by Dziugaite & Roy (2017) relies on implicit assumptions about the algorithm and the population distribution (for other tasks different priors would be required), such as: \u2018the population distribution satisfies that with high probability, SGD (on the specific network architecture of interest) finds a flat global minimum point (with respect to the empirical loss) in the parameter space not too far away from the initialization point.\u2019 Or alternatively, \u2018SGD (on the specific network architecture of interest) cannot learn any collection of functions H0 that is suitable for Theorem 3.\u2019 Laid out explicitly in this manner, it becomes clear that these assumptions are not obviously true. This invites further research to understand for which population distributions and algorithms the assumptions hold, and whether there might exist more natural and compelling assumptions that suffice for obtaining bounds of comparable tightness."
        },
        {
            "heading": "D ON DEFINITIONS OF OVERPARAMETERIZATION",
            "text": "There are a number of definitions of overparameterization that are common in the machine learning literature, but there does not appear to be a single formal definition that is universally agreed upon. One contribution of this paper is that we offer an alternative formal definition of overparameterization that is close to well-known notions, and also enables proving meaningful mathematical\ntheorems. In this appendix we discuss our definition and its connections to some common definitions.\nFollowing are three common definitions. In these definitions, there is a learning task in which a machine learning system (e.g., a neural network) is trained using a training set of n labeled samples. The hypothesis classH is the collection of classification functions that the machine learning system can represent. We state these definition informally, in the sense some value is required to be large without specifying exact quantities.\nDefinition A. The number of independently-tunable parameters in the machine learning system is significantly larger than the number of samples in the training set. Namely,H = {hw \u2236 w \u2208 Rk} and k \u226b n. Definition B. The learning system can interpolate arbitrary data. Namely, H can realize any labeling for any distinct x1, . . . , xm, for some m\u226b n. Definition C. The size of the training set is smaller than the VC dimension, namely, VC(H)\u226b n.\nFor convenience, our definition of overparameterization is restated here. As before, one should think ofH as the collection of classification functions that are realizable by a learning system of interest. Definition 2 (Restated). Let H be a hypothesis class, let n,T \u2208 N, let \u03b1,\u03b2 \u2265 0, and let D = {Di}Ti=1 be a finite collection of H-realizable distributions. We say that (H,D) is an (\u03b1,\u03b2,n)overparameterized setting if (H,D) is not (\u03b1,\u03b2,n)-learnable (as in Definition 1).\nOur definition is more general than Definitions A, B and C in that our definition is (typically) implied by the other definitions. This means that when we prove that a bound cannot be tight in the overparameterized setting according to our definition, that in particular implies that it cannot be tight in any setting that is overparameterized according to the other definitions. This makes our impossibility results stronger.\nThe implications hold as follows:\n\u2022 Definition A \u00d4\u21d2 Definition C. This implication holds for many neural networks. It is well-known that large neural networks have large VC dimension. For instance, Theorem 1 in Maass (1994) states that under mild assumptions, any neural network with at least 3 layers has VC dimension linear in the number of edges in the network. Maass (1994) showed this for networks with threshold activation functions and binary inputs, and Bartlett et al. (1998) note that \u201dIt is easy to show that these results imply similar lower bounds\u201d for networks with ReLU activation. A similar result holds also for networks with 2 layers (Sakurai, 1993).\n\u2022 Definition B \u00d4\u21d2 Definition C. If a classH can realize any labeling for some x1, . . . , xm then the VC ofH is at least m.\n\u2022 Definition C \u00d4\u21d2 Definition 2. This implication is Item 2 in the proof of Theorem 2 on page 19.\nOur definition does not merely generalize the common definitions listed above \u2014 it generalizes them in a desirable way. Definitions B and C pertains to a case where the hypothesis class can express every possible labeling of the training set or of a shattered set. However, this seems a bit rigid. What if the network or hypothesis class can express every labeling except one? Or can express most but not all labelings? Intuitively, the network is still very much overparameterized, even if there are some specific choices of labelings that it cannot express. The essence of overparameterization is that the network can express too many of the possible labelings \u2014 not necessarily every single one of them. What is \u2018too many\u2019? Our answer, as captured in Definition 2, is that a network is overparameterized (can express \u2018too many\u2019 labelings) if it can typically express many different labelings that are consistent with the training set but disagree on the test set, leading to poor expected performance on the test set.\nIn conclusion, Definition 2 is natural, impossibility results for Definition 2 imply impossibility results for the other definitions, and Definition 2 generalizes the other definitions in a desirable way.\nWe end with two additional remarks concerning definitions of overparmetrization.\n1. Note that Definition A also has the weakness that it is sometimes possible to parameterize the same hypothesis class with varying numbers of parameters. For instance, a 1-layer fully-connected network with linear activations and a multi-layer fully-connected network with linear activations represent the same hypothesis class (both networks simply multiply the input vector by a matrix), but the number of parameters can be very different between these two networks.\n2. A curious reader might wonder why we do not use the definition of PAC learning in Definition 2. That is, why not say that H is an (\u03b1,\u03b2,n)-overparameterized setting if H is not (\u03b1,\u03b2,n)-PAC-learnable. Working with such a definition will yield quantitatively weaker results. For example, an analogue of Theorem 1 (that uses the aforementioned definition) will still prove that VC classes are not estimable, so any estimator fails for at least one combination of an algorithm and a distribution; our definition yields the stronger result of Theorem 1, that the failure of each estimator is across many combinations of algorithms and distributions."
        },
        {
            "heading": "E PROOFS OF THEOREM 1 AND THEOREM 2",
            "text": "Theorem 2 is a corollary of the more general Theorem 1 which shows that in the overparameterized setting, any estimator of the true loss will fail over many combinations of ERM algorithms and distributions.\nBefore the proof we present the type of distributions over ERM algorithms Theorem 1 applies for.\nWe remind the reader that an ERM algorithm is a function A \u2236 (X \u00d7Y)\u2217 \u2192 YX whose output is any consistent function with the sample S that belongs toH. Definition 7 (Bayes-like Random ERM). Let H be a hypothesis class and D = {Di}Ti=1 a set of distributions. We say that the distribution DERM over ERM algorithms is Bayes-like if for any fixed S it holds that\nPDERM (AERM(S) = h) = \u2211i\u2236 hi=h PDi(S) \u2211Tj=1 PDj(S)\nwhere hi is the labeling function associated with the distribution Di. If AERM \u223c DERM , we say AERM is Bayes-like Random ERM .\nReminder: All spaces we consider in the paper are finite so the definition of Bayes-like Random ERM is a well-defined random variable over a finite sample space.\nClearly, any algorithm A chosen with non-zero probability over DERM is an ERM algorithm. Theorem 1. Let H be a hypothesis class, \u2113 be the 0 \u2212 1 loss, D = {Di}Ti=1 be a finite collection of H-realizable distributions each associated with a hypothesis hi, and (H,D) an (\u03b1,\u03b2,n) overparameterized setting. For any h \u2208 HX , let Ah be an ERM algorithm that outputs h for any input sample S consistent with h. Then, for any Bayes-like distribution DERM over ERM algorithms and any estimator E ofH at least one of the following conditions does not hold:\n1. With probability at least 1 \u2212 \u03b3 over I \u223c U([T ]) and S \u223c DnI , \u2223E (AhI (S), S) \u2212LDI (AhI (S))\u2223 < \u03f5.\n2. With probability at least 1 \u2212 \u03b2 + \u03b3 over I \u223c U([T ]), S \u223c DnI , and AERM \u223c DERM , \u2223E (AERM(S), S) \u2212LDI (AERM(S))\u2223 < \u03b1 \u2212 \u03f5,\n.\nIn particular,H is not (\u03b1/2, \u03b2/2, n)-estimable.\nProof. To see whyH is not (\u03b1/2, \u03b2/2, n)-estimable if item 1 or item 2 do not hold, choose \u03b3 \u2236= \u03b2/2 and \u03f5 \u2236= \u03b1/2.\nIf item 1 does not hold, it means there exists an algorithm Ahi and a distribution Di such that \u2223E (Ahi(S), S) \u2212LDi(Ah(S))\u2223 \u2265 \u03f5\nwith probability greater than \u03b2/2 over Di soH is not (\u03b1/2, \u03b2/2, n)-estimable. If item 2 does not hold, it means there exists an ERM algorithm AERM and a distribution Di such that \u2223E (AERM(S), S) \u2212LDi(Ah(S))\u2223 \u2265 \u03b1 \u2212 \u03f5 with probability greater than \u03b2/2 over Di soH is not (\u03b1/2, \u03b2/2, n)-estimable. Let AERM \u223c DERM be a Bayes-like Random ERM . The key ingredient in our proof is to show that the following two probability measures P1 and P2 over (X \u00d7Y)n \u00d7H are equal so P1(S,h) = P2(S,h) for every pair (S,h) \u2208 (X \u00d7Y)n \u00d7H.\n1. P1: the pair (SI , hI) is generated by choosing an index I \u223c U[T ], then SI \u223c DI , and hI is the labeling function associated with DI .\n2. P2: the pair (SI ,AERM(SI)) is generated by choosing an index I \u223c U[T ], then SI \u223c DI , and AERM \u223c DERM independently.\nEqualities (4) - (8) and equalities (9) - (14) show that P1 and P2 are indeed equal.\nP1(S,h) = T\n\u2211 i=1 P (I = i) \u22c5 P (S\u2223I = i) \u22c5 P (hI = h\u2223I = i, S) (4)\n= T\n\u2211 i=1 P (I = i) \u22c5 P (S\u2223I = i) \u22c5 P (hI = h\u2223I = i) (5)\n= 1 T\nT\n\u2211 i=1 P (S\u2223I = i) \u22c5 P (hI = h\u2223I = i) (6)\n= 1 T \u2211\ni\u2236 hi=h P (S\u2223I = i) (7)\n= 1 T \u2211\ni\u2236 hi=h PDi(S) (8)\nThe first equality holds by law of total probability, the second equality holds since hI is independent of I given S, the third equality holds since I is uniformly distributed, the fourth equality holds since P (hI = h\u2223I = i) is either 1 or 0, and the fifth equality holds by the definition of Di.\nP2(S,h) = T\n\u2211 j=1 P (I = j) \u22c5 P (S\u2223I = j) \u22c5 P (AERM(S) = h\u2223I = j, S) (9)\n= T\n\u2211 i=1 P (I = j) \u22c5 P (S\u2223I = j) \u22c5 P (AERM(S) = h\u2223S) (10)\n= 1 T\nT\n\u2211 j=1 P (S\u2223I = j) \u22c5 P (AERM(S) = h\u2223S) (11)\n= 1 T\nT\n\u2211 j=1\nP (S\u2223I = j) \u22c5 \u2211i\u2236 hi=h PDi(S)\n\u2211Ti=1 PDj(S) (12)\n= 1 T \u2211i\u2236 hi=h PDi(S) \u2211Ti=1 PDj(S) T \u2211 j=1 PDi(S) (13)\n= 1 T \u2211\ni\u2236 hi=h PDi(S) (14)\nThe first equality holds by law of total probability, the second equality holds since AERM(S) is independent of I given S, the third equality holds since I is uniformly distributed, the fourth equality holds by the definition of the distribution DERM , and the fifth and sixth equalities are trivial. Assume item 1 in the theorem holds: \u2223E (AhI (S), S) \u2212LDI (AhI (S))\u2223 < \u03f5 with probability 1 \u2212 \u03b3 over I \u223c U[T ] and S \u223c DI . Since LDI (AhI (S)) = 0, we have that E (AhI (S), S) < \u03f5 with probability 1 \u2212 \u03b3 over I \u223c U[T ] and S \u223c DI . So, on the one hand, we have E (h,S) < \u03f5 with probability 1 \u2212 \u03b3 over the probability measure P1. Since we are in an (\u03b1,\u03b2,n)-overparameterized setting, then for any algorithm A we have that LDI (A(S)) > \u03b1 with probability at least \u03b2 over I \u223c U[T ] and S \u223c DI . So, on the other hand, we have LDI (AERM(S)) > \u03b1 with probability of at least \u03b2 over I \u223c U[T ], S \u223c DI , and AERM \u223c DERM . Since P1 = P2 we can combine the two statements above about P1 and P2 and have by the union bound that \u2223E (AERM(S), S) \u2212LDI (AERM(S))\u2223 > \u03b1 \u2212 \u03f5, holds with probability of at least \u03b2 \u2212 \u03b3 over I \u223c U[T ], S \u223c DI , and AERM \u223c DERM . So item 2 in the theorem cannot hold which concludes the proof.\nWe now use Theorem 1 and the definition of a Bayes-like random ERM to prove Theorem 2.\nTheorem 2. Let H be a hypothesis class of VC dimension d \u226b 1, and \u2113 be the 0 \u2212 1 loss. Let X \u2282 X be a set of size d shattered by HX = {hi}2 d i=1 \u2282 H and let {Di} 2d\ni=1 be the set of realizable distributions that correspond to HX , where for all i the marginal of Di on X is uniform over X . Let ERMHX be the set of all deterministic ERM algorithms forHX . For any h \u2208HX , let Ah be an ERM algorithm that outputs h for any input sample S consistent with h. Then, for any estimator E ofH, at least one of the following conditions does not hold:\n1. With probability at least 1/2 over I \u223c U([2d]) and S \u223c DnI ,\n\u2223E (AhI (S), S) \u2212LDI (AhI (S))\u2223 < d \u2212 n 4d .\n2. With probability at least 1/2\u2212o(1) over I \u223c U([2d]), S \u223c DnI , andAERM \u223c U(ERMHX ),\n\u2223E (AERM(S), S) \u2212LDI (AERM(S))\u2223 < d \u2212 n 4d \u2212 o(1),\nwhere U(ERMHX ) denotes the uniform distribution over ERMHX .\nIn particular, H is not (d\u2212n 4d \u2212 o(1),1/2 \u2212 o(1), n)-estimable for any n \u2264 d/2. The notation o(1) denotes quantities that vanish as d goes to infinity.\nProof. The proof is an application of Theorem 1 with the following two items:\n1. DERM = U[ERMHX ] is a Bayes-like distribution over ERM algorithms for {Di} 2d i=1,.\n2. For any algorithm A, with probability 1 \u2212 o(1) over I \u223c U([2d]) and S \u223c DnI , it holds that\nLDI (A(S)) > d \u2212 n 2d \u2212 o(1).\nTo apply Theorem 1 we have from Item 2 that \u03b1 = d\u2212n 2d \u2212 o(1) and \u03b2 = 1 \u2212 o(1) for any n \u2264 d/2.\nProof for Item 1:\nOn the one hand, PDERM (AERM(S) = h) = 1\u2223{h\u2236 h is consistent with S}\u2223 for h that is consistent with S and PDERM (AERM(S) = h) = 0 otherwise. This follows from the definition of U[ERMHX ].\nOn the other hand, for any fixed h we have the quantity \u2211i\u2236 hi=h PDi(S) \u2211Tj=1 PDj (S) . The summands in the denominator are non-zero only for indices that correspond to functions hi that are consistent with S. All such summands are equal since we have the same underlying distribution over the space X . Since all the functions {hi}2 d\ni=1 are different, the sum in the numerator contains one summand which equals any other summand in the denominator if h is consistent with S and equals 0 otherwise.\nCombining the two statements above completes the proof:\nPDERM (AERM(S) = h) = \u2211i\u2236 hi=h PDi(S) \u2211Tj=1 PDj(S) .\nProof for Item 2:\nLetA be an ERM algorithm. We note that for every realizable S overHX that consists of m distinct data points we have\nEILDI (A(S)) = d \u2212m 2d . (15)\nwhere I \u223c U[2d]. More so, for a fixed S that consist of m distinct data points, LDI (A(S)) is a random variable which is a sum of d \u2212m i.i.d. Bernoulli random variables with parameter 1/2 that is divided by d. By Hoeffding\u2019s inequality,\nP(LDI (A(S)) < d \u2212m 2d \u2212 d 0.6 d ) < exp(\u2212 2d 1.2 d \u2212m) < exp(\u22122d 0.2),\nwhich implies that for any S with \u2223S\u2223 = n it holds that\nP(LDI (A(S)) < d \u2212 n 2d \u2212 o(1)) < o(1)\nThen, with probability 1 \u2212 o(1) over I \u223c U([2d]) and S \u223c DnI , it holds that\nLDI (A(S)) > d \u2212 n 2d \u2212 o(1)."
        },
        {
            "heading": "F PROOF OF THEOREM 3",
            "text": "We start with a remark. Remark. The converse of Theorem 3 is false. Over the same overparameterized setting as in Theorem 3, an algorithm can simultaneously not learn H0 well and be not estimable. To see why, consider binary classification with the 0 \u2212 1 loss and take any algorithm A that (\u03f5, \u03b4)-learns H0 and define Aneg(S) = A(S) where h is the hypothesis with opposite labels to h. Aneg does not (1 \u2212 \u03f5,1 \u2212 \u03b4, n)-learn H0 and by the same arguments as in Theorem 3, we get that it is also not (\u03b1\u2212\u03f5 2 , \u03b3 2 \u2212 1\u2212\u03b2 T T1 +\u03b4(1+T0T1 ) 2 , n)-estimable.\nProof of Theorem 3. If (H,D) is an (\u03b1,\u03b2,n)-over parametrized setting and A (\u03f5, \u03b4, n)-learns (H,D0), thenA does not (\u03b1,1 \u2212 \u03b2TT1 + \u03b4T0 T1 , n)-learn (H,D1). This stems from the following steps.\n\u03b2 \u2264 ES,I1LDI (A(S))\u2265\u03b1\n= T0 T ES,I01LDI0 (A(S))\u2265\u03b1 + T1 T ES,I11LDI1 (A(S))\u2265\u03b1\n\u2264 T0 T ES,I01LDI0 (A(S))\u2265\u03f5 + T1 T ES,I11LDI1 (A(S))\u2265\u03b1\n\u2264 T0 T \u03b4 + T1 T ES,I11LDI1 (A(S))\u2265\u03b1\n= T0 T \u03b4 + T1 T P (LDI1 (A(S)) \u2265 \u03b1)\nThe first inequality holds by the definition of the overparameterized setting. The first equality from the law of total expectation. The second inequality holds because decreasing \u03b1 to \u03f5 only increase the probability that the indicator function outputs 1. The third inequality holds since A (\u03f5, \u03b4, n)learns (H,D0). The second equality holds since the expectation of the indicator function equals the probability for the event the indicator function underscores to happen.\nThis yields that with probability at least \u03b2T T1 \u2212 \u03b4T0 T1 over I1 \u223c T0 +U[T1] and S \u223c DnI1\nLDI1 (A(S)) \u2265 \u03b1.\nNow we show that A is not (\u03b1\u2212\u03f5 2 , \u03b7, n)-estimable with respect to H and loss \u2113. For that end, we use Theorem 1 in Angel & Spinka (2021). We denote as \u03c9 the coupling between S0 \u223c DnI0 and S1 \u223c DnI1 . We have that with probability \u03b3 over \u03c9, S0 = S1. Now, for any estimator E , we get our claim through the following steps where B = {S0 = S1, LDI0 (A(S0)) < \u03f5,LDI1 (A(S1)) > \u03b1}.\nE \u03c9 [1\u2223E(A,S0)\u2212LDI0 (A(S0))\u2223\u2265\u03b1\u2212\u03f52 + 1\u2223E(A,S1)\u2212LDI1 (A(S1))\u2223\u2265\u03b1\u2212\u03f52 ] =\nP(B) E \u03c9\u2223B [1\u2223E(A,S0)\u2212LDI0 (A(S0))\u2223\u2265\u03b1\u2212\u03f52 + 1\u2223E(A,S1)\u2212LDI1 (A(S1))\u2223\u2265\u03b1\u2212\u03f52 ] +\nP(Bc) E \u03c9\u2223Bc [1\u2223E(A,S0)\u2212LDI0 (A(S0))\u2223\u2265\u03b1\u2212\u03f52 + 1\u2223E(A,S1)\u2212LDI1 (A(S1))\u2223\u2265\u03b1\u2212\u03f52 ] \u2265\nP(B) E \u03c9\u2223B [1\u2223E(A,S0)\u2212LDI0 (A(S0))\u2223\u2265\u03b1\u2212\u03f52 + 1\u2223E(A,S1)\u2212LDI1 (A(S1))\u2223\u2265\u03b1\u2212\u03f52 ] \u2265\nP(S0 = S1, LDI0 (A(S0)) < \u03f5,LDI1 (A(S1)) \u2265 \u03b1) \u2265\n\u03b3 \u2212 \u03b4 \u2212 (1 \u2212 \u03b2T T1 + \u03b4T0 T1 ) =\n2\u03b7\nThe first equality holds by the law of total expectation for any event B. The first inequality holds since the expectations are over non-negative random variables. The second inequality holds since we assigned B = {S0 = S1, LDI0 (A(S0)) < \u03f5,LDI1 (A(S1)) > \u03b1} and when B occurs for we have E(A, S0) = E(A, S1) and any such assignment of E(A, S0) forces at least one of the indicator functions to take value 1. The third inequality holds by the union bound.\nThe proof now concludes since\nEI0,S01\u2223E(A,S0)\u2212LDI0 (A(S0))\u2223\u2265 \u03b1\u2212\u03f5 2 = E\u03c91\u2223E(A,S0)\u2212LDI0 (A(S0))\u2223\u2265\u03b1\u2212\u03f52 and\nEI1,S11\u2223E(A,S1)\u2212LDI1 (A(S1))\u2223\u2265 \u03b1\u2212\u03f5 2 = E\u03c91\u2223E(A,S1)\u2212LDI1 (A(S1))\u2223\u2265\u03b1\u2212\u03f52\nso at least one of the following items holds:\n\u2022 With probability at least \u03b7 over I0 and S0 it holds that\n\u2223E(A, S0) \u2212LDI0 (A(S0))\u2223 \u2265 \u03b1 \u2212 \u03f5 2 .\n\u2022 With probability at least \u03b7 over I1 and S1 it holds that\n\u2223E(A, S1) \u2212LDI1 (A(S1))\u2223 \u2265 \u03b1 \u2212 \u03f5 2 ."
        },
        {
            "heading": "G FINITE FIELDS AND LIMITATIONS ON ESTIMABILITY IN MULTICLASS CLASSIFICATION",
            "text": "Let us first revise Def. 5 from the main text, since it is a preliminary for the proofs to follow: Definition 5. Let X = Fdq , Y = Fq , andH = Linq (d). We say a learning algorithmA \u2236 (X \u00d7Y)\u2217 \u2192 H (possibly randomized) is an ERM algorithm with a linear bias if for every sample size n \u2264 d, we can associate (A, n) with a linear subspace Hn \u2282 H of dimension n such that if \u2223S\u2223 = n and S is consistent with some function in Hn, then A(S) \u2208 Hn. We denote by Alin the set of all ERM algorithms with a linear bias.\nWe restate the definition of linear functions and add further definitions: Definition 8.\n\u2022 Let q be prime and denote by Fq the finite field with q elements. The hypothesis class of linear functionals Linq (d) over the vector space Fdq is defined as\nLinq (d) \u2261 (Fdq) \u2217 \u2236= {fa \u2236 Fdq \u2192 Fq \u2236 a \u2208 Fdq , fa(x) =\nd\n\u2211 i=1 ai \u22c5 xi mod q} .\n\u2022 We partition Linq(d) into the linear subspaces Linq,i(d) \u2236= {fa \u2208 Linq(d) \u2236 a1 = i} with \u2223Linq,i(d)\u2223 = qd\u22121 and Linq,i(d,n) \u2236= {fa \u2208 Linq(d) \u2236 (a1 = i)\u2227(\u2200j \u2208 [n + 2, . . . , d] \u2236 aj = 0)}, where i \u2208 {0, . . . q \u2212 1} and \u2223Linq,i(d,n)\u2223 = qn.\n\u2022 We denote by X \u2208 Fn\u00d7dq the matrix obtained by stacking the inputs {xi}ni=1 as rows.\n\u2022 We denote by ei the canonical basis vector with entry 1 in position i and 0 everywhere else.\n\u2022 Further, we denote by Ai \u2282 Alin the class of ERMs that are biased towards Linq,i(d,n). In more detail: whenever A \u2208 Ai receives a sample S of size n and there exist hypotheses in Linq,i(d,n) consistent with the labels of S, A outputs one of them. Otherwise A outputs an arbitrary hypothesis from Linq(d,n)."
        },
        {
            "heading": "G.1 PROOF OF LEMMA 1",
            "text": "Lemma 1. Each two distinct functions f, h \u2208 Linq(d) agree on a fraction 1/q of the space and the 0 \u2212 1 risk of the function h over samples from Df = f \u25c7 U(Fdq) is given by\nLDf (h) = { 0 h = f 1 \u2212 1/q h \u2260 f .\nProof. Denote the coefficient vectors of f, h by af , ah, respectively. Moreover, denote by Xa \u2286 X the subset of the domain on which f and h agree, i.e., the fraction over which they agree is \u2223Xa\u2223/\u2223X \u2223. Then, for any given input x \u2208 Fdq , f(x) = h(x) if and only if c \u22c5 x = 0 mod q where c \u2236= af \u2212 ah mod q.\nNow assume w.l.o.g. that the respective last entries of ah and af are distinct (if not, perform an appropriate permutation). Then, the first d \u2212 1 entries of x can be chosen arbitrarily and there are qd\u22121 distinct such choices. Thereafter, the last entry xd is fixed to be cd \u22c5 xd = q \u2212 \u2211d\u22121i=1 ci \u22c5 xi mod q. Note that for q prime such an xd \u2208 Fq always exists and is unique. In summary, we have that \u2223Xa\u2223 = qd\u22121 which together with \u2223X \u2223 = qd gives the first desired claim. The second claim then simply follows from observing that x \u223c U(X ) together with the fact that we are using the 0 \u2212 1 loss."
        },
        {
            "heading": "G.2 LEMMA 2",
            "text": "Lemma 2. The probability that an n1 \u00d7 n2 matrix with coefficients drawn i.i.d. from U({0, . . . , q \u2212 1}) has rank r is given by\nRq(n1, n2, r) = [n2r ]q r\n\u2211 l=0 (\u22121)r\u2212l[rl]qq n1(l\u2212n2)+(r\u2212l2 ) (16)\nwhere [nk]q \u2236=\u220f k\u22121 i=0 qn\u2212i\u22121 qk\u2212i\u22121 denote the so-called Gaussian coefficients.\nIn particular, the probability that an n1 \u00d7n2 matrix with coefficients drawn i.i.d. from U({0, . . . , q\u2212 1}) has full rank is given by\nRq(n1, n2, c1) = c1\u22121 \u220f k=0 (1 \u2212 qk\u2212c2). (17)\nwhere c1 \u2236=min{n1, n2} and c2 \u2236=max{n1, n2}\nProof. The first statement is a Corollary of (Blake & Studholme, 2006, Corollary 2.2).\nThe second statement can be inferred from the first, but also follows directly from the following simple iterative construction: assume w.l.o.g. that n1 \u2264 n2. Then, at each time t = 0, . . . n1 \u2212 1 we add one row to the matrix. Assuming that each entry of the matrix is sampled i.i.d. uniformly at random, the probability that at time t the new row is linearly independent of all previous rows is then given by 1 \u2212 qt\u2212n2 since there are qt possible linear combinations of t rows, and there are qn2 vectors of length n2 in total."
        },
        {
            "heading": "G.3 LEMMA 3",
            "text": "Lemma 3. Denote by X\u2212 \u2208 Fn\u00d7n+1q the matrix obtained from X \u2208 Fn\u00d7dq by dropping all but the first n + 1 columns. Denote k \u2236= rank(X\u2212) and assume that the labels y of the samples S = (X,y) are generated by some f \u2208 Linq,i(d,n), i.e., y = f(X) where f \u2236 Fn\u00d7dq \u2192 Fnq is understood to be applied row-wise. Then,\n\u2022 If the vector e1 is spanned by the rows of X\u2212, there exist qn+1\u2212k functions in Linq,i(d,n) consistent with S and no consistent function in all other classes Linq,j(d,n), j \u2260 i.\n\u2022 If the vector e1 is not spanned by the rows of X\u2212, there exist qn\u2212k functions consistent with S in each Linq,j(d,n), j \u2208 {0, . . . , q \u2212 1}.\nProof. Let fa be a linear functions parametrized by coefficient vectors a = [a1, . . . , ad]. Further, let a\u2032 \u2208 Fn+1q denote the truncated coefficient vectors of a over the first n + 1 coordinates. Assume first that e1 is spanned by the rows of X\u2212. Then, only functions in Linq,i(d,n) can be consistent with S. This is because a\u20321 = c\u22ba \u22c5X\u2212 \u22c5 a\u2032 = c\u22ba \u22c5 y is uniquely determined for c \u2208 Fnq if c is chosen such that it encodes the linear combination of rows of X\u2212 that yields e1. The preceding equation hence holds iff a1 = i due to the assumption that f \u2208 Linq,i(d,n). Since the rank of X\u2212 is k, its null space has dimension n + 1 \u2212 k and there exist qn+1\u2212k consistent functions in total because X\u2212 \u22c5 (a + b) = y is also consistent for all b \u2208 null(X\u2212) assuming that the data was generated by fa. Assume now that the rows of X\u2212 do not span e1. Then, we can construct the reduced matrix Xred \u2208 Fk\u00d7(n+1)q and a vector yred \u2208 Fkq by retaining only k = rank(X\u2212) linearly independent rows\nof X\u2212 and the corresponding entries of y. Note that since the sample S is generated by some f \u2208 Linq,i(d,n), we have that X\u2212 \u22c5a\u2032 = y if and only if Xred \u22c5a\u2032 = yred. Hence it is sufficient to work with the reduced system of equations in order to check for the consistency of a function fa.\nNext, we append above the top of the matrix Xred the row e1 and n \u2212 k further canonical basis vectors ei1 , . . . , ein\u2212k in order to obtain a full rank matrix Xext. This is always possible because e1 is not spanned by assumption and furthermore there always exist at least n + 1 \u2212 k canonical basis vectors which are not spanned by the rows of Xred.\nWith this setup, one can then obtain for given X and y a function fh \u2208 Linq,m(d,n) with m \u2208 {0, . . . , q \u2212 1} arbitrary and reduced coefficient vector h\u2032 \u2236= [m h\u0302\u22ba]\u22ba \u2208 Fn+1q by computing the unique solution of the fully determined system of equations\n\u23a1\u23a2\u23a2\u23a2\u23a2\u23a2\u23a2\u23a2\u23a2\u23a3 e1 ei1 \u22ee ein\u2212k Xred \u23a4\u23a5\u23a5\u23a5\u23a5\u23a5\u23a5\u23a5\u23a5\u23a6 [m h\u0302 ] = \u23a1\u23a2\u23a2\u23a2\u23a2\u23a3 m z yred \u23a4\u23a5\u23a5\u23a5\u23a5\u23a6 (18)\nfor z \u2208 Fn\u2212kq arbitrarily chosen. By construction, the function fh with h = [h\u2032,0, . . . ,0]\u22ba will then be in Linq,m(d,n) and be consistent with the sample (X,y). Moreover, since z \u2208 Fn\u2212kq can be chosen freely, there are qn\u2212k such functions in each class Linq,j(d,n), j \u2208 {0, . . . , q \u2212 1}."
        },
        {
            "heading": "G.4 PROOF OF THEOREM 4",
            "text": "We consider the learning settings with realizable distributions over H\u2032 \u2236= H0 \u222a H1 where H0 \u2236= Linq,0(d,n) and H1 \u2236= Linq,1(d,n). The algorithms we consider come from A0, the class of ERMs that are biased toH0 = Linq,0(d,n), that is, if there exists a consistent hypothesis inH0, the algorithm must choose one such hypothesis. As mentioned previously, it is sufficient to consider such ERMs in order to show learnability and estimability results over the larger class of linearly constrained ERMs Alin due to a symmetry argument.\nWe first show in Lemma 4 that the learnability condition (condition 1 in Theorem 3) holds for H0. In Lemma 5 we show that attempting to learn aboveH\u2032 corresponds to an overparameterized setting. It then follows Theorem 4 then follows from an upper bound on the TV-distance which implies that condition 2 in Theorem 3 cannot hold, i.e., H\u2032 is not estimable. Since Linq(d) \u2287 H\u2032, this directly implies thatH = Linq(d) is also not estimable with the same parameters. Lemma 4. Linq,0(d,n) is (0, \u03b4, n)-learnable over D0 = {f \u25c7 U(X )\u2223f \u2208 Linq,0(d,n)} with any A \u2208 A0 for n \u2208 [d \u2212 1] and \u03b4 > \u2211n\u22121k=0 (1 \u2212 qk\u2212n) \u22c5Rq(n,n, k) where Rq is defined as in Lemma 2.\nProof. For the analysis one can drop the columns 1, n + 2, n + 3, . . . d of X as hypotheses in Linq,0(d,n) are invariant w.r.t. to these coordinates and it is sufficient for A to only check for consistency. Thereby we obtain the reduced input matrix X \u2032 of dimensions n \u00d7 n. Clearly, any A \u2208 A0 outputs the ground truth hypothesis w.p. 1 as soon as X \u2032 has full rank. On the other hand, once X \u2032 does not have full rank, A might output the wrong hypothesis depending on its preference, i.e., depending on which of the multiple consistent functions it outputs. It can be easily verified that any preference (be it deterministic or random) of A amongst consistent hypotheses leads to the same error probability since the random variable I that selects the labeling function is uniformly distributed.\nSetting \u03f5 = 0, we get by the law of total probability that Linq,0(d,n) is (0, \u03b4, n)-learnable by any A \u2208 A0 if\n\u03b4 > P({AERM(S) \u2260 f}) (19)\n= n\u22121 \u2211 k=0 P({AERM(S) \u2260 f}\u2223{X \u2032 has rank k}) \u22c5 P({X \u2032 has rank k}) (20) = n\u22121 \u2211 k=0 (1 \u2212 qk\u2212n) \u22c5Rq(n,n, k) (21)\nwhere for the last equality we used the following two facts:\nConditioned on the event that rank(X \u2032) = k, A returns the ground truth with probability qk\u2212n. This is because Pf({A(S) = f}\u2223S) is uniform over all f \u2208 Linq,0(d,n) consistent with S and since there are qn\u2212k functions consistent with k linearly independent samples (see the argument in Lemma 3).\nFor controlling the probability of rank deficiency we used the result stated in Lemma 2.\nLemma 5. Linq,0(d,n) \u222a Linq,1(d,n) is not (\u03b1,\u03b2,n)-learnable with any A \u2208 A0 over D = {f \u25c7 U(X )\u2223f \u2208 Linq,0(d,n) \u222a Linq,1(d,n)} for n \u2208 [d \u2212 1] and any (\u03b1,\u03b2) such that simultaneously \u03b1 < (q \u2212 1)/q and \u03b2 < 1\n2 \u2211 n k=0 ((qk\u2212n \u2212 2qk\u2212n\u22121) q k\u22121 qn+1\u22121 + 2 \u2212 q k\u2212n) \u00d7Rq(n,n + 1, k).\nProof. Note that learnability is trivial for \u03b1 \u2265 (1 \u2212 1/q) = q\u22121 q\nsince this is an upper bound for the risk of linear hypotheses according to Lemma 1. Hence we consider the case \u03b1 < (q \u2212 1)/q. Then, by definition, (\u03b1,\u03b2)-learnability is precluded for any algorithm that outputs a linear hypothesis once \u03b2 < P({A(S) \u2260 f}) since {A(S) \u2260 f} implies LDf (A(S)) = q\u22121q . In the sequel we aim to derive P({A(S) \u2260 f}) for the case where A \u2208 A0.\nDenote by X\u2212 \u2208 Fn\u00d7(n+1)q the matrix obtained by dropping all but the first n + 1 columns of X . Denoting the events E1 \u2236= {A(S) \u2260 f}, E2,i \u2236= {f \u2208 Linq,i(d,n)}, E3 \u2236= {e1 spanned by the rows of X\u2212} and E4,k \u2236= {X\u2212 has rank k}, we have by the law of total probability that for i \u2208 {0,1},\nP(E1\u2223E2,i \u2229E4,k) = P(E1\u2223E2,i \u2229E3 \u2229E4,k)P(E3\u2223E2,i \u2229E4,k) (22) + P(E1\u2223E2,i \u2229Ec3 \u2229E4,k)P(Ec3\u2223E2,i \u2229E4,k) (23)\n= P(E1\u2223E2,i \u2229E3 \u2229E4,k)P(E3\u2223E4,k) + P(E1\u2223E2,i \u2229Ec3 \u2229E4,k)P(Ec3\u2223E4,k). (24)\nWe can quantify the terms appearing above as follows:\nP(E1\u2223E2,i \u2229 E3 \u2229 E4,k) = 1 \u2212 qk\u2212n\u22121 for i \u2208 {0,1} since we know from Lemma 3 that E3 \u2229 E4,k implies that there are qn+1\u2212k consistent functions in Linq,i(d,n). As f is selected uniformly at random from Linq,i(d,n), the statement follows.\nP(E1\u2223E2,0 \u2229 Ec3 \u2229 E4,k) = 1 \u2212 qk\u2212n because we know from Lemma 3 that Ec3 \u2229 E4,k implies that there are qn\u2212k consistent functions in each Linq,i(d,n). P(E1\u2223E2,1 \u2229Ec3 \u2229E4,k) = 1 since A will almost surely select a hypothesis from Linq,0(d,n) even though f \u2208 Linq,i(d,n) if e1 is not spanned.\nP(E3\u2223E4,k) = 1 \u2212 P(Ec3\u2223E4,k) = q k\u22121 qn+1\u22121 follows from the fact that X \u2212 spans some k-dimensional row space uniformly at random and there are qk \u2212 1 possible non-zero linear combinations of k linearly independent rows and qn+1 \u2212 1 non-zero vectors of length n + 1.\nCombining above facts we have that Linq,0(d,n) \u222aLinq,1(d,n) is not (\u03b1,\u03b2,n)-learnable by A for \u03b1 < q\u22121\nq and\n\u03b2 < P(E1) = 1\n\u2211 i=0\nn\n\u2211 k=0 P(E1\u2223E2,i \u2229E4,k)P(E2,i \u2229E4,k) (25)\n= 1\n\u2211 i=0\nP(E2,i) n\n\u2211 k=0 (P(E1\u2223E2,i \u2229E3 \u2229E4,k) \u22c5 P(E3\u2223E4,k) (26)\n+ P(E1\u2223E2,i \u2229Ec3 \u2229E4,k) \u22c5 P(Ec3\u2223E4,k)) \u22c5 P(E4,k) (27)\n= 1 2\nn\n\u2211 k=0 ((1 \u2212 qk\u2212n\u22121) \u22c5 q k \u2212 1 qn+1 \u2212 1 + (1 \u2212 q k\u2212n) \u22c5 (1 \u2212 q k \u2212 1 qn+1 \u2212 1) (28)\n+ (1 \u2212 qk\u2212n\u22121) \u22c5 q k \u2212 1 qn+1 \u2212 1 + (1 \u2212 qk \u2212 1 qn+1 \u2212 1)) \u00d7Rq(n,n + 1, k) (29)\n= 1 2\nn\n\u2211 k=0 ((qk\u2212n \u2212 2qk\u2212n\u22121) q k \u2212 1 qn+1 \u2212 1 + 2 \u2212 q k\u2212n) \u00d7Rq(n,n + 1, k). (30)\nUsing Lemmas 4 and 5, we are now ready to prove Theorem 4: Theorem 4. For every A \u2208 Alin and every n \u2264 d \u2212 1, A is not ((q \u2212 1)/2q, \u03b7, n)-estimable with respect to Linq (d) and the 0 \u2212 1 loss where \u03b7 = 12 \u2211 n k=0 [(qk\u2212n \u2212 2qk\u2212n\u22121 \u2212 1) q k\u22121 qn+1\u22121 + 2 \u2212 q\nk\u2212n] \u00d7 Rq(n,n + 1, k) \u2212\u2211n\u22121k=0 (1 \u2212 qk\u2212n) \u22c5Rq(n,n, k). In particular, for q > 10, it holds that \u03b7 > 0.4, and generally, \u03b7 = 1\n2 \u2212 1 q + o(1/q).\nProof. For all product distributions appearing in this section assume that DX = U(Fdq). Let D = {Di}q n+1 i=1 be the set of Linq(d,n)-realizable distributions such that D0 = {Di} qn i=1 is realizable over Linq,0(d,n), and D1 = {Di}q n+1\ni=qn+1 is realizable over Linq,1(d,n). As we are working with countable measures,\ndTV (S0, S1) = 1\n2 \u2225 P \u2212Q\u22251 =\n1 2 \u2211 S \u2223P (S) \u2212Q(S)\u2223 (31)\nwhere P and Q denote the probability measures associated with the distributions DnI0 and D n I1 , respectively, where I0 \u223c U([qn]) and I1 \u223c qn +U([qn]). First, we note that we need only sum over samples S such that their corresponding input matrices span e1 since Lemma 3 asserts that once e1 is not spanned, the number of consistent functions in both classes is the same. Since the measures P and Q are uniform both w.r.t. to the inputs and the labeling functions, they both are proportional to the numbers of consistent functions (each multiplied by the same constant factor). Further, we know from Lemma 3, that once e1 is spanned, the number of consistent functions in Linq,i(d,n) is non-zero iff S \u223c f \u25c7DX with f \u2208 Linq,i(d,n). Hence only one of P (S) and Q(S) can be non-zero at a time. Therefore,\ndTV (S0, S1) = 1\n2 \u2211 S\u2236{e1 spanned} \u2223P (S) \u2212Q(S)\u2223 (32)\n= 1 2 ( \u2211\nS\u2236{e1 spanned} P (S) + \u2211 S\u2236{e1 spanned} Q(S)) (33)\n= P({e1 spanned}). (34) According to the definitions in Theorem 3 we can hence pick\n\u03b3 = 1 \u2212 P({e1 spanned}) (35)\n= 1 \u2212 n\n\u2211 k=0 qk \u2212 1 qn+1 \u2212 1 \u00d7Rq(n,n + 1, k) (36)\nwhere we used of the fact that P({e1 spanned}\u2223{rank(X\u2212) = k}) = q k\u22121\nqn+1\u22121 .\nPlugging in above \u03b3 and values of \u03b1,\u03b2, \u03b4 in accordance to Lemmas 4 and 5 into Theorem 3, we finally obtain that Linq,0(d,n) \u222a Linq,1(d,n) is not (\u03bd, \u03b7, n)-estimable if simultaneously \u03bd < (\u03b1 \u2212 \u03f5)/2 = (q \u2212 1)/2q and\n\u03b7 < \u03b3 2 \u2212 1 + \u03b4 \u2212 \u03b2 T T1 + \u03b4 T0 T1 2 (37)\n= \u03b3 2 + \u03b2 \u2212 \u03b4 \u2212 1 2 (38)\n= 1 2 \u22c5 [1 \u2212\nn\n\u2211 k=0 qk \u2212 1 qn+1 \u2212 1 \u00d7Rq(n + 1, n, k)] (39)\n+ 1 2\nn\n\u2211 k=0 [(qk\u2212n \u2212 2qk\u2212n\u22121) q k \u2212 1 qn+1 \u2212 1 + 2 \u2212 q k\u2212n] \u00d7Rq(n,n + 1, k) (40)\n\u2212 n\u22121 \u2211 k=0 (1 \u2212 qk\u2212n) \u22c5Rq(n,n, k) \u2212 1 2 (41)\n= 1 2\nn\n\u2211 k=0 [(qk\u2212n \u2212 2qk\u2212n\u22121 \u2212 1) q k \u2212 1 qn+1 \u2212 1 + 2 \u2212 q k\u2212n] \u00d7Rq(n,n + 1, k) (42) \u2212 n\u22121 \u2211 k=0 (1 \u2212 qk\u2212n) \u22c5Rq(n,n, k) (43)\nwhere we used the fact that T0 = T1 = T /2. To get the asymptotic result, we make use of the following two bounds:\n1/q \u2212 o(1/q) \u2264 q n \u2212 1\nqn+1 \u2212 1 \u2264 1/q (44)\nRq(n,n + 1, n) \u2265 1 \u2212 o(1/q). (45) To see that equation 45 holds, recall from Lemma 2 that Rq(n,n+1, n) = (1\u2212q\u2212n\u22121) \u22c5 . . . \u22c5(1\u2212q\u22122). Now assume towards induction that the partial product \u03c0m \u2236=\u220fmj=2(1\u2212 q\u2212j), m \u2265 2 is in 1\u2212o(1/q). Then, \u03c0m+1 \u2208 (1 \u2212 o(1/q)) \u22c5 (1 \u2212 q\u2212m+1) is also in 1 \u2212 o(1/q) since q\u2212m+1 \u2208 o(1/q). We have for the base case that \u03c02 = 1 \u2212 q\u22122 is in 1 \u2212 o(1/q), hence the claim follows. Using above facts we can then lower bound equation 42 \u2212 equation 43 by lower bounding the first sum by only considering the term for which k = n, and upper bounding the sum in equation 43 by 1\u2212Rq(n,n,n) where Rq(n,n,n) \u2265 1\u22121/q\u2212o(1/q) due to an induction argument similar as above. Thereby we obtain that equation 42 \u2212 equation 43 is lower bounded by\n1 2 [(1 \u2212 2/q \u2212 1)(1/q \u2212 o(1/q)) + 2 \u2212 1](1 \u2212 o(1/q)) \u2212 [1 \u2212 (1 \u2212 1/q \u2212 o(1/q))] (46)\n= 1 2 \u2212 1 q \u2212 o(1/q). (47)\nWe thereby proved non-estimability of algorithms A \u2208 A0 with respect to H = Lin(d) and distributions families D0,D1 (defined at the beginning of the proof).\nFinally, we extend this result to the class of all linearly biased ERMs in Alin via the following reduction:\nRemark (Reduction). Without loss of generality, one can focus on the class A0 whenever proving learnability and estimability results about Alin (see Definition 5). This follows from a reduction of non-estimability with algorithmsA\u2032 \u2208 Alin to non-estimability with algorithmsA \u2208 A0 as informally discussed below.\nFirst note that any linearly constrained ERM A \u2208 Alin is implicitly parametrized by some \u03c3 \u2208 Fdq/{0} and \u03ba \u2208 Fq such that A is biased towards selecting hypotheses fb \u2208 Linq(d) such that \u2211di=1 \u03c3i \u22c5 bi = \u03ba. For example, in the case of A \u2208 A0, we have \u03c3 = e1 and \u03ba = 0.\nLet us briefly recap the setup in Theorem 4.\n\u2022 First, we introduced the d dimensional function space Linq(d).\n\u2022 Based on n, we linearly mapped the space onto the n+1 dimensional subspace Linq(d,n).\n\u2022 We assumed that A is biased to the n dimensional subspace of functions fa with a1 = 0.\n\u2022 Finally, we showed learnability (with A) over the subspace H0 = Linq,0(d,n) and nonlearnability over the subspaceH0 \u222aH1, whereH1 = Linq,1(d,n).\nWe now sketch how for arbitrary A\u2032 \u2208 Alin with bias coefficients (\u03c3,\u03ba) one can find appropriate subspaces of Linq(d) that admit essentially the same properties as the ones studied in the proof of Theorem 4 for A \u2208 A0. For W \u2282 Fdq some linear subspace define by Linq(W ) the space of linear functions over Fdq with coefficients from W . Now consider the following setup: given \u03c3,\u03ba as defined above, project Fdq onto an n + 1 dimensional subspace V such that Linq(V ) \u2282 Linq(d) via an appropriate linear map \u03a0 such that \u03c3 is not in its kernel. Now define \u03c3\u2032 = \u03a0\u03c3 \u2260 0 and letH\u20320 be the n dimensional subspace of V consisting of functions fb such that \u2211n+1i=1 \u03c3\u2032i \u22c5 bi = \u03ba. Similarly, define H\u20321 to be the set consisting of functions fb such that \u2211n+1i=1 \u03c3\u2032i \u22c5 bi = \u03ba + 1. The reduction now boils down to the fact that showing learnability of H\u20320 and non-learnability of H\u20320 \u222a H\u20321 (together with distribution families D\u20320,D\u20321 consisting of distributions with uniform marginals over Fdq and labelings from H\u20320,H\u20321, respectively) with A\u2032 can be shown analogously to the setup (A,H0,H1,D0,D1) from Theorem 4 via simple some adaptions of the steps involved in proving it. In some more (but not full) detail:\n\u2022 Since the linear spaces Linq(d,n) and Linq(V ) (and their above mentioned subspaces) have the same cardinalities, it follows that all proofs based solely on counting functions subject to a fixed number of linear constraints carry over one-to-one.\n\u2022 Combining linear coefficients of two linear functions fa, fb \u2208 Linq(d) trivially yields another linear function fa+b \u2208 Linq(d). Hence Lemma 1 applies.\n\u2022 Any linear combination (modulo q) of i.i.d. random variables uniform over Fq is again uniform. This fact together with our assumption of i.i.d. uniform inputs means that Lemma 2 carries over.\n\u2022 For deriving the numbers of consistent functions in Lemma 3, we first \u2019preprocess\u2019 the inputs via the mapping \u03a0X with \u03a0 as defined above. Recall that in the original setup this mapping simply amounted to truncating the input vectors.\n\u2022 One can also easily adapt Lemma 5 since spanning e1 has the same probability as spanning any arbitrary fixed \u03c3\u2032. The TV distance appearing in the final steps of showing Theorem 4 is the same in both setups for the same reason."
        },
        {
            "heading": "H LIMITATIONS ON ESTIMABILITY IN BINARY CLASSIFICATION",
            "text": "Let us restate Theorem 5 for convenience:\nTheorem 5. For everyA \u2208 Alin (possibly randomized) and every n \u2264 d\u22121,A is not (0.25,0.14, n)estimable with respect to Lin2 (d) and the 0\u2212 1 loss. More so, for every deterministicA \u2208 Alin and every 6 \u2264 n \u2264 d, A is not (0.25,0.32, n)-estimable with respect to Lin2 (d) and the 0 \u2212 1 loss."
        },
        {
            "heading": "H.1 PROOF OF THEOREM 5 FOR A DETERMINISTIC",
            "text": "Proof. We consider the case q = 2 and examine when estimability is not possible for any algorithm A \u2208 A0 by bounding the accuracy of the optimal estimator E\u2217. The result once again carries over to the more general caseA \u2208 Alin due the reduction argument described in the remark appearing at the end of the previous section.\nDue to an argument analogous to the one in Lemma 3, we have that for n \u2264 d and rank(X) = k, there are 2d\u2212k consistent functions in Lin2(d). By definition, in order to have (\u03bd, \u03b7, n)-estimability, E\u2217 can have failure probability at most \u03b7(\u03bd, n) for given \u03bd, n. Since for any given S, f is uniform over all consistent functions in Lin(d), the optimal estimator E\u2217 assigns 0 whenever k = n, since in this case any A \u2208 A0 outputs the ground truth almost surely. Moreover, any estimator E\u2217 that is optimal for a fixed error level \u03bd must necessarily assign a value c\u03bd \u2208 (0.5 \u2212 \u03bd,0.5] whenever k < n, since then there are multiple consistent functions in Lin2(d), and simultaneously P({A(S) = h}) \u2264 1\n2 for ground truth hypothesis h. To expand on this, note that\nif E\u2217 were to assign any value in [0,0.5 \u2212 \u03bd] while there exist m \u2265 2 functions consistent with S, this would cause the fidelity terms Fi \u2236= \u2223E\u2217(A, S)\u2212LDi(A(S))\u2223 to exceed the threshold \u03bd in m\u22121 of the instances and only in a single instance fall below \u03bd. This would obviously yield an increased overall error probability and hence be suboptimal. We can therefore assume w.l.o.g. that E\u2217 has c\u03bd = 0.5. Given that \u03bd = 0.25 and n < d, this estimator then fails (i.e. exceeds the error level \u03bd) over {f \u25c7 U(X )\u2223f \u2208 Lin2,0(d,n) \u222aLin2,1(d,n)} with probability\nP({E\u2217(A, S) \u2260 LD(A(S))}) = 1\n2\nn\n\u2211 k=0 (2k\u2212n\u22121 \u22c5 2 k \u2212 1 2n+1 \u2212 1 + 2 k\u2212n \u22c5 (1 \u2212 2 k \u2212 1 2n+1 \u2212 1) (48)\n+ 2k\u2212n\u22121 \u22c5 2 k \u2212 1\n2n+1 \u2212 1) \u00d7R2(n,n + 1, k) (49)\n= n\n\u2211 k=0\n2k\u2212n\u22121 \u22c5R2(n,n + 1, k) (50)\nwhere the derivation is analogous to the one of \u03b2 in Lemma 5.\nOn the other hand, for n = d, from a similar argument it follows that over {f \u25c7U(X )\u2223f \u2208 Lin2(d)}, given that rank(X) = k, A picks the ground truth with probability 2k\u2212d and hence E\u2217 fails with probability\nP({E\u2217(A, S) \u2260 LD(A(S))}) = d\u22121 \u2211 k=0 2k\u2212d \u22c5R2(n,n, k). (51)\nCombining equation 50 with equation 51 we obtain that for all n \u2264 d, over Lin2(d), the optimal estimator E\u2217 fails with probability\nP({E\u2217(A, S) \u2260 LD(A(S))}) = m\n\u2211 k=0\n2k\u2212m\u22121 \u22c5R2(n,m + 1, k). (52)\nwhere m \u2236= min{n, d \u2212 1}. It can be easily verified that for fixed d, equation 52 is monotonically decreasing in n. Moreover, when fixing n = d, equation 52 is increasing in d and exceeds 0.32 for all d \u2265 6."
        },
        {
            "heading": "H.2 PROOF OF THEOREM 5 FOR A RANDOM",
            "text": "Proof. Assume w.l.o.g. that A randomly outputs a consistent function in Lin2,0 whenever possible. Define E\u2212 = {rank(X\u2212) = n} \u2229 {e1 not spanned}. Over the class Lin2(d,n), it follows from equation 44 together with the fact that R2(n,n + 1, n) > 0.57 for all n \u2265 1 that\nP(E\u2212) > 0.57 \u22c5 0.5 (53) where X\u2212 denotes the reduced n\u00d7 (n+1) input matrix (recall that over Lin2(d,n) it is sufficient to process the first n + 1 coordinates of the inputs). But in this case, there exists exactly one consistent\nfunction in each Lin2,0(d,n) and Lin2,1(d,n). Due to a argument similar to the one presented in the preceding subsection, we know that upon observing E\u2212, an optimal estimator E\u2217 of the population risk assigns 0 since this yields the optimal accuracy. But then, E\u2217 exceeds the error level \u03bd = 0.25 under the event {h \u2208 Lin2,1} for h denoting the ground truth labeling function. Since this event has probability 0.5, we can conclude that E\u2217 fails with probability at least 0.57 \u22c5 0.5 \u22c5 0.5 > 0.14. Since this implies that Lin2,0(d,n) \u222a Lin2,1(d,n) is not (0.25,0.14, n)-estimable, if follows that the superset Lin2(d) is also not estimable with the same parameters."
        }
    ],
    "title": "FANTASTIC GENERALIZATION MEASURES",
    "year": 2023
}