{
    "abstractText": "We find that large language models (LLMs) are more likely to modify humanwritten text than AI-generated text when tasked with rewriting. This tendency arises because LLMs often perceive AI-generated text as high-quality, leading to fewer modifications. We introduce a method to detect AI-generated content by prompting LLMs to rewrite text and calculating the editing distance of the output. We dubbed our geneRative AI Detection viA Rewriting method Raidar. Raidar significantly improves the F1 detection scores of existing AI content detection models \u2013 both academic and commercial \u2013 across various domains, including News, creative writing, student essays, code, Yelp reviews, and arXiv papers, with gains of up to 29 points. Operating solely on word symbols without high-dimensional features, our method is compatible with black box LLMs, and is inherently robust on new content. Our results illustrate the unique imprint of machine-generated text through the lens of the machines themselves.",
    "authors": [
        {
            "affiliations": [],
            "name": "Chengzhi Mao"
        },
        {
            "affiliations": [],
            "name": "Carl Vondrick"
        },
        {
            "affiliations": [],
            "name": "Hao Wang"
        },
        {
            "affiliations": [],
            "name": "Junfeng Yang"
        }
    ],
    "id": "SP:e325b1378361edf9871b54c5895e1fa2d63456c5",
    "references": [
        {
            "authors": [
                "Mohammad Asfour",
                "Juan Carlos Murillo"
            ],
            "title": "Harnessing large language models to simulate realistic human responses to social engineering attacks: A case study",
            "venue": "International Journal of Cybersecurity Intelligence & Cybercrime,",
            "year": 2023
        },
        {
            "authors": [
                "Anton Bakhtin",
                "Sam Gross",
                "Myle Ott",
                "Yuntian Deng",
                "Marc\u2019Aurelio Ranzato",
                "Arthur Szlam"
            ],
            "title": "Real or fake? learning to discriminate machine from human generated text",
            "year": 1906
        },
        {
            "authors": [
                "A Stevie Bergman",
                "Gavin Abercrombie",
                "Shannon Spruit",
                "Dirk Hovy",
                "Emily Dinan",
                "Y-Lan Boureau",
                "Verena Rieser"
            ],
            "title": "Guiding the release of safer e2e conversational ai through value sensitive design",
            "venue": "In Proceedings of the 23rd Annual Meeting of the Special Interest Group on Discourse and Dialogue. Association for Computational Linguistics,",
            "year": 2022
        },
        {
            "authors": [
                "Joseph Berkson"
            ],
            "title": "Application of the logistic function to bio-assay",
            "venue": "Journal of the American Statistical Association,",
            "year": 1944
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Souradip Chakraborty",
                "Amrit Singh Bedi",
                "Sicheng Zhu",
                "Bang An",
                "Dinesh Manocha",
                "Furong Huang"
            ],
            "title": "On the possibilities of ai-generated text detection",
            "venue": "arXiv preprint arXiv:2304.04736,",
            "year": 2023
        },
        {
            "authors": [
                "Tianqi Chen",
                "Carlos Guestrin"
            ],
            "title": "Xgboost: A scalable tree boosting system",
            "venue": "In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,",
            "year": 2016
        },
        {
            "authors": [
                "Aakanksha Chowdhery",
                "Sharan Narang",
                "Jacob Devlin",
                "Maarten Bosma",
                "Gaurav Mishra",
                "Adam Roberts",
                "Paul Barham",
                "Hyung Won Chung",
                "Charles Sutton",
                "Sebastian Gehrmann"
            ],
            "title": "Palm: Scaling language modeling with pathways",
            "venue": "arXiv preprint arXiv:2204.02311,",
            "year": 2022
        },
        {
            "authors": [
                "Debby RE Cotton",
                "Peter A Cotton",
                "J Reuben Shipway"
            ],
            "title": "Chatting and cheating: Ensuring academic integrity in the era of chatgpt",
            "venue": "Innovations in Education and Teaching International,",
            "year": 2023
        },
        {
            "authors": [
                "Yao Dou",
                "Maxwell Forbes",
                "Rik Koncel-Kedziorski",
                "Noah A Smith",
                "Yejin Choi"
            ],
            "title": "Is gpt-3 text indistinguishable from human text? scarecrow: A framework for scrutinizing machine text",
            "venue": "arXiv preprint arXiv:2107.01294,",
            "year": 2021
        },
        {
            "authors": [
                "Tiziano Fagni",
                "Fabrizio Falchi",
                "Margherita Gambini",
                "Antonio Martella",
                "Maurizio Tesconi"
            ],
            "title": "Tweepfake: About detecting deepfake tweets",
            "venue": "Plos one,",
            "year": 2021
        },
        {
            "authors": [
                "Sebastian Gehrmann",
                "Hendrik Strobelt",
                "Alexander M Rush"
            ],
            "title": "Gltr: Statistical detection and visualization of generated text",
            "venue": "arXiv preprint arXiv:1906.04043,",
            "year": 2019
        },
        {
            "authors": [
                "Daphne Ippolito",
                "Daniel Duckworth",
                "Chris Callison-Burch",
                "Douglas Eck"
            ],
            "title": "Automatic detection of generated text is easiest when humans are fooled",
            "year": 1911
        },
        {
            "authors": [
                "Ganesh Jawahar",
                "Muhammad Abdul-Mageed",
                "Laks VS Lakshmanan"
            ],
            "title": "Automatic detection of machine generated text: A critical survey",
            "venue": "arXiv preprint arXiv:2011.01314,",
            "year": 2020
        },
        {
            "authors": [
                "Di Jin",
                "Zhijing Jin",
                "Joey Tianyi Zhou",
                "Peter Szolovits"
            ],
            "title": "Is bert really robust? natural language attack on text classification and entailment",
            "year": 1907
        },
        {
            "authors": [
                "Daniel Kang",
                "Xuechen Li",
                "Ion Stoica",
                "Carlos Guestrin",
                "Matei Zaharia",
                "Tatsunori Hashimoto"
            ],
            "title": "Exploiting programmatic behavior of llms: Dual-use through standard security attacks",
            "venue": "arXiv preprint arXiv:2302.05733,",
            "year": 2023
        },
        {
            "authors": [
                "John Kirchenbauer",
                "Jonas Geiping",
                "Yuxin Wen",
                "Jonathan Katz",
                "Ian Miers",
                "Tom Goldstein"
            ],
            "title": "A watermark for large language models",
            "venue": "arXiv preprint arXiv:2301.10226,",
            "year": 2023
        },
        {
            "authors": [
                "Takeshi Kojima",
                "Shixiang Shane Gu",
                "Machel Reid",
                "Yutaka Matsuo",
                "Yusuke Iwasawa"
            ],
            "title": "Large language models are zero-shot reasoners",
            "venue": "Advances in neural information processing systems,",
            "year": 2022
        },
        {
            "authors": [
                "Kalpesh Krishna",
                "Yixiao Song",
                "Marzena Karpinska",
                "John Wieting",
                "Mohit Iyyer"
            ],
            "title": "Paraphrasing evades detectors of ai-generated text, but retrieval is an effective defense",
            "venue": "arXiv preprint arXiv:2303.13408,",
            "year": 2023
        },
        {
            "authors": [
                "Vladimir I Levenshtein"
            ],
            "title": "Binary codes capable of correcting deletions, insertions, and reversals",
            "venue": "Soviet Physics Doklady,",
            "year": 1966
        },
        {
            "authors": [
                "Junyi Li",
                "Tianyi Tang",
                "Wayne Xin Zhao",
                "Jian-Yun Nie",
                "Ji-Rong Wen"
            ],
            "title": "Pretrained language models for text generation: A survey",
            "venue": "arXiv preprint arXiv:2201.05273,",
            "year": 2022
        },
        {
            "authors": [
                "Xiang Lisa Li",
                "Percy Liang"
            ],
            "title": "Prefix-tuning: Optimizing continuous prompts for generation",
            "venue": "arXiv preprint arXiv:2101.00190,",
            "year": 2021
        },
        {
            "authors": [
                "Weixin Liang",
                "Mert Yuksekgonul",
                "Yining Mao",
                "Eric Wu",
                "James Zou"
            ],
            "title": "Gpt detectors are biased against non-native english writers",
            "venue": "arXiv preprint arXiv:2304.02819,",
            "year": 2023
        },
        {
            "authors": [
                "Fatemehsadat Mireshghallah",
                "Justus Mattern",
                "Sicun Gao",
                "Reza Shokri",
                "Taylor Berg-Kirkpatrick"
            ],
            "title": "Smaller language models are better black-box machine-generated text detectors",
            "venue": "arXiv preprint arXiv:2305.09859,",
            "year": 2023
        },
        {
            "authors": [
                "Yisroel Mirsky",
                "Ambra Demontis",
                "Jaidip Kotak",
                "Ram Shankar",
                "Deng Gelei",
                "Liu Yang",
                "Xiangyu Zhang",
                "Maura Pintor",
                "Wenke Lee",
                "Yuval Elovici"
            ],
            "title": "The threat of offensive ai to organizations",
            "venue": "Computers & Security,",
            "year": 2022
        },
        {
            "authors": [
                "Eric Mitchell",
                "Yoonho Lee",
                "Alexander Khazatsky",
                "Christopher D Manning",
                "Chelsea Finn"
            ],
            "title": "Detectgpt: Zero-shot machine-generated text detection using probability curvature",
            "venue": "arXiv preprint arXiv:2301.11305,",
            "year": 2023
        },
        {
            "authors": [
                "Yikang Pan",
                "Liangming Pan",
                "Wenhu Chen",
                "Preslav Nakov",
                "Min-Yen Kan",
                "William Yang Wang"
            ],
            "title": "On the risk of misinformation pollution with large language models",
            "venue": "arXiv preprint arXiv:2305.13661,",
            "year": 2023
        },
        {
            "authors": [
                "Hammond Pearce",
                "Baleegh Ahmad",
                "Benjamin Tan",
                "Brendan Dolan-Gavitt",
                "Ramesh Karri"
            ],
            "title": "Asleep at the keyboard? assessing the security of github copilot\u2019s code contributions",
            "venue": "IEEE Symposium on Security and Privacy (SP),",
            "year": 2022
        },
        {
            "authors": [
                "Alec Radford",
                "Jeffrey Wu",
                "Rewon Child",
                "David Luan",
                "Dario Amodei",
                "Ilya Sutskever"
            ],
            "title": "Language models are unsupervised multitask learners",
            "venue": "OpenAI blog,",
            "year": 2019
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Tao Xu",
                "Greg Brockman",
                "Christine McLeavey",
                "Ilya Sutskever"
            ],
            "title": "Robust speech recognition via large-scale weak supervision",
            "venue": "In International Conference on Machine Learning,",
            "year": 2023
        },
        {
            "authors": [
                "Vinu Sankar Sadasivan",
                "Aounon Kumar",
                "Sriram Balasubramanian",
                "Wenxiao Wang",
                "Soheil Feizi"
            ],
            "title": "Can ai-generated text be reliably detected",
            "venue": "arXiv preprint arXiv:2303.11156,",
            "year": 2023
        },
        {
            "authors": [
                "Ilia Shumailov",
                "Zakhar Shumaylov",
                "Yiren Zhao",
                "Yarin Gal",
                "Nicolas Papernot",
                "Ross Anderson"
            ],
            "title": "The curse of recursion: Training on generated data makes models forget",
            "venue": "arXiv preprint arxiv:2305.17493,",
            "year": 2023
        },
        {
            "authors": [
                "Mohammed Latif Siddiq",
                "Shafayat H Majumder",
                "Maisha R Mim",
                "Sourov Jajodia",
                "Joanna CS Santos"
            ],
            "title": "An empirical study of code smells in transformer-based code generation techniques",
            "venue": "IEEE 22nd International Working Conference on Source Code Analysis and Manipulation (SCAM),",
            "year": 2022
        },
        {
            "authors": [
                "Irene Solaiman",
                "Miles Brundage",
                "Jack Clark",
                "Amanda Askell",
                "Ariel Herbert-Voss",
                "Jeff Wu",
                "Alec Radford",
                "Gretchen Krueger",
                "Jong Wook Kim",
                "Sarah Kreps"
            ],
            "title": "Release strategies and the social impacts of language models",
            "venue": "arXiv preprint arXiv:1908.09203,",
            "year": 2019
        },
        {
            "authors": [
                "Ruixiang Tang",
                "Yu-Neng Chuang",
                "Xia Hu"
            ],
            "title": "The science of detecting llm-generated texts",
            "venue": "arXiv preprint arXiv:2303.07205,",
            "year": 2023
        },
        {
            "authors": [
                "Vivek Verma",
                "Eve Fleisig",
                "Nicholas Tomlin",
                "Dan Klein"
            ],
            "title": "Ghostbuster: Detecting text ghostwritten by large language models",
            "venue": "arXiv preprint arXiv:2305.15047,",
            "year": 2023
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "Fei Xia",
                "Ed Chi",
                "Quoc V Le",
                "Denny Zhou"
            ],
            "title": "Chain-of-thought prompting elicits reasoning in large language models",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Susan Zhang",
                "Stephen Roller",
                "Naman Goyal",
                "Mikel Artetxe",
                "Moya Chen",
                "Shuohui Chen",
                "Christopher Dewan",
                "Mona Diab",
                "Xian Li",
                "Xi Victoria Lin"
            ],
            "title": "Opt: Open pre-trained transformer language models",
            "venue": "arXiv preprint arXiv:2205.01068,",
            "year": 2022
        },
        {
            "authors": [
                "Wangchunshu Zhou",
                "Yuchen Eleanor Jiang",
                "Peng Cui",
                "Tiannan Wang",
                "Zhenxin Xiao",
                "Yifan Hou",
                "Ryan Cotterell",
                "Mrinmaya Sachan"
            ],
            "title": "Recurrentgpt: Interactive generation of (arbitrarily) long text",
            "venue": "arXiv preprint arXiv:2305.13304,",
            "year": 2023
        },
        {
            "authors": [
                "Yongchao Zhou",
                "Andrei Ioan Muresanu",
                "Ziwen Han",
                "Keiran Paster",
                "Silviu Pitis",
                "Harris Chan",
                "Jimmy Ba"
            ],
            "title": "Large language models are human-level prompt engineers",
            "year": 1910
        },
        {
            "authors": [
                "Andy Zou",
                "Zifan Wang",
                "J Zico Kolter",
                "Matt Fredrikson"
            ],
            "title": "Universal and transferable adversarial attacks on aligned language models",
            "venue": "arXiv preprint arXiv:2307.15043,",
            "year": 2023
        },
        {
            "authors": [
                "Liang"
            ],
            "title": "We investigate if our approach can detect non-native English writers better or if it is biased against them, as shown by prior detection methods. Following the setup from Liang et al Liang et al. (2023), we use the Hewlett Foundation\u2019s Automated Student Assessment Prize (ASAP) dataset and adopt the first 200 datasets",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Large language models (LLMs) demonstrate exceptional capabilities in text generation (Cha, 2023; Brown et al., 2020; Chowdhery et al., 2022), such as question answering and executable code generation. The increasing deployment and accessibility of those LLM also pose serious risks (Bergman et al., 2022; Mirsky et al., 2022). For example, LLMs create cybersecurity threats, such as facilitating phishing attacks (Kang et al., 2023), generating propaganda (Pan et al., 2023), disseminating fake or biased content on social media, and lowering the bar for social engineering (Asfour & Murillo, 2023). In education, they can lead to academic dishonesty (Cotton et al., 2023). Pearce et al. (2022); Siddiq et al. (2022) have revealed that LLM-generated code can introduce security vulnerabilities to program. Radford et al. (2023); Shumailov et al. (2023) also find LLM-generated content is inferior to human content and can contaminate foundation models\u2019 training. Detecting and auditing those machine-generated text will thus be crucial to mitigate the potential downside of LLMs.\nA plethora of works have investigated detecting machine-generated content (Sadasivan et al., 2023). Early methods, including Bakhtin et al. (2019); Fagni et al. (2021); Gehrmann et al. (2019); Ippolito et al. (2019); Jawahar et al. (2020), were effective before the emergence of sophisticated GPT models, yet the recent LLMs have made traditional heuristic-based detection methods increasingly inadequate Verma et al. (2023); Gehrmann et al. (2019). Current techniques (Mitchell et al., 2023; Verma et al., 2023) rely on LLM\u2019s numerical output metrics. Gehrmann et al. (2019); Ippolito et al. (2019); Solaiman et al. (2019) use token log probability. However, those features are not available in black box models, including state-of-the-art ones (e.g., GPT-3.5 and GPT-4). Furthermore, the highdimensional features employed by existing methods often include redundant and spurious attributes, leading the model to overfit to incorrect features.\nIn this paper, we present Raidar, a simple and effective method for detecting machine-generated text by prompting LLMs to rewrite it. Similar to how humans prompt LLMs for coherent and highquality text generation, our method uses rewriting prompts to gain additional contextual information about the input for more accurate detection.\nOur key hypothesis is that text from auto-regressive generative models retains a consistent structure, which another such model will likely to also have a low loss and treat it as high quality. We observe that machine-generated text is less frequently altered upon rewriting compared to human-written text, regardless of the models used; see Figure 1 as an example. Our approach Raidar shows how\nPublished as a conference paper at ICLR 2024\nDetecting Machine-Generated Text by Editing Distance\nto capitalize on this insight to create detectors for machine-generated text. Raidar operates on the symbolic word output from LLMs, eliminating the need for deep neural network features, which boosts its robustness, generalizability, and adaptability. By focusing on the character editing distance between the original and rewritten text, Raidar is semantically agnostic, reducing irrelevant and spurious correlations. This feature-agnostic design also allows for seamless integration with the latest LLM models that only provide word output via API. Importantly, our detector does not require the original generating model, allowing model A to detect the output of model B.\nVisualizations, empirical experiments show that our simple rewriting-based algorithm Raidar significantly improves detection for several established paragraph-level detection benchmarks. Raidar advances the state-of-the-art detection methods (Verma et al., 2023; Mitchell et al., 2023) by up to 29 points. Our method generalizes to six different datasets and domains, and it is robust when detecting text generated from different language models, such as Ada, Text-Davinci-002, Claude, and GPT-3.5, even though the model has never been trained on text generated from those models. In addition, our detection remains robust even when the text generation is aware of our detection mechanism and uses tailored prompts to bypass our detection. Our data and code is available at https://github.com/cvlab-columbia/RaidarLLMDetect.git."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Machine Text Generation. Machine generated text has achieved high quality as model improves (Radford et al., 2019; Li et al., 2022; Zhou et al., 2023; Zhang et al., 2022; Gehrmann et al., 2019; Brown et al., 2020; Chowdhery et al., 2022). The release of ChatGPT enables instructional following text synthesis for the public Cha (2023). (Dou et al., 2021; Jawahar et al., 2020) demonstrate that machines can potentially leave distinctive signals in the generated text, but these signals can be difficult to detect and may require specialized techniques.\nDetecting Machine Generated Text. Detecting AI-generated text has been studied before the emergence of LLM (Bakhtin et al., 2019; Fagni et al., 2021; Gehrmann et al., 2019; Ippolito et al., 2019). Jawahar et al. (2020) provided a detailed survey for machine-generated text detection.\nThe high quality of recent LLM generation makes detection to be challenging (Verma et al., 2023). Chakraborty et al. (2023) studies when it is possible to detect LLM-generated content. Tang et al. (2023) surveys literature for detecting LLM generated texts. Sadasivan et al. (2023) show that the detection AUROC is upper bounded by the gap between the machine text and human text. The state-of-the-art LLM detection algorithm (Verma et al., 2023; Mitchell et al., 2023) requires access to the probability and loss output from the LLM for the scoring model, yet those numerical metrics and features are not available for the latent GPT-3.5 and GPT-4. Mitchell et al. (2023) requires the scoring model and the target model to be the same. Ghostbuster (Verma et al., 2023) operates under the assumption that the scoring and target model are different, but it still requires access to generated documents from the target model. In addition, the output from the above deep scoring models can contain nuisances and spurious features, and can also be manipulated by adversarial attacks (Jin et al., 2019; Zou et al., 2023), making detection not robust. Another line of work aims to watermark the AI-generated text to enable detection (Kirchenbauer et al., 2023).\nBypassing Machine Text Detection. Krishna et al. (2023) showed rephrase can remove watermark. Krishna et al. (2023); Sadasivan et al. (2023) show that paraharase can efficiently evade detection, including DetectGPT (Mitchell et al., 2023), GLTR (Gehrmann et al., 2019), OpenAI\u2019s generated text detectors, and other zero-shot methods Ippolito et al. (2019); Solaiman et al. (2019). There is a line of work that watermarks the generated text to enable future detection. However, they are shown to be easily broken by rephrasing, too. Our detection can be robust to rephrasing.\nPrompt Engineering. Prompting is the most effective and popular strategy to adapt and instruct LLM to perform tasks Li & Liang (2021); Zhou et al. (2022); Wei et al. (2022); Kojima et al. (2022). Zero-shot GPT prompts the GPT model by asking \u201cis the input generated by GPT\u201d to predict if this is GPT generated (Verma et al., 2023). However, since GPTs are not trained to perform this task, they struggle. In contrast, our work constructs a few rewriting prompts to access the inherent invariance and equivariance of the input. While we can also perform an optimization-based search for better prompt (Zhou et al., 2022), we leave this for future work."
        },
        {
            "heading": "3 DETECTING MACHINE GENERATED TEXT BY REWRITING",
            "text": "We present our approach Raidar for detecting large language models generated text via rewriting. We first talk about the rewriting prompt design to access the property of the input text, then introduce our approach that detects based on the output symbolic modifications."
        },
        {
            "heading": "3.1 REWRITING TEXT VIA LANGUAGE MODELS AND PROMPTS",
            "text": "Let F (\u00b7) be a large language model. Given an input text x, our goal is to classify the label y, which indicates whether it is generated by a machine. The key observation of our method is that given the same rewriting prompt, such as asking the LLM model to \u201crewrite the input text,\u201d an LLM-written text will be accepted by the language model as a high-quality input with inherently lower loss, which leads to few modifications at rewriting. In contrast, a human-written text will be unfavoured by LLM and edited more by the language models.\nWe will use the invariance between the output and the input to measure how much LLM prefers the given input. We hypothesize that LLM will produce invariant output when rewriting its own generated text because another auto-regressive prediction will tend to produce text in a similar pattern. We define this property as the invariance property.\nInvaraince. Given data x, we apply a transformation to the data via prompting the LLM with prompt p. If the data x is produced from LLM, then the transformation p that aims to rewrite the input should introduce a small change. We construct the invariance measurement as L = D(F (p,x),x), where D(\u00b7) denotes the modification distance.\nWe manually create the prompt p to access this invariance. We do not study automatic ways to generate prompts Zhou et al. (2022); Li & Liang (2021), which can be done in future work by optimizing the prompt. In this work, we will show that even a single manually written prompt can achieve a significant difference in invariance behavior. We show a few of our prompts here:"
        },
        {
            "heading": "1. Help me polish this:",
            "text": ""
        },
        {
            "heading": "2. Rewrite this for me:",
            "text": ""
        },
        {
            "heading": "3. Refine this for me please:",
            "text": "where the goal is to make LLM modify more when rewriting human text and be more invariant when modifying LLM-generated text.\nEquivariance. In addition, we hypothesize that GPT data will be equivariant to the data generated by itself. Equivariance means that, if we transform the input, perform the rewriting, and undo the transformation, it will produce the same output as directly rewriting the input.\nWe achieve the transformation for large language models by appending a prompt T to the input and asking the LLM to produce the transformed output. We denote the reversal of the transformation as T\u22121, which is another prompt that writes in the opposite way as T . Equivariance can be measured by the following distance: L = D(F (T\u22121, F (p, F (T,x))), F (p,x)).\nHere we show two examples for the equivariance transformation prompt T and T\u22121:"
        },
        {
            "heading": "T: Write this in the opposite meaning: T\u22121: Write this in the opposite meaning:",
            "text": ""
        },
        {
            "heading": "T: Rewrite to Expand this: T\u22121: Rewrite to Concise this:",
            "text": "By rewriting the sentence with the opposite meaning twice, the sentence should be converted back to its original if the LLM is equivariant to the examples. Note that this transformation T is based on the language model prompt.\nOutput Uncertainty Measurement. We also assume that LLM-generated text will be more stable, when asked to rewrite multiple times than human-written text. We thus explore the variance of the output as a detection measurement. Denote the prompt to be p. The k-th generation results from LLM would be x\u2032k = F (p,x). Due to the randomness in language generation, x \u2032 k will be different. We denote the editing distance between two outputs A and B as D(A,B). We construct the uncertainty measurement as:U = \u2211K\u22121 i=1 \u2211K j=i D(x \u2032 i,x \u2032 j). Note that, in contrast to the invariance and equivariance, this metric only uses the output, and the original input is not in the calculation of the output uncertainty."
        },
        {
            "heading": "3.2 MEASURING CHANGE IN REWRITING",
            "text": "We treat the output of LLM as symbolic representations that encode information about the data. In contrast to Mitchell et al. (2023); Verma et al. (2023), our detection algorithm does not use continuous, numerical representations of the word tokens. Instead, our algorithm operates totally on the discrete, symbolic representations from the LLM. By prompting LLM, our method obtains additional information about the input text via the rewriting difference. We will show how to measure the rewriting change below:\nBag-of-words edit. We use the change of bag-of-words to capture the edit created by LLM. We compute the number of common bags of n-words divided by the length of the input.\nLevenshtein Score. Levenshtein score (Levenshtein, 1966) is a popular metric for measuring the minimum number of single-character edits, including deletion and addition, to change one string to the other. We use standard dynamic programming to calculate the Levenshtein distance. A higher score denotes the two strings are more similar. We use Levenshtein(A,B) to denote the edit distance between string A and B. Let the rewriting output sk = F (pk,x). We obtain the ratio via:\nDk(x, sk) = 1\u2212 Levenshtein(sk,x) max(len(sk), len(x) .\nWe use ratio because the feature of editing difference should be independent of the text length. The invariance, equivariance, and uncertainty measured by the above metric will be used as features for\na binary classifier, which predicts the generation source of the text. For details of the algorithm, please refer to Appendix A.3.\nOur design enjoys several advantages. First, since we only access the discrete token output from LLM, our algorithm requires minimal access to the LLM models. Given that the major state-ofthe-art LLM models, like GPT-3.5-turbo and GPT-4 from OpenAI, are black-box models and only provide API for accessing the discrete tokens rather than the probabilistic values, our algorithm is general and compatible with them. Second, since our representation is discrete, it is more robust in the sense that it will be invariant to the perturbations and shifting in the input space. Lastly, our symbolic representations enable us to construct the following measurements that are none differentiable, which introduces extra burden and cost for gradient-based adversarial attempts to bypass our detection model."
        },
        {
            "heading": "4 RESULTS",
            "text": "We conduct experiments on detecting AI-generated text on paragraph level and compare it to the state of the art. To further understand factors that affect detection performance, we also study the robustness of our method under input aiming to evade our detection, detection accuracy on text generated from different LLM sources, and evaluate our method with different LLM for rewriting."
        },
        {
            "heading": "4.1 DATASET",
            "text": "To evaluate our approach to the challenging, paragraph-level machine-generated text detection, we experiment with the following datasets.\nCreative Writing Dataset is a language dataset based on the subreddit WritingPrompts, which is creative writing by a community based on the prompts. We use the dataset generated by Verma et al. (2023). We focus on detecting paragraph-level data, which is generated by text-davinci-003.\nNews Dataset is based on the Reuters 50-50 authorship identification dataset. We use the machinegenerated text from Verma et al. (2023) via text-davinci-003.\nStudent Essay Dataset The dataset is based on the British Academic Written English corpus and generated by Verma et al. (2023).\nCode Dataset. The goal is to detect if the Python code has been written by GPT, which can be important for education. We adopt the HumanEval dataset (Chen et al., 2021) as the human-written code, and ask GPT-3.5-turbo to perform the same task and generate the code.\nYelp Review Dataset. Yelp reviews tend to be short and challenging to detect. We use the first 2000 human reviews from the Yelp Review Dataset, and generate concise reviews via GPT-3.5-turbo in a similar length as the human written one.\nArXiv Paper Abstract. We investigate if we can detect GPT written paragraphs in academic papers. Our dataset contains 350 abstracts from ICLR papers from 2015 to 2021, which are human-written texts since ChatGPT was not released then. We use GPT-3.5-turbo to generate an abstract based on the paper\u2019s title and the first 15 words from the abstract."
        },
        {
            "heading": "4.2 BASELINES",
            "text": "GPT Zero-shot (Verma et al., 2023) performs detection by directly asking GPT if the input is written by GPT or not. We use the same prompt as Verma et al. (2023) to query GPT.\nGPTZero (Tian, 2023) is an commercial machine text detection service.\nDetectGPT (Mitchell et al., 2023) is the state-of-the-art thresholding approach to detect GPTgenerated text, which achieved 99-point performance over a longer input context, yet its performance on shorter text is unknown. It thresholds the curvature of the input to perform detection. We use the facebook/opt-2.7B for the scoring model.\nGhostbuster (Verma et al., 2023) is the state-of-the-art classifier for machine generated text detection. It uses probabilistic output from large language models as features, and performs feature selection to train an optimal classifier."
        },
        {
            "heading": "4.3 MAIN RESULTS",
            "text": "We use GPT-3.5-Turbo as the LLM to rewrite the input text. Once we obtain the editing distance feature from the rewriting, we use Logistic Regression (Berkson, 1944) or XGBoost (Chen & Guestrin, 2016) to perform the binary classification. We compare our results on three datasets from Verma et al. (2023), as well as our created three datasets, in Table 15. Our method Raidar outperforms the Ghostbuster method by up to 29 points, which achieves the best results over all baselines. In Table 2, we follow the out-of-distribution (OOD) experiment setup in Verma et al. (2023), where we trained the detection classifier on one dataset and evaluated on the other. For the OOD experiment, our method still improves by up to 32 points, demonstrating the effectiveness of our approach over prior methods."
        },
        {
            "heading": "4.4 ANALYSIS",
            "text": "Detection Robustness against Rephrased Text Generation to Evade Detection. Krishna et al. (2023); Sadasivan et al. (2023) show that paraphrasing can often evade detection. In Table 15, we show that our approach can detect GPT text when they are not adversarially rephrased. However, a sophisticated adversary might craft prompts for GPT such that the resulting text, when rewritten, undergoes significant changes, thereby evading our detection. We modify the GPT input using the following rephrases:"
        },
        {
            "heading": "1. Help me rephrase it in human style",
            "text": ""
        },
        {
            "heading": "2. Help me rephrase it, so that another GPT rewriting will cause a lot",
            "text": "of modifications\nTable 3 reveals that while our detector, trained on the default single prompt data, can be bypassed by adversarial rephrasing (left columns). In the right columns, we show results when trained on two of the prompts and tested on the remaining prompts. The detectors are trained on multi-prompt data, which enhances its robustness. Even when tested against unseen adversarial prompts, our detector still identifies machine-generated content designed to elude it, achieving up to 93 points on F1 score. One exception is on the Yelp dataset; the \u201cno adaptive prompt\u201d has lower performance on \u201cmultiple training prompts\u201d than \u201csingle training prompts\u201d. We suspect it is due to the Yelp dataset introducing a larger data difference when prompted differently, and this \u201cmultiple training prompts\u201d setup will decrease performance due to training and testing on different prompts. In general, results in Table 3 demonstrate that with proper training, our method can be still robust under rephrased text to evade detection, underscoring the significance of diversifying prompt types when learning our detector.\nSource of Generated Data. In our main experiment, we train our detector on text generated from GPT-3.5. We study if our model can still detect machine-generated text when they are generated from a different language model. In Table 4, we conduct experiments on text generated from Ada, text-davinci-002, and GPT-3.5 model. For all experiments, we use the same GPT-3.5 to rewrite.\nFor in-distribution experiments, we train the detector on data generated from the respective language model. Despite all rewrites being from GPT-3.5, we achieved up to 96 F1 score points. Notably, GPT-3.5 excels at detecting Ada-generated content, indicating our method\u2019s versatility in identifying\nboth low (Ada) and high-quality (GPT-3.5) data, even they are generated from a different model. We also evaluate our detection efficiency on the Claude (Anthropic, 2023) generated text on student essay (Verma et al., 2023), where we achieve an F1 score of 57.80.\nIn the out-of-distribution experiment, we train the detector on data from two language models, assuming it is unaware that the test text will be generated from the third model. Despite a performance drop on detecting the out-of-distribution test data generated from the third model, our method remains effective in detecting content from this unseen model, underscoring our approach\u2019s robustness and adaptability, with up to 91 points on F1 score.\nType of Detection Model. Mireshghallah et al. (2023) showed that model size affects performance in perturbation-based detection methods. Given the same input text generated from GPT-3.5, We explore our approach\u2019s efficacy with alternative rewriting models with different size. In addition to using the costly GPT-3.5 to rewrite, we incorporate two smaller models, Ada and Text-Davinci-002, and evaluate their detection performance when they are used to rewrite. In Table 5, while all models achieve significant detection performance, our results indicate that a larger rewriting language model enhances detection performance in our method.\nImpact of Different Prompts. Figure 6 displays the detection F1 score for various prompts across three datasets. While Mitchell et al. (2023) employs up to 100 perturbations to query LLM and compute curvature from loss, our approach achieves high detection performance using just a single rewriting prompt.\nImpact of Content Length. We assess our detection method\u2019s performance across varying input lengths using the Yelp Review dataset in Figure 5. Longer inputs, in general, achieve higher detection performance. Notably, while many algorithms fail with shorter inputs (Tian, 2023; Verma et al., 2023), our method can achieve 74 points of detection F1 score even with inputs as brief as ten words, highlighting the effectiveness of our approach."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "We introduce Raidar, an approach to use rewriting editing distance to detect machine-generated text. Our results demonstrate improved detection performance across several benchmarks and stateof-the-art detection methods. Our method is still effective when detecting text generated from novel language models and text generated via prompts that aim to bypass our detection. Our findings show that integrating the inherent structure of large language models can provide useful information to detect text generated from those language models, opening up a new direction for detecting machinegenerated text."
        },
        {
            "heading": "A APPENDIX",
            "text": "A.1 DATA CREATION\nOur dataset selection was driven by the need to address emerging challenges and gaps in current research. We incorporated news, creative writing, and essays from the established Ghostbuster Verma et al. (2023) to maintain continuity with prior work. Recognizing the growing capabilities of Language Learning Models (LLMs) like ChatGPT in generating code, and the accompanying security issues, we included code as a novel and pertinent text data type. Additionally, we analyzed Yelp reviews to explore LLMs\u2019 potential for generating fake reviews, a concern overlooked in previous studies, which could significantly influence public opinion about businesses. Lastly, we included arXiv data to address recent concerns over the use of GPT in academic writing, reflecting on its ethical implications.\nCode Dataset. Human eval dataset offers code specification and the completed code for each data point. We first use GPT to generate a detailed description of the function of the code by prompting it with \u201cDescribe what this code does code specificationcode\u201d. The result, termed pseudo code, is an interpretation of the code. Subsequently, we prompt GPT with \u201dI want to do this pseudo code, help me write code starting with this code specification,\u201d to generate Python code that adheres to the given input-output format and specifications. This way, we create the AI-generated code data.\nYelp Reviews Dataset. When tasked with crafting a synthetic Yelp review, prompting GPT-3.5 with \u201dHelp me write a review based on this original review\u201d resulted in verbose and lengthy text. However, we discovered that using the prompt \u201dWrite a very short and concise review based on this: original review\u201d yielded the most effective and succinct AI-generated reviews.\nArXiv Dataset. In our experiment with Arxiv data, which includes titles and abstracts, we synthesized abstracts by using the title and the first 15 words of the original abstract. We employed the prompt \u201cThe title is title, start with first 15 words, write a short concise abstract based on this:\u201d, which successfully generated realistic abstracts that align with the titles.\nA.2 DATASET STATISTICS\nIn Table 6 and Table 7, we show each dataset\u2019s size, median, min, and max length on human-written and machine-generated ones, respectively.\nWe show the algorithm for invariance, equivariance, and uncertainty based algorithms. We denote the learned classifier as C.\nA.4 ANALYSIS\nQuality of the Machine-Generated Content. LLM tends to treat the text generated by the machine as high quality and conducts few edits. We conduct a human study on whether the text generated by machines is indeed of higher quality than that written by humans. This study focused on the Yelp and Arxiv datasets. Participants were presented with two pieces of text designed for the same\nAlgorithm 1 Detecting LLM Generated Content via Output Invariance 1: Input: Text input x, rephrase prompt Pk, where k = 1, ...,K. 2: Output: Class prediction y\u0302 3: Inference: 4: for k = 1, ...,K do 5: Obtain LLM output Sk = F (Pk,x) 6: Calculate bag-of-words edit Rk and the Levenshtein Score Dk 7: end for 8: Make final prediction via y = C([R1, R2, ..., RK , D1, D2, ..., DK ])\npurpose, one authored by a human and the other by a machine, and were asked to judge which was of higher quality. The study involved three users, and for each dataset, we randomly selected 20 examples for evaluation. The results, detailed in Table 8, generally indicate that human-written texts are of similar or higher quality compared to those generated by machines.\nRobustness of Our Method to LLM Fine-tuning. We run the experiment on GPT-3.5-Turbo and GPT-4-Turbo. GPT-4-Turbo can be roughly treated as a realistic, advanced, continual fine-tuned LLM on new real-world data from GPT-3.5-Turbo. We show the results in Table 9. Our method is robust to LLM finetuned. Despite a drop in detection performance, it still outperforms the established state-of-the-art zero-shot detector.\nRobustness of Our Method to Non-native Speaker. Prior work showed that LLM detectors are biased against non-native English writers, because non-native English writing is limited in linguistic expressions and is often detected as AI-generated Liang et al. (2023). We investigate if our approach can detect non-native English writers better or if it is biased against them, as shown by prior detection methods.\nFollowing the setup from Liang et al Liang et al. (2023), we use the Hewlett Foundation\u2019s Automated Student Assessment Prize (ASAP) dataset and adopt the first 200 datasets in our study, which is a dataset from non-native speakers on TOEFL essays on 8-th grade level in the US. We create the machine-generated answer for the TOEFL essay via the following prompt:\nWrite an essay based on this:\nAlgorithm 2 Detecting LLM Generated Content via Output Equivariance 1: Input: Text input x. 2: Output: Class prediction y\u0302 3: Inference: 4: for k = 1, ...,K do 5: Create transformation prompt Tk and inverse transformation prompt T\u2032k, create rephrase\nprompt Pk. 6: Obtain LLM output Mk = F (Tk,x) 7: Obtain LLM output M\u2032k = F (Pk,Mk) 8: Obtain LLM output Sk = F (T\u2032k,M \u2032 k)\n9: Calculate bag-of-words edit Rk and the Levenshtein Score Dk 10: end for 11: Make final prediction via y = C([R1, R2, ..., RK , D1, D2, ..., DK ])\nAlgorithm 3 Detecting LLM generated Content via Output Uncertainty 1: Input: Text input x. 2: Output: Class prediction y\u0302 3: Inference: 4: Given rephrase prompt P 5: for k = 1, ...,K do 6: Obtain LLM output Sk = F (P,x) 7: end for 8: for k = 1, ...,K do 9: for j = k, ...,K do 10: Calculate bag-of-words edit Rk,j and the Levenshtein Score Dk,j 11: end for 12: end for 13: Make final prediction via y = C([R1,2, R1,3, ..., RK\u22121,K , D1,2, D1,3, ..., RK\u22121,K ])\nWe show the detection result in Table 10. Our method does not discriminate the non-English speaker, and reaches a similar level of detection performance on high-quality writing (abstract from accepted ICLR papers). Since both ASAP and Arxiv are written by humans, they will be treated as lowquality text that does not match the inherent inertia in LLM models, and thus will both be modified more than the machine-generated text. Our detection algorithm will classify those texts with more modifications than humans. Thus, both non-native and efficient writers will be correctly classified by our approach. Since our algorithm only relies on the word edit distance, it does not rely on the superficial semantics of the text for detection. Thus, our approach generalizes well from academic ICLR abstract to non-native English writing on the 8th grade level, with only less than 1 point of performance drop.\nDetection Performance by combining rewrites from multiple LLMs. In Table 11, we show detection performance when combining GPT-3.5 rewrites with other LLMs, including Ada, Davinci, and both. We find combining rewriting from multiple LLMs can improve performance over Arxiv detection, but not on Yelp.\nDetection performance by adding edit distance between the rewritten texts from different LLMs as additional features. In Table 12, we show the detection performance. We can achieve better detection performance leveraging this new feature.\nDetection performance by combining features of invariance, equivariance, and uncertainty. We conduct experiments in Table 13, on the two dataset we studied, we cannot further improve performance.\nDetection performance under different input length. We show the trend in Figure 7, Figure 8, and Figure 9.\nStatistical significance of the number of changes (deletions, insertions) done by the selective generative models between humans and machine-generated texts. We calculate the t-statistic\nand calculate the p-value. In Table 14, we show the p-value for the two distributions shown in Figure 2. Since the p-value is much smaller than 0.05, it demonstrates that the number of changes between human and machine-generated text is significant.\nA.5 IMPLEMENTATION DETAILS\nThe training and testing domain for Table 2. For all experiments in Table 2, we use logistic regression, and use the same source and target for invariance, equivariance, and uncertainty. For News, we train on Creative Writing and test on News. For Creative Writing, we train on News and test on Creative Writing. FOr Student Essay, we train on News, and test on student Essay.\nClassifier choice for Table 1 and Table 2. We use logistic regression for all our experiments except for on the student essay dataset, where we find XGBoost achieves better performance."
        }
    ],
    "year": 2024
}