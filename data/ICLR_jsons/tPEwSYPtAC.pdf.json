{
    "abstractText": "Generalizing to out-of-distribution (OOD) data or unseen domain, termed OOD generalization, still lacks appropriate theoretical guarantees. Canonical OOD bounds focus on different distance measurements between source and target domains but fail to consider the optimization property of the learned model. As empirically shown in recent work, the sharpness of learned minima influences OOD generalization. To bridge this gap between optimization and OOD generalization, we study the effect of sharpness on how a model tolerates data change in domain shift which is usually captured by \"robustness\" in generalization. In this paper, we give a rigorous connection between sharpness and robustness, which gives better OOD guarantees for robust algorithms. It also provides a theoretical backing for \"flat minima leads to better OOD generalization\". Overall, we propose a sharpness-based OOD generalization bound by taking robustness into consideration, resulting in a tighter bound than non-robust guarantees. Our findings are supported by the experiments on a ridge regression model, as well as the experiments on deep learning classification tasks.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yingtian Zou"
        },
        {
            "affiliations": [],
            "name": "Kenji Kawaguchi"
        },
        {
            "affiliations": [],
            "name": "Yingnan Liu"
        },
        {
            "affiliations": [],
            "name": "Jiashuo Liu"
        },
        {
            "affiliations": [],
            "name": "Mong-Li Lee"
        },
        {
            "affiliations": [],
            "name": "Wynne Hsu"
        }
    ],
    "id": "SP:7742ed2d7825a811f0fad78eb48011230bf24f7d",
    "references": [
        {
            "authors": [
                "Kartik Ahuja",
                "Ethan Caballero",
                "Dinghuai Zhang",
                "Jean-Christophe Gagnon-Audet",
                "Yoshua Bengio",
                "Ioannis Mitliagkas",
                "Irina Rish"
            ],
            "title": "Invariance principle meets information bottleneck for out-ofdistribution generalization",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Alnur Ali",
                "J Zico Kolter",
                "Ryan J Tibshirani"
            ],
            "title": "A continuous-time view of early stopping for least squares regression",
            "venue": "In The 22nd international conference on artificial intelligence and statistics,",
            "year": 2019
        },
        {
            "authors": [
                "Martin Arjovsky",
                "L\u00e9on Bottou",
                "Ishaan Gulrajani",
                "David Lopez-Paz"
            ],
            "title": "Invariant risk minimization",
            "venue": "arXiv preprint arXiv:1907.02893,",
            "year": 2019
        },
        {
            "authors": [
                "Benjamin Aubin",
                "Agnieszka S\u0142owik",
                "Martin Arjovsky",
                "Leon Bottou",
                "David Lopez-Paz"
            ],
            "title": "Linear unit-tests for invariance discovery",
            "venue": "arXiv preprint arXiv:2102.10867,",
            "year": 2021
        },
        {
            "authors": [
                "Peter Bandi",
                "Oscar Geessink",
                "Quirine Manson",
                "Marcory Van Dijk",
                "Maschenka Balkenhol",
                "Meyke Hermsen",
                "Babak Ehteshami Bejnordi",
                "Byungjae Lee",
                "Kyunghyun Paeng",
                "Aoxiao Zhong"
            ],
            "title": "From detection of individual metastases to classification of lymph node status at the patient level: the camelyon17 challenge",
            "venue": "IEEE Transactions on Medical Imaging,",
            "year": 2018
        },
        {
            "authors": [
                "Shai Ben-David",
                "John Blitzer",
                "Koby Crammer",
                "Alex Kulesza",
                "Fernando Pereira",
                "Jennifer Wortman Vaughan"
            ],
            "title": "A theory of learning from different domains",
            "venue": "Machine learning,",
            "year": 2010
        },
        {
            "authors": [
                "John Blitzer",
                "Koby Crammer",
                "Alex Kulesza",
                "Fernando Pereira",
                "Jennifer Wortman"
            ],
            "title": "Learning bounds for domain adaptation",
            "venue": "Advances in neural information processing systems,",
            "year": 2007
        },
        {
            "authors": [
                "Jean-Christophe Bourin",
                "Eun-Young Lee",
                "Minghua Lin"
            ],
            "title": "Positive matrices partitioned into a small number of hermitian blocks",
            "venue": "Linear Algebra and its Applications,",
            "year": 2013
        },
        {
            "authors": [
                "Junbum Cha",
                "Sanghyuk Chun",
                "Kyungjae Lee",
                "Han-Cheol Cho",
                "Seunghyun Park",
                "Yunsung Lee",
                "Sungrae Park"
            ],
            "title": "Swad: Domain generalization by seeking flat minima",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Jeremy M Cohen",
                "Simran Kaur",
                "Yuanzhi Li",
                "J Zico Kolter",
                "Ameet Talwalkar"
            ],
            "title": "Gradient descent on neural networks typically occurs at the edge of stability",
            "venue": "arXiv preprint arXiv:2103.00065,",
            "year": 2021
        },
        {
            "authors": [
                "Nicolas Courty",
                "R\u00e9mi Flamary",
                "Amaury Habrard",
                "Alain Rakotomamonjy"
            ],
            "title": "Joint distribution optimal transportation for domain adaptation",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Simon S Du",
                "Jayanth Koushik",
                "Aarti Singh",
                "Barnab\u00e1s P\u00f3czos"
            ],
            "title": "Hypothesis transfer learning via transformation functions",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Yingjun Du",
                "Xiantong Zhen",
                "Ling Shao",
                "Cees GM Snoek"
            ],
            "title": "Metanorm: Learning to normalize few-shot batches across domains",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "John Duchi",
                "Hongseok Namkoong"
            ],
            "title": "Learning models with uniform performance via distributionally robust optimization",
            "venue": "arXiv preprint arXiv:1810.08750,",
            "year": 2018
        },
        {
            "authors": [
                "John C Duchi",
                "Hongseok Namkoong"
            ],
            "title": "Learning models with uniform performance via distributionally robust optimization",
            "venue": "The Annals of Statistics,",
            "year": 2021
        },
        {
            "authors": [
                "Laurent El Ghaoui",
                "Herv\u00e9 Lebret"
            ],
            "title": "Robust solutions to least-squares problems with uncertain data",
            "venue": "SIAM Journal on matrix analysis and applications,",
            "year": 1997
        },
        {
            "authors": [
                "Pierre Foret",
                "Ariel Kleiner",
                "Hossein Mobahi",
                "Behnam Neyshabur"
            ],
            "title": "Sharpness-aware minimization for efficiently improving generalization",
            "venue": "arXiv preprint arXiv:2010.01412,",
            "year": 2020
        },
        {
            "authors": [
                "Pascal Germain",
                "Amaury Habrard",
                "Fran\u00e7ois Laviolette",
                "Emilie Morvant"
            ],
            "title": "A pac-bayesian approach for domain adaptation with specialization to linear classifiers",
            "venue": "In International conference on machine learning. PMLR,",
            "year": 2013
        },
        {
            "authors": [
                "Pascal Germain",
                "Amaury Habrard",
                "Fran\u00e7ois Laviolette",
                "Emilie Morvant"
            ],
            "title": "A new pac-bayesian perspective on domain adaptation",
            "venue": "In International conference on machine learning",
            "year": 2016
        },
        {
            "authors": [
                "Ishaan Gulrajani",
                "David Lopez-Paz"
            ],
            "title": "In search of lost domain generalization",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Pavel Izmailov",
                "Dmitrii Podoprikhin",
                "Timur Garipov",
                "Dmitry Vetrov",
                "Andrew Gordon Wilson"
            ],
            "title": "Averaging weights leads to wider optima and better generalization",
            "venue": "arXiv preprint arXiv:1803.05407,",
            "year": 2018
        },
        {
            "authors": [
                "Kenji Kawaguchi",
                "Zhun Deng",
                "Kyle Luh",
                "Jiaoyang Huang"
            ],
            "title": "Robustness implies generalization via data-dependent generalization bounds",
            "venue": "In International Conference on Machine Learning",
            "year": 2022
        },
        {
            "authors": [
                "Daniel Kifer",
                "Shai Ben-David",
                "Johannes Gehrke"
            ],
            "title": "Detecting change in data streams",
            "venue": "In VLDB,",
            "year": 2004
        },
        {
            "authors": [
                "Pang Wei Koh",
                "Shiori Sagawa",
                "Henrik Marklund",
                "Sang Michael Xie",
                "Marvin Zhang",
                "Akshay Balsubramani",
                "Weihua Hu",
                "Michihiro Yasunaga",
                "Richard Lanas Phillips",
                "Irena Gao"
            ],
            "title": "Wilds: A benchmark of in-the-wild distribution shifts",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Masanori Koyama",
                "Shoichiro Yamaguchi"
            ],
            "title": "Out-of-distribution generalization with maximal invariant predictor",
            "year": 2020
        },
        {
            "authors": [
                "Yann LeCun"
            ],
            "title": "The mnist database of handwritten digits. http://yann",
            "venue": "lecun. com/exdb/mnist/,",
            "year": 1998
        },
        {
            "authors": [
                "Da Li",
                "Yongxin Yang",
                "Yi-Zhe Song",
                "Timothy M Hospedales"
            ],
            "title": "Deeper, broader and artier domain generalization",
            "venue": "In Proceedings of the IEEE international conference on computer vision,",
            "year": 2017
        },
        {
            "authors": [
                "Da Li",
                "Yongxin Yang",
                "Yi-Zhe Song",
                "Timothy Hospedales"
            ],
            "title": "Learning to generalize: Meta-learning for domain generalization",
            "venue": "In Proceedings of the AAAI conference on artificial intelligence,",
            "year": 2018
        },
        {
            "authors": [
                "Jiashuo Liu",
                "Zheyuan Hu",
                "Peng Cui",
                "Bo Li",
                "Zheyan Shen"
            ],
            "title": "Heterogeneous risk minimization",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Kaifeng Lyu",
                "Zhiyuan Li",
                "Sanjeev Arora"
            ],
            "title": "Understanding the generalization benefit of normalization layers: Sharpness reduction",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Yishay Mansour",
                "Mehryar Mohri",
                "Afshin Rostamizadeh"
            ],
            "title": "Domain adaptation: Learning bounds and algorithms",
            "venue": "arXiv preprint arXiv:0902.3430,",
            "year": 2009
        },
        {
            "authors": [
                "Song Mei",
                "Andrea Montanari"
            ],
            "title": "The generalization error of random features regression: Precise asymptotics and the double descent curve",
            "venue": "Communications on Pure and Applied Mathematics,",
            "year": 2022
        },
        {
            "authors": [
                "John P Miller",
                "Rohan Taori",
                "Aditi Raghunathan",
                "Shiori Sagawa",
                "Pang Wei Koh",
                "Vaishaal Shankar",
                "Percy Liang",
                "Yair Carmon",
                "Ludwig Schmidt"
            ],
            "title": "Accuracy on the line: on the strong correlation between out-of-distribution and in-distribution generalization",
            "venue": "In International Conference on Machine Learning. PMLR,",
            "year": 2021
        },
        {
            "authors": [
                "Mehryar Mohri",
                "Afshin Rostamizadeh",
                "Ameet Talwalkar"
            ],
            "title": "Foundations of machine learning",
            "venue": "MIT press,",
            "year": 2018
        },
        {
            "authors": [
                "Edward Moroshko",
                "Blake E Woodworth",
                "Suriya Gunasekar",
                "Jason D Lee",
                "Nati Srebro",
                "Daniel Soudry"
            ],
            "title": "Implicit bias in deep linear classification: Initialization scale vs training accuracy",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Jonas Peters",
                "Dominik Janzing",
                "Bernhard Sch\u00f6lkopf"
            ],
            "title": "Elements of causal inference: foundations and learning algorithms",
            "year": 2017
        },
        {
            "authors": [
                "Henning Petzka",
                "Michael Kamp",
                "Linara Adilova",
                "Cristian Sminchisescu",
                "Mario Boley"
            ],
            "title": "Relative flatness and generalization",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Mohammad Pezeshki",
                "Oumar Kaba",
                "Yoshua Bengio",
                "Aaron C Courville",
                "Doina Precup",
                "Guillaume Lajoie"
            ],
            "title": "Gradient starvation: A learning proclivity in neural networks",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Alexandre Rame",
                "Corentin Dancette",
                "Matthieu Cord"
            ],
            "title": "Fishr: Invariant gradient variances for outof-distribution generalization",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Ievgen Redko",
                "Emilie Morvant",
                "Amaury Habrard",
                "Marc Sebban",
                "Youn\u00e8s Bennani"
            ],
            "title": "A survey on domain adaptation theory: learning bounds and theoretical guarantees",
            "venue": "arXiv preprint arXiv:2004.11829,",
            "year": 2020
        },
        {
            "authors": [
                "Herbert Robbins"
            ],
            "title": "A remark on stirling\u2019s formula",
            "venue": "The American mathematical monthly,",
            "year": 1955
        },
        {
            "authors": [
                "Mateo Rojas-Carulla",
                "Bernhard Sch\u00f6lkopf",
                "Richard Turner",
                "Jonas Peters"
            ],
            "title": "Invariant models for causal transfer learning",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2018
        },
        {
            "authors": [
                "Shiori Sagawa",
                "Pang Wei Koh",
                "Tatsunori B Hashimoto",
                "Percy Liang"
            ],
            "title": "Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization",
            "year": 1911
        },
        {
            "authors": [
                "Shiori Sagawa",
                "Aditi Raghunathan",
                "Pang Wei Koh",
                "Percy Liang"
            ],
            "title": "An investigation of why overparameterization exacerbates spurious correlations",
            "venue": "In International Conference on Machine Learning. PMLR,",
            "year": 2020
        },
        {
            "authors": [
                "Soroosh Shafieezadeh Abadeh",
                "Peyman M Mohajerin Esfahani",
                "Daniel Kuhn"
            ],
            "title": "Distributionally robust logistic regression",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2015
        },
        {
            "authors": [
                "Jian Shen",
                "Yanru Qu",
                "Weinan Zhang",
                "Yong Yu"
            ],
            "title": "Wasserstein distance guided representation learning for domain adaptation",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2018
        },
        {
            "authors": [
                "Yuge Shi",
                "Jeffrey Seely",
                "Philip HS Torr",
                "N Siddharth",
                "Awni Hannun",
                "Nicolas Usunier",
                "Gabriel Synnaeve"
            ],
            "title": "Gradient matching for domain generalization",
            "venue": "arXiv preprint arXiv:2104.09937,",
            "year": 2021
        },
        {
            "authors": [
                "Baochen Sun",
                "Kate Saenko"
            ],
            "title": "Deep coral: Correlation alignment for deep domain adaptation",
            "venue": "In European conference on computer vision,",
            "year": 2016
        },
        {
            "authors": [
                "Yoav Wald",
                "Amir Feder",
                "Daniel Greenfeld",
                "Uri Shalit"
            ],
            "title": "On calibration and out-of-domain generalization",
            "venue": "Advances in neural information processing systems,",
            "year": 2021
        },
        {
            "authors": [
                "Olivia Wiles",
                "Sven Gowal",
                "Florian Stimberg",
                "Sylvestre-Alvise Rebuffi",
                "Ira Ktena",
                "Krishnamurthy Dj Dvijotham",
                "Ali Taylan Cemgil"
            ],
            "title": "A fine-grained analysis on distribution shift",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Huan Xu",
                "Shie Mannor"
            ],
            "title": "Robustness and generalization",
            "venue": "Machine learning,",
            "year": 2012
        },
        {
            "authors": [
                "Shen Yan",
                "Huan Song",
                "Nanxiang Li",
                "Lincan Zou",
                "Liu Ren"
            ],
            "title": "Improve unsupervised domain adaptation with mixup training",
            "venue": "arXiv preprint arXiv:2001.00677,",
            "year": 2020
        },
        {
            "authors": [
                "Huaxiu Yao",
                "Yu Wang",
                "Sai Li",
                "Linjun Zhang",
                "Weixin Liang",
                "James Zou",
                "Chelsea Finn"
            ],
            "title": "Improving out-of-distribution robustness via selective augmentation",
            "venue": "arXiv preprint arXiv:2201.00299,",
            "year": 2022
        },
        {
            "authors": [
                "Haotian Ye",
                "Chuanlong Xie",
                "Tianle Cai",
                "Ruichen Li",
                "Zhenguo Li",
                "Liwei Wang"
            ],
            "title": "Towards a theoretical framework of out-of-distribution generalization",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Xiao Zhang",
                "Yaodong Yu",
                "Lingxiao Wang",
                "Quanquan Gu"
            ],
            "title": "Learning one-hidden-layer relu networks via gradient descent",
            "venue": "In The 22nd international conference on artificial intelligence and statistics,",
            "year": 2019
        },
        {
            "authors": [
                "Xingxuan Zhang",
                "Linjun Zhou",
                "Renzhe Xu",
                "Peng Cui",
                "Zheyan Shen",
                "Haoxin Liu"
            ],
            "title": "Towards unsupervised domain generalization",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Yuchen Zhang",
                "Tianle Liu",
                "Mingsheng Long",
                "Michael Jordan"
            ],
            "title": "Bridging theory and algorithm for domain adaptation",
            "venue": "In International Conference on Machine Learning. PMLR,",
            "year": 2019
        },
        {
            "authors": [
                "Han Zhao",
                "Shanghang Zhang",
                "Guanhang Wu",
                "Jos\u00e9 MF Moura",
                "Joao P Costeira",
                "Geoffrey J Gordon"
            ],
            "title": "Adversarial multiple source domain adaptation",
            "venue": "Advances in neural information processing systems,",
            "year": 2018
        },
        {
            "authors": [
                "Kai Zhong",
                "Zhao Song",
                "Prateek Jain",
                "Peter L Bartlett",
                "Inderjit S Dhillon"
            ],
            "title": "Recovery guarantees for one-hidden-layer neural networks",
            "venue": "In International conference on machine learning,",
            "year": 2017
        },
        {
            "authors": [
                "Zhao"
            ],
            "title": "VCdim(F\u2206F) is at most 2d",
            "venue": "Further, with Theorem 2 (BenDavid et al.,",
            "year": 2018
        },
        {
            "authors": [
                "Zhao"
            ],
            "title": "2018), the M = 1 for the normalized regression loss. Combine them all, we conclude the proof. Corollary C.7",
            "venue": "If K \u2192 \u221e,M",
            "year": 2018
        },
        {
            "authors": [
                "Petzka et al. Petzka"
            ],
            "title": "Let l : Y \u00d7 Y \u2192 R+denote a loss function, \u03b5 and \u03b4 two positive (small) real numbers, S \u2286 X \u00d7 Y a finite sample set, and A \u2208 Rm\u00d7m a matrix. A model f(x) = (\u03c8 \u25e6 \u03c6)(x) with \u03c6(X ) \u2286 R is called ((\u03b4, S,A)",
            "venue": "\u03b5)-feature",
            "year": 2021
        },
        {
            "authors": [
                "Kawaguchi"
            ],
            "title": "2022) to compute the K in an inverse image of the \u03b5\u2212 covering",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Machine learning systems are typically trained on a given distribution of data and achieve good performance on new, unseen data that follows the same distribution as the training data. Out-ofDistribution (OOD) generalization requires machine learning systems trained in the source domain to generalize to unseen data or target domains with different distributions from the source domain. A myriad of algorithms (Sun & Saenko, 2016; Arjovsky et al., 2019; Sagawa et al., 2019; Koyama & Yamaguchi, 2020; Pezeshki et al., 2021; Ahuja et al., 2021) aim to learn the invariant components along the distribution shifting. Optimization-based methods such as (El Ghaoui & Lebret, 1997; Duchi & Namkoong, 2018; Liu et al., 2021; Rame et al., 2022) focus on maximizing robustness by optimizing for worst-case error over an uncertainty distribution set. While these methods are sophisticated, they do not always perform better than Empirical Risk Minimization (ERM) when evaluated across different datasets (Gulrajani & Lopez-Paz, 2021; Wiles et al., 2022). This raises the question of how to understand the OOD generalization of algorithms and which criteria should be used to select models that are provably better (Gulrajani & Lopez-Paz, 2021). These questions highlight the need for more theoretical research in the field of OOD generalization (Ye et al., 2021).\nTo characterize the generalization gap between the source domain and the target domain, a canonical method (Blitzer et al., 2007) from domain adaptation theory decouples this gap into an In-Distribution (ID) generalization and a hypothesis-specific Out-of-Distribution (OOD) distance. However, this distance is based on the notion of VC-dimension (Kifer et al., 2004), resulting in a loose bound due to the large size of the hypothesis class in the modern overparameterized neural networks. Subsequent works improve the bound based on Rademacher Complexity (Du et al., 2017), whereas Germain et al. (2016) improves the bound based on PAC-Bayes. Unlike the present paper, these works did not consider algorithmic robustness, which has natural interpretation and advantages for distribution shifts In this work, we consider algorithmic robustness to derive the OOD generalization bound. The key idea is to partition the input space into K non-overlapping subspaces such that the error difference in the model\u2019s performance between any pair of points in each subspace is bounded by some constant \u03f5. Within each subspace, any distributional shift is considered subtle for the robust\nmodel thus leading to less impact on OOD generalization. Figure 1 illustrates this with the two distributions where the target domain has a distributional shift from the source domain. Compared to existing non-robust OOD generalization bounds Zhao et al. (2018), our new generalization error does not depend on hypothesis size, which is more reliable in the overparameterized regime. Our goal is to measure the generalizability of a model by considering how it is robust to this shift and achieves a tighter bound than existing works.\nAlthough robustness captures the tolerance to distributional shift, it is intractable to compute robustness constant \u03f5 due to the inaccessibility of target distribution. The robustness definition in Xu & Mannor (2012) indicates that the loss landscape induced by the model\u2019s parameters is closely tied to its robustness. To gain a deeper understanding of robustness, we further study the learned model from an optimization perspective. As shown in (Lyu et al., 2022; Petzka et al., 2021), when the loss landscape is \"flat\", there is a good generalization, which is also observed in OOD settings (Izmailov et al., 2018; Cha et al., 2021). However, the relationship between robustness and this geometric property of the loss landscape, termed Sharpness, remains an open question. In this paper, we establish a provable dependence between robustness and sharpness for ReLU random neural network classes. It allows us to replace robustness constant \u03f5 with the sharpness of a learned model which is only computed from the training dataset that addresses the problem mentioned above. Our result of the interplay between robustness and sharpness can be applied to both ID and OOD generalization bounds. We also show an example to generalize our result beyond our assumption and validate it empirically.\nOur main contributions can be summarized as follows:\n\u2022 We proposed a new framework for Out-of-distribution/ Out-of-domain generalization bounds. In this framework, we use robustness to capture the tolerance of distribution shift which leads to tighter upper bounds generally.\n\u2022 We reveal the underlying connection between the robustness and sharpness of the loss landscape and use this connection to enrich our robust OOD bounds under one-hidden layer ReLU NNs. This is the first optimization-based bound in Out-of-Distribution/Domain generalization.\n\u2022 We studied two cases in ridge regression and classification which support and generalize our main theorem well. All the experimental results corroborate our findings well."
        },
        {
            "heading": "2 PRELIMINARY",
            "text": "Notations We use [n] to denote the integers set {i}ni=1. We use \u2225 \u00b7 \u2225 to denote the \u21132-norm (Euclidean norm). In vector form, wi denotes the i-th instance while the wj is the j-th element of the vector w and we use |w| for the element-wise absolute value of vector w. We use n, d for the training set size and input dimension. O is the Big-O notation."
        },
        {
            "heading": "2.1 PROBLEM FORMULATION",
            "text": "Consider a source domain and a target domain of the OOD generalization problem where we use DS , DT to represent the source and target distribution respectively. Let each D be the probability\nmeasure of sample z from sample space Z = X \u00d7 Y with X \u2208 Rd. In the source domain, we have a training set S = {zi}ni=1,\u2200i \u2208 [n], zi \u223c DS while the target is to learn a model f \u2208 F with S and parameters \u03b8 \u2208 \u0398 where f : \u0398\u00d7 X 7\u2192 R generalizes well. Given loss function \u2113 : R\u00d7 R\u2192 R+, which is for short, the expected risk over the source distribution DS will be\nLS(f\u03b8) \u225c Ez\u223cDS [\u2113\u03b8(z)] = Ez\u223cDS [\u2113(f(\u03b8,x), y)], L\u0302S(f\u03b8) \u225c 1\nn \u2211 zi\u2208S [\u2113\u03b8(zi)].\nWe use \u2113\u03b8(z) as the shorthand. The OOD generalization is to measure between target domain expected risk LT (f\u03b8) and the source domain empirical risk L\u0302S(f\u03b8) which involves two parts: (1) In-domain generalization error gap between empirical risk and expected risk LS(f\u03b8) in the source domain. (2) Out-of-Domain distance between source and target domains. A model-agnostic example in Zhao et al. (2018) gave the following uniform bound: Proposition 2.1 (Zhao et al. Zhao et al. (2018) Theorem 2 & 3.2). With hypothesis class F and pseudo dimension Pdim(F) = d\u2032, unlabeled empirical datasets from source and target distribution D\u0302S and D\u0302T with size n each, then with probability at least 1\u2212 \u03b4, for all f \u2208 F ,\nLT (f) \u2264 L\u0302S(f) + 1\n2 dF\u2206F\n( D\u0302T ; D\u0302S ) +O (\u221a d\u2032/n ) where dF\u2206F (D\u0302T ; D\u0302S) := 2 supAf\u2208A{f(x)\u2295f\u2032(x):f,f\u2032\u2208F}\n\u2223\u2223\u2223PD\u0302S [Af ]\u2212 PD\u0302T [Af ]\u2223\u2223\u2223 and \u2295 is the XOR operator. Specific form of O( \u221a |F|/n) is defined in Appendix C.6."
        },
        {
            "heading": "2.2 ALGORITHMIC ROBUSTNESS",
            "text": "Definition 2.2 (Robustness, Xu & Mannor (2012)). A learning model f\u03b8 on training set S is (K, \u03f5(\u00b7))robust, forK \u2208 N, if Z can be partitioned intoK disjoint sets, denoted by {Ci}Ki=1, such that \u2200s \u2208 S we have\ns, z \u2208 Ci,\u2200i \u2208 [K]\u21d2 |\u2113\u03b8(s)\u2212 \u2113\u03b8(z)| \u2264 \u03f5(S).\nThis definition captures the robustness of the model in terms of the input. Within each partitioned set Ci, the loss difference between any sample z belonging to Ci and training sample s \u2208 Ci will be upper bounded by the robustness constant \u03f5(S). The generalization result given by Xu & Mannor (2012) provides a framework to bound the empirical risk with algorithmic robustness which has been stated in Appendix C. Based on this framework, we are able to reframe the existing OOD generalization theory."
        },
        {
            "heading": "3 MAIN RESULTS",
            "text": "In this section, we propose a new Out-of-Distribution (OOD) generalization bound for robust algorithms that have not been extensively studied yet. We then compare our result to the existing domain shift bound in Proposition 2.1 and discuss its implications for OOD and domain generalization problems by considering algorithmic robustness. To further explain the introduced robustness, we connect it to the sharpness of the minimum (a widely concerned geometric property in optimization) by showing a rigorous dependence between robustness and sharpness. This interplay will give us a better understanding of the OOD generalization problem, and meanwhile, provide more information on the final generalization bound. Detailed assumptions are clarified in Appendix B.1."
        },
        {
            "heading": "3.1 ROBUST OOD GENERALIZATION BOUND",
            "text": "The main concern in OOD generalization is to measure the domain shift of a learned model. However, existing methods fail to consider the intrinsic property of the model, such as robustness. Definition 2.2 gives us a new robustness measurement to the model trained on dataset S where the training set S is a collection of i.i.d. data pair (x, y) sampled from source distribution DS with size n. The measurement provides an intuition that if a test sample from the target domain is similar to a specific group of training samples, their losses will be similar as well. In other words, the model\u2019s robustness is a reflection of its ability to generalize to unseen data. Our first theorem shows that by utilizing the \"robustness\" measurement, we can more effectively handle domain shifts by setting a tolerance\nrange for distribution changes. This robustness measurement, therefore, provides a useful tool for addressing OOD generalization.\nTheorem 3.1. Let D\u0302T be the empirical distribution of size n drawn from DT . Assume that the loss \u2113 is upper bounded by M. With probability at least 1\u2212 \u03b4 (over the choice of the samples S), for every f\u03b8 trained on S satisfying (K, \u03f5(S))-robust, we have\nLT (f\u03b8) \u2264 L\u0302S(f\u03b8) +Md(\u03f5,K)(S, D\u0302T ) + 2\u03f5(S) + 3M \u221a 2K ln 2 + 2 ln(2/\u03b4)\nn (1)\nwhere the total variation distance d(\u03f5,K) for discrete empirical distributions is defined by:\n\u2200i \u2208 [n], ni(S) := #(z \u2208 S \u2229 Ci), d(\u03f5,K)(S, D\u0302T ) := K\u2211 i=1 \u2223\u2223\u2223\u2223\u2223ni(S)n \u2212 ni(D\u0302T )n \u2223\u2223\u2223\u2223\u2223 (2)\nand ni(S), ni(D\u0302T ) are the number of samples from S and D\u0302T that fall into the set Ci, respectively. Remark. The result can be decomposed into in-domain generalization and out-domain distance |LT (f\u03b8)\u2212 LS(f\u03b8)| (please refer to Lemma C.1). Both of them depend on robustness \u03f5(S).\nSee proof in Appendix C. The last three terms on the RHS of (1) are distribution distance, robustness constant, and error term, respectively. Unlike traditional distribution measures, we partition the sample space and the distributional shift separately in the K sub-groups instead of measuring it point-wisely. We argue that the d(\u03f5,K)(S, D\u0302T ) can be zero measure if all small changes happen within the same partition where a 2D illustrative case is shown in Figure 1. Under the circumstances, our distribution distance term will be significantly smaller than Proposition 2.1 as the target distribution is essentially a perturbation of the source distribution. As a robust OOD generalization measure, our bound characterizes how robust the learned model is to negligible distributional perturbations. To prevent a bound that expands excessively, we also propose an alternate solution tailored for non-robust algorithms (K \u2192\u221e) as follows. Corollary 3.2. If K \u2192 \u221e, the domain shift bound |LT (f\u03b8) \u2212 LS(f\u03b8)| can be replaced to the distribution distance in Proposition 2.1 where\n|LT (f\u03b8)\u2212 LS(f\u03b8)| \u2264 1\n2 dF\u2206F (S;DT ) \u2264\n1 2 dF\u2206F (S; D\u0302T ) +O(\n\u221a d\u2032/n) (3)\nwhere the pseudo dimension Pdim(F) = d\u2032. The proof is in Appendix C.1. As dictated in Theorem 3.1, when K \u2192\u221e, the use infinite number of partitions on the data distribution leads to meaningless robustness. However, Corollary C.7 suggests that our bound can be replaced by dF\u2206F (DS ;DT ) in the limit of infinite K. This avoids computing a vacuous bound for non-robust algorithms. In summary, Theorem 3.1 presents a novel approach for quantifying distributional shifts by incorporating the concept of robustness. Our framework is particularly beneficial when a robust algorithm is able to adapt to local shifts in the distribution. Additionally, our data-dependent result remains valid and useful in the overparameterized regime, since K does not depend on the model size."
        },
        {
            "heading": "3.2 SHARPNESS AND ROBUSTNESS",
            "text": "Clearly, robustness is inherently tied to the optimization properties of a model, particularly the curvature of the loss landscape. One direct approach to characterize this geometric curvature, referred to as \"sharpness,\" involves analyzing the Hessian matrix (Foret et al., 2020; Cohen et al., 2021). Recent research (Petzka et al., 2021) has shown that the concept of \u201crelative flatness\", the sharpness in this paper, has a strong correlation with model generalization. However, the impact of relative flatness on OOD generalization remains uncertain, even within the convex setting. To address this problem, we aim to investigate the interplay between robustness and sharpness. With the following definition of sharpness, we endeavor to establish an OOD generalization bound rooted in optimization principles. Definition 3.3 (Sharpness, Petzka et al. (2021)). For a twice differentiable loss function L(w) =\u2211\ns\u2208S \u2113w(s), w \u2208 Rm with a sample set S, the sharpness is defined by \u03ba(w, S,A) := \u27e8w,w\u27e9 \u00b7 tr (HS,A(w)) (4)\nwhere HS,A is the Hessian matrix of loss L(w) w.r.t. w with hypothesis set A and input set S.\nAs per Definition 3.3, sharpness is characterized by the sum of all the eigenvalues of the Hessian matrix, scaled by the parameter norm. Each eigenvalue of the Hessian reflects the rate of change of the loss derivative in the corresponding eigenspace. Therefore the smaller value of \u03ba indicates a flatter minimum. In Cha et al. (2021), they suggest that flatter minima will improve the OOD generalization, but fail to deliver an elaborate analysis of the Hessian matrix. In this section, we begin with the random ReLU Neural Networks parameterized by \u03b8 = ({ai}i\u2208[m],w) where w = [w1, ..., wm]\u22a4 is the trainable parameter. Let A = [a1, ...,am], the whole function class is defined as\nf(w, A,x) \u225c 1\u221a d m\u2211 i=1 wi\u03c3 (x,ai) : wi \u2208 R,ai \u223c Unif(Sd\u22121( \u221a d)), i \u2208 [m] (5)\nwhere \u03c3(\u00b7) is the ReLU activation function and a are random vectors uniformly distributed on n-dim hypersphere whose surface is a n \u2212 1 manifold. We then define any convex loss function \u2113(f(w, A,x), y) : R\u00d7 R\u2192 R+. The corresponding empirical minimizer in the source domain will be: w\u0302 = argminw 1n \u2211n i=1 \u2113(f(w, A,xi), yi). With w\u0302, we are interested in loss geometry over the sample domain (\u2113w\u0302,A(z) for short). Intuitively, a flatter minimum on the loss landscape is expected to be more robust to varying input. Suppose the sample space Z can be partitioned into K disjoint sets. For each set Ci, i \u2208 [K], the loss difference is upper bounded by \u03f5(S,A). Given z \u2208 S, we have\n\u03f5(S,A) \u225c max i\u2208[K] sup z,z\u2032\u2208Ci\n|\u2113w\u0302,A(z)\u2212 \u2113w\u0302,A(z\u2032)| . (6)\nAs an alternative form of robustness, the \u03f5(S,A) in (6) captures the \"maximum\" loss difference between any two samples in each partition and depends on the convexity and smoothness of the loss function in the input domain. Given a training set S and any initialization w0, the robustness \u03f5(S,A) of a learned model fw\u0302 will be determined. It explicitly reflects the smoothness of the loss function in each (pre-)partitioned set. Nevertheless, its connection to the sharpness of the loss function in parameter space still remains unclear. In order to address this gap, we establish a connection between sharpness and robustness in Theorem 3.4. Notably, this interplay holds implications not only for OOD but also for in-distribution generalization.\nTheorem 3.4. Assume for any A, the loss function \u2113w\u0302,A(z) w.r.t. sample z satisfies the LHessian Lipschitz continuity (refer to Definition B.2) within every set Ci,\u2200i \u2208 [K]. Let zi(A) = argmaxz\u2208Ci\u2229S \u2113w\u0302,A(z). Define Mi to be the set of global minima in Ci, suppose \u2203z\u2217i (A) \u2208 Mi such that for some \u03c1i(L) > 0, \u2225zi(A) \u2212 z\u2217i (A)\u2225 \u2264 \u03c1i(L) L almost surely, then let \u03c1max(L) = max{\u03c1i(L), i \u2208 [K]}, \u2225x\u22252 \u2261 R(d) and n\u2032 \u2264 n,\u2208 N+, w.p\np = min { 2 \u03c0 arccos ( R(d)\u2212 1 2 ) , \u2223\u2223\u2223\u22231\u2212 \u221a2d\u22124\u221a\u03c0R(d)e 14d\u22129 \u2223\u2223\u2223\u2223} over {ai}mi=1 \u223c Unif(Sd\u22121(\u221ad)) we have\n\u03f5(S,A) \u2264 \u03c1max(L) 2\n2L2\n([ n\u2032 +O ( d\nm\n)] \u03ba(w\u0302, S,A) + 4\u03c1max(L)\n3\n) . (7)\nRemark. Given the training set S, we can estimate factor n\u0302 that n\u2032 \u2264 n\u0302 by comparing the maximum Hessian norm w.r.t. zj to the sum of all the Hessian norms over {zi}i\u2208[n]. Note that the smoothness condition only applies to every partitioned set (locally) where it is much weaker than the global requirement for the loss function to be satisfied We also discuss the difference between our results and Petzka et al. (2021) in Appendix F. The chosen family of loss functions that applied to our theorem can be found in Appendix B.1. Corollary 3.5. Let w\u0302min be the minimum value of |w\u0302|. Suppose \u2200x \u223c Unif(Sd\u22121( \u221a d)) and |\u22022\u2113(f(w\u0302, A,x), y)/\u2202f2| is bounded by [M\u03031, M\u03032]. If m = Poly(d), d > 2, \u03c1max(L) < (w\u03022minM\u03031\u03c3\u0303(d,m))/(2d) taking expectation over all xj \u2208 S, j \u2208 [n] and all ai \u2208 A \u223c Unif(Sd\u22121( \u221a d))\u2200i \u2208 [m], we have\nES,A [\u03f5(S,A)] \u2264 ES,A 7\u03c1max(L)\n2\n6L2\n( n\u2032\u03ba(w\u0302, S,A) + M\u03032 ) . (8)\nwhere \u03c3\u0303(d,m) = Ea\u223cUnif(Sd\u22121(\u221ad))\u03bbmin( \u2211m i=1 aia \u22a4 i Gii) > 0 is the minimum eigenvalue and Gii is product constant of Gegenbauer polynomials (definition can be founded in Appendix B).\nSee proof in Appendix D. From Theorem 3.4 we can see, the robustness constant \u03f5(S,A) is (pointwise) upper bounded by the sharpness of the learned model, as measured by the quantity \u03ba(w\u0302, S,A), and the parameter \u03c1max(L). It should be noted that the parameter \u03c1max(L) depends on the partition, and as the number of partitions increases, the region \u03c1max(L) of the input domain becomes smaller, thereby making sharpness the dominant term in reflecting the model\u2019s robustness within the small local area. In Corollary 3.5, we show a stronger connection when the partition satisfies some conditions. Overall, this bound states that the larger the sharpness of the model \u03ba(w\u0302, S,A), the larger the upper bound on the robustness parameter \u03f5(S,A). This result aligns with the intuition that a sharper model is more prone to overfitting the training domain and is less robust in the unseen domain. While the dependency is not exact, it still can be regarded as an alternative approach that avoids the explicit computation of the intractable robustness term. By substituting this upper bound for \u03f5(S,A) into Theorem 3.1, we derive a sharpness-based OOD generalization bound. This implies that the OOD generalization error will have a high probability of being small if the learned model is flat enough. Unlike existing works, our generalization bound provides more information about how optimization property influences performance when generalizing to OOD data. It bridges the gap between robustness and sharpness which can also be generalized to non-OOD learning problems. Moreover, we provide a better theoretical grounding for an empirical observation that a flat minimum improves domain generalization (Cha et al., 2021) by pinpointing a clear dependence on sharpness."
        },
        {
            "heading": "3.3 CASE STUDY",
            "text": "To better demonstrate the relationship between sharpness and robustness, we provide two specific examples: (1) linear ridge regression; (2) two-layer diagonal neural networks for classification.\nExample 3.6. In ridge regression models, \u03f5(S,A) has a reverse relationship to the regularization parameter \u03b2. \u03b2 \u2191, the more probably flatter minimum \u03ba \u2193 and less sensitivity \u03f5 \u2193 of the learned model could be. Following the previous notation, we have \u2203c\u03031 > 0 such that \u03f5(S,A) \u2264 c\u03031\u03ba(\u03b8\u0302, S) + o\u0303d where o\u0303d has a smaller order than \u03ba(\u03b8\u0302, S) for large d (proof refer to Appendix E.1).\nAs suggested in Ali et al. (2019), let\u2019s consider a generic response model y|\u03b8\u2217 \u223c (X\u03b8\u2217, \u03c32I) where X \u2208 Rn\u00d7d, d > n. The least-square empirical minimizer of the ridge regression problem will be:\n\u03b8\u0302 = argmin \u03b8\n1 2n \u2225X\u03b8 \u2212 y\u22252 + \u03b2 2 \u2225\u03b8\u22252 = (X\u22a4X + n\u03b2In)\u22121X\u22a4y =M\u22121X\u22a4y (9)\nLet S be the training set. It\u2019s trivial to get the sharpness of a quadratic loss function where\n\u03ba(\u03b8\u0302, S) = \u2225\u03b8\u0302\u22252 tr(X\u22a4X/n+ \u03b2In) = \u2225\u03b8\u0302\u22252 tr(M) (10)\nIt\u2019s obvious that both of the above two equations depend on the same matrix M = X\u22a4X/n+ \u03b2In. For fixed training samples X , we have \u03ba(\u03b8\u0302, S) = O(\u03b2\u22121) in the limit of \u03b2. Then it\u2019s clear that a higher penalty \u03b2 leads to a flatter minimum. This intuition is rigorously proven in Appendix E.1. According to Theorem 3.1 and Theorem 3.4, a flatter minimum probably associates with lower robustness constant \u03f5(S,A). Thus it enjoys a lower OOD generalization error gap. In ridge regression, this phenomenon can be reflected by the regularization coefficient \u03b2. Therefore, in general, the larger \u03b2 is, the lower the sharpness \u03ba(\u03b8\u0302, S) and variance are. As a consequence, larger \u03b2 learns a more robust model resulting in a lower OOD generalization error gap. This idea is later verified in the distributional shift experiments, shown as Figure 2.\nExample 3.7. We consider a classification problem using a 2-layer diagonal linear network with exp-loss. The robustness \u03f5(S,A) has a similar relationship in Theorem 3.4. Given training set S, after iterations t > T\u03f5, \u2203c\u03032 > 0, \u03f5(S,A) \u2264 c\u03032 supt\u2265T\u03f5 \u03ba(\u03b8(t), S).\nIn addition to the regression and linear models, we have obtained a similar relationship for 2-layer diagonal linear networks, which are commonly used in the kernel and rich regimes as well as in intermediate settings (Moroshko et al., 2020). Example 3.7 demonstrates that the relationship also holds true when the model is well-trained, even exp-loss does not satisfy the P\u0141 condition. By extending our theorems to these more complex frameworks, we go beyond our initial assumptions and offer insights into broader applications. Later experiments on non-linear NN also support our statements. However, we still need a unified theorem for general function classes with fewer assumptions."
        },
        {
            "heading": "4 RELATED WORK",
            "text": "Despite various methods (Sun & Saenko, 2016; Sagawa et al., 2019; Shi et al., 2021; Shafieezadeh Abadeh et al., 2015; Li et al., 2018; Cha et al., 2021; Du et al., 2020; Zhang et al., 2022) that have been proposed to overcome the poor generalization brought by unknown distribution shifts, the underlying principles and theories still remain underexplored. As pointed out in Redko et al. (2020); Miller et al. (2021), different tasks that address distributional shifts, such as domain adaptation, OOD, and domain generalization, are collectively referred to as \"transfer transductive learning\" and share similar generalization theories. In general, the desired generalization bound will be split into In-Distribution/Domain (ID) generalization error and Out-of-Distribution/Domain (OOD) distance. Since Blitzer et al. (2007) establish a VC-dimension-based framework to estimate the domain shift gap by a divergence term, many following works make the effort to improve this term in the following decades, such as Discrepancy (Mansour et al., 2009), Wasserstein measurement (Courty et al., 2017; Shen et al., 2018), Integral Probability Metrics (IPM) (Zhang et al., 2019b; Ye et al., 2021) and \u03b2-divergence (Germain et al., 2016). Among them, new generalization tools like PAC-Bayes, Rademacher Complexity, and Stability are also applied. However, few of them discuss how the sharpness reacts to data distributional shifts.\nBeyond this canonical framework, Ye et al. (2021) reformulate the OOD generalization problem and provide a generalization bound using the concepts of \"variation\" and \"informativeness.\" The causal framework proposed in Peters et al. (2017); Rojas-Carulla et al. (2018) focuses on the impact of interventions on robust optimization over test distributions. However, none of these frameworks consider the optimization process of a model and how it affects OOD generalization. Inspired by previous investigation on the effect of sharpness on ID generalization (Lyu et al., 2022; Petzka et al., 2021), recent work in Cha et al. (2021) found that flatter minima can also improve OOD generalization. Nevertheless, they lack a sufficient theoretical foundation for the relationship between the \"sharpness\" of a model and OOD generalization, but end with a union bound of Blitzer et al. (2007)\u2019s result. In this paper, we aim to provide a rigorous examination of this relationship."
        },
        {
            "heading": "5 EXPERIMENTS",
            "text": "In light of space constraints, we present only a portion of our experimental results to support the validity of our theorems and findings. For comprehensive results, please refer to the Appendix G"
        },
        {
            "heading": "5.1 RIDGE REGRESSION IN DISTRIBUTIONAL SHIFTING",
            "text": "Following Duchi & Namkoong (2021), we investigated the ridge regression on distributional shift. We randomly generate \u03b8\u22170 \u2208 Rd in spherical space, and data from the following generating process: X\niid\u223c N (0, 1), y = X\u03b8\u22170 . To simulate distributional shift, we randomly generate a perpendicular unit vector \u03b8\u22a50 to \u03b8 \u2217 0 . Let \u03b8 \u22a5 0 , \u03b8 \u2217 0 be the basis vectors, then shifted ground-truth will be computed from the basis by \u03b8\u2217\u03b1 = \u03b8 \u2217 0 \u00b7 cos(\u03b1) + \u03b8\u22a50 \u00b7 sin(\u03b1). For the source domain, we use \u03b8\u22170 as our training distribution. We randomly sample 50 data points and train a linear classifier with a gradient descent of 3000 iterations. By minimizing the objective function in (9), we can get the empirical optimum \u03b8\u0302. Then we gradually shift the distribution by increasing \u03b1 to get different target domains. Along distribution shifting, the test loss \u2113(\u03b8\u0302,y\u03b1) will increase. As shown in Figure 2, the test loss will culminate in around 3 rads due to the maximum distribution shifting. Comparing different levels of regularization, we found that the larger L2-penalty \u03b2 brings lower OOD generalization error which is shown as darker purple lines. This plot bears out our intuition in the previous section. As stated in the aforementioned case, the sharpness of ridge regression should inversely depend on \u03b2. Correspondingly, we compute sharpness using the definition equation (4) by averaging ten different results. For each trial, we use the same training and test data for every \u03b2. The sharpness of each ridge regressor is shown in the legend of Figure 2. As we can see, larger \u03b2 leads to less sharpness."
        },
        {
            "heading": "5.2 SHARPER MINIMUM HURTS OOD GENERALIZATION",
            "text": "In our results, we proved that the upper bound of OOD generalization error involves the sharpness of the trained model. Here we empirically verified our theoretical insight. We follow the experiment setting in DomainBed (Gulrajani & Lopez-Paz, 2021). To easily compute the sharpness, we choose\nthe 4-layer MLP on RotatedMNIST dataset where RotatedMNIST is a rotation of MNIST handwritten digit dataset (LeCun, 1998) with different angles ranging from [0\u25e6, 15\u25e6, 30\u25e6, 45\u25e6, 60\u25e6, 75\u25e6]. In this codebase, each environment refers to selecting a domain (a specific rotation angle) as the test domain/OOD test dataset while training on all other domains. After getting the trained model of each environment, we compute the sharpness using all domain training sets based on the implementation of Petzka et al. (2021). To this end, we plot the performances of Empirical Minimization Risk (ERM), SWAD (Cha et al., 2021), Mixup (Yan et al., 2020) and GroupDRO (Sagawa et al., 2019) with 6 seeds of each. Then we measure the sharpness of all these minima. Figure 3 shows the relationship between model sharpness and out-of-domain accuracy. The tendency is clear that flat minima give better OOD performances. In general, different environments can not be plotted together due to different training sets. However, we found the middle 4 environments are similar tasks and thus plot them together for a clearer trend. In addition, different algorithms lead to different feature scales which may affect the scale of the sharpness. To address this, we align their scales when putting them together. For more individual results, please refer to Figure 8 in the appendix."
        },
        {
            "heading": "5.3 COMPARISON OF GENERALIZATION BOUNDS",
            "text": "To analyze our generalization bounds, we follow the toy example experiments in Sagawa et al. (2020). In this experiment, the distribution shift terms and generalization error terms can be explicitly computed. Furthermore, their synthetic experiment considers the spurious correlation across distribution shifts which is now a general formulation of OOD generalization (Wald et al., 2021; Aubin et al., 2021; Yao et al., 2022). Consider data x = [xcore, xspu] \u2208 that consist of two features: core feature and spurious feature. The features are generated from the following rule:\nxcore | y \u223c N ( y1, \u03c32core Id ) xspu | a \u223c N ( a1, \u03c32spuId ) where y \u2208 {\u22121, 1} is the label, and a \u2208 {\u22121, 1} is the spurious attribute. Data with y = a forms the majority group of size nmaj, and data with y = \u2212a forms minority group of size nmin. Total number of training points n = nmaj + nmin. The spurious correlation probability, pmaj = nmaj n defines the probability of y = a in training data. In testing, we always have pmaj = 0.5. The metric, worst-group error Sagawa et al. (2019) is defined as\nErrwg(w) := max i\u2208[4] Ex,y|gi [\u21130\u22121(w; (x, y))]\nwhere \u21130\u22121 is the 0\u2212 1 loss in binary classification. Here we compare the robustness of our proposed OOD generalization bound and the baseline in Proposition 2.1. We also give the comparison to other baselines, like PAC-Bayes DA bound in the Appendix G.\nAlong model size We plot the generalization error of the random feature logistic regression along the model size increases in Figure 4(a). In this experiment, we follow the hyperparameter setup of Sagawa et al. (2020) by setting the number of points n = 500, data dimension 2d = 200 with 100 on each feature. majority fraction pmaj = 0.9 and noises \u03c32spu = 1, \u03c3 2 core = 100. The worst-group error turns out to be nearly the same as the model size increases. However, in Figure 4(b), the error term in domain shift bound Proposition 2.1(A) will keep increasing when the model size is increasing. In contrast, our domain shift bound at order \u221a K is independent of the model size which addresses the limitation of their bound. We follow Kawaguchi et al. (2022) to compute K in an inverse image of the \u03f5-covering in a randomly projected space (see details in appendix). We set the same value K = 1, 000 in our experiment. Different from the baseline, K is data dependent and leads to a constant concentration error term along with model size increases. Analogously, our OOD generalization bound will not explode as model size increases (shown in Figure 4(c)).\nAlong distribution shift In addition, we are interested in characterizing OOD generalization when test distribution shifts from train distribution by varying the correlation probability pmaj during data generation. As shown in Figure 4(d), when pmaj = 0.5, there is no distributional shift between training and test data due to no spurious features correlated to training data. Thus, the training and test distributions align closer and closer when pmaj < 0.5 and increase, resulting in an initial decrease in the test error for the worst-case group. However, as pmaj > 0.5 and deviates from 0.5, introducing the spurious features, a shift in the distribution occurs. This deviation is likely to impact the worst-case group differently, leading to an increase in the test error. As displayed in Figure 4(e) and Figure 4(f), our distribution distance and generalization bound can capture the distribution shifts but are tighter than the baseline."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "In this paper, we provide a more interpretable and informative theory to understand Out-ofDistribution (OOD) generalization Based on the notion of robustness, we propose a robust OOD bound that effectively captures the algorithmic robustness in the presence of shifting data distributions. In addition, our in-depth analysis of the relationship between robustness and sharpness further illustrates that sharpness has a negative impact on generalization. Overall, our results advance the understanding of OOD generalization and the principles that govern it."
        },
        {
            "heading": "7 ACKNOWLEDGEMENT",
            "text": "This research/project is supported by the National Research Foundation, Singapore under its AI Singapore Programme (AISG Award No: AISG-GC-2019-001-2A), (AISG Award No: AISG2-TC2023-010-SGIL) and the Singapore Ministry of Education Academic Research Fund Tier 1 (Award No: T1 251RES2207). We would appreciate the contributions of Fusheng Liu, Ph.D. student at NUS."
        },
        {
            "heading": "A ADDITIONAL EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "A.1 SHARPNESS V.S. OOD GENERALIZATION ON PACS AND WILDS-CAMELYON17",
            "text": "To evaluate our theorem more deeply, we examine the relationship between our defined sharpness and OOD generalization error on larger-scale real-world datasets, Wilds-Camelyon17 Bandi et al. (2018); Koh et al. (2021) and PACS Li et al. (2017). Wilds-Camelyon17 dataset includes 455,954 tumor and normal tissue slide images from five hospitals (environments). One of the hospitals is assigned as the test environment by the dataset publisher. Distribution shift arises from variations in patient population, slide staining, and image acquisition. PACS dataset contains 9,991 images of 7 objects in 4 visual styles (environments): art painting, cartoon, photo, and sketch. Following the common setup in Gulrajani & Lopez-Paz (2021), each environment is used as a test environment in turn. We follow the practice in Petzka et al. (2021) to compute the sharpness using the Hessian matrix from the last Fully-Connected (FC) layer of each model. For the Wilds-Camelyon17 dataset, we test the sharpness of 18 ERM models trained with different random seeds and hyperparameters. Figure 5 shows the result. For the PACS dataset, we run 60 ERM models with different random seeds and hyperparameters for each test environment. To get a clearer correlation, we align the points from 4 environments by their mean performance. Figure 6 shows the result. From the two figures, we can observe a clear correlation between sharpness and out-of-distribution (OOD) accuracy. Sharpness tends to hurt the OOD performance of the model. The result is consistent with what we report in Figure 3. It shows that the correlation between sharpness and OOD accuracy can also be observed on large-scale datasets."
        },
        {
            "heading": "B NOTATIONS AND DEFINITIONS",
            "text": "Notations We use [n] denote the integers set {i}ni=1. \u2225\u00b7\u2225 represents \u21132-norm \u2225\u00b7\u22252 for short. Without loss of generality, we use \u2113(f(\u03b8,x),y) for the loss function of model f\u03b8 on data pair z = (x,y), which is denoted as \u2113\u03b8(z)) and we use n, d for training set size and input dimension. Note that we generally follow the notations in the original papers.\n\u2022 LS ,LT : expected risk of the source domain and target domain, respectively. The corresponding empirical version will be L\u0302S , L\u0302T\n\u2022 {Ci}Ki=1: K partitions on sample space and Ci denotes each partitioned set.\n\u2022 DS ,DT : distributions of source and target domain. Their sampled dataset will be denoted as D\u0302S , D\u0302T accordingly.\n\u2022 \u03b8: In our setting, \u03b8 = (w, {a}mi ) denotes the model parameters where w is the trainable parameters and {ai}m are the random features (we also use A = [a1, ...,am] for short notation in many places). w\u0302 is the minimizer of empirical loss.\n\u2022 M : upper bound of the loss function.\n\u2022 S = {(xi, yi)}ni /X = [x1, ...xn]: training data of size n. Definition B.1 (Robustness, Xu & Mannor (2012)). A learning algorithm A on training set S is (K, \u03f5(\u00b7))-robust, for K \u2208 N, if Z can be partitioned into K disjoint sets, denoted by {Ck}Kk=1, such that for all s \u2208 S, z \u2208 Z we have\n\u2200s, z \u2208 Ci,\u2200k \u2208 [K], |\u2113 (AS , s)\u2212 \u2113 (AS , z)| \u2264 \u03f5(S,A).\nDefinition B.2 (Hessian Lipschitz continuous). For a twice differentiable function f : Rn \u2192 R, it has L-Lipschitz continuous Hessian for domain x,y are vectors in Ci if\u2225\u2225\u22072f(y)\u2212\u22072f(x)\u2225\u2225 \u2264 Li\u2225y \u2212 x\u2225 where Li > 0 depends on input domain Ci and \u2225 \u00b7 \u2225 is L2 norm. Then for all K domains \u222aKi=1Ci, let L := max{Li|i \u2208 [K]} be the uniform Lipschitz constant, so we have\u2225\u2225\u22072f(y)\u2212\u22072f(x)\u2225\u2225 \u2264 Li\u2225y \u2212 x\u2225 \u2264 L\u2225y \u2212 x\u2225,\u2200i \u2208 [K], (x,y) \u2208 Ci which is uniformly bounded with L.\nLemma B.3 (Hessian Lipschitz Lemma). If f is twice differentiable and has L-Lipschitz continuous Hessian, then\u2223\u2223\u2223\u2223f(y)\u2212 f(x)\u2212 \u27e8\u2207f(x),y \u2212 x\u27e9 \u2212 12 \u2329\u22072f(x)(y \u2212 x), (y \u2212 x)\u232a \u2223\u2223\u2223\u2223 \u2264 L6 \u2225y \u2212 x\u22253."
        },
        {
            "heading": "Gegenbauer Polynomials",
            "text": "We briefly define Gegenbauer polynomials here whose details can be found in Appendix of Mei & Montanari (2022). First, we denote Sd\u22121(r) = {x \u2208 Rd : \u2225x\u2225 = r} as the uniform spherical distribution with the radius r on d\u2212 1 manifold. Let \u03c4d be the probability measure on Sd\u22121 and and the inner product in functional space L2([\u2212d, d], \u00b5d) denoted as \u27e8\u00b7, \u00b7\u27e9L2 and \u2225 \u00b7 \u2225L2 :\n\u27e8f, g\u27e9L2 \u2261 \u222b Sd\u22121( \u221a d) f(x)g(x)\u00b5d( dx).\nFor any function \u03c3 \u2208 L2([\u2212d, d], \u03c4d), where \u03c4d is the distribution of \u27e8x,y\u27e9/ \u221a d (x,y \u223c Sd\u22121( \u221a d)), the orthogonal basis {Qdt } forms the Gegenbauer polynomial of degree t(t \u2265 0), its spherical harmonics coefficients \u03bbd,t(\u03c3) can be expressed as:\n\u03bbd,t(\u03c3) = \u222b [\u2212 \u221a d, \u221a d] \u03c3(x)Q (d) t ( \u221a dx)\u03c4d(x),\nthen the Gegenbauer generating function holds in L2 ( [\u2212 \u221a d, \u221a d], \u03c4d ) sense\n\u03c3(x) = \u221e\u2211 k=0 \u03bbd,t(\u03c3)Nd,tQ (d) t ( \u221a dx)\nwhere Nd,t is the normalized factor depending on the norm of input."
        },
        {
            "heading": "B.1 ASSUMPTIONS",
            "text": "We discuss and list all assumptions we used in our theorems. The purposes are to offer clarity regarding the specific assumptions required for each theorem and ensure that the assumptions made in our theorems are well-founded and reasonable, reinforcing the validity and reliability of our results."
        },
        {
            "heading": "OOD Generalization",
            "text": "(Setting): Given a full sample space Z , source and target distributions are two different measures over this whole sample domainZ . The purpose is to study the robust algorithms in the OOD generalization setting.\n(Assumptions): For any sample \u2200z \u2208 Z , the loss function is bounded \u2113\u03b8(z) \u2208 [0,M ]. This assumption generally follows the original paper Xu & Mannor (2012). While it is possible to relax this assumption and derive improved bounds, our primary objective is to formulate a framework for robust OOD generalization and establish a clear connection with the optimization properties of the model."
        },
        {
            "heading": "Robustness and Sharpness",
            "text": "(Setting): In order to give a fine-grained analysis, we follow the common choice where a two-layer ReLU Neural Network function class is widely analyzed in most literature, i.e. Neural Tangent Kernel, non-kernel (rich) regime Moroshko et al. (2020) and random feature models. Among them, we select the following random feature models as our function class:\nf(w, A,x) \u225c 1\u221a d m\u2211 i=1 wi\u03c3 (x,ai) : wi \u2208 R,ai \u223c Unif(Sd\u22121( \u221a d)), i \u2208 [m]\nwhere m is the hidden size and A = [a1, ...,am] contains random vectors uniformly distributed on n-dim hypersphere whose surface is a n\u2212 1 manifold. \u03c3(a\u22a4x) = (a\u22a4x)I{a\u22a4x} denotes the ReLU activation function and I is the indicator function. w = [w1, ..., wm]\u22a4 is the trainable parameter. We choose the common loss functions: (1) Homogeneity in regression; (2) (Binary) Cross-Entropy Loss; (3) Negative Log Likelihood (NLL) loss;\n(Assumptions):\n(i) Let Ci, i \u2208 [K] be any set from whole partitions \u222aKi=1Ci, we assume \u2200z \u2208 Ci, the loss function \u2113w\u0302,A(z) satisfies L-Hessian Lipschitz for all i \u2208 [K] (See details in Definition B.2). Note that we only require this assumption to hold within each partition, instead of holding globally. In general, the smoothness and convexity condition is actually equivalent to locally convex which is a weak assumption for most function classes.\n(ii) Consider an optimization problem in each partition Ci, i \u2208 [K]. Let one of the training points zi(A) \u2208 S\u2229Ci be the initial point and z\u2217i (A) \u2208Mi is the corresponding nearest local minima where Mi is the local minima set of partition Ci. For some \u03c1i(L) > 0, we assume \u2225zi(A) \u2212 z\u2217i (A)\u2225 \u2264 \u03c1i(L)/L holds a.s.. It ensures the hessian norm \u2225H(zi(A))\u2225 has the lower bound. Similar conditions and estimations can be found in Zhang et al. (2019a).\n(iii) To simplify the computation of probability, we assume \u03bei = a\u22a4i x obeys a rotationally invariant distribution.\n(iv) For Corollary 3.5, we make additional assumptions that loss function \u2113() satisfied a bounded condition where the second derivative \u2200x \u223c Unif(Sd\u22121( \u221a d)), |\u22022\u2113(f(w\u0302, A,x), y)/\u2202f2| with re-\nspect to its argument f(w\u0302, A,x) should be bounded by [M\u03031, M\u03032]. Note, we consider the case where the data \u2200x \u223c Unif(Sd\u22121( \u221a d)) while m = Poly(d) is to ensure the positive definiteness of\u2211m\ni=1 aia \u22a4 i \u2208 Rd\u00d7d almost surely."
        },
        {
            "heading": "C PROOF TO DOMAIN SHIFT",
            "text": "Lemma C.1. Let D\u0302T be the empirical distribution of size n drawn from DT . The loss \u2113 is upper bounded by M. With probability at least 1\u2212 \u03b4 (over the choice of the samples), for every f\u03b8 trained on S, we have\nLT (f\u03b8) \u2264 LS(f\u03b8) +Md(\u03f5,K)(S, D\u0302T ) + \u03f5(S) + 2M \u221a 2K ln 2 + 2 ln(1/\u03b4)\nn (11)\nwhere \u2200i \u2208 [n], ni(S) := #(z \u2208 S \u2229 Ci), d(\u03f5,K)(S, D\u0302T ) := K\u2211 i=1 \u2223\u2223\u2223\u2223\u2223ni(S)n \u2212 ni(D\u0302T )n \u2223\u2223\u2223\u2223\u2223 (12)\nand ni(S), ni(D\u0302T ) are the number of samples from S and D\u0302T that fall into the set Ci, respectively.\nProof. In the following generalization statement, we use \u2113(f\u03b8, z) to denote the error obtained with input z and hypothesis function f\u03b8 for better illustration. By definition we have,\nLT (f\u03b8)\u2212 LS(f\u03b8) := Ez\u2032\u223cDT \u2113(f\u03b8, z\u2032)\u2212 Ez\u223cDS \u2113(f\u03b8, z). (13)\nThen we make the K partitions for source distribution DS . Let ni be the size of collection set of points x fall into the partition Ci where ni is the i.i.d.multinomial random variable with (ps(C1), ..., ps(CK)). We use parallel notation for target distribution DT with S\u2032i \u223c (pt(C1), ..., pt(CK)). Since\nEz\u223cDS \u2113(f\u03b8, z) = K\u2211 i=1 Ez\u223cDS (\u2113(f\u03b8, z\u2032)|z \u2208 Ci)ps(Ci)\nEz\u2032\u223cDT \u2113(f\u03b8, z \u2032) = K\u2211 i=1 Ez\u2032\u223cDT (\u2113(f\u03b8, z\u2032)|z\u2032 \u2208 Ci)pt(Ci)\n(14)\nand thus we have\nLT (f\u03b8)\u2212 LS(f\u03b8) = K\u2211 i=1 E(\u2113(f\u03b8, z\u2032)|z\u2032 \u2208 Ci)pt(Ci)\u2212 E(\u2113(f\u03b8, z)|z \u2208 Ci)ps(Ci)\n\u00b1 E(\u2113(f\u03b8, z\u2032)|z\u2032 \u2208 Ci)ps(Ci)\n= K\u2211 i=1 (E(\u2113(f\u03b8, z\u2032)|z\u2032 \u2208 Ci)) (pt(Ci)\u2212 ps(Ci))\n+ K\u2211 i=1 [E(\u2113(f\u03b8, z\u2032)|z\u2032 \u2208 Ci)\u2212 E(\u2113(f\u03b8, z)|z \u2208 Ci)] ps(Ci)\n\u2264 K\u2211 i=1 (E(\u2113(f\u03b8, z\u2032)|z\u2032 \u2208 Ci)) (pt(Ci)\u2212 ps(Ci)) + \u03f5(S,A).\n(15)\nIf we sample empirical distribution S, D\u0302T of size n each drawn from DS and DT , respectively. (n1, ..., nK) are the i.i.d. random variables belongs to Ci. We use the parallel notation n\u2032i for target distribution.\nd(\u03f5,K)(S, D\u0302T ) := K\u2211 i \u2223\u2223\u2223\u2223\u2223ni(S)n \u2212 ni(D\u0302T )n \u2223\u2223\u2223\u2223\u2223 . (16)\nFurther, we have K\u2211 i=1 (E(\u2113(f\u03b8, z\u2032)|z\u2032 \u2208 Ci)) (pt(Ci)\u2212 ps(Ci))\u2212 K\u2211 i=1 (E(\u2113(f\u03b8, z\u2032)|z\u2032 \u2208 Ci)) ( ni(D\u0302T ) n \u2212 ni(S) n )\n= K\u2211 i=1 (E(\u2113(f\u03b8, z\u2032)|z\u2032 \u2208 Ci))\n( pt(Ci)\u2212\nni(D\u0302T ) n\n) \u2212\nK\u2211 i=1 (E(\u2113(f\u03b8, z\u2032)|z\u2032 \u2208 Ci)) ( ps(Ci)\u2212 ni(S) n )\n\u2264M K\u2211 i=1 \u2223\u2223\u2223\u2223\u2223pt(Ci)\u2212 ni(D\u0302T )n \u2223\u2223\u2223\u2223\u2223+M K\u2211 i=1 \u2223\u2223\u2223\u2223ps(Ci)\u2212 ni(S)n \u2223\u2223\u2223\u2223 .\n(17)\nWith Breteganolle-Huber-Carol inequality we have\nK\u2211 i=1 \u2223\u2223\u2223\u2223ni(S)n \u2212 ps(Ci) \u2223\u2223\u2223\u2223 \u2264 \u221a 2K ln 2 + 2 ln(1/\u03b4) n . (18)\nTo integrate these two inequalities, we have K\u2211 i=1 (E(\u2113(f\u03b8, z\u2032)|z\u2032 \u2208 Ci))(pt(Ci)\u2212 ps(Ci))\n\u2264 K\u2211 i=1 (E(\u2113(f\u03b8, z\u2032)|z\u2032 \u2208 Ci)) ( ni(D\u0302T ) n \u2212 ni(S) n ) + 2M \u221a 2K ln 2 + 2 ln(1/\u03b4) n\n\u2264Md(\u03f5,K)(S, D\u0302T ) + 2M \u221a 2K ln 2 + 2 ln(1/\u03b4)\nn .\n(19)\nIn summary with probability 1\u2212 \u03b4 we have LT (f\u03b8) \u2264 LS(f\u03b8) +Md(\u03f5,K)(S, D\u0302T ) + \u03f5(S) + 2M \u221a 2K ln 2 + 2 ln(1/\u03b4)\nn (20)\nwhich completes the proof.\nWith the result of the domain (distribution) shift and the relationship between sharpness and robustness, we can move forward to the final OOD generalization error bound. First, we state the context of ID robustness bound in Xu & Mannor (2012) as follows.\nLemma C.2 (Xu et al.Xu & Mannor (2012)). Assume that for all h \u2208 H and z \u2208 Z , the loss is upper bounded by M i.e., \u2113(h, z) \u2264M . If the learning algorithm A is (K, \u03f5(\u00b7))-robust, then for any \u03b4 > 0, with probability at least 1\u2212 \u03b4 over an iid draw of n samples S = (zi)ni=1, it holds that:\nEz [\u2113 (AS , z)] \u2264 1\nn n\u2211 i=1 \u2113 (AS , zi) + \u03f5(S) +M \u221a 2K ln 2 + 2 ln(1/\u03b4) n\nAs the conclusive results, we briefly prove the following result by summarizing Lemma C.2 and Lemma C.1.\nTheorem C.3 (Restatement of Theorem 3.1). Let D\u0302T be the empirical distribution of size n drawn from DT . The loss \u2113 is upper bounded by M. With probability at least 1\u2212 \u03b4 (over the choice of the samples), for every f\u03b8 trained on S, we have\nLT (\u03b8) \u2264 L\u0302S(\u03b8) +Md(\u03f5,K)(S, D\u0302T ) + 2\u03f5(S) + 3M \u221a 2K ln 2 + 2 ln(2/\u03b4)\nn . (21)\nProof. Firstly, with Lemma C.1 and probability as least 1\u2212 \u03b42 , we have LT (f\u03b8) \u2264 LS(f\u03b8) +Md(\u03f5,K)(D\u0302S , D\u0302T ) + 2M \u221a 2K ln 2 + 2 ln(2/\u03b4)\nn + \u03f5(S)\nSecondly, with Lemma C.2 (Xu & Mannor (2012) Theorem 3) and probability as least 1\u2212 \u03b42 , we have\u2223\u2223\u2223LS(f\u03b8)\u2212 L\u0302S(f\u03b8)\u2223\u2223\u2223 \u2264 \u03f5(S) +M\u221a2K ln 2 + 2 ln(2/\u03b4) n\nBy taking the union bound, we conclude our final result that with probability at least 1\u2212 \u03b4 LT (f\u03b8) \u2264 L\u0302S(f\u03b8) + 3M \u221a 2K ln 2 + 2 ln(2/\u03b4)\nn + 2\u03f5(S) +Md(\u03f5,K)(S, D\u0302T ) (22)\nHere \u03f5(S) is the robustness constant that we can replace with any sharpness measure."
        },
        {
            "heading": "C.1 PROOF TO COROLLARY 3.2",
            "text": "Definition C.4. dF\u2206F (DT ;DS) := 2 supA(f)\u2208AF\u2206F |PrDS (A(f))\u2212 PrDT (A(f))| and F\u2206F is defined as: F\u2206F := {f(x)\u2295 f \u2032(x) : f, f \u2032 \u2208 F} where \u2295 is the XOR operator e.g. I(f \u2032(x) \u0338= f(x)). Lemma C.5 (Lemma 2 Zhao et al. (2018)). If Pdim(F) = d\u2032, then VCdim(F\u2206F) \u2264 2d\u2032 . Proposition C.6 (Zhao et al. (2018)). LetF be a hypothesis class with pseudo dimension Pdim(F) = d\u2032. If D\u0302S is the empirical distributions generated with n i.i.d.. samples from source domain, and D\u0302T is the empirical distribution on the target domain generated from n samples without labels, then with probability at least 1\u2212 \u03b4, for all f \u2208 F , we have:\nLT (f) \u2264 L\u0302S(f) + E\u2217 + \u221a 2d\u2032 log end\u2032\nn + \u221a log 2\u03b4 2n\n+ 1\n2 dF\u2206F\n( D\u0302T ; D\u0302S ) + 4 \u221a 2d\u2032 ln(2n) + ln 4\u03b4\nn\ufe38 \ufe37\ufe37 \ufe38 (Empirical div Error)\n(23)\nwhere E\u2217 = L\u0302S(f\u2217)+ L\u0302T (f\u2217) is the total error of best hypothesis f\u2217 over source and target domain.\nProof. With Lemma 4 (Zhao et al., 2018), we have\nLT (f) \u2264 L\u0302S(f) + 1\n2 dF\u2206F + E\u2217\nwhere E\u2217 = inf\nf \u2032\u2208F LS(f \u2032) + LT (f \u2032).\nLemma 6 (Zhao et al., 2018), which is actually Lemma 1 in (Ben-David et al., 2010), shows the following results\ndF\u2206F (DT ;DS) \u2264 dF\u2206F ( D\u0302T ; D\u0302S ) + 4 \u221a VCdim(F\u2206F) ln(2n) + ln 2\u03b4\nn .\nAs suggested in Zhao et al. (2018), VCdim(F\u2206F) is at most 2d\u2032. Further, with Theorem 2 (BenDavid et al., 2010), we have at probability at least 1\u2212 \u03b42\nLT (f) \u2264 LS(f) + 1\n2 dF\u2206F\n( D\u0302T ; D\u0302S ) + 4 \u221a VCdim(F\u2206F) ln(2n) + ln 2\u03b4\nn + E\u2217\n\u2264 LS(f) + 1\n2 dF\u2206F\n( D\u0302T ; D\u0302S ) + 4 \u221a 2d\u2032 ln(2n) + ln 2\u03b4\nn + E\u2217\n(24)\nUsing in-domain generalization error Lemma 11.6 (Mohri et al., 2018), with probability at least 1\u2212 \u03b42 the result is\nLS(f) \u2264 L\u0302S(f) +M \u221a 2d\u2032 log end\u2032\nn +M \u221a log 1\u03b4 2n\nNote in Zhao et al. (2018), the M = 1 for the normalized regression loss. Combine them all, we conclude the proof.\nCorollary C.7. If K \u2192 \u221e,M = 1, domain shift bound |LT (f\u03b8) \u2212 LS(f\u03b8)| will be reduced to (Empirical div Error) in Proposition C.6 where\n|LT (f\u03b8)\u2212 LS(f\u03b8)| \u2264 1\n2 dF\u2206F (DS ;DT ) \u2264 (Empirical div Error) (25)\nProof. According to Theorem C.3, we have\nLT (f\u03b8)\u2212 LS(f\u03b8) = K\u2211 i E(\u2113(f\u03b8, z\u2032)|z\u2032 \u2208 Ci)pt(Ci)\u2212 E(\u2113(f\u03b8, z)|z \u2208 Ci)ps(Ci) (26)\nIf K \u2192\u221e, let\u2019s define a domain that U := \u22c3\u221e\ni=1 Ci. The equation (26) will be LT (f\u03b8)\u2212 LS(f\u03b8) = \u222b z\u2032\u2208U \u2113(f\u03b8, z \u2032)pt(z \u2032)dz \u2212 \u222b z\u2208U \u2113(f\u03b8, z)ps(z)dz\n= \u222b z\u2032\u2208DT \u2113(f\u03b8, z \u2032)pt(z \u2032)dz \u2212 \u222b z\u2208DS \u2113(f\u03b8, z)ps(z)dz = Ez\u2032\u223cDT \u2113(f\u03b8, z\u2032)\u2212 Ez\u223cDS \u2113(f\u03b8, z).\n(27)\nIn this case, we have,\n|LT (f\u03b8)\u2212 LS(f\u03b8)| = |Ez\u2032\u223cDT \u2113(f\u03b8, z\u2032)\u2212 Ez\u223cDS \u2113(f\u03b8, z)|\n\u2264 \u222b \u221e 0 |PrDT (\u2113(f(\u03b8,x\u2032), y\u2032) > t) dt\u2212 PrDS (\u2113(f(\u03b8,x), y) > t) dt|\n= \u222b 1 0 |PrDT (\u2113(f(\u03b8,x\u2032), y\u2032) > t)\u2212 PrDS (\u2113(f(\u03b8,x), y) > t)| dt (M = 1) \u2264 sup t\u2208[0,1] sup f(\u03b8,\u00b7)\u2208F |PrDT (\u2113(f(\u03b8,x\u2032), y\u2032) > t)\u2212 PrDS (\u2113(f(\u03b8,x), y) > t)|\n\u2264 sup A(f)\u2208AF\u2206F |PrDT (A(f))\u2212 PrDS (A(f))|\n= 1\n2 dF\u2206F (DS ;DT ) \u2264 (Empirical div error)\n(28)\nwhere AF\u2206F represents a learning algorithm under the hypothesis F\u2206F = {f(x)\u2295 f \u2032(x) : f, f \u2032 \u2208 F}, which completes the proof."
        },
        {
            "heading": "D SHARPNESS AND ROBUSTNESS",
            "text": "Lemma D.1 (positive definiteness of Hessian). Let w\u0302min be the minimum value of |w\u0302| and x\u2217 = argminx\u2208Unif(Sd\u22121( \u221a d)) \u2113(f(w\u0302, A,x),y). For any A = (a1, ...,am),ai \u223c Unif(S d\u22121( \u221a d))\u2200i \u2208\n[m] denote \u03c3\u0303(d,m) = \u03bbmin( \u2211m\ni=1 aia \u22a4 i Gii) > 0 be the minimum eigenvalue, where Gij =\u2211\u221e\nt=0 \u03bb 2 d,t(\u03c3)N 2 d,tQ (d) t (\u27e8ai,aj\u27e9/\n\u221a d) is the polynomial product constant. If m = Poly(d), the\nhessian H(x\u2217) can be lower bound by\nEx\u2217\u223cUnif(Sd\u22121(\u221ad))H(x \u2217) \u2ab0 w\u0302\n2 min\u03c3\u0303(d,m)M\u03031\nd Id. (29)\nProof. As suggested in Lemma D.6 of (Zhong et al., 2017), we have a similar result to bound the local positive definiteness of Hessian. By previous definition, the Hessian w.r.t. x has a following partial order\nH(x\u2217) = D2f (x \u2217, y\u2217)\nd\n m\u2211 i=1 m\u2211 j=1 w\u0302iw\u0302jaia \u22a4 j \u03c3 \u2032(a\u22a4i x \u2217)\u03c3\u2032(a\u22a4j x \u2217)  \u2ab0 M\u03031\nd  m\u2211 i=1 m\u2211 j=1 w\u0302iw\u0302jaia \u22a4 j \u03c3 \u2032(a\u22a4i x \u2217)\u03c3\u2032(a\u22a4j x \u2217)  \u2ab0 w\u0302\n2 minM\u03031 d  m\u2211 i=1 m\u2211 j=1 w\u0302iw\u0302jaia \u22a4 j \u03c3 \u2032(a\u22a4i x \u2217)\u03c3\u2032(a\u22a4j x \u2217)  (30)\nFor the ReLU activation function, we further have\n\u03c3(a\u22a4x\u2217) \u2265 \u03c3\u2032(a\u22a4x\u2217) (31)\nWe extend the \u03c3 \u2208 L2([\u2212 \u221a d, \u221a d], \u03c4d) (where \u03c4d is the distribution of \u27e8x1,x2\u27e9/ \u221a d) by Gegenbauer polynomials that\n\u03c3(x) = \u221e\u2211 t=0 \u03bbd,t(\u03c3)Nd,tQ (d) t ( \u221a dx). (32)\nLet A = (a1, ...,am) \u2208 Rm\u00d7d. We assume \u2200x \u2208 Unif(Sd\u22121( \u221a d)). Lemma C.7 in (Mei & Montanari, 2022), suggests that\nU = ( Ex\u2217\u223cUnif(Sd\u22121(\u221ad))[\u03c3(\u27e8ai,x \u2217\u27e9/ \u221a d)\u03c3(\u27e8ai,x\u2217\u27e9/ \u221a d)] ) i,j\u2208[m] \u2208 Rm\u00d7m (33)\nwhich shows matrix U is a positive definite matrix. Similarly, taking the expectation over x\u2217, terms in RHS of (30) bracket can be rewritten as\nEx\u2217\u223cUnif(Sd\u22121(\u221ad))  m\u2211 i=1 m\u2211 j=1 aia \u22a4 j \u03c3 \u2032(a\u22a4i x \u2217)\u03c3\u2032(a\u22a4j x \u2217)  \u2ab0\nm\u2211 i=1 m\u2211 j=1 aia \u22a4 j Ex\u2217 [\u03c3(a\u22a4i x\u2217/ \u221a d)\u03c3(a\u22a4j x \u2217/ \u221a d)]\n(34)\nBesides, we have the following property of Gegenbauer polynomials,\n1. For x,y \u2208 Sd\u22121( \u221a d)\u2329\nQ (d) j (\u27e8x, \u00b7\u27e9), Q (d) k (\u27e8y, \u00b7\u27e9) \u232a L2(Sd\u22121( \u221a d),\u03b3d) = 1 Nd,k \u03b4jkQ (d) k (\u27e8x,y\u27e9).\n2. For x,y \u2208 Sd\u22121( \u221a d)\nQ (d) k (\u27e8x,y\u27e9) =\n1\nNd,k Nd,k\u2211 i=1 Y (d) ki (x)Y (d) ki (y).\nwhere spherical harmonics {Y (d)lj }1\u2264j\u2264Nd,l forms an orthonormal basis which gives the following results\nEx\u2217 [\u03c3(a\u22a4i x\u2217/ \u221a d)\u03c3(a\u22a4j x \u2217/ \u221a d)] = \u221e\u2211 t=0 \u03bb2d,t(\u03c3)N 2 d,tEx\u2217Q (d) t (\u27e8ai,x\u2217\u27e9/ \u221a d)Q (d) t (\u27e8aj ,x\u2217\u27e9/ \u221a d)\n= \u221e\u2211 t=0 \u03bb2d,t(\u03c3)N 2 d,tQ (d) t (\u27e8ai,aj\u27e9/ \u221a d) = Gij <\u221e.\n(35) Hence, we have\nm\u2211 i=1 m\u2211 j=1 aia \u22a4 j Ex\u2217 [\u03c3(a\u22a4i x\u2217/ \u221a d)\u03c3(a\u22a4j x \u2217/ \u221a d)] = m\u2211 i=1 aia \u22a4 i Gii +O(1/d)Var(a) (36)\nSince m = Poly(d), and {a}i\u2208[m] are i.i.d, then rank (\u2211m i=1 aia \u22a4 i Gii ) = rank ( AA\u22a4 ) = d. Let\n\u03c3\u0303(d,m) = Ea\u03bbmin( \u2211m i=1 aia \u22a4 i Gii) > 0 we have\nEx\u2217H(x\u2217) \u2ab0 w\u03022min\u03c3\u0303(d,m)M\u03031\nd Id (37)\nLemma D.2. Let \u22c3K\nk=1 Ck be the whole domain, the notion of (\u03f5,K)-robustness is described by\n\u03f5(S,A) \u225c max Ci\u2282 \u22c3K k=1 Ck sup z,z\u2032i\u2208Ci,z\u2208S |\u2113w\u0302,A(z)\u2212 \u2113w\u0302,A(z\u2032i)| .\nDefineMi be the set of global minima in Ci, where\nMi \u225c {z(A)|z(A) = min z\u2208Ci \u2113w\u0302,A(z)}\nsuppose for some maximum training loss point zi(A) \u2208 {\nmax z\u2208Ci\u2229S\n\u2113w\u0302,A(z)\u2212 \u2113w\u0302,A(z\u2217i (A)) }\nthere \u2203z\u2217i (A) where z\u2217i (A) \u225c arg min\nz\u2208Mi \u2225z \u2212 zi(A)\u2225\nsuch that \u2225zi(A)\u2212 z\u2217(A)\u2225 \u2264 \u03c1i(L)L almost surely hold for any A \u2208 Unif(S d\u22121( \u221a d)) and for any A, \u2113w\u0302,A(z) is L-Hessian Lipschitz continuous. Then the \u03f5(S,A) can be bounded by\n\u03f5(S,A) \u2264 max i\u2208[K]\n\u03c1i(L) 2\n2L2 (\u2225\u2225\u22072\u2113w\u0302,A(zi(A))\u2225\u2225+ 4\u03c1i(L) 3 )\nProof. Let z \u2208 S be a collection of (x, y) from the training set S and z\u2032i denote any collection from the set Ci. We define local minima setMi (which is the global minima set of Ci). Assume that for some maximum point zi(A) \u2208 maxz\u2208Ci\u2229S \u2113w\u0302,A(z), there exists a z\u2217i (A) \u2208 Mi almost surely for all A \u223c Unif(Sd\u22121( \u221a d)) such that\nz\u2217i (A) = arg min z\u2208Mi\nf := {z \u2208Mi : \u2225zi(A)\u2212 z\u2225} s.t. \u2225zi(A)\u2212 z\u2225 \u2264 \u03c1i(L)\nL (38)\nBy definition, \u03f5(S,A) can be rewritten as\n\u03f5(S,A) = max i\u2208[K] sup z,z\u2032i\u2208Ci,z\u2208S\n|\u2113w\u0302,A(z)\u2212 \u2113w\u0302,A(z\u2032i)|\n= max i\u2208[K] sup z\u2208Ci\u2229S,z\u2217\u2208Mi\n\u2113w\u0302,A(z)\u2212 \u2113w\u0302,A(z\u2217)\n= max i\u2208[K]\n\u2113w\u0302,A(zi(A))\u2212 \u2113w\u0302,A(z\u2217i (A)).\n(39)\nAccording to Lemma B.3, we have\n\u03f5(S,A) = max i\u2208[K]\n\u2113w\u0302,A(zi(A))\u2212 \u2113w\u0302,A(z\u2217i (A))\n(i) \u2264 max i\u2208[K] \u27e8\u2207\u2113w\u0302,A(z\u2217i (A)), zi(A)\u2212 z\u2217i (A)\u27e9\n+ 1\n2\n\u2329 \u22072\u2113w\u0302,A(z\u2217i (A))(zi(A)\u2212 z\u2217i (A)), zi(A)\u2212 z\u2217i (A) \u232a + L\n6 \u2225zi(A)\u2212 z\u2217\u22253\n=max i\u2208[K]\n1\n2\n\u2329 \u22072\u2113w\u0302,A(z\u2217i (A))(zi(A)\u2212 z\u2217i (A)), zi(A)\u2212 z\u2217i (A) \u232a + L\n6 \u2225zi(A)\u2212 z\u2217i (A)\u22253\n\u2264max i\u2208[K]\n1\n2 \u2225\u2225\u22072\u2113w\u0302,A(z\u2217i (A))\u2225\u2225 \u2225zi(A)\u2212 z\u2217i (A)\u22252 + L\n6 \u2225zi(A)\u2212 z\u2217i (A)\u22253 (Cauchy-Schwarz)\n(40) where (i) support by the fact\u2207\u2113w\u0302,A(z\u2217) = 0. With Lipschitz continuous Hessian we have\n\u2225\u22072\u2113w\u0302,A(z\u2217i (A))\u2225 \u2264 L\u2225zi(A)\u2212 z\u2217i (A)\u2225+ \u2225\u22072\u2113w\u0302,A(zi(A))\u2225. (41)\nOverall, we have\n\u03f5(S,A) \u2264 max i\u2208[K]\n1\n2 \u2225\u2225\u22072\u2113w\u0302,A(z\u2217i (A))\u2225\u2225 \u2225zi(A)\u2212 z\u2217i (A)\u22252 + L6 \u2225zi(A)\u2212 z\u2217i (A)\u22253 \u2264 max\ni\u2208[K]\n1\n2\n( \u2225\u22072\u2113w\u0302,A(zi(A))\u2225+ L\u2225zi(A)\u2212 z\u2217i (A)\u2225 ) \u2225zi(A)\u2212 z\u2217i (A)\u22252\n+ L\n6 \u2225zi(A)\u2212 z\u2217i (A)\u22253\n\u2264 max i\u2208[K]\n1\n2\n( \u2225\u22072\u2113w\u0302,A(zi(A))\u2225+ \u03c1i(L) ) \u03c1i(L)2 L2 + \u03c1i(L) 3 6L2\n= max i\u2208[K]\n\u03c1i(L) 2\n2L2 \u2225\u22072\u2113w\u0302,A(zi(A))\u2225+\n2\u03c1i(L) 3\n3L2\n(42)\nwhich completes the proof.\nLemma D.3 (Lemma 2.1 Bourin et al. (2013)). For every matrix in M+n+m partitioned into blocks, we have a decomposition[\nA X X\u2217 B\n] = U [ A 0 0 0 ] U\u2217 + V [ 0 0 0 B ] V \u2217\nfor some unitaries U, V \u2208Mn+m. Lemma D.4. Then, given an arbitrary partitioned positive semi-definite matrix,\u2225\u2225\u2225\u2225[ A XX\u2217 B ]\u2225\u2225\u2225\u2225 s \u2264 \u2225A\u2225s + \u2225B\u2225s\nfor all symmetric norms.\nProof. In lemma D.3 we have[ A X X\u2217 B ] = U [ A 0 0 0 ] U\u2217 + V [ 0 0 0 B ] V \u2217\nfor some unitaries U, V \u2208Mn+m. The result then follows from the simple fact that symmetric norms are non-decreasing functions of the singular values where f = \u2225 \u00b7 \u2225s : M 7\u2192 R, we have\nf ([ A X X\u2217 B ]) \u2264 f ( U [ A 0 0 0 ] U\u2217 ) + f ( V [ 0 0 0 B ] V \u2217 )\nLemma D.5. For a \u223c Unif(Sd\u22121( \u221a d)) and x are some vector \u2208 Rd with norm \u2225x\u2225 \u2261 \u221a R(d) \u2265 d, we have\nP(\u27e8x,a\u27e92 \u2265 \u2225a\u22252) \u2265 min  2 arccos ( 1\u221a R(d) ) \u03c0 , \u2223\u2223\u2223\u2223\u22231\u2212 \u221a 2d\u2212 4\u221a \u03c0R(d) exp ( 1 4d\u2212 9 )\u2223\u2223\u2223\u2223\u2223  (43)\nProof. We can replace the unit vector of a with e by\nP(\u27e8x,a\u27e92 \u2265 \u2225a\u22252) = P(\u27e8x, e\u27e92 \u2265 1) (44) Similarly, we can replace x by unit vector s such that\nP(\u27e8x, e\u27e92 \u2265 1) = P ( \u27e8s, e\u27e92 \u2265 1\nR(d)\n) (45)\nSolving \u27e8s, e\u27e92 = 1R(d) , we get\n\u27e8s, e\u27e92 = cos2 \u03d5 = 1 R(d) \u21d2 \u03d5 = arccos\u00b1 1\u221a R(d)\n(46)\nIn this case, the probability will converge to 1 as R(d) increases. As is known to us, surface area of Sd\u22121 equals\nAd = r d\u22121 2\u03c0\nd/2 \u0393 ( d 2 ) (47) An area Cd of the spherical cap equals\nAcapd (r) = \u222b \u03d5 0 Ad\u22121(r sin \u03b8)rd\u03b8 = 2\u03c0(d\u22121)/2 \u0393 ( d\u22121 2 ) rd\u22121 \u222b \u03d5 0 sind\u22122 \u03b8d\u03b8. (48)\nwhere \u0393 ( n\u2212 12 ) = (2(n\u22121))! 22(n\u22121)(n\u22121)! \u221a \u03c0.\n1) When d = 1, almost surely we have\nP((x \u00b7 e)2 \u2265 e2) = P(x2 \u2265 1) = 1. (49)\n2) When d = 2, we have a \u223c S2 where S2 is a circle r = 1 and the probability is the angle between s, e how much the vectors span within the circle where\nP ( \u27e8s, e\u27e92 \u2265 1\nR(d)\n) = 2 \u222b \u03d5 0 rd\u03b8\n\u03c0 =\n2\u03d5\n\u03c0 . (50)\n3) When d \u2265 3, the probability equals\nP ( |\u27e8s, e\u27e9| \u2265 1\u221a\nR(d)\n) = Acapd (r)\n1 2Ad\n= 1\u2212 A\u0303capd (r)\n1 2Ad\n(51)\nwhere A\u0303capd (r) is the remaining area of cutting the hyperspherical caps in half of the sphere,\nA\u0303capd (r) = 2\u03c0(d\u22121)/2 \u0393 ( d\u22121 2 ) \u222b \u03c02 \u03d5 sind\u22122 \u03b8d\u03b8\n\u2264 2\u03c0 (d\u22121)/2 \u0393 ( d\u22121 2 ) \u222b \u03c02 \u03d5 sin \u03b8d\u03b8 \u2264 2\u03c0 (d\u22121)/2\n\u0393 ( d\u22121 2 ) (\u2212 cos \u03c0 2 + cos\u03d5)\n= 2\u03c0(d\u22121)/2 \u0393 ( d\u22121 2\n) cos\u03d5. (52)\n3-a) If d is even then \u0393 ( d 2 ) = ( d 2 \u2212 1 ) !, so\n\u0393 ( d\u22121 2 ) \u0393 ( d 2 ) = (d\u2212 2)!\u221a\u03c0 2d\u22122 ( d 2 \u2212 1 ) !2 = \u221a \u03c0 2d\u22122 ( d\u2212 2 d\u22122 2 ) . (53)\nRobbins\u2019 bounds (Robbins, 1955) imply that for any positive integer d\n4d\u221a \u03c0d exp ( \u2212 1 8d\u2212 1 ) < ( 2d d ) < 4d\u221a \u03c0d exp ( \u2212 1 8d+ 1 ) . (54)\nSo we have 2Acapd (r)\nAd \u2264\n2\u0393 ( d 2 ) \u221a \u03c0\u0393 ( d\u22121 2 ) = 2d\u22121 \u03c0 ( d\u2212 2 d\u22122 2 )\u22121 cos\u03d5\n< 2d\u22121\n\u03c0\n\u221a \u03c0(d\u2212 2)/2\n2d\u22122 exp ( \u2212 14d\u22129 ) cos\u03d5 < \u221a 2d\u2212 4\n\u221a \u03c0 exp ( \u2212 14d\u22129\n) cos\u03d5. (55)\nSo the probability will be P ( \u27e8s, e\u27e92 \u2265 1\nR(d)\n) > 1\u2212\n\u221a 2d\u2212 4\u221a \u03c0 exp\n( 1\n4d\u2212 9\n) cos\u03d5. (56)\nSuppose R(d) = kd, k > 1, we have\nlim d\u2192\u221e\n\u221a 2d\u2212 4\n\u221a \u03c0kd exp ( \u2212 14d\u22129\n) =\u221a 2 k\u03c0 . (57)\n3-b) Similarly, if d is odd, then \u0393 ( d\u22121 2 ) = ( d\u22123 2 ) !, so\n\u0393 ( d\u22121 2 ) \u0393 ( d 2 ) = 2d\u22122 (d\u221232 )!2 (d\u2212 2)! \u221a \u03c0 = 2d\u22122 (d\u2212 2) \u221a \u03c0 ( d\u2212 3 d\u22123 2 )\u22121 . (58)\nIf d = 3 then\nP ( \u27e8s, e\u27e92 \u2265 1\nR(d)\n) \u2265 1\u2212 2 \u221a \u03c0\n2 \u221a \u03c0 cos\u03d5 = 1\u2212 1\u221a R(d) . (59)\nIf d > 3 then Robbins\u2019 bounds imply that \u221a \u03c0\u0393 ( d\u22121 2 ) 2\u0393 ( d 2 ) = 2d\u22124 d\u2212 2 ( d\u2212 3 d\u22123 2 )\u22121 > \u221a \u03c0(d\u2212 3)/2 d\u2212 2 exp ( 1 4d\u2212 11 ) . (60)\nThus, the probability will be at least P ( \u27e8s, e\u27e92 \u2265 1\nR(d)\n) > 1\u2212 d\u2212 2\u221a\n\u03c0(d\u2212 3) exp ( \u2212 1 4d\u2212 11 ) cos\u03d5. (61)\nTo simplify the result, we compare the minimum probability that for \u2200d \u2265 3 P ( \u27e8s, e\u27e92 \u2265 1\nR(d)\n) > 1\u2212 d\u2212 2\u221a\n\u03c0(d\u2212 3) exp ( \u2212 1 4d\u2212 11 ) cos\u03d5\n= 1\u2212 d\u2212 2\u221a \u03c0(d\u2212 3) exp\n( 1\n4d\u2212 11\n) cos\u03d5\n= 1\u2212 d\u2212 2\u221a \u03c0(d\u2212 3)R(d) exp\n( 1\n4d\u2212 11\n) cos\u03d5\n> 1\u2212 \u221a 2d\u2212 4\u221a \u03c0R(d) exp\n( 1\n4d\u2212 9\n) (62)\nOverall, we have \u2200d \u2208 N+,\nP ( \u27e8s, e\u27e92 \u2265 1\nR(d)\n) > min  2 arccos ( 1\u221a R(d) ) \u03c0 , \u2223\u2223\u2223\u2223\u22231\u2212 \u221a 2d\u2212 4\u221a \u03c0R(d) exp ( 1 4d\u2212 9 )\u2223\u2223\u2223\u2223\u2223  . (63)"
        },
        {
            "heading": "D.1 PROOF TO THEOREM 3.4",
            "text": "Proof. LetAd = (ai)i\u2208[d] i.i.d.\u223c Unif(Sd\u22121(\n\u221a d)). We consider the random ReLU NN function class\nto be\nFrelu(Ad) = { f(w, A,x) =\n1\u221a d m\u2211 i=1 wi\u03c3 ( x\u22a4ai ) : wi \u2208 R, i \u2208 [m] } where A = [a1, ...,am] \u2208 Rd\u00d7m. The empirical minimizer of the source domain is\nw\u0302 = min w\u2208Rd\n1\nn \u2211 xi,yi\u2208S \u2113(f(w, A,xi), yi) = 1 n \u2211 zi\u2208S \u2113w\u0302,A(zi). (64)\nThen with the chain rule, the first derivative of any input x at w\u0302 will be\n\u2207x\u2113 (f(w\u0302, A,x), y) = \u2202\u2113 (f(w\u0302, A,x), y)\n\u2202f(w\u0302, A,x)\n\u2202\u2113f(w\u0302, A,x)\n\u03c3(Ax)\n\u2202\u03c3(Ax)\n\u2202x\n= D1f (x, y)\u221a\nd m\u2211 i=1 w\u0302iaiI { a\u22a4i x \u2265 0 } = D1f (x, y)\u221a\nd w\u0302\u22a4\u03c3\u2032(Ax)\n(65)\nwhere a short notationD1f (x, y) denotes the first order directional derivative of f(w\u0302, A,x) \u03c3 \u2032(Ax) \u2208 Rm\u00d7d is the Jacobian matrix w.r.t. input x. Apparently, the second order derivative is represented as D2f (x, y), thus the Hessian will be\n\u22072x\u2113(f(w\u0302, A,x), y) = D2f (x, y)\nd\nm\u2211 i=1 m\u2211 j=1 w\u0302iw\u0302jaia \u22a4 j I { a\u22a4i x \u2265 0 ,a\u22a4j x \u2265 0 } = D2f (x, y)\nd \u03c3\u2032(Ax)\u22a4w\u0302w\u0302\u22a4\u03c3\u2032(Ax)\n(66)\nSimilarly, we have\n\u22072y\u2113(f(w\u0302, A,x), y) = D2y(x, y) \u00b7 (sgn(y))2 \u2217 \u2264 D2f (x, y) (67)\nwhere sgn(y) is the sign function. \u2217 holds under our choice of the family of loss functions.\n1. Homogeneity in regression, i.e. L1, MSE, MAE, Huber Loss, we have |D2y(x, y)| = |D2f (x, y)|;\n2. (Binary) Cross-Entropy Loss:\nD2y(x, y) = \u2202 2 ( y \u2211 i exp(x)/ C\u2211 c=1 exp(xc) ) /\u2202y2 = 0;\n3. Negative Log Likelihood (NLL) loss: D2y(x, y) = 0.\nBesides, as a convex loss function, D2y(x, y) \u2265 0. Hence, the range ofD2y(x, y) will be [0, D2f (x, y)].\nTo combine with robustness, we denote z = (x; y),\u2208 Rd+1. Therefore, the Hessian of z will be\nH(z|S,A) := \u22072x\u2113(f(w\u0302, A,x), y) \u22022\u2113(f(w\u0302,A,x),y))\u2202y\u2202x( \u22022\u2113(f(w\u0302,A,x),y)\n\u2202y\u2202x\n)\u22a4 \u22072y(\u2113(f(w\u0302, A,x), y))  . (68) With Lemma D.4, the spectral norm of Hessian z will be bounded by\n\u2225H(z)\u2225 \u2264 \u2225\u2225\u22072x\u2113(f(w\u0302, A,x), y)\u2225\u2225+ \u2223\u2223\u22072y\u2113(f(w\u0302, A,x), y)\u2223\u2223 . (69)\nThe first term in (69) can be further bounded by\u2225\u2225D2f (x, y)\u03c3\u2032(Ax)\u22a4w\u0302w\u0302\u22a4\u03c3\u2032(Ax)\u2225\u2225 \u2264 |D2f (x, y)|\u2225\u2225w\u0302w\u0302\u22a4\u2225\u2225\u2225\u2225\u03c3\u2032(Ax)\u03c3\u2032(Ax)\u22a4\u2225\u2225 = D2f (x, y)\u2225w\u0302\u22252\n\u2225\u2225\u03c3\u2032(Ax)\u03c3\u2032(Ax)\u22a4\u2225\u2225 (70) where the convexity of loss functions \u2200x, y,D2f (x, y) \u2265 0 supports the last equation. The right term has the facts that\n\u2225\u03c3\u2032(Ax)\u03c3\u2032(Ax)\u22a4\u2225 \u2264 \u2225\u03c3\u2032(Ax)\u03c3\u2032(Ax)\u22a4\u2225F = tr ( \u03c3\u2032(Ax)\u03c3\u2032(Ax)\u22a4 ) . (71)\nIn summary, we have the following inequality: \u2225H(z)\u2225 \u2264 D2f (x, y) ( 1\nd \u2225w\u0302\u22252 \u2225\u2225\u03c3\u2032(Ax)\u03c3\u2032(Ax)\u22a4\u2225\u2225+ 1) \u2264 D2f (x, y) ( 1\nd \u2225w\u0302\u22252 tr\n( \u03c3\u2032(Ax)\u03c3\u2032(Ax)\u22a4 ) + 1\n)\n= D2f (x, y) 1 d \u2225w\u0302\u22252 m\u2211 j=1 \u2225aj\u22252 I { a\u22a4j x \u2265 0 } + 1  (72)\nIn Lemma D.2, it depends on some zi = (xi, yi) \u2208 S \u2229 Ci that\n\u03f5(S,A) \u2264 max i\u2208[K]\n\u03c1i(L) 2\n2L2\n( \u2225H(zi(A))\u2225+ 4\u03c1i(L)\n3\n)\n\u2264 max i\u2208[K]\n\u03c1i(L) 2\n2L2\nD2f (xi, yi) \u2225w\u0302\u22252\nd m\u2211 j=1 \u2225aj\u22252 I { a\u22a4j xt \u2265 0 } + 1 + 4\u03c1i(L) 3  \u2264 \u03c1max(L) 2\n2L2 D2f (xk, yk) d \u2225w\u0302\u22252 m\u2211 j=1 \u2225aj\u22252 I { a\u22a4j xk \u2265 0 } + 4\u03c1max(L) 3 + o\u0303\u03ba  (73)\nwhere \u03c1max(L) = max{\u03c1i(L)}Ki=0, the o\u0303\u03ba = O(\u2113\u2032\u2032(f,xk, yk)\u2225w\u0302\u22252 \u2211m j=1 \u2225aj\u2225 2 I { a\u22a4j xk \u2265 0 } d/m) is a smaller order term compared to first term, since m \u226b d. Last equality, the maximum can be taken as we find maximum (xk, yk) \u2208 C\u0302 \u2208 {Ci}i\u2208[K]. Because xk, yk \u2208 S is one of the training sample k \u2208 [n], there must exist n\u2032 \u2208 [0, n] that\nD2f (xk, yk)\u2225w\u0302\u22252 m\u2211 j=1 \u2225aj\u22252 I { a\u22a4j xk \u2265 0 } = \u2225w\u0302\u22252n \u2032\nn n\u2211 k=1 D2f (xk, yk) d m\u2211 j=1 \u2225aj\u22252I{a\u22a4j xk \u2265 0}. (74)\nRecall that the sharpness of parameter w\u0302 is defined by\n\u03ba(w\u0302, S,A) := \u2225w\u0302\u22252 tr[HS,A(w\u0302)]\n= \u2225w\u0302\u22252 1 n n\u2211 j=1 D2f (xj , yj) \u00b7 tr ( \u03c3(Axj)\u03c3(Axj) \u22a4) = \u2225w\u0302\u22252 1\nn n\u2211 j=1 D2f (xj , yj) d m\u2211 i=1 (a\u22a4i xj) 2I { a\u22a4i xj \u2265 0 } .\n(75)\nLet the \u03bei = a\u22a4i x \u223c D(\u03be) and the expectation of E(\u03bei > 0) = qi where D(\u03be) is some rotationally invariant distribution, i.e. uniform or normal distribution. Under this circumstance, the sample mean of \u03bei still obeys the same family distribution as D(t\u03be). Thus, we have\nP  m\u2211 j=1 \u03beiI {\u03bei \u2265 0} \u2265 m\u2211 j=1 \u2225aj\u22252I {\u03bei \u2265 0}  = P  qi\u2211 j=1 \u03bei \u2265 qi\u2211 j=1 \u2225aj\u22252 \n= P((a\u22a4x)2 \u2265 \u2225a\u22252) = Exp(x)\n(76)\nWith Lemma D.5, we have at least a probability at\nP((a\u22a4x)2 \u2265 \u2225a\u22252) = min\n{ 2\n\u03c0 arccos\n( R(d)\u2212 1 2 ) , \u2223\u2223\u2223\u2223\u22231\u2212 \u221a 2d\u2212 4\u221a \u03c0R(d) e 1 4d\u22129 \u2223\u2223\u2223\u2223\u2223 }\n(77)\nthe following inequality holds,\n\u03f5(S,A) \u2264 \u03c1max(L) 2\n2L2\n( n\u2032\u03ba(w\u0302, S,A) + 4\u03c1max(L)\n3 + o\u0303\u03ba ) = \u03c1max(L) 2\n2L2\n([ n\u2032 +O ( d\nm\n)] \u03ba(w\u0302, S,A) + 4\n3 \u03c1max(L)\n) (78)"
        },
        {
            "heading": "D.2 PROOF TO COROLLARY 3.5",
            "text": "Corollary D.6 (Restatement of Corollary 3.5). Let w\u0302min be the minimum value of |w\u0302|. Suppose \u2200x \u223c Unif(Sd\u22121( \u221a d)) and |\u22022\u2113(f(w\u0302, A,x), y)/\u2202f2| is bounded by [M\u03031, M\u03032]. If m > d, \u03c1max(L) < (w\u0302 2 minM\u03031\u03c3\u0303(d,m))/(2d) for any A = (a1, ...,am),ai \u223c Unif(Sd\u22121( \u221a d))\u2200i \u2208 [m],\ntaking expectation over all xj \u2208 Unif(Sd\u22121( \u221a d)) in S, we have\nES,A [\u03f5(S,A)] \u2264 ES,A 7\u03c1max(L)\n2\n6L2\n( n\u2032\u03ba(w\u0302, S,A) + M\u03032 ) . (79)\nwhere \u03c3\u0303(d,m) = Ea\u03bbmin( \u2211m i=1 aia \u22a4 i Gii) > 0 is the minimum eigenvalue and Gii is product constant of Gegenbauer polynomials\nGij = \u221e\u2211 t=0 \u03bb2d,t(\u03c3)N 2 d,tQ (d) t (\u27e8ai,aj\u27e9/ \u221a d).\nProof. In our main theorem, with some probability, we have the following relation\n\u03f5(S,A) \u2264 \u03c1max(L) 2\n2L2\n( n\u2032\u03ba(w\u0302, S,A) + 4\u03c1max(L)\n3 + d\u03ba\n) .\nSo, we are concerned about the relation between the \u03ba(w\u0302, S,A) second term. If \u03c1i(L) < \u03ba(w\u0302, S,A), we may say the RHS is dominated by sharpness term \u03ba(w\u0302, S,A) as well as the main effect is taken by the sharpness. As suggested in Lemma D.1, we have\nEx\u2217\u223cUnif(Sd\u22121(\u221ad))H(x \u2217) \u2ab0 w\u0302\n2 minM\u03031\u03c3\u0303A(m, d)\nd Id (80)\nwhere x\u2217 is the global minimum over the whole set. As defined in (38), the following condition holds true\n\u2203z\u2217i (A) \u2208Mi, \u2225zi(A)\u2212 z\u2217i (A)\u2225 \u2264 \u03c1i(L)\nL , (81)\nand with Hessian Lipschitz, the relation is almost surely for arbitrary x that\nEz\u2217i (A)\u2225H(zi(A))\u2212H(z \u2217 i (A))\u2225 \u2264 L\u2225zi(A)\u2212 z\u2217i (A)\u2225 \u2264 \u03c1i(L)\n\u2264 \u2225\u2225\u2225\u2225\u2225 w\u03022minM\u03031\u03c3\u0303(d,m)2d Id \u2225\u2225\u2225\u2225\u2225\n\u2264 EA,x\u2217 1\n2 \u2225H(x\u2217)\u2225\n\u2264 Ez\u2217i (A) 1 2 \u2225H(z\u2217i (A))\u2225.\n(82)\nObviously, \u2225H(z\u2217i (A))\u2225 > 2\u03c1i(L). Following Lemma A.2 of Zhang et al. (2019a), for z\u2217i (A),\u2200z(A) \u2208 Ci, we have a similar result that\n\u03c3\u0303min(H(z(A))) \u2265 \u03c3\u0303min(H(z\u2217i (A))\u2212 \u2225H(z(A))\u2212H(z\u2217i (A))\u2225 \u2265 \u03c1i(L) (83)\nwhere \u03c3\u0303min denotes the minimum singular value. With Lemma D.2, we know that\nES,A\u03f5(S,A) \u2264 ES,A max i\u2208[K]\n\u03c1i(L) 2\n2L2\n( \u2225H(zi(A))\u2225+ 4\u03c1i(L)\n3\n) . (84)\nWe also have another condition that |D2f (x, y)| := \u2223\u2223\u2223\u2223\u22022\u2113(f(w\u0302, A,x), y)\u2202f2 \u2223\u2223\u2223\u2223 \u2208 [M\u03031, M\u03032],\u2200x, y. (85) Combine all these results, we finally have\nES,A\u03f5(S,A) \u2264 ES,A max i\u2208[K]\n\u03c1i(L) 2\n2L2\n( 1 + 4\n3\n) \u2225H(zi(A))\u2225\n\u2264 ES,A 7\u03c1max(L)\n2\n6L2\nD2f (xk, yk) d \u2225w\u0302\u22252 m\u2211 j=1 \u2225aj\u22252 I { a\u22a4j xk \u2265 0 } + M\u03032  . (86) Recall the definition of \u03ba(w\u0302, S,A) in the main theorem that\nES,A\u03ba(w\u0302, S,A) \u2264 E{x}n\u2225w\u0302\u22252 1\nn n\u2211 j=1 D2f (xj , yj) d m\u2211 i=1 (a\u22a4i xj) 2I { a\u22a4i xj \u2265 0 } (87)\nLook at the second sum, we have E{xj}n,{ai}m m\u2211 i=1 (a\u22a4i xj) 2I { a\u22a4i xj \u2265 0 } = m\u2211 i=1 ExjEai\u2225ai\u22252\u2225xj\u22252 cos2(\u03b2)I { a\u22a4i xj \u2265 0 } =\nm\u2211 i=1 ExjEai\u2225ai\u22252\u2225I { a\u22a4i xj \u2265 0 } d cos2(\u03b2).\n(88)\nSuppose x and a are i.i.d. from Unif(Sd\u22121(1)), let u = \u27e8x,a\u27e9, we have a well-known result that Eu[u2] = E[\u27e8x,a\u27e92]\n= E[\u2225x\u22252\u2225a\u22252 cos2(\u03b2)]\n= E[cos2(\u03b2)] = 1\nd , x,a \u2208 Rd\n(89)\nTherefore, in (88), m\u2211 i=1 ExjEai\u2225ai\u22252\u2225I { a\u22a4i xj \u2265 0 } d cos2(\u03b2) = m\u2211 i=1 ExjEai\u2225ai\u22252\u2225I { a\u22a4i xj \u2265 0 } (90)\nand we have (based on proof of main theorem), ES,A\u03f5(S,A)\n\u2264 7\u03c1max(L) 2\n6L2\n\u2225w\u0302\u22252n\u2032 n n\u2211 j=1 D2f (xj , yj) d m\u2211 i=1 ExjEai\u2225ai\u22252I{a\u22a4i xj \u2265 0}+ M\u03032  \u2264 ES,A 7\u03c1max(L) 2\n6L2\n( n\u2032\u03ba(w\u0302, S,A) + M\u03032\n) (91)"
        },
        {
            "heading": "E CASE STUDY",
            "text": "To better illustrate our theorems, we here give two different cases for clearly picturing intuition. The first case is the very basic model, ridge regression. As is known to us, ridge regression provides a straightforward way (by punishing the \u21132 norm of the weights) to reduce the \"variance\" of the model in order to avoid overfitting. In this case, this mechanism is equivalent to reducing the model\u2019s sharpness. Example E.1. In ridge regression models, the robustness constant \u03f5 has a reverse relationship to regularization parameter \u03b2 where \u03b2 \u2191, the more probably flatter minimum \u03ba \u2193 and less sensitivity \u03f5 \u2193 of the learned model could be. Follow the previous notation that \u03f5(S,A) denotes the robustness and \u03ba(\u03b8\u0302, S) is the sharpness on training set S, then we have\n\u03c4 > 0, c \u2208 (0, n], \u03f5(S,A) \u2264 c\u03ba(\u03b8\u0302, S) + o\u0303d where o\u0303d is a much smaller order than \u03ba(\u03b8\u0302, S)."
        },
        {
            "heading": "E.1 RIDGE REGRESSION",
            "text": "We consider a generic response model as stated in Ali et al. (2019).\ny|\u03b8\u2217 \u223c (X\u03b8\u2217, \u03c32I)\nridge regression minimization problem is defined by\nmin \u03b8\n1 2n \u2225X\u03b8 \u2212 y\u22252 + \u03b2 2 \u2225\u03b8\u22252, X \u2208 Rn\u00d7d, n < d. (92)\nThe least-square solution of ridge regression is \u03b8\u0302 = ( X\u22a4X + n\u03b2I )\u22121 X\u22a4y. (93)\nWith minimizer \u03b8\u0302, we now focus on its geometry w.r.t. x. Let S = {z}ni = (X,y) be the training set, (Z,\u03a3, \u03c1) be a measure space. Consider the bounded sample set Z such that\n\u2203M > 0, \u03c1(Z) < +\u221e. (94)\nThe Z can be partitioned into K disjoint sets {Ci}i\u2208[K]. By definition, we have robustness defined by each partition Ci, \u2200z, z\u2032 \u2208 Ci, \u2223\u2223\u2223\u2113(\u03b8\u0302, z)\u2212 \u2113(\u03b8\u0302, z\u2032)\u2223\u2223\u2223 \u2264 \u03f5(S,A). (95)\nFor this convex function \u2113(\u03b8\u0302, z), we have the following upper bound in the whole sample domain\n\u03f5(S) = max i\u2208[K] sup z,z\u2032\u2208Ci \u2223\u2223\u2223\u2113(\u03b8\u0302, z)\u2212 \u2113(\u03b8\u0302, z\u2032)\u2223\u2223\u2223 = max\ni\u2208[K] sup z\u2208Ci\n\u2113(\u03b8\u0302, z)\u2212 \u2113(\u03b8\u0302, z\u2217i \u2208 Ci)\n\u2264 sup zj\u2208Z\u2229S \u2113(\u03b8\u0302, zj)\u2212 \u2113(\u03b8\u0302, z\u2217)\n(96)\nwhere the z\u2217i , z \u2217 are the global minimum point in Ci and whole domain Z = \u22c3K i Ci, respectively. zj is a training data point that has the maximum loss difference from the optimum. Specifically, it as well as the augmented form of \u03b8\u0302 can be expressed as\nz = [x1, ..., xd, y] \u22a4, \u03b8\u0302+ = [\u03b8\u03021, ..., \u03b8\u0302d,\u22121]\u22a4,\u2208 Rd+1.\nThen the loss difference can be rewritten as\n\u2113\u03b8\u0302+(z) = (\u03b8\u0302 \u22a4 +z) 2 = (\u03b8\u0302\u22a4x\u2212 y)2 \u21d2 H(\u2113\u03b8\u0302+(z))is P.S.D matrix.\nIt is a convex function with regards to z such that\n\u03f5(S) \u2264 sup zj\u2208S \u2113\u03b8\u0302+(zj)\u2212 \u2113\u03b8\u0302+(z \u2217)\n= sup zj\u2208S\n\u2207\u2113\u03b8\u0302+(z \u2217)\u22a4(zj \u2212 z\u2217) +\n1 2 (zj \u2212 z\u2217)\u22a4H(\u2113\u03b8\u0302(z \u2217))(zj \u2212 z\u2217)\n= sup zj\u2208S\n1 2 (zj \u2212 z\u2217)\u22a4H(\u2113\u03b8\u0302+(z \u2217))(zj \u2212 z\u2217)\n\u2264 sup zj\u2208S\n1 2 \u2225H(\u2113\u03b8\u0302+(z \u2217))\u2225\u2225zj \u2212 z\u2217\u22252\n(97)\nwhere the second equality is supported by convexity and the third equality is due to \u2113\u03b8\u0302+(zj) = 0. Further, with Lemma D.4, we have\n\u2225H(\u2113\u03b8\u0302+(z \u2217))\u2225 = \u2225\u2225\u2225\u2225\u2225\u2225\u2225  \u03b8\u0302\u03b8\u0302\u22a4\n\u22022\u2113\u03b8\u0302+ (zj))\n\u2202y\u2202x( \u22022\u2113\u03b8\u0302+ (zj))\n\u2202x\u2202y\n)\u22a4 1\n \u2225\u2225\u2225\u2225\u2225\u2225\u2225 \u2264 \u2225\u2225\u2225\u03b8\u0302\u03b8\u0302\u22a4\u2225\u2225\u2225+ 1. (98)\nIn (97), we can also bound the norm of the input difference by\n\u2225zj \u2212 z\u2217\u22252 \u2264 \u2225xj \u2212 x\u2217\u22252 + (yj \u2212 y\u2217)2 (99)\nand then for simplicity, assume \u2225x\u2217\u22252 \u2264 \u2225xj\u22252 we have\n\u03f5(S) \u2264 sup xj\u2208S\n1\n2 (\u2225\u2225\u2225\u03b8\u0302\u03b8\u0302\u22a4\u2225\u2225\u2225+ 1) (\u2225xj \u2212 x\u2217\u22252 + (yj \u2212 y\u2217)2) \u2264 sup\nxj\u2208S\n1\n2 (\u2225\u2225\u2225\u03b8\u0302\u2225\u2225\u22252 + 1)(\u2225xj\u22252 + \u2225x\u2217\u22252 + (yj \u2212 y\u2217)2) \u2264 sup\nxj\u2208S\n1\n2 \u2225\u2225\u2225\u03b8\u0302\u2225\u2225\u22252 (\u2225xj\u22252 + \u2225x\u2217\u22252)+O(d) \u2264 sup\nxj\u2208S\n\u2225\u2225\u2225\u03b8\u0302\u2225\u2225\u22252 \u2225xj\u22252 +O(d) (100)\nwhere \u2225\u2225\u2225\u03b8\u0302\u2225\u2225\u22252 \u2225xj\u22252 = O(d2) is the dominate term for large d. Now, let\u2019s look at the relation to\nsharpness. By definition, \u03ba(\u03b8\u0302, S) = \u2225\u03b8\u0302\u22252 tr ( H\u03b8\u0302(\u2113(\u03b8\u0302, S)) ) = \u2225\u03b8\u0302\u22252 tr ( X\u22a4X\nn + \u03b2I\n) . (101)\nSince\ntr\n( X\u22a4X\nn + \u03b2I\n) = tr ( X\u22a4X\nn\n) + tr (\u03b2I) = tr ( XX\u22a4\nn\n) + \u03b2 = 1\nn n\u2211 j \u2225xj\u22252 + \u03b2, (102)\nso we have\n\u03ba(\u03b8\u0302, S) = \u2225\u03b8\u0302\u22252 1 n n\u2211 j \u2225xj\u22252 + \u03b2\u2225\u03b8\u0302\u22252. (103)\nAs is known to us, the \"variance\" of ridge estimator Ali et al. (2019) can be defined by Var(\u03b8\u0302) \u225c tr ( \u03b8\u0302\u03b8\u0302\u22a4 ) = tr (( XTX + n\u03b2I )\u22121 XTyy\u22a4X ( XTX + n\u03b2I )\u22121) . (104)\nNote that \u03b8\u0302\u03b8\u0302\u22a4 is a PSD with rank(\u03b8\u0302\u03b8\u0302\u22a4) = 1, thus it has only one eigenvalue \u03bb(\u03b8\u0302\u03b8\u0302\u22a4) = \u2225\u03b8\u0302\u22252 > 0. Var(\u03b8) = tr ( \u03b8\u0302\u03b8\u0302\u22a4 ) = \u2225\u03b8\u0302\u22252 = O(\u03b2\u22122) (105)\nBy definition, the covariance matrix E[yy\u22a4] is a diagonal matrix with entries of \u03c32. Averagely, we have tr ( \u03b8\u0302\u03b8\u0302\u22a4 ) = \u03c32 tr [( X\u22a4X + n\u03b2I )\u22121 X\u22a4X ( X\u22a4X + n\u03b2I )\u22121] = \u03c32\nn tr\n[ X\u22a4X\nn\n( X\u22a4X\nn + \u03b2I\n)\u22122]\n= \u03c32\nn d\u2211 i=1\n\u03bbi(X \u22a4X/n)\n(\u03bbi(X\u22a4X/n) + \u03b2) 2\n(106)\nwhere X \u22a4X n and\n( XTX n + \u03b2I )\u22121 are simultaneously diagonalizable and commutable. Therefore,\nthe greater \u03b2 is, the smaller tr ( \u03b8\u0302\u03b8\u0302\u22a4 ) is.\nConclusions From our above analysis, we have the following conditions.\n\u2022 Upper bound of robustness \u03f5(S) \u2264 supxj\u2208S \u2225\u03b8\u0302\u2225 2\u2225xj\u22252 +O(d). \u2022 Sharpness expression \u03ba(\u03b8\u0302, S) = \u2225\u03b8\u0302\u22252 (\n1 n \u2211 xj ,yj\u2208S \u2225xj\u2225 2 + \u03b2 ) .\n\u2022 Variance expression Var(\u03b8) = \u2225\u03b8\u0302\u22252.\n1) First, let\u2019s discuss how \u03b2 influences sharpness. \u03ba(\u03b8\u0302, S) = \u2225\u03b8\u0302\u22252 tr ( H\u03b8\u0302(\u2113(\u03b8\u0302, S)) ) = y\u22a4X ( X\u22a4X + n\u03b2I )\u22122 X\u22a4y  1 n \u2211 xj ,yj\u2208S \u2225xj\u22252 + \u03b2\n = O(\u03b2\u22121).\n(107)\nAs dictated in above equation, the \u03ba(\u03b8\u0302, S) = O(\u03b2\u22121) where sharpness holds an inverse relationship to \u03b2.\n2) Now, it\u2019s trivial to get the relationship between robustness and sharpness by combining the first two points. Because xj , yj \u2208 S in supermum is one of the training samples there exists a constant c < n and o\u0303d = o(d2) such that\n\u03f5(S) \u2264 c n n\u2211 j ( \u2225\u03b8\u0302\u22252\u2225xj\u22252 ) + o\u0303d\n\u2264 c n n\u2211 j ( \u2225\u03b8\u0302\u22252\u2225xj\u22252 ) + c\u03b2\u2225\u03b8\u0302\u22252 + o\u0303d\n= c\u03ba(\u03b8\u0302, S) + o\u0303d.\n(108)\nThis relation is consistent with Theorem 3.4 where the robustness is upper bounded by n\u2032 times sharpness \u03ba(\u03b8\u0302, S). Besides, the relation is simpler here, where robustness only depends on sharpness without other coefficients before \u03ba(\u03b8\u0302, S).\n3) Finally, the relation between \u03b2 and robustness will be\n\u03f5(S) \u2264 c n n\u2211 j ( Var(\u03b8)\u2225xj\u22252 ) + o\u0303d = c\u03c32 n d\u2211 i=1\n\u03bbi(X \u22a4X/n)\n(\u03bbi(X\u22a4X/n) + \u03b2) 2\n\u2211n j \u2225xj\u22252\nn + o\u0303d\n= \u03c32\nn d\u2211 i=1\n\u03bbi(X \u22a4X/n)\n(\u03bbi(X\u22a4X/n) + \u03b2) 2\n\u2211d j \u03bbj(X \u22a4X/n)\nn + o\u0303d\n= O(\u03b2\u22122).\n(109)\nwhere the \u03f5(S) somehow is the order of O(\u03b2\u22122) in the limit of \u03b2. It\u2019s clear to us that the greater \u03b2 is, the less sensitive (more robust) model we can get. In practical, we show that \"over-robust\" may hurt the model\u2019s performance (we show the detail empirically in Figure 7).\nE.2 2-LAYER DIAGONAL LINEAR NETWORK CLASSIFICATION\nHowever, our main theorem assumes the loss function satisfying Polyak-\u0141ojasiewicz (P\u0141) condition. To extend our result to a more general case, here we study a 2-layer diagonal linear network classification problem whose loss is exponential-based and not satisfied the P\u0141 condition. Example E.2. We consider a classification problem using a 2-layer diagonal linear network with exp-loss. The robustness \u03f5(S,A) has a similar relationship in Theorem 3.4. Given training set S, after iterations t > T\u03f5, \u2203C2 > 0, \u03f5(S,A) \u2264 C2 supt\u2265T\u03f5 \u03ba(\u03b8(t), S).\nGiven a training set S = (X, y), X = [x1, ...,xn], X \u2208 Rd\u00d7n. A depth-2 diagonal linear networks with parameters u = [u+,u\u2212]\u22a4 \u2208 R2d specified by:\nf(u,x) = \u27e8u2+ \u2212 u2\u2212,x\u27e9 We consider exponential loss where L(t) = 1n \u2211n\ni=1 exp(\u2212x\u22a4i \u03b8(t)yi) and yi \u2208 {\u22121, 1}. It has the same tail behavior and thus similar asymptotic properties as the logistic or cross-entropy loss. WLOG,\nwe assume \u2200i \u2208 [n] : yi = 1 such that xi = yixi. Suggest by Moroshko et al. (2020), we have\n\u03b8(t) = 2\u03b12 sinh ( 4X \u222b t 0 r(s)ds ) where r(t) = 1nexp(\u2212X\n\u22a4\u03b8(t)) and \u2225r(t)\u22251 = L(t). Note that d\u03b8(t)\ndt =\n4\nn\n\u221a \u03b82(t) + 4\u03b141 \u25e6X exp ( \u2212X\u22a4\u03b8(t) ) = A(t)Xr(t) (110)\nwhere A(t) = diag ( 4 \u221a \u03b82(t) + 4\u03b141 ) .\nPart i, sharpness. First derivative and Hessian can be obtained by\n\u2207\u03b8L(t) = \u2212(Xr(t))\u22a4\nH\u03b8(L(t)) = \u2212 \u2202(Xr(t))\u22a4\n\u2202\u03b8(t) = n\u2211 i=1 ri(t)XiX \u22a4 i .\n(111)\nWith the definition of sharpness, we can get\n\u03ba(\u03b8(t), S) = \u2225\u03b8(t)\u22252 tr(H\u03b8[L(t)])\n= \u2225\u03b8(t)\u22252 tr\n( n\u2211\ni=1\nri(t)XiX \u22a4 i\n)\n= \u2225\u03b8(t)\u22252 tr\n( n\u2211\ni=1\nri(t)X \u22a4 i Xi\n)\n= \u2225\u03b8(t)\u22252 (\nn\u2211 i=1\nri(t)\u2225xi\u22252 ) .\n(112)\nPart ii, robustness. Now, let\u2019s use the same discussion of the robustness constant in the previous case. Follow the previous definition of \u03f5(S,A), after some iteration number T\u03f5 it is defined by\n\u03f5(S,A) := sup i\u2208[n],t\u2265T\u03f5\n|nrj(t)\u2212 r\u2217(t)| (113)\nwhere nrj(t) is the (denormalized) point-wise loss of xj and r\u2217(t) denotes the minimum loss of point x\u2217. There exists n\u2032 < n, we have\n\u03f5(S,A) = sup t\u2265T\u03f5 n\u2032 (\u2225r(t)\u22251 \u2212 r\u2217(t)) \u2264 sup t\u2265T\u03f5 n\u2032\u2225r(t)\u22251. (114)\nLet \u2225xmin\u2225 = mini\u2208[n] \u2225xi\u2225, the above equation\n\u03f5(S,A) \u2264 1 \u2225xmin\u22252 sup t\u2265T\u03f5 (n\u2032\u2225r(t)\u2225\u2225xmin\u22252)\n= n\u2032\n\u2225xmin\u22252 sup t\u2265T\u03f5\n( n\u2211\ni=1\nri(t)\u2225xmin\u22252 )\n\u2264 n \u2032\n\u2225xmin\u22252 sup t\u2265T\u03f5\n( n\u2211\ni=1\nri(t)\u2225xi\u22252 ) .\n(115)\nPart iii, connection. Compare the last part of (112) and (115), we found that robustness and sharpness depend on the same term. Further, we can say that for any step t, \u2225\u03b8(t)\u22252 will have the upper bound. From Lemma 11 of Moroshko et al. (2020), we have the following inequality,\n\u2225\u03b8(t)\u2225\u221e \u2264 2\u03b12 sinh ( x\u0304\n2\u03b322\u03b1 2 \u03b3\u0303(t)\n) (116)\nwhere L(t) = exp(\u2212\u03b3\u0303(t)). Then, we can bound the \u2225\u03b8(t)\u22252 via:\nC1 \u2264 \u2225\u03b8(t)\u22252 \u2264 d\u2225\u03b8(t)\u22252\u221e = 4d\u03b14 sinh 2\n( x\u0304\n2\u03b322\u03b1 2 \u03b3\u0303(t)\n) <\u221e (117)\nNote that C1 > 0 In summary, we have\n\u03f5(S,A) \u2264 n \u2032\nC1\u2225xmin\u2225 sup t\u2265T\u03f5 \u03ba(\u03b8(t), S) \u2264 C2 sup t\u2265T\u03f5 \u03ba(\u03b8(t), S) (118)\nwhere C2 = n \u2032\nC1\u2225xmin\u2225 > 0 is a constant.\nPart iv, sanity check\u2013 asymptotic. Asymptotically, as t \u2192 \u221e, L(t) \u2192 0 while \u2225\u03b8(t)\u22252 will be explode. So if sharpness \u03ba(\u03b8(t), X)\u2192\u221e, then it will fail to imply robustness.\n\u03ba(\u03b8(t), S) = \u2225\u03b8(t)\u22252 (\nn\u2211 i=1\nri(t)\u2225xi\u22252 )\n\u2264 \u2225\u03b8(t)\u22252 (\nn\u2211 i=1\nri(t)\u2225xmax\u22252 )\n= \u2225\u03b8(t)\u22252L(\u03b8(t))\u2225xmax\u22252\n= \u03bamax(\u03b8(t), S)\n(119)\nLet \u03bamax(\u03b8(t), X) be a upper bound of \u03ba(\u03b8(t), X) at any time step t. The dynamics will be\nd\u03bamax(\u03b8(t), S)\ndt =\u2207\u03b8\u03bamax(\u03b8(t), S)\u03b8\u0307(t) =\u2225xmax\u22252 tr(XX\u22a4) ( \u2225\u03b8(t)\u22252L\u0307(t) + 2L(t)\u03b8(t)\u22a4\u03b8\u0307(t) ) =\u2225xmax\u22252 tr(XX\u22a4) ( \u2212\u2225\u03b8(t)\u22252(Xr(t))\u22a4A(t)Xr(t) + 2L(t)\u03b8(t)\u22a4A(t)Xr(t)\n) =\u2225xmax\u22252 tr(XX\u22a4) ( 2L(t)\u03b8(t)\u22a4 \u2212 \u2225\u03b8(t)\u22252(Xr(t))\u22a4 ) A(t)Xr(t)\n(120)\nAs t\u2192\u221e, we have L(t)\u2192 0, thus it is easy to converge that\nlim t\u2192\u221e\nd\u03bamax(\u03b8(t), S)\ndt = \u2212\u2225xmax\u22252\u2225\u03b8(t)\u22252(Xr(t))\u22a4A(t)Xr(t) = \u2212\u2225xmax\u22252\u2225\u03b8(t)\u22252L\u0307(t) = 0\n(121) As we can see, the dynamics of the derivative of \u03bamax(\u03b8(t), X) is decreasing to 0 as t\u2192\u221e which means the sharpness \u03ba(\u03b8(t), X) will be upper bounded by a converged number \u03ba(\u03b8(t\u2192\u221e), X) < C\u221e <\u221e. So sharpness will not explode."
        },
        {
            "heading": "F COMPARISON TO FEATURE ROBUSTNESS",
            "text": ""
        },
        {
            "heading": "F.1 THEIR RESULTS",
            "text": "Definition F.1 (Feature robustness Petzka et al. Petzka et al. (2021)). Let \u2113 : Y \u00d7 Y \u2192 R+denote a loss function, \u03f5 and \u03b4 two positive (small) real numbers, S \u2286 X \u00d7 Y a finite sample set, and A \u2208 Rm\u00d7m a matrix. A model f(x) = (\u03c8 \u25e6 \u03d5)(x) with \u03d5(X ) \u2286 Rm is called ((\u03b4, S,A), \u03f5)-feature robust, if\n\u2223\u2223\u2223E\u03d5F (f, S, \u03b1A)\u2223\u2223\u2223 \u2264 \u03f5 for all 0 \u2264 \u03b1 \u2264 \u03b4. More generally, for a probability distribution A on perturbation matrices in Rm, we define\nE\u03d5F (f, S,A) = EA\u223cA [ E\u03d5F (f, S,A) ] ,\nand call the model ((\u03b4,S,A), \u03f5)-feature robust on average over A, if \u2223\u2223\u2223E\u03d5F (f, S, \u03b1A)\u2223\u2223\u2223 \u2264 \u03f5 for 0 \u2264 \u03b1 \u2264 \u03b4 Theorem F.2 (Theorem 5 Petzka et al. Petzka et al. (2021)). Consider a model f(x,w) = g(w\u03d5(x)) as above, a loss function \u2113 and a sample set S, and let Om \u2282 Rm\u00d7m denote the set of orthogonal matrices. Let \u03b4 be a positive (small) real number and w = \u03c9 \u2208 Rd\u00d7m denote parameters at a local minimum of the empirical risk on a sample set S. If the labels satisfy that y (\u03d5\u03b4A (xi)) = y (\u03d5 (xi)) = yi for all (xi, yi) \u2208 S and all \u2225A\u2225 \u2264 1, then f(x, \u03c9) is ((\u03b4, S,Om) , \u03f5)-feature robust on average over Om for \u03f5 = \u03b4 2 2m\u03ba \u03d5(\u03c9) +O ( \u03b43 ) ."
        },
        {
            "heading": "Proof sketch",
            "text": "1. With assumption that y[\u03d5\u03b4A(xi)] = yi for all (xi, yi) \u2208 S and all \u2225A\u2225 \u2264 1, feature perturbation around w is\nE\u03d5F (f, S, \u03b1A) + Eemp(w, S) = Eemp(w + \u03b1wA,S)\n2. Since Taylor expansion for local minimum w = \u03c9 will only remains second order term, thus\nEemp(w + \u03b1wA,S) = Eemp(\u03c9, S) + \u03b12\n2 d\u2211 s,t=1 (\u03c9sA)Hs,t(\u03c9, \u03d5(S))(\u03c9tA) \u22a4\n3. With basic algebra, one can easily get\nEA\u223cOm [ E\u03d5F (f, S, \u03b1A) ] \u2264 \u03b4 2\n2 d\u2211 s,t=1 EA\u223cOm [ (\u03c9sA)Hs,t (\u03c9tA) T ] +O ( \u03b43 )\n= \u03b42\n2m d\u2211 s,t=1 \u27e8\u03c9s, \u03c9t\u27e9 \u00b7 Tr (Hs,t) +O ( \u03b43 )\n= \u03b42\n2m \u03ba\u03d5(\u03c9) +O\n( \u03b43 )"
        },
        {
            "heading": "F.2 COMPARISON TO OUR RESULTS",
            "text": "We first summarize the commonalities and differences between their results and ours:\n\u2022 Both of us consider the robustness of the model. But they define the feature robustness while we study the loss robustness Xu & Mannor (2012) which has been studied for many years.\n\u2022 They consider a non-standard generalization gap by decomposing it into representativeness and the expected deviation of the loss around the sample points. But we strive to integrate sharpness into the general generalization guarantees.\nFor point 1, their defined feature robustness trivially depends on the sharpness. Because the sharpness (the curvature information) is just defined by the robust perturbation areas around the desired point. From step 2 in the above proof sketch we can see, the hessian w.r.t. \u03c9 is exactly the second expansion of perturbed expected risk. So we think this definition provides less information about the optimization landscape. In contrast, we consider the loss robustness for two reasons: 1) it is easy to get in practice without finding the orthogonal matrices Om first. 2) we highlight its dependence on the data manifold.\nFor point 2, we try to integrate this optimization property (sharpness) into the standard generalization frameworks in order to get a clearer interplay. Unlike feature robustness, the robustness defined by loss function will be easier analyzed in generalization tools, because it\u2019s hard and vague to define the \"feature\" in general. Besides, our result will also benefit the data-dependent bounds Xu & Mannor (2012); Kawaguchi et al. (2022)."
        },
        {
            "heading": "G EXPERIMENTS AND DETAILS",
            "text": ""
        },
        {
            "heading": "G.1 RIDGE REGRESSION WITH DISTRIBUTIONAL SHIFTING",
            "text": "As we stated before, we followed the Duchi & Namkoong (2021) to investigate the ridge regression on distributional shift. We randomly generate \u03b8\u22170 \u2208 Rd in spherical space, and data from\nX iid\u223c N (0, 1), y = X\u03b8\u22170 (122)\nTo simulate distributional shift, we randomly generate a perpendicular unit vector \u03b8\u22a50 to \u03b8 \u2217 0 . Let \u03b8\u22a50 , \u03b8 \u2217 0 be the basis vectors, then shifted ground-truth will be computed from the basis by \u03b8 \u2217 \u03b1 = \u03b8\u22170 \u00b7 cos(\u03b1)+ \u03b8\u22a50 \u00b7 sin(\u03b1). For the source domain, we use \u03b8\u22170 as our training distribution. We randomly\nsample 50 data points and train a linear classifier with a gradient descent of 3000 iterations. Starting from \u03b1 = 0, we gradually increase the \u03b1 to generate different distributional shifts.\nFrom the left panel in Figure 7 we can see that a larger penalty suffers from lower loss increasing when \u03b2 ranges from 0 to 2. Since we consider a cycling shift of label space, 180\u25e6 corresponds to the maximum shift thus leading to the highest loss increase. According to our analysis of ridge regression, larger \u03b2 means a flatter minimum and more robustness, resulting in a better OOD generalization. This experiment verifies our theoretical analysis. However, it is important to note that too large a coefficient of \u21132 regularization will hurt the performance. As shown in Figure 7 (right panel), the curse of underfitting (indicated by brown colors) appears when \u03b2 > 4."
        },
        {
            "heading": "Maximum Shift Maximum Shift",
            "text": ""
        },
        {
            "heading": "G.2 ADDITIONAL RESULTS ON SHARPNESS",
            "text": "Here we show more experimental results on RotatedMNIST which is a rotation of MNIST handwritten digit dataset with different angles ranging from 0\u25e6, 15\u25e6, 30\u25e6, 45\u25e6, 60\u25e6, 75\u25e6 (6 domains/distributions). For each environment, we select a domain as a test domain and train the model on all other domains. Then OOD generalization test accuracy will be reported on the test domain. In our experiments, we run each algorithm with 12 different seeds, thus getting 12 trained models of different minima. Then we compute the sharpness (see Algorithm 1) for all these models and then plot them in Figure 8. For algorithms, we choose Empirical Risk Minimization (ERM), and Stochastic Weight Averaging (SWA) as the plain OOD generalization algorithm which is shown in the first column. In robust optimization algorithms, we choose Group Distributional Robust Optimization Sagawa et al. (2019). We also choose CORALSun & Saenko (2016) as a multi-source domain generalization algorithm. Among these different types of out-of-domain generalization algorithms, we can conclude that sharpness will affect the test accuracy on the OOD dataset.\nExperimental configurations are listed in Table 1. For each model, we run the 5000 iterations and choose the last model as the test model. To ease the computational burden, we choose the 3-layer MLP to compute the sharpness.\nFrom Figure 8 we can see, the sharpness has an inverse relationship with the out-of-domain generalization performance for every model in each individual environment. To make it clear, we plot similar\ntasks from environment 1 to 4 as the last row. Thus, we can see a clearer tendency in all algorithms. It verified our Theorem 3.1. Note that all algorithms have different feature scales. One may need to normalize the results of different algorithms when plotting them together.\nAlgorithm 1 Pseudocode of model sharpness computation Require: feature layer f(x), training loss \u2113 Ensure: Sharpness S\nGet Jacobian matrix w.r.t. feature layer J = \u2207\u2113(f(x), y) for each gradient vector Ji in J\u22a4 do\nCompute Hessian w.r.t. element i, j of f(x) by \u2202Ji\u2202fl(x) end for We store Hessian in the variable H Initialize sharpness S = 0 for i in feature layer.shape[0] do\nfor j in feature layer.shape[0] do Retrieve the hessian value of i, j element via h\u2190 H[:, j, i, :] Sharpness sij \u2190 Trace(h) \u2217 fij(x)2 S = S + sij\nend for end for"
        },
        {
            "heading": "G.3 COMPARE OUR ROBUST BOUND TO OTHER OOD GENERALIZATION BOUNDS",
            "text": ""
        },
        {
            "heading": "G.3.1 COMPUTATION OF OUR BOUNDS",
            "text": "First, we follow Kawaguchi et al. (2022) to compute the K in an inverse image of the \u03f5\u2212 covering in a randomly projected space. The main idea is to partition input space in a projected space with transformation matrix A\u0303. The specific steps will be (1) To generate a random matrix A\u0303, we i.i.d. sample each entry from the Uniform Distribution U(0, 1). (2) Each row of the random matrix A \u2208 R3\u00d7d is then normalized so that Ax \u2208 [0, 1]3, i.e. Aij = A\u0303ij/ \u2211d j=1 A\u0303ij (3) After generating a random matrix A, we use the \u03f5-covering of the space of u = Ax to define the pre-partition {C\u0303i}Ki=1."
        },
        {
            "heading": "G.3.2 COMPUTATION OF PAC-BAYES BOUND",
            "text": "We follow the definition to compute expected dis\u03c1 in Germain et al. (2013) where\nDefinition G.1. LetH be a hypothesis class. For any marginal distributions DS and DT over X , any distribution \u03c1 onH, the domain disagreement dis\u03c1 (DS , DT ) between DS and DT is defined by,\ndis\u03c1 (DS , DT ) def = \u2223\u2223\u2223\u2223 Eh,h\u2032\u223c\u03c12 [RDT (h, h\u2032)\u2212RDS (h, h\u2032)] \u2223\u2223\u2223\u2223 .\nSince the dis\u03c1 (DS , DT ) is defined as the expected distance, we can compute its empirical version according to their theoretical upper bound as follows.\nProposition G.2 (Germain et al. Germain et al. (2013) Theorem 3). For any distributions DS and DT over X , any set of hypothesisH, any prior distribution \u03c0 overH, any \u03b4 \u2208 (0, 1], and any real number \u03b1 > 0, with a probability at least 1\u2212 \u03b4 over the choice of S \u00d7 T \u223c (DS \u00d7DT )m, for every \u03c1 onH, we have\ndis\u03c1 (DS , DT ) \u2264 2\u03b1 [ dis\u03c1(S, T ) + 2KL(\u03c1\u2225\u03c0) ln 2\u03b4 m\u00d7\u03b1 + 1 ] \u2212 1\n1\u2212 e\u22122\u03b1\n. With dis\u03c1(S, T ), we can then compute the final generalization bound by the following inequality \u2200\u03c1 onH, RPT (G\u03c1)\u2212RPT ( G\u03c1\u2217T ) \u2264 RPS (G\u03c1)\n+ dis\u03c1 (DS , DT ) +RDT ( G\u03c1, G\u03c1\u2217T ) +RDS ( G\u03c1, G\u03c1\u2217T )\nwith \u03c1\u2217T = argmin\u03c1RPT (G\u03c1) is the best target posterior, and RD ( G\u03c1, G\u03c1\u2217T ) = Eh\u223c\u03c1Eh\u2032\u223c\u03c1\u2217TRD (h, h \u2032).\nNote that we ignore the expected errors over the best hypothesis by assuming the RD ( G\u03c1, G\u03c1\u2217T ) = 0. We apply the same operation in E\u2217 of Proposition C.6 as well."
        },
        {
            "heading": "G.3.3 COMPARISONS",
            "text": "In this section, we add some additional experiments on comparing to other baselines, i.e. PAC-Bayes bounds Germain et al. (2013). As shown in the first row of Figure 9, our robust framework has a smaller distribution distance in the bound compared to the two baselines when increasing the model size. In the second row, we have similar results in final generalization bounds. From the third and fourth rows we can see, our bound is tighter than baselines when suffering distributional shifts."
        }
    ],
    "title": "TOWARDS ROBUST OUT-OF-DISTRIBUTION GENERALIZATION BOUNDS VIA SHARPNESS",
    "year": 2024
}