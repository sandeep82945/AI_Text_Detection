{
    "abstractText": "Diffusion models are a powerful class of generative models which simulate stochastic differential equations (SDEs) to generate data from noise. While diffusion models have achieved remarkable progress, they have limitations in unpaired imageto-image (I2I) translation tasks due to the Gaussian prior assumption. Schr\u00f6dinger Bridge (SB), which learns an SDE to translate between two arbitrary distributions, have risen as an attractive solution to this problem. Yet, to our best knowledge, none of SB models so far have been successful at unpaired translation between high-resolution images. In this work, we propose Unpaired Neural Schr\u00f6dinger Bridge (UNSB), which expresses the SB problem as a sequence of adversarial learning problems. This allows us to incorporate advanced discriminators and regularization to learn a SB between unpaired data. We show that UNSB is scalable and successfully solves various unpaired I2I translation tasks. Code: https://github.com/cyclomon/UNSB",
    "authors": [
        {
            "affiliations": [],
            "name": "SCHR\u00d6DINGER BRIDGE"
        },
        {
            "affiliations": [],
            "name": "Beomsu Kim"
        },
        {
            "affiliations": [],
            "name": "Gihyun Kwon"
        },
        {
            "affiliations": [],
            "name": "Kwanyoung Kim"
        },
        {
            "affiliations": [],
            "name": "Jong Chul Ye"
        }
    ],
    "id": "SP:30431f398b12e08862fad5deb6a921c3eced6745",
    "references": [
        {
            "authors": [
                "Mohamed Ishmael Belghazi",
                "Aristide Baratin",
                "Sai Rajeswar",
                "Sherjil Ozair",
                "Yoshua Bengio",
                "Aaron Courville",
                "R Devon Hjelm"
            ],
            "title": "Mutual information neural estimation",
            "year": 2018
        },
        {
            "authors": [
                "Sagie Benaim",
                "Lior Wolf"
            ],
            "title": "One-sided unsupervised domain mapping",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Valentin De Bortoli",
                "James Thornton",
                "Jeremy Heng",
                "Arnaud Doucet"
            ],
            "title": "Diffusion schr\u00f6dinger bridge with applications to score-based generative modeling",
            "venue": "NeurIPS,",
            "year": 2021
        },
        {
            "authors": [
                "Charlotte Bunne",
                "Ya-Ping Hsieh",
                "Marco Cuturi",
                "Andreas Krause"
            ],
            "title": "The schr\u00f6dinger bridge between gaussian measures has a closed form",
            "venue": "In AISTATS,",
            "year": 2023
        },
        {
            "authors": [
                "Kenneth F Caluya",
                "Abhishek Halder"
            ],
            "title": "Reflected schr\u00f6dinger bridge: Density control with path constraints",
            "venue": "American Control Conference (ACC),",
            "year": 2021
        },
        {
            "authors": [
                "Runfa Chen",
                "Wenbing Huang",
                "Binghui Huang",
                "Fuchun Sun",
                "Bin Fang"
            ],
            "title": "Reusing discriminators for encoding: Towards unsupervised image-to-image translation",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Tianrong Chen",
                "Guan-Horng Liu",
                "Evangelos Theodorou"
            ],
            "title": "Likelihood training of schr\u00f6dinger bridge using forward-backward sdes theory",
            "venue": "In ICLR,",
            "year": 2022
        },
        {
            "authors": [
                "Yongxin Chen",
                "Tryphon T Georgiou",
                "Michele Pavon"
            ],
            "title": "Stochastic control liaisons: Richard sinkhorn meets gaspard monge on a schrodinger bridge",
            "venue": "Siam Review,",
            "year": 2021
        },
        {
            "authors": [
                "Yunjey Choi",
                "Youngjung Uh",
                "Jaejun Yoo",
                "Jung-Woo Ha"
            ],
            "title": "Stargan v2: Diverse image synthesis for multiple domains",
            "year": 2022
        },
        {
            "authors": [
                "Hyungjin Chung",
                "Jeongsol Kim",
                "Michael Thompson Mccann",
                "Marc Louis Klasky",
                "Jong Chul Ye"
            ],
            "title": "Diffusion posterior sampling for general noisy inverse problems",
            "venue": "In ICLR,",
            "year": 2023
        },
        {
            "authors": [
                "Mauricio Delbracio",
                "Peyman Milanfar"
            ],
            "title": "Inversion by direct iteration: An alternative to denoising diffusion for image restoration",
            "venue": "arxiv preprint arXiv:2303.11435,",
            "year": 2023
        },
        {
            "authors": [
                "Laurent Dinh",
                "David Krueger",
                "Yoshua Bengio"
            ],
            "title": "Nice: Non-linear independent components estimation",
            "venue": "arxiv preprint arXiv:1410.8516,",
            "year": 2015
        },
        {
            "authors": [
                "Huan Fu",
                "Mingming Gong",
                "Chaohui Wang",
                "Kayhan Batmanghelich",
                "Kun Zhang",
                "Dacheng Tao"
            ],
            "title": "Geometry-consistent generative adversarial networks for one-sided unsupervised domain mapping",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2019
        },
        {
            "authors": [
                "Ian Goodfellow",
                "Jean Pouget-Abadie",
                "Mehdi Mirza",
                "Bing Xu",
                "David Warde-Farley",
                "Sherjil Ozair",
                "Aaron Courville",
                "Yoshua Bengio"
            ],
            "title": "Generative adversarial nets",
            "venue": "NeurIPS,",
            "year": 2014
        },
        {
            "authors": [
                "Nikita Gushchin",
                "Alexander Kolesov",
                "Alexander Korotin",
                "Dmitry Vetrov",
                "Evgeny Burnaev"
            ],
            "title": "Entropic neural optimal transport via diffusion processes",
            "venue": "In NeurIPS,",
            "year": 2023
        },
        {
            "authors": [
                "Nikita Gushchin",
                "Alexander Kolesov",
                "Alexander Korotin",
                "Dmitry Vetrov",
                "Evgeny Burnaev"
            ],
            "title": "Entropic neural optimal transport via diffusion processes",
            "venue": "In NeurIPS,",
            "year": 2023
        },
        {
            "authors": [
                "Nikita Gushchin",
                "Alexander Kolesov",
                "Petr Mokrov",
                "Polina Karpikova",
                "Andrey Spiridonov",
                "Evgeny Burnaev",
                "Alexander Korotin"
            ],
            "title": "Building the bridge of schr\u00f6dinger: A continuous entropic optimal transport benchmark",
            "venue": "In NeurIPS Track on Datasets and Benchmarks,",
            "year": 2023
        },
        {
            "authors": [
                "Amir Hertz",
                "Ron Mokady",
                "Jay Tenenbaum",
                "Kfir Aberman",
                "Yael Pritch",
                "Daniel Cohen-Or"
            ],
            "title": "Promptto-prompt image editing with cross attention control",
            "venue": "arXiv preprint arXiv:2208.01626,",
            "year": 2022
        },
        {
            "authors": [
                "Martin Heusel",
                "Hubert Ramsauer",
                "Thomas Unterthiner",
                "Bernhard Nessler",
                "Sepp Hochreiter"
            ],
            "title": "Gans trained by a two time-scale update rule converge to a local nash equilibrium",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Jonathan Ho",
                "Ajay Jain",
                "Pieter Abbeel"
            ],
            "title": "Denoising diffusion probabilistic models",
            "venue": "In NeurIPS,",
            "year": 2020
        },
        {
            "authors": [
                "Xun Huang",
                "Ming-Yu Liu",
                "Serge Belongie",
                "Jan Kautz"
            ],
            "title": "Multimodal unsupervised image-to-image translation",
            "venue": "In Proceedings of the European Conference on Computer Vision (ECCV),",
            "year": 2018
        },
        {
            "authors": [
                "Phillip Isola",
                "Jun-Yan Zhu",
                "Tinghui Zhou",
                "Alexei A Efros"
            ],
            "title": "Image-to-image translation with conditional adversarial networks",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Chanyong Jung",
                "Gihyun Kwon",
                "Jong Chul Ye"
            ],
            "title": "Exploring patch-wise semantic relation for contrastive learning in image-to-image translation tasks",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2022
        },
        {
            "authors": [
                "Tero Karras",
                "Timo Aila",
                "Samuli Laine",
                "Jaakko Lehtinen"
            ],
            "title": "Progressive growing of GANs for improved quality, stability, and variation",
            "venue": "In ICLR,",
            "year": 2018
        },
        {
            "authors": [
                "Diederik P. Kingma",
                "Max Welling"
            ],
            "title": "Auto-encoding variational bayes",
            "venue": "In ICLR,",
            "year": 2014
        },
        {
            "authors": [
                "Alexander Korotin",
                "Daniil Selikhanovych",
                "Evgeny Burnaev"
            ],
            "title": "Neural optimal transport",
            "venue": "In ICLR,",
            "year": 2023
        },
        {
            "authors": [
                "Christian L\u00e9onard"
            ],
            "title": "A survey of the schr\u00f6dinger problem and some of its connections with optimal transport",
            "venue": "arXiv preprint arXiv:1308.0215,",
            "year": 2013
        },
        {
            "authors": [
                "Jae Hyun Lim",
                "Aaron Courville",
                "Christopher Pal",
                "Chin-Wei Huang"
            ],
            "title": "Ar-dae: Towards unbiased neural entropy estimation",
            "venue": "In ICML,",
            "year": 2020
        },
        {
            "authors": [
                "Guan-Horng Liu",
                "Tianrong Chen",
                "Oswin So",
                "Evangelos A Theodorou"
            ],
            "title": "Deep generalized schr\u00f6dinger bridge",
            "venue": "arXiv preprint arXiv:2209.09893,",
            "year": 2022
        },
        {
            "authors": [
                "Guan-Horng Liu",
                "Arash Vahdat",
                "De-An Huang",
                "Evangelos A. Theodorou",
                "Weili Nie",
                "Anima Anandkumar"
            ],
            "title": "I2sb: Image-to-image schr\u00f6dinger bridge",
            "venue": "arxiv preprint arXiv:2302.05872,",
            "year": 2023
        },
        {
            "authors": [
                "Chenlin Meng",
                "Yutong He",
                "Yang Song",
                "Jiaming Song",
                "Jiajun Wu",
                "Jun-Yan Zhu",
                "Stefano Ermon"
            ],
            "title": "Sdedit: Guided image synthesis and editing with stochastic differential equations",
            "venue": "In ICLR,",
            "year": 2022
        },
        {
            "authors": [
                "Taesung Park",
                "Alexei A. Efros",
                "Richard Zhang",
                "Jun-Yan Zhu"
            ],
            "title": "Contrastive learning for unpaired image-to-image translation",
            "venue": "Computer Vision \u2013 ECCV",
            "year": 2020
        },
        {
            "authors": [
                "Aram-Alexandre Pooladian",
                "Heli Ben-Hamu",
                "Carles Domingo-Enrich",
                "Brandon Amos",
                "Yaron Lipman",
                "Ricky T.Q. Chen"
            ],
            "title": "Multisample flow matching: Straightening flows with minibatch couplings",
            "venue": "In ICML,",
            "year": 2023
        },
        {
            "authors": [
                "Paolo Dai Pra"
            ],
            "title": "A stochastic control appraoch to reciprocal diffusion processes",
            "venue": "Applied Mathematics and Optimization,",
            "year": 1991
        },
        {
            "authors": [
                "Robin Rombach",
                "Andreas Blattmann",
                "Dominik Lorenz",
                "Patrick Esser",
                "Bj\u00f6rn Ommer"
            ],
            "title": "Highresolution image synthesis with latent diffusion models",
            "year": 2022
        },
        {
            "authors": [
                "Hamza Ruzayqat",
                "Alexandros Beskos",
                "Dan Crisan",
                "Ajay Jasra",
                "Nikolas Kantas"
            ],
            "title": "Unbiased estimation using a class of diffusion processes",
            "venue": "Journal of Computational Physics,",
            "year": 2023
        },
        {
            "authors": [
                "Antoine Salmona",
                "Agn\u00e8s Desolneux",
                "Julie Delon",
                "Valentin De Bortoli"
            ],
            "title": "Can push-forward generative models fit multimodal distributions",
            "venue": "In NeurIPS,",
            "year": 2022
        },
        {
            "authors": [
                "Yuyang Shi",
                "Valentin De Bortoli",
                "George Deligiannidis",
                "Arnaud Doucet"
            ],
            "title": "Conditional simulation using diffusion schr\u00f6dinger bridges",
            "venue": "In Uncertainty in Artificial Intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "Yuyang Shi",
                "Valentin De Bortoli",
                "Andrew Campbell",
                "Arnaud Doucet"
            ],
            "title": "Diffusion schr\u00f6dinger bridge matching",
            "venue": "In NeurIPS,",
            "year": 2023
        },
        {
            "authors": [
                "Jascha Sohl-Dickstein",
                "Eric A. Weiss",
                "Niru Maheswaranathan",
                "Surya Ganguli"
            ],
            "title": "Deep unsupervised learing using nonequilibrium thermodynamics",
            "venue": "In ICML,",
            "year": 2015
        },
        {
            "authors": [
                "Jiaming Song",
                "Chenlin Meng",
                "Stefano Ermon"
            ],
            "title": "Denoising diffusion implicit models",
            "venue": "In ICLR,",
            "year": 2021
        },
        {
            "authors": [
                "Yang Song",
                "Jascha Sohl-Dickstein",
                "Diederik P. Kingma",
                "Abhishek Kumar",
                "Stefano Ermon",
                "Ben Poole"
            ],
            "title": "Score-based generative modeling through stochastic differential equations",
            "venue": "In ICLR,",
            "year": 2021
        },
        {
            "authors": [
                "Xuan Su",
                "Jiaming Song",
                "Chenlin Meng",
                "Stefano Ermon"
            ],
            "title": "Dual diffusion implicit bridges for image-to-image translation",
            "venue": "In ICLR,",
            "year": 2023
        },
        {
            "authors": [
                "Ella Tamir",
                "Martin Trapp",
                "Arno Solin"
            ],
            "title": "Transport with support: Data-conditional diffusion bridges",
            "venue": "arXiv preprint arXiv:2301.13636,",
            "year": 2023
        },
        {
            "authors": [
                "James Thornton",
                "Michael Hutchinson",
                "Emile Mathieu",
                "Valentin De Bortoli",
                "Yee Whye Teh",
                "Arnaud Doucet"
            ],
            "title": "Riemannian diffusion schr\u00f6dinger bridge",
            "venue": "arXiv preprint arXiv:2207.03024,",
            "year": 2022
        },
        {
            "authors": [
                "Alexander Tong",
                "Nikolay Malkin",
                "Guillaume Huguet",
                "Yanlei Zhang",
                "Jarrid Rector-Brooks",
                "Kilian Fatras",
                "Guy Wolf",
                "Yoshua Bengio"
            ],
            "title": "Conditional flow matching: Simulation-free dynamic optimal transport",
            "venue": "arxiv preprint arXiv:2302.00482,",
            "year": 2023
        },
        {
            "authors": [
                "Francisco Vargas",
                "Pierre Thodoroff",
                "Austen Lamacraft",
                "Neil Lawrence"
            ],
            "title": "Solving schr\u00f6dinger bridges via maximum likelihood",
            "year": 2021
        },
        {
            "authors": [
                "Gefei Wang",
                "Yuling Jiao",
                "Qian Xu",
                "Yang Wang",
                "Can Yang"
            ],
            "title": "Deep generative learning via schr\u00f6dinger bridge",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Weilun Wang",
                "Wengang Zhou",
                "Jianmin Bao",
                "Dong Chen",
                "Houqiang Li"
            ],
            "title": "Instance-wise hard negative example generation for contrastive learning in unpaired image-to-image translation",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV),",
            "year": 2021
        },
        {
            "authors": [
                "Zhisheng Xiao",
                "Karsten Kreis",
                "Arash Vahdat"
            ],
            "title": "Tackling the generative learning trilemma with denoising diffusion gans",
            "venue": "In ICLR,",
            "year": 2022
        },
        {
            "authors": [
                "Qinsheng Zhang",
                "Yongxin Chen"
            ],
            "title": "Path integral sampler: a stochastic control approach for sampling",
            "venue": "arXiv preprint arXiv:2111.15141,",
            "year": 2021
        },
        {
            "authors": [
                "Min Zhao",
                "Fan Bao",
                "Chongxuan Li",
                "Jun Zhu"
            ],
            "title": "Egsde: Unpaired image-to-image translation via energy-guided stochastic differential equations",
            "venue": "In NeurIPS,",
            "year": 2022
        },
        {
            "authors": [
                "Chuanxia Zheng",
                "Tat-Jen Cham",
                "Jianfei Cai"
            ],
            "title": "The spatially-correlative loss for various image translation tasks",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2021
        },
        {
            "authors": [
                "Jun-Yan Zhu",
                "Taesung Park",
                "Phillip Isola",
                "Alexei A. Efros"
            ],
            "title": "Unpaired image-to-image translation using cycle-consistent adversarial networks",
            "venue": "In Proceedings of the IEEE International Conference on Computer Vision (ICCV),",
            "year": 2017
        },
        {
            "authors": [
                "DDGAN Xiao"
            ],
            "title": "For I2I tasks, we used CUT loss as regularization. On Summer2Winter translation task, we used a pre-trained VGG16 network as our feature selection source, following the strategy in the previous work by Zheng et al",
            "year": 2022
        },
        {
            "authors": [
                "Belghazi"
            ],
            "title": "2020) provide methods to estimate mutual information of two random variables by optimizing a neural network",
            "year": 2018
        },
        {
            "authors": [
                "i.e",
                "x-samples"
            ],
            "title": "For comparison, we provide cFID numbers for SB p(x|y) learned by ENOT (Gushchin et al., 2023b) and unconditional p(x|y) = p(x) where we use 5k ground-truth samples from p(x)",
            "venue": "cFID",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020; Song et al., 2021a;b), a class of generative model which generate data by simulating stochastic differential equations (SDEs), have achieved remarkable progress over the past few years. They are capable of diverse and high-quality sample synthesis (Xiao et al., 2022), a desiderata which was difficult to obtain for several widely acknowledged models such as Generative Adversarial Networks (GANs) (Goodfellow et al., 2014), Variational Autoencoders (VAEs) (Kingma & Welling, 2014), and flow-based models (Dinh et al., 2015). Furthermore, the iterative nature of diffusion models proved to be useful for a variety of downstream tasks, e.g., image restoration (Chung et al., 2023) and conditional generation (Rombach et al., 2022).\nHowever, unlike GANs and their family which are free to choose any prior distribution, diffusion models often assume a simple prior, such as the Gaussian distribution. In other words, the initial condition for diffusion SDEs is generally fixed to be Gaussian noise. The Gaussian assumption prevents diffusion models from unlocking their full potential in unpaired image-to-image translation tasks such as domain transfer, style transfer, or unpaired image restoration.\nSchr\u00f6dinger bridges (SBs), a subset of SDEs, present a promising solution to this issue. They solve the entropy-regularized optimal transport (OT) problem, enabling translation between two arbitrary distributions. Their flexibility have motivated several approaches to solving SBs via deep learning. For instance, Bortoli et al. (2021) extends the Iterative Proportional Fitting (IPF) procedure to the continuous setting, Chen et al. (2022) proposes likelihood training of SBs using Forward-Backward SDEs theory, and Tong et al. (2023) solves the SB problem with a conditional variant of flow matching.\nSome methods have solved SBs assuming one side of the two distributions is simple. Wang et al. (2021a) developed a two-stage unsupervised procedure for estimating the SB between a Dirac delta and data. I2SB (Liu et al., 2023) and InDI (Delbracio & Milanfar, 2023) uses paired data to learn SBs between Dirac delta and data. Su et al. (2023) discovered DDIMs (Song et al., 2021a) as SBs between\ndata and Gaussian distributions. Hence, they were able to perform image-to-image translation by concatenating two DDIMs, i.e., by passing through an intermediate Gaussian distribution.\nYet, to the best of our knowledge, no work so far has successfully trained SBs for direct translation between high-resolution images in the unpaired setting. Most methods demand excessive computation, and even if it is not the case, we observe poor results. In fact, all representative SB methods fail even on the simple task of translating points between two concentric spheres as dimension increases.\nIn this work, we first identify the main culprit behind the failure of SB for unpaired image-to-image translation as the curse of dimensionality. As the dimension of the considered data increases, the samples become increasingly sparse, failing to capture the shape of the underlying image manifold. This sampling error then biases the SB optimal transport map, leading to an inaccurate SB.\nBased on the self-similarity of SB, referring to the intriguing property that SB restricted to a subinterval of its time domain also a SB, we therefore propose the Unpaired Neural Schr\u00f6dinger Bridge (UNSB), which formulates the SB problem as a sequence of transport cost minimization problems under the constraint on the KL divergence between the true target distribution and the model distribution. We show that its Lagrangian formulation naturally expresses the SB a composition of generators learned via adversarial learning (Goodfellow et al., 2014). One of the important advantages of the UNSB formulation of SB is that it allows us to mitigate the curse of dimensionality on two levels: we can extend the discriminator in adversarial learning to a more advanced one, and we can add regularization to enforce the generator to learn a mapping which aligns with our inductive bias. Furthermore, all components of UNSB are scalable, so UNSB is naturally scalable as well to large scale image translation problems.\nExperiments on toy and practical image-to-image translation tasks demonstrate that UNSB opens up a new research direction for applying diffusion models to large scale unpaired translation tasks. Our contributions can be summarized as follows.\n\u2022 We identify the cause behind the failure of previous SB methods for image-to-image translation as the curse of dimensionality. We empirically verify this by using a toy task as a sanity check for whether an OT-based method is robust to the curse of dimensionality.\n\u2022 We propose UNSB, which formulates SB as Lagrangian formulation under the constraint on the KL divergence between the true target distribution and the model distribution. This leads to the composition of generators learned via adversarial learning that overcome the curse of dimensionality with advanced discriminators.\n\u2022 UNSB improves upon the Denoising Diffusion GAN (Xiao et al., 2022) by enabling translation between arbitrary distributions. Furthermore, based on the comparison with other existing unpaired translation methods, we demonstrate that UNSB is indeed a generalization of them by overcoming their shortcomings."
        },
        {
            "heading": "2 RELATED WORKS",
            "text": "Schr\u00f6dinger Bridges. The Schr\u00f6dinger bridge (SB) problem, commonly referred to as the entropyregularized Optimal Transport (OT) problem (Schr\u00f6dinger, 1932; L\u00e9onard, 2013), is the task of learning a stochastic process that transitions from an initial probability distribution to a terminal distribution over time, while subject to a reference measure. It is closely connected to the field of stochastic control problems (Caluya & Halder, 2021; Chen et al., 2021). Recently, the remarkable characteristic of the SB problem, which allows for the choice of arbitrary distributions as the initial and terminal distributions, has facilitated solutions to various generative modeling problems. In particular, novel algorithms leveraging Iterative Proportional Fitting (IPF) (Bortoli et al., 2021; Vargas et al., 2021) have been proposed to approximate score-based diffusion. Building upon these algorithms, several variants have been introduced and successfully applied in diverse fields such as, inverse problems (Shi et al., 2022), Mean-Field Games (Liu et al., 2022), constrained transport problems (Tamir et al., 2023), Riemannian manifolds (Thornton et al., 2022), and path samplers (Zhang & Chen, 2021; Ruzayqat et al., 2023). Some methods have solved SBs assuming one side of the two distributions is simple. In the unsupervised setting, Wang et al. (2021a) first learns a SB between a Dirac delta and noisy data, and then denoises the noisy samples. In the supervised setting, I2SB (Liu et al., 2023) and InDI (Delbracio & Milanfar, 2023) used paired data to learn SBs\nbetween Dirac delta and data. DDIB (Su et al., 2023) concatenates two SBs between data and the Gaussian distribution for image-to-image translation. Recent works (Gushchin et al., 2023a; Shi et al., 2023) have achieved unpaired image-to-image translation for \u2264 128\u00d7 128 resolution images. But, they are computationally intensive, often taking several days to train. To our best knowledge, our work represents the first endeavor to efficiently learn SBs between higher resolution unpaired images.\nUnpaired image-to-image translation. The aim of image-to-image translation (I2I) is to produce an image in the target domain that preserves the structural similarity to the source image. A seminal work in I2I is pix2pix (Isola et al., 2017), which undertook the task with paired training images, utilizing a straightforward pixel-wise regularization strategy. However, this approach is not applicable when dealing with unpaired training settings. In the context of unpaired data settings, early methods like (Zhu et al., 2017; Huang et al., 2018) maintained the consistency between the source and output images through a two-sided training strategy, namely cycle-consistency. However, these models are plagued by inefficiencies in training due to the necessity of additional generator training. In order to circumvent this issue, recent I2I models have shifted their focus to one-sided I2I translation. They aim to preserve the correspondence between the input and output through various strategies, such as geometric consistency (Fu et al., 2019) and mutual information regularization (Benaim & Wolf, 2017). Recently, Contrastive Unpaired Translation (CUT) and its variants (Park et al., 2020; Jung et al., 2022; Wang et al., 2021b; Zheng et al., 2021) have demonstrated improvements in I2I tasks by refining patch-wise regularization strategies. Despite the impressive performance demonstrated by previous GAN-based models in the I2I domain, this paper demonstrates that our SB-based approach paves the way for further improvements in the I2I task by introducing the iterative refinement through SB that overcomes the potential mode collapsing issue from a single-step generator from GAN. We believe that it represents a new paradigm in the ongoing advancement of image-to-image translation."
        },
        {
            "heading": "3 SCHR\u00d6DINGER BRIDGES AND THE CURSE OF DIMENSIONALITY",
            "text": "Given two distributions \u03c00, \u03c01 on Rd, the Schr\u00f6dinger Bridge problem (SBP) seeks the most likely random process {xt : t \u2208 [0, 1]} that interpolates \u03c00 and \u03c01. Specifically, let \u2126 be the path space on Rd, i.e., the space of continuous functions from [0, 1] to Rd, and let P(\u2126) be the space of probability measures on \u2126. Then, the SBP solves\nQSB = argmin Q\u2208P(\u2126) DKL(Q\u2225W\u03c4 ) s.t. Q0 = \u03c00, Q1 = \u03c01 (1)\nwhere W\u03c4 is the Wiener measure with variance \u03c4 , and Qt denotes the marginal of Q at time t. We call QSB the Schr\u00f6dinger Bridge (SB) between \u03c00 and \u03c01.\nThe SBP admits multiple alternative formulations, and among those, the stochastic control formulation and the static formulation play crucial roles in our work. Both formulations will serve as theoretical bases for our Unpaired Neural Schr\u00f6dinger Bridge algorithm, and the static formulation will shed light on why previous SB methods have failed on unpaired image-to-image translation tasks.\nStochastic control formulation. The stochastic control formulation (Pra, 1991) shows that {xt} \u223c QSB can be described by an It\u00f4 SDE\ndxt = u SB t dt+ \u221a \u03c4 dwt (2)\nwhere the time-varying drift uSBt is a solution to the stochastic control problem\nuSBt = argmin u\nE [\u222b 1\n0\n1 2 \u2225ut\u22252 dt\n] s.t. { dxt = ut dt+ \u221a \u03c4 dwt\nx0 \u223c \u03c00, x1 \u223c \u03c01 (3)\nassuming u satisfies certain regularity conditions. Eq. (3) says that among the SDEs of the form Eq. (2) with boundaries \u03c00 and \u03c01, the drift for the SDE describing the SB has minimum energy. This formulation also reveals two useful properties of SBs. First, {xt} is a Markov chain, and second, {xt} converges to the optimal transport ODE trajectory as \u03c4 \u2192 0. Intuitively, \u03c4 controls the amount of randomness in the trajectory {xt}. Static formulation. The static formulation of SBP shows that sampling from QSB is extremely simple, assuming we know the joint distribution of the SB at t \u2208 {0, 1}, denoted as QSB01 . Concretely, for {xt} \u223c QSB, conditioned upon initial and terminal points x0 and x1, the density of xt can be described by a Gaussian density (Tong et al., 2023)\np(xt|x0,x1) = N (xt|tx1 + (1\u2212 t)x0, t(1\u2212 t)\u03c4I). (4) Hence, to simulate the SB given an initial point x0, we may sample xt according to\np(xt|x0) = \u222b p(xt|x0,x1) dQSB1|0(x1|x0) (5)\nwhere QSB1|0 denotes the conditional distribution of x1 given x0. Surprisingly, Q SB 01 is shown to be a solution to the entropy-regularized optimal transport problem\nQSB01 = argmin \u03b3\u2208\u03a0(\u03c00,\u03c01) E(x0,x1)\u223c\u03b3 [\u2225x0 \u2212 x1\u2225 2]\u2212 2\u03c4H(\u03b3) (6)\nwhere \u03a0(\u03c00, \u03c01) is the collection of joint distributions whose marginals are \u03c00 and \u03c01, and H denotes the entropy function. For discrete \u03c00 and \u03c01, it is possible to find QSB01 via the Sinkhorn-Knopp algorithm, and this observation has inspired several algorithms (Tong et al., 2023; Pooladian et al., 2023) for approximating the SB.\nCurse of dimensionality. According to the static formulation of the SBP, any algorithm for solving the SBP can be interpreted as learning an interpolation according to the entropy-regularized optimal transport Eq. (6) of the marginals \u03c00 and \u03c01. However, in practice, we only have a finite number of samples {xn0}Nn=1 \u223c \u03c00 and {xm1 }Mm=1 \u223c \u03c01. This means, the SB is trained to transport samples between the empirical distributions 1 N \u2211N n=1 \u03b4xn0 and 1 M \u2211M m=1 \u03b4xm1 . Due to the curse of dimensionality, the samples fail to describe the image manifolds correctly in high dimension. Ultimately, QSB01 yield image pairs that do not meaningfully correspond to one another (see Figure 1 or the result for Neural Optimal Transport (NOT) in Figure 5).\nTo illustrate this phenomenon in the simplest scenario, we consider the case where \u03c00 and \u03c01 are supported uniformly on two concentric d-spheres of radii 1 and 2, respectively. Then, samples from QSB01 should have near-one cosine similarity, since QSB should transport samples from \u03c00 radially outwards. However, when we draw M = N = 1000 i.i.d. samples from \u03c00 and \u03c01, and calculate the cosine similarity between (x0,x1) \u223c QSB01 where QSB01 is estimated using the Sinkhorn-Knopp algorithm, we observe decreasing similarity as dimension increases (see Figure 2). Thus, in a high dimension, QSB approximated by Sinkhorn-Knopp will interpolate between nearly orthogonal points."
        },
        {
            "heading": "4 UNPAIRED NEURAL SCHR\u00d6DINGER BRIDGE (UNSB)",
            "text": "We now explain our novel UNSB algorithm which shows that SB can be expressed as a composition of generators learned via adversarial learning.\nSpecifically, given a partition {ti}Ni=0 of the unit interval [0, 1] such that t0 = 0, tN = 1, and ti < ti+1, we can simulate SB according to the Markov chain decomposition\np({xtn}) = p(xtN |xtN\u22121)p(xtN\u22121 |xtN\u22122) \u00b7 \u00b7 \u00b7 p(xt1 |xt0)p(xt0). (7)\nThe decomposition Eq. (7) allows us to learn the SB inductively: we learn p(xti+1 |xti) assuming we are able to sample from p(xti). Having approximated p(xti+1 |xti), we can sample from p(xti+1), so we may learn p(xti+2 |xti+1). Since the distribution of xt0 = x0 is already known as \u03c00, our main interest lies in learning the transition probabilities p(xti+1 |xti) assuming we can sample from p(xti). This procedure is applied recursively for i = 0, . . . , N \u2212 1. Let q\u03d5i(x1|xti) be a conditional distribution parametrized by a DNN with parameter \u03d5i. Intuitively, q\u03d5i(x1|xti) is a generator which predicts the target domain image for xti . We define\nq\u03d5i(xti ,x1) := q\u03d5i(x1|xti)p(xti), q\u03d5i(x1) := Ep(xti )[q\u03d5i(x1|xti)]. (8)\nThe following theorem shows how to optimize \u03d5i and sample from p(xti+1 |xti). Theorem 1. For any ti, consider the following constrained optimization problem\nmin \u03d5i\nLSB(\u03d5i, ti) := Eq\u03d5i (xti ,x1)[\u2225xti \u2212 x1\u2225 2]\u2212 2\u03c4(1\u2212 ti)H(q\u03d5i(xti ,x1)) (9)\ns.t. LAdv(\u03d5i, ti) := DKL(q\u03d5i(x1)\u2225p(x1)) = 0 (10)\nand define the distributions\np(xti+1 |x1,xti) := N (xti+1 |si+1x1 + (1\u2212 si+1)xti , si+1(1\u2212 si+1)\u03c4(1\u2212 ti)I) (11)\nwhere si+1 := (ti+1 \u2212 ti)/(1\u2212 ti) and\nq\u03d5i(xti+1 |xti) := Eq\u03d5i (x1|xti )[p(xti+1 |x1,xti)], q\u03d5i(xti+1) := Ep(xti )[q\u03d5i(xti+1 |xti)]. (12)\nIf \u03d5i solves Eq. (9), then we have\nq\u03d5i(x1|xti) = p(x1|xti), q\u03d5i(xti+1 |xti) = p(xti+1 |xti), q\u03d5i(xti+1) = p(xti+1). (13)\nProof Sketch. Using the stochastic control formulation of SB, we show that the SB satisfies a certain self-similarity property, which states that the SB restricted any sub-interval of [0, 1] is also a SB. Note that Eqs. (11), (12) and (9) under the constraint Eq. (10) are indeed counter-parts of the static formulation Eqs. (4), (5), and (6), respectively, for the restricted domain [ti, 1]. The static formulation of SB restricted to the interval [ti, 1] shows that q\u03d5i(x1|xti) = p(x1|xti) is the solution to Eq. (9). If we assume \u03d5i solves Eq. (9) such that q\u03d5i(x1|xti) = p(x1|xti), Eq. (13) follows by simple calculation. The detailed proof can be found in the Appendix.\nIn practice, by incorporating the equality constraint in Eq. (10) into the loss with a Lagrange multiplier, we obtain the UNSB objective for a single time-step ti\nmin \u03d5i LUNSB(\u03d5i, ti) := LAdv(\u03d5i, ti) + \u03bbSB,tiLSB(\u03d5i, ti). (14)\nSince it is impractical to use separate parameters \u03d5i for each time-step ti and to learn p(xti+1 |xti) sequentially for i = 0, . . . , N \u2212 1, we replace q\u03d5i(x1|xti), which takes xti as input, with a timeconditional DNN q\u03d5(x1|xti), which shares a parameter \u03d5 for all time-steps ti, and takes the tuple (xti , ti) as input. Then, we optimize the sum of LUNSB(\u03d5, ti) over i = 0, . . . , N \u2212 1. Training. For the training of our UNSB, we first randomly choose a time-step ti to optimize. To calculate LUNSB(\u03d5, ti), we sample xti and x1 \u223c \u03c01, where \u03c01 denotes the target distribution. The sampling procedure of xti will be described soon. The sample xti is then passed through q\u03d5(x1|xti) to obtain x1(xti) that refers to the estimated target data sample given xti . The pairs (xti ,x1(xti)) and (x1,x1(xti)) are then used to compute LSB(\u03d5, ti) and LAdv(\u03d5, ti) in Eq. (9) and Eq. (10), respectively. Specifically, we estimate the entropy term in LSB with a mutual information estimator,\nusing the fact that for a random variable X , I(X,X) = H(X) where I denotes mutual information. We then estimate the divergence in LAdv with adversarial learning. Intuitively, x1 and x1(xti) are \u201creal\u201d and \u201cfake\u201d inputs to the discriminator. This process is shown in the training stage of Figure 3.\nGeneration of intermediate and final samples. We now describe the sampling procedure for the intermediate samples for training and inference. We simulate the Markov chain Eq. (7) using q\u03d5 as follows: given xtj \u223c q\u03d5(xtj ) (note that xtj = x0 \u223c \u03c00 if j = 0), we predict the target domain image x1(xtj ) \u223c q\u03d5(x1|xtj ). We then sample xtj+1 \u223c q\u03d5(xtj+1) according to Eq. (11) by interpolating x0 and x1(xtj ) and adding Gaussian noise. Repeating this procedure for j = 0 . . . , i\u2212 1, we get xti \u223c q\u03d5(xti). With optimal \u03d5, by Theorem 1, xti \u223c q\u03d5(xti) = p(xti). Thus, the trajectory {x1(xti) : i = 0, . . . , N \u2212 1} can be viewed as an iterative refinement of the predicted target domain sample. This process is illustrated within the generation stage of Figure 3."
        },
        {
            "heading": "4.1 COMBATING THE CURSE OF DIMENSIONALITY",
            "text": "Advanced discriminator in adversarial learning. One of the most important advantages of the formulation Eq. (14) is that we can replace the KL-divergence in LAdv by any divergence or metric which measures discrepancy between two distributions. Such divergence or metric can be estimated through adversarial learning, i.e., its Kantorovich dual. For example,\n2 \u00b7DJSD(q\u03d5i(x1)\u2225p(x1))\u2212 log(4) = max D Ep(x1)[logD(x1)] + Eq\u03d5i (x1)[log(1\u2212D(x1))] (15)\nwhere D is a discriminator. This allows us to use various adversarial learning techniques to mitigate the curse of dimensionality. For instance, instead of using a standard discriminator which distinguishes generated and real samples on the instance level, we can use a Markovian discriminator which distinguishes samples on the patch level. The Markovian discriminator is effective at capturing high-frequency characteristics (e.g., style) of the target domain data (Isola et al., 2017).\nRegularization. Furthermore, we augment the UNSB objective with regularization, which enforces the generator network q\u03d5 to satisfy consistency between predicted x1 and the initial point x0:\nLReg(\u03d5, ti) := Ep(x0,xti )Eq\u03d5(x1|xti )[R(x0,x1)] (16)\nHere, R is a scalar-valued differentiable function which quantifies an application-specific measure of similarity between its inputs. In other words, R reflects our inductive bias for similarity between two images. Thus, the regularized UNSB objective for time ti is\nLUNSB(\u03d5, ti) := LAdv(\u03d5, ti) + \u03bbSB,tiLSB(\u03d5, ti) + \u03bbReg,tiLReg(\u03d5, ti), (17) which is the final objective in our UNSB algorithm."
        },
        {
            "heading": "4.2 SANITY CHECK ON TOY DATA",
            "text": "Two shells. With the two shells data of Section 3, we show that representative SB methods suffer from the curse of dimensionality, whereas UNSB does not, given appropriate discriminator and regularization. For SB methods, we consider Sinkhorn-Knopp (SK), SB Conditional Flow Matching (SBCFM)\n(Tong et al., 2023), Diffusion SB (DSB) (Bortoli et al., 2021), and SB-FBSDE (Chen et al., 2022). For baselines, we use recommended settings, and for UNSB, we train the discriminator to distinguish real and fake samples by input norms, and choose negative cosine similarity as R in Eq. (16).\nOnly 1k samples from each \u03c00, \u03c01 are used throughout the training. In Figure 4, we see all baselines either fail to transport samples to the target data manifold or fail to maintain high cosine similarity between input and outputs. On the other hand, UNSB is robust to dimension.\nTwo Gaussians. We also verify whether UNSB can learn the SB between two Gaussians. The SB between Gaussian distributions is also Gaussian, and its mean and covariance can be derived in a closed form (Bunne et al., 2023). Thus, we may measure the error between approximated and true solutions exactly. We consider learning the SB between N (\u22121, I) and N (1, I) in a 50-dimensional space. In Table 1, we see that UNSB recovers the mean and covariance of the ground-truth SB relatively accurately."
        },
        {
            "heading": "4.3 RELATION TO OTHER UNPAIRED TRANSLATION METHODS",
            "text": "UNSB vs. GANs. N = 1 version of UNSB is nearly equivalent to GAN-based translation methods such as CUT, i.e., UNSB may be interpreted as a multi-step generalization of GAN methods. Multi-step models are able to fit complex mappings that GANs are unable to by breaking down complex maps into a composition of simple maps. That is why diffusion models often achieve better performance than GANs (Salmona et al., 2022). Similarly, we believe the multi-step nature of UNSB arising from SB allow it to obtain better performance than GAN-based translation methods.\nUNSB vs. previous diffusion methods. Previous diffusion-based methods such as SDEdit (Meng et al., 2022) also translate images along SDEs. However, the SDEs used by such methods (e.g., VP-SDE or VE-SDE) may be suboptimal in terms of transport cost, resulting in large NFEs. On the other hand, SB allows fast generation, as it learns the shortest path between domains. Furthermore, UNSB uses adversarial learning, so it may generate feasible samples even with NFE = 1. On the contrary, diffusion methods do not perform so well on complex tasks such as Horse2Zebra."
        },
        {
            "heading": "5 RESULTS FOR UNPAIRED IMAGE-TO-IMAGE TRANSLATION",
            "text": "In this section, we provide experimental results of UNSB for large scale unpaired image-to-image translation tasks.\nUNSB settings. We use the Markovian discriminator in LAdv and the patch-wise contrastive matching loss (Park et al., 2020) in LReg. We set the number of timesteps as N = 5, so we discretize the unit interval into t0, t1, . . . , t5. We set \u03bbSB,ti = \u03bbReg,ti = 1 and \u03c4 = 0.01. We measure the sample quality of x1(xti) for each i = 0, . . . , N\u22121. So, NFE = i denotes sample evaluation or visualization of x1(xti\u22121). Other details are deferred to the Appendix.\nEvaluation. We use four benchmark datasets: Horse2Zebra, Map2Cityscape, Summer2Winter, and Map2Satellite. All images are resized into 256\u00d7256. We use the FID score (Heusel et al., 2017) and the KID score (Chen et al., 2020) to measure sample quality.\nBaselines. We note all SB methods SBCFM, DSB, and SB-FBSDE do not provide results on \u2265 256 resolution images in their paper due to scalability issues. While we were able to make SBCFM work on 256 resolution images, the results were poor, so we deferred its results to the Appendix. Instead, we used Neural Optimal Transport (NOT) (Korotin et al., 2023) as a representative for OT. For GAN methods, we used CycleGAN (Zhu et al., 2017), MUNIT (Huang et al., 2018), DistanceGAN (Benaim & Wolf, 2017), GcGAN (Fu et al., 2019), and CUT (Park et al., 2020). For diffusion, we used SDEdit (Meng et al., 2022) and P2P (Hertz et al., 2022) with LDM (Rombach et al., 2022)."
        },
        {
            "heading": "Method NFE Time Horse2Zebra Summer2Winter Label2Cityscape Map2Satellite",
            "text": "Since LDM is trained on natural images, we can use LDM on Horse2Zebra and Summer2Winter, but not on Label2Cityscape and Map2Satellite. Hence, we omit diffusion results on those two tasks.\nComparison results. We show quantitative results in Table 2, where we observe our model outperforms baseline methods in all datasets. In particular, out model largely outperformed early GAN-based translation methods. When compared to recent models such as CUT, our model still shows better scores. NOT suffers from degraded performance in all of the datasets.\nQualitative comparison in Figure 5 provides insight into the superior performance of UNSB. Our model successfully translates source images to the target domain while preventing structural inconsis-\ntency. For other GAN-based methods, the model outputs do not properly reflect the target domain information, and some models fail to preserve source image structure. For NOT, we observe that the model failed to overcome the curse of dimensionality. Specifically, NOT hallucinates structures not present in the source image (for instance, see the first row of Figure 5), leading to poor samples. For diffusion-based methods, we see they fail to fully reflect the target domain style.\nNFE analysis. We investigate the relationship between NFE and the translated sample quality. In Figure 6 top, we observe that using NFE = 1 resulted in relatively poor performance, sometimes even worse than existing one-step GAN methods. However, we see that increasing the NFE consistently improved generation quality: best FIDs were achieved for NFE values between 3 and 5 for all datasets. This trend is also evident in our qualitative comparisons. Looking at Figure 1, we observe that as NFE increases, there are gradual improvements in image quality. However, UNSB at times suffers from the problem where artifacts occur with large NFE, as shown in Figure 6 bottom. We speculate this causes increasing FID for large NFEs on datasets such as Map2Satellite.\nAblation study. Unlike GAN-based methods, UNSB is able to generate samples in multiple steps. Also unlike SB methods, UNSB is able to use advanced discriminators and regularization. To understand the influence of those factors on the final performance, we performed an ablation study on the Horse2Zebra dataset. We report the results in Table 3. In the case where we use neither advanced discriminator nor regularization (corresponding to the previous SB method SBCFM), we see poor results. Then, as we add multistep generation, advanced discriminator (Markovian discriminator, denoted \u201cPatch\u201d), and regularization, the result gradually improves until we obtain the best performance given by UNSB. This implies that the three components of UNSB play orthogonal roles.\nStochasticity analysis. SB is stochastic and should return diverse outputs. In Figure 7, we see meaningful variation in the generated images. Hence, UNSB indeed learns a stochastic map.\nTransport cost analysis. SB solves OT, so inputs and outputs must be close. In Figure 8, we compare input-output distance for UNSB pairs and SK pairs computed between actual dataset images. we observe\nthat input-output distance for UNSB is smaller than those of SK. This implies UNSB successfully generalizes beyond observed points to generate pairs which are close under L2 norm and are realistic."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "In this work, we proposed Unpaired Neural Schr\u00f6dinger Bridge (UNSB) which solves the Schr\u00f6dinger Bridge problem (SBP) via adversarial learning. UNSB formulation of SB allowed us to combine SB with GAN training techniques for unpaired image-to-image translation. We demonstrated the scalability and effectiveness of UNSB through various data-to-data or image-to-image translation tasks. In particular, while all previous methods for SB or OT fail, UNSB achieved results that often surpass those of one-step models. Overall, our work opens up a previously unexplored research direction for applying diffusion models to unpaired image translation."
        },
        {
            "heading": "ETHICS AND REPRODUCIBILITY STATEMENTS",
            "text": "Ethics statement. UNSB extends diffusion to translation between two arbitrary distributions, allowing us to explore a wider range of applications. In particular, UNSB may be used in areas with beneficial impacts, such as medical image restoration. However, UNSB may also be used to create malicious content such as fake news, and this must be prevented through proper regulation.\nReproducibility statement. Pseudo-codes and hyper-parameters are described in the main paper and the Appendix."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "This research was supported by the National Research Foundation of Korea (NRF) (RS202300262527), Field-oriented Technology Development Project for Customs Administration funded by the Korean government (the Ministry of Science & ICT and the Korea Customs Service) through the National Research Foundation (NRF) of Korea under Grant NRF2021M3I1A1097910 & NRF2021M3I1A1097938, Korea Medical Device Development Fund grant funded by the Korea government (the Ministry of Science and ICT, the Ministry of Trade, Industry, and Energy, the Ministry of Health & Welfare, the Ministry of Food and Drug Safety) (Project Number: 1711137899, KMDF PR 20200901 0015), and Culture, Sports, and Tourism R&D Program through the Korea Creative Content Agency grant funded by the Ministry of Culture, Sports and Tourism in 2023."
        },
        {
            "heading": "A PROOFS",
            "text": "Lemma 1 (Self-similarity). Let [ta, tb] \u2286 [0, 1] and {xt} \u223c QSB. The SB restricted to [ta, tb], defined as the distribution of {xt}|[ta,tb] := {xt(s) : s \u2208 [0, 1]} where t(s) := ta + (tb \u2212 ta)s solves\nmin Q\u2208P(\u2126)\nDKL(Q\u2225W\u03c4(tb\u2212ta)) s.t. Q0 = QSBta , Q1 = Q SB tb . (18)\nProof. We claim uSBt restricted to the interval [ta, tb] is also a SB from QSBta to Q SB tb . Suppose the claim is false, so there is another drift u\u0302t on [ta, tb] such that\nE [\u222b tb\nta\n\u2225u\u0302t\u22252 dt ] < E [\u222b tb ta \u2225uSBt \u22252 dt ] and { dxt = u\u0302t dt+ \u221a \u03c4 dwt, xta \u223c QSBta , xtb \u223c Q SB tb .\nWe can extend u\u0302t to the entire interval [0, 1] by defining\nu\u0302t =  uSBt if 0 \u2264 t < ta, u\u0302t if ta \u2264 t < tb, uSBt if tb \u2264 t < 1.\nWe then have E [\u222b 1\n0\n\u2225u\u0302t\u22252 dt ] < E [\u222b 1 0 \u2225uSBt \u22252 dt ] and { dxt = u\u0302t dt+ \u221a \u03c4 dwt, x0 \u223c \u03c00, x1 \u223c \u03c01,\nwhich contradicts our assumption that uSBt solves Eq. (3). We note that by change of time variable,\ndxt = u SB t dt+ \u221a \u03c4 dwt, ta \u2264 t \u2264 tb\nis equivalent to the SDE dxs = (tb \u2212 ta)uSBt(s) ds+ \u221a \u03c4(tb \u2212 ta) dws, 0 \u2264 s \u2264 1\nBy comparing the stochastic control formulation Eq. (3) with the original SBP Eq. (1), we see that this means the reference Wiener measure for the SBP restricted to [ta, tb] has variance \u03c4(tb\u2212 ta).\nLemma 2 (Static formulation of restricted SBs). Let t \u2208 [ta, tb] \u2286 [0, 1] and {xt} \u223c QSB. Then\np(xt|xta ,xtb) = N (xt|s(t)xtb + (1\u2212 s(t))xta , s(t)(1\u2212 s(t))\u03c4(tb \u2212 ta)I) (19)\nwhere s(t) := (t\u2212 ta)/(tb \u2212 ta) is the inverse function of t(s). Moreover,\nQSBtatb = argmin \u03b3\u2208\u03a0(Qta ,Qtb ) E(xta ,xtb )\u223c\u03b3 [\u2225xta \u2212 xtb\u2225 2]\u2212 2\u03c4(tb \u2212 ta)H(\u03b3). (20)\nProof. Translate the SBP Eq. (18) into the static formulation, taking into account the reduced variance \u03c4(tb \u2212 ta) of the reference Weiner measure W\u03c4(tb\u2212ta).\nProof of Theorem 1. Let ta = ti and tb = 1 in Lemma 2. If the class of distributions expressed by q\u03d5i(x1|xti) is sufficiently large, for any \u03b3 \u2208 \u03a0(Qti ,Q1),\nd\u03b3(xti ,x1) = q\u03d5i(xti ,x1) (21)\nfor some \u03d5i which satisfies\nDKL(q\u03d5i(x1)\u2225p(x1)) = 0. (22)\nThus, Eq. (20) and Eq. (9) under the constraint Eq. (10) are equivalent optimization problems, which yield the same solutions, namely p(xti ,x1). Then, with optimal \u03d5i,\nq\u03d5i(x1|xti)p(xti) = q\u03d5i(xti ,x1) = p(xti ,x1). (23)\nTogether with the observation that Eq. (11) is identical to Eq. (19), Eq. (23) implies Eq. (13)."
        },
        {
            "heading": "B OMITTED EXPERIMENT DETAILS",
            "text": "Training. All experiments are conducted on a single RTX3090 GPU. On each dataset, we train our UNSB network for 400 epochs with batch size 1 and Adam optimizer with \u03b21 = 0.5, \u03b22 = 0.999, and initial learning rate 0.0002. Learning rate is decayed linearly to zero until the end of training. All images are resized into 256\u00d7 256 and normalized into range [\u22121, 1]. For SB training and simulation, we discretize the unit interval [0, 1] into 5 intervals with uniform spacing. We used \u03bbSB = \u03bbReg = 1 and \u03c4 = 0.01. To estimate the entropy loss, we used mutual information neural estimation method Belghazi et al. (2018). To incorporate timestep embedding and stochastic conditioning into out UNSB network, we used positional embedding and AdaIN layers, respectively, following the implementation of DDGAN Xiao et al. (2022). For I2I tasks, we used CUT loss as regularization. On Summer2Winter translation task, we used a pre-trained VGG16 network as our feature selection source, following the strategy in the previous work by Zheng et al. (2021).\nEntropy approximation. We first describe how to estimate the entropy of a general random variable. We observe that for a random variable X ,\nI(X,X) = H(X)\u2212H(X|X)\ufe38 \ufe37\ufe37 \ufe38 =0 = H(X) (24)\nso mutual information may be used to estimate the entropy of a random variable. Works such as Belghazi et al. (2018) and Lim et al. (2020) provide methods to estimate mutual information of two random variables by optimizing a neural network. For instance, Belghazi et al. (2018) tells us that\nI\u0398(X,Z) := sup \u03b8\u2208\u0398\nEPXZ [T\u03b8]\u2212 log ( EPX\u2297PZ [eT\u03b8 ] ) (25)\nwhere T\u03b8 is a neural network parametrized by \u03b8 \u2208 \u0398 is capable of approximating I(X,Z) up to arbitrary accuracy. Hence, we can estimate H(X) = I(X,X) by setting X = Z and optimizing Eq. (25). In the UNSB objective Eq. (9), we use Eq. (25) to estimate the entropy of X = Z = (x1,xti) where (xti ,x1) \u223c q\u03d5i(xti ,x1) for q\u03d5i defined in Eq. (8). In practice, we use the same neural net architecture for T\u03b8 and the discriminator. We use the same network for T\u03b8 for each time-step ti by incorporating time embedding into T\u03b8. T\u03b8 is updated once with gradient ascent every q\u03d5i update, analogous to adversarial training."
        },
        {
            "heading": "Evaluation.",
            "text": "\u2022 Metric calculation codes \u2013 FID : https://github.com/mseitzer/pytorch-fid \u2013 KID : https://github.com/alpc91/NICE-GAN-pytorch\n\u2022 Evaluation protocol: we follow standard procedure, as described in Park et al. (2020). \u2022 KID measurement\n\u2013 DistanceGAN, MUNIT, NOT, SBCFM : trained from scratch using official code. \u2013 Baselines excluding above four methods : KID measured using samples generated by\nmodels from official repository. \u2022 FID measurement\n\u2013 Horse2Zebra and Label2Cityscape : numbers taken from Table 1 in Park et al. (2020). \u2013 Summer2Winter and Map2Satellite : all baselines trained using official code.\nMethod FID \u2193 KID\u2193 SBCFM 229.5 18.8\nOurs 35.7 0.587\nTable 4: Quantitative comparison result with another SB-based method on Horse2Zebra. The baseline model performance is severely degraded, while our generated results show superior perceptual quality."
        },
        {
            "heading": "C ADDITIONAL EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "C.1 OTHER SB METHODS",
            "text": "In this part, we compare our proposed UNSB with other Schr\u00f6dinger bridge based image translation methods. The recent work of I2SB and InDI require supervised training setting, therefore we cannot compare our proposed method with the baselines. For most of other SB-based methods, they focus on relatively easy problem, where one side of distribution is simple. Since we firstly proposed applying SB-based approach to high-resolution I2I problem, there is no direct baseline to compare. However, we compared our model with SB-based approach of Schr\u00f6dinger Bridge Conditional Flow Matching (SBCFM) (Shi et al., 2022) by adapting the SBCFM to high-resolution case.\nIn Table 4 and Fig. 9, we can see that the baseline SBCFM model could not generate the proper outputs where the outputs are totally unrelated to the input image. The results show that the model could not overcome the curse of dimensionality problem."
        },
        {
            "heading": "C.2 MORE STOCHASTICITY ANALYSIS",
            "text": "In Figure 10, we visualize output pixel-wise standard deviation given input for more samples. We note that all pixel values are normalized into [0, 1]. A SB between two domains is stochastic, so it does one-to-many translation. Thus, a necessary condition for a model to be a SB is stochasticity. Figure 10 shows that UNSB satisfies this necessary condition.\nGreater std for foreground pixels tells us UNSB does one-to-many generation, and generated images lie in the target domain. For instance, a model which simply adds Gaussian noise also does one-tomany generation, but is not meaningful. UNSB output shows high variation for locations which are relevant to the target domain (Zebra)."
        },
        {
            "heading": "C.3 MALE2FEMALE TRANSLATION",
            "text": "We provide results on the Male2Female translation task with the CelebA-HQ-256 dataset (Karras et al., 2018). Baseline methods are EGSDE (Zhao et al., 2022), StarGAN v2 (Choi et al., 2022), and NOT. UNSB is trained according to Appendix B. We used official pre-trained models for EGSDE and StarGAN v2 and trained NOT from scratch using the official code. We use the evaluation protocol in CUT (Park et al., 2020) to evaluate all models, i.e., we use test set images to measure FID. We note that (Zhao et al., 2022) and (Choi et al., 2022) use a different evaluation protocol in their papers, (they use train set images to measure FID) so the numbers in our paper and their papers may differ.\nResults are shown in Table 5 and Figure 11. We note that UNSB produces outputs closer to the target image domain, as evidenced by FID and KID. Also, EGSDE, StarGAN v2, and NOT often fail to preserve input structure information. For instance, EGSDE fails to preserve eye direction or hair style. StarGAN v2 hallucinates earrings and fails to preserve hair style. NOT changes mouth shape (teeth are showing for outputs).\nC.4 ENTROPIC OT BENCHMARK\nWe evaluate UNSB on the entropic OT benchmark provided by Gushchin et al. (2023c) for 64 \u00d7 64 resolution images. We use \u03f5 = 0.1 (using the notation in (Gushchin et al., 2023c)) and follow their training and evaluation protocol. Specifically, we trained UNSB to translate noisy CelebA images (i.e., y-samples) to clean CelebA images (i.e., x-samples). For comparison, we provide cFID numbers for SB p(x|y) learned by ENOT (Gushchin et al.,\n2023b) and unconditional p(x|y) = p(x) where we use 5k ground-truth samples from p(x). cFID for ENOT is taken from Table 7 in Gushchin et al. (2023c).\nIn Table 6, we see that UNSB performs much better than the unconditional map p(x|y) = p(x) but not quite as well as ENOT. We emphasize that this is only a preliminary result with ad-hoc hyper-parameter choices due to time constraints, and we expect better results with further tuning. Some points of improvement are:\n\u2022 Using more samples to measure cFID. We only used 100 x-samples for each y-sample. The benchmark provides 5k x-samples for each y-sample, so using more x-samples per y-sample could improve the cFID.\n\u2022 Better architecture design. UNSB architecture is specialized to \u2265 256\u00d7 256 resolution images, whereas the benchmark consists of 64\u00d7 64 resolution images. Currently, we set UNSB networks for this task as resize(x, 64) \u25e6G(x) \u25e6 resize(x, 256), where resize(x, s) is a function which resizes images to s\u00d7 s resolution, and G(x) is the UNSB network used in our main experiments.\n\u2022 Better regularization. CUT regularization is specialized to translation between clean images, whereas the benchmark consists of translation between noisy and clean images.\n\u2022 Longer training. We only trained UNSB for 150k iterations. On, for instance, Male2Female, we needed to train about 1.7m iterations to get good results.\nC.5 ZEBRA2HORSE TRANSLATION\nWe show that UNSB is capable of reverse translation on tasks in our paper as well. Specifically, we compare CycleGAN and UNSB on Zebra2Horse translation task. In Table 7, we see that UNSB beats CycleGAN in terms of both FID and KID. In Figure 12, we observe UNSB successfully preserves input structure as well."
        },
        {
            "heading": "C.6 MORE ABLATION STUDY",
            "text": "In Table 8, we provide another ablation study result on the Summer2Winter translation task, where we observe similar trends as before. Adding Markovian discrminator, regularization, and multi-step sampling monotonically increases the performance. This tells us the components of UNSB play orthogonal roles in unpaired image-to-image translation."
        },
        {
            "heading": "C.7 ADDITIONAL SAMPLES",
            "text": "256\u00d7256 images. In Fig. 13, 14, we show additional generated samples from our proposed UNSB and baseline models for our tasks in our main paper.\n512\u00d7512 images. In Fig. 15, we show samples from UNSB for Cat2Dog translation with AFHQ 512\u00d7512 resolution images, NFE = 5. This is a preliminary result with UNSB at training epoch 100."
        },
        {
            "heading": "D DISCUSSION ON SCENE-LEVEL VS. OBJECT-LEVEL TRANSFER",
            "text": "We note that unpaired image-to-image translation tasks can roughly be categorized into scenelevel transfer and object-level transfer. In scene-level transfer (e.g., Horse2Zebra, Summer2Winter, etc.), mostly color and texture need to be altered during translation. In object-level transfer (e.g., Male2Female, Cat2Dog, etc.), object identities need to be altered during translation. We discuss differences between UNSB and diffusion-based translation methods on scene-level and object-level transfer tasks, respectively.\nScene-level transfer. While diffusion-based methods such as SDEdit are capable of translating between high-resolution images, we note that they do not perform so well on scene-level translation tasks considered in our paper. For instance, see Table 2 for SDEdit and P2P results on Horse2Zebra, where they show 97.3 FID and 60.9 FID, respectively. UNSB achieves 35.7 FID.\nFigure 5a provides some clue as to why diffusion-based methods do not perform so well. P2P fails to preserve structural integrity (in the first row, the direction of the right horse is reversed) and SDEdit fails to properly translate images to zebra domain. Similar phenomenon occurs for Summer2Winter task as well. On the other hand, UNSB does not suffer from this problem as it uses a Markovian discriminator along with CUT regularization.\nObject-level transfer. Diffusion-based methods perform reasonably on object-level translation tasks such as Male2Female or Wild2Cat. We speculate this is because object-level translation tasks use images with cropped and centered subjects, so it is relatively easy to preserve structural integrity. In fact, as shown in Figure 16, semantically meaningful pairs on such dataset pairs can be obtained even with Sinkhorn-Knopp (a computational optimal transport method), which does not rely on deep learning. However, Sinkhorn-Knopp is unable to find meaningful pairs on Horse2Zebra, where subjects are not cropped and centered."
        }
    ],
    "year": 2024
}