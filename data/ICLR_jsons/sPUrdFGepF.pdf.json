{
    "abstractText": "In this paper, we present Consistent4D, a novel approach for generating 4D dynamic objects from uncalibrated monocular videos. Uniquely, we cast the 360degree dynamic object reconstruction as a 4D generation problem, eliminating the need for tedious multi-view data collection and camera calibration. This is achieved by leveraging the object-level 3D-aware image diffusion model as the primary supervision signal for training dynamic Neural Radiance Fields (DyNeRF). Specifically, we propose a cascade DyNeRF to facilitate stable convergence and temporal continuity under the supervision signal which is discrete along the time axis. To achieve spatial and temporal consistency, we further introduce an interpolation-driven consistency loss. It is optimized by minimizing the L2 distance between rendered frames from DyNeRF and interpolated frames from a pretrained video interpolation model. Extensive experiments show that our Consistent4D can perform competitively to prior art alternatives, opening up new possibilities for 4D dynamic object generation from monocular videos, whilst also demonstrating advantage for conventional text-to-3D generation tasks. Figure 1: Video-to-4D results achieved by our method. We show the renderings of 2 objects at 2 viewpoints and 3 timestamps.",
    "authors": [],
    "id": "SP:6e5d36cfe8868b63d1bf48ebd6f04ddb23c4165d",
    "references": [
        {
            "authors": [
                "Andreas Blattmann",
                "Robin Rombach",
                "Huan Ling",
                "Tim Dockhorn",
                "Seung Wook Kim",
                "Sanja Fidler",
                "Karsten Kreis"
            ],
            "title": "Align your latents: High-resolution video synthesis with latent diffusion models",
            "year": 2023
        },
        {
            "authors": [
                "Ang Cao",
                "Justin Johnson"
            ],
            "title": "Hexplane: A fast representation for dynamic scenes",
            "year": 2023
        },
        {
            "authors": [
                "Eric R Chan",
                "Marco Monteiro",
                "Petr Kellnhofer",
                "Jiajun Wu",
                "Gordon Wetzstein"
            ],
            "title": "pi-gan: Periodic implicit generative adversarial networks for 3d-aware image synthesis",
            "year": 2021
        },
        {
            "authors": [
                "Rui Chen",
                "Yongwei Chen",
                "Ningxin Jiao",
                "Kui Jia"
            ],
            "title": "Fantasia3d: Disentangling geometry and appearance for high-quality text-to-3d content",
            "year": 2023
        },
        {
            "authors": [
                "Matt Deitke",
                "Ruoshi Liu",
                "Matthew Wallingford",
                "Huong Ngo",
                "Oscar Michel",
                "Aditya Kusupati",
                "Alan Fan",
                "Christian Laforte",
                "Vikram Voleti",
                "Samir Yitzhak Gadre"
            ],
            "title": "Objaverse-xl: A universe of 10m+ 3d objects",
            "venue": "arXiv preprint,",
            "year": 2023
        },
        {
            "authors": [
                "Congyue Deng",
                "Chiyu Jiang",
                "Charles R Qi",
                "Xinchen Yan",
                "Yin Zhou",
                "Leonidas Guibas",
                "Dragomir Anguelov"
            ],
            "title": "Nerdi: Single-view nerf synthesis with language-guided diffusion as general image priors",
            "venue": "In arXiv preprint,",
            "year": 2022
        },
        {
            "authors": [
                "Sara Fridovich-Keil",
                "Giacomo Meanti",
                "Frederik Warburg",
                "Benjamin Recht",
                "Angjoo Kanazawa"
            ],
            "title": "K-planes: Explicit radiance fields in space, time, and appearance",
            "venue": "In arXiv preprint,",
            "year": 2023
        },
        {
            "authors": [
                "Chen Gao",
                "Ayush Saraf",
                "Johannes Kopf",
                "Jia-Bin Huang"
            ],
            "title": "Dynamic view synthesis from dynamic monocular video",
            "venue": "In ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "Hang Gao",
                "Ruilong Li",
                "Shubham Tulsiani",
                "Bryan Russell",
                "Angjoo Kanazawa"
            ],
            "title": "Monocular dynamic view synthesis: A reality check",
            "venue": "In NeurIPS,",
            "year": 2022
        },
        {
            "authors": [
                "Jun Gao",
                "Tianchang Shen",
                "Zian Wang",
                "Wenzheng Chen",
                "Kangxue Yin",
                "Daiqing Li",
                "Or Litany",
                "Zan Gojcic",
                "Sanja Fidler"
            ],
            "title": "Get3d: A generative model of high quality 3d textured shapes learned from images. 2022b",
            "year": 2022
        },
        {
            "authors": [
                "Yuan-Chen Guo",
                "Ying-Tian Liu",
                "Ruizhi Shao",
                "Christian Laforte",
                "Vikram Voleti",
                "Guan Luo",
                "ChiaHao Chen",
                "Zi-Xin Zou",
                "Chen Wang",
                "Yan-Pei Cao",
                "Song-Hai Zhang"
            ],
            "title": "threestudio: A unified framework for 3d content generation",
            "venue": "https://github.com/threestudio-project/ threestudio,",
            "year": 2023
        },
        {
            "authors": [
                "Jonathan Ho",
                "William Chan",
                "Chitwan Saharia",
                "Jay Whang",
                "Ruiqi Gao",
                "Alexey Gritsenko",
                "Diederik P Kingma",
                "Ben Poole",
                "Mohammad Norouzi",
                "David J Fleet"
            ],
            "title": "Imagen video: High definition video generation with diffusion models",
            "year": 2022
        },
        {
            "authors": [
                "Zhewei Huang",
                "Tianyuan Zhang",
                "Wen Heng",
                "Boxin Shi",
                "Shuchang Zhou"
            ],
            "title": "Real-time intermediate flow estimation for video frame interpolation",
            "year": 2022
        },
        {
            "authors": [
                "Phillip Isola",
                "Jun-Yan Zhu",
                "Tinghui Zhou",
                "Alexei"
            ],
            "title": "A Efros. Image-to-image translation with conditional adversarial networks. 2017",
            "year": 2017
        },
        {
            "authors": [
                "Muhammed Kocabas",
                "Nikos Athanasiou",
                "Michael J Black"
            ],
            "title": "Vibe: Video inference for human body pose and shape estimation",
            "venue": "In CVPR,",
            "year": 2020
        },
        {
            "authors": [
                "Ruilong Li",
                "Julian Tanke",
                "Minh Vo",
                "Michael Zollh\u00f6fer",
                "J\u00fcrgen Gall",
                "Angjoo Kanazawa",
                "Christoph Lassner"
            ],
            "title": "Tava: Template-free animatable volumetric actors",
            "venue": "In ECCV,",
            "year": 2022
        },
        {
            "authors": [
                "Tianye Li",
                "Mira Slavcheva",
                "Michael Zollhoefer",
                "Simon Green",
                "Christoph Lassner",
                "Changil Kim",
                "Tanner Schmidt",
                "Steven Lovegrove",
                "Michael Goesele",
                "Richard Newcombe"
            ],
            "title": "Neural 3d video synthesis from multi-view video",
            "venue": "In CVPR,",
            "year": 2022
        },
        {
            "authors": [
                "Zhengqi Li",
                "Simon Niklaus",
                "Noah Snavely",
                "Oliver Wang"
            ],
            "title": "Neural scene flow fields for space-time view synthesis of dynamic scenes",
            "year": 2021
        },
        {
            "authors": [
                "Chen-Hsuan Lin",
                "Jun Gao",
                "Luming Tang",
                "Towaki Takikawa",
                "Xiaohui Zeng",
                "Xun Huang",
                "Karsten Kreis",
                "Sanja Fidler",
                "Ming-Yu Liu",
                "Tsung-Yi Lin"
            ],
            "title": "Magic3d: High-resolution text-to-3d content creation",
            "year": 2023
        },
        {
            "authors": [
                "Ruoshi Liu",
                "Rundi Wu",
                "Basile Van Hoorick",
                "Pavel Tokmakov",
                "Sergey Zakharov",
                "Carl Vondrick"
            ],
            "title": "Zero-1-to-3: Zero-shot one image to 3d object",
            "venue": "In arXiv preprint,",
            "year": 2023
        },
        {
            "authors": [
                "Naureen Mahmood",
                "Michael J Black"
            ],
            "title": "Smpl: A skinned multiperson linear model",
            "venue": "In ACM transactions on graphics,",
            "year": 2015
        },
        {
            "authors": [
                "Chenlin Meng",
                "Yutong He",
                "Yang Song",
                "Jiaming Song",
                "Jiajun Wu",
                "Jun-Yan Zhu",
                "Stefano Ermon"
            ],
            "title": "Sdedit: Guided image synthesis and editing with stochastic differential equations",
            "venue": "In ICLR,",
            "year": 2021
        },
        {
            "authors": [
                "Keunhong Park",
                "Utkarsh Sinha",
                "Jonathan T Barron",
                "Sofien Bouaziz",
                "Dan B Goldman",
                "Steven M Seitz",
                "Ricardo Martin-Brualla"
            ],
            "title": "Nerfies: Deformable neural radiance fields",
            "venue": "In CVPR,",
            "year": 2021
        },
        {
            "authors": [
                "Keunhong Park",
                "Utkarsh Sinha",
                "Peter Hedman",
                "Jonathan T Barron",
                "Sofien Bouaziz",
                "Dan B Goldman",
                "Ricardo Martin-Brualla",
                "Steven M Seitz"
            ],
            "title": "Hypernerf: A higher-dimensional representation for topologically varying neural radiance fields",
            "venue": "In arXiv preprint,",
            "year": 2021
        },
        {
            "authors": [
                "Ben Poole",
                "Ajay Jain",
                "Jonathan T Barron",
                "Ben Mildenhall"
            ],
            "title": "Dreamfusion: Text-to-3d using 2d diffusion",
            "venue": "In ICLR,",
            "year": 2023
        },
        {
            "authors": [
                "Albert Pumarola",
                "Enric Corona",
                "Gerard Pons-Moll",
                "Francesc Moreno-Noguer"
            ],
            "title": "D-nerf: Neural radiance fields for dynamic scenes",
            "venue": "In CVPR,",
            "year": 2021
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "year": 2021
        },
        {
            "authors": [
                "Zhongzheng Ren",
                "Xiaoming Zhao",
                "Alex Schwing"
            ],
            "title": "Class-agnostic reconstruction of dynamic objects from videos",
            "year": 2021
        },
        {
            "authors": [
                "Robin Rombach",
                "Andreas Blattmann",
                "Dominik Lorenz",
                "Patrick Esser",
                "Bj\u00f6rn Ommer"
            ],
            "title": "Highresolution image synthesis with latent diffusion models",
            "year": 2022
        },
        {
            "authors": [
                "Katja Schwarz",
                "Yiyi Liao",
                "Michael Niemeyer",
                "Andreas Geiger"
            ],
            "title": "Graf: Generative radiance fields for 3d-aware image synthesis",
            "year": 2020
        },
        {
            "authors": [
                "Junyoung Seo",
                "Wooseok Jang",
                "Min-Seop Kwak",
                "Jaehoon Ko",
                "Hyeonsu Kim",
                "Junho Kim",
                "Jin-Hwa Kim",
                "Jiyoung Lee",
                "Seungryong Kim"
            ],
            "title": "Let 2d diffusion model know 3d-consistency for robust text-to-3d generation",
            "venue": "In arXiv preprint,",
            "year": 2023
        },
        {
            "authors": [
                "Ruizhi Shao",
                "Zerong Zheng",
                "Hanzhang Tu",
                "Boning Liu",
                "Hongwen Zhang",
                "Yebin Liu"
            ],
            "title": "Tensor4d: Efficient neural 4d decomposition for high-fidelity dynamic reconstruction and rendering",
            "year": 2023
        },
        {
            "authors": [
                "Uriel Singer",
                "Shelly Sheynin",
                "Adam Polyak",
                "Oron Ashual",
                "Iurii Makarov",
                "Filippos Kokkinos",
                "Naman Goyal",
                "Andrea Vedaldi",
                "Devi Parikh",
                "Justin Johnson"
            ],
            "title": "Text-to-4d dynamic scene generation",
            "year": 2023
        },
        {
            "authors": [
                "Junshu Tang",
                "Tengfei Wang",
                "Bo Zhang",
                "Ting Zhang",
                "Ran Yi",
                "Lizhuang Ma",
                "Dong Chen"
            ],
            "title": "Makeit-3d: High-fidelity 3d creation from a single image with diffusion prior",
            "venue": "In arXiv preprint,",
            "year": 2023
        },
        {
            "authors": [
                "Edgar Tretschk",
                "Ayush Tewari",
                "Vladislav Golyanik",
                "Michael Zollh\u00f6fer",
                "Christoph Lassner",
                "Christian Theobalt"
            ],
            "title": "Non-rigid neural radiance fields: Reconstruction and novel view synthesis of a dynamic scene from monocular video",
            "venue": "In ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "Thomas Unterthiner",
                "Sjoerd Van Steenkiste",
                "Karol Kurach",
                "Raphael Marinier",
                "Marcin Michalski",
                "Sylvain Gelly"
            ],
            "title": "Towards accurate generative models of video: A new metric & challenges",
            "year": 2018
        },
        {
            "authors": [
                "Dor Verbin",
                "Peter Hedman",
                "Ben Mildenhall",
                "Todd Zickler",
                "Jonathan T Barron",
                "Pratul P Srinivasan"
            ],
            "title": "Ref-nerf: Structured view-dependent appearance for neural radiance fields",
            "venue": "In CVPR",
            "year": 2022
        },
        {
            "authors": [
                "Minh Vo",
                "Yaser Sheikh",
                "Srinivasa G Narasimhan"
            ],
            "title": "Spatiotemporal bundle adjustment for dynamic 3d human reconstruction in the wild",
            "year": 2020
        },
        {
            "authors": [
                "Haochen Wang",
                "Xiaodan Du",
                "Jiahao Li",
                "Raymond A Yeh",
                "Greg Shakhnarovich"
            ],
            "title": "Score jacobian chaining: Lifting pretrained 2d diffusion models for 3d generation",
            "year": 2022
        },
        {
            "authors": [
                "Zhengyi Wang",
                "Cheng Lu",
                "Yikai Wang",
                "Fan Bao",
                "Chongxuan Li",
                "Hang Su",
                "Jun Zhu"
            ],
            "title": "Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation",
            "year": 2023
        },
        {
            "authors": [
                "Shangzhe Wu",
                "Tomas Jakab",
                "Christian Rupprecht",
                "Andrea Vedaldi. Dove"
            ],
            "title": "Learning deformable 3d objects by watching videos",
            "venue": "In arXiv preprint,",
            "year": 2021
        },
        {
            "authors": [
                "Shangzhe Wu",
                "Ruining Li",
                "Tomas Jakab",
                "Christian Rupprecht",
                "Andrea Vedaldi"
            ],
            "title": "Magicpony: Learning articulated 3d animals in the wild",
            "venue": "In arXiv preprint,",
            "year": 2022
        },
        {
            "authors": [
                "Tianhao Wu",
                "Fangcheng Zhong",
                "Andrea Tagliasacchi",
                "Forrester Cole",
                "Cengiz Oztireli. D"
            ],
            "title": "2\u0302 nerf: Self-supervised decoupling of dynamic and static objects from a monocular video. 2022b",
            "year": 2022
        },
        {
            "authors": [
                "Wenqi Xian",
                "Jia-Bin Huang",
                "Johannes Kopf",
                "Changil Kim"
            ],
            "title": "Space-time neural irradiance fields for free-viewpoint video",
            "venue": "In CVPR,",
            "year": 2021
        },
        {
            "authors": [
                "Gengshan Yang",
                "Minh Vo",
                "Natalia Neverova",
                "Deva Ramanan",
                "Andrea Vedaldi",
                "Hanbyul Joo"
            ],
            "title": "Banmo: Building animatable 3d neural models from many casual videos",
            "year": 2022
        },
        {
            "authors": [
                "Richard Zhang",
                "Phillip Isola",
                "Alexei A Efros",
                "Eli Shechtman",
                "Oliver Wang"
            ],
            "title": "The unreasonable effectiveness of deep features as a perceptual metric",
            "venue": "cvpr,",
            "year": 2018
        },
        {
            "authors": [
                "Daquan Zhou",
                "Weimin Wang",
                "Hanshu Yan",
                "Weiwei Lv",
                "Yizhe Zhu",
                "Jiashi Feng"
            ],
            "title": "Magicvideo: Efficient video generation with latent diffusion models",
            "year": 2022
        },
        {
            "authors": [
                "Silvia Zuffi",
                "Angjoo Kanazawa",
                "David W Jacobs",
                "Michael J Black"
            ],
            "title": "3d menagerie: Modeling the 3d shape and pose of animals",
            "year": 2017
        },
        {
            "authors": [
                "Silvia Zuffi",
                "Angjoo Kanazawa",
                "Michael J. Black"
            ],
            "title": "Lions and tigers and bears: Capturing nonrigid, 3d, articulated shape from images",
            "year": 2018
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Perceiving dynamic 3D information from visual observations is one of the fundamental yet challenging problems in computer vision, which is the key to a broad range of downstream applications e.g., virtual content creation, autonomous driving simulation, and medical image analysis. However, due to the high complexity nature of dynamic 3D signals, it is rather difficult to recover such information from a single monocular video observation. As a result, existing dynamic object reconstruction approaches usually take synchronized multi-view videos as inputs (Li et al., 2022b;a; Shao et al., 2023), or rely on training data containing effective multi-view cues (e.g., teleporting cameras or quasi-static scenes (Li et al., 2021; Pumarola et al., 2021; Park et al., 2021b;a)). However, current reconstruction approaches often fail in reconstructing regions that were not observed in input sequences (Gao et al., 2022a). Moreover, multi-view data capturing requires synchronized camera rigs and meticulous calibrations, which inevitably limit the methods to potential real-world applications.\nOn the other hand, given only a video clip of a dynamic object, humans are capable of depicting the appearance, geometry, and movement of the object. This is achieved by prior knowledge of\nvisual appearance accumulated through human life. In contrast to the multi-view video setting, our approach favors such simple and practical input settings of a static monocular video. The static monocular video offers several advantages: ease of collection for handheld cameras, minimizing the risk of motion blur due to camera movement, and obviating the need for camera parameter estimation. As a static monocular video does not provide effective multi-view information for reconstruction, we instead opt for the generation approaches.\nInspired by recent advancements in text-to-3D (Poole et al., 2023; Wang et al., 2022; Chen et al., 2023; Wang et al., 2023) and image-to-3D (Deng et al., 2022; Tang et al., 2023; Melas-Kyriazi et al., 2023) techniques, in this work, we present a novel video-to-4D generation approach termed Consistent4D. In this approach, we represent dynamic objects through a specially designed Cascade DyNeRF and leverage a pre-trained 2D diffusion model to regulate the DyNeRF optimization. To ensure both spatial and temporal consistency, we introduce an Interpolation-driven Consistency Loss (ICL), which minimizes the L2 distance between frames rendered by DyNeRF and frames interpolated by a pre-trained video interpolation model. The ICL loss not only enhances consistency in 4D generation but also mitigates multi-face issues in 3D generation.\nWe have extensively evaluated our approach on both synthetic videos rendered from animated 3D model and in-the-wild videos collected from the Internet. To summarize, the contribution of this work includes:\n\u2022 We propose a video-to-4D framework for dynamic object generation from a statically captured monocular video. A specially designed Cascade DyNeRF is applied to represent the object and is optimized through the Score Distillation Sampling (SDS) loss by a pretrained 2D diffusion model. Moreover, for the comprehensiveness of the framework, we train a lightweight vide enhancer to improve the rendering of 4D object as an optional post-processing step.\n\u2022 To address the challenge of maintaining temporal and spatial consistency in 4D generation, we introduce a novel Interpolation-driven consistency loss to improve the spatial and temporal consistency of the 4D generation. The proposed interpolation-driven consistency loss can significantly improve both video-to-4D and text-to-3D generation quality.\n\u2022 We extensively evaluate our method on both synthetic and in-the-wild videos collected from the Internet, showing promising results for the new task of video-to-4D generation."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "3D Generation 3D generation aims to generate 3D content conditionally or unconditionally. Early works mainly make use of GAN, i.e., generate category-specific 3D objects or scenes from random noise after learning the category prior (Schwarz et al., 2020; Chan et al., 2021; Gao et al., 2022b). Recently, general-purpose 3D generation has been enabled by text-to-image diffusion model pretrained on Internet-scale data, and it also becomes more controllable, e.g., controlled by text prompt or single image. The pioneer work of text-to-3D is DreamFusion (Poole et al., 2023), which proposes Score Distillation Sampling (SDS) loss to leverage the image diffusion model for neural radiance field (NeRF) training. The following works (Lin et al., 2023; Chen et al., 2023; Wang et al., 2023) further enhance the visual quality of the generated object by using mesh representation, Variational Score Distillation, etc. However, the challenging problem, multi-face Janus problem, has not been addressed in the above works. Image is another popular condition for 3D generation. Different from 3D reconstruction, which focuses on the reconstruction of visible regions from multi-view images, 3D generation usually has only a single image and relies much on the image diffusion model to generate invisible regions of the object (Melas-Kyriazi et al., 2023; Tang et al., 2023; Liu et al., 2023). So many works simply translate the image to words using Lora and then exploit text-to3D methods (Melas-Kyriazi et al., 2023; Tang et al., 2023; Seo et al., 2023). One exception is Zero123 (Liu et al., 2023), which trains a 3D-aware image-to-image model using multi-view data and could generate a novel view of the object in the input image directly. Benefiting from multi-view data training, the multi-face Janus problem has been alleviated to some extent in Zero123.\n4D Reconstruction 4D reconstruction, aka dynamic scene reconstruction, is a challenging task. Some early works focus on object-level reconstruction and adopt parametric shape mod-\nels (Matthew Loper & Black, 2015; Vo et al., 2020) as representation. In recent years, dynamic neural radiance field become popular, and convenient dynamic scene reconstruction is enabled. These works can be classified into two categories: a deformed scene is directly modeled as a NeRF in canonical space with a time-dependent deformation (Pumarola et al., 2021; Park et al., 2021a;b; Wu et al., 2022b; Tretschk et al., 2021) or time-varying NeRF in the world space (Gao et al., 2021; Li et al., 2021; Xian et al., 2021; Fridovich-Keil et al., 2023; Cao & Johnson, 2023). Some of them require multi-view synchronized data to reconstruct dynamic scenes, however, data collection and calibration is not convenient (Li et al., 2022b; Shao et al., 2023). So, reconstruction from monocular videos gain attention. However, those monocular methods either require teleporting camera or quaic-static scenes (Pumarola et al., 2021; Park et al., 2021a;b), which are not representative of daily life scenarios (Gao et al., 2022a).\n4D Generation 4D generation extends 3D generation to space+time domain and thus is more challenging. Early works are mainly category-specific and adopt parametric shape models (Zuffi et al., 2017; 2018; Vo et al., 2020; Kocabas et al., 2020) as representation. They usually take images or videos as conditions and need category-specific 3D templates or per-category training from a collection of images or videos (Ren et al., 2021; Wu et al., 2021; Yang et al., 2022; Wu et al., 2022a). Recently, one zero-shot category-agnostic work, text-to-4D (Singer et al., 2023), achieves general-purpose dynamic scene generation from text prompt. It follows DreamFusion (Poole et al., 2023) and extends it to the time domain by proposing a three-stage training framework. However, the quality of generated scenes is limited due to low-quality video diffusion models."
        },
        {
            "heading": "3 PRELIMINARIES",
            "text": ""
        },
        {
            "heading": "3.1 SCORE DISTILLATION SAMPLING FOR IMAGE-TO-3D",
            "text": "Score Distillation Sampling (SDS) is first proposed in DreamFusion (Poole et al., 2023) for text-to3D tasks. It enables the use of a 2D text-to-image diffusion model as a prior for optimization of a NeRF. We denote the NeRF parameters as \u03b8, text-to-image diffusion model as \u03d5, text prompt as \u03c1, the rendering image and the noisy image as x and z, the SDS loss is defined as:\n\u2207\u03b8LSDS (\u03d5,x) = E\u03c4,\u03f5 [ \u03c9(t) (\u03f5\u0302\u03b8 (zt; \u03c1, \u03c4)\u2212 \u03f5) \u2202x\n\u2202\u03b8\n] , (1)\nwhere \u03c4 is timestamps in diffusion process, \u03f5 denotes noise, and \u03c9 is a weighted function. Intuitively, this loss perturbs x with a random amount of noise corresponding to the timestep \u03c4 , and estimates an update direction that follows the score function of the diffusion model to move to a higher density region.\nBesides text-to-3D, SDS is also widely used in image-to-3D tasks. Zero123 (Liu et al., 2023) is one prominent representative. It proposes a viewpoint-conditioned image-to-image translation diffusion model fine-tuned from Stable Diffusion (Rombach et al., 2022), and exploits this 3D-aware image diffusion model to optimize a NeRF using SDS loss. This image diffusion model takes one image, denoted by Iin, and relative camera extrinsic between target view and input view, denoted by (R,T), as the input, and outputs the target view image Iout. Compared with the original text-to-image diffusion model, text prompt in Equation 1 is not required in this model cause the CLIP embedding of the input image and the relative viewpoint change replace the text prompt. Then Equation 1 could be re-written as:\n\u2207\u03b8LSDS (\u03d5,x) = E\u03c4,\u03f5 [ \u03c9(t) (\u03f5\u0302\u03b8 (zt; Iin,R,T, \u03c4)\u2212 \u03f5) \u2202x\n\u2202\u03b8\n] , (2)"
        },
        {
            "heading": "3.2 K-PLANES",
            "text": "K-planes (Fridovich-Keil et al., 2023) is a simple and effective dynamic NeRF method which factorizes a dynamic 3D volume into six feature planes (i.e., hex-plane), denoted as P = {Po}, where o \u2208 {xy, yz, xz, xt, yt, zt}. The first three planes correspond to spatial dimensions, while the last three planes capture spatiotemporal variations. Each of the planes is structured as a M \u00d7M \u00d7 F tensor in the memory, where M represents the size of the plane and F is the feature size that encodes scene density and color information. Let t denote the timestamp of a video clip, given a point\np = (x, y, z, t) in the 4D space, we normalize the coordinate to the range [0,M) and subsequently project it onto the six planes using the equation f(p)o = Po(\u03b9o(p)), where \u03b9o is the projection from a space point p to a pixel on the o\u2019th plane. The plane feature f(p)o is extracted via bilinear interpolation. The six plane features are combined using the Hadamard product (i.e., element-wise multiplication), to produce a final feature vector as follows:\nf(p) = \u220f\no\u2208{xy,yz,xz,xt,yt,zt}\nf(p)o, (3)\nThen, the color and density of p is calculated as c(p) = c(f(p)) and d(p) = d(f(p)), where c and d denotes mlps for color and density."
        },
        {
            "heading": "4 METHOD",
            "text": "In this work, we target to generate a 360\u25e6 dynamic object from a statically captured monocular video. To achieve this goal, we develop a framework consisting of a DyNeRF and a video enhancer, supervised by the pre-trained 2D diffusion model in Zero123 and a GAN, respectively. As shown in Figure 2, our framework contains two stages, in the first stage, we train a specially designed cascade DyNeRF using SDS loss and image reconstruction loss. To guarantee spatial and temporal consistency, we propose a novel Interpolation-driven Consistency Loss as the extra regularization for the DyNeRF. Inspired by pix2pix (Isola et al., 2017), we apply GAN to train a lightweight video enhancer. In such a way, we obtain a DyNeRF from which we can render 360\u25e6 view of the dynamic object, and the rendered results can be further enhanced by the video enhancer.\nIn the following sections, we will first introduce our design of the Cascade DyNeRF, and then illustrate the Interpolation-driven Consistency loss. Video enhancer is described in the third section. At last, we detail the training loss."
        },
        {
            "heading": "4.1 CASCADE DYNERF",
            "text": "Existing DyNeRF methods mainly assume the supervision signals are temporally coherent, however, this assumption does not hold in our pipeline due to the use of the image diffusion models with no time notion. In order to minimize the impact of temporal discontinuity in the supervision signals, we are prone to 4D representations which naturally guarantee a certain level of temporal continuity. Therefore, we build our DyNeRF based on K-planes (Fridovich-Keil et al., 2023) which exploits\ntemporal interpolation, an operator naturally inclined to temporal smoothing. Empirically, maintaining temporal coherence is possible when the time resolution of spatiotemporal planes is small; however, this results in over-smoothed images where finer details are lost. In contrast, when the time resolution is large, the quality of the images is enhanced, but the continuity of images within the same time series diminishes. To achieve both temporal continuity and high image quality, we adjust the multi-scale technique from the original paper and introduced Cascade DyNeRF.\nLet us denote the scale index by s. In original K-planes, multi-scale features are exploited by concatenation along feature dimension, then the color and density could be calculated as:\nc(p) = c(concat({f(p)s)}Ss=1), d(p) = d(concat({f(p)s)}Ss=1), (4) where S is the number of scales. In our setting, simple concatenation is hard to balance between image quality and temporal consistency. So we propose to leverage the cascade architecture and let the low-resolution planes learn temporally coherent dynamic objects with a certain degree of oversmoothing, and let the high-resolution planes learn the residual between the above results and the target ones. The final color and density are the addition of results from planes across the scale. That is,\nc(p)s = s\u2211\nk=1\nc(f(p)k), d(p)s = s\u2211\nk=1\nd(f(p)k), (5)\nwhere k indicates the scale index. Note that SDS loss and other losses are applied to the rendering results of each scale to guarantee that planes with higher resolution only learn the residual between results from previous scales and the target object. In this way, we can improve temporal consistency without sacrificing much object quality."
        },
        {
            "heading": "4.2 INTERPOLATION-DRIVEN CONSISTENCY LOSS",
            "text": "Video generation methods usually train an inter-frame interpolationi module to enhance the temporal consistency between keyframes (Ho et al., 2022; Zhou et al., 2022; Blattmann et al., 2023). Inspired by this, we exploit a pre-trained light-weighted video interpolation model and propose Interpolationdriven Consistency Loss to enhance the spatiotemporal consistency of the 4D generation.\nThe interpolation model adopted in this work is RIFE (Huang et al., 2022), which takes a pair of consecutive images as well as the interpolation ratio \u03b3 (0 < \u03b3 < 1) as the input, and outputs the interpolated image. In our case, we first render a batch of images that are either spatially continuous or temporally continuous, denoted by {x}Jj=1, where J is the number of images in a batch. Let us denote the video interpolation model as \u03c8, the interpolated image as x\u0302, then we calculate the Interpolation-driven Consistency Loss as:\nx\u0302j = \u03c8(x1,xJ , \u03b3j),\nLICL = J\u22121\u2211 j=2 \u2225xj \u2212 x\u0302j\u22252, (6)\nwhere \u03b3j = j\u22121J\u22121 , and 2 \u2264 j \u2264 J \u2212 1.\nThis simple yet effective loss enhances the continuity between frames thus improving the spatiotemporal consistency in dynamic object generation by a large margin. Moreover, we find the spatial version of this loss alleviates the multi-face problem in 3D generation tasks as well. Please refer to the experiment sections to see quantitative and qualitative results. The Interpolation-driven Consistency Loss and some other regularization losses are added with SDS loss in Equation 2, details of which can be found in the experiment section."
        },
        {
            "heading": "4.3 CROSS-FRAME VIDEO ENHANCER",
            "text": "Sometimes image sequence rendered from the optimized DyNeRF suffers from artifacts, such as blurry edges, small floaters, and insufficient smoothness, especially when the object motion is abrupt or complex. To further improve the quality of rendered videos, we design a lightweight video enhancer and optimize it via GAN, following pix2pix (Isola et al., 2017). The real images are obtained with image-to-image technique (Meng et al., 2021) using a super-resolution diffusion model, and the fake images are the rendered ones.\nTo better exploit video information, We add cross-frame attention to the UNet architecture in pix2pix, i.e., each frame will query information from two adjacent frames. We believe this could enable better consistency and image quality. Denote the feature map before and after cross-frameattention as F and F\u2032j , we have:\nF \u2032j = Attention(Qj ,Kj ,Vj), Qj = flatten(Fj), Kj = Vj = flatten(concat(Fj\u22121, Fj+1),\n(7)\nwhere Q, K and V denotes query, key, and value in attention mechanism, and concat denotes the concatenation along the width dimension."
        },
        {
            "heading": "4.4 OPIMIZATION",
            "text": "We optimize the dynamic NeRF using SDS loss LSDS in Eq. 2 and ICL loss LICL in Eq. 6. Besides, we apply reconstruction loss Lrec and foreground mask loss Lm for the input view following Guo et al. (2023). 3D normal smooth loss Ln (Guo et al., 2023) and orientation loss Lori (Verbin et al., 2022) are utilized to achieve better geometry. Therefore, the final optimization objective for dynamic NeRF is calculated as:\nL = \u03bb1LSDS + \u03bb2LICL + \u03bb3Lrec + \u03bb4Lm + \u03bb5Ln + \u03bb6Lori (8) For video enhancer in optional post-processing step, the loss function is the same as pix2pix (Isola\net al., 2017)."
        },
        {
            "heading": "5 EXPERIMENT",
            "text": "We have conducted extensive experiments to evaluate the proposed Consistent4D generator using both synthetic data and in-the-wild data. The experimental setup, comparison with dynamic NeRF baselines, and ablations are provided in the following sections."
        },
        {
            "heading": "5.1 IMPLEMENTATION DETAILS",
            "text": "Data Preparation For each input video, we initially segment the foreground object utilizing SAM (Kirillov et al., 2023) and subsequently sample 32 frames uniformly. The majority of the input videos span approximately 2 seconds, with some variations extending to around 1 second or exceeding 5 seconds. For the ablation study of video sampling, please refer to the appendix A.3.\nTraining During SDS and interpolation consistency optimization, we utilize zero123-xl trained by Deitke et al. (2023) as the diffusion model for SDS loss. For Cascade DyNeRF, we set s = 2, i.e., we have coarse-level and fine-level DyNeRFs.The spatial and temporal resolution of Cascade DyNeRF are configured to 50 and 8 for coarse-level, and 100 and 16 for fine-level, respectively. We first train DyNeRF with batch size 4 and resolution 64 for 5000 iterations. Then we decrease the batch size to 1 and increase the resolution to 256 for the next 5000 iteration training. ICL is employed in the initial 5000 iterations with a probability of 25%, and we sample consecutive temporal frames at intervals of one frame and sample consecutive spatial frames at angular intervals of 5\u25e6-15\u25e6 in azimuth. SDS loss weight is set as 0.1 and reconstruction loss weight is set as 500. The learning rate is set as 0.01 and the optimizer is Adam. In the post video enhancing stage, we train the video enhancer with a modified Unet architecture. The learning rate is set as 0.002, the batch size is 16, and the training epoch is 200. The optimization of dynamic NeRF and video enhancer cost about 2.5 hours and 15 minutes on a single V100 GPU. For details, please refer to the appendix A.4."
        },
        {
            "heading": "5.2 COMPARISONS WITH OTHER METHODS",
            "text": "To date, few methods have been developed for 4D generation utilizing video obtained from a static camera, so we compare our method with approaches with 4D modeling capabilities, i.e., DNeRF (Pumarola et al., 2021) and K-planes (Fridovich-Keil et al., 2023), as well as approaches with 3d generation ability, i.e. Zero123 (Liu et al., 2023). For a fair comparison, video enhancer is not applied here.\nQuantitative Results To quantitatively evaluate the proposed video-4D generation method, we select and download seven animated models, namely Pistol, Guppie, Crocodie, Monster, Skull, Trump,\nAurorus, from Sketchfab (ske, 2023) and render the multi-view videos by ourselves, as shown in Figure 3 and appendix A.2. We render one input view for scene generation and 4 testing views for our evaluation. For evaluation metrics, we provide image-level metrics, LPIPS (Zhang et al., 2018) and CLIP (Radford et al., 2021), as well as video-level metric, Frechet Video Distance (FVD) (Unterthiner et al., 2018). LPIPS and CLIP are computed between testing and rendered videos in a per-frame way, reflecting single-frame quality. FVD is computed between video pairs, taking both single-frame quality and temporal coherence in the entire video into consideration. We report the scores averaged over the four testing views of seven objects in Table. 1a (detailed metrics on each object can be found in the appendix). As shown in Table 1a, our dynamic 3D generation produces the best quantitative results over the other two dynamic NeRF methods on all metrics, which well aligns with the qualitative comparisons shown in Figure 3. Zero123 (per-frame reconstruction) has advantages over our method in terms of image-level metric, however, it lags behind the proposed method by a clear margin in terms of video-level metric FVD, which indicates severe temporal incoherence in Zero123 outputs.\nQualitative Results The outcomes of our method and those of other baselines are illustrated in Figure 3. It is observable that both D-NeRF and HyperNeRF methods struggle to achieve satisfactory results in novel views, owing to the absence of multi-view information in the training data. Zero123,\nalthough outperforms ours in terms of image quality, suffers from severe temporal inconsistency, which could be observed via the video attached in the supplementary material. In contrast, leveraging the strengths of the generation model as well as the temporal modeling capability of dynamic NeRF, our method proficiently generates a 360\u25e6 representation of the dynamic object. For additional results, please refer to the appendix A.1."
        },
        {
            "heading": "5.3 ABLATIONS",
            "text": "We perform ablation studies for every component within our framework. For clarity, the video enhancer is excluded when conducting ablations for SDS and interpolation consistency optimization. Quantitative results averaged on seven objects in the synthetic dataset are provided in Table 1b. Considering that the primary objective of introducing Cascade DyNeRF and ICL loss is to enhance spatial and temporal coherence, we advise readers to prioritize the video-level metric (FVD) over image-level metrics (LPIPS and CLIP) in evaluating this ablation study. The notable improvements observed in FVD scores underscore the efficacy of the proposed Cascade DyNeRF and ICL loss (as shown in the first four rows). Below, we provide qualitative results on dynamic objects generated from in-the-wild monocular video captured in real-world or segmented from the animated films. This is intended to better demonstrate the effectiveness of each proposed module in real-world applications.\nIn p u t\nv id eo T em p o ra l IC\nL S p at ia l IC L\nIC L\nw /o\nI C\nL\nt = t1 t = t2 t = t1 t = t2 t = t1 t = t2 t = t1 t = t2\n(a) Video-to-4D. For each dynamic object, we render it from a novel view for two timestamps with textureless rendering for each timestamp. For clarity, we describe the input videos as follows (from left to right): blue jay pecking, T-rex roaring, corgi smiling, seagull turning around.\nCascade DyNeRF In Figure 11 (see in appendix), we conduct an ablation study for Cascade DyNeRF. Specifically, we substitute Cascade DyNeRF with the original K-planes architecture, maintaining all other settings unchanged. In the absence of the cascade architecture, the training proves to be\nunstable, occasionally yielding incomplete or blurry objects, as demonstrated by the first and second objects in Figure 11. In some cases, while the model manages to generate a complete object, the moving parts of the object lack clarity, exemplified by the leg and beak of the bird. Conversely, the proposed Cascade DyNeRF exhibits stable training, leading to relatively satisfactory generation results.\nInterpolation-driven Consistency Loss The introduction of Interpolation-driven Consistency Loss (ICL) stands as a significant contribution of our work. Therefore, we conduct extensive experiments to investigate both its advantages and potential limitations. Figure 4a illustrates the ablation of both spatial and temporal Interpolation-driven Consistency Loss (ICL) in the video-to-4D task. Without ICL, the objects generated exhibit spatial and temporal inconsistency, as evidenced by the multiface/foot issue in the blue jay and T-rex, and the unnatural pose of the seagull. Additionally, color discrepancies, such as the black backside of the corgi, are also noticeable. Employing either spatial or temporal ICL mitigates the multi-face issue, and notably, the use of spatial ICL also alleviates the color defect problem. Utilizing both spatial and temporal ICL concurrently yields superior results. We further perform a user study, depicted in Figure 2a, which includes results w/ and w/o ICL for 20 objects. For efficiency in evaluation, cases in which both methods fail are filtered out in this study. 20 users participate in this evaluation, and the results unveiled a preference for results w/ ICL in 75% of the cases.\nWe further explore whether ICL could alleviate multi-face problems for text-to-3D tasks. We compared the success rate of DreamFusion implemented w/ and w/o the proposed ICL loss. For the sake of fairness and rigor, we collect all prompts related to animals from the official DreamFusion project page, totaling 230. 20 users are asked to participate in this non-cherry-pick user study, where we establish three criteria for a successful generation: alignment with the text prompt, absence of multi-face issues, and clarity in geometry and texture. We visualize the statistics in Table 2b and Table 2c. The results show that although users have different understandings of successful generation, results w/ ICL always outperform results w/o it. For a comprehensive understanding, qualitative comparisons are presented in Figure 4b, which indicates the proposed technique effectively alleviates the multi-face Janus problem and thus promotes the success rate. Implementation details about text-to-3D can be found in the appendix A.4, and we also analyze the failure cases and limitations of ICL in A.5.\nCross-frame Video Enhancer In Figure 12 (see in appendix), we show the proposed cross-frame video enhancer could improve uneven color distribution and smooth out the rough edges, as shown in almost all figures, and remove some floaters, as indicated by the cat in the red and green box."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "We introduce a two-stage framework, named Consistent4D, aimed at generating 360\u25e6 4D objects from uncalibrated monocular videos captured by a stationary camera. In the first stage, we develop a Cascade DyNeRF, designed to facilitate stable training under the discrete supervisory signals provided by an image-to-image diffusion model. More crucially, we introduce an Interpolation-driven Consistency Loss to enhance spatial and temporal consistency in 4D generation tasks. In the optional second stage, we train a lightweight video enhancer to rectify scattered color discrepancies and eliminate minor floating artifacts. Comprehensive experiments conducted on both synthetic and in-the-wild data demonstrate the effectiveness of our method."
        },
        {
            "heading": "A APPENDIX",
            "text": "A.1 ADDITIONAL VISUALIZATION RESULTS\nIn Figure 5, we present the result of our method on four in-the-wild videos. For clarity, we describe the input videos as follows: robot dancing, squirrel feeding, toy-spiderman dancing, toy-rabbit deforming. Due to limited space, the reviewers are strongly recommended to watch the video in the attached files to see various visualization results.\nA.2 DATA USED IN VIDEO-TO-4D QUANTITATIVE EVALUATION\nSin three dynamic objects are shown in Figure 3, we only visualize the rest four here, as shown in Figure 6. The observation is similar to the results in the main paper. Additionally, we provide the details of quantitative comparison in Table 3\nA.3 THE NUMBER OF FRAMES\nFor simplicity, we sample each input video to 32 frames in all experiments. However, we find input videos without sampling sometimes give slightly better results, as shown in Figure 7.\nA.4 IMPLEMENTATION DETAILS\nA.4.1 CASCADE DYNERF\nInitialization We follow Maigc3D (Lin et al., 2023) to initialize the dynamic NeRF. Specifically, the blob scale and standard deviation of the density are set as 10.0 and 0.5. The activation function is softplus.\nForward propagation To help the reader get a clear understanding of the Cascade DyNeRF, we provide the pseudo-code (pytorch style) of its forward process in Listing 1.\nOptimization We optimize the Dynamic NeRF using Equation 8, where \u03bb1 = 0.1, \u03bb2 = 2500, \u03bb3 = 500, \u03bb4 = 50, 5 = 2.0, and \u03bb6 is initially 1 and increased to 20 linearly until 5000 iterations. Reconstruction loss and foreground mask, alternate with SDS loss to optimize the model. When calculating SDS loss, the guidance scale of the diffusion model is set as 5, and the maximum/minimum percent of noise added to the rendering images decreases linearly from 0.98/0.8 at the beginning of the training to 0.25/0.2 at the medium of the training, and then kept unchanged.\nA.4.2 VIDEO ENHANCER\nFor video enhancer architecture, we follow pix2pix (Isola et al., 2017) except for that we modify the unet256 architecture to a light-weighted version, with only three up/down layers and one crossframe attention layer. Our codebase for video enhancer is 1the official GitHub repository of pix2pix, and we adopt all the default settings in their code except for Unet architecture, the learning rate and the training epochs, with the last two already mentioned in the main paper. For Unet(our video enhancer), we illustrate the architecture in Figure 8 and describe it as belows. The feature dimensions for the unet layers are set as 64, 128, and 256. Besides, we inject a cross-attention layer in the inner layer of the unet to enable the current frame to query information from adjacent frames. For generating real images, we use DeepFloyd-IF stage II (dee, 2023) in an image-to-image way (Meng\na DSLR photo of a shiba inu playing golf wearing tartan\ngolf clothes and hat\na zoomed out DSLR photo of a\nkangaroo sitting on a bench\nplaying the accordion\nFigure 10: Text-to-3D failure cases.\net al., 2021) with denoising strength set as 0.35. Since this model is a diffusion model designed for single-image super-resolution, its outputs are images with improved quality yet in lack of temporal coherence. The input image, i.e., the rendered image, is resized to 64\u00d7 64 and the output resolution is 256 \u00d7 256. The prompt needed by the diffusion model is manually set, i.e., we use the \u201da \u2217\u201d as the prompt, in which \u2217 is the category of the dynamic object. For example, the prompts for dynamic objects in Figure 12 are a bird, a cat, a minions. The prompt cloud also be obtained from image or video caption models, or large language models.\nA.4.3 TEXT-TO-3D DETAILS\nWe choose Threestudio built by (Guo et al., 2023) as the codebase since it is the best public implementation we could find. DeepFloy-IF (dee, 2023) is employed as the diffusion model, and all default tricks in Threestudio are utilized. The hyper-parameters for results w/ and w/o ICL, such as batch size and learning rate, are kept consistent between the implementations w/ and w/o ICL, except for those related to ICL. We train the model for 5000 iterations, the first 1000 iterations with batch size 8 and resolution 64, and the rest 4000 with batch size 2 and resolution 256. The learning rate is 0.01 and the optimizer is Adam, the same as the default setting in Threestudio. The ICL loss is applied in the first 1000 iterations with probability 30% and weight 2000.\nA.5 FAILURE CASES\nVideo-to-4D Since the video-to-4D task in this paper is very challenging, our method actually has many failure cases. For example, we fail to generate the dynamic object when the motion is complex\nor abrupt, as shown in Figure 9. In Figure 9, the dog\u2019s tail disappears in the second image because the tail is occluded in the input image when t = t2. The frog, which is jumping up and down fast, gets blurry when t = t1.\nText-to-3D When applying ICL in text-to-3D, we find some multi-face cases that could not be alleviated, and we show them in Figure 10.\nA.6 LIMITATIONS\nAlthough the proposed method achieves promising results for 360\u25e6 dynamic object generation, our method has the following limitations: 1) Our method relies on a pre-trained diffusion model, and this limits the generalization ability of our method. Particularly, since the diffusion model adopted in this work is trained on synthetic dataset, our model might have worse performance when the input image/video is from the real-world. 2) The performance of our model relies on the quality of input video. We find when the input video is noisy, our model might not be able to generate the dynamic object in the video. 3) The training of our model costs more than 2 hours per object, and the long training time might present a challenge for practical deployment.\nclass CascadeDyNeRF:\ndef __init__(self):\nself.act_rgb = nn.Sigmoid() self.act_density = nn.Softplus() ...\ndef forward(self, rgb_feature, density_feature, **kwargs): \u2019\u2019\u2019 rgb_feaature: Tensor, [L, B, N, C], L-the number of cascade\n\u21aa\u2192 layers, B-batch size, N-the number of sampling points, C-feature dim\ndensity_feature: Tensor, shape [L, B, N, C] \u2019\u2019\u2019 # cascade features rgb_feature_cascade = torch.cumsum(rgb_feature, dim=0) density_feature_cascade = torch.cumsum(density_feature, dim=0)\n# activation rgb_feature_cascade = self.act_rgb(rgb_feature_cascade) density_feature_cascade =\n\u21aa\u2192 self.act_density(self.get_density(density_feature_cascade))\n# rendering num_cascade = rgb_feature_cascade.shape[0] rgb_img_cascade, depth_img_cascade, opacity_mask_cascade,\n\u21aa\u2192 3d_normal_cascade = [], [], [], []\nfor l in range(num_cascade): rgb_img, depth_img, opacity_mask, 3d_normal =\n\u21aa\u2192 self.renderer(rgb_feature_cascade[l], \u21aa\u2192 density_feature_cascade[l], **kwargs) # [B, H, W, 3], \u21aa\u2192 [B, H, W, 1], [B, H, W, 1], [B, N, 3]\nrgb_img_cascade.append(rgb_img) depth_img_cascade.append(depth_img) opacity_mask_cascade.append(opacity_mask) 3d_normal_cascade.append(3d_normal)\nreturn rgb_img_cascade, depth_img_cascade, opacity_mask_cascade, \u21aa\u2192 3d_normal_cascade\nListing 1: Pesudo code of Cascade DyNeRF in pytorch style"
        }
    ],
    "year": 2023
}