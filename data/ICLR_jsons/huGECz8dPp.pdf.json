{
    "abstractText": "The Information Bottleneck (IB) principle offers an information-theoretic framework for analyzing the training process of deep neural networks (DNNs). Its essence lies in tracking the dynamics of two mutual information (MI) values: between the hidden layer output and the DNN input/target. According to the hypothesis put forth by Shwartz-Ziv & Tishby (2017), the training process consists of two distinct phases: fitting and compression. The latter phase is believed to account for the good generalization performance exhibited by DNNs. Due to the challenging nature of estimating MI between high-dimensional random vectors, this hypothesis was only partially verified for NNs of tiny sizes or specific types, such as quantized NNs. In this paper, we introduce a framework for conducting IB analysis of general NNs. Our approach leverages the stochastic NN method proposed by Goldfeld et al. (2019) and incorporates a compression step to overcome the obstacles associated with high dimensionality. In other words, we estimate the MI between the compressed representations of high-dimensional random vectors. The proposed method is supported by both theoretical and practical justifications. Notably, we demonstrate the accuracy of our estimator through synthetic experiments featuring predefined MI values and comparison with MINE (Belghazi et al., 2018). Finally, we perform IB analysis on a close-to-real-scale convolutional DNN, which reveals new features of the MI dynamics.",
    "authors": [],
    "id": "SP:9c98a9d09feb9980dee9c493d17f7d258e5e714f",
    "references": [
        {
            "authors": [
                "Linara Adilova",
                "Bernhard C. Geiger",
                "Asja Fischer"
            ],
            "title": "Information plane analysis for dropout neural networks, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Rana Ali Amjad",
                "Bernhard Geiger"
            ],
            "title": "Learning representations for neural network-based classification using the information bottleneck principle",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, PP,",
            "year": 2018
        },
        {
            "authors": [
                "Rana Ali Amjad",
                "Kairen Liu",
                "Bernhard C. Geiger"
            ],
            "title": "Understanding neural networks and individual neuron importance via information-ordered cumulative ablation",
            "venue": "IEEE Transactions on Neural Networks and Learning Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Mohamed Ishmael Belghazi",
                "Aristide Baratin",
                "Sai Rajeshwar",
                "Sherjil Ozair",
                "Yoshua Bengio",
                "Aaron Courville",
                "Devon Hjelm"
            ],
            "title": "Mutual information neural estimation",
            "venue": "Proceedings of the 35th International Conference on Machine Learning,",
            "year": 2018
        },
        {
            "authors": [
                "Thomas Berrett",
                "Richard Samworth"
            ],
            "title": "Nonparametric independence testing via mutual information",
            "venue": "Biometrika, 106,",
            "year": 2017
        },
        {
            "authors": [
                "Thomas B. Berrett",
                "Richard J. Samworth",
                "Ming Yuan"
            ],
            "title": "Efficient multivariate entropy estimation via k-nearest neighbour distances",
            "venue": "Ann. Statist., 47(1):288\u2013318,",
            "year": 2019
        },
        {
            "authors": [
                "I.D. Butakov",
                "S.V. Malanchuk",
                "A.M. Neopryatnaya",
                "A.D. Tolmachev",
                "K.V. Andreev",
                "S.A. Kruglik",
                "E.A. Marshakov",
                "A.A. Frolov"
            ],
            "title": "High-dimensional dataset entropy estimation via lossy compression",
            "venue": "Journal of Communications Technology and Electronics, 66(6):764\u2013768,",
            "year": 2021
        },
        {
            "authors": [
                "Mattia Cerrato",
                "Marius K\u00f6ppel",
                "Roberto Esposito",
                "Stefan Kramer"
            ],
            "title": "Invariant representations with stochastically quantized neural networks",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence, 37:6962\u20136970,",
            "year": 2023
        },
        {
            "authors": [
                "Xi Chen",
                "Yan Duan",
                "Rein Houthooft",
                "John Schulman",
                "Ilya Sutskever",
                "Pieter Abbeel"
            ],
            "title": "Infogan: Interpretable representation learning by information maximizing generative adversarial nets",
            "venue": "Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems",
            "year": 2016
        },
        {
            "authors": [
                "Thomas M. Cover",
                "Joy A. Thomas"
            ],
            "title": "Elements of Information Theory (Wiley Series in Telecommunications and Signal Processing)",
            "year": 2006
        },
        {
            "authors": [
                "Luke Nicholas Darlow",
                "Amos Storkey"
            ],
            "title": "What information does a resnet compress",
            "year": 2020
        },
        {
            "authors": [
                "Anupam Datta",
                "Shayak Sen",
                "Yair Zick"
            ],
            "title": "Algorithmic transparency via quantitative input influence: Theory and experiments with learning systems",
            "venue": "In 2016 IEEE Symposium on Security and Privacy (SP),",
            "year": 2016
        },
        {
            "authors": [
                "Adar Elad",
                "Doron Haviv",
                "Yochai Blau",
                "Tomer Michaeli"
            ],
            "title": "Direct validation of the information bottleneck principle for deep nets",
            "venue": "IEEE/CVF International Conference on Computer Vision Workshop (ICCVW),",
            "year": 2019
        },
        {
            "authors": [
                "Zhun Fan",
                "Jiahong Wei",
                "Guijie Zhu",
                "Jiajie Mo",
                "Wenji Li"
            ],
            "title": "Evolutionary neural architecture search for retinal vessel segmentation, 2020",
            "year": 2020
        },
        {
            "authors": [
                "Charles Fefferman",
                "Sanjoy Mitter",
                "Hariharan Narayanan"
            ],
            "title": "Testing the manifold hypothesis",
            "venue": "Journal of the American Mathematical Society, 29,",
            "year": 2013
        },
        {
            "authors": [
                "Marylou Gabri\u00e9",
                "Andre Manoel",
                "Cl\u00e9ment Luneau",
                "jean barbier",
                "Nicolas Macris",
                "Florent Krzakala",
                "Lenka Zdeborov\u00e1"
            ],
            "title": "Entropy and mutual information in models of deep neural networks",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Bernhard C. Geiger"
            ],
            "title": "On information plane analyses of neural network classifiers\u2014a review",
            "venue": "IEEE Transactions on Neural Networks and Learning Systems, 33(12):7039\u20137051,",
            "year": 2022
        },
        {
            "authors": [
                "Z. Goldfeld",
                "K. Greenewald",
                "J. Niles-Weed",
                "Y. Polyanskiy"
            ],
            "title": "Convergence of smoothed empirical measures with applications to entropy estimation",
            "venue": "IEEE Transactions on Information Theory,",
            "year": 2020
        },
        {
            "authors": [
                "Ziv Goldfeld",
                "Ewout van den Berg",
                "Kristjan H. Greenewald",
                "Igor V. Melnyk",
                "Nam H. Nguyen",
                "Brian Kingsbury",
                "Yury Polyanskiy"
            ],
            "title": "Estimating information flow in deep neural networks",
            "year": 2019
        },
        {
            "authors": [
                "Kristjan H. Greenewald",
                "Brian Kingsbury",
                "Yuancheng Yu"
            ],
            "title": "High-dimensional smoothed entropy estimation via dimensionality reduction",
            "venue": "In IEEE International Symposium on Information Theory, ISIT 2023,",
            "year": 2023
        },
        {
            "authors": [
                "Wa He",
                "Yuting Wu",
                "Peng Liang",
                "Gang Hao"
            ],
            "title": "Using darts to improve mold id recognition model based on mask r-cnn",
            "venue": "Journal of Physics: Conference Series, 1518:012042,",
            "year": 2020
        },
        {
            "authors": [
                "Jean-Yves Audibert"
            ],
            "title": "Intrinsic dimensionality estimation of submanifolds in r",
            "venue": "In Proceedings of the 22nd international conference on Machine learning,",
            "year": 2005
        },
        {
            "authors": [
                "G.E. Hinton",
                "R.R. Salakhutdinov"
            ],
            "title": "Reducing the dimensionality of data with neural networks",
            "venue": "Science, 313(5786):504\u2013507,",
            "year": 2006
        },
        {
            "authors": [
                "Maksims Ivanovs",
                "Roberts Kadikis",
                "Kaspars Ozols"
            ],
            "title": "Perturbation-based methods for explaining deep neural networks: A survey",
            "venue": "Pattern Recognition Letters,",
            "year": 2021
        },
        {
            "authors": [
                "Hlynur J\u00f3nsson",
                "Giovanni Cherubini",
                "Evangelos Eleftheriou"
            ],
            "title": "Convergence behavior of dnns with mutual-information-based",
            "venue": "regularization. Entropy,",
            "year": 2020
        },
        {
            "authors": [
                "Diederik P. Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A method for stochastic optimization, 2017",
            "year": 2017
        },
        {
            "authors": [
                "Andreas Kirsch",
                "Clare Lyle",
                "Yarin Gal"
            ],
            "title": "Unpacking information bottlenecks: Unifying informationtheoretic objectives in deep learning, 2021",
            "year": 2021
        },
        {
            "authors": [
                "L.F. Kozachenko",
                "N.N. Leonenko"
            ],
            "title": "Sample estimate of the entropy of a random vector",
            "venue": "Problems Inform. Transmission,",
            "year": 1987
        },
        {
            "authors": [
                "Mark A. Kramer"
            ],
            "title": "Nonlinear principal component analysis using autoassociative neural networks",
            "venue": "AIChE Journal,",
            "year": 1991
        },
        {
            "authors": [
                "Yann LeCun",
                "Corinna Cortes",
                "CJ Burges"
            ],
            "title": "Mnist handwritten digit database",
            "venue": "ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist,",
            "year": 2010
        },
        {
            "authors": [
                "Hanxiao Liu",
                "Karen Simonyan",
                "Yiming Yang"
            ],
            "title": "Darts: Differentiable architecture",
            "venue": "search. ArXiv,",
            "year": 2019
        },
        {
            "authors": [
                "Stephan Sloth Lorenzen",
                "Christian Igel",
                "Mads Nielsen"
            ],
            "title": "Information bottleneck: Exact analysis of (quantized) neural networks",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "David McAllester",
                "Karl Stratos"
            ],
            "title": "Formal limitations on the measurement of mutual information",
            "venue": "Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics,",
            "year": 2020
        },
        {
            "authors": [
                "Youssef Mroueh",
                "Igor Melnyk",
                "Pierre Dognin",
                "Jarret Ross",
                "Tom Sercu"
            ],
            "title": "Improved mutual information estimation",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Morteza Noshad",
                "Yu Zeng",
                "Alfred O. Hero"
            ],
            "title": "Scalable mutual information estimation using dependence graphs",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2019
        },
        {
            "authors": [
                "Hieu Pham",
                "Melody Y. Guan",
                "Barret Zoph",
                "Quoc V. Le",
                "Jeff Dean"
            ],
            "title": "Efficient neural architecture search via parameter sharing",
            "year": 2018
        },
        {
            "authors": [
                "Ben Poole",
                "Sherjil Ozair",
                "Aaron Van Den Oord",
                "Alex Alemi",
                "George Tucker"
            ],
            "title": "On variational bounds of mutual information",
            "venue": "Proceedings of the 36th International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Danilo Jimenez Rezende",
                "Shakir Mohamed"
            ],
            "title": "Variational inference with normalizing flows",
            "venue": "In Proceedings of the 32nd International Conference on International Conference on Machine Learning - Volume 37,",
            "year": 2015
        },
        {
            "authors": [
                "Marco Ribeiro",
                "Sameer Singh",
                "Carlos Guestrin"
            ],
            "title": "why should i trust you?\": Explaining the predictions of any classifier",
            "venue": "Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,",
            "year": 2016
        },
        {
            "authors": [
                "Ramprasaath Rs",
                "Michael Cogswell",
                "Abhishek Das",
                "Ramakrishna Vedantam",
                "Devi Parikh",
                "Dhruv Batra"
            ],
            "title": "Grad-cam: Visual explanations from deep networks via gradient-based localization",
            "venue": "International Journal of Computer Vision, 128,",
            "year": 2020
        },
        {
            "authors": [
                "Stephan R. Sain"
            ],
            "title": "Adaptive kernel density estimation",
            "venue": "PhD thesis, Rice University,",
            "year": 1994
        },
        {
            "authors": [
                "Abdolreza Sayyareh"
            ],
            "title": "A new upper bound for kullback-leibler divergence",
            "venue": "Applied Mathematical Sciences, 5:3303\u20133317,",
            "year": 2011
        },
        {
            "authors": [
                "Rajat Sen",
                "Ananda Theertha Suresh",
                "Karthikeyan Shanmugam",
                "Alexandros G Dimakis",
                "Sanjay Shakkottai"
            ],
            "title": "Model-powered conditional independence test",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Ravid Shwartz-Ziv",
                "Naftali Tishby"
            ],
            "title": "Opening the black box of deep neural networks via information, 2017",
            "year": 2017
        },
        {
            "authors": [
                "Jost Tobias Springenberg",
                "Alexey Dosovitskiy",
                "Thomas Brox",
                "Martin Riedmiller"
            ],
            "title": "Striving for simplicity: The all convolutional net",
            "year": 2015
        },
        {
            "authors": [
                "Nitish Srivastava",
                "Geoffrey Hinton",
                "Alex Krizhevsky",
                "Ilya Sutskever",
                "Ruslan Salakhutdinov"
            ],
            "title": "Dropout: A simple way to prevent neural networks from overfitting",
            "venue": "Journal of Machine Learning Research,",
            "year": 2014
        },
        {
            "authors": [
                "Thomas Steinke",
                "Lydia Zakynthinou"
            ],
            "title": "Reasoning About Generalization via Conditional Mutual Information",
            "venue": "Proceedings of Thirty Third Conference on Learning Theory, volume 125 of Proceedings of Machine Learning Research,",
            "year": 2020
        },
        {
            "authors": [
                "Thanh Tang Nguyen",
                "Jaesik Choi"
            ],
            "title": "Markov information bottleneck to improve information flow in stochastic neural networks. Entropy",
            "venue": "doi: 10.3390/e21100976. URL https://www.mdpi.com/1099-4300/21/10/976",
            "year": 2019
        },
        {
            "authors": [
                "Naftali Tishby",
                "Noga Zaslavsky"
            ],
            "title": "Deep learning and the information bottleneck principle",
            "venue": "IEEE Information Theory Workshop (ITW), pp",
            "year": 2015
        },
        {
            "authors": [
                "Naftali Tishby",
                "Fernando C. Pereira",
                "William Bialek"
            ],
            "title": "The information bottleneck method",
            "venue": "In Proc. of the 37-th Annual Allerton Conference on Communication, Control and Computing,",
            "year": 1999
        },
        {
            "authors": [
                "Berwin Turlach"
            ],
            "title": "Bandwidth selection in kernel density estimation: A review",
            "venue": "Technical Report,",
            "year": 1999
        },
        {
            "authors": [
                "Stanislaw Weglarczyk"
            ],
            "title": "Kernel density estimation and its application",
            "venue": "ITM Web of Conferences,",
            "year": 2018
        },
        {
            "authors": [
                "Kristoffer Wickstr\u00f8m",
                "Sigurd L\u00f8kse",
                "Michael Kampffmeyer",
                "Shujian Yu",
                "Jose Principe",
                "Robert Jenssen"
            ],
            "title": "Information plane analysis of deep neural networks via matrix-based renyi\u2019s entropy and tensor kernels, 2019",
            "year": 2019
        },
        {
            "authors": [
                "Bichen Wu",
                "Xiaoliang Dai",
                "Peizhao Zhang",
                "Yanghan Wang",
                "Fei Sun",
                "Yiming Wu",
                "Yuandong Tian",
                "P\u00e9ter Vajda",
                "Yangqing Jia",
                "Kurt Keutzer"
            ],
            "title": "Fbnet: Hardware-aware efficient convnet design via differentiable neural architecture",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2019
        },
        {
            "authors": [
                "Aolin Xu",
                "Maxim Raginsky"
            ],
            "title": "Information-theoretic analysis of generalization capability of learning algorithms",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Shujian Yu",
                "Kristoffer Wickstr\u00f8m",
                "Robert Jenssen",
                "Jos\u00e9 C. Pr\u00edncipe"
            ],
            "title": "Understanding convolutional neural networks with information theory: An initial exploration",
            "venue": "IEEE Transactions on Neural Networks and Learning Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Erik \u0160trumbelj",
                "Igor Kononenko"
            ],
            "title": "Explaining prediction models and individual predictions with feature contributions",
            "venue": "Knowledge and Information Systems, 41:647\u2013665,",
            "year": 2013
        },
        {
            "authors": [
                "Butakov"
            ],
            "title": "The first article focuses on entropy estimation via lossy compression. The main theoretical result of the paper in question is the following upper bound of random vector entropy: Statement 10 (Butakov et al. (2021))",
            "year": 2023
        },
        {
            "authors": [
                "Greenewald"
            ],
            "title": "Let X be a random vector of dimension n, let Z",
            "year": 2023
        },
        {
            "authors": [
                "Poole"
            ],
            "title": "2019) it has been shown that other complex parametric NN-based estimators (NJW, JS, InfoNCE, etc.) exhibit poor performance during the estimation of MI between a pair of 20-dimensional incompressible (i.e., not lying along a manifold) synthetic vectors",
            "year": 2019
        },
        {
            "authors": [
                "Goldfeld"
            ],
            "title": "As the mutual information estimation is tightly connected to the entropy estimation, this problem also manifests itself in our task. Although this difficulty affects every MI estimator, classical estimators may be assumed to be more prone to the curse of dimensionality, as they are usually too basic to grasp a manifold-like low-dimensional structure of high-dimensional data",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "The information-theoretic analysis of deep neural networks (DNNs) is a developing branch of the deep learning theory, which may provide a robust and interpretable way to measure the performance of deep models during training and inference. This type of analysis might complement current non-transparent meta-optimization algorithms for architecture search, like ENAS (Pham et al., 2018), DARTS (Liu et al., 2019; Wu et al., 2019; He et al., 2020), evolutionary algorithms (Fan et al., 2020), and others. This method may also provide new approaches to explainable AI via estimation of information flows in NNs (Tishby & Zaslavsky, 2015; Xu & Raginsky, 2017; Goldfeld et al., 2019; Steinke & Zakynthinou, 2020; Amjad et al., 2022) or via independence testing (Berrett & Samworth, 2017; Sen et al., 2017), as opposed to existing methods of local analysis of a model (Ribeiro et al., 2016; Springenberg et al., 2015; Rs et al., 2020; Ivanovs et al., 2021) or methods based on complex manipulations with data (Lipovetsky & Conklin, 2001; \u0160trumbelj & Kononenko, 2013; Datta et al., 2016). Information-theoretic quantities can also be considered as regularization terms or training objectives (Tishby & Zaslavsky, 2015; Chen et al., 2016; Belghazi et al., 2018).\nThe information-theoretic analysis of DNNs relies on the Information Bottleneck (IB) principle proposed in Tishby et al. (1999). This concept was later developed in Tishby & Zaslavsky (2015) and applied to DNNs in Shwartz-Ziv & Tishby (2017). The major idea of the IB approach is to track the dynamics of two mutual information (MI) values: I(X;L) between the hidden layer output (L) and the DNN input (X) and I(Y ;L) between the hidden layer output and the target of the model (Y ). As a result of the IB analysis, the authors of the latter article put forth the so-called fitting-compression hypothesis, which states that the training process consists of two phases: a feature-extraction \u201cfitting\u201d phase (both MI values grow) and a representation compression phase (I(Y ;L) grows while I(X;L) decreases). The authors conjectured the compression phase to account for the good generalization performance exhibited by DNNs. However, it is still debated whether empirical confirmations of the compression phase are related to improper mutual information estimators, activation function choice,\nor other implementation details. For a more complete overview of current IB-related problems, we refer the reader to Geiger (2022).\nIn the original work by Shwartz-Ziv & Tishby (2017), a quantization (or binning) approach was proposed to estimate MI. However, this approach encountered two primary challenges. Firstly, the MI estimate was highly sensitive to the bin size selection. Secondly, for a fixed training epoch, when the training weights are held constant, L becomes a deterministic function of X , resulting in the MI being independent of the DNN parameters (and infinite for practically all regimes of interest if we speak about continuous case and reasonable activation functions, see e.g., Amjad & Geiger (2018)). The subsequent papers addressed the aforementioned problems. To tackle the infinite MI problem it was proposed to consider (a) stochastic NNs (Goldfeld et al., 2019; Tang Nguyen & Choi, 2019; Adilova et al., 2023), (b) quantized NNs (Lorenzen et al., 2022) or (c) a mixture of them (Cerrato et al., 2023). Simple and inconsistent binning entropy estimators have been replaced with estimators more appropriate for continuous random variables (Gabri\u00e9 et al., 2018; Goldfeld et al., 2019; Goldfeld et al., 2020; Adilova et al., 2023).\nHowever, the high-dimensional problem still holds, as the sample complexity (the least number of samples required for an estimation within a fixed additive gap) of any entropy estimator is proven to depend on the dimension exponentially (Goldfeld et al., 2020; McAllester & Stratos, 2020). Due to the challenging nature of estimating MI between high-dimensional random vectors, the fittingcompression hypothesis has only been verified for tiny NNs or special classes of models with tractable information-theoretic quantities (e.g., Gabri\u00e9 et al. (2018); Lorenzen et al. (2022)). Some existing works on IB-analysis of large networks also exhibit signs of the curse of dimensionality (Goldfeld et al., 2019; Adilova et al., 2023). We mention papers that suggest using lower bounds or other surrogate objectives (Belghazi et al., 2018; Elad et al., 2019; Poole et al., 2019; Darlow & Storkey, 2020; J\u00f3nsson et al., 2020; Kirsch et al., 2021; McAllester & Stratos, 2020), advanced binning (Noshad et al., 2019) or even other definitions of entropy (Wickstr\u00f8m et al., 2019; Yu et al., 2021) in order to perform IB-analysis of large networks. It may be assumed that these methods can partially overcome the curse of dimensionality via utilizing the internal data structure implicitly, or simply from the fact that non-conventional information theory might be less prone to the curse of dimensionality.\nIn contrast to the approaches mentioned above, we propose a solution to the curse of dimensionality problem by explicitly compressing the data. Since most datasets exhibit internal structure (according to the manifold hypothesis (Fefferman et al., 2013)), it is usually sufficient to estimate informationtheoretic quantities using compressed or latent representations of the data. This enables the application of conventional and well-established information-theoretic approaches to real-world machine learning problems. In the recent work of Butakov et al. (2021), the compression was used to obtain the upper bound of the random vector entropy. However, it is necessary to precisely estimate or at least bound from both sides the entropy alternation under compression in order to derive the MI estimate. In the work of Greenewald et al. (2023), two-sided bounds are obtained, but only in the special case of linear compression and smoothed distributions. Our work heavily extends these ideas by providing new theoretical statements and experimental results for MI estimation via compression-based entropy estimation. We stress out the limitations of the previous approaches more thoroughly in the Appendix.\nOur contribution is as follows. We introduce a comprehensive framework for conducting IB analysis of general NNs. Our approach leverages the stochastic NN method proposed in Goldfeld et al. (2019) and incorporates a compression step to overcome the obstacles associated with high dimensionality. In other words, we estimate the MI between the compressed representations of high-dimensional random vectors. We provide a theoretical justification of MI estimation under lossless and lossy compression. The accuracy of our estimator is demonstrated through synthetic experiments featuring predefined MI values and comparison with MINE (Belghazi et al., 2018). Finally, the experiment with convolutional DNN classifier of the MNIST handwritten digits dataset (LeCun et al., 2010) is performed. The experiment shows that there may be several compression/fitting phases during the training process. It may be concluded that phases revealed by information plane plots are connected to different regimes of learning (i.e. accelerated, stationary, or decelerated drop of loss function).\nIt is important to note that stochastic NNs serve as proxies for analyzing real NNs. This is because injecting small amounts of noise have negligible effects on outputs of layers, and the introduced randomness allows for reasonable estimation of information-theoretic quantities that depend on NN parameters. We also mention that injecting noise during training is provenerformance and generalization capabilities (Hinton et al., 2012; Srivastava et al., 2014).\nThe paper is organized as follows. In Section 2, we provide the necessary background and introduce key concepts used throughout the paper. Section 3 describes our proposed approach for estimating mutual information under compression, along with theoretical justifications and bounds. In Section 4, we develop a general framework for testing mutual information estimators on synthetic datasets. This framework is utilized in Section 5 to evaluate MINE and four selected mutual information estimators, complemented by the proposed compression step. The best-performing method is then applied in Section 6 to perform information plane analysis on a convolutional NN classifier trained on the MNIST dataset. Finally, the results are discussed in Section 7. We provide all the proofs in the Appendix, as well as discussion of state-of-the-art methods other than MINE."
        },
        {
            "heading": "2 PRELIMINARIES",
            "text": "Consider random vectors, denoted as X : \u2126 \u2192 Rn and Y : \u2126 \u2192 Rm, where \u2126 represents the sample space. Let us assume that these random vectors have absolutely continuous probability density functions (PDF) denoted as \u03c1(x), \u03c1(y), and \u03c1(x, y), respectively, where the latter refers to the joint PDF. The differential entropy of X is defined as follows\nh(X) = \u2212E log \u03c1(x) = \u2212 \u222b\nsupp X\n\u03c1(x) log \u03c1(x) dx,\nwhere suppX \u2286 Rn represents the support of X , and log(\u00b7) denotes the natural logarithm. Similarly, we define the joint differential entropy as h(X,Y ) = \u2212E log \u03c1(x, y) and conditional differential entropy as h(X | Y ) = \u2212E log \u03c1 (X|Y ) = \u2212EY ( EX|Y=y log \u03c1(X | Y = y) ) . Finally, the mutual information (MI) is given by I(X;Y ) = h(X)\u2212 h(X | Y ), and the following equivalences hold I(X;Y ) = h(X)\u2212 h(X | Y ) = h(Y )\u2212 h(Y | X), (1)\nI(X;Y ) = h(X) + h(Y )\u2212 h(X,Y ). (2)\nNote that suppX or suppY may have measure zero, indicating a singular distribution. In such cases, if the supports are manifolds, PDFs can be treated as induced probability densities, and dx and dy can be seen as area elements of the corresponding manifolds. Hence, all the previous definitions remain valid.\nAn alternative definition of MI can be given using the Kullback-Leibler divergence DKL( \u00b7 ).\nI(X;Y ) = \u222b \u03c1(x, y) log \u03c1(x, y)\n\u03c1(x)\u03c1(y) dxdy = DKL(\u03c1(x, y)||\u03c1(x)\u03c1(y)).\nIn the following discussion, we make use of an important property of MI, which is its invariance under nonsingular mappings between smooth manifolds. In the next statement we show that the MI can be measured between compressed representations of random vectors. Statement 1. Let \u03be : \u2126 \u2192 Rn\u2032 be an absolutely continuous random vector, and let f : Rn\u2032 \u2192 Rn be an injective piecewise-smooth mapping with Jacobian Jf ,satisfying n \u2265 n\u2032 and det ( JTf Jf ) \u0338= 0\nalmost everywhere. Let either \u03b7 be a discrete random variable, or (\u03be, \u03b7) be an absolutely continuous random vector. Then I(\u03be; \u03b7) = I (f(\u03be); \u03b7) (3) Remark 1. In what follows by \u03be : \u2126 \u2192 Rn\u2032 we denote the compressed representation of X , n\u2032 \u2264 n.\nRecall that we utilize the stochastic neural network (NN) approach to address the problem of infinite mutual information I(X; f(X)) for a deterministic mapping f . As demonstrated in Goldfeld et al. (2019), introducing stochasticity enables proper MI estimation between layers of the network. The stochastic modification of a network serves as a proxy to determine the information-theoretic properties of the original model.\nA conventional feedforward NN can be defined as an acyclic computational graph that can be topologically sorted:\nL0 \u225c X, L1 := f1(L0), L2 := f2(L0, L1), . . . , Y\u0302 \u225c Ln := fn(L0, . . . , Ln\u22121),\nwhere L0, . . . , Ln denote the outputs of the network\u2019s layers. The stochastic modification is defined similarly, but using the Markov chain stochastic model:\nDefinition 1. The sequence of random vectors L0, . . . , Ln is said to form a stochastic neural network with input X and output Y\u0302 , if L0 \u225c X , Y\u0302 \u225c Ln, and\nL0 \u2212\u2192 (L0, L1) \u2212\u2192 . . . \u2212\u2192 (L0, . . . , Ln) is a Markov chain; Lk represents outputs of the k-th layer of the network.\nOur primary objective is to track I(Li;Lj) during the training process. In the subsequent sections, we assume the manifold hypothesis to hold for X . In such case, under certain additional circumstances (continuity of fk, small magnitude of injected stochasticity) this hypothesis can also be assumed for Lk, thereby justifying the proposed method."
        },
        {
            "heading": "3 MUTUAL INFORMATION ESTIMATION VIA COMPRESSION",
            "text": "In this section, we explore the application of lossless and lossy compression to estimation of MI between high-dimensional random vectors. We mention the limitations of conventional MI estimators, propose and theoretically justify a complementary lossy compression step to address the curse dimensionality, and derive theoretical bounds on the MI estimate under lossy compression."
        },
        {
            "heading": "3.1 MUTUAL INFORMATION ESTIMATION",
            "text": "Let {(xk, yk)}Nk=1 be a sequence of samples from the joint distribution of random vectors X and Y . Our goal is to estimate the mutual information between X and Y , denoted as I(X;Y ), based on these samples. The most straightforward way to achieve this is to estimate all the components in equation 1 or equation 2 via entropy estimators. More advanced methods of MI estimation, like MINE (Belghazi et al., 2018), are also applicable. However, according to Theorem 1 in Goldfeld et al. (2020) and Theorem 4.1 in McAllester & Stratos (2020), sample complexity of any entropy (and MI) estimator is exponential in dimension. We show that this obstacle can be overcome if data possesses low-dimensional internal structure.\nIn our work, we make the assumption of the manifold hypothesis (Fefferman et al., 2013), which posits that data lie along or close to some manifold in multidimensional space. This hypothesis is believed to hold for a wide range of structured data, and there are datasets known to satisfy this assumption precisely (e.g., photogrammetry datasets, as all images are parametrized by camera position and rotation). In our study, we adopt a simplified definition of the manifold hypothesis: Definition 2. A random vector X : \u2126 \u2192 Rn strictly satisfies the manifold hypothesis iff there exist \u03be : \u2126 \u2192 Rn\u2032 and f : Rn\u2032 \u2192 Rn satisfying the conditions of Statement 1, such that X = f(\u03be). A random vector X \u2032 : \u2126 \u2192 Rn loosely satisfies the manifold hypothesis iff X \u2032 = X + Z, where X strictly satisfies the manifold hypothesis, and Z is insignificant in terms of some metric.\nTo overcome the curse of dimensionality, we propose learning the manifold with autoencoders (Kramer, 1991; Hinton & Salakhutdinov, 2006) and applying conventional estimators to the compressed representations.To address the issue of measure-zero support, we consider the probability measure induced on the manifold.\nLet us consider an absolutely continuous X , compressible via autoencoder A = D \u25e6 E. Corollary 1. Let E\u22121 : Rn\u2032 \u2287 E(suppX) \u2192 Rn and E(X) : \u2126 \u2192 Rn\u2032 exist, let (E\u22121 \u25e6 E)(X) \u2261 X , let X , Y , E(X) and E\u22121 satisfy conditions of the Statement 1. Then\nI(X;Y ) = I(E(X);Y ),\nIn case of absolutely continuous (X,Y ), the mutual information estimate can be defined as follows:\nI\u0302(X;Y ) \u225c h\u0302(E(X)) + h\u0302(Y )\u2212 h\u0302(E(X), Y ) (4) In case of absolutely continuous X and discrete Y , it is impractical to use equation 2, as the (induced) joined probability distribution is neither absolutely continuous nor discrete. However, equation 1 is still valid:\nh(X | Y ) = \u2211\ny\u2208supp Y pY (y) \u00b7\n[ \u2212 \u222b \u03c1X(x | Y = y) log (\u03c1X(x | Y = y)) dx ]\n\ufe38 \ufe37\ufe37 \ufe38 h(X|Y=y)\nProbabilities pY can be estimated using empirical frequencies: p\u0302Y (y) = 1N \u00b7 |{k | yk = y}|. Conditional entropy h(X | Y = y) can be estimated using corresponding subsets of {xk}Nk=1: h\u0302(X | Y = y) = h\u0302({xk | yk = y}). The mutual information estimate in this case can be defined as follows:\nI\u0302(X;Y ) \u225c h\u0302(E(X))\u2212 \u2211\ny\u2208supp Y p\u0302Y (y) \u00b7 h\u0302(E(X) | Y = y) (5)\nAccording to the strong law of large numbers, p\u0302 a.s.\u2212\u2192 p. That is why the convergence of the proposed MI estimation methods solely relies on the convergence of the entropy estimator used in equation 4 and equation 5. Note that this method can be obviously generalized to account for compressible Y ."
        },
        {
            "heading": "3.2 BOUNDS FOR MUTUAL INFORMATION ESTIMATE",
            "text": "It can be shown that it is not possible to derive non-trivial bounds for I(E(X);Y ) in general case if the conditions of Corollary 1 do not hold. Let us consider a simple linear autoencoder that is optimal in terms of mean squared error, such as principal component analysis-based autoencoder. The following statement demonstrates cases where the proposed method of estimating mutual information through lossy compression fails. Statement 2. For any given \u03ba \u2265 0 there exist random vectors X : \u2126 \u2192 Rn, Y : \u2126 \u2192 Rm, and a non-trivial linear autoencoder A = D\u25e6E with latent space dimension n\u2032 < n that is optimal in terms of minimizing mean squared error E \u2225X \u2212A(X)\u22252, such that I(X;Y ) = \u03ba and I(E(X);Y ) = 0.\nThis statement demonstrates that an arbitrary amount of information can be lost through compression of the data. It arises from the fact that \u201cless significant\u201d in terms of metric spaces does not align with \u201cless significant\u201d in terms of information theory. However, with additional assumptions, a more useful theoretical result can be obtained. Statement 3. Let X , Y , and Z be random variables such that I(X;Y ) and I ((X,Z);Y ) are defined. Let f be a function of two arguments such that I(f(X,Z);Y ) is defined. If there exists a function g such that X = g(f(X,Z)), then the following chain of inequalities holds:\nI(X;Y ) \u2264 I (f(X,Z);Y ) \u2264 I((X,Z);Y ) \u2264 I(X;Y ) + h(Z)\u2212 h(Z | X,Y )\nIn this context, f(X,Z) can be interpreted as compressed noisy data, X as denoised data, and g as a perfect denoising decoder. The term h(Z) can be upper-bounded via entropy of Gaussian distribution of the same variance, h(Z | X,Y ) can be lower-bounded in special cases (e.g., when Z is a sum of independent random vectors, at least one of which is of finite entropy); see Section B of the Appendix for details. We also note the special case where the data lost by compression can be considered as independent random noise. Corollary 2. Let X , Y , Z, f , and g satisfy the conditions of the Statement 3. Let also random variables (X,Y ) and Z be independent. Then I(X;Y ) = I (f(X,Z);Y ).\nWe note that (a) the presented bounds cannot be further improved unless additional assumptions are made (e.g., linearity of f in (Greenewald et al., 2023)); (b) additional knowledge about the connection between X , Y , and Z is required to properly utilize the bounds. Other bounds can also be derived (Sayyareh, 2011; Belghazi et al., 2018; Poole et al., 2019), but they do not take advantage of the compression aspect.\nThe provided theoretical analysis and additional results from Section B of the Appendix show that the proposed method allows for tracking the true value of MI within the errors of a third-party estimator ran on compressed data and the derived bounds imposed by the compression itself."
        },
        {
            "heading": "4 SYNTHETIC DATASET GENERATION",
            "text": "In order to test the proposed mutual information estimator, we developed an universal method for synthetic dataset generation with defined information-theoretic properties. This method yields two\nrandom vectors, X and Y , with a predefined value of mutual information I(X;Y ). The method requires X and Y to be images of normally distributed vectors under known nonsingular smooth mappings. The generation consists of two steps. First, a normal vector (\u03be, \u03b7) \u223c N (0,M) is considered, where \u03be \u223c N (0, In\u2032), \u03b7 \u223c N (0, Im\u2032), and n\u2032, m\u2032 are dimensions of \u03be and \u03b7, respectively. The covariance matrix M is chosen to satisfy I(\u03be; \u03b7) = \u03ba, where \u03ba is an arbitrary non-negative constant. Statement 4. For every \u03ba \u2265 0 and every n\u2032,m\u2032 \u2208 N exists a matrix M \u2208 R(n\u2032+m\u2032)\u00d7(n\u2032+m\u2032) such that (\u03be, \u03b7) \u223c N (0,M), \u03be \u223c N (0, In\u2032), \u03b7 \u223c N (0, Im\u2032) and I(\u03be; \u03b7) = \u03ba.\nAfter generating the correlated normal random vectors (\u03be, \u03b7) with the desired mutual information, we apply smooth nonsingular mappings to obtain X = f(\u03be) and Y = g(\u03b7). According to Statement 1, this step preserves the mutual information, so I(\u03be; \u03b7) = I(X;Y )."
        },
        {
            "heading": "5 COMPARISON OF THE ENTROPY ESTIMATORS",
            "text": "The MI estimate is acquired according to Subsection 3.1. To estimate the entropy terms in equation 1 or equation 2, we leverage conventional entropy estimators, such as kernel density-based (Turlach, 1999; Sayyareh, 2011; Sain, 1994) and Kozachenko-Leonenko estimators (original Kozachenko & Leonenko (1987) and weighted Berrett et al. (2019) versions). To test the accuracy of these approaches, we use datasets sampled from synthetic random vectors with known MI. We generate these datasets in accordance with Section 4.\nTo examine the impact of the compression step proposed in Subsection 3.1, we utilize a special type of synthetic datasets. Synthetic data lies on a manifold of small dimension. This is achieved by generating a low-dimensional dataset and then embedding it into a high-dimensional space by a smooth mapping (so the Statement 1 can be applied). Then, the acquired datasets are compressed via autoencoders. Finally, the obtained results are fed into a mutual information estimator.\nAlgorithm 1 and Figure 2 describe the proposed mutual information estimation quality measurement. We run several experiments with f and g mapping normal distributions to rasterized images of geometric shapes (e.g., rectangles) or 2D plots of smooth functions (e.g., Gaussian functions).1 The results are presented in Figures 3 and 4. The blue and green curves correspond to the estimates of MIs marked by the corresponding colors in Figure 2. Thus, we see that the compression step does not lead to bad estimation accuracy, especially for the weighted Kozachenko-Leonenko (WKL) estimator, which demonstrates the best performance. Note that we do not plot estimates for uncompressed data, as all the four tested classical estimators completely fail to correctly estimate MI for such high dimensions; for more information, we refer to Section E.3 in the Appendix. We also conduct experiments with MINE (without compression), for which we train the critic network of the same complexity, as we use for the autoencoder.\nAlgorithm 1 Measure mutual information estimation quality on high-dimensional synthetic datasets 1: Generate two datasets of samples from normal vectors \u03be and \u03b7 with given mutual information as\ndescribed in section 4 \u2013 {(xk, yk)}Nk=1. 2: Choose functions f and g satisfying conditions of the Statement 1 (so I(\u03be; \u03b7) = I(f(\u03be); g(\u03b7)))\nand obtain datasets for f(\u03be) and g(\u03b7) \u2013 {f(xk)}Nk=1, {g(yk)}Nk=1. 3: Train autoencoders AX = DX \u25e6 EX , AY = DY \u25e6 EY on {f(xk)}, {g(yk)} respectively. 4: Obtain datasets for (EX \u25e6 f)(\u03be) and (EY \u25e6 g)(\u03b7).\nWe assume that EX , EY satisfy conditions of the Corollary 1, so we expect I(\u03be; \u03b7) = I(f(\u03be); g(\u03b7)) = I ( (EX \u25e6 f)(\u03be); (EY \u25e6 g)(\u03b7) ) 5: Estimate I((EX \u25e6 f)(\u03be); (EY \u25e6 g)(\u03b7) ) and compare the estimated value with the exact one.\n1Due to the high complexity of the used f and g, we do not define these functions in the main text; instead, we refer to the source code published along with the paper."
        },
        {
            "heading": "6 INFORMATION FLOW IN DEEP NEURAL NETWORKS",
            "text": "This section is dedicated to the information flow estimation in DNNs via the proposed method. We estimate the information flow in a convolutional classifier of the MNIST handwritten digits dataset. This neural network is simple enough to be quickly trained and tested, but at the same time, is complex enough to suffer from the curse of dimensionality. The dataset consists of images of size 28\u00d7 28 = 784 pixels. It was shown in Hein & Audibert (2005) that these images have a relatively low latent space dimension, approximately 12\u201313. If the preservation of only the main features is desired, the latent space can even be narrowed down to 3\u201310. Although the proposed experimental setup is nowadays considered to be toy and small, it is still problematic for the IB-analysis, as it was shown in Goldfeld et al. (2019).\nIt can be concluded from the previous section that the weighted Kozachenko-Leonenko estimator is superior to the other methods tested in this paper. That is why it is used in experiments with the DNN classifier described in the current section. The analyzed network is designed to return the output of every layer. To avoid the problem of a deterministic relationship between input and output, we apply Gaussian dropout with a small variance after each layer. This allows for the better generalization during the training (Srivastava et al., 2014) and finite values of MI during the IB-analysis (Adilova et al., 2023). Lossy compression of input images X is performed via a convolutional autoencoder with a latent dimension of dlatentX . Lossy compression of layer outputs Li is performed via principal component analysis with dlatentLi as the number of principal components, as it showed to be faster and not significantly worse than general AE approach in this particular case. The general algorithm is described in Algorithm 2.\nAlgorithm 2 Estimate information flow in the neural network during training\n1: Compress the input dataset {xk}Nk=1 via the input encoder EX : cXk = EX(xk). 2: for epoch : 1, . . . , number of epochs do 3: for Li : layers do 4: Collect outputs of the layer Li: yLik = Li(xk). Each layer must be noisy/stochastic. 5: Compress the outputs via the layer encoder ELi : c Li k = ELi(y Li k ). 6: Estimate I(EX(X);ELi(Li)) and I(ELi(Li);Y (X)), where Y maps inputs to true targets. 7: end for 8: Perform one-epoch training step of the network. 9: end for\nL1: Conv2d(1, 8, ks=3), LeakyReLU(0.01) L2: Conv2d(8, 16, ks=3), LeakyReLU(0.01) L3: Conv2d(16, 32, ks=3), LeakyReLU(0.01) L4: Dense(32, 32), LeakyReLU(0.01) L5: Dense(32, 10), LogSoftMax\nTable 1: The architecture of the MNIST convolution-DNN classifier used in this paper.\nWe use the architecture of the classification network provided in Table 1. We train our network with a learning rate of 10\u22124 using the Nvidia Titan RTX. We use dlatentX = d latent Li\n= 4. For other hyperparameters, we refer to Section F of the Appendix and to the source code.\nThe acquired information plane plots are provided in Figure 5. As the direction of the plots with respect to the epoch can be deduced implicitly (the\nlower left corner of the plot corresponds to the first epochs), we color the lines according to the dynamics of the loss function per epoch. We do this to emphasize one of the key observations: the first transition from fitting to the compression phase coincides with an acceleration of the loss function decrease. It is also evident that there is no clear large-scale compression phase. Moreover, it seems that the number of fitting and compression phases can vary from layer to layer."
        },
        {
            "heading": "7 DISCUSSION",
            "text": "An information-theoretic approach to explainable artificial intelligence and deep neural network analysis seems promising, as it is interpretable, robust, and relies on well-developed information theory. However, the direct application of information-theoretic analysis still poses some problems.\nWe have shown that it is possible to apply information analysis to compressed representations of datasets or models\u2019 outputs. To justify our approach, we have acquired several theoretical results regarding mutual information estimation under lossless and lossy compression. These results suggest that this approach is applicable to real datasets. Although it has been shown that an arbitrary amount of information can be lost due to compression, the information required for optimal decompression is still preserved.\nWe have also developed a framework to test conventional mutual information estimators complemented with the proposed lossy compression step. This framework allows the generation of pairs of high-dimensional datasets with small internal (latent) dimensions and a predefined quantity of mutual information. The conducted numerical experiments have shown that the proposed method performs well, especially when entropy estimation is done via a weighted Kozachenko-Leonenko estimator. Other methods tend to underestimate or overestimate mutual information.\nFinally, an information plane experiment with the MNIST dataset classifier has been carried out. This experiment has shown that the dynamics of information-theoretic quantities during the training of DNNs are indeed non-trivial. However, it is not clear whether the original fitting-compression hypothesis holds, as there is no clear large-scale compression phase after the fitting. We suggest that there may be several compression/fitting phases during the training of real-scale neural networks.\nAn interesting observation has also been made: the first compression phase coincides with the rapid decrease of the loss functions It may be concluded that the phases revealed by information plane plots are connected to different regimes of learning (i.e., accelerated, stationary, or decelerated drop of the loss function). However, we note that this observation is not the main contribution of our work, and further investigation has to be carried out in order to support this seeming connection with more evidence and theoretical basis.\nFuture work. As further research, we consider using normalizing flows (Rezende & Mohamed, 2015) to improve our approach. Normalizing flows are invertible smooth mappings that provide means of lossless and information-preserving compression. They can be used to transform the joint distribution to a Gaussian, thus facilitating mutual information estimation. Besides, we will apply our method to various large neural networks and perform corresponding information plane analysis."
        },
        {
            "heading": "A COMPLETE PROOFS",
            "text": "Statement 1. Let \u03be : \u2126 \u2192 Rn\u2032 be an absolutely continuous random vector, and let f : Rn\u2032 \u2192 Rn be an injective piecewise-smooth mapping with Jacobian Jf ,satisfying n \u2265 n\u2032 and det ( JTf Jf ) \u0338= 0\nalmost everywhere. Let either \u03b7 be a discrete random variable, or (\u03be, \u03b7) be an absolutely continuous random vector. Then\nI(\u03be; \u03b7) = I (f(\u03be); \u03b7) (3)\nProof of Statement 1. For any function f , let us denote \u221a det ( JTf (x)Jf (x) ) (area transformation\ncoefficient) by \u03b1f (x) where it exists.\nForemost, let us note that in both cases, \u03c1\u03be(x | \u03b7) and \u03c1f(\u03be)(x\u2032 | \u03b7) = \u03c1\u03be(x | \u03b7)/\u03b1f (x) exist. Hereinafter, we integrate over supp \u03be \u2229 {x | \u03b1f (x) \u0338= 0} instead of supp \u03be; as \u03b1f \u0338= 0 almost everywhere by the assumption, the values of the integrals are not altered.\nAccording to the definition of the differential entropy, h(f(\u03be)) = \u2212 \u222b \u03c1\u03be(x)\n\u03b1f (x) log\n( \u03c1\u03be(x)\n\u03b1f (x)\n) \u03b1f (x) dx =\n= \u2212 \u222b\n\u03c1\u03be(x) log (\u03c1\u03be(x)) dx+ \u222b \u03c1\u03be(x) log (\u03b1f (x)) dx =\n= h(\u03be) + E log\u03b1f (\u03be).\nh(f(\u03be) | \u03b7) = E\u03b7 ( \u2212 \u222b\n\u03c1\u03be(x | \u03b7) \u03b1f (x) log ( \u03c1\u03be(x | \u03b7) \u03b1f (x) ) \u03b1f (x) dx ) =\n= E\u03b7 ( \u2212 \u222b \u03c1\u03be(x | \u03b7) log (\u03c1\u03be(x | \u03b7)) dx+ \u222b \u03c1\u03be(x | \u03b7) log (\u03b1f (x)) dx ) =\n= h(\u03be | \u03b7) + E log\u03b1f (\u03be)\nFinally, by the MI definition,\nI(f(\u03be); \u03b7) = h(f(\u03be))\u2212 h(f(\u03be) | \u03b7) = h(\u03be)\u2212 h(\u03be | \u03b7) = I(\u03be; \u03b7).\nCorollary 1. Let E\u22121 : Rn\u2032 \u2287 E(suppX) \u2192 Rn and E(X) : \u2126 \u2192 Rn\u2032 exist, let (E\u22121 \u25e6 E)(X) \u2261 X , let X , Y , E(X) and E\u22121 satisfy conditions of the Statement 1. Then\nI(X;Y ) = I(E(X);Y ),"
        },
        {
            "heading": "Proof of Corollary 1.",
            "text": "I(X;Y ) = I ( (E\u22121 \u25e6 E)(X);Y ) = I (E(X);Y )\ufe38 \ufe37\ufe37 \ufe38\nfrom the Statement 1\nStatement 2. For any given \u03ba \u2265 0 there exist random vectors X : \u2126 \u2192 Rn, Y : \u2126 \u2192 Rm, and a non-trivial linear autoencoder A = D\u25e6E with latent space dimension n\u2032 < n that is optimal in terms of minimizing mean squared error E \u2225X \u2212A(X)\u22252, such that I(X;Y ) = \u03ba and I(E(X);Y ) = 0.\nProof of Statement 2. Let us consider the following three-dimensional Gaussian vector (X1, X2, Y ) \u225c (X,Y ):\nX \u223c N ( 0, [ 1 0 0 \u03c3 ]) , Y \u223c N (0, 1) (X1, X2, Y ) \u223c N ( 0, [ 1 0 0 0 \u03c3 a 0 a 1 ]) ,\nwhere cov(X2, Y ) = a \u225c \u221a 1\u2212 e\u22122\u03ba , cov(X1, Y ) = 0 (so X1 and Y are independent). Let the intrinsic dimension be n\u2032 = 1, and \u03c3 < 1. According to principal component analysis, the optimal linear encoder is defined up to a scalar factor by the equality E(X) = X1. However, I(X;Y ) = \u2212 12 ln ( 1\u2212 a2 ) = \u03ba (see the Statement 5), but I(E(X);Y ) = 0, as X1 and Y are independent.\nStatement 3. Let X , Y , and Z be random variables such that I(X;Y ) and I ((X,Z);Y ) are defined. Let f be a function of two arguments such that I(f(X,Z);Y ) is defined. If there exists a function g such that X = g(f(X,Z)), then the following chain of inequalities holds:\nI(X;Y ) \u2264 I (f(X,Z);Y ) \u2264 I((X,Z);Y ) \u2264 I(X;Y ) + h(Z)\u2212 h(Z | X,Y )\nProof of Statement 3. According to data processing inequality (Cover & Thomas, 2006), I (f(X,Z);Y ) \u2264 I (X,Z;Y ). As I(X;Y ) = I (g(f(X,Z));Y ), I(X;Y ) \u2264 I (f(X,Z);Y ). Note that as DPI is optimal, additional assumptions on f , X , Y and Z are required to tighten the bounds.\nThe last inequality is derived via the following equations from Cover & Thomas (2006):\nI(X,Z;Y ) = I(X;Y ) + I(Y ;Z | X) I(X,Y ;Z) = I(X;Z) + I(Y ;Z | X)\nAs I(X;Z) \u2265 0,\nI(X;Y ) + I(X,Y ;Z) = I(X;Y ) + I(Y ;Z | X) + I(X;Z) \u2265 \u2265 I(X;Y ) + I(Y ;Z | X) = I(X,Z;Y )\nFinally, recall that I(X,Y ;Z) = h(Z)\u2212 h(Z | X,Y ).\nCorollary 2. Let X , Y , Z, f , and g satisfy the conditions of the Statement 3. Let also random variables (X,Y ) and Z be independent. Then I(X;Y ) = I (f(X,Z);Y ).\nProof of Corollary 2. Since (X,Y ) and Z are independent, I (X,Z;Y ) = I(X;Y ), which implies I(X;Y ) = I (f(X,Z);Y ) according to the Statement 3.\nStatement 4. For every \u03ba \u2265 0 and every n\u2032,m\u2032 \u2208 N exists a matrix M \u2208 R(n\u2032+m\u2032)\u00d7(n\u2032+m\u2032) such that (\u03be, \u03b7) \u223c N (0,M), \u03be \u223c N (0, In\u2032), \u03b7 \u223c N (0, Im\u2032) and I(\u03be; \u03b7) = \u03ba.\nProof of Statement 4. We divide the proof into the following statements:\nStatement 5. Let (\u03be, \u03b7) \u223c N (0,M) be a Gaussian pair of (scalar) random variables with unit variance such that I(\u03be; \u03b7) = \u03ba. Then\nM = [ 1 a a 1 ] , a = \u221a 1\u2212 e\u22122\u03ba (6)\nProof. Differential entropy of multivariate normal distribution N (\u00b5,\u03a3) is h = 12 ln (det (2\u03c0e \u00b7 \u03a3)). This and equation 2 leads to the following:\n\u03ba = I(\u03be; \u03b7) = 1\n2 ln(2\u03c0e) +\n1 2 ln(2\u03c0e)\u2212 1 2 ln ( (2\u03c0e)2 \u00b7 (1\u2212 a2) ) = \u22121 2 ln ( 1\u2212 a2 ) a = \u221a 1\u2212 e\u22122\u03ba\nStatement 6. Let \u03be and \u03b7 be independent random variables. Then I(\u03be; \u03b7) = 0, h(\u03be, \u03b7) = h(\u03be)+h(\u03b7).\nProof. We consider only the case of absolutely continuous \u03be. As \u03be and \u03b7 are independent, \u03c1\u03be(x | \u03b7 = y) = \u03c1\u03be(x). That is why I(\u03be; \u03b7) = h(\u03be)\u2212 h(\u03be | \u03b7) = h(\u03be)\u2212 h(\u03be) = 0, according to the definition of MI. The second equality is derived from equation 2.\nCorollary 3. Let \u03be1, \u03be2 and \u03b71, \u03b72 be random variables, independent in the following tuples: (\u03be1, \u03be2), (\u03b71, \u03b72) and ((\u03be1, \u03b71), (\u03be2, \u03b72)). Then I ((\u03be1, \u03be2); (\u03b71, \u03b72)) = I(\u03be1; \u03b71) + I(\u03be2; \u03b72)\nProof. From equation 2 and Statement 6 the following chain of equalities is derived: I ((\u03be1, \u03be2); (\u03b71, \u03b72)) = h(\u03be1, \u03be2) + h(\u03b71, \u03b72)\u2212 h(\u03be1, \u03be2, \u03b71, \u03b72) =\n= h(\u03be1) + h(\u03be2) + h(\u03b71) + h(\u03b72)\u2212 h(\u03be1, \u03b71)\u2212 h(\u03be2, \u03b72) = = I(\u03be1; \u03b71) + I(\u03be2; \u03b72)\nThe Statement 5 and Corollary 3 provide us with a trivial way of generating dependent normal random vectors with a defined mutual information. Firstly, we consider \u039e \u223c N (0,M \u2032), where M \u2032 is a (n\u2032 +m\u2032)\u00d7 (n\u2032 +m\u2032) block-diagonal matrix with blocks from equation 6. The number of blocks is k = min{n\u2032,m\u2032} (other diagonal elements are units). The parameter \u03ba for each block equals I(\u03be; \u03b7)/k, where I(\u03be; \u03b7) is the desired mutual information of the resulting vectors. The components of \u039e are then rearranged to get (\u03be, \u03b7) \u223c N (0,M), where \u03be \u223c N (0, In\u2032) and \u03b7 \u223c N (0, Im\u2032). The final structure of M is as follows:\nM =  1 a 1 a . . . . . . a 1 a 1\n\ufe38 \ufe37\ufe37 \ufe38 n\u2032 . . . \ufe38 \ufe37\ufe37 \ufe38 m\u2032 . . .\n (7)"
        },
        {
            "heading": "B ENTROPY BOUNDS",
            "text": "In this section, we provide several theoretical results that complement the bounds proposed in Section 3.2. The following inequalities can be used to bound the entropy terms in Statement 3. Statement 7 (Cover & Thomas (2006), Theorem 8.6.5). Let X be a random vector with covariance matrix R. Then h(X) \u2264 h(N (0, R)). Statement 8. Let X,Z : \u2126 \u2192 Rn be independent random vectors. Then h(X + Z) \u2265 h(Z).\nProof. Recall that h(X,X + Z) = h(X + Z) + h(X | X + Z) = h(X) + h(X + Z | X),\nfrom which the following is derived: h(X + Z) = h(X) + h(X + Z | X)\u2212 h(X | X + Z)\nNote that h(X + Z | X) = EX h(x + Z | X = x) = h(Z | X). As X and Z are independent, h(Z | X) = h(Z). Thus, we derive the following:\nh(X + Z) = h(X) + h(Z)\u2212 h(X | X + Z) = h(Z) + I(X;X + Z)\ufe38 \ufe37\ufe37 \ufe38 \u22650 \u2265 h(Z)\nStatement 9. Let X : \u2126 \u2192 Rn\u00d7n and Z : \u2126 \u2192 Rn be a random matrix and vector, correspondingly. Let X and Z be independent. Then h(X \u00b7 Z) \u2265 h(Z) + E (ln |detX|).\nProof. Note that h(X \u00b7 Z | X) = EX h(x \u00b7 Z | X = x) = h(Z | X) + E (ln |detX|). The rest of the proof is the same as for Statement 8:\nh(X \u00b7 Z) = h(X) + h(Z) + E (ln |detX|)\u2212 h(X | X \u00b7 Z) = = h(Z) + I(X;X \u00b7 Z)\ufe38 \ufe37\ufe37 \ufe38\n\u22650\n+ E (ln |detX|) \u2265 h(Z) + E (ln |detX|) .\nCorollary 4. Let X,Z : \u2126 \u2192 Rn be independent random vectors. Then h(X \u2299 Z) \u2265 h(Z) +\u2211n i=1 E (ln |Xi|), where \u2299 is an element-wise product.\nProof. Note that X \u2299 Z = diag(X) \u00b7 Z, and log |det diag(X)| = \u2211n\ni=1 ln |Xi|. We then apply Statement 9.\nNote that entropy terms in Statements 8 and 9 can be conditioned. The independence requirement should then be replaced by independence under corresponding conditions.\nWe also note that Statement 7 can utilize autoencoder reconstruction error (via error covariance matrix), and Statements 8, 9 \u2013 magnitude of random vector and injected noise, which is of particular use, as this information is easily accessible in a typical experimental setup.\nPractical use cases include using Statement 8 when stochasticity is introduced via additive noise (e.g., Goldfeld et al. (2019)) and Corollary 4 when stochasticity is introduced via multiplicative noise (e.g., Adilova et al. (2023))."
        },
        {
            "heading": "C LIMITATIONS OF PREVIOUS WORKS",
            "text": "In this section, we stress the novelty of our contribution to the problem of high-dimensional MI estimation.\nIn the Introduction, we mention the works of Butakov et al. (2021) and Greenewald et al. (2023). The first article focuses on entropy estimation via lossy compression. The main theoretical result of the paper in question is the following upper bound of random vector entropy: Statement 10 (Butakov et al. (2021)). Let X be a random vector of dimension n, let Z \u223c N (0, \u03c32In\u2032). Let A = D \u25e6 E be an autoencoder of input dimension n and latent dimension n\u2032. Then\nh(X) \u2264 h(E(X) + Z)\u2212 n\u2032 ( c+ log(\u03c32)\n2\n) + n ( c+ log(\u03a32)\n2\n) ,\nwhere\nc = log(2\u03c0e)\n2 , \u03a32 =\n1 n E [ \u2225X \u2212D(E(X) + Z)\u22252 ] This bound takes advantage of the compression aspect, as it incorporates the reconstruction mean squared error \u03a32. However, it is important to note several limitations of the proposed bound. Firstly, this Statement is insufficient to acquire any bound of MI, as MI is computed via difference of entropy terms (see equation 1 and equation 2), so a two-sided bound of entropy is required. Secondly, this bound is derived in case of additive Gaussian noise being injected into the latent representation of the vector. It is inapplicable to other cases of stochasticity injection (e.g., noise added to the vector itself) and, moreover, deteriorates when \u03c3 \u2192 0. That is why we consider this result inapplicable to the task of MI estimation in the current form.\nNow, consider the following two-sided bound derived in the work of Greenewald et al. (2023): Statement 11 (Greenewald et al. (2023)). Let X be a random vector of dimension n, let Z \u223c N (0, \u03c32In). Let E be a PCA-projector to a linear manifold of dimension n\u2032 with explained variances denoted by \u03bbi in the descending order. Then2\nn\u2212 n\u2032 2 log ( 2\u03c0e\u03c32 ) \u2264 h(X + Z)\u2212 h(E(X + Z)) \u2264 n\u2212 n \u2032 2 log ( 2\u03c0e(\u03bbn\u2032+1 + \u03c3 2) )\nThis bound also takes advantage of the compression aspect, as it incorporates the reconstruction mean squared error via \u03bbi. We also note that, as the bound is two-sided, corresponding bounds of MI estimate under Gaussian convolution and linear compression can be derived: Corollary 5. Under the conditions of Statement 11\n|I(X + Z;Y )\u2212 I(E(X + Z);Y )| \u2264 n\u2212 n \u2032\n2 log\n( 1 +\n\u03bbn\u2032+1 \u03c32 ) 2We believe \u201c+Z\u201d in \u201ch(E(X + Z))\u201d to be missing in the original article; counterexample: X = const.\n0 2\n4\n6\n8\n10\n0 2 4 6 8 10\ntrue value compr. est. (AE) compr. est. (PCA)\n(a) 32 \u00d7 32 images of 2D Gaussians (n\u2032 = m\u2032 = 2)\n0\n2\n4\n6\n8\n10\n0 2 4 6 8 10\ntrue value compr. est. (AE) compr. est. (PCA)\n(b) 32\u00d7 32 images of rectangles (n\u2032 = m\u2032 = 4)\n0\n0.5\n1\n1.5\n2\n2.5\n3\n3.5\n4\n0 0.5 1 1.5 2 2.5 3 3.5 4\ntrue value compr. est. (AE) compr. est. (PCA)\n(c) Highly-nonlinear manifold in R32 (n\u2032 = m\u2032 = 2)\nFigure 6: Comparison of nonlinear AE and linear PCA performance in task of MI estimation via lossy compression, 5 \u00b7103 samples. Along x axes is I(X;Y ), along y axes is I\u0302(X;Y ). WKL entropy estimator is used\nProof. From the equation 2 we acquire\nI(X + Z;Y )\u2212 I(E(X + Z);Y ) = = [h(X + Z)\u2212 h(E(X + Z))]\u2212 [h(X + Z, Y )\u2212 h(E(X + Z), Y )]\nBounds from Statement 11 can be applied to the joint entropy, as Y can be viewed as additional components of the vector X , unaffected by smoothing and compression.\nTo acquire the upper bound of difference of MI terms, we apply the upper bound from Statement 11 to the first difference of entropy terms and the lower bound to the second, and vice versa in case of the lower bound. After simplification, we acquire the desired formula.\nCorollary 5 provides a useful result, as (a) the difference between the true MI and MI under compression is bounded, (b) the bound converges to zero as \u03bbn\u2032+1 \u2192 0 (which corresponds to the lossless compression). However, as the authors of the original paper mention, this bound deteriorates when \u03c3 \u2192 0, which coincides with Statement 2. The linearity of the encoder E is another limitation we have to mention. Although the possibility of extension to nonlinear dimensionality reduction approaches is mentioned in the paper, it is unclear if the derived bounds could be directly transferred to the nonlinear case. To accomplish this, one has to propose a nonlinear generalization of explained variance and provide a more general analysis of entropy alternation via discarding nonlinear components. We also perform tests with synthetic data to show that autoencoder-based approach outperforms PCA-based in case of nonlinear manifolds (see Figure 6). Although the gap is relatively small for the datasets we used in Section 4, it is possible to provide an example of a highly nonlinear manifold, in which case the linear compression is very lossy (see Figure 6c, the synthetic data generator is provided in the source code).\nFinally, we note that similar, or even tighter bounds can be derived from the theoretical results of our work.\nCorollary 6. Under the conditions of Statement 11 the following inequalities hold:\n0 \u2264 I(X + Z;Y )\u2212 I(E(X + Z);Y ) \u2264 n\u2212 n \u2032\n2 log\n( 1 +\n\u03bbn\u2032+1 \u03c32\n)\nProof. To avoid notation conflicts, we denote X and Z used in Statement 3 as X \u2032 and Z \u2032 correspondingly. We then choose f = E, g = Id, X \u2032 = E(X + Z), Z \u2032 = X + Z \u2212 E(X + Z) (so Z \u2032 \u22a5 X \u2032), which yields the following inequalities:\nI(X \u2032;Y ) \u2264 I (X \u2032, Z \u2032;Y ) \u2264 I(X \u2032;Y ) + h(Z \u2032)\u2212 h(Z \u2032 | X \u2032, Y )\nWe then utilize Statements 7 and 8 to bound the entropy terms:\nh(Z \u2032) \u2264 n\u2212 n \u2032 2 log ( 2\u03c0e(\u03bbn\u2032+1 + \u03c3 2) ) , h(Z \u2032 | X \u2032, Y ) \u2265 n\u2212 n \u2032 2 log ( 2\u03c0e\u03c32 )\nThus, the following result is acquired:\nI(X \u2032;Y ) \u2264 I (X \u2032, Z \u2032;Y ) \u2264 I(X \u2032;Y ) + n\u2212 n \u2032\n2 log\n( 1 +\n\u03bbn\u2032+1 \u03c32 ) Now recall that X \u2032 = E(X + Z), (X \u2032, Z \u2032) \u223c X \u2032 + Z \u2032 = X + Z. This yields the final result."
        },
        {
            "heading": "D LIMITATIONS OF OTHER ESTIMATORS",
            "text": "In this section, we provide a brief overview of modern entropy and MI estimators that achieve a relative practical success in dealing with the curse of dimensionality. We provide reasoning why we choose MINE (Belghazi et al., 2018) as the only modern MI estimator among the mentioned in the Introduction to compare our results with.\n\u2022 MINE is widely considered as a good benchmark estimator and featured in several recent works (Poole et al., 2019; J\u00f3nsson et al., 2020; McAllester & Stratos, 2020; Mroueh et al., 2021). As MINE is a neural estimator, it is theoretically able to grasp latent structure of data, thus performing compression implicitly.\n\u2022 Other lower/upper bounds and surrogate objectives. \u2013 According to McAllester & Stratos (2020), not many methods in question outperform\nMINE. In fact, among the other methods mentioned in McAllester & Stratos (2020), only the difference of entropies (DoE) estimator achieves good results during a standard correlated Gaussians test. Unfortunately, DoE requires good parametrized and differentiable (by parameters) estimates of two PDFs, which is difficult to achieve in the case of complex multidimensional distributions.\n\u2013 According to an another overview (Poole et al., 2019), the methods in question have various significant trade-offs. Some of them require parts of the original distribution (like \u03c1Y |X ) or even some information-theoretic quantities (like h(X)) to be tractable, which is not achievable without utilizing special kinds of stochastic NNs. The others heavily rely on fitting a critic function to partially reproduce the original distribution, which leads to a poor bias-variance trade-off (it is illustrated by the results of these estimators in a standard correlated Gaussians test, see Figure 2 in Poole et al. (2019)).\n\u2013 Compared to autoencoders, critic networks in methods in question are usually unstable and hard to train, see experiments in Poole et al. (2019); McAllester & Stratos (2020). We also have witnessed this instability while conducting experiments with MINE, see the attached source code.\nWe, however, note that all these methods are of great use for building information-theoretic training objectives (as they are differentiable and usually represent upper or lower bounds). In addition to the limitations mentioned above, we would like to note that the relative practical success of the modern NN-based MI estimators might be attributed to the data compression being performed implicitly. In the work of Poole et al. (2019) it has been shown that other complex parametric NN-based estimators (NJW, JS, InfoNCE, etc.) exhibit poor performance during the estimation of MI between a pair of 20-dimensional incompressible (i.e., not lying along a manifold) synthetic vectors. These vectors, however, are of much simpler structure than the synthetic datasets used in our work (Gaussian vectors and x 7\u2192 x3 mapping applied to Gaussian vectors in Poole et al. (2019) versus high-dimensional images of geometric shapes and functions in our work). We interpret this phenomenon as a practical manifestation of the universal problem of MI estimation, which also affects the performance of modern NN-based MI estimators in the case of hard-to-compress data.\n\u2022 EDGE (Noshad et al., 2019) is a generalized version of the original binning estimator proposed in Shwartz-Ziv & Tishby (2017): the binning operation is replaced by a more general hashing. We suppose that this method suffers from the same problems revealed in Goldfeld et al. (2019), unless a special hashing function admitting manifold-like or cluster-like structure of complex high-dimensional data is used.\n\u2022 Other definitions of entropy. We are interested in fitting-compression hypothesis (Tishby & Zaslavsky, 2015; Shwartz-Ziv & Tishby, 2017) which is formulated for the classical\nmutual information, so other definitions are not appropriate for this particular task. We also note that the classical theory of information is well-developed and provides rigorous theoretical results (e.g., data processing inequality, which we used to prove Statement 3).\n\u2022 We also mention the approach proposed in Adilova et al. (2023), where h(L | X) is computed via a closed-form formula for Gaussian distribution and Monte-Carlo sampling. However, we note the following drawbacks of this method: (a) a closed-form formula is applicable to the entropy estimation only for the first stochastic NN layer, (b) a general-case estimator still has to be utilized to estimate h(L) (in the work of Adilova et al. (2023), the plug-in estimator from Goldfeld et al. (2019) is used; this estimator also suffers from the curse of dimensionality)."
        },
        {
            "heading": "E CLASSICAL ENTROPY ESTIMATORS",
            "text": "In this section, we provide definitions of conventional entropy estimators used to conduct the experiments, as well as provide proofs that these estimators fail in case of high-dimensional data."
        },
        {
            "heading": "E.1 KERNEL DENSITY ESTIMATION",
            "text": "The estimation of the probability density function for codes ck in the latent space plays an important role in the proposed method of mutual information estimation. There are many methods for probability density function estimation (e.g., Weglarczyk (2018); Kozachenko & Leonenko (1987); Berrett et al. (2019)). One of the most popular methods for solving this problem is kernel density estimation (KDE). In this section, we study this method in application to entropy estimation.\nLet \u03c1X,k(x) be a density estimate at a point x, which is obtained from the sampling {xk}Nk=1 without the k-th element by KDE with the kernel K. We get the following expression for the density:\n\u03c1\u0302b,\u2212k(x) = 1\nbn (N \u2212 1) N\u2211 l=0 l \u0338=k K ( x\u2212 xl b ) (8)\nHere, K(x) = 1\n(2\u03c0)n/2 exp\n( \u2212\u2225x\u2225 2\n2\n) is a standard Gaussian kernel.3\nThe entropy estimate is obtained via Leave-One-Out method. Densities at each sample xk are calculated according to the formula 8.\nH\u0302(X) = 1\nN N\u2211 k=1 log \u03c1\u0302b,\u2212k(xk) (9)"
        },
        {
            "heading": "E.1.1 MAXIMUM-LIKELIHOOD",
            "text": "The optimal bandwidth can be selected in accordance with the minimization of the Kullback-Leibler divergence between the estimated distributions and the empirical one (\u03c1\u0302emp(x) = 1N \u2211N k=1 \u03b4(x\u2212xk)). This is equivalent to selecting the bandwidth as a maximum likelihood estimate:\nb\u0302 = argmax b H\u0302(X) = argmax b N\u2211 k=1 log \u03c1\u0302b,\u2212k(xk) (10)\nThe experiments have shown that this method tends to underestimate mutual information, and the difference increases with an increasing true value of mutual information.\nAsymptotic: the entropy estimation and bandwidth selection take O (n log n), compression takes O (n), resulting in a total time complexity of O (n log n)\n3Hereinafter, it is possible to use any kernel with infinite support as K, but the Gaussian one is preferable because of its light tails and infinite differentiability."
        },
        {
            "heading": "E.1.2 LEAST SQUARES ERROR",
            "text": "Now let us consider the Least Square Cross Validation method (see Turlach (1999); Sain (1994)). In this method, bandwidth selection is based on the minimization of the mean squared error between the exact density and the corresponding kernel density estimate. We minimize the following expression:\nISE(b) = \u222b Rn (\u03c1\u0302b(x)\u2212 \u03c1(x))2 dx\nHere, \u03c1 is the true probability density function, and \u03c1\u0302b(x) is the estimate with the bandwidth b:\n\u03c1\u0302b(x) = 1\nbnN N\u2211 k=1 K ( x\u2212 xk b ) Since the true distribution is unknown, we substitute \u03c1 with \u03c1\u0302emp. This leads to the following objective function to be minimized:\n1\nN2 N\u2211 i=1 N\u2211 j=1 Jb(xi \u2212 xj)\u2212 2 N N\u2211 i=1 \u03c1\u0302b,\u2212i(xi),\nwhere\nJb(\u03be) = \u222b Rn 1 b2n K (x b ) K ( x\u2212 \u03be b ) dx,\nwhich can be computed via the Fourier transform.\nAsymptotic: The entropy estimation takes O (n log n), compression takes O (n), same as KDE ML. However, the optimal bandwidth selection takes O ( n2 )\ndue to the quadratic complexity of the minimized objective. Therefore, this algorithm has a total time complexity of O ( n2 ) , making KDE LSE asymptotically the slowest algorithm implemented within this research."
        },
        {
            "heading": "E.2 KOZACHENKO-LEONENKO",
            "text": "There is another method of entropy estimation, which was proposed by Kozachenko and Leonenko in Kozachenko & Leonenko (1987). The main feature of this method is that it utilizes k-nearest neighbor density estimation instead of KDE."
        },
        {
            "heading": "E.2.1 NON-WEIGHTED KOZACHENKO-LEONENKO",
            "text": "Let {xk}Nk=1 \u2286 Rn be the sampling of random vector X . Let us denote r\u0302(x) = min 1\u2264k\u2264N r(x, xk) the distance to the nearest neighbour using the metric r (by default, r is Euclidean metric).\nAccording to Kozachenko & Leonenko (1987), the density estimation at x is given by:\n\u03c1\u0302(x) = 1\n\u03b3 \u00b7 r\u0302(x)n \u00b7 c1(n) \u00b7 (N \u2212 1) ,\nwhere c1(n) = \u03c0n/2/\u0393(n/2 + 1) is a unit n-dimensional ball volume and \u03b3 is a constant which makes the entropy estimate unbiased (ln \u03b3 = c2 \u2248 0.5772 is the Euler constant). Asymptotic: the entropy estimation takes O (n log n), compression takes O (n), resulting in a total time complexity of O (n log n)."
        },
        {
            "heading": "E.2.2 WEIGHTED KOZACHENKO-LEONENKO",
            "text": "The main drawback of the conventional Kozachenko-Leonenko estimator is the bias that occurs in dimensions higher than 3. This issue can be addressed by using weighted nearest neighbors estimation. A modified estimator is proposed in Berrett et al. (2019):\nH\u0302wN = 1\nN N\u2211 i=1 k\u2211 j=1 wi log \u03be(j),i\nwhere w is the weight vector, \u03be(j),i = e\u2212\u03a8(j) \u00b7 c1(n) \u00b7 (N \u2212 1) \u00b7 \u03c1d(j),i, \u03a8 denotes the digamma function. We choose the weight vector w = (w1, . . . , wk) as follows. For k \u2208 N let\nW(k) = w \u2208 Rk : k\u2211\nj=1\nwj \u00b7 \u0393 ( j + 2\u2113n ) \u0393(j) = 0 for \u2113 = 1, . . . , \u230an 4 \u230b ,\nk\u2211 j=1 wj = 1 and wj = 0 if j \u0338\u2208 {\u230a k n \u230b , \u230a 2k n \u230b , . . . , k } and let the w be a vector from W(k) with the least l2-norm. Asymptotic: the entropy estimation takes O (n log n), weight selection \u2013 O ( k3 ) = O (1), compression \u2013 O (n), resulting in a total time complexity of O (n log n)."
        },
        {
            "heading": "E.3 LIMITATIONS OF CLASSICAL ENTROPY ESTIMATORS",
            "text": "Although the entropy estimation is an example of a classical problem, it is still difficult to acquire estimates for high-dimensional data, as the estimation requires an exponentially (in dimension) large number of samples (see Goldfeld et al. (2020); McAllester & Stratos (2020)). As the mutual information estimation is tightly connected to the entropy estimation, this problem also manifests itself in our task. Although this difficulty affects every MI estimator, classical estimators may be assumed to be more prone to the curse of dimensionality, as they are usually too basic to grasp a manifold-like low-dimensional structure of high-dimensional data.\nIn this section, we provide experimental proofs of classical estimators\u2019 inability to yield correct MI estimates in the high-dimensional case. We utilize the same tests with images of 2D Gaussians used in Section 5 Figure 3. However, due to computational reasons, the size of the images is reduced to 4\u00d7 4 and 8\u00d7 8 (so the data is of even smaller dimension compared to Section 5). The results are presented int Table 2. For a comparison we also provide the results for WKL estimator fed with the PCA-compressed data.\nNote that although WKL estimator performs better in Section 5 due to lower bias, it is outperformed by the original KL estimator in the case of uncompressed data due to lower variance. However, this observation is not of great importance, as all the four methods perform extremely poorly in case of 8\u00d7 8 images and bigger."
        },
        {
            "heading": "F TECHNICAL DETAILS",
            "text": "In this section, we describe the technical details of our experimental setup: architecture of the neural networks, hyperparameters, etc.\nFor the tests described in Section 5, we use architectures listed in Table 3. The autoencoders are trained via Adam (Kingma & Ba, 2017) optimizer on 5 \u00b7 103 images with a batch size 5 \u00b7 103, a learning rate 10\u22123 and MAE loss for 2 \u00b7 103 epochs. The MINE critic network is trained via Adam optimizer on 5 \u00b7 103 images with a batch size 512, a learning rate 10\u22123 for 5 \u00b7 103 epochs. For the experiments described in Section 6, we use architectures listed in Table 4. The input data autoencoder is trained via Adam optimizer on 5 \u00b7 104 images with a batch size 1024, a learning rate\n10\u22123 and MAE loss for 2 \u00b7 102 epochs; the latent dimension equals 4. The convolutional classifier is trained via Adam optimizer on 5 \u00b7 104 images with a batch size 1024, a learning rate 10\u22124 and NLL loss for 100 epochs; Outputs of the layers are compressed via PCA into 4-dimensional vectors. Mutual information is estimated via WKL estimator with 5 nearest neighbours.\nHere we do not define fi and gi used in the tests with synthetic data, as these functions smoothly map low-dimensional vectors to high-dimensional images and, thus, are very complex. A Python implementation of the functions in question is available in the supplementary material, see the file source/source/python/mutinfo/utils/synthetic.py."
        }
    ],
    "year": 2023
}