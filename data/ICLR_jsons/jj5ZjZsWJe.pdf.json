{
    "abstractText": "Communication compression has gained great interest in Federated Learning (FL) for the potential of alleviating its communication overhead. However, communication compression brings forth new challenges in FL due to the interplay of compression-incurred information distortion and inherent characteristics of FL such as partial participation and data heterogeneity. Despite the recent development, the existing approaches either cannot accommodate arbitrary data heterogeneity or partial participation, or require stringent conditions on compression. In this paper, we revisit the seminal stochastic controlled averaging method by proposing an equivalent but more efficient/simplified formulation with halved uplink communication costs, building upon which we propose two compressed FL algorithms, SCALLION and SCAFCOM, to support unbiased and biased compression, respectively. Both the proposed methods outperform the existing compressed FL methods in terms of communication and computation complexities. Moreover, SCALLION and SCAFCOM attain fast convergence rates under arbitrary data heterogeneity and without any additional assumptions on compression errors. Experiments show that SCALLION and SCAFCOM outperform recent compressed FL methods under the same communication budget.",
    "authors": [
        {
            "affiliations": [],
            "name": "Xinmeng Huang"
        },
        {
            "affiliations": [],
            "name": "Ping Li"
        },
        {
            "affiliations": [],
            "name": "Xiaoyun Li"
        }
    ],
    "id": "SP:f0cf373ffdcc526097cbe76019cc28398007a152",
    "references": [
        {
            "authors": [
                "Sulaiman A Alghunaim"
            ],
            "title": "Local exact-diffusion for decentralized optimization and learning",
            "venue": "arXiv preprint arXiv:2302.00620,",
            "year": 2023
        },
        {
            "authors": [
                "Dan Alistarh",
                "Demjan Grubic",
                "Jerry Li",
                "Ryota Tomioka",
                "Milan Vojnovic"
            ],
            "title": "QSGD: communication-efficient SGD via gradient quantization and encoding",
            "venue": "In Advances in Neural Information Processing Systems (NIPS),",
            "year": 2017
        },
        {
            "authors": [
                "Dan Alistarh",
                "Torsten Hoefler",
                "Mikael Johansson",
                "Nikola Konstantinov",
                "Sarit Khirirat",
                "C\u00e9dric Renggli"
            ],
            "title": "The convergence of sparsified gradient methods",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2018
        },
        {
            "authors": [
                "Debraj Basu",
                "Deepesh Data",
                "Can Karakus",
                "Suhas N. Diggavi"
            ],
            "title": "Qsparse-local-SGD: Distributed SGD with quantization, sparsification and local computations",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2019
        },
        {
            "authors": [
                "Jeremy Bernstein",
                "Yu-Xiang Wang",
                "Kamyar Azizzadenesheli",
                "Animashree Anandkumar"
            ],
            "title": "SIGNSGD: compressed optimisation for non-convex problems",
            "venue": "In Proceedings of the 35th International Conference on Machine Learning (ICML),",
            "year": 2018
        },
        {
            "authors": [
                "Aleksandr Beznosikov",
                "Samuel Horv\u00e1th",
                "Peter Richt\u00e1rik",
                "Mher Safaryan"
            ],
            "title": "On biased compression for distributed learning",
            "venue": "arXiv preprint arXiv:2002.12410,",
            "year": 2020
        },
        {
            "authors": [
                "Zachary Charles",
                "Zachary Garrett",
                "Zhouyuan Huo",
                "Sergei Shmulyian",
                "Virginia Smith"
            ],
            "title": "On largecohort training for federated learning",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2021
        },
        {
            "authors": [
                "Wenlin Chen",
                "Samuel Horv\u00e1th",
                "Peter Richt\u00e1rik"
            ],
            "title": "Optimal client sampling for federated learning",
            "year": 2022
        },
        {
            "authors": [
                "Xiangyi Chen",
                "Xiaoyun Li",
                "Ping Li"
            ],
            "title": "Toward communication efficient adaptive gradient method",
            "venue": "In Proceedings of the ACM-IMS Foundations of Data Science Conference (FODS),",
            "year": 2020
        },
        {
            "authors": [
                "Xuxing Chen",
                "Tesi Xiao",
                "Krishnakumar Balasubramanian"
            ],
            "title": "Optimal algorithms for stochastic bilevel optimization under relaxed smoothness conditions",
            "venue": "arXiv preprint arXiv:2306.12067,",
            "year": 2023
        },
        {
            "authors": [
                "Ziheng Cheng",
                "Xinmeng Huang",
                "Pengfei Wu",
                "Kun Yuan"
            ],
            "title": "Momentum benefits non-iid federated learning simply and provably",
            "venue": "In The Twelfth International Conference on Learning Representations,",
            "year": 2024
        },
        {
            "authors": [
                "Laurent Condat",
                "Grigory Malinovsky",
                "Peter Richt\u00e1rik"
            ],
            "title": "Tamuna: Accelerated federated learning with local training and partial participation",
            "venue": "arXiv preprint arXiv:2302.09832,",
            "year": 2023
        },
        {
            "authors": [
                "Rudrajit Das",
                "Anish Acharya",
                "Abolfazl Hashemi",
                "Sujay Sanghavi",
                "Inderjit S. Dhillon",
                "Ufuk Topcu"
            ],
            "title": "Faster non-convex federated learning via global and local momentum",
            "venue": "In Proceedings of the Thirty-Eighth Conference on Uncertainty in Artificial Intelligence (UAI),",
            "year": 2022
        },
        {
            "authors": [
                "Tim Dettmers"
            ],
            "title": "8-bit approximations for parallelism in deep learning",
            "venue": "In Proceedings of the 4th International Conference on Learning Representations (ICLR), San Juan,",
            "year": 2016
        },
        {
            "authors": [
                "Ilyas Fatkhullin",
                "Igor Sokolov",
                "Eduard Gorbunov",
                "Zhize Li",
                "Peter Richt\u00e1rik"
            ],
            "title": "EF21 with bells & whistles: Practical algorithmic extensions of modern error feedback",
            "venue": "arXiv preprint arXiv:2110.03294,",
            "year": 2021
        },
        {
            "authors": [
                "Ilyas Fatkhullin",
                "Alexander Tyurin",
                "Peter Richt\u00e1rik"
            ],
            "title": "Momentum provably improves error feedback",
            "venue": "arXiv preprint arXiv:2305.15155,",
            "year": 2023
        },
        {
            "authors": [
                "Hongchang Gao",
                "An Xu",
                "Heng Huang"
            ],
            "title": "On the convergence of communication-efficient local SGD for federated learning",
            "venue": "In Proceedings of the Thirty-Fifth AAAI Conference on Artificial Intelligence (AAAI),",
            "year": 2021
        },
        {
            "authors": [
                "Micha\u0142 Grudzie\u0144",
                "Grigory Malinovsky",
                "Peter Richt\u00e1rik"
            ],
            "title": "Improving accelerated federated learning with compression and importance sampling",
            "venue": "arXiv preprint arXiv:2306.03240,",
            "year": 2023
        },
        {
            "authors": [
                "Luyao Guo",
                "Sulaiman A Alghunaim",
                "Kun Yuan",
                "Laurent Condat",
                "Jinde Cao"
            ],
            "title": "Randcom: Random communication skipping method for decentralized stochastic optimization",
            "venue": "arXiv preprint arXiv:2310.07983,",
            "year": 2023
        },
        {
            "authors": [
                "Farzin Haddadpour",
                "Mohammad Mahdi Kamani",
                "Aryan Mokhtari",
                "Mehrdad Mahdavi"
            ],
            "title": "Federated learning with compression: Unified analysis and sharp guarantees",
            "venue": "In Proceedings of the 24th International Conference on Artificial Intelligence and Statistics (AISTATS),",
            "year": 2021
        },
        {
            "authors": [
                "Yutong He",
                "Xinmeng Huang",
                "Yiming Chen",
                "Wotao Yin",
                "Kun Yuan"
            ],
            "title": "Lower bounds and accelerated algorithms in distributed stochastic optimization with communication compression",
            "venue": "arXiv preprint arXiv:2305.07612,",
            "year": 2023
        },
        {
            "authors": [
                "Yutong He",
                "Xinmeng Huang",
                "Kun Yuan"
            ],
            "title": "Unbiased compression saves communication in distributed optimization: When and how much",
            "venue": "arXiv preprint arXiv:2305.16297,",
            "year": 2023
        },
        {
            "authors": [
                "Samuel Horv\u00f3th",
                "Chen-Yu Ho",
                "Ludovit Horvath",
                "Atal Narayan Sahu",
                "Marco Canini",
                "Peter Richt\u00e1rik"
            ],
            "title": "Natural compression for distributed deep learning",
            "venue": "In Mathematical and Scientific Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Xinmeng Huang",
                "Yiming Chen",
                "Wotao Yin",
                "Kun Yuan"
            ],
            "title": "Lower bounds and nearly optimal algorithms in distributed learning with communication compression",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2022
        },
        {
            "authors": [
                "Peng Jiang",
                "Gagan Agrawal"
            ],
            "title": "A linear speedup analysis of distributed deep learning with sparse and quantized communication",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2018
        },
        {
            "authors": [
                "Sai Praneeth Karimireddy",
                "Quentin Rebjock",
                "Sebastian U. Stich",
                "Martin Jaggi"
            ],
            "title": "Error feedback fixes SignSGD and other gradient compression schemes",
            "venue": "In Proceedings of the 36th International Conference on Machine Learning (ICML),",
            "year": 2019
        },
        {
            "authors": [
                "Sai Praneeth Karimireddy",
                "Martin Jaggi",
                "Satyen Kale",
                "Mehryar Mohri",
                "Sashank J Reddi",
                "Sebastian U Stich",
                "Ananda Theertha Suresh"
            ],
            "title": "Mime: Mimicking centralized stochastic algorithms in federated learning",
            "venue": "arXiv preprint arXiv:2008.03606,",
            "year": 2020
        },
        {
            "authors": [
                "Sai Praneeth Karimireddy",
                "Satyen Kale",
                "Mehryar Mohri",
                "Sashank J. Reddi",
                "Sebastian U. Stich",
                "Ananda Theertha Suresh"
            ],
            "title": "SCAFFOLD: stochastic controlled averaging for federated learning",
            "venue": "In Proceedings of the 37th International Conference on Machine Learning (ICML),",
            "year": 2020
        },
        {
            "authors": [
                "Prashant Khanduri",
                "Pranay Sharma",
                "Haibo Yang",
                "Mingyi Hong",
                "Jia Liu",
                "Ketan Rajawat",
                "Pramod K. Varshney"
            ],
            "title": "STEM: A stochastic two-sided momentum algorithm achieving nearoptimal sample and communication complexities for federated learning",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2021
        },
        {
            "authors": [
                "Anastasia Koloskova",
                "Sebastian U. Stich",
                "Martin Jaggi"
            ],
            "title": "Decentralized stochastic optimization and gossip algorithms with compressed communication",
            "venue": "In Proceedings of the 36th International Conference on Machine Learning (ICML),",
            "year": 2019
        },
        {
            "authors": [
                "Jakub Kone\u010dn\u1ef3",
                "H Brendan McMahan",
                "Felix X Yu",
                "Peter Richt\u00e1rik",
                "Ananda Theertha Suresh",
                "Dave Bacon"
            ],
            "title": "Federated learning: Strategies for improving communication efficiency",
            "venue": "arXiv preprint arXiv:1610.05492,",
            "year": 2016
        },
        {
            "authors": [
                "Yann LeCun"
            ],
            "title": "The mnist database of handwritten digits. http://yann",
            "venue": "lecun. com/exdb/mnist/,",
            "year": 1998
        },
        {
            "authors": [
                "Qinbin Li",
                "Yiqun Diao",
                "Quan Chen",
                "Bingsheng He"
            ],
            "title": "Federated learning on non-iid data silos: An experimental study",
            "venue": "In Proceedings of the 38th IEEE International Conference on Data Engineering (ICDE),",
            "year": 2022
        },
        {
            "authors": [
                "Tian Li",
                "Anit Kumar Sahu",
                "Ameet Talwalkar",
                "Virginia Smith"
            ],
            "title": "Federated learning: Challenges, methods, and future directions",
            "venue": "IEEE Signal Process. Mag.,",
            "year": 2020
        },
        {
            "authors": [
                "Tian Li",
                "Anit Kumar Sahu",
                "Manzil Zaheer",
                "Maziar Sanjabi",
                "Ameet Talwalkar",
                "Virginia Smith"
            ],
            "title": "Federated optimization in heterogeneous networks",
            "venue": "In Proceedings of Machine Learning and Systems (MLSys),",
            "year": 2020
        },
        {
            "authors": [
                "Xiang Li",
                "Kaixuan Huang",
                "Wenhao Yang",
                "Shusen Wang",
                "Zhihua Zhang"
            ],
            "title": "On the convergence of FedAvg on non-IID data",
            "venue": "In Proceedings of the 8th International Conference on Learning Representations",
            "year": 2020
        },
        {
            "authors": [
                "Xiaoyun Li",
                "Ping Li"
            ],
            "title": "Analysis of error feedback in federated non-convex optimization with biased compression: Fast convergence and partial participation",
            "venue": "In Proceedings of the 40th International Conference on Machine Learning (ICML),",
            "year": 2023
        },
        {
            "authors": [
                "Xiaoyun Li",
                "Belhal Karimi",
                "Ping Li"
            ],
            "title": "On distributed adaptive optimization with gradient compression",
            "venue": "In Proceedings of the Tenth International Conference on Learning Representations (ICLR), Virtual Event,",
            "year": 2022
        },
        {
            "authors": [
                "Zhize Li",
                "Hongyan Bao",
                "Xiangliang Zhang",
                "Peter Richt\u00e1rik"
            ],
            "title": "PAGE: A simple and optimal probabilistic gradient estimator for nonconvex optimization",
            "venue": "In Proceedings of the 38th International Conference on Machine Learning (ICML),",
            "year": 2021
        },
        {
            "authors": [
                "Xianfeng Liang",
                "Shuheng Shen",
                "Jingchang Liu",
                "Zhen Pan",
                "Enhong Chen",
                "Yifei Cheng"
            ],
            "title": "Variance reduced local SGD with lower communication complexity",
            "year": 1912
        },
        {
            "authors": [
                "Tao Lin",
                "Sebastian U. Stich",
                "Kumar Kshitij Patel",
                "Martin Jaggi"
            ],
            "title": "Don\u2019t use large mini-batches, use local SGD",
            "venue": "In Proceedings of the 8th International Conference on Learning Representations (ICLR),",
            "year": 2020
        },
        {
            "authors": [
                "Yanli Liu",
                "Yuan Gao",
                "Wotao Yin"
            ],
            "title": "An improved analysis of stochastic gradient descent with momentum",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2020
        },
        {
            "authors": [
                "Amirhossein Malekijoo",
                "Mohammad Javad Fadaeieslam",
                "Hanieh Malekijou",
                "Morteza Homayounfar",
                "Farshid Alizadeh-Shabdiz",
                "Reza Rawassizadeh"
            ],
            "title": "FEDZIP: A compression framework for communication-efficient federated learning",
            "venue": "arXiv preprint arXiv:2102.01593,",
            "year": 2021
        },
        {
            "authors": [
                "Brendan McMahan",
                "Eider Moore",
                "Daniel Ramage",
                "Seth Hampson",
                "Blaise Ag\u00fcera y Arcas"
            ],
            "title": "Communication-efficient learning of deep networks from decentralized data",
            "venue": "In Proceedings of the 20th International Conference on Artificial Intelligence and Statistics (AISTATS),",
            "year": 2017
        },
        {
            "authors": [
                "Mehryar Mohri",
                "Gary Sivek",
                "Ananda Theertha Suresh"
            ],
            "title": "Agnostic federated learning",
            "venue": "In Proceedings of the 36th International Conference on Machine Learning (ICML),",
            "year": 2019
        },
        {
            "authors": [
                "Boris T Polyak"
            ],
            "title": "Some methods of speeding up the convergence of iteration methods",
            "venue": "Ussr computational mathematics and mathematical physics,",
            "year": 1964
        },
        {
            "authors": [
                "Sashank J. Reddi",
                "Zachary Charles",
                "Manzil Zaheer",
                "Zachary Garrett",
                "Keith Rush",
                "Jakub Kone\u010dn\u00fd",
                "Sanjiv Kumar",
                "Hugh Brendan McMahan"
            ],
            "title": "Adaptive federated optimization",
            "venue": "In Proceedings of the 9th International Conference on Learning Representations (ICLR), Virtual Event,",
            "year": 2021
        },
        {
            "authors": [
                "Amirhossein Reisizadeh",
                "Aryan Mokhtari",
                "Hamed Hassani",
                "Ali Jadbabaie",
                "Ramtin Pedarsani"
            ],
            "title": "FedPAQ: A communication-efficient federated learning method with periodic averaging and quantization",
            "venue": "In Proceedings of the 23rd International Conference on Artificial Intelligence and Statistics (AISTATS),",
            "year": 2020
        },
        {
            "authors": [
                "Peter Richt\u00e1rik",
                "Igor Sokolov",
                "Ilyas Fatkhullin"
            ],
            "title": "EF21: A new, simpler, theoretically better, and practically faster error feedback",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2021
        },
        {
            "authors": [
                "Abdurakhmon Sadiev",
                "Grigory Malinovsky",
                "Eduard Gorbunov",
                "Igor Sokolov",
                "Ahmed Khaled",
                "Konstantin Burlachenko",
                "Peter Richt\u00e1rik"
            ],
            "title": "Federated optimization algorithms with random reshuffling and gradient compression",
            "venue": "arXiv preprint arXiv:2206.07021,",
            "year": 2022
        },
        {
            "authors": [
                "Mher Safaryan",
                "Egor Shulgin",
                "Peter Richt\u00e1rik"
            ],
            "title": "Uncertainty principle for communication compression in distributed and federated learning and the search for an optimal compressor",
            "venue": "Information and Inference: A Journal of the IMA,",
            "year": 2022
        },
        {
            "authors": [
                "Frank Seide",
                "Hao Fu",
                "Jasha Droppo",
                "Gang Li",
                "Dong Yu"
            ],
            "title": "1-bit stochastic gradient descent and its application to data-parallel distributed training of speech DNNs",
            "venue": "In Proceedings of the 15th Annual Conference of the International Speech Communication Association (INTERSPEECH),",
            "year": 2014
        },
        {
            "authors": [
                "Sebastian U. Stich"
            ],
            "title": "Local SGD converges fast and communicates little",
            "venue": "In Proceedings of the 7th International Conference on Learning Representations (ICLR),",
            "year": 2019
        },
        {
            "authors": [
                "Sebastian U. Stich",
                "Jean-Baptiste Cordonnier",
                "Martin Jaggi"
            ],
            "title": "Sparsified SGD with memory",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2018
        },
        {
            "authors": [
                "Jianyu Wang",
                "Gauri Joshi"
            ],
            "title": "Cooperative SGD: A unified framework for the design and analysis of local-update SGD algorithms",
            "venue": "J. Mach. Learn. Res.,",
            "year": 2021
        },
        {
            "authors": [
                "Jianyu Wang",
                "Qinghua Liu",
                "Hao Liang",
                "Gauri Joshi",
                "H. Vincent Poor"
            ],
            "title": "Tackling the objective inconsistency problem in heterogeneous federated optimization",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2020
        },
        {
            "authors": [
                "Jianyu Wang",
                "Vinayak Tantia",
                "Nicolas Ballas",
                "Michael G. Rabbat"
            ],
            "title": "SlowMo: Improving communication-efficient distributed SGD with slow momentum",
            "venue": "In Proceedings of the 8th International Conference on Learning Representations (ICLR),",
            "year": 2020
        },
        {
            "authors": [
                "Jianqiao Wangni",
                "Jialei Wang",
                "Ji Liu",
                "Tong Zhang"
            ],
            "title": "Gradient sparsification for communicationefficient distributed optimization",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2018
        },
        {
            "authors": [
                "Wei Wen",
                "Cong Xu",
                "Feng Yan",
                "Chunpeng Wu",
                "Yandan Wang",
                "Yiran Chen",
                "Hai Li"
            ],
            "title": "Terngrad: Ternary gradients to reduce communication in distributed deep learning. In Advances in neural information processing systems (NIPS)",
            "year": 2017
        },
        {
            "authors": [
                "Jiaxiang Wu",
                "Weidong Huang",
                "Junzhou Huang",
                "Tong Zhang"
            ],
            "title": "Error compensated quantized SGD and its applications to large-scale distributed optimization",
            "venue": "In Proceedings of the 35th International Conference on Machine Learning (ICML),",
            "year": 2018
        },
        {
            "authors": [
                "Han Xiao",
                "Kashif Rasul",
                "Roland Vollgraf"
            ],
            "title": "Fashion-MNIST: a novel image dataset for benchmarking machine learning algorithms",
            "venue": "arXiv preprint arXiv:1708.07747,",
            "year": 2017
        },
        {
            "authors": [
                "Yan Yan",
                "Tianbao Yang",
                "Zhe Li",
                "Qihang Lin",
                "Yi Yang"
            ],
            "title": "A unified analysis of stochastic momentum methods for deep learning",
            "venue": "In Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence (IJCAI),",
            "year": 2018
        },
        {
            "authors": [
                "Haibo Yang",
                "Minghong Fang",
                "Jia Liu"
            ],
            "title": "Achieving linear speedup with partial worker participation in non-iid federated learning",
            "venue": "In Proceedings of the 9th International Conference on Learning Representations (ICLR), Virtual Event,",
            "year": 2021
        },
        {
            "authors": [
                "Kai Yang",
                "Tao Jiang",
                "Yuanming Shi",
                "Zhi Ding"
            ],
            "title": "Federated learning via over-the-air computation",
            "venue": "IEEE Trans. Wirel. Commun.,",
            "year": 2022
        },
        {
            "authors": [
                "Yeojoon Youn",
                "Bhuvesh Kumar",
                "Jacob Abernethy"
            ],
            "title": "Accelerated federated optimization with quantization. In Workshop on Federated Learning: Recent Advances and New Challenges (in Conjunction with NeurIPS",
            "year": 2022
        },
        {
            "authors": [
                "Hao Yu",
                "Rong Jin",
                "Sen Yang"
            ],
            "title": "On the linear speedup analysis of communication efficient momentum SGD for distributed non-convex optimization",
            "venue": "In Proceedings of the 36th International Conference on Machine Learning (ICML),",
            "year": 2019
        },
        {
            "authors": [
                "Hao Yu",
                "Sen Yang",
                "Shenghuo Zhu"
            ],
            "title": "Parallel restarted SGD with faster convergence and less communication: Demystifying why model averaging works for deep learning",
            "venue": "In Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence (AAAI),",
            "year": 2019
        },
        {
            "authors": [
                "Kun Yuan",
                "Yiming Chen",
                "Xinmeng Huang",
                "Yingya Zhang",
                "Pan Pan",
                "Yinghui Xu",
                "Wotao Yin"
            ],
            "title": "DecentLaM: Decentralized momentum SGD for large-batch deep training",
            "venue": "In Proceedings of the 2021 IEEE/CVF International Conference on Computer Vision (ICCV),",
            "year": 2021
        },
        {
            "authors": [
                "Kun Yuan",
                "Sulaiman A Alghunaim",
                "Xinmeng Huang"
            ],
            "title": "Removing data heterogeneity influence enhances network topology dependence of decentralized sgd",
            "venue": "Journal of Machine Learning Research,",
            "year": 2023
        },
        {
            "authors": [
                "Nesterov Yurri"
            ],
            "title": "Introductory Lectures on Convex Optimization: A Basic Course",
            "year": 2004
        },
        {
            "authors": [
                "Xinwei Zhang",
                "Mingyi Hong",
                "Sairaj V. Dhople",
                "Wotao Yin",
                "Yang Liu"
            ],
            "title": "FedPD: A federated learning framework with adaptivity to non-iid data",
            "venue": "IEEE Trans. Signal Process.,",
            "year": 2021
        },
        {
            "authors": [
                "Haoyu Zhao",
                "Boyue Li",
                "Zhize Li",
                "Peter Richt\u00e1rik",
                "Yuejie Chi"
            ],
            "title": "BEER: fast O(1/T) rate for decentralized nonconvex optimization with communication compression",
            "venue": "In Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2022
        },
        {
            "authors": [
                "Yue Zhao",
                "Meng Li",
                "Liangzhen Lai",
                "Naveen Suda",
                "Damon Civin",
                "Vikas Chandra"
            ],
            "title": "Federated learning with non-IID data",
            "venue": "arXiv preprint arXiv:1806.00582,",
            "year": 2018
        },
        {
            "authors": [
                "\u22c6 (white"
            ],
            "title": "square). B DETAILED IMPLEMENTATIONS OF SCAFFOLD Algorithm 3 SCAFFOLD: Stochastic controlled averaging for FL (Karimireddy et al., 2020b) 1: Input: initial model x and control variables",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Federated learning (FL) is a powerful paradigm for large-scale machine learning (Konec\u030cny\u0300 et al., 2016; McMahan et al., 2017; Yang et al., 2020) in situations where data and computational resources are dispersed among diverse clients such as phones, tablets, sensors, banks, hospitals (Kairouz et al., 2021). FL enjoys the advantage of distributed optimization on the efficiency of computational resources as the local clients conduct computations simultaneously. Moreover, FL provides the first layer of protection for data privacy as the local data never leaves the local device during training. Here, we first summarize the significant challenges in algorithmic development and theory of FL:\n\u2022 Severe data heterogeneity. Unlike in classic distributed training, the local data distribution in FL can vary significantly (i.e., non-iid clients), reflecting practical scenarios where local data held by clients is highly personalized (Zhao et al., 2018; Kairouz et al., 2021; Yuan et al., 2023; Li et al., 2022a). When multiple local training steps are taken, the local models become \u201cbiased\u201d toward minimizing the local losses instead of the global loss, hindering the convergence quality of the global model (Mohri et al., 2019; Li et al., 2020c;a).\n\u2022 Partial client participation. Not all clients can always join the training, e.g., due to unstable connections or active selection (Li et al., 2020a). Consequently, only a fraction of clients are involved in each FL training round to interact with the central server. This slows down the convergence of the global model due to less accessible data/information per round (Charles et al., 2021; Chen et al., 2022; Li & Li, 2023).\n\u2022 Heavy communication workload. The cost of model transmission can be a major challenge in FL systems with limited bandwidth (e.g., portable wireless devices), especially for models with millions or billions of parameters. Therefore, communication compression, a technique that aims to reduce the volume of information transmitted, has gained growing research interests in FL (Basu et al., 2019; Reisizadeh et al., 2020; Haddadpour et al., 2021; Li & Li, 2023).\nThe classic FL approach, FEDAVG (Konec\u030cny\u0300 et al., 2016; McMahan et al., 2017; Stich, 2019), performs multiple gradient-descent steps within each accessible client before communicating with the central server. FEDAVG is notably hampered by data heterogeneity and partial client participation (Karimireddy et al., 2020b; Li et al., 2020c; Yang et al., 2021) due to the \u201cclient drift\u201d effect.\n\u2217The work is conducted at LinkedIn \u2014 Bellevue, 98004 WA, USA. Xinmeng Huang is a Ph.D. student in the Graduate Group of Applied Mathematics and Computational Science at the University of Pennsylvania.\nFurthermore, when communication compression is employed, the adverse effect of data heterogeneity can be amplified due to the interplay of client drift and inaccurate message aggregation caused by compression (Basu et al., 2019; Reisizadeh et al., 2020; Haddadpour et al., 2021; Gao et al., 2021; Malekijoo et al., 2021; Li & Li, 2023); see Figure 3 in Appendix A for illustration. This naturally raises the question regarding the theory and utility of compressed FL approaches:\nCan we design FL approaches that accommodate arbitrary data heterogeneity, local updates, and partial participation, as well as support communication compression?\nNone of the existing algorithms have successfully achieved this goal in non-convex FL, despite a few studies in the strongly-convex and deterministic scenarios (Grudzien\u0301 et al., 2023; Youn et al., 2022; Condat et al., 2023; Sadiev et al., 2022). For instance, FEDPAQ (Reisizadeh et al., 2020), FEDCOM (Haddadpour et al., 2021), QSPARSE-SGD (Basu et al., 2019), LOCAL-SGD-C (Gao et al., 2021) consider compressed FL algorithms under iid clients. FEDCOMGATE (Haddadpour et al., 2021), designed for unbiased compressors, does not support biased compressors, and their analysis does not consider partial client participation. FED-EF (Li & Li, 2023) focuses on biased compression with error feedback (EF) (Seide et al., 2014; Karimireddy et al., 2019) and partial client participation. However, the convergence analysis requires the assumption of bounded gradient dissimilarity on the data heterogeneity and shows an extra slow-down factor under partial participation, suggesting a theoretical limitation of EF in FL. Moreover, both Haddadpour et al. (2021) and Li & Li (2023) impose stringent conditions on compression errors (see Appendix C.4 for more details)."
        },
        {
            "heading": "1.1 MAIN RESULTS AND CONTRIBUTIONS",
            "text": "We develop SCALLION and SCAFCOM, two compressed FL algorithms for unbiased and biased compressors, respectively, that are practical to implement, robust to data heterogeneity and partial participation, and exhibit superior theoretical convergence. Table 1 presents the comparison of convergence rates of our results with prior works. Specifically, the main contributions are:\n\u2022 We revisit SCAFFOLD (Karimireddy et al., 2020b) and propose a simplified formulation. The new implementation reduces the uplink communication cost by half, requiring each client to transmit only one increment variable (of the same size as the model), instead of two variables.\n\u2022 Builing on the new formulation, we propose SCALLION method employs unbiased compressors for the communication of increment variables. We establish its convergence result for non-convex objectives. SCALLION obtains the state-of-the-art communication and computation complexities for FL under unbiased compressors and supports partial client participation.\n\u2022 We further develop SCAFCOM which enables biased compressors for broader applications. Local momentum is applied to guarantee fast convergence and improve empirical performance. The communication and computation complexities of SCAFCOM improve prior results by significant margins, particularly when compression is aggressive.\n\u2022 We conduct experiments to illustrate the effectiveness of SCALLION and SCAFCOM. Our empirical results show that the proposed methods achieve comparable performance to fullprecision FL methods with substantially reduced communication costs, and outperform recent compressed FL methods under the same communication budget.\nNotably, our analysis only requires the smoothness of objectives and bounded variance of stochastic gradients, without additional assumptions on data heterogeneity or compression errors, unlike prior works (Appendix C.4). To our best knowledge, SCALLION and SCAFCOM are the first stochastic FL methods that exhibit robustness to arbitrary data heterogeneity, partial participation, local updates, and accommodate communication compression relying solely on standard compressibilities."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Communication compression & error feedback. Two popular approaches are commonly employed to compress communication in distributed systems: quantization and sparsification, which are often modeled as unbiased or biased operators. Notable quantization examples include Sign (Seide et al., 2014; Bernstein et al., 2018), low-bit fixed rounding (Dettmers, 2016), random dithering (Alistarh et al., 2017), TernGrad (Wen et al., 2017), and natural compression (Horvo\u0301th et al., 2022). Sparsification compressors work by transmitting a small subset of entries from the input vector (Wangni et al., 2018; Stich et al., 2018). In distributed training, unbiased compressors\nusually can be applied in place of the full-precision gradients to get reasonable convergence and empirical performance. However, directly using biased compressors may slow down convergence or even lead to divergence (Beznosikov et al., 2020; Li & Li, 2023). To mitigate this, the technique of error feedback (EF) is first proposed in Seide et al. (2014), which has proven effective in addressing biased compressors (Stich et al., 2018; Karimireddy et al., 2019) and has inspired many subsequent works (Wu et al., 2018; Alistarh et al., 2018; Li et al., 2022b). Moreover, a variant scheme of EF called EF21 is introduced recently (Richta\u0301rik et al., 2021). EF21 compresses increments of deterministic gradients and offers superior theoretical guarantees compared to vanilla error feedback.\nHeterogeneity and compression in FL. Federated learning has gained great prominence since the introduction of FEDAVG, proposed by McMahan et al. (2017) to improve the communication efficiency of classic distributed training. Subsequent studies reveal its susceptibility to data heterogeneity (i.e., non-iid clients) due to the \u201cclient-drift\u201d effect, particularly when not all clients participate in training (Stich, 2019; Yu et al., 2019b; Wang & Joshi, 2021; Lin et al., 2020; Wang et al., 2020b; Li et al., 2020c; Yang et al., 2021). Substantial efforts have been made to address client heterogeneity in FL (Liang et al., 2019; Li et al., 2020b;a; Wang et al., 2020a; Zhang et al., 2021; Haddadpour et al., 2021; Alghunaim, 2023; Cheng et al., 2024; Guo et al., 2023) and develop FL protocols involving variance reduction techniques or adaptive optimizers (Karimireddy et al., 2020b; Reddi et al., 2021; Chen et al., 2020). Notably, SCAFFOLD introduced by Karimireddy et al. (2020b) leverages control variables to mitigate the impact of data heterogeneity and partial client participation.\nTo further reduce communication costs, communication compression has been integrated into FL, leading to methods such as FEDPAQ (Reisizadeh et al., 2020), FEDCOMGATE (Haddadpour et al., 2021), FED-EF (Li & Li, 2023). Due to information distortion incurred by compression, those compressed FL methods either lack the robustness to data heterogeneity and partial participation, or rely on stringent conditions of compressors, surpassing standard unbiased/contractive compressibility.\nFederated learning with momentum. The momentum mechanism in optimization traces back to Nesterov\u2019s acceleration (Yurri, 2004) and the heavy-ball method (Polyak, 1964), which have been extended to stochastic optimization (Yan et al., 2018; Yu et al., 2019a; Liu et al., 2020) and other domains (Yuan et al., 2021; He et al., 2023b;a; Chen et al., 2023; Fatkhullin et al., 2023). In the context of FL, momentum has been widely incorporated and shown to enhance performance (Wang et al., 2020b; Karimireddy et al., 2020a; Khanduri et al., 2021; Das et al., 2022; Cheng et al., 2024)."
        },
        {
            "heading": "3 PROBLEM SETUP",
            "text": "Formally, in federated learning, we aim to minimize the following objective:\nmin x\u2208Rd\nf(x) := 1\nN N\u2211 i=1 fi(x) where fi(x) := E\u03bei\u223cDi [F (x; \u03bei)],\nwhere \u03bei represents a local data sample of client i following data distribution Di, F (x; \u03bei) represents the loss function evaluated at model x and sample \u03bei, and fi(x) is the local objective w.r.t. Di. In practice, the data distributions Di across clients may vary significantly (referred to as data heterogeneity), resulting in the inequality fi(x) \u0338= fj(x) for different clients i and j. Consequently, a globally stationary model x\u22c6 with \u2207f(x\u22c6) = 0 may not be a stationary point of the local objectives. In contrast, if the clients are homogeneous (i.e. following a common data distribution D), we would have f1(x) = \u00b7 \u00b7 \u00b7 = fN (x) and a globally stationary model would also be stationary for each client. Throughout the paper, we use \u2225\u00b7\u2225 to denote the \u21132 vector norm and use [N ] to denote {1, . . . , N} for N \u2208 N+. The notation \u2272 denotes inequalities that hold up to numeric numbers; \u2273 and \u224d are utilized similarly. We state the following standard assumptions required for our convergence analysis.\nAssumption 1. Each local objective fi has L-Lipschitz gradient, i.e., for any x, y \u2208 Rd and 1 \u2264 i \u2264 N , it holds that \u2225\u2207fi(x)\u2212\u2207fi(y)\u2225 \u2264 L\u2225x\u2212 y\u2225. Assumption 2. There exists \u03c3 \u2265 0 such that for any x \u2208 Rd and 1 \u2264 i \u2264 N , E\u03bei [\u2207F (x; \u03bei)] = \u2207fi(x) and E\u03bei [\u2225\u2207F (x; \u03bei)\u2212\u2207fi(x)\u22252] \u2264 \u03c32, where \u03bei \u223c Di are iid local samples at client i."
        },
        {
            "heading": "4 SCALLION: UNBIASED COMPRESSED COMMUNICATION",
            "text": "In this section, we first revisit the seminal SCAFFOLD algorithm (Karimireddy et al., 2020b), which requires communicating two variables (of the same size as the model) from client to server per communication round. We present a new formulation with only a single variable for uplink communication for each client. We then propose SCALLION, which employs unbiased compressors to reduce the communication workload of SCAFFOLD and provide the convergence analysis."
        },
        {
            "heading": "4.1 BACKGROUND OF SCAFFOLD",
            "text": "The SCAFFOLD algorithm (Karimireddy et al., 2020b) maintains local control variables {cti}Ni=1 on clients and a global control variable ct on the server. Let St \u2286 [N ] (with |St| = S) be the set of accessible (active) clients to interact with the server in the t-th round. In each training round, SCAFFOLD conducts K local updates within each active client i \u2208 St by\nyt,k+1i := y t,k i \u2212 \u03b7l(\u2207F (y t,k i ; \u03be t,k i )\u2212 c t i + c t), for k = 0, . . . ,K \u2212 1, (1) where yt,ki is the local model in client i initialized with the server model y t,0 i := x\nt and \u03b7l is the local learning rate. Here, the subscript i represents the client index, while the superscripts t and k denote the outer and inner loop indexes corresponding to communication rounds and local-update steps, respectively. Upon the end of local training steps, clients update local control variables as:\nct+1i :=\n{ cti \u2212 ct + xt\u2212yt,Ki \u03b7lK\n, if i \u2208 St, cti, otherwise.\n(2)\nThe increments of local model yt,Ki \u2212 xt and control variable c t+1 i \u2212 cti, of each participating client i \u2208 St, are then sent to the central server and aggregated to update the global model parameters:\nxt+1 := xt + \u03b7g S \u2211 i\u2208St (yt,Ki \u2212 x t), ct+1 := ct + 1 N \u2211 i\u2208St (ct+1i \u2212 c t i),\nwhere \u03b7g is the global learning rate. The detailed description of SCAFFOLD can be found in Appendix B. Notably, the control variables of SCAFFOLD track local gradients such that cti \u2248 \u2207fi(xt) and ct \u2248 \u2207f(xt), thereby mimicking the ideal update through \u2207F (yt,ki ; \u03be t,k i )\u2212 cti + ct \u2248 \u2207f(xt) given \u2207F (yt,ki ; \u03be t,k i ) \u2248 \u2207fi(y t,k i ) and y t,k i \u2248 xt. Consequently, the local updates are nearly synchronized in the presence of data heterogeneity without suffering from client drift. While the introduction of control variables enables SCAFFOLD to converge robustly with arbitrarily heterogeneous clients and partial client participation, the original implementation of SCAFFOLD described above requires clients to communicate both updates of local models yt,Ki \u2212 xt and control variables ct+1i \u2212 cti (also see Karimireddy et al. (2020b, Alg. 1, line 13)). This results in a doubled client-to-server communication cost and more obstacles to employing communication compression, compared to its counterparts without control variables such as FEDAVG."
        },
        {
            "heading": "4.2 DEVELOPMENT OF SCALLION",
            "text": "We present an equivalent implementation of SCAFFOLD which only requires a single variable for uplink communication and is readily employable for communication compression. Expanding the\nlocal updates yt,Ki \u2212 xt and control variables c t+1 i \u2212 cti by exploiting (1) and (2), we have\nct+1i \u2212 c t i = xt \u2212 yt,Ki \u03b7lK \u2212 ct = 1 K K\u22121\u2211 k=0 \u2207F (yt,ki ; \u03be t,k i )\u2212 c t i \u225c \u2206 t i, (3)\nyt,Ki \u2212 x t = \u2212\u03b7l K\u22121\u2211 k=0 ( \u2207F (yt,ki ; \u03be t,k i )\u2212 c t i + c t ) = \u2212\u03b7lK(\u2206ti + ct). (4)\nIn (3) and (4), the updates of local models and control variables share a common component, the increment variables \u2206ti. Since the global control variable c\nt is inherently maintained by the server, updates ct+1i \u2212 cti and thus y t,K i \u2212 xt can be recovered by the server upon receiving \u2206ti. Hence, the global model and control variable can be equivalently updated as\nxt+1 = xt \u2212 \u03b7g\u03b7lK S \u2211 i\u2208St ( \u2206ti + c t ) and ct+1 = ct + 1 N \u2211 i\u2208St \u2206ti. (5)\nWe only need to communicate \u2206ti. In the above formulation, by communicating the increment variables \u2206ti and applying (5) at the server accordingly, SCAFFOLD can be implemented equivalently with a halved uplink communication cost, compared to the original one (Karimireddy et al., 2020b). Notably, our new implementation only modifies the communication procedure, and maintain the same local updates as in Karimireddy et al. (2020b); see Algorithm 4 in Appendix B. Benefits of compressing \u2206ti. Importantly, the new implementation provides a simpler backbone for communication compression as only the transmission of \u2206ti is to be compressed. Moreover, unlike compressing local gradients as adopted in prior literature, compressing \u2206ti asymptotically eliminates compression errors even in the presence of data heterogeneity. Consider the case of deterministic gradients for simplicity. Based on the update rules of SCAFFOLD, if hypothetically the training approached a steady stage where xt is close to a stationary point x\u22c6, we expect to have cti \u2248 \u2207fi(x\u22c6) and ct = 1N \u2211N i=1 c t i \u2248 \u2207f(x\u22c6) = 0. Thus, the directions in local updates satisfy \u2207fi(yt,k) \u2212 cti + ct \u2248 0 so that x\u22c6 \u2248 xt \u2248 yt,1 \u2248 \u00b7 \u00b7 \u00b7 \u2248 yt,K . The definition of \u2206ti in (3) implies \u2206ti = 1 K \u2211K\u22121 k=0 \u2207fi(y t,k i ) \u2212 cti \u2248 0. Namely, the increment variable \u2206ti gradually vanishes as the algorithm iterates. Therefore, taking an \u03c9-unbiased compressor as an example (see Definition 1), compressing \u2206ti results in a vanishing compression error E[\u2225Ci(\u2206ti) \u2212 \u2206ti\u22252] \u2264 \u03c9\u2225\u2206ti\u22252 \u2192 0, regardless of data heterogeneity. In contrast, if one considers compressing local gradients directly, a constantly large compression error is introduced in each communication round E[\u2225Ci(\u2207fi(xt))\u2212 \u2207fi(xt)\u22252] \u2264 \u03c9\u2225\u2207fi(xt)\u22252 \u2192 \u03c9\u2225\u2207fi(x\u22c6)\u22252 \u0338= 0. The constant \u2225\u2207fi(x\u22c6)\u22252 can be extremely large under severe data heterogeneity, resulting in algorithmic susceptibility to data heterogeneity.\nAlgorithm 1 SCALLION: SCAFFOLD with single compressed uplink communication 1: Input: initial model x0 and control variables {c0i }Ni=1, c0; local learning rate \u03b7l; global learning\nrate \u03b7g; local steps K; number of sampled clients S; scaling factor \u03b1 \u2208 [0, 1] 2: for t = 0, \u00b7 \u00b7 \u00b7 , T \u2212 1 do 3: Uniformly sample clients St \u2286 [N ] with |St| = S 4: for client i \u2208 St in parallel do 5: Receive xt and ct; initialize yt,0i = x t 6: for k = 0, . . . ,K \u2212 1 do 7: Compute a mini-batch gradient gt,ki = \u2207F (y t,k i ; \u03be t,k i ) 8: Locally update yt,k+1i = y t,k i \u2212 \u03b7l(g t,k i \u2212 cti + ct) 9: end for\n10: Compute \u03b4ti = \u03b1 ( xt\u2212yt,Ki \u03b7lK \u2212 ct )\n11: Compress and send \u03b4\u0303ti = Ci(\u03b4ti) to the server \u25b7 \u03b1 = 1 and Ci = I recovers SCAFFOLD 12: Update ct+1i = c t i + \u03b4\u0303 t i (for i /\u2208 St, c t+1 i = c t i) 13: end for 14: Update xt+1 = xt \u2212 \u03b7g\u03b7lKS \u2211 i\u2208St(\u03b4\u0303 t i + c t)\n15: Update ct+1 = ct + 1N \u2211 i\u2208St \u03b4\u0303 t i 16: end for\nFollowing the above argument regarding compression, we now propose to transmit the compressed proxy Ci(\u03b1\u2206ti) of the increment variable \u2206ti, leading to the SCALLION method as presented in Algorithm 1. Here Ci is the compressor utilized by client i while the scaling factor \u03b1 \u2208 [0, 1] is introduced to stabilize the updates of control variables {cti}Ni=1 and can be viewed as the learning rate of control variables. When \u03b1 = 1 and {Ci}Ni=1 are the identity mappings (i.e., no compression), SCALLION will reduce to SCAFFOLD with our new implementation, i.e., Algorithm 4. The algorithmic comparison of SCALLION with other existing approaches is in Appendix C.2."
        },
        {
            "heading": "4.3 CONVERGENCE OF SCALLION",
            "text": "To study the convergence of SCALLION under communication compression, we first consider compressors satisfying the following standard unbiased compressibility.\nDefinition 1 (\u03c9-UNBIASED COMPRESSOR). There exists \u03c9 \u2265 0 such that for any input x \u2208 Rd and each client-associated compressor Ci : Rd \u2192 Rd, E[Ci(x)] = x and E[\u2225Ci(x) \u2212 x\u22252] \u2264 \u03c9\u2225x\u22252, where the expectation is taken over the randomness of the compressor Ci. Examples that satisfy Definition 1 include random sparsification and dithering; see Appendix C.1 for details. When communication compression with \u03c9-unbiased compressors is employed, the convergence of the proposed SCALLION (Algorithm 1) is justified as follows.\nTheorem 1 (SCALLION WITH UNBIASED COMPRESSION). Under Assumptions 1, 2, and mutually independent \u03c9-unbiased compressors, if we initialize c0i = \u2207fi(x0) and c0 = \u2207f(x0), and set \u03b7l, \u03b7g , and \u03b1 as in (29), then SCALLION converges as\n1\nT T\u22121\u2211 t=0 E[\u2225\u2207f(xt)\u22252] \u2272 \u221a (1 + \u03c9)L\u2206\u03c32 SKT + ( (1 + \u03c9)N2L2\u22062\u03c32 S3KT 2 )1/3 + (1 + \u03c9)NL\u2206 ST , (6) where \u2206 \u225c f(x0)\u2212minx f(x). A detailed version and the proof are in Appendix E. Asymptotic complexities of SCALLION. When using full-batch gradients (i.e., \u03c3 \u2192 0), all terms involving \u03c3 in (6) vanish. Consequently, the bottleneck of FL algorithms boils down to the rounds of client-to-server communication. On the other hand, when gradients are very noisy (i.e., \u03c3 is extremely large), the \u03c3/ \u221a T -dependent term dominates others in which \u03c3 are with lower orders. In this case, the performance is mainly hampered by the number of gradient evaluations. Following (Fatkhullin et al., 2023), we refer to the total number of communication rounds in the regime \u03c3 \u2192 0 and gradient evaluations in the regime \u03f5 \u2192 0 required by per client to attain E[\u2225\u2207f(x\u0302)\u22252] \u2264 \u03f5 as the asymptotic communication complexity and computation complexity1. Theorem 1 shows N(1+\u03c9)S\u03f5 asymptotic communication complexity and 1+\u03c9 S\u03f52 asymptotic computation complexity of SCALLION. Here, we focus on presenting the impact of stationarity \u03f5, compression \u03c9, client participation S and N , and local steps K in asymptotic complexities. Comparison with prior compressed FL methods. Table 1 provides a summary of non-convex FL methods employing unbiased compressors under full client participation. We observe that SCALLION matches the state-of-the-art asymptotic communication and computation complexities under non-iid clients. In particular, while having the same asymptotic complexities as FEDCOMGATE (Haddadpour et al., 2021), SCALLION does not depend on a large uniform bound of compression errors (see Appendix C.4) in convergence and thus has a superior convergence rate.\nTo sum up, based on the above discussion, we demonstrate that SCALLION theoretically improves existing FL methods with unbiased compression."
        },
        {
            "heading": "5 SCAFCOM: BIASED COMPRESSION WITH MOMENTUM",
            "text": "While SCALLION achieves superior convergence speed under unbiased compression, its analysis cannot be adapted to biased compressors (also known as contractive compressors) to attain fast convergence rates. In this section, we propose an algorithm called SCAFCOM as a complement of SCALLION to accommodate biased communication compression in FL.\n1The complexities justify convergence rates in terms of the \u03c3/ \u221a T -dependent and 1/T -dependent terms.\nFor instance, for the rate in (6), the asymptotic communication complexity T \u224d N(1+\u03c9) S\u03f5 is derived from (1+\u03c9)NL\u2206\nST \u224d \u03f5 while the asymptotic computation complexity SKT N \u224d 1+\u03c9 N\u03f52 follows from\n\u221a (1+\u03c9)L\u2206\u03c32\nSKT \u224d \u03f5.\nAlgorithm 2 SCAFCOM: SCAFFOLD with momentum-enhanced compression 1: Input: initial model x0 and control variables {c0i }Ni=1, c0; local learning rate \u03b7l; global learning\nrate \u03b7g; local steps K; number of sampled clients S; momentum \u03b2 \u2208 [0, 1] 2: for t = 0, \u00b7 \u00b7 \u00b7 , T \u2212 1 do 3: Uniformly sample clients St \u2286 [N ] with |St| = S 4: for client i \u2208 St in parallel do 5: Receive xt and ct; initialize yt,0i = x t 6: for k = 0, . . . ,K \u2212 1 do 7: Compute a mini-batch gradient gt,ki = \u2207F (y t,k i ; \u03be t,k i ) 8: Locally update yt,k+1i = y t,k i \u2212 \u03b7l(g t,k i \u2212 cti + ct) 9: end for\n10: Update vt+1i = (1\u2212 \u03b2)vti + \u03b2 ( xt\u2212yt,Ki \u03b7lK + cti \u2212 ct ) (for i /\u2208 St, vt+1i = vti )\n11: Compute \u03b4ti = v t+1 i \u2212 cti\n12: Compress and send \u03b4\u0303ti = Ci(\u03b4ti) to the server \u25b7 \u03b2 = 1 and Ci = I recovers SCAFFOLD 13: Update ct+1i = c t i + \u03b4\u0303 t i (for i /\u2208 St, c t+1 i = c t i) 14: end for 15: Update xt+1 = xt \u2212 \u03b7g\u03b7lKS \u2211 i\u2208St(\u03b4\u0303 t i + c t)\n16: Update ct+1 = ct + 1N \u2211 i\u2208St \u03b4\u0303 t i 17: end for"
        },
        {
            "heading": "5.1 DEVELOPMENT OF SCAFCOM",
            "text": "In the literature, biased compressors are commonly modeled by contractive compressibility.\nDefinition 2 (q2-CONTRACTIVE COMPRESSOR). There exists q \u2208 [0, 1) such that for any input x \u2208 Rd and each client-associated compressor Ci : Rd \u2192 Rd, E[\u2225Ci(x) \u2212 x\u22252] \u2264 q2\u2225x\u22252, where the expectation is taken over the randomness of the compressor Ci. Notably, compared to unbiased compressors satisfying Definition 1, contractive compressors, though potentially having smaller squared compression errors, no longer enjoy the unbiasedness; see common contractive compressors in Appendix C.1. Compared to their counterparts with unbiased compressors, approaches employing biased compressors typically (i) require stringent assumptions, e.g., bounded gradients (Seide et al., 2014; Koloskova et al., 2019; Basu et al., 2019) or bounded gradient dissimilarity (Huang et al., 2022; Li & Li, 2023), (ii) rely on impractical algorithmic structure, e.g., a large amount of gradient computation (Fatkhullin et al., 2021), (iii) have weak convergence guarantees, e.g., worse dependence on compression parameter q (Zhao et al., 2022).\nRecently, Fatkhullin et al. (2023) shows that tactfully incorporating momentum into communication compression can effectively mitigate the influence of biased compression and thus presents the linear speedup in terms of the number of clients. Inspired by their findings, we introduce an extra momentum variable vti on each client i to overcome the adverse effect of biased compression. This leads to the SCAFCOM method, as presented in Algorithm 2. When client i participates in the t-th round, an additional momentum variable vti is updated as\nvt+1i := (1\u2212 \u03b2)vti + \u03b2\n( xt \u2212 yt,Ki\n\u03b7lK + cti \u2212 ct\n) = (1\u2212 \u03b2)vti + \u03b2\nK K\u22121\u2211 k=0 \u2207F (yt,ki ; \u03be t,k i ),\nwhere {yt,ki } are the intermediate local models and \u03b2 is the momentum factor. We then set v t+1 i \u2212 cti = (1\u2212\u03b2)vti+\u03b2K\u22121 \u2211K\u22121 k=0 \u2207F (y t,k i ; \u03be t,k i )\u2212cti to be (compressively) communicated, as opposed\nto K\u22121 \u2211K\u22121\nk=0 \u2207F (y t,k i ; \u03be t,k i ) \u2212 cti in SCAFFOLD and \u03b1(K\u22121 \u2211K\u22121 k=0 \u2207F (y t,k i ; \u03be t,k i ) \u2212 cti) in\nSCALLION. Compared to the gradient K\u22121 \u2211K\u22121\nk=0 \u2207F (y t,k i ; \u03be t,k i ) yielded by a single local loop,\nthe momentum variable vt+1i has a smaller variance due to its accumulation nature, thereby refining the convergence behavior under biased compression. Finally, note that similar to SCALLION, SCAFCOM only transmits one compressed variable in the uplink communication, and recovers SCAFFOLD when \u03b2 = 1 and {Ci}Ni=1 are the identity mapping (i.e., no compression). Connection with SCALLION. Notably, the difference between SCAFCOM and SCALLION lies in the utilization of momentum; see the colored highlights in Algorithm 1 and Algorithm 2.\nSpecifically, if we replace line 10 of SCAFCOM with the following formula:\nvt+1i := (1\u2212 \u03b1)cti + \u03b1\n( xt \u2212 yt,Ki\n\u03b7lK + cti \u2212 ct\n) = (1\u2212 \u03b1)cti + \u03b1\nK K\u22121\u2211 k=0 \u2207F (yt,ki ; \u03be t,k i ),\nthen SCAFCOM recovers SCALLION (Algorithm 1) with \u03b2 = \u03b1. Note that in this case, the memorization of vti is no longer needed to be retained, which is consistent with the design of SCALLION. We also remark that the roles of the scaling factor \u03b1 and momentum \u03b2 vary in SCALLION and SCAFCOM. In SCALLION, \u03b1 stabilizes the updates of control variables {cti}Ni=1 while SCAFCOM sets \u03b2 to mainly address the biasedness issue of contractive compressors."
        },
        {
            "heading": "5.2 CONVERGENCE OF SCAFCOM",
            "text": "The convergence of SCAFCOM under q2-contractive compression is established as follows. Theorem 2 (SCAFCOM WITH BIASED COMPRESSION). Under Assumption 1, 2, and q2contractive compressors {Ci}Ni=1, if we initialize c0i = v0i = \u2207fi(x0) and c0 = \u2207f(x0), and set \u03b7l, \u03b7g , and \u03b2 as in (55), then SCAFCOM converges as\n1\nT T\u22121\u2211 t=0 E[\u2225\u2207f(xt)\u22252] \u2272 \u221a L\u2206\u03c32 SKT + ( N2L2\u22062\u03c32 (1\u2212 q)S2KT 2 )1/3 + ( N3L3\u22063\u03c32 (1\u2212 q)2S3KT 3 )1/4 + NL\u2206 (1\u2212 q)ST . Furthermore, it is known that one can convert any \u03c9-unbiased compressor Ci into a q2-contractive compressors with q2 = \u03c91+\u03c9 through scaling 1 1+\u03c9Ci : x 7\u2192 1 1+\u03c9Ci(x) (Safaryan et al., 2022, Lemma 1). Thus, SCAFCOM can also employ unbiased compressors through this scaling:: Corollary 1 (SCAFCOM WITH UNBIASED COMPRESSION). When employing unbiased compressors (after scaling) in communication, the convergence of SCAFCOM is upper bounded by\u221a\nL\u2206\u03c32 SKT +\n( (1 + \u03c9)N2L2\u22062\u03c32\nS2KT 2\n)1/3 + ( (1 + \u03c9)2N3L3\u22063\u03c32\nS3KT 3\n)1/4 + (1 + \u03c9)NL\u2206\nST . (7)\nRemark 1. Corollary 1 is obtained by directly plugging in the relation q2 = \u03c91+\u03c9 into Theorem 2 without exploiting the unbiasedness of compressors. However, it is feasible to refine the 1/T 2/3 and 1/T 3/4 terms in (7) by taking advantage of unbiasedness. We omit the proof here for conciseness.\nAsymptotic complexities of SCAFCOM. Following the result of Theorem 2, SCAFCOM with biased compression has an asymptotic communication complexity of NS(1\u2212q)\u03f5 and an asymptotic computation complexity of 1S\u03f52 to attain E[\u2225\u2207f(x\u0302)\u2225\n2] \u2264 \u03f5. On the other hand, when adopting unbiased compressors with scaling, Corollary 1 reveals that SCAFCOM has an asymptotic communication complexity of N(1+\u03c9)S\u03f5 and an asymptotic computation complexity of 1 S\u03f52 . Comparison with prior compressed FL methods. In Table 1, we compare SCAFCOM with existing FL algorithms with biased compression under full client participation. We see that SCAFCOM outperforms prior results with biased compression in the asymptotic communication complexity by at least a factor 1/(1 \u2212 q). Moreover, inferior to SCAFCOM, the existing FL methods with biased compression cannot tolerate unbounded data heterogeneity or even require homogeneous data. Moreover, QSPARSE-SGD (Basu et al., 2019) and LOCAL-SGD-C (Gao et al., 2021) only converge under full client participation. Notably, when employing unbiased compression, SCAFCOM enhances the asymptotic computation complexity by a factor of 1 + \u03c9 compared to SCALLION, surpassing all prior FL methods with unbiased compression. Furthermore, under partial client participation, our rate is better than that of FED-EF (Li & Li, 2023) by a factor of \u221a N/S, overcoming the drawback of the standard error feedback under partial participation in federated learning.\nBased on discussions in Section 5, we demonstrate that SCAFCOM, as a unified approach, outperforms existing compressed FL methods under both unbiased and biased compression."
        },
        {
            "heading": "6 EXPERIMENTS",
            "text": "We present experiments to demonstrate the efficacy of our proposed methods. Since the saving in communication costs of various compressors has been well justified in literature (see, e.g., Haddadpour et al. (2021); Li & Li (2023)), we focus on (i) validating that SCALLION and SCAFCOM can empirically match SCAFFOLD (full-precision) with substantially reduced communication costs, and (ii) showing that SCALLION and SCAFCOM outperform prior methods with the same communication budget. We defer more experimental details and results to Appendix G.\nWe compared the proposed algorithms with baselines including FED-EF (Li & Li, 2023), FEDCOMGATE (Haddadpour et al., 2021), FED-SGD (also known as FEDAVG (Yang et al., 2021)), and SCAFFOLD (Karimireddy et al., 2020b) on two standard FL datasets: MNIST (LeCun, 1998), FMNIST (Xiao et al., 2017)). Note that FED-SGD and SCAFFOLD employ full-precision (i.e., uncompressed) communication and we implement SCAFFOLD with our new formulation (Algorithm 4) for a fair comparison. We simulate biased compression with TOP-r operators (see Example 3). Specifically, we adopt TOP-0.01 and TOP-0.05, where only the largest 1% and 5% entries in absolute values are transmitted in communication. For unbiased compression, we utilize random dithering (see Example 2), with 2 bits and 4 bits per entry, respectively. Since all the compressed FL methods in our experiments transmit one variable in the uplink communication, their communication costs are essentially the same when the same compressor is applied. Therefore, for clarity of comparisons, we will plot the metrics versus the number of training rounds.\nSCAFCOM with biased compression. In Figure 1, we present the train loss and test accuracy of our proposed SCAFCOM (\u03b2 = 0.2) and FED-EF (Li & Li, 2023), both using biased TOP-r compressors. We observe on both datasets: (i) under the same degree of compression (i.e., the value of r here), SCAFCOM outperforms FED-EF in terms of both training loss and test accuracy, thanks to controlled variables and the local momentum in SCAFCOM; (ii) SCAFCOM with TOP0.01 can achieve very close test accuracy as SCAFFOLD (full-precision), and SCAFCOM with TOP-0.05 match those of SCAFFOLD, leading to the same performance while saving 20 - 100x uplink communication costs; (iii) the performance of SCAFCOM and FED-EF approaches that of the full-precision counterparts (i.e., SCAFFOLD and FED-SGD) as compression assuages.\nSCALLION with unbiased compression. In Figure 2, we plot the same set of experimental results and compare SCALLION (\u03b1 = 0.1) with FedCOMGATE (Haddadpour et al., 2021), both applying unbiased random dithering (Alistarh et al., 2017) with 2 and 4 bits per entry. Similarly, we see that SCALLION outperforms FedCOMGATE under the same compressors. The SCALLION curves of both 2-bit and 4-bit compression basically overlap that of SCAFFOLD, and 4-bit compression slightly performs better than 2-bit compression in later training rounds. Since random dithering also introduces sparsity in compressed output, the 4-bit compressor already provides around 100x communication compression, and the 2-bit compressor saves more communication costs."
        },
        {
            "heading": "7 CONCLUSION",
            "text": "This paper proposes two compressed federated learning (FL) algorithms, SCALLION and SCAFCOM, to support unbiased and biased compression in FL. The proposed methods are built upon our new implementation of stochastic controlled averaging, along with local momentum, and communication compression. Theoretically, under minimal assumptions, SCALLION and SCAFCOM match or improve the state-of-the-art convergence of compressed FL methods. Moreover, SCALLION and SCAFCOM are the first stochastic FL methods, to the best of our knowledge, that exhibit robustness to arbitrary data heterogeneity, partial participation, local updates, and also accommodate communication compression relying solely on standard compressibilities. Empirically, experiments show that SCALLION and SCAFCOM outperform prior compressed FL methods and perform comparably to full-precision FL approaches at a substantially reduced communication cost."
        },
        {
            "heading": "B DETAILED IMPLEMENTATIONS OF SCAFFOLD",
            "text": "Algorithm 3 SCAFFOLD: Stochastic controlled averaging for FL (Karimireddy et al., 2020b) 1: Input: initial model x0 and control variables {c0i }Ni=1, c0; local learning rate \u03b7l; global learning\nrate \u03b7g; local steps K; number of sampled clients S 2: for t = 0, \u00b7 \u00b7 \u00b7 , T \u2212 1 do 3: Uniformly sample clients St \u2286 [N ] with |St| = S 4: for client i \u2208 St in parallel do 5: Receive xt and ct; initialize yt,0i = x t 6: for k = 0, . . . ,K \u2212 1 do 7: Compute a mini-batch gradient gt,ki = \u2207F (y t,k i ; \u03be t,k i ) 8: Locally update yt,k+1i = y t,k i \u2212 \u03b7l(g t,k i \u2212 cti + ct)\n9: end for 10: Update ct+1i = c t i \u2212 ct + xt\u2212yt,Ki \u03b7lK\n(for i /\u2208 St, ct+1i = cti) 11: Send yt,Ki \u2212 xt and c t+1 i \u2212 cti to the server 12: end for 13: Update xt+1 = xt + \u03b7gS \u2211 i\u2208St(y t,K i \u2212 xt)\n14: Update ct+1 = ct + 1N \u2211 i\u2208St(c t+1 i \u2212 cti) 15: end for\nThe original implementation of SCAFFOLD (Karimireddy et al., 2020b) is stated in Algorithm 3 where no compression is employed in communication. In this implementation, each participating client needs to transmit the increments of both local model yt,K \u2212yt,0 and control variable ct+1i \u2212cti to the server at the end of local updates, resulting to two rounds of uplink communication for per training iteration.\nBy communicating the increment variable \u2206ti, we can implement SCAFFOLD equivalently with only a single round of uplink communication for each participating client, as described in Algorithm 4.\nAlgorithm 4 A simplified formulation of SCAFFOLD with single-variable uplink communication 1: Input: initial model x0 and control variables {c0i }Ni=1, c0; local learning rate \u03b7l; global learning\nrate \u03b7g; local steps K; number of sampled clients S 2: for t = 0, \u00b7 \u00b7 \u00b7 , T \u2212 1 do 3: Uniformly sample clients St \u2286 [N ] with |St| = S 4: for client i \u2208 St in parallel do 5: Receive xt and ct; initialize yt,0i = x t 6: for k = 0, . . . ,K \u2212 1 do 7: Compute a mini-batch gradient gt,ki = \u2207F (y t,k i ; \u03be t,k i ) 8: Locally update yt,k+1i = y t,k i \u2212 \u03b7l(g t,k i \u2212 cti + ct)\n9: end for 10: Compute \u2206ti = xt\u2212yt,Ki \u03b7lK\n\u2212 ct and send \u2206ti to the server 11: Update ct+1i = c t i +\u2206 t i (for i /\u2208 St, c t+1 i = c t i) 12: end for 13: Update xt+1 = xt \u2212 \u03b7g\u03b7lKS \u2211 i\u2208St(\u2206 t i + c t)\n14: Update ct+1 = ct + 1N \u2211 i\u2208St \u2206 t i 15: end for"
        },
        {
            "heading": "C MORE DETAILS, DISCUSSIONS, AND COMPARISONS",
            "text": "C.1 UNBIASED AND BIASED COMPRESSORS\nExamples of popular unbiased compressors include:\nExample 1 (RANDOM SPARSIFICATION (WANGNI ET AL., 2018)). For any s \u2208 [d], the random-s sparsification is defined as C : x 7\u2192 ds (\u03be \u2299 x) where \u2299 denotes the entry-wise product and \u03be \u2208 {0, 1}d is a uniformly random binary vector with s non-zero entries. This random-s sparsification is an \u03c9-unbiased compressor with \u03c9 = d/s\u2212 1. Example 2 (RANDOM DITHERING (ALISTARH ET AL., 2017)). For any b \u2208 N+, the random dithering with b-bits per entry is defined as C : x 7\u2192 \u2225x\u2225 \u00d7 sign(x) \u2299 \u03b6(x) where {\u03b6k}dk=1 are independent random variables such that\n\u03b6k(x) :=\n{\u230a 2b|xk|/\u2225x\u2225 \u230b /2b, with probability \u23082b|xk|/\u2225x\u2225\u2309 \u2212 2b|xk|/\u2225x\u2225,\u2308\n2b|xk|/\u2225x\u2225 \u2309 /2b, otherwise,\nwhere \u230a\u00b7\u230b and \u2308\u00b7\u2309 are the floor and ceiling functions, respectively. This random dithering with b-bits per entry is an \u03c9-unbiased compressor with \u03c9 = min{d/4b, \u221a d/2b}.\nCommon biased compressors include (Li & Li, 2023):\nExample 3 (TOP-r OPERATOR). For any r \u2208 [0, 1], the Top-r operator is defined as C : x 7\u2192 (1{k \u2208 Sr(x)}xk)dk=1 where Sr(x) is the set of the largest r \u00d7 d entries of x in absolute values. Top-r operator is a q2-contractive compressor with q2 = 1\u2212 r. Example 4 (GROUPED SIGN). Given a partition of [d] with M groups (e.g., layers of neural networks) {Im}Mm=1, the grouped sign with partition {Im}Mm=1 is defined as C : x 7\u2192\u2211M\nm=1 \u2225xIm\u22251 \u2299 sign(x)Im/|Im|. This grouped sign operator is a q2-contractive compressor with q2 = 1\u2212 1/max1\u2264m\u2264M |Im|.\nC.2 COMPARISON OF SCALLION WITH PRIOR METHODS\nComparison with FEDPAQ (Reisizadeh et al., 2020), FEDCOM (Haddadpour et al., 2021), FEDEF (Li & Li, 2023). All of them boil down to the FEDAVG algorithm (McMahan et al., 2017) when no compression is conducted. As such, their convergence is significantly hampered by data heterogeneity across clients due to client drift. The former two works do not consider partial participation, and FED-EF suffers from an extra slow-down factor in the convergence rate under partial participation. In opposition, SCALLION roots from SCAFFOLD, and is robust to arbitrary data heterogeneity and partial participation.\nComparison with FEDCOMGATE (Haddadpour et al., 2021). FEDCOMGATE applies compression over the uplink communication of the VRL-SGD algorithm (Liang et al., 2019), a gradient-tracking-based FL method which is different from SCAFFOLD. FEDCOMGATE suggests conducting K = O(1/(N\u03f5)) local steps, demanding solving local problems to an extremely accurate resolution. Moreover, it additionally requires uniformly bounded compression errors E[\u2225 1N \u2211 i\u2208[N ] Ci(xi)\u22252\u2212\u2225 1 N \u2211 i\u2208[N ] xi\u22252] \u2264 G2A, which are invalid for practical compressors such as random sparsification (Wangni et al., 2018) and random dithering (Alistarh et al., 2017). In addition, both the convergence for FEDCOMGATE and VRL-SGD is only established when all clients participate in training. It is unclear if their convergence can be adapted to partial client participation. In contrast, SCALLION converges at a state-of-the-art rate that admits a flexible number of local steps and client sampling and employs standard unbiased compressors (i.e., Definition 1).\nC.3 CONNECTION BETWEEN SCAFCOM AND ERROR FEEDBACK\nSCAFCOM does not directly pertain to the vanilla error feedback (Seide et al., 2014; Stich, 2019), a technique widely used to tackle biased compression, but relates to the newly proposed EF21 mechanism (Richta\u0301rik et al., 2021). If one sets \u03b2 = 1 in SCAFCOM, then the message K\u22121 \u2211K\u22121 k=0 \u2207F (y t,k i ; \u03be t,k i ) \u2212 cti would be compressed and the control variable would be updated as ct+1i = c t i + Ci(K\u22121 \u2211K\u22121 k=0 \u2207F (y t,k i ; \u03be t,k i ) \u2212 cti). Under the simplification where \u03c3 = 0 (i.e., full-batch gradients), K = 1 (i.e., no local updates), S = N (i.e., full-client participation), it becomes ct+1i = c t i + Ci(\u2207fi(xt)\u2212 cti) and the global model is updated as xt+1 = xt\u2212 \u03b7g\u03b7lct+1 with\nct+1 = 1N \u2211N i=1 c t+1 i , recovering the recursion of EF21.\nC.4 WEAK ASSUMPTIONS OF SCALLION AND SCAFCOM\nDue to comprehensive challenges in compressed FL, to facilitate convergence analysis, most existing approaches require additional stringent conditions or assumptions that are not necessarily valid in practice, including but not restricted to:\nmax 1\u2264i\u2264N\n\u2225\u2207fi(x)\u2225 \u2264 G, (Bounded Gradient Norm (Basu et al., 2019))\n1\nN N\u2211 i=1 \u2225\u2207fi(x)\u2212\u2207f(x)\u22252 \u2264 \u03b62,\n(Bounded Gradient Dissimilarity (Jiang & Agrawal, 2018; Li & Li, 2023; Gao et al., 2021))\nE \u2225\u2225\u2225\u2225\u2225 1N N\u2211 i=1 Ci(xi) \u2225\u2225\u2225\u2225\u2225 2 \u2212 \u2225\u2225\u2225\u2225\u2225 1N N\u2211 i=1 xi \u2225\u2225\u2225\u2225\u2225 2  \u2264 G2A.\n(Bounded Compression Error (Haddadpour et al., 2021))\nE \u2225\u2225\u2225\u2225\u2225 1N N\u2211 i=1 Ci(xi)\u2212 1 N N\u2211 i=1 xi \u2225\u2225\u2225\u2225\u2225 2  \u2264 q2A \u2225\u2225\u2225\u2225\u2225 1N N\u2211 i=1 xi \u2225\u2225\u2225\u2225\u2225 2 .\n(Averaged Contraction (Alistarh et al., 2018; Li & Li, 2023))\nAs a result, their convergence rates inevitably depend on the large constants G, \u03b6, GA, (1\u2212 qA)\u22121. In contrast, the convergence results of SCALLION and SCAFCOM presented do not rely on any such condition."
        },
        {
            "heading": "D PRELIMINARIES OF PROOFS",
            "text": "Letting \u03b3 \u225c \u03b7g\u03b7lK, the recursion of SCALLION and SCAFCOM can be formulated as xt+1 = xt \u2212 \u03b3d\u0303t+1 where d\u0303t+1 = 1S \u2211 i\u2208St \u03b4\u0303 t i + c t and \u03b4\u0303ti = Ci(\u03b4ti). For clarity, we let gti \u225c 1K \u2211K\u22121 k=0 g t,k i ,\ngt \u225c 1N \u2211N i=1 g t i , d t+1 \u225c 1S \u2211 i\u2208St \u03b4 t i +c t. We will abbreviate \u2211N i=1, \u2211K\u22121 k=0 , \u2211N i=1 \u2211K\u22121 k=0 as \u2211 i,\u2211\nk, \u2211 i,k, respectively, when there is no confusion. We also define the auxiliary variable U t :=\n1 NK \u2211N i=1 \u2211K\u22121 k=1 E[\u2225y t,k i \u2212 xt\u2225]2 to facilitate the analyses. It is worth noting that ct \u2261 1N \u2211N i=1 c t i for all t \u2265 0 in both SCALLION and SCAFCOM. Besides, the exclusive recursions are as follows. For SCALLION. Due to client sampling, it holds that\nct+1i = { cti + Ci(\u03b1(gti \u2212 cti)) if i \u2208 St cti otherwise\n(8)\nand dt+1 = 1S \u2211 i\u2208St \u03b1(g t i \u2212 cti) + ct\nFor SCAFCOM. We additionally let ut+1i \u225c v t i +\u03b2(g t i \u2212 vti). Then, due to client sampling, it holds that\n(vt+1i , c t+1 i ) =\n{ (ut+1i , c t i + Ci(u t+1 i \u2212 cti)) if i \u2208 St\n(vti , c t i) otherwise\n(9)\nand dt+1 = 1S \u2211 i\u2208St(u t+1 i \u2212 cti) + ct. Similarly, we let vt \u225c 1N \u2211N i=1 v t i and u\nt+1 \u225c 1 N \u2211N i=1 u t+1 i = (1\u2212 \u03b2)vt + \u03b2gt.\nLet F\u22121 = \u2205 and F t,ki := \u03c3(\u222a{\u03be t,j i }0\u2264j<k \u222a F t\u22121) and F t := \u03c3((\u222ai\u2208[N ]F t,K i ) \u222a {St}) for all t \u2265 0 where \u03c3(\u00b7) means the \u03c3-algebra. We use E[\u00b7] to indicate the expectation taking all sources of randomness into account. We will frequently use the following fundamental lemmas. Lemma 1 ((Cheng et al., 2024)). Given any a1, \u00b7 \u00b7 \u00b7 , aN , b \u2208 Rd and a = 1N \u2211 i\u2208[N ] ai and uniform sampling S \u2282 [N ] without replacement such that |S| = S, it holds that\nES \u2225\u2225\u2225\u2225\u2225 1S \u2211 i\u2208S ai \u2225\u2225\u2225\u2225\u2225 2  \u2264\u2225a\u22252 + 1 SN \u2211 i \u2225ai \u2212 a\u22252 \u2264 \u2225a\u22252 + 1 SN \u2211 i \u2225ai \u2212 b\u22252.\nLemma 2 ((Karimireddy et al., 2020b)). Suppose {X1, \u00b7 \u00b7 \u00b7 , X\u03c4} \u2286 Rd be random variables that are potentially dependent. If their marginal means and variances satisfy E[Xi] = \u00b5i and E[\u2225Xi \u2212 \u00b5i\u22252] \u2264 \u03c32, then it holds that\nE \u2225\u2225\u2225\u2225\u2225 \u03c4\u2211\ni=1\nXi \u2225\u2225\u2225\u2225\u2225 2  \u2264 \u2225\u2225\u2225\u2225\u2225 \u03c4\u2211 i=1 \u00b5i \u2225\u2225\u2225\u2225\u2225 2 + \u03c42\u03c32.\nIf they are correlated in the Markov sense with conditional means and variances satisfying E[Xi|Xi\u22121, \u00b7 \u00b7 \u00b7X1] = \u00b5i and E[\u2225Xi \u2212 \u00b5i\u22252] \u2264 \u03c32 [XH: check here E[\u2225Xi \u2212 \u00b5i\u22252 | \u00b5i] \u2264 \u03c32]. Then a tighter bound holds\nE \u2225\u2225\u2225\u2225\u2225 \u03c4\u2211\ni=1\nXi \u2225\u2225\u2225\u2225\u2225 2  \u2264 2E \u2225\u2225\u2225\u2225\u2225 \u03c4\u2211\ni=1\n\u00b5i \u2225\u2225\u2225\u2225\u2225 2 + 2\u03c4\u03c32.\nLemma 3. Under Assumption 1, for any \u03b8 \u2208 [0, 1] and v, v1, . . . , vN \u2208 F t\u22121, it holds that E[\u2225(1\u2212 \u03b8)v + \u03b8(gt \u2212\u2207f(xt))\u22252]\n\u2264min { 2E[\u2225(1\u2212 \u03b8)v\u22252] + 3\u03b82L2U t, (1\u2212 \u03b8)E[\u2225v\u22252] + 2\u03b8L2U t } + 2\u03b82\u03c32\nNK (10)\nand 1\nN \u2211 i E[\u2225(1\u2212 \u03b8)vi + \u03b8(gti \u2212\u2207fi(xt))\u22252]\n\u2264min\n{ 2\nN \u2211 i E[\u2225(1\u2212 \u03b8)vi\u22252] + 3\u03b82L2U t, 1\u2212 \u03b8 N \u2211 i\nE[\u2225vi\u22252] + 2\u03b8L2U t } + 2\u03b82\u03c32\nK . (11)\nProof. We mainly focus on proving (10) as (11) can be established similarly. Using Lemma 2, we have\nE[\u2225(1\u2212 \u03b8)v + \u03b8(gt \u2212\u2207f(xt))\u22252]\n=E[\u2225(1\u2212 \u03b8)v\u22252] + E \u2329(1\u2212 \u03b8)v, \u03b8 NK \u2211 i,k \u2207F (yt,ki ; \u03be t,k i )\u2212\u2207fi(x t) \u232a + \u03b82E  \u2225\u2225\u2225\u2225\u2225\u2225 1NK \u2211 i,k \u2207F (yt,ki ; \u03be t,k i )\u2212\u2207fi(x t) \u2225\u2225\u2225\u2225\u2225\u2225 2 \n\u2264E[\u2225(1\u2212 \u03b8)v\u22252] + E \u2329(1\u2212 \u03b8)v, \u03b8 NK \u2211 i,k \u2207fi(yt,ki )\u2212\u2207fi(x t) \u232a + 2\u03b82E  \u2225\u2225\u2225\u2225\u2225\u2225 1NK \u2211 i,k \u2207fi(yt,ki )\u2212\u2207fi(x t) \u2225\u2225\u2225\u2225\u2225\u2225 2 + 2\u03b82\u03c32 NK\n\u2264E  \u2225\u2225\u2225\u2225\u2225\u2225(1\u2212 \u03b8)v + \u03b8NK \u2211 i,k \u2207fi(yt,ki )\u2212\u2207fi(x t) \u2225\u2225\u2225\u2225\u2225\u2225 2 \n+ \u03b82E  \u2225\u2225\u2225\u2225\u2225\u2225 1NK \u2211 i,k \u2207fi(yt,ki )\u2212\u2207fi(x t) \u2225\u2225\u2225\u2225\u2225\u2225 2 + 2\u03b82\u03c32 NK .\nBy further applying Sedrakyan\u2019s inequality \u2225(1 \u2212 \u03b8)v + \u03b8v\u2032\u22252 \u2264 (1 \u2212 \u03b8)\u2225v\u22252 + \u03b8\u2225v\u2032\u22252 and Assumption 1, we have\nE[\u2225(1\u2212 \u03b8)v + \u03b8(gt \u2212\u2207f(xt))\u22252]\n\u2264(1\u2212 \u03b8)E[\u2225v\u22252] + \u03b8(1 + \u03b8)E  \u2225\u2225\u2225\u2225\u2225\u2225 1NK \u2211 i,k \u2207fi(yt,ki )\u2212\u2207fi(x t) \u2225\u2225\u2225\u2225\u2225\u2225 2 + 2\u03b82\u03c32 NK\n\u2264(1\u2212 \u03b8)E[\u2225v\u22252] + 2\u03b8L2U t + 2\u03b8 2\u03c32\nNK .\nThe other upper bound of (10) follows from \u2225(1\u2212 \u03b8)v + \u03b8v\u2032\u22252 \u2264 2\u2225v\u22252 + 2\u03b82\u2225v\u2032\u22252."
        },
        {
            "heading": "E PROOF OF SCALLION",
            "text": "In this section, we prove the convergence result of SCALLION with unbiased compression, where we additionally define x\u22121 := x0. We thus have E[\u2225xt \u2212 xt\u22121\u22252] = 0 for t = 0. Note that x\u22121 is defined for the purpose of notation and is not utilized in our algorithms.\nLemma 4 (DESCENT LEMMA). Under Assumptions 1 and 2, it holds for all t \u2265 0 and \u03b3 > 0 that E[f(xt+1)]\n\u2264E[f(xt)]\u2212 \u03b3 2 E[\u2225\u2207f(xt)\u22252]\n\u2212 ( 1\n2\u03b3 \u2212 L 2\n) E[\u2225xt+1 \u2212 xt\u22252] + 4\u03b3 ( 1 + (1 + \u03c9)\u03b12\nS\n) L2E[\u2225xt \u2212 xt\u22121\u22252]\n+ 8\u03b3(1 + \u03c9)\u03b12\u03c32\nSK + 4\u03b3\n( 1 + (1 + \u03c9)\u03b12\nS\n) L2U t\n+ \u03b3 ( 4E[\u2225ct \u2212\u2207f(xt\u22121)\u22252] + 4(1 + \u03c9)\u03b1 2\nS\n1\nN \u2211 i E[\u2225cti \u2212\u2207fi(xt\u22121)\u22252]\n) . (12)\nProof. By Lemma 2 of Li et al. (2021), we have\nf(xt+1) \u2264 f(xt)\u2212 \u03b3 2 \u2225\u2207f(xt)\u22252 \u2212\n( 1\n2\u03b3 \u2212 L 2\n) \u2225xt+1 \u2212 xt\u22252 + \u03b3\n2 \u2225d\u0303t+1 \u2212\u2207f(xt)\u22252\nwhere d\u0303t+1 = 1S \u2211 i\u2208St \u03b4\u0303 t i + c t. Letting dt+1 \u225c 1S \u2211 i\u2208St \u03b4 t i + c t, we further have\nf(xt+1) \u2264f(xt)\u2212 \u03b3 2 \u2225\u2207f(xt)\u22252 \u2212\n( 1\n2\u03b3 \u2212 L 2\n) \u2225xt+1 \u2212 xt\u22252\n+ \u03b3\u2225d\u0303t+1 \u2212 dt+1\u22252 + \u03b3\u2225dt+1 \u2212\u2207f(xt)\u22252. (13) For \u2225dt+1\u2212\u2207f(xt)\u22252, using Lemma 2 and the fact that ct \u2261 1N \u2211 i c t i and d t+1 = 1S \u2211 i\u2208St \u03b1(g t i\u2212 cti) + c t, we have\nE[\u2225dt+1 \u2212\u2207f(xt)\u22252]\n=E \u2225\u2225\u2225\u2225\u2225ct + 1S \u2211 i\u2208St \u03b1(gti \u2212 cti)\u2212\u2207f(xt) \u2225\u2225\u2225\u2225\u2225 2 \n\u2264E[\u2225(1\u2212 \u03b1)(ct \u2212\u2207f(xt)) + \u03b1(gt \u2212\u2207f(xt))\u22252] + \u03b1 2\nSN \u2211 i E[\u2225gti \u2212 cti\u22252]. (14)\nUsing (10) and Assumption 1, we further have E[\u2225(1\u2212 \u03b1)(ct \u2212\u2207f(xt)) + \u03b1(gt \u2212\u2207f(xt)\u22252]\n\u22642E[\u2225ct \u2212\u2207f(xt)\u22252] + 3\u03b12U t + 2\u03b1 2\u03c32\nNK\n\u22644E[\u2225ct \u2212\u2207f(xt\u22121)\u22252] + 4L2E[\u2225xt \u2212 xt\u22121\u22252] + 3\u03b12L2U t + 2\u03b1 2\u03c32\nNK . (15)\nSimilarly, using (11) and Assumption 1, we have \u03b12\nSN \u2211 i E[\u2225gti \u2212 cti\u22252] \u2264 2\u03b12 SN \u2211 i E[\u2225cti \u2212\u2207fi(xt)\u22252] + 2\u03b12 SN \u2211 i E[\u2225gti \u2212\u2207fi(xt)\u22252]\n\u22642\u03b1 2\nSN \u2211 i E[\u2225cti \u2212\u2207fi(xt)\u22252] + 4\u03b12 S L2U t + 4\u03b12\u03c32 SK\n\u22644\u03b1 2\nSN \u2211 i E[\u2225cti \u2212\u2207fi(xt\u22121)\u22252] + 4\u03b12L2 S E[\u2225xt \u2212 xt\u22121\u22252]\n+ 4\u03b12\nS L2U t +\n4\u03b12\u03c32\nSK . (16)\nPlugging (15) and (16) into (14), we obtain E[\u2225dt+1 \u2212\u2207f(xt)\u22252]\n\u22644E[\u2225ct \u2212\u2207f(xt\u22121)\u22252] + 4\u03b1 2\nS\n1\nN \u2211 i E[\u2225vti \u2212\u2207fi(xt\u22121)\u22252]\n+ 4(1 + S\u22121)\u03b12L2U t + 4 ( 1 + \u03b12\nS\n) L2E[\u2225xt \u2212 xt\u22121\u22252] + (N\u22121 + S\u22121)4\u03b1 2\u03c32\nK . (17)\nFor \u2225d\u0303t+1 \u2212 dt+1\u22252, using mutual independence and Definition 2, we have\nE[\u2225d\u0303t+1 \u2212 dt+1\u22252] =E \u2225\u2225\u2225\u2225\u2225 1S \u2211 i\u2208St Ci(\u03b1(gti \u2212 cti))\u2212 \u03b1(gti \u2212 cti) \u2225\u2225\u2225\u2225\u2225 2 \n\u2264\u03c9\u03b1 2\nS2 E [\u2211 i\u2208St \u2225gti \u2212 cti\u22252 ] = \u03c9\u03b12 S 1 N \u2211 i E[\u2225gti \u2212 cti\u22252].\nThen applying the same relaxation in (16), we obtain\nE[\u2225d\u0303t+1 \u2212 dt+1\u22252] \u22644\u03c9\u03b1 2\nSN \u2211 i E[\u2225cti \u2212\u2207fi(xt\u22121)\u22252] + 4\u03c9\u03b12L2 S E[\u2225xt \u2212 xt\u22121\u22252]\n+ 4\u03c9\u03b12\nS U t +\n4\u03c9\u03b12\u03c32\nSK . (18)\nPlugging (17) and (18) into (13) and noting N\u22121 \u2264 S\u22121, we complete the proof.\nGiven Lemma 4, the rest is to bound \u2225ct \u2212\u2207f(xt\u22121)\u22252, \u2225cti \u2212\u2207fi(xt\u22121)\u22252.\nLemma 5. Under Assumptions 1 and 2, it holds for all t \u2265 0 that E[\u2225ct+1 \u2212\u2207f(xt)\u22252]\n\u2264 ( 1\u2212 S\u03b1\n2N\n) E[\u2225ct \u2212\u2207f(xt)\u22252] + 4(1 + \u03c9)\u03b1 2S\nN2 1 N \u2211 i E[\u2225cti \u2212\u2207fi(xt\u22121)\u22252]\n+\n( 2N\nS\u03b1 +\n4(1 + \u03c9)\u03b12S\nN2\n) L2E[\u2225xt \u2212 xt\u22121\u22252]\n+\n( 2S\nN +\n4(1 + \u03c9)\u03b1S\nN2\n) \u03b1L2U t + 6(1 + \u03c9)\u03b12S\u03c32\nN2K . (19)\nProof. Using (8) and Lemma 1, we have E[\u2225ct+1 \u2212\u2207f(xt)\u22252]\n=E \u2225\u2225\u2225\u2225\u2225 1S \u2211 i\u2208St S N Ci(\u03b1(gti \u2212 cti)) + ct \u2212\u2207f(xt) \u2225\u2225\u2225\u2225\u2225 2 \n\u2264E \u2225\u2225\u2225\u2225\u2225 1S \u2211 i\u2208St S\u03b1 N (gti \u2212 cti) + ct \u2212\u2207f(xt) \u2225\u2225\u2225\u2225\u2225 2 + \u03c9\u03b12 N2 \u2211 i\u2208St E[\u2225gti \u2212 cti\u22252]\n\u2264E [\u2225\u2225\u2225\u2225S\u03b1N (gt \u2212 ct) + ct \u2212\u2207f(xt) \u2225\u2225\u2225\u22252 ] + (1 + \u03c9)\u03b12S N2 1 N \u2211 i E[\u2225gti \u2212 cti\u22252] (20)\nUsing (10), Young\u2019s inequality, and Assumption 1, we further have\nE [\u2225\u2225\u2225\u2225S\u03b1N (gt \u2212 ct) + ct \u2212\u2207f(xt) \u2225\u2225\u2225\u22252 ]\n\u2264 ( 1\u2212 S\u03b1\nN\n) E[\u2225ct \u2212\u2207f(xt)\u22252] + 2\u03b1SL 2\nN U t +\n2\u03b12S2\u03c32\nN3K \u2264 ( 1\u2212 S\u03b1\n2N\n) E[\u2225ct \u2212\u2207f(xt\u22121)\u22252] + 2NL 2\nS\u03b1 E[\u2225xt \u2212 xt\u22121\u22252] + 2\u03b1SL\n2\nN U t +\n2\u03b12S2\u03c32\nN3K .(21)\nUsing Young\u2019s inequality and Assumption 1, we can obtain (1 + \u03c9)\u03b12S\nN2 1 N \u2211 i E[ \u2225\u2225gti \u2212 cti\u2225\u22252]\n\u2264 (1 + \u03c9)\u03b1 2S N2 1 N \u2211 i ( 2E[\u2225cti \u2212\u2207fi(xt)\u22252] + 2E[\u2225gti \u2212\u2207fi(xt)\u22252] ) \u2264 (1 + \u03c9)\u03b1 2S\nN2 1 N \u2211 i ( 4E[\u2225cti \u2212\u2207fi(xt\u22121)\u22252] + 4L2E[\u2225xt \u2212 xt\u22121\u22252] + 2E[\u2225gti \u2212\u2207fi(xt)\u22252] ) .\n(22) Using (11), we have\n1\nN \u2211 i E[\u2225gti \u2212\u2207fi(xt)\u22252] \u2264 2L2U t + 2\u03c32 K . (23)\nPlugging (23) into (22), we reach (1 + \u03c9)\u03b12S\nN2 1 N \u2211 i E[ \u2225\u2225gti \u2212 cti\u2225\u22252]\n\u2264 (1 + \u03c9)\u03b1 2S N2 1 N \u2211 i ( 4E[\u2225cti \u2212\u2207fi(xt\u22121)\u22252] + 4L2E[\u2225xt \u2212 xt\u22121\u22252] + 4L2U t + 4\u03c32 K ) .(24)\nCombining (20), (21), (24) together and using \u03b1 2S2\u03c32 N3K \u2264 (1+\u03c9)\u03b12S\u03c32 N2K completes the proof.\nLemma 6. Under Assumptions 1 and 2, suppose 0 \u2264 \u03b1 \u2264 14(\u03c9+1) , then it holds for all t \u2265 0 that 1\nN \u2211 i E[\u2225ct+1i \u2212\u2207fi(x t)\u22252]\n\u2264 ( 1\u2212 S\u03b1\n4N\n) 1\nN \u2211 i E[\u2225cti \u2212\u2207fi(xt\u22121)\u22252] + 4NL2 S\u03b1 E[\u2225xt \u2212 xt\u22121\u22252]\n+ 3\u03b1L2S\nN U t +\n2(1 + \u03c9)\u03b12S\u03c32\nNK . (25)\nProof. Using (8), we have 1\nN \u2211 i E[\u2225ct+1i \u2212\u2207fi(x t)\u22252]\n= 1\nN \u2211 i (( 1\u2212 S N ) E[\u2225cti \u2212\u2207fi(xt)\u22252] + S N E[\u2225Ci(\u03b1(gti \u2212 cti)) + cti \u2212\u2207fi(xt)\u22252] ) \u2264 1 N \u2211 i (( 1\u2212 S N ) E[\u2225cti \u2212\u2207fi(xt)\u22252] + S N E[\u2225\u03b1(gti \u2212 cti) + cti \u2212\u2207fi(xt)\u22252]\n+ S\u03c9\u03b12\nN E[\u2225gti \u2212 cti\u22252]\n) (26)\nNote that S\nN\n1\nN \u2211 i E[\u2225\u03b1(gti \u2212 cti) + cti \u2212\u2207fi(xt)\u22252]\n= S\nN\n1\nN \u2211 i E[\u2225\u03b1(gti \u2212\u2207fi(xt)) + (1\u2212 \u03b1)(cti \u2212\u2207fi(xt))\u22252]\n\u2264 S N ( 1\u2212 \u03b1 N \u2211 i E[\u2225cti \u2212\u2207fi(xt)\u22252] + 2\u03b1L2U t + 2\u03b12\u03c32 K ) (27)\nand by applying (11),\n\u03c9\u03b12S\nN\n1\nN \u2211 i E[\u2225gti \u2212 cti\u22252] = 4\u03c9\u03b12S N 1 N \u2211 i E [\u2225\u2225\u2225\u222512(gti \u2212\u2207fi(xt))\u2212 12(\u2207fi(xt)\u2212 cti) \u2225\u2225\u2225\u22252 ]\n\u22644\u03c9\u03b1 2S\nN\n( 1\n2N \u2211 i E[\u2225\u2207fi(xt)\u2212 cti\u22252] + L2U t + \u03c32 2K\n) . (28)\nPlugging (27) and (28) into (26), we obtain 1\nN \u2211 i E[\u2225ct+1i \u2212\u2207fi(x t)\u22252]\n\u2264 ( 1\u2212 S\u03b1(1\u2212 2\u03c9\u03b1)\nN\n) 1\nN \u2211 i E[\u2225cti \u2212\u2207fi(xt)\u22252] + (2 + 4\u03c9\u03b1)\u03b1SL2 N U t + 2(1 + \u03c9)\u03b12S\u03c32 NK\n\u2264 ( 1\u2212 S\u03b1\n2N\n) 1\nN \u2211 i E[\u2225cti \u2212\u2207fi(xt)\u22252] + 3\u03b1SL2 N U t + 2(1 + \u03c9)\u03b12S\u03c32 NK\nwhere we use \u03b1 \u2264 14(\u03c9+1) in the last inequality. By further using Young\u2019s inequality and Assumption 1, we obtain\n1\nN \u2211 i E[\u2225ct+1i \u2212\u2207fi(x t)\u22252]\n\u2264 ( 1\u2212 S\u03b1\n4N\n) 1\nN \u2211 i E[\u2225cti \u2212\u2207fi(xt\u22121)\u22252] + 4NL2 S\u03b1 E[\u2225xt \u2212 xt\u22121\u22252]\n+ 3\u03b1L2S\nN U t +\n2(1 + \u03c9)\u03b12S\u03c32\nNK .\nLemma 7. Under Assumptions 1 and 2, it holds for any t \u2265 0 and \u03b7lKL \u2264 12 that U t \u2264 9e 2K2\u03b72l N \u2211 i ( E[\u2225cti \u2212\u2207fi(xt\u22121)\u22252] + L2E[\u2225xt \u2212 xt\u22121\u22252] + E[\u2225\u2207f(xt)\u22252] ) + e2K\u03b72l \u03c3 2.\nProof. The proof is similar to that of Lemma 12.\nTheorem 3. Under Assumptions 1 and 2, suppose clients are associated with mutually independent \u03c9-unbiased compressors, if we initialize c0i = 1 B \u2211B b=1 \u2207F (x0; \u03bebi ), c0 = 1 N \u2211N i=1 c 0 i with {\u03bebi }Bb=1 iid\u223c Di and B \u2273 \u03c3 2 NL\u2206 (c 0 i \u2192 \u2207fi(x0) as B \u2192 \u221e), set\n\u03b7lKL \u2264 \u221a \u03b1(1 + \u03c9)\n1400e2N , \u03b7g\u03b7lKL =\n27\u03b1S\nN ,\n\u03b1 = ( 4(1 + \u03c9) + ( (1 + \u03c9)TS\u03c32\nN2KL\u2206\n)1/2 + ( (1 + \u03c9)T\u03c32\nNKL\u2206\n)1/3)\u22121 ,\n(29)\nthen SCALLION converges as\n1\nT T\u22121\u2211 t=0 E[\u2225\u2207f(xt)\u22252] \u2272 \u221a (1 + \u03c9)L\u2206\u03c32 SKT + ( (1 + \u03c9)N2L2\u22062\u03c32 S3KT 2 )1/3 + (1 + \u03c9)NL\u2206 ST\nwhere \u2206 \u225c f(x0)\u2212minx f(x).\nProof. Adding (19) \u00d7 10\u03b3N\u03b1S to (12), we have\nE[f(xt+1)] + 10\u03b3N\n\u03b1S E[\u2225ct+1 \u2212\u2207f(xt)\u22252] \u2264E[f(xt)] + \u03b3 ( 10N \u03b1S \u2212 1 ) E[\u2225ct \u2212\u2207f(xt\u22121)\u22252]\u2212 \u03b3 2 E[\u2225\u2207f(xt)\u22252]\n\u2212 ( 1\n2\u03b3 \u2212 L 2\n) E[\u2225xt+1 \u2212 xt\u22252] + \u03b3L2 ( 44(1 + \u03c9)\u03b1\nS +\n24N2\n\u03b12S2\n) E[\u2225xt \u2212 xt\u22121\u22252]\n+ \u03b3(1 + \u03c9)\n( 8\u03b12\nS +\n60\u03b1\nN\n) \u03c32\nK + \u03b3L2\n( 24 + 4(1 + \u03c9)\u03b12\nS +\n40(1 + \u03c9)\u03b1\nN\n) U t\n+ 40\u03b3(1 + \u03c9)\u03b1\nN\n1\nN \u2211 i E[\u2225cti \u2212\u2207fi(xt\u22121)\u22252]. (30)\nAdding (25) \u00d7 164\u03b3(1+\u03c9)S to (30), we have\nE[f(xt+1)] + 10\u03b3N \u03b1S E[\u2225ct+1 \u2212\u2207f(xt)\u22252] + 164\u03b3(1 + \u03c9) S 1 N \u2211 i E[\u2225ct+1i \u2212\u2207fi(x t)\u22252]\n\u2264E[f(xt)] + \u03b3 ( 10N \u03b1S \u2212 1 ) E[\u2225ct \u2212\u2207f(xt\u22121)\u22252]\n+\n( 164\u03b3(1 + \u03c9)\nS \u2212 \u03b3(1 + \u03c9) N\n) 1\nN \u2211 i E[\u2225cti \u2212\u2207fi(xt\u22121)\u22252]\n\u2212 \u03b3 2 E[\u2225\u2207f(xt)\u22252]\u2212\n( 1\n2\u03b3 \u2212 L 2\n) E[\u2225xt+1 \u2212 xt\u22252]\n+ \u03b3L2 ( 44(1 + \u03c9)\u03b1\nS +\n24N2 \u03b12S2 + 656(1 + \u03c9)N \u03b1S2\n) E[\u2225xt \u2212 xt\u22121\u22252]\n+ \u03b3(1 + \u03c9)\n( 8\u03b12\nS +\n60\u03b1\nN +\n328(1 + \u03c9)\u03b12\nN\n) \u03c32\nK + \u03b3L2 ( 24 + 4(1 + \u03c9)\u03b12\nS +\n522(1 + \u03c9)\u03b1\nN\n) U t. (31)\nDefining the Lyapunov function (x\u22121 := x0)\n\u03a6t := E[f(xt)] + 10\u03b3N \u03b1S E[\u2225ct \u2212\u2207f(xt\u22121)\u22252] + 164\u03b3(1 + \u03c9) S 1 N \u2211 i E[\u2225cti \u2212\u2207fi(xt\u22121)\u22252],\nfollowing (31), we obtain \u03a6t+1 \u2212 \u03a6t\n\u2264\u2212 \u03b3 2 E[\u2225\u2207f(xt)\u22252]\u2212 \u03b3E[\u2225ct \u2212\u2207f(xt\u22121)\u22252]\u2212 \u03b3(1 + \u03c9) N 1 N \u2211 i E[\u2225cti \u2212\u2207fi(xt\u22121)\u22252]\n\u2212 ( 1\n2\u03b3 \u2212 L 2\n) E[\u2225xt+1 \u2212 xt\u22252]\n+ \u03b3L2 ( 44(1 + \u03c9)\u03b1\nS +\n24N2 \u03b12S2 + 656(1 + \u03c9)N \u03b1S2\n) E[\u2225xt \u2212 xt\u22121\u22252]\n+ \u03b3(1 + \u03c9)\n( 8\u03b12\nS +\n60\u03b1\nN +\n328(1 + \u03c9)\u03b12\nN\n) \u03c32\nK + \u03b3L2 ( 24 + 4(1 + \u03c9)\u03b12\nS +\n522(1 + \u03c9)\u03b1\nN\n) U t. (32)\nSince N \u2265 S \u2265 1 and \u03b1 \u2264 1/(4(1 + \u03c9)), we have\n9e2K2\u03b72l L 2\n( 24 + 4(1 + \u03c9)\u03b12\nS +\n522(1 + \u03c9)\u03b1\nN ) \u22649e2K2\u03b72l L2 ( 24 + 1\n4 +\n261\n2\n) \u2264 1400e2K2\u03b72l L2 \u2264 \u03b1(1 + \u03c9)\nN \u2264 1 4 . (33)\nCombining (33) with Lemma 7, we have \u03b3L2 ( 24 + 4(1 + \u03c9)\u03b12\nS +\n522(1 + \u03c9)\u03b1\nN\n) U t\n\u2264\u03b3E[\u2225ct \u2212\u2207f(xt\u22121)\u22252] + \u03b3(1 + \u03c9) N 1 N \u2211 i E[\u2225cti +\u2207fi(xt\u22121)\u22252]\n+ \u03b3 4 E[\u2225\u2207f(xt)\u22252] + \u03b3L\n2\n4 E[\u2225xt \u2212 xt\u22121\u22252] + \u03b3\u03b1(1 + \u03c9)\u03c3\n2\nNK . (34)\nPlugging (34) into (32), we reach \u03a6t+1 \u2212 \u03a6t\n\u2264\u2212 \u03b3 4 E[\u2225\u2207f(xt)\u22252] + \u03b3(1 + \u03c9)\n( 8\u03b12\nS +\n60\u03b1\nN +\n328(1 + \u03c9)\u03b12\nN\n) \u03c32\nK \u2212 ( 1\n2\u03b3 \u2212 L 2\n) E[\u2225xt+1 \u2212 xt\u22252]\n+ \u03b3L2 ( 44(1 + \u03c9)\u03b1\nS +\n25N2 \u03b12S2 + 656(1 + \u03c9)N \u03b1S2\n) E[\u2225xt \u2212 xt\u22121\u22252]. (35)\nDue to the choice of \u03b3 = \u03b7g\u03b7lK and \u03b1 \u2264 14(\u03c9+1) , it holds that\n1 2\u03b3 \u2265 L 2 + \u03b3L2\n( 44(1 + \u03c9)\u03b1\nS +\n25N2 \u03b12S2 + 656(1 + \u03c9)N \u03b1S2\n) .\nAveraging (35) over k and noting \u2225x0 \u2212 x\u22121\u22252 = 0, \u03b1 = O((1 + \u03c9)\u22121), we obtain\n1\nT T\u22121\u2211 t=0 E[\u2225\u2207f(xt)\u22252] \u2272\u03a6 0 \u2212 \u03a6T+1 \u03b3T + (1 + \u03c9) ( \u03b12 S + \u03b1 N + (1 + \u03c9)\u03b12 N ) \u03c32 K\n\u2272 \u03a60 \u2212 \u03a6T+1\n\u03b3T + (1 + \u03c9)\n( \u03b12\nS +\n\u03b1\nN\n) \u03c32\nK .\nBy the definition of \u03a6t, it holds that\n\u03a60 \u2212 \u03a6T+1\n\u03b3T \u2272 L\u2206 \u03b3T + N \u03b1S E[\u2225c0 \u2212\u2207f(x0)\u22252] T + (1 + \u03c9) S 1 N\n\u2211 i E[\u2225c0i \u2212\u2207fi(x0)\u22252]\nT\n\u2272 L\u2206\nT\nN\n\u03b1S +\n\u03c32\n\u03b1SBT where we use the choice of \u03b3, \u03b1, and the initialization of {c0i }i\u2208[N ] and c0 in the second inequality. Due to the choice of B, we have \u03c3 2\n\u03b1SBT \u2272 L\u2206 T N \u03b1S and thus\n1\nT T\u22121\u2211 t=0 E[\u2225\u2207f(xt)\u22252] \u2272L\u2206 T N \u03b1S + (1 + \u03c9) ( \u03b12 S + \u03b1 N ) \u03c32 K . (36)\nPlugging the choice of \u03b1 into (36) completes the proof."
        },
        {
            "heading": "F PROOF OF SCAFCOM",
            "text": "In this section, we prove the convergence result of SCAFCOM with biased compression, where we additionally define x\u22121 := x0. We thus have E[\u2225xt \u2212 xt\u22121\u22252] = 0 for t = 0. Note that x\u22121 is defined for the purpose of notation and is not utilized in our algorithms.\nLemma 8 (DESCENT LEMMA). Under Assumptions 1 and 2, it holds for all t \u2265 0 and \u03b3 > 0 that E[f(xt+1)]\n\u2264E[f(xt)]\u2212 \u03b3 2 E[\u2225\u2207f(xt)\u22252]\u2212\n( 1\n2\u03b3 \u2212 L 2\n) E[\u2225xt+1 \u2212 xt\u22252] + 16\u03b3L2E[\u2225xt \u2212 xt\u22121\u22252]\n+ \u03b3 ( 4E[\u2225vt \u2212\u2207f(xt\u22121)\u22252] + 12\u03b2 2\nN \u2211 i E[\u2225vti \u2212\u2207fi(xt\u22121)\u22252] + 12 N \u2211 i E[\u2225vti \u2212 cti\u22252]\n)\n+ 8\u03b3\u03b22\u03c32\nK + 9\u03b3\u03b22L2U t. (37)\nProof. By Lemma 2 of Li et al. (2021), we have\nf(xt+1) \u2264 f(xt)\u2212 \u03b3 2 \u2225\u2207f(xt)\u22252 \u2212\n( 1\n2\u03b3 \u2212 L 2\n) \u2225xt+1 \u2212 xt\u22252 + \u03b3\n2 \u2225d\u0303t+1 \u2212\u2207f(xt)\u22252\nwhere d\u0303t+1 = 1S \u2211 i\u2208St \u03b4\u0303 t i + c t. Letting dt+1 \u225c 1S \u2211 i\u2208St \u03b4 t i + c t, we further have\nf(xt+1) \u2264f(xt)\u2212 \u03b3 2 \u2225\u2207f(xt)\u22252 \u2212\n( 1\n2\u03b3 \u2212 L 2\n) \u2225xt+1 \u2212 xt\u22252\n+ \u03b3\u2225d\u0303t+1 \u2212 dt+1\u22252 + \u03b3\u2225dt+1 \u2212\u2207f(xt+1)\u22252. (38) For \u2225dt+1 \u2212 \u2207f(xt+1)\u22252, using Lemma 2 and the fact that ct \u2261 1N \u2211 i c t i and d\nt+1 = 1 S \u2211 i\u2208St(u t+1 i \u2212 cti) + ct, we have\nE[\u2225dt+1 \u2212\u2207f(xt)\u22252]\n=E \u2225\u2225\u2225\u2225\u2225ct + 1S \u2211 i\u2208St (ut+1i \u2212 c t i)\u2212\u2207f(xt) \u2225\u2225\u2225\u2225\u2225 2 \n\u2264E[\u2225ut+1 \u2212\u2207f(xt)\u22252] + 1 SN \u2211 i E[\u2225ut+1i \u2212 c t i\u22252]\n=E[\u2225(1\u2212 \u03b2)(vt \u2212\u2207f(xt)) + \u03b2(gt \u2212\u2207f(xt))\u22252] + 1 SN \u2211 i E[\u2225vti + \u03b2(gti \u2212 vti)\u2212 cti\u22252].(39)\nUsing (10) and Assumption 1, we have E[\u2225(1\u2212 \u03b2)(vt \u2212\u2207f(xt)) + \u03b2(gt \u2212\u2207f(xt)\u22252]\n\u22642E[\u2225vt \u2212\u2207f(xt)\u22252] + 3\u03b22L2U t + 2\u03b2 2\u03c32\nNK\n\u22644E[\u2225vt \u2212\u2207f(xt\u22121)\u22252] + 4L2E[\u2225xt \u2212 xt\u22121\u22252] + 3\u03b22L2U t + 2\u03b2 2\u03c32\nNK . (40)\nSimilarly, using (11) and Assumption 1, we have 1\nSN \u2211 i E[\u2225vti + \u03b2(gti \u2212 vti)\u2212 cti\u22252]\n= 1\nSN \u2211 i E[\u2225vti + \u03b2(\u2207fi(xt)\u2212\u2207fi(xt\u22121)) + \u03b2(\u2207fi(xt\u22121)\u2212 vti) + \u03b2(gti \u2212\u2207fi(xt))\u2212 cti\u22252]\n\u2264 2 SN \u2211 i E[\u2225vti + \u03b2(\u2207fi(xt)\u2212\u2207fi(xt\u22121)) + \u03b2(\u2207fi(xt\u22121)\u2212 vti)\u2212 cti\u22252] + 3\u03b22L2U t S + 2\u03b22\u03c32 SK\n\u2264 6 SN \u2211 i ( E[\u2225vti \u2212 cti\u22252] + \u03b22E[\u2225vti \u2212\u2207fi(xt\u22121)\u22252] + \u03b22L2E[\u2225xt \u2212 xt\u22121\u22252] ) + 3\u03b22L2U t S + 2\u03b22\u03c32 SK . (41) Plugging (40) and (41) into (39), we obtain\nE[\u2225dt+1 \u2212\u2207f(xt)\u22252]\n\u22644E[\u2225vt \u2212\u2207f(xt\u22121)\u22252] + 6\u03b2 2\nS\n1\nN \u2211 i E[\u2225vti \u2212\u2207fi(xt\u22121)\u22252] + 6 SN \u2211 i E[\u2225vti \u2212 cti\u22252]\n+ 3(1 + S\u22121)\u03b22L2U t + ( 4 + 6\u03b22\nS\n) L2E[\u2225xt \u2212 xt\u22121\u22252] + (N\u22121 + S\u22121)2\u03b2 2\u03c32\nK . (42)\nFor \u2225d\u0303t+1 \u2212 dt+1\u22252, using Young\u2019s inequality and Definition 2, we have\nE[\u2225d\u0303t+1 \u2212 dt+1\u22252] =E \u2225\u2225\u2225\u2225\u2225 1S \u2211 i\u2208St Ci(ut+1i \u2212 c t i)\u2212 (ut+1i \u2212 c t i) \u2225\u2225\u2225\u2225\u2225 2 \n\u2264q 2\nS E [\u2211 i\u2208St \u2225ut+1i \u2212 c t i\u22252 ] = q2 N \u2211 i E[\u2225ut+1i \u2212 c t i\u22252]\n= q2\nN \u2211 i E[\u2225vti + \u03b2(gti \u2212 vti)\u2212 cti\u22252].\nThen applying the same relaxation in (41), we obtain\nE[\u2225d\u0303t+1 \u2212 dt+1\u22252]\n\u22646q 2\nN \u2211 i ( E[\u2225vti \u2212 cti\u22252] + \u03b22E[\u2225vti \u2212\u2207fi(xt\u22121)\u22252] + \u03b22L2E[\u2225xt \u2212 xt\u22121\u22252] ) + 3\u03b22q2L2U t + 2\u03b22q2\u03c32\nK . (43)\nPlugging (42) and (43) into (38) and noting N\u22121 \u2264 S\u22121 \u2264 1, q2 \u2264 1, we complete the proof.\nGiven Lemma 8, the rest is to bound \u2225vt \u2212\u2207f(xt\u22121)\u22252, \u2225vti \u2212\u2207fi(xt\u22121)\u22252, and \u2225vti \u2212 cti\u22252.\nLemma 9. Under Assumptions 1 and 2, it holds for all t \u2265 0 that E[\u2225vt+1 \u2212\u2207f(xt)\u22252]\n\u2264 ( 1\u2212 S\u03b2\n2N\n) E[\u2225vt \u2212\u2207f(xt\u22121)\u22252] + 4\u03b2 2S\nN2 1 N \u2211 i E[\u2225vti \u2212\u2207fi(xt\u22121)\u22252]\n+ 6NL2 S\u03b2 E[\u2225xt \u2212 xt\u22121\u22252] + 6\u03b2SL 2 N U t + 6\u03b22S\u03c32 N2K . (44)\nProof. Using (9) and Lemma 1, we have E[\u2225vt+1 \u2212\u2207f(xt)\u22252]\n=E \u2225\u2225\u2225\u2225\u2225 1S \u2211 i\u2208St S\u03b2 N (gti \u2212 vti) + vt \u2212\u2207f(xt) \u2225\u2225\u2225\u2225\u2225 2 \n\u2264E [\u2225\u2225\u2225\u2225S\u03b2N (gt \u2212 vt) + vt \u2212\u2207f(xt) \u2225\u2225\u2225\u22252 ] + \u03b22S N2 1 N \u2211 i E[ \u2225\u2225gti \u2212 vti\u2225\u22252]\n=E [\u2225\u2225\u2225\u2225(1\u2212 S\u03b2N ) (vt \u2212\u2207f(xt)) + S\u03b2 N (gt \u2212\u2207f(xt)) \u2225\u2225\u2225\u22252 ] + \u03b22S N2 1 N \u2211 i E[ \u2225\u2225gti \u2212 vti\u2225\u22252]. (45)\nUsing (10), Young\u2019s inequality, and Assumption 1, we further have\nE [\u2225\u2225\u2225\u2225(1\u2212 S\u03b2N ) (vt \u2212\u2207f(xt)) + S\u03b2 N (gt \u2212\u2207f(xt)) \u2225\u2225\u2225\u22252 ]\n\u2264 ( 1\u2212 S\u03b2\nN\n) E[\u2225vt \u2212\u2207f(xt)\u22252] + 2\u03b2SL 2\nN U t +\n2\u03b22S2\u03c32\nN3K \u2264 ( 1\u2212 S\u03b2\n2N\n) E[\u2225vt \u2212\u2207f(xt\u22121)\u22252] + 2NL 2\nS\u03b2 E[\u2225xt \u2212 xt\u22121\u22252] + 2\u03b2SL\n2\nN U t +\n2\u03b22S2\u03c32\nN3K .(46)\nUsing Young\u2019s inequality and Assumption 1, we can obtain \u03b22S\nN2 1 N \u2211 i E[ \u2225\u2225gti \u2212 vti\u2225\u22252]\n\u2264\u03b2 2S N2 1 N \u2211 i ( 2E[\u2225\u2207fi(xt)\u2212 vti\u22252] + 2E[\u2225gti \u2212\u2207fi(xt)\u22252] ) \u2264\u03b2 2S\nN2 1 N \u2211 i ( 4E[\u2225vti \u2212\u2207fi(xt\u22121)\u22252] + 4L2E[\u2225xt \u2212 xt\u22121\u22252] + 2E[\u2225gti \u2212\u2207fi(xt)\u22252] ) . (47)\nUsing Lemma 2 and Assumption 1, we have 1\nN \u2211 i E[\u2225gti \u2212\u2207fi(xt)\u22252]\n\u2264 2 N \u2211 i E \u2225\u2225\u2225\u2225\u2225 1K \u2211 k \u2207fi(yt,ki )\u2212\u2207fi(x t) \u2225\u2225\u2225\u2225\u2225 2 + 2\u03c32 K \u2264 2L2U t + 2\u03c3 2 K . (48)\nPlugging (48) into (47), we reach \u03b22S\nN2 1 N \u2211 i E[ \u2225\u2225gti \u2212 vti\u2225\u22252]\n\u2264\u03b2 2S N2 1 N \u2211 i ( 4E[\u2225vti \u2212\u2207fi(xt\u22121)\u22252] + 4L2E[\u2225xt \u2212 xt\u22121\u22252] + 4L2U t + 4\u03c32 K ) . (49)\nCombining (45), (46), (49) together and using \u03b2 2S2\u03c32 N3K \u2264 \u03b22S\u03c32 N2K , \u03b22SL2 N2 \u2264 NL2 S\u03b2 , \u03b22SL2 N2 \u2264 \u03b2SL2 N completes the proof.\nLemma 10. Under Assumptions 1 and 2, it holds for all t \u2265 0 that 1\nN \u2211 i E[\u2225vt+1i \u2212\u2207fi(x t)\u22252]\n\u2264 ( 1\u2212 S\u03b2\n2N\n) 1\nN \u2211 i E[\u2225vti \u2212\u2207fi(xt\u22121)\u22252] + 2NL2 S\u03b2 E[\u2225xt \u2212 xt\u22121\u22252]\n+ 2\u03b2L2S\nN U t +\n2\u03b22S\u03c32\nNK . (50)\nProof. Using (9), we have 1\nN \u2211 i E[\u2225vt+1i \u2212\u2207fi(x t)\u22252]\n= 1\nN \u2211 i (( 1\u2212 S N ) E[\u2225vti \u2212\u2207fi(xt)\u22252] + S N E [\u2225\u2225\u03b2(gti \u2212 vti) + vti \u2212\u2207fi(xt)\u2225\u22252])\n\u2264 ( 1\u2212 S\u03b2\nN\n) 1\nN \u2211 i E[\u2225vti \u2212\u2207fi(xt)\u22252] + 2\u03b2L2S N U t + 2\u03b22S\u03c32 NK\nwhere the inequality due to 1\nN \u2211 i E [\u2225\u2225\u03b2(gti \u2212 vti) + vti \u2212\u2207fi(xt)\u2225\u22252]\n= 1\nN \u2211 i E [\u2225\u2225\u03b2(gti \u2212\u2207fi(xt)) + (1\u2212 \u03b2)(vti \u2212\u2207fi(xt))\u2225\u22252]\n\u22641\u2212 \u03b2 N \u2211 i E[\u2225vti \u2212\u2207fi(xt)\u22252] + 2\u03b2L2U t + 2\u03b22\u03c32 K\nby applying (11). By further using Young\u2019s inequality and Assumption 1, we obtain 1\nN \u2211 i E[\u2225vt+1i \u2212\u2207fi(x t)\u22252]\n\u2264 ( 1\u2212 S\u03b2\n2N\n) 1\nN \u2211 i E[\u2225vti \u2212\u2207fi(xt\u22121)\u22252] + 2NL2 S\u03b2 E[\u2225xt \u2212 xt\u22121\u22252]\n+ 2\u03b2L2S\nN U t +\n2\u03b22S\u03c32\nNK .\nLemma 11. Under Assumptions 1 and 2, it holds for all t \u2265 0 that 1\nN \u2211 i E[\u2225vt+1i \u2212 c t+1 i \u2225 2]\n\u2264 ( 1\u2212 S(1\u2212 q)\nN\n) 1\nN \u2211 i E[\u2225vti \u2212 cti\u22252] + 4\u03b22q2S (1\u2212 q)N 1 N \u2211 i E[\u2225vti \u2212\u2207fi(xt\u22121)\u22252]\n+ 4\u03b22L2q2S (1\u2212 q)N E[\u2225xt \u2212 xt\u22121\u22252] + 3\u03b2 2q2SL2 (1\u2212 q)N U t + 2\u03b22q2S\u03c32 NK . (51)\nProof. Using (9) and Definition 2, we have\nE[\u2225vt+1i \u2212 c t+1 i \u2225 2] =\n( 1\u2212 S\nN\n) E[\u2225vti \u2212 cti\u22252] + S\nN E[\u2225ut+1i \u2212 Ci(u t+1 i \u2212 c t i)\u2212 cti)\u22252]\n\u2264 ( 1\u2212 S\nN\n) E[\u2225vti \u2212 cti\u22252] + S\nN\nq2\nN \u2211 i E[\u2225ut+1i \u2212 c t i\u22252]\n= ( 1\u2212 S\nN\n) E[\u2225vti \u2212 cti\u22252] + S\nN\nq2\nN \u2211 i E[\u2225vti + \u03b2(gti \u2212 vti)\u2212 cti\u22252] (52)\nwhere ut+1i \u225c v t i + \u03b2(g t i \u2212 vti). Using Lemma 2 and Assumption 1, we have\nq2\nN \u2211 i E[\u2225vti + \u03b2(gti \u2212 vti)\u2212 cti\u22252] = q2 N \u2211 i E[\u2225vti + \u03b2(\u2207fi(xt)\u2212 vti) + \u03b2(gti \u2212\u2207fi(xt))\u2212 cti\u22252]\n= q2\nN \u2211 i ( E[\u2225vti \u2212 cti + \u03b2(\u2207fi(xt)\u2212 vti)\u22252]\n+ 2\u03b2E [\u2329 vti \u2212 cti + vti + \u03b2(\u2207fi(xt)\u2212 vti), 1\nK \u2211 k \u2207fi(yt,ki )\u2212\u2207fi(x t)\n\u232a]\n+\u03b22E \u2225\u2225\u2225\u2225\u2225 1K \u2211 k \u2207F (yt,ki ; \u03be t,k i )\u2212\u2207fi(x t) \u2225\u2225\u2225\u2225\u2225 2 \n\u2264q 2\nN \u2211 i ( E[\u2225vti \u2212 cti + \u03b2(\u2207fi(xt)\u2212 vti)\u22252]\n+ 2\u03b2E [\u2329 vti \u2212 cti + vti + \u03b2(\u2207fi(xt)\u2212 vti), 1\nK \u2211 k \u2207fi(yt,ki )\u2212\u2207fi(x t)\n\u232a]\n+2\u03b22E \u2225\u2225\u2225\u2225\u2225 1K \u2211 k \u2207fi(yt,ki )\u2212\u2207fi(x t) \u2225\u2225\u2225\u2225\u2225 2 + 2\u03b22\u03c32 K  \u2264q 2\nN \u2211 i E \u2225\u2225\u2225\u2225\u2225vti \u2212 cti + \u03b2(\u2207fi(xt)\u2212 vti) + \u03b2K \u2211 k (\u2207fi(yt,ki )\u2212\u2207fi(x t)) \u2225\u2225\u2225\u2225\u2225 2 \n+ \u03b22q2L2U t + 2\u03b22q2\u03c32\nK .\nBy further using Sedrakyan\u2019s inequality and Assumption 1, we obtain q2\nN \u2211 i E[\u2225vti + \u03b2(gti \u2212 vti)\u2212 cti\u22252]\n\u2264q 2\nN \u2211 i ( 1 q E[\u2225vti \u2212 cti\u22252] + 2\u03b22 1\u2212 q E[\u2225\u2207fi(xt)\u2212 vti\u22252]\n+ 2\u03b22\n1\u2212 q E \u2225\u2225\u2225\u2225\u2225 1K \u2211 k \u2207fi(yt,ki )\u2212\u2207fi(x t) \u2225\u2225\u2225\u2225\u2225 2 + \u03b22q2L2U t + 2\u03b22q2\u03c32 K\n\u2264q 2\nN \u2211 i\n( 1\nq E[\u2225vti \u2212 cti\u22252] +\n4\u03b22\n1\u2212 q E[\u2225vti \u2212\u2207fi(xt\u22121)\u22252] +\n4\u03b22L2\n1\u2212 q E[\u2225xt \u2212 xt\u22121\u22252]\n+ 2\u03b22L2 1\u2212 q 1 K \u2211 i E[\u2225yt,ki \u2212 x t\u22252]\n) + \u03b22q2L2U t + 2\u03b22q2\u03c32\nK\n\u2264 q N \u2211 i E[\u2225vti \u2212 cti\u22252] + 4\u03b22q2 1\u2212 q 1 N \u2211 i E[\u2225\u2207fi(xt\u22121)\u2212 vti\u22252] + 4\u03b22L2q2 1\u2212 q E[\u2225xt \u2212 xt\u22121\u22252]\n+ ( 1 + 2\n1\u2212 q\n) \u03b22q2L2U t + 2\u03b22q2\u03c32\nK . (53)\nBy combining (53) with (52) and using 1 \u2264 1/(1\u2212 q), we finish the proof.\nLemma 12. Under Assumptions 1 and 2, it holds for any t \u2265 0 and \u03b7lKL \u2264 12 that U t\n\u22649e 2K2\u03b72l N \u2211 i ( E[\u2225cti \u2212 vti\u22252] + E[\u2225vti \u2212\u2207fi(xt\u22121)\u22252] + L2E[\u2225xt \u2212 xt\u22121\u22252] + E[\u2225\u2207f(xt)\u22252] ) + e2K\u03b72l \u03c3 2.\nProof. When K = 1, U t = 0 trivially for all t \u2265 0 so we consider K \u2265 2 below. Using Young\u2019s inequality, we have\nE[\u2225yt,k+1i \u2212 x t\u22252] =E[\u2225yt,ki \u2212 \u03b7l(g t,k i \u2212 c t i + c t)\u2212 xt\u22252] \u2264E[\u2225yt,ki \u2212 \u03b7l(\u2207f(y t,k i )\u2212 c t i + c t)\u2212 xt\u22252] + \u03b72l \u03c32\n\u2264 ( 1 + 1\nK \u2212 1\n) E[\u2225yt,ki \u2212 x t\u22252] +K\u03b72l E[\u2225\u2207f(y t,k i )\u2212 c t i + c t\u22252] + \u03b72l \u03c32.\nBy further using Young\u2019s inequality and Assumption 1, we obtain\nK\u03b72l E[\u2225\u2207f(y t,k i )\u2212 c t i + c t\u22252] =K\u03b72l E[\u2225\u2207f(y t,k i )\u2212\u2207fi(x\nt)\u2212 (cti \u2212\u2207fi(xt)) + ct \u2212\u2207f(xt) +\u2207f(xt)\u22252] \u22643K\u03b72l L2E[\u2225y t,k i \u2212 x\nt\u22252] + 3K\u03b72l E[\u2225cti \u2212\u2207fi(xt)\u2212 ct +\u2207f(xt)\u22252] + 3K\u03b72l E[\u2225\u2207f(xt)\u22252]. Using Young\u2019s inequality, we have\n3K\u03b72l N \u2211 i E[\u2225cti \u2212\u2207fi(xt)\u2212 ct +\u2207f(xt)\u22252]\n\u22643K\u03b7 2 l\nN \u2211 i E[\u2225cti \u2212\u2207fi(xt)\u22252]\n\u22649K\u03b7 2 l\nN \u2211 i ( E[\u2225cti \u2212 vti\u22252] + E[\u2225vti \u2212\u2207fi(xt\u22121)\u22252] + L2E[\u2225xt \u2212 xt\u22121\u22252] ) By combining the above inequalities together, we have\n1\nN \u2211 i E[\u2225yt,k+1i \u2212 x t\u22252]\n\u2264 ( 1 + 1\nK \u2212 1 + 3K\u03b72l L 2\n) 1\nN \u2211 i E[\u2225yt,ki \u2212 x t\u22252] + \u03b72l \u03c32\n+ 9K\u03b72l N \u2211 i ( E[\u2225cti \u2212 vti\u22252] + E[\u2225vti \u2212\u2207fi(xt\u22121)\u22252] + L2E[\u2225xt \u2212 xt\u22121\u22252] + E[\u2225\u2207f(xt)\u22252] )\n\u2264 \u00b7 \u00b7 \u00b7 \u2264 k\u2211\n\u2113=0\n( 1 + 1\nK \u2212 1 + 3K\u03b72l L 2 )\u2113( 9K\u03b72l N \u2211 i ( E[\u2225cti \u2212 vti\u22252] + E[\u2225vti \u2212\u2207fi(xt\u22121)\u22252]\n+ L2E[\u2225xt \u2212 xt\u22121\u22252] + E[\u2225\u2207f(xt)\u22252] ) + \u03b72l \u03c3 2\n)\n\u2264 k\u2211\n\u2113=0\n( 1 + 2\nK \u2212 1 )\u2113( 9K\u03b72l N \u2211 i ( E[\u2225cti \u2212 vti\u22252] + E[\u2225vti \u2212\u2207fi(xt\u22121)\u22252]\n+ L2E[\u2225xt \u2212 xt\u22121\u22252] + E[\u2225\u2207f(xt)\u22252] ) + \u03b72l \u03c3 2 ) , (54)\nwhere we use \u03b7lKL \u2264 12 so that 3K\u03b7 2 l L 2 \u2264 1K\u22121 in the last inequality. Iterating and averaging (54) over k = 0, . . . ,K \u2212 1, we obtain\nU t \u2264 1 K \u2211 k k\u22121\u2211 \u2113=0 ( 1 + 2 K \u2212 1 )\u2113( 9K\u03b72l N \u2211 i ( E[\u2225cti \u2212 vti\u22252] + E[\u2225vti \u2212\u2207fi(xt\u22121)\u22252]\n+L2E[\u2225xt \u2212 xt\u22121\u22252] + E[\u2225\u2207f(xt)\u22252] ) + \u03b72l \u03c3 2\n)\n\u2264 K\u22122\u2211 \u2113=0 ( 1 + 2 K \u2212 1 )K\u22121( 9K\u03b72l N \u2211 i ( E[\u2225cti \u2212 vti\u22252] + E[\u2225vti \u2212\u2207fi(xt\u22121)\u22252]\n+L2E[\u2225xt \u2212 xt\u22121\u22252] + E[\u2225\u2207f(xt)\u22252] ) + \u03b72l \u03c3 2\n)\n\u2264e2K\u03b72l \u03c32 + 9e2K2\u03b72l\nN\n\u2211 i ( E[\u2225cti \u2212 vti\u22252] + E[\u2225vti \u2212\u2207fi(xt\u22121)\u22252]\n+L2E[\u2225xt \u2212 xt\u22121\u22252] + E[\u2225\u2207f(xt)\u22252] )\nwhere we use the fact ( 1 + 2K\u22121 )2 \u2264 e2 in the last inequality.\nTheorem 4. Under Assumptions 1 and 2, supposing clients are associated with q2-contractive compressors, if we initialize c0i = v 0 i = 1 B \u2211B b=1 \u2207F (x0; \u03bebi ), c0 = 1 N \u2211N i=1 c 0 i with {\u03bebi }Bb=1\niid\u223c Di and B \u2273 \u03c3 2\n(1\u2212q)L\u2206 (c 0 i \u2192 \u2207fi(x0) as B \u2192 \u221e), set\n\u03b7lKL \u2264\n\u221a \u03b2(1\u2212 q)2\n36e2N(189(1\u2212 q)2 + 306\u03b22) , \u03b7g\u03b7lKL =\n( 20N\n\u03b2S +\n28N\n(1\u2212 q)S\n)\u22121 ,\n\u03b2 = ( 1 + ( TS\u03c32\nN2KL\u2206\n)1/2 + ( TS\u03c32\nNK(1\u2212 q)L\u2206\n)1/3 + ( TS\u03c32\nNK(1\u2212 q)2L\u2206\n)1/4)\u22121 ,\n(55)\nthen SCAFCOM converges as\n1\nT T\u22121\u2211 t=0 E[\u2225\u2207f(xt)\u22252] \u2272 \u221a L\u2206\u03c32 SKT + ( N2L2\u22062\u03c32 (1\u2212 q)S2KT 2 )1/3 + ( N3L3\u22063\u03c32 (1\u2212 q)2S3KT 3 )1/4 + NL\u2206 (1\u2212 q)ST\nwhere \u2206 \u225c f(x0)\u2212minx f(x).\nProof. Adding (44) \u00d7 8\u03b3N\u03b2S + (51) \u00d7 13\u03b3N (1\u2212q)S to (37), we have\nE[f(xt+1)] + 8\u03b3N \u03b2S E[\u2225vt+1 \u2212\u2207f(xt)\u22252] + 14\u03b3N (1\u2212 q)S 1 N \u2211 i E[\u2225vt+1i \u2212 c t+1 i \u2225 2]\n\u2264E[f(xt)] + 8\u03b3N \u03b2S\nE[\u2225vt \u2212\u2207f(xt\u22121)\u22252] + \u03b3 ( 13N (1\u2212 q)S \u2212 1 ) 1 N \u2211 i E[\u2225vti \u2212 cti\u22252]\n\u2212 \u03b3 2 E[\u2225\u2207f(xt)\u22252]\u2212\n( 1\n2\u03b3 \u2212 L 2\n) E[\u2225xt+1 \u2212 xt\u22252]\n+ \u03b3L2 ( 16 + 48N2\n\u03b22S2 +\n52\u03b22q2\n(1\u2212 q)2\n) E[\u2225xt \u2212 xt\u22121\u22252]\n+ \u03b3 ( 8\u03b22 + 48\u03b2\nN +\n26\u03b22q2\n1\u2212 q\n) \u03c32\nK + \u03b3L2\n( 9\u03b22 + 48 + 39\u03b22q2\n(1\u2212 q)2\n) U t\n+ \u03b3 ( 12\u03b22 + 32\u03b2\nN +\n52\u03b22q2\n(1\u2212 q)2\n) 1\nN \u2211 i E[\u2225vti \u2212\u2207fi(xt\u22121)\u22252].\nUsing q, \u03b2 \u2208 [0, 1] and 1 \u2264 S \u2264 N to simplify coefficients, we obtain\nE[f(xt+1)] + 8\u03b3N \u03b2S E[\u2225vt+1 \u2212\u2207f(xt)\u22252] + 13\u03b3N (1\u2212 q)S 1 N \u2211 i E[\u2225vt+1i \u2212 c t+1 i \u2225 2]\n\u2264E[f(xt)] + 8\u03b3N \u03b2S\nE[\u2225vt \u2212\u2207f(xt\u22121)\u22252] + \u03b3 ( 13N (1\u2212 q)S \u2212 1 ) 1 N \u2211 i E[\u2225vti \u2212 cti\u22252]\n\u2212 \u03b3 2 E[\u2225\u2207f(xt)\u22252]\u2212\n( 1\n2\u03b3 \u2212 L 2\n) E[\u2225xt+1 \u2212 xt\u22252]\n+ \u03b3L2 ( 64N2\n\u03b22S2 +\n52\u03b22\n(1\u2212 q)2\n) E[\u2225xt \u2212 xt\u22121\u22252]\n+ \u03b3\n( 48\u03b2\nN +\n26\u03b22\n1\u2212 q\n) \u03c32\nK + \u03b3L2\n( 48 + 39\u03b22\n(1\u2212 q)2\n) U t\n+ \u03b3\n( 32\u03b2\nN +\n64\u03b22\n(1\u2212 q)2\n) 1\nN \u2211 i E[\u2225vti \u2212\u2207fi(xt\u22121)\u22252]. (56)\nNow adding (50) \u00d7 66\u03b3( 1S + 2\u03b2N (1\u2212q)2S ) to (56) and defining the Lyapunov function (x \u22121 := x0)\n\u03a8t :=E[f(xt)] + 8\u03b3N\n\u03b2S E[\u2225vt \u2212\u2207f(xt\u22121)\u22252]\n+ 13\u03b3N (1\u2212 q)S 1 N \u2211 i E[\u2225vti \u2212 cti\u22252] + 66\u03b3 ( 1 S + 2\u03b2N (1\u2212 q)2S ) 1 N \u2211 i E[\u2225vti \u2212\u2207fi(xt\u22121)\u22252],\nwe obtain \u03a8t+1 \u2212\u03a8t\n\u2264\u2212 \u03b3\n( 1\nN \u2211 i E[\u2225vti \u2212 cti\u22252] + ( \u03b2 N + 2\u03b22 (1\u2212 q)2 ) 1 N \u2211 i E[\u2225vti \u2212\u2207fi(xt\u22121)\u22252] )\n\u2212 \u03b3 2 E[\u2225\u2207f(xt)\u22252]\u2212\n( 1\n2\u03b3 \u2212 L 2\n) E[\u2225xt+1 \u2212 xt\u22252]\n+ \u03b3L2 ( 64N2\n\u03b22S2 +\n52\u03b22\n(1\u2212 q)2 + 132\n( N\n\u03b2S2 +\n2N2\n(1\u2212 q)2S2\n)) E[\u2225xt \u2212 xt\u22121\u22252]\n+ \u03b3\n( 32\u03b2\nN +\n64\u03b22 1\u2212 q + 132\n( \u03b22\nN +\n2\u03b23\n(1\u2212 q)2\n)) \u03c32\nK + \u03b3L2 ( 48 + 39\u03b22\n(1\u2212 q)2 + 132\n( \u03b2\nN +\n2\u03b22\n(1\u2212 q)2\n)) U t.\nUsing q, \u03b2 \u2208 [0, 1] and 1 \u2264 S \u2264 N to simplify coefficients, we obtain \u03a8t+1 \u2212\u03a8t\n\u2264\u2212 \u03b3\n( 1\nN \u2211 i E[\u2225vti \u2212 cti\u22252] + ( \u03b2 N + 2\u03b22 (1\u2212 q)2 ) 1 N \u2211 i E[\u2225vti \u2212\u2207fi(xt\u22121)\u22252] )\n\u2212 \u03b3 2 E[\u2225\u2207f(xt)\u22252]\u2212\n( 1\n2\u03b3 \u2212 L 2\n) E[\u2225xt+1 \u2212 xt\u22252]\n+ \u03b3L2 ( 196N2\n\u03b22S2 +\n52\u03b22\n(1\u2212 q)2 +\n264N2\n(1\u2212 q)2S2\n) E[\u2225xt \u2212 xt\u22121\u22252]\n+ \u03b3\n( 164\u03b2\nN +\n64\u03b22 1\u2212 q + 264\u03b23 (1\u2212 q)2\n) \u03c32\nK + \u03b3L2\n( 180 + 303\u03b22\n(1\u2212 q)2\n) U t. (57)\nUsing 9e2K2\u03b72l L 2 ( 180 + 303\u03b2 2 (1\u2212q)2 ) \u2264 \u03b24N and Lemma 12, we have\n\u03b3L2 ( 180 + 303\u03b22\n(1\u2212 q)2\n) U t\n\u2264\u03b3\n( 1\nN \u2211 i E[\u2225cti \u2212 vti\u22252] + ( \u03b2 N + 2\u03b22 (1\u2212 q)2 ) 1 N \u2211 i E[\u2225vti \u2212\u2207fi(xt\u22121)\u22252] )\n+ \u03b3 4 E[\u2225\u2207f(xt)\u22252] + \u03b3L\n2\n4 E[\u2225xt \u2212 xt\u22121\u22252] + \u03b3\u03b2\u03c3\n2\nNK . (58)\nPlugging (58) into (57), we reach \u03a8t+1 \u2212\u03a8t\n\u2264\u2212 \u03b3 4 E[\u2225\u2207f(xt)\u22252]\n\u2212 ( 1\n2\u03b3 \u2212 L 2\n) E[\u2225xt+1 \u2212 xt\u22252] + \u03b3L2 ( 197N2\n\u03b22S2 +\n316N2\n(1\u2212 q)2S2\n) E[\u2225xt \u2212 xt\u22121\u22252]\n+ \u03b3\n( 165\u03b2\nN +\n64\u03b22 1\u2212 q + 264\u03b23 (1\u2212 q)2\n) \u03c32\nK . (59)\nDue to the choice of \u03b3 = \u03b7g\u03b7lK, it holds that 1\n2\u03b3 \u2265 L 2 + \u03b3L2\n( 197N2\n\u03b22S2 +\n264\u03b23\n(1\u2212 q)2\n) ,\nAveraging (59) over k and noting \u2225x0 \u2212 x\u22121\u22252 = 0, we obtain\n1\nT T\u22121\u2211 t=0 E[\u2225\u2207f(xt)\u22252] \u2272\u03a8 0 \u2212\u03a8T+1 \u03b3T + ( \u03b2 N + \u03b22 1\u2212 q +\n\u03b23\n(1\u2212 q)2\n) \u03c32\nK .\nNote that, by the definition of \u03a8t, it holds that \u03a80 \u2212\u03a8T+1\n\u03b3T\n\u2272 L\u2206\n\u03b3T +\nN\n\u03b2S E[\u2225v0 \u2212\u2207f(x0)\u22252] T\n+ N\n(1\u2212 q)S\n1 N \u2211 i E[\u2225v0i \u2212 c0i \u22252]\nT +\n( 1\nS +\n\u03b2N\n(1\u2212 q)2S\n) 1 N \u2211 i E[\u2225v0i \u2212\u2207fi(x0)\u22252]\nT\n\u2272 L\u2206\nT\n( N\n\u03b2S +\nN\n(1\u2212 q)S\n) + \u03c32\nBT\n( 1\n\u03b2S +\nN\n(1\u2212 q)S +\n\u03b2N\n(1\u2212 q)2S ) where we use the choice of \u03b3 and the initialization of {v0i }i\u2208[N ], {c0i }i\u2208[N ], and c0 in the second inequality. Due to the choice of B, we have\n\u03c32\nB\n( 1\n\u03b2S +\nN\n(1\u2212 q)S +\n\u03b2N\n(1\u2212 q)2S\n) \u2272 L\u2206 ( N\n\u03b2S +\nN\n(1\u2212 q)S ) and consequently\n1\nT T\u22121\u2211 t=0 E[\u2225\u2207f(xt)\u22252] \u2272L\u2206 T ( N \u03b2S +\nN\n(1\u2212 q)S\n) + ( \u03b2\nN +\n\u03b22\n1\u2212 q +\n\u03b23\n(1\u2212 q)2\n) \u03c32\nK . (60)\nPlugging the choice of \u03b2 into (60) completes the proof.\nG IMPLEMENTATION DETAILS & MORE EXPERIMENTS\nG.1 DATASETS, ALGORITHMS AND TRAINING SETUP\nDatasets and model. We test our algorithms on two standard FL datasets: MNIST dataset (LeCun, 1998) and Fashion MNIST dataset (Xiao et al., 2017). The MNIST dataset contains 60,000 training images and 10,000 test images. Each image is a gray-scale handwritten digit from 0 to 9 (10 classes in total) with 784 pixels. The FMNIST dataset has the same dataset sizes and the number of pixels per image whereas each image falls into 10 categories of fashion products (e.g., bag, dress), making the learning task more challenging. Following (Karimireddy et al., 2020b), we train a (non-convex) fully-connected neural network with 2 hidden layers with 256 and 128 neurons, respectively. We use ReLU as the activation function and the cross-entropy loss as the training objective.\nAlgorithms. We implement our two proposed methods and two recent compressed FL algorithms, with biased and unbiased compression, respectively:\n\u2022 (Biased) FED-EF (Li & Li, 2023): Federated learning with biased compression and standard error feedback. Since our proposed algorithms conduct SGD-type updates in the server, we compare them with its FED-EF-SGD variant.\n\u2022 (Biased) SCAFCOM (our Algorithm 2): Biased compression for FL with stochastic controlled averaging and local momentum. The momentum \u03b2 in Algorithm 2 is tuned over a fine grid on [0.05, 1].\n\u2022 (Unbiased) FEDCOMGATE (Haddadpour et al., 2021): Federated learning with unbiased compression. It uses the gradient-tracking technique to alleviate data heterogeneity.\n\u2022 (Unbiased) SCALLION (our Algorithm 1): Unbiased compression for FL with stochastic controlled averaging. The local scaling factor \u03b1 is tuned over a fine grid on [0.05, 1].\nBesides the compressed FL algorithms, we also test the corresponding full-precision baselines: FEDSGD (also known as FEDAVG (Yang et al., 2021)) and SCAFFOLD (Karimireddy et al., 2020b), both with two-sided (global and local) learning rates. For a fair comparison, we execute SCAFFOLD with our new implementation in experiments, corresponding to the special cases of SCAFCOM (Ci = I , \u03b2 = 1) and of SCALLION (Ci = I , \u03b1 = 1). Notably, under a fixed random seed, our implementation yields the same training trajectory as Karimireddy et al. (2020b) at a halved uplink communication cost (by only sending one variable per participating client).\nIn the experiments, biased compression is simulated with TOP-r operators (our Example 3). Specifically, we experiment with TOP-0.01 and TOP-0.05, where only the largest 1% and 5% entries in absolute values are transmitted in communication. For unbiased compression, we utilize random dithering (our Example 2), with 2 bits and 4 bits per entry, respectively. We tune the combination of the global learning rate \u03b7g and the local learning rate \u03b7l over the 2D grid {0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10}2. The combination of learning rates with the highest test accuracy is reported for each algorithm and hyper-parameter choice (e.g., \u03b2, \u03b1, and degree of compression).\nFederated learning setting. In our experiments, the training data are distributed across N = 200 clients, in a highly heterogeneous setting following (Li & Li, 2023). The training data samples are split into 400 shards each containing samples from only one class. Then, each client is randomly assigned two shards of data. Therefore, every client only possesses training samples from at most two classes. All the clients share the same initial model at T = 0. In each round of client-server interaction, we uniformly randomly pick S = 20 clients to participate in FL training, i.e., the partial participation rate is 10%. Each participating client performs K = 10 local training steps using the local data, with a mini-batch size 32. All the presented results are averaged over 5 independent runs with the same model initialization for all the algorithms.\nG.2 RESULTS\nSCAFCOM with biased compression. In Figure 4, we first present the train loss and test accuracy of our proposed SCAFCOM (Algorithm 2) with momentum \u03b2 = 0.2 and FED-EF (Li & Li, 2023), both using biased TOP-r compressors. We observe:\n\u2022 In general, under the same degree of compression (i.e., the value of r in the case), SCAFCOM outperforms FED-EF in terms of both training loss and test accuracy, thanks to controlled variables and the local momentum in SCAFCOM.\n\u2022 On both datasets, SCAFCOM with TOP-0.01 can achieve very close test accuracy as the full-precision SCAFFOLD, and SCAFCOM with TOP-0.05 essentially match those of full-precision SCAFFOLD. Hence, we can reach the same performance while saving 20 - 100x uplink communication costs.\n\u2022 For both SCAFCOM and FED-EF, as the degree of compression decreases (i.e., r increases), their performance approaches that of the corresponding FL methods under fullprecision communication (i.e., SCAFFOLD and FED-SGD).\nSCALLION with unbiased compression. In Figure 5, we plot the same set of experimental results and compare SCALLION (\u03b1 = 0.1) with FedCOMGATE (Haddadpour et al., 2021), both applying unbiased random dithering (Alistarh et al., 2017) with 2 and 4 bits per entry. Similarly, we see that SCALLION outperforms FedCOMGATE under the same degree of compression (number of bits per entry). The SCALLION curves of both 2-bit and 4-bit compression basically overlap that of SCAFFOLD, and 4-bit compression slightly performs better than 2-bit compression in later training rounds. Since random dithering also introduces sparsity in compressed variables, the 4-bit compressor already provides around 100x communication compression, and the 2-bit compressor saves more communication costs.\nImpact of \u03b2 and \u03b1. The momentum factor \u03b2 in SCAFCOM and the scaling factor \u03b1 in SCALLION are two important tuning parameters of our proposed methods. As an example, in Figure 6, we report the test accuracy of SCAFCOM with TOP-0.01 (left column) and SCALLION (right column) 2-bit random dithering, for various \u03b2 and \u03b1 values, respectively. From the results, we see that SCAFCOM can converge with a wide range of \u03b2 \u2208 [0.05, 1], and \u03b2 = 0.2 performs the best on both datasets (so we presented the results with \u03b2 = 0.2 in Figure 1). For SCALLION, we report three \u03b1-values, \u03b1 = 0.05, 0.1, 0.2. When \u03b1 > 0.5, the training of SCALLION becomes unstable for 2-bit quantization. As we use more bits, larger \u03b1 could be allowed. This is because, random dithering may hugely scale up the transmitted (compressed) entries, especially for low-bit quantization. When the scaling factor \u03b1 is too large in this case, the updates of local control variables become unstable, which further incapacitates the proper dynamic of the local/global training. Thus, for SCALLION with low-bit random dithering, we typically need a relatively small \u03b1. As presented in Figure 2, \u03b1 = 0.1 yields the best overall performance. In general, we should tune parameters \u03b2 and \u03b1 in SCAFCOM and SCALLION practically to reach the best performance."
        }
    ],
    "year": 2024
}