{
    "abstractText": "Offline reinforcement learning (RL) allows agents to learn effective, returnmaximizing policies from a static dataset. Three popular algorithms for offline RL are Conservative Q-Learning (CQL), Behavior Cloning (BC), and Decision Transformer (DT), from the class of Q-Learning, Imitation Learning, and Sequence Modeling respectively. A key open question is: which algorithm is preferred under what conditions? We study this question empirically by exploring the performance of these algorithms across the commonly used D4RL and ROBOMIMIC benchmarks. We design targeted experiments to understand their behavior concerning data suboptimality, task complexity, and stochasticity. Our key findings are: (1) DT requires more data than CQL to learn competitive policies but is more robust; (2) DT is a substantially better choice than both CQL and BC in sparse-reward and low-quality data settings; (3) DT and BC are preferable as task horizon increases, or when data is obtained from human demonstrators; and (4) CQL excels in situations characterized by the combination of high stochasticity and low data quality. We also investigate architectural choices and scaling trends for DT on ATARI and D4RL and make design/scaling recommendations. We find that scaling the amount of data for DT by 5x gives a 2.5x average score improvement on ATARI.",
    "authors": [
        {
            "affiliations": [],
            "name": "SHOULD WE PREFER"
        },
        {
            "affiliations": [],
            "name": "DECISION TRANSFORMERS"
        },
        {
            "affiliations": [],
            "name": "Prajjwal Bhargava"
        },
        {
            "affiliations": [],
            "name": "Rohan Chitnis"
        },
        {
            "affiliations": [],
            "name": "Alborz Geramifard"
        },
        {
            "affiliations": [],
            "name": "Shagun Sodhani"
        },
        {
            "affiliations": [],
            "name": "Amy Zhang"
        }
    ],
    "id": "SP:00376608bb03c6b1669e1f38503d6483b28cbbef",
    "references": [
        {
            "authors": [
                "Rishabh Agarwal",
                "Dale Schuurmans",
                "Mohammad Norouzi"
            ],
            "title": "An optimistic perspective on offline reinforcement learning",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Anurag Ajay",
                "Yilun Du",
                "Abhi Gupta",
                "Joshua Tenenbaum",
                "Tommi Jaakkola",
                "Pulkit Agrawal"
            ],
            "title": "Is conditional generative modeling all you need for decision-making",
            "venue": "arXiv preprint arXiv:2211.15657,",
            "year": 2022
        },
        {
            "authors": [
                "Arthur Argenson",
                "Gabriel Dulac-Arnold"
            ],
            "title": "Model-based offline planning",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Shikhar Bahl",
                "Abhinav Gupta",
                "Deepak Pathak"
            ],
            "title": "Human-to-robot imitation in the wild",
            "venue": "In RSS,",
            "year": 2022
        },
        {
            "authors": [
                "Michael Bain",
                "Claude Sammut"
            ],
            "title": "A framework for behavioural cloning",
            "venue": "In Machine Intelligence",
            "year": 1995
        },
        {
            "authors": [
                "Marc G. Bellemare",
                "Will Dabney",
                "R\u00e9mi Munos"
            ],
            "title": "A distributional perspective on reinforcement learning",
            "venue": "In Proceedings of the 34th International Conference on Machine Learning - Volume 70,",
            "year": 2017
        },
        {
            "authors": [
                "David Brandfonbrener",
                "Alberto Bietti",
                "Jacob Buckman",
                "Romain Laroche",
                "Joan Bruna"
            ],
            "title": "When does return-conditioned supervised learning work for offline reinforcement learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Lili Chen",
                "Kevin Lu",
                "Aravind Rajeswaran",
                "Kimin Lee",
                "Aditya Grover",
                "Misha Laskin",
                "Pieter Abbeel",
                "Aravind Srinivas",
                "Igor Mordatch"
            ],
            "title": "Decision transformer: Reinforcement learning via sequence modeling",
            "venue": "Advances in neural information processing systems,",
            "year": 2021
        },
        {
            "authors": [
                "Jiafei Duan",
                "Samson Yu",
                "Hui Li Tan",
                "Hongyuan Zhu",
                "Cheston Tan"
            ],
            "title": "A survey of embodied ai: From simulators to research tasks",
            "venue": "IEEE Transactions on Emerging Topics in Computational Intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "Scott Emmons",
                "Benjamin Eysenbach",
                "Ilya Kostrikov",
                "Sergey Levine"
            ],
            "title": "Rvs: What is essential for offline RL via supervised learning",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Damien Ernst",
                "Pierre Geurts",
                "Louis Wehenkel"
            ],
            "title": "Tree-based batch mode reinforcement learning",
            "venue": "Journal of Machine Learning Research,",
            "year": 2005
        },
        {
            "authors": [
                "Justin Fu",
                "Aviral Kumar",
                "Ofir Nachum",
                "George Tucker",
                "Sergey Levine"
            ],
            "title": "D4rl: Datasets for deep data-driven reinforcement learning",
            "venue": "arXiv preprint arXiv:2004.07219,",
            "year": 2020
        },
        {
            "authors": [
                "Scott Fujimoto",
                "Shixiang (Shane) Gu"
            ],
            "title": "A minimalist approach to offline reinforcement learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Scott Fujimoto",
                "Shixiang Shane Gu"
            ],
            "title": "A minimalist approach to offline reinforcement learning",
            "venue": "Advances in neural information processing systems,",
            "year": 2021
        },
        {
            "authors": [
                "Scott Fujimoto",
                "Herke Hoof",
                "David Meger"
            ],
            "title": "Addressing function approximation error in actorcritic methods",
            "venue": "In International conference on machine learning,",
            "year": 2018
        },
        {
            "authors": [
                "Scott Fujimoto",
                "David Meger",
                "Doina Precup"
            ],
            "title": "Off-policy deep reinforcement learning without exploration",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Kanishk Gandhi",
                "Siddharth Karamcheti",
                "Madeline Liao",
                "Dorsa Sadigh"
            ],
            "title": "Eliciting compatible demonstrations for multi-human imitation learning",
            "venue": "In 6th Annual Conference on Robot Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Wonjoon Goo",
                "Scott Niekum"
            ],
            "title": "Know your boundaries: The necessity of explicit behavioral cloning in offline rl",
            "venue": "arXiv preprint arXiv:2206.00695,",
            "year": 2022
        },
        {
            "authors": [
                "Tuomas Haarnoja",
                "Aurick Zhou",
                "Pieter Abbeel",
                "Sergey Levine"
            ],
            "title": "Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor",
            "venue": "Proceedings of the 35th International Conference on Machine Learning,",
            "year": 2018
        },
        {
            "authors": [
                "Danijar Hafner",
                "Timothy P Lillicrap",
                "Mohammad Norouzi",
                "Jimmy Ba"
            ],
            "title": "Mastering atari with discrete world models",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Jonathan Ho",
                "Stefano Ermon"
            ],
            "title": "Generative adversarial imitation learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2016
        },
        {
            "authors": [
                "Jordan Hoffmann",
                "Sebastian Borgeaud",
                "Arthur Mensch",
                "Elena Buchatskaya",
                "Trevor Cai",
                "Eliza Rutherford",
                "Diego de Las Casas",
                "Lisa Anne Hendricks",
                "Johannes Welbl",
                "Aidan Clark"
            ],
            "title": "Training compute-optimal large language models",
            "venue": "arXiv preprint arXiv:2203.15556,",
            "year": 2022
        },
        {
            "authors": [
                "Ahmed Hussein",
                "Mohamed Medhat Gaber",
                "Eyad Elyan",
                "Chrisina Jayne"
            ],
            "title": "Imitation learning: A survey of learning methods",
            "venue": "ACM Computing Surveys (CSUR),",
            "year": 2017
        },
        {
            "authors": [
                "Michael Janner",
                "Qiyang Li",
                "Sergey Levine"
            ],
            "title": "Offline reinforcement learning as one big sequence modeling problem",
            "venue": "Advances in neural information processing systems,",
            "year": 2021
        },
        {
            "authors": [
                "Michael Janner",
                "Yilun Du",
                "Joshua Tenenbaum",
                "Sergey Levine"
            ],
            "title": "Planning with diffusion for flexible behavior synthesis",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Rahul Kidambi",
                "Aravind Rajeswaran",
                "Praneeth Netrapalli",
                "Thorsten Joachims. Morel"
            ],
            "title": "Model-based offline reinforcement learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Rahul Kidambi",
                "Aravind Rajeswaran",
                "Praneeth Netrapalli",
                "Thorsten Joachims"
            ],
            "title": "Morel: Modelbased offline reinforcement learning",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Diederik P. Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "CoRR, abs/1412.6980,",
            "year": 2014
        },
        {
            "authors": [
                "Ilya Kostrikov",
                "Ashvin Nair",
                "Sergey Levine"
            ],
            "title": "Offline reinforcement learning with implicit q-learning",
            "venue": "arXiv preprint arXiv:2110.06169,",
            "year": 2021
        },
        {
            "authors": [
                "Aviral Kumar",
                "Justin Fu",
                "Matthew Soh",
                "George Tucker",
                "Sergey Levine"
            ],
            "title": "Stabilizing off-policy q-learning via bootstrapping error reduction",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Aviral Kumar",
                "Aurick Zhou",
                "George Tucker",
                "Sergey Levine"
            ],
            "title": "Conservative q-learning for offline reinforcement learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Aviral Kumar",
                "Rishabh Agarwal",
                "Xinyang Geng",
                "George Tucker",
                "Sergey Levine"
            ],
            "title": "Offline q-learning on diverse multi-task data both scales and generalizes",
            "venue": "In International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Cassidy Laidlaw",
                "Stuart Russell",
                "Anca Dragan"
            ],
            "title": "Bridging rl theory and practice with the effective horizon, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Sascha Lange",
                "Thomas Gabel",
                "Martin A. Riedmiller"
            ],
            "title": "Batch reinforcement learning",
            "venue": "In Reinforcement Learning,",
            "year": 2012
        },
        {
            "authors": [
                "Kuang-Huei Lee",
                "Ofir Nachum",
                "Sherry Yang",
                "Lisa Lee",
                "C. Daniel Freeman",
                "Sergio Guadarrama",
                "Ian Fischer",
                "Winnie Xu",
                "Eric Jang",
                "Henryk Michalewski",
                "Igor Mordatch"
            ],
            "title": "Multi-game decision transformers",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Sergey Levine",
                "Aviral Kumar",
                "George Tucker",
                "Justin Fu"
            ],
            "title": "Offline reinforcement learning: Tutorial, review, and perspectives on open problems",
            "venue": "arXiv preprint arXiv:2005.01643,",
            "year": 2020
        },
        {
            "authors": [
                "Hao Liu",
                "Pieter Abbeel"
            ],
            "title": "Behavior from the void: Unsupervised active pre-training",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Yiren Lu",
                "Justin Fu",
                "George Tucker",
                "Xinlei Pan",
                "Eli Bronstein",
                "Becca Roelofs",
                "Benjamin Sapp",
                "Brandyn White",
                "Aleksandra Faust",
                "Shimon Whiteson",
                "Dragomir Anguelov",
                "Sergey Levine"
            ],
            "title": "Imitation is not enough: Robustifying imitation with reinforcement learning for challenging driving scenarios, 2022",
            "venue": "URL https://arxiv.org/abs/2212.11419",
            "year": 2022
        },
        {
            "authors": [
                "Ajay Mandlekar",
                "Danfei Xu",
                "Josiah Wong",
                "Soroush Nasiriany",
                "Chen Wang",
                "Rohun Kulkarni",
                "Li FeiFei",
                "Silvio Savarese",
                "Yuke Zhu",
                "Roberto Mart\u00edn-Mart\u00edn"
            ],
            "title": "What matters in learning from offline human demonstrations for robot manipulation",
            "venue": "In arXiv preprint arXiv:2108.03298,",
            "year": 2021
        },
        {
            "authors": [
                "Paul Michel",
                "Omer Levy",
                "Graham Neubig"
            ],
            "title": "Are sixteen heads really better than one",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Volodymyr Mnih",
                "Koray Kavukcuoglu",
                "David Silver",
                "Andrei A. Rusu",
                "Joel Veness",
                "Marc G. Bellemare",
                "Alex Graves",
                "Martin A. Riedmiller",
                "Andreas Kirkeby Fidjeland",
                "Georg Ostrovski",
                "Stig Petersen",
                "Charlie Beattie",
                "Amir Sadik",
                "Ioannis Antonoglou",
                "Helen King",
                "Dharshan Kumaran",
                "Daan Wierstra",
                "Shane Legg",
                "Demis Hassabis"
            ],
            "title": "Human-level control through deep reinforcement learning",
            "venue": "Nature, 518:529\u2013533,",
            "year": 2015
        },
        {
            "authors": [
                "Allen Nie",
                "Yannis Flet-Berliac",
                "Deon Jordan",
                "William Steenbergen",
                "Emma Brunskill"
            ],
            "title": "Dataefficient pipeline for offline reinforcement learning with limited data",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Manu Orsini",
                "Anton Raichuk",
                "Leonard Hussenot",
                "Damien Vincent",
                "Robert Dadashi",
                "Sertan Girgin",
                "Matthieu Geist",
                "Olivier Bachem",
                "Olivier Pietquin",
                "Marcin Andrychowicz"
            ],
            "title": "What matters for adversarial imitation learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Keiran Paster",
                "Sheila McIlraith",
                "Jimmy Ba"
            ],
            "title": "You can\u2019t count on luck: Why decision transformers and rvs fail in stochastic environments",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Deepak Pathak",
                "Pulkit Agrawal",
                "Alexei A. Efros",
                "Trevor Darrell"
            ],
            "title": "Curiosity-driven exploration by self-supervised prediction",
            "venue": "In International Conference on Machine Learning (ICML),",
            "year": 2017
        },
        {
            "authors": [
                "Rafael Figueiredo Prudencio",
                "Marcos ROA Maximo",
                "Esther Luna Colombini"
            ],
            "title": "A survey on offline reinforcement learning: Taxonomy, review, and open problems",
            "venue": "IEEE Transactions on Neural Networks and Learning Systems,",
            "year": 2023
        },
        {
            "authors": [
                "Martin L Puterman"
            ],
            "title": "Markov decision processes",
            "venue": "Handbooks in operations research and management science,",
            "year": 1990
        },
        {
            "authors": [
                "Antonin Raffin",
                "Ashley Hill",
                "Adam Gleave",
                "Anssi Kanervisto",
                "Maximilian Ernestus",
                "Noah Dormann"
            ],
            "title": "Stable-baselines3: Reliable reinforcement learning implementations",
            "venue": "Journal of Machine Learning Research,",
            "year": 2021
        },
        {
            "authors": [
                "J\u00fcrgen Schmidhuber"
            ],
            "title": "Reinforcement learning upside down: Don\u2019t predict rewards - just map them to actions",
            "venue": "CoRR, abs/1912.02875,",
            "year": 2019
        },
        {
            "authors": [
                "Rupesh Kumar Srivastava",
                "Pranav Shyam",
                "Filipe Wall Mutz",
                "Wojciech Ja\u015bkowski",
                "J\u00fcrgen Schmidhuber"
            ],
            "title": "Training agents using upside-down reinforcement",
            "venue": "learning. ArXiv,",
            "year": 2019
        },
        {
            "authors": [
                "Richard S Sutton",
                "Andrew G Barto"
            ],
            "title": "Reinforcement learning: An introduction",
            "venue": "MIT press,",
            "year": 2018
        },
        {
            "authors": [
                "Denis Tarasov",
                "Alexander Nikulin",
                "Dmitry Akimov",
                "Vladislav Kurenkov",
                "Sergey Kolesnikov"
            ],
            "title": "CORL: Research-oriented deep offline reinforcement learning library",
            "venue": "In 3rd Offline RL Workshop: Offline RL as a \u201dLaunchpad\u201d,",
            "year": 2022
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Chen Wang",
                "Linxi Fan",
                "Jiankai Sun",
                "Ruohan Zhang",
                "Li Fei-Fei",
                "Danfei Xu",
                "Yuke Zhu",
                "Anima Anandkumar"
            ],
            "title": "Mimicplay: Long-horizon imitation learning by watching human play",
            "venue": "arXiv preprint arXiv:2302.12422,",
            "year": 2023
        },
        {
            "authors": [
                "Denis Yarats",
                "David Brandfonbrener",
                "Hao Liu",
                "Michael Laskin",
                "Pieter Abbeel",
                "Alessandro Lazaric",
                "Lerrel Pinto"
            ],
            "title": "Don\u2019t change the algorithm, change the data: Exploratory data for offline reinforcement learning",
            "venue": "arXiv preprint arXiv:2201.13425,",
            "year": 2022
        },
        {
            "authors": [
                "Tianhe Yu",
                "Garrett Thomas",
                "Lantao Yu",
                "Stefano Ermon",
                "James Y Zou",
                "Sergey Levine",
                "Chelsea Finn",
                "Tengyu Ma"
            ],
            "title": "Mopo: Model-based offline policy optimization",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Tianhe Yu",
                "Garrett Thomas",
                "Lantao Yu",
                "Stefano Ermon",
                "James Y Zou",
                "Sergey Levine",
                "Chelsea Finn",
                "Tengyu Ma"
            ],
            "title": "Mopo: Model-based offline policy optimization",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "fonbrener"
            ],
            "title": "In this paper, we will focus on Conservative Q-Learning (CQL)",
            "venue": "CQL (Kumar et al.,",
            "year": 2022
        }
    ],
    "sections": [
        {
            "text": "Offline reinforcement learning (RL) allows agents to learn effective, returnmaximizing policies from a static dataset. Three popular algorithms for offline RL are Conservative Q-Learning (CQL), Behavior Cloning (BC), and Decision Transformer (DT), from the class of Q-Learning, Imitation Learning, and Sequence Modeling respectively. A key open question is: which algorithm is preferred under what conditions? We study this question empirically by exploring the performance of these algorithms across the commonly used D4RL and ROBOMIMIC benchmarks. We design targeted experiments to understand their behavior concerning data suboptimality, task complexity, and stochasticity. Our key findings are: (1) DT requires more data than CQL to learn competitive policies but is more robust; (2) DT is a substantially better choice than both CQL and BC in sparse-reward and low-quality data settings; (3) DT and BC are preferable as task horizon increases, or when data is obtained from human demonstrators; and (4) CQL excels in situations characterized by the combination of high stochasticity and low data quality. We also investigate architectural choices and scaling trends for DT on ATARI and D4RL and make design/scaling recommendations. We find that scaling the amount of data for DT by 5x gives a 2.5x average score improvement on ATARI."
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "Offline reinforcement learning (RL) (Levine et al., 2020; Lange et al., 2012; Ernst et al., 2005) aims to leverage existing datasets of agent behavior in an environment to produce effective policies. A key open question is: which learning method is preferred for offline RL? In this paper, we empirically investigate this question. Among many offline RL algorithms, we focus on three that have been studied extensively and are thus relatively easy to interpret and compare: Conservative Q-Learning (CQL) (Kumar et al., 2020), Behavior Cloning (BC) (Bain and Sammut, 1995), and Decision Transformer (DT) (Chen et al., 2021). CQL, from the class of Q-Learning (Sutton and Barto, 2018), uses temporal difference (TD) updates to learn a value function through bootstrapping. In theory, it can learn effective policies even from highly suboptimal trajectories in stochastic environments, but in practice, it suffers from instability and sensitivity to hyperparameters in the offline setting (Brandfonbrener et al., 2022). BC, from the family of Imitation Learning (Hussein et al., 2017), mimics the behavior policy of the data; however, it relies on the data being high-quality. DT, from the class of Sequence Modeling, is a recently popularized paradigm that aims to transfer the success of Transformers (Vaswani et al., 2017) into offline RL (Chen et al., 2021; Janner et al., 2021), yet they have shown to struggle with stochastic dynamics (Brandfonbrener et al., 2022).\nWe design targeted experiments to understand how these three algorithms perform as we vary properties of the data, task, and environment. Our experiments are conducted across the commonly used D4RL, ROBOMIMIC, and ATARI benchmarks. Table 1 shows a high-level summary of our key findings. In Section 4.1, we begin by establishing baseline results in our benchmark tasks for CQL, BC, and DT, for both dense-reward and sparse-reward settings. Then, we perform experiments to answer several key questions, which form the core contributions of this paper:\n\u2022 (Sections 4.2, 4.3, 4.4) How are agents affected by the presence of suboptimal data? As the notion of suboptimality can take on many meanings in offline RL, we consider three definitions:\nAlgorithm Property\nSPARSE BEST X% WORST X% LENGTH NOISE HORIZON SPACE STOCHASTICITY\nDT \u2713 \u00d7 \u2713 \u00d7 \u2713 \u2713 \u00d7 \u00d7 CQL \u00d7 \u2713 \u2713 \u00d7 \u2713 \u00d7 \u00d7 \u2713 BC \u00d7 \u00d7 \u00d7 \u2713 \u00d7 \u2713 \u00d7 \u00d7\nOur key findings are: (1) DT is more robust than CQL but also requires more data; (2) DT is the best choice in sparse-reward and low-quality data settings; (3) DT and BC are preferable as task horizon increases, or when data is obtained from suboptimal human demonstrators; (4) CQL excels in situations characterized by the combination of high stochasticity and low data quality; (5) larger DT models require less training, and scaling the amount of data improves scores on ATARI.\nOur work aligns with a recent research trend that examines the trade-offs among various algorithms for offline RL. Brandfonbrener et al. (2022) outlined theoretical conditions, such as nearly deterministic dynamics and prior knowledge of the conditioning function, under which Sequence Modeling (called \u201cRCSL\u201d in their work) is preferable. Our paper extends their research by asking carefully designed questions targeted to provide novel empirical insights. Kumar et al. (2023) investigated when Q-Learning might be favored over Imitation Learning. Our research expands on this inquiry by incorporating the recently popularized DT as part of the Sequence Modeling paradigm, thereby providing insight into the training dynamics and learned policies of each algorithm.\nWe hope this paper helps researchers determine which offline RL algorithm to use in their application. Throughout the paper, we provide practical guidance on which algorithm is preferred, given characteristics of the application. Code and data: https://github.com/facebookresearch/rl_paradigm."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Our work addresses the question of which learning algorithm is most suitable for offline RL (Levine et al., 2020; Fu et al., 2020; Prudencio et al., 2023) by examining three prominent algorithms from the following paradigms in the field: Q-Learning, Imitation Learning, and Sequence Modeling (Brandfonbrener et al., 2022). We chose CQL and DT as representative algorithms for Q-Learning and Sequence Modeling, respectively, based on their popularity in the literature and their strong performance across standard benchmarks (Kumar et al., 2020; 2023; Chen et al., 2021; Lee et al., 2022; Kumar et al.,\n2023). Other options are possible as well such as BCQ (Fujimoto et al., 2019), BEAR (Kumar et al., 2019a), and IQL (Kostrikov et al., 2021). The Trajectory Transformer is another algorithm within the Sequence Modeling paradigm (Janner et al., 2021) (model-based). Similar to Sequence Modeling, other studies have explored approaches that aim to learn policies conditioned on state and reward (or return) to predict actions (Schmidhuber, 2019; Srivastava et al., 2019; Brandfonbrener et al., 2022; Kumar et al., 2019b; Emmons et al., 2022), in both online and offline settings. Finally, despite its simplicity, Behavior Cloning (BC) remains a widely used baseline among Imitation Learning algorithms (Kumar et al., 2023; Chen et al., 2021; Ho and Ermon, 2016; Fujimoto and Gu, 2021a), and thus was chosen as the representative algorithm from the Imitation Learning paradigm. Alternative options, such as TD3-BC (Fujimoto and Gu, 2021b), could also be considered.\nWhile our research focuses on these three paradigms, it is also worth mentioning model-based RL approaches, which have begun to increase in popularity recently. These approaches have delivered promising results in various settings (Janner et al., 2022; Yu et al., 2020a; Kidambi et al., 2020a; Argenson and Dulac-Arnold, 2021), but we do not examine them in our work, instead choosing to focusing on the most prominent paradigms in offline RL (Tarasov et al., 2022).\nIn light of the recent interest in scaling foundational models (Hoffmann et al., 2022), both Kumar et al. (2023) and Lee et al. (2022) have demonstrated that DT scales with parameter size. Moreover, Kumar et al. (2023) indicate that CQL performs better on suboptimal dense data in the Atari domain. Our findings concur with these studies but offer a more comprehensive perspective, as we also explore sample efficiency, as well as the scaling of parameters and data together."
        },
        {
            "heading": "3 PRELIMINARIES",
            "text": "Here, we discuss brief background (see Appendix C for more details) and our experimental setup."
        },
        {
            "heading": "3.1 BACKGROUND",
            "text": "In reinforcement learning (RL), an agent interacts with a Markov decision process (MDP) (Puterman, 1990), taking actions that provide a reward and transition the state according to an unknown dynamics model. The agent\u2019s objective is to learn a policy that maximizes its return, the sum of expected rewards. In offline RL (Levine et al., 2020), agents cannot interact with the MDP, but instead learn from a fixed dataset of transitions D = {(s, a, r, s\u2032)}, generated from an unknown behavior policy. Q-Learning (Sutton and Barto, 2018) uses temporal difference (TD) updates to estimate the value of actions via bootstrapping. We focus on Conservative Q-Learning (CQL) (Kumar et al., 2020), which addresses overestimation by constraining Q-values so that they lower-bound the true value function. Behavior Cloning (BC) (Bain and Sammut, 1995) is a simple algorithm that mimics the behavior policy via supervised learning on D. Sequence Modeling is a recently popularized class of offline RL algorithms that trains autoregressive models to map the trajectory history to the next action. We focus on the Decision Transformer (DT) (Chen et al., 2021), in which the learned policy produces action distributions conditioned on trajectory history and desired returns-to-go, R\u0302t\u2032 = \u2211H t=t\u2032 rt. DT is trained using supervised learning on the actions. Conditioning on returns-to-go enables DT to learn from suboptimal data and produce a wide range of behaviors during inference."
        },
        {
            "heading": "3.2 EXPERIMENTAL SETUP",
            "text": "Data: We consider tasks from two benchmarks: D4RL and ROBOMIMIC, chosen due to their popularity (Nie et al., 2022; Goo and Niekum, 2022). We also explore the HUMANOID GYM environment, which is not part of D4RL; in the process, we generate D4RL-style datasets for HUMANOID (details in Appendix D). We additionally conduct experiments in ATARI, which allows us to study the scaling properties of DT with image observations. All tasks are deterministic and fully observed (see Laidlaw et al. (2023) for a description of why deterministic MDPs are still challenging in RL) and have continuous state and action spaces, except for ATARI, which has discrete state and action spaces.\nIn D4RL (Fu et al., 2020), we focus on the HALFCHEETAH, HOPPER and WALKER tasks. For each task, three data splits are available: medium, medium replay, and medium-expert. These splits differ in size and quality. The medium split is obtained by early stopping the training of a SAC agent (Haarnoja et al., 2018) and collecting 1M samples from the partially trained behavior policy.\nThe medium replay split contains \u223c100-200K samples, obtained by recording all interactions of the agent until it reaches a medium level of performance. The medium-expert split contains 2M samples obtained by concatenating expert demonstrations with medium data. In ROBOMIMIC (Mandlekar et al., 2021), we consider four tasks: Lift, Can, Square, and Transport. Each task requires a robot to manipulate objects into desired configurations; see Mandlekar et al. (2021) for details. For each task, three data splits are provided: proficient human (PH), multi-human (MH), and machine-generated (MG). PH data has 200 demonstrations collected by a single experienced teleoperator, while MH data has 300 demonstrations collected by teleoperators at varying proficiency levels. MG data has 300 trajectories, obtained by rolling out various checkpoints along the training of a SAC agent, and has a mixture of expert and suboptimal data. Appendix E gives details on ATARI (Agarwal et al., 2020).\nEvaluation Metrics: On D4RL, we evaluate agents on normalized average returns, following Fu et al. (2020). On ROBOMIMIC, we measure success rates following Mandlekar et al. (2021). ATARI scores were normalized following Hafner et al. (2021). Scores are averaged over 100 evaluation episodes for D4RL and ATARI, and 50 for ROBOMIMIC. All experiments report the mean score and standard deviation over five independent seeds of training and evaluation.\nDetails: For DT, we used a context length of 20 for D4RL and 1 for ROBOMIMIC; see Section A for experiments and discussion on how context length impacts DT. All agents have fewer than 2.1M parameters (for instance, in D4RL, we have the following parameter counts: BC = 77.4k, CQL = 1.1M, DT = 730k). BC and CQL use MLP architectures. For more details, see Appendix H."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "4.1 ESTABLISHING BASELINE RESULTS",
            "text": "We start by analyzing the baseline performance of the three agents on each dataset since we will make alterations to the data in subsequent experiments. We studied both the sparse-reward and the dense-reward regimes in D4RL and ROBOMIMIC. Because D4RL datasets have dense rewards only, we created the sparse-reward versions by exposing the sum of rewards in each trajectory only on the last timestep. For ROBOMIMIC experiments, we used the MG dataset (Section 3.2), which contains both sparse-reward and dense-reward splits. See Table 2 and Table 3 for results.\nWe noticed three key trends. (1) DT was generally better than or comparable to CQL in the densereward regime. For example, on ROBOMIMIC, DT outperformed CQL and BC by 154% and 51%, respectively. However, DT performed about 8% worse than CQL on D4RL. (2) DT was quite robust to reward sparsification, outperforming CQL and BC by 88% and 98% respectively on D4RL, and 194% and 62% respectively on ROBOMIMIC. These results are especially interesting because the dense and sparse settings have the same states and actions in the dataset; simply redistributing reward to the last step of each trajectory caused CQL\u2019s performance to reduce by half in the case of D4RL, while DT\u2019s performance remained unchanged. A likely reason is that sparser rewards mean CQL must propagate TD errors over more timesteps to learn effectively, while DT conditions on the returns-to-go at each state, and so is less affected by reward redistribution. (3) BC was never competitive with the best agent due to the suboptimality of the data collection policy.\nWe note that our methodology behind making D4RL tasks sparse is the same as (Chen et al., 2021) and might break Markovian dynamics. We provide an additional data point on the Maze2D environment, which provides sparse and dense rewards following Markovian dynamics, in Table 6 in Appendix G.\nDataset DT CQL BCSparse Dense Sparse Dense\nLift 93.2 \u00b1 3.2 96 \u00b1 1.2 60 \u00b1 13.2 68.4 \u00b1 6.2 59.2 \u00b1 6.19 Can 83.2 \u00b1 0 83.2 \u00b1 1.6 0\u00b1 0 2 \u00b1 1.2 55.2 \u00b1 5.8\nAverage 88.2 \u00b1 1.6 89.6 \u00b1 1.4 30 \u00b1 6.6 35.2 \u00b1 3.7 57.2 \u00b1 6"
        },
        {
            "heading": "4.2 HOW DOES THE AMOUNT AND QUALITY OF DATA AFFECT EACH AGENT\u2019S PERFORMANCE?",
            "text": "In this section, we aim to understand how agent performance varies given various amounts of highreturn and low-return dense-reward data. To do this, we sorted the trajectories based on their returns and trained the agents on the \u201cbest\u201d and \u201cworst\u201d X% of the data for various values of X. Analyzing each agent\u2019s performance on the best X% enables us to understand sample efficiency: how quickly do agents learn from high-return trajectories? Analyzing the performance on the worst X% enables us to understand how well the agents learn from low-quality data. See Figure 1 for results on D4RL.\nThe \u201c-best\u201d curves show that both CQL and DT improved as they observed more high-return data, but CQL was more sample-efficient, reaching its highest score at \u223c5% of the dataset, while DT required \u223c20%. We hypothesize that CQL is best in the low-data regime because in this case, the behavior policy is closer to the optimal one. However, as evidenced by the \u201cCQL-best\u201d line in Figure 1a going down between 20% and 80%, adding lower-return data could sometimes harm CQL, perhaps because (1) the difference between the optimal policy and the behavior policy grew larger and (2) TD updates had less opportunity to propagate values for high-return states. Meanwhile, DT was more stable and never worsened with more data, likely because conditioning on returns-to-go allowed it to differentiate trajectories of varying quality. BC\u2019s performance was best with a small amount of high-return data and then deteriorated greatly, which is expected because BC requires expert data.\nThe \u201c-worst\u201d curves show that DT learned from bad data 33% faster on average than CQL in medium replay (Figure 1a), but that they performed similarly in medium expert (Figure 1b). This is sensible because the low-return trajectories in medium replay are far worse than those in medium expert, and we have already seen that DT is more stable than CQL when the behavior policy is more suboptimal.\nIn Figure 10 in Appendix G.2, we show the same graphs as Figure 1 but for the sparse-reward D4RL dataset. This experiment revealed two new takeaways: (1) DT-best is far more sample-efficient and performant than CQL-best when rewards are sparse; (2) suboptimal data plays a more critical role for CQL in the sparse-reward setting than in the dense-reward setting.\nPractical Takeaways: 1) CQL is the most sample-efficient agent given a small amount of high-quality data; 2) While DT requires more data than CQL, it scales more robustly with additional suboptimal data due to DT\u2019s reliance on returns-to-go being more robust to variance in rewards; 3) DT can be slightly better than CQL with very low-quality data; 4) DT and CQL are preferable over BC, especially in the presence of suboptimal data."
        },
        {
            "heading": "4.3 HOW ARE AGENTS AFFECTED WHEN TRAJECTORY LENGTHS IN THE DATASET INCREASE?",
            "text": "In this section, we study how performance varies as a function of trajectory lengths in the dataset; this is an important question because, in practice, data suboptimality often manifests as longer demonstrations. To study this question, we turned to human demonstrations, which are qualitatively different from synthetic data (Orsini et al., 2021): human behavior is multi-modal and may be non-Markovian, so humans exhibit greater diversity in demonstration lengths when solving a task.\nWe used the ROBOMIMIC benchmark, which contains PH (proficient human) and MH (multi-human) sparse-reward datasets (Section 3.2). Because the rewards are fixed and given at the end of trajectories, the lengths of the trajectories are a proxy for optimality, as highlighted by Mandlekar et al. (2021). The MH datasets are further broken into \u201cBetter,\u201d \u201cOkay,\u201d and \u201cWorse\u201d splits based on the proficiency of the demonstrator. We leveraged this to do more granular experimentation. See Table 4 for results.\nWe see that BC outperformed all other agents. All methods performed best using shortest trajectories and deteriorated similarly as trained on longer trajectories. For full results, refer to Table 13 in Appendix G. The finding that BC performed best is especially interesting in light of Table 3, where BC performed far worse than DT. The difference is that Table 3 was on MG (machine-generated) data, while Table 4 studies human-generated data. Orsini et al. (2021) have found the source of data to play a prominent role in determining how well an agent performs on a task. This finding is consistent with many prior works (Brandfonbrener et al., 2022; Mandlekar et al., 2021; Bahl et al., 2022; Gandhi et al., 2022; Wang et al., 2023; Lu et al., 2022) that have found Imitation Learning to work better than Q-Learning when the policy is trained on human-generated data. We corroborate this finding on the Adroit Pen-human-v0 task, also at Table 5.\nAs trajectory lengths increase, bootstrapping methods are susceptible to worse performance because values must be propagated over longer time horizons. This is especially a challenge in sparse-reward settings and may explain why we observed CQL performing far worse than BC and DT.\nGiven that we used a context length of 1 for DT in ROBOMIMIC (Section 3.2), the key differences between BC and DT are 1) conditioning on returns-to-go, and 2) the MLP versus Transformer architecture. We hypothesize that BC performs better than DT because the PH and MH datasets\nare high-quality enough for Imitation Learning to be effective while being too small for Sequence Modeling. Refer to Appendix F for a detailed study that attempts to disentangle these differences.\nPractical Takeaway: Agents are affected similarly as trajectory lengths are increased, but when the data was generated by humans, BC is preferable."
        },
        {
            "heading": "4.4 HOW ARE AGENTS AFFECTED WHEN RANDOM DATA IS ADDED TO THE DATASET?",
            "text": "This section explores the impact of adding an equal amount of data collected from a random policy to our training dataset. We considered two strategies to ensure our results were not skewed based on a particular strategy for collecting random data. In \u201cStrategy 1,\u201d we rolled out a uniformly random policy from sampled initial states. In \u201cStrategy 2,\u201d we rolled out a pre-trained DT agent for several steps and executed a single uniformly random action. The number of steps in each rollout was chosen uniformly at random to be within 1 standard deviation of the average trajectory length in the offline dataset. We can see that Strategy 1 adds random transitions around initial states, while Strategy 2 adds random transitions along the entire trajectory leading to goal states. Because results did not differ significantly between the two strategies (see Appendix G.4), Figure 2 shows results averaged across both. Akin to Section 4.1, we consider dense- and sparse-reward settings in D4RL and ROBOMIMIC.\nCompared to BC, both CQL, and DT demonstrated enhanced robustness to injected data, experiencing less than 10% deterioration. However, the resilience of these agents manifests differently. CQL\u2019s resilience is more volatile than DT\u2019s, as evidenced by the larger standard deviations of blue bars compared to orange ones. In Appendix G.4, we present a per-task breakdown of the results in Figure 2, which shows several intriguing trends. CQL\u2019s performance varies greatly across tasks: it improves in some, remains stable in others, and deteriorates in the rest. Interestingly, when CQL\u2019s performance on the original dataset is poor, adding random data can occasionally improve it, as corroborated by Figure 1. CQL\u2019s volatility is particularly apparent in ROBOMIMIC, with Figure 15 showing that CQL deteriorates nearly 100% on the Lift PH dataset but improves on the Can PH dataset by nearly 2x.\nThe low deterioration of BC on ROBOMIMIC MG is likely just because the MG data was generated from several checkpoints of training a SAC agent, and therefore already has significantly lower data quality than either ROBOMIMIC PH or D4RL. In Section 4.3, we found BC to be superior when the behavior policy was driven by humans. However, when suboptimal data is mixed with high-quality human data, DT becomes preferable over BC.\nPractical Takeaway: BC is likely to suffer the most from the presence of noisy data, while DT and CQL are more robust. However, DT is more reliably performant than CQL in this setting."
        },
        {
            "heading": "4.5 HOW ARE AGENTS AFFECTED BY THE COMPLEXITY OF THE TASK?",
            "text": "We now focus on understanding how increasing the task complexity impacts the performance of our agents. Two major factors contributing to task complexity are the dimensionality of the state space and the horizon of the task MDP. To understand the impact of state space dimensionality, we utilized the HUMANOID environment, which has 376-dimensional states, along with other D4RL tasks, which have substantially smaller state spaces. To understand the impact of task horizon, we utilized the ROBOMIMIC PH datasets for Lift, Can, Square, and Transport tasks (listed in order of increasing task\nhorizon). Although trajectory lengths in the dataset are an artifact of the behavior policy while task horizon is an inherent property of the task, we found average trajectory lengths in the dataset to be a useful proxy for quantifying task horizon, because exactly computing task horizon is non-trivial. Akin to the previous section, we experimented with both the PH datasets and the same datasets with an equal amount of random data added in. Figure 3 shows the results averaged on tasks with identical dimensions (left) and task horizon (right) for DT, CQL, and BC.\nThe performance of all agents degraded roughly equally as the state space dimensionality increased (11 \u2192 17 \u2192 376). Regarding task horizon, with high-quality data (PH), all three agents started near a 100% success rate, but BC deteriorated least quickly, followed by DT and then CQL. In the presence of the suboptimal random data (PH-suboptimal), DT performed the best while BC performed poorly, consistent with our observations in Section 4.4. Additionally, CQL benefited from the addition of suboptimal data, as seen by comparing the solid and dotted blue lines. This suggests that adding such data can improve the performance of CQL in long-horizon tasks, consistent with Figure 1.\nPractical Takeaway: All agents experience similar deterioration when the dimensionality of the state space is increased. When the task horizon is increased, DT remains a robust choice, but BC may be preferable when data is known to be high-quality."
        },
        {
            "heading": "4.6 HOW DO AGENTS BEHAVE IN STOCHASTIC ENVIRONMENTS ?",
            "text": "Understanding the behavior of these agents in stochastic environment is pivotal for practical applications. We evaluated our previously trained agents under the influence of Gaussian noise added to the predicted action during evaluation, with a probability of p on each timestep as follows: action = action+(N (0, 1) \u2217\u03c3+\u00b5), where \u03c3 and \u00b5 are stochasticity parameters. This modification (Fujimoto et al., 2018) is intended to simulate the effect of stochastic environment dynamics. Our findings, presented in Figure 5, reveal that all three agents experience a similar degradation in performance when trained on medium expert data with moderate stochasticity (p = 0.25, \u03c3 = 0.25). However, in the case of sub-optimal (medium replay) data and a higher stochasticity setting (p = 0.5, \u03c3 = 0.5),\nCQL demonstrated superior resilience compared to DT and BC. Building on the observation made by Paster et al. (2022), who looked at DT\u2019s limitations in stochastic environments, our study provides evidence indicating that DT\u2019s performance may hold up reasonably well with CQL in continuous action spaces when stochasticity is moderate and data is high-quality.\nPractical Takeaway: While DT exhibits a comparable decline to CQL when trained on high-quality data in continuous action spaces, CQL is expected to be relatively more robust as data quality declines or stochasticity increases."
        },
        {
            "heading": "4.7 SCALING PROPERTIES OF DECISION TRANSFORMERS ON ATARI",
            "text": "Based on the relative robustness of DT in many of the previous sections, we studied the scaling properties of DT along three dimensions: the amount of training data, the number of model parameters, and both jointly. We focused on the ATARI benchmark because of its high-dimensional observations, for which we expect scaling to be most helpful. We scaled the number of model parameters by increasing the number of layers in the DT architecture \u2208 {6, 8, 12, 16, 18, 20, 24, 32, 64}. The results are shown in Figure 4. The performance of DT reliably increased with more data up until 1.5M timesteps in the dataset (blue, left), but was insensitive to or perhaps even hurt by increasing the number of parameters (green, right). Upon scaling DT to a certain size (3.5M+ parameters), we observed that the larger model outperformed its smaller counterpart given the same amount of data. Additionally, we discuss architectural properties of DT in Appendix A.\nPractical Takeaway: When scaling up DT for complex environments, prioritize scaling the data before the model size. Scaling both simultaneously may improve performance further."
        },
        {
            "heading": "5 LIMITATIONS AND FUTURE WORK",
            "text": "This work addressed the question of which learning method amongst CQL, BC and DT should be preferred for offline reinforcement learning. One of the limitations of our work is that we could broaden our study to include more representative algorithms from each paradigm, such as Implicit Q-Learning (Kostrikov et al., 2021) and Trajectory Transformers (Janner et al., 2021), as well as paradigms we did not explore here, such as model-based offline RL (Kidambi et al., 2020b; Yu et al., 2020b) and diffusion models (Ajay et al., 2022). However, we note that resource limitations make this challenging: each datapoint in every plot of our experiments required around 1,000+ GPU-hours, considering the aggregations over domains and random seeds. Each agent we add exponentially increases the computational demand, exceeding our budget. We also hope to evaluate on a larger set of benchmarks that includes compositional tasks, like those in embodied AI (Duan et al., 2022)."
        },
        {
            "heading": "A ARCHITECTURAL PROPERTIES OF DECISION TRANSFORMERS",
            "text": "Here, we study the impact of architectural properties of DT, namely the context length, # of attention heads, # of layers, and embedding size. Full experimental results are in Appendix I.\nContext length: The use of a context window makes DT dependent on the history of states, actions, and rewards, unlike CQL and BC. Figure 6 (left) demonstrates the role of context length for DT. Having a context length larger than 1 (i.e., no history) did not benefit DT on D4RL, while it helped on ATARI, where performance is maximized at a context length of 64. This finding indicates that some tasks may benefit from a more extensive knowledge of history than others. The deterioration in performance, as context length increases, is likely due to DT overfitting to certain trajectories.\nAttention Heads: While the importance of the number of Transformer attention heads has been noted in NLP (Michel et al., 2019), it is an open question how the trends carry over to offline RL. Figure 6 (right) shows the impact of this hyperparameter on DT. We observed monotonic improvement on ATARI, but no improvement on D4RL. One of the primary reasons for this discrepancy is that there is more room for an agent to extract higher rewards on ATARI than D4RL. For instance, DT achieved an expert-normalized score of over 320 on the BREAKOUT ATARI game, but never gets over 120 in D4RL. This suggests there is more room for scaling the data/parameters to improve results on ATARI.\nNumber of layers: This was discussed in Section 4.7, with results in Figure 4.\nEmbedding size: Increasing the embedding size of DT beyond 256 did not result in any improvement in ATARI; see Table 22 in Appendix I for results."
        },
        {
            "heading": "B ADDITIONAL RESULTS",
            "text": "In this section, we provide additional data points on all three agents on different environments of D4RL.\nPen-Human-v0 from Adroit has been generated from human demonstrations and is higher dimensional compared to other D4RL tasks. We observed that DT and BC outperform CQL, while (Brandfonbrener et al., 2022) showed that DT outperformed IQL.\nTrajectory stitching: Maze environments such as Antmaze and Maze2D require agents to perform trajectory stitching. As (Brandfonbrener et al., 2022) noted, DT based methods require trajectory level information prohibiting DT to make use of information across trajectories. It is due to these reasons that Q-Learning based methods might be considered preferable in the case data requires trajectory stitching to be performed."
        },
        {
            "heading": "C EXTENDED BACKGROUND",
            "text": "In the reinforcement learning (RL) problem, an agent interacts with a Markov decision process (MDP) (Puterman, 1990), defined as a tuple (S,A, T,R,H), where S and A denote the state and action spaces, T (s\u2032, a, s) = Pr(s\u2032|s, a) is the transition model, R(s, a) \u2208 R is the reward function, and H is the (finite) horizon. At each timestep, the agent takes an action a \u2208 A, the environment state transitions according to T , and the agent receives a reward according to R. The agent does not know T or R. Its objective is to obtain a policy \u03c0 : S \u2192 A such that acting under the policy maximizes its return, the sum of expected rewards: E[ \u2211H t=0 R(st, \u03c0(st))]. In offline RL (Levine et al., 2020), agents cannot interact with the MDP but instead learn from a fixed dataset of transitions D = {(s, a, r, s\u2032)}, generated from an unknown behavior policy.\nQ-Learning and CQL. One of the most widely studied learning paradigms is Q-Learning (Sutton and Barto, 2018), which uses temporal difference (TD) updates to estimate the value of taking actions from states via bootstrapping. Although Q-Learning promises to provide a general-purpose decision-making framework, in the offline setting algorithms such as Conservative Q-Learning (CQL) (Kumar et al., 2020) and Implicit Q-Learning (IQL) (Kostrikov et al., 2021) are often unstable and highly sensitive to the choice of hyperparameters (Brandfonbrener et al., 2022). In this paper, we will focus on Conservative Q-Learning (CQL). CQL (Kumar et al., 2020) proposes a modification to the standard Q-learning algorithm to address the overestimation problem by constraining the Q-values so that they do not exceed a lower bound. This constraint is highly effective in the offline setting because it mitigates the distributional shift between the behavior policy and the learned policy. More concretely, CQL adds a regularization term to the standard Bellman update:\nmin \u03b8 \u03b1\n( Es\u223cD [ log (\u2211 a\u2032 exp(Q\u03b8(s, a \u2032)) )] \u2212Es,a\u223cD [Q\u03b8(s, a)] ) + TDError(\u03b8;D), (1)\nwhere \u03b1 is the weight given to the first term, and the second term is the distributional TDError(\u03b8;D) under the dataset, from C51 (Bellemare et al., 2017).\nImitation Learning and BC. When the dataset comes from expert demonstrations or is otherwise near-optimal, researchers often turn to imitation learning algorithms such as Behavior Cloning (BC) or TD3+BC (Fujimoto and Gu, 2021b) to train a policy. BC is a simple imitation learning algorithm that performs supervised learning to map states s in the dataset to their associated actions a. Due to the reward-agnostic nature of BC, it typically requires expert demonstrations to learn an effective policy. The clear downside of BC is that the imitator cannot be expected to attain higher performance than was attained in the dataset.\nSequence Modeling and DT. Sequence modeling is a recently popularized class of offline RL algorithms that includes the Decision Transformer (Chen et al., 2021) and the Trajectory Transformer (Janner et al., 2021). These algorithms train autoregressive sequence models that map the history of the trajectory to the next action. In the Decision Transformer (DT) model, the learned policy produces action distributions conditioned on trajectory history and desired returns-to-go R\u0302t\u2032 = \u2211T t=t\u2032 rt. This leads to the following trajectory representation, used for both training and inference: \u03c4 = (R\u03021, s1, a1, R\u03022, s2, a2, . . . , R\u0302T , sT , aT ). Conditioning on returns-to-go enables DT to learn effective policies from suboptimal data and produce a range of behaviors during inference."
        },
        {
            "heading": "D ADDITIONAL DATASET DETAILS",
            "text": "Humanoid Data In this section, we present the details of the HUMANOID offline Reinforcement Learning (RL) dataset created for our experiments. We trained a Soft Actor-Critic (SAC) agent (Haarnoja et al., 2018) for 3 million steps and selected the best-performing agent, which achieved a score of 5.5k, to generate the expert split. To create the medium split, we utilized an agent that performed at one-third of the expert\u2019s performance. We then generated the medium-expert split by concatenating the medium and expert splits. Our implementation is based on (Raffin et al., 2021) and employs the default hyperparameters for the SAC agent (behavior policy). Table 7 displays the performance of all agents across all splits of the HUMANOID task. Furthermore, Table 8 provides statistical information on the HUMANOID dataset.\nRobomimic: We visualize the return distribution of ROBOMIMIC tasks, employing a discount factor of 0.99, as illustrated in Figure 7 and Figure 8. It becomes evident that the discount factor has a significant impact on the\ndata\u2019s optimality characteristics. PH features shorter trajectories, resulting in a higher proportion of high-return data."
        },
        {
            "heading": "E ADDITIONAL EVALUATION DETAILS",
            "text": "We specify our sampling procedure used to evaluate the ATARI benchmark, used in Section 4.7 and Section A. The ATARI offline dataset (Agarwal et al., 2020) contains the interaction of a DQN agent (Mnih et al., 2015) as it is trained progressively in 50 buffers. Each observation in the dataset contains the last 4 frames of the game, stacked together. Buffers 1 and 50 would contain interactions of the DQN agent when it is naive and expert respectively. Our results are averaged across four different experiments. Each experiment performed a sampling of 500k timestep data from the buffers numbered 1) 1-50 2) 40-50 (DQN is competitive), 3) 45-50, and 4) 49-50 (DQN is expert). We study the architectural and scaling properties of DT in this dataset, where we consider four games: BREAKOUT, QBERT, SEAQUEST, and PONG. We follow the protocol of Lee et al. (2022) and Kumar et al. (2023) by training on the Atari DQN Replay dataset, which used sticky actions, but then evaluating with sticky actions disabled."
        },
        {
            "heading": "F DISENTANGLING DT AND BC",
            "text": "In this section, we experimented with an additional baseline, \u201cBC Transformer,\u201d a modified version of DT that does not perform conditioning on a returns-to-go vector and has a context length of 1. As previously mentioned, we set the context length to 1 in the case of DT for running experiments on ROBOMIMIC as well. This section aims to investigate the discrepancy between the performance of DT and BC, specifically, we wanted to understand how much of the discrepancy can be attributed to RTG conditioning compared to architectural differences between DT and BC which is typically implemented using a stack of MLPs. By having the BC Transformer baseline, the only distinguishing component between it and BC is the architecture. We observed that BC is often a better-performing agent than BC Transformer on PH Table 9, MG Table 10, and MH tasks Table 11. Additionally, it can also be observed that RTG conditioning only plays a critical role when the distribution of reward shows variation. Unlike expert data such as PH and MH where the RTG vector remained the same, we see DT outperforming BC Transformer on MG significantly."
        },
        {
            "heading": "G ADDITIONAL RESULTS ON D4RL AND ROBOMIMIC",
            "text": "This section contains results obtained on individual tasks of D4RL and ROBOMIMIC benchmarks. We use the averaged-out results obtained on all of the tasks from the respective benchmark for our analysis.\nG.1 ESTABLISHING BASELINES\nThis section presents baseline results for individual tasks in both sparse and dense settings of the D4RL. The average outcomes are detailed in Table 2 (Section subsection 4.1). Our observations indicate that DT consistently outperforms CQL and BC in nearly all tasks within the sparse setting of the D4RL benchmark. Although CQL achieves a marginally higher average return on the Hopper task (3.4% ahead of DT for medium and medium-replay splits), it also exhibits significantly higher volatility, as evidenced by the standard deviations. In contrast, DT remains competitive and robust. In the sparse reward setting, CQL surpasses BC by 5.2%. As highlighted in Section subsection 4.1, CQL is most effective in the dense reward setting of the D4RL benchmark.\nG.2 HOW DOES THE AMOUNT AND QUALITY OF DATA AFFECT EACH AGENT\u2019S PERFORMANCE?\nFigure 9 illustrates the performance of agents on individual tasks within the D4RL benchmark as data quality and quantity are varied. DT improved or plateaued (upon reaching maximum performance) as additional data was provided. In contrast, CQL exhibits volatility, displaying significant performance declines in HOPPER and WALKER2D medium-replay tasks. The performance of BC tends to deteriorate when trained on low-return data.\nFigure 10 illustrates the performance behavior of agents as the quantity and quality of data are adjusted in a sparse setting on the D4RL dataset. A more detailed exploration of this behavior across individual tasks is presented in Figure Figure 11.\nTwo key observations can be made from these results. 1) In the sparse reward setting, DT becomes a markedly more sample-efficient choice, with its performance either improving or remaining steady as the quantity of data increases. In contrast, CQL displays greater variability and fails to exceed BC in scenarios involving expert data (medium-expert). 2) The sub-optimal data plays a significantly more important role for CQL in sparse settings as compared to dense settings. Our hypothesis is that the sparsity of feedback makes learning about course correction more critical than learning from expert demonstrations. Notably, we discovered that the worst 10% of data features trajectories with substantially higher return coverage, which contributes to greater diversity within the data. This, in turn, enhances CQL\u2019s ability to learn superior Q-values (course correction) when compared to the best 10% of data in a medium-expert data setting.\nG.3 HOW ARE AGENTS AFFECTED WHEN TRAJECTORY LENGTHS IN THE DATASET INCREASE?\nTable 13 showcases the performance of all agents across individual ROBOMIMIC tasks, encompassing both synthetic and human-generated datasets. DT surpasses both agents in all synthetic tasks within the ROBOMIMIC benchmark, for both sparse and dense settings. Interestingly, BC demonstrates a strong aptitude for numerous tasks with human-generated data, particularly excelling on the SQUARE task.\nG.4 HOW ARE AGENTS AFFECTED WHEN SUBOPTIMAL DATA IS ADDED TO THE DATASET?\nFigure 12 depicts the behavior of agents when random data is introduced according to \u201cStrategy 1\u201d in the dense reward data regime of the D4RL benchmark. As previously described, \u201cStrategy 1\u201d involves rolling out a uniformly random policy from sampled initial states to generate random data. Our observations indicate that both CQL and DT maintain stable performance, while BC exhibits instabilities, occasionally failing as observed in the HALF CHEETAH task.\nFigure 13 displays the behavior of agents as random data is incorporated according to Strategy 2 in the dense reward data regime of the D4RL benchmark. In \"Strategy 2,\" we roll out a pre-trained agent for a certain number of steps, execute a single uniformly random action, and then repeat the process. While Strategy 1 produces random transitions primarily clustered around initial states, Strategy 2 generates random transitions across the entire manifold of states, spanning from initial states to high-reward goal states.\nFigure 14 illustrates the behavior of agents when random data is introduced according to Strategy 2 in the sparse reward data regime of the D4RL benchmark. We observed that the performance of CQL drastically declined on the HOPPER-MEDIUM-REPLAY task, while its performance stayed the same on other tasks.\nFigure 15 depicts the behavior of agents when random data is incorporated following Strategy 1 in the sparse reward data regime (human-generated) of the ROBOMIMIC benchmark. Notably, we observed drastically different performance trends with CQL. Its performance plummeted by 80% when the proportion of random data reached 51%. In contrast, its performance nearly doubled on the CAN MG task when the proportion of random data was increased to 30%.\nFigure 16 illustrates the behavior of agents when random data is introduced according to Strategy 2 in the sparse reward data regime (human-generated) of the ROBOMIMIC benchmark.\nFigure 17 and Figure 18 presents the behavior of agents as random data is added following Strategy 2 in sparse and dense reward data regime (synthetic) respectively on the ROBOMIMIC benchmark. In all four scenarios, DT maintained its peak performance reasonably well, indicating its resilience in very noisy data settings. Conservative Q-Learning (CQL), however, showed signs of deterioration on both dense and sparse variants of LIFT tasks. Notably, CQL failed to perform on the CAN task. As mentioned earlier, BC didn\u2019t exhibit deterioration, possibly because the Mixed-Goal (MG) data already contains a substantial amount of highly sub-optimal data."
        },
        {
            "heading": "H ADDITIONAL EXPERIMENTAL DETAILS",
            "text": "We used the original author implementation as a reference for our experiments wherever applicable. In settings where a new implementation was required, we referenced the implementation which has been known to provide competitive/state-of-the-art results on D4RL. We provide details on compute and hyperparameters below.\nCompute All experiments were run on an A100 GPU. Most experiments with DT typically require 10-15 hours of training. Experiments with CQL and BC require 5-10 hours of training. We used Pytorch 1.12 for our implementation.\nHyperparameters We mentioned all the hyperparameters used across various algorithms below. Our implementations are based on original author-provided implementations, without any modifications to the hyperparameters. To learn more about the selection of hyperparameters, we recommend viewing the associated papers. Due to the stable training objective of DT and BC, both of these agents do not require substantial hyperparameter sweep experiments. DT was trained using Adam optimizer (Kingma and Ba, 2014) with a Multi-Step Learning Rate Scheduler. Each experiment was run five times to account for seed variance."
        },
        {
            "heading": "I ABLATION STUDY TO DETERMINE THE IMPORTANCE OF ARCHITECTURAL COMPONENTS OF DT",
            "text": "In this section, we present the results of our ablation study, which was conducted to assess the significance of various architectural components of the DT. To isolate the impact of individual hyperparameters, we altered one at a time while keeping all others constant. Our findings indicate that the ATARI benchmark is better suited for examining scaling trends compared to the D4RL benchmark. This is likely due to the bounded rewards present in D4RL tasks, which may limit the ability to identify meaningful trends. We did not observe any significant patterns in the D4RL context. A key insight from this investigation is that the performance of DT, when averaged across Atari games, improved as we increased the number of attention heads. However, we did not notice a similar trend when scaling the number of layers (Figure 4). It is also important to mention that the original DT study featured two distinct implementations of the architecture. The DT variant used for reporting results on ATARI benchmark had 8 heads and 6 layers, while the one employed for D4RL featured a single head and 3 layers."
        },
        {
            "heading": "J DT ON EXORL",
            "text": "We additionally conduct smaller-scale experiments in EXORL, which allows us to study the performance of DT on reward-free play data. Typical offline RL datasets are collected from a behavior policy that aims to optimize some (unknown) reward. Contrary to this practice, the EXORL benchmark (Yarats et al., 2022) was obtained from reward-free exploration. After the acquisition of an (s, a, s\u2032) dataset, a reward function is chosen and used to include rewards in the data. This same reward function is used during evaluation. We consider the WALKER WALK, WALKER RUN, and WALKER STAND environments (APT). All scores are averaged over 10 evaluation episodes.\nIn the following section, we present the results obtained using DT on three distinct environments from the EXORL framework. Returns-to-go in the tables presented below represent returns-to-value provided to DT at the time of inference. Upon comparing the metrics in the EXORL study, we noticed that DT\u2019s performance falls short compared to CQL, which may be attributed to the data being collected in a reward-free setting. Although investigating the behavior of these agents in reward-free settings presents an avenue for future research, we propose the following hypothesis. Typically, the exploration of new states in a reward-free environment is conducted through heuristics such as curiosity (ICM) (Pathak et al., 2017) or entropy maximization (APT) (Liu and Abbeel, 2021). The reward functions defined by these heuristics differ from those used for relabeling data when training offline RL agents. Consequently, bootstrapping-based methods might be better equipped to learn a mapping between the reward function determined by the heuristic and the one employed for data relabeling."
        }
    ],
    "year": 2024
}