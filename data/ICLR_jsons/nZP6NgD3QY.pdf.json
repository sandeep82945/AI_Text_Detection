{
    "abstractText": "Multi-task learning (MTL) aims to empower a model to tackle multiple tasks simultaneously. A recent development known as task arithmetic has revealed that several models, each fine-tuned for distinct tasks, can be directly merged into a single model to execute MTL without necessitating a retraining process using the initial training data. Nevertheless, this direct addition of models often leads to a significant deterioration in the overall performance of the merged model. This decline occurs due to potential conflicts and intricate correlations among the multiple tasks. Consequently, the challenge emerges of how to merge pre-trained models more effectively without using their original training data. This paper introduces an innovative technique called Adaptive Model Merging (AdaMerging). This approach aims to autonomously learn the coefficients for model merging, either in a task-wise or layer-wise manner, without relying on the original training data. Specifically, our AdaMerging method operates as an automatic, unsupervised task arithmetic scheme. It leverages entropy minimization on unlabeled test samples from the multi-task setup as a surrogate objective function to iteratively refine the merging coefficients of the multiple models. Our experimental findings across eight tasks demonstrate the efficacy of the AdaMerging scheme we put forth. Compared to the current state-of-the-art task arithmetic merging scheme, AdaMerging showcases a remarkable 11% improvement in performance. Notably, AdaMerging also exhibits superior generalization capabilities when applied to unseen downstream tasks. Furthermore, it displays a significantly enhanced robustness to data distribution shifts that may occur during the testing phase. The code is available at AdaMerging.",
    "authors": [
        {
            "affiliations": [],
            "name": "Enneng Yang"
        },
        {
            "affiliations": [],
            "name": "Zhenyi Wang"
        },
        {
            "affiliations": [],
            "name": "Li Shen"
        },
        {
            "affiliations": [],
            "name": "Shiwei Liu"
        },
        {
            "affiliations": [],
            "name": "Guibing Guo"
        },
        {
            "affiliations": [],
            "name": "Xingwei Wang"
        },
        {
            "affiliations": [],
            "name": "Dacheng Tao"
        }
    ],
    "id": "SP:3013d5a9ad751f95f73ae4aebfbba76ff7141c96",
    "references": [
        {
            "authors": [
                "Samuel Ainsworth",
                "Jonathan Hayase",
                "Siddhartha Srinivasa"
            ],
            "title": "Git re-basin: Merging models modulo permutation symmetries",
            "venue": "In ICLR,",
            "year": 2023
        },
        {
            "authors": [
                "Rich Caruana"
            ],
            "title": "Multitask learning",
            "venue": "Machine learning,",
            "year": 1997
        },
        {
            "authors": [
                "Junbum Cha",
                "Sanghyuk Chun",
                "Kyungjae Lee",
                "Han-Cheol Cho",
                "Seunghyun Park",
                "Yunsung Lee",
                "Sungrae Park"
            ],
            "title": "Swad: Domain generalization by seeking flat",
            "venue": "minima. NeurIPS,",
            "year": 2021
        },
        {
            "authors": [
                "Zhao Chen",
                "Vijay Badrinarayanan",
                "Chen-Yu Lee",
                "Andrew Rabinovich"
            ],
            "title": "Gradnorm: Gradient normalization for adaptive loss balancing in deep multitask networks",
            "venue": "In ICML,",
            "year": 2018
        },
        {
            "authors": [
                "Zhao Chen",
                "Jiquan Ngiam",
                "Yanping Huang",
                "Thang Luong",
                "Henrik Kretzschmar",
                "Yuning Chai",
                "Dragomir Anguelov"
            ],
            "title": "Just pick a sign: Optimizing deep multitask models with gradient sign dropout",
            "venue": "NeurIPS,",
            "year": 2020
        },
        {
            "authors": [
                "Gong Cheng",
                "Junwei Han",
                "Xiaoqiang Lu"
            ],
            "title": "Remote sensing image scene classification: Benchmark and state of the art",
            "venue": "Proceedings of the IEEE,",
            "year": 2017
        },
        {
            "authors": [
                "Mircea Cimpoi",
                "Subhransu Maji",
                "Iasonas Kokkinos",
                "Sammy Mohamed",
                "Andrea Vedaldi"
            ],
            "title": "Describing textures in the wild",
            "venue": "In CVPR, pp",
            "year": 2014
        },
        {
            "authors": [
                "Ronan Collobert",
                "Jason Weston"
            ],
            "title": "A unified architecture for natural language processing: Deep neural networks with multitask learning",
            "venue": "In ICML, pp",
            "year": 2008
        },
        {
            "authors": [
                "Jacob Devlin",
                "Ming-Wei Chang",
                "Kenton Lee",
                "Kristina Toutanova"
            ],
            "title": "BERT: pre-training of deep bidirectional transformers for language understanding",
            "venue": "NAACL-HLT",
            "year": 2019
        },
        {
            "authors": [
                "Ke Ding",
                "Xin Dong",
                "Yong He",
                "Lei Cheng",
                "Chilin Fu",
                "Zhaoxin Huan",
                "Hai Li",
                "Tan Yan",
                "Liang Zhang",
                "Xiaolu Zhang"
            ],
            "title": "Mssm: a multiple-level sparse sharing model for efficient multi-task learning",
            "venue": "In SIGIR,",
            "year": 2021
        },
        {
            "authors": [
                "Daxiang Dong",
                "Hua Wu",
                "Wei He",
                "Dianhai Yu",
                "Haifeng Wang"
            ],
            "title": "Multi-task learning for multiple language translation",
            "venue": "In ACL,",
            "year": 2015
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "Lucas Beyer",
                "Alexander Kolesnikov",
                "Dirk Weissenborn",
                "Xiaohua Zhai",
                "Thomas Unterthiner",
                "Mostafa Dehghani",
                "Matthias Minderer",
                "Georg Heigold",
                "Sylvain Gelly",
                "Jakob Uszkoreit",
                "Neil Houlsby"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
            "year": 2021
        },
        {
            "authors": [
                "Ronald A Fisher"
            ],
            "title": "On the mathematical foundations of theoretical statistics. Philosophical transactions of the Royal Society of London. Series A, containing papers of a mathematical or physical character",
            "year": 1922
        },
        {
            "authors": [
                "Yves Grandvalet",
                "Yoshua Bengio"
            ],
            "title": "Semi-supervised learning by entropy minimization",
            "venue": "NeurIPS,",
            "year": 2004
        },
        {
            "authors": [
                "Pengsheng Guo",
                "Chen-Yu Lee",
                "Daniel Ulbricht"
            ],
            "title": "Learning to branch for multi-task learning",
            "venue": "In ICML,",
            "year": 2020
        },
        {
            "authors": [
                "Vipul Gupta",
                "Santiago Akle Serrano",
                "Dennis DeCoste"
            ],
            "title": "Stochastic weight averaging in parallel: Large-batch training that generalizes well",
            "venue": "In ICLR. OpenReview.net,",
            "year": 2020
        },
        {
            "authors": [
                "Hussein Hazimeh",
                "Zhe Zhao",
                "Aakanksha Chowdhery",
                "Maheswaran Sathiamoorthy",
                "Yihua Chen",
                "Rahul Mazumder",
                "Lichan Hong",
                "Ed Chi"
            ],
            "title": "Dselect-k: Differentiable selection in the mixture of experts with applications to multi-task learning",
            "venue": "NeurIPS,",
            "year": 2021
        },
        {
            "authors": [
                "Yun He",
                "Xue Feng",
                "Cheng Cheng",
                "Geng Ji",
                "Yunsong Guo",
                "James Caverlee"
            ],
            "title": "Metabalance: Improving multi-task recommendations via adapting gradient magnitudes of auxiliary tasks",
            "year": 2022
        },
        {
            "authors": [
                "Patrick Helber",
                "Benjamin Bischke",
                "Andreas Dengel",
                "Damian Borth"
            ],
            "title": "Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification",
            "venue": "IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing,",
            "year": 2019
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Thomas Dietterich"
            ],
            "title": "Benchmarking neural network robustness to common corruptions and perturbations",
            "year": 2019
        },
        {
            "authors": [
                "Edward J Hu",
                "Phillip Wallis",
                "Zeyuan Allen-Zhu",
                "Yuanzhi Li",
                "Shean Wang",
                "Lu Wang",
                "Weizhu Chen"
            ],
            "title": "Lora: Low-rank adaptation of large language models",
            "venue": "In ICLR,",
            "year": 2021
        },
        {
            "authors": [
                "Chengsong Huang",
                "Qian Liu",
                "Bill Yuchen Lin",
                "Tianyu Pang",
                "Chao Du",
                "Min Lin"
            ],
            "title": "Lorahub: Efficient cross-task generalization via dynamic lora composition",
            "venue": "arXiv preprint arXiv:2307.13269,",
            "year": 2023
        },
        {
            "authors": [
                "Gabriel Ilharco",
                "Marco Tulio Ribeiro",
                "Mitchell Wortsman",
                "Ludwig Schmidt",
                "Hannaneh Hajishirzi",
                "Ali Farhadi"
            ],
            "title": "Editing models with task arithmetic",
            "venue": "In ICLR,",
            "year": 2023
        },
        {
            "authors": [
                "Junguang Jiang",
                "Baixu Chen",
                "Junwei Pan",
                "Ximei Wang",
                "Dapeng Liu",
                "Jie Jiang",
                "Mingsheng Long"
            ],
            "title": "Forkmerge: Mitigating negative transfer in auxiliary-task learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2024
        },
        {
            "authors": [
                "Xisen Jin",
                "Xiang Ren",
                "Daniel Preotiuc-Pietro",
                "Pengxiang Cheng"
            ],
            "title": "Dataless knowledge fusion by merging weights of language models",
            "venue": "In ICLR,",
            "year": 2023
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "arXiv preprint arXiv:1412.6980,",
            "year": 2014
        },
        {
            "authors": [
                "Jonathan Krause",
                "Michael Stark",
                "Jia Deng",
                "Li Fei-Fei"
            ],
            "title": "3d object representations for fine-grained categorization",
            "venue": "In ICCV workshops,",
            "year": 2013
        },
        {
            "authors": [
                "Yann LeCun"
            ],
            "title": "The mnist database of handwritten digits. http://yann",
            "venue": "lecun. com/exdb/mnist/,",
            "year": 1998
        },
        {
            "authors": [
                "Weishi Li",
                "Yong Peng",
                "Miao Zhang",
                "Liang Ding",
                "Han Hu",
                "Li Shen"
            ],
            "title": "Deep model fusion: A survey",
            "venue": "arXiv preprint arXiv:2309.15698,",
            "year": 2023
        },
        {
            "authors": [
                "Xiang Li",
                "Kaixuan Huang",
                "Wenhao Yang",
                "Shusen Wang",
                "Zhihua Zhang"
            ],
            "title": "On the convergence of fedavg on non-iid data",
            "venue": "In ICLR,",
            "year": 2019
        },
        {
            "authors": [
                "Jian Liang",
                "Ran He",
                "Tieniu Tan"
            ],
            "title": "A comprehensive survey on test-time adaptation under distribution shifts",
            "venue": "arXiv preprint arXiv:2303.15361,",
            "year": 2023
        },
        {
            "authors": [
                "Petro Liashchynskyi",
                "Pavlo Liashchynskyi"
            ],
            "title": "Grid search, random search, genetic algorithm: a big comparison for nas",
            "venue": "arXiv preprint arXiv:1912.06059,",
            "year": 2019
        },
        {
            "authors": [
                "Bo Liu",
                "Xingchao Liu",
                "Xiaojie Jin",
                "Peter Stone",
                "Qiang Liu"
            ],
            "title": "Conflict-averse gradient descent for multi-task learning",
            "venue": "NeurIPS,",
            "year": 2021
        },
        {
            "authors": [
                "Chang Liu",
                "Chenfei Lou",
                "Runzhong Wang",
                "Alan Yuhan Xi",
                "Li Shen",
                "Junchi Yan"
            ],
            "title": "Deep neural network fusion via graph matching with applications to model ensemble and federated learning",
            "venue": "In ICML,",
            "year": 2022
        },
        {
            "authors": [
                "Jialin Liu",
                "Antoine Moreau",
                "Mike Preuss",
                "Jeremy Rapin",
                "Baptiste Roziere",
                "Fabien Teytaud",
                "Olivier Teytaud"
            ],
            "title": "Versatile black-box optimization",
            "venue": "In Proceedings of the 2020 Genetic and Evolutionary Computation Conference,",
            "year": 2020
        },
        {
            "authors": [
                "Shikun Liu",
                "Edward Johns",
                "Andrew J. Davison"
            ],
            "title": "End-to-end multi-task learning with attention",
            "venue": "In CVPR, pp. 1871\u20131880",
            "year": 2019
        },
        {
            "authors": [
                "Xiaodong Liu",
                "Pengcheng He",
                "Weizhu Chen",
                "Jianfeng Gao"
            ],
            "title": "Multi-task deep neural networks for natural language understanding",
            "venue": "Association for Computational Linguistics,",
            "year": 2019
        },
        {
            "authors": [
                "Yongxi Lu",
                "Abhishek Kumar",
                "Shuangfei Zhai",
                "Yu Cheng",
                "Tara Javidi",
                "Rogerio Feris"
            ],
            "title": "Fullyadaptive feature sharing in multi-task networks with applications in person attribute classification",
            "venue": "In CVPR,",
            "year": 2017
        },
        {
            "authors": [
                "Jiaqi Ma",
                "Zhe Zhao",
                "Xinyang Yi",
                "Jilin Chen",
                "Lichan Hong",
                "Ed H. Chi"
            ],
            "title": "Modeling task relationships in multi-task learning with multi-gate mixture-of-experts",
            "venue": "In SIGKDD,",
            "year": 2018
        },
        {
            "authors": [
                "Michael S Matena",
                "Colin A Raffel"
            ],
            "title": "Merging models with fisher-weighted",
            "venue": "averaging. NeurIPS,",
            "year": 2022
        },
        {
            "authors": [
                "Ishan Misra",
                "Abhinav Shrivastava",
                "Abhinav Gupta",
                "Martial Hebert"
            ],
            "title": "Cross-stitch networks for multi-task learning",
            "venue": "In CVPR,",
            "year": 2016
        },
        {
            "authors": [
                "Leann Myers",
                "Maria J Sirois"
            ],
            "title": "Spearman correlation coefficients, differences between",
            "venue": "Encyclopedia of statistical sciences,",
            "year": 2004
        },
        {
            "authors": [
                "Shuaicheng Niu",
                "Jiaxiang Wu",
                "Yifan Zhang",
                "Yaofo Chen",
                "Shijian Zheng",
                "Peilin Zhao",
                "Mingkui Tan"
            ],
            "title": "Efficient test-time model adaptation without forgetting",
            "venue": "In ICML,",
            "year": 2022
        },
        {
            "authors": [
                "Shuaicheng Niu",
                "Jiaxiang Wu",
                "Yifan Zhang",
                "Zhiquan Wen",
                "Yaofo Chen",
                "Peilin Zhao",
                "Mingkui Tan"
            ],
            "title": "Towards stable test-time adaptation in dynamic wild world",
            "venue": "In ICLR,",
            "year": 2023
        },
        {
            "authors": [
                "Guillermo Ortiz-Jimenez",
                "Alessandro Favero",
                "Pascal Frossard"
            ],
            "title": "Task arithmetic in the tangent space: Improved editing of pre-trained models",
            "venue": "In NeurIPS,",
            "year": 2023
        },
        {
            "authors": [
                "Adam Paszke",
                "Sam Gross",
                "Soumith Chintala",
                "Gregory Chanan",
                "Edward Yang",
                "Zachary DeVito",
                "Zeming Lin",
                "Alban Desmaison",
                "Luca Antiga",
                "Adam Lerer"
            ],
            "title": "Automatic differentiation in pytorch",
            "year": 2017
        },
        {
            "authors": [
                "Xipeng Qiu",
                "Tianxiang Sun",
                "Yige Xu",
                "Yunfan Shao",
                "Ning Dai",
                "Xuanjing Huang"
            ],
            "title": "Pre-trained models for natural language processing: A survey",
            "venue": "Science China Technological Sciences,",
            "year": 2020
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "venue": "In ICML,",
            "year": 2021
        },
        {
            "authors": [
                "Subhankar Roy",
                "Martin Trapp",
                "Andrea Pilzer",
                "Juho Kannala",
                "Nicu Sebe",
                "Elisa Ricci",
                "Arno Solin"
            ],
            "title": "Uncertainty-guided source-free domain adaptation",
            "venue": "In ECCV,",
            "year": 2022
        },
        {
            "authors": [
                "Ozan Sener",
                "Vladlen Koltun"
            ],
            "title": "Multi-task learning as multi-objective optimization",
            "venue": "In NeurIPS,",
            "year": 2018
        },
        {
            "authors": [
                "Claude Elwood Shannon"
            ],
            "title": "A mathematical theory of communication",
            "venue": "The Bell system technical journal,",
            "year": 1948
        },
        {
            "authors": [
                "Derun Song",
                "Enneng Yang",
                "Guibing Guo",
                "Li Shen",
                "Linying Jiang",
                "Xingwei Wang"
            ],
            "title": "Multi-scenario and multi-task aware feature interaction for recommendation system",
            "year": 2024
        },
        {
            "authors": [
                "Johannes Stallkamp",
                "Marc Schlipsing",
                "Jan Salmen",
                "Christian Igel"
            ],
            "title": "The german traffic sign recognition benchmark: a multi-class classification competition",
            "venue": "In IJCNN,",
            "year": 2011
        },
        {
            "authors": [
                "Trevor Standley",
                "Amir Zamir",
                "Dawn Chen",
                "Leonidas Guibas",
                "Jitendra Malik",
                "Silvio Savarese"
            ],
            "title": "Which tasks should be learned together in multi-task learning",
            "venue": "In ICML,",
            "year": 2020
        },
        {
            "authors": [
                "George Stoica",
                "Daniel Bolya",
                "Jakob Bjorner",
                "Taylor Hearn",
                "Judy Hoffman"
            ],
            "title": "Zipit! merging models from different tasks without training",
            "venue": "arXiv preprint arXiv:2305.03053,",
            "year": 2023
        },
        {
            "authors": [
                "Ximeng Sun",
                "Rameswar Panda",
                "Rogerio Feris",
                "Kate Saenko"
            ],
            "title": "Adashare: Learning what to share for efficient deep multi-task learning",
            "venue": "NeurIPS,",
            "year": 2020
        },
        {
            "authors": [
                "Simon Vandenhende",
                "Stamatios Georgoulis",
                "Wouter Van Gansbeke",
                "Marc Proesmans",
                "Dengxin Dai",
                "Luc Van Gool"
            ],
            "title": "Multi-task learning for dense prediction tasks: A survey",
            "year": 2021
        },
        {
            "authors": [
                "Dequan Wang",
                "Evan Shelhamer",
                "Shaoteng Liu",
                "Bruno Olshausen",
                "Trevor Darrell"
            ],
            "title": "Tent: Fully test-time adaptation by entropy minimization",
            "venue": "In ICLR,",
            "year": 2021
        },
        {
            "authors": [
                "Hongyi Wang",
                "Mikhail Yurochkin",
                "Yuekai Sun",
                "Dimitris Papailiopoulos",
                "Yasaman Khazaeni"
            ],
            "title": "Federated learning with matched averaging",
            "venue": "In ICLR,",
            "year": 2020
        },
        {
            "authors": [
                "Zhenyi Wang",
                "Xiaoyang Wang",
                "Li Shen",
                "Qiuling Suo",
                "Kaiqiang Song",
                "Dong Yu",
                "Yan Shen",
                "Mingchen Gao"
            ],
            "title": "Meta-learning without data via wasserstein distributionally-robust model fusion",
            "venue": "In UAI,",
            "year": 2022
        },
        {
            "authors": [
                "Zhenyi Wang",
                "Li Shen",
                "Tiehang Duan",
                "Qiuling Suo",
                "Le Fang",
                "Wei Liu",
                "Mingchen Gao"
            ],
            "title": "Distributionally robust memory evolution with generalized divergence for continual learning",
            "year": 2023
        },
        {
            "authors": [
                "Mitchell Wortsman",
                "Gabriel Ilharco",
                "Samir Ya Gadre",
                "Rebecca Roelofs",
                "Raphael Gontijo-Lopes",
                "Ari S Morcos",
                "Hongseok Namkoong",
                "Ali Farhadi",
                "Yair Carmon",
                "Simon Kornblith"
            ],
            "title": "Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time",
            "venue": "PMLR,",
            "year": 2022
        },
        {
            "authors": [
                "Chengyue Wu",
                "Teng Wang",
                "Yixiao Ge",
                "Zeyu Lu",
                "Ruisong Zhou",
                "Ying Shan",
                "Ping Luo"
            ],
            "title": "pi-tuning: Transferring multimodal foundation models with optimal multi-task interpolation",
            "venue": "In ICML,",
            "year": 2023
        },
        {
            "authors": [
                "Sen Wu",
                "Hongyang R. Zhang",
                "Christopher R\u00e9"
            ],
            "title": "Understanding and improving information transfer in multi-task learning",
            "venue": "In ICLR. OpenReview.net,",
            "year": 2020
        },
        {
            "authors": [
                "Jianxiong Xiao",
                "Krista A Ehinger",
                "James Hays",
                "Antonio Torralba",
                "Aude Oliva"
            ],
            "title": "Sun database: Exploring a large collection of scene",
            "venue": "categories. IJCV,",
            "year": 2016
        },
        {
            "authors": [
                "Prateek Yadav",
                "Derek Tam",
                "Leshem Choshen",
                "Colin Raffel",
                "Mohit Bansal"
            ],
            "title": "Resolving interference when merging models",
            "venue": "NeurIPS,",
            "year": 2023
        },
        {
            "authors": [
                "Enneng Yang",
                "Junwei Pan",
                "Ximei Wang",
                "Haibin Yu",
                "Li Shen",
                "Xihua Chen",
                "Lei Xiao",
                "Jie Jiang",
                "Guibing Guo"
            ],
            "title": "Adatask: A task-aware adaptive learning rate approach to multi-task learning",
            "venue": "In AAAI,",
            "year": 2023
        },
        {
            "authors": [
                "Jason Yosinski",
                "Jeff Clune",
                "Yoshua Bengio",
                "Hod Lipson"
            ],
            "title": "How transferable are features in deep neural networks",
            "venue": "NeurIPS,",
            "year": 2014
        },
        {
            "authors": [
                "Tianhe Yu",
                "Saurabh Kumar",
                "Abhishek Gupta",
                "Sergey Levine",
                "Karol Hausman",
                "Chelsea Finn"
            ],
            "title": "Gradient surgery for multi-task learning",
            "venue": "NeurIPS,",
            "year": 2020
        },
        {
            "authors": [
                "Netzer Yuval"
            ],
            "title": "Reading digits in natural images with unsupervised feature learning",
            "venue": "In NIPS Workshop on Deep Learning and Unsupervised Feature Learning,",
            "year": 2011
        },
        {
            "authors": [
                "Jinghan Zhang",
                "Shiqi Chen",
                "Junteng Liu",
                "Junxian He"
            ],
            "title": "Composing parameter-efficient modules with arithmetic operations",
            "venue": "arXiv preprint arXiv:2306.14870,",
            "year": 2023
        },
        {
            "authors": [
                "Xiao"
            ],
            "title": "2016) is a scene classification dataset, which contains images in 397 classes, with a total of 108,754 images, and each class has at least 100 images. \u2022 Stanford Cars (Cars) (Krause et al., 2013) is a car classification dataset, which contains 196 classes of cars and a total of 16,185 images. Each class in the training set and test set is divided at a ratio of 1:1",
            "year": 2013
        },
        {
            "authors": [
                "Fisher Merging (Matena",
                "Raffel"
            ],
            "title": "2022) calculates the Fisher information matrix (Fisher, 1922) to measure the importance of each parameter when merging models, and model merging is performed according to the guidance of this importance",
            "year": 1922
        },
        {
            "authors": [
                "\u2022 RegMean (Jin"
            ],
            "title": "2023) imposes a constraint when merging models, that is, the L2 distance between the merged model and a single model is required",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Multi-task learning (MTL) is a technique that enables the transfer of knowledge (Wu et al., 2020; Wang et al., 2023; Jiang et al., 2024) among multiple tasks by efficiently sharing model parameters, leading to improvements in overall performance (Caruana, 1997; Liu et al., 2019b; Vandenhende et al., 2021) across a variety of tasks. Consequently, it has garnered significant attention in fields such as computer vision (Misra et al., 2016; Chen et al., 2018; 2020), natural language processing (Collobert & Weston, 2008; Dong et al., 2015), and recommendation systems (Ma et al., 2018; Yang et al., 2023; Song et al., 2024). In the context of foundation models, there are two key considerations. On the one hand, it is highly inefficient to pursue the traditional MTL approach for large pre-trained models by collecting a large volume of training data due to the high data labeling and computation cost. On the other hand, the advent of pre-trained models\u2019 popularity (Qiu et al., 2020) has led to a prevalent practice among downstream tasks. These tasks independently fine-tune the same pre-trained model, such as ViT (Dosovitskiy et al., 2021) or BERT (Devlin et al., 2019), and subsequently release these fine-tuned models, often without disclosing the specifics of their original training data. Consequently, there has emerged a recent trend in the research community, focused on exploring methodologies for effectively merging multiple independently trained models without relying on their training data for the purpose of MTL (Matena & Raffel, 2022; Jin et al., 2023; Ainsworth et al., 2023; Ilharco et al., 2023; Huang et al., 2023; Ortiz-Jimenez et al., 2023; Yadav et al., 2023; Li et al., 2023).\n\u2217Corresponding author\nRecently, a novel concept in MTL known as task arithmetic has emerged (Ilharco et al., 2023). Task arithmetic introduces the notion of a \u201ctask vector\u201d, which can be described as a vector of weights fine-tuned specifically for a given task, subtracted from the corresponding pre-trained weights (as illustrated in Fig. 2(a)). Essentially, a task vector serves as a unique representation for a particular task. Research in this area, focusing on methods centered around task vectors (Ilharco et al., 2023; Yadav et al., 2023), has demonstrated that by summing multiple task vectors and integrating them into a pretrained model, a new model can be created with the capability to handle multi-task learning effectively (as depicted in Fig. 2(b)). However, despite the promising results, there still exists a substantial performance gap between task vector-based MTL methods, such as Task Arithmetic (Ilharco et al., 2023) and Ties-Merging (Yadav et al., 2023), and traditional MTL approaches, as highlighted in Fig. 1. This disparity in performance suggests that further research and refinement are required to bridge the existing gap and unlock the full potential of task vector-based MTL methodologies.\nA critical observation in the analysis of task vector-based MTL methods is the significance of the merging coefficient (denoted as \u03bb in Fig. 2(b)) associated with the task vector. This coefficient plays a pivotal role in determining the average accuracy of the final MTL model. As illustrated in Fig. 1, particularly in the cases of Task Arithmetic (indicated by the yellow line) and Ties-Merging (represented by the blue line), an ill-suited merging coefficient can lead to a situation where the model struggles to effectively perform MTL. In such scenarios, the average accuracy across multiple tasks becomes unacceptably low. This sensitivity to the merging coefficient may stem from potential conflicts (Guo et al., 2020; Vandenhende et al., 2021) or intricate relationships (Ma et al., 2018; Standley et al., 2020) among the multiple tasks, which make the merging process highly susceptible to the choice of this coefficient. Consequently, one of the primary challenges encountered in task vector-based MTL lies in determining the appropriate task vector merging coefficients that facili-\ntate the optimal integration of multiple tasks, all without relying on the original training data for each task. Additionally, it is more desirable and flexible to fine-tune different coefficients for different layers within the merged model. However, when dealing with a substantial number of tasks and layers, traditional approaches such as grid search (Liashchynskyi & Liashchynskyi, 2019) or combinatorial optimization search (Liu et al., 2020) become impractical for identifying suitable model merging coefficients. Hence, addressing this issue efficiently and effectively remains a challenging research problem in the field of task vector-based MTL.\nIn this paper, our inspiration comes from test-time adaptation schemes aimed at optimizing model generalization when faced with previously unseen test data (Wang et al., 2021; Liang et al., 2023). Building upon these concepts, we introduce an innovative automatic unsupervised multi-task model merging scheme. This scheme leverages the minimization of prediction distribution entropy on unlabeled multi-task test data as a surrogate objective to adaptively learn model merging coefficients. The intuitive motivation of entropy minimization is to make the model produce a more deterministic output when faced with a given input, which can lead to a more robust and accurate model. Our approach begins with an analysis of the relationship between entropy and prediction loss across eight tasks. As depicted in Fig. 3(a), our observations reveal that samples with lower entropy also exhibit smaller prediction losses. Furthermore, we calculate the Spearman correlation coefficient (Myers & Sirois, 2004) to quantify the relationship between entropy and prediction loss, as illustrated in Fig. 3(b). The results affirm a positive correlation between entropy and prediction loss, confirming that entropy can serve as a suitable proxy objective for optimization purposes. Subsequently, we put forth two adaptive model merging schemes, collectively referred to as AdaMerging. These schemes are designed to automatically learn a merging coefficient for each task vector or each layer of each task vector, as depicted in Fig. 2(c) and (d). To update these merging coefficients, we employ entropy minimization as a proxy objective, thereby enhancing the adaptability and performance of the multi-task model merging process.\nFinally, we conduct a comprehensive evaluation to ascertain the superiority of AdaMerging when compared to existing task vector-based methods, revealing its advantages in three key aspects: (i) Significantly Higher MTL Performance: Our extensive testing across eight task vectors demonstrated that AdaMerging\u2019s adaptive learning merging coefficient significantly enhances the average accuracy across multiple tasks. For instance, on the ViT-B/32, AdaMerging improved approximately 5.0% to 11.0% over Task Arithmetic and Ties-Merging. (ii) Substantially Improved Generalization: Our evaluation on two sets of previously unseen downstream tasks underscored AdaMerging\u2019s superior generalization capabilities, resulting in improvements ranging from 4.4% to 9.1% when compared to Task Arithmetic and Ties-Merging. (iii) Robust to Test Data Distribution Shifts: In addition to performance gains, AdaMerging exhibited substantially enhanced robustness in multi-task testing across seven distribution drifts, with an average improvement of 8.45% compared to Task Arithmetic.\nThis paper makes four significant contributions: (i) We re-examine existing task vector-based multitask learning (MTL) methods and unveil the substantial influence of model merging coefficients on the average MTL performance. (ii) We introduce a novel approach called AdaMerging, which autonomously learns merging coefficients in an unsupervised manner. This method can adaptively determine coefficients for different task vectors (Task-wise AdaMerging) or individual layers within different task vectors (Layer-wise AdaMerging). (iii) We establish a strong positive correlation between entropy minimization and loss minimization on MTL\u2019s test data. This correlation signifies that these metrics can effectively serve as proxy objectives for optimizing the model merging coefficients within AdaMerging. (iv) We conduct comprehensive experiments to validate our method. The results demonstrate its substantial improvements in performance, generalization capabilities, and robustness compared to state-of-the-art (SOTA) task vector-based model merging methods."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Joint Training for Multi-Task Learning. The joint training method gathers training data from multiple tasks to learn these tasks simultaneously (Caruana, 1997) to achieve knowledge transfer (Wu et al., 2023). Existing works mainly focus on mitigating task conflicts from a architecture (Misra et al., 2016; Sun et al., 2020) or optimization (Sener & Koltun, 2018; Liu et al., 2021) perspective. Architectural-based methods mitigate task interference by sparsifying (Liu et al., 2019a; Ding et al., 2021), branching (Lu et al., 2017; Guo et al., 2020) or modularizing (Ma et al., 2018; Hazimeh et al., 2021) shared structures. Optimization-based methods balance multiple tasks from the perspectives of task training weights (Sener & Koltun, 2018; Liu et al., 2019a), gradient dominance (Chen et al., 2018; He et al., 2022; Yang et al., 2023), and gradient conflicts (Yu et al., 2020; Chen et al., 2020; Liu et al., 2021). However, the conventional approaches for collecting raw data across multiple tasks for joint training face challenges that may render them unsuitable in the era of foundation models. This is primarily due to either (i) their computational inefficiency stemming from the high computation cost for updating the pre-trained models or (ii) numerous data owners refrain from releasing valuable or privacy-sensitive raw data. Instead, they opt to share models fine-tuned on these pre-trained models.\nModel Merging for Multi-task Learning. The practice of model merging has emerged as a promising solution to enhance model generalization and facilitate MTL. The first type of research\ninvolves merging multiple models, all initially trained on the same task, with the aim of enhancing the model\u2019s overall generalization (Gupta et al., 2020; Cha et al., 2021; Wortsman et al., 2022; Wang et al., 2022) or to perform federated learning (Li et al., 2019; Wang et al., 2020; Liu et al., 2022). The other type of work attempts to merge models for different tasks to perform MTL (Matena & Raffel, 2022; Jin et al., 2023; Ainsworth et al., 2023; Stoica et al., 2023; Ortiz-Jimenez et al., 2023; Zhang et al., 2023; Ilharco et al., 2023; Yadav et al., 2023). This paper primarily concentrates on the latter approach. However, simple model averaging alone can significantly deteriorate performance across multiple tasks. Consequently, in recent years, numerous advanced techniques have surfaced to mitigate the performance loss associated with model merging. For example, Fisher Merging (Matena & Raffel, 2022) employs the Fisher information matrix (Fisher, 1922) to measure the importance of individual model parameter. Subsequently, it leverages this importance metric to guide the model merging. However, the computation of the Fisher information matrix becomes computationally and memory-intensive when dealing with a large number of model parameters. RegMean (Jin et al., 2023) suggests minimizing the L2 distance between the merged model and each individual model. However, this approach necessitates the precomputation and provision of the inner product matrix for the training dataset. This information may not be accessible if the model owner chooses not to disclose it. In recent developments, Task Arithmetic (Ilharco et al., 2023), introduces the concept of \u201ctask vectors\u201d. This approach demonstrates that merging task vectors to create a consolidated model can effectively facilitate MTL. Building upon this foundation, PEM Composition (Zhang et al., 2023) extends the task arithmetic framework to incorporate the merging of LoRA (Hu et al., 2021) models. Taking this a step further, Ties-Merging (Yadav et al., 2023) addresses task conflicts within the Task Arithmetic paradigm. It accomplishes this by resetting redundant parameters, resolving sign conflicts, and exclusively merging parameters that exhibit sign-consistency. Task vector-based studies overlook a critical challenge encountered when dealing with a diverse collection of models, i.e., the coefficients governing the model merging process play a pivotal role in achieving optimal merging performance. In contrast, our work specifically emphasizes and addresses this issue to bridge the performance gap.\nOverall, our work has three essential differences from existing task vector-based MTL schemes: (i) They share a merging coefficient across all task vectors, limiting the flexibility of task vector combinations. By contrast, our method adopts different merging coefficients across different tasks or even different layers, substantially enhancing the flexibility of adaptations. (ii) Existing works employ grid-searching the merging coefficients, thus lacking a guiding principle and is costly and infeasible when the number of tasks is large, while our work takes entropy minimization as a proxy objective to optimize the merging coefficients efficiently and automatically. (iii) We significantly improve multi-task performance, generalization to unseen tasks, and robustness to test data distribution shifts."
        },
        {
            "heading": "3 METHODOLOGY",
            "text": "We define the notation and model merging problem in Sec. 3.1, and briefly describe the solution based on task vectors. In Sec.3.2, we introduce the proposed AdaMerging method in detail."
        },
        {
            "heading": "3.1 PRELIMINARIES",
            "text": "Notation: Let f\u03b8(xi) \u2192 y\u0302i be a neural network model parameterized by a set of weights \u03b8 = {\u03b81, \u03b82, . . . , \u03b8L}, which takes xi \u2208 Rd as an input data and outputs the predicted value y\u0302i \u2208 RC . Among them, \u03b8l is the weight of the l-th (l \u2208 {1, 2, . . . , L}) layer, L represents the number of layers of the network f , d represents the dimension of the input data xi, and C represents the number of classes. Without loss of generality, we assume that the weights of a well-known pre-trained model, e.g., ViT (Dosovitskiy et al., 2021) or BERT (Devlin et al., 2019)), are \u03b8pre = {\u03b81pre, \u03b82pre, . . . , \u03b8Lpre}. There are K tasks, and each of them has fine-tuned \u03b8pre on their own private training data {xi, yi} Ntrk i=1 , N trk represents the number of training samples for task k. Consequently, the model\u2019s weights after fine-tuning for task k are recorded as \u03b8k = {\u03b81k, \u03b82k, . . . , \u03b8Lk }.\nProblem Definition: The model merging problem is defined as how to combine weights {\u03b8k}Kk=1 to get a new weight \u03b8MTL without necessitating a retraining process using the initial task\u2019s training data, and ensure that f\u03b8MTL can perform tasks 1, 2, . . . ,K simultaneously. A straightforward approach is to perform weight averaging, i.e., \u03b8MTL = 1K \u2211K k=1 \u03b8k, however the performance of this approach usually drops dramatically (Ilharco et al., 2023; Yadav et al., 2023).\nTask Arithmetic: A recent research (Ilharco et al., 2023) defines the concept of \u201ctask vectors\u201d and completes various task arithmetic operations based on task vectors, such as adding multiple task vectors to the pre-trained weight \u03b8pre to perform MTL. Specifically, as shown in Fig. 2(a), the task vector Tk w.r.t task k is defined as a vector obtained by performing a subtraction operation with the fine-tuned weights \u03b8k and the pre-trained weights \u03b8pre, i.e., Tk = \u03b8k \u2212 \u03b8pre. Furthermore, multiple task vectors {Tk}Kk=1 are added and merged into the pre-trained model, \u03b8MTL = \u03b8pre + \u03bb \u2211K\nk=1 Tk, where the coefficient \u03bb represents the importance of model merging. On this basis, TiesMerging (Yadav et al., 2023) shows that some parameter values in the task vector may be redundant, or the signs of the parameters may conflict, and direct merging will cause performance losses. Based on this assumption, they proposed to perform three steps of Trim, Elect Sign and Disjoint Merge on merging task vectors. We combine these steps and abbreviate them as one \u03a6() operation. Therefore, model merging in Ties-Merging can be expressed as \u03b8MTL = \u03b8pre + \u03bb \u2211K k=1 \u03a6(Tk).\nTask arithmetic is a simple and effective idea. As shown in Fig. 1, task vectors based MTL model merging methods, i.e., Task Arithmetic (blue line), Ties-Merging (yellow line), are significantly better than simple weight averaging scheme (pink line). However, there is still a clear gap between them and the traditional MTL (black line). In addition, task vector-based model merging methods are very sensitive to the merging coefficient \u03bb. An ill-suited \u03bb will cause the performance to be lower than the weighted average, or even reach unacceptably low accuracy. When the number of tasks is large, grid searching the merging coefficients for each task vector is expensive. This motivates us to conduct further research to narrow the performance gap between task vector-based MTL and traditional MTL."
        },
        {
            "heading": "3.2 ADAPTIVE MODEL MERGING FOR MULTI-TASK LEARNING",
            "text": "In this section, we propose an unsupervised adaptive model merging method for task vectors based MTL, called AdaMerging. It makes the merging coefficient of each task vector learnable (Task-wise AdaMerging). Furthermore, different layers of a task vector can also automatically learn different merging coefficients in AdaMerging (Layer-wise AdaMerging)."
        },
        {
            "heading": "3.2.1 ADAMERGING: ADAPTIVE MODEL MERGING",
            "text": "Task-wise AdaMerging: As shown in Fig. 2(c), our Task-wise AdaMerging assigns a separate merging coefficient \u03bbk to each task vector Tk, that is: \u03b8MTL = \u03b8pre + \u2211K k=1 \u03bbkTk. Task-wise AdaMerging allows task vectors Tk that have a positive transfer to the average MTL performance to occupy a larger proportion in \u03b8MTL, while task vector Tk\u2032 that is harmful to MTL will reduce their contribution to the merging weight \u03b8MTL, thereby improving the average MTL performance.\nLayer-wise AdaMerging: However, Task-wise AdaMerging may not be enough to alleviate the interference of task vectors. In the deep neural network model, the information learned by each layer is different. For example, the lower layer may learn general features, while the higher layers may learn task-specific features (Yosinski et al., 2014). Therefore, when merging task vectors, the weights {T 1k , T 2k , . . . , TLk } of different layers for each task vector Tk should also have different contributions {\u03bb1k, \u03bb2k, . . . , \u03bbLk } to the final multi-task weights \u03b8MTL. Based on this, we propose the Layer-wise AdaMerging scheme shown in Fig. 2(d), which is formalized as: \u03b8MTL = { \u03b8lMTL }L l=1\n={ \u03b8lpre + \u2211K k=1 \u03bb l kT l k }L l=1 , where L represents the number of layers.\nAdaMerging++: The above AdaMerging adaptively merges the original task vector Tk in the Task Arithmetic (Ilharco et al., 2023). Naturally, it can also adaptively merge the task vector \u03a6(Tk) after removing parameter redundant values and sign conflicts in Ties-Merging (Yadav et al., 2023). We call this variant AdaMerging++, and the corresponding Task-wise AdaMerging++ and Layerwise AdaMerging++ versions are formalized as \u03b8MTL = \u03b8pre +\n\u2211K k=1 \u03bbk\u03a6(Tk) and \u03b8MTL ={\n\u03b8lMTL }L l=1 = { \u03b8lpre + \u2211K k=1 \u03bb l k\u03a6(T l k) }L l=1 , respectively.\nNow, AdaMerging/AdaMerging++ faces a critical challenge, that is, we only have the task vector of each task without their initial training data. How to optimize merging coefficients {\u03bbk}Kk=1 (or {\u03bblk} K,L k=1,l=1)? Our inspiration to solve this challenge comes from test-time adaptation (Wang et al.,\n2021; Niu et al., 2022; 2023), they adapt the weights of the trained model on unseen test data to cope with the distribution shifts on the test data."
        },
        {
            "heading": "3.2.2 ENTROPY OPTIMIZATION",
            "text": "We use entropy minimization on multi-task unlabeled test samples as an optimization surrogate objective function to update the merging coefficients {\u03bbk}Kk=1 (or {\u03bblk} K,L k=1,l=1) in our AdaMerging.\nEntropy Minimization: For a sample xi, the predicted output of a neural network f\u03b8(xi) is y\u0302i, the corresponding Shannon entropy (Shannon, 1948) is expressed as H(y\u0302i) = \u2212 \u2211C c p (y\u0302i,c) log p (y\u0302i,c), where p(y\u0302i,c) \u2208 [0, 1] represents the probability that the input xi is predicted to be the c-th class. Previous research on test-time adaptation (Wang et al., 2021; Niu et al., 2023) found that optimizing the model\u2019s parameters based on entropy minimization (Grandvalet & Bengio, 2004; Roy et al., 2022), minH(y\u0302i), on test samples can make the model adapt to unseen test data distributions.\nHowever, it is unclear whether entropy minimization can be used as an effective surrogate objective function in multi-task model merging. To verify whether entropy minimization can be used as a proxy objective for MTL loss, we performed the analysis on the eight tasks used in the experiment. First, we combine the test data of the eight tasks as (X,Y ) = {{xi, yi} Ntek i=1}Kk=1, and compute the prediction of the multi-task model f\u03b8MTL on test data as Y\u0302 = {{f\u03b8MTL(xi)} Ntek i=1}Kk=1. Next, we calculate the loss between the real label Y and the predicted value Y\u0302 case-by-case and obtain L(Y, Y\u0302 ) = {\u2113(yi, y\u0302i)}|Y\u0302 |i=1, where |Y\u0302 | represents the total number of test samples for all tasks, and \u2113 represents a loss function, such as cross-entropy. We also calculate the entropy of each sample on the test set and get H(Y\u0302 ) = {H(y\u0302i)}|Y\u0302 |i=1. Finally, we analyze the correlation between entropy H(Y\u0302 ) and prediction loss L(Y, Y\u0302 ) from two aspects. (i) We divide the multi-task samples into multiple intervals based on entropy H(Y\u0302 ) from small to large, such as I = {I1, I2, . . . , I11} = {(0.0, 0.1], (0.1 \u2212 0.2], . . . , (1.0,\u221e)}, and count the average prediction loss of the samples contained in each interval It (t \u2208 {1, 2, . . . , 11}). As shown in Fig. 3(a), we observe that the average loss corresponding to the interval with small entropy is also smaller. (ii) We also directly calculated the Spearman correlation coefficient (Myers & Sirois, 2004) of entropy H(Y\u0302 ) and prediction loss L(Y, Y\u0302 ). As shown in Fig. 3, we observe that the average correlation between the two on multi-task data (i.e., dark purple \u201cALL\u201d) is as high as 0.87. Therefore, we can conclude that entropy minimization (i.e., minH(Y\u0302 )) can serve as an effective surrogate objective for loss minimization (i.e., minL(Y, Y\u0302 )) on MTL. In Fig. 10 of the Appendix, we further verify that this correlation exists across different training stages of model merging.\nOptimization Objective: Based on the above verification, we take entropy minimization as the optimization proxy goal of the model merging coefficient in our AdaMerging/AdaMerging++. For example, the optimization form of the merging coefficient in Task-wise AdaMerging is:\nmin \u03bb1,\u03bb2,...,\u03bbK K\u2211 k=1 \u2211 xi\u2208Bk H(f\u03b8MTL(xi)) , where \u03b8MTL = \u03b8pre + K\u2211 k=1 \u03bbkTk,\nwhere Bk represents a batch of unlabeled test samples sampled in task k. The coefficient {\u03bbk}Kk=1 can be updated iteratively by obtaining the gradient through backpropagation. This is trivial with automatic differentiation tools like Pytorch (Paszke et al., 2017). It should be emphasized that, on the one hand, we do not need all test data to be available. Even if only 0.1% or 1% of unlabeled tests are available, our method can have significant performance improvements. On the other hand, our extra training time is also very cheap. These results are presented in the appendix."
        },
        {
            "heading": "4 EXPERIMENT",
            "text": "In this section, we introduce the experimental setup in Sec. 4.1 and the experimental results in Sec. 4.2. Due to page limitations, some details and results are shown in the Appendix."
        },
        {
            "heading": "4.1 EXPERIMENT SETUP",
            "text": "Datasets and Models: Following Ilharco et al. (2023) and Yadav et al. (2023), we study task vectors based multi-task model merging on eight image classification datasets: SUN397 (Xiao et al., 2016), Cars (Krause et al., 2013), RESISC45 (Cheng et al., 2017), EuroSAT (Helber et al., 2019), SVHN (Yuval, 2011), GTSRB (Stallkamp et al., 2011), MNIST (LeCun, 1998), DTD (Cimpoi et al., 2014). We provide a more detailed description of the dataset in the Appendix A. In the main text, we use the Vit-B/32 and ViT-L/14 architectures in CLIP (Radford et al., 2021) as pre-trained models to conduct experiments. We also report the results on the Vit-B/16 architecture in the Appendix B.\nBaselines and Metric: Our baselines are mainly divided into two categories, one is non-model merging, i.e., Individual and Traditional MTL; and the other is various advanced model merging methods, such as Weight Averaging, Fisher Merging (Matena & Raffel, 2022), RegMean (Jin et al., 2023), Task Arithmetic (Ilharco et al., 2023) and Ties-Merging (Yadav et al., 2023). Baseline details are provided in Appendix A. Among them, Task Arithmetic and Ties-Merging are task vectors based MTL methods, which are also our most important baselines. In addition, our methods include Task-wise AdaMerging, Task-wise AdaMerging++, Layer-wise AdaMerging, and Layer-wise AdaMerging++. Unless otherwise specified, our method uses the Layer-wise version. We report the average accuracy (i.e., Avg Acc) of MTL model on the test set of all tasks as an evaluation metric."
        },
        {
            "heading": "4.2 PERFORMANCE, GENERALIZATION, ROBUSTNESS",
            "text": "In this section, we demonstrate the superiority of our approach over SOTA methods for merging task vectors by evaluating it from three key perspectives: performance, generalization and robustness.\nSignificantly Higher MTL Performance. We verify that the proposed AdaMerging method significantly outperforms existing model merging methods in performance. As shown in Tab. 1 and Tab. 2, we tested the performance of merging ViT-B/32 and ViT-L/14 on eight tasks, respectively. We have the following observations: (i) Individual and Traditional MTL methods achieve the optimal performance, which are 90.5% and 88.9% under ViT-B/32. However, they all rely on initial training data for multiple tasks. Additionally, independent fine-tuning requires storing a model for each task. (ii) Weight Averaging is the simplest model merging solution. Naturally, its performance is also the lowest. Furthermore, Fisher Merging merged models by calculating parameter importance, and RegMean imposed the constraint that the distance between the merged MTL model and a single model is close. Both of them perform better compared to the Weight Averaging. (iii) Advanced task vectors\nbased multi-task merging methods (i.e., Task Arithmetic and Ties-Merging) have achieved good performance. For example, Ties-Merging has achieved the performance in ViT-B/32 and ViT-L/14 by 72.4% and 86.0%, respectively. However, there is still a big gap between this and Traditional MTL (i.e., 88.9% and 93.5%, respectively). (iv) Our Task-wise AdaMerging and Task-wise AdaMerging++ use unsupervised learnable coefficients to merge task vectors in Task Arithmetic and Ties-Merging respectively, bringing 2% and 1.3% performance improvements respectively on ViT-B/32. Thanks to the more fine-grained fusion solution, on ViT-B/32, our Layer-wise AdaMerging and Layer-wise AdaMerging++ bring 11% and 8.7% performance improvements compared to Task Arithmetic and Ties-Merging, while on ViT-L/14, our method brought improvements of 6.3% and 5.0%. Our AdaMerging greatly reduces the gap between model merging and traditional MTL solutions.\nSubstantially Improved Generalization. MTL hopes to transfer the knowledge of old tasks to new tasks and improve the generalization of the MTL model. To this end, we compare the performance of AdaMerging and task vector-based model merging methods (Task Arithmetic and Ties-Merging) on two sets of unseen tasks. In Tab. 3, we merge the task vectors corresponding to six tasks and test on two unseen tasks (i.e. their task vectors are not merged). We observe: (i) On the six seen tasks, AdaMerging and AdaMerging++ are significantly better than Task Arithmetic and Ties-Merging. (ii) More importantly, AdaMerging method maintains this superiority on two unseen tasks. For example, on the two tasks of MNIST and EuroSAT, the average performance of AdaMerging and AdaMerging++ improved by 8.3% and 9.1%, respectively, compared with Task Arithmetic and Ties-Merging. In addition, on the two unseen tasks of RESISC45 and SVHN, the average accuracy improvements of AdaMerging and AdaMerging++ are 4.4% and 5.4%, respectively. These results indicate that our AdaMerging and AdaMerging++ methods generalize better to unseen tasks.\nRobust to Test Data Distribution Shifts. Considering that the model provider only releases the fine-tuned model and does not expose the original training data, the model merger\u2019s test data may differ from the model owner\u2019s training data. we tested whether AdaMerging is still effective when the test data distribution shifts significantly. Following Hendrycks & Dietterich (2019), we created 7 corruption test data, and examples of corrupted images are shown Fig. 5 in Appendix B. The results on ViT-B/32 are shown in Tab. 4. On clean test data, AdaMerging has an 8.2% performance improvement compared to Task Arithmetic. On the corruption test datasets of Motion Blur, Impulse Noise, Gaussian Noise, Pixelate, Spatter, Contrast and JPEG Compression, AdaMerging\u2019s performance is 11.2%, 6.7%, 5.8%, 8.9%, 6.7%, 10.1% and 9.8% higher than Task Arithmetic respectively. These evidences fully demonstrate that our AdaMerging is more robust to test data distribution shifts.\nSummary: Our AdaMerging/AdaMerging++ allows us to adapt to unlabeled test data of task vectors, unlabeled test data of unseen tasks, or unlabeled corruption data in an unsupervised way when training model merging coefficients, thereby optimizing the best suitable model merging coefficients are used to obtain a model with better performance, generalization or robustness."
        },
        {
            "heading": "4.3 ADAMERGING ANALYSIS",
            "text": "Task-wise Coefficients. In Tab. 5, we consistently observe that the merging coefficients of each task vector are inconsistent. When the number of tasks is relatively large, it is obviously undesirable to grid search the coefficients of each task, but our AdaMerging avoids this manual search process.\nLayer-wise Coefficients. Fig. 4 shows the merging coefficients learned by Layer-wise AdaMerging and AdaMerging++ on ViT-B/32 respectively. We observed that: (i) The coefficients learned by each layer of each task vector are different, which shows that the importance of each layer in the model merging process is different. (ii) The coefficients learned by shallow layers are generally smaller than those of deep layers, which indicates that shallow layers rely more on the weights of the pre-trained model rather than the weights provided by task vectors, while the deep layers rely more on the weights provided by the task vectors. This may be since the shallow layer learns general features, which are cross-task, while the deep layer learns task-specific features (Yosinski et al., 2014)."
        },
        {
            "heading": "5 CONCLUSION AND FUTURE WORK",
            "text": "Advanced task arithmetic shows that new models built by merging multiple task vectors into a pretrained model can execute MTL without needing original training data. However, task vector-based MTL methods are very sensitive to the merging coefficient. In this paper, we propose an adaptive model merging scheme (abbreviated as AdaMerging) to solve this problem, which takes entropy minimization as a surrogate objective to automatically learn the merging coefficients for each task vector or layer. Experimental results show that the proposed AdaMerging is superior to the current SOTA model merging methods in multi-task performance, generalization and robustness. In the future, we plan to further explore model merging solutions for different architectures."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "Li Shen is supported by STI 2030\u2014Major Projects (No. 2021ZD0201405). Enneng Yang and Guibing Guo are supported by the National Natural Science Foundation of China under Grant No. 62032013, the Science and technology projects in Liaoning Province (No. 2023JH3/10200005), and the Fundamental Research Funds for the Central Universities under Grant No. N2317002."
        },
        {
            "heading": "A EXPERIMENT SETTINGS",
            "text": "This section provides a detailed dataset description, baseline description, and training details.\nDataset Details. Following Task Arithmetic (Ilharco et al., 2023), Ties-Merging (Yadav et al., 2023), we study multi-task model merging on eight image classification datasets below.\n\u2022 SUN397 (Xiao et al., 2016) is a scene classification dataset, which contains images in 397 classes, with a total of 108,754 images, and each class has at least 100 images. \u2022 Stanford Cars (Cars) (Krause et al., 2013) is a car classification dataset, which contains 196 classes of cars and a total of 16,185 images. Each class in the training set and test set is divided at a ratio of 1:1. \u2022 RESISC45 (Cheng et al., 2017) is a remote sensing image scene classification data set. It contains 45 classes of scenes and a total of 31,500 images, of which there are approximately 700 images in each class. \u2022 EuroSAT (Helber et al., 2019) is a satellite image classification dataset containing 27,000 labeled and geo-referenced images in 10 classes. \u2022 SVHN (Yuval, 2011) is a real-world digital classification data set extracted from house numbers in Google Street View images. There are 10 classes in total. The training set contains 73,257 samples, the test set contains 26,032 samples, and 531,131 additional simple samples can be used as additional training data. \u2022 GTSRB (Stallkamp et al., 2011) is a traffic sign classification dataset, which contains 43 classes of traffic signs with a total sample size of more than 50,000. \u2022 MNIST (LeCun, 1998) is a benchmark dataset for image classification. It contains grayscale images of handwritten digits in 10 classes. The number of images in the training and test sets is 60,000 and 10,000 respectively. The number of images in each class is balanced. \u2022 DTD (Cimpoi et al., 2014) is a texture classification data set, which contains 47 classes, a total of 5,640 images, and each class has approximately 120 images.\nBaseline Details. Our experiments involve the following seven comparison methods and four variations of our method.\n\u2022 Individual means that each task uses an independent fine-tuned model, which has no interference between tasks, but cannot perform multiple tasks simultaneously. \u2022 Traditional MTL collects the original training data of all tasks together to train a multi-task model. It can be used as a reference upper bound for model merging work. \u2022 Weight Averaging is the simplest method of model merging, which directly averages the parameters of multiple models. It can be used as a lower bound for model merging. \u2022 Fisher Merging (Matena & Raffel, 2022) calculates the Fisher information matrix (Fisher, 1922) to measure the importance of each parameter when merging models, and model merging is performed according to the guidance of this importance. \u2022 RegMean (Jin et al., 2023) imposes a constraint when merging models, that is, the L2 distance between the merged model and a single model is required to be as small as possible. \u2022 Task Arithmetic (Ilharco et al., 2023) first defines the concept of \u201ctask vectors\u201d and merges task vectors into a pre-trained model to execute multi-task learning. \u2022 Ties-Merging (Yadav et al., 2023) further solves the task conflict problem in Task Arithmetic (Ilharco et al., 2023). It eliminates redundant parameters and resolves symbol conflicts through three steps: Trim, Elect Sign, and Disjoint Merge. \u2022 Task-wise AdaMerging (Ours) is based on Task Arithmetic (Ilharco et al., 2023), which uses an unsupervised method to automatically learn the merging coefficient of the task vector in Task Arithmetic. \u2022 Task-wise AdaMergign++ (Ours) is based on Ties-Merging (Yadav et al., 2023), which uses an unsupervised approach to learn a merging coefficient for each task vector in TiesMerging. \u2022 Layer-wise AdaMerging (Ours) automatically learns a merging coefficient for each layer of each task vector in Task Arithmetic (Ilharco et al., 2023). \u2022 Layer-wise AdaMergign++ (Ours) uses an unsupervised approach to learn a merging coefficient for each layer of each task vector in Ties-Merging (Yadav et al., 2023).\nImplementation Details. For the seven baseline methods, we follow the experimental settings in Task Arithmetic (Ilharco et al., 2023) and Ties-Merging (Yadav et al., 2023). In our experiments,\nthe merging coefficient \u03bb of Task Arithmetic and Ties-Merging is set to 0.3 by default. For our four variants, we initialize all coefficients {\u03bbk}Kk=1 (or {\u03bblk} K,L k=1,l=1) to 0.3 by default before learning and then update them unsupervised. We use an Adam optimizer (Kingma & Ba, 2014) to update the merging coefficients, with the learning rate set to 0.001, the momentum to (0.9, 0.999), and the batch size to 16. To avoid significantly increasing training costs, we only trained 500 iterations to update the merging coefficient. Pre-trained models ViT-B/32, ViT-B/16 and ViT-L/14 from CLIP (Radford et al., 2021) like Task Arithmetic (Ilharco et al., 2023) and Ties-Merging (Yadav et al., 2023)."
        },
        {
            "heading": "B EXPERIMENT RESULTS",
            "text": "B.1 PERFORMANCE, GENERALIZATION AND ROBUSTNESS\nPerformance. Tab. 6 shows the average accuracy of merging ViT-B/16 on eight tasks. We can observe that: (i) Ties-Merging alleviates the conflict problem of task vectors in Task Arithmetic, thus achieving a 3.2% performance improvement compared to Task Arithmetic. (ii) Our Task-wise AdaMerging and AdaMerging++ automatically learn a merging coefficient for each task vector in Task Arithmetic and Ties-Merging, thus bringing about 2.2% and 1.0% performance improvements, respectively. (iii) Our Layer-wise AdaMerging and AdaMerging++ further adaptively learn a merging coefficient for each layer of each task vector in Task Arithmetic and Ties-Merging, ultimately achieving performance improvements of 11.1% and 8.7%. These results further demonstrate the effectiveness of our AdaMerging scheme in multi-task model merging.\nGeneralization. As shown in Tab. 7, we demonstrate the generalization of Layer-wise AdaMerging under the ViT-B/16 architecture. In the two unseen test tasks of EuroSAT and MNIST (their corresponding task vectors are not merged), our AdaMerging improved the average accuracy by 2.3% compared to Task Arithmetic. On two unseen tasks, RESISC45 and SVHN, the average accuracy increased by 1.1%. This shows that AdaMerging has better generalization properties.\nRobustness. Tab. 8 shows the robustness test of AdaMerging and Task Arithmetic based on ViT-B/16 on seven corruption test datasets. Fig. 5 shows an example of corruption. We can observe that in the test datasets Motion Blur, Impulse Noise, Gaussian Noise, Pixelate, Spatter, Contrast and JPEG Compression where the distribution drifts, the average accuracy of AdaMerging is 9.9%, 8.2%, 7.8%, 6.8%, 12.4%, 9.5% and 9.7% higher than that of Task Arithmetic, respectively. This shows that our AdaMerging is more robust to test data distribution shifts than Task Arithmetic.\nB.2 ANALYSIS EXPERIMENT\nTask Relationship Analysis. As shown in Fig. 6(a) and (b), we show the correlation between pairs of task vectors in ViT-B/32 and ViT-L/14, respectively. We observe a phenomenon consistent with Task Arithmetic (Ilharco et al., 2023), that is, these task vectors are almost orthogonal to each other.\nIn particular, there are very few task vectors with high similarity between them, such as SVHN and MNIST, because they are both handwritten digit recognition tasks. The orthogonality of task vectors provides good initial conditions for model merging, indicating that they have the potential to be combined into a single model, and our results show that this is indeed the case. Further, we merge four groups of task vectors with different correlation degrees, namely (SVHN, MNIST), (SVHN, GTSRB), (SVHN, SUN397), and (SVHN, EuroSAT). The results are shown in Fig. 7. We observe that under task vectors with different degrees of correlation, our AdaMerging technique is always effective because it aims to adaptively learn optimal merging coefficients.\nImpact of the Amount of Available Test Data on Performance. The AdaMerging proposed in this paper requires an unlabeled test dataset to perform entropy minimization optimization. Having all test data available may be unrealistic in some scenarios. In this section, we verify the performance changes of AdaMerging when different amounts (e.g., 0.1%, 1%, 5%, 100%) of test data are available. As shown in Fig. 8 and Tab. 9, we observed that even when only 0.1% of unlabeled test data are available, our AdaMerging and AdaMerging++ still have a performance improvement of 4.9% and\n5.5%, respectively, compared to Task Arithmetic and Ties-Merging. In addition, when 5% of the data are available, it can almost achieve a performance comparable to 100% of the data. This shows that our AdaMerging is valuable and can bring significant performance improvements even with a small amount of data.\nSupervised AdaMerging Analysis. This paper uses unsupervised entropy minimization as a proxy objective for supervised cross-entropy loss to optimize model merging coefficients. Therefore, AdaMerging trained with supervised cross-entropy loss should be an upper bound on our unsupervised AdaMerging. As shown in Fig. 9 and Tab. 10, we observe that the performance of our unsupervised AdaMerging version is very close to that of the supervised AdaMerging version. For example, the Avg Acc of supervised Task-wise AdaMerging is 71.3%, while the Avg Acc of our unsupervised Task-wise AdaMerging is 71.1%. This also further verifies that it is reasonable for us to use entropy minimization as a proxy for cross-entropy loss in merging coefficients learning.\nParameter Cost Analysis. As shown in Tab. 11, our AdaMerging introduces very few coefficients that need to be updated. The total number of parameters of the eight task vectors is 907,589,640, and our Task-wise AdaMerging only added 8 parameters, and Layer-wise AdaMerging added 1,248 parameters.\nTime Cost Analysis. As shown in Tab. 12, we show the performance that AdaMerging can achieve under different training costs (based on a single GeForce RTX 3090). We observed that our AdaMerging brought about a 2% performance improvement when it took 7.5 minutes longer than Task Arithmetic. When training for 50 minutes, AdaMerging brought an 8% performance improvement. This shows that AdaMering is very cost-effective and can bring significant performance improvements with only a small amount of training time.\nMerging Coefficients Visualization. Fig. 11 shows the changes during the iteration process of merging coefficient optimization of each task vector in Task-wise AdaMerging and AdaMerging++, which is shown every ten steps. In addition, Fig. 12 and Fig. 13 show the merging coefficients of eight task vectors learned by Layer-wise AdaMerging and AdaMerging++ on ViT-B/16 respectively. Finally, Fig. 14 and Fig. 15 show the coefficients learned under ViT-L/14. We can clearly observe that in different layers of different task vectors, the learned merging coefficients are different. Finding the merging coefficients of so many layers through grid search is almost impossible.\nVisualization of Spearman\u2019s Correlation Coefficient Between Entropy and Loss. As shown in Fig. 10, we show the correlation changes of unsupervised entropy minimization and supervised cross-entropy loss at different training stages (i.e., the number of iterations are {0, 100, 200, 300, 400, 500} respectively). We observe that in the merging coefficients learning process of AdaMerging, entropy minimization and cross-entropy loss always have a high correlation. Therefore, entropy minimization can be used as a surrogate objective to optimize model merging coefficients.\nVisualization of Correlation between Entropy and Loss. As shown in Fig. 16, we analyze the correlation between the entropy and the model\u2019s prediction loss for eight tasks (or datasets) on the initial merged model. As described in Sec. 3.2.2, in each dataset, we sort the entropy on the test samples from small to large into eleven groups and observe the average loss of sample prediction within each group. We observe that groups with smaller entropy generally have smaller average losses. Therefore, it is reasonable to take Shannon entropy minimization as an unsupervised optimization surrogate objective for loss (e.g., cross-entropy) minimization.\n0 100 200 300 400 500 Iteration\n0.5\n0.6\n0.7\n0.8\n0.9\nSp ea\nrm an\nC or\nre la\ntio n\nC oe\nffi ci\nen t\n0.92 0.94 0.94 0.94 0.94 0.93\nSVHN\n0 100 200 300 400 500 Iteration\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nSp ea\nrm an\nC or\nre la\ntio n\nC oe\nffi ci\nen t\n0.92 0.95\n0.98 0.99 0.99 0.99\nGTSRB\n0 100 200 300 400 500 Iteration\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nSp ea\nrm an\nC or\nre la\ntio n\nC oe\nffi ci\nen t\n1.00 1.00 1.00 1.00 1.00 1.00\nMNIST\nFigure 10: Spearman correlation coefficient between entropy H(Y\u0302 ) and avareage loss L(Y, Y\u0302 ) on eight tasks (or datasets) at different stages of training(e.g., Iteration={0, 100, 200, 300, 400, 500}), and we observed a high positive correlation.\n0 10 20 30 40 50 iteration(\u00d710)\n0.0\n0.2\n0.4\n0.6\nk\nSUN397 Cars\nRESISC45 EuroSAT\nSVHN GTSRB\nMNIST DTD\n0 10 20 30 40 50 iteration(\u00d710)\n0.0\n0.2\n0.4\n0.6\n0.8\nk\nSUN397 Cars\nRESISC45 EuroSAT\nSVHN GTSRB\nMNIST DTD\nFigure 11: Model merging coefficients {\u03bbk}Kk=1 change with respect to training steps on ViT-B/32: (a) Task-wise AdaMerging; (b) Task-wise AdaMerging++. Each line represents the change process of the coefficient \u03bbk of a task vector Tk (k \u2208 {1, 2, . . . ,K})."
        }
    ],
    "title": "ADAMERGING: ADAPTIVE MODEL MERGING FOR MULTI-TASK LEARNING",
    "year": 2024
}