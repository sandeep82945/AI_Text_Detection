{
    "abstractText": "Recent studies have highlighted the potential of Lipschitz-based methods for training certifiably robust neural networks against adversarial attacks. A key challenge, supported both theoretically and empirically, is that robustness demands greater network capacity and more data than standard training. However, effectively adding capacity under stringent Lipschitz constraints has proven more difficult than it may seem, evident by the fact that state-of-the-art approach tend more towards underfitting than overfitting. Moreover, we posit that a lack of careful exploration of the design space for Lipshitz-based approaches has left potential performance gains on the table. In this work, we provide a more comprehensive evaluation to better uncover the potential of Lipschitz-based certification methods. Using a combination of novel techniques, design optimizations, and synthesis of prior work, we are able to significantly improve the state-of-the-art VRA for deterministic certification on a variety of benchmark datasets, and over a range of perturbation sizes. Of particular note, we discover that the addition of large \u201cCholesky-orthogonalized residual dense\u201d layers to the end of existing state-of-the-art Lipschitz-controlled ResNet architectures is especially effective for increasing network capacity and performance. Combined with filtered generative data augmentation, our final results further the state of the art deterministic VRA by up to 8.5 percentage points.",
    "authors": [],
    "id": "SP:d0b522770d9d79b325c8b3187faa36b7efe5ba22",
    "references": [
        {
            "authors": [
                "Cem Anil",
                "James Lucas",
                "Roger Gross"
            ],
            "title": "Sorting out Lipschitz function approximation",
            "venue": "In International Conference on Machine Learning (ICML),",
            "year": 2019
        },
        {
            "authors": [
                "Alexandre Araujo",
                "Aaron J Havens",
                "Blaise Delattre",
                "Alexandre Allauzen",
                "Bin Hu"
            ],
            "title": "A unified algebraic perspective on lipschitz neural networks",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Shekoofeh Azizi",
                "Simon Kornblith",
                "Chitwan Saharia",
                "Mohammad Norouzi",
                "David J Fleet"
            ],
            "title": "Synthetic data from diffusion models improves imagenet classification",
            "venue": "arXiv preprint arXiv:2304.08466,",
            "year": 2023
        },
        {
            "authors": [
                "Sebastien Bubeck",
                "Mark Sellke"
            ],
            "title": "A universal law of robustness via isoperimetry",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Nicholas Carlini",
                "Florian Tramer",
                "J Zico Kolter"
            ],
            "title": "certified!!) adversarial robustness for free",
            "venue": "arXiv preprint arXiv:2206.10550,",
            "year": 2022
        },
        {
            "authors": [
                "Jeremy Cohen",
                "Elan Rosenfeld",
                "Zico Kolter"
            ],
            "title": "Certified adversarial robustness via randomized smoothing",
            "venue": "In International Conference on Machine Learning (ICML),",
            "year": 2019
        },
        {
            "authors": [
                "Jeremy Cohen",
                "Elan Rosenfeld",
                "Zico Kolter"
            ],
            "title": "Certified adversarial robustness via randomized smoothing",
            "venue": "In international conference on machine learning,",
            "year": 2019
        },
        {
            "authors": [
                "Francesco Croce",
                "Matthias Hein"
            ],
            "title": "Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks",
            "venue": "In International Conference on Machine Learning (ICML),",
            "year": 2020
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "Lucas Beyer",
                "Alexander Kolesnikov",
                "Dirk Weissenborn",
                "Xiaohua Zhai",
                "Thomas Unterthiner",
                "Mostafa Dehghani",
                "Matthias Minderer",
                "Georg Heigold",
                "Sylvain Gelly"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
            "venue": "arXiv preprint arXiv:2010.11929,",
            "year": 2020
        },
        {
            "authors": [
                "Kai Hu",
                "Andy Zou",
                "Zifan Wang",
                "Klas Leino",
                "Matt Fredrikson"
            ],
            "title": "Scaling in depth: Unlocking robustness certification on imagenet",
            "venue": "arXiv preprint arXiv:2301.12549,",
            "year": 2023
        },
        {
            "authors": [
                "Yujia Huang",
                "Huan Zhang",
                "Yuanyuan Shi",
                "J. Zico Kolter",
                "Anima Anandkumar"
            ],
            "title": "Training certifiably robust neural networks with efficient local lipschitz bounds",
            "venue": "In NIPS,",
            "year": 2021
        },
        {
            "authors": [
                "Jongheon Jeong",
                "Jinwoo Shin"
            ],
            "title": "Consistency regularization for certified robustness of smoothed classifiers",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Jongheon Jeong",
                "Sejun Park",
                "Minkyu Kim",
                "Heung-Chang Lee",
                "Do-Guk Kim",
                "Jinwoo Shin"
            ],
            "title": "Smoothmix: Training confidence-calibrated smoothed classifiers for certified robustness",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Tero Karras",
                "Miika Aittala",
                "Timo Aila",
                "Samuli Laine"
            ],
            "title": "Elucidating the design space of diffusion-based generative models",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Klas Leino"
            ],
            "title": "Identifying, analyzing, and addressing weaknesses in deep networks: Foundations for conceptually sound neural networks, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Klas Leino"
            ],
            "title": "Limitations of piecewise linearity for efficient robustness certification",
            "venue": "arXiv preprint arXiv:2301.08842,",
            "year": 2023
        },
        {
            "authors": [
                "Klas Leino",
                "Zifan Wang",
                "Matt Fredrikson"
            ],
            "title": "Globally-robust neural networks",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Dhruv Mahajan",
                "Ross Girshick",
                "Vignesh Ramanathan",
                "Kaiming He",
                "Manohar Paluri",
                "Yixuan Li",
                "Ashwin Bharambe",
                "Laurens Van Der Maaten"
            ],
            "title": "Exploring the limits of weakly supervised pretraining",
            "venue": "In Proceedings of the European conference on computer vision (ECCV),",
            "year": 2018
        },
        {
            "authors": [
                "Laurent Meunier",
                "Blaise J Delattre",
                "Alexandre Araujo",
                "Alexandre Allauzen"
            ],
            "title": "A dynamical system perspective for Lipschitz neural networks",
            "venue": "Proceedings of the 39th International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Alexander Quinn Nichol",
                "Prafulla Dhariwal"
            ],
            "title": "Improved denoising diffusion probabilistic models",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Bernd Prach",
                "Christoph H Lampert"
            ],
            "title": "Almost-orthogonal layers for efficient general-purpose lipschitz networks",
            "venue": "In European Conference on Computer Vision,",
            "year": 2022
        },
        {
            "authors": [
                "Hadi Salman",
                "Jerry Li",
                "Ilya Razenshteyn",
                "Pengchuan Zhang",
                "Huan Zhang",
                "Sebastien Bubeck",
                "Greg Yang"
            ],
            "title": "Provably robust deep learning via adversarially trained smoothed classifiers",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Hadi Salman",
                "Mingjie Sun",
                "Greg Yang",
                "Ashish Kapoor",
                "J Zico Kolter"
            ],
            "title": "Denoised smoothing: A provable defense for pretrained classifiers",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Sahil Singla",
                "Soheil Feizi"
            ],
            "title": "Skew orthogonal convolutions",
            "venue": "Proceedings of the 38th International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Christian Szegedy",
                "Wojciech Zaremba",
                "Ilya Sutskever",
                "Joan Bruna",
                "Dumitru Erhan",
                "Ian J. Goodfellow",
                "Rob Fergus"
            ],
            "title": "Intriguing properties of neural networks",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2014
        },
        {
            "authors": [
                "Hugo Touvron",
                "Piotr Bojanowski",
                "Mathilde Caron",
                "Matthieu Cord",
                "Alaaeldin El-Nouby",
                "Edouard Grave",
                "Gautier Izacard",
                "Armand Joulin",
                "Gabriel Synnaeve",
                "Jakob Verbeek"
            ],
            "title": "Resmlp: Feedforward networks for image classification with data-efficient training",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "Asher Trockman",
                "J Zico Kolter"
            ],
            "title": "Orthogonalizing convolutional layers with the cayley transform",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2021
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Ruigang Wang",
                "Ian Manchester"
            ],
            "title": "Direct parameterization of lipschitz-bounded deep networks",
            "venue": "In International Conference on Machine Learning,",
            "year": 2023
        },
        {
            "authors": [
                "Xiaojun Xu",
                "Linyi Li",
                "Bo Li"
            ],
            "title": "Lot: Layer-wise orthogonal training on improving l2 certified robustness",
            "venue": "arXiv preprint arXiv:2210.11620,",
            "year": 2022
        },
        {
            "authors": [
                "Zhuolin Yang",
                "Linyi Li",
                "Xiaojun Xu",
                "Bhavya Kailkhura",
                "Tao Xie",
                "Bo Li"
            ],
            "title": "On the certified robustness for ensemble models and beyond",
            "venue": "arXiv preprint arXiv:2107.10873,",
            "year": 2021
        },
        {
            "authors": [
                "Tan Yu",
                "Xu Li",
                "Yunfeng Cai",
                "Mingming Sun",
                "Ping Li"
            ],
            "title": "S2-mlp: Spatial-shift mlp architecture for vision",
            "venue": "In Proceedings of the IEEE/CVF winter conference on applications of computer vision,",
            "year": 2022
        },
        {
            "authors": [
                "Runtian Zhai",
                "Chen Dan",
                "Di He",
                "Huan Zhang",
                "Boqing Gong",
                "Pradeep Ravikumar",
                "Cho-Jui Hsieh",
                "Liwei Wang"
            ],
            "title": "Macer: Attack-free and scalable robust training via maximizing certified radius",
            "year": 2001
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Intentionally crafted perturbations (adversarial examples) have the potential to alter the predictions made by neural networks (Szegedy et al., 2014). Many methods have been proposed to improve the robustness of deep networks, either empirically or provably. In safety-critical domains especially, guarantees against adversarial examples are indispensable. Commonly, provable defenses provide certificates of local robustness to accompany a model\u2019s prediction; i.e., predictions should be guaranteed to be consistent within an \u2113p-norm-bounded \u03f5-ball around the input. The success of robustness certification techniques is measured by the verified robust accuracy (VRA)\u2014the fraction of points with correct predictions that are proven to be \u03f5-locally robust.\nTo date, a look at public robustness certification leaderboards1 shows that the best results are achieved by variants of Randomized Smoothing (RS) (Carlini et al., 2022; Cohen et al., 2019b; Jeong et al., 2021; Salman et al., 2019; Yang et al., 2021). However, there are two primary limitations associated with RS. To begin with, RS only offers a probabilistic guarantee, typically configured to have a 0.1% false positive certification rate. Perhaps more importantly, the inference of RS involves substantial computational overhead\u2014this limitation is significant enough that these methods are typically tested on only a 1% subset of the ImageNet validation dataset due to timing constraints.\nAnother successful family of methods perform certification using Lipschitz bounds (Araujo et al., 2023; Hu et al., 2023; Leino et al., 2021; Trockman & Kolter, 2021; Wang & Manchester, 2023). Essentially, the Lipschitz constant of the neural network provides a bound on the maximum change in output for a given input perturbation, making it possible to certify local robustness. Compared with RS-based methods, Lipschitz-based methods can provide deterministic certification, and are efficient enough to perform robustness certification at scale, e.g., on the full ImageNet (Hu et al., 2023). While Lipschitz-based methods are promising in terms of both deterministic certification and efficiency, there is a noticeable performance gap between these methods and RS-based methods. It is not established, however, that this\n1https://sokcertifiedrobustness.github.io/leaderboard (accessed Sept. 2023)\ndiscrepancy is tied to a fundamental limitation of deterministic certification. In this work, we aim to narrow the gap between Lipschitz-based and RS-based methods.\nOne important avenue for improving the performance of Lipschitz-based certification is through increasing model capacity (ability to fit data). Bubeck & Sellke (2021) have shown that robust classification requires more capacity than is necessary for standard learning objectives, and Leino (2023) has shown more specifically that further capacity is required for tight Lipschitz-based certification. But while increasing model capacity for standard training is trivial\u2014adding more blocks/layers, increasing the network width and using self-attention mechanisms are all possible approaches\u2014in Lipschitz-based certified training, the picture is more nuanced because the network\u2019s Lipschitz constant is tightly controlled, limiting the function\u2019s expressiveness. Thus, even models with many parameters may still underfit the training objective.\nIn addition, we find that an apparent limitation preventing prior work from discovering the full potential of Lipschitz-based certification stems from the framing and evaluation setup. Specifically, most prior work is framed around a particular novel technique intended to supersede the state-of-the-art, necessitating evaluations centered on standardized benchmark hyperparameter design spaces, rather than exploring more general methods for improving performance (e.g., architecture choice, data pipeline, etc.). Although we introduce several of our own innovations, we present this work as more of a \u201cmaster class\u201d on optimizing Lipschitz-based robustness certification that draws from and synthesizes many techniques from prior work to achieve the best overall performance. This angle lets us explore design choices meant to be synergistic to the overall Lipschitz-based approach, rather than restricting us to choices tailored for head-to-head comparisons.\nThis work provides a more comprehensive evaluation to illuminate the potential of Lipschitz-based certification methods. First and foremost, we find that by delving more thoroughly into the design space of Lipschitz-based approaches, we can improve the state-of-the-art VRA for deterministic certification significantly on a variety of benchmark datasets, and over a range of perturbation sizes. In the process, we propose a number of additional techniques not already used by the prior literature that contribute to these large performance improvements. That is, our results are achieved using a combination of design optimization, novel techniques, and synthesis of prior work.\nAfter covering the relevant background in Section 2, we begin in Section 3 with a brief survey of the design space for Lipschitz-based certified training, focusing on three key components: (1) architecture choice, (2) methods for controlling the Lipschitz constant, and (3) data augmentation. First, we cover the various architecture innovations and building blocks that have been used in the prior literature. Based on an analysis of the challenges faced by existing work, and motivated by the goal of efficiently increasing network capacity, we propose additional directions to explore along the architecture axis, including two novel network building blocks. Next, we provide an overview of the existing methods used for controlling the Lipschitz constant during training, and propose one of our own that can be combined with other approaches. Third we discuss the role data augmentation plays in training high-capacity models. Specifically, we cover DDPM (Karras et al., 2022), which prior work has found helpful for certified training, and propose an alteration to the typical augmentation strategy that we find further boosts performance. Section 4 provides an in-depth evaluation that explores along the three dimensions identified in Section 3, shedding light on the most promising design choices, and demonstrating the significant performance improvements we achieve in this work. Finally, Section 5 concludes the paper."
        },
        {
            "heading": "2 BACKGROUND",
            "text": "The problem of providing provable guarantees against adversarial examples is typically formalized using local robustness, which guards against the specific class of small-norm adversarial examples. A classification model, F(x)=argmaxif(x)i, is \u03f5-locally robust at point x if \u2200x\u2032 . ||x\u2212x\u2032||p\u2264\u03f5=\u21d2 F(x)=F(x\u2032). Given an upper bound, K, of the Lipschitz constant of f , which is an upper bound of ||f(x)\u2212f(x\u2032)/||x\u2212x\u2032||, we can bound the maximum impact of an \u2113p-norm-bounded perturbation. Namely, if j =F(x) is the network\u2019s prediction at x, F is locally robust at x if \u2200i=\u0338j . f(x)j\u2212 \u221a 2K\u03f5>f(x)i. Note that a tighter variant of this certification procedure has been proposed in the literature, e.g., by Leino et al. (2021). Of course, this certification procedure may be very loose, both because it may be hard to obtain a tight bound on the Lipschitz constant, and because the Lipschitz constant provides a only worst-case analysis of the model\u2019s local behavior at x. The key to Lipschitz-based certification is that both of these issues are mitigated by the fact that the Lipschitz constant is tightly controlled (either by hard constraints or by regularization), and the certification procedure is incorporated into the model\u2019s loss function.\nLipschitz-based certification methods in the literature have a few common elements. First, all use essentially the same aforementioned certification procedure. Second, the Lipschitz bound for the entire network is bounded using a product of the layer-wise Lipschitz constants. The key variation between methods is how they perform what we will call Lipschitz control, which ensures that (1) the Lipschitz bound does not explode, and (2) that the learned function can be (reasonably) tightly certified using the procedure above. We characterize these variations further in Section 3.2."
        },
        {
            "heading": "3 DESIGN SPACE",
            "text": "We now turn to brief survey and analysis of the design space for Lipschitz-based certified training. We focus on three primary axes of the design space: (1) architecture choice, (2) methods for controlling the Lipschitz constant, and (3) data augmentation, covered in sections 3.1\u20133. We include a discussion of what prior work has done in each axis, as well as our analysis and proposals for further exploration."
        },
        {
            "heading": "3.1 ARCHITECTURES",
            "text": "Lipschitz-based certification has typically made use of a small set of architecture building blocks that are compatible with the overall approach described in Section 2. This includes 1-Lipschitz activation functions, dense layers, convolutional layers, and residual layers (with a few variations). Modules such as pooling, batch normalization, and attention are not frequently used, either because they lead to loose Lipschitz bounds, or because they are not Lipschitz at all. While ReLU activations are 1-Lipschitz, in the context of Lipschitz-based certification, they have been ubiquitously replaced by MinMax activations, whose gradient norm preservation property has been shown to be invaluable for this setting by Anil et al. (2019).\nUnfortunately, while the space of architectures may be important to explore for maximizing deterministic VRA, prior work has had relatively little exploration here, often using benchmark architectures first proposed half a decade ago. On the other hand, new methods for performing Lipschitz control that present results on larger architectures may come across as misleading, as it becomes unclear if the performance benefits come from the added capacity or the Lipschitz control method. In this work we resolve this by exploring these axes more independently.\nWe begin with the LiResNet (Hu et al., 2023) architecture as a reference point because it performs best on CIFAR10/100 and Tiny-ImageNet datasets. The LiResNet architecture is composed of 4 parts: (1) the stem layer, a single convolution layer, to convert images in to feature maps; (2) the backbone, a stack of several residual convolutional blocks, to extract features from the feature maps; (3) the neck, 1\u223c 2 layers to convert the feature map into a flattened vector; and (4) the classification head for predictions. Prior to recent innovations that made residual blocks effective for Lipschitz-based certification, deep architectures were not practical. However, with the LiResNet architecture, Hu et al. were able to increase the model capacity by increasing the number of blocks L and the number of channels used by the block D. Unfortunately, they report diminishing returns beyond a dozen or so blocks (and D\u223c512), at which point the network capacity is not even enough to overfit the training dataset of CIFAR-10.\nWe posit that stacking the same block is less effective for adding capacity in Lipschitz-based training, where the network Lipschitz is tightly controlled. Specifically, since the Lipschitz constant is bounded by the product of all blocks\u2019 Lipschitz bound, we hypothesize that any looseness in the layer-wise bounds compounds, causing overly-deep models to become over-regularized, effectively destroying its capacity. We therefore propose exploration of additional architecture features that can more effectively add capacity beyond the baseline LiResNet architecture.\nAttention-like Mechanisms. Attention mechanisms (Dosovitskiy et al., 2020; Vaswani et al., 2017) have shown excellent ability to improve model capabilities in standard training. However, attention cannot be directly applied to Lipschitz based training since it does not have a Lipschitz bound. One alternative could be Spatial-MLP (Touvron et al., 2022; Yu et al., 2022). Convolution layers extract local features while the Spatial-MLP can extract non-local features. Combination of the two different operations may allow richer features. Let X \u2208RC\u00d7S\u00d7S denotes the feature map with channels C, height and width S and W \u2208RS2\u00d7S2denotes the weights. The formulation of a Spatial-MLP block is (bias ignored):\nX[c,h,w]=X[c,h,w]+ S\u2211 p=1 S\u2211 q=1 W [hS+w,pS+q]X[c,p,q]. (1)\nThe lipschitz constant of this operation is \u2225I+W\u22252. We also consider using a group of Spatial-MLPs with more parameters to increase model capacity. Suppose we use G groups (The number of channels C should be divisible by G), we would have Wi\u2208RS\n2\u00d7S2,1\u2264 i\u2264G as the weights. The formulation of a group Res-MLP block is ( k is the integer in [cG/C,cG/C+1)):\nX[c,h,w]=X[c,h,w]+ S\u2211 p=1 S\u2211 q=1 Wk[hS+w,pS+q]X[c,p,q]. (2)\nThe lipschitz constant of this operation is maxi(\u2225I+Wi\u22252).\nDense Layers. Another solution is to add large fully connected (i.e., dense) layers after the neck. Early deep architectures like VGG employ this practice and recent work in Lipschitz based training also gets mileage from many large dense layers (Araujo et al., 2023).\nWe also propose a variation on standard dense layers inspired by the LiResNet block of Hu et al., which adds residual connections to a single convolutional by modifying the layer as f(x) = x+ conv(x). Analogously for a dense layer with weight matrix W , we can add residual connections to form what we call a residual dense layer as f(x)=(W+I)x."
        },
        {
            "heading": "3.2 LIPSCHITZ CONTROL",
            "text": "Lipschitz-based certification requires the network to have a low Lipschitz constant since an upper bound on the Lipschitz constant is used to approximate output changes from input perturbations, and if it is too large, certification becomes difficult. There are two primary categories of Lipschitz control used in the literature: (1) Lipschitz regularization and (2) Lipschitz constraints.\nThe prevailing Lipschitz regularization approach is GloRo training proposed by Leino et al. (2021). In this approach, the layer-wise Lipschitz constant are computed as part of the forward pass and used to incorporate Lipschitz-based certification into the training objective. Thus the gradient provides feedback to keep the Lipschitz constant under control and optimized for certification. GloRo regularization is used by Hu et al. (2023), who achieve the current state-of-the-art VRA.\nA wide variety of Lipschitz constraint approaches exist, typically using special re-parameterizations that each linear layer\u2019s weights to be orthogonal (the Lipschitz constant of an orthogonal transformation is 1). We consider several of these approaches in our design space, described below.\nCayley transformation. (Trockman & Kolter, 2021) For skew-symmetric matrix V , W=(I+V )\u22121(I\u2212V ) is orthogonal, thus f(x;V )=Wx is 1-Lipschitz. Matrix exponential. (Singla & Feizi, 2021) For skew-symmetric matrix V , W=exp(V ) is orthogonal, thus f(x;V )=Wx is 1-Lipschitz.\nLayer-wise Orthogonal Training Layer. (Xu et al., 2022) For non-singular matrix V , (V V \u22a4)\u221212V is orthogonal. To obtain a differentiable inverse square root, Newton\u2019s iteration steps are performed.\nAlmost Orthogonal Layer. Prach & Lampert (2022) shows f(x;V ) = V diag( \u2211\nj|V \u22a4V |ij)\u22121x is 1-Lipschitz.\nSDP-based Lipschitz Layer. Araujo et al. (2023) shows h(x;W,q)=x\u22122Wdiag( \u2211 j |W\u22a4W |ij qj qi )\u22121\u03c3(Wx)\nis 1-Lipschitz with 1-Lipschitz activation \u03c3(\u00b7). Sandwich Layer. Wang & Manchester (2023) shows\nh(x;A,B,\u03a6)= \u221a 2A\u22a4\u03a8\u03c3(\u03a8\u22121Bx)\nis 1-Lipschitz with 1-Lipschitz activation \u03c3(\u00b7) if \u22252A\u22a4B\u2225 \u2264 1 holds. The condition is obtained by construct a long orthogonal matrix using Cayley transformation.\nIn addition to the above Lipschitz constrained layers from prior work, we propose an approach to orthogonalize weights using Cholesky decomposition. Suppose \u03a3 is a symmetric positive definite matrix,\nthere exists an unique lower triangular matrix L=Cholesky(\u03a3) such that LL\u22a4=\u03a3. Then for non-singular matrix V , SolveTriangularSystem ( Cholesky(V V \u22a4),V ) is orthogonal. The motivation of this Cholesky-base orthogonalization comes from Gram\u2013Schmidt process to obtain an orthogonal matrix. SolveTriangularSystem ( Cholesky(V V \u22a4),V ) is the same as the Gram\u2013Schmidt process result of V . Cholesky-base orthogonalization is more numerically stable and efficient. Cholesky-based orthogonalization is typically twice as fast as Cayley transformation to obtain an orthogonal matrix. We propose the following 1-Lipschitz layer:\nCholesky-Orthogonalized Residual Layer Let V \u2208Rn\u00d7n be the parameter, and W is the Cholesky-base orthogonalization result of I+V where I is the identity matrix. The layer is formulated as f(x;V )=Wx.\nWith the residual formula (analogous to the residual dense layer proposed in Section 3.1), the training of the model can be more effective in the case of stacking multiple such layers.\nAlthough some studies may find certain approaches can approximate certain functions more smoothly, there is no direct theory showing one method has general advantages over others for Lipschitz control. Thus we conduct a fair comparison of all above approaches to find an optimal method empirically. We note that it is also possible to combine various methods of Lipschitz control. Although we do not try all combinations, in our experiments, we use GloRo regularization for convolutional layers while combining different Lipschitz control techniques for the dense layers. See Section 4.2 for details."
        },
        {
            "heading": "3.3 DATA AUGMENTATION WITH GENERATED MODELS",
            "text": "Prior work (Hu et al., 2023) uses IDDPM (Nichol & Dhariwal, 2021) (obtain a FID of 3.27 for CIFAR-10) to generate samples. We would like to know if the performance of certificated robustness can be improved if using generative samples of better quality. We use the elucidating diffusion model (EDM) (Karras et al., 2022) to generate new samples, which obtain a FID of 1.79 for CIFAR-10. For each dataset (CIFAR10, CIFAR100 and Tiny-ImageNet), we train the diffusion models on the corresponding training set using the setting recommended by EDM. Unless otherwise specified, the diffusion models are class-conditional thus the generated images have pseudo-labels.\nWe also train a standard (non-robust) classification model on each dataset. We use the ResNeXt101 (32x48d) model weakly supervised pre-trained on 940 million (224\u00d7224) images (Mahajan et al., 2018). We freeze the backbone and only fine-tune the last classification layer with the training dataset. This model archives 94%, 86% and 82% test accuracy on CIFAR-10, CIFAR-100, and Tiny-ImageNet respectively. We use this classification model\u2019s prediction probability of the pseudo-label to score every generated image. Images with the least 20% scores are filtered."
        },
        {
            "heading": "4 EVALUATION",
            "text": "We now present our evaluation, which includes an exploration of the axes of the design space discussed in Section 3. We begin by showcasing our final result\u2014i.e., the best configuration discovered from our design space exploration\u2014and compare its performance to the best VRA results reported in prior work (Section 4.1). To summarize, our ultimate configuration is based on the L12W512 LiResNet architecture proposed by (Hu et al., 2023), i.e., its backbone contains 12 linear residual convolution blocks with 512 channels each. We modify this architecture by replacing the neck with Cholesky-orthogonalized dense layers (which were previously controlled using GloRo regularization), and adding 8 Cholesky-orthogonalized residual dense layers with 2048 neurons each to the end of the neck. This model is trained using our improved generated data augmentation pipeline (see Table 4 for details). We refer to this proposed configuration as \u201cLiResNet++.\u201d We provide further details on the exact training parameters in Appendix B.\nNext, in section 4.2, we provide an overview breaking down the various improvements we applied to reach our final configuration, followed by more detailed ablation studies comparing various Lipschitz control methods and data augmentation generation pipelines. Finally, in Section 4.3, we compare our method with randomized smoothing based methods, demonstrating that our work bridges the gap between deterministic and stochastic certification."
        },
        {
            "heading": "4.1 COMPARISON WITH PRIOR WORKS",
            "text": "We compare LiResNet++ with the following works from the literature: GloRo Nets with TRADES loss, Cayley, Local-Lip Net, SOC with Householder and Certification Regularization (HH+CR), CPL, SLL, Sandwich and LiResNet, which are selected for having been shown to surpass other approaches. Table 1 presents the clean and certified accuracy with different radius of certification 36/255,72/255, and 108/255 on CIFAR-10/100 and Tiny-ImageNet. We can see that our approach outperforms all existing architectures with significant margins on clean accuracy and certified accuracy for all values of \u03f5. On CIFAR-10/100, our model improves the certificated accuracy at \u03f5= 36/255 by more than 8%. We also compare the empirical robustness of the proposed method with some recent work in Section A.\nTo date, Hu et al. (2023) is the only work to report results on ImageNet. However they do not use generated data on ImageNet. We generated 2 million samples using guided diffusion to boost our model. Other settings are the same as those on CIFAR-10/100. With the improved model and generated data, we further improve the certification accuracy on ImageNet by 3.3%."
        },
        {
            "heading": "4.2 ABLATION STUDIES",
            "text": "The cumulative effects from each proposed modification. Since LiResNet++ is based on LiResNet (Hu et al., 2023), one would be interested to see the breakdown of the improvements from each modification. Table 2 shows the results. Modifications in the grey were not applied to the final model.\nWe explore different ways of using spatial MLPs. As shown in Table 2, we can either add all spatial-MLP blocks after the last convolution layer or insert one spatial-MLP block after every convolution layer. However, the improvement is very limited. We also consider using spatial MLPs with groups so that it can obtain larger model capacity from more parameters. We experiment with 2,4,\u00b7\u00b7\u00b7,32 groups, and there is no great difference. Although spatial-MLP shows comparable performance to transformers and conv-net in standard training, the capacity of spatial-MLP may not be enough for certificated training in a Lipschitz bounded setting.\nAdding dense layers to the model can increase the model capacity significantly. Dense layers can easily introduce millions of parameters with fairly small computational cost. In standard training, this property can make large dense layers prone to overfitting. While in Lipschitz based training, the smoothness of the model is controlled by the network Lipschitz constant thus large dense layers can improve the certification robustness effectively. Adding more dense layers (16 layers) diminishing return, thus we choose 8 dense layers in our final configuration.\nOrthogonalizing the dense layers is another effective way to improve the certification robustness. However, as we mentioned earlier we do not apply orthogonalization to convolutions since it has to apply FFT and inverse FFT to the feature map, which we believe is not a common practice for real-world application. Better pipeline to apply generated data to augment training also provides significant improvement. We have a more detailed experimental analysis later.\nAblation Study on the Lipschitz Control Mechanism We conduct a fair comparison between the various approaches to Lipschitz control discussed in Section 3.2 to find an optimal block. Specifically, we use the same backbone, LiResNet++ and train the model with the same setting on the same data (original dataset plus generated data). We only change the Lipschitz control mechanism for the dense layers\u2014for convolutions we still use Gloro-based regularization from the loss function. Table 3 shows the results on CIFAR10/100 datasets and Figure 1 (left) shows the learning curve of certificated accuracy on the test set. We can find that it performs better if parameterizing the dense layer weights are orthogonal matrices (Cayley, Matrix Exp, Cholesky). Cholesky-based Orthogonal Layer and Matrix Exponential perform similar, but Matrix Exponential is slower. It takes 32.4, 37.8 and 51.2 seconds to train one epoch with Cholesky, Cayley and Matrix Exp respectively on CIFAR-10 dataset using the same A100 machine. Cholesky-based Orthogonal Layer is the optimal Lipschitz control choice for the dense layers considering both performance and efficiency.\nFrom Figure 1, we see that using only GloRo regularization for controlling the Lipschitz constant of the final dense layers performs comparatively poorly in the early training stages but surpasses SLL and Sandwich orthogonalization after \u223c300-600 epochs. Because the dense layers are not constrained to be orthogonal in this case, the model requires more steps to learn a nearly orthogonal transformation. Note that this is particularly pronounced in the case of large dense layers, as opposed to the convolutional layers, which also rely on GloRo regularization. Leino (2022) has shown that a reliable gradient signal for orthogonalizing a linear transformation requires more power iterations as the dimension of the eigenvector increases. For large dense layers, the eigenvectors are high-dimensional, as compared to those of convolutions, which depend only on the size of the kernel (which is typically small). Thus we expect GloRo regularization to converge more slowly on dense layers than on convolutional ones.\nAblation Study on the generated data augmentation As shown in Table 2, a better pipeline to apply generated data augmentation can improve certification robustness significantly. Table 4 shows a detailed study on the effects of different pipeline. Switching to a better generator provides consistent improvements on both datasets no matter the sampled images are filtered or not. Using a stronger classification model to remove samples with least 20% low confidence pseudo-labels can also help. On CIFAR-100, the improvement is more significant. The reason is that CIFAR100 has more categories and therefore the\ndiffusion model generates a higher proportion of images with unmatched pseudo-labels. Using these labels can harm robust training. In the second part of the table, we study the ratio of real and generated samples in a batch. We find that seeing more generated samples can significantly improve the model\u2019s certification robustness.\nHowever, if we only use generated samples to train the model (real/generated sample ratio = 0: 1), it suffers from overfitting and the performance is decreased. From this experiment, we think the reason the generated data helps is not that the generated data is of better quality, but generated data are easier to classify on average (training on generated data has a faster training accuracy convergence). As we mentioned before, Lipschitz-based training suffers from underfitting and much of the model capacity is used to remember hard samples, including outliers and samples very close to the decision boundary. Learning from these hard samples do not improve robustness since these samples are not naturally non-robust. When trained with generated samples (which are easier), the percent of hard samples in the dataset is decreased and the model can focus more on learning a robust decision boundary. As a con-\ntrary, generated data not always improve the performance of standard accuracy (Azizi et al., 2023) even the SOTA diffusion model is used. In the standard training setting, neural network can fit hard samples more easily. Adding too many generated samples (In Azizi et al. (2023)\u2019s setting 6 times of the original training set), the test accuracy would decrease since most hard samples help generalization and their proportion has decreased with generated data added."
        },
        {
            "heading": "4.3 COMPARISON WITH RANDOMIZED SMOOTHING",
            "text": "To date, the methods that achieve the best certified performance are derived from randomized smoothing (RS) Cohen et al. (2019a). As we discussed, Lipschitz-based methods demonstrate advantages over RS has in terms of their efficiency and the guarantee that they provide. We provide the first comparison between these methods in Table 5. Notably, we are able to outperform several recent RS-based approaches on all certification radii."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "In this paper, our primary objective is to enhance the certified robustness of neural networks. We contend that a significant problem of existing Lipschitz-based models is their limited capacity, which hinders their ability to even overfit small datasets. To address this challenge, we have reexamined network architectures and basic building blocks to control network Lipschitz and have proposed three solutions to mitigate this issue. Firstly, we show that a combination of dense layers and convolutions can effectively expand the model\u2019s capacity. Secondly, we introduce the Cholesky Residual Layer, which serves as an efficient building block for achieving orthogonal weights. Thirdly, we have explored an improved pipeline for utilizing generated data to enhance Lipschitz-based training. Through extensive experiments, we have demonstrated the effectiveness of our approach. Our final results have pushed the boundaries of deterministic certified accuracy on CIFAR-10/100 datasets, surpassing the state of the art by up to 8.5 percentage points. Our method opens up a promising avenue to bridge the gap between probabilistic and deterministic certification methods."
        },
        {
            "heading": "A EMPIRICAL EVALUATION OF OUR MODEL",
            "text": "We compare the empirical robustness of the proposed method with some recent work. We use AutoAttack (Croce & Hein, 2020), an ensemble of diverse parameter-free attacks, to measure the empirical robustness and Table 6 present the results. Results of SLL and SandWich come from Wang & Manchester (2023). We can find that our certification performance can be even higher than the empirical robustness performance of prior work."
        },
        {
            "heading": "B TRAINING DETAILS",
            "text": "We follow most of the training settings used in (Hu et al., 2023). The first difference is that we change the maximum training perturbation radius to \u03f5train = 108/255 as we need to report certificated robustness performance of this radius during inference following [2,3,4]. The original setting is \u03f5train=72/255 as Hu et al. only report the certificated robustness performance at \u03f5test = 36/255. The second difference is the choice of generated data. We generated the generated data as described in (Hu et al., 2023). We train the model with a batch size of 1024 where 256 samples come from the original dataset and the rest 768 samples are generated. We do not change other settings including the learning rate."
        },
        {
            "heading": "C A NOTE ABOUT SLL RESULTS IN TABLE 3",
            "text": "We discovered an issue in the publicly-available SLL implementation that causes the SLL layer to be non-Lipschitz. After communicating with the authors over email, we devised an implementation that addressed the issue. Our results using SLL layers in Table 3 are based on this fix, and do not reflect the performance of the current public implementation of SLL."
        }
    ],
    "title": "EFFECTIVELY LEVERAGING CAPACITY FOR IMPROVED DETERMINISTIC ROBUSTNESS CERTIFICATION",
    "year": 2023
}