{
    "abstractText": "In distributed deep learning with data parallelism, synchronizing gradients at each training step can cause a huge communication overhead, especially when many nodes work together to train large models. Local gradient methods, such as Local SGD, address this issue by allowing workers to compute locally for H steps without synchronizing with others, hence reducing communication frequency. While H has been viewed as a hyperparameter to trade optimization efficiency for communication cost, recent research indicates that setting a proper H value can lead to generalization improvement. Yet, selecting a proper H is elusive. This work proposes a theory-grounded method for determiningH , named the Quadratic Synchronization Rule (QSR), which recommends dynamically settingH in proportion to 1 \u03b72 as the learning rate \u03b7 decays over time. Extensive ImageNet experiments on ResNet and ViT show that local gradient methods with QSR consistently improve the test accuracy over other synchronization strategies. Compared with the standard data parallel training, QSR enables Local AdamW on ViT-B to cut the training time on 16 or 64 GPUs down from 26.7 to 20.2 hours or from 8.6 to 5.5 hours and, at the same time, achieves 1.12% or 0.84% higher top-1 validation accuracy.",
    "authors": [],
    "id": "SP:f0a0d42c3aaa9861a571fa0ae0215cf7d3d095be",
    "references": [
        {
            "authors": [
                "Sanjeev Arora",
                "Nadav Cohen",
                "Wei Hu",
                "Yuping Luo"
            ],
            "title": "Implicit regularization in deep matrix factorization",
            "venue": "Advances in Neural Information Processing Systems",
            "year": 2019
        },
        {
            "authors": [
                "Sanjeev Arora",
                "Zhiyuan Li",
                "Abhishek Panigrahi"
            ],
            "title": "Understanding gradient descent on the edge of stability in deep learning",
            "venue": "Proceedings of the 39th International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Debraj Basu",
                "Deepesh Data",
                "Can Karakus",
                "Suhas Diggavi"
            ],
            "title": "Qsparse-local-SGD: Distributed SGD with quantization, sparsification and local computations",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Lucas Beyer",
                "Xiaohua Zhai",
                "Alexander Kolesnikov"
            ],
            "title": "Better plain vit baselines for imagenet-1k",
            "venue": "arXiv preprint arXiv:2205.01580,",
            "year": 2022
        },
        {
            "authors": [
                "Guy Blanc",
                "Neha Gupta",
                "Gregory Valiant",
                "Paul Valiant"
            ],
            "title": "Implicit regularization for deep neural networks driven by an ornstein-uhlenbeck like process",
            "venue": "Proceedings of Thirty Third Conference on Learning Theory, volume 125 of Proceedings of Machine Learning Research,",
            "year": 2020
        },
        {
            "authors": [
                "Radford",
                "Ilya Sutskever",
                "Dario Amodei"
            ],
            "title": "Language models are few-shot learners",
            "venue": "CoRR, abs/2005.14165,",
            "year": 2020
        },
        {
            "authors": [
                "Kai Chen",
                "Qiang Huo"
            ],
            "title": "Scalable training of deep learning machines by incremental block training with intra-block parallel optimization and blockwise model-update filtering",
            "venue": "In 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2016
        },
        {
            "authors": [
                "Xiangning Chen",
                "Cho-Jui Hsieh",
                "Boqing Gong"
            ],
            "title": "When vision transformers outperform resnets without pre-training or strong data augmentations",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "L\u00e9na\u00efc Chizat",
                "Francis Bach"
            ],
            "title": "Implicit bias of gradient descent for wide two-layer neural networks trained with the logistic loss",
            "venue": "Proceedings of Thirty Third Conference on Learning Theory, volume 125 of Proceedings of Machine Learning Research,",
            "year": 2020
        },
        {
            "authors": [
                "Jeremy Cohen",
                "Simran Kaur",
                "Yuanzhi Li",
                "J Zico Kolter",
                "Ameet Talwalkar"
            ],
            "title": "Gradient descent on neural networks typically occurs at the edge of stability",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Aditya Cowsik",
                "Tankut Can",
                "Paolo Glorioso"
            ],
            "title": "Flatter, faster: scaling momentum for optimal speedup of sgd",
            "venue": "arXiv preprint arXiv:2210.16400,",
            "year": 2022
        },
        {
            "authors": [
                "Alex Damian",
                "Tengyu Ma",
                "Jason D Lee"
            ],
            "title": "Label noise SGD provably prefers flat global minimizers",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Alex Damian",
                "Eshaan Nichani",
                "Jason D. Lee"
            ],
            "title": "Self-stabilization: The implicit bias of gradient descent at the edge of stability",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "Lucas Beyer",
                "Alexander Kolesnikov",
                "Dirk Weissenborn",
                "Xiaohua Zhai",
                "Thomas Unterthiner",
                "Mostafa Dehghani",
                "Matthias Minderer",
                "Georg Heigold",
                "Sylvain Gelly",
                "Jakob Uszkoreit",
                "Neil Houlsby"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Felix Draxler",
                "Kambis Veschgini",
                "Manfred Salmhofer",
                "Fred Hamprecht"
            ],
            "title": "Essentially no barriers in neural network energy landscape",
            "venue": "In International conference on machine learning,",
            "year": 2018
        },
        {
            "authors": [
                "Pierre Foret",
                "Ariel Kleiner",
                "Hossein Mobahi",
                "Behnam Neyshabur"
            ],
            "title": "Sharpness-aware minimization for efficiently improving generalization",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Pierre Foret",
                "Ariel Kleiner",
                "Hossein Mobahi",
                "Behnam Neyshabur"
            ],
            "title": "Sharpness-aware minimization for efficiently improving generalization",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Jonathan Frankle",
                "Gintare Karolina Dziugaite",
                "Daniel Roy",
                "Michael Carbin"
            ],
            "title": "Linear mode connectivity and the lottery ticket hypothesis",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Timur Garipov",
                "Pavel Izmailov",
                "Dmitrii Podoprikhin",
                "Dmitry P Vetrov",
                "Andrew G Wilson"
            ],
            "title": "Loss surfaces, mode connectivity, and fast ensembling of dnns",
            "venue": "Advances in neural information processing systems,",
            "year": 2018
        },
        {
            "authors": [
                "Rong Ge",
                "Yunwei Ren",
                "Xiang Wang",
                "Mo Zhou"
            ],
            "title": "Understanding deflation process in overparametrized tensor decomposition",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Priya Goyal",
                "Piotr Doll\u00e1r",
                "Ross Girshick",
                "Pieter Noordhuis",
                "Lukasz Wesolowski",
                "Aapo Kyrola",
                "Andrew Tulloch",
                "Yangqing Jia",
                "Kaiming He"
            ],
            "title": "Accurate, large minibatch SGD: Training imagenet in 1 hour",
            "venue": "arXiv preprint arXiv:1706.02677,",
            "year": 2017
        },
        {
            "authors": [
                "Diego Granziol",
                "Stefan Zohren",
                "Stephen Roberts"
            ],
            "title": "Learning rates as a function of batch size: A random matrix theory approach to neural network training",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2022
        },
        {
            "authors": [
                "Xinran Gu",
                "Kaifeng Lyu",
                "Longbo Huang",
                "Sanjeev Arora"
            ],
            "title": "Why (and when) does local SGD generalize better than SGD",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Vipul Gupta",
                "Santiago Akle Serrano",
                "Dennis DeCoste"
            ],
            "title": "Stochastic weight averaging in parallel: Large-batch training that generalizes well",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Farzin Haddadpour",
                "Mohammad Mahdi Kamani",
                "Mehrdad Mahdavi",
                "Viveck Cadambe"
            ],
            "title": "Local SGD with periodic averaging: Tighter analysis and adaptive synchronization",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Wenqing Hu",
                "Chris Junchi Li",
                "Lei Li",
                "Jian-Guo Liu"
            ],
            "title": "On the diffusion approximation of nonconvex stochastic gradient descent",
            "venue": "arXiv preprint arXiv:1705.07562,",
            "year": 2017
        },
        {
            "authors": [
                "Gao Huang",
                "Zhuang Liu",
                "Laurens Van Der Maaten",
                "Kilian Q Weinberger"
            ],
            "title": "Densely connected convolutional networks",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Hikaru Ibayashi",
                "Masaaki Imaizumi"
            ],
            "title": "Exponential escape efficiency of SGD from sharp minima in non-stationary regime",
            "venue": "arXiv preprint arXiv:2111.04004,",
            "year": 2021
        },
        {
            "authors": [
                "P Izmailov",
                "AG Wilson",
                "D Podoprikhin",
                "D Vetrov",
                "T Garipov"
            ],
            "title": "Averaging weights leads to wider optima and better generalization",
            "venue": "In 34th Conference on Uncertainty in Artificial Intelligence 2018,",
            "year": 2018
        },
        {
            "authors": [
                "Peter Izsak",
                "Moshe Berchansky",
                "Omer Levy"
            ],
            "title": "How to train bert with an academic budget",
            "venue": "arXiv preprint arXiv:2104.07705,",
            "year": 2021
        },
        {
            "authors": [
                "Stanis\u0142aw Jastrz\u0119bski",
                "Zachary Kenton",
                "Devansh Arpit",
                "Nicolas Ballas",
                "Asja Fischer",
                "Yoshua Bengio",
                "Amos Storkey"
            ],
            "title": "Three factors influencing minima in SGD",
            "venue": "arXiv preprint arXiv:1711.04623,",
            "year": 2017
        },
        {
            "authors": [
                "Ziwei Ji",
                "Matus Telgarsky"
            ],
            "title": "Directional convergence and alignment in deep learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Yiding Jiang",
                "Behnam Neyshabur",
                "Hossein Mobahi",
                "Dilip Krishnan",
                "Samy Bengio"
            ],
            "title": "Fantastic generalization measures and where to find them",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Jikai Jin",
                "Zhiyuan Li",
                "Kaifeng Lyu",
                "Simon Shaolei Du",
                "Jason D. Lee"
            ],
            "title": "Understanding incremental learning of gradient descent: A fine-grained analysis of matrix sensing",
            "venue": "Proceedings of the 40th International Conference on Machine Learning,",
            "year": 2023
        },
        {
            "authors": [
                "Peter Kairouz",
                "H Brendan McMahan",
                "Brendan Avent",
                "Aur\u00e9lien Bellet",
                "Mehdi Bennis",
                "Arjun Nitin Bhagoji",
                "Kallista Bonawitz",
                "Zachary Charles",
                "Graham Cormode",
                "Rachel Cummings"
            ],
            "title": "Advances and open problems in federated learning",
            "venue": "Foundations and Trends\u00ae in Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Michael Kamp",
                "Mario Boley",
                "Daniel Keren",
                "Assaf Schuster",
                "Izchak Sharfman"
            ],
            "title": "Communicationefficient distributed online prediction by dynamic model synchronization. In Machine Learning and Knowledge Discovery in Databases: European Conference",
            "venue": "ECML PKDD 2014, Nancy,",
            "year": 2014
        },
        {
            "authors": [
                "Sai Praneeth Karimireddy",
                "Satyen Kale",
                "Mehryar Mohri",
                "Sashank Reddi",
                "Sebastian Stich",
                "Ananda Theertha Suresh"
            ],
            "title": "Scaffold: Stochastic controlled averaging for federated learning",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Nitish Shirish Keskar",
                "Dheevatsa Mudigere",
                "Jorge Nocedal",
                "Mikhail Smelyanskiy",
                "Ping Tak Peter Tang"
            ],
            "title": "On large-batch training for deep learning: Generalization gap and sharp minima",
            "venue": "In International Conference on Learning Representations,",
            "year": 2017
        },
        {
            "authors": [
                "Nitish Shirish Keskar",
                "Dheevatsa Mudigere",
                "Jorge Nocedal",
                "Mikhail Smelyanskiy",
                "Ping Tak Peter Tang"
            ],
            "title": "On large-batch training for deep learning: Generalization gap and sharp minima",
            "venue": "In International Conference on Learning Representations,",
            "year": 2017
        },
        {
            "authors": [
                "Ahmed Khaled",
                "Konstantin Mishchenko",
                "Peter Richt\u00e1rik"
            ],
            "title": "Tighter theory for local SGD on identical and heterogeneous data",
            "venue": "In International Conference on Artificial Intelligence and Statistics,",
            "year": 2020
        },
        {
            "authors": [
                "Bobby Kleinberg",
                "Yuanzhi Li",
                "Yang Yuan"
            ],
            "title": "An alternative view: When does SGD escape local minima",
            "venue": "Proceedings of the 35th International Conference on Machine Learning,",
            "year": 2018
        },
        {
            "authors": [
                "Jakub Kone\u010dn\u1ef3",
                "H Brendan McMahan",
                "Felix X Yu",
                "Peter Richt\u00e1rik",
                "Ananda Theertha Suresh",
                "Dave Bacon"
            ],
            "title": "Federated learning: Strategies for improving communication efficiency",
            "venue": "arXiv preprint arXiv:1610.05492,",
            "year": 2016
        },
        {
            "authors": [
                "Alex Krizhevsky"
            ],
            "title": "One weird trick for parallelizing convolutional neural networks",
            "venue": "arXiv preprint arXiv:1404.5997,",
            "year": 2014
        },
        {
            "authors": [
                "Guillaume Leclerc",
                "Andrew Ilyas",
                "Logan Engstrom",
                "Sung Min Park",
                "Hadi Salman",
                "Aleksander M\u0105dry"
            ],
            "title": "Ffcv: Accelerating training by removing data bottlenecks",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2023
        },
        {
            "authors": [
                "Conglong Li",
                "Ammar Ahmad Awan",
                "Hanlin Tang",
                "Samyam Rajbhandari",
                "Yuxiong He"
            ],
            "title": "1-bit lamb: communication efficient large-scale large-batch training with lamb\u2019s convergence speed",
            "venue": "IEEE 29th International Conference on High Performance Computing, Data, and Analytics (HiPC),",
            "year": 2022
        },
        {
            "authors": [
                "Mengtian Li",
                "Ersin Yumer",
                "Deva Ramanan"
            ],
            "title": "Budgeted training: Rethinking deep neural network training under resource constraints",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Qianxiao Li",
                "Cheng Tai",
                "Weinan E"
            ],
            "title": "Stochastic modified equations and dynamics of stochastic gradient algorithms i: Mathematical foundations",
            "venue": "Journal of Machine Learning Research,",
            "year": 2019
        },
        {
            "authors": [
                "Tian Li",
                "Anit Kumar Sahu",
                "Manzil Zaheer",
                "Maziar Sanjabi",
                "Ameet Talwalkar",
                "Virginia Smith"
            ],
            "title": "Federated optimization in heterogeneous networks",
            "venue": "Proceedings of Machine learning and systems,",
            "year": 2020
        },
        {
            "authors": [
                "Xiang Li",
                "Kaixuan Huang",
                "Wenhao Yang",
                "Shusen Wang",
                "Zhihua Zhang"
            ],
            "title": "On the convergence of fedavg on non-iid data",
            "venue": "In International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "Yuanzhi Li",
                "Tengyu Ma",
                "Hongyang Zhang"
            ],
            "title": "Algorithmic regularization in over-parameterized matrix sensing and neural networks with quadratic activations",
            "venue": "Proceedings of the 31st Conference On Learning Theory,",
            "year": 2018
        },
        {
            "authors": [
                "Zhiyuan Li",
                "Yuping Luo",
                "Kaifeng Lyu"
            ],
            "title": "Towards resolving the implicit bias of gradient descent for matrix factorization: Greedy low-rank learning",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Zhiyuan Li",
                "Sadhika Malladi",
                "Sanjeev Arora"
            ],
            "title": "On the validity of modeling SGD with stochastic differential equations (sdes)",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Zhiyuan Li",
                "Tianhao Wang",
                "Sanjeev Arora"
            ],
            "title": "What happens after SGD reaches zero loss?\u2013a mathematical framework",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Tao Lin",
                "Sebastian U. Stich",
                "Kumar Kshitij Patel",
                "Martin Jaggi"
            ],
            "title": "Don\u2019t use large mini-batches, use Local SGD",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Hong Liu",
                "Sang Michael Xie",
                "Zhiyuan Li",
                "Tengyu Ma"
            ],
            "title": "Same pre-training loss, better downstream: Implicit bias matters for language models",
            "venue": "Proceedings of the 40th International Conference on Machine Learning,",
            "year": 2023
        },
        {
            "authors": [
                "Ze Liu",
                "Yutong Lin",
                "Yue Cao",
                "Han Hu",
                "Yixuan Wei",
                "Zheng Zhang",
                "Stephen Lin",
                "Baining Guo"
            ],
            "title": "Swin transformer: Hierarchical vision transformer using shifted windows",
            "venue": "In Proceedings of the IEEE/CVF international conference on computer vision,",
            "year": 2021
        },
        {
            "authors": [
                "Zhuang Liu",
                "Hanzi Mao",
                "Chao-Yuan Wu",
                "Christoph Feichtenhofer",
                "Trevor Darrell",
                "Saining Xie. A convnet for the"
            ],
            "title": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp",
            "venue": "11976\u201311986, 2022.",
            "year": 2020
        },
        {
            "authors": [
                "Kaifeng Lyu",
                "Jian Li"
            ],
            "title": "Gradient descent maximizes the margin of homogeneous neural networks",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Kaifeng Lyu",
                "Zhiyuan Li",
                "Runzhe Wang",
                "Sanjeev Arora"
            ],
            "title": "Gradient descent on two-layer nets: Margin maximization and simplicity bias",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Kaifeng Lyu",
                "Zhiyuan Li",
                "Sanjeev Arora"
            ],
            "title": "Understanding the generalization benefit of normalization layers: Sharpness reduction, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Chao Ma",
                "Lexing Ying"
            ],
            "title": "On linear stability of SGD and input-smoothness of neural networks",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Chao Ma",
                "Daniel Kunin",
                "Lei Wu",
                "Lexing Ying"
            ],
            "title": "Beyond the quadratic approximation: The multiscale structure of neural network loss landscapes",
            "venue": "Journal of Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Wei-Chiu Ma",
                "Shenlong Wang",
                "Rui Hu",
                "Yuwen Xiong",
                "Raquel Urtasun"
            ],
            "title": "Deep rigid instance scene flow",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Sadhika Malladi",
                "Kaifeng Lyu",
                "Abhishek Panigrahi",
                "Sanjeev Arora"
            ],
            "title": "On the SDEs and scaling rules for adaptive gradient algorithms",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Gideon Mann",
                "Ryan T. McDonald",
                "Mehryar Mohri",
                "Nathan Silberman",
                "Dan Walker"
            ],
            "title": "Efficient large-scale distributed training of conditional maximum entropy models",
            "venue": "In Advances in Neural Information Processing Systems",
            "year": 2009
        },
        {
            "authors": [
                "Brendan McMahan",
                "Eider Moore",
                "Daniel Ramage",
                "Seth Hampson",
                "Blaise Aguera y Arcas"
            ],
            "title": "Communication-efficient learning of deep networks from decentralized data",
            "venue": "In Artificial intelligence and statistics,",
            "year": 2017
        },
        {
            "authors": [
                "Mor Shpigel Nacson",
                "Suriya Gunasekar",
                "Jason Lee",
                "Nathan Srebro",
                "Daniel Soudry"
            ],
            "title": "Lexicographic and depth-sensitive margins in homogeneous and non-homogeneous deep models",
            "venue": "Proceedings of the 36th International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Giorgi Nadiradze",
                "Amirmojtaba Sabour",
                "Peter Davies",
                "Shigang Li",
                "Dan Alistarh"
            ],
            "title": "Asynchronous decentralized sgd with quantized and local updates",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Behnam Neyshabur",
                "Srinadh Bhojanapalli",
                "David Mcallester",
                "Nati Srebro"
            ],
            "title": "Exploring generalization in deep learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Jose Javier Gonzalez Ortiz",
                "Jonathan Frankle",
                "Mike Rabbat",
                "Ari Morcos",
                "Nicolas Ballas"
            ],
            "title": "Tradeoffs of Local SGD at scale: An empirical study",
            "venue": "arXiv preprint arXiv:2110.08133,",
            "year": 2021
        },
        {
            "authors": [
                "Daniel Povey",
                "Xiaohui Zhang",
                "Sanjeev Khudanpur"
            ],
            "title": "Parallel training of dnns with natural gradient and parameter averaging",
            "venue": "arXiv preprint arXiv:1410.7455,",
            "year": 2014
        },
        {
            "authors": [
                "Noam Razin",
                "Nadav Cohen"
            ],
            "title": "Implicit regularization in deep learning may not be explainable by norms",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Noam Razin",
                "Asaf Maman",
                "Nadav Cohen"
            ],
            "title": "Implicit regularization in hierarchical tensor factorization and deep convolutional neural networks",
            "venue": "Proceedings of the 39th International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Sashank J Reddi",
                "Zachary Charles",
                "Manzil Zaheer",
                "Zachary Garrett",
                "Keith Rush",
                "Jakub Kone\u010dn\u1ef3",
                "Sanjiv Kumar",
                "Hugh Brendan McMahan"
            ],
            "title": "Adaptive federated optimization",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Christopher J. Shallue",
                "Jaehoon Lee",
                "Joseph Antognini",
                "Jascha Sohl-Dickstein",
                "Roy Frostig",
                "George E. Dahl"
            ],
            "title": "Measuring the effects of data parallelism on neural network training",
            "venue": "Journal of Machine Learning Research,",
            "year": 2019
        },
        {
            "authors": [
                "Shuheng Shen",
                "Yifei Cheng",
                "Jingchang Liu",
                "Linli Xu"
            ],
            "title": "Stl-sgd: Speeding up local sgd with stagewise communication period",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Samuel Smith",
                "Erich Elsen",
                "Soham De"
            ],
            "title": "On the generalization benefit of noise in stochastic gradient descent",
            "venue": "Proceedings of the 37th International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Samuel L Smith",
                "Benoit Dherin",
                "David Barrett",
                "Soham De"
            ],
            "title": "On the origin of implicit regularization in stochastic gradient descent",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Daniel Soudry",
                "Elad Hoffer",
                "Mor Shpigel Nacson",
                "Suriya Gunasekar",
                "Nathan Srebro"
            ],
            "title": "The implicit bias of gradient descent on separable data",
            "venue": "Journal of Machine Learning Research,",
            "year": 2018
        },
        {
            "authors": [
                "Daniel Soudry",
                "Elad Hoffer",
                "Nathan Srebro"
            ],
            "title": "The implicit bias of gradient descent on separable data",
            "venue": "In International Conference on Learning Representations,",
            "year": 2018
        },
        {
            "authors": [
                "Sebastian U Stich"
            ],
            "title": "Local SGD converges fast and communicates little",
            "venue": "In International Conference on Learning Representations,",
            "year": 2018
        },
        {
            "authors": [
                "Dominik St\u00f6ger",
                "Mahdi Soltanolkotabi"
            ],
            "title": "Small random initialization is akin to spectral learning: Optimization and generalization guarantees for overparameterized low-rank matrix reconstruction",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Hang Su",
                "Haoyu Chen"
            ],
            "title": "Experiments on parallel training of deep neural network using model averaging",
            "venue": "arXiv preprint arXiv:1507.01239,",
            "year": 2015
        },
        {
            "authors": [
                "Hanlin Tang",
                "Shaoduo Gan",
                "Ammar Ahmad Awan",
                "Samyam Rajbhandari",
                "Conglong Li",
                "Xiangru Lian",
                "Ji Liu",
                "Ce Zhang",
                "Yuxiong He"
            ],
            "title": "1-bit adam: Communication efficient large-scale training with adam\u2019s convergence speed",
            "venue": "In Proceedings of the 38th International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Nikhil Vyas",
                "Depen Morwani",
                "Rosie Zhao",
                "Gal Kaplun",
                "Sham Kakade",
                "Boaz Barak"
            ],
            "title": "Beyond implicit bias: The insignificance of sgd noise in online learning",
            "venue": "arXiv preprint arXiv:2306.08590,",
            "year": 2023
        },
        {
            "authors": [
                "Jianyu Wang",
                "Gauri Joshi"
            ],
            "title": "Adaptive communication strategies to achieve the best error-runtime trade-off in local-update SGD",
            "venue": "Proceedings of Machine Learning and Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Jianyu Wang",
                "Gauri Joshi"
            ],
            "title": "Cooperative SGD: A unified framework for the design and analysis of local-update SGD algorithms",
            "venue": "Journal of Machine Learning Research,",
            "year": 2021
        },
        {
            "authors": [
                "Jianyu Wang",
                "Vinayak Tantia",
                "Nicolas Ballas",
                "Michael Rabbat"
            ],
            "title": "Slowmo: Improving communication-efficient distributed SGD with slow momentum",
            "venue": "In International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "Runzhe Wang",
                "Sadhika Malladi",
                "Tianhao Wang",
                "Kaifeng Lyu",
                "Zhiyuan Li"
            ],
            "title": "The marginal value of momentum for small learning rate sgd",
            "venue": "arXiv preprint arXiv:2307.15196,",
            "year": 2023
        },
        {
            "authors": [
                "Blake Woodworth",
                "Kumar Kshitij Patel",
                "Sebastian Stich",
                "Zhen Dai",
                "Brian Bullins",
                "Brendan Mcmahan",
                "Ohad Shamir",
                "Nathan Srebro"
            ],
            "title": "Is local sgd better than minibatch sgd",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Mitchell Wortsman",
                "Gabriel Ilharco",
                "Samir Ya Gadre",
                "Rebecca Roelofs",
                "Raphael Gontijo-Lopes",
                "Ari S Morcos",
                "Hongseok Namkoong",
                "Ali Farhadi",
                "Yair Carmon",
                "Simon Kornblith"
            ],
            "title": "Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Mitchell Wortsman",
                "Suchin Gururangan",
                "Shen Li",
                "Ali Farhadi",
                "Ludwig Schmidt",
                "Michael Rabbat",
                "Ari S. Morcos"
            ],
            "title": "lo-fi: distributed fine-tuning without communication",
            "venue": "Transactions on Machine Learning Research,",
            "year": 2023
        },
        {
            "authors": [
                "Lei Wu",
                "Chao Ma",
                "Weinan E"
            ],
            "title": "How sgd selects the global minima in over-parameterized learning: A dynamical stability perspective",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Zeke Xie",
                "Issei Sato",
                "Masashi Sugiyama"
            ],
            "title": "A diffusion theory for deep learning dynamics: Stochastic gradient descent exponentially favors flat minima",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Hang Xu",
                "Wenxuan Zhang",
                "Jiawei Fei",
                "Yuzhe Wu",
                "Tingwen Xie",
                "Jun Huang",
                "Yuchen Xie",
                "Mohamed Elhoseiny",
                "Panos Kalnis"
            ],
            "title": "SLAMB: Accelerated large batch training with sparse communication",
            "venue": "In Proceedings of the 40th International Conference on Machine Learning,",
            "year": 2023
        },
        {
            "authors": [
                "Yang You",
                "Zhao Zhang",
                "Cho-Jui Hsieh",
                "James Demmel",
                "Kurt Keutzer"
            ],
            "title": "Imagenet training in minutes",
            "venue": "In Proceedings of the 47th International Conference on Parallel Processing,",
            "year": 2018
        },
        {
            "authors": [
                "Yang You",
                "Jing Li",
                "Sashank Reddi",
                "Jonathan Hseu",
                "Sanjiv Kumar",
                "Srinadh Bhojanapalli",
                "Xiaodan Song",
                "James Demmel",
                "Kurt Keutzer",
                "Cho-Jui Hsieh"
            ],
            "title": "Large batch optimization for deep learning: Training BERT in 76 minutes",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Hao Yu",
                "Sen Yang",
                "Shenghuo Zhu"
            ],
            "title": "Parallel restarted SGD with faster convergence and less communication: Demystifying why model averaging works for deep learning",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2019
        },
        {
            "authors": [
                "Chiyuan Zhang",
                "Samy Bengio",
                "Moritz Hardt",
                "Benjamin Recht",
                "Oriol Vinyals"
            ],
            "title": "Understanding deep learning requires rethinking generalization",
            "venue": "In International Conference on Learning Representations,",
            "year": 2017
        },
        {
            "authors": [
                "Xiaohui Zhang",
                "Jan Trmal",
                "Daniel Povey",
                "Sanjeev Khudanpur"
            ],
            "title": "Improving deep neural network acoustic models using generalized maxout networks",
            "venue": "In 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2014
        },
        {
            "authors": [
                "Fan Zhou",
                "Guojing Cong"
            ],
            "title": "On the convergence properties of a k-step averaging stochastic gradient descent algorithm for nonconvex optimization",
            "venue": "In Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence,",
            "year": 2018
        },
        {
            "authors": [
                "Tongtian Zhu",
                "Fengxiang He",
                "Kaixuan Chen",
                "Mingli Song",
                "Dacheng Tao"
            ],
            "title": "Decentralized SGD and average-direction SAM are asymptotically equivalent",
            "venue": "Proceedings of the 40th International Conference on Machine Learning,",
            "year": 2023
        },
        {
            "authors": [
                "Zhanxing Zhu",
                "Jingfeng Wu",
                "Bing Yu",
                "Lei Wu",
                "Jinwen Ma"
            ],
            "title": "The anisotropic noise in stochastic gradient descent: Its behavior of escaping from sharp minima and regularization effects",
            "venue": "Proceedings of the 36th International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Martin Zinkevich",
                "Markus Weimer",
                "Lihong Li",
                "Alex Smola"
            ],
            "title": "Parallelized stochastic gradient descent",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2010
        },
        {
            "authors": [
                "Yu"
            ],
            "title": "timizing the selection of the synchronization period H from an optimization perspective. The conventional approach sets H as a constant throughout training. In this setup, a series of studies (e.g.,Khaled et al",
            "venue": "Haddadpour et al",
            "year": 2018
        },
        {
            "authors": [
                "Shen"
            ],
            "title": "should be set as the smallest value that reduces the communication cost to an acceptable level to minimize the negative impact on optimization. To better trade-off between optimization and generalization, researchers introduced various adaptive communication strategies. Kamp et al. (2014) designed a synchronization protocol controlled by the variance in model parameters",
            "venue": "Haddadpour et al",
            "year": 2019
        },
        {
            "authors": [
                "Gupta"
            ],
            "title": "small-scale datasets like CIFAR-10/100. Generalization perspectives on selecting H . While a larger H usually hurts optimization, it can sometimes improve generalization",
            "venue": "Apart from Lin et al",
            "year": 2020
        },
        {
            "authors": [
                "Ma"
            ],
            "title": "2022) elucidate that GD inherently biases towards flatter regions on the loss landscape. Specifically, under some regularity conditions, they show that GD will eventually enter the \u201cEdge of Stability\u201d(Cohen et al., 2020), where the maximum eigenvalue of the loss Hessian stays around 2/learning rate, and then constantly moves towards flatter minima. Going beyond GD, another line of work studies how gradient noise in SGD helps reduce sharpness",
            "year": 2018
        },
        {
            "authors": [
                "Ma",
                "Ying"
            ],
            "title": "2021) showed that gradient noise can cause training instability around sharp minima, and hence, the iterate can only settle around flat minima",
            "venue": "(Garipov et al.,",
            "year": 2021
        },
        {
            "authors": [
                "Blanc"
            ],
            "title": "2021c) assume the existence of a minimizer manifold and show that gradient noise provably drives the iterate towards flatter minima on this manifold",
            "venue": "Cowsik et al",
            "year": 2023
        },
        {
            "authors": [
                "Gu"
            ],
            "title": "permutation of train data points, and partition the data points evenly among the K workers. Then at each local step of each worker, Sample() sequentially takes samples from its own partition. Once there are too few remaining samples to form a complete batch, a new permutation is sampled and a new epoch starts",
            "year": 2023
        },
        {
            "authors": [
                "Beyer"
            ],
            "title": "2022), we use a larger batch size (B = 4096 or 16384 as opposed to their 1024) and use AdamW instead of Adam. As for gradient clipping, we set it as 1 for standard",
            "year": 2022
        },
        {
            "authors": [
                "AdamW following Beyer"
            ],
            "title": "2021). However, for Local AdamW, the smaller batch size locally leads to larger gradient noise and, hence larger gradient norm for local updates",
            "year": 2021
        },
        {
            "authors": [
                "Gu"
            ],
            "title": "2023), we assume that L(\u03b8) and \u03a3(\u03b8) are C\u221e-smooth",
            "year": 2023
        },
        {
            "authors": [
                "Li"
            ],
            "title": "argmin\u03b8\u2208UL(\u03b8). Assumption E.2 is motivated by recent empirical observations that low-loss solutions on the loss landscape are not isolated but path-connected (Garipov et al., 2018",
            "venue": "Draxler et al.,",
            "year": 2018
        },
        {
            "authors": [
                "Gu"
            ],
            "title": "2023)\u2019s analysis, below we compute the moments of \u03c6 \u2212 \u03c6 through a series of lemmas",
            "year": 2023
        },
        {
            "authors": [
                "Gu"
            ],
            "title": "The key insight is that the dynamics of each worker before averaging in each round is just the standard SGD with a smaller batch size, Bloc. Since the distance bound to \u0393, Theorem",
            "venue": "\u03b7\u03b4",
            "year": 2023
        },
        {
            "authors": [
                "Gu"
            ],
            "title": "Again, the key insight is that the dynamics of each worker before averaging in each round is just the standard SGD with a smaller batch size, Bloc. Since the SDE approximation theorem for Local SGD, Theorem",
            "year": 2023
        },
        {
            "authors": [],
            "title": "\u03b7\u22123 relies on the rapid decaying learning rate within a synchronization period via ablation studies on ViT-B. In Table 6(a), we replace the cosine decay schedule with a variant of the step decay schedule in Smith et al. (2020). In Table 6(b), we run both scalings with a modified cosine decay schedule that ceases to decay at some epoch t\u2032\u2032. QSR consistently outperforms H \u223c \u03b7\u22123 in both cases",
            "year": 2020
        },
        {
            "authors": [
                "Smith"
            ],
            "title": "In our step decay schedule, given a total of 300 epochs, the learning rate remains at its peak until epoch 150, after which it is divided by 2 every 30 epochs. See Figure 7 for an illustration",
            "year": 2020
        },
        {
            "authors": [
                "H round"
            ],
            "title": "COMPARISON WITH LOCAL SGD/ADAMW + SWAP In this section, we compare QSR with the modified Stochastic Weight Averaging in Parallel (SWAP) algorithm, termed \u201cLocal SGD/AdamW + SWAP\u201d. Specifically, the original SWAP proposed by (Gupta et al., 2020) uses SGD for the majority of the training process and only switches",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "The growing scale of deep learning necessitates distributed training to reduce the wall-clock time. Data parallel training is a foundational technique that distributes the workload of gradient computation toK workers, also serving as a key building block of more advanced parallel strategies. At each step of this method, each worker first computes gradients on their own local batches of data. Then, they take an average over local gradients, which typically involves a costly All-Reduce operation. Finally, they update the model parameter with the averaged gradient and a gradient-based optimizer OPT, e.g., SGD, AdamW. In this paper, we term the data parallel implementation of optimizer OPT as \u201cParallel OPT\u201d. See Algorithm 1 for the pseudocode. The cost for this data parallelism is obvious. Frequent gradient synchronization can induce huge communication overhead as the number of workers and model size grow, severely hindering the scalability of distributed training (Tang et al., 2021; Li et al., 2022; Xu et al., 2023).\nOne approach to reducing this communication overhead is Local SGD (Stich, 2018; Zhou & Cong, 2018; Woodworth et al., 2020). Rather than synchronizing gradients at every step, Local SGD allows workers to independently train their local replicas using their own local batches with SGD updates. It is only after completing H > 1 local steps that these workers synchronize, where the model parameters get averaged over all replicas. Notably, while we mention SGD, this approach can be readily adapted to other popular optimizers. In this paper, if a gradient-based optimizer OPT is used for local updates, we term the variant as \u201cLocal OPT\u201d (e.g., Local SGD, Local AdamW), and collectively refer to this class of approaches as local gradient methods. We provide a pseudocode for local gradient methods in Algorithm 2.\nThe main focus of this paper is to study the best strategies to set the synchronization period H (i.e., the number of local steps per communication round) in local gradient methods. While setting H to a larger value reduces communication, a very large H can hinder the training loss from decreasing at normal speed, since the local replicas may significantly diverge from each other before averaging. Indeed, it has been observed empirically that larger H leads to higher training loss after the same number of steps (Wang & Joshi, 2021; Ortiz et al., 2021), and efforts to analyze the convergence of local gradient methods in theory usually end up with loss bounds increasing with H (Khaled et al.,\n2020; Stich, 2018; Haddadpour et al., 2019; Yu et al., 2019). To better trade-off between communication cost and optimization speed, Kamp et al. (2014); Wang & Joshi (2019); Haddadpour et al. (2019); Shen et al. (2021) proposed adaptive synchronization schemes, such as linearly increasing H as the iteration goes on (Haddadpour et al., 2019), or adjusting H based on the variance in model parameters (Kamp et al., 2014). Nonetheless, their effectiveness has only been validated on linear models or small-scale datasets, e.g., CIFAR-10/100.\nAll these strategies are developed to avoid sacrificing too much training loss, but training loss is never the final evaluation metric that one cares about in deep learning. Due to the overparameterized nature of modern neural networks, reaching the same training loss does not correspond to the same performance on test data. It has also been long known that the choice of optimizers or hyperparameters can change not only the optimization speed of the training loss but also their implicit bias towards solutions with different test accuracies.\nThe presence of this implicit bias indeed complicates the picture of setting H in local gradient methods. Though a large H might be harmful for training loss, it has been observed empirically that setting H properly can sometimes improve rather than hurt the final test accuracy. Lin et al. (2020) are the first to report this phenomenon. Comparing with running just the standard data parallel SGD (equivalent to H = 1), they observed that switching from SGD to Local SGD (H > 1) halfway through consistently leads to higher final test accuracy. Local SGD with this specific schedule of H is designated as Post-local SGD. Lin et al. (2020)\u2019s work opens up a new angle in setting H in local gradient methods, yet, the proposed schedule in Post-local SGD, referred to as the post-local schedule in this paper, is suboptimal in improving test accuracy. It was later reported by Ortiz et al. (2021) that Post-local SGD does not improve much on ImageNet. For both stepwise decay and cosine decay learning rate schedules, the test accuracy improvement of Post-local SGD diminishes as learning rate decreases. Further, it remains unclear whether the generalization benefit continues to appear when the optimizer is changed from SGD to adaptive gradient methods such as Adam/AdamW, which are now indispensable for training large models.\nOur Contributions. In this paper, we aim to propose a general and effective H schedule that can be readily applied to various optimizers and neural network models. Specifically, we introduce a simple yet effective strategy, called Quadratic Synchronization Rule (QSR), for dynamically adjusting the synchronization period according to the learning rate: given a learning rate schedule, we set H proportional to \u03b7\u22122 as the learning rate \u03b7 decays. This rule is largely inspired by a previous theoretical work (Gu et al., 2023), which shows that the generalization benefits arise only if H = \u2126( 1\u03b7 ) when \u03b7 \u2192 0, but did not make any recommendation on how to set H . Our main contributions are:\n1. We propose the Quadratic Synchronization Rule (QSR) to simultaneously reduce the wall-clock time and improve the final test accuracy of local gradient methods. Based on the theoretical insights in Theorem 3.1, we provide a theoretical separation among data parallel SGD, Local\nSGD with H \u223c \u03b7\u22121, and Local SGD with QSR in terms of SDE approximations. We show that QSR can help reduce sharpness faster and hence improve generalization.\n2. We demonstrate with ImageNet experiments that QSR can consistently improve the final test accuracy of ResNet-152 and ViT-B over other synchronization strategies, including constantperiod and post-local schedules, and also H \u223c \u03b7\u22121 which one will expect to be optimal from the optimization perspective (Figure 1).\n3. We thoroughly validate the efficacy of QSR not only for Local SGD but also for Local AdamW, which is arguably more suitable for training large models. We also validate its efficacy for cosine, linear and step decay learning rate schedules that are commonly used in practice.\n4. We evaluate the communication efficiency of QSR on a 64-GPU NVIDIA GeForce RTX 3090 cluster. As an illustrative example, the standard data parallel AdamW takes 8.6 hours to train ViT-B for 300 epochs. With our QSR, Local AdamW cuts the training time down to 5.5 hours with even higher test accuracy."
        },
        {
            "heading": "2 OUR METHOD: QUADRATIC SYNCHRONIZATION RULE",
            "text": "In this section, we first formulate the local gradient methods and then present our Quadratic Synchronization Rule (QSR) in detail.\nLocal Gradient Methods. Given any gradient-based optimizer OPT, the corresponding local gradient method consists of multiple communication rounds. At the s-th round, each of the K workers (say the k-th) gets a local copy of the global iterate \u03b8\u0304(s), i.e., \u03b8(s)k,0 \u2190 \u03b8\u0304(s), and then performs H steps of local updates. At the h-th local step of the s-th round, which corresponds to the (sH+h)-th iteration globally, each worker gets a batch of Bloc samples (\u03be (s) k,h,1, . . . , \u03be (s) k,h,Bloc\n) from a globally shared dataset D\u0303, computes the gradient on that batch, and updates the model with optimizer OPT and learning rate \u03b7sH+h:\n\u03b8 (s) k,h+1 \u2190 OPT(\u03b8 (s) k,h, \u03b7sH+h, g (s) k,h) where g (s) k,h =\n1\nBloc Bloc\u2211 i=1 \u2207\u2113(\u03b8(s)k,h; \u03be (s) k,h,i). (1)\nAfter finishing H steps of local updates, all workers average their local models to generate the next global iterate: \u03b8\u0304(s+1) \u2190 1K \u2211K k=1 \u03b8 (s) k,H . Note that conventional local gradient methods set the synchronization period as a constant, denoted as H , throughout training. See also Algorithm 2.\nQuadratic Synchronization Rule. Given a learning rate schedule \u03b7t, t \u2208 {0, \u00b7 \u00b7 \u00b7 , T \u2212 1} that decays with time, instead of keeping H constant, we propose to dynamically increase the synchronization periodH(s) at each round s as the learning rate decreases. More specifically, if at the global iteration t we need to start a new communication round, then we set\nH(s) := max { Hbase, \u230a( \u03b1\n\u03b7t\n)2\u230b} . (2)\nHere Hbase is a constant indicating the minimum number of local steps one would like to use for each round, which should be set according to the relative cost of computation and communication. The coefficient \u03b1, termed the \u201cgrowth coefficient\u201d henceforth, is a hyperparameter controlling how fast H(s) increases as \u03b7t decreases.\nAs suggested by our later theorem 3.1, \u03b1 should be set as a small constant. In our experiments, we tune \u03b1 properly between 0.01 and 0.5 and test the effectiveness of our proposed method with Hbase = 2, 4, 8. Note that the last communication round may not finish exactly at the last iteration of the learning rate schedule. If this is the case, we force a synchronization at the last step by setting H(s) := T \u2212 t. A surprising part of our method is that we use the power 2 in the above formula (2). This choice of power 2 is inspired by the analysis in Gu et al. (2023), which suggests that setting H = \u2126( 1\u03b7 ) is beneficial for reducing the sharpness of the local landscape. Indeed, H(s) could have been set to\nH(s) := max { Hbase, \u230a( \u03b1 \u03b7t )\u03b3\u230b} for any \u03b3. However, using \u03b3 = 2 is crucial for the success of our method, and we will provide a theoretical justification of this choice in Section 3, together with empirical evidence. We also visualize the H schedule for QSR in Figure 5 in the appendix.\nDealing with Learning Rate Warmup. Many learning rate schedules use a warmup phase where the learning rate increases linearly from 0 to \u03b7max, and then decays monotonically. This warmup phase is often used to avoid the instability caused by the initial large learning rate (Goyal et al., 2017). Our rule is not directly compatible with the warmup phase, since it is designed for learning rate decay, but the learning rate increases rather than decreases in this phase. Practically, we recommend setting H(s) as the value to be used in the communication round right after the warmup."
        },
        {
            "heading": "3 THEORETICAL MOTIVATIONS OF QUADRATIC SYNCHRONIZATION RULE",
            "text": "To justify our choice of power 2, we build on the same theoretical setup as Gu et al. (2023) to analyze the Stochastic Differential Equation (SDE) approximation of SGD and Local SGD using different scalings of H with respect to \u03b7. Though the learning rate continuously decays over time in most of our experiments, it does not usually change much within a couple of epochs. Inspired by this, we take a quasistatic viewpoint: consider a significant period of time where the learning rate is relatively constant, and directly treat the learning rate as a real constant \u03b7. First, we recap Gu et al. (2023)\u2019s theory that applies to Local SGD withH \u223c \u03b7\u22121, then we show how to generalize the result to our rule where H \u223c \u03b7\u22122, leading to a stronger implicit bias towards flatter minima. Setup. Consider optimizing the loss function L(\u03b8) := E\u03be\u223cD\u0303[\u2113(\u03b8; \u03be)], where \u03b8 \u2208 Rd is the parameter vector and \u2113(\u03b8; \u03be) is the loss function for a single data sample \u03be drawn from a training set/training distribution D\u0303. We use \u03a3(\u03b8) := Cov\u03be\u223cD\u0303[\u2207\u2113(\u03b8; \u03be)] to denote the covariance matrix of the stochastic gradient \u2207\u2113(\u03b8; \u03be) at \u03b8. Following Gu et al. (2023), we make regularity assumptions on L(\u03b8),\u03a3(\u03b8) and \u2225\u2207\u2113(\u03b8; \u03be)\u22252 in Assumption E.1, and we assume that L has a manifold \u0393 of minimizers in Assumption E.2. Our analysis is based on SDE approximations near \u0393, providing a clean view of how different choices of H affect the selection of minimizers by Local SGD.\nSDE approximations of SGD and Local SGD. SDE is a powerful tool to precisely characterize the effect of noise in SGD, leading to many applications such as Linear Scaling Rule (Goyal et al., 2017). The SDE d\u03b8(t) = \u2212\u2207L(\u03b8(t))dt+ 1\u221a\nB \u03a3(\u03b8(t))1/2dWt is conventionally used in the\nliterature (Jastrze\u0328bski et al., 2017; Smith et al., 2020; Li et al., 2021b), where Wt is the standard Wiener process. In this SDE, each discrete step corresponds to a continuous time interval of length \u03b7, and the expected gradient and gradient noise become a deterministic drift term and a stochastic diffusion term, respectively. When the training proceeds to a point \u03b8(t) near a minimizer \u03b60 on the manifold \u0393, the gradient \u2207L(\u03b8(t)) is almost zero but the gradient noise 1\u221a\nB \u03a3(\u03b8(t))1/2dWt drives\nthe parameter to diffuse locally. This can be captured by a careful first-order approximation of the dynamics, leading to an Ornstein-Uhlenbeck process (Zhu et al., 2019; Li et al., 2019a; Izmailov et al., 2018). However, these rough approximations only hold for about O(\u03b7\u22121) steps, whereas neural networks in practice are usually trained for much longer.\nRecently, a series of works (Blanc et al., 2020; Damian et al., 2021; Li et al., 2021c) study the dynamics of SGD on a longer horizon. They show that higher-order terms can accumulate over time and drive this local diffusion to gradually move on the manifold \u0393. Among them, Li et al. (2021c) precisely characterized this with an SDE tracking the gradient flow projection of \u03b8(t) on \u0393, denoted as \u03a6(\u03b8(t)) (see Definition E.1). Here, \u03a6(\u03b8(t)) can be thought of as a natural \u201ccenter\u201d of the local diffusion. This SDE, termed as Slow SDE, tracks the dynamics of SGD over O(\u03b7\u22122) steps, which is much longer than the O(\u03b7\u22121) horizon for conventional SDEs. To provide a theoretical understanding of why Local SGD generalizes better than SGD, Gu et al. (2023) derived the Slow SDEs for Local SGD using the scaling H \u223c \u03b7\u22121. By comparing the Slow SDEs, they argued that Local SGD drifts faster to flatter minima than SGD. However, their analysis does not encompass the more aggressive scaling H \u223c \u03b7\u22122 recommended by our QSR. Recognizing this gap, we derive the Slow SDE for this scaling, enriching the theoretical framework for the generalization behavior of Local SGD. Below, we first present the Slow SDEs for SGD and Local SGD with H \u223c \u03b7\u22121 and H \u223c \u03b7\u22122, then we interpret why H \u223c \u03b7\u22122 may generalize better. Definition 3.1 (Slow SDE for SGD, informal, (Li et al., 2021c; Gu et al., 2023)). Given \u03b60 \u2208 \u0393, define \u03b6(t) as the solution to the following SDE with initial condition \u03b6(0) = \u03b60:\nd\u03b6(t) = P\u03b6 ( 1\u221a B \u03a3\n1/2 \u2225 (\u03b6)dWt\ufe38 \ufe37\ufe37 \ufe38\n(a) diffusion on \u0393\n\u2212 12B\u22073L(\u03b6)[\u03a3\u0302\u2662(\u03b6)]dt\ufe38 \ufe37\ufe37 \ufe38 (b) drift on \u0393\n) . (3)\nHere, P\u03b6 is a projection operator of differential forms to ensure that taking an infinitesimal step from \u03b6 \u2208 \u0393 remains on the manifold \u0393. B is the total batch size. \u03a3\u2225(\u03b6) and \u03a3\u0302\u2662(\u03b6) are certain PSD matrices related to gradient noise and Hessian. See Definition E.2 for the full definition. Definition 3.2 (Slow SDE for Local SGD with H \u223c \u03b7\u22121, informal (Gu et al., 2023)). Consider the scaling H = \u03b2/\u03b7 for some constant \u03b2. Given \u03b60 \u2208 \u0393, define \u03b6(t) as the solution to the following SDE with initial condition \u03b6(0) = \u03b60:\nd\u03b6(t) = P\u03b6 ( 1\u221a B \u03a3\n1/2 \u2225 (\u03b6)dWt\ufe38 \ufe37\ufe37 \ufe38\n(a) diffusion on \u0393\n\u2212 12B\u22073L(\u03b6)[\u03a3\u0302\u2662(\u03b6)]dt\ufe38 \ufe37\ufe37 \ufe38 (b) drift on \u0393, same as SGD \u2212K\u221212B \u22073L(\u03b6)[\u03a8\u0302(\u03b6;H\u03b7)]dt\ufe38 \ufe37\ufe37 \ufe38 (c) an extra drift term on \u0393\n) , (4)\nwhere K is the number of workers, B,\u03a3\u2225(\u03b6) and \u03a3\u0302\u2662(\u03b6) are the same as in Definition 3.1. Here, \u03a8\u0302(\u03b6;\u03b2) is a PSD matrix depending on gradient noise and Hessian. It scales with \u03b2 as lim\u03b2\u21920 \u03a8\u0302(\u03b6;\u03b2) = 0, lim\u03b2\u2192+\u221e \u03a8\u0302(\u03b6;\u03b2) = \u03a3\u0302\u2662(\u03b6). 1 See Definition E.3 for the full definition. Definition 3.3 (Slow SDE for Local SGD with QSR). Given \u03b60 \u2208 \u0393, define \u03b6(t) as the solution to the following SDE with initial condition \u03b6(0) = \u03b60:\nd\u03b6(t) = P\u03b6 ( 1\u221a B \u03a3\n1/2 \u2225 (\u03b6)dWt\ufe38 \ufe37\ufe37 \ufe38\n(a) diffusion on \u0393\n\u2212 K2B\u22073L(\u03b6)[\u03a3\u0302\u2662(\u03b6)]dt\ufe38 \ufe37\ufe37 \ufe38 (b) drift on \u0393,K times larger\n) , (5)\nwhere K,B,\u03a3\u2225(\u03b6) and \u03a3\u0302\u2662(\u03b6) are defined in Definitions 3.1 and 3.2.\nThe following approximation theorem indicates that when the learning rate \u03b7 and the growth coefficient \u03b1 for QSR are small, the above Slow SDEs closely track their discrete counterparts. The approximation theorem for QSR is new, and we defer the proof to Appendix E.2. Theorem 3.1 (Weak Approximations). Let T > 0 be a constant and \u03b6(t) be the solution to one of the above Slow SDEs with the initial condition \u03b6(0) = \u03a6(\u03b8(0)) \u2208 \u0393. Let g(\u03b8) be any C4-smooth function.\n1. (Gu et al., 2023) For SGD, let \u03b6(t) be the solution to (3). Then, max0\u2264s\u2264 T \u03b72 |E[g(\u03a6(\u03b8s))] \u2212\nE[g(\u03b6(s\u03b72))]| = O\u0303(\u03b70.25). 2. (Gu et al., 2023) For Local SGD with H = \u03b2/\u03b7 for some constant \u03b2, let \u03b6(t) be the solution to\n(4). Then, max0\u2264s\u2264 T H\u03b72 |E[g(\u03a6(\u03b8(s)))]\u2212 E[g(\u03b6(sH\u03b72))]| = O\u0303(\u03b70.25).\n3. For Local SGD with H = (\u03b1\u03b7 ) 2, where the positive constant \u03b1 is small but larger than\n\u2126(\u03b7\u03b3) for all \u03b3 > 0, let \u03b6(t) be the solution to (5). Then, max0\u2264s\u2264 T H\u03b72 |E[g(\u03a6(\u03b8(s)))] \u2212 E[g(\u03b6(sH\u03b72))]| = O(\u03b12).\nHere, O( \u00b7 ) and O\u0303(\u00b7) hide constants that are independent of \u03b1 and \u03b7 but can depend on g and T . O\u0303(\u00b7) also hides log terms.\nBy comparing the Slow SDEs, we can predict the generalization order for different scaling as QSR > {H \u223c \u03b7\u22121} > {constant H}, which we explain in detail below. Interpretation of the Slow SDEs. We first focus on the Slow SDE for SGD (3). The key component of this Slow SDE is the drift term (b), which comes from higher-order approximations of the aforementioned local diffusion that happens in O(\u03b7\u22121) steps. Viewing \u22073L(\u03b6)[\u03a3\u0302\u2662(\u03b6)] as a semi-gradient of \u27e8\u22072L(\u03b6), \u03a3\u0302\u2662(\u03b6)\u27e9 that discards the dependence of \u03b8 in \u03a3\u0302\u2662(\u03b6), we can interpret the Slow SDE as a continuous version of a semi-gradient method for reducing \u27e8\u22072L(\u03b6), \u03a3\u0302\u2662(\u03b6)\u27e9 on \u0393. Since the Hessian matrix \u22072L(\u03b6) determines the local curvature of the loss landscape, we can conclude from the Slow SDE that SGD tends to reduce sharpness and move towards flatter minimizers in O(\u03b7\u22122) steps. Reduced sharpness has been shown to yield better sample complexity bounds in specific theoretical settings. For details, we refer readers to Li et al. (2021c).\n1Given \u03b6, \u03a8\u0302(\u03b6;\u03b2) is a monotonically increasing function of \u03b2 in the eigenspace of the Hessian matrix \u22072L(\u03b6).\nNow, we turn to the Slow SDE for QSR. Compared with the SDE for SGD, it possesses a K times larger drift term, leading to much faster sharpness reduction than SGD. An intuitive explanation for why this extra drift arises is as follows. Since the local batch size is K times smaller than the global one, this local diffusion at each worker is much more significant than that in parallel SGD, thereby leading to an extra drift term in Slow SDE accumulated from higher-order terms.\nThe case of Local SGD withH = \u03b2/\u03b7 is somewhere in between QSR and SGD. Compared with the SDE for SGD, it has an extra drift term (c), where \u03b2 serves as the knob to control the magnitude of the drift term. For small \u03b2, \u03a8\u0302(\u03b6) diminishes to zero, yielding the same SDE as SGD. By contrast, as \u03b2 goes to infinity, \u03a8\u0302(\u03b6) approximates \u03a3\u0302\u2662(\u03b6), leading to the Slow SDE for QSR.\nComparison of different scalings. Based on the interpretation, keeping H constant as \u03b7 diminishes is equivalent to setting a small \u03b2 for H = \u03b2/\u03b7, making the extra drift term negligible and thus yielding nearly no generalization benefit over SGD. Conversely, the SDE for H = \u03b2/\u03b7 converges to the SDE of QSR in the limit \u03b2 \u2192 \u221e, maximizing the drift term. But in practice, \u03b2 cannot be arbitrarily large. In Theorem 3.3 of Gu et al. (2023), the distance between the iterate and \u0393 blows up as O\u0303(\u221a\u03b2\u03b7), suggesting that setting a very large \u03b2 for a not-so-small \u03b7 can blow up the loss. Therefore, the generalization performance ofH\u223c\u03b7\u22121 is expected to be worse than QSR. In summary, the order of generalization performance predicted by our theory is QSR > {H \u223c \u03b7\u22121} > {constant H}. Experimental results in Figure 2 validate that this order of generalization performance for different scalings holds not only for Local SGD but also for Local AdamW. For Local SGD we additionally have {constantH}\u2248 {parallel SGD} since parallel SGD is mathematically equivalent to Local SGD with H = 1. Apart from H \u223c \u03b7\u22121 and H \u223c \u03b7\u22122, we have also tried a more aggressive scaling, H \u223c \u03b7\u22123, but it does not provide consistent improvements over QSR. See Appendix G for more discussion."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": "In this section, we empirically demonstrate that QSR not only improves the test accuracy of local gradient methods but also reduces the wall-clock time of standard data parallel training, with a focus on the ImageNet classification task (Russakovsky et al., 2015). Our experiments include Local SGD on ResNet-152 (He et al., 2016), and Local AdamW on ViT-B with patch size 16x16 (Dosovitskiy et al., 2021). We briefly outline our training configuration below. See Appendix C for full details.\nBaselines. For QSR with base synchronization period Hbase, we benchmark their performance against two baselines running the same number of epochs: \u2460 Local SGD/AdamW with constant synchronization period H = Hbase, and \u2461 parallel SGD/AdamW. When comparing with these baselines, we mainly focus on validating that (a) QSR maintains or sometimes outperforms the communication efficiency of \u2460, thus communicating much less than \u2461, and (b) QSR improves the generalization performance of \u2460, even surpassing \u2461 in test accuracy.\nComparison with other synchronization strategies. Besides the above two baselines, other potential baselines include \u2462 Post-local SGD, \u2463 the scaling of H \u223c \u03b7\u22121, and \u2464 large batch training with batch size H \u00d7 B, which we discuss below. \u2462 is proposed for the same purpose as QSR: to improve communication efficiency and generalization together. However, it is less communication efficient than our QSR because it starts with parallel SGD and sustains this for a significant frac-\ntion of the training duration, leading to a limited reduction in communication. Also, as shown by our comparison in Figure 1(a) (also observed in Ortiz et al. 2021), its generalization benefits over SGD appear shortly after switching and diminish in the end. \u2463 is inspired by Gu et al. (2023) and may also improve generalization while reducing communication, but we have conducted a thorough comparison between QSR and \u2463 in Figure 2, demonstrating the superiority of QSR. \u2464 has the same communication efficiency as Local SGD with the same constant H (\u2460), but it has been observed to have worse test accuracy than parallel SGD/AdamW without scaling up the batch size (\u2461), which we also observe in Table 2. For the above reasons, we mainly compare with baselines \u2460 and \u2461.\nHardware. We conduct the experiments on Tencent Cloud, where each machine is equipped with 8 NVIDIA GeForce RTX 3090 GPUs. The machines are interconnected by a 25Gbps network. Since intra-machine communication speed is not substantially faster than inter-machine speed on our specific hardware, we treat each GPU as an independent worker and set the batch size on each GPU as Bloc = 256. In this paper, we use axb GPUs to denote a machines with b GPUs each.\nTraining Setup. Our experiments on ResNet-152 follow the 200-epoch recipe in Foret et al. (2021b) except that we use 5 epochs of linear learning rate warmup. For experiments on ViT-B, we follow the simple and effective 300-epoch recipe proposed in Beyer et al. (2022) with RandAugment and Mixup. We use the cosine decay unless otherwise stated. The hyperparameters (primarily learning rate and weight decay) are optimally tuned for all baselines. We explore Hbase = 2, 4 for ResNet-152 and Hbase = 4, 8 for ViT-B. This choice stems from the observation that the communication overhead for ResNet-152 is smaller than ViT-B (see Table 4). To tune the growth coefficient \u03b1 for QSR, we first fix the learning rate schedule and then search among a few values of \u03b1. The \u03b1 values we explore typically allow the training to start with Hbase, maintain H = Hbase for an initial period to optimize the training loss, and gradually increase H as \u03b7 decays in the late phase."
        },
        {
            "heading": "4.1 QSR IMPROVES GENERALIZATION",
            "text": "Through experiments spanning various batch sizes and learning rate schedules, in this subsection, we illustrate that QSR consistently enhances the generalization of gradient methods, even outperforming the communication-intensive data parallel approach.\nMain results. We first present our main results for batch size B = 4096 on 2x8 GPUs, covering Local SGD on ResNet-152 and Local AdamW on ViT-B. As shown in Table 1, QSR significantly improves the validation accuracy of local gradient methods by up to 0.8% on ResNet-152 and 1.7% on ViT-B, despite inducing higher training loss. The results support the thesis that the improvement in generalization is due to the implicit regularization of local gradient noise instead of better optimization. Noticeably, QSR surpasses the data parallel approach in validation accuracy by 0.7% on ResNet-152 and by 1.1% on ViT-B while cutting the communication volume to less than 25%. As an added benefit of increasing the synchronization interval in line with the decaying learning rate, QSR further reduces communication overhead, even halving the communication volume compared to Local AdamW with a fixed synchronization period on ViT-B.\nThe advantages of QSR are more pronounced for ViT-B compared to ResNet-152. This is probably because vision transformers are general-purpose architectures with less image-specific inductive bias than CNNs (Dosovitskiy et al., 2021; Chen et al., 2021). As a result, they may benefit more from external regularization effects, such as those induced by adding local steps.\nScaling up the batch size. In Table 2, when scaling the training up to 8x8 GPUs with total batch sizeB = 16384, we observe a drop in test accuracy for both data parallel approach and local gradient\nmethods. This generalization degradation for large batch training, which has been widely observed in the literature (Shallue et al., 2019; Jastrze\u0328bski et al., 2017; You et al., 2018), probably arises from a reduced level of gradient noise associated with increased batch size (Keskar et al., 2017b; Smith et al., 2021). While the Linear Scaling Rule for SGD (Krizhevsky, 2014; Goyal et al., 2017) and the Square Root Scaling Rule (Malladi et al., 2022; Granziol et al., 2022) for adaptive gradient methods \u2013 which increase the learning rate in proportion to the total batch size or its square root \u2013 can mitigate this degradation, they cannot fully bridge the gap. In Table 2, the test accuracy drop persists even when we tune the learning rate for all baselines. Applying QSR to local gradient methods can help reduce this generalization gap. It improves the validation accuracy of local gradient methods by up to 0.6% on ResNet-152 and 1.5% on ViT-B. This enables local gradient methods to achieve comparable validation accuracy as the data parallel approach on ResNet or outperform it by 0.8% on ViT while communicating considerably less.\nMethod and Communcation Volume\nParallel AdamW, 100% Local AdamW (H = 4), 25%\nQSR (Hbase = 4), 9.3%\nOther learning rate schedules. So far, our experiments are conducted with the cosine learning rate schedule, which is a common choice for training modern deep neural nets (Liu et al., 2021; 2022; Brown et al., 2020). To further validate the efficacy of QSR, we now investigate other popular learning rate schedules, including linear (Li et al., 2020a; Izsak et al., 2021; Leclerc et al., 2023) and step decay (He et al., 2016; Huang et al., 2017; Ma et al., 2019). See Figure 4 for a visualization of these schedules. Figure 3 presents the results for Local AdamW on ViT-B with linear decay, where the peak learning rates for baselines are tuned optimally. QSR improves the test accuracy of Local AdamW by a significant margin of 1.4%, even outperforming parallel AdamW by 0.6% while cutting the communication volume to only 9.3%. The step\ndecay scheduler divides the learning rate by factors such as 2 or 10 at some specified epochs. Given the absence of standard recipes to determine the decay points in our training setup, we derive a step decay schedule from the cosine decay by rounding its learning rate to powers of 2, which is defined as \u03b7step(t) := 2round(log2 \u03b7cos(t)). As shown in Table 3, QSR exhibits strong generalization performance with this decay schedule, enhancing the test accuracy of local gradient methods by up to 0.8% on ResNet-152 and 1.5% on ViT-B. It even surpasses the communication-intensive parallel SGD by 0.7% on ResNet and parallel AdamW by 1% on ViT."
        },
        {
            "heading": "4.2 QSR REDUCES WALL-CLOCK TIME",
            "text": "In addition to improving generalization, our original motivation for adopting local steps is to reduce communication overhead and hence reduce the wall-clock time. In this section, we confirm this for training with 2x8 and 8x8 GPUs, as shown in Table 4. See also Appendix F for our method\nof measuring the communication time. In our setup, scaling the training from 2x8 to 8x8 GPUs increases the communication overhead for both models. Notably, on 8x8 GPUs, communication accounts for almost half of the total training time for ViT-B. Since communication makes up a larger portion of the total time for ViT-B compared to ResNet-152, the speedup from QSR is more significant on ViT-B: the time is cut from 26.7 to 20.2 hours on 2x8 GPUs, and 8.6 to 5.5 hours on 8x8 GPUs. As discussed in Section 4.1, compared to the constant period local gradient method, QSR further reduces the communication cost by increasing the synchronization period in the late phase. For example, applying QSR to Local AdamW with H = 4 further reduces the time by 1 hour for ViT training on 2x8 GPUs.\nDiscussion on the choice ofHbase. As elaborated in Section 2, Hbase indicates the minimum synchronization period and should be determined based on the communication overhead. For ResNet152, given that communication only accounts for 3.3 out of 20.7 hours on 2x8 GPUs and 1.3 out of 5.7 hours on 8x8 GPUs, setting Hbase as 2 or 4 suffices to reduce the communication time to an inconsequential amount. By contrast, the communication overhead for ViT-B is more prominent, motivating us to consider larger values ofHbase, such as 4 and 8. As shown in Tables 1 and 2, Hbase introduces a tradeoff between communication efficiency and final test accuracy. For instance, when training ResNet-152 with batch size 16384, one can either choose Hbase = 2 to achieve comparable test accuracy as parallel SGD, or Hbase = 4 to further halve the communication volume at the expense of a 0.6% drop in test accuracy. One probable explanation for this accuracy drop for larger Hbase can be worse optimization in the early training phase, where the learning rate is large."
        },
        {
            "heading": "5 DISCUSSIONS AND FUTURE DIRECTIONS",
            "text": "This paper primarily focuses on relatively large models trained with long horizons, and proposes the Quadratic Synchronization Rule (QSR). As validated by our experiments, QSR effectively improves test accuracy and communication efficiency simultaneously for training large vision models (ResNet-152 and ViT-B) with quite a few hundred epochs. However, on the downside, for smaller models trained with shorter horizons, QSR may not consistently deliver noticeable generalization improvements (see Table 5). Nonetheless, training in this regime is not costly, either, making it less of a critical concern. Another limitation of our work is that the effectiveness of QSR relies on the implicit regularization effects of noise, but when training large models with unsupervised learning on massive data, regularization techniques might not be necessary to bridge the gap between the training and population loss (Vyas et al., 2023). Still, recent work (Liu et al., 2023) has found that the same pretraining loss can lead to different internal representations and thus different downstream performances. We leave it to future work to explore and design communication-efficient methods for unsupervised learning, particularly language model pretraining, that improve models\u2019 transferability to downstream tasks."
        },
        {
            "heading": "1 Introduction 1",
            "text": ""
        },
        {
            "heading": "2 Our Method: Quadratic Synchronization Rule 3",
            "text": ""
        },
        {
            "heading": "3 Theoretical Motivations of Quadratic Synchronization Rule 4",
            "text": ""
        },
        {
            "heading": "4 Experiments 6",
            "text": "4.1 QSR Improves Generalization . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n4.2 QSR Reduces Wall-clock Time . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8"
        },
        {
            "heading": "5 Discussions and Future Directions 9",
            "text": ""
        },
        {
            "heading": "A Additional Related Works 19",
            "text": ""
        },
        {
            "heading": "B PseudoCode 21",
            "text": ""
        },
        {
            "heading": "C Experimental Details 22",
            "text": "C.1 Training Details for ResNet-152 . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\nC.2 Training Details For ViT-B . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23"
        },
        {
            "heading": "D Additional Experiments on ResNet-50 24",
            "text": ""
        },
        {
            "heading": "E Supplementary Materials for Section 3 25",
            "text": "E.1 Missing Definitions and Assumptions . . . . . . . . . . . . . . . . . . . . . . . . 25\nE.2 Proof for Theorem 3.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26"
        },
        {
            "heading": "F Details for Communication Time Measurement 32",
            "text": ""
        },
        {
            "heading": "G Discussion on More Aggressive Scalings 33",
            "text": "H Comparison with Local SGD/AdamW + SWAP 35"
        },
        {
            "heading": "A ADDITIONAL RELATED WORKS",
            "text": "Advances in local gradient methods. Local gradient methods are a class of communicationefficient algorithms for distributed training. In this approach, workers update their models locally and average the model parameters every time they finish H steps of updates. Dating back to Mann et al. (2009) and Zinkevich et al. (2010), local gradient methods have been widely used to improve communication efficiency in both datacenter distributed training Zhang et al. (2014); Povey et al. (2014); Su & Chen (2015); Chen & Huo (2016) and Federated Learning (Kairouz et al., 2021; McMahan et al., 2017; Li et al., 2019b; Konec\u030cny\u0300 et al., 2016). Many variants have been proposed to facilitate the convergence speed. Examples include using control variates (Karimireddy et al., 2020), adding proximal terms to local loss functions (Li et al., 2020b), and applying adaptivity on top of each communication round (Wang et al., 2019; Reddi et al., 2020). Local gradient methods can also be readily combined with orthogonal approaches like communication compression (Basu et al., 2019) and asynchronous updates (Nadiradze et al., 2021) for further communication cost reduction.\nOptimization perspectives on selecting H . Extensive prior research has been devoted to optimizing the selection of the synchronization period H from an optimization perspective. The conventional approach sets H as a constant throughout training. In this setup, a series of studies (e.g.,Khaled et al. (2020); Stich (2018); Haddadpour et al. (2019); Yu et al. (2019)) established convergence bounds for the training loss, which typically degrade asH gets larger. leading to a trade-off between communication efficiency and model accuracy. Drawing upon these theoretical results, H should be set as the smallest value that reduces the communication cost to an acceptable level to minimize the negative impact on optimization. To better trade-off between optimization and generalization, researchers introduced various adaptive communication strategies. Kamp et al. (2014) designed a synchronization protocol controlled by the variance in model parameters. Haddadpour et al. (2019) suggested linearly increasing H as the iteration goes on. Shen et al. (2021) introduced a stagewise communication scheme that halves the learning rate \u03b7 while doubles H every time the training has finished a predefined stage. Aimed at optimizing the convergence of training loss with respect to wall-clock time, Wang & Joshi (2019) proposed a strategy that starts with infrequent communication and gradually decreases H as training progresses. Nonetheless, the effectiveness of these adaptive communication strategies has only been empirically validated on linear models or small-scale datasets like CIFAR-10/100.\nGeneralization perspectives on selecting H . While a larger H usually hurts optimization, it can sometimes improve generalization. Apart from Lin et al. (2020) that has been discussed in detail in Section 1, similar observations have been reported by Gupta et al. (2020) and Wortsman et al. (2023). Specifically, Gupta et al. (2020) introduced the Stochastic Weight Averaging in Parallel (SWAP) algorithm, which runs parallel SGD until a target training accuracy, then lets workers perform local updates with a final model averaging. Their empirical results validate SWAP\u2019s superior generalization performance over parallel SGD. When using LAMB (You et al., 2020) as the optimizer, Wortsman et al. (2023) find that complete local fine-tuning, followed by a single model averaging in the end (equivalent to setting H as the total number of iterations), outperforms the standard parallel LAMB in test accuracy under distribution shifts. Another relevant method is the \u201cmodel soup\u201d (Wortsman et al., 2022), which averages multiple models fine-tuned with different hyperparameters and turns out to beat the single model in test accuracy. Our paper focuses on designing the synchronization scheme best for generalization.\nImplicit bias of optimizers. The success of deep learning lies in its remarkable ability to generalize to unseen data, though it possesses the capacity to fit randomly labeled data (Zhang et al., 2017). A significant contributing factor to this success is the implicit bias inherent in popular optimizers like Gradient Descent (GD) and Stochastic Gradient Descent (SGD). Specifically, these optimizers favor minima that exhibit good generalization, without explicitly encoding such bias into the training loss. A lot of studies have been devoted to characterizing this implicit bias, some through the lens of margin maximization (Soudry et al., 2018b;a; Lyu & Li, 2020; Ji & Telgarsky, 2020; Chizat & Bach, 2020; Nacson et al., 2019), and some others focus on the simplicity bias from small initialization (Li et al., 2018; Razin & Cohen, 2020; Arora et al., 2019; Li et al., 2021a; Lyu et al., 2021; Razin et al., 2022; St\u00f6ger & Soltanolkotabi, 2021; Ge et al., 2021; Jin et al., 2023). The line of work most closely related to our paper interprets the implicit bias via sharpness reduction. The connection between flatter minima and better generalization is a commonly held belief that has\nbeen investigated both theoretically (Hochreiter & Schmidhuber, 1997; Neyshabur et al., 2017) and empirically (Keskar et al., 2017a; Jiang et al., 2020). Drawing on this insight, Foret et al. (2021a) introduced SAM optimizer, which delivers superior generalization performance by explicitly penalizing sharpness. Recent theoretical studies (Arora et al., 2022; Lyu et al., 2022; Damian et al., 2023; Ma et al., 2022) elucidate that GD inherently biases towards flatter regions on the loss landscape. Specifically, under some regularity conditions, they show that GD will eventually enter the \u201cEdge of Stability\u201d(Cohen et al., 2020), where the maximum eigenvalue of the loss Hessian stays around 2/learning rate, and then constantly moves towards flatter minima. Going beyond GD, another line of work studies how gradient noise in SGD helps reduce sharpness. Wu et al. (2018); Hu et al. (2017); Ma & Ying (2021) showed that gradient noise can cause training instability around sharp minima, and hence, the iterate can only settle around flat minima. Kleinberg et al. (2018); Zhu et al. (2019); Xie et al. (2021); Ibayashi & Imaizumi (2021) analyzed the escaping behavior of SGD from sharp minima. Motivated by recent empirical observations that low-loss solutions on the loss landscape are path-connected (Garipov et al., 2018; Draxler et al., 2018; Frankle et al., 2020) rather than isolated, Blanc et al. (2020); Damian et al. (2021); Li et al. (2021c) assume the existence of a minimizer manifold and show that gradient noise provably drives the iterate towards flatter minima on this manifold. Cowsik et al. (2022); Wang et al. (2023) discuss how momentum preserves or strengthens this effect. Also through the lens of sharpness reduction, the recent work by Gu et al. (2023) explains the generalization benefit of Local SGD, as discussed in Section 3. Zhu et al. (2023) elucidate that a similar implicit bias also manifests in decentralized training by making connections to certain variants of SAM."
        },
        {
            "heading": "B PSEUDOCODE",
            "text": "We present the pseudocodes for standard data parallel methods and local gradient methods below.\nAlgorithm 1: Parallel OPT: Data Parallel Methods on K Workers 1 Input: loss function \u2113(\u03b8; \u03be), initial parameter \u03b8(0) 2 Hyperparameters: total number of iterations T 3 Hyperparameters: learning rate schedule \u03b7t, t \u2208 {0, \u00b7 \u00b7 \u00b7 , T}, local batch size Bloc 4 t\u2190 0 ; // initialize the global iteration number 5 for t = 0, . . . , R\u2212 1 do 6 for each worker k do in parallel 7 (\u03be\n(s) k,t,1, . . . , \u03be (s) k,t,Bloc )\u2190 Sample() ; // sample a local batch 8 g\n(t) k \u2190 1Bloc \u2211Bloc i=1 \u2207\u2113(\u03b8(t); \u03be (t) k,i) ; // computing the local gradient\n9 end 10 g(t) \u2190 1K \u2211K k=1 g (t) k ; // All-Reduce aggregation of local gradients 11 \u03b8(t+1) \u2190 OPT(\u03b8(t), \u03b7t, g(t)) ; // update the model with optimizer OPT 12 end\nAlgorithm 2: Local OPT: Local Gradient Methods on K Workers 1 Input: loss function \u2113(\u03b8; \u03be), initial parameter \u03b8\u0304(0) 2 Hyperparameters: total number of rounds R 3 Hyperparameters: learning rate schedule \u03b7t, t \u2208 {0, \u00b7 \u00b7 \u00b7 , T}, local batch size Bloc 4 t\u2190 0 ; // initialize the global iteration number 5 for s = 0, . . . , R\u2212 1 do 6 H(s) \u2190 GetH(s) ; // get synchronization period for the current round 7 for each worker k do in parallel 8 \u03b8\n(s) k,0 \u2190 \u03b8\u0304(0) ; // maintain a local copy of the global model parameter\n9 for h = 0, . . . ,H(s) \u2212 1 do 10 (\u03be\n(s) k,h,1, . . . , \u03be (s) k,h,Bloc )\u2190 Sample() ; // sample a local batch 11 g\n(s) k,h \u2190 1Bloc \u2211Bloc i=1 \u2207\u2113(\u03b8 (s) k,h; \u03be (s) k,h,i) ; // computing the local gradient\n12 \u03b8 (s) k,h+1 \u2190 OPT(\u03b8 (s) k,h, \u03b7t+h, g (s) k,h) ; // update the local model with optimizer OPT 13 end 14 end 15 \u03b8\u0304(s+1) \u2190 1K \u2211K k=1 \u03b8 (s) k,H(s) ; // All-Reduce aggregation of local model parameters 16 t\u2190 t+H(s) ; // update the global iteration number 17 end\nSampling local batches. In Algorithms 1 and 2, Sample() returns a local batch for each worker. In our experiments, local batches are sampled without replacement at each epoch, which is standard for distributed training (Goyal et al., 2017; Lin et al., 2020; Ortiz et al., 2021). More specifically, at the beginning of each epoch, all the workers use the same random seed to draw a shared random permutation of train data points, and partition the data points evenly among the K workers. Then at each local step of each worker, Sample() sequentially takes samples from its own partition. Once there are too few remaining samples to form a complete batch, a new permutation is sampled and a new epoch starts. For our theoretical analysis, following Gu et al. (2023), we assume Sample() takes samples with replacement, i.e., the K workers are taking i.i.d. samples from the globally shared dataset/distribution. See Appendix B in Gu et al. (2023) for pseudocodes of sampling with and without replacement.\nSetting synchronization periods. In Algorithm 2, GetH(s) is a function that returns the synchronization period H(s) for the current round. Conventionally, H(s) is chosen as a fixed value, so GetH(s) always returns a constant. In this paper, we study howH(s) should change as training goes on, e.g., in QSR, GetH(s) works as specified in Section 2.\nC EXPERIMENTAL DETAILS\nThis section lists the additional experimental details omitted in the main text.\nSoftware and platform. We use Pytorch Distributed with NCCL backend to support multinode distributed training and use FFCV (Leclerc et al., 2022) to accelerate data loading of ImageNet.\nSampling scheme. We employ the \u201csampling without replacement\u201d scheme, as described in Appendix B.\nC.1 TRAINING DETAILS FOR RESNET-152\nWe generally follow the recipe in Foret et al. (2021b) to train ResNet-152. Specifically, we set the momentum as 0.9 and the weight decay \u03bb as 0.0001. For data augmentation, we employ random resized crop and random horizontal flip. We additionally use label smoothing 0.1. We adopt a local batch size Bloc = 256 through 8 gradient accumulations. Therefore, the batch size for BatchNorm is 32. This choice stems from our observation that a smaller batch size for BatchNorm enhances the test accuracy of parallel SGD. Since the BatchNorm statistics on each worker are estimated on the local model parameter, we pass 100 batches, each of size 32, to estimate the BatchNorm statistics on the global parameter before evaluation.\nTraining details for batch size 4096. We search the optimal peak learning rate \u03b7max of the cosine learning rate schedule among {0.4, 0.8, 1.6} for all baseline algorithms, i.e., parallel SGD and Local SGD with constant synchronization period H = 2 and H = 4. The learning rate yielding the highest final test accuracy is selected. We find that \u03b7max = 0.8 is optimal for all the baseline algorithms. For QSR with Hbase = 2 and Hbase = 4, we directly set \u03b7max = 0.8. We search \u03b1 among {0.2, 0.25, 0.3}, and choose \u03b1 = 0.2 and 0.25 for QSR with Hbase = 2 and 4 respectively. Regarding other communication strategies in Figure 1(a), we set the switching point at epoch 100 and employ H = 8 for Post-local SGD. For H = \u03b2/\u03b7, we search \u03b2 among {0.6, 0.8, 1, 1.2}, finally selecting \u03b2 = 1.\nTraining details for batch size 16384. The hyperparameter tuning procedure for B = 16384 is similar to that of B = 4096. We search \u03b7max among {0.8, 1.6, 3.2} for all baseline algorithms, including SGD and Local SGD with constant synchronization period Hbase = 2 and Hbase = 4. We find that \u03b7max = 3.2 yields the highest final test accuracy for all of them. However, for QSR, we find that peak learning rate \u03b7max = 3.2 is excessively large, causing the dynamic scheduling to be triggered too late in the training process. This late triggering leaves insufficient training time for the training to fully leverage the generalization benefits introduced by local steps. Consequently, we set \u03b7max = 1.6 for QSR with Hbase = 2 and 4. We search \u03b1 among {0.2, 0.25, 0.3}, and choose \u03b1 = 0.2 for both QSR with Hbase = 2 and 4.\nTraining details for the step decay scheduler. In our experiments with step decay, we employ a batch size of 4096. Given that our step decay scheduler is derived from the cosine decay, we only need to specify the weight decay \u03bb, and peak learning rate \u03b7max. These are set identically to the values used in our cosine decay experiments. For QSR, we search the growth coefficient \u03b1 among {0.2, 0.3} and choose 0.2 for both Hbase = 2 and 4. Training details for experiments in Appendix H. For Local SGD + SWAP experiments in Appendix H, we use the cosine learning rate schedule with peak learning rate \u03b7max = 0.8. We start with Local SGD with a constant synchronization period H = 4 and explore the switching point t0 from {175, 180, 185, 190}.\nC.2 TRAINING DETAILS FOR VIT-B\nFor training ViT-B, we primarily follow the 300- epoch recipe proposed by Beyer et al. (2022). Specifically, we replace the [cls] token of the original ViT token with global average pooling and use fixed 2D sin-cos position rather than learned positional embeddings. Our implementation of the model architecture follows the high-starred repository 2 by Phil Wang. Apart from random resized crop and random horizontal flip, we employ RandAugment with parameters (2, 10) and MixUp with a coefficient of 0.2 for data augmentation. Different from Beyer et al. (2022), we use a larger batch size (B = 4096 or 16384 as opposed to their 1024) and use AdamW instead of Adam.\nAs for gradient clipping, we set it as 1 for standard AdamW following Beyer et al. (2022); Dosovitskiy et al. (2021) and Chen et al. (2021). However, for Local AdamW, the smaller batch size locally leads to larger gradient noise and, hence larger gradient norm for local updates. This calls for an increase in the gradient clipping threshold. We find that the\ntraining process remains stable even when we remove gradient clipping (equivalent to setting the clipping threshold to +\u221e) for most of the hyperparameter configurations we tested. For ease of tuning, we choose to turn off gradient clipping for Local AdamW unless otherwise stated.\nTraining details for batch size 4096. We use 10k iterations for learning rate warmup following (Beyer et al., 2022; Dosovitskiy et al., 2021; Chen et al., 2021). For parallel AdamW and Local AdamW (H = 4), we explore combinations of \u03b7max and weight decay \u03bb from the grid {0.05, 0.1}\u00d7 {0.004, 0.008, 0.016}. To optimize the final test accuracy, we select \u03b7max = 0.008, \u03bb = 0.1 for parallel AdamW and \u03b7max = 0.008, \u03bb = 0.05 for Local AdamW (H = 4). For Local AdamW (H = 8), keeping \u03bb = 0.05, we conduct a grid search for \u03b7max among {0.004, 0.008, 0.016} and choose \u03b7max = 0.008. For QSR withHbase = 4 and 8, we directly use \u03b7max = 0.008 and \u03bb = 0.05. To optimize \u03b1, we search among {0.015, 0.0175, 0.02} and find \u03b1 = 0.0175 works best for both QSR with Hbase = 4 and 8. Regarding the communication strategy of H = \u03b2/\u03b7 in Figure 1(b), we explore \u03b2 among {0.025, 0.03, 0.035, 0.04}, settling on \u03b2 = 0.03. In Figure 5, we also visualize the H schedule for Local AdamW with a constant synchronization period and with QSR.\nTraining details for batch size 16384. To keep the same portion of the total budget for learning rate warmup as B = 4096, we set the warmup iterations to 2.5k. We set \u03bb as 0.1 and 0.05 for parallel AdamW and Local AdamW, respectively. We search for the optimal \u03b7max among {0.004, 0.008, 0.016} and select \u03b7max = 0.004 for parallel AdamW, \u03b7max = 0.016 for Local AdamW withH = 4 and 8. We adopt the same \u03bb and \u03b7max as Local AdamW for QSR. For QSR with Hbase = 4, we search for the optimal \u03b1 among {0.015, 0.0175, 0.02} and choose \u03b1 = 0.0175. For QSR with Hbase = 8, we search for the optimal \u03b1 among {0.01, 0.0175}, finally picking \u03b1 = 0.01. Training details for linear and step decay schedulers. For both step and linear decay schedulers, we employ a batch size of 4096. For the step decay scheduler, the peak learning rate \u03b7max and weight decay \u03bb are set identically to the values used in our cosine decay experiments. We search the growth coefficient \u03b1 for QSR among {0.015, 0.0175} and choose 0.015 for both Hbase = 4 and 8. For linear decay, we use the same weight decay as our cosine decay experiments. We explore \u03b7max values from {0.004, 0.008, 0.016} for baselines, finally picking \u03b7max = 0.008 for parallel AdamW and \u03b7max = 0.016 for Local AdamW. For QSR, we adopt the same \u03b7max and \u03b1 as in our cosine decay experiments. Additionally, we add a gradient clipping threshold of 4 for Local AdamW with a constant synchronization period to stabilize training.\n2https://github.com/lucidrains/vit-pytorch\nTraining details for experiments in Appendix G. For the experiments in Table 6, we employ the same weight decay \u03bb and peak learning rate \u03b7max as used in the cosine schedule. Specifically, we set \u03bb = 0.1, \u03b7max = 0.008 for parallel AdamW and \u03bb = 0.05, \u03b7max = 0.008 for Local AdamW. In Table 6(a), for the cubic rule, we search \u03c1 among {0.0025, 0.005, 0075, 0.01} and opt for \u03c1 = 0.0075, which gives the highest test accuracy. For QSR, we adopt the same \u03b1 value, 0.0175, as in our cosine decay experiments. In Table 6(b), we set \u03c1 = 0.0075 and \u03b1 = 0.0175 for the cubic rule and QSR, respectively, which are optimal for the original cosine decay schedule, as indicated by Figure 6. As mentioned in Section 2, the final synchronization period may be truncated. Specifically, workers are forced to synchronize at the last iteration if the last synchronization period exceeds the remaining iterations. However, the modified cosine schedule experiments seek to validate that the cubic rule can produce an overly large H when the learning rate is constant. To prevent the truncation from distorting the results, we present the test accuracy at the conclusion of the last full synchronization period, which is not truncated, for both scalings.\nTraining details for experiments in Appendix H. For Local AdamW + SWAP experiments in Figure 9(b), we use the cosine learning rate schedule with peak learning rate \u03b7max = 0.008 and weight decay \u03bb = 0.05. We start with Local AdamW with a constant synchronization period H = 4 and explore the switching point t0 from {220, 240, 260, 280}."
        },
        {
            "heading": "D ADDITIONAL EXPERIMENTS ON RESNET-50",
            "text": "Our paper primarily focuses on training relatively large models with long horizons and proposes QSR to effectively improve the generalization while saving communication. However, on the flip side, QSR may not always yield noticeable generalization benefits for smaller models trained with shorter horizons. As shown in Table 5, for the 90-epoch training of ResNet-50 with cosine learning rate decay, the generalization benefit of QSR over Local SGD with a constant communication period is negligible. Nonetheless, training in this regime is not costly, either, making it less of a critical concern. Specifically, completing this 90-epoch training of ResNet-50 requires only 6.6 hours on a single machine equipped with 8 NVIDIA GeForce RTX 3090 GPUs. In comparison, the 300-epoch training of ViT investigated in the main text necessitates over 50 hours on the same setup."
        },
        {
            "heading": "E SUPPLEMENTARY MATERIALS FOR SECTION 3",
            "text": "E.1 MISSING DEFINITIONS AND ASSUMPTIONS\nFor a function F : Rd \u2192 Rd, we use \u2202F (\u03b8) to denote its Jacobian at \u03b8 and use \u22022F (\u03b8) to denote the second order derivative at \u03b8. For any matrix M \u2208 Rd\u00d7d, \u22022F (\u03b8)[M ] = \u2211i\u2208[d]\u27e8\u22022Fi\u2202\u03b82 ,M\u27e9ei where ei is the i-th vector of the standard basis. For convenience, we write \u22022(\u2207L)(\u03b8)[M ] as \u22073L(\u03b8)[M ]. Assumption E.1. Following Gu et al. (2023), we assume that L(\u03b8) and \u03a3(\u03b8)1/2 are C\u221e-smooth on Rd. We also assume that \u2225\u2207\u2113(\u03b8; \u03be)\u22252 is uniformly bounded for all \u03b8 and \u03be. Assumption E.2. \u0393 is a C\u221e-smooth, (d \u2212m)-dimensional compact submanifold of Rd such that any \u03b6 \u2208 \u0393 is a local minimizer of L and rank(\u22072L(\u03b6)) = m. Additionally, there exists an open neighborhood U of \u0393 such that \u0393 = argmin\u03b8\u2208UL(\u03b8).\nAssumption E.2 is motivated by recent empirical observations that low-loss solutions on the loss landscape are not isolated but path-connected (Garipov et al., 2018; Draxler et al., 2018; Frankle et al., 2020). It is also adopted by Li et al. (2021c); Lyu et al. (2022); Gu et al. (2023).\nDefinition E.1 (Gradient Flow Projection). Fix \u03b8null /\u2208 \u0393. For x \u2208 Rd, the gradient flow starting from x is the solution to dx(t)dt = \u2212\u2207L(x(t)) with the initial conditionx(0) = x. The gradient flow projection of x is defined as \u03a6(x) := limt\u2192+\u221e x(t) if the limit exists and belongs to \u0393. Otherwise, \u03a6(x) := \u03b8null.\nDefinition E.2 (Slow SDE for SGD, formal). Given \u03b60 \u2208 \u0393, define \u03b6(t) as the solution to the following SDE with initial condition \u03b6(0) = \u03b60:\nd\u03b6(t) = P\u03b6 ( 1\u221a B \u03a3\n1/2 \u2225 (\u03b6)dWt\ufe38 \ufe37\ufe37 \ufe38\n(a) diffusion on \u0393\n\u2212 12B\u22073L(\u03b6)[\u03a3\u0302\u2662(\u03b6)]dt\ufe38 \ufe37\ufe37 \ufe38 (b) drift on \u0393\n) . (6)\nHere, for any \u03b6 \u2208 \u0393, P\u03b6 is a projection operator that maps any differential form AdWt + bdt in It\u00f4 calculus to \u2202\u03a6(\u03b6)AdWt + ( \u2202\u03a6(\u03b6)b+ 12\u2202 2\u03a6(\u03b6)[AA\u22a4] ) , which guarantees \u03b6 to remain on the manifold after taking such an infinitesimal step. B is the total batch size. \u03a3\u2225(\u03b6) := \u2202\u03a6(\u03b6)\u03a3(\u03b6)\u2202\u03a6(\u03b6) is the covariance matrix of gradient noise projected onto the tangent space of \u03b6 at \u0393, and \u03a3\u0302\u2662(\u03b6) is the noise covariance in the rest, with coordinates rescaled in the eigenbasis {(\u03bbi,vi)}di=1 of\u22072L(\u03b6):\n\u03a3\u0302\u2662(\u03b6) := \u2211 i,j:(\u03bbi \u0338=0)\u2228(\u03bbj \u0338=0) 1 \u03bbi+\u03bbj \u2329 \u03a3(\u03b6)\u2212\u03a3\u2225(\u03b6),viv\u22a4j \u232a viv \u22a4 j .\nDefinition E.3 (Slow SDE for Local SGD with H \u223c \u03b7\u22121, formal). Consider the scaling H = \u03b2/\u03b7 for some constant \u03b2. Given \u03b60 \u2208 \u0393, define \u03b6(t) as the solution to the following SDE with initial condition \u03b6(0) = \u03b60:\nd\u03b6(t) = P\u03b6 ( 1\u221a B \u03a3\n1/2 \u2225 (\u03b6)dWt\ufe38 \ufe37\ufe37 \ufe38\n(a) diffusion on \u0393\n\u2212 12B\u22073L(\u03b6)[\u03a3\u0302\u2662(\u03b6)]dt\ufe38 \ufe37\ufe37 \ufe38 (b) drift on \u0393, same as SGD \u2212K\u221212B \u22073L(\u03b6)[\u03a8\u0302(\u03b6;H\u03b7)]dt\ufe38 \ufe37\ufe37 \ufe38 (c) an extra drift term on \u0393\n) , (7)\nwhere K is the number of workers, B,\u03a3\u2225(\u03b6) and \u03a3\u0302\u2662(\u03b6) are the same as in Definition 3.1. Here, \u03a8\u0302(\u03b6;\u03b2) is a PSD matrix depending on gradient noise and Hessian defined as follows:\n\u03a8\u0302(\u03b6) := \u2211 i,j:(\u03bbi \u0338=0)\u2228(\u03bbj \u0338=0) \u03c8(\u03b7H\u00b7(\u03bbi+\u03bbj)) \u03bbi+\u03bbj \u2329 \u03a3\u2662(\u03b6),viv \u22a4 j \u232a viv \u22a4 j , (8)\nwhere {vi}di=1 is a set of eigenvectors of \u22072L(\u03b6) that forms an orthonormal eigenbasis, and \u03bb1, . . . , \u03bbd are the corresponding eigenvalues. Additionally, \u03c8(x) := e \u2212x\u22121+x x for x \u0338= 0 and \u03c8(0) = 0.\nNotice that \u03c8(x) monotonically increases in x and has the limit limx\u21920 \u03c8(x) = 0 and limx\u2192\u221e \u03c8(x) = 1. Therefore, given \u03b6, \u03a8\u0302(\u03b6;\u03b2) is a monotonically increasing function of \u03b2 in the eigenspace of the Hessian matrix\u22072L(\u03b6).\nE.2 PROOF FOR THEOREM 3.1\nWe consider the asymptotics that \u03b7 \u2192 0, \u03b1 \u2192 0 and \u03b1 = \u2126(\u03b7\u03b3) for all \u03b3 > 0. We use big-O notation to hide constants independent of \u03b7, \u03b1, and use big-O\u0303 notations to hides constants independent of \u03b7, \u03b1 and also polylog factors of \u03b7, \u03b1. We define \u03d5(s) := \u03a6(\u03b8\u0304(s)) and let Rtot := \u230a TH\u03b72 \u230b = \u230a T\u03b12 \u230b be the total number of rounds.\nProof outline. The general framework of our proof follows (Li et al., 2019a), which demonstrates the close tracking between SGD iterates and the conventional SDE by examining the moments of parameter changes over a small observation interval \u03b7. However, their analysis is not directly applicable to our case. Their SDE approximation is only valid for O(\u03b7\u22121) steps while our QSR involves multiple communication rounds, each with O(\u03b7\u22122) steps. To tackle this challenge, we treat each round as a continuous-time observation interval of length \u03b12, and then establish that the moments of changes in the manifold projection of Local SGD and the corresponding slow SDE (5), specifically the moments of \u03d5(s+1) \u2212 \u03d5(s) and \u03b6((s+ 1)\u03b12)\u2212 \u03b6(s\u03b12), are closely aligned. Notably, though the results in (Gu et al., 2023) serve as a building block to compute the moments of \u03d5(s+1) \u2212 \u03d5(s) in Lemmas E.1 to E.3, their analysis is not trivially extendable to QSR. This is because their analysis depends on the condition H\u03b7 = O(1), and many bounds therein explode as H\u03b7 \u2192 \u221e, e.g., Theorem 3.3, Lemmas I.14 and I.16. In the context of QSR, where H\u03b7 = \u03b12\u03b7 goes to infinity as \u03b7 approaches 0, the condition H\u03b7 = O(1) is violated, rendering the analysis in Gu et al. (2023) ineffective for QSR.\nIn the following lemma, we present equivalent forms of (3), (4) and (5) that are less intuitive but more friendly to mathematical analysis.\nTheorem E.1. Equations (3), (4), (5) can be rewritten as the following SDEs, respectively:\nd\u03b6(t) = 1\u221a B \u2202\u03a6(\u03b6)\u03a3(\u03b6)1/2dWt + 1 2B \u22022\u03a6(\u03b6)[\u03a3(\u03b6)]dt, (9)\nd\u03b6(t) = 1\u221a B \u2202\u03a6(\u03b6)\u03a3(\u03b6)1/2dWt + 1 2B \u22022\u03a6(\u03b6)[\u03a3(\u03b6) + (K \u2212 1)\u03a8(\u03b6)]dt, (10)\nd\u03b6(t) = 1\u221a B \u2202\u03a6(\u03b6)\u03a3(\u03b6)1/2dWt + K 2B \u22022\u03a6(\u03b6)[\u03a3(\u03b6)]dt. (11)\nProof. Directly apply Lemmas I.1 to I.5 of Gu et al. (2023), and we have this theorem.\nBased on Gu et al. (2023)\u2019s analysis, below we compute the moments of \u03d5(s+1) \u2212 \u03d5(s) through a series of lemmas. Then, we follow Gu et al. (2023)\u2019s method of moments to derive the SDE approximation.\nLemma E.1. For any round s \u2264 Rtot and any worker k \u2208 [K], if \u03d5(s) \u2208 \u0393, then it holds with probability at least 1 \u2212 \u03b4, where \u03b4 = O(poly(\u03b7)), that \u03a6(\u03b8(s)k,H) \u2208 \u0393 and \u2225\u03b8 (s) k,H \u2212 \u03a6(\u03b8 (s) k,H)\u22252 =\nO( \u221a \u03b7 log 1\u03b7\u03b4 ).\nProof. The key insight is that the dynamics of each worker before averaging in each round is just the standard SGD with a smaller batch size, Bloc. Since the distance bound to \u0393, Theorem 3.3 in Gu et al. (2023), also applies to SGD by taking K \u2032 = 1 and H \u2032 = 1\u03b7 , we can apply this result to obtain that \u2225\u03b8(s)k,H \u2212 \u03a6(\u03b8 (s) k,H)\u22252 = O( \u221a \u03b7 log 1\u03b7\u03b4 ).\nBefore computing the moments of the change in manifold projection for each worker \u03a6(\u03b8(s)k,H) \u2212 \u03d5(s), we introduce Preliminary Lemmas E.1 and E.2. Specifically, the It\u00f4-Taylor expansion lemma E.1 is a straightforward application of Lemma B.7 of (Malladi et al., 2022) on a bounded set. Preliminary Lemma E.2 is adapted from Lemma 26 of (Li et al., 2019a).\nLet X(t) be the solution to the SDE dX(t) = b(X(t))dt+ \u03c3(X(t))dWt, where b(\u00b7) : Rd \u2192 Rd is the drift function and \u03c3(\u00b7) : Rd \u2192 Rd\u00d7d is the diffusion matrix. Both b(\u00b7) and \u03c3(\u00b7) belong to C4. Let S be a bounded invariant set of the SDE. That is, if X(0) \u2208 S, for any t \u2265 0, X(t) \u2208 S almost surely. Let \u03b7e be the \u201ceffective learning rate\u201d, which can be viewed as the length of the continuous-time observation interval for X(t). Then we have the following lemma. Preliminary Lemma E.1 (It\u00f4-Taylor expansion). Let g : Rd \u2192 R be any C4-smooth function. Define\nAg(x) := \u2211 i\u2208[D] bi(x)\u2202ig(x) + 1 2 \u2211 i,j\u2208[D] \u2211 l\u2208[D] \u03c3i,l(x)\u03c3l,j(x)  \u22022i,jg(x). (12) Given X(t) = x \u2208 S, there exists a constant C independent of \u03b7e such that\n|E[g(X(t+ \u03b7e))\u2212 g(x)\u2212 \u03b7eAg(x)]| \u2264 C\u03b72e\nProof. WLOG, we prove the case for t = 0. Due to the Markovian property of It\u00f4 processes, the same proof can be done for any t > 0 by a time shift. Give X(0) = x \u2208 S, by It\u00f4\u2019s lemma,\ng(X(\u03b7e)) = g(x) + \u222b \u03b7e 0 Ag(X(s))ds+ \u222b \u03b7e 0 \u27e8\u039bg(X(s)),dWs\u27e9 ,\nwhere \u039b(x) := \u03c3(x)\u22a4\u2207g(x).\nFurther apply It\u00f4\u2019s lemma to Ag(X(s)) and we have\ng(X(\u03b7e)) = g(x) + \u222b \u03b7e 0 ( Ag(x) + \u222b s 0 A2g(X(r))dr + \u222b s 0 \u27e8\u039bAg(X(r)),dWr\u27e9 ) ds\n+ \u222b \u03b7e 0 \u27e8\u039bg(X(s)),dWs\u27e9\n= g(x) + \u03b7eAg(x) + \u222b \u03b7e 0 \u222b s 0 A2g(X(r))drds\n+ \u222b \u03b7e 0 \u222b s 0 \u27e8\u039bAg(X(r)),dWr\u27e9ds+ \u222b \u03b7e 0 \u27e8\u039bg(X(s)),dWs\u27e9 .\nTake expectation on both sides, and the last two terms become zero: Eg(X(\u03b7e)) = g(x) + \u03b7eAg(x) + \u222b \u03b7e 0 \u222b s 0 A2g(X(r))drds.\nSince X(s) belongs to the bounded set S, there exists a constant C independent of \u03b7e such that |A2g(y)| \u2264 C for all y \u2208 S. Therefore,\n|E[g(X(\u03b7e))\u2212 g(x)\u2212 \u03b7eAg(x)]| \u2264 C\u03b72e . Preliminary Lemma E.2 (Adaptation of Lemma 26 in (Li et al., 2019a)). Given X(t) = x \u2208 S , denote the change in X(s) over time interval \u03b7e as \u2206\u0303(x, t, \u03b7e) := X(t + \u03b7e) \u2212 x. Then, for all x \u2208 S and t \u2265 0, there exists a constant C \u2032 independent of \u03b7e such that\nE[ n+1\u220f j=1 \u2223\u2223\u2223\u2206\u0303ij (x, t, \u03b7e)\u2223\u2223\u2223] \u2264 C \u2032\u03b7 n+12e , \u22001 \u2264 i1, \u00b7 \u00b7 \u00b7 , in+1 \u2264 d, where n \u2265 1.\nProof. WLOG, we prove the case for t = 0. Due to the Markovian property of It\u00f4 processes, the same proof can be done for any t > 0 by a time shift. Denote \u2206\u0303(x) := \u2206\u0303(x, 0, \u03b7e) for brevity. By definition,\n\u2206\u0303(x) = \u222b \u03b7e 0 b(X(s))ds+ \u222b \u03b7e 0 \u03c3(X(s))dWs.\nBy triangle inequality, for all i \u2208 [d],\u2223\u2223\u2223\u2206\u0303i(x)\u2223\u2223\u2223 \u2264 \u2225\u2225\u2225\u2225\u222b \u03b7e 0 b(X(s))ds \u2225\u2225\u2225\u2225 2 + \u2225\u2225\u2225\u2225\u222b \u03b7e 0 \u03c3(X(s))dWs \u2225\u2225\u2225\u2225 2 .\nTherefore,\nE[ n+1\u220f j=1 \u2223\u2223\u2223\u2206\u0303ij (x)\u2223\u2223\u2223] \u2264 (E\u2225\u2225\u2225\u2225\u222b \u03b7e 0 b(X(s))ds \u2225\u2225\u2225\u2225 2 + E \u2225\u2225\u2225\u2225\u222b \u03b7e 0 \u03c3(X(s))dWs \u2225\u2225\u2225\u2225 2 )n+1\n\u2264 2n E \u2225\u2225\u2225\u2225\u222b \u03b7e\n0\nb(X(s))ds \u2225\u2225\u2225\u2225 2\ufe38 \ufe37\ufe37 \ufe38\nT1\n n+1 + 2n E \u2225\u2225\u2225\u2225\u222b \u03b7e\n0\n\u03c3(X(s))dWs \u2225\u2225\u2225\u2225 2\ufe38 \ufe37\ufe37 \ufe38\nT2\n n+1 .\nBy triangle inequality,\nT1 \u2264 E [\u222b \u03b7e\n0\n\u2225b(X(s)\u22252ds ] .\nBy Cauchy-Schwarz inequality and It\u00f4\u2019s isometry, T2 \u2264 \u221a E \u2225\u2225\u2225\u2225\u222b \u03b7e\n0\n\u03c3(X(s))dWs \u2225\u2225\u2225\u22252 2 = \u221a E \u222b \u03b7e 0 tr[\u03c3(X(s)\u22a4\u03c3(X(s))]ds.\nSince X(s) \u2208 S almost surely and S is a bounded set, there exists constants C1 and C2 such that T1 \u2264 C1\u03b7e, T2 \u2264 C2\u03b70.5e . Substituting the bounds for T1 and T2 back, we have the lemma.\nLemma E.2. For any round s \u2264 Rtot and any worker k \u2208 [K], given \u03d5(s) \u2208 \u0393, then\nE[\u03a6(\u03b8(s)k,H)\u2212 \u03d5(s) | \u03d5(s)] = \u03b12\n2Bloc \u22022\u03a6(\u03d5(s))[\u03a3(\u03d5(s))] +O(\u03b14), (13)\nE [ (\u03a6(\u03b8\n(s) k,H)\u2212 \u03d5(s))(\u03a6(\u03b8 (s) k,H)\u2212 \u03d5(s))\u22a4 | \u03d5(s)\n] = \u03b12\nBloc \u03a3\u2225(\u03d5\n(s)) +O(\u03b14), (14)\nE [ (\u03a6(\u03b8\n(s) k,H)\u2212 \u03d5(s))\u22973 | \u03d5(s)\n] = O(\u03b14), (15)\nE [ \u2225\u03a6(\u03b8(s)k,H)\u2212 \u03d5(s)\u222562 | \u03d5(s) ] = O(\u03b16). (16)\nProof. Again, the key insight is that the dynamics of each worker before averaging in each round is just the standard SGD with a smaller batch size, Bloc. Since the SDE approximation theorem for Local SGD, Theorem 3.2 in Gu et al. (2023), also applies to SGD by taking K \u2032 = 1 and H \u2032 = 1\u03b7 , we can apply this result to obtain that, for any C4-smooth function g(\u03b8), it holds for \u03b6 defined in (9) with the initial condition \u03b6(0) = \u03d5(s) that\n|E[g(\u03a6(\u03b8(s)k,H))]\u2212 E[g(\u03b6(T \u2032))]| = O\u0303(\u03b70.25), (17)\nwhere T \u2032 = \u03b12 is the continuous-time observation interval.\nTo establish a connection between the moments of \u03a6(\u03b8(s)k,t) \u2212 \u03d5(s) and those of \u03b6(T \u2032) \u2212 \u03d5(s), we can let the function g(\u03b8) to take specific forms, each returning a single coordinate of \u03b8 \u2212 \u03d5(s), (\u03b8 \u2212 \u03d5(s))(\u03b8 \u2212 \u03d5(s))\u22a4, (\u03b8 \u2212 \u03d5(s))\u22973 and \u2225\u03b8 \u2212 \u03d5(s)\u222562. For example, to relate \u03a6(\u03b8(s)k,H) \u2212 \u03d5(s) to \u03b6(T \u2032) \u2212 \u03d5(s), let g(\u03b8) = \u27e8e1,\u03b8\u27e9 where e1 = (1, 0, \u00b7 \u00b7 \u00b7 , 0)\u22a4. Substitute g into (17), and we get | \u2329 e1,E[\u03a6(\u03b8(s)k,H)\u2212 \u03d5(s)]\u2212 E[\u03a6(\u03b6(T \u2032))\u2212 \u03d5(s)] \u232a | = O\u0303(\u03b70.25). We can obtain the same results\nfor all coordinates by letting g(\u03b8) = \u27e8ei,\u03b8\u27e9 for all i \u2208 [D]. Therefore, |E[\u03a6(\u03b8(s)k,H) \u2212 \u03d5(s)] \u2212 E[\u03b6(T \u2032)\u2212\u03d5(s)]| = O\u0303(\u03b70.25). Similarly, we can show that the LHS of (14) to (16) are only changed by O\u0303(\u03b70.25) = o(poly(\u03b1)) when replacing \u03a6(\u03b8(s)k,H) with \u03b6(T \u2032).\nThen, it suffices to compute the moments for \u03b6(T \u2032) and verify that they match the RHS of (13) to (16). Since \u0393 is compact and invariant for the SDE (11) (Lemma I.39 in (Gu et al., 2023)), we can apply the It\u00f4-Taylor expansion in Preliminary Lemma E.1 with \u03b7e = \u03b12, X(t) = \u03b6(t),\nb(\u03b6) = K2B\u2202 2\u03a6(\u03b6)[\u03a3(\u03b6)] and \u03c3(\u03b6) = \u221a 1 2B\u2202\u03a6(\u03b6)\u03a3 1/2(\u03b6).\nTo obtain the first moment (13), let g(\u03b6) = \u2329 e1, \u03b6 \u2212 \u03d5(s) \u232a and substitute it into (12). By Preliminary Lemma E.1, we have \u2223\u2223\u2223E[\u03b61(T \u2032)]\u2212 b1(\u03d5(s))\u2223\u2223\u2223 = O(T \u20322) = O(\u03b14). We can repeat this process for all coordinates of \u03b6(T \u2032) to obtain\nE[\u03b6(T \u2032)\u2212 \u03d5(s) | \u03d5(s)] = \u03b1 2\n2Bloc \u22022\u03a6(\u03d5(s))[\u03a3(\u03d5(s))] +O(\u03b14), (18)\nand thus (13). For the second moment (14), define g(i,j)(\u03b6) = \u2329 Mi,j , (\u03b6 \u2212 \u03d5(s))(\u03b6 \u2212 \u03d5(s))\u22a4 \u232a , where Mi\u2032,j\u2032 ={\n1, (i\u2032, j\u2032) = (i, j), 0, otherwise . Since \u2202i\u2032g(i,j)(\u03b6) = 0 for all i\u2032, the first term of Ag(i,j)(\u03b6) vanishes. It suf-\nfices to compute the second term. When i = j, \u22022i\u2032,j\u2032g (i,i)(\u03b6) =\n{ 2, (i\u2032, j\u2032) = (i, i)\n0, otherwise . Therefore,\nAg(i,i)(\u03b6) = \u2211 l\u2208[D] \u03c3i,l(\u03b6)\u03c3l,i(\u03b6), \u2200i \u2208 [D]. (19)\nWhen i \u0338= j, \u22022i\u2032,j\u2032g(i,j)(\u03b6) = { 1, (i\u2032, j\u2032) \u2208 {(i, j), (j, i)} 0, otherwise . Therefore,\nAg(i,j)(\u03b6) = \u2211 l\u2208[D] \u03c3i,l(\u03b6)\u03c3l,j(\u03b6), i \u0338= j. (20)\nCombining (19) and (20) and noticing that g(i,j)(\u03d5(s)) = 0 for all i, j, we have\nE[(\u03b6(T \u2032)\u2212 \u03d5(s))(\u03b6(T \u2032)\u2212 \u03d5(s))\u22a4 | \u03d5(s)] = \u03b1 2\nB \u03a3\u2225(\u03d5\n(s)) +O(\u03b14), (21)\nand thus (14). For the third moment (15), define g(i,j,l)(\u03b6) = \u2329 ei \u2297 ej \u2297 el, (\u03a6(\u03b8(s)k,H)\u2212 \u03d5(s))\u22973 \u232a . Noticing that \u2202i\u2032g (i,j,l)(\u03d5(s)) = 0 for all i\u2032 and \u22022i\u2032,j\u2032g (i,j,l)(\u03d5(s)) = 0 for all (i\u2032, j\u2032), we have\nE [ (\u03b6(T \u2032)\u2212 \u03d5(s))\u22973 | \u03d5(s) ] = O(\u03b14), (22)\nand thus (15).\nFinally, by directly applying Preliminary Lemma E.2, we have E [ \u2225\u03b6(T \u2032)\u2212 \u03d5(s)\u222562 | \u03d5(s) ] = O(\u03b16) (23)\nand thus (16).\nNow we are ready to compute the moments for \u03d5(s+1) \u2212 \u03d5(s) at each round: Lemma E.3. For any round s \u2264 Rtot, given \u03d5(s) \u2208 \u0393, then\nE[\u03d5(s+1) \u2212 \u03d5(s) | \u03d5(s)] = \u03b1 2\n2Bloc \u22022\u03a6(\u03d5(s))[\u03a3(\u03d5(s))] +O(\u03b14), (24)\nE [ (\u03d5(s+1) \u2212 \u03d5(s))(\u03d5(s+1) \u2212 \u03d5(s))\u22a4 | \u03d5(s) ] = \u03b12\nB \u03a3\u2225(\u03d5\n(s)) +O(\u03b14), (25) E [ \u2225\u03d5(s+1) \u2212 \u03d5(s)\u222562 | \u03d5(s) ] = O(\u03b16). (26)\nProof. Let \u22061 := 1K \u2211K k=1 \u03a6(\u03b8 (s) k,H)\u2212 \u03d5(s). By Lemma E.2,\nE[\u22061] = 1\nK K\u2211 k=1 E[\u03a6(\u03b8(s)k,H)\u2212 \u03d5(s)]\n= \u03b12\n2Bloc \u22022\u03a6(\u03d5(s))[\u03a3(\u03d5(s))] +O(\u03b14),\nE[\u22061\u2206\u22a41 ] = 1\nK2 K\u2211 j=1 K\u2211 k=1 E [ (\u03a6(\u03b8 (s) j,H)\u2212 \u03d5(s))(\u03a6(\u03b8 (s) k,H)\u2212 \u03d5(s))\u22a4 ] = K \u00b7 \u03b1 2\nB \u03a3\u2225(\u03d5\n(s)) +O(\u03b14) +K(K \u2212 1) \u00b7 O(\u03b14)\n= \u03b12\nBloc \u03a3\u2225(\u03d5\n(s)) +O(\u03b14).\nLet \u22062 := 1K \u2211K k=1 \u03b8 (s) k,H \u2212 \u03d5(s). Then \u22062 = \u22061 + 1K \u2211K k=1(\u03b8 (s) k,H \u2212 \u03a6(\u03b8 (s) k,H)). Finally, let \u22063 := \u03a6( 1 K \u2211K k=1 \u03b8 (s) k,H) \u2212 \u03d5(s). By Lemma E.1 it holds with probability at least 1 \u2212 \u03b4 that \u2225\u03b8(s)k,H \u2212 \u03a6(\u03b8 (s) k,H)\u22252 = O( \u221a \u03b7 log 1\u03b7 ) and thus \u2225\u22062 \u2212\u22061\u22252 = O( \u221a \u03b7 log 1\u03b7 ). Let \u03b4 = \u03b7 100. Since \u2225\u2202\u03a6( \u00b7 )\u22252 is always bounded by O(1), we can always add an error of O(\u03b4) to our bounds for the moments and ignore the possibility that this event does not happen. To prove (24), we do Taylor expansion of \u03a6 at \u03d5(s), then\nE[\u22063] = E[\u03a6(\u03d5(s) +\u22062)\u2212 \u03d5(s)] = E[\u03a6(\u03d5(s) +\u22061)\u2212 \u03d5(s)] +O (\u221a \u03b7 log 1\u03b7 + \u03b4 )\n= E [ \u2202\u03a6(\u03d5(s))\u22061 +O ( \u2225\u22061\u222522 )] +O (\u221a \u03b7 log 1\u03b7 + \u03b4 ) = \u03b12\n2Bloc \u22022\u03a6(\u03d5(s))[\u03a3(\u03d5(s))] +O(\u03b14).\nThe last equation uses the fact that \u2202\u03a6(\u03d5), for \u03d5 \u2208 \u0393, is a projection matrix onto the tangent space of \u0393 at \u03b8 (Lemma 4.3 of (Li et al., 2021c)).\nTo prove (25), again we do Taylor expansion of \u03a6 at \u03d5(s) to connect \u03a6(\u03d5(s) +\u22062) with \u03a6(\u03d5(s) + \u22061) and obtain:\nE[\u22063\u2206\u22a43 ] = E [ (\u03a6(\u03d5(s) +\u22062)\u2212 \u03d5(s))(\u03a6(\u03d5(s) +\u22062)\u2212 \u03d5(s))\u22a4 ] = E [ (\u03a6(\u03d5(s) +\u22061)\u2212 \u03d5(s))(\u03a6(\u03d5(s) +\u22061)\u2212 \u03d5(s))\u22a4 ] +O (\u221a \u03b7 log 1\u03b7 + \u03b4 ) .\nApplying the second-order Taylor expansion gives E[\u22063\u2206\u22a43 ] = E [( \u2202\u03a6(\u03d5(s))\u22061 )( \u2202\u03a6(\u03d5(s))\u22061 )\u22a4 + ( \u22022\u03a6(\u03d5(s))[\u22061,\u22061]\u2202\u03a6(\u03d5 (s))\u22061 +\u2206 \u22a4 1 \u2202\u03a6(\u03d5 (s))\u22022\u03a6(\u03d5(s))[\u22061,\u22061] \u22a4 )\n+O(\u2225\u22061\u222542) ] +O (\u221a \u03b7 log 1\u03b7 + \u03b4 ) .\nBy (15) and the fact that \u2225\u22022\u03a6(\u03d5(s))\u22252 is bounded, the above equation can be simplified to E[\u22063\u2206\u22a43 ] = E [ \u2202\u03a6(\u03d5(s))\u22061\u2206 \u22a4 1 \u2202\u03a6(\u03d5 (s)) ] +O(\u03b14) +O(\u03b14) +O (\u221a \u03b7 log 1\u03b7 + \u03b4 ) = \u03b12\nB \u03a3\u2225(\u03d5\n(s)) +O(\u03b14).\nFinally, for (26), we can repeat the above process to bound E[\u2225\u22061\u222532], and then conclude that E[\u2225\u22063\u222532] = O(\u03b16).\nNow we are ready to prove our main theorem.\nProof for Theorem 3.1. Let \u03b6(t) be the solution of (11). Let r be some integer greater than s. If \u03d5(s) \u2208 \u0393, define \u03b6\u0302s,r as the random variable sampled from the distribution of \u03b6(\u03b12r) conditioned on \u03b6(\u03b12s) = \u03d5(s). If \u03d5(s) = \u03b8null /\u2208 \u0393, define \u03b6\u0302s,r = 0. If \u03b8 \u2208 \u0393, define u(\u03b8, t1, t2) the expected value of g(\u03b6(t2)) conditioned on \u03b6(t1) = \u03b8. If \u03b8 = \u03b8null, define u(\u03b8, t1, t2) = 0. That is,\nu(\u03b8, t1, t2) := { E[g(\u03b6(t2)) | \u03b6(t1) = \u03b8], \u03b8 \u2208 \u0393, 0, \u03b8 /\u2208 \u0393.\nFor all n \u2264 Rtot, we have |E[g(\u03d5(n))]\u2212 E[g(\u03b6(n\u03b12))]| = |E[g(\u03b6\u0302n,n)]\u2212 E[g(\u03b6\u03020,n)]|\n\u2264 n\u22121\u2211 s=0 |E[g(\u03b6\u0302s+1,n)]\u2212 E[g(\u03b6\u0302s,n)]|\n= n\u22121\u2211 s=0 \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223E [ E[g(\u03b6\u0302s+1,n) | \u03d5(s+1)] ] \u2212 E [ E[g(\u03b6\u0302s,n) | \u03d5(s) ] \ufe38 \ufe37\ufe37 \ufe38\nTs \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223. By the law of total expectation and the Markovian property of It\u00f4 process,\nTs = E [ E[g(\u03b6(n\u03b12)) | \u03b6(s\u03b12) = \u03d5(s)] ] = E { E [ E[g(\u03b6(n\u03b12)) | \u03b6((s+ 1)\u03b12)]\n\u2223\u2223\u2223\u2223\u03b6(s\u03b12) = \u03d5(s)]} = E[u(\u03b6\u0302s,s+1, (s+ 1)\u03b12, n\u03b12)].\nTherefore,\n|E[g(\u03d5(n))]\u2212 E[g(\u03b6(n\u03b12))]| = n\u22121\u2211 s=0 \u2223\u2223\u2223\u2223\u2223\u2223\u2223E[u(\u03d5(s+1), (s+ 1)\u03b12, n\u03b12)]\u2212 E[u(\u03b6\u0302s,s+1, (s+ 1)\u03b12, n\u03b12)]\ufe38 \ufe37\ufe37 \ufe38T \u2032s \u2223\u2223\u2223\u2223\u2223\u2223\u2223 .\nBy the law of total expectation,\n|T \u2032s | \u2264 |E[u(\u03d5(s+1), (s+ 1)\u03b12, n\u03b12)\u2212 u(\u03b6\u0302s,s+1, (s+ 1)\u03b12, n\u03b12) | \u03d5(s),\u03d5(s+1) \u2208 \u0393]\ufe38 \ufe37\ufe37 \ufe38 As |\n+ |u(0)|P(\u03d5(s) /\u2208 \u0393 or \u03d5(s+1) /\u2208 \u0393), where the latter term comes from the definition of u(\u03b8, t1, t2) and \u03b6\u0302s,r. By Lemma I.11 in (Gu et al., 2023), there exists a constant \u03f5 such that if min\u03d5\u2208\u0393 \u2225\u03b8\u2212\u03d5\u22252 \u2264 \u03f5, then \u03a6(\u03b8) \u2208 \u0393. Therefore, substituting \u03b4 = \u03b7100 into Lemma E.1, we can conclude that the latter term is at most O(\u03b7100). For As, notice that the two terms differ only in the first position. By Lemma E.3 and (18) to (23), the moments of \u03d5(s+1) \u2212 \u03d5(s) and \u03b6\u0302s,s+1 \u2212 \u03d5(s) are close to each other. Therefore, it suffices to discuss the smoothness of u and perform Taylor expansion. By Proposition 25 of (Li et al., 2019a), since g \u2208 C4, u(\u03d5, t1, t2) satisfies the compatibility condition for the Whitney Extension Theorem for \u03d5 \u2208 \u0393. Therefore, there exists a function u\u0303(\u03d5, t1, t2) that is C4 in \u03d5 for \u03d5 \u2208 Rd and satisfies u\u0303(\u03d5, t1, t2) = u(\u03d5, t1, t2) for all \u03d5 \u2208 \u0393. Denote u\u0303(\u03d5, s\u03b12, n\u03b12) as u\u0303s,n(\u03d5) for brevity. Now, we can safely substitute u in As with u\u0303 and perform Taylor expansion:\nAs = E[u\u0303(\u03d5(s) + (\u03d5(s+1) \u2212 \u03d5(s))), (s+ 1)\u03b12, n\u03b12) | \u03d5(s),\u03d5(s+1) \u2208 \u0393]\ufe38 \ufe37\ufe37 \ufe38 A\u2032s\n\u2212 E[u\u0303(\u03d5(s) + (\u03b6\u0302s,s+1 \u2212 \u03d5(s))), (s+ 1)\u03b12, n\u03b12) | \u03d5(s) \u2208 \u0393]\ufe38 \ufe37\ufe37 \ufe38 A\u2032\u2032s .\nA\u2032s = u\u0303(\u03d5(s)) + \u2329 \u2202u\u0303s+1,n(\u03d5 (s)),E[\u03d5(s+1) \u2212 \u03d5(s) | \u03d5(s),\u03d5(s+1) \u2208 \u0393] \u232a\n+ 1\n2\n\u2329 \u22022u\u0303s+1,n(\u03d5 (s)),E[(\u03d5(s+1) \u2212 \u03d5(s))(\u03d5(s+1) \u2212 \u03d5(s))\u22a4 | \u03d5(s),\u03d5(s+1) \u2208 \u0393] \u232a\n+O(E[\u2225\u03d5(s+1) \u2212 \u03d5(s)\u222532 | \u03d5(s),\u03d5(s+1) \u2208 \u0393]).\nA\u2032\u2032s = u\u0303(\u03d5(s)) + \u2329 \u2202u\u0303s+1,n(\u03d5 (s)),E[\u03b6\u0302s,s+1 \u2212 \u03d5(s) | \u03d5(s) \u2208 \u0393] \u232a\n+ 1\n2\n\u2329 \u22022u\u0303s+1,n(\u03d5 (s)),E[(\u03b6\u0302s,s+1 \u2212 \u03d5(s))(\u03b6\u0302s,s+1 \u2212 \u03d5(s))\u22a4 | \u03d5(s) \u2208 \u0393] \u232a\n+O(E[\u2225\u03b6\u0302s,s+1 \u2212 \u03d5(s)\u222532 | \u03d5(s) \u2208 \u0393])\nSubstituting in \u03b4 = \u03b7100 into Lemma E.1, we can conclude that, given \u03d5(s) \u2208 \u0393, the event {\u03d5(s+1) \u2208 \u0393} happens with probability at least 1 \u2212 \u03b7100. We can replace the condition \u03d5(s),\u03d5(s+1) \u2208 \u0393 with \u03d5(s) \u2208 \u0393 in A\u2032s with an error of only O(\u03b7100). Therefore,\nAs = \u2329 \u2202u\u0303s+1,n(\u03d5 (s)),E[(\u03d5(s+1) \u2212 \u03d5(s))\u2212 (\u03b6\u0302s,s+1 \u2212 \u03d5(s)) | \u03d5(s) \u2208 \u0393] \u232a\n+ 1\n2\n\u2329 \u22022u\u0303s+1,n(\u03d5 (s)),E[((\u03d5(s) \u2212 \u03d5(s))(\u03b6\u0302s,s+1 \u2212 \u03d5(s))\u22a4\n\u2212 (\u03b6\u0302s,s+1 \u2212 \u03d5(s))(\u03b6\u0302s,s+1 \u2212 \u03d5(s))\u22a4 | \u03d5(s) \u2208 \u0393] \u232a\n+O(E[\u2225\u03d5(s+1) \u2212 \u03d5(s)\u222532 | \u03d5(s) \u2208 \u0393]) +O(E[\u2225\u03b6\u0302s,s+1 \u2212 \u03d5(s)\u222532 | \u03d5(s) \u2208 \u0393]) +O(\u03b7100).\nSince \u03d5(s) \u2208 \u0393 where \u0393 is a compact set, both \u2225\u2202u\u0303s+1,n(\u03d5(s))\u22252 and \u2225\u22022u\u0303s+1,n(\u03d5(s))\u22252 are bounded. Substituting Lemma E.3 and (18) to (23) to the expression of As, we have As = O(\u03b14) and thus |T \u2032s | = O(\u03b14). Summing |T \u2032s | up, we have |E[g(\u03d5(n))] \u2212 E[g(\u03b6(n\u03b12))]| \u2264 O(n\u03b14) \u2264 O(\u03b12), which completes the proof."
        },
        {
            "heading": "F DETAILS FOR COMMUNICATION TIME MEASUREMENT",
            "text": "It is straightforward to measure the time duration for the entire training, but it is hard to directly measure the communication time due to the asynchronous nature of CUDA computation. Hence, in our experiments, we derive the communication time from the difference in total training time across runs with various communication frequencies.\nSpecifically, let T totpara, T tot H1 be the total time durations of data parallel approaches and local gradient methods with H = H1, respectively. Also let T commpara , T comm H1\nbe their communication times, and T comppara , T comp H1\nbe their computation time. Ideally, setting the synchronization period to H1 reduces the communication volume exactly by a factor of 1H1 , so these variables satisfy the following relationships:\nT compH1 = T comp para ,\nT commH1 = 1\nH1 T commpara ,\nT commH1 + T comp H1 = T totH1 , T commpara + T comp para = T tot para.\nThen we can express the communication and computation times in terms of the total time duration T totpara and T tot H1 :\nT commpara = H1T comm H1 = H1 H1 \u2212 1 (T totpara \u2212 T totH1 ),\nT comppara = T comp H1 = T totpara \u2212 T commpara = H1\nH1 \u2212 1 T totH1 \u2212\n1\nH1 \u2212 1 T totpara.\nTherefore, we empirically measure the total time duration T\u0303 totpara and T\u0303 tot H1 for some H1, then use the following formulas to obtain estimates of the communication and computation times:\nT\u0303 commpara = H1\nH1 \u2212 1 (T\u0303 totpara \u2212 T\u0303 totH1 ), (27)\nT\u0303 comppara = H1\nH1 \u2212 1 T\u0303 totH1 \u2212\n1\nH1 \u2212 1 T\u0303 totpara. (28)\nThese estimates are very predictive for the total time duration of local gradient methods with a different H . For example, when H = H2, we can predict the total time duration T totH2 as follows:\nT commH2 \u2248 1\nH2 T\u0303 commpara , (29)\nT totH2 \u2248 1\nH2 T\u0303 commpara + T\u0303 comp para . (30)\nWe find that the relative error |T\u0303 totH2\u2212T tot H2 | T\u0303 totH2\n\u00d7 100%, where T totH2 denotes the measured total time, is only \u223c 1% across all configurations in Table 4, where we set H1 = 2, H2 = 4 for ResNet-152 and H1 = 4, H2 = 8 for ViT-B. The small relative error suggests that our method offers a close approximation to the actual time. For this reason, in Table 4, we report the communication time estimated by (27) and (29) for data-parallel approaches and local gradient methods with a constant synchronization period.\nFor QSR, since its communication volume relative to data parallel approaches, denoted as fQSR, can be easily computed given the learning rate schedule, the growth coefficient \u03b1 and the base synchronization period Hbase, we can estimate its communication time T commQSR in a similar vein to (27) and (29):\nT commQSR \u2248 fQSRT\u0303 commpara . (31) We report the communication time estimated by (31) in Table 4 for QSR."
        },
        {
            "heading": "G DISCUSSION ON MORE AGGRESSIVE SCALINGS",
            "text": "Apart from the scalings discussed in Section 3, one can consider more aggressive scalings, e.g.,H = \u230a(\u03c1/\u03b7)\u22123\u230b. Compared with QSR H = \u230a(\u03b1/\u03b7)\u22122\u230b that uses the same amount of communication, this cubic synchronization rule communicates more frequently at earlier stages but much less at later stages. Our theory in Theorem 3.1 suggests that taking H \u223c \u03b7\u22123 blows up the approximation error, but as shown in Figure 6, this cubic rule with a properly tuned \u03c1 can either outperform or underperform the QSR in test accuracy, depending on the training scenarios.\nWe argue that this is because our quasistatic view may break in the very late phase of cosine decay, where the learning rate decays so fast that \u03b7t sees significant decay within a single communication round. As an example where the cubic rule performs better, we plot in Figure 8 the test accuracy\ncurves of the QSR and cubic rule for training ViT-B with Local AdamW and batch size 4096. Same as our experiment setup in Section 4, the learning rate peaks at the value 0.008 and then decays to nearly zero (10\u22126) following a cosine decay schedule. Setting H = \u230a(0.0075/\u03b7)\u22123\u230b results in consistently worse test accuracy than QSR (with the same communication volume) before epoch 265. However, during the final communication round, which spans from epoch 265 to 300, the cubic rule catches up with QSR. During this period, the learning rate dramatically decreases from 3.5 \u00d7 10\u22124 to nearly zero, but our quasistatic view assumes that the learning rate \u03b7t should remain relatively constant for at least one communication round.\nBased on the above observation, we argue that the cubic rule offers benefits over QSR only for certain schedules that have a rapid tail of learning rate decay near the end of training. To validate this view, we replace the cosine decay schedule with a variant of the step decay schedule in Smith et al. (2020). In our step decay schedule, given a total of 300 epochs, the learning rate remains at its peak until epoch 150, after which it is divided by 2 every 30 epochs. See Figure 7 for an illustration. Unlike the cosine schedule, this step decay schedule maintains a constant learning rate for a significant amount of time. As shown in Table 6(a), the cubic rule yields inferior generalization performance compared with our QSR, even after careful tuning of \u03c1. See Appendix C.2 for training details.\nAnother way to corroborate our view is to run both scalings with a modified cosine learning rate schedule, which ceases to decay after a specific epoch t\u2032\u2032 and remains con-\nstant until training ends. See Figure 7 for an illustration of this modified cosine schedule. As shown in Table 6(b), QSR consistently outperforms the cubic rule across various choices of t\u2032\u2032. Further training details can be found in Appendix C.2. The probable reason is that when the learning rate is held constant, the cubic rule results in an excessively large H , negatively impacting optimization.\nGiven these failure cases of the cubic rule, we generally recommend using the QSR and leave it to future work to design a better rule to deal with schedules that have a rapid tail of learning rate decay."
        },
        {
            "heading": "H Schedule and Communcation Volume",
            "text": ""
        },
        {
            "heading": "H COMPARISON WITH LOCAL SGD/ADAMW + SWAP",
            "text": "In this section, we compare QSR with the modified Stochastic Weight Averaging in Parallel (SWAP) algorithm, termed \u201cLocal SGD/AdamW + SWAP\u201d. Specifically, the original SWAP proposed by (Gupta et al., 2020) uses SGD for the majority of the training process and only switches to local updates at some t0 near the end, thus saving less communication than QSR. To compare SWAP with QSR at a similar level of communication volume, we experiment with the modified SWAP, which starts with Local SGD/AdamW using a constant communication period Hbase and, after some time t0, lets workers perform local updates with a final model averaging. As shown in Figure 9, QSR outperforms Local SGD/AdamW SWAP though we have tuned t0 carefully for the latter."
        }
    ],
    "year": 2023
}