{
    "abstractText": "In this paper, we present a novel diffusion model called SyncDreamer that generates multiview-consistent images from a single-view image. Using pretrained large-scale 2D diffusion models, recent work Zero123 (Liu et al., 2023b) demonstrates the ability to generate plausible novel views from a single-view image of an object. However, maintaining consistency in geometry and colors for the generated images remains a challenge. To address this issue, we propose a synchronized multiview diffusion model that models the joint probability distribution of multiview images, enabling the generation of multiview-consistent images in a single reverse process. SyncDreamer synchronizes the intermediate states of all the generated images at every step of the reverse process through a 3D-aware feature attention mechanism that correlates the corresponding features across different views. Experiments show that SyncDreamer generates images with high consistency across different views, thus making it well-suited for various 3D generation tasks such as novel-view-synthesis, text-to-3D, and image-to-3D. Project page: https://liuyuan-pal.github.io/SyncDreamer/.",
    "authors": [
        {
            "affiliations": [],
            "name": "Yuan Liu"
        },
        {
            "affiliations": [],
            "name": "Cheng Lin"
        },
        {
            "affiliations": [],
            "name": "Zijiao Zeng"
        },
        {
            "affiliations": [],
            "name": "Xiaoxiao Long"
        },
        {
            "affiliations": [],
            "name": "Lingjie Liu"
        },
        {
            "affiliations": [],
            "name": "Taku Komura"
        },
        {
            "affiliations": [],
            "name": "Wenping Wang"
        }
    ],
    "id": "SP:0591e4156d4e183dbf0abb7e90592266c4c5f21c",
    "references": [
        {
            "authors": [
                "Titas Anciukevi\u010dius",
                "Zexiang Xu",
                "Matthew Fisher",
                "Paul Henderson",
                "Hakan Bilen",
                "Niloy J Mitra",
                "Paul Guerrero"
            ],
            "title": "Renderdiffusion: Image diffusion for 3d reconstruction, inpainting and generation",
            "year": 2023
        },
        {
            "authors": [
                "Mohammadreza Armandpour",
                "Huangjie Zheng",
                "Ali Sadeghian",
                "Amir Sadeghian",
                "Mingyuan Zhou"
            ],
            "title": "Re-imagine the negative prompt algorithm: Transform 2d diffusion into 3d, alleviate janus problem and beyond",
            "venue": "arXiv preprint arXiv:2304.04968,",
            "year": 2023
        },
        {
            "authors": [
                "Omer Bar-Tal",
                "Lior Yariv",
                "Yaron Lipman",
                "Tali Dekel"
            ],
            "title": "Multidiffusion: Fusing diffusion paths for controlled image generation",
            "venue": "arXiv preprint arXiv:2302.08113,",
            "year": 2023
        },
        {
            "authors": [
                "Eric R Chan",
                "Connor Z Lin",
                "Matthew A Chan",
                "Koki Nagano",
                "Boxiao Pan",
                "Shalini De Mello",
                "Orazio Gallo",
                "Leonidas J Guibas",
                "Jonathan Tremblay",
                "Sameh Khamis"
            ],
            "title": "Efficient geometry-aware 3d generative adversarial networks",
            "year": 2022
        },
        {
            "authors": [
                "Eric R Chan",
                "Koki Nagano",
                "Matthew A Chan",
                "Alexander W Bergman",
                "Jeong Joon Park",
                "Axel Levy",
                "Miika Aittala",
                "Shalini De Mello",
                "Tero Karras",
                "Gordon Wetzstein"
            ],
            "title": "Generative novel view synthesis with 3d-aware diffusion models",
            "year": 2023
        },
        {
            "authors": [
                "Angel X Chang",
                "Thomas Funkhouser",
                "Leonidas Guibas",
                "Pat Hanrahan",
                "Qixing Huang",
                "Zimo Li",
                "Silvio Savarese",
                "Manolis Savva",
                "Shuran Song",
                "Hao Su"
            ],
            "title": "Shapenet: An information-rich 3d model repository",
            "venue": "arXiv preprint arXiv:1512.03012,",
            "year": 2015
        },
        {
            "authors": [
                "Hansheng Chen",
                "Jiatao Gu",
                "Anpei Chen",
                "Wei Tian",
                "Zhuowen Tu",
                "Lingjie Liu",
                "Hao Su"
            ],
            "title": "Singlestage diffusion nerf: A unified approach to 3d generation and reconstruction",
            "venue": "In ICCV,",
            "year": 2023
        },
        {
            "authors": [
                "Rui Chen",
                "Yongwei Chen",
                "Ningxin Jiao",
                "Kui Jia"
            ],
            "title": "Fantasia3d: Disentangling geometry and appearance for high-quality text-to-3d content creation",
            "venue": "arXiv preprint arXiv:2303.13873,",
            "year": 2023
        },
        {
            "authors": [
                "Yiwen Chen",
                "Chi Zhang",
                "Xiaofeng Yang",
                "Zhongang Cai",
                "Gang Yu",
                "Lei Yang",
                "Guosheng Lin"
            ],
            "title": "It3d: Improved text-to-3d generation with explicit view synthesis",
            "venue": "arXiv preprint arXiv:2308.11473,",
            "year": 2023
        },
        {
            "authors": [
                "Yen-Chi Cheng",
                "Hsin-Ying Lee",
                "Sergey Tulyakov",
                "Alexander G Schwing",
                "Liang-Yan Gui"
            ],
            "title": "Sdfusion: Multimodal 3d shape completion, reconstruction, and generation",
            "year": 2023
        },
        {
            "authors": [
                "Florinel-Alin Croitoru",
                "Vlad Hondru",
                "Radu Tudor Ionescu",
                "Mubarak Shah"
            ],
            "title": "Diffusion models in vision: A survey",
            "year": 2023
        },
        {
            "authors": [
                "Matt Deitke",
                "Dustin Schwenk",
                "Jordi Salvador",
                "Luca Weihs",
                "Oscar Michel",
                "Eli VanderBilt",
                "Ludwig Schmidt",
                "Kiana Ehsani",
                "Aniruddha Kembhavi",
                "Ali Farhadi"
            ],
            "title": "Objaverse: A universe of annotated 3d objects",
            "year": 2023
        },
        {
            "authors": [
                "Congyue Deng",
                "Chiyu Jiang",
                "Charles R Qi",
                "Xinchen Yan",
                "Yin Zhou",
                "Leonidas Guibas",
                "Dragomir Anguelov"
            ],
            "title": "Nerdi: Single-view nerf synthesis with language-guided diffusion as general image priors",
            "venue": "In CVPR,",
            "year": 2023
        },
        {
            "authors": [
                "Kangle Deng",
                "Gengshan Yang",
                "Deva Ramanan",
                "Jun-Yan Zhu"
            ],
            "title": "3d-aware conditional image synthesis",
            "venue": "In CVPR,",
            "year": 2023
        },
        {
            "authors": [
                "Laura Downs",
                "Anthony Francis",
                "Nate Koenig",
                "Brandon Kinman",
                "Ryan Hickman",
                "Krista Reymann",
                "Thomas B McHugh",
                "Vincent Vanhoucke"
            ],
            "title": "Google scanned objects: A high-quality dataset of 3d scanned household",
            "year": 2022
        },
        {
            "authors": [
                "Ziya Erko\u00e7",
                "Fangchang Ma",
                "Qi Shan",
                "Matthias Nie\u00dfner",
                "Angela Dai"
            ],
            "title": "Hyperdiffusion: Generating implicit neural fields with weight-space diffusion",
            "venue": "arXiv preprint arXiv:2303.17015,",
            "year": 2023
        },
        {
            "authors": [
                "George Fahim",
                "Khalid Amin",
                "Sameh Zarif"
            ],
            "title": "Single-view 3d reconstruction: A survey of deep learning methods",
            "venue": "Computers & Graphics,",
            "year": 2021
        },
        {
            "authors": [
                "Kui Fu",
                "Jiansheng Peng",
                "Qiwen He",
                "Hanxiao Zhang"
            ],
            "title": "Single image 3d object reconstruction based on deep learning: A review",
            "venue": "Multimedia Tools and Applications,",
            "year": 2021
        },
        {
            "authors": [
                "Rinon Gal",
                "Yuval Alaluf",
                "Yuval Atzmon",
                "Or Patashnik",
                "Amit H Bermano",
                "Gal Chechik",
                "Daniel Cohen-Or"
            ],
            "title": "An image is worth one word: Personalizing text-to-image generation using textual inversion",
            "venue": "arXiv preprint arXiv:2208.01618,",
            "year": 2022
        },
        {
            "authors": [
                "Jun Gao",
                "Tianchang Shen",
                "Zian Wang",
                "Wenzheng Chen",
                "Kangxue Yin",
                "Daiqing Li",
                "Or Litany",
                "Zan Gojcic",
                "Sanja Fidler"
            ],
            "title": "Get3d: A generative model of high quality 3d textured shapes learned from images. NeurIPS, 2022",
            "year": 2022
        },
        {
            "authors": [
                "Jiatao Gu",
                "Lingjie Liu",
                "Peng Wang",
                "Christian Theobalt"
            ],
            "title": "Stylenerf: A style-based 3d-aware generator for high-resolution image synthesis",
            "venue": "In ICLR,",
            "year": 2021
        },
        {
            "authors": [
                "Jiatao Gu",
                "Qingzhe Gao",
                "Shuangfei Zhai",
                "Baoquan Chen",
                "Lingjie Liu",
                "Josh Susskind"
            ],
            "title": "Learning controllable 3d diffusion models from single-view images",
            "venue": "arXiv preprint arXiv:2304.06700,",
            "year": 2023
        },
        {
            "authors": [
                "Jiatao Gu",
                "Alex Trevithick",
                "Kai-En Lin",
                "Joshua M Susskind",
                "Christian Theobalt",
                "Lingjie Liu",
                "Ravi Ramamoorthi"
            ],
            "title": "Nerfdiff: Single-image view synthesis with nerf-guided distillation from 3d-aware diffusion",
            "venue": "In ICML,",
            "year": 2023
        },
        {
            "authors": [
                "Yuan-Chen Guo",
                "Ying-Tian Liu",
                "Ruizhi Shao",
                "Christian Laforte",
                "Vikram Voleti",
                "Guan Luo",
                "ChiaHao Chen",
                "Zi-Xin Zou",
                "Chen Wang",
                "Yan-Pei Cao",
                "Song-Hai Zhang"
            ],
            "title": "threestudio: A unified framework for 3d content generation",
            "venue": "https://github.com/threestudio-project/ threestudio,",
            "year": 2023
        },
        {
            "authors": [
                "Anchit Gupta",
                "Wenhan Xiong",
                "Yixin Nie",
                "Ian Jones",
                "Barlas O\u011fuz"
            ],
            "title": "3dgen: Triplane latent diffusion for textured mesh generation",
            "venue": "arXiv preprint arXiv:2303.05371,",
            "year": 2023
        },
        {
            "authors": [
                "Jonathan Ho",
                "Ajay Jain",
                "Pieter Abbeel"
            ],
            "title": "Denoising diffusion probabilistic models",
            "venue": "In NeurIPS,",
            "year": 2020
        },
        {
            "authors": [
                "Yukun Huang",
                "Jianan Wang",
                "Yukai Shi",
                "Xianbiao Qi",
                "Zheng-Jun Zha",
                "Lei Zhang"
            ],
            "title": "Dreamtime: An improved optimization strategy for text-to-3d content creation",
            "venue": "arXiv preprint arXiv:2306.12422,",
            "year": 2023
        },
        {
            "authors": [
                "Heewoo Jun",
                "Alex Nichol"
            ],
            "title": "Shap-e: Generating conditional 3d implicit functions",
            "venue": "arXiv preprint arXiv:2305.02463,",
            "year": 2023
        },
        {
            "authors": [
                "Animesh Karnewar",
                "Niloy J Mitra",
                "Andrea Vedaldi",
                "David Novotny"
            ],
            "title": "Holofusion: Towards photo-realistic 3d generative modeling",
            "venue": "In ICCV,",
            "year": 2023
        },
        {
            "authors": [
                "Animesh Karnewar",
                "Andrea Vedaldi",
                "David Novotny",
                "Niloy J Mitra"
            ],
            "title": "Holodiffusion: Training a 3d diffusion model using 2d images",
            "venue": "In CVPR,",
            "year": 2023
        },
        {
            "authors": [
                "Hiroharu Kato",
                "Tatsuya Harada"
            ],
            "title": "Learning view priors for single-view 3d reconstruction",
            "venue": "In CVPR,",
            "year": 2019
        },
        {
            "authors": [
                "Seung Wook Kim",
                "Bradley Brown",
                "Kangxue Yin",
                "Karsten Kreis",
                "Katja Schwarz",
                "Daiqing Li",
                "Robin Rombach",
                "Antonio Torralba",
                "Sanja Fidler"
            ],
            "title": "Neuralfield-ldm: Scene generation with hierarchical latent diffusion models",
            "year": 2023
        },
        {
            "authors": [
                "Yuseung Lee",
                "Kunho Kim",
                "Hyunjin Kim",
                "Minhyuk Sung"
            ],
            "title": "Syncdiffusion: Coherent montage via synchronized joint diffusions",
            "venue": "arXiv preprint arXiv:2306.05178,",
            "year": 2023
        },
        {
            "authors": [
                "Jiabao Lei",
                "Jiapeng Tang",
                "Kui Jia"
            ],
            "title": "Generative scene synthesis via incremental view inpainting using rgbd diffusion models",
            "year": 2022
        },
        {
            "authors": [
                "Xueting Li",
                "Sifei Liu",
                "Kihwan Kim",
                "Shalini De Mello",
                "Varun Jampani",
                "Ming-Hsuan Yang",
                "Jan Kautz"
            ],
            "title": "Self-supervised single-view 3d reconstruction via semantic consistency",
            "venue": "In ECCV,",
            "year": 2020
        },
        {
            "authors": [
                "Chen-Hsuan Lin",
                "Jun Gao",
                "Luming Tang",
                "Towaki Takikawa",
                "Xiaohui Zeng",
                "Xun Huang",
                "Karsten Kreis",
                "Sanja Fidler",
                "Ming-Yu Liu",
                "Tsung-Yi Lin"
            ],
            "title": "Magic3d: High-resolution text-to-3d content creation",
            "year": 2023
        },
        {
            "authors": [
                "Minghua Liu",
                "Chao Xu",
                "Haian Jin",
                "Linghao Chen",
                "Zexiang Xu",
                "Hao Su"
            ],
            "title": "One-2-3-45: Any single image to 3d mesh in 45 seconds without per-shape optimization",
            "venue": "arXiv preprint arXiv:2306.16928,",
            "year": 2023
        },
        {
            "authors": [
                "Ruoshi Liu",
                "Rundi Wu",
                "Basile Van Hoorick",
                "Pavel Tokmakov",
                "Sergey Zakharov",
                "Carl Vondrick"
            ],
            "title": "Zero-1-to-3: Zero-shot one image to 3d object",
            "venue": "In ICCV,",
            "year": 2023
        },
        {
            "authors": [
                "Xinhang Liu",
                "Shiu-hong Kao",
                "Jiaben Chen",
                "Yu-Wing Tai",
                "Chi-Keung Tang"
            ],
            "title": "Deceptive-nerf: Enhancing nerf reconstruction using pseudo-observations from diffusion models",
            "venue": "arXiv preprint arXiv:2305.15171,",
            "year": 2023
        },
        {
            "authors": [
                "Zhen Liu",
                "Yao Feng",
                "Michael J Black",
                "Derek Nowrouzezahrai",
                "Liam Paull",
                "Weiyang Liu"
            ],
            "title": "Meshdiffusion: Score-based generative 3d mesh modeling",
            "venue": "In ICLR,",
            "year": 2023
        },
        {
            "authors": [
                "Luke Melas-Kyriazi",
                "Iro Laina",
                "Christian Rupprecht",
                "Andrea Vedaldi"
            ],
            "title": "Realfusion: 360deg reconstruction of any object from a single image",
            "year": 2023
        },
        {
            "authors": [
                "Ben Mildenhall",
                "Pratul P Srinivasan",
                "Matthew Tancik",
                "Jonathan T Barron",
                "Ravi Ramamoorthi",
                "Ren Ng"
            ],
            "title": "Nerf: Representing scenes as neural radiance fields for view synthesis",
            "venue": "In ECCV,",
            "year": 2020
        },
        {
            "authors": [
                "Norman M\u00fcller",
                "Yawar Siddiqui",
                "Lorenzo Porzi",
                "Samuel Rota Bulo",
                "Peter Kontschieder",
                "Matthias Nie\u00dfner"
            ],
            "title": "Diffrf: Rendering-guided 3d radiance field diffusion",
            "year": 2023
        },
        {
            "authors": [
                "Alex Nichol",
                "Heewoo Jun",
                "Prafulla Dhariwal",
                "Pamela Mishkin",
                "Mark Chen"
            ],
            "title": "Point-e: A system for generating 3d point clouds from complex prompts",
            "venue": "arXiv preprint arXiv:2212.08751,",
            "year": 2022
        },
        {
            "authors": [
                "Michael Niemeyer",
                "Andreas Geiger"
            ],
            "title": "Giraffe: Representing scenes as compositional generative neural feature fields",
            "venue": "In CVPR,",
            "year": 2021
        },
        {
            "authors": [
                "Evangelos Ntavelis",
                "Aliaksandr Siarohin",
                "Kyle Olszewski",
                "Chaoyang Wang",
                "Luc Van Gool",
                "Sergey Tulyakov"
            ],
            "title": "Autodecoding latent 3d diffusion models",
            "venue": "arXiv preprint arXiv:2307.05445,",
            "year": 2023
        },
        {
            "authors": [
                "Ben Poole",
                "Ajay Jain",
                "Jonathan T Barron",
                "Ben Mildenhall"
            ],
            "title": "Dreamfusion: Text-to-3d using 2d diffusion",
            "venue": "In ICLR,",
            "year": 2023
        },
        {
            "authors": [
                "Guocheng Qian",
                "Jinjie Mai",
                "Abdullah Hamdi",
                "Jian Ren",
                "Aliaksandr Siarohin",
                "Bing Li",
                "HsinYing Lee",
                "Ivan Skorokhodov",
                "Peter Wonka",
                "Sergey Tulyakov"
            ],
            "title": "Magic123: One image to high-quality 3d object generation using both 2d and 3d diffusion priors",
            "venue": "arXiv preprint arXiv:2306.17843,",
            "year": 2023
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "year": 2021
        },
        {
            "authors": [
                "Amit Raj",
                "Srinivas Kaza",
                "Ben Poole",
                "Michael Niemeyer",
                "Nataniel Ruiz",
                "Ben Mildenhall",
                "Shiran Zada",
                "Kfir Aberman",
                "Michael Rubinstein",
                "Jonathan Barron"
            ],
            "title": "Dreambooth3d: Subject-driven text-to-3d generation",
            "venue": "arXiv preprint arXiv:2303.13508,",
            "year": 2023
        },
        {
            "authors": [
                "Jeremy Reizenstein",
                "Roman Shapovalov",
                "Philipp Henzler",
                "Luca Sbordone",
                "Patrick Labatut",
                "David Novotny"
            ],
            "title": "Common objects in 3d: Large-scale learning and evaluation of real-life 3d category reconstruction",
            "year": 2021
        },
        {
            "authors": [
                "Robin Rombach",
                "Andreas Blattmann",
                "Dominik Lorenz",
                "Patrick Esser",
                "Bj\u00f6rn Ommer"
            ],
            "title": "Highresolution image synthesis with latent diffusion models",
            "year": 2022
        },
        {
            "authors": [
                "Chitwan Saharia",
                "William Chan",
                "Saurabh Saxena",
                "Lala Li",
                "Jay Whang",
                "Emily L Denton",
                "Kamyar Ghasemipour",
                "Raphael Gontijo Lopes",
                "Burcu Karagol Ayan",
                "Tim Salimans"
            ],
            "title": "Photorealistic text-to-image diffusion models with deep language understanding",
            "venue": "NeurIPS,",
            "year": 2022
        },
        {
            "authors": [
                "Kyle Sargent",
                "Jing Yu Koh",
                "Han Zhang",
                "Huiwen Chang",
                "Charles Herrmann",
                "Pratul Srinivasan",
                "Jiajun Wu",
                "Deqing Sun"
            ],
            "title": "Vq3d: Learning a 3d-aware generative model on imagenet",
            "venue": "arXiv preprint arXiv:2302.06833,",
            "year": 2023
        },
        {
            "authors": [
                "Johannes Lutz Sch\u00f6nberger",
                "Enliang Zheng",
                "Marc Pollefeys",
                "Jan-Michael Frahm"
            ],
            "title": "Pixelwise view selection for unstructured multi-view stereo",
            "venue": "In ECCV,",
            "year": 2016
        },
        {
            "authors": [
                "Katja Schwarz",
                "Yiyi Liao",
                "Michael Niemeyer",
                "Andreas Geiger"
            ],
            "title": "Graf: Generative radiance fields for 3d-aware image synthesis",
            "venue": "NeurIPS,",
            "year": 2020
        },
        {
            "authors": [
                "Hoigi Seo",
                "Hayeon Kim",
                "Gwanghyun Kim",
                "Se Young Chun"
            ],
            "title": "Ditto-nerf: Diffusion-based iterative text to omni-directional 3d model",
            "venue": "arXiv preprint arXiv:2304.02827,",
            "year": 2023
        },
        {
            "authors": [
                "Junyoung Seo",
                "Wooseok Jang",
                "Min-Seop Kwak",
                "Jaehoon Ko",
                "Hyeonsu Kim",
                "Junho Kim",
                "Jin-Hwa Kim",
                "Jiyoung Lee",
                "Seungryong Kim"
            ],
            "title": "Let 2d diffusion model know 3d-consistency for robust text-to-3d generation",
            "venue": "arXiv preprint arXiv:2303.07937,",
            "year": 2023
        },
        {
            "authors": [
                "Qiuhong Shen",
                "Xingyi Yang",
                "Xinchao Wang"
            ],
            "title": "Anything-3d: Towards single-view anything reconstruction in the wild",
            "venue": "arXiv preprint arXiv:2304.10261,",
            "year": 2023
        },
        {
            "authors": [
                "Yichun Shi",
                "Peng Wang",
                "Jianglong Ye",
                "Mai Long",
                "Kejie Li",
                "Xiao Yang"
            ],
            "title": "Mvdream: Multi-view diffusion for 3d generation",
            "venue": "arXiv preprint arXiv:2308.16512,",
            "year": 2023
        },
        {
            "authors": [
                "Jascha Sohl-Dickstein",
                "Eric Weiss",
                "Niru Maheswaranathan",
                "Surya Ganguli"
            ],
            "title": "Deep unsupervised learning using nonequilibrium thermodynamics",
            "venue": "In ICML,",
            "year": 2015
        },
        {
            "authors": [
                "Jiaming Song",
                "Chenlin Meng",
                "Stefano Ermon"
            ],
            "title": "Denoising diffusion implicit models",
            "venue": "arXiv preprint arXiv:2010.02502,",
            "year": 2020
        },
        {
            "authors": [
                "Mohammed Suhail",
                "Carlos Esteves",
                "Leonid Sigal",
                "Ameesh Makadia"
            ],
            "title": "Generalizable patch-based neural rendering",
            "venue": "In ECCV,",
            "year": 2022
        },
        {
            "authors": [
                "Stanislaw Szymanowicz",
                "Christian Rupprecht",
                "Andrea Vedaldi"
            ],
            "title": "Viewset diffusion:(0-) imageconditioned 3d generative models from 2d data",
            "venue": "arXiv preprint arXiv:2306.07881,",
            "year": 2023
        },
        {
            "authors": [
                "Junshu Tang",
                "Tengfei Wang",
                "Bo Zhang",
                "Ting Zhang",
                "Ran Yi",
                "Lizhuang Ma",
                "Dong Chen"
            ],
            "title": "Makeit-3d: High-fidelity 3d creation from a single image with diffusion prior",
            "venue": "In ICCV,",
            "year": 2023
        },
        {
            "authors": [
                "Shitao Tang",
                "Fuyang Zhang",
                "Jiacheng Chen",
                "Peng Wang",
                "Yasutaka Furukawa"
            ],
            "title": "Mvdiffusion: Enabling holistic multi-view image generation with correspondence-aware diffusion",
            "venue": "arXiv preprint arXiv:2307.01097,",
            "year": 2023
        },
        {
            "authors": [
                "Maxim Tatarchenko",
                "Stephan R Richter",
                "Ren\u00e9 Ranftl",
                "Zhuwen Li",
                "Vladlen Koltun",
                "Thomas Brox"
            ],
            "title": "What do single-view 3d reconstruction networks learn",
            "year": 2019
        },
        {
            "authors": [
                "Ayush Tewari",
                "Tianwei Yin",
                "George Cazenavette",
                "Semon Rezchikov",
                "Joshua B Tenenbaum",
                "Fr\u00e9do Durand",
                "William T Freeman",
                "Vincent Sitzmann"
            ],
            "title": "Diffusion with forward models: Solving stochastic inverse problems without direct supervision",
            "venue": "arXiv preprint arXiv:2306.11719,",
            "year": 2023
        },
        {
            "authors": [
                "Christina Tsalicoglou",
                "Fabian Manhardt",
                "Alessio Tonioni",
                "Michael Niemeyer",
                "Federico Tombari"
            ],
            "title": "Textmesh: Generation of realistic 3d meshes from text prompts",
            "venue": "arXiv preprint arXiv:2304.12439,",
            "year": 2023
        },
        {
            "authors": [
                "Hung-Yu Tseng",
                "Qinbo Li",
                "Changil Kim",
                "Suhib Alsisan",
                "Jia-Bin Huang",
                "Johannes Kopf"
            ],
            "title": "Consistent view synthesis with pose-guided diffusion models",
            "year": 2023
        },
        {
            "authors": [
                "Haochen Wang",
                "Xiaodan Du",
                "Jiahao Li",
                "Raymond A Yeh",
                "Greg Shakhnarovich"
            ],
            "title": "Score jacobian chaining: Lifting pretrained 2d diffusion models for 3d generation",
            "venue": "In CVPR,",
            "year": 2023
        },
        {
            "authors": [
                "Peng Wang",
                "Lingjie Liu",
                "Yuan Liu",
                "Christian Theobalt",
                "Taku Komura",
                "Wenping Wang"
            ],
            "title": "Neus: Learning neural implicit surfaces by volume rendering for multi-view reconstruction",
            "venue": "NeurIPS,",
            "year": 2021
        },
        {
            "authors": [
                "Tengfei Wang",
                "Bo Zhang",
                "Ting Zhang",
                "Shuyang Gu",
                "Jianmin Bao",
                "Tadas Baltrusaitis",
                "Jingjing Shen",
                "Dong Chen",
                "Fang Wen",
                "Qifeng Chen"
            ],
            "title": "Rodin: A generative model for sculpting 3d digital avatars using diffusion",
            "venue": "In CVPR,",
            "year": 2023
        },
        {
            "authors": [
                "Zhengyi Wang",
                "Cheng Lu",
                "Yikai Wang",
                "Fan Bao",
                "Chongxuan Li",
                "Hang Su",
                "Jun Zhu"
            ],
            "title": "Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation",
            "venue": "arXiv preprint arXiv:2305.16213,",
            "year": 2023
        },
        {
            "authors": [
                "Zhou Wang",
                "Alan C Bovik",
                "Hamid R Sheikh",
                "Eero P Simoncelli"
            ],
            "title": "Image quality assessment: from error visibility to structural similarity",
            "year": 2004
        },
        {
            "authors": [
                "Daniel Watson",
                "William Chan",
                "Ricardo Martin-Brualla",
                "Jonathan Ho",
                "Andrea Tagliasacchi",
                "Mohammad Norouzi"
            ],
            "title": "Novel view synthesis with diffusion models",
            "venue": "arXiv preprint arXiv:2210.04628,",
            "year": 2022
        },
        {
            "authors": [
                "Jinbo Wu",
                "Xiaobo Gao",
                "Xing Liu",
                "Zhengyang Shen",
                "Chen Zhao",
                "Haocheng Feng",
                "Jingtuo Liu",
                "Errui Ding"
            ],
            "title": "Hd-fusion: Detailed text-to-3d generation leveraging multiple noise estimation",
            "venue": "arXiv preprint arXiv:2307.16183,",
            "year": 2023
        },
        {
            "authors": [
                "Jianfeng Xiang",
                "Jiaolong Yang",
                "Binbin Huang",
                "Xin Tong"
            ],
            "title": "3d-aware image generation using 2d diffusion models",
            "venue": "arXiv preprint arXiv:2303.17905,",
            "year": 2023
        },
        {
            "authors": [
                "Yiheng Xie",
                "Towaki Takikawa",
                "Shunsuke Saito",
                "Or Litany",
                "Shiqin Yan",
                "Numair Khan",
                "Federico Tombari",
                "James Tompkin",
                "Vincent Sitzmann",
                "Srinath Sridhar"
            ],
            "title": "Neural fields in visual computing and beyond",
            "venue": "In Computer Graphics Forum,",
            "year": 2022
        },
        {
            "authors": [
                "Dejia Xu",
                "Yifan Jiang",
                "Peihao Wang",
                "Zhiwen Fan",
                "Yi Wang",
                "Zhangyang Wang"
            ],
            "title": "Neurallift-360: Lifting an in-the-wild 2d photo to a 3d object with 360 views",
            "venue": "arXiv e-prints, pp",
            "year": 2022
        },
        {
            "authors": [
                "Yao Yao",
                "Zixin Luo",
                "Shiwei Li",
                "Tian Fang",
                "Long Quan"
            ],
            "title": "Mvsnet: Depth inference for unstructured multi-view stereo",
            "venue": "In ECCV,",
            "year": 2018
        },
        {
            "authors": [
                "Paul Yoo",
                "Jiaxian Guo",
                "Yutaka Matsuo",
                "Shixiang Shane Gu"
            ],
            "title": "Dreamsparse: Escaping from plato\u2019s cave with 2d frozen diffusion model given sparse views",
            "year": 2023
        },
        {
            "authors": [
                "Chaohui Yu",
                "Qiang Zhou",
                "Jingliang Li",
                "Zhe Zhang",
                "Zhibin Wang",
                "Fan Wang"
            ],
            "title": "Points-to-3d: Bridging the gap between sparse points and shape-controllable text-to-3d generation",
            "venue": "arXiv preprint arXiv:2307.13908,",
            "year": 2023
        },
        {
            "authors": [
                "Jason J. Yu",
                "Fereshteh Forghani",
                "Konstantinos G. Derpanis",
                "Marcus A. Brubaker"
            ],
            "title": "Long-term photometric consistent novel view synthesis with diffusion models",
            "venue": "In ICCV,",
            "year": 2023
        },
        {
            "authors": [
                "Xiaohui Zeng",
                "Arash Vahdat",
                "Francis Williams",
                "Zan Gojcic",
                "Or Litany",
                "Sanja Fidler",
                "Karsten Kreis"
            ],
            "title": "Lion: Latent point diffusion models for 3d shape generation",
            "venue": "NeurIPS,",
            "year": 2022
        },
        {
            "authors": [
                "Biao Zhang",
                "Jiapeng Tang",
                "Matthias Niessner",
                "Peter Wonka"
            ],
            "title": "3dshape2vecset: A 3d shape representation for neural fields and generative diffusion models",
            "venue": "In SIGGRAPH,",
            "year": 2023
        },
        {
            "authors": [
                "Jingbo Zhang",
                "Xiaoyu Li",
                "Ziyu Wan",
                "Can Wang",
                "Jing Liao"
            ],
            "title": "Text2nerf: Text-driven 3d scene generation with neural radiance fields",
            "venue": "arXiv preprint arXiv:2305.11588,",
            "year": 2023
        },
        {
            "authors": [
                "Richard Zhang",
                "Phillip Isola",
                "Alexei A Efros",
                "Eli Shechtman",
                "Oliver Wang"
            ],
            "title": "The unreasonable effectiveness of deep features as a perceptual metric",
            "year": 2018
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Humans possess a remarkable ability to perceive 3D structures from a single image. When presented with an image of an object, humans can easily imagine the other views of the object. Despite great progress (Yao et al., 2018; Tewari et al., 2020; Wang et al., 2021; Mildenhall et al., 2020; Xie et al., 2022) brought by neural networks in computer vision or graphics fields for extracting 3D information from images, generating multiview-consistent images from a single-view image of an object is still a challenging problem due to the limited 3D information available in an image.\nRecently, diffusion models (Rombach et al., 2022; Ho et al., 2020) have demonstrated huge success in 2D image generation, which unlocks new potential for 3D generation tasks. However, directly training a generalizable 3D diffusion model (Wang et al., 2023b; Jun & Nichol, 2023; Nichol et al., 2022; Mu\u0308ller et al., 2023) usually requires a large amount of 3D data while existing 3D datasets are insufficient for capture the complexity of arbitrary 3D shapes. Therefore, recent methods (Poole et al., 2023; Wang et al., 2023a;c; Lin et al., 2023; Chen et al., 2023b) resort to distilling pretrained text-to-image diffusion models for creating 3D models from texts, which shows impressive results on this text-to-3D task. Some works (Tang et al., 2023a; Melas-Kyriazi et al., 2023; Xu et al., 2022; Raj et al., 2023) extend such a distillation process to train a neural radiance field (Mildenhall et al., 2020) (NeRF) for the image-to-3D task. In order to utilize pretrained text-to-image models, these methods have to perform textual inversion (Gal et al., 2022) to find a suitable text description of the input image. However, the distillation process along with the textual inversion usually takes a long time to generate a single shape and requires tedious parameter tuning for satisfactory quality. Moreover, due to the abundance of specific details in an image, such as object category, appearance, and pose, it is challenging to accurately represent an image using a single word embedding, which results in a decrease in the quality of 3D shapes reconstructed by the distillation method.\n\u2217Corresponding Authors.\nInstead of distillation, some recent works (Watson et al., 2022; Gu et al., 2023b; Deng et al., 2023a; Zhou & Tulsiani, 2023; Tseng et al., 2023; Yu et al., 2023b; Chan et al., 2023; Tewari et al., 2023; Zhang et al., 2023b; Xiang et al., 2023) apply 2D diffusion models to directly generate multiview images for the 3D reconstruction task. The key problem is how to maintain the multiview consistency when generating images of the same object. To improve the multiview consistency, these methods allow the diffusion model to condition on the input images (Zhou & Tulsiani, 2023; Tseng et al., 2023; Watson et al., 2022; Liu et al., 2023b; Yu et al., 2023b), previously generated images (Tewari et al., 2023; Chan et al., 2023) or renderings from a neural field (Gu et al., 2023b). Although some impressive results are achieved for specific object categories from ShapeNet (Chang et al., 2015) or Co3D (Reizenstein et al., 2021), how to design a diffusion model to generate multiview-consistent images for arbitrary objects still remains unsolved.\nIn this paper, we propose a simple yet effective framework to generate multiview-consistent images for the single-view 3D reconstruction of arbitrary objects. The key idea is to extend the diffusion framework (Ho et al., 2020) to model the joint probability distribution of multiview images. We show that modeling the joint distribution can be achieved by introducing a synchronized multiview diffusion model. Specifically, for N target views to be generated, we construct N shared noise predictors respectively. The reverse diffusion process simultaneously generates N images by N corresponding noise predictors, where information across different images is shared among noise predictors by attention layers on every denoising step. Thus, we name our framework SyncDreamer which synchronizes intermediate states of all noise predictors on every step in the reverse process.\nSyncDreamer has the following characteristics that make it a competitive tool for lifting 2D singleview images to 3D. First, SyncDreamer retains strong generalization ability by initializing its weights from the pretrained Zero123 (Liu et al., 2023b) model which is finetuned from the Stable Diffusion model (Rombach et al., 2022) on the Objaverse (Deitke et al., 2023) dataset. Thus, SyncDreamer is able to reconstruct shapes from both photorealistic images and hand drawings as shown in Fig. 1. Second, SyncDreamer makes the single-view reconstruction easier than the distillation methods. Because the generated images are consistent in both geometry and appearance, we can simply run a vanilla NeRF (Mildenhall et al., 2020) or a vanilla NeuS (Wang et al., 2021) without using any special losses for reconstruction. Given the generated images, one can easily reckon the final reconstruction quality while it is hard for distillation methods to know the output reconstruction quality beforehand. Third, SyncDreamer maintains creativity and diversity when inferring 3D information, which enables generating multiple reasonable objects from a given image as shown in Fig. 4. In comparison, previous distillation methods can only converge to one single shape.\nWe quantitatively compare SyncDreamer with baseline methods on the Google Scanned Object (Downs et al., 2022) dataset. The results show that, in comparison with baseline methods, SyncDreamer is able to generate more consistent images and reconstruct better shapes from input single-view images. We further demonstrate that SyncDreamer supports various styles of 2D input like cartoons, sketches, ink paintings, and oil paintings for generating consistent views and reconstructing 3D shapes, which verifies the effectiveness of SyncDreamer in lifting 2D images to 3D."
        },
        {
            "heading": "2 RELATED WORK",
            "text": ""
        },
        {
            "heading": "2.1 DIFFUSION MODELS",
            "text": "Diffusion models (Ho et al., 2020; Rombach et al., 2022; Croitoru et al., 2023) have shown impressive results on 2D image generation. Concurrent work MVDiffusion (Tang et al., 2023b) also adopts the multiview diffusion formulation to synthesize textures or panoramas with known geometry. We propose similar formulations in SyncDreamer but with unknown geometry. MultiDiffusion (Bar-Tal et al., 2023) and SyncDiffusion (Lee et al., 2023) correlate multiple diffusion models for different regions of a 2D image. Many recent works (Nichol et al., 2022; Jun & Nichol, 2023; Mu\u0308ller et al., 2023; Zhang et al., 2023a; Liu et al., 2023d; Wang et al., 2023b; Gupta et al., 2023; Cheng et al., 2023; Karnewar et al., 2023b; Anciukevic\u030cius et al., 2023; Zeng et al., 2022; Erkoc\u0327 et al., 2023; Chen et al., 2023a; Kim et al., 2023; Ntavelis et al., 2023; Gu et al., 2023a; Karnewar et al., 2023a) try to repeat the success of diffusion models on the 3D generation task. However, the scarcity of 3D data makes it difficult to directly train diffusion models on 3D and the resulting generation quality is still much worse and less generalizable than the counterpart image generation models, though some works (Anciukevic\u030cius et al., 2023; Chen et al., 2023a; Karnewar et al., 2023b) are trying to only use 2D images for training 3D diffusion models."
        },
        {
            "heading": "2.2 USING 2D DIFFUSION MODELS FOR 3D",
            "text": "Instead of directly learning a 3D diffusion model, many works resort to using high-quality 2D diffusion models (Rombach et al., 2022; Saharia et al., 2022) for 3D tasks. Pioneer works DreamFusion (Poole et al., 2023) and SJC (Wang et al., 2023a) propose to distill a 2D text-to-image generation model to generate 3D shapes from texts. Follow-up works (Chen et al., 2023b; Wang et al., 2023c; Seo et al., 2023a; Yu et al., 2023a; Lin et al., 2023; Seo et al., 2023b; Tsalicoglou et al., 2023; Zhu & Zhuang, 2023; Huang et al., 2023; Armandpour et al., 2023; Wu et al., 2023; Chen et al., 2023c) improve such text-to-3D distillation methods in various aspects. Many works (Tang et al., 2023a; Melas-Kyriazi et al., 2023; Qian et al., 2023; Xu et al., 2022; Raj et al., 2023; Shen et al., 2023) also apply such a distillation pipeline in the single-view reconstruction task. Though some impressive results are achieved, these methods usually require a long time for textual inversion (Liu et al., 2023a) and NeRF optimization and they do not guarantee to get satisfactory results.\nOther works (Watson et al., 2022; Gu et al., 2023b; Deng et al., 2023a; Zhou & Tulsiani, 2023; Tseng et al., 2023; Chan et al., 2023; Yu et al., 2023b; Tewari et al., 2023; Yoo et al., 2023; Szymanowicz et al., 2023; Tang et al., 2023b; Xiang et al., 2023; Liu et al., 2023c; Lei et al., 2022) directly apply the 2D diffusion models to generate multiview images for 3D reconstruction. (Tseng et al., 2023; Yu et al., 2023b) are conditioned on the input image by attention layers for novel-view synthesis in indoor scenes. Our method also uses attention layers but is intended for object reconstruction. (Xiang et al., 2023; Zhang et al., 2023b) resort to estimated depth maps to warp and inpaint for novel-view image generation, which strongly relies on the performance of the external single-view depth estimator. Two concurrent works (Chan et al., 2023; Tewari et al., 2023) generate new images in an autoregressive render-and-generate manner, which demonstrates good performances on specific object categories or scenes. In comparison, SyncDreamer is targeted to reconstruct arbitrary objects and generates all images in one reverse process. The concurrent work Viewset Diffusion (Szymanowicz et al., 2023) shares a similar idea to generate a set of images. The differences between SyncDreamer and Viewset Diffusion are that SyncDreamer does not require predicting a radiance field like Viewset Diffusion but only uses attention to synchronize the states among views and SyncDreamer fixes the viewpoints of generated views for better convergence. Another concurrent work MVDream (Shi et al., 2023) also proposes multiview generation for the text-to-3D task while our work aims to reconstruct shapes from single-view images."
        },
        {
            "heading": "2.3 OTHER SINGLE-VIEW RECONSTRUCTION METHODS",
            "text": "Single-view reconstruction is a challenging ill-posed problem. Before the prosperity of generative models used in 3D reconstruction, there are many works (Tatarchenko et al., 2019; Fu et al., 2021; Kato & Harada, 2019; Li et al., 2020; Fahim et al., 2021) that reconstruct 3D shapes from single-view images by regression (Li et al., 2020) or retrieval (Tatarchenko et al., 2019), which have difficulty in generalizing to new categories. Recent NeRF-GAN methods (Niemeyer & Geiger, 2021; Chan et al., 2022; Gu et al., 2021; Schwarz et al., 2020; Gao et al., 2022; Deng et al., 2023b) learn to generate NeRFs for specific categories like human or cat faces. These NeRF-GANs achieve impressive results on single-view image reconstruction but fail to generalize to arbitrary objects. Although some recent\nworks also attempt to generalize NeRF-GAN to ImageNet (Skorokhodov et al., 2023; Sargent et al., 2023), training NeRF-GANs for arbitrary objects is still challenging."
        },
        {
            "heading": "3 METHOD",
            "text": "Given an input view y of an object, our target is to generate multiview images of the object. We assume that the object is located at the origin and is normalized inside a cube of length 1. The target images are generated on N fixed viewpoints looking at the object with azimuths evenly ranging from 0\u25e6 to 360\u25e6 and elevations of 30\u25e6. To improve the multiview consistency of generated images, we formulate this generation process as a multiview diffusion model. In the following, we begin with a review of diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020)."
        },
        {
            "heading": "3.1 DIFFUSION",
            "text": "Diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020) aim to learn a probability model p\u03b8(x0) = \u222b p\u03b8(x0:T )dx1:T where x0 is the data and x1:T := x1, ...,xT are latent variables. The joint distribution is characterized by a Markov Chain (reverse process)\np\u03b8(x0:T ) = p(xT ) T\u220f t=1 p\u03b8(xt\u22121|xt), (1)\nwhere p(xT ) = N (xT ;0, I) and p\u03b8(xt\u22121|xt) = N (xt\u22121;\u00b5\u03b8(xt, t), \u03c32t I). \u00b5\u03b8(xt, t) is a trainable component while the variance \u03c32t is untrained time-dependent constants (Ho et al., 2020). The target is to learn the \u00b5\u03b8 for the generation. To learn \u00b5\u03b8, a Markov chain called forward process is constructed as\nq(x1:T |x0) = T\u220f\nt=1\nq(xt|xt\u22121), (2)\nwhere q(xt|xt\u22121) = N (xt; \u221a 1\u2212 \u03b2txt\u22121, \u03b2tI) and \u03b2t are all constants. DDPM (Ho et al., 2020) shows that by defining\n\u00b5\u03b8(xt, t) = 1\u221a \u03b1t\n( xt \u2212\n\u03b2t\u221a 1\u2212 \u03b1\u0304t \u03f5\u03b8(xt, t)\n) , (3)\nwhere \u03b1t and \u03b1\u0304t are constants derived from \u03b2t and \u03f5\u03b8 is a noise predictor, we can learn \u03f5\u03b8 by \u2113 = Et,x0,\u03f5 [ \u2225\u03f5\u2212 \u03f5\u03b8( \u221a \u03b1\u0304tx0 + \u221a 1\u2212 \u03b1\u0304t\u03f5, t)\u22252 ] , (4)\nwhere \u03f5 is a random variable sampled from N (0, I)."
        },
        {
            "heading": "3.2 MULTIVIEW DIFFUSION",
            "text": "Applying the vanilla DDPM model to generate novel-view images separately would lead to difficulty in maintaining multiview consistency across different views. To address this problem, we formulate the generation process as a multiview diffusion model that correlates the generation of each view. Let us denote the N images that we want to generate on the predefined viewpoints as {x(1)0 , ...,x (N) 0 } where suffix 0 means the time step 0. We want to learn the joint distribution of all these views p\u03b8(x (1:N) 0 |y) := p\u03b8(x (1) 0 , ...,x (N) 0 |y). In the following discussion, all the probability functions are conditioned on the input view y so we omit y for simplicity.\nThe forward process of the multiview diffusion model is a direct extension of the vanilla DDPM in Eq. 2, where noises are added to every view independently by\nq(x (1:N) 1:T |x (1:N) 0 ) = T\u220f t=1 q(x (1:N) t |x (1:N) t\u22121 ) = T\u220f t=1 N\u220f n=1 q(x (n) t |x (n) t\u22121), (5)\nwhere q(x(n)t |x (n) t\u22121) = N (x (n) t ; \u221a 1\u2212 \u03b2tx(n)t\u22121, \u03b2tI). Similarly, following Eq. 1, the reverse process is constructed as\np\u03b8(x (1:N) 0:T ) = p(x (1:N) T ) T\u220f t=1 p\u03b8(x (1:N) t\u22121 |x (1:N) t ) = p(x (1:N) T ) T\u220f t=1 N\u220f n=1 p\u03b8(x (n) t\u22121|x (1:N) t ), (6)\nwhere p\u03b8(x (n) t\u22121|x (1:N) t ) = N (x (n) t\u22121;\u00b5 (n) \u03b8 (x (1:N) t , t), \u03c3 2 t I). Note that the second equation in Eq. 6 holds because we assume a diagonal variance matrix. However, the mean \u00b5(n)\u03b8 of n-th view x (n) t\u22121 depends on the states of all the views x(1:N)t . Similar to Eq. 3, we define \u00b5 (n) \u03b8 and the loss by\n\u00b5 (n) \u03b8 (x (1:N) t , t) = 1\u221a \u03b1t\n( x (n) t \u2212\n\u03b2t\u221a 1\u2212 \u03b1\u0304t \u03f5 (n) \u03b8 (x (1:N) t , t)\n) . (7)\n\u2113 = E t,x\n(1:N) 0 ,n,\u03f5 (1:N)\n[ \u2225\u03f5(n) \u2212 \u03f5(n)\u03b8 (x (1:N) t , t)\u22252 ] , (8)\nwhere \u03f5(1:N) is the standard Gaussian noise of size N \u00d7 H \u00d7W added to all N views, \u03f5(n) is the noise added to the n-th view, and \u03f5(n)\u03b8 is the noise predictor on the n-th view. Training procedure. In one training step, we first obtain N images x(1:N)0 of the same object from the dataset. Then, we sample a timestep t and the noise \u03f5(1:N) which is added to all the images x (1:N) 0 to obtain x (1:N) t . After that, we randomly select a view n and apply the corresponding noise predictor \u03f5(n)\u03b8 on the selected view to predict the noise. Finally, the L2 distance between the sampled noise \u03f5(n) and the predicted noise is computed as the loss for the training.\nSynchronized N -view noise predictor. The proposed multiview diffusion model can be regarded as N synchronized noise predictors {\u03f5(n)\u03b8 |n = 1, ..., N}. On each time step t, each noise predictor \u03f5(n) is in charge of predicting noise on its corresponding view x(n)t to get x (n) t\u22121. Meanwhile, these noise predictors are synchronized because, on every denoising step, every noise predictor exchanges information with each other by correlating the states x(1:N)t of all the other views. In practical implementation, we use a shared UNet for all N noise predictors and put the viewpoint difference between the input view and the n-th target view \u2206v(n), and the states x(1:N)t of all views as conditions to this shared noise predictor, i.e., \u03f5(n)\u03b8 (x (1:N) t , t) = \u03f5\u03b8(x (n) t ; t,\u2206v (n),x (1:N) t ). The detailed computation of the viewpoint difference can be found in the supplementary material."
        },
        {
            "heading": "3.3 3D-AWARE FEATURE ATTENTION FOR DENOISING",
            "text": "In this section, we discuss how to implement the synchronized noise predictor \u03f5\u03b8(x (n) t ; t,\u2206v (n),x (1:N) t ,y) by correlating the multiview features using a 3D-aware attention scheme. The overview is shown in Fig. 2.\nBackbone UNet. Similar to previous works (Ho et al., 2020; Rombach et al., 2022), our noise predictor \u03f5\u03b8 contains a UNet which takes a noisy image as input and then denoises the image. To ensure the generalization ability, we initialize the UNet from the pretrained weights of Zero123 (Liu et al., 2023b) which is a generalizable model with the ability to generate novel-view images from a given image of an object. Zero123 concatenates the input view with the noisy target view as the input to UNet. Then, to encode the viewpoint difference \u2206v(n) in UNet, Zero123 reuses the text attention layers of Stable Diffusion to process the concatenation of \u2206v(n) and the CLIP feature (Radford et al., 2021) of the input image. We follow the same design as Zero123 and empirically freeze the UNet and the text attention layers when training SyncDreamer. Experiments to verify these choices are presented in Sec. 4.4.\n3D-aware feature attention. The remaining problem is how to correlate the states x(1:N)t of all the target views for the denoising of the current noisy target view x(n)t . To enforce consistency among multiple generated views, it is desirable for the network to perceive the corresponding features in 3D space when generating the current image. To achieve this, we first construct a 3D volume with V 3 vertices and then project the vertices onto all the target views to obtain the features. The features from each target view are extracted by convolution layers and are concatenated to form a spatial feature volume. Next, a 3D CNN is applied to the feature volume to capture and process spatial relationships. In order to denoise n-th target view, we construct a view frustum that is pixel-wise aligned with this view, whose features are obtained by interpolating the features from the spatial volume. Finally, on every intermediate feature map of the current view in the UNet, we apply a new depth-wise attention layer to extract features from the pixel-wise aligned view-frustum feature volume along the depth dimension. The depth-wise attention is similar to the epipolar attention layers in Suhail et al. (2022); Zhou & Tulsiani (2023); Tseng et al. (2023); Yu et al. (2023b) as discussed in the supplementary material.\nDiscussion. There are two primary design considerations in this 3D-aware feature attention UNet. First, the spatial volume is constructed from all the target views and all the target views share the same spatial volume for denoising, which implies a global constraint that all target views are looking at the same object. Second, the added new attention layers only conduct attention along the depth dimension, which enforces a local epipolar line constraint that the feature for a specific location should be consistent with the corresponding features on the epipolar lines of other views."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "4.1 EXPERIMENT PROTOCOL",
            "text": "Evaluation dataset. Following (Liu et al., 2023b;a), we adopt the Google Scanned Object (Downs et al., 2022) dataset as the evaluation dataset. To demonstrate the generalization ability to arbitrary objects, we randomly chose 30 objects ranging from daily objects to animals. For each object, we render an image with a size of 256\u00d7256 as the input view. We additionally evaluate some images collected from the Internet and the Wiki of Genshin Impact. More results are included in the supplementary materials.\nBaselines. We adopt Zero123 (Liu et al., 2023b), RealFusion (Melas-Kyriazi et al., 2023), Magic123 (Qian et al., 2023), One-2-3-45 (Liu et al., 2023a), Point-E (Nichol et al., 2022) and Shap-\nE (Jun & Nichol, 2023) as baseline methods. Given an input image of an object, Zero123 (Liu et al., 2023b) is able to generate novel-view images of the same object from different viewpoints. Zero123 can also be incorporated with the SDS loss (Poole et al., 2023) for 3D reconstruction. We adopt the implementation of ThreeStudio (Guo et al., 2023) for reconstruction with Zero123, which includes many optimization strategies to achieve better reconstruction quality than the original Zero123 implementation. RealFusion (Melas-Kyriazi et al., 2023) is based on Stable Diffusion (Rombach et al., 2022) and the SDS loss for single-view reconstruction. Magic123 (Qian et al., 2023) combines Zero123 (Liu et al., 2023b) with RealFusion (Melas-Kyriazi et al., 2023) to further improve the reconstruction quality. One-2-3-45 (Liu et al., 2023a) directly regresses SDFs from the output images of Zero123 and we use the official hugging face online demo (Face, 2023) to produce the results. Point-E (Nichol et al., 2022) and Shap-E (Jun & Nichol, 2023) are 3D generative models trained on a large internal OpenAI 3D dataset, both of which are able to convert a single-view image into a point cloud or a shape encoded in an MLP. For Point-E, we convert the generated point clouds to SDFs for shape reconstruction using the official models.\nMetrics. We mainly focus on two tasks, novel view synthesis (NVS) and single view 3D reconstruction (SVR). On the NVS task, we adopt the commonly used metrics, i.e., PSNR, SSIM (Wang et al., 2004) and LPIPS (Zhang et al., 2018). To further demonstrate the multiview consistency of the generated images, we also run the MVS algorithm COLMAP (Scho\u0308nberger et al., 2016) on the generated images and report the reconstructed point number. Because MVS algorithms rely on multiview consistency to find correspondences to reconstruct 3D points, more consistent images would lead to more reconstructed points. On the SVR task, we report the commonly used Chamfer Distances (CD) and Volume IoU between ground-truth shapes and reconstructed shapes. Since the shapes generated by Point-E (Nichol et al., 2022) and Shap-E (Jun & Nichol, 2023) are defined in a different canonical coordinate system, we manually align the generated shapes of these two methods to the ground-truth shapes before computing these metrics. Considering randomness in the generation, we report the min, max, and average metrics on 8 objects in the supplementary material."
        },
        {
            "heading": "4.2 CONSISTENT NOVEL-VIEW SYNTHESIS",
            "text": "For this task, the quantitative results are shown in Table 1 and the qualitative results are shown in Fig. 3. By applying a NeRF model to distill the Stable Diffusion model (Poole et al., 2023; Rombach et al., 2022), RealFusion (Melas-Kyriazi et al., 2023) shows strong multiview consistency producing more reconstructed points but is unable to produce visually plausible images as shown in Fig. 3. Zero123 (Liu et al., 2023b) produces visually plausible images but the generated images are\nnot multiview-consistent. Our method is able to generate images that not only are semantically consistent with the input image but also maintain multiview consistency in colors and geometry. Meanwhile, for the same input image, Our method can generate different plausible instances using different random seeds as shown in Fig. 4."
        },
        {
            "heading": "4.3 SINGLE VIEW RECONSTRUCTION",
            "text": "We show the quantitative results in Table 2 and the qualitative comparison in Fig. 5. PointE (Nichol et al., 2022) and Shap-E (Jun & Nichol, 2023) tend to produce incompleted meshes. Directly distilling Zero123 (Liu et al., 2023b) generates shapes that are coarsely aligned with the input image, but the reconstructed surfaces are rough and not consistent with input images in detailed parts. Magic123 (Qian et al., 2023) produces much smoother meshes but heavily relies on the estimated depth values on the input view, which may lead to incorrect results when the depth estimator is not robust. One-2-3-45 (Liu et al., 2023a) reconstructs meshes from the\nmultiview-inconsistent outputs of Zero123, which is able to capture the general geometry but also loses details. In comparison, our method achieves the best reconstruction quality with smooth surfaces and detailed geometry."
        },
        {
            "heading": "4.4 DISCUSSIONS",
            "text": "In this section, we further conduct a set of experiments to evaluate the effectiveness of our designs.\nGeneralization ability. To show the generalization ability, we evaluate SyncDreamer with 2D designs or hand drawings like sketches, cartoons, and traditional Chinese ink paintings, which are usually created manually by artists and exhibit differences in lighting effects and color space from real-world images. The results are shown in Fig. 6. Despite the significant differences in lighting and shadow effects between these images and the real-world images, our algorithm is still able to perceive their reasonable 3D geometry and produce multiview-consistent images.\nWithout 3D-aware feature attention. To show how the proposed 3D-aware feature attention improves multiview consistency, we discard the 3D-aware attention module in SyncDreamer and train this model on the same training set. This actually corresponds to finetuning a Zero123 model with fixed viewpoints. As we can see in Fig. 7, such a model still cannot produce images with strong consistency, which demonstrates the necessity of the 3D-aware attention module in generating multiview-consistent images.\nInitializing from Stable Diffusion instead of Zero123 (Liu et al., 2023b). An alternative strategy is to initialize our model from Stable Diffusion (Rombach et al., 2022). However, the results shown in Fig. 7 indicate that initializing from Stable Diffusion exhibits a worse generalization ability than from Zero123. Based on our observations, we find that the batch size plays an important role in enhancing the stability and efficacy of learning 3D priors from a diverse dataset like Objaverse.\nHowever, due to limited GPU memories, our batch size is 192 which is smaller than the 1536 used by Zero123. Finetuning on Zero123 enables SyncDreamer to utilize the 3D priors of Zero123.\nTraining UNet. During the training of SyncDreamer, another feasible solution is to not freeze the UNet and the related layers initialized from Zero123 but further finetune them together with the volume condition module. As shown in Fig. 7, the model without freezing these layers tends to predict the input object as a thin plate, especially when the input images are 2D hand drawings. We speculate that this phenomenon is caused by overfitting, likely due to the numerous thin-plate objects within the Objaverse dataset and the fixed viewpoints employed during our training process.\nRuntime. SyncDreamer uses about 40s to sample 64 images (4 instances) with 50 DDIM (Song et al., 2020) sampling steps on a 40G A100 GPU. Our runtime is slightly longer than Zero123 because we need to construct the spatial feature volume on every step."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "In this paper, we present SyncDreamer to generate multiview-consistent images from a single-view image. SyncDreamer adopts a synchronized multiview diffusion to model the joint probability distribution of multiview images, which thus improves the multiview consistency. We design a novel architecture that uses the Zero123 as the backbone and a new volume condition module to model cross-view dependency. Extensive experiments demonstrate that SyncDreamer not only efficiently generates multiview images with strong consistency, but also achieves improved reconstruction quality compared to the baseline methods with excellent generalization to various input styles."
        },
        {
            "heading": "6 ACKNOWLEDGEMENT",
            "text": "This research is sponsored by the Innovation and Technology Commission of the HKSAR Government under the InnoHK initiative and Ref. T45-205/21-N of Hong Kong RGC. We sincerely thank Zhiyang Dou, Peng Wang, and Jiepeng Wang from AnySyn3D for discussions. This work is based on the computation resources from Tencent Taiji platform."
        }
    ],
    "year": 2024
}