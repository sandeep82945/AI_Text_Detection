{
    "abstractText": "In order to train networks for verified adversarial robustness, it is common to over-approximate the worst-case loss over perturbation regions, resulting in networks that attain verifiability at the expense of standard performance. As shown in recent work, better trade-offs between accuracy and robustness can be obtained by carefully coupling adversarial training with over-approximations. We hypothesize that the expressivity of a loss function, which we formalize as the ability to span a range of trade-offs between lower and upper bounds to the worst-case loss through a single parameter (the over-approximation coefficient), is key to attaining stateof-the-art performance. To support our hypothesis, we show that trivial expressive losses, obtained via convex combinations between adversarial attacks and IBP bounds, yield state-of-the-art results across a variety of settings in spite of their conceptual simplicity. We provide a detailed analysis of the relationship between the over-approximation coefficient and performance profiles across different expressive losses, showing that, while expressivity is essential, better approximations of the worst-case loss are not necessarily linked to superior robustness-accuracy trade-offs.",
    "authors": [
        {
            "affiliations": [],
            "name": "VERIFIED ROBUSTNESS"
        }
    ],
    "id": "SP:9190b997c7392c2423aa81a954f37d12a8be77e3",
    "references": [
        {
            "authors": [
                "Kendra Albert",
                "Maggie Delano",
                "Bogdan Kulynych",
                "Ram Shankar Siva Kumar"
            ],
            "title": "Adversarial for good? how the adversarial ml community\u2019s values impede socially beneficial uses of attacks",
            "venue": "In ICML 2021 workshop on A Blessing in Disguise: The Prospects and Perils of Adversarial Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Ross Anderson",
                "Joey Huchette",
                "Will Ma",
                "Christian Tjandraatmadja",
                "Juan Pablo Vielma"
            ],
            "title": "Strong mixed-integer programming formulations for trained neural networks",
            "venue": "Mathematical Programming,",
            "year": 2020
        },
        {
            "authors": [
                "Anish Athalye",
                "Nicholas Carlini",
                "David A. Wagner"
            ],
            "title": "Obfuscated gradients give a false sense of security: circumventing defenses to adversarial examples",
            "venue": "In International Conference on Machine Learning,",
            "year": 2018
        },
        {
            "authors": [
                "Stanley Bak",
                "Changliu Liu",
                "Taylor Johnson"
            ],
            "title": "The second international verification of neural networks competition (VNN-COMP 2021): Summary and results",
            "venue": "arXiv preprint arXiv:2109.00498,",
            "year": 2021
        },
        {
            "authors": [
                "Mislav Balunovic",
                "Martin Vechev"
            ],
            "title": "Adversarial training and provable defenses: Bridging the gap",
            "venue": "International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "E. Botoeva",
                "P. Kouvaros",
                "J. Kronqvist",
                "A. Lomuscio",
                "R. Misener"
            ],
            "title": "Efficient verification of neural networks via dependency analysis",
            "venue": "In AAAI Conference on Artificial Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Rudy Bunel",
                "Ilker Turkaslan",
                "Philip HS Torr",
                "Pushmeet Kohli",
                "M Pawan Kumar"
            ],
            "title": "A unified view of piecewise linear neural network verification",
            "venue": "Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Rudy Bunel",
                "Alessandro De Palma",
                "Alban Desmaison",
                "Krishnamurthy Dvijotham",
                "Pushmeet Kohli",
                "Philip HS Torr",
                "M Pawan Kumar"
            ],
            "title": "Lagrangian decomposition for neural network verification",
            "venue": "Conference on Uncertainty in Artificial Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Rudy Bunel",
                "Jingyue Lu",
                "Ilker Turkaslan",
                "P Kohli",
                "P Torr",
                "M Pawan Kumar"
            ],
            "title": "Branch and bound for piecewise linear neural network verification",
            "venue": "Journal of Machine Learning Research,",
            "year": 2020
        },
        {
            "authors": [
                "Rich Caruana"
            ],
            "title": "Multitask learning",
            "venue": "Machine Learning,",
            "year": 1997
        },
        {
            "authors": [
                "Patryk Chrabaszcz",
                "Ilya Loshchilov",
                "Frank Hutter"
            ],
            "title": "A downsampled variant of ImageNet as an alternative to the CIFAR datasets",
            "year": 2017
        },
        {
            "authors": [
                "Jeremy Cohen",
                "Elan Rosenfeld",
                "Zico Kolter"
            ],
            "title": "Certified adversarial robustness via randomized smoothing",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Francesco Croce",
                "Matthias Hein"
            ],
            "title": "Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Sumanth Dathathri",
                "Krishnamurthy (Dj) Dvijotham",
                "Alex Kurakin",
                "Aditi Raghunathan",
                "Jonathan Uesato",
                "Rudy Bunel",
                "Shreya Shankar",
                "Jacob Steinhardt",
                "Ian Goodfellow",
                "Percy Liang",
                "Pushmeet Kohli"
            ],
            "title": "Enabling certification of verification-agnostic networks via memory-efficient semidefinite programming",
            "venue": "In Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Alessandro De Palma",
                "Harkirat Singh Behl",
                "Rudy Bunel",
                "Philip H.S. Torr",
                "M. Pawan Kumar"
            ],
            "title": "Scaling the convex barrier with active sets",
            "venue": "International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Alessandro De Palma",
                "Harkirat Singh Behl",
                "Rudy Bunel",
                "Philip H.S. Torr",
                "M. Pawan Kumar"
            ],
            "title": "Scaling the convex barrier with sparse dual algorithms",
            "venue": "arXiv preprint arXiv:2101.05844,",
            "year": 2021
        },
        {
            "authors": [
                "Alessandro De Palma",
                "Rudy Bunel",
                "Alban Desmaison",
                "Krishnamurthy Dvijotham",
                "Pushmeet Kohli",
                "Philip HS Torr",
                "M Pawan Kumar"
            ],
            "title": "Improved branch and bound for neural network verification via Lagrangian decomposition",
            "venue": "arXiv preprint arXiv:2104.06718,",
            "year": 2021
        },
        {
            "authors": [
                "Alessandro De Palma",
                "Rudy Bunel",
                "Krishnamurthy Dvijotham",
                "M. Pawan Kumar",
                "Robert Stanforth"
            ],
            "title": "IBP regularization for verified adversarial robustness via branch-and-bound",
            "venue": "In ICML 2022 Workshop on Formal Verification of Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Jia Deng",
                "Wei Dong",
                "Richard Socher",
                "Li-Jia Li",
                "Kai Li",
                "Li Fei-Fei"
            ],
            "title": "Imagenet: A large-scale hierarchical image database",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2009
        },
        {
            "authors": [
                "Yinpeng Dong",
                "Fangzhou Liao",
                "Tianyu Pang",
                "Hang Su",
                "Jun Zhu",
                "Xiaolin Hu",
                "Jianguo Li"
            ],
            "title": "Boosting adversarial attacks with momentum",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Ruediger Ehlers"
            ],
            "title": "Formal verification of piece-wise linear feed-forward neural networks. Automated Technology for Verification and Analysis, 2017",
            "year": 2017
        },
        {
            "authors": [
                "Jiameng Fan",
                "Wenchao Li"
            ],
            "title": "Adversarial training and provable robustness: A tale of two objectives",
            "venue": "In AAAI Conference on Artificial Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Alhussein Fawzi",
                "Matej Balog",
                "Aja Huang",
                "Thomas Hubert",
                "Bernardino Romera-Paredes",
                "Mohammadamin Barekatain",
                "Alexander Novikov",
                "Francisco J R Ruiz",
                "Julian Schrittwieser",
                "Grzegorz Swirszcz"
            ],
            "title": "Discovering faster matrix multiplication algorithms with reinforcement learning",
            "year": 2022
        },
        {
            "authors": [
                "Claudio Ferrari",
                "Mark Niklas Mueller",
                "Nikola Jovanovi\u0107",
                "Martin Vechev"
            ],
            "title": "Complete verification via multi-neuron relaxation guided branch-and-bound",
            "venue": "International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Ian J Goodfellow",
                "Jonathon Shlens",
                "Christian Szegedy"
            ],
            "title": "Explaining and harnessing adversarial examples",
            "venue": "International Conference on Learning Representations,",
            "year": 2015
        },
        {
            "authors": [
                "Sven Gowal",
                "Krishnamurthy Dvijotham",
                "Robert Stanforth",
                "Rudy Bunel",
                "Chongli Qin",
                "Jonathan Uesato",
                "Timothy Mann",
                "Pushmeet Kohli"
            ],
            "title": "On the effectiveness of interval bound propagation for training verifiably robust models",
            "venue": "Workshop on Security in Machine Learning,",
            "year": 2018
        },
        {
            "authors": [
                "P. Henriksen",
                "A. Lomuscio"
            ],
            "title": "Efficient neural network verification via adaptive refinement and adversarial search",
            "venue": "In European Conference on Artificial Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "P. Henriksen",
                "A. Lomuscio"
            ],
            "title": "Deepsplit: An efficient splitting method for neural network verification via indirect effect analysis",
            "venue": "In Proceedings of the 30th International Joint Conference on Artificial Intelligence",
            "year": 2021
        },
        {
            "authors": [
                "Timothy Hickey",
                "Qun Ju",
                "Maarten H Van Emden"
            ],
            "title": "Interval arithmetic: From principles to implementation",
            "venue": "Journal of the ACM (JACM),",
            "year": 2001
        },
        {
            "authors": [
                "Yujia Huang",
                "Huan Zhang",
                "Yuanyuan Shi",
                "J Zico Kolter",
                "Anima Anandkumar"
            ],
            "title": "Training certifiably robust neural networks with efficient local lipschitz bounds",
            "venue": "In Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Sergey Ioffe",
                "Christian Szegedy"
            ],
            "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
            "venue": "In International Conference on Machine Learning,",
            "year": 2015
        },
        {
            "authors": [
                "Nikola Jovanovi\u0107",
                "Mislav Balunovi\u0107",
                "Maximilian Baader",
                "Martin T. Vechev"
            ],
            "title": "On the paradox of certified training",
            "venue": "In Transactions on Machine Learning Research,",
            "year": 2022
        },
        {
            "authors": [
                "John Jumper",
                "Richard Evans",
                "Alexander Pritzel",
                "Tim Green",
                "Michael Figurnov",
                "Olaf Ronneberger",
                "Kathryn Tunyasuvunakool",
                "Russ Bates",
                "Augustin \u017d\u00eddek",
                "Anna Potapenko"
            ],
            "title": "Highly accurate protein structure prediction with alphafold",
            "year": 2021
        },
        {
            "authors": [
                "Guy Katz",
                "Clark Barrett",
                "David Dill",
                "Kyle Julian",
                "Mykel Kochenderfer"
            ],
            "title": "Reluplex: An efficient SMT solver for verifying deep neural networks",
            "venue": "Computer Aided Verification,",
            "year": 2017
        },
        {
            "authors": [
                "A. Krizhevsky",
                "G. Hinton"
            ],
            "title": "Learning multiple layers of features from tiny images",
            "venue": "Master\u2019s thesis,",
            "year": 2009
        },
        {
            "authors": [
                "Vitaly Kurin",
                "Alessandro De Palma",
                "Ilya Kostrikov",
                "Shimon Whiteson",
                "M. Pawan Kumar"
            ],
            "title": "In defense of the unitary scalarization for deep multi-task learning",
            "venue": "In Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "J. Lan",
                "Y. Zheng",
                "A. Lomuscio"
            ],
            "title": "Tight neural network verification via semidefinite relaxations and linear reformulations",
            "venue": "In AAAI Conference on Artificial Intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "Ya Le",
                "Xuan S. Yang"
            ],
            "title": "Tiny imagenet visual recognition challenge",
            "year": 2015
        },
        {
            "authors": [
                "Yann LeCun",
                "Corinna Cortes",
                "CJ Burges"
            ],
            "title": "Mnist handwritten digit database",
            "venue": "ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist,",
            "year": 2010
        },
        {
            "authors": [
                "Sungyoon Lee",
                "Woojin Lee",
                "Jinseong Park",
                "Jaewook Lee"
            ],
            "title": "Towards better understanding of training certifiably robust models against adversarial examples",
            "venue": "In Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Klas Leino",
                "Zifan Wang",
                "Matt Fredrikson"
            ],
            "title": "Globally-robust neural networks",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Alessio Lomuscio",
                "Lalit Maganti"
            ],
            "title": "An approach to reachability analysis for feed-forward",
            "venue": "ReLU neural networks",
            "year": 2017
        },
        {
            "authors": [
                "Aleksander Madry",
                "Aleksandar Makelov",
                "Ludwig Schmidt",
                "Dimitris Tsipras",
                "Adrian Vladu"
            ],
            "title": "Towards deep learning models resistant to adversarial attacks",
            "venue": "International Conference on Learning Representations,",
            "year": 2018
        },
        {
            "authors": [
                "Yuhao Mao",
                "Mark Niklas M\u00fcller",
                "Marc Fischer",
                "Martin Vechev"
            ],
            "title": "TAPS: Connecting certified and adversarial training",
            "year": 2023
        },
        {
            "authors": [
                "Yuhao Mao",
                "Mark Niklas M\u00fcller",
                "Marc Fischer",
                "Martin Vechev"
            ],
            "title": "Understanding certified training with interval bound propagation",
            "year": 2023
        },
        {
            "authors": [
                "Matthew Mirman",
                "Timon Gehr",
                "Martin Vechev"
            ],
            "title": "Differentiable abstract interpretation for provably robust neural networks",
            "venue": "International Conference on Machine Learning,",
            "year": 2018
        },
        {
            "authors": [
                "Mark Niklas M\u00fcller",
                "Christopher Brix",
                "Stanley Bak",
                "Changliu Liu",
                "Taylor Johnson"
            ],
            "title": "The second international verification of neural networks competition (VNN-COMP 2021): Summary and results",
            "venue": "arXiv preprint arXiv:2212.10376,",
            "year": 2022
        },
        {
            "authors": [
                "Mark Niklas M\u00fcller",
                "Gleb Makarchuk",
                "Gagandeep Singh",
                "Markus P\u00fcschel",
                "Martin Vechev"
            ],
            "title": "PRIMA: General and precise neural network certification via scalable convex hull approximations",
            "venue": "Proceedings of the ACM on Programming Languages,",
            "year": 2022
        },
        {
            "authors": [
                "Mark Niklas M\u00fcller",
                "Franziska Eckert",
                "Marc Fischer",
                "Martin Vechev"
            ],
            "title": "Certified training: Small boxes are all you need",
            "venue": "In International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Adam Paszke",
                "Sam Gross",
                "Francisco Massa",
                "Adam Lerer",
                "James Bradbury",
                "Gregory Chanan",
                "Trevor Killeen",
                "Zeming Lin",
                "Natalia Gimelshein",
                "Luca Antiga",
                "Alban Desmaison",
                "Andreas Kopf",
                "Edward Yang",
                "Zachary DeVito",
                "Martin Raison",
                "Alykhan Tejani",
                "Sasank Chilamkurthy",
                "Benoit Steiner",
                "Lu Fang",
                "Junjie Bai",
                "Soumith Chintala"
            ],
            "title": "Pytorch: An imperative style, high-performance deep learning library",
            "venue": "Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Aditi Raghunathan",
                "Jacob Steinhardt",
                "Percy S Liang"
            ],
            "title": "Semidefinite relaxations for certifying robustness to adversarial examples",
            "venue": "Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Hadi Salman",
                "Greg Yang",
                "Jerry Li",
                "Pengchuan Zhang",
                "Huan Zhang",
                "Ilya Razenshteyn",
                "S\u00e9bastien Bubeck"
            ],
            "title": "Provably robust deep learning via adversarially trained smoothed classifiers",
            "venue": "In Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Zhouxing Shi",
                "Yihan Wang",
                "Huan Zhang",
                "Jinfeng Yi",
                "Cho-Jui Hsieh"
            ],
            "title": "Fast certified robust training with short warmup",
            "venue": "In Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Karen Simonyan",
                "Andrew Zisserman"
            ],
            "title": "Very deep convolutional networks for large-scale image recognition",
            "venue": "In International Conference on Learning Representations,",
            "year": 2015
        },
        {
            "authors": [
                "Gagandeep Singh",
                "Timon Gehr",
                "Matthew Mirman",
                "Markus P\u00fcschel",
                "Martin Vechev"
            ],
            "title": "Fast and effective robustness certification",
            "venue": "Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Gagandeep Singh",
                "Rupanshu Ganvir",
                "Markus P\u00fcschel",
                "Martin Vechev"
            ],
            "title": "Beyond the single neuron convex barrier for neural network certification",
            "venue": "Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Gagandeep Singh",
                "Timon Gehr",
                "Markus P\u00fcschel",
                "Martin Vechev"
            ],
            "title": "An abstract domain for certifying neural networks",
            "venue": "Proceedings of the ACM on Programming Languages,",
            "year": 2019
        },
        {
            "authors": [
                "Naman D Singh",
                "Francesco Croce",
                "Matthias Hein"
            ],
            "title": "Revisiting adversarial training for ImageNet: Architectures, training and generalization across threat models",
            "year": 2023
        },
        {
            "authors": [
                "Sahil Singla",
                "Soheil Feizi"
            ],
            "title": "Skew orthogonal convolutions",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Sahil Singla",
                "Soheil Feizi"
            ],
            "title": "Improved techniques for deterministic l2 robustness",
            "venue": "In Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Sahil Singla",
                "Surbhi Singla",
                "Soheil Feizi"
            ],
            "title": "Improved deterministic l2 robustness on CIFAR-10 and CIFAR-100",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Teruo Sunaga"
            ],
            "title": "Theory of an interval algebra and its application to numerical analysis",
            "venue": "RAAG Memoirs,",
            "year": 1958
        },
        {
            "authors": [
                "Christian Szegedy",
                "Wojciech Zaremba",
                "Ilya Sutskever",
                "Joan Bruna",
                "Dumitru Erhan",
                "Ian Goodfellow",
                "Rob Fergus"
            ],
            "title": "Intriguing properties of neural networks",
            "venue": "International Conference on Learning Representations,",
            "year": 2014
        },
        {
            "authors": [
                "Christian Tjandraatmadja",
                "Ross Anderson",
                "Joey Huchette",
                "Will Ma",
                "Krunal Patel",
                "Juan Pablo Vielma"
            ],
            "title": "The convex relaxation barrier, revisited: Tightened single-neuron relaxations for neural network verification",
            "venue": "Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Asher Trockman",
                "J. Zico Kolter"
            ],
            "title": "Orthogonalizing convolutional layers with the cayley transform",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Jonathan Uesato",
                "Brendan O\u2019Donoghue",
                "Aaron van den Oord",
                "Pushmeet Kohli"
            ],
            "title": "Adversarial risk and the dangers of evaluating against weak attacks",
            "venue": "In International Conference on Machine Learning,",
            "year": 2018
        },
        {
            "authors": [
                "Shiqi Wang",
                "Huan Zhang",
                "Kaidi Xu",
                "Xue Lin",
                "Suman Jana",
                "Cho-Jui Hsieh",
                "J Zico Kolter"
            ],
            "title": "Beta-CROWN: Efficient bound propagation with per-neuron split constraints for complete and incomplete neural network verification",
            "venue": "Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Yihan Wang",
                "Zhouxing Shi",
                "Quanquan Gu",
                "Cho-Jui Hsieh"
            ],
            "title": "On the convergence of certified robust training with interval bound propagation",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Eric Wong",
                "Zico Kolter"
            ],
            "title": "Provable defenses against adversarial examples via the convex outer adversarial polytope",
            "venue": "International Conference on Machine Learning,",
            "year": 2018
        },
        {
            "authors": [
                "Eric Wong",
                "Frank Schmidt",
                "Jan Hendrik Metzen",
                "J. Zico Kolter"
            ],
            "title": "Scaling provable adversarial defenses",
            "venue": "Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Eric Wong",
                "Leslie Rice",
                "J. Zico Kolter"
            ],
            "title": "Fast is better than free: Revisiting adversarial training",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Kai Xiao",
                "Vincent Tjeng",
                "Nur Muhammad Shafiullah",
                "Aleksander Madry"
            ],
            "title": "Training for faster adversarial robustness verification via inducing relu stability",
            "venue": "International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "Derrick Xin",
                "Behrooz Ghorbani",
                "Ankush Garg",
                "Orhan Firat",
                "Justin Gilmer"
            ],
            "title": "Do current multi-task optimization methods in deep learning even help",
            "venue": "In Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Kaidi Xu",
                "Zhouxing Shi",
                "Huan Zhang",
                "Yihan Wang",
                "Kai-Wei Chang",
                "Minlie Huang",
                "Bhavya Kailkhura",
                "Xue Lin",
                "Cho-Jui Hsieh"
            ],
            "title": "Automatic perturbation analysis for scalable certified robustness and beyond",
            "venue": "In Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Kaidi Xu",
                "Huan Zhang",
                "Shiqi Wang",
                "Yihan Wang",
                "Suman Jana",
                "Xue Lin",
                "Cho-Jui Hsieh"
            ],
            "title": "Fast and complete: Enabling complete neural network verification with rapid and massively parallel incomplete verifiers",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Xiaojun Xu",
                "Linyi Li",
                "Bo Li"
            ],
            "title": "Lot: Layer-wise orthogonal training on improving l2 certified robustness",
            "venue": "In Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Bohang Zhang",
                "Tianle Cai",
                "Zhou Lu",
                "Di He",
                "Liwei Wang"
            ],
            "title": "Towards certifying l-infinity robustness using neural networks with l-inf-dist neurons",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Bohang Zhang",
                "Du Jiang",
                "Di He",
                "Liwei Wang"
            ],
            "title": "Boosting the certified robustness of l-infinity distance nets",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Bohang Zhang",
                "Du Jiang",
                "Di He",
                "Liwei Wang"
            ],
            "title": "Rethinking lipschitz neural networks and certified robustness: A boolean function perspective",
            "venue": "In Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Huan Zhang",
                "Tsui-Wei Weng",
                "Pin-Yu Chen",
                "Cho-Jui Hsieh",
                "Luca Daniel"
            ],
            "title": "Efficient neural network robustness certification with general activation functions",
            "venue": "Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Huan Zhang",
                "Hongge Chen",
                "Chaowei Xiao",
                "Sven Gowal",
                "Robert Stanforth",
                "Bo Li",
                "Duane Boning",
                "Cho-Jui Hsieh"
            ],
            "title": "Towards stable and efficient training of verifiably robust neural networks",
            "venue": "International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Huan Zhang",
                "Shiqi Wang",
                "Kaidi Xu",
                "Linyi Li",
                "Bo Li",
                "Suman Jana",
                "Cho-Jui Hsieh",
                "J Zico Kolter"
            ],
            "title": "General cutting planes for bound-propagation-based neural network verification",
            "venue": "Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Shi et al",
                "M\u00fcller"
            ],
            "title": "2023), we normalize all datasets and use random horizontal flips and random cropping as data augmentation on CIFAR-10, TinyImageNet and downscaled ImageNet. Complying with common experimental practice",
            "venue": "Gowal et al",
            "year": 2023
        },
        {
            "authors": [
                "2021 Shi et al",
                "2023 M\u00fcller et al",
                "Mao"
            ],
            "title": "2023a). In particular, mirroring recent work (M\u00fcller et al., 2023; Mao et al., 2023a), we employ a version modified by Shi et al. (2021) to employ BatchNorm (Ioffe & Szegedy, 2015) after every layer. Complying with Shi et al. (2021), at training time we compute IBP bounds to BatchNorm layers using the batch statistics from unperturbed data",
            "year": 2021
        },
        {
            "authors": [
                "Shi"
            ],
            "title": "2021), we use train = 0.2 for = 0.1 and train = 0.4 for = 0.3",
            "venue": "On MNIST,",
            "year": 2021
        },
        {
            "authors": [
                "Shi"
            ],
            "title": "Training starts with the standard cross-entropy loss during warm-up. Then, the perturbation radius is gradually increased to the target value train (using the same schedule as Shi et al. (2021)) for a fixed number of epochs during the ramp-up phase. During warmp-up and ramp-up, the regularization introduced by Shi et al",
            "year": 2021
        },
        {
            "authors": [
                "2022 De Palma et al",
                "2023 M\u00fcller et al",
                "Mao"
            ],
            "title": "2023a), we employ `1 regularization throughout training. Hyper-parameters The hyper-parameter configurations used in table 1 are presented in table 3. The training schedules for MNIST and CIFAR-10 with = 2/255 follow the hyper-parameters from Shi et al. (2021)",
            "year": 2023
        },
        {
            "authors": [
                "M\u00fcller"
            ],
            "title": "On TinyImageNet, we employ the same configuration as CIFAR-10 with = 2/255, and ImageNet64 relies on the TinyImageNet schedule from Shi et al. (2021), who did not benchmark on it. Following previous work (Shi et al., 2021; M\u00fcller et al., 2023), we use a batch size of 256 for MNIST, and of 128 everywhere else",
            "year": 2023
        },
        {
            "authors": [
                "Balunovic",
                "2020 Vechev",
                "2022). As Shi De Palma et al"
            ],
            "title": "2021), we clip target epsilon values to the fifth decimal digit. The main object of our tuning were the \u03b1 coefficients from the CC-IBP and MTL-IBP losses (see \u00a74) and the `1 regularization coefficient: for both, we first selected an order of magnitude, then the significand via manual",
            "year": 2021
        },
        {
            "authors": [
                "Shi"
            ],
            "title": "GPUs of the following models: Nvidia Titan V, Nvidia Titan XP, and Nvidia Titan XP. The reported timings for the training experiments (table 1) were run on a Nvidia Titan V GPU and using the 20-core Intel i9-7900X CPU",
            "venue": "RTX",
            "year": 2080
        },
        {
            "authors": [
                "Wong"
            ],
            "title": "2020) to prevent catastrophic overfitting. Table 8 does not display significant performance differences depending on the step size, highlighting the lack of catastrophic overfitting. We speculate that this is linked to the regularizing effect of the IBP bounds employed within CC-IBP and MTL-IBP",
            "venue": "On all considered settings,",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "In spite of recent and highly-publicized successes (Jumper et al., 2021; Fawzi et al., 2022), serious concerns over the trustworthiness of deep learning systems remain. In particular, the vulnerability of neural networks to adversarial examples (Szegedy et al., 2014; Goodfellow et al., 2015) questions their applicability to safety-critical domains. As a result, many authors devised techniques to formally prove the adversarial robustness of trained neural networks (Lomuscio & Maganti, 2017; Ehlers, 2017; Katz et al., 2017). These algorithms solve a non-convex optimization problem by coupling network over-approximations with search techniques (Bunel et al., 2018). While the use of network verifiers was initially limited to models with hundreds of neurons, the scalability of these tools has steadily increased over the recent years (Bunel et al., 2020a; Botoeva et al., 2020; Henriksen & Lomuscio, 2020; Xu et al., 2021; De Palma et al., 2021c; Wang et al., 2021; Henriksen & Lomuscio, 2021; Ferrari et al., 2022), reaching networks of the size of VGG16 (Simonyan & Zisserman, 2015; M\u00fcller et al., 2022). Nevertheless, significant gaps between state-of-the-art verified and empirical robustness persist, the latter referring to specific adversarial attacks. For instance, on ImageNet (Deng et al., 2009), Singh et al. (2023) attain an empirical robustness of 57.70% to `\u221e perturbations of radius = 4/255, whereas the state-of-the-art verified robust accuracy (on a downscaled version of the dataset (Chrabaszcz et al., 2017)) is 9.54% against = 1/255 (Zhang et al., 2022b). In fact, while empirical robustness can be achieved by training the networks against inexpensive attacks (adversarial training) (Madry et al., 2018; Wong et al., 2020), the resulting networks remain hard to verify.\nIn order to train networks amenable to formal robustness verification (verified training), a number of works (Mirman et al., 2018; Gowal et al., 2018; Wong & Kolter, 2018; Wong et al., 2018; Zhang et al., 2020; Xu et al., 2020; Shi et al., 2021) directly compute the loss over the network over-approximations that are later used for formal verification (verified loss). The employed approximations are typically loose, as the difficulty of the optimization problem associated with training increases with approximation tightness, resulting in losses that are hard to train against (Lee et al., 2021; Jovanovic\u0301 et al., 2022). As a result, while the resulting models are easy to verify, these networks display sub-par standard accuracy. Another line of work enhances adversarial training with verifiability-inducing techniques (Xiao et al., 2019; Balunovic & Vechev, 2020; De Palma et al., 2022): by exploiting stronger verifiers, these algorithms yield better results against small perturbations, but under-perform in other\nsettings. More recently, SABR (M\u00fcller et al., 2023), combined the advantages of both strategies by computing the loss on over-approximations of small \u201cboxes\" around adversarial attacks and tuning the size of these boxes on each setting, attaining state-of-the-art performance. These results were further improved, at the expense of conceptual simplicity and with increased overhead, by carefully coupling IBP or SABR with latent-space adversarial attacks (Mao et al., 2023a). While recent work combine attacks with over-approximations in different ways, they typically seek to accurately represent the exact worst-case loss over perturbation regions (M\u00fcller et al., 2023; Mao et al., 2023a).\nAiming to provide a novel understanding of the recent verified training literature, we define a class of loss functions, which we call expressive, that range from the adversarial to the verified loss when tuning a parameter \u03b1 \u2208 [0, 1]. We hypothesize that this notion of expressivity is crucial to produce state-of-the-art trade-offs between accuracy and verified robustness, and provide support as follows:\n\u2022 We show that SABR is expressive, and illustrate how other expressive losses can be trivially designed via convex combinations, presenting two examples: (i) CC-IBP, based on combinations between adversarial and over-approximated network outputs within the loss, (ii) MTL-IBP, relying on combinations between the adversarial and verified losses. \u2022 We present a comprehensive experimental evaluation of CC-IBP and MTL-IBP on\nbenchmarks from the robust vision literature. In spite of their conceptual simplicity, both methods attain state-of-the-art performance, demonstrating the importance of expressivity for verified robustness. In particular, improvements upon literature results are relatively large on TinyImageNet (Le & Yang, 2015) and downscaled (64\u00d764) ImageNet (Chrabaszcz et al., 2017), where MTL-IBP improves on both standard and verified robust accuracy by from 1.98% to 3.92% points while only adding minimal overhead compared to standard IBP training. We provide code as part of the supplementary material. \u2022 We analyze the effect of the \u03b1 parameter on robustness-accuracy trade-offs, highlighting\nthe importance of expressivity in order to maximize performance. Differently from common assumptions in the area, for CC-IBP, MTL-IBP and SABR, better approximations of the worst-case loss do not necessarily correspond to performance improvements."
        },
        {
            "heading": "2 BACKGROUND",
            "text": "We employ the following notation: uppercase letters for matrices (for example, A), boldface letters for vectors (for example, a), brackets for intervals and vector indices ([a,b] and a[i], respectively), for the Hadamard product, J\u00b7K for integer ranges, [a]+ := max(a,0) and [a]\u2212 := min(a,0). Let f : RS \u00d7 Rd \u2192 Ro be a neural network with parameters \u03b8 \u2208 RS and accepting d-dimensional inputs. Given a dataset (x,y) \u223c D with points x \u2208 Rd and labels y \u2208 Rl, the goal of robust training is to obtain a network that satisfies a given property P : Ro \u00d7 Rl \u2192 {0, 1} on D:\n(x,y) \u223c D =\u21d2 P (f(\u03b8,x),y). We will focus on classification problems, with scalar labels y \u2208 N, and on verifying robustness to adversarial perturbations around the inputs. Defining z(\u03b8,x, y) := (1 f(\u03b8,x)[y]\u2212 f(\u03b8,x)) and given a perturbation domain C(x, ) := {x0 : \u2016x0 \u2212 x\u2016p \u2264 }, we can write:\nP (f(\u03b8,x), y) = [( argmini 6=y z(\u03b8,x0, y)[i] > 0 ) \u2200 x0 \u2208 C(x, ) ] . (1)"
        },
        {
            "heading": "2.1 ADVERSARIAL TRAINING",
            "text": "Given a loss function L : Ro \u00d7 Rl \u2192 R, a network is typically trained by optimizing min\u03b8 E(x,y)\u2208D [L(f(\u03b8,x),y)] through variants of Stochastic Gradient Descent (SGD). Hence, training for adversarial robustness requires a surrogate loss adequately representing equation (1). Borrowing from the robust optimization literature, Madry et al. (2018) introduce the so-called robust loss:\nL\u2217(f(\u03b8,x), y) := max x\u2032\u2208C(x, ) L(f(\u03b8,x\u2032), y). (2)\nHowever, given the non-convexity of the maximization problem, Madry et al. (2018) propose to approximate its computation by a lower bound obtained via a local optimizer (adversarial attack). Denoting by xadv \u2208 C(x, ) the point returned by the attack, the adversarial loss is defined as follows:\nLadv(f(\u03b8,x), y) := L(f(\u03b8,xadv), y) \u2264 L\u2217(f(\u03b8,x), y). (3)\nAdversarially trained models typically display strong empirical robustness to adversarial attacks and a reduced standard accuracy compared to models not trained for robustness. However, formal verification methods fail to formally prove their robustness in a feasible time (Xiao et al., 2019). Furthermore, as repeatedly shown in the past (Uesato et al., 2018; Athalye et al., 2018), these defenses may break under stronger or conceptually different attacks (Croce & Hein, 2020). As a result, a number of algorithms have been designed to make networks more amenable to formal verification."
        },
        {
            "heading": "2.2 TRAINING FOR VERIFIED ROBUSTNESS",
            "text": "As seen from equation (1), a network is provably robust if one can show that all logit differences z(\u03b8,x0, y), except the one corresponding to the ground truth, are positive for all x0 \u2208 C(x, )."
        },
        {
            "heading": "2.2.1 NEURAL NETWORK VERIFICATION",
            "text": "Formal verification amounts to computing the sign of the following optimization problem:\nmin x0 min i6=y\nz(\u03b8,x0, y)[i] s.t. x0 \u2208 C(x, ) (4)\nProblem (4) is known to be NP-hard (Katz et al., 2017). Indeed, solving it is as hard as exactly computing the robust loss from equation (2). Therefore, it is typically replaced by more tractable lower bounds (incomplete verification): if the bound is positive, then the network is proved to be robust. Incomplete verifiers will only prove a subset of the properties, with tighter approximations leaving fewer properties undecided. Let us denote lower bounds to the logit differences by:\n\u00af z(\u03b8,x, y) \u2264 z(\u03b8,x0, y) \u2200 x0 \u2208 C(x, ).\nFurthermore, let Wk be the weight matrix of the k-th layer of the network outputting z(\u03b8,x0, y), and bk its bias. An immediate application of interval arithmetic (Sunaga, 1958; Hickey et al., 2001) to neural networks, Interval Bound Propagation (IBP) (Gowal et al., 2018; Mirman et al., 2018), provides a popular and inexpensive incomplete verifier. For a n-layer ReLU network, it computes\n\u00af z(\u03b8,x, y) as l\u0302n from the following iterative procedure:\nl\u03021 = minx0\u2208C(x, )W1x0 + b1 u\u03021 = maxx0\u2208C(x, )W1x0 + b1 { u\u0302k = [Wk]+ [u\u0302k\u22121]+ + [Wk]\u2212 [\u0302lk\u22121]+ + bk l\u0302k = [Wk]+ [\u0302lk\u22121]+ + [Wk]\u2212[u\u0302k\u22121]+ + bk } k \u2208 J2, nK .\nIBP corresponds to solving an approximation of problem (4) where all the network layers are replaced by their bounding boxes. For instance, given some l\u0302 and u\u0302 from the above procedure, the ReLUs are replaced by the over-approximation depicted in figure 1. The cost of computing\n\u00af z(\u03b8,x, y) via IBP\nroughly corresponds to four network evaluations.\nl\u0302 u\u0302\nx = u\u0302\nx\u0302\nx\nFigure 1: IBP ReLU overapproximation, with x\u0302 \u2208 [l\u0302, u\u0302] for the considered input domain.\nIf an exact solution to (the sign of) problem (4) is required, incomplete verifiers can be plugged in into a branch-and-bound framework (Bunel et al., 2018; 2020b). These algorithms provide an answer on any given property (complete verification) by recursively splitting the optimization domain (branching), and computing bounds to the solutions of the sub-problems. Lower bounds are obtained via incomplete verifiers, whereas adversarial attacks provide upper bounds. For ReLU activations and high-dimensional inputs, branching is typically performed implicitly by splitting a ReLU into its two linear pieces De Palma et al. (2021c); Ferrari et al. (2022)."
        },
        {
            "heading": "2.2.2 TRAINING METHODS",
            "text": "The verified training literature commonly employs the following assumption (Wong & Kolter, 2018): Assumption 2.1. The loss function L is translation-invariant: L(\u2212z(\u03b8,x, y), y) = L(f(\u03b8,x), y)). Furthermore, L(\u2212z(\u03b8,x, y), y) is monotonic increasing with respect to \u2212z(\u03b8,x0, y)[i] if i 6= y.\nAssumption 2.1 holds for popular loss functions such as cross-entropy. In this case, one can provide an upper bound to the robust loss L\u2217 via the so-called verified loss Lver(f(\u03b8,x), y), computed on a lower bound to the logit differences\n\u00af z(\u03b8,x, y) obtained from an incomplete verifier (Wong & Kolter, 2018):\nL\u2217(f(\u03b8,x), y) \u2264 Lver(f(\u03b8,x), y) := L(\u2212 \u00af z(\u03b8,x, y), y). (5)\nA number of verified training algorithms employ a loss of the form Lver(f(\u03b8,x), y) above as fundamental component, where bounds \u2212\n\u00af z(\u03b8,x, y) are obtained via IBP (Gowal et al., 2018;\nShi et al., 2021), inexpensive linear relaxations (Wong & Kolter, 2018), or a mixture of the two (Zhang et al., 2020). IBP training has been extensively studied (Wang et al., 2022; Mao et al., 2023b). In order to stabilize training, the radius of the perturbation over which the bounds are computed is gradually increased from 0 to the target value (ramp-up). Furthermore, earlier approaches (Gowal et al., 2018; Zhang et al., 2020) may linearly transition from the natural to the robust loss: (1\u2212 \u03ba) L(f(\u03b8,x), y) + \u03ba Lver(f(\u03b8,x), y), with \u03ba linearly increasing from 0 to 0.5 or 1 during ramp-up. Zhang et al. (2020) may further transition from lower bounds\n\u00af z(\u03b8,x, y) partly obtained\nvia linear relaxations (CROWN-IBP) to IBP bounds within Lver(f(\u03b8,x), y): (1\u2212 \u03ba) L(f(\u03b8,x), y) + \u03ba L(\u2212 ((1\u2212 \u03b2)\n\u00af zIBP(\u03b8,x, y) + \u03b2 \u00af zCROWN-IBP(\u03b8,x, y)) , y), with \u03b2 linearly decreased from 1 to 0. A\nmore recent method (Shi et al., 2021) removes both transitions and significantly reduces training times by employing BatchNorm (Ioffe & Szegedy, 2015) along with specialized initialization and regularization. These algorithms attain strong verified adversarial robustness, verifiable via inexpensive incomplete verifiers, while paying a relatively large price in standard accuracy.\nAnother line of research seeks to improve standard accuracy by relying on stronger verifiers and coupling adversarial training (\u00a72.1) with verifiability-inducing techniques. Examples include maximizing network local linearity (Xiao et al., 2019), performing the attacks in the latent space over network overapproximations (COLT) (Balunovic & Vechev, 2020), minimizing the area of over-approximations used at verification time while attacking over larger input regions (IBP-R) (De Palma et al., 2022), and optimizing over the sum of the adversarial and IBP losses through a specialized optimizer (Fan & Li, 2021). In most cases `1 regularization was found to be beneficial. A more recent work, named SABR (M\u00fcller et al., 2023), proposes to compute bounds via IBP over a parametrized subset of the input domain that includes an adversarial attack. Let us denote by Proj(a,A) the Euclidean projection of a on set A. Given x\u03bb := Proj(xadv, C(x, \u2212 \u03bb )), SABR relies on the following loss:\nL(\u2212 \u00af z\u03bb(\u03b8,x, y), y), where \u00af z\u03bb(\u03b8,x, y) \u2264 z(\u03b8,x0, y) \u2200 x0 \u2208 C(x\u03bb, \u03bb ) \u2286 C(x, ). (6)\nThe SABR loss is not necessarily an upper bound for L\u2217(f(\u03b8,x), y). However, it continuously interpolates between L(f(\u03b8,xadv), y), which it matches when \u03bb = 0, and Lver(f(\u03b8,x), y), matched when \u03bb = 1. By tuning \u03bb \u2208 [0, 1], SABR attains state-of-the-art performance under complete verifiers (M\u00fcller et al., 2023). The recent STAPS (Mao et al., 2023a), combines SABR with improvements on COLT, further improving its performance on some benchmarks at the cost of added complexity."
        },
        {
            "heading": "3 LOSS EXPRESSIVITY FOR VERIFIED TRAINING",
            "text": "As seen in \u00a72.2.2, state-of-the-art verified training algorithms rely on coupling adversarial attacks with over-approximations. Despite this commonality, there is considerable variety in terms of how these are combined, leading to a proliferation of heuristics. We now outline a simple property, which we call expressivity, that we will show to be key to effective verified training schemes: Definition 3.1. A parametrized family of losses L\u03b1(\u03b8,x, y) is expressive if:\n\u2022 L(f(\u03b8,xadv), y) \u2264 L\u03b1(\u03b8,x, y) \u2264 Lver(f(\u03b8,x), y) \u2200 \u03b1 \u2208 [0, 1];\n\u2022 L\u03b1(\u03b8,x, y) is monotonically increasing with \u03b1;\n\u2022 L0(\u03b8,x, y) = L(f(\u03b8,xadv), y); \u2022 L1(\u03b8,x, y) = Lver(f(\u03b8,x), y).\nExpressive losses can range between the adversarial loss in equation (3) to the verified loss in equation (5) through the value of a single parameter \u03b1 \u2208 [0, 1], the over-approximation coefficient. As a result, they will be able to span a wide range of trade-offs between verified robustness and standard accuracy. Intuitively, if \u03b1 \u2248 0, then L\u03b1(\u03b8,x, y) << L\u2217(f(\u03b8,x), y), resulting in networks hard to verify even with complete verifiers. If \u03b1 \u2248 1, then L\u03b1(\u03b8,x, y) \u2248 Lver(f(\u03b8,x), y), producing networks with larger verified robustness when using the incomplete verifier employed to compute\n\u00af z(\u03b8,x, y) but with lower standard accuracy. In general, the most effective \u03b1 will maximize\nverifiability via the verifier to be employed post-training while preserving as much standard accuracy as possible. The exact value will depend on the efficacy of the verifier and on the difficulty of the verification problem at hand, with larger \u03b1 values corresponding to less precise verifiers. The loss of SABR from equation (6) satisfies definition 3.1 when setting \u03bb = \u03b1 (see appendix A). In order\nto demonstrate the centrality of expressivity, we now present two minimalistic instantiations of the definition and later show that they yield state-of-the-art results on a variety of benchmarks (\u00a76.1)."
        },
        {
            "heading": "4 EXPRESSIVITY THROUGH CONVEX COMBINATIONS",
            "text": "We outline two simple ways to obtain expressive losses through convex combinations between adversarial attacks and bounds from incomplete verifiers. Pseudo-code is provided in appendix C."
        },
        {
            "heading": "4.1 CC-IBP",
            "text": "A family of expressive losses can be easily obtained by taking Convex Combinations (CC) of adversarial and lower bounds to logit differences within the loss function:\nL\u03b1,CC(\u03b8,x, y) := L(\u2212 [(1\u2212 \u03b1) z(\u03b8,xadv, y) + \u03b1 \u00af z(\u03b8,x, y)] , y). (7)\nProposition 4.1. The parametrized loss L\u03b1,CC(\u03b8,x, y) is expressive according to definition 3.1. Proof. Trivially, L0,CC(\u03b8,x, y) = L(f(\u03b8,xadv), y), and L1,CC(\u03b8,x, y) = Lver(f(\u03b8,x), y). By definition\n\u00af z(\u03b8,x, y) \u2264 z(\u03b8,xadv, y), as xadv \u2208 C(x, ).\nThen, owing to assumption 2.1 and to z(\u03b8,xadv, y)[y] = \u00af z(\u03b8,x, y)[y] = 0, as \u00af z(\u03b8,x, y) \u2264 (1 \u2212 \u03b1) z(\u03b8,xadv, y) + \u03b1 \u00af z(\u03b8,x, y) \u2264 z(\u03b8,xadv, y) for \u03b1 \u2208 [0, 1], L(f(\u03b8,xadv), y) \u2264 L\u03b1(\u03b8,x, y) \u2264 Lver(f(\u03b8,x), y), which concludes the proof.\nThe analytical form of equation (7) when L is the cross-entropy loss, including a comparison to the loss from SABR, is presented in appendix B. In order to minimize computational costs and to yield favorable optimization problems (Jovanovic\u0301 et al., 2022), we employ IBP to compute\n\u00af z(\u03b8,x, y), and\nrefer to the resulting algorithm as CC-IBP. We remark that a similar convex combination was employed in the CROWN-IBP (Zhang et al., 2020) loss (see \u00a72.2.2). However, while CROWN-IBP takes a combination between IBP and CROWN-IBP lower bounds to z(\u03b8,x, y), we replace the latter by logit differences associated to an adversarial attack. Furthermore, rather than increasing the combination coefficient from 0 to 1 during ramp-up, we propose to employ a constant and tunable \u03b1 throughout training."
        },
        {
            "heading": "4.2 MTL-IBP",
            "text": "Convex combinations can be also performed between loss functions L, resulting in: L\u03b1,MTL(\u03b8,x, y) := (1\u2212 \u03b1)L(f(\u03b8,xadv), y) + \u03b1 Lver(f(\u03b8,x), y). (8)\nThe above loss lends itself to a Multi-Task Learning (MTL) interpretation (Caruana, 1997), with empirical and verified adversarial robustness as the tasks. Under this view, borrowing terminology from multi-objective optimization, L\u03b1,MTL(\u03b8,x, y) amounts to a scalarization of the multi-task problem. Proposition 4.2. The parametrized loss L\u03b1,MTL(\u03b8,x, y) is expressive. Furthermore, if L(\u00b7, y) is convex with respect to its first argument, L\u03b1,CC(\u03b8,x, y) \u2264 L\u03b1,MTL(\u03b8,x, y) \u2200 \u03b1 \u2208 [0, 1]. Proof. The proof for the first part of the statement closely follows the proof for proposition 4.1, while not requiring assumption 2.1. L\u03b1,CC(\u03b8,x, y) \u2264 L\u03b1,MTL(\u03b8,x, y) \u2200 \u03b1 \u2208 [0, 1] trivially follows from the definition of a convex function (Boyd & Vandenberghe, 2004, section 3.1.1) and equation (5).\nThe second part of proposition 4.2, which applies to cross-entropy, shows that L\u03b1,MTL(\u03b8,x, y) can be seen as an upper bound to L\u03b1,CC(\u03b8,x, y) for \u03b1 \u2208 [0, 1]. Owing to definition 3.1, the bound will be tight when \u03b1 \u2208 {0, 1}. Appendix B shows an analytical form for equation (8) when L is the cross-entropy loss. As in section 4.1, we use IBP to compute Lver(f(\u03b8,x), y); we refer to the resulting algorithm as MTL-IBP. Given its simplicity, ideas related to equation (8) have previously been adopted in the literature. However, they have never been explored in the context of expressivity and the role of \u03b1 was never explicitly investigated. Fan & Li (2021) present a loss equivalent to equation (8), treated as a baseline for the proposed algorithm, but only consider \u03b1 = 0.5 and use it within a costly layer-wise optimization scheme similar to COLT (Balunovic & Vechev, 2020). While Fan & Li (2021) propose a specialized multi-task optimizer (AdvIBP), we instead opt for scalarizations, which have recently been shown to yield state-of-the-art results on multi-task benchmarks when appropriately tuned and regularized (Kurin et al., 2022; Xin et al., 2022). Gowal et al. (2018) take a convex combination between the natural (rather than adversarial) loss and the IBP loss; however, instead of tuning a constant \u03b1, they linearly transition it from 0 to 0.5 during ramp-up (see \u00a72.2.2). Mirman et al. (2018) employ L(f(\u03b8,x), y) + 0.1 (\u2212softplus(mini6=y\n\u00af z(\u03b8,x, y)[i]))."
        },
        {
            "heading": "5 RELATED WORK",
            "text": "As outlined in \u00a72.2.2, many verified training algorithms work by either directly upper bounding the robust loss (Gowal et al., 2018; Mirman et al., 2018; Wong & Kolter, 2018; Zhang et al., 2020; Xu et al., 2020; Shi et al., 2021), or by inducing verifiability on top of adversarial training (Xiao et al., 2019; Balunovic & Vechev, 2020; De Palma et al., 2022; M\u00fcller et al., 2023; Mao et al., 2023a): this work falls in the latter category. Algorithms belonging to these classes are typically employed against `\u221e perturbations. Other families of methods include Lipschitz-based regularization for `2 perturbations (Leino et al., 2021; Huang et al., 2021), and architectures that are designed to be inherently robust for either `2 (Trockman & Kolter, 2021; Singla & Feizi, 2021; Singla et al., 2022; Singla & Feizi, 2022; Xu et al., 2022) or `\u221e (Zhang et al., 2021; 2022a) perturbations, for which SortNet (Zhang et al., 2022b) is the most recent and effective algorithm. A prominent line of work has focused on achieving robustness with high probability through randomization (Cohen et al., 2019; Salman et al., 2019): we here focus on deterministic methods.\nIBP, presented in \u00a72.2.1, is arguably the simplest incomplete verifier. A popular class of algorithms replaces the activation by over-approximations corresponding to pairs of linear bound, each respectively providing a lower or upper bound to the activation function (Wong & Kolter, 2018; Singh et al., 2018; Zhang et al., 2018; Singh et al., 2019b). Tighter over-approximations can be obtained by representing the convex hull of the activation function (Ehlers, 2017; Bunel et al., 2020a; Xu et al., 2021), the convex hull of the composition of the activation with the preceding linear layer (Anderson et al., 2020; Tjandraatmadja et al., 2020; De Palma et al., 2021a;b), interactions within the same layer (Singh et al., 2019a; M\u00fcller et al., 2022), or by relying on semi-definite relaxations (Raghunathan et al., 2018; Dathathri et al., 2020; Lan et al., 2022). In general, the tighter the bounds, the more expensive their computation. As seen in the recent neural network verification competitions (Bak et al., 2021; M\u00fcller et al., 2022), state-of-the-art complete verifiers based on branch-and-bound typically rely on specialized and massively-parallel incomplete verifiers (Wang et al., 2021; Ferrari et al., 2022; Zhang et al., 2022c), paired with custom branching heuristics (Bunel et al., 2020b; De Palma et al., 2021c; Ferrari et al., 2022)."
        },
        {
            "heading": "6 EXPERIMENTAL EVALUATION",
            "text": "We now present experimental results that support the centrality of expressivity, as defined in \u00a73, to verified training. In \u00a76.1 we compare CC-IBP and MTL-IBP with results from the literature, showing that even minimalistic instantiations of the definition attain state-of-the-art performance on a variety of benchmarks. In \u00a76.2 and \u00a76.3 we study the effect of the over-approximation coefficient on the performance profiles of expressive losses, highlighting their sensitivity to its value and showing that better approximations of the branch-and-bound loss do not necessarily results in better performance.\nWe implemented CC-IBP and MTL-IBP in PyTorch (Paszke et al., 2019), starting from the training pipeline by Shi et al. (2021), based on automatic LiRPA (Xu et al., 2020), and exploiting their specialized initialization and regularization techniques. All our experiments use the same 7-layer architecture as previous work (Shi et al., 2021; M\u00fcller et al., 2023), which employs BatchNorm (Ioffe & Szegedy, 2015) after every layer. Except on CIFAR-10 with = 2/255, for which an 8-step attack is used as in previous work (M\u00fcller et al., 2023), we employ randomly-initialized single-step attacks (Wong et al., 2020) to compute xadv and force the points to be on the perturbation boundary via a large step size (see appendices F.6 and F.2 for ablations on the step size and employed adversarial attack, respectively). Unless specified otherwise, the reported verified robust accuracies for our experiments are computed using branch-and-bound, with a setup similar to IBP-R (De Palma et al., 2022). Specifically, we use the OVAL framework (Bunel et al., 2020b; De Palma et al., 2021c), with a variant of the UPB branching strategy (De Palma et al., 2022) and \u03b1-\u03b2-CROWN as the bounding algorithm (Wang et al., 2021) (see appendix D). Details pertaining to the employed datasets, hyper-parameters, network architectures, and computational setup are reported in appendix E."
        },
        {
            "heading": "6.1 COMPARISON WITH LITERATURE RESULTS",
            "text": "Table 1 compares the results of both CC-IBP and MTL-IBP with relevant previous work in terms of standard and verified robust accuracy against `\u221e norm perturbations. We benchmark on the MNIST (LeCun et al., 2010), CIFAR-10 (Krizhevsky & Hinton, 2009), TinyImageNet (Le & Yang,\n2015), and downscaled ImageNet (64\u00d7 64) (Chrabaszcz et al., 2017) datasets. The results from the literature report the best performance attained by each algorithm on any architecture, thus providing a summary of the state-of-the-art. We report the algorithm across TAPS and STAPS (Mao et al., 2023a) with the best per-setting performance, measured as the mean between standard and verified robust accuracies. Training times, obtained on different hardware depending on the literature source, are included when available to provide an indication of the overhead associated with each algorithm. In order to produce a fair comparison with literature results, in this section we comply with the\nseemingly standard practice in the area (Gowal et al., 2018; Zhang et al., 2020; Shi et al., 2021; M\u00fcller et al., 2023) and directly tune on the evaluation sets. Table 1 show that, in terms of robustnessaccuracy trade-offs, both CC-IBP and MTL-IBP attain state-of-the-art performance. In particular, no results from the literature correspond to better trade-offs in any of the considered benchmarks except for CIFAR-10 with = 8/255, where specialized architectures such as SortNet attain better robustness-accuracy trade-offs. Observe that the improvements of MTL-IBP and CC-IBP upon literature results become more significant on larger datasets. On TinyImageNet, MTL-IBP attains 32.76% standard accuracy and 24.41% verified robust accuracy (respectively +3.78% and +1.98% compared to STAPS). On downscaled ImageNet, MTL-IBP reaches 20.15% (+3.92% compared to CROWN-IBP) standard accuracy and 12.13% (+2.59% compared to SortNet) verified robust accuracy. Furthermore, owing to the use of FGSM for the attack on all benchmarks except one, MTL-IBP and CC-IBP tend to incur less training overhead than the current state-of-the-art, making them more scalable and easier to tune. In addition, MTL-IBP performs significantly better than the closely-related AdvIBP (see \u00a74.2) on all benchmarks, confirming the effectiveness of an appropriately tuned scalarization (Xin et al., 2022). Fan & Li (2021) provide results for the use of L0.5,MTL(\u03b8,x, y) within a COLT-like stage-wise training procedure, treated as a baseline, on CIFAR-10 with = 8/255, displaying 26.22% IBP verified robust accuracy. These results are markedly worse than those we report for MTL-IBP, which attains 34.61% IBP verified robust accuracy with \u03b1 = 0.5 in table 4 (see appendix E). We believe this discrepancy to be linked to the commonalities with COLT, which under-performs in this setting, and, in line with similar results in the multi-task literature (Kurin et al., 2022), to our use of specialized initialization and regularization from Shi et al. (2021). Appendix F.1 presents verified robust accuracies under less expensive verifiers. Remarkably, on TinyImageNet and downscaled ImageNet, MTL-IBP attains better verified robust accuracy than literature results even without resorting to complete verifiers. Appendix F.5 provides an indication of experimental variability for MTL-IBP and CC-IBP by repeating the experiments for a single setup a further 3 times."
        },
        {
            "heading": "6.2 SENSITIVITY TO OVER-APPROXIMATION COEFFICIENT",
            "text": "Figure 2 reports standard, adversarial, and verified robust accuracies under different verifiers when varying the value of \u03b1 for CC-IBP and MTL-IBP. BaB denotes verified accuracy under the complete verifier from table 1, while CROWN/IBP the one under the best bounds between full backward CROWN and IBP. Adversarial robustness is computed against the attacks performed within the employed BaB framework (see appendix D). As expected, the standard accuracy steadily decreases with increasing \u03b1 for both algorithms and, generally speaking (except for MTL-IBP when = 8/255), the IBP verified accuracy will increase with \u03b1. However, as seen on SABR (M\u00fcller et al., 2023), the behavior of the adversarial and verified robust accuracies under tighter verifiers is less consistent. In fact, depending on the setting and the algorithm, a given accuracy may first increase and then decrease with \u03b1. This sensitivity highlights the need for careful tuning according to the desired robustnessaccuracy trade-off and verification budget. For instance, figure 2 suggests that, for = 8/255, the best trade-offs between BaB verified accuracy and standard accuracy lie around \u03b1 = 0.5 for both CC-IBP and MTL-IBP. When = 2/255, this will be around \u03b1 = 10\u22122 for CC-IBP, and with \u03b1 \u2208 (10\u22123, 10\u22122) for MTL-IBP, whose natural accuracy decreases more sharply with \u03b1 (see appendix E). Larger \u03b1\nvalues appear to be beneficial against larger perturbations, where branch-and-bound is less effective. Appendix F.4 carries out a similar analysis on the loss values associated to the same networks."
        },
        {
            "heading": "6.3 BRANCH-AND-BOUND LOSS AND APPROXIMATIONS",
            "text": "A common assumption in previous work is that better approximations of the worst-case loss from equation (2) will result in better trade-offs between verified robustness and accuracy (M\u00fcller et al., 2023; Mao et al., 2023a). In order to investigate the link between expressive losses, the worst-case loss, and a principled tuning process, we conduct an in-depth study on a CIFAR-10 validation set for CC-IBP, MTL-IBP and SABR. Specifically, we evaluate on a holdout set taken from the last 20% of the CIFAR-10 training images, and train the same architecture employed in \u00a76.1 on the remaining 80%, tuning \u03b1 \u2208 {1\u00d7 10\u22123, 3\u00d7 10\u22123, 7\u00d7 10\u22123, 1\u00d7 10\u22122, 3\u00d7 10\u22122} for = 2/255 and \u03b1 \u2208 {0.3, 0.4, 0.5, 0.6, 0.7} for = 8/255. Given that computing the worst-case loss on networks of practical interest is intractable, we replace it by LBaB(f(\u03b8,xi), y), computed by running branch-andbound (BaB) for 15 seconds on each logit difference, without returning early for verified properties so as to tighten the bound. Figure 3 reports values for the standard loss, the branch-and-bound loss and two measures for its approximation: the average value of (L\u03b1(\u03b8,xi, y)\u2212 LBaB(f(\u03b8,xi), y)) (BaB Error) and the square root of the average of its square (BaB RMSE). Observe that the best-performing model, here defined as the one with the smallest sum of the standard and BaB losses, does not necessarily correspond to the points with the best BaB loss approximation. In particular, over-approximations (BaB Error > 0) display better validation trade-offs for = 2/255, and underapproximations for = 8/255. Given that LBaB(f(\u03b8,x), y) \u2265 L\u2217(f(\u03b8,x), y), the results on = 2/255 demonstrate that accurate worst-case loss approximations do not necessarily correspond to better performance: neither in terms of verified robustness alone, nor in terms of trade-offs with accuracy. As shown in appendix F.3, the selected \u03b1 values result in similar (state-of-the-art for ReLU networks) CIFAR-10 robustness-accuracy trade-offs across the three considered expressive losses, suggesting the importance of expressivity, rather than of the specific employed heuristic, for verified training."
        },
        {
            "heading": "7 CONCLUSIONS",
            "text": "We defined a class of parametrized loss functions, which we call expressive, that range from the adversarial loss to the IBP loss by means of a single parameter \u03b1 \u2208 [0, 1]. We argued that expressivity is key to maximize trade-offs between verified robustness and standard accuracy, and supported this claim by extensive experimental results. In particular, we showed that SABR is expressive, and demonstrated that even two minimalistic instantiations of the definition obtained through convex combinations, CC-IBP and MTL-IBP, attain state-of-the-art results on a variety of benchmark. Furthermore, we provided a detailed analysis of the influence of the over-approximation coefficient \u03b1 on the properties of networks trained via CC-IBP, MTL-IBP and SABR, demonstrating that, differently from what previously conjectured, better approximations of the branch-and-bound loss do not necessarily result in better performance."
        },
        {
            "heading": "8 ETHICS STATEMENT",
            "text": "We do not foresee any immediate negative application of networks that are provably robust to adversarial perturbations. Nevertheless, the development of systems that are more robust will likely hinder any positive use of adversarial examples (Albert et al., 2021), potentially amplifying the harm caused by unethical machine learning systems. Furthermore, while we demonstrated state-of-the-art performance on a suite of commonly-employed vision datasets, this does not guarantee the effectiveness of our methods beyond the tested benchmarks and, in particular, on different data domains."
        },
        {
            "heading": "9 REPRODUCIBILITY STATEMENT",
            "text": "We provide code to reproduce our experiments as part of the supplementary material. Pseudo-code for both losses can be found in appendix C. Training details, hyper-parameters and computational setups are provided in sections 4, 6 and appendix E. We provide an indication of experimental variability on a single benchmark in appendix F.5."
        },
        {
            "heading": "A EXPRESSIVITY OF SABR",
            "text": "Proposition A.1. The SABR (M\u00fcller et al., 2023) loss function L(\u2212 \u00af z\u03bb(\u03b8,x, y), y), defined in equation (6), is expressive according to definition 3.1.\nProof. Recalling equation (6), \u00af z\u03bb(\u03b8,x, y) \u2264 z(\u03b8,x\u2032, y) \u2200 x\u2032 \u2208 C(x\u03bb, \u03bb ) \u2286 C(x, ), with\nx\u03bb := Proj(xadv, C(x, \u2212 \u03bb )). We can hence define \u03b4\u03bb(\u03b8,x\u2032, y) := \u00af z\u03bb(\u03b8,x, y) \u2212 z(\u03b8,x\u2032, y) \u2264 0\nfor any x\u2032 \u2208 C(x\u03bb, \u03bb ). As C(x\u03bb\u2032 , \u03bb\u2032 ) \u2286 C(x\u03bb, \u03bb ) for any \u03bb\u2032 \u2264 \u03bb, \u00af z\u03bb(\u03b8,x, y) will monotonically\ndecrease with \u03bb, and so will \u03b4\u03bb(\u03b8,x\u2032, y). We can hence re-write the SABR loss as:\nL(\u2212 \u00af z\u03bb(\u03b8,x, y), y) = L(\u2212 [z(\u03b8,xadv, y) + ( \u00af z\u03bb(\u03b8,x, y)\u2212 z(\u03b8,xadv, y))] , y)\n= L(\u2212 [z(\u03b8,xadv, y) + \u03b4\u03bb(\u03b8,xadv, y)] , y).\nThen, \u03b40(\u03b8,xadv, y) = 0, as x0 = xadv and hence \u00af z0(\u03b8,x, y) = z(\u03b8,xadv, y). Furthermore, \u00af z1(\u03b8,x, y) = \u00af z(\u03b8,x, y), proving the last two points of definition 3.1. The remaining requirements follow from using assumption 2.1 and the non-positivity and monotonicity of \u03b4\u03bb(\u03b8,xadv, y)."
        },
        {
            "heading": "B LOSS DETAILS FOR CROSS-ENTROPY",
            "text": "Assuming the underlying loss function L to be cross-entropy, we now present an analytical form of the expressive losses presented in \u00a74.1 and \u00a74.2, along with a comparison with the loss from SABR (M\u00fcller et al., 2023).\nWhen applied on the logit differences z(\u03b8,x, y) (see \u00a72.2.2), the cross-entropy can be written as L(\u2212z(\u03b8,x, y), y) = log(1 + \u2211 i 6=y e\n\u2212z(\u03b8,x,y)[i]) by exploiting the properties of logarithms and z(\u03b8,x, y)[y] = 0 (see \u00a72). In fact:\nL(\u2212z(\u03b8,x, y), y) = \u2212 log (\ne\u2212z(\u03b8,x,y)[y]\u2211 i e \u2212z(\u03b8,x,y)[i]\n)\n= \u2212 log\n( 1\n1 + \u2211 i 6=y e \u2212z(\u03b8,x,y)[i]\n) = log 1 +\u2211 i 6=y e\u2212z(\u03b8,x,y)[i]  ."
        },
        {
            "heading": "B.1 CC-IBP",
            "text": "Proposition B.1. When L(\u2212z(\u03b8,x, y), y) = log(1 + \u2211 i 6=y e\n\u2212z(\u03b8,x,y)[i]), the loss L\u03b1,CC(\u03b8,x, y) from equation (7) takes the following form:\nL\u03b1,CC(\u03b8,x, y) = log 1 +\u2211 i 6=y ( e\u2212z(\u03b8,xadv,y)[i] )(1\u2212\u03b1)( e\u2212\u00af z(\u03b8,x,y)[i] )\u03b1 .\nProof. Using the properties of exponentials:\nL\u03b1,CC(\u03b8,x, y) = L(\u2212 [(1\u2212 \u03b1) z(\u03b8,xadv, y) + \u03b1 \u00af z(\u03b8,x, y)] , y)\n= log 1 +\u2211 i 6=y e\u2212[(1\u2212\u03b1) z(\u03b8,xadv,y)+\u03b1 \u00af z(\u03b8,x,y)][i]  = log\n1 +\u2211 i 6=y ( e\u2212(1\u2212\u03b1) z(\u03b8,xadv,y)[i] )( e\u2212\u03b1 \u00af z(\u03b8,x,y)[i] )\n= log 1 +\u2211 i 6=y ( e\u2212z(\u03b8,xadv,y)[i] )(1\u2212\u03b1)( e\u2212\u00af z(\u03b8,x,y)[i] )\u03b1 ."
        },
        {
            "heading": "B.2 MTL-IBP",
            "text": "Proposition B.2. When L(\u2212z(\u03b8,x, y), y) = log(1 + \u2211 i6=y e\n\u2212z(\u03b8,x,y)[i]), the loss L\u03b1,MTL(\u03b8,x, y) from equation (8) takes the following form:\nL\u03b1,MTL(\u03b8,x, y) = log  1 +\u2211\ni6=y\ne\u2212z(\u03b8,xadv,y)[i] (1\u2212\u03b1)1 +\u2211 i 6=y e\u2212\u00af z(\u03b8,x,y)[i] \u03b1  .\nProof. Using the properties of logarithms and exponentials: L\u03b1,MTL(\u03b8,x, y) = (1\u2212 \u03b1)L(f(\u03b8,xadv), y) + \u03b1 Lver(f(\u03b8,x), y)\n= (1\u2212 \u03b1) log 1 +\u2211 i 6=y e\u2212z(\u03b8,xadv,y)[i] + \u03b1 log 1 +\u2211 i6=y e\u2212\u00af z(\u03b8,x,y)[i]  = log  1 +\u2211\ni 6=y\ne\u2212z(\u03b8,xadv,y)[i] (1\u2212\u03b1) + log 1 +\u2211 i 6=y e\u2212\u00af z(\u03b8,x,y)[i] \u03b1\n= log  1 +\u2211\ni 6=y\ne\u2212z(\u03b8,xadv,y)[i] (1\u2212\u03b1)1 +\u2211 i 6=y e\u2212\u00af z(\u03b8,x,y)[i] \u03b1  .\nBy comparing propositions B.1 and B.2, we can see that verifiability tends to play a relatively larger role on L\u03b1,MTL(\u03b8,x, y). In particular, given that z(\u03b8,xadv, y) >\n\u00af z(\u03b8,x, y),\nif the network displays a large margin of empirical robustness to a given incorrect class (z(\u03b8,xadv, y)[i] >> 0, with i 6= y), the weight of the verifiability of that class on L\u03b1,CC(\u03b8,x, y) will be greatly reduced. On the other hand, if the network has a large margin of empirical robustness to all incorrect classes (z(\u03b8,xadv, y) >> 0), verifiability will still have a relatively large weight on L\u03b1,MTL(\u03b8,x, y). These observations comply with proposition 4.2, which shows that L\u03b1,CC(\u03b8,x, y) \u2264 L\u03b1,MTL(\u03b8,x, y), indicating that L\u03b1,MTL(\u03b8,x, y) is generally more skewed towards verifiability."
        },
        {
            "heading": "B.3 COMPARISON OF CC-IBP WITH SABR",
            "text": "Proposition B.3. When L(\u2212z(\u03b8,x, y), y) = log(1 + \u2211 i 6=y e\n\u2212z(\u03b8,x,y)[i]), the loss L(\u2212\n\u00af z\u03bb(\u03b8,x, y), y) from equation (6) takes the following form:\nL(\u2212 \u00af z\u03bb(\u03b8,x, y), y) = log 1 +\u2211 i6=y ( e\u2212z(\u03b8,xadv,y)[i] )( e\u2212\u03b4\u03bb(\u03b8,xadv,y)[i] ) , where \u03b4\u03bb(\u03b8,xadv, y) \u2264 0 is monotonically decreasing with \u03bb.\nProof. We can use \u03b4\u03bb(\u03b8,x\u2032, y) := \u00af z\u03bb(\u03b8,x, y)\u2212 z(\u03b8,x\u2032, y) \u2264 0, which is monotonically decreasing with \u03bb as seen in the proof of proposition A.1. Then, exploiting the properties of the exponential, we can trivially re-write the SABR loss L(\u2212\n\u00af z\u03bb(\u03b8,x, y), y) as:\nL(\u2212 \u00af z\u03bb(\u03b8,x, y), y) = L(\u2212 [z(\u03b8,xadv, y) + \u03b4\u03bb(\u03b8,x, y)] , y)\n= log 1 +\u2211 i 6=y e\u2212[z(\u03b8,xadv,y)+\u03b4\u03bb(\u03b8,x,y)][i]  = log\n1 +\u2211 i 6=y ( e\u2212z(\u03b8,xadv,y)[i] )( e\u2212\u03b4\u03bb(\u03b8,x,y)[i] ) .\nBy comparing propositions B.1 and B.3, we can see that the SABR and CC-IBP losses share a similar form, and they both upper-bound the adversarial loss. The main differences are the following: (i) the adversarial term is scaled down in CC-IBP, (ii) CC-IBP computes lower bounds over the entire input region, (iii) the over-approximation term of CC-IBP uses a scaled lower bound of the logit differences, rather than its difference with the adversarial z."
        },
        {
            "heading": "C PSEUDO-CODE",
            "text": "Algorithm 1 provides pseudo-code for both CC-MTL and MTL-IBP.\nAlgorithm 1 CC/MTL-IBP training loss computation 1: Input: Network f(\u03b8, \u00b7), input x, label y, perturbation set C(x, ), loss function L,\nhyper-parameters \u03b1, \u03bb 2: Compute xadv using an attack on C(x, ) 3: Compute\n\u00af z(\u03b8,x, y) via IBP on C(x, ) as described in section 2.2.1\n4: if CC-IBP then 5: z(\u03b8,xadv, y)\u2190 (1 f(\u03b8,xadv)[y]\u2212 f(\u03b8,xadv)) 6: L\u03b1(\u03b8,x, y)\u2190 L(\u2212 [(1\u2212 \u03b1) z(\u03b8,xadv, y) + \u03b1\n\u00af z(\u03b8,x, y)] , y)\n7: else if MTL-IBP then 8: Lver(f(\u03b8,x), y)\u2190 L(\u2212\n\u00af z(\u03b8,x, y), y)\n9: L\u03b1(\u03b8,x, y)\u2190 (1\u2212 \u03b1)L(f(\u03b8,xadv), y) + \u03b1 Lver(f(\u03b8,x), y) 10: end if 11: return L\u03b1(\u03b8,x, y) + \u03bb||\u03b8||1"
        },
        {
            "heading": "D BRANCH-AND-BOUND FRAMEWORK",
            "text": "We now outline the details of the Branch-and-Bound (BaB) framework employed to verify the networks trained via CC-IBP and MTL-IBP (see \u00a76). Before resorting to more expensive verifiers, we first run IBP (Gowal et al., 2018) and CROWN (Zhang et al., 2018) from the automatic LiRPA library (Xu et al., 2020) as an attempt to quickly verify the property. If the attempt fails, we run BaB with a timeout of 1800 seconds per data point unless specified otherwise, possibly terminating well before the timeout (except on the MNIST experiments) when the property is deemed likely to time out. Similarly to a recent verified training work (De Palma et al., 2022), we base our verifier on the publicly available OVAL framework (Bunel et al., 2020a;b; De Palma et al., 2021c) from VNN-COMP-2021 (Bak et al., 2021).\nLower bounds to logit differences A crucial component of a BaB framework is the strategy employed to compute\n\u00af z(\u03b8,x, y). As explained in \u00a72.2.1, the computation of the bound relies on\na network over-approximation, which will generally depend on \u201cintermediate bounds\": bounds to intermediate network pre-activations. For ReLU network, the over-approximation quality will depend on the tightness of the bounds for \u201cambiguous\" ReLUs, which cannot be replaced by a linear function (0 is contained in their pre-activation bounds). We first use IBP to inexpensively compute intermediate bounds for the whole network. Then, we refine the bounds associated to ambiguous ReLUs by using 5 iterations of \u03b1-CROWN (Xu et al., 2021) with a step size of 1 decayed by 0.98 at every iteration, without jointly optimizing over them so as to reduce the memory footprint. Intermediate bounds are computed only once at the root node, that is, before any branching is performed. If the property is not verified at the root, we use \u03b2-CROWN (Wang et al., 2021) as bounding algorithm for the output bounds, with a dynamic number of iterations (see (De Palma et al., 2021c, section 5.1.2)), and initialized from the parent BaB sub-problem. We employ a step size of 0.1, decayed by 0.98 at each iteration.\nAdversarial attacks Counter-examples to the properties are searched for by evaluating the network on the input points that concretize the lower bound\n\u00af z(\u03b8,x, y) according to the employed network\nover-approximation. This is repeated on each BaB sub-problem, and also used to mark sub-problem infeasibility when\n\u00af z(\u03b8,x, y) overcomes the corresponding input point evaluation. In addition, some\nof these points are used as initialization (along with random points in the perturbation region) for a 500-step MI-FGSM Dong et al. (2018) adversarial attack, which is run repeatedly for each property using a variety of randomly sampled hyper-parameter (step size value and decay, momentum) settings, and early stopped if success is deemed unlikely. In \u00a7?? we report a network to be empirically robust if both the above procedure and a 40-step PGD attack (Madry et al., 2018) (run before BaB) fail to find an adversarial example.\nBranching Branching is performed through activation spitting: we create branch-and-bound subproblems by splitting a ReLU into its two linear functions. In other words, given intermediate bounds on pre-activations x\u0302k \u2208 [\u0302lk, u\u0302k] for k \u2208 J2, n\u2212 1K, if the split is performed at the i-th neuron of the k-th layer, l\u0302k[i] \u2265 0 is enforced on a sub-problem, u\u0302k[i] \u2264 0 on the other. We employ a slightly modified version of the UPB branching strategy (De Palma et al., 2022, equation (7)), where the bias of the upper constraint from the Planet relaxation (ReLU convex hull, also known as triangle relaxation), [\u2212l\u0302k]+ [u\u0302k]+\nu\u0302k\u2212l\u0302k , is replaced by the area of the entire relaxation, [\u2212l\u0302k]+ [u\u0302k]+. Intuitively,\nthe area of the over-approximation should better represent the weight of an ambiguous neuron in the overall neural network relaxation than the distance of the upper constraint from the origin. In practice, we found the impact on BaB performance to be negligible. In particular, we tested our variant against the original UPB, leaving the remaining BaB setup unchanged, on the CC-IBP trained network for MNIST with = 0.1 from table 1: they both resulted in 98.43% verified robust accuracy."
        },
        {
            "heading": "E EXPERIMENTAL DETAILS",
            "text": "We present additional details pertaining to the experiments from \u00a76 and appendix F. In particular, we outline dataset and training details, hyper-parameters, computational setup and the employed network architecture."
        },
        {
            "heading": "E.1 DATASETS",
            "text": "MNIST (LeCun et al., 2010) is a 10-class (each class being a handwritten digit) classification dataset consisting of 28\u00d7 28 greyscale images, with 60,000 training and 10,000 test points. CIFAR10 (Krizhevsky & Hinton, 2009) consists of 32\u00d732 RGB images in 10 different classes (associated to subjects such as airplanes, cars, etc.), with 50,000 training and 10,000 test points. TinyImageNet (Le & Yang, 2015) is a miniature version of the ImageNet dataset, restricted to 200 classes of 64\u00d7 64 RGB images, with 100,000 training and 10,000 validation images. The employed downscaled ImageNet (Chrabaszcz et al., 2017) is a 1000-class dataset of 64\u00d7 64 RGB images, with 1,281,167 training and 50,000 validation images. Similarly to previous work (Xu et al., 2020; Shi et al., 2021; M\u00fcller et al., 2023), we normalize all datasets and use random horizontal flips and random cropping as data augmentation on CIFAR-10, TinyImageNet and downscaled ImageNet. Complying with common experimental practice Gowal et al. (2018); Xu et al. (2020); Shi et al. (2021); M\u00fcller et al. (2023), we train on the entire training sets, and evaluate MNIST and CIFAR-10 on their test sets, ImageNet and TinyImageNet on their validation sets."
        },
        {
            "heading": "E.2 NETWORK ARCHITECTURE, TRAINING SCHEDULE AND HYPER-PARAMETERS",
            "text": "We outline the practical details of our training procedure.\nNetwork Architecture Independently of the dataset, all our experiments employ a 7-layer architecture that enjoys widespread usage in the verified training literature (Gowal et al., 2018; Zhang et al., 2020; Shi et al., 2021; M\u00fcller et al., 2023; Mao et al., 2023a). In particular, mirroring recent work (M\u00fcller et al., 2023; Mao et al., 2023a), we employ a version modified by Shi et al. (2021) to employ BatchNorm (Ioffe & Szegedy, 2015) after every layer. Complying with Shi et al. (2021), at training time we compute IBP bounds to BatchNorm layers using the batch statistics from unperturbed data. The statistics used at evaluation time are instead computed including both the original data and the attacks used during training. The details of the architecture, often named CNN7 in the literature, are presented in table 2.\nTrain-Time Perturbation Radius\nemployed loss function is increased. In other to achieve a similar effect, we employ the original perturbation radius to compute the lower bounds\n\u00af z(\u03b8,x, y) and use a larger perturbation radius to\ncompute xadv for both CC-IBP and MTL-IBP. Specifically, we use train = 4.2, the radius employed by IBP-R (De Palma et al., 2022) to perform the attack and to compute the algorithm\u2019s regularization terms.\nTraining Schedule Following Shi et al. (2021), training proceeds as follows: the network is initialized according to the method presented by Shi et al. (2021). Training starts with the standard cross-entropy loss during warm-up. Then, the perturbation radius is gradually increased to the target value train (using the same schedule as Shi et al. (2021)) for a fixed number of epochs during the ramp-up phase. During warmp-up and ramp-up, the regularization introduced by Shi et al. (2021) is added to the objective. Finally, towards the end of training, and after ramp-up has ended, the learning rate is decayed twice. In line with previous work (Xiao et al., 2019; De Palma et al., 2022; M\u00fcller et al., 2023; Mao et al., 2023a), we employ `1 regularization throughout training.\nHyper-parameters The hyper-parameter configurations used in table 1 are presented in table 3. The training schedules for MNIST and CIFAR-10 with = 2/255 follow the hyper-parameters from Shi et al. (2021). Similarly to M\u00fcller et al. (2023), we increase the number of epochs after ramp-up for CIFAR-10 with = 8/255: we use 260 epochs in total. On TinyImageNet, we employ the same configuration as CIFAR-10 with = 2/255, and ImageNet64 relies on the TinyImageNet schedule from Shi et al. (2021), who did not benchmark on it. Following previous work (Shi et al., 2021; M\u00fcller et al., 2023), we use a batch size of 256 for MNIST, and of 128 everywhere else. As Shi et al. (2021); M\u00fcller et al. (2023), we use a learning rate of 5 \u00d7 10\u22125 for all experiments, decayed twice by a factor of 0.2, and use gradient clipping with threshold equal to 10. Coefficients\nfor the regularization introduced by Shi et al. (2021) are set to the values suggested in the author\u2019s official codebase: \u03bb = 0.5 on MNIST and CIFAR-10, \u03bb = 0.2 for TinyImageNet. On ImageNet64, which was not included in their experiments, we use \u03bb = 0.2 as for TinyImageNet given the dataset similarities. Single-step attacks were forced to lay on the boundary of the perturbation region (see appendix F.6), while we complied with previous work for the 8-step PGD attack (Balunovic & Vechev, 2020; De Palma et al., 2022). As Shi et al. (2021), we clip target epsilon values to the fifth decimal digit. The main object of our tuning were the \u03b1 coefficients from the CC-IBP and MTL-IBP losses (see \u00a74) and the `1 regularization coefficient: for both, we first selected an order of magnitude, then the significand via manual search. We do not perform early stopping. Unless otherwise stated, all the ablation studies re-use the hyper-parameter configurations from table 3. In particular, the ablation from \u00a76.2 use the single-step attack for CIFAR-10 with = 2/255. Furthermore, the ablation from appendix F.2 uses \u03b1 = 5 \u00d7 10\u22122 for CC-IBP and \u03b1 = 1 \u00d7 10\u22122 for MTL-IBP in order to ease verification."
        },
        {
            "heading": "E.3 COMPUTATIONAL SETUP",
            "text": "While each experiment (both for training and post-training verification) was run on a single GPU at a time, we used a total of 6 different machines and 12 GPUs. In particular, we relied on the following CPU models: Intel i7-6850K, i9-7900X CPU, i9-10920X, i9-10940X, AMD 3970X. The employed GPU models for most of the experiments were: Nvidia Titan V, Titan XP, and Titan XP. The experiments of \u00a76.3 and appendix F.3 on CIFAR-10 for = 8/255 were run on the following GPU models: RTX 4070 Ti, RTX 2080 Ti, RTX 3090. 8 GPUs of the following models: Nvidia Titan V, Nvidia Titan XP, and Nvidia Titan XP. The reported timings for the training experiments (table 1) were run on a Nvidia Titan V GPU and using the 20-core Intel i9-7900X CPU."
        },
        {
            "heading": "E.4 SOFTWARE ACKNOWLEDGMENTS",
            "text": "Our training code is mostly built on the publicly available training pipeline by Shi et al. (2021), which was released under a 3-Clause BSD license. We further rely on the automatic LiRPA library (Xu et al., 2020), also released under a 3-Clause BSD license, and on the OVAL complete verification framework (Bunel et al., 2020a; De Palma et al., 2021c), released under a MIT license. MNIST and CIFAR-10 are taken from torchvision.datasets (Paszke et al., 2019); we downloaded TinyImageNet from the Stanford CS231n course website (http://cs231n.stanford.edu/ TinyImageNet-200.zip), and downscaled ImageNet from the ImageNet website (https: //www.image-net.org/download.php)."
        },
        {
            "heading": "F ADDITIONAL EXPERIMENTAL RESULTS",
            "text": "We now present experimental results omitted from \u00a76."
        },
        {
            "heading": "F.1 ABLATION: POST-TRAINING VERIFICATION",
            "text": "Table 4 reports the verified robust accuracy of the models trained with CC-IBP and MTL-IBP from table 1 under the verification schemes outlined in \u00a76.2. As expected, the verified robust accuracy decreases when using inexpensive incomplete verifiers. The difference between BaB and CROWN/IBP accuracies varies with the setting, with larger perturbations displaying smaller gaps. Furthermore, CROWN bounds do not improve over those obtained via IBP on larger perturbations, mirroring previous observations on IBP-trained networks (Zhang et al., 2020). We note that MTL-IBP already attains state-of-the-art verified robustness under CROWN/IBP bounds on TinyImageNet and downscaled ImageNet (see table 1)."
        },
        {
            "heading": "F.2 ABLATION: ATTACK TYPES",
            "text": "Table 5 examines the effect of the employed attack type (how to compute xadv in equations (7) and (8)) on the performance of CC-IBP and MTL-IBP, focusing on CIFAR-10. PGD denotes an 8-step PGD (Madry et al., 2018) attack with constant step size \u03b7 = 0.25 , FGSM a single-step attack with \u03b7 \u2265 2.0 , whereas None indicates that no attack was employed (in other words, xadv = x). While FGSM and PGD use the same hyper-parameter configuration for CC-IBP and MTL-IBP from table 1, \u03b1 is raised when xadv = x in order to increase verifiability (see appendix E). As shown in table 5, the use of adversarial attacks, especially for = 2/255, are crucial to attain state-of-the-art verified robust accuracy. Furthermore, the use of inexpensive attacks only adds a minimal overhead (roughly 15% on = 2/255). If an attack is indeed employed, neither CC-IBP nor MTL-IBP appear to be particularly sensitive to the strength of the adversary for = 8/255, where FGSM slightly outperforms PGD. On the other hand, PGD improves the overall performance on = 2/255, leading both CC-IBP and MTL-IBP to better robustness-accuracy trade-offs than those reported by previous work on this benchmark. In general, given the overhead and the relatively small performance difference, FGSM appears to be preferable to PGD on most benchmarks."
        },
        {
            "heading": "F.3 TEST EVALUATION OF MODELS FROM \u00a76.3",
            "text": "Table 6 reports the results obtained by training the \u03b1 values for CC-IBP, MTL-IBP and SABR found in figure 3 on the entirety of the CIFAR-10 training set, and evaluating on the test set. All the other hyperparameters comply with those detailed in appendix E. In order to reflect the use of train = 4.2/255 for the attack computation in CC-IBP and MTL-IBP for = 2/255 (see appendix E), on that setup we compute the SABR loss over the train-time perturbation radius of train = 4.2/255. Branch-and-bound is run with a timeout of 600 seconds. The three expressive losses display similar trade-offs between verified robustness and standard accuracy, with CC-IBP slightly outperforming the other algorithms on = 2/255 and MTL-IBP on = 8/255. Observe that SABR yields a strictly better trade-off than the one reported by the original authors (79.24% and 62.84% standard and verified robust accuracy, respectively (M\u00fcller et al., 2023)), highlighting the importance of careful tuning. Given that, for = 2/255, the \u201csmall box\" may be well out of the target perturbation set, the result suggest that the performance of SABR itself is linked to definition 3.1 rather than to the selection of a subset of the target perturbation set containing the worst-case input. We remark that this may be in contrast with the explanation from the authors (M\u00fcller et al., 2023). By comparing with literature results from table 1\n(typically directly tuned on the test set, see appendix E.2), and given the heterogeneity across the three considered losses, we can conclude that satisfying expressivity leads to state-of-the-art results on ReLU networks. Finally, the test results confirm that minimizing the approximation of the branchand-bound loss does not necessarily correspond to superior robustness-accuracy trade-offs (see \u00a76.3).\nF.4 LOSS SENSITIVITY TO \u03b1\nFigure 4 compares the standard, adversarial (under a 40-step PGD attack with \u03b7 = 0.035 ) and verified losses (under IBP and CROWN/IBP bounds) of the networks from figure 2 with the CC-IBP and MTL-IBP losses (see \u00a74) computed with the corresponding training hyper-parameters. In all cases, their behavior closely follows the adversarial loss, which they upper bound. By comparing with figure 2, we conclude that the losses perform at their best in terms of trade-offs between BaB accuracy and standard accuracy when the losses used for training lower bound losses resulting from inexpensive incomplete verifiers. All verified losses converge to similar values for increasing \u03b1 coefficients."
        },
        {
            "heading": "F.5 EXPERIMENTAL VARIABILITY",
            "text": "In order to provide an indication of experimental variability for the results from \u00a7 6, we repeat a small subset of our experiments from table 1, those on CIFAR-10 with = 8/255, a further three times. Table 7 reports the mean, maximal and minimal values in addition to the sample standard deviation for both the standard and verified robust accuracies. In all cases, the experimental variability is relatively small."
        },
        {
            "heading": "F.6 ABLATION: ADVERSARIAL ATTACK STEP SIZE",
            "text": "Section F.2 investigates the influence of the type of attack used to compute z(\u03b8,xadv, y) at training time on the performance of CC-IBP and MTL-IBP. We now perform a complementary ablation by\nkeeping the attack fixed to randomly-initialized FGSM (Goodfellow et al., 2015; Wong et al., 2020) and changing the attack step size \u03b7. The results from \u00a76 (except for the PGD entry in table 5) employ a step size \u03b7 = 10.0 (any value larger than 2.0 yields the same effect) in order to force the attack on the boundary of the perturbation region. Specifically, we report the performance for \u03b7 = 1.25 , which was shown by Wong et al. (2020) to prevent catastrophic overfitting. Table 8 does not display significant performance differences depending on the step size, highlighting the lack of catastrophic overfitting. We speculate that this is linked to the regularizing effect of the IBP bounds employed within CC-IBP and MTL-IBP. On all considered settings, the use of larger \u03b7 results in larger standard accuracy, and a decreased verified robust accuracy."
        }
    ],
    "year": 2023
}