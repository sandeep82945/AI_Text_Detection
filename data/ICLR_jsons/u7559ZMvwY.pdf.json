{
    "abstractText": "The deep neural networks are known to be vulnerable to well-designed adversarial attacks. The most successful defense technique based on adversarial training (AT) can achieve optimal robustness against particular attacks but cannot generalize well to unseen attacks. Another effective defense technique based on adversarial purification (AP) can enhance generalization but cannot achieve optimal robustness. Meanwhile, both methods share one common limitation on the degraded standard accuracy. To mitigate these issues, we propose a novel pipeline called Adversarial Training on Purification (AToP), which comprises two components: perturbation destruction by random transforms (RT) and purifier model fine-tuned (FT) by adversarial loss. RT is essential to avoid overlearning to known attacks, resulting in the robustness generalization to unseen attacks and FT is essential for the improvement of robustness. To evaluate our method in an efficient and scalable way, we conduct extensive experiments on CIFAR-10, CIFAR-100, and ImageNette to demonstrate that our method achieves state-of-the-art results and exhibits generalization ability against unseen attacks.",
    "authors": [
        {
            "affiliations": [],
            "name": "Guang Lin"
        },
        {
            "affiliations": [],
            "name": "Chao Li"
        },
        {
            "affiliations": [],
            "name": "Jianhai Zhang"
        },
        {
            "affiliations": [],
            "name": "Toshihisa Tanaka"
        },
        {
            "affiliations": [],
            "name": "Qibin Zhao"
        }
    ],
    "id": "SP:e3b185621c3b74d9acc46f02f4ed4aad41efdaf3",
    "references": [
        {
            "authors": [
                "Anish Athalye",
                "Nicholas Carlini",
                "David Wagner"
            ],
            "title": "Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples",
            "venue": "In International conference on machine learning,",
            "year": 2018
        },
        {
            "authors": [
                "Anish Athalye",
                "Logan Engstrom",
                "Andrew Ilyas",
                "Kevin Kwok"
            ],
            "title": "Synthesizing robust adversarial examples",
            "venue": "In International conference on machine learning,",
            "year": 2018
        },
        {
            "authors": [
                "Maximilian Augustin",
                "Alexander Meinke",
                "Matthias Hein"
            ],
            "title": "Adversarial robustness on in-and outdistribution improves explainability",
            "venue": "In European Conference on Computer Vision,",
            "year": 2020
        },
        {
            "authors": [
                "Yatong Bai",
                "Brendon G Anderson",
                "Aerin Kim",
                "Somayeh Sojoudi"
            ],
            "title": "Improving the accuracyrobustness trade-off of classifiers via adaptive smoothing",
            "venue": "arXiv preprint arXiv:2301.12554,",
            "year": 2023
        },
        {
            "authors": [
                "Yassine Bakhti",
                "Sid Ahmed Fezza",
                "Wassim Hamidouche",
                "Olivier D\u00e9forges"
            ],
            "title": "Ddsa: A defense against adversarial attacks using deep denoising sparse autoencoder",
            "venue": "IEEE Access,",
            "year": 2019
        },
        {
            "authors": [
                "Nicholas Carlini",
                "David Wagner"
            ],
            "title": "Towards evaluating the robustness of neural networks",
            "venue": "In 2017 ieee symposium on security and privacy (sp),",
            "year": 2017
        },
        {
            "authors": [
                "Yair Carmon",
                "Aditi Raghunathan",
                "Ludwig Schmidt",
                "John C Duchi",
                "Percy S Liang"
            ],
            "title": "Unlabeled data improves adversarial robustness",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Sizhe Chen",
                "Zhehao Huang",
                "Qinghua Tao",
                "Yingwen Wu",
                "Cihang Xie",
                "Xiaolin Huang"
            ],
            "title": "Adversarial attack on attackers: Post-process to mitigate black-box score-based query attacks",
            "venue": "Conference on neural information processing systems,",
            "year": 2022
        },
        {
            "authors": [
                "Jeremy Cohen",
                "Elan Rosenfeld",
                "Zico Kolter"
            ],
            "title": "Certified adversarial robustness via randomized smoothing",
            "venue": "In international conference on machine learning,",
            "year": 2019
        },
        {
            "authors": [
                "Francesco Croce",
                "Matthias Hein"
            ],
            "title": "Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks",
            "venue": "In International conference on machine learning,",
            "year": 2020
        },
        {
            "authors": [
                "Francesco Croce",
                "Maksym Andriushchenko",
                "Vikash Sehwag",
                "Edoardo Debenedetti",
                "Nicolas Flammarion",
                "Mung Chiang",
                "Prateek Mittal",
                "Matthias Hein"
            ],
            "title": "Robustbench: a standardized adversarial robustness benchmark",
            "venue": "In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round",
            "year": 2021
        },
        {
            "authors": [
                "Jiequan Cui",
                "Zhuotao Tian",
                "Zhisheng Zhong",
                "Xiaojuan Qi",
                "Bei Yu",
                "Hanwang Zhang"
            ],
            "title": "Decoupled kullback-leibler divergence loss",
            "venue": "arXiv preprint arXiv:2305.13948,",
            "year": 2023
        },
        {
            "authors": [
                "Tao Dai",
                "Yan Feng",
                "Bin Chen",
                "Jian Lu",
                "Shu-Tao Xia"
            ],
            "title": "Deep image prior based defense against adversarial examples",
            "venue": "Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Yao Deng",
                "Xi Zheng",
                "Tianyi Zhang",
                "Chen Chen",
                "Guannan Lou",
                "Miryung Kim"
            ],
            "title": "An analysis of adversarial attacks and defenses on autonomous driving models",
            "venue": "IEEE international conference on pervasive computing and communications (PerCom),",
            "year": 2020
        },
        {
            "authors": [
                "Gavin Weiguang Ding",
                "Yash Sharma",
                "Kry Yik Chau Lui",
                "Ruitong Huang"
            ],
            "title": "Mma training: Direct input space margin maximization through adversarial training",
            "venue": "In International Conference on Learning Representations,",
            "year": 2019
        },
        {
            "authors": [
                "Hadi M Dolatabadi",
                "Sarah Erfani",
                "Christopher Leckie"
            ],
            "title": "l-inf robustness and beyond: Unleashing efficient adversarial training",
            "venue": "In European Conference on Computer Vision,",
            "year": 2022
        },
        {
            "authors": [
                "Ian J Goodfellow",
                "Jonathon Shlens",
                "Christian Szegedy"
            ],
            "title": "Explaining and harnessing adversarial examples",
            "venue": "International Conference on Learning Representations,",
            "year": 2015
        },
        {
            "authors": [
                "Sven Gowal",
                "Chongli Qin",
                "Jonathan Uesato",
                "Timothy Mann",
                "Pushmeet Kohli"
            ],
            "title": "Uncovering the limits of adversarial training against norm-bounded adversarial examples",
            "venue": "arXiv preprint arXiv:2010.03593,",
            "year": 2020
        },
        {
            "authors": [
                "Sven Gowal",
                "Sylvestre-Alvise Rebuffi",
                "Olivia Wiles",
                "Florian Stimberg",
                "Dan Andrei Calian",
                "Timothy A Mann"
            ],
            "title": "Improving robustness using generated data",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Dan Hendrycks",
                "Kimin Lee",
                "Mantas Mazeika"
            ],
            "title": "Using pre-training can improve model robustness and uncertainty",
            "venue": "In International conference on machine learning,",
            "year": 2019
        },
        {
            "authors": [
                "Uiwon Hwang",
                "Jaewoo Park",
                "Hyemi Jang",
                "Sungroh Yoon",
                "Nam Ik Cho"
            ],
            "title": "Puvae: A variational autoencoder to purify adversarial examples",
            "venue": "IEEE Access,",
            "year": 2019
        },
        {
            "authors": [
                "Alex Krizhevsky",
                "Geoffrey Hinton"
            ],
            "title": "Learning multiple layers of features from tiny images",
            "venue": "Technical Report,",
            "year": 2009
        },
        {
            "authors": [
                "C Laidlaw",
                "S Singla",
                "S Feizi"
            ],
            "title": "Perceptual adversarial robustness: Defense against unseen threat models",
            "venue": "In International Conference on Learning Representations (ICLR),",
            "year": 2021
        },
        {
            "authors": [
                "Minjong Lee",
                "Dongwoo Kim"
            ],
            "title": "Robust evaluation of diffusion-based adversarial purification",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV),",
            "year": 2023
        },
        {
            "authors": [
                "Bai Li",
                "Changyou Chen",
                "Wenlin Wang",
                "Lawrence Carin"
            ],
            "title": "Certified adversarial robustness with additive noise",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Aleksander Madry",
                "Aleksandar Makelov",
                "Ludwig Schmidt",
                "Dimitris Tsipras",
                "Adrian Vladu"
            ],
            "title": "Towards deep learning models resistant to adversarial attacks",
            "venue": "International Conference on Learning Representations,",
            "year": 2018
        },
        {
            "authors": [
                "Aleksander Madry",
                "Aleksandar Makelov",
                "Ludwig Schmidt",
                "Dimitris Tsipras",
                "Adrian Vladu"
            ],
            "title": "Towards deep learning models resistant to adversarial attacks",
            "venue": "In International Conference on Learning Representations,",
            "year": 2018
        },
        {
            "authors": [
                "Weili Nie",
                "Brandon Guo",
                "Yujia Huang",
                "Chaowei Xiao",
                "Arash Vahdat",
                "Anima Anandkumar"
            ],
            "title": "Diffusion models for adversarial purification",
            "venue": "International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Tianyu Pang",
                "Min Lin",
                "Xiao Yang",
                "Jun Zhu",
                "Shuicheng Yan"
            ],
            "title": "Robustness and accuracy could be reconcilable by (proper) definition",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Adam Paszke",
                "Sam Gross",
                "Francisco Massa",
                "Adam Lerer",
                "James Bradbury",
                "Gregory Chanan",
                "Trevor Killeen",
                "Zeming Lin",
                "Natalia Gimelshein",
                "Luca Antiga"
            ],
            "title": "Pytorch: An imperative style, high-performance deep learning library",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Omid Poursaeed",
                "Tianxing Jiang",
                "Harry Yang",
                "Serge Belongie",
                "Ser-Nam Lim"
            ],
            "title": "Robustness and generalization via generative adversarial training",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Edward Raff",
                "Jared Sylvester",
                "Steven Forsyth",
                "Mark McLean"
            ],
            "title": "Barrage of random transforms for adversarially robust defense",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Sylvestre-Alvise Rebuffi",
                "Sven Gowal",
                "Dan A Calian",
                "Florian Stimberg",
                "Olivia Wiles",
                "Timothy Mann"
            ],
            "title": "Fixing data augmentation to improve adversarial robustness",
            "venue": "arXiv preprint arXiv:2103.01946,",
            "year": 2021
        },
        {
            "authors": [
                "Gwonsang Ryu",
                "Daeseon Choi"
            ],
            "title": "A hybrid adversarial training for deep learning model and denoising network resistant to adversarial examples",
            "venue": "Applied Intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "Changhao Shi",
                "Chester Holtz",
                "Gal Mishne"
            ],
            "title": "Online adversarial purification based on selfsupervision",
            "venue": "International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Vignesh Srinivasan",
                "Csaba Rohrer",
                "Arturo Marban",
                "Klaus-Robert M\u00fcller",
                "Wojciech Samek",
                "Shinichi Nakajima"
            ],
            "title": "Robustifying models against adversarial attacks by langevin dynamics",
            "venue": "Neural Networks,",
            "year": 2021
        },
        {
            "authors": [
                "David Stutz",
                "Matthias Hein",
                "Bernt Schiele"
            ],
            "title": "Confidence-calibrated adversarial training: Generalizing to unseen attacks",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Christian Szegedy",
                "Wojciech Zaremba",
                "Ilya Sutskever",
                "Joan Bruna",
                "Dumitru Erhan",
                "Ian Goodfellow",
                "Rob Fergus"
            ],
            "title": "Intriguing properties of neural networks",
            "venue": "International Conference on Learning Representations,",
            "year": 2014
        },
        {
            "authors": [
                "Jihoon Tack",
                "Sihyun Yu",
                "Jongheon Jeong",
                "Minseon Kim",
                "Sung Ju Hwang",
                "Jinwoo Shin"
            ],
            "title": "Consistency regularization for adversarial robustness",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "Florian Tram\u00e8r",
                "Alexey Kurakin",
                "Nicolas Papernot",
                "Ian Goodfellow",
                "Dan Boneh",
                "Patrick McDaniel"
            ],
            "title": "Ensemble adversarial training: Attacks and defenses",
            "venue": "International Conference on Learning Representations,",
            "year": 2018
        },
        {
            "authors": [
                "Florian Tramer",
                "Nicholas Carlini",
                "Wieland Brendel",
                "Aleksander Madry"
            ],
            "title": "On adaptive attacks to adversarial example defenses",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Giorgio Ughini",
                "Stefano Samele",
                "Matteo Matteucci"
            ],
            "title": "Trust-no-pixel: A remarkably simple defense against adversarial attacks based on massive inpainting",
            "venue": "In 2022 International Joint Conference on Neural Networks (IJCNN),",
            "year": 2022
        },
        {
            "authors": [
                "Zekai Wang",
                "Tianyu Pang",
                "Chao Du",
                "Min Lin",
                "Weiwei Liu",
                "YAN Shuicheng"
            ],
            "title": "Better diffusion models further improve adversarial training",
            "venue": "In International Conference on Machine Learning. PMLR,",
            "year": 2023
        },
        {
            "authors": [
                "QuanLin Wu",
                "Hang Ye",
                "Yuntian Gu",
                "Huishuai Zhang",
                "Liwei Wang",
                "Di He"
            ],
            "title": "Denoising masked autoencoders help robust classification",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Chaowei Xiao",
                "Jun-Yan Zhu",
                "Bo Li",
                "Warren He",
                "Mingyan Liu",
                "Dawn Song"
            ],
            "title": "Spatially transformed adversarial examples",
            "venue": "In International Conference on Learning Representations,",
            "year": 2018
        },
        {
            "authors": [
                "Yuzhe Yang",
                "Guo Zhang",
                "Dina Katabi",
                "Zhi Xu"
            ],
            "title": "Me-net: Towards effective adversarial robustness with matrix estimation",
            "venue": "International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Sergey Zagoruyko",
                "Nikos Komodakis"
            ],
            "title": "Wide residual networks",
            "venue": "In Procedings of the British Machine Vision Conference",
            "year": 2016
        },
        {
            "authors": [
                "Hongyang Zhang",
                "Yaodong Yu",
                "Jiantao Jiao",
                "Eric Xing",
                "Laurent El Ghaoui",
                "Michael Jordan"
            ],
            "title": "Theoretically principled trade-off between robustness and accuracy",
            "venue": "In International conference on machine learning,",
            "year": 2019
        },
        {
            "authors": [
                "Jingfeng Zhang",
                "Jianing Zhu",
                "Gang Niu",
                "Bo Han",
                "Masashi Sugiyama",
                "Mohan Kankanhalli"
            ],
            "title": "Geometry-aware instance-reweighted adversarial training",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Deep neural networks (DNNs) have been shown to be susceptible to adversarial examples (Szegedy et al., 2014; Deng et al., 2020), which are generated by adding small, human-imperceptible perturbations to natural images, but completely change the prediction results to DNNs, leading to disastrous implications (Goodfellow et al., 2015). Since then, numerous methods have been proposed to defend against adversarial examples. Notably, adversarial training (AT) has gained significant attention as a robust learning algorithm against adversarial attacks (Goodfellow et al., 2015; Madry et al., 2018a). AT has proven effective in achieving state-of-the-art robustness against known attacks, albeit with a tendency to overfit the specific adversarial examples seen during training. Consequently, within the framework of AT, the persistent conundrum revolves around striking a balance between standard accuracy and robustness (Zhang et al., 2019), a dilemma that has been a subject of enduring concern. Furthermore, the robustness of AT defense against multiple or even unforeseen attacks still remains challenging (Poursaeed et al., 2021; Laidlaw et al., 2021; Tack et al., 2022).\nMore recently, the concept of adversarial purification (AP) has emerged, aiming to eliminate perturbations from potentially attacked images before inputting them into the classifier model (Yang et al., 2019; Shi et al., 2021; Nie et al., 2022). In contrast to AT methods, AP operates as a pre-processing step that can defend against unseen attacks without retraining the classifier model. However, AP exhibits lower robustness against known attacks compared to AT methods. Meanwhile, both AT and AP have a common limitation on the degraded standard accuracy (Tramer et al., 2020; Chen et al., 2022), which restricts their applicability in real-world applications. As summarized in Table 1, the present landscape of defense methods is confronted with a formidable challenge: How can we\n* Correspondence to: Qibin Zhao <qibin.zhao@riken.jp>\nenhance robustness against known attacks while maintaining generalization to unseen attacks and preserving standard accuracy on clean examples?\nTo tackle these challenges with the framework of AT and AP, we propose a novel defense technique that conceptionally separates AP method into two components: perturbation destruction by random transforms and fine-tuning the purifier model via supervised AT called Adversarial Training on Purification (AToP), as shown in Figure 1. Specifically, we develop three types of random transforms and conduct a comprehensive investigation into their effectiveness in augmenting robustness and generalization against attacks. The purifier model is designed to reconstruct clean examples from corrupted inputs, which can also ensure accurate classification outputs regardless of whether the inputs are corrupted adversarial examples or corrupted clean examples. Furthermore, more importantly, we introduce the adversarial loss calculated from the output of classifier model to fine-tune the purifier model in a supervised manner while keeping the classifier model fixed. Differing from the traditional AP methods, our method can learn the purifier model which can not only generate high-quality purified examples but also avoid generating examples to incorrect classes.\nWe empirically evaluate the performance of our method by comparing with the latest AT and AP methods across various attacks (i.e., FGSM (Goodfellow et al., 2015), PGD (Madry et al., 2018b), CW (Carlini & Wagner, 2017), AutoAttack (Croce & Hein, 2020), and StAdv (Xiao et al., 2018)) and conduct extensive experiments on CIFAR-10, CIFAR-100 and ImageNette employing multiple classifier models and purifier models. The experimental results demonstrate that our method achieves state-of-the-art results and exhibits robust generalization against unseen attacks. Furthermore, our method significantly improves the performance of the purifier model in robust classification, surpassing the performance of the same model used in recent studies (Ughini et al., 2022; Wu et al., 2023). To provide a more comprehensive insight into our results, we conduct visual comparisons among multiple examples. In summary, the contributions of this work are as follows.\n\u2022 We propose adversarial training on purification (AToP), a novel defense technique that effectively combines the strengths of adversarial training and adversarial purification.\n\u2022 We separate adversarial purification process into two components, including perturbation destruction and fine-tuning purifier model, and investigate their impacts on robustness.\n\u2022 We conduct extensive experiments to empirically demonstrate that the proposed method significantly improves the performance of the purifier model in robust classification."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Adversarial training is initially proposed by Goodfellow et al. (2015) to defend against adversarial attacks. While adversarial training has been demonstrated as an effective defense against attacks, it remains susceptible to unseen attacks (Stutz et al., 2020; Poursaeed et al., 2021; Laidlaw et al., 2021; Tack et al., 2022). Additionally, retraining the classifier model leads to substantial training expenses and could severely impair the standard accuracy of the model (Madry et al., 2018a; Tram\u00e8r et al., 2018; Zhang et al., 2019). In a study conducted by Raff et al. (2019), the impact of random transforms on the robustness of defense method was investigated. Their research revealed that the inclusion of a broader range of random transforms applied to images before adversarial training can significantly improve robustness. However, the utilization of random transforms also led to the loss of semantic information, which can severely affect the accuracy. Although our framework\nuses random transforms and AT strategy, the essential difference with these works is that we aim to train the purifier model to recover semantics from corrupted examples using AT, rather than training the classifier model to learn the perturbation features using AT. Yang et al. (2019) also proposed a method that combines adversarial training and purification. Specifically, a purifier model was used prior to the classifier model, and AT was only applied to the classifier. Meanwhile, Ryu & Choi (2022) applied a similar pipeline but fine-tuned both the classifier model and the purifier model using AT. Consequently, the training process became time-intensive. In contrast, our method applies the adversarial loss to train the purifier model, which avoids the need for retraining the classifier model and mitigating the risk of overfitting specific attacks.\nAdversarial purification aims to purify adversarial examples before classification, which has emerged as a promising defense method (Shi et al., 2021; Srinivasan et al., 2021). Currently, the most prevalent pipeline within AP involves the utilization of a generative model to recover purified examples from the adversarial examples. Some prior studies (Bakhti et al., 2019; Hwang et al., 2019) employed consistency losses to train the purifier model, which obtains similar results to AT methods. Nevertheless, these methods primarily capture the distribution of specific perturbations, which also limits their effectiveness in defending against unseen attacks. Other studies (Nie et al., 2022; Ughini et al., 2022; Wu et al., 2023) employed the pre-trained generator model as a purifier model. These methods have acquired the ability to recover information from noisy images through extensive exposure to clean examples, enabling them to provide an effective defense against even unseen attacks. However, there is a limitation of the existing pre-trained generator-based purifier model, where all parameters are fixed and cannot be further improved for known attacks (Dai et al., 2022). To address these issues, we employ a pre-trained generator model as an initial purifier model, which can be subsequently fine-tuned using examples in a supervised manner, i.e., adversarial training on purifier model. Additionally, we introduce an adversarial loss derived from classifier outputs rather than consistency loss derived from purifier outputs. As shown in the experiments, our method achieves improved robustness accuracy without compromising generalization against unseen attacks."
        },
        {
            "heading": "3 METHOD",
            "text": "In this section, we introduce a novel defense technique called Adversarial Training on Purification (AToP), which comprises two components: random transform (RT) and fine-tuning (FT) the purifier model. Specifically, RT aims to destruct adversarial perturbations, irrespective of the type of attack, thereby achieving effective defense against even unseen attacks. On the other hand, FT aims to generate purified examples from corrupted inputs with true label information, thus further enhancing robustness.\nThe entire classification system comprises three processes: transform denoted t(\u00b7), purifier denoted g(\u00b7), and classifier denoted f(\u00b7). Given an input x, the pipeline of output y can be formulated as\nxt = t (x; \u03b8t) , x\u0302 = g (xt; \u03b8g) , y = f (x\u0302; \u03b8f ) , (1)\nwhere \u03b8t, \u03b8g, \u03b8f represent parameters involved in the transform, purifier, and classifier, respectively."
        },
        {
            "heading": "3.1 DESTRUCT PERTURBATION STRUCTURE BY RANDOM TRANSFORMS",
            "text": "We utilize three types of random transforms, ranging from simple to complex cases.\nThe first type of transform (RT1) utilizes a binary mask m with small patches of size p\u00d7 p that are randomly missing with missing rate r (Yang et al., 2019). As shown in Eq. 2, given any input x, we take the element-wise multiplication m\u2299 x,\nxt = m\u2299 x, x\u0302 = g(xt). (2)\nThe second type of transform (RT2) involves the addition of Gaussian noise \u03b7 before applying the random mask m to further destruct the perturbations. As shown in Figure 2(a), we have\nxt = m\u2299 (x+ \u03b7), x\u0302 = g(xt). (3)\nRandomized smoothing by adding Gaussian noise is a method that provides provable defenses (Li et al., 2019; Cohen et al., 2019) where Cohen et al. (2019) theoretically provides robustness guarantees\nfor classifiers. In the latest work, Wu et al. (2023) enhance robustness by learning representations through a generator model, as shown in Eq. 3.\nThe third type of transform (RT3) involves the repetition of N transformations on a single image to ensure that all pixel values are regenerated by the purifier model, thereby removing as many perturbations as possible (Ughini et al., 2022). As shown in Figure 2(b), the noisy image is randomly covered by N non-overlapping masks mi. After being processed by purifier model, only the pixels generated by the purifier model are aggregated to yield x\u0302, which is\nxit = mi \u2299 (x+ \u03b7), xig = g(xit), x\u0302 = \u2211N\ni=1 (1\u2212mi)\u2299 xig. (4)"
        },
        {
            "heading": "3.2 FINE-TUNING THE PURIFIER MODEL WITH ADVERSARIAL LOSS",
            "text": "Algorithm 1 Adversarial Training on Purification method (AToP) Require: Training examples x, ground truth y, parameters of classifier model \u03b8f , parameters of\npurifier model \u03b8g , training epoch Nep 1: Initialize \u03b8f and \u03b8g with pre-trained classifier model and pre-trained purifier model. 2: for epoch = 1...Nep do 3: Build adversarial examples x\u2032 with adversarial attacks: x\u2032 \u2190 x+ \u03b4 4: Freeze \u03b8f and update \u03b8g with gradient descent based on loss in Eq. (9). 5: \u03b8g \u2190 \u03b8g \u2212\u2207\u03b8g 6: end for 7: return purifier model with \u03b8g\nHaving described the random transforms, we proceed to provide a description of the purifier model and fine-tuning process. Specifically, based on pre-trained generator-based purifier model trained by the original loss function Lorg, we have redesigned the loss function, incorporating an adversarial loss to fine-tune the purifier model:\nL\u03b8g = Lorg (x, \u03b8g) + \u03bb \u00b7 Lcls (x, y, \u03b8g, \u03b8f ) , (5) where \u03bb is held constant as hyperparameter. In the paper, we improve upon two recent studies, and the original loss function Lorg is denoted as Ldf in the GAN-based model (Ughini et al., 2022), and as Lmae in the AE-based model (Wu et al., 2023). For instance, we utilize the GAN model called DeepFill as the purifier model g, in conjunction with RT3 and the classifier model F. Given any input x, our defense process is defined as below:\nxit = t (x; \u03b8t) = RT3(x) = mi \u2299 (x+ \u03b7), (6)\nxig = g ( xit; \u03b8g ) = DeepFill(xit), x\u0302 = \u2211N i=1 (1\u2212mi)\u2299 xig, (7)\ny = f (x\u0302; \u03b8f ) = F(x\u0302), (8)\nwhere \u03b8t contains three components: Gaussian standard \u03c3, missing rate r, and mask number N . The parameter \u03b8f is derived from the pre-trained classifier model. To perform robust fine-tuning of the purifier model using adversarial training, we aim to optimize the model by freezing classifier parameter \u03b8f and updating purifier parameter \u03b8g, where \u03b8f and \u03b8g are initialized by the pre-trained classifier model and the pre-trained generator model, respectively, as shown in Algorithm 1.\nFor pre-training GAN-based model (Ughini et al., 2022), we have L\u03b8g = Ldf (x, \u03b8g) = E[D(x)]\u2212 E[D(g(t(x), \u03b8g))] + E[\u2225x\u2212 g(t(x), \u03b8g)\u2225\u21131 ]. To further optimize \u03b8g, we propose our AToP loss function and fine-tune the pre-trained GAN-based model, which comprise an original loss Ldf , as well as an additional adversarial loss Lcls. Thus, we have\nL\u03b8g = Ldf (x \u2032, \u03b8g) + \u03bb \u00b7 Lcls (x\u2032, y, \u03b8g, \u03b8f )\n= E[D(x\u2032)]\u2212 E[D(g(t(x\u2032), \u03b8g))] + E[\u2225x\u2032 \u2212 g(t(x\u2032), \u03b8g)\u2225\u21131 ] + \u03bb \u00b7max\n\u03b4 CE {y,F[g(t(x\u2032), \u03b8g)]} ,wherex\u2032 = x+ \u03b4.\n(9)\nHere, Ldf comprises the first three terms, where D represents the discriminator, and g is the generatorbased purifier model, which plays similar roles as the generative adversarial network model. During training, the discriminator D is responsible for distinguishing between real examples and the purified examples g(t(x), \u03b8g). Simultaneously, the purifier model g generates high-quality examples to deceive the discriminator D. The third term is consistency loss \u2225x\u2212 g(t(x), \u03b8g)\u2225\u21131 , which encourages strong image restoration ability to the original examples x. Consequently, Ldf effectively ensures that the purified examples are not only high-quality but also closely resemble its original example. If RT is sufficiently strong to the extent that t(x) is essentially Gaussian noise, Ldf loss can ensure the purified examples are generated from the same distribution of x and are of high-quality. Conversely, if RT is minimal such as t(x) = x, Ldf is instrumental in guaranteeing that the purifier model can achieve an exact recovery of the original examples. Consequently, if RT is strong enough to destruct the adversarial perturbations, the model can guarantee to generate examples from the original distribution while avoiding the reconstruction of adversarial perturbations. Thus, the effectiveness remains unaffected even for unseen attacks.\nDuring fine-tuning, Lcls corresponds to the last term, which serves the purpose of further ensuring that our purified examples x\u0302 do not contain sufficient adversarial perturbations that fool the classifier. On the other hand, if RT is very strong and purified examples x\u0302 do not contain any adversarial perturbations, Lcls can also ensure x\u0302 will not be high-quality examples drawn from different classes. Intuitively, the adversarial perturbation \u03b4 is generated by maximizing classification loss, which can be used as the new examples to train our purifier model such that adversarial perturbations can be reconstructed up to the degree that remains correct classification. Unlike standard AT, our adversarial loss is not used to update the classifier model but to optimize the purifier model. As compared to existing AP methods using adversarial examples (Bakhti et al., 2019; Hwang et al., 2019), our adversarial loss is essentially more robust than either consistency loss or generator-style loss. Finally, \u03bb > 0 represents a weight hyperparameter to balance the generative error and classification error."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": "In this section, we conduct extensive experiments on CIFAR-10, CIFAR-100 and ImageNette across various transforms, classifier models and purifier models on attack benchmarks. Our findings demonstrate that the proposed method achieves state-of-the-art results and exhibits generalization ability against unseen attacks. Specifically, compared to against AutoAttack l\u221e on CIFAR-10, our method improves the robust accuracy by 13.28% on GAN-based purifier model and 14.45% on AE-based purifier model, respectively. Furthermore, we empirically study the random transforms and disccuss their impacts on robust accuracy. Finally, we provide visual examples (more details can be found in supplementary materials) to provide a more comprehensive understanding of our results."
        },
        {
            "heading": "4.1 EXPERIMENTAL SETUP",
            "text": "Datasets and model architectures: We conduct extensive experiments on CIFAR-10, CIFAR-100 (Krizhevsky et al., 2009) and ImageNette (a subset of 10 classified classes from ImageNet) (Howard, 2021) to empirically validate the effectiveness of the proposed methods against adversarial attacks. For the classifier model, we utilize the pre-trained ResNet model (He et al., 2016) and WideResNet\nmodel (Zagoruyko & Komodakis, 2016). For the purifier model, we utilize the pre-trained GAN-based model (Ughini et al., 2022) and AE-based model (Wu et al., 2023) and utilize adversarial examples generated by FGSM for the fine-tuning.\nAdversarial attacks: We evaluate our method against various attacks: We utilize AutoAttack l\u221e and l2 threat models (Croce & Hein, 2020) as one benchmark, which is a powerful attack that combines both white-box and black-box attacks, and utilizes Expectation Over Time (EOT) (Athalye et al., 2018b) to tackle the stochasticity introduced by random transforms. To consider unseen attacks without lp-norm, we utilize spatially transformed adversarial examples (StAdv) (Xiao et al., 2018) for validation. Additionally, we generate adversarial examples using some standard methods including Fast Gradient Sign Method (FGSM) (Goodfellow et al., 2015), Projected Gradient Descent (PGD) (Madry et al., 2018b) and Carlini-Wagner (CW) Attack (Carlini & Wagner, 2017).\nEvaluation metrics: We evaluate the performance of defense methods using two metrics: standard accuracy and robust accuracy, obtained by testing on clean examples and adversarial examples, respectively. Due to the high computational cost of testing models with multiple attacks, following guidance by Nie et al. (2022), we randomly select 512 images from the test set for robust evaluation.\nTraining details: After experimental testing, we have determined the hyperparameters: Gaussian standard deviation \u03c3 = 0.25, mask size p = P/8 (P is side length of input x), mask number N = 4, missing rate r = 0.25 and weight \u03bb = 0.1. All experiments presented in the paper are conducted\nDefense method CIFAR CIFAR CIFAR10, l\u221e 10, l2 100, l\u221e Wu et al. (2023) 44.73 45.12 38.48 Wu et al. (2023) 59.18 65.43 40.23+AToP (Ours)\nUghini et al. (2022) 59.57 63.69 38.89 Ughini et al. (2022) 72.85 80.47 43.55+AToP (Ours)\nunder these hyperparameters and performed by NVIDIA RTX A5000 Graphics Card with 24GB GDDR6 GPU memory, CUDA v11.7 and cuDNN v8.5.0 in PyTorch v1.13.11 (Paszke et al., 2019)."
        },
        {
            "heading": "4.2 COMPARISON WITH STATE-OF-THE-ART METHODS",
            "text": "We evaluate our method against AutoAttack l\u221e and l2 threat models and compare its robustness to state-of-the-art methods listed in RobustBench (Croce et al., 2021). All experiments presented in this section are utilizing RT2 and GAN-based model, which yielded the best results in the experiment.\nResult analysis on CIFAR-10: Table 2 shows the performance of the defense methods against AutoAttack l\u221e (\u03f5 = 8/255) threat model on CIFAR-10 with WideResNet-28-10 and WideResNet70-16 classifier models. Our method outperforms all other methods without extra data (the dataset introduced by Carmon et al. (2019)) in terms of both standard accuracy and robust accuracy. Specifically, compared to the second-best method, our method improves the robust accuracy by 2.21% on WideResNet-28-10 and by 5.08% on WideResNet-70-16, respectively. Furthermore, in terms of robust accuracy, our method even outperforms the methods with extra data. Table 3 shows the performance of the defense methods against AutoAttack l2 (\u03f5 = 0.5) threat model. Our method outperforms all other methods without extra data in terms of robust accuracy. Specifically, compared to the second-best method, our method improves the robust accuracy by 1.89% on WideResNet-28-10 and by 0.75% on WideResNet-70-16, respectively.\nTable 5 shows the performance of the two generator-based purifier models (Ughini et al., 2022; Wu et al., 2023) against AutoAttack l\u221e (\u03f5 = 8/255) and l2 (\u03f5 = 0.5) threat models with WideResNet-2810 classifier model. Specifically, our method improves the robust accuracy by 14.45% and 20.31% on AE-based purifier model, 13.28% and 16.87% on GAN-based purifier model, respectively. The results demonstrate that our method can significantly improve the robustness of the purifier model.\nResult analysis on CIFAR-100: Table 4 shows the performance of the defense methods against AutoAttack l\u221e (\u03f5 = 8/255) threat model on CIFAR-100 with WideResNet-28-10 and WideResNet70-16 classifier models. Our method outperforms all other methods without extra data in terms of both standard accuracy and robust accuracy. Specifically, compared to the second-best method, our method improves the robust accuracy by 4.37% on WideResNet-28-10 and by 1.77% on WideResNet-70-16, respectively. Table 5 shows the performance of the models against AutoAttack l\u221e (\u03f5 = 8/255) and the observations are basically consistent with CIFAR-10, demonstrating the effectiveness of our method in enhancing robust classification across different datasets."
        },
        {
            "heading": "4.3 DEFEND AGAINST UNSEEN ATTACKS",
            "text": "As previously mentioned, the mainstream adversarial training (AT) methods have shown limitations in defending against unseen attacks (Laidlaw et al., 2021; Dolatabadi et al., 2022), even can only defend against known attacks. To demonstrate the generalization ability of our method with RT2 and GAN, we conduct experiments in which defense methods are evaluated against attacks with\nvarying constraints (including AutoAttack l\u221e, l2 and StAdv non-lp threat models) on CIFAR-10 with ResNet-50 classifier model.\nTable 6 shows that AT method can achieve robustness against particular attacks seen during training (as indicated by Acc. with underscore) but struggles to generalize robustness to unseen attacks. The intuitive idea is to apply AT across all different attacks to obtain a robust model. However, it is challenging to train such a model due to the inherent differences among all sorts of attacks. Furthermore, the emergence of new attack techniques continually makes it impractical to train the model to defend against all attacks. We combine AT and AP as a new defense method, which outperforms all other methods in terms of both standard accuracy and robust accuracy. Specifically, compared to the second-best method, our method improves the robust accuracy by 1.2%, 2.5% and 1.4% on AutoAttack l\u221e, l2 and StAdv non-lp, respectively. The results demonstrate that our method achieves state-of-the-art results and exhibits generalization ability against unseen attacks."
        },
        {
            "heading": "4.4 ABLATION STUDIES",
            "text": "We conduct ablation studies to validate the effectiveness of factors within our method in defending against some standard attacks. Considering the robustness misestimation caused by obfuscated gradients of purifier model, we use BPDA (Athalye et al., 2018a) following the setting by Yang et al. (2019), which approximates the gradient of purifier model as 1 during backward pass. Additionally, we use EOT (Athalye et al., 2018b) following the setting by Lee & Kim (2023) with 20 iterations.\nResult analysis on CIFAR-10: Table 7 shows the performance of the defense model utilizing different transforms against FGSM, PGD and CW on CIFAR-10 with ResNet-18 classifier model. The numbers after the attack represent the number of steps. For each transform, our method significantly outperforms the method without AToP in terms of both standard accuracy and robust accuracy.\nFigure 4a shows the standard accuracy and robust accuracy of the purifier model trained with clean examples and adversarial examples, respectively. The results demonstrate that while utilizing clean examples can mitigate the impact of semantic loss induced by random transforms, thereby improving the standard accuracy. Moreover, utilizing adversarial examples can improve the robust accuracy. Specifically, AToP with clean examples achieves the best standard accuracy, while AToP with\nadversarial examples achieves the best robustness. Additionally, AP is a pre-processing technique that can combine with an adversarial trained classifier to further enhance robust accuracy, as shown in Figure 4c. However, the improvement comes at the cost of a reduction in standard accuracy.\nFor the more comprehensive evaluation, we follow the guidance of Lee & Kim (2023) to evaluate our method against PGD+EOT with l2 threat (\u03f5 = 1.0). Table 8 shows the performance of the defense model with different transforms against PGD-10 and PGD-20. The results show that our method significantly outperforms the method without AToP in terms of both standard accuracy and robust accuracy, demonstrating the effectiveness of our method in enhancing the performance of the purifier model in robust classification.\nResult analysis on ImageNette: Table 9 shows the performance of the defense model with different transforms against FGSM, PGD and CW on ImageNette with ResNet-34 classifier model. Figure 4b shows the standard accuracy and robust accuracy of the purifier model trained with clean examples and adversarial examples, respectively. The experimental observations on ImageNette are basically consistent with CIFAR-10, with our method significantly outperforming the method without AToP in terms of robust accuracy. On the other hand, we can observe that while employing more powerful random transforms within the same AToP setting, the perturbations will be gradually destructed, resulting in an increase in robustness; however, semantic information will also be lost, decreasing standard accuracy. This trade-off presents a limitation that prevents the continuous improvement of robust accuracy. Specifically, RT2 achieves optimal robustness while maintaining a satisfactory standard accuracy. Figure 3 shows the purified images obtained by inputting clean examples and adversarial examples into purifier model with various RTs. As previously mentioned, while RTs effectively destruct perturbations, they can also introduce the loss of semantic information.\nSo far, we have conducted extensive experiments across various attacks, classifiers, generator-based purifiers, datasets, and RT settings, which have consistently demonstrated the effectiveness of our method in enhancing robust classification.\nLimitations: Given that our method involves the fine-tuning of the generator model, the complexity of the generator model has a direct impact on the computational cost of AToP. Initially, we explore\nfine-tuning the diffusion model to improve its robustness. However, as purified images need to be fed into the classifier, generating 1000 images and fine-tuning the diffusion model requires 144 minutes. In contrast, with the same setting, the GAN-based purifier model only requires 62 seconds."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "In the paper, we propose a novel defense technique called AToP that utilizes adversarial training to optimize the purifier model. To demonstrate the robustness of our method, we conduct extensive experiments on CIFAR-10, CIFAR-100 and ImageNette. In defense of various attacks under multiple transforms, classifier and purifier architectures, our method consistently achieves state-of-the-art results and exhibits generalization ability against unseen attacks. Ablation studies further highlights that our method can significantly improve the performance of the purifier model on robust classification. Despite the large improvements achieved, our method has a major limitation: AToP requires training on the purifier model, and as the complexity of the purifier model increases, so does the training cost. However, exploring this direction presents an intriguing research opportunity for developing a more efficient defense method by combining AT and AP."
        }
    ],
    "year": 2024
}