{
    "abstractText": "With the success of Neural Radiance Field (NeRF) in 3D-aware portrait editing, a variety of works have achieved promising results regarding both quality and 3D consistency. However, these methods heavily rely on per-prompt optimization when handling natural language as editing instructions. Due to the lack of labeled human face 3D datasets and effective architectures, the area of humaninstructed 3D-aware editing for open-world portraits in an end-to-end manner remains under-explored. To solve this problem, we propose an end-to-end diffusionbased framework termed InstructPix2NeRF, which enables instructed 3D-aware portrait editing from a single open-world image with human instructions. At its core lies a conditional latent 3D diffusion process that lifts 2D editing to 3D space by learning the correlation between the paired images\u2019 difference and the instructions via triplet data. With the help of our proposed token position randomization strategy, we could even achieve multi-semantic editing through one single pass with the portrait identity well-preserved. Besides, we further propose an identity consistency module that directly modulates the extracted identity signals into our diffusion process, which increases the multi-view 3D identity consistency. Extensive experiments verify the effectiveness of our method and show its superiority against strong baselines quantitatively and qualitatively.",
    "authors": [],
    "id": "SP:af2109f936582b08c186cba067218f5d71e87694",
    "references": [
        {
            "authors": [
                "Rameen Abdal",
                "Hsin-Ying Lee",
                "Peihao Zhu",
                "Menglei Chai",
                "Aliaksandr Siarohin",
                "Peter Wonka",
                "Sergey Tulyakov"
            ],
            "title": "3davatargan: Bridging domains for personalized editable avatars",
            "venue": "In IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Shivangi Aneja",
                "Justus Thies",
                "Angela Dai",
                "Matthias Nie\u00dfner"
            ],
            "title": "Clipface: Text-guided editing of textured 3d morphable models",
            "venue": "In ACM SIGGRAPH 2023 Conference Proceedings, SIGGRAPH,",
            "year": 2023
        },
        {
            "authors": [
                "Fan Bao",
                "Chongxuan Li",
                "Yue Cao",
                "Jun Zhu"
            ],
            "title": "All are worth words: a vit backbone for score-based diffusion models",
            "venue": "CoRR, abs/2209.12152,",
            "year": 2022
        },
        {
            "authors": [
                "Fan Bao",
                "Chongxuan Li",
                "Jun Zhu",
                "Bo Zhang"
            ],
            "title": "Analytic-dpm: an analytic estimate of the optimal reverse variance in diffusion probabilistic models",
            "venue": "In The Tenth International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Fan Bao",
                "Shen Nie",
                "Kaiwen Xue",
                "Chongxuan Li",
                "Shi Pu",
                "Yaole Wang",
                "Gang Yue",
                "Yue Cao",
                "Hang Su",
                "Jun Zhu"
            ],
            "title": "One transformer fits all distributions in multi-modal diffusion at scale",
            "venue": "In International Conference on Machine Learning, ICML,",
            "year": 2023
        },
        {
            "authors": [
                "Tim Brooks",
                "Aleksander Holynski",
                "Alexei A Efros"
            ],
            "title": "Instructpix2pix: Learning to follow image editing instructions",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Shengqu Cai",
                "Anton Obukhov",
                "Dengxin Dai",
                "Luc Van Gool"
            ],
            "title": "Pix2nerf: Unsupervised conditional pi-gan for single image to neural radiance fields translation",
            "venue": "In IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Eric R. Chan",
                "Marco Monteiro",
                "Petr Kellnhofer",
                "Jiajun Wu",
                "Gordon Wetzstein"
            ],
            "title": "Pi-gan: Periodic implicit generative adversarial networks for 3d-aware image synthesis",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Eric R. Chan",
                "Connor Z. Lin",
                "Matthew A. Chan",
                "Koki Nagano",
                "Boxiao Pan",
                "Shalini De Mello",
                "Orazio Gallo",
                "Leonidas J. Guibas",
                "Jonathan Tremblay",
                "Sameh Khamis",
                "Tero Karras",
                "Gordon Wetzstein"
            ],
            "title": "Efficient geometry-aware 3d generative adversarial networks",
            "venue": "In IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Jiankang Deng",
                "Jia Guo",
                "Jing Yang",
                "Niannan Xue",
                "Irene Cotsia",
                "Stefanos Zafeiriou"
            ],
            "title": "Arcface: Additive angular margin loss for deep face recognition",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Prafulla Dhariwal",
                "Alexander Nichol"
            ],
            "title": "Diffusion models beat gans on image synthesis",
            "venue": "Advances in neural information processing systems,",
            "year": 2021
        },
        {
            "authors": [
                "Rinon Gal",
                "Or Patashnik",
                "Haggai Maron",
                "Amit H. Bermano",
                "Gal Chechik",
                "Daniel Cohen-Or"
            ],
            "title": "Stylegan-nada: Clip-guided domain adaptation of image generators",
            "venue": "ACM Trans. Graph.,",
            "year": 2022
        },
        {
            "authors": [
                "Xiao Han",
                "Yukang Cao",
                "Kai Han",
                "Xiatian Zhu",
                "Jiankang Deng",
                "Yi-Zhe Song",
                "Tao Xiang",
                "KwanYee K. Wong"
            ],
            "title": "Headsculpt: Crafting 3d head avatars with text",
            "venue": "arXiv preprint arXiv:2306.03038,",
            "year": 2023
        },
        {
            "authors": [
                "Ayaan Haque",
                "Matthew Tancik",
                "Alexei Efros",
                "Aleksander Holynski",
                "Angjoo Kanazawa"
            ],
            "title": "Instruct-nerf2nerf: Editing 3d scenes with instructions",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2023
        },
        {
            "authors": [
                "Jonathan Ho",
                "Tim Salimans"
            ],
            "title": "Classifier-free diffusion guidance",
            "venue": "NeurIPS",
            "year": 2021
        },
        {
            "authors": [
                "Jonathan Ho",
                "Ajay Jain",
                "Pieter Abbeel"
            ],
            "title": "Denoising diffusion probabilistic models",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Xun Huang",
                "Serge J. Belongie"
            ],
            "title": "Arbitrary style transfer in real-time with adaptive instance normalization",
            "venue": "In IEEE International Conference on Computer Vision, ICCV,",
            "year": 2017
        },
        {
            "authors": [
                "Yuming Jiang",
                "Ziqi Huang",
                "Xingang Pan",
                "Chen Change Loy",
                "Ziwei Liu"
            ],
            "title": "Talk-to-edit: Finegrained facial editing via dialog",
            "venue": "In IEEE/CVF International Conference on Computer Vision, ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "Tero Karras",
                "Timo Aila",
                "Samuli Laine",
                "Jaakko Lehtinen"
            ],
            "title": "Progressive growing of gans for improved quality, stability, and variation",
            "venue": "In International Conference on Learning Representations,",
            "year": 2018
        },
        {
            "authors": [
                "Tero Karras",
                "Samuli Laine",
                "Timo Aila"
            ],
            "title": "A style-based generator architecture for generative adversarial networks",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Yushi Lan",
                "Xuyi Meng",
                "Shuai Yang",
                "Chen Change Loy",
                "Bo Dai"
            ],
            "title": "Self-supervised geometryaware encoder for style-based 3d gan inversion",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Jianhui Li",
                "Jianmin Li",
                "Haoji Zhang",
                "Shilong Liu",
                "Zhengyi Wang",
                "Zihao Xiao",
                "Kaiwen Zheng",
                "Jun Zhu"
            ],
            "title": "Preim3d: 3d consistent precise image attribute editing from a single image",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2023
        },
        {
            "authors": [
                "Connor Z Lin",
                "David B Lindell",
                "Eric R Chan",
                "Gordon Wetzstein"
            ],
            "title": "3d gan inversion for controllable portrait image animation",
            "venue": "arXiv preprint arXiv:2203.13441,",
            "year": 2022
        },
        {
            "authors": [
                "Nan Liu",
                "Shuang Li",
                "Yilun Du",
                "Antonio Torralba",
                "Joshua B. Tenenbaum"
            ],
            "title": "Compositional visual generation with composable diffusion models",
            "venue": "In Computer Vision - ECCV 2022 - 17th European Conference,",
            "year": 2022
        },
        {
            "authors": [
                "Cheng Lu",
                "Yuhao Zhou",
                "Fan Bao",
                "Jianfei Chen",
                "Chongxuan Li",
                "Jun Zhu"
            ],
            "title": "Dpm-solver: A fast ODE solver for diffusion probabilistic model sampling in around",
            "venue": "NeurIPS,",
            "year": 2022
        },
        {
            "authors": [
                "Mohit Mendiratta",
                "Xingang Pan",
                "Mohamed Elgharib",
                "Kartik Teotia",
                "Mallikarjun B R",
                "Ayush Tewari",
                "Vladislav Golyanik",
                "Adam Kortylewski",
                "Christian Theobalt"
            ],
            "title": "Avatarstudio: Text-driven editing of 3d dynamic human head avatars, 2023",
            "year": 2023
        },
        {
            "authors": [
                "Ben Mildenhall",
                "Pratul P Srinivasan",
                "Matthew Tancik",
                "Jonathan T Barron",
                "Ravi Ramamoorthi",
                "Ren Ng"
            ],
            "title": "Nerf: Representing scenes as neural radiance fields for view synthesis",
            "venue": "Communications of the ACM,",
            "year": 2021
        },
        {
            "authors": [
                "Michael Niemeyer",
                "Andreas Geiger"
            ],
            "title": "GIRAFFE: representing scenes as compositional generative neural feature fields",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Or Patashnik",
                "Zongze Wu",
                "Eli Shechtman",
                "Daniel Cohen-Or",
                "Dani Lischinski"
            ],
            "title": "Styleclip: Textdriven manipulation of stylegan imagery",
            "venue": "In IEEE/CVF International Conference on Computer Vision, ICCV,",
            "year": 2021
        },
        {
            "authors": [
                "William Peebles",
                "Saining Xie"
            ],
            "title": "Scalable diffusion models with transformers",
            "venue": "arXiv preprint arXiv:2212.09748,",
            "year": 2022
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark",
                "Gretchen Krueger",
                "Ilya Sutskever"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "venue": "In Proceedings of the 38th International Conference on Machine Learning, ICML,",
            "year": 2021
        },
        {
            "authors": [
                "Daniel Roich",
                "Ron Mokady",
                "Amit H Bermano",
                "Daniel Cohen-Or"
            ],
            "title": "Pivotal tuning for latent-based editing of real images",
            "venue": "ACM Transactions on Graphics (TOG),",
            "year": 2022
        },
        {
            "authors": [
                "Robin Rombach",
                "Andreas Blattmann",
                "Dominik Lorenz",
                "Patrick Esser",
                "Bj\u00f6rn Ommer"
            ],
            "title": "Highresolution image synthesis with latent diffusion models",
            "venue": "In IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Olaf Ronneberger",
                "Philipp Fischer",
                "Thomas Brox"
            ],
            "title": "U-net: Convolutional networks for biomedical image segmentation",
            "venue": "In Medical Image Computing and Computer-Assisted Intervention MICCAI,",
            "year": 2015
        },
        {
            "authors": [
                "Yujun Shen",
                "Ceyuan Yang",
                "Xiaoou Tang",
                "Bolei Zhou"
            ],
            "title": "Interfacegan: Interpreting the disentangled face representation learned by gans",
            "venue": "IEEE transactions on pattern analysis and machine intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Jascha Sohl-Dickstein",
                "Eric A. Weiss",
                "Niru Maheswaranathan",
                "Surya Ganguli"
            ],
            "title": "Deep unsupervised learning using nonequilibrium thermodynamics",
            "venue": "In Proceedings of the 32nd International Conference on Machine Learning, ICML,",
            "year": 2015
        },
        {
            "authors": [
                "Jiaming Song",
                "Chenlin Meng",
                "Stefano Ermon"
            ],
            "title": "Denoising diffusion implicit models",
            "venue": "In 9th International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Yang Song",
                "Stefano Ermon"
            ],
            "title": "Generative modeling by estimating gradients of the data distribution",
            "venue": "In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Jianxin Sun",
                "Qiyao Deng",
                "Qi Li",
                "Muyi Sun",
                "Min Ren",
                "Zhenan Sun"
            ],
            "title": "Anyface: Free-style textto-face synthesis and manipulation",
            "venue": "In IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Jingxiang Sun",
                "Xuan Wang",
                "Yichun Shi",
                "Lizhen Wang",
                "Jue Wang",
                "Yebin Liu"
            ],
            "title": "IDE-3D: interactive disentangled editing for high-resolution 3d-aware portrait synthesis",
            "venue": "ACM Trans. Graph.,",
            "year": 2022
        },
        {
            "authors": [
                "Omer Tov",
                "Yuval Alaluf",
                "Yotam Nitzan",
                "Or Patashnik",
                "Daniel Cohen-Or"
            ],
            "title": "Designing an encoder for stylegan image manipulation",
            "venue": "ACM Trans. Graph.,",
            "year": 2021
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N. Gomez",
                "Lukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems",
            "year": 2017
        },
        {
            "authors": [
                "Tengfei Wang",
                "Bo Zhang",
                "Ting Zhang",
                "Shuyang Gu",
                "Jianmin Bao",
                "Tadas Baltrusaitis",
                "Jingjing Shen",
                "Dong Chen",
                "Fang Wen",
                "Qifeng Chen"
            ],
            "title": "Rodin: A generative model for sculpting 3d digital avatars using diffusion",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Zongze Wu",
                "Dani Lischinski",
                "Eli Shechtman"
            ],
            "title": "Stylespace analysis: Disentangled controls for stylegan image generation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Weihao Xia",
                "Yujiu Yang",
                "Jing-Hao Xue",
                "Baoyuan Wu"
            ],
            "title": "Tedigan: Text-guided diverse face image generation and manipulation",
            "venue": "In IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Jiaxin Xie",
                "Hao Ouyang",
                "Jingtan Piao",
                "Chenyang Lei",
                "Qifeng Chen"
            ],
            "title": "High-fidelity 3d gan inversion by pseudo-multi-view optimization",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Lvmin Zhang",
                "Maneesh Agrawala"
            ],
            "title": "Adding conditional control to text-to-image diffusion models",
            "venue": "arXiv preprint arXiv:2302.05543,",
            "year": 2023
        },
        {
            "authors": [
                "Kaiwen Zheng",
                "Cheng Lu",
                "Jianfei Chen",
                "Jun Zhu"
            ],
            "title": "Improved techniques for maximum likelihood estimation for diffusion odes",
            "venue": "In International Conference on Machine Learning, ICML,",
            "year": 2023
        },
        {
            "authors": [
                "Wu"
            ],
            "title": "2021), We use attribute altering (AA) to measure the change of the desired attribute",
            "year": 2021
        },
        {
            "authors": [
                "Abdal"
            ],
            "title": "2023), we computed the mean differences (Md) and standard deviation differences (Sd) metrics between the depth maps of the editing results and the depth map of the randomly sampled images in EG3D to measure 3D consistency",
            "year": 2023
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "While existing 3D portrait editing methods (Cai et al., 2022; Lin et al., 2022; Sun et al., 2022b; Li et al., 2023; Xie et al., 2023; Lan et al., 2023) explored the latent space manipulation of 3D GAN models and made significant progress, they only support preset attribute editing and cannot handle natural language. The explosion of language models has made it possible to enjoy the freedom and friendliness of the natural language interface. Recently, many excellent text-supported image editing methods have emerged, such as Talk-To-Edit (Jiang et al., 2021), StyleCLIP (Patashnik et al., 2021), TediGAN (Xia et al., 2021), AnyFace (Sun et al., 2022a) and InstructPix2Pix (Brooks et al., 2023). However, these methods are typically limited to the 2D domain and cannot directly produce 3D results. While it is possible to connect a 2D text-supported editing model to a 3D inversion model for text-supported 3D-aware editing, this will result in extra loss of identity information as the original face is invisible in the second stage. Thus, an end-to-end model is more desirable for better efficiency and performance.\nRodin (Wang et al., 2023) and ClipFace (Aneja et al., 2023) explored end-to-end text-guided 3Daware face editing. Rodin trains a conditional diffusion model in the roll-out tri-plane feature space with 100K 3D avatars generated by a synthetic engine. Rodin achieves text-guided 3D-aware manipulation by conditioning the diffusion process with the manipulated embedding, which consists of the CLIP (Radford et al., 2021) image embedding and a direction in the CLIP text embedding. However, since Rodin can only generate avatar faces, it cannot handle real-world face editing. ClipFace learns a texture mapper and an expression mapper with CLIP loss and uses the generated UV map and deformed 3D mesh to achieve text-guided manipulation in the synthetic domain. However, ClipFace cannot handle real-world face editing and is time-consuming and inconvenient because a separate encoder needs to be trained for each text prompt.\nHuman instructions are better for expressing editing intent than descriptive prompts. However, focusing on noun phrases, existing text-guided 3D-aware face editing methods are challenging to\nunderstand verbs in instructions. For example, descriptive-prompt-driven editing methods usually treat \u201cremove the eyeglasses\u201d as putting on the eyeglasses, as shown in Appendix A.2.1. Therefore, end-to-end 3D-aware portrait editing from an input image with human instructions is a critical and fascinating task, which aims to achieve precise 3D-consistent editing with a user-friendly interface. To our knowledge, we are the first to explore this area. We analyze that it is critical to design an efficient framework incorporating human instructions with 3D-aware portrait editing. In addition, due to the lack of multi-view supervision, it is challenging to maintain the consistency of identity and editing effects when performing 3D-aware editing from a single image, especially multi-semantic editing.\nIn this work, we address these issues by proposing a novel framework termed InstructPix2NeRF, which enables precise 3D-aware portrait editing from a single image guided by human instructions. We prepare a triplet dataset of each sample consisting of a 2D original face, a 2D edited face, and a human instruction semantically representing a single change from the original face to the edited face. Training on a large number of images and single human instructions, our model enables instructed 3D-aware editing with many single instructions even multiple instructions, rather than training the model for each human instruction. Although trained on a single-instruction dataset, InstructPix2NeRF enables multi-instruction editing during inference. InstructPix2NeRF can perform instructed 3D-aware face editing thanks to the three key ingredients below.\nFirstly, we design a novel end-to-end framework combining diffusion and NeRF-based generators. The state-of-the-art architecture for text-guided image editing is typically based on diffusion models (Rombach et al., 2022; Zhang & Agrawala, 2023; Brooks et al., 2023). At the same time, 3D portrait modeling commonly relies on NeRF-based GANs (Chan et al., 2022; Sun et al., 2022b; Niemeyer & Geiger, 2021; Chan et al., 2021). We efficiently combine these two aspects to design an effective cross-modal editing model. Specifically, we use the inversion encoder PREIM3D (Li et al., 2023) to obtain the w+ latent codes of the 2D original and edited faces, then train a transformer-based diffusion model in the 3D latent space. The CLIP\u2019s text embedding of instructions is injected into the diffusion process via a cross-attention block after the self-attention block. Latent codes with biconditional sampling of text and images are decoded to geometry and multiple views with a single pass through the NeRF-based generator.\nSecondly, we propose a Token Position Randomization (TPR) training strategy to handle multiple editing requirements via one single pass. By TPR, instruction tokens are put in a random position of the token sequence, enabling the model to fulfill multiple editing requirements simultaneously while preserving the facial identity to a large extent.\nThirdly, we propose an identity consistency module that consists of an identity modulation part and an identity regularization part. We replace layer-normalization layers in the transformer block\nwith adaptive layer norm (adaLN), which modulates the identity signal. The identity regularization loss is calculated between the original face image and a typical face image generated by a one-step prediction of latent code when the diffusion timestep is less than the threshold.\nCombining the three key components together, InstructPix2NeRF enables instructed and 3D consistent portrait editing from a single image. With one 15-step DDIM(Song et al., 2021; Lu et al., 2022) sampling, our model can output a portrait and geometry with attributes or style guided by instructions in a few seconds. Figure 1 shows the editing results produced by our method. We recommend watching our video containing a live interactive instruction editing demonstration. To facilitate progress in the field, we will be completely open-sourcing the model, training code, and the data we have curated. We expect our method to become a strong baseline for future works towards instructed 3D-aware face editing."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "NeRF-based 3D generation and manipulation. Neural Radiance Field (NeRF) (Mildenhall et al., 2021) has significantly impacted 3D modeling. Early NeRF models struggle to generate diverse scenes since the models are trained on a lot of pose images for every scene. Recent NeRF methods, such as GIRAFFE(Niemeyer & Geiger, 2021), EG3D(Chan et al., 2022), and IDE-3D(Sun et al., 2022b), integrate NeRF into the GAN framework to generate class-specific diverse scenes. These works have paved the way for 3D-aware objects editing methods like IDE-3D (Sun et al., 2022b) PREIM3D (Li et al., 2023), HFGI3D (Xie et al., 2023), and E3DGE (Lan et al., 2023) that perform semantic manipulation in the latent space of NeRF-based GANs in specific domain, such as human faces, cars, and cats. Despite the promising results of these methods, they cannot handle natural language.\nDiffusion models. The diffusion probabilistic models (DPMs) (Sohl-Dickstein et al., 2015; Song & Ermon, 2019; Ho et al., 2020) can generate high-quality data from Gaussian noise through a forward noise addition process and a learnable reverse denoising process. By training a noise predictor using UNet (Ronneberger et al., 2015; Dhariwal & Nichol, 2021; Song & Ermon, 2019; Bao et al., 2022b) or transformers (Vaswani et al., 2017; Bao et al., 2022a; Peebles & Xie, 2022) backbone, the diffusion model can stably learn the probability distribution on ultra-large datasets and hold extensive applications such as image generation (Rombach et al., 2022), multi-model data generation (Bao et al., 2023), image editing (Zhang & Agrawala, 2023; Brooks et al., 2023), and likelihood estimation(Zheng et al., 2023). Initially, the diffusion model had difficulty generating high-resolution images in pixel space. Recently, the Latent Diffusion (Rombach et al., 2022), which combines the VAE and the diffusion model, significantly improves generation efficiency and image resolution. Inspired by latent diffusion (Rombach et al., 2022), we combine the diffusion model with NeRF-base GAN to efficiently implement latent 3D diffusion.\nText-guided editing. The previous approach for text-guided editing (Jiang et al., 2021) usually involves training a text encoder to map text input or human instructions to a linear or non-linear editing space, which may not handle out-of-domain text and instructions well. Recently, The pretrained CLIP (Radford et al., 2021) model contains vast vision and language prior knowledge, significantly accelerating the development of vision-language tasks. StyleCLIP (Patashnik et al., 2021) calculates the normalized difference between CLIP text embeddings of the target attribute and the\nneutral class as the target direction \u2206t, which is then applied to fit a style space manipulation direction \u2206s. InstructPix2Pix combines the abilities of GPT (Brown et al., 2020) and Stable Diffusion (Rombach et al., 2022) to generate a multi-modal dataset and fine-tunes Stable Diffusion to achieve instructed diverse editing. However, these methods are focused on editing 2D images and do not enable 3D-aware editing. Recently, Rodin (Wang et al., 2023), ClipFace (Aneja et al., 2023), and IDE3D-NADA (Sun et al., 2022b; Gal et al., 2022) explored text-guided 3D-aware editing. However, Rodin and ClipFace can only be applied to the synthesis face, and ClipFace and IDE3D-NADA require optimization for each text prompt. Meanwhile, Instruct-NeRF2NeRF (Haque et al., 2023), AvatarStudio (Mendiratta et al., 2023), and HeadSculpt (Han et al., 2023) have achieved success in optimizing text-driven single-3D-scene editing. However, these methods require a 3D scene rather than a single image, and they take tens of minutes for each scene.\nAs shown in Table 1, IDE-3D, E3DGE, and PREIM3D cannot handle natural language, ClipFace and IDE3D-NADA rely on per-prompt optimization, Rodin and ClipFace cannot be applied to realworld faces."
        },
        {
            "heading": "3 DATA PREPARATION",
            "text": "Inspired by InstructPix2Pix (Brooks et al., 2023), we prepared a multimodal instruction-following triplet dataset, where triplet data consists of an original face, an edited face, and a human instruction representing a single change from the original to the edited. Specifically, given a face, we use pretrained 2D editing models to produce a 2D edited face and use the large language model ChatGPT (Brown et al., 2020) to generate the corresponding editing instructions.\nPaired image generation. We leverage two off-the-shelf 2D image editing methods, e4e (Tov et al., 2021) and InstructPix2Pix (Brooks et al., 2023), to generate paired images on FFHQ (Karras et al., 2019). We use e4e, the widely used state-of-the-art face attribute editing method, to generate 22K pairs of images with 44 attributes editing. As for InstructPix2Pix, we have selected 150 portraitrelated instructions from its instruction set using keywords. The faces edited with these instructions were filtered by a face identification model (Deng et al., 2021) to obtain 18K paired images.\nInstruction generation. In-context learning in large language models (LMs) demonstrates the power that rivals some supervised learning. For the paired images generated by e4e, we provide ChatGPT with some example data consisting of the paired attribute labels and corresponding handwritten transfer instructions. We tell it that the instructions represent a single change between attributes. After that, we asked ChatGPT to generate 20-30 transfer instructions with the provided attributes of the pairs. For the paired image generated by InstructPix2Pix, since there are already human instructions, we asked ChatGPT to generate 20-30 instructions representing the same semantic meaning on this basis.\nAssigning human instructions to 40K paired images, we obtained a final triplet dataset containing 640K examples, each consisting of a single semantic instruction and paired image. See Appendix for more detailed data preparation."
        },
        {
            "heading": "4 METHOD",
            "text": "Given input image and human instructions, our goal is to generate multi-view images and geometry, which behave as intended by human instructions and can keep other attributes and identities unchanged. With the NeRF-based generator and inversion encoder, our goal can be translated into generating latent code w that represents the edited face image. The latent code w will be given to the NeRF-based generator to produce multi-view images conditioned on camera pose. Figure 2 illustrates the whole architecture of our method. We will introduce the key components of our pipeline in the following subsections."
        },
        {
            "heading": "4.1 CONDITIONAL LATENT 3D DIFFUSION",
            "text": "Our method is based on the NeRF-based generator G and inversion encoder E. The NeRF-based generator, such as EG3D (Chan et al., 2022), can generate multi-view images from a Gaussian noise z \u2208 Z \u2286 R512 conditioned on camera parameters c. The noise z is mapped to the intermediate latent\ncode w = f(z) \u2208 W \u2286 Rk\u2217512, which is used to produce tri-plane features through the convolutional network. A small MLP decoder is used to interpret the features retrieved from 3D positions as color and density, rendered into multi-view images conditioned on camera pose c, described as X = G(w, c).\nThe NeRF-based inversion encoder E (Li et al., 2023; Lan et al., 2023) learns the features of a large number of images with different poses to reconstruct 3D representation from a single image. The encoder maps an input image X to the latent code w, which can be used to produce a novel view X \u2032 of the input image : w = E(X),\nX \u2032 = G(E(X), c).\n(1)\nwhere c is the camera pose.\nPerforming the diffusion process in the latent space combines the strength of other generators and accelerates training, especially in the 3D generation area. Here, there are several candidate latent spaces, such as Z , W , and W+. The W space consists of k repetitive 512-dimensional latent vectors fed into convolutional layers of different resolutions, but the W+ space consists of k distinct latent vectors. It\u2019s demonstrated that k distinct latent vectors can increase the representation capacity of the generator than k identical latent vectors (Shen et al., 2020; Li et al., 2023). Although EG3D is trained on the real-world face dataset FFHQ, our simple experiments in Appendix A.2.2 demonstrate that it is possible to generate some out-of-domain 3D faces, such as bronze statues, and cartoons, using fixed EG3D fed W+ latent code. Moreover, W+ space is considered more disentangled than Z and W spaces (Tov et al., 2021; Patashnik et al., 2021). Therefore, we choose W+ space to perform the diffusion process in our work.\nDiffusion model architecture. To obtain the latent code w guided by human instructions, we employ a diffusion model to learn the correlation between the paired images and the instructions. Notably, transformers (Vaswani et al., 2017) show a promising ability to capture complex interactions and dependencies between various modalities (Bao et al., 2023). In this paper, our diffusion backbone is Diffusion Transformer (DiT) (Peebles & Xie, 2022). We have made the following modifications to DiT: (i) add an input header that enables paired latent codes diffusion, (ii) add a cross-attention block after the self-attention block and introduce CLIP text embedding here, and (iii) add an identity embedding module that is plugged into the norm layer of transformer block using adaptive layer norm described in section 4.3.\nGiven an input image Xo and a human instruction T , our goal can be formulated as learning the conditional distribution p(w|Xo, T ). The inversion encoder E is applied to obtain the latent code wo = E(Xo) \u2208 R14\u2217512, we = E(Xe) \u2208 R14\u2217512 for the original image Xo and corresponding 2D edited image Xe respectively. We add noise to the latent code we with a fixed schedule, producing a noisy version wet at step t, t \u2208 T . Our model is trained as a noise predictor \u03f5\u03b8 to predict the added\nnoise conditioned on image and text. The image conditioning cI consists of two parts: concatenation of wo and we, and identity modulation. The text conditioning cT is realized by adding a multi-head cross-attention block with the CLIP text embedding following the multi-head self-attention block. To adapt to the model, the latent code would be reshaped. Take 512*512 resolution for example, after padding 2 zero vectors for w \u2208 R14\u2217512, we reshape the latent code to the shape 512 \u2217 4 \u2217 4. The conditional latent diffusion objective is:\nLdiff = Ewe,cI ,cT ,\u03f5\u223cN (0,1),t[\u2225\u03f5\u2212 \u03f5\u03b8(wet, t, cI , cT )\u2225 2 2], (2)\nwhere cI is the image conditioning, cT is the text conditioning."
        },
        {
            "heading": "4.2 TOKEN POSITION RANDOMIZATION",
            "text": "In our preliminary experiments, we have observed that when editing with multiple instructions, the more forward-positioned instructions are easier to show up in the edited image. We analyze this issue and attribute it to the training data being single instruction edited. In natural language processing, text conditioning requires that text be tokenized into a sequence of tokens, which is 77 in length in this paper. Only the first few sequence positions are usually non-zero when we train with single instruction data. It is intuitive to think that the cross-attention mechanism might pay more attention to the head of multiple instructions.\nTo achieve better editing results for the multiple instructions, we propose token position randomization, randomly setting the starting position of the text instruction tokens. This strategy makes the model more balanced in its attention to the components of multiple instructions. As a result, multisemantic editing can be performed in a way that fulfills all the editing requirements while preserving identity well. In our paper, we randomly set the starting position in [0, 30], with the last non-zero token position being less than 77. Ablation studies in Figure 5, and Table 3 show the effectiveness of the token position randomization strategy."
        },
        {
            "heading": "4.3 IDENTITY CONSISTENCY MODULE",
            "text": "For precise face editing, preserving the input subject\u2019s identity is challenging, especially in 3D space. Latent code can be considered as compression of an image that loses some information, including identity. To tackle this, We impose an identity compensation module that directly modulates the extracted identity information into the diffusion process. A two-layer MLP network maps the identity features extracted from the original face into the same dimension as the diffusion timestep embedding. We regress dimensionwise scale and shift parameters \u03b3 and \u03b2 from the sum of the embeddings of diffusion timestep t, and the embeddings of identity feature extracted from the portrait using a face identification model (Deng et al., 2021).\nTo improve 3D identity consistency further, we explicitly encourage face identity consistency at different poses by adding identity regularization loss between the 2D edited image and the rendered image with yaw = 0, and pitch = 0.\nLID = 1\u2212 \u27e8F (Xe), F (G(w\u0303e0, c0))\u27e9, (3)\nwhere F is the face identification model which extracts the feature of the face, Xe is the 2D edited face, w\u0303e0 is the one-step prediction of latent code, c0 is the camera pose with yaw=0, pitch=0.\nWhen the diffusion timestep is large, it will lead to a poor w for one-step prediction. Thus, this loss is calculated only for samples with timestep t less than the timestep threshold tth.\nThe total loss is: L = Ldiff + \u03bbidLID, (4)\nwhere \u03bbid is the weight of LID. \u03bbid is set to 0.1 in our experiments."
        },
        {
            "heading": "4.4 IMAGE AND TEXT CONDITIONING",
            "text": "Classifier-free guidance trains an unconditional denoising diffusion model together with the conditional model instead of training a separate classifier (Ho & Salimans, 2021). Liu et al. (2022) show that composable conditional diffusion models can generate images containing all concepts conditioned on a set of concepts by composing score estimates. Following (Ho & Salimans, 2021; Liu\net al., 2022; Brooks et al., 2023), we train a single network to parameterize the image-text conditional, the only-image conditional, and the unconditional model. We train the unconditional model simply by setting cI = \u2205, cT = \u2205 with probability p1, similarly, only setting cT = \u2205 with probability p2 for the only-image-conditional model \u03f5\u03b8(wot, cI , \u2205). In our paper, we set p1 = 0.05, p2 = 0.05 as hyperparameters.\nIn the inference phase, we add a little bit of noise to the latent of the input image (usually 15 steps) to obtain wot and then use our model to perform conditional denoising. The model predicts three score estimates, the image-text conditional \u03f5\u03b8(wot, cI , cT ), the only-image conditional \u03f5\u03b8(wot, cI , \u2205), and the unconditional \u03f5\u03b8(wot, \u2205, \u2205). cT = \u2205 indicates that the text takes an empty character. cI = \u2205 means that the concatenation wo takes zero and identity modulation takes zero. Image and text conditioning sampling can be performed as follows:\n\u03f5\u0303\u03b8(wot, cI , cT ) =\u03f5\u03b8(wot, \u2205, \u2205) + sI(\u03f5\u03b8(wot, cI , \u2205)\u2212 \u03f5\u03b8(wot, \u2205, \u2205)) + sT (\u03f5\u03b8(wot, cI , cT )\u2212 \u03f5\u03b8(wot, cI , \u2205))\n(5)\nwhere sI and sT are the guidance scales for alignment with the input image and the text instruction, respectively."
        },
        {
            "heading": "5 EXPERIMENTS",
            "text": "Given that the field of instructed 3D-aware face editing is under-explored, we have systematically designed and developed a series of models that serve as our baselines. We compare our method with three baselines: Talk-To-Edit (Jiang et al., 2021) followed by PREIM3D (Li et al., 2023), InstructPix2Pix (Brooks et al., 2023) followed by PREIM3D, and a method similar to the img2img mode of Stable Diffusion (Rombach et al., 2022). More implementation details are provided in the Appendix A.1.\n*: Same as Table 2.\nWe use the official pre-trained models and code for Talk-To-Edit, InstructPix2Pix, and PREIM3D in the comparison experiments. The metrics are calculated on the first 300 images from CelebA-HQ. To evaluate the 3D capability, we uniformly rendered 4 views from yaw angles between [\u221230\u25e6, 30\u25e6] and pitch angles between [\u221220\u25e6, 20\u25e6] for an input image. We evaluate our method using ID, CLIP, AA, AD, Md, and Sd, defined in the Appendix A.1.3.\nQualitative evaluation. We present examples of the instructed editing results in Figure 3. Due to using a word bank, Talk-To-Edit does not recognize part of the instructions and cannot handle multiple instructions. Img2img does not disentangle the attributes and struggles in aligning text editing requirements with images, leading to changes in some other attributes. For example, the fourth row in Figure 3 shows the additional semantics of becoming younger. InstructPix2Pix usually leads to variations in hue and identity consistency, such as the skin tones in the third row of Figure 3. Our method achieves better text instruction correspondence, disentanglement, and 3D consistency than baselines. More editing results are provided in Appendix A.3\nQuantitative evaluation. We chose three typical facial attributes, bangs, eyeglasses, and smile, to evaluate our method and baseline quantitatively. To be fair, our instruction test set consists of selected instructions from Talk-To-Edit, InstructPix2Pix, and InstructPix2NeRF, where each model contributes 15 instructions, such as \u2019Make her happy instead.\u2019 and \u2019The eyeglasses could be more obvious.\u2019 The metrics on the multiple instructions are measured with six combinations of the above three attributes in different orders.\nAs shown in Table 2, 3, our method performs better than the baselines. Since img2img doesn\u2019t disentangle the editing requirements and is prone to cause changes on other attributes, it has a\nlow ID score and a high AA score. Talk-To-Edit sometimes responds only marginally to editing instructions, its CLIP and AA scores are significantly worse than other methods. InstructPix2Pix scores well, but still below our method. See Appendix A.2.3 for more evaluation of attribute editing. We conducted a user study and as shown in Table 6, our method outperforms the baselines."
        },
        {
            "heading": "Put eyeglasses on him",
            "text": "Ablation of token position randomization To verify the effectiveness of token position randomization training, two models were trained, one using the token position randomization training strategy and the other not. Table 3 and Figure 8 show that the model with TPR performs better than the other model when editing with multiple instructions.\nTo further compare the performance of the two models for multiple instructions of different lengths, we performed multiple instructions editing of lengths 1-4, involving bangs, eyeglasses, smile, age. The average improvement in AA and CLIP scores measures the effect. As shown in Figure 5, the two models give comparable AA and CLIP scores when using single instruction editing. Still, as the instruction length increases, our model shows a better correspondence for text instruction.\nAblation of identity consistency module Injecting learned identity information into the diffusion process, the identity modulation compensates for losing information in latent space. The identity regularization loss explicitly guides the model to preserve the identity of the input subject. As shown in Table 4 and Figure 5, the model with the identity consistency module significantly improves the identity consistency scores and visuals."
        },
        {
            "heading": "6 CONCLUSIONS",
            "text": "In this paper, to solve the instructed 3D-aware face editing, we propose a novel conditional latent 3D diffusion model, enabling instructed 3D-aware precise portrait editing interactively. To support the editing of multiple instructions editing that was not available in previous methods, we propose the token position randomization training strategy. Besides, we propose an identity consistency module consisting of identity modulation and identity loss to improve 3D identity consistency. We expect our method to become a strong baseline for future works towards instructed 3D-aware face editing. Our method can be used for interactive 3D applications such as virtual reality and metaverse.\nLimitations. One limitation of our work is that some semantically identical instructions may produce slight errors. For example, although \u201dturn the hair color to pink\u201d and \u201dchange her hair into pink\u201d both want to change the hair color to pink, they will obtain different pink. Moreover, the same instruction will also have color differences for different people. We have some losses for details such as eye shape and eyelashes. These are issues that need to be addressed in this area in the future."
        },
        {
            "heading": "A APPENDIX",
            "text": "In the Appendix, we first provide implementation details, including the model parameters and training dataset. We follow with additional experiments and visual results. We highly recommend watching our video, which contains a live demonstration of interactive instructed editing.\nA.1 IMPLEMENTATION DETAILS"
        },
        {
            "heading": "A.1.1 EXPERIMENT SETTING",
            "text": "We train our conditional diffusion model on the dataset we prepared from FFHQ (Karras et al., 2019) and use CelebA-HQ (Karras et al., 2018) for evaluation. In our experiments, we use pretrained EG3D model (Chan et al., 2022), pretrained PREIM3D model (Li et al., 2023), and pretrained \u2019ViTH-14\u2019 CLIP model (Radford et al., 2021). The Diffusion Transformer is modified from the backbone of \u2019DiT B/1\u2019, adding an input header, a text condition module with the cross-attention mechanism, and an identity modulation module. The number of parameters in the model is 1.15 Billion. We set tth = 600, \u03bbid = 0.1 and trained the model on a 4-card NVIDIA GeForce RTX 3090 for 6 days with a batch size of 20 on a single card."
        },
        {
            "heading": "A.1.2 TRAINING DATASET",
            "text": "The triplet data encourages the model to learn the correlation between the change from pairs of images and the instruction. Since the perceptual changes between the paired images are well-defined, we can easily generate corresponding instructions. Figure 6 shows the data preparation. For the paired images generated by e4e, we provide ChatGPT with some example data consisting of the paired attribute labels and a few corresponding handwritten transfer instructions. We guide ChatGPT by following these steps.\na. You are now an excellent data generation assistant. b. The rule for generating data is that I give you an input label and an output label, and you help me generate an instruction that represents the change from the input label to the output label. c. These are some examples. example 1\ninput label: eyeglasses output label: without eyeglasses instruction: remove the eyeglasses.\nexample 2 input label: no beard output label: beard man instruction: give him some beard example 3 input label: an old lady output label: a young girl instruction: make her look more youthful. example 4 input label: brown hair output label: blond hair instruction: turn the hair to blond d. I will give you an input label and an output label to generate 10 institutions based on the rules and examples above.\ninput label: a small nose output label: a big nose\nFor the paired image generated by InstructPix2Pix, we guide ChatGPT like this, \u201dYou are an excellent instruction generation assistant. I give you a face editing instruction, please generate 30 instructions that are semantically identical to this one.\u201d.\nOur dataset has about 40K paired images and the corresponding 3K instructions, with e4e generating 22K paired images and InstructPix2Pix generating 18K paired images. For the images generated by e4e, 500 images share about 30 instructions. For the images generated by InstructPix2Pix, 120\nimages share about 10 instructions. The resolution of the image is 512*512. All instructions are single editing requirements. We train our model with the dataset cropped as EG3D (Chan et al., 2022) and PREIM3D (Li et al., 2023). The dataset examples are provided in Figure 7."
        },
        {
            "heading": "A.1.3 EVALUATION",
            "text": "Test data. The image test dataset is the first 300 images from CelebA-HQ (Karras et al., 2018). The instruction test set used in the comparison experiments consists of selected instructions from Talk-To-Edit, InstructPix2Pix, and InstructPix2NeRF, where each model contributes 15 instructions, such as \u2019Make her happy instead.\u2019 and \u2019The eyeglasses could be more obvious.\u2019. The multiple instruction comprises 3 single instructions that are concated together. We show the test instructions in Table 7, 8, and 9.\nBaselines. Talk-To-Edit and InstructPix2Pix are the most popular methods enabling instructed 2D editing. We perform 2D portrait editing guided by the instructions and then get the 3D-aware edited images using the state-of-the-art 3D inversion encoder PREIM3D. For another baseline img2img, we only trained a text-to-3D diffusion model using the instructions and edited images in the prepared dataset. Similar to the img2img mode of Stable Diffusion, we apply denoising sampling to the latent code of the input image with a small amount of noise added, conditional on the instruction.\nMetrics. The subject\u2019s multi-view identity consistency (ID) is measured by the average ArcFace feature similarity score (Deng et al., 2021) between the sampled images and the input image. We evaluate the precision of instructed editing with the directional CLIP similarity (Gal et al., 2022), which is calculated by the cosine similarity of the CLIP-space direction between the input image and multi-view edited images and the CLIP-space direction between the input prompt and edited prompt. Here, our input prompt is composed of attribute labels with a probability greater than 0.9 in an off-the-shelf multi-label classifier based on ResNet50 (Huang & Belongie, 2017), and the edited\nAdd bangs to her hairstyle.Trim his hair to have bangs Add some bangs\nchange her hair to blondmake her have blonde hair turn the hair blonde\nMake her wear eyeglasses.add a pair of eyeglasses Add glasses to her face.\nprompt is appended by the input prompt with the attribute label you want to edit. Following (Li et al., 2023; Wu et al., 2021), We use attribute altering (AA) to measure the change of the desired attribute and use attribute dependency (AD) to measure the change in other attributes when modifying a specific attribute. Attribute altering (AA) is the attribute logit change \u2206lt normalized by the standard deviation \u03c3(l) when detecting attribute t by the classifier, and attribute dependency (AD) is the other attribute logit change. Following Abdal et al. (2023), we computed the mean differences (Md) and standard deviation differences (Sd) metrics between the depth maps of the editing results and the depth map of the randomly sampled images in EG3D to measure 3D consistency."
        },
        {
            "heading": "A.2 MORE EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "A.2.1 PROMPT-DRIVEN EDITING",
            "text": "Text-image pairing contributes significantly to text-to-image and image captioning. Focusing on adjectives and nouns in the text, prompt-driven editing methods perform better on descriptive prompts than instructed text. For example, StyleClip (Patashnik et al., 2021), the state-of-the-art text-guided 2D face editing method, usually misinterprets some verbs, as shown in Figure 9."
        },
        {
            "heading": "A.2.2 EXPLORING THE W+ SPACE OF EG3D",
            "text": "EG3D, the state-of-the-art NeRF-based generator in the face domain, is trained on a real-world face dataset. The original EG3D can generate high-resolution multi-view-consistent images and highquality 3D geometry from the Gaussian noise. Although the original EG3D can only generate faces with a similar distribution to the FFHQ data, we get a wider range of reasonable face images by navigating in the W+ space. The intermediate latent space of EG3D is a manifold of k identical 512-dimensional latent vectors. Using k distinct latent vectors rather than identical ones, we can greatly extend the generation capability of the fixed EG3D. Figure 17 illustrates our exploration of W+ space in EG3D."
        },
        {
            "heading": "A.2.3 MORE ATTRIBUTE EDITING",
            "text": "We evaluated the instructed editing performance of more attributes, as shown in Table 5. Our method outperforms InstructPix2Pix and img2img in AA, AD, and ID metrics."
        },
        {
            "heading": "A.2.4 USER STUDY",
            "text": "To perceptually evaluate the instructed 3D-aware editing performance, we conduct a user study in Table 6. We collected 1,440 votes from 30 volunteers, who evaluated the text instruction correspondence and multi-view identity consistency of editing results. Each volunteer is given a source image, our editing result, and baseline editing, and asked to choose the better one, as shown in Figure 18 The user study shows our method outperforms the baselines."
        },
        {
            "heading": "A.2.5 W+ OPTIMIZATION ABLATION",
            "text": "In 2D image editing, although optimization-based inversion may be more time-consuming than encoder-based inversion, it produces better identity consistency. We replace the inversion encoder with latent optimization in our pipeline to see if it improves identity consistency. Since each optimization takes several minutes, we cannot perform optimizations during training, but only at inference time.\nIn our experiments, we considered two configurations: Direct W+ optimization and PTI Roich et al. (2022) optimization. Direct W+ optimization involves optimizing the W+ vector while keeping the generator fixed. PTI (Pivotal Tuning Inversion) technique fine-tunes the generator based on the initial value provided by direct optimization. We conducted 500 steps of optimization on the W+ vector, and PTI added 100 steps of fine-tuning the generator.\nThe results of these experiments are presented in Figure 10, where we compare the outcomes of direct W+ optimization, PTI, and the encoder-based method. The results show that directly replacing the encoder with an optimization method during inference will lead to a severe decrease in both editing effect and identity consistency. We attribute this issue to the deviation between the model and data distribution. The model learns a conditional distribution within the encoder\u2019s inversion space during training. When the encoder is replaced by an optimization method during inference, the data distribution used for inference mismatches the learned model distribution. This mismatch results in greater identity drift and undesirable editing outcomes.\nWhile conducting W+ optimization during training (much larger compute) could potentially address the distribution deviation problem, it may introduce artifacts in novel views, as pointed out by PREIM3D. This is due to optimization being performed on a single image during training. In summary, while direct optimization of the W+ vector is an interesting concept, our experiments suggest that it may not necessarily lead to improved identity preservation and editing results compared to the encoder-based approach."
        },
        {
            "heading": "A.2.6 EFFECTS OF THE BACKGROUND",
            "text": "To verify the effect of background on the editing results, we edited images of the same person in different scenes. We show the results in Figure 11, where the first two rows are the same subject, the middle two rows are the same subject and the last two rows are the same subject. The results show that the background has no obvious impact on the editing results. However, note that when editing colors, particularly when the color being edited is close to the background color, there can be some blending between the foreground and background elements."
        },
        {
            "heading": "A.2.7 THE TRIPLET DATA MECHANISM ABLATION",
            "text": "The triplet data mechanism plays a crucial role in achieving accurate and disentangled image editing. To provide a more thorough understanding of its importance, we conducted an ablation study. Our comparison involves the img2img model, which can be considered as using a text-image pairing data mechanism, in contrast to our method which utilizes the triplet data mechanism. Unlike InstructPix2NeRF, img2img has no paired images, but the rest of the network structure is the same.\nThe results of the ablation study, as shown in 3 and Table 2, 3, and 5, demonstrate that the triplet data mechanism significantly contributes to the quality of editing in terms of identity preservation (ID) and attribute dependency (AD) when attribute altering (AA) is close to equal.\nOur method consistently outperforms img2img in preserving identity across various attributes, as indicated by the higher ID scores. Moreover, the triplet data mechanism helps reduce attribute dependency, ensuring that changes to one attribute do not excessively affect others. These results highlight that the triplet data mechanism encourages the model to learn the correlations between changes in pairs of images and the corresponding instructions, leading to more precise and disentangled editing. In conclusion, the triplet data mechanism is essential for achieving high-quality image editing results."
        },
        {
            "heading": "A.3 MORE VISUAL RESULTS",
            "text": "We provide a large number of instructed editing results produced by InstructPix2NeRF in Figure 12, 13, 14, 15, and 16\nDiversity and generalization. We realize the importance of diversity in the evaluation of image editing methods and strive to provide a comprehensive evaluation. As described in Appendix A.1.1 Experimental Settings, our model is trained on the FFHQ dataset, which consists of 70,000 highquality faces featuring vast diversity in terms of age, ethnicity, and background, while also exhibiting comprehensive representation of accessories such as glasses, sunglasses, and hats. This diverse training data ensures that our model can generalize to various races and attributes. As shown in Figures 14 and 15, our models are capable of handling different races, hairstyles, ages, and other attributes. In Figure 16, we demonstrate this capability using the results of editing scenarios featuring characters from this year\u2019s movie \u201dMission: Impossible \u2013 Dead Reckoning Part One.\u201d\nChange the hair color to blonde Change the hair color to blonde\nMake him smile Make him smile\nGive him bushy eyebrows Give him bushy eyebrows\nMake her a cartoon character Make her a cartoon character\nTurn his hair color to pink Turn his hair color to pink\nChange the hair color to blonde Change the hair color to blonde\nFigure 11: The effects of background.\nTurn her into a vampire Turn her into a vampire\nGive it a zombie makeover Give it a zombie makeover\nMake it look like a sketch by Edward\nHopper\nMake it look like a sketch by Edward\nHopper\nMake him a cartoon character Make her a cartoon character\nTurn the painting into a bronze statue Turn the painting into a bronze statue\nFigure 12: More Visual Results.\nTurn the hair color to pink Turn the hair color to pink\nTurn the hair color to pink, put eyeglasses\non her\nTurn the hair color to pink, put eyeglasses\non her\nGive the portrait a comic book look Give the portrait a comic book look\nPut eyeglasses on his face, give him a\ngoatee\nPut eyeglasses on his face, give him a\ngoatee\nRemove the beard Remove the beard\nFigure 13: More Visual Results.\nMake her eyes appear narrower Make her eyes appear narrower\nStyle her hair with a pink wig Style her hair with a pink wig\nShave the beard off Shave the beard off"
        },
        {
            "heading": "Put eyeglasses on her Put eyeglasses on her",
            "text": "Add some beard Add some beard\nGive her thicker, bushier eyebrows Add bangs to the hairstyle\nTurn the portrait into a bronze statue Turn the portrait into a bronze statue"
        }
    ],
    "year": 2023
}