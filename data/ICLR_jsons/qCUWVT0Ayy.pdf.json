{
    "abstractText": "Graphic layout generation plays a significant role in user engagement and information perception. Existing methods primarily treat layout generation as a numerical optimization task, focusing on quantitative aspects while overlooking the semantic information of layout, such as the relationship between each layout element. In this paper, we propose LayoutNUWA, the first model that treats layout generation as a code generation task to enhance semantic information and harnesses the hidden layout expertise of large language models (LLMs). More concretely, we develop a Code Instruct Tuning (CIT) approach comprising three interconnected modules: 1) the Code Initialization (CI) module quantifies the numerical conditions and initializes them as HTML code with strategically placed masks; 2) the Code Completion (CC) module employs the formatting knowledge of LLMs to fill in the masked portions within the HTML code; 3) the Code Rendering (CR) module transforms the completed code into the final layout output, ensuring a highly interpretable and transparent layout generation procedure that directly maps code to a visualized layout. We attain significant state-of-the-art performance (even over 50% improvements) on multiple datasets, showcasing the strong capabilities of LayoutNUWA. Our code is available at https://github.com/ProjectNUWA/LayoutNUWA. \u2217Both authors contributed equally to this research. During Zecheng\u2019s internship under the mentorship of Chenfei at MSRA. \u2020Corresponding author.",
    "authors": [
        {
            "affiliations": [],
            "name": "Zecheng Tang"
        },
        {
            "affiliations": [],
            "name": "Chenfei Wu"
        },
        {
            "affiliations": [],
            "name": "Juntao Li"
        },
        {
            "affiliations": [],
            "name": "Nan Duan"
        }
    ],
    "id": "SP:e02f954a4fe86003a2aedb4708d4eb73735b43e9",
    "references": [
        {
            "authors": [
                "Diego Martin Arroyo",
                "Janis Postels",
                "Federico Tombari"
            ],
            "title": "Variational transformer networks for layout generation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "S\u00e9bastien Bubeck",
                "Varun Chandrasekaran",
                "Ronen Eldan",
                "Johannes Gehrke",
                "Eric Horvitz",
                "Ece Kamar",
                "Peter Lee",
                "Yin Tat Lee",
                "Yuanzhi Li",
                "Scott Lundberg"
            ],
            "title": "Sparks of artificial general intelligence: Early experiments with gpt-4",
            "venue": "arXiv preprint arXiv:2303.12712,",
            "year": 2023
        },
        {
            "authors": [
                "Shang Chai",
                "Liansheng Zhuang",
                "Fengying Yan"
            ],
            "title": "Layoutdm: Transformer-based diffusion model for layout generation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Huiwen Chang",
                "Han Zhang",
                "Lu Jiang",
                "Ce Liu",
                "William T Freeman"
            ],
            "title": "Maskgit: Masked generative image transformer",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Wenhu Chen",
                "Xueguang Ma",
                "Xinyi Wang",
                "William W Cohen"
            ],
            "title": "Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks",
            "venue": "arXiv preprint arXiv:2211.12588,",
            "year": 2022
        },
        {
            "authors": [
                "Chin-Yi Cheng",
                "Forrest Huang",
                "Gang Li",
                "Yang Li"
            ],
            "title": "Play: Parametrically conditioned layout generation using latent diffusion",
            "venue": "arXiv preprint arXiv:2301.11529,",
            "year": 2023
        },
        {
            "authors": [
                "Biplab Deka",
                "Zifeng Huang",
                "Chad Franzen",
                "Joshua Hibschman",
                "Daniel Afergan",
                "Yang Li",
                "Jeffrey Nichols",
                "Ranjitha Kumar"
            ],
            "title": "Rico: A mobile app dataset for building data-driven design applications",
            "venue": "In Proceedings of the 30th annual ACM symposium on user interface software and technology,",
            "year": 2017
        },
        {
            "authors": [
                "Xinhan Di",
                "Pengqian Yu"
            ],
            "title": "Multi-agent reinforcement learning of 3d furniture layout simulation in indoor graphics scenes",
            "venue": "arXiv preprint arXiv:2102.09137,",
            "year": 2021
        },
        {
            "authors": [
                "Weixi Feng",
                "Wanrong Zhu",
                "Tsu-jui Fu",
                "Varun Jampani",
                "Arjun Akula",
                "Xuehai He",
                "Sugato Basu",
                "Xin Eric Wang",
                "William Yang Wang"
            ],
            "title": "Layoutgpt: Compositional visual planning and generation with large language models",
            "venue": "arXiv preprint arXiv:2305.15393,",
            "year": 2023
        },
        {
            "authors": [
                "Tsu-Jui Fu",
                "William Yang Wang",
                "Daniel McDuff",
                "Yale Song"
            ],
            "title": "Doc2ppt: automatic presentation slides generation from scientific documents",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "Yuxian Gu",
                "Pei Ke",
                "Xiaoyan Zhu",
                "Minlie Huang"
            ],
            "title": "Learning instructions with unlabeled data for zero-shot cross-task generalization",
            "venue": "arXiv preprint arXiv:2210.09175,",
            "year": 2022
        },
        {
            "authors": [
                "Shunan Guo",
                "Zhuochen Jin",
                "Fuling Sun",
                "Jingwen Li",
                "Zhaorui Li",
                "Yang Shi",
                "Nan Cao"
            ],
            "title": "Vinci: an intelligent graphic design system for generating advertising posters",
            "venue": "In Proceedings of the 2021 CHI conference on human factors in computing systems,",
            "year": 2021
        },
        {
            "authors": [
                "Ari Holtzman",
                "Jan Buys",
                "Li Du",
                "Maxwell Forbes",
                "Yejin Choi"
            ],
            "title": "The curious case of neural text degeneration",
            "year": 1904
        },
        {
            "authors": [
                "Or Honovich",
                "Thomas Scialom",
                "Omer Levy",
                "Timo Schick"
            ],
            "title": "Unnatural instructions: Tuning language models with (almost) no human labor",
            "venue": "arXiv preprint arXiv:2212.09689,",
            "year": 2022
        },
        {
            "authors": [
                "Or Honovich",
                "Uri Shaham",
                "Samuel R Bowman",
                "Omer Levy"
            ],
            "title": "Instruction induction: From few examples to natural language task descriptions",
            "venue": "arXiv preprint arXiv:2205.10782,",
            "year": 2022
        },
        {
            "authors": [
                "Mude Hui",
                "Zhizheng Zhang",
                "Xiaoyi Zhang",
                "Wenxuan Xie",
                "Yuwang Wang",
                "Yan Lu"
            ],
            "title": "Unifying layout generation with a decoupled diffusion model",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Naoto Inoue",
                "Kotaro Kikuchi",
                "Edgar Simo-Serra",
                "Mayu Otani",
                "Kota Yamaguchi"
            ],
            "title": "Layoutdm: Discrete diffusion model for controllable layout generation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Srinivasan Iyer",
                "Xi Victoria Lin",
                "Ramakanth Pasunuru",
                "Todor Mihaylov",
                "D\u00e1niel Simig",
                "Ping Yu",
                "Kurt Shuster",
                "Tianlu Wang",
                "Qing Liu",
                "Punit Singh Koura"
            ],
            "title": "Opt-iml: Scaling language model instruction meta learning through the lens of generalization",
            "venue": "arXiv preprint arXiv:2212.12017,",
            "year": 2022
        },
        {
            "authors": [
                "Zhaoyun Jiang",
                "Shizhao Sun",
                "Jihua Zhu",
                "Jian-Guang Lou",
                "Dongmei Zhang"
            ],
            "title": "Coarse-to-fine generative modeling for graphic layouts",
            "venue": "In Proceedings of the AAAI conference on artificial intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "Akash Abdu Jyothi",
                "Thibaut Durand",
                "Jiawei He",
                "Leonid Sigal",
                "Greg Mori"
            ],
            "title": "Layoutvae: Stochastic scene layout generation from a label set",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2019
        },
        {
            "authors": [
                "Kotaro Kikuchi",
                "Edgar Simo-Serra",
                "Mayu Otani",
                "Kota Yamaguchi"
            ],
            "title": "Constrained graphic layout generation via latent optimization",
            "venue": "In Proceedings of the 29th ACM International Conference on Multimedia,",
            "year": 2021
        },
        {
            "authors": [
                "Xiang Kong",
                "Lu Jiang",
                "Huiwen Chang",
                "Han Zhang",
                "Yuan Hao",
                "Haifeng Gong",
                "Irfan Essa"
            ],
            "title": "Blt: bidirectional layout transformer for controllable layout generation",
            "venue": "In European Conference on Computer Vision,",
            "year": 2022
        },
        {
            "authors": [
                "Hsin-Ying Lee",
                "Lu Jiang",
                "Irfan Essa",
                "Phuong B Le",
                "Haifeng Gong",
                "Ming-Hsuan Yang",
                "Weilong Yang"
            ],
            "title": "Neural design network: Graphic layout generation with constraints",
            "venue": "In Computer Vision\u2013 ECCV 2020: 16th European Conference,",
            "year": 2020
        },
        {
            "authors": [
                "Fengheng Li",
                "An Liu",
                "Wei Feng",
                "Honghe Zhu",
                "Yaoyu Li",
                "Zheng Zhang",
                "Jingjing Lv",
                "Xin Zhu",
                "Junjie Shen",
                "Zhangang Lin"
            ],
            "title": "Relation-aware diffusion model for controllable poster layout generation",
            "venue": "arXiv preprint arXiv:2306.09086,",
            "year": 2023
        },
        {
            "authors": [
                "Jianan Li",
                "Jimei Yang",
                "Aaron Hertzmann",
                "Jianming Zhang",
                "Tingfa Xu"
            ],
            "title": "Layoutgan: Generating graphic layouts with wireframe discriminators",
            "year": 1901
        },
        {
            "authors": [
                "Xiang Li",
                "John Thickstun",
                "Ishaan Gulrajani",
                "Percy S Liang",
                "Tatsunori B Hashimoto"
            ],
            "title": "Diffusionlm improves controllable text generation",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Quanyu Long",
                "Tianze Luo",
                "Wenya Wang",
                "Sinno Jialin Pan"
            ],
            "title": "Domain confused contrastive learning for unsupervised domain adaptation",
            "venue": "arXiv preprint arXiv:2207.04564,",
            "year": 2022
        },
        {
            "authors": [
                "James MacQueen"
            ],
            "title": "Some methods for classification and analysis of multivariate observations",
            "venue": "In Proceedings of the fifth Berkeley symposium on mathematical statistics and probability,",
            "year": 1967
        },
        {
            "authors": [
                "Swaroop Mishra",
                "Daniel Khashabi",
                "Chitta Baral",
                "Hannaneh Hajishirzi"
            ],
            "title": "Cross-task generalization via natural language crowdsourcing instructions",
            "venue": "arXiv preprint arXiv:2104.08773,",
            "year": 2021
        },
        {
            "authors": [
                "Niklas Muennighoff",
                "Thomas Wang",
                "Lintang Sutawika",
                "Adam Roberts",
                "Stella Biderman",
                "Teven Le Scao",
                "M Saiful Bari",
                "Sheng Shen",
                "Zheng-Xin Yong",
                "Hailey Schoelkopf"
            ],
            "title": "Crosslingual generalization through multitask finetuning",
            "venue": "arXiv preprint arXiv:2211.01786,",
            "year": 2022
        },
        {
            "authors": [
                "Maxwell Nye",
                "Anders Johan Andreassen",
                "Guy Gur-Ari",
                "Henryk Michalewski",
                "Jacob Austin",
                "David Bieber",
                "David Dohan",
                "Aitor Lewkowycz",
                "Maarten Bosma",
                "David Luan"
            ],
            "title": "Show your work: Scratchpads for intermediate computation with language models",
            "venue": "arXiv preprint arXiv:2112.00114,",
            "year": 2021
        },
        {
            "authors": [
                "Peter O\u2019Donovan",
                "Aseem Agarwala",
                "Aaron Hertzmann"
            ],
            "title": "Designscape: Design with interactive layout suggestions",
            "venue": "In Proceedings of the 33rd annual ACM conference on human factors in computing systems,",
            "year": 2015
        },
        {
            "authors": [
                "Peter O\u2019Donovan",
                "Aseem Agarwala",
                "Aaron Hertzmann"
            ],
            "title": "Learning layouts for single-pagegraphic designs",
            "venue": "IEEE transactions on visualization and computer graphics,",
            "year": 2014
        },
        {
            "authors": [
                "Akshay Gadi Patil",
                "Omri Ben-Eliezer",
                "Or Perel",
                "Hadar Averbuch-Elor"
            ],
            "title": "Read: Recursive autoencoders for document layout generation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops,",
            "year": 2020
        },
        {
            "authors": [
                "Samyam Rajbhandari",
                "Jeff Rasley",
                "Olatunji Ruwase",
                "Yuxiong He"
            ],
            "title": "Zero: Memory optimizations toward training trillion parameter models",
            "venue": "In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis,",
            "year": 2020
        },
        {
            "authors": [
                "Baptiste Rozi\u00e8re",
                "Jonas Gehring",
                "Fabian Gloeckle",
                "Sten Sootla",
                "Itai Gat",
                "Xiaoqing Ellen Tan",
                "Yossi Adi",
                "Jingyu Liu",
                "Tal Remez",
                "J\u00e9r\u00e9my Rapin"
            ],
            "title": "Code llama: Open foundation models for code",
            "venue": "arXiv preprint arXiv:2308.12950,",
            "year": 2023
        },
        {
            "authors": [
                "Victor Sanh",
                "Albert Webson",
                "Colin Raffel",
                "Stephen H Bach",
                "Lintang Sutawika",
                "Zaid Alyafeai",
                "Antoine Chaffin",
                "Arnaud Stiegler",
                "Teven Le Scao",
                "Arun Raja"
            ],
            "title": "Multitask prompted training enables zero-shot task generalization",
            "venue": "arXiv preprint arXiv:2110.08207,",
            "year": 2021
        },
        {
            "authors": [
                "Xiaojuan Tang",
                "Zilong Zheng",
                "Jiaqi Li",
                "Fanxu Meng",
                "Song-Chun Zhu",
                "Yitao Liang",
                "Muhan Zhang"
            ],
            "title": "Large language models are in-context semantic reasoners rather than symbolic reasoners",
            "venue": "arXiv preprint arXiv:2305.14825,",
            "year": 2023
        },
        {
            "authors": [
                "Hugo Touvron",
                "Louis Martin",
                "Kevin Stone",
                "Peter Albert",
                "Amjad Almahairi",
                "Yasmine Babaei",
                "Nikolay Bashlykov",
                "Soumya Batra",
                "Prajjwal Bhargava",
                "Shruti Bhosale"
            ],
            "title": "Llama 2: Open foundation and fine-tuned chat models",
            "venue": "arXiv preprint arXiv:2307.09288,",
            "year": 2023
        },
        {
            "authors": [
                "Xuezhi Wang",
                "Jason Wei",
                "Dale Schuurmans",
                "Quoc Le",
                "Ed Chi",
                "Sharan Narang",
                "Aakanksha Chowdhery",
                "Denny Zhou"
            ],
            "title": "Self-consistency improves chain of thought reasoning in language models",
            "venue": "arXiv preprint arXiv:2203.11171,",
            "year": 2022
        },
        {
            "authors": [
                "Jason Wei",
                "Maarten Bosma",
                "Vincent Y Zhao",
                "Kelvin Guu",
                "Adams Wei Yu",
                "Brian Lester",
                "Nan Du",
                "Andrew M Dai",
                "Quoc V Le"
            ],
            "title": "Finetuned language models are zero-shot learners",
            "venue": "arXiv preprint arXiv:2109.01652,",
            "year": 2021
        },
        {
            "authors": [
                "Jason Wei",
                "Xuezhi Wang",
                "Dale Schuurmans",
                "Maarten Bosma",
                "Ed Chi",
                "Quoc Le",
                "Denny Zhou"
            ],
            "title": "Chain of thought prompting elicits reasoning in large language models",
            "year": 2022
        },
        {
            "authors": [
                "Lijun Wu",
                "Xu Tan",
                "Di He",
                "Fei Tian",
                "Tao Qin",
                "Jianhuang Lai",
                "Tie-Yan Liu"
            ],
            "title": "Beyond error propagation in neural machine translation: Characteristics of language also matter",
            "venue": "arXiv preprint arXiv:1809.00120,",
            "year": 2018
        },
        {
            "authors": [
                "Yang Wu",
                "Yanyan Zhao",
                "Zhongyang Li",
                "Bing Qin",
                "Kai Xiong"
            ],
            "title": "Improving cross-task generalization with step-by-step instructions",
            "venue": "arXiv preprint arXiv:2305.04429,",
            "year": 2023
        },
        {
            "authors": [
                "Can Xu",
                "Qingfeng Sun",
                "Kai Zheng",
                "Xiubo Geng",
                "Pu Zhao",
                "Jiazhan Feng",
                "Chongyang Tao",
                "Daxin Jiang"
            ],
            "title": "Wizardlm: Empowering large language models to follow complex instructions",
            "venue": "arXiv preprint arXiv:2304.12244,",
            "year": 2023
        },
        {
            "authors": [
                "Hanwei Xu",
                "Yujun Chen",
                "Yulun Du",
                "Nan Shao",
                "Yanggang Wang",
                "Haiyu Li",
                "Zhilin Yang"
            ],
            "title": "Zeroprompt: Scaling prompt-based pretraining to 1,000 tasks improves zero-shot generalization",
            "venue": "arXiv preprint arXiv:2201.06910,",
            "year": 2022
        },
        {
            "authors": [
                "Kota Yamaguchi"
            ],
            "title": "Canvasvae: Learning to generate vector graphic documents",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Cheng-Fu Yang",
                "Wan-Cyuan Fan",
                "Fu-En Yang",
                "Yu-Chiang Frank Wang"
            ],
            "title": "Layouttransformer: Relation-aware scene layout generation",
            "year": 2020
        },
        {
            "authors": [
                "Xuyong Yang",
                "Tao Mei",
                "Ying-Qing Xu",
                "Yong Rui",
                "Shipeng Li"
            ],
            "title": "Automatic generation of visualtextual presentation layout",
            "venue": "ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM),",
            "year": 2016
        },
        {
            "authors": [
                "Eric Zelikman",
                "Yuhuai Wu",
                "Jesse Mu",
                "Noah Goodman"
            ],
            "title": "Star: Bootstrapping reasoning with reasoning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Junyi Zhang",
                "Jiaqi Guo",
                "Shizhao Sun",
                "Jian-Guang Lou",
                "Dongmei Zhang"
            ],
            "title": "Layoutdiffusion: Improving graphic layout generation by discrete diffusion probabilistic models",
            "venue": "arXiv preprint arXiv:2303.11589,",
            "year": 2023
        },
        {
            "authors": [
                "Ruohong Zhang",
                "Yau-Shian Wang",
                "Yiming Yang"
            ],
            "title": "Generation-driven contrastive self-training for zero-shot text classification with instruction-tuned gpt",
            "venue": "arXiv preprint arXiv:2304.11872,",
            "year": 2023
        },
        {
            "authors": [
                "Shengyu Zhang",
                "Linfeng Dong",
                "Xiaoya Li",
                "Sen Zhang",
                "Xiaofei Sun",
                "Shuhe Wang",
                "Jiwei Li",
                "Runyi Hu",
                "Tianwei Zhang",
                "Fei Wu"
            ],
            "title": "Instruction tuning for large language models: A survey",
            "venue": "arXiv preprint arXiv:2308.10792,",
            "year": 2023
        },
        {
            "authors": [
                "Xinru Zheng",
                "Xiaotian Qiao",
                "Ying Cao",
                "Rynson WH Lau"
            ],
            "title": "Content-aware generative modeling of graphic design layouts",
            "venue": "ACM Transactions on Graphics (TOG),",
            "year": 2019
        },
        {
            "authors": [
                "Xu Zhong",
                "Jianbin Tang",
                "Antonio Jimeno Yepes"
            ],
            "title": "Publaynet: largest dataset ever for document layout analysis",
            "venue": "In 2019 International Conference on Document Analysis and Recognition (ICDAR),",
            "year": 2019
        }
    ],
    "sections": [
        {
            "text": "\u2217Both authors contributed equally to this research. During Zecheng\u2019s internship under the mentorship of Chenfei at MSRA. \u2020Corresponding author."
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "Graphic layout, which refers to the organization and positioning of design elements, significantly influences the way users engage with and perceive the presented information (Lee et al., 2020). As a growing research field, layout generation (Li et al., 2019; Yang et al., 2020) aims to create diverse and realistic layouts that streamline the design process and cater to various applications, such as user interfaces (Deka et al., 2017; Jiang et al., 2022), indoor scenes (Di & Yu, 2021; Feng et al., 2023), document layouts (Zheng et al., 2019; Yamaguchi, 2021), presentation slides (Fu et al., 2022), etc.\nCurrent approaches (Jyothi et al., 2019; Li et al., 2019; Arroyo et al., 2021; Zhang et al., 2023a) regard each element in the layout as numerical tuples (c, x, y, w, h), in which c indicates the element category, x and y represent coordinates, w and h correspond to width and height. For example, autoregressive-based methods (Yang et al., 2020; Jiang et al., 2022) view the tuple as a sequence and predict their values sequentially, while diffusion-based methods (Chai et al., 2023; Inoue et al., 2023) consider the tuple as a whole and predict their values through a denoising approach. Despite adopting different generative models, all of these methods fundamentally consider layout generation as a numerical tuple optimization task. However, representing layouts as numerical tuples has its limitations, as it primarily focuses on capturing the quantitative aspects of the layout, such as positions and sizes, while lacking semantic information, e.g., the attribute of each numerical value, which may limit the model\u2019s ability to capture more complex and rich layout information.\nAn insightful question emerges from the limitations of existing methods in layout generation: can we integrate semantic information into the layout generation process to enrich the overall representation and enhance the quality of the generated layouts? Addressing this question brings forth two major benefits: firstly, it bolsters the understanding of relationships among various layout elements, and secondly, it enables us to tap into the semantic capabilities of LLMs (Tang et al., 2023), resulting in more intricate and contextually relevant layouts for a wide range of applications (Jiang et al., 2022). Considering the inherent logical nature of layouts, which involve dependency relationships among layout elements, and the fact that each graphic layout can be represented with a fixed structure sequence, code languages emerge as a promising alternative. Code languages can encompass numerical and semantic information while possessing a strong logical foundation (Chen et al., 2022), which can thus bridge the gap between existing methods and the desired enriched representation.\nBased on the above observations, we propose LayoutNUWA, a groundbreaking model that revolutionizes the layout generation task by treating it as a code generation task. Our innovative approach is designed to not only enhance the semantic information within layouts but also seamlessly leverage the expertise of LLMs in the layout generation process. To achieve this, we design a Code Instruct Tuning (CIT) approach comprising three interconnected modules: 1) firstly, the Code Initialization (CI) module quantifies the numerical conditions and initializes them as HTML code with strategically placed masks, paving the way for more meaningful and coherent layouts; 2) secondly, the Code Completion (CC) module employs the formatting knowledge of LLMs to fill in the masked portions within the HTML code, thereby harnessing the power of LLMs to improve the accuracy and consistency of the generated layouts; 3) lastly, the Code Rendering (CR) module transforms the completed code into the final layout output, ensuring a highly interpretable and transparent layout generation procedure that directly maps code to a visualized layout.\nExperiments across a variety of conditional layout generation tasks on three datasets, i.e., Rico (Deka et al., 2017), PubLayNet (Zhong et al., 2019) and Magazine (Zheng et al., 2019), highlight the superiority of our method, in which LayoutNUWA can significantly outperform all the baselines and shows comparable results with the task-specific models. Furthermore, LayoutNUWA can achieve at least a 50% improvement in performance compared to the best baseline on the low-resource datasets, e.g., the Magazine dataset. In a nutshell, our contributions can be outlined as follows:\n\u2022 We introduce LayoutNUWA, the first model that treats the layout generation task as a code generation task, effectively harnessing the hidden layout expertise of LLMs.\n\u2022 We propose Code Instruct Tuning, which empowers the model to adhere to instructions and enriches the semantic information of layout, resulting in precise and standardized code.\n\u2022 We attain significant state-of-the-art performance on multiple datasets, showcasing the robust capabilities of LayoutNUWA."
        },
        {
            "heading": "2 RELATED WORK",
            "text": ""
        },
        {
            "heading": "2.1 LAYOUT GENERATION",
            "text": "Automatic layout generation, an important task for automatic graphical design for various scenarios such as document layouts (Zheng et al., 2019; Zhong et al., 2019; Yamaguchi, 2021; Fu et al., 2022), posters (Yang et al., 2016; Guo et al., 2021; Li et al., 2023) and user interface (Deka et al., 2017), has been recently extensively researched. Early approaches for layout generation involve embedding design rules into manually-defined energy functions (O\u2019Donovan et al., 2014; O\u2019Donovan et al., 2015), while other methods have explored generative models such as GANs and VAEs for generating numerical graphic and scene layouts, including LayoutGAN (Li et al., 2019), LayoutVAE (Jyothi et al., 2019), LayoutGAN++ (Kikuchi et al., 2021), NDN (Lee et al., 2020) and READ (Patil et al., 2020). Apart from them, transformer-based approaches utilize self-attention mechanisms to learn numerical contextual relationships between elements and achieve layout completion based on partial layout inputs (Yang et al., 2020; Kong et al., 2022; Feng et al., 2023). Recently, with the prevalence of diffusion models, several works also adopted diffusion models to tackle a broader range of conditional layout generation (Chai et al., 2023; Inoue et al., 2023; Zhang et al., 2023a; Hui et al., 2023; Cheng et al., 2023). However, existing methods primarily treat layout generation as a numerical optimization task, focusing on quantitative aspects while overlooking the semantic information of layout, such as the relationship between each layout element. Different from previous works, we convert the layout generation task into the code generation task to directly generate the layout in code language and thus utilize the rich knowledge from LLMs, which can significantly improve the FID by 50% in the Magazine dataset in \u00a7 4.2."
        },
        {
            "heading": "2.2 INSTRUCTION TUNING",
            "text": "Instruction tuning represents the process of fine-tuning LLMs on the instruction dataset in a supervised fashion, which narrows the gap between the next-word prediction manner of LLMs and the users\u2019 objective of having LLMs adhere to human instructions (Zhang et al., 2023c). Early attempts on instruction tuning involve multi-task training with manually-written descriptions about different tasks (Mishra et al., 2021; Wei et al., 2021; Sanh et al., 2021; Xu et al., 2022; Muennighoff et al., 2022; Iyer et al., 2022) or automatically generated instructions (Wang et al., 2022; Gu et al., 2022; Zhang et al., 2023b; Honovich et al., 2022a;b). Apart from controlling the LLMs through input instruction, Nye et al. (2021) show that LLM can handle more complex tasks by generating the intermediate steps and Wei et al. (2022) propose chain-of-thought technique by enriching the instruction with intermediate reasoning step descriptions, which endows LLMs with better performance (Wang et al., 2022; Zelikman et al., 2022; Wu et al., 2023; Xu et al., 2023). However, the instruction tuning methods mentioned above are primarily intended for text generation tasks and not ideal for layout generation tasks, which involve numerical optimization. Thus, we propose a code instruction tuning method that is specially designed for the layout generation task. Experiments in \u00a7 5.1 indicate that the performance significantly drops if the code instruction tuning is not adopted."
        },
        {
            "heading": "3 METHODOLOGY",
            "text": ""
        },
        {
            "heading": "3.1 PROBLEM FORMULATION",
            "text": "The layout generation task aims to generate a well-organized layout S = {si}Ni=1, with N representing the number of elements in the layout. Each element, si = (ci, xi, yi, wi, hi), consists of the following components: ci is the category, xi, yi indicate the center location, and wi, hi represent the width and height, respectively. In this study, we focus on the conditional layout generation task, wherein partial components in si are masked with M , and the complete layout S should be predicted by model f\u03b8 conditioned on the remaining components S\\M :\nS = f\u03b8(S\\M ) (1)\nPrevious works (Jyothi et al., 2019; Yang et al., 2020; Inoue et al., 2023) regard each element si as a sequence of numerical values, e.g., (0, 10, 20, 25, 30), and train a model to directly generate these values. However, this approach overlooks the semantic information of the components, thus limiting the model\u2019s understanding of the layout semantics. Based on this observation, we propose\na new problem definition, where we convert the input S\\M and output S into a code language and view the layout generation task as a code generation task:\nCODE(S) = f\u03b8(CODE(S\\M )) (2) Eq. 2 has the following 3 advantages compared with Eq. 1:\n\u2022 Semantic Insights: By converting the numerical values into code language, the model can better capture the semantic relationships between different components of the layout.\n\u2022 LLM Utilization: By using code language, the model can further leverage the knowledge of Large Language Models (LLMs) and thus enhance the quality of the generated layouts.\n\u2022 Model Scalability: The code language has a stronger expressive capability compared to numerical values, which allows the addition of more attributes for layout elements."
        },
        {
            "heading": "3.2 CODE INSTRUCT TUNING",
            "text": "As shown in Fig. 1, we propose Code Instruct Tuning (CIT) with three modules: (1) Code Initialization module converts layout into masked code language with dynamic templates; (2) Code Completion module inputs the masked code to LLMs to generate complete code; (3) Code Rendering module directly renders code to the final graphic layout. We illustrate these modules below."
        },
        {
            "heading": "3.2.1 CODE INITIALIZATION",
            "text": "Element Quantization We quantify the numerical values of i-th element position {xi, yi} and size {wi, hi} in the layout with Adaptive Quantization method (Inoue et al., 2023) that applies k-Means algorithm (MacQueen et al., 1967) to cluster the position and size information of each element, addressing the highly imbalanced distribution of these values, e.g., elements may overlap or cluster together. Different from the previous works (Chai et al., 2023; Zhang et al., 2023a; Inoue et al., 2023), we use absolute position to represent the coordinates rather than relative positions. This aligns with code language and allows direct rendering of layouts without necessitating coordinate conversion, thereby preventing potential information loss. We maintain precision up to one decimal place and directly convert the clustered results into strings.\nTemplate Construction The overview of template construction is shown in Fig. 2. We construct the templates based on the most common web page layout code, HTML, which contains a wealth of information and is easily accessed by LLMs during the pre-training process (Touvron et al., 2023; Rozie\u0300re et al., 2023). Specifically, in HTML code, each element is described with a tag that provides information about the content or the element structure. Since the elements in the layout are regular squares, we chose the <rect> tag as the content tag to describe each element:\n<rect data-category={ci} x={xi} y={yi} width={wi} height={hi}>\nwhere ci is the element category in textual format and {xi, yi, wi, hi} are the quantified position and size of the i-th element. Then, to combine all the elements into a unified structure, we used an opening tag and a closing tag to define the boundaries of each layout, which can be written as:\n<html><body><svg width={W} height={H}> ... </svg></body></html>\nwhere W and H are the background width and height of the layout.\nIn order to facilitate better learning of layout in various domains and tasks and leverage the instruction-following capabilities of LLMs, we design the following prompts:"
        },
        {
            "heading": "I want to generate layout in {Domain} style. Please generate the",
            "text": "layout according to the {Task Condition} I provide:\nwhere the {domain} and the {Task Condition} will vary according to different domains and tasks. For instance, for the RICO dataset, we set Domain as \u201cmobile UI\u201d, and for the layout completion task, we set Task Condition as \u201cremaining values\u201d."
        },
        {
            "heading": "3.2.2 CODE COMPLETION",
            "text": "To construct the conditional input of the layout generation task, we utilize the mask tokens of LLMs to represent the masked values M and let the model predict the masked values within the HTML code. Different from previous works (Chai et al., 2023; Zhang et al., 2023a; Inoue et al., 2023) that applied the customized numerical vocabulary, we employ the LLM\u2019s token vocabulary directly. By doing so, we can leverage the knowledge of the numerical tokens inherited in the LLMs. Considering that almost all the LLMs follow auto-regressive generation manner that brings significant limitation to the layout generation task since the model should predict the same layout under different element orders, even if the layout doesn\u2019t have a naturally defined order (Yang et al., 2020). Thus, we design a self-consistency strategy that randomly permutes the order of the input elements in the layout within a mini-batch. Meanwhile, in order to adapt LLMs to different conditional layout generation tasks, we have performed multi-task modeling on the same layout, utilizing various conditions and implementing a joint loss for these tasks. Given the permutation times K and task numbers T , the joint loss, denoted as L(\u00b7), for each layout S can be written as:\nL(S | \u03b8) = T\u2211\nt=1 N\u2211 j=1 K\u2211 k=1 L(s (k) j \\M (t) j | \u03b8), (3)\nwhere \u03b8 is the model parameters and sj denote the j-th element in the layout S."
        },
        {
            "heading": "3.2.3 CODE RENDERING",
            "text": "Most existing works require the extra conversion step to render the graphic layouts (Yang et al., 2020; Chai et al., 2023; Zhang et al., 2023a), e.g., converting the relative position to the absolute position, causing the information loss. Different from previous work, LayoutNUWA allows for immediate rendering as it generates the absolute position directly. Besides, considering the potential output issues such as boundary overflow (Inoue et al., 2023) and format errors, we employ regular expressions to remove mismatched formats and implement clipping operations for elements that exceed the background size."
        },
        {
            "heading": "4 EXPERIMENT",
            "text": ""
        },
        {
            "heading": "4.1 EXPERIMENTAL SETTINGS",
            "text": "Datasets We evaluate the model performance on three widely used public datasets. RICO (Deka et al., 2017) is a user interface design dataset for mobile applications containing 25 element categories and 66K+ UI layouts. PubLayNet (Zhong et al., 2019) consists of 360K+ layouts for documents with 5 element categories. Magazine (Zheng et al., 2019) is a low-resource magazine layout dataset containing around 4K annotated layouts and 6 element categories. We follow LayoutDM (Inoue et al., 2023) to view the original validation data as the testing set and pre-process all three datasets by discarding the layouts containing more than 25 elements as well as splitting the filtered data into the training and new validation sets by 95% and 5%.\nEvaluation Metrics We employ four metrics to evaluate the generation results comprehensively, including Frechet Inception Distance (FID), Maximum Interaction over Union (mIoU), Alignment (Align.), and Overlap. Among them, FID compares the distribution of generated and real layouts. Similar to the previous work (Inoue et al., 2023), we utilize an enhanced feature extraction model for layouts (Kikuchi et al., 2021) to compute the FID score. We measure the conditional similarity between generated and real layouts using mIoU, which is done by calculating the maximum IoU between bounding boxes of generated and real layouts with the same type set. Alignment and Overlap scores are calculated following the previous work (Li et al., 2019) to evaluate proper element alignment and overlapping in a generated layout, and it is worth noting that we ignore normal overlaps, e.g., elements on top of the background, and discard the layouts that failed to generate. For reference, we show the evaluation results between the validation set and test set as Real data.\nTasks and Baselines We evaluate LayoutNUWA on three conditional layout generation tasks1, including the Category to Size and Position (C \u2192 S+P) task, the Category and Size to Position (C+S \u2192 P) task, and the Completion task. More concretely, the C \u2192 S+P task requires the model to predict the position and size of the element based on its category. For the C+S \u2192 P task, the model predicts the position of the element based on both its size and category. Finally, in the completion task, the element\u2019s size and position values are randomly masked up to 80%, and the model predicts the entire layout using the remaining values. We compare LayoutNUWA with six strong baselines, including LayoutTrans (Yang et al., 2020), BLT (Kong et al., 2022), LayoutGAN++ (Kikuchi et al., 2021), MaskGIT (Chang et al., 2022), DiffusionLM (Li et al., 2022) and LayoutDM (Inoue et al., 2023). For the above baselines, we implement them with the official code directly.\nImplementation Details We implement LayoutNUWA with two 7B LLMs: LLaMA2 (L2) (Touvron et al., 2023) and CodeLLaMA (CL) (Rozie\u0300re et al., 2023). We train LayoutNUWA with two settings: (1) Domain-Specific (DS) setting, where the model is trained on distinct datasets, and (2) Domain-Agnostic (DA) setting, where the model is trained on all three datasets, including RICO, PubLayNet, and Magazine. The default configuration for LayoutNUWA utilizes CodeLLaMA (CL) and Domain-Agnostic (DA), i.e., LayoutNUWA-CL-DA. We set permutation times K = 10 and task numbers T = 3. For model training, we use DeepSpeed Library (Rajbhandari et al., 2020) to run all experiments on 64 NVIDIA V100 GPUs. We apply Top-p sampling (Holtzman et al., 2019) for inference, where p = 0.9 and the temperature is 0.6. We set the maximum generation length as 1024 for each sample to ensure the completeness of the layout code."
        },
        {
            "heading": "4.2 QUANTITATIVE EVALUATION",
            "text": "We report the model performance on three datasets: the Magazine dataset in Tab. 1, RICO, and PubLayNet datasets in Tab. 2. For the Magazine dataset, LayoutNUWA demonstrates a remarkable performance by significantly surpassing all baseline measures across all tasks. Moreover, it outperforms the strong baseline LayoutDM by more than 50% when assessed with the FID metric.\nThe significant improvements in Tab. 1 are due to three aspects: 1) previous approaches generated numerical values, while LayoutNUWA generates code with labels, which greatly benefits the model\n1We also report the model performance on three datasets under the unconditional generation setting in Appendix E for space limitation.\nby utilizing the semantic information of layout attributes such as width, height, position, and category; 2) none of the previous methods used LLMs. However, we have introduced LLMs for the first time, which has resulted in significant performance enhancements, i.e., performance has improved from 19.206 to 9.741. Furthermore, when we use CodeLLaMA, which is tuned on code language, the performance improves even further to 8.985; 3) since different domains require distinct layout formats, early numerical-based methods could only be trained in a domain-specific manner. However, LayoutNUWA is based on code structure, which can be trained in a domain-agnostic manner, allowing for complementary among data from various domains, thus further improving FID to 8.791.\nWe have also conducted extensive experiments on two other datasets: RICO and PubLayNet, as shown in Tab. 2. The LayoutNUWA notably surpasses all baseline methods in the majority of tasks. Although it does not achieve the best performance in two specific tasks, it still secures at least the second-highest performance in those instances. This shows the strong generalization of the LayoutNUWA. It is worth mentioning that our model also achieves closer Align. and Overlap scores to the Real Data compared to the baselines. Although previous work has suggested that refinement and discriminator processes can contribute to improving the Align. and Overlap (Inoue et al., 2023; Li et al., 2019) scores, our method attains better results without employing these steps."
        },
        {
            "heading": "4.3 QUALITATIVE EVALUATION",
            "text": "We render the generated layout code with the Code Rendering (CR) method, and Fig. 3 shows the sampled rendering results of the PubLayNet dataset. By comparing with other baselines, we can observe that the layouts generated by LayoutNUWA exhibit excellent element alignment, and the proportion of overlap between elements is minimal. Additionally, our results are the most consistent with the Real Design data, i.e., the size and position of the generated element are essentially consistent with the real design, indicating that by treating the layout generation task as a code generation task, LayoutNUWA has successfully learned the distribution of document layouts, thus result in more precise and realistic layouts. More generated cases can be referred to Fig. 11 in the appendix."
        },
        {
            "heading": "5 ABLATION STUDY",
            "text": "We investigate the effectiveness of the CIT tuning method in Sec. 5.1 and compare the impact of different output formats and fine-tuning in Sec. 5.2. More concretely, we set the LayoutNUWA-L2DS model as the basic setting and conduct the ablation studies on the Magazine dataset."
        },
        {
            "heading": "5.1 EFFECT OF TUNING METHODS",
            "text": "We progressively reduce the modules in CIT and fine-tune the model using the corresponding constructed data. Specifically, we first exclude the code template and directly convert the element information into an ordered sequence S with a task instruction before it, i.e., the instruction tuning method. Then, we further remove the task instruction and directly fine-tune the model using data from different tasks separately, i.e., the numerical tuning method. As shown in Tab. 3, we can observe that the model performance has declined significantly without the code template, and it can only work in the DS setting since the model can simply generate repetitive and out-of-order results that are inconsistent with the element sequence in the DA setting. Furthermore, the numerical tuning method can only support the DS setting as there is no task instruction for the model to distinguish between different tasks, and the model performance is far inferior compared to those of the CIT as such an approach overlooks the rich semantic information among the elements and can not calibrate the prior code knowledge of LLMs."
        },
        {
            "heading": "5.2 EFFECT OF OUTPUT FORMAT AND FINETUNING",
            "text": "We compared the effects of the model output in code format and numerical format. For the numerical output format, we designed a Code Infilling task, which involves making the LLM predict only the masked values rather than predicting the entire code sequence. As shown in Tab. 4, we can find that generating in numerical format will increase the failure ratio of model generations, e.g., the model will\ngenerate repetitive results, and significantly decrease the model performance. This is because the layout generated by the conditional layout generation task should be logical, while only predicting the masked parts can lead to discrete values that lack logic. Besides, Due to the influence of the autoregressive manner, where the content generated in the next step depends on the previous history, this phenomenon may result in a higher failure probability of model generation when predicting layouts with more masked values. We also conduct a comparison between LayoutNUWA and GPT4 (Bubeck et al., 2023). Specifically, we allow GPT-4 to perform inference by constructing the input using the CIT method. Tab. 5 shows code instruct tuning for LLM is necessary, as using LLM in a zero-shot manner leads to a high fail rate (100% fail rate of LLaMA2 and around 30% for GPT-4)."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "In this paper, we propose LayoutNUWA, a groundbreaking approach that treats layout generation as a code generation task, effectively enriching the semantic information of layouts and leveraging the hidden expertise of LLMs. Extensive experiments on multiple datasets have demonstrated the superiority of our method. This research has the potential to revolutionize the field of layout generation and pave the way for further exploration and development of semantic-aware layout generation approaches in various applications.\nETHICS STATEMENT\nThis research is done in alignment with Microsoft\u2019s responsible AI principles."
        },
        {
            "heading": "A LIMITATIONS",
            "text": "Since LayoutNUWA employs the autoregressive (AR) LLMs as the backbone, our method naturally inherits the shortcomings of the AR models:\n\u2022 The generation speed is slower than the non-autoregressive models (Chang et al., 2022).\n\u2022 It suffers from the error propagation problem (Wu et al., 2018), i.e., as shown in Fig. 4, especially when training is insufficient, where the content generated later in the sequence may be negatively affected by the errors in the content generated earlier.\nAdditionally, the model may suffer from the domain confusion issue Long et al. (2022) to some extent under the Domain-Agnostic setting as illustrated in Appendix F. In our future work, we will address these challenges and make improvements to generate better graphic layouts."
        },
        {
            "heading": "B COMPARISON WITH GPT-4",
            "text": "We utilize the GPT-4 model with the commercial API and strictly follow the usage policy 2. We report the detailed performance of the GPT-4 model in Tab. 6 and show several rendered graphic layouts in Fig. 10. We can observe that the content generated by GPT-4 in the zero-shot setting primarily follows the layout design rule, which further confirms the potential capability of LLMs in generating layouts when guided by the CIT approach. However, when compared to LayoutNUWA, there are several issues with the results generated by GPT-4: 1) the distribution of elements is uneven, with elements tending to be concentrated in certain areas, such as the left side of the canvas; 2) the element sizes are inconsistent, for instance, in some graphic layouts, there might be one or two large elements, which results in the high scores of the mIOU and Overlap metrics for some tasks; 3) there is a significant discrepancy between the data distribution of generated content and the real data."
        },
        {
            "heading": "C HUMAN EVALUATION",
            "text": "We conduct the human evaluation for the model performance on the RICO and PubLayNet datasets. Specifically, We compare LayoutNUWA with two other strong baselines, including LayoutDM (Inoue et al., 2023) and LayoutTransformer (Yang et al., 2020), and randomly sample 25 graphic layouts generated from each model. We invite the annotators to choose which model performs better according to two evaluation settings: 1) quality evaluation based on the detail depiction, overlapping degree, and layout rationality in each layout; 2) diversity evaluation based on the diversity of the element arrangement in each layout. We hire 10 annotators to give their preferences, and the results are shown in Fig. 5(a) and Fig. 5(b). We can observe that layoutNUWA significantly outperforms the other two strong baselines, i.e., LayoutDM and LayoutTransformer, in terms of both generation quality and generation diversity. More generated cases can be referred to Fig. 10 (Magazine dataset) and Fig. 11 (RICO and PubLayNet datasets).\nQuality Diversity 0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6 0.60 0.56\n0.32 0.32\n0.08 0.12\nLayoutNUWA LayoutDM LayoutTrans\n(a) Human evaluation on the RICO dataset.\nQuality Diversity 0.0\n0.1\n0.2\n0.3\n0.4\n0.5 0.48 0.44\n0.36 0.32\n0.16\n0.24\nLayoutNUWA LayoutDM LayoutTrans\n(b) Human evaluation on the PubLayNet dataset.\n2https://openai.com/policies/terms-of-use"
        },
        {
            "heading": "D SAMPLED TRAINING-TESTING PAIRS WITH SIMILAR LAYOUT DISTRIBUTION",
            "text": "Considering that the qualitative generation results of LayoutNUWA look very similar to the actual design (Fig. 3), in order to further verify the fairness and rationality of our experiment, and to prove that the model is not overfitting on some specific training data, we used DocSim to select content from the training dataset that is similar to the Layout distribution of the test dataset. We can observe that the layout distribution between the test data and some of the training data has some similarities. For example, in the PubLayNet dataset, the Title Element appears at the top of the overall layout, but it is not entirely consistent.\nTesting Case Training Case 1 Training Case 2 Testing Case Training Case 1 Training Case 2 Testing Case Training Case 1 Training Case 2\nR I C OMagazine PubLayNet\nFigure 5: Sampled Training-Testing pairs by DocSim metric."
        },
        {
            "heading": "E MODEL PERFORMANCE UNDER UNCONDITIONAL SETTING",
            "text": "For unconditional generation, models generate 1,200 samples with the random seed. For LayoutNUWA-CL-DS, we randomly provide the empty templates without any prior information, e.g., category name, positions, etc.3 However, since LayoutNUWA-CL-DA is trained under the domain-agnostic setting, we must provide it with the domain prompt, e.g., \u201cI want to generate layout in mobile UI style\u201d for the RICO dataset. We report Align. and FID score of generation results in Tab. 7, where we can observe that LayoutNUWA-CL-DS can obtain the best results on all FID scores and also achieve the best/comparable performance in Align. Score. As for the LayoutNUWACL-DA model, although the performance is not as good as the CL-DS setting, the results are still better than the baseline (some indicators are comparable to the baseline). We hypothesize that this may be due to potential domain confusion, i.e., the model has learned common features from previous datasets, which may have a negative effect on a specific domain. Stronger prior information is required to guide the model to generate layouts that are more consistent with a certain domain, such as element categories, etc.\nFor qualitative evaluation, we render the generation results in Fig. 9. We can observe that the graphic layouts generated by LayoutNUWA are more compact, with a lower failure rate, and their distribution is closer to that of real data. This can be attributed to the given template which defines the output format of the model and the utilization of the prior code knowledge from LLMs.\n3This process is similar to the fixed length provided by the previous works.\nModels RICO PubLayNet Magazine\nAlign. (\u2192) FID (\u2193) Align. (\u2192) FID (\u2193) Align. (\u2192) FID (\u2193)\nLayoutTrans 0.008 12.286 0.015 33.416 0.043 32.083 MaskGIT 0.003 60.724 0.004 28.836 0.003 157.560 DiffusionLM 0.019 23.997 0.016 18.720 0.043 37.951 LayoutDM 0.010 6.456 0.011 13.437 0.037 59.507\nLayoutNUWA-CL-DS (ours) 0.008 5.674 0.007 8.914 0.092 24.108 LayoutNUWA-CL-DA (ours) 0.014 6.932 0.013 9.208 0.073 28.930\nReal Data 0.004 6.250 0.001 1.850 1.693 6.695\nFigure 7: PubLayNet Dataset\nFigure 8: RICO Dataset\nFigure 9: Magazine Dataset\nFigure 9: Cases generated by LayoutNUWA under the Unconditional Generation setting. We suggest zooming in on the monitor for better viewing."
        },
        {
            "heading": "F MODEL PERFORMANCE UNDER MIXED DOMAIN SETTING",
            "text": "Considering the impressiveness of the domain-agnostic model, we also take into account the issue of domain confusion (Long et al., 2022), that is, the data from one domain may affect another domain. Here, we design a toy experiment with the mixed domain setting, that is, applying the domain conditions of Magazine and RICO separately as prefixes as well as the code template behind using data from another domain. For example, we can utilize the below instruction to induce model generation under the Magazine \u2192 RICO mixed domain:\nI want to generate layout in magazine style. Please generate the layout according to the {Task Condition} I provide: {RICO code template}\nAs shown in Tab. 8, we can observe that when given more prior layout information, such as in the C + S \u2192 P setting, the model is less affected by domain confusion as the model tends to \u201cresist\u201d such\ndomain confusion, i.e., The performance of LayoutNUWA is much worse when domain confusion setting is applied compared to when it is not used. However, when there is a lack of layout prior conditions, such as in the C \u2192 S + P or Completion setting, the model is greatly affected by the domain confusion as such issue simply causes minor disturbances to the results, i.e., the model \u201cyields to\u201d the domain confusion under such circumstance. On one hand, this toy experiment reflects the significant impact of prior knowledge of code templates brought to the model, which can make LLM resist such domain confusion. On the other hand, the knowledge of the code template may have overwhelmed some of the information provided by the domain condition, which can lead to the generated content being weakly associated with the domain condition when there is insufficient code template information.\nIn future work, especially when adapting the LLMs to multiple layout distributions simultaneously, more attention needs to be paid to how to make models distinguish different domains by designing stronger constraints."
        },
        {
            "heading": "G MORE TRAINING DETAILS",
            "text": "We deployed and trained the model based on the open-source LLM training framework LLaMA-X4 and HuggingFace5. We optimized the training using DeepSpeed Zero3 technology (Rajbhandari et al., 2020). For the DS settings, we set the learning rate to 5e-5. For the DA settings, we set the learning rate to 5e-6 to prevent model explosion. We trained on 128 V100 units until the loss on the development set converged, at which point we stopped the training. We will open-source all the training details of this article after the anonymization period.\n4https://github.com/AetherCortex/Llama-X 5https://huggingface.co/"
        }
    ],
    "title": "LAYOUTNUWA: REVEALING THE HIDDEN LAYOUT EXPERTISE OF LARGE LANGUAGE MODELS",
    "year": 2024
}