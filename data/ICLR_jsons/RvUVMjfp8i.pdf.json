{
    "abstractText": "Semi-supervised learning (SSL) has emerged as a promising paradigm to alleviate the dependency on abundant labeled data by harnessing the power of unlabeled data. Although many SSL algorithms have been proposed, their performance in practical applications is not robust because the assumption that labeled and unlabeled data are consistent does not hold. In open environments, the sources of labeled and unlabeled data may differ, leading to inconsistent data distributions and even data spaces. This paper points out that previous research on robust SSL has approached the problem from a static perspective, thereby only achieving local robustness rather than global robustness. We reshape the research framework of robust SSL by using the Robustness Analysis Curve (RAC) and the associated metrics defined based on it. Based on these metrics, we build a benchmark that encompasses three types of open environments: inconsistent data distributions, inconsistent label spaces, and inconsistent feature spaces to assess the performance of widely used statistical and deep SSL algorithms with tabular, image, and text datasets. This paper also conducted a detailed analysis, based on experimental results and theory, on how to make SSL algorithms more robust in open environments.",
    "authors": [
        {
            "affiliations": [],
            "name": "A BENCHMARK"
        }
    ],
    "id": "SP:c1545250d8b3e7a04964b4528a539554f050ebd9",
    "references": [
        {
            "authors": [
                "Shai Ben-David",
                "Tyler Lu",
                "D\u00e1vid P\u00e1l"
            ],
            "title": "Does unlabeled data provably help? worst-case analysis of the sample complexity of semi-supervised learning",
            "venue": "In Proceedings of the 21st Annual Conference on Computational Learning Theory, pp",
            "year": 2008
        },
        {
            "authors": [
                "Kristin P Bennett",
                "Ayhan Demiriz",
                "Richard Maclin"
            ],
            "title": "Exploiting unlabeled data in ensemble methods",
            "venue": "In Proceedings of the 8th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp",
            "year": 2002
        },
        {
            "authors": [
                "David Berthelot",
                "Nicholas Carlini",
                "Ekin D Cubuk",
                "Alex Kurakin",
                "Kihyuk Sohn",
                "Han Zhang",
                "Colin Raffel"
            ],
            "title": "Remixmatch: Semi-supervised learning with distribution alignment and augmentation anchoring",
            "venue": "In Proceedings of the 8th International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Avrim Blum",
                "Tom Mitchell"
            ],
            "title": "Combining labeled and unlabeled data with co-training",
            "venue": "In Proceedings of the 11th Annual Conference on Computational Learning Theory, pp",
            "year": 1998
        },
        {
            "authors": [
                "Barbara Caputo",
                "Henning M\u00fcller",
                "Jesus Martinez-Gomez",
                "Mauricio Villegas",
                "Burak Acar",
                "Novi Patricia",
                "Neda Marvasti",
                "Suzan \u00dcsk\u00fcdarl\u0131",
                "Roberto Paredes",
                "Miguel Cazorla"
            ],
            "title": "Imageclef 2014: Overview and analysis of the results",
            "venue": "In Proceedings of 5th International Conference of the Cross-Language Evaluation Forum for European Languages,",
            "year": 2014
        },
        {
            "authors": [
                "Paola Cascante-Bonilla",
                "Fuwen Tan",
                "Yanjun Qi",
                "Vicente Ordonez"
            ],
            "title": "Curriculum labeling: Revisiting pseudo-labeling for semi-supervised learning",
            "venue": "In Proceedings of the AAAI conference on artificial intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Olivier Chapelle",
                "Bernhard Scholkopf",
                "Alexander Zien"
            ],
            "title": "Semi-Supervised Learning",
            "year": 2006
        },
        {
            "authors": [
                "Hao Chen",
                "Ran Tao",
                "Yue Fan",
                "Yidong Wang",
                "Jindong Wang",
                "Bernt Schiele",
                "Xing Xie",
                "Bhiksha Raj",
                "Marios Savvides"
            ],
            "title": "Softmatch: Addressing the quantity-quality trade-off in semi-supervised learning",
            "venue": "In Proceedings of the 12th International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Tianqi Chen",
                "Carlos Guestrin"
            ],
            "title": "Xgboost: A scalable tree boosting system",
            "venue": "In Proceedings of the 22nd ACM SIGKDD Conference on Knowledge Discovery and Data Mining,",
            "year": 2016
        },
        {
            "authors": [
                "Yanbei Chen",
                "Xiatian Zhu",
                "Wei Li",
                "Shaogang Gong"
            ],
            "title": "Semi-supervised learning under class distribution mismatch",
            "venue": "In Proceedings of the 36th AAAI Conference on Artificial Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Ekin D Cubuk",
                "Barret Zoph",
                "Jonathon Shlens",
                "Quoc V Le"
            ],
            "title": "Randaugment: Practical automated data augmentation with a reduced search space",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops,",
            "year": 2020
        },
        {
            "authors": [
                "Zhen Fang",
                "Jie Lu",
                "Feng Liu",
                "Junyu Xuan",
                "Guangquan Zhang"
            ],
            "title": "Open set domain adaptation: Theoretical bound and algorithm",
            "venue": "IEEE Transactions on Neural Networks and Learning Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Lan-Zhe Guo",
                "Yu-Feng Li"
            ],
            "title": "Class-imbalanced semi-supervised learning with adaptive thresholding",
            "venue": "In Proceedings of the 39th International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Lan-Zhe Guo",
                "Zhen-Yu Zhang",
                "Yuan Jiang",
                "Yu-Feng Li",
                "Zhi-Hua Zhou"
            ],
            "title": "Safe deep semisupervised learning for unseen-class unlabeled data",
            "venue": "In Proceedings of the 37th International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Rundong He",
                "Zhongyi Han",
                "Xiankai Lu",
                "Yilong Yin"
            ],
            "title": "Safe-student for safe deep semi-supervised learning with unseen-class unlabeled data",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Zhe Huang",
                "Mary-Joy Sidhom",
                "Benjamin Wessler",
                "Michael C Hughes"
            ],
            "title": "Fix-a-step: Semisupervised learning from uncurated unlabeled data",
            "venue": "In International Conference on Artificial Intelligence and Statistics,",
            "year": 2023
        },
        {
            "authors": [
                "Zhuo Huang",
                "Chao Xue",
                "Bo Han",
                "Jian Yang",
                "Chen Gong"
            ],
            "title": "Universal semi-supervised learning",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Zhuo Huang",
                "Jian Yang",
                "Chen Gong"
            ],
            "title": "They are not completely useless: Towards recycling transferable unlabeled data for class-mismatched semi-supervised learning",
            "venue": "IEEE Transactions on Multimedia,",
            "year": 2022
        },
        {
            "authors": [
                "Lin-Han Jia",
                "Lan-Zhe Guo",
                "Zhi Zhou",
                "Shao Jiejing",
                "Yu Ke-Xiang",
                "Yu-Feng Li"
            ],
            "title": "Bidirectional adaptation for robust semi-supervised learning with inconsistent data distributions",
            "venue": "In Proceedings of the 40th International Conference on Machine Learning,",
            "year": 2023
        },
        {
            "authors": [
                "Lin-Han Jia",
                "Lan-Zhe Guo",
                "Zhi Zhou",
                "Yu-Feng Li"
            ],
            "title": "Lamda-ssl: Semi-supervised learning in python",
            "venue": "Science China Information Sciences,",
            "year": 2023
        },
        {
            "authors": [
                "Thorsten Joachims"
            ],
            "title": "Transductive inference for text classification using support vector machines",
            "venue": "In Proceedings of the 16th International Conference on Machine Learning,",
            "year": 1999
        },
        {
            "authors": [
                "Jogendra Nath Kundu",
                "Naveen Venkat",
                "Ambareesh Revanur",
                "R Venkatesh Babu"
            ],
            "title": "Towards inheritable models for open-set domain adaptation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2020
        },
        {
            "authors": [
                "Samuli Laine",
                "Timo Aila"
            ],
            "title": "Temporal ensembling for semi-supervised learning",
            "venue": "In Proceedings of the 5th International Conference on Learning Representations,",
            "year": 2017
        },
        {
            "authors": [
                "Dong-Hyun Lee"
            ],
            "title": "Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks",
            "venue": "In Proceedings of the 30th International Conference on Machine Learning Workshop on Challenges in Representation Learning,",
            "year": 2013
        },
        {
            "authors": [
                "Yu-Feng Li",
                "Zhi-Hua Zhou"
            ],
            "title": "Towards making unlabeled data never hurt",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 2014
        },
        {
            "authors": [
                "Yu-Feng Li",
                "Lan-Zhe Guo",
                "Zhi-Hua Zhou"
            ],
            "title": "Towards safe weakly supervised learning",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Yinhan Liu",
                "Myle Ott",
                "Naman Goyal",
                "Jingfei Du",
                "Mandar Joshi",
                "Danqi Chen",
                "Omer Levy",
                "Mike Lewis",
                "Luke Zettlemoyer",
                "Veselin Stoyanov"
            ],
            "title": "Roberta: A robustly optimized bert pretraining approach",
            "year": 1907
        },
        {
            "authors": [
                "Andrew Maas",
                "Raymond E Daly",
                "Peter T Pham",
                "Dan Huang",
                "Andrew Y Ng",
                "Christopher Potts"
            ],
            "title": "Learning word vectors for sentiment analysis",
            "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics,",
            "year": 2011
        },
        {
            "authors": [
                "Julian McAuley",
                "Jure Leskovec"
            ],
            "title": "Hidden factors and hidden topics: understanding rating dimensions with review text",
            "venue": "In Proceedings of the 7th ACM conference on Recommender systems,",
            "year": 2013
        },
        {
            "authors": [
                "Takeru Miyato",
                "Andrew M. Dai",
                "Ian J. Goodfellow"
            ],
            "title": "Adversarial training methods for semisupervised text classification",
            "venue": "In Proceedings of the 5th International Conference on Learning Representations,",
            "year": 2017
        },
        {
            "authors": [
                "Takeru Miyato",
                "Shin-ichi Maeda",
                "Masanori Koyama",
                "Shin Ishii"
            ],
            "title": "Virtual adversarial training: a regularization method for supervised and semi-supervised learning",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 2018
        },
        {
            "authors": [
                "Sangwoo Mo",
                "Jong-Chyi Su",
                "Chih-Yao Ma",
                "Mido Assran",
                "Ishan Misra",
                "Licheng Yu",
                "Sean Bell"
            ],
            "title": "Ropaws: Robust semi-supervised representation learning from uncurated data",
            "venue": "In Proceedings of the 11th International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Balas K Natarajan"
            ],
            "title": "On learning sets and functions",
            "venue": "Machine Learning,",
            "year": 1989
        },
        {
            "authors": [
                "Avital Oliver",
                "Augustus Odena",
                "Colin A Raffel",
                "Ekin Dogus Cubuk",
                "Ian Goodfellow"
            ],
            "title": "Realistic evaluation of deep semi-supervised learning algorithms",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Pau Panareda Busto",
                "Juergen Gall"
            ],
            "title": "Open set domain adaptation",
            "venue": "In Proceedings of the IEEE International Conference on Computer Vision,",
            "year": 2017
        },
        {
            "authors": [
                "Xingchao Peng",
                "Ben Usman",
                "Neela Kaushik",
                "Dequan Wang",
                "Judy Hoffman",
                "Kate Saenko"
            ],
            "title": "Visda: A synthetic-to-real benchmark for visual domain adaptation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops,",
            "year": 2018
        },
        {
            "authors": [
                "Kuniaki Saito",
                "Shohei Yamamoto",
                "Yoshitaka Ushiku",
                "Tatsuya Harada"
            ],
            "title": "Open set domain adaptation by backpropagation",
            "venue": "In Proceedings of the European Conference on Computer Vision,",
            "year": 2018
        },
        {
            "authors": [
                "Kuniaki Saito",
                "Donghyun Kim",
                "Kate Saenko"
            ],
            "title": "Openmatch: Open-set semi-supervised learning with open-set consistency regularization",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Behzad M Shahshahani",
                "David A Landgrebe"
            ],
            "title": "The effect of unlabeled samples in reducing the small sample size problem and mitigating the hughes phenomenon",
            "venue": "IEEE Transactions on Geoscience and Remote Sensing,",
            "year": 1994
        },
        {
            "authors": [
                "Kihyuk Sohn",
                "David Berthelot",
                "Nicholas Carlini",
                "Zizhao Zhang",
                "Han Zhang",
                "Colin A Raffel",
                "Ekin Dogus Cubuk",
                "Alexey Kurakin",
                "Chun-Liang Li"
            ],
            "title": "Fixmatch: Simplifying semi-supervised learning with consistency and confidence",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Nasim Souly",
                "Concetto Spampinato",
                "Mubarak Shah"
            ],
            "title": "Semi supervised semantic segmentation using generative adversarial network",
            "venue": "In Proceedings of IEEE International Conference on Computer Vision,",
            "year": 2017
        },
        {
            "authors": [
                "Jong-Chyi Su",
                "Zezhou Cheng",
                "Subhransu Maji"
            ],
            "title": "A realistic evaluation of semi-supervised learning for fine-grained classification",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Antti Tarvainen",
                "Harri Valpola"
            ],
            "title": "Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "VN Vapnik",
                "A Ya Chervonenkis"
            ],
            "title": "On the uniform convergence of relative frequencies of events to their probabilities",
            "venue": "Theory of Probability Its Applications,",
            "year": 1971
        },
        {
            "authors": [
                "Vikas Verma",
                "Kenji Kawaguchi",
                "Alex Lamb",
                "Juho Kannala",
                "Arno Solin",
                "Yoshua Bengio",
                "David Lopez-Paz"
            ],
            "title": "Interpolation consistency training for semi-supervised learning",
            "venue": "Neural Networks,",
            "year": 2022
        },
        {
            "authors": [
                "Feiyu Wang",
                "Qin Wang",
                "Wen Li",
                "Dong Xu",
                "Luc Van Gool"
            ],
            "title": "Revisiting deep semi-supervised learning: An empirical distribution alignment framework and its generalization bound",
            "venue": "arXiv preprint arXiv:2203.06639,",
            "year": 2022
        },
        {
            "authors": [
                "Yidong Wang",
                "Hao Chen",
                "Yue Fan",
                "Wang Sun",
                "Ran Tao",
                "Wenxin Hou",
                "Renjie Wang",
                "Linyi Yang",
                "Zhi Zhou",
                "Lan-Zhe Guo"
            ],
            "title": "Usb: A unified semi-supervised learning benchmark for classification",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Yidong Wang",
                "Hao Chen",
                "Qiang Heng",
                "Wenxin Hou",
                "Yue Fan",
                "Zhen Wu",
                "Jindong Wang",
                "Marios Savvides",
                "Takahiro Shinozaki",
                "Bhiksha Raj"
            ],
            "title": "Freematch: Self-adaptive thresholding for semi-supervised learning",
            "venue": "In Proceedings of the 11th International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Qizhe Xie",
                "Zihang Dai",
                "Eduard Hovy",
                "Thang Luong",
                "Quoc Le"
            ],
            "title": "Unsupervised data augmentation for consistency training",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Qing Yu",
                "Daiki Ikami",
                "Go Irie",
                "Kiyoharu Aizawa"
            ],
            "title": "Multi-task curriculum framework for open-set",
            "year": 2024
        },
        {
            "authors": [
                "Hongyi Zhang",
                "Moustapha Cisse",
                "Yann N Dauphin",
                "David Lopez-Paz"
            ],
            "title": "mixup: Beyond empirical",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Joachims"
            ],
            "title": "semi-supervised support vector machines",
            "venue": "SSL methods Shahshahani & Landgrebe",
            "year": 1994
        },
        {
            "authors": [
                "Lee"
            ],
            "title": "Lee, 2013) firstly make unlabeled samples affect the learning process by assigning pseudo-labels. For consistency regularization methods, Laine et al. (Laine & Aila, 2017) and Tarvainen et al. (Tarvainen & Valpola, 2017) optimize the consistency between current and historical predictions by using the EMA (Exponential Moving Average) mechanism",
            "year": 2022
        },
        {
            "authors": [
                "Oliver et al.(Oliver"
            ],
            "title": "2018) demonstrate that using unlabeled data from unseen classes",
            "year": 2018
        },
        {
            "authors": [
                "Huang"
            ],
            "title": "2022) follow this issue and propose many effective strategies",
            "year": 2022
        },
        {
            "authors": [
                "Panareda Busto",
                "Gall"
            ],
            "title": "2017) conduct research on the issue of open-set (universal) domain adaptation",
            "year": 2017
        },
        {
            "authors": [
                "Pi Model(Laine",
                "Aila"
            ],
            "title": "2017) augments the data twice randomly and uses the results of the two augmentations as inputs of the neural network respectively. The inconsistency of the prediction results is used as the unsupervised loss",
            "year": 2017
        },
        {
            "authors": [
                "Mean Teacher(Tarvainen",
                "Valpola"
            ],
            "title": "2017) relies on the idea of knowledge distillation, where the prediction results of the teacher model are used as pseudo-labels to train the student model to ensure the consistency of the prediction results. It uses EMA for the student model\u2019s parameters as the teacher model",
            "year": 2017
        },
        {
            "authors": [
                "VAT(Miyato"
            ],
            "title": "2018) adds adversarial noise rather than random noise to the data so that the worst performance of the model can be better when the data is affected by noise within a certain range, which corresponds to the zero-sum game in game theory and Min-Max problem in optimization",
            "year": 2018
        },
        {
            "authors": [
                "ICT(Verma"
            ],
            "title": "2022) linearly interpolates data and prediction results by Mixup. The unsupervised loss is obtained by the interpolation consistency",
            "year": 2022
        },
        {
            "authors": [
                "UDA(Xie"
            ],
            "title": "2020) performs data augmentation on the unlabeled samples and then compares the prediction results before and after the augmentation. The thresholds are used for sample selection for both labeled and unlabeled data",
            "year": 2020
        },
        {
            "authors": [
                "FixMatch(Sohn"
            ],
            "title": "2020) uses both strong and weak data augmentation and the inconsistency of prediction results between them is used as the unsupervised loss. A fixed threshold is used for sample selection",
            "year": 2020
        },
        {
            "authors": [
                "FlexMatch(Zhang"
            ],
            "title": "2021) uses a dynamic threshold on the basis of FixMatch. It sets a lower confidence threshold for the classes that are more difficult to learn",
            "year": 2021
        },
        {
            "authors": [
                "FreeMatch(Wang"
            ],
            "title": "2022c) employs a more precise dynamic threshold, where the threshold setting takes into account both the model\u2019s training phase and the disparities between categories. It also incorporates a regularization term to facilitate equitable predictions between categories",
            "year": 2022
        },
        {
            "authors": [
                "SoftMatch(Chen"
            ],
            "title": "2023) no longer adheres to the paradigm of filtering samples through confidence threshold, and instead replaces sample selection with sample weighting. The sample weights are utilized to achieve a better balance between the quantity and quality of pseudo-labeled data",
            "year": 2023
        },
        {
            "authors": [
                "UASD(Chen"
            ],
            "title": "2020) ensembles model predictions to produce probability predictions for unlabeled samples, and uses threshold based on confidence to filter out OOD samples",
            "year": 2020
        },
        {
            "authors": [
                "CAFA(Huang"
            ],
            "title": "2021) takes into account both the inconsistency in labeling spaces and data distributions. It employs a scoring mechanism to filter out samples from new classes and then utilizes unsupervised domain adaptation to alleviate distribution inconsistency, thus obtaining higher-quality pseudo-labels",
            "year": 2021
        },
        {
            "authors": [
                "MTCF(Yu"
            ],
            "title": "2020) leverages the concept of curriculum learning. It uses a joint optimization framework, which updates the network parameters and the OOD score alternately to detect the OOD samples and achieve high performance on the classification simultaneously",
            "year": 2020
        },
        {
            "authors": [
                "Fix A Step(Huang"
            ],
            "title": "2023) views all OOD unlabeled samples as potentially helpful. It modifies gradient descent updates to prevent optimizing a multi-task SSL loss from hurting labeled-set",
            "year": 2023
        },
        {
            "authors": [
                "Natarajan dimension(Natarajan"
            ],
            "title": "1989) is an extension of Vapnik-Chervonen dimension Vapnik & Chervonenkis (1971) in multi-classification problems. We denote Ndim(H) the Natarajan dimension of a hypothesis space H. To simplify the expression, we denote the variance term associated with the hypothesis space complexity in the generalization error with the number of samples n, the number of classes k, and the probability \u03b4",
            "year": 1971
        }
    ],
    "sections": [
        {
            "text": "1 INTRODUCTION\nLabel scarcity is one of the most significant challenges faced by machine learning in practical applications. Semi-supervised learning (SSL)(Chapelle et al., 2006; Oliver et al., 2018), which is a promising paradigm that aims to leverage abundant unlabeled data to achieve excellent performance with scarce labeled data. SSL has attracted considerable attention in recent years and has been implemented in various fields such as image classification (Sohn et al., 2020), semantic segmentation (Souly et al., 2017), and text classification (Miyato et al., 2017).\nIt is a common expectation that utilizing unlabeled data will always improve model performance. However, the reality is that using unlabeled data does not always guarantee performance improvement and can even lead to a significant decrease than using only labeled data (Ben-David et al., 2008; Li & Zhou, 2014; Li et al., 2021). The primary reason is that previous SSL algorithms were designed for closed environments, assuming that labeled and unlabeled data are from the same data space and identically distributed. In many real-world scenarios, however, the sources of labeled and unlabeled data may not be consistent. The more prevalent situation involves dealing with open environments (Zhou, 2022), where data distribution, as well as feature and label spaces, may differ.\nObviously, the scope and value of SSL will be greatly enhanced if unlabeled data from other sources can be effectively utilized. Therefore, addressing the robustness in different challenging environments is a problem that must be tackled. Robust SSL focuses on how to utilize inconsistent unlabeled data from open environments to accomplish SSL, as shown in Figure 1. Some recent works have investigated robust SSL(Guo et al., 2020; Chen et al., 2020; Huang et al., 2021), mainly targeting the situation with inconsistent data distribution and label space.\nResearch on robust SSL is still not mature enough. Many previous works(Huang et al., 2021; Yu et al., 2020; Jia et al., 2023a) have often approached robustness from a static perspective, considering only a single state of unlabeled data and failing to use scientific tools and metrics for robustness analysis, resulting in partial and subjective analysis and evaluation. Furthermore, the current research on robust SSL still has several other shortcomings, including vague definitions, incomplete consideration of inconsistencies, lack of theoretical support, and more.\nTo avoid mistaking adaptability to a specific unlabeled dataset as robustness in an open environment, we denote the degree of inconsistency between labeled and unlabeled data as t and describe robustness as the overall adaptability of an algorithm to all degrees of inconsistency t. For example, in scenarios where all unlabeled data are inconsistent, t can describe the current degree of data distribution shift or the proportion of inconsistent features and categories. In scenarios where both consistent and inconsistent samples coexist, we can also use t to describe the proportion of inconsistent samples. Referring to (Oliver et al., 2018; Chen et al., 2020), we define the function describing the change of accuracy over inconsistency t as Acc(t) and plot it as the Robustness Analysis Curve (RAC). Based on RAC, we derive a series of metrics that describe the performance variations. Among these metrics, the area under the curve (AUC) reflects the overall performance when the inconsistency parameter t follows a uniform distribution. Even in cases where t does not follow a uniform distribution, the expected accuracy (EA) can be used to evaluate the overall performance of the algorithm. The worst-case accuracy (WA) reflects the performance of the algorithm in the worst-case scenario. The expected variation magnitude (EVM) describes the extent of performance variations, while the variation stability (VS) characterizes the stability of performance changes. The robust correlation coefficient (RCC) describes the overall direction of performance changes with respect to t. Based on these metrics, we have provided clear definitions for robustness in open environments. We differentiate between expected robustness defined based on EA, and worst-case robustness defined based on WA.\nBased on the definition of open environments in (Zhou, 2022), we study the robustness of algorithms under three different types of inconsistencies: data distribution, feature space, and label space. This definition is comprehensive and can theoretically describe all variations observed in real-world scenarios. For each type of inconsistency, we evaluate the robustness of commonly used SSL algorithms on tabular, image, and text data.\nWe have conducted a comprehensive analysis of the evaluation results from the perspectives of environments, algorithms, and metrics. For environments, we observed that the inconsistent feature and label spaces often lead to weaker robustness compared to inconsistent data distributions. For algorithms, we compared the strengths and weaknesses of various statistical SSL, classical deep SSL, and robust deep SSL algorithms. For evaluation metrics, we make the following observations: the inconsistency between Acc(0) and AUC, WA, the consistency between EVM and VS, and the indefinite sign of RCC. To investigate how to achieve robustness, we conduct extensive theoretical analyses of robust SSL in open environments. We find that inconsistencies in data distribution, feature space, and label space all affect the model\u2019s generalization ability, and can be alleviated through distribution alignment, feature mapping functions, and sample filtering or weighting, respectively.\nOur Contributions. Our contributions can be summarized in five aspects. Firstly, we have expanded the research scope of robustness SSL from a singular state to global states, encompassing three types of environmental changes. Secondly, we extended the use of the RAC for analyzing robustness and proposed multiple analysis metrics based on RAC. Thirdly, based on these metrics, we provided clear distinctions and definitions for different types of robustness. Fourthly, we evaluated the performance of commonly used SSL algorithms in diverse open environments and data types. Fifthly, we conducted extensive theoretical analysis on generalization error and optimization of robust SSL, providing theoretical guidance for designing robust algorithms.\n2 FORMALIZATION OF ROBUST SSL IN OPEN ENVIRONMENTS\nSSL algorithms leverage both labeled and unlabeled data for the learning process. In a closed environment, all data generated from the consistent distribution P (x, y), x \u2208 X , y \u2208 Y on consistent data space X \u00d7Y \u2286 Rd\u00d7{0, \u00b7 \u00b7 \u00b7 , k\u22121} where d and k respectively represent the number of features and classes. In SSL, we are given nl labeled samples DL = {(xi, yi)|(xi, yi) \u223c P (x, y)}nli=1 and nu unlabeled samples DU = {(xi)|xi \u223c P (x)}nui=1 where P (x) is the marginal distribution of P (x, y). The purpose is to learn a predictor with the smallest generalization error on P (x, y).\nIn an open environment, we assume that all data originates from a global space X \u2217 \u00d7 Y\u2217 \u2286 Rd\u2217 \u00d7{0, . . . , k\u2217\u22121} where d\u2217 and k\u2217 respectively represent the number of features and classes that appear throughout the entire learning process. There exists an invariant data distribution P (y\u2217|x\u2217) for x\u2217 \u2208 X \u2217 and y\u2217 \u2208 Y\u2217. We denote the degree of inconsistency of data distribution, feature space, or label space between unlabeled and labeled data as t \u2208 [0, 1]. A higher t indicates a greater\ninconsistency. For any t, there is an inconsistent distribution denoted as Pt(x\u2217, y\u2217). However, we can only obtain a projected distribution Pt(xt, yt) in a subspace Xt \u00d7 Yt, where Xt \u2282 X \u2217 and Yt \u2282 Y\u2217. We denote \u03b8(t) as the function describing the ratio of inconsistent samples in unlabeled dataset with respect to t. For robust SSL with any t, we are given nl labeled samples from P0(x, y), (1\u2212\u03b8(t)) \u00b7nu consistent unlabeled samples from P0(x) and \u03b8(t) \u00b7 nu inconsistent unlabeled samples from Pt(x).\n3 INTRODUCTION TO RAC AND EVALUATION METRICS\nClassical SSL algorithms only consider the consistent state of unlabeled data with labeled data. Even the majority of Robust SSL algorithms(Huang et al., 2021; Jia et al., 2023a) only consider one of all inconsistent states of unlabeled data and fail to reflect the global robustness. In fact, studying the robustness of algorithms requires a dynamic perspective to investigate the change of algorithm performance with varying degrees of data inconsistency. Currently, there is a lack of adequate tools and evaluation metrics for the analysis and study of robustness. Some works(Oliver et al., 2018; Chen et al., 2020; Cascante-Bonilla et al., 2021) on open-set SSL have plotted curves depicting the change in error rate with variations in label space inconsistency which can effectively reflect global robustness. We expand this type of curve to more complex environments with 3 types of inconsistency. We define the function that describes the change in model accuracy with inconsistency as Acc and plot the accuracy change curve to analyze the robustness, which we refer to as the RAC. The RAC represents the correspondence between the inconsistency t on the horizontal axis and the accuracy Acc(t) on the vertical axis.\nIn order to provide a comprehensive evaluation of SSL algorithms, we have defined multiple metrics to assess the robustness of these algorithms as unlabeled data varies. Unlike previous classical SSL evaluations that only assess Acc(0) and previous robust SSL evaluations that only assess Acc(t) for a specific t, our established evaluation framework based on RAC can reflect global robustness.\n\u2022 Area Under the Curve (AUC): The area under RAC reflects the overall performance of algorithms under different t, considering each t equally important.\nAUC(Acc) = \u222b 1 0 Acc(t)dt (1)\n\u2022 Expected Accuracy (EA): In many scenarios, due to the varying probabilities of different inconsistencies t, we cannot assume that different values of t are equally important. Therefore, we extend the AUC to the expected accuracy when t follows a distribution PT (t). EA\nis equivalent to the inner product of functions Acc and PT . EA(PT , Acc) = \u27e8PT , Acc\u27e9 = \u222b 1 0 PT (t)Acc(t)dt (2) \u2022 Worst-case Accuracy (WA): We aim for the algorithm to perform reasonably well even in the worst-case scenarios. This can be seen as a maximization of the minimum performance, which leads us to define WA to reflect the performance under the worst conditions.\nWA(Acc) = min t\u2208[0,1] Acc(t) (3)\n\u2022 Expected Variation Magnitude (EVM): In robust SSL, we aim for algorithms to maintain relatively stable performance across different inconsistencies. Therefore, we define EVM to measure the average magnitude of performance changes across varying inconsistencies. In the formula, Acc\u2032 represents the derivative of Acc.\nEVM(Acc) = \u222b 1 0 |Acc\u2032(t)|dt (4)\n\u2022 Variation Stability (VS): To measure the stability of change in performance, we define VS to assess how steadily the accuracy changes with t. VS, in fact, is the variance of the first derivative of ACC. The higher the VS, the more random the trend of change.\nV S(Acc) = \u222b 1 0 [Acc\u2032(t)\u2212 ( \u222b 1 0 Acc\u2032(t)dt)]2dt (5)\n\u2022 Robust Correlation Coefficient (RCC): We not only pay attention to the magnitude of performance changes with inconsistency but also care about the direction of these changes. To address this, we define a metric called RCC, which represents the Pearson Correlation Coefficient between accuracy and inconsistency t.\nRCC(Acc) =\n\u222b 1 0 Acc(t) \u00b7 tdt\u2212 \u222b 1 0 Acc(t)dt\u221a\u222b 1\n0 t2dt\u2212 1 \u00b7 \u221a\u222b 1 0 Acc2(t)dt\u2212 ( \u222b 1 0 Acc(t)dt)2\n(6)\nOverall, in open environments, we analyze RAC to reflect the robustness of an algorithm or model as the inconsistency between unlabeled and labeled data changes. Regarding the performance with changes in the environment, we are usually concerned with several questions: whether the overall performance is good enough, whether the worst-case performance is excessively poor, whether there is a significant fluctuation in the performance, whether the trend of performance change is stable, and whether the performance deteriorates as the inconsistency increases. EA and its special case AUC reflect the overall performance. WA reflects the worst-case performance. EVM reflects the magnitude of performance variation. VS reflects the stability of the performance variation. RCC reflects the overall trend of performance variation. These metrics are all defined based on accuracy and can be extended to other different metrics by replacing the function Acc.\n4 DEFINITION OF ALGORITHMIC ROBUSTNESS IN OPEN-ENVIRONMENTS\nDue to the previous work having a static definition of robustness, as well as the confusion caused by different interpretations of the concept of robustness, we have redefined robustness and differentiated between different forms of robustness. It requires the algorithm to obtain corresponding models for different environments to achieve robustness. We have defined the expected robustness and worst-case robustness of algorithms based on the EA and WA metrics respectively.\nIn open environments, the algorithm is used to yield different models for different given values of t. If the algorithm can achieve good excepted performance across various values of t, we consider it to possess expected algorithmic robustness. If the algorithm can achieve not too bad performance for all values of t, we consider it to possess worst-case algorithmic robustness. Definition 1. SSL algorithm A returns a model ft \u2208 F using DL and DtU for any inconsistency level t where F is the hypothesis space of A. Let Acc(t) denote the accuracy of ft on a fixed test dataset. When the inconsistency t follows a distribution PT (t), if there exists \u03c3E such that |Acc(0)\u2212 EA(PT , Acc)| \u2264 \u03c3E , we say that A exhibits \u03c3E-expected algorithmic robustness in the current open environment. If there exists \u03c3W such that |Acc(0)\u2212WA(Acc)| \u2264 \u03c3W , we say that A exhibits \u03c3W -worst-case algorithmic robustness in the current open environment.\n5 BENCHMARK RESULTS\nWe introduced the construction of the Robust SSL Benchmark and presented a large number of experimental results in the appendix.\n5.1 ALGORITHMS USED FOR EVALUATION\nFor statistical SSL algorithms, we selected the most representative algorithms from the four major categories for evaluation. These include the Semi-supervised Gaussian Mixture Model (SSGMM)(Shahshahani & Landgrebe, 1994) from the SSL generative algorithms, TSVM(Joachims et al., 1999) from the semi-supervised support vector machine algorithms, Label Propagation(Zhu & Ghahramani, 2003) and Label Spreading(Zhou et al., 2003) from the graph-based SSL algorithms, Tri-Training(Zhou & Li, 2005) from the disagreement-based SSL algorithms and Assemble(Bennett et al., 2002) from the ensemble-based SSL algorithms.\nFor deep SSL algorithms, we have considered diversity, applicability, and performance and selected 10 classical algorithms: Pseudo Label(Lee, 2013), PiModel(Laine & Aila, 2017), MeanTeacher(Tarvainen & Valpola, 2017), ICT(Verma et al., 2022), VAT(Miyato et al., 2018), UDA(Xie et al., 2020), FixMatch(Sohn et al., 2020), FlexMatch(Zhang et al., 2021), FreeMatch(Wang et al., 2022c) and SoftMatch(Chen et al., 2023). We also have selected 4 robust algorithms: UASD(Chen et al., 2020), CAFA(Huang et al., 2021), MTCF(Yu et al., 2020), and Fix-A-Step(Huang et al., 2023).\n5.2 BASIC SETTING OF THE BECHMARK\nFor all the experiments, we use mainstream supervised learning algorithms as baselines. For tabular data, we use XGBoost(Chen & Guestrin, 2016) as the benchmark for statistical learning algorithms and adopt FT-Transformer(Wang et al., 2022a) as the baseline and backbone for deep learning algorithms. For visual data, we use ResNet50(He et al., 2016) as the baseline and backbone. For text data, we use the RoBERTa(Liu et al., 2019) model as the benchmark and backbone.\nIn deep SSL algorithms, data augmentation is utilized. For image data, we employ random flipping and cropping as weak augmentations and RandAugment(Cubuk et al., 2020) as a strong augmentation. For tabular data, we apply Gaussian noise for data augmentation, and the intensity of the noise is controlled to adjust the strength. For text data, we perform data augmentation through synonym replacement, and the number of replacements is controlled to adjust the strength.\nWe plotted the RAC and performed statistical analysis on various evaluation metrics for different methods. For the plotting of the RAC curve, we sampled six t values [0, 0.2, 0.4, 0.6, 0.8, 1] for all open environments. To ensure reliability, we conducted three experiments for each sampling point with seed values of 0 \u223c 2. The average of these experiments was used to plot the curve. Linear interpolation was performed between adjacent sampling points. Only the RAC shown in Figure 2 and evaluation results shown in Table 1 of partial deep SSL algorithms in the open environment with inconsistent label spaces constructed based on the CIFAR10 dataset are presented in the main text. Other results can be found in the appendix.\nThe evaluation of the algorithm was performed based on the LAMDA-SSL toolkit(Jia et al., 2023b). All experiments of deep SSL algorithms are conducted with 4 NVIDIA GeForce RTX 3090 GPUs and 12 NVIDIA Tesla V100 GPUs.\n5.3 SSL UNDER INCONSISTENT DATA DISTRIBUTIONS\nWe set t as the inconsistency rate between the distributions of labeled and unlabeled data.\nFor tabular data, we evaluated all statistical SSL algorithms and deep SSL algorithms on the iris, wine, and letter datasets. Additionally, we evaluated all deep SSL algorithms on the larger dataset covertype. We calculated the centroids of each class in the data and used the distance between samples and class centroids to filter samples, thus constructing an environment with inconsistent data distribution.\nFor image data, we used the Image-CLEF(Caputo et al., 2014) and VisDA(Peng et al., 2018) datasets, which are commonly used in the field of transfer learning with inconsistent distributions.\nFor text data, we utilized the Amazon reviews(McAuley & Leskovec, 2013) and IMDB movie reviews(Maas et al., 2011) datasets, which have different distributions in sentiment classification, to construct environments with inconsistent distributions.\n5.4 SSL UNDER INCONSISTENT FEATURE SPACES\nWe set t as the inconsistency rate between the feature spaces of labeled and unlabeled data.\nFor tabular data, we used datasets that are consistent with the environment of inconsistent distribution. However, we introduced feature space inconsistency by randomly masking features. Each masked portion was filled with the mean value of the labeled data.\nFor image data, we used the CIFAR10 and CIFAR100 datasets. To create an environment with inconsistent feature space, we converted the images to grayscale, resulting in the loss of two color channels. The missing channels were filled with the preserved channel.\nFor text data, we used the agnews(Zhang et al., 2015) dataset. To construct an environment with inconsistent feature space, we employed text truncation and filled truncated portions with \u201c< pad >\u201d.\n5.5 SSL UNDER INCONSISTENT LABEL SPACES\nWe set t as the inconsistency rate between the labeling spaces of labeled and unlabeled data.\nFor tabular data, we used datasets that are consistent with the environment of inconsistent distribution. We constructed inconsistent labeled space environments by randomly selecting some classes and discarding the labeled data belonging to these classes.\nFor image data, we used the CIFAR10 and CIFAR100 datasets. The construction of inconsistent label space also utilized the same method.\nFor text data, we used the agnews dataset and the construction method is the same too.\n6 ANALYSIS OF EXPERIMENTAL RESULTS\nWe have conducted a comprehensive analysis of the evaluation results from the perspectives of environments, algorithms, and metrics.\n6.1 ANALYSIS OF ENVIRONMENTS\nRefering to table 3, Using the results from all experiments, we have calculated the average expected robustness (under the uniform distribution of PT ) and the average worst-case robustness of SSL algorithms under different inconsistency settings. According to the definition, lower values of \u03c3E and \u03c3W imply stronger robustness. Considering all experimental results, it is not difficult to observe that the robustness of SSL algorithms is much lower in cases where there is inconsistency between the feature space and the label space, compared to cases where there is inconsistency in data distribution. In fact, inconsistencies between the feature and label spaces can both be considered as a greater degree of inconsistency in data distribution. The former can be viewed as a distribution shift where all missing features are assumed to take default values, while the latter can be seen as a distribution shift where the probability of all missing classes for samples is 0.\n6.2 ANALYSIS OF ALGORITHMS\nWe compared the robustness of different algorithms in various environments and conducted an in-depth analysis of both the performance of the algorithms and the reasons behind their performance.\n6.2.1 STATISTICAL SSL METHODS\nRefering to table 4, based on the experiments of statistical SSL algorithms, we found that generative SSL algorithms SSGMM(Shahshahani & Landgrebe, 1994), due to its direct assumption of the form of data distribution, heavily rely on the consistency between labeled and unlabeled data. Its overall performance and robustness in open environments are the poorest among all algorithms. On the other hand, the SVM-based method TSVM(Joachims et al., 1999) relies only on support vectors for boundary determination, graph-based methods Label Propagation(Zhu & Ghahramani, 2003) and Label Spreading(Zhou et al., 2003) rely mainly on smoothness assumption and the disagreementbased method Tri-Training(Zhou & Li, 2005) which enhances fault tolerance through error correction mechanisms. These methods exhibit lower reliance on the identical distribution assumption and possess better robustness. Meanwhile, the ensemble learning-based SSL algorithm Assemble(Bennett et al., 2002), not only demonstrates the best performance but also remarkable robustness, showcasing the advantages of using multiple learners.\n6.2.2 CLASSICAL DEEP SSL METHODS\nRefering to table 5, based on the experiments of classical deep SSL algorithms, we find if the difference between the two components used for calculating inconsistency in the unsupervised loss is small, the algorithm exhibits strong robustness but its overall performance does not vary significantly compared to supervised learning. For instance, PseudoLabel(Lee, 2013) leverages only the inconsistency between soft and hard labels, Pi Model(Laine & Aila, 2017) utilizes the inconsistency caused by weak data augmentation, and MeanTeacher(Tarvainen & Valpola, 2017) utilizes the inconsistency between current predictions and predictions based on the Exponential Moving Average (EMA) mechanism. These algorithms possess notable robustness. VAT(Miyato et al., 2018) which involves adversarial perturbations, achieves a higher performance upper limit, yet its robustness is relatively weaker.\nFor strong data augmentation used in SSL algorithms, Mixup(Zhang et al., 2018), which operates between labeled and unlabeled data, effectively mitigates inconsistency between them by mixture, leading to strong robustness in Mixup-based SSL algorithms such as ICT(Verma et al., 2022). In\ncontrast, the effectiveness of strong data augmentation applied exclusively to unlabeled data relies on the assumption of unchanged labels before and after augmentation. However, in scenarios with significant inconsistency, this assumption might not hold. For example, RandAugment(Cubuk et al., 2020) results in a considerable reduction in robustness.\nIn cases of high inconsistency, a fixed threshold applied to unlabeled data might overly centralize the distribution of unlabeled data, further exacerbating the inconsistency with labeled data. So among the classic deep semi-supervised learning algorithms evaluated, FixMatch(Sohn et al., 2020), which combines RandAugment and a fixed threshold, demonstrates the poorest robustness. Improved algorithms based on FixMatch, such as FlexMatch(Zhang et al., 2021), FreeMatch(Wang et al., 2022c), and SoftMatch(Chen et al., 2023), have enhanced its robustness to some extent through dynamic thresholds or replacing filtering functions with weighting functions. In comparison to FixMatch, UDA(Xie et al., 2020) sets thresholds for both labeled and unlabeled data, mitigating the inconsistency induced by sample selection to a large extent. This significantly improves the robustness of the UDA over FixMatch.\n6.2.3 ROBUST DEEP SSL METHODS\nRefering to table 5, we analyzed the robustness of current mainstream robust SSL algorithms and found that they have yet to strike a balance between upper-performance limits and robustness. While some algorithms exhibit good local robustness, they lack global robustness, and in some cases, perform worse than classical deep SSL algorithms. UASD(Chen et al., 2020) updates the model by leveraging the inconsistency between predictions from the current and previous rounds, without extensive data augmentation, resulting in strong robustness. CAFA(Huang et al., 2021), which builds upon the Pi Model and utilizes Mixup to alleviate inconsistency, also demonstrates notable robustness. These algorithms exhibit significant robustness, yet their overall performance is not exceptionally remarkable. MTCF(Yu et al., 2020) employs a curriculum learning mechanism, gradually adapting the learner to inter-data inconsistency and filtering out unseen class samples, ensuring reasonable worst-case robustness. In our evaluation, Fix-A-Step(Huang et al., 2023) is based on FixMatch by default, which prevents it from showcasing global robustness. Furthermore, it attempts to utilize OOD (Out of Distribution) samples during training, but in cases of extreme data inconsistency, this carries the risk of exacerbating the problem.\n6.3 ANALYSIS OF METRICS\n6.3.1 INCONSISTENCY AMONG ACC(0), AUC, AND WA\nIn many cases, SSL algorithms that have a high Acc(0) may not necessarily have high WA or even AUC, and they might perform even worse than supervised learning. This is because the mechanisms that work well under the identical distribution assumption may not be applicable or even detrimental in non-identically distributed scenarios.\n6.3.2 CONSISTENCY BETWEEN EVM AND VS\nWe have found that the EVM and VS metrics exhibit a high level of consistency, despite their completely different definitions. This indicates that for a robust SSL algorithm, its performance is less sensitive to changes in inconsistency level, and the direction of performance change is more stable and predictable. On the other hand, for a non-robust SSL algorithm, not only does it exhibit larger variations in performance, but the performance changes are also more unstable, showing greater randomness. Using such an algorithm in an open environment is extremely unsafe, as we cannot estimate its worst-case performance.\n6.3.3 UNCERTAINTY SIGNS OF RCC\nIn general understanding, it is commonly believed that non-identically distributed data often leads to a decrease in model performance. However, this is not always the case. Sometimes, moderate differences can actually contribute to improving model performance. This is primarily because, when there is a scarcity of labeled data, the labeled dataset may not fully represent the overall data distribution, resulting in ambiguity in the classification boundaries. In such cases, utilizing slightly shifted unlabeled data can actually yield better discriminative classification boundaries.\n7 THEORETICAL RESEARCH ON ROBUSTNESS\nIn order to analyze how to improve the robustness of algorithms in open environments, we have established a theoretical framework for studying the generalization performance and robustness optimization methods of SSL algorithms in open environments. Jia et al.(Jia et al., 2023b) established a theoretical framework for SSL in the case of different distributions between labeled and unlabeled data. However, it does not consider the situation where the feature space and label space are inconsistent. We extend their theoretical framework to deal with inconsistent data distributions, label spaces, and feature spaces simultaneously.\nFirst, we define the projection operations \u03a0X and \u03a0Y to project data distributions originating from different feature and label spaces onto the same spaces with labeled data. Secondly, we formally define two types of inconsistencies: feature space inconsistency DiscF and label space inconsistency DiscL, both of which represent additional generalization errors caused during the space projection process. Combined with the distribution inconsistency within the same data space DiscD defined in (Jia et al., 2023a), these constitute three types of inconsistencies in total. Finally, we analyze the SSL process in an open environment and ultimately conclude that the generalization error in SSL consists of five components: bias caused by the learner, variance caused by data sampling, and three types of inconsistencies caused by open environments.\nThe conclusions drawn from theoretical analysis are as follows: inconsistencies in data distribution, feature space, and label space can all harm the generalization performance of the model. To alleviate the issue of data distribution inconsistency, it is primarily dependent on aligning the distributions based on the existing predictor, thereby optimizing the term DiscD. To alleviate the issue of feature space inconsistency, it is primarily dependent on the feature mapping function, which requires the learning algorithm to accurately infer unobserved features based on the observed features, thereby optimizing the term DiscF . To alleviate the issue of label space inconsistency, it primarily relies on sample selection and weighting functions, which require robust SSL algorithms to accurately detect and mitigate the negative impact of unfavorable samples, thereby optimizing the term DiscL. The detailed theoretical research on robustness is shown in appendix A.4.\n8 CONCLUSION\nThe research on robust SSL is an essential step toward the practical application of SSL. This article provides a reshaped perspective on the research scope, tools and metrics, definitions, evaluation environments, and theoretical frameworks in this field. The subsequent details about this work will be continuously supplemented and improved, and it will be open-sourced at under the MIT license.\nREFERENCES\nShai Ben-David, Tyler Lu, and Da\u0301vid Pa\u0301l. Does unlabeled data provably help? worst-case analysis of the sample complexity of semi-supervised learning. In Proceedings of the 21st Annual Conference on Computational Learning Theory, pp. 33\u201344, 2008.\nKristin P Bennett, Ayhan Demiriz, and Richard Maclin. Exploiting unlabeled data in ensemble methods. In Proceedings of the 8th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 289\u2013296, 2002.\nDavid Berthelot, Nicholas Carlini, Ekin D Cubuk, Alex Kurakin, Kihyuk Sohn, Han Zhang, and Colin Raffel. Remixmatch: Semi-supervised learning with distribution alignment and augmentation anchoring. In Proceedings of the 8th International Conference on Learning Representations, 2020.\nAvrim Blum and Tom Mitchell. Combining labeled and unlabeled data with co-training. In Proceedings of the 11th Annual Conference on Computational Learning Theory, pp. 92\u2013100, 1998.\nBarbara Caputo, Henning Mu\u0308ller, Jesus Martinez-Gomez, Mauricio Villegas, Burak Acar, Novi Patricia, Neda Marvasti, Suzan U\u0308sku\u0308darl\u0131, Roberto Paredes, and Miguel Cazorla. Imageclef 2014: Overview and analysis of the results. In Proceedings of 5th International Conference of the Cross-Language Evaluation Forum for European Languages, pp. 192\u2013211, 2014.\nPaola Cascante-Bonilla, Fuwen Tan, Yanjun Qi, and Vicente Ordonez. Curriculum labeling: Revisiting pseudo-labeling for semi-supervised learning. In Proceedings of the AAAI conference on artificial intelligence, pp. 6912\u20136920, 2021.\nOlivier Chapelle, Bernhard Scholkopf, and Alexander Zien. Semi-Supervised Learning. The MIT Press, 2006.\nHao Chen, Ran Tao, Yue Fan, Yidong Wang, Jindong Wang, Bernt Schiele, Xing Xie, Bhiksha Raj, and Marios Savvides. Softmatch: Addressing the quantity-quality trade-off in semi-supervised learning. In Proceedings of the 12th International Conference on Learning Representations, 2023.\nTianqi Chen and Carlos Guestrin. Xgboost: A scalable tree boosting system. In Proceedings of the 22nd ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pp. 785\u2013794, 2016.\nYanbei Chen, Xiatian Zhu, Wei Li, and Shaogang Gong. Semi-supervised learning under class distribution mismatch. In Proceedings of the 36th AAAI Conference on Artificial Intelligence, pp. 3569\u20133576, 2020.\nEkin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated data augmentation with a reduced search space. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pp. 702\u2013703, 2020.\nZhen Fang, Jie Lu, Feng Liu, Junyu Xuan, and Guangquan Zhang. Open set domain adaptation: Theoretical bound and algorithm. IEEE Transactions on Neural Networks and Learning Systems, 32(10):4309\u20134322, 2020.\nLan-Zhe Guo and Yu-Feng Li. Class-imbalanced semi-supervised learning with adaptive thresholding. In Proceedings of the 39th International Conference on Machine Learning, pp. 8082\u20138094, 2022.\nLan-Zhe Guo, Zhen-Yu Zhang, Yuan Jiang, Yu-Feng Li, and Zhi-Hua Zhou. Safe deep semisupervised learning for unseen-class unlabeled data. In Proceedings of the 37th International Conference on Machine Learning, pp. 3897\u20133906, 2020.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 770\u2013778, 2016.\nRundong He, Zhongyi Han, Xiankai Lu, and Yilong Yin. Safe-student for safe deep semi-supervised learning with unseen-class unlabeled data. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14585\u201314594, 2022.\nZhe Huang, Mary-Joy Sidhom, Benjamin Wessler, and Michael C Hughes. Fix-a-step: Semisupervised learning from uncurated unlabeled data. In International Conference on Artificial Intelligence and Statistics, pp. 8373\u20138394, 2023.\nZhuo Huang, Chao Xue, Bo Han, Jian Yang, and Chen Gong. Universal semi-supervised learning. In Advances in Neural Information Processing Systems, pp. 26714\u201326725, 2021.\nZhuo Huang, Jian Yang, and Chen Gong. They are not completely useless: Towards recycling transferable unlabeled data for class-mismatched semi-supervised learning. IEEE Transactions on Multimedia, 2022.\nLin-Han Jia, Lan-Zhe Guo, Zhi Zhou, Shao Jiejing, Yu Ke-Xiang, and Yu-Feng Li. Bidirectional adaptation for robust semi-supervised learning with inconsistent data distributions. In Proceedings of the 40th International Conference on Machine Learning, 2023a.\nLin-Han Jia, Lan-Zhe Guo, Zhi Zhou, and Yu-Feng Li. Lamda-ssl: Semi-supervised learning in python. Science China Information Sciences, 2023b.\nThorsten Joachims et al. Transductive inference for text classification using support vector machines. In Proceedings of the 16th International Conference on Machine Learning, pp. 200\u2013209, 1999.\nJogendra Nath Kundu, Naveen Venkat, Ambareesh Revanur, R Venkatesh Babu, et al. Towards inheritable models for open-set domain adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12376\u201312385, 2020.\nSamuli Laine and Timo Aila. Temporal ensembling for semi-supervised learning. In Proceedings of the 5th International Conference on Learning Representations, 2017.\nDong-Hyun Lee. Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks. In Proceedings of the 30th International Conference on Machine Learning Workshop on Challenges in Representation Learning, pp. 896, 2013.\nYu-Feng Li and Zhi-Hua Zhou. Towards making unlabeled data never hurt. IEEE Transactions on Pattern Analysis and Machine Intelligence, 37(1):175\u2013188, 2014.\nYu-Feng Li, Lan-Zhe Guo, and Zhi-Hua Zhou. Towards safe weakly supervised learning. IEEE Transactions on Pattern Analysis and Machine Intelligence, 43(1):334\u2013346, 2021.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.\nAndrew Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, and Christopher Potts. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pp. 142\u2013150, 2011.\nJulian McAuley and Jure Leskovec. Hidden factors and hidden topics: understanding rating dimensions with review text. In Proceedings of the 7th ACM conference on Recommender systems, pp. 165\u2013172, 2013.\nTakeru Miyato, Andrew M. Dai, and Ian J. Goodfellow. Adversarial training methods for semisupervised text classification. In Proceedings of the 5th International Conference on Learning Representations, 2017.\nTakeru Miyato, Shin-ichi Maeda, Masanori Koyama, and Shin Ishii. Virtual adversarial training: a regularization method for supervised and semi-supervised learning. IEEE Transactions on Pattern Analysis and Machine Intelligence, 41(8):1979\u20131993, 2018.\nSangwoo Mo, Jong-Chyi Su, Chih-Yao Ma, Mido Assran, Ishan Misra, Licheng Yu, and Sean Bell. Ropaws: Robust semi-supervised representation learning from uncurated data. In Proceedings of the 11th International Conference on Learning Representations, 2022.\nBalas K Natarajan. On learning sets and functions. Machine Learning, 4(1):67\u201397, 1989.\nAvital Oliver, Augustus Odena, Colin A Raffel, Ekin Dogus Cubuk, and Ian Goodfellow. Realistic evaluation of deep semi-supervised learning algorithms. In Advances in Neural Information Processing Systems, pp. 3239\u20133250, 2018.\nPau Panareda Busto and Juergen Gall. Open set domain adaptation. In Proceedings of the IEEE International Conference on Computer Vision, pp. 754\u2013763, 2017.\nXingchao Peng, Ben Usman, Neela Kaushik, Dequan Wang, Judy Hoffman, and Kate Saenko. Visda: A synthetic-to-real benchmark for visual domain adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pp. 2021\u20132026, 2018.\nKuniaki Saito, Shohei Yamamoto, Yoshitaka Ushiku, and Tatsuya Harada. Open set domain adaptation by backpropagation. In Proceedings of the European Conference on Computer Vision, pp. 153\u2013168, 2018.\nKuniaki Saito, Donghyun Kim, and Kate Saenko. Openmatch: Open-set semi-supervised learning with open-set consistency regularization. In Advances in Neural Information Processing Systems, pp. 25956\u201325967, 2021.\nBehzad M Shahshahani and David A Landgrebe. The effect of unlabeled samples in reducing the small sample size problem and mitigating the hughes phenomenon. IEEE Transactions on Geoscience and Remote Sensing, 32(5):1087\u20131095, 1994.\nKihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao Zhang, Han Zhang, Colin A Raffel, Ekin Dogus Cubuk, Alexey Kurakin, and Chun-Liang Li. Fixmatch: Simplifying semi-supervised learning with consistency and confidence. In Advances in Neural Information Processing Systems, pp. 596\u2013608, 2020.\nNasim Souly, Concetto Spampinato, and Mubarak Shah. Semi supervised semantic segmentation using generative adversarial network. In Proceedings of IEEE International Conference on Computer Vision, pp. 5688\u20135696, 2017.\nJong-Chyi Su, Zezhou Cheng, and Subhransu Maji. A realistic evaluation of semi-supervised learning for fine-grained classification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12966\u201312975, 2021.\nAntti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. In Advances in Neural Information Processing Systems, pp. 1195\u20131204, 2017.\nVN Vapnik and A Ya Chervonenkis. On the uniform convergence of relative frequencies of events to their probabilities. Theory of Probability Its Applications, 16(2):264\u2013280, 1971.\nVikas Verma, Kenji Kawaguchi, Alex Lamb, Juho Kannala, Arno Solin, Yoshua Bengio, and David Lopez-Paz. Interpolation consistency training for semi-supervised learning. Neural Networks, 145: 90\u2013106, 2022.\nFeiyu Wang, Qin Wang, Wen Li, Dong Xu, and Luc Van Gool. Revisiting deep semi-supervised learning: An empirical distribution alignment framework and its generalization bound. arXiv preprint arXiv:2203.06639, 2022a.\nYidong Wang, Hao Chen, Yue Fan, Wang Sun, Ran Tao, Wenxin Hou, Renjie Wang, Linyi Yang, Zhi Zhou, Lan-Zhe Guo, et al. Usb: A unified semi-supervised learning benchmark for classification. In Advances in Neural Information Processing Systems, pp. 3938\u20133961, 2022b.\nYidong Wang, Hao Chen, Qiang Heng, Wenxin Hou, Yue Fan, Zhen Wu, Jindong Wang, Marios Savvides, Takahiro Shinozaki, Bhiksha Raj, et al. Freematch: Self-adaptive thresholding for semi-supervised learning. In Proceedings of the 11th International Conference on Learning Representations, 2022c.\nQizhe Xie, Zihang Dai, Eduard Hovy, Thang Luong, and Quoc Le. Unsupervised data augmentation for consistency training. In Advances in Neural Information Processing Systems, pp. 6256\u20136268, 2020.\nQing Yu, Daiki Ikami, Go Irie, and Kiyoharu Aizawa. Multi-task curriculum framework for open-set semi-supervised learning. In Proceedings of the 16th European Conference on Computer Vision, pp. 438\u2013454, 2020.\nBowen Zhang, Yidong Wang, Wenxin Hou, Hao Wu, Jindong Wang, Manabu Okumura, and Takahiro Shinozaki. Flexmatch: Boosting semi-supervised learning with curriculum pseudo labeling. In Advances in Neural Information Processing Systems, pp. 18408\u201318419, 2021.\nHongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. In Proceedings of the 6th International Conference on Learning Representations, 2018.\nXiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text classification. Advances in neural information processing systems, 2015.\nDengyong Zhou, Olivier Bousquet, Thomas Lal, Jason Weston, and Bernhard Scho\u0308lkopf. Learning with local and global consistency. In Advances in Neural Information Processing Systems, pp. 321\u2013328, 2003.\nJieli Zhou, Baoyu Jing, Zeya Wang, Hongyi Xin, and Hanghang Tong. Soda: Detecting covid-19 in chest x-rays with semi-supervised open set domain adaptation. IEEE/ACM Transactions on Computational Biology and Bioinformatics, 19(5):2605\u20132612, 2021.\nZhi-Hua Zhou. Open-environment machine learning. National Science Review, 9(8):nwac123, 2022.\nZhi-Hua Zhou and Ming Li. Tri-training: Exploiting unlabeled data using three classifiers. IEEE Transactions on Knowledge and Data Engineering, 17(11):1529\u20131541, 2005.\nXiaojin Zhu and Zoubin Ghahramani. Semi-supervised learning using gaussian fields and harmonic functions. In Proceedings of the 20th International Conference on Machine Learning, pp. 912\u2013919, 2003.\nA APPENDIX\nA.1 RELATED WORKS\nSSL spans two important ages: statistical learning and deep learning. At the stage of statistical learning, SSL methods can be categorized into five representative learning paradigms: generative SSL methods Shahshahani & Landgrebe (1994), semi-supervised support vector machines (S3VM) Joachims et al. (1999), graph-based SSL Zhou et al. (2003), disagreement-based SSL Blum & Mitchell (1998) and ensemble-based SSLBennett et al. (2002).\nWith the development of deep learning, deep SSL algorithms have received widespread attention in recent years. Classical deep SSL algorithms mainly have two strategies: pseudo-labeling and consistency regularization. For pseudo-labeling methods, Lee et al. (Lee, 2013) firstly make unlabeled samples affect the learning process by assigning pseudo-labels. For consistency regularization methods, Laine et al. (Laine & Aila, 2017) and Tarvainen et al. (Tarvainen & Valpola, 2017) optimize the consistency between current and historical predictions by using the EMA (Exponential Moving Average) mechanism. Verma et al. (Verma et al., 2022) and Berthelot et al. (Berthelot et al., 2020) optimize the consistency based on Mixup (Zhang et al., 2018). Miyato et al. (Miyato et al., 2018) and Xie et al. (Xie et al., 2020) optimize the consistency between the predictions of original data and augmented data. Sogn et al. (Sohn et al., 2020) propose FixMatch which finds the combination of pseudo-labeling and consistency regularization based on strong augmentation can often achieve better results. Many works (Zhang et al., 2021; Chen et al., 2023; Wang et al., 2022c) have made modifications and improvements to Fixmatch by dynamic adjustment of thresholds or weights, resulting in outstanding performance.\nHowever, classical SSL algorithms usually only perform well when all samples come from the same distribution. To apply SSL techniques to wider applications, there is an urgent need to study robust SSL methods that do not suffer severe performance degradation when unlabeled data is corrupted. Oliver et al.(Oliver et al., 2018) demonstrate that using unlabeled data from unseen classes can actually\nhurt performance compared to not using any unlabeled data at all. Many subsequent works(Yu et al., 2020; Guo et al., 2020; Saito et al., 2021; Cascante-Bonilla et al., 2021; Guo & Li, 2022; He et al., 2022; Huang et al., 2022) follow this issue and propose many effective strategies. Chen et al.(Chen et al., 2020) investigate the SSL scenario with inconsistent label distributions between labeled and unlabeled data. Huang et al.(Huang et al., 2021) and Mo et al.(Mo et al., 2022) propose a framework to solve the problem where both label space and feature distributions are inconsistent. Jia et al. (Jia et al., 2023a) build a theoretical foundation and algorithmic framework for addressing SSL under non-identical distribution. However, many of the above works have limitations in their comprehensive understanding of changes in the environments, as well as in the evaluation of robustness. There are also many works(Zhou et al., 2021; Fang et al., 2020; Kundu et al., 2020; Saito et al., 2018; Panareda Busto & Gall, 2017) conduct research on the issue of open-set (universal) domain adaptation which concerns both inconsistent data distributions and label spaces. However, they all aim to perform as well as possible on the unlabeled target domain at the expense of the performance on the labeled source domain. This contradicts the purpose of SSL, which utilizes a large amount of unlabeled data to alleviate the label scarcity in the source domain.\nZhou(Zhou, 2022) provided a clear definition for open environments, stating that in open environments, data distribution, feature space, and label space may be inconsistent and constantly changing. As this definition of an open environment is comprehensive and can theoretically describe all inconsistent learning scenarios, we have adopted this definition to delineate the scope of robust SSL.\nOliver et al.Oliver et al. (2018) established a classic benchmark for SSL, which has remarkable foresight in considering SSL under scenarios of inconsistent labeling spaces. They utilized a curve depicting error rates as a function of the degree of inconsistency for analysis. However, this work evaluated only 5 algorithms using simple image datasets. Su et al.(Su et al., 2021) build a realistic benchmark using Semi-iNat dataset which contains uncurated unlabeled data of images of birds and other species in the wild. Wang et al.(Wang et al., 2022b). established a benchmark for semisupervised learning on more diverse datasets, but only considered the classic SSL settings within closed environments. Huang et al.(Huang et al., 2023) build a clinically relevant SSL benchmark with different class spaces and feature distributions called Heart2Heart and propose a method called Fix-A-Step.\nA.2 LIMITATIONS\nAlthough this paper has modeled and constructed complex open environments, it should be noted that the complexity of the real world may exceed the dataset we have constructed, and it may be difficult to evaluate, analyze, and explain with limited evaluation metrics and theoretical frameworks. Specifically, there are at least two complex real-world scenarios that our benchmark cannot effectively address.\nFor cases where multiple inconsistencies coexist and their degrees vary simultaneously, a highdimensional vector t rather than a one-dimensional variable t is required to represent the combination of multiple inconsistencies. While the evaluation methods and metrics we employed can be readily extended to high-dimensional cases, this leads to an exponential increase in computational resource consumption, specifically on the order of \u0398(sdim(t)), where s represents the number of samples taken for each dimension. Finding ways to reduce the evaluation complexity in scenarios involving high-dimensional inconsistencies is an urgent and unresolved issue.\nThe evaluation of robustness relies on the definition of inconsistencies. In our framework, t = 0 corresponds to a scenario without any inconsistency, while t = 1 represents the maximum inconsistency situation. This implies that during evaluation, it is necessary to estimate the potential maximum inconsistency that aligns with t = 1. However, real-world inconsistency might surpass our predefined maximum inconsistency value, meaning that scenarios with t > 1 could arise. In such cases, the previous evaluation results would become ineffective.\nA.3 INTRODUCTION TO ALGORITHMS USED IN THE BENCHMARK\nA.3.1 STATISTICAL SSL ALGORITHMS\n1. SSGMM: SSGMM(Shahshahani & Landgrebe, 1994) assumes that data is generated by a Gaussian mixture model, that is, the marginal distribution of samples can be expressed as\nthe result of mixing several Gaussian distributions together, and each Gaussian distribution is given a weight.\n2. TSVM: TSVM(Joachims et al., 1999) infers labels of unlabeled samples and finds dividing hyperplanes that maximize the distance from support vectors. It continuously finds pairs of unlabeled heterogeneous samples and exchanges their labels until no more pairs can be found.\n3. Label Propagation: Label Propagation(Zhu & Ghahramani, 2003) uses samples as nodes, and the relationship between the samples as edges. The purpose of Label Propagation algorithm is to propagate the labels from labeled data to unlabeled data through the graph.\n4. Label Spreading: Label Spreading(Zhou et al., 2003) penalizes misclassified labeled samples rather than banning misclassification completely which is different from Label Propagation fixing labels of labeled samples during the spreading process.\n5. Tri-Training: Tri-Traing(Zhou & Li, 2005) uses three learners with divergence. Tri-Training makes divergence by data sampling. The disagreement between learners is utilized for optimizing interactively.\n6. Assemble: Assemble(Bennett et al., 2002) an extents AdaBoost to the field of SSL by giving pseudo-labels to unlabeled data. It pays more attention to the samples with poor learning effects of the current ensemble learner in each round and continuously improves the robustness using new base learners.\nA.3.2 CLASSICAL DEEP SSL ALGORITHMS\n1. Pseudo Label(Lee, 2013) takes the label with the highest confidence as the pseudo-label and uses cross-entropy obtained from the pseudo-label as the unsupervised loss.\n2. Pi Model(Laine & Aila, 2017) augments the data twice randomly and uses the results of the two augmentations as inputs of the neural network respectively. The inconsistency of the prediction results is used as the unsupervised loss.\n3. Mean Teacher(Tarvainen & Valpola, 2017) relies on the idea of knowledge distillation, where the prediction results of the teacher model are used as pseudo-labels to train the student model to ensure the consistency of the prediction results. It uses EMA for the student model\u2019s parameters as the teacher model.\n4. VAT(Miyato et al., 2018) adds adversarial noise rather than random noise to the data so that the worst performance of the model can be better when the data is affected by noise within a certain range, which corresponds to the zero-sum game in game theory and Min-Max problem in optimization.\n5. ICT(Verma et al., 2022) linearly interpolates data and prediction results by Mixup. The unsupervised loss is obtained by the interpolation consistency.\n6. UDA(Xie et al., 2020) performs data augmentation on the unlabeled samples and then compares the prediction results before and after the augmentation. The thresholds are used for sample selection for both labeled and unlabeled data respectively.\n7. FixMatch(Sohn et al., 2020) uses both strong and weak data augmentation and the inconsistency of prediction results between them is used as the unsupervised loss. A fixed threshold is used for sample selection.\n8. FlexMatch(Zhang et al., 2021) uses a dynamic threshold on the basis of FixMatch. It sets a lower confidence threshold for the classes that are more difficult to learn.\n9. FreeMatch(Wang et al., 2022c) employs a more precise dynamic threshold, where the threshold setting takes into account both the model\u2019s training phase and the disparities between categories. It also incorporates a regularization term to facilitate equitable predictions between categories.\n10. SoftMatch(Chen et al., 2023) no longer adheres to the paradigm of filtering samples through confidence threshold, and instead replaces sample selection with sample weighting. The sample weights are utilized to achieve a better balance between the quantity and quality of pseudo-labeled data.\nA.3.3 ROBUST DEEP SSL ALGORITHMS\n1. UASD(Chen et al., 2020) ensembles model predictions to produce probability predictions for unlabeled samples, and uses threshold based on confidence to filter out OOD samples.\n2. CAFA(Huang et al., 2021) takes into account both the inconsistency in labeling spaces and data distributions. It employs a scoring mechanism to filter out samples from new classes and then utilizes unsupervised domain adaptation to alleviate distribution inconsistency, thus obtaining higher-quality pseudo-labels.\n3. MTCF(Yu et al., 2020) leverages the concept of curriculum learning. It uses a joint optimization framework, which updates the network parameters and the OOD score alternately to detect the OOD samples and achieve high performance on the classification simultaneously.\n4. Fix A Step(Huang et al., 2023) views all OOD unlabeled samples as potentially helpful. It modifies gradient descent updates to prevent optimizing a multi-task SSL loss from hurting labeled-set accuracy.\nA.4 THEORETICAL RESEARCH ON ROBUST SSL\nNatarajan dimension(Natarajan, 1989) is an extension of Vapnik-Chervonen dimension Vapnik & Chervonenkis (1971) in multi-classification problems. We denote Ndim(H) the Natarajan dimension of a hypothesis space H. To simplify the expression, we denote the variance term associated with the hypothesis space complexity in the generalization error with the number of samples n, the number of classes k, and the probability \u03b4:\nvar(H, n, k, \u03b4) =\n\u221a 16Ndim(H) ln \u221a 2nk + 8 ln 2\u03b4\nn (7)\nIn an open environment, we assume that all data originates from a global space X \u2217 \u00d7 Y\u2217 \u2286 Rd\u2217 \u00d7 {0, . . . , k\u2217 \u2212 1}. There exists an invariant data distribution P (y\u2217|x\u2217) for x\u2217 \u2208 X \u2217 and y\u2217 \u2208 Y\u2217. For any t, there is an inconsistent distribution denoted as Pt(x\u2217, y\u2217). According to the total probability theorem, Pt(x\u2217) = \u2211 yi\u2208Y\u2217 [Pt(yi)Pt(x \u2217|yi)] for all x\u2217 \u2208 X \u2217.\nHowever, we can only obtain a projected distribution Pt(xt, yt) in a subspace Xt \u00d7 Yt from the global distribution Pt(x\u2217, y\u2217) in the global space X \u2217 \u00d7 Y\u2217, where Xt \u2282 X \u2217 and Yt \u2282 Y\u2217. We denote X\u0304t = X \u2217/Xt as the unobserved features and Y\u0304t = Y\u2217/Yt as the unobserved classes when the inconsistency rate is t. In this case, the observed inconsistent data follows the distribution\nPt(x) = \u2211 yi\u2208Yt (Pt(yi)Pt(x \u2217|yi)) pt(x\u0304|x) , where x \u2208 Xt \u2286 R dt and x\u0304 \u2208 X\u0304t \u2286 Rd \u2217\u2212dt , according to \u2200yi \u2208 Y\u0304t, Pt(yi) = 0 and Pt(x\u2217) = Pt(x\u0304, x) = Pt(x)Pt(x\u0304|x). In SSL, labeled data are used to train a pseudo-label predictor h \u2208 H : X0 \u2192 Y0 where H is the hypothesis space of pseudo label predictor in order to obtain unlabeled dataset with pseudo-labels, denoted as D\u0303U = {(XU1 , y\u0303U1 ), (XU2 , y\u0303U2 ), . . . , (XUnu , y\u0303 U nu)}. There is also a function w : X0 \u2192 R used for sample weighting or selection. We denote the weighted unlabeled dataset without pseudolabels as DwU = w(DU ) and the weighted unlabeled dataset with noisy pseudo-labels as D\u0303 w U =\nw(D\u0303U ). We denote the sum of weights of all unlabeled samples as nwu = \u2211\n(x,y)\u2208DU w(x). We additionally denote the proportion of inconsistent samples in the unlabeled dataset after sample\nweighting as \u03b8w(t) = \u2211nu i=(1\u2212\u03b8(t))nu+1 w(xi)\u2211nu\ni=1 w(xi) . We define a weighted distribution as the inner product\nof a distribution function and a weighting function, such as Pw(x) = w(x)P (x) and Pw(x, y) = w(x)P (x, y).\nSSL algorithms use both DL and D\u0303wU for training a target predictor f \u2208 F where F is the hypothesis space of the target predictor. Due to different feature spaces, feature mapping functions mapXt\u2192X0 : Xt \u2192 X0 is also required to map the input into the domain of definition of the model. For the distribution Pt(x\u2217), we define its projection onto any target label space Y \u2032 as \u03a0Y\u2032 [Pt(x\u2217)] =\u2211\nyi\u2208Yt I(yi \u2208 Y \u2032)Pt(yi)Pt(x \u2217|yi). The joint distribution after projecting onto the label space Y \u2032 is \u03a0Y\u2032 [Pt(x \u2217, y)] = ( \u2211\nyi\u2208Yt I(yi \u2208 Y \u2032)Pt(yi)Pt(x \u2217|yi))P (y|x\u2217).\nFor the distribution Pt(x\u2217), we define its projection onto any feature space X \u2032 as \u03a0X \u2032 [Pt(x\u2217)] = Pt(mapX\u2217\u2192X \u2032(x\n\u2217)), x\u2217 \u223c Pt(x\u2217). The joint distribution after projecting onto the feature space X \u2032 is \u03a0X \u2032 [Pt(x\u2217, y)] = Pt(mapX\u2217\u2192X \u2032(x\u2217), y), (x\u2217, y) \u223c Pt(x\u2217, y). We define the discrepancy between the distribution Pt(x\u2217) and the label space Y \u2032 as:\nDiscL(Pt(x \u2217),Y \u2032) = 1\u2212 \u222b x\u2217\u223cPt(x\u2217) \u03a0Y\u2032 [Pt(x \u2217)]dx\u2217. (8)\nFor the data distribution Pt(x\u2217) and the observed data Pt(x), we define their discrepancy on an arbitrary feature space X \u2032 with respect to the feature mapping function mapXt\u2192X \u2032 and the model function f defined on the domain of X \u2032 as\nDiscF (Pt(x), Pt(x \u2217),mapXt\u2192X \u2032 , f) = |P(x\u2217,y)\u2208Pt(x\u2217,y)(f(mapX\u2217\u2192X \u2032(x)) \u0338= y)\n\u2212 P(x,y)\u2208Pt(x,y)(f(mapXt\u2192X \u2032(x)) \u0338= y)|. (9)\nFor two data distributions P \u2032(x, y) and P \u2032\u2032(x, y) defined on the same feature space and label space, their distributional difference with respect to the model function f can be defined by the following discrepancy:\nDiscD(P \u2032(x, y), P \u2032\u2032(x, y), f) = |P(x,y)\u223cP \u2032(x,y)(f(x) \u0338= y)\u2212 P(x,y)\u223cP \u2032\u2032(x,y)(f(x) \u0338= y)| (10)\nAs a result, we can obtain the error rate of pseudo-labeling in the weighted or filtered unlabeled dataset obtained through robust SSL.\nTheorem A.1. For any pseudo-label predictor h \u2208 H, 0 \u2264 \u03b41 \u2264 1 and 0 \u2264 \u03b42 \u2264 1, with the probability of at least (1\u2212 \u03b41)(1\u2212 \u03b42):\nE\u0302(h,w,mapXt\u2192X0 , DUt)\n\u2264E\u0302(h,DL) + var(H, nl, k0, \u03b41) + var(H, nwu t, k0, \u03b42) + \u03b8 w(t)DiscL(P w t (x \u2217),Y0) +\u03b8w(t)DiscF (\u03a0Y0 [P w t (x)],\u03a0Y0 [P w t (x \u2217)],mapXt\u2192X0 , h) +\u03b8w(t)DiscD(\u03a0X0 [\u03a0Y0 [P w t (x \u2217, y)]], P0(x, y), h) (11)\nwhere E\u0302(h,DL) is the empirical error of h on DL and E\u0302(h,w,mapXt\u2192X0 , DUt) is the empirical error of h on DU with ground truth labels.\nBased on the above label noise rate bound of the unlabeled dataset, we can estimate the generalization error bound of the robust SSL algorithm trained with the labeled dataset and this unlabeled dataset.\nTheorem A.2. For any target predictor f \u2208 F , pseudo-label predictor h \u2208 H, 0 \u2264 \u03b41 \u2264 1, 0 \u2264 \u03b42 \u2264 1 and 0 \u2264 \u03b43 \u2264 1, with the probability of at least (1\u2212 \u03b41)(1\u2212 \u03b42)(1\u2212 \u03b43):\nE(f, P0(x, y)|h,w,mapXt\u2192X0 , DL, DUt)\n\u2264 nl nl + nwu t E\u0302(f,DL) + nwu t nl + nwu t E\u0302(f, D\u0303wUt) + var(F , nl + n w u t, k0, \u03b41) + nwu t\nnl + nwu t (\u03b8w(t)DiscL(P w t (x \u2217),Y0)\n+\u03b8w(t)DiscF (\u03a0Y0 [P w t (x)],\u03a0Y0 [P w t (x \u2217)],mapXt\u2192X0 , f) +\u03b8w(t)DiscD(\u03a0X0 [\u03a0Y0 [P w t (x \u2217, y)]], P0(x, y), f)) + nwu t\nnl + nwu t (E\u0302(h,DL) + var(H, nl, k, \u03b42) + var(H, nwu t, k0, \u03b43)\n+\u03b8w(t)DiscL(P w t (x \u2217),Y0) + \u03b8w(t)DiscF (\u03a0Y0 [Pt(x)],\u03a0Y0 [Pwt (x\u2217)],mapXt\u2192X0 , h) +\u03b8w(t)DiscD(\u03a0X0 [\u03a0Y0 [P w t (x \u2217, y)]], P0(x, y), h)) (12)\nwhere E\u0302(f, D\u0303wUt) is the weighted disagreement rate between the noisy pseudo-labels and the prediction results of f on the weighted unlabeled dataset D\u0303wUt).\nUsing the results from all experiments, we have calculated the average expected robustness (under the uniform distribution of PT ) and the average worst-case robustness of SSL algorithms under different inconsistency settings. About the results, please refer to table 3.\nA.5.2 STATISTICS FOR ROBUSTNESS OF DIFFERENT ALGORITHMS\nWe computed the average expected robustness and average worst-case robustness of statistical SSL and deep SSL methods separately under experiment settings that are common. This was done to evaluate the differences in robustness between different algorithms.\nAbout the average robustness of statistical SSL algorithms, please refer to table 4.\nAbout the average robustness of deep SSL algorithms, please refer to table 5.\nA.6 DETAILED SETTINGS\nA.6.1 DATASETS\nData Distribution\n1. Wine, Iris, Letter, Covertype: To construct datasets with inconsistent distributions, in each class, we calculate the center of all samples and sort these samples according to the distance between them and the center in ascending order. The first nc \u2217 0.5 samples are used as labeled data which can be which can be regarded as being obtained by sampling from P0(x, y). and the rest nc \u2217 0.5 samples are used as inconsistent unlabeled data. For each t, the nc \u2217 0.5 \u2217 (t\u2212 1s ) to nc \u2217 0.5 \u2217 t samples are used as labeled data which can which can be regarded as being obtained by sampling from Pt(x). \u03b8(t) = 1 for every t. 5 or 10 perclass samples of source domain data are used as labeled data and the rest are used as test data.\n2. Image-CLEF, VisDA: The Image-CLEF dataset consists of 3 domains, which can be combined into 6 source-domain to target-domain pairs. All source domain data can be regarded as being obtained by sampling from Psource(x, y) and all target domain data can be regarded as being obtained by sampling from Ptarget(x, y). We set P0(x, y) = Psource(x, y) and Pt(x, y) = Ptarget(x, y) for all t \u0338= 0. From the source-domain data, 100 samples are taken as labeled data. Half of the remaining source-domain samples are used as test data, while the other half is combined with the target-domain data to form an unlabeled dataset. The total number of unlabeled data nu is min(0.5 \u2217 (ns \u2212 100), nt) where ns is the number of samples in the source domain and nt is the number of samples in the target domain. \u03b8(t) = t\n3. IMDB-Amazon: The IMDB and Amazon dataset can be considered as source and target domains respectively. All source domain data can be regarded as being obtained by sampling from Psource(x, y) and all target domain data can be regarded as being obtained by sampling from Ptarget(x, y). We set P0(x, y) = Psource(x, y) and Pt(x, y) = Ptarget(x, y) for all t \u0338= 0. From the source-domain data, 100 samples are taken as labeled data. Half of the remaining source-domain samples are used as test data, while the other half is combined with the target-domain data to form an unlabeled dataset. The total number of unlabeled data nu is min(0.5 \u2217 (ns \u2212 100), nt) where ns is the number of samples in the source domain and nt is the number of samples in the target domain. \u03b8(t) = t for every t. For inconsistency rate t, the unlabeled dataset is combined with nu \u2217 (1\u2212 t) samples for the source domain and nu \u2217 t samples from the target domain.\nFeature Space\n1. Wine, Iris, Letter, Covertype: 50% of all samples can be used as source domain data, and the rest are used as target domain data. 5 or 10 samples perclass of source domain data are used as labeled data which can be regarded as being obtained by sampling from P0(x, y), and the rest are used as test data. For every t, All target domain data randomly dropping t \u2217 d features are used as unlabeled data which can be regarded as being obtained by sampling from Pt(x, y). \u03b8(t) = 1 for every t.\n2. CIFAR10, CIFAR100: 50% of all samples can be used as source domain data which can be regarded as being obtained by sampling from Psource(x, y), and the rest are used as target domain data. 20 samples perclass of source domain data are used as labeled data. All target domain data are transformed to grey images by dropping 2 channels which can be regarded as being obtained by sampling from Ptarget(x, y). We set P0(x, y) = Psource(x, y) and Pt(x, y) = Ptarget(x, y) for all t \u0338= 0. \u03b8(t) = t for every t. For inconsistency rate t, the unlabeled dataset is combined with nu \u2217 (1\u2212 t) samples for the source domain and nu \u2217 t samples from the target domain.\n3. Agnews: 50% of all samples can be used as source domain data which can be regarded as being obtained by sampling from Psource(x, y), and the rest are used as target domain data. 100 samples of source domain are used as labeled data and the rest are used as test data. 50% target domain sentences are used as IID samples and the other 50% target domain sentences that drop 50% tokens are used as OOD samples which can be regarded as being obtained by sampling from Ptarget(x, y). We set P0(x, y) = Psource(x, y) and Pt(x, y) = Ptarget(x, y) for all t \u0338= 0. The number of unlabeled data nu is set to min(nI/(1\u2212 t), nO/t where nI and nD are the numbers of IID and OOD samples respectively. The unlabeled dataset is combined with nu \u2217 (1\u2212 t) IID and nu \u2217 t OOD samples. \u03b8(t) = t for every t.\nLabel Space\n1. Wine, Iris, Letter, Covertype: 50% of all samples can be used as source domain data, and the rest are used as target domain data. (k + 1)//2 classes of source data are saved and the rest samples are dropped which can be regarded as being obtained by sampling from Psource(x, y). 5 or 10 samples perclass of saved source domain data are used as labeled data and the rest are used as test data. The target domain samples with saved classes are used as OOD samples which can be regarded as being obtained by sampling from Ptarget(x, y), and the target samples with dropped classes are used as IID samples. The number of unlabeled date nu is set to min(nI/(1\u2212 t), nO/t) where nI and nD are the numbers of IID and OOD samples respectively. The unlabeled dataset is combined with nu \u2217 (1\u2212 t) IID and nu \u2217 t OOD samples. We set P0(x, y) = Psource(x, y) and Pt(x, y) = Ptarget(x, y) for all t \u0338= 0. \u03b8(t) = t for every t.\n2. CIFAR10, CIFAR100: (k + 1)/2 classes of all samples are used as source domain data, and the rest are used as target domain data. 20 samples per class of the source domain are used as labeled data. For inconsistency rate t, the unlabeled dataset is combined with nt \u2217 (1 \u2212 t) samples for the source domain and nt \u2217 t samples from the target domain where nt is the number of target domain samples. We set P0(x, y) = Psource(x, y) and Pt(x, y) = Ptarget(x, y) for all t \u0338= 0. \u03b8(t) = t for every t.\n3. Agnews: (k + 1)/2 classes of all samples are used as source domain data and the rest are used as target domain data. 100 samples of the source domain are used as labeled data. For inconsistency rate t, the unlabeled dataset is combined with nt \u2217 (1 \u2212 t) samples for the source domain and nt \u2217 t samples from the target domain where nt is the number of target domain samples. We set P0(x, y) = Psource(x, y) and Pt(x, y) = Ptarget(x, y) for all t \u0338= 0. \u03b8(t) = t for every t.\nA.6.2 BASELINES\n1. XGBoost: the parameter of uselabelencoder is set to False, the parameter evalmetric is set to \u201dlogloss\u201d.\n2. FT-Transformers: the number of layers is set to 8, the dimension of tokens is set to 192, and the number of heads is set to 8.\n3. ResNet50: the Resnet50 pre-trained on ImageNet from torchvision.models is directly used.\n4. Roberta: the pre-trained model \u201droberta-base\u201d from transformers package is directly used.\nA.6.3 STATISTICAL METHODS\n1. SSGMM: the number of iterations is set to 300.\n2. TSVM: the parameter Cl is set to 15, the parameter Cu is set to 0.0001, and the method to deal with multi-classification tasks is set to \u201done vs rest\u201d.\n3. Label Propagation: the hyperparameters provided by scikit-learn in default are used.\n4. Label Spreading: the hyperparameters provided by scikit-learn in default are used.\n5. Tri-Training: all the base learners are set to XGBoost classifier consistent with the baseline.\n6. Assemble: the number of iterations T is set to 30, and all the base learners are set to XGBoost classifier consistent with the baseline.\nA.6.4 CLASSICAL DEEP METHODS\n1. PseudoLabel: the ratio of unsupervised loss \u03bbu is set to 1.0, and the threshold is set to 0.95.\n2. Pi Model: the ratio of unsupervised loss \u03bbu is set to 1.0, the warmup rate of unsupervised loss wu is set to 0.4, and the ratio of unsupervised loss \u03bbu is set to max( tT \u00b7w , 1.0) where t is current iteration and T is the number of iterations.\n3. Mean Teacher: the EMA decay is set to 0.999, the warmup rate of unsupervised loss wu is set to 0.4, and the ratio of unsupervised loss \u03bbu is set to max( tT \u00b7w , 1.0) where t is current iteration and T is the number of iterations.\n4. VAT: the ratio of unsupervised loss \u03bbu is set to 0.3, the ratio of entropy minimization loss \u03bbentmin is set to 0.06, the number of iterations for adversarial training itvat is set to 1, the degree of adversarial noise is set to 6.\n5. ICT: the ratio of unsupervised loss \u03bbu is set to 100, the parameter of Beta distribution in Mixup is set to 0.5.\n6. UDA: the ratio of unsupervised loss \u03bbu is set to 1.0, the threshold is set to 0.8, and the temperature of softmax is set to 0.4.\n7. FixMatch: the ratio of unsupervised loss \u03bbu is set to 1.0, the threshold is set to 0.95, and the temperature of softmax is set to 0.5.\n8. FlexMatch: the ratio of unsupervised loss \u03bbu is set to 1.0, the basic threshold is set to 0.95, the temperature of softmax is set to 0.5, and the threshold warmup mechanism is used.\n9. FreeMatch: the ratio of unsupervised loss \u03bbu is set to 1.0, the EMA decay is set to 0.999, the threshold is set to 0.95, the temperature of softmax is set to 0.5.\n10. SoftMatch: the ratio of unsupervised loss \u03bbu is set to 1.0, the basic threshold is set to 0.95, the temperature of softmax is set to 0.5, and the distribution alignment mechanism is used.\nA.6.5 ROBUST DEEP METHODS\n1. UASD: the ratio of unsupervised loss \u03bbu is set to 1.0, the threshold is set to 0.95. 2. CAFA: the base SSL algorithm used is Pi Model, the warmup rate of unsupervised loss\nwu is set to 415 , The perturbation magnitude \u03f5 is set to 0.014 and the Beta distribution parameter \u03b1 is set to 0.75, the warmup rate of adversarial loss wa is set to 815 , the ratio of unsupervised loss \u03bbu is exp(\u22125 \u00b7 (1\u2212min( tT \u00b7wu , 1.0))\n2) and the ratio of adversarial loss \u03bba is exp(\u22125 \u00b7 (1\u2212min( tT \u00b7wa , 1.0)) 2) in the t-th iteration where T is the number of iterations.\n3. MTCF: the ratio of unsupervised loss \u03bbu is set to 75, the temperature T is set to 0.5, the parameter of Beta distribution in Mixup is set to 0.75.\n4. Fix A Step: the parameter of Beta distribution in Mixup is set to 0.75, FixMatch is set to the base SSL method, and all the hyperparameters are the same as FixMatch.\nA.6.6 DATA AUGMENTATION\n1. agnews and imdb/amazon: the weak and strong augmentations are synonyms replacement with 1 and 5 words respectively.\n2. wine, iris, letter, covertype: the weak and strong augmentations are Gaussian noise with 0.1 and 0.2 rates respectively.\n3. CIFAR10, CIFAR100, VisDA, Image-CLEF: the weak augmentation is RandomHorizontalFlip, and the strong augmentation is RandAugment.\nA.6.7 OTHERS\n1. batch size: the batch size for imdb/amazon dataset is 8, the batch size for agnews dataset is 16, the batch size for Image-CLEF and VisDA dataset is 32, the batch size for CIFAR10 and CIFAR100 is 64, the batch size for tabular datasets is 64.\n2. iteration: the iteration for Image-CLEF and VisDA dataset is 2000, the iteration for tabular dataset is set to 1000, the iteration for ag news and imdb/amazon is set to 5000, the iteration for CIFAR10 and CIFAR100 is 100000.\n3. optimizer: the optimizer for all datasets is SGD with learning rate 5e-4 and momentum 0.9. 4. scheduler: the scheduler for all datasets is CosineWarmup with num cycles 7/16.\nA.7 FORMULA DERIVATION AND THEORETICAL PROOF\nA.7.1 DERIVATION OF EVALUATION METRICS\nSince the formulas for the metrics AUC, EA, WA, and EVM can be directly obtained through definitions, we primarily focus on deriving the formulas for the metrics VS and RCC.\n1. VS: According to the definition, the metric VS is used to measure the stability of model performance changes, that is whether the derivative of Acc(t), Acc\u2032(t), fluctuates significantly. We describe the magnitude of fluctuations using the variance, and thus, VS is defined as the variance of Acc\u2032(t). We denote the expectation of the variable x as E(x) and the standard deviation of the variable x as \u03c3(X).\nV S(Acc)\n=\u03c32(Acc\u2032)\n= \u222b 1 0 [Acc\u2032(t)\u2212 E(Acc\u2032)]2dt\n= \u222b 1 0 [Acc\u2032(t)\u2212 ( \u222b 1 0 Acc\u2032(t)dt)]2dt (13)\n2. RCC: According to the definition, the metric RCC is used to measure the correlation between model performance and the inconsistency factor t. The Pearson correlation coefficient effectively quantifies the correlation between two variables. We denote the covariance between the variables x and Y as COV (x) and the Pearson correlation coefficient between the variables x and Y as \u03c1(X,Y ).\n\u03c1(X,Y )\n= COV (X,Y )\n\u03c3(X)\u03c3(Y )\n= E(XY )\u2212 E(X)E(Y )\u221a E(X2)\u2212 E2(X) \u221a E(Y 2)\u2212 E2(Y )\n(14)\nWe can directly apply the formula for the Pearson correlation coefficient.\nRCC(Acc)\n=\u03c1(Acc, t) = E(Acc \u00b7 t)\u2212 E(Acc)E(t)\u221a\nE(Acc2)\u2212 E2(Acc) \u221a E(t2)\u2212 E2(t)\n=\n\u222b 1 0 Acc(t) \u00b7 tdt\u2212 \u222b 1 0 Acc(t)dt \u222b 1 0 tdt\u221a\u222b 1\n0 Acc2(t)dt\u2212 ( \u222b 1 0 Acc(t)dt)2 \u00b7 \u221a\u222b 1 0 t2dt\u2212 ( \u222b 1 0 tdt)2\n=\n\u222b 1 0 Acc(t) \u00b7 tdt\u2212 \u222b 1 0 Acc(t)dt\u221a\u222b 1\n0 Acc2(t)dt\u2212 ( \u222b 1 0 Acc(t)dt)2 \u00b7 \u221a\u222b 1 0 t2dt\u2212 1\n(15)\nA.7.2 PROOF OF THEORETICAL RESULTS\nProof of Theorem 1\nIn the case of using only nl labeled samples for supervised learning, for any h \u2208 H and 0 \u2264 \u03b41 \u2264 1, with the probability of at least 1\u2212 \u03b41:\nE(h, P0(x, y)) \u2264 E\u0302(h,DL) + var(H, nl, k, \u03b41) (16)\nwhere E\u0302(h,DL) is the empirical error of h on the dataset DL and E(f, P0(x, y)) is the generalization error of f on the distribution of labeled data P0(x, y).\nIn SSL, when all samples are from the same distribution, for any t, dataset DUt with nu samples are from the same distribution Pt(x, y) = P0(x, y). For any h \u2208 H and 0 \u2264 \u03b42 \u2264 1, with the probability of at least 1\u2212 \u03b42:\nE\u0302(h,DtU ) \u2264 E(h, Pt(x, y)) + var(H, nu, k, \u03b42) = E(h, P0(x, y)) + var(H, nu, k, \u03b42) (17)\nAccording to eqs. (16) and (17), for any pseudo-label predictor h \u2208 H, 0 \u2264 \u03b41 \u2264 1 and 0 \u2264 \u03b42 \u2264 1, with the probability of at least (1\u2212 \u03b41)(1\u2212 \u03b42):\nE\u0302(h,DUt) \u2264 E\u0302(h,DL) + var(H, nl, k, \u03b41) + var(H, nu, k, \u03b42) (18)\nWhen labeled data and unlabeled data are from different distributions, for any h \u2208 H: E(h, Pt(x, y))\n\u2264E(h, P0(x, y)) + |Px,y\u223cP0(x,y)(h(x) \u0338= y)\u2212 Px,y\u223cPt(x,y)(h(x) \u0338= y)| =E(h, P0(x, y)) +Disc(h, P0(x, y), Pt(x, y)) (19)\nAccording to eqs. (16), (17) and (19), for any pseudo-label predictor h \u2208 H, 0 \u2264 \u03b41 \u2264 1 and 0 \u2264 \u03b42 \u2264 1, with the probability of at least (1\u2212 \u03b41)(1\u2212 \u03b42):\nE\u0302(h, Pt(x, y))\n\u2264E(h, P0(x, y)) +DiscD(P0(x, y), Pt(x, y), h) \u2264E\u0302(h,DL) + var(H, nl, k, \u03b41) + var(H, nu, k, \u03b42) +DiscD(P0(x, y), Pt(x, y), h) (20)\nTaking into account that in SSL, a weighting function w is often used to either weigh or filter unlabeled samples, it\u2019s the weighted unlabeled data that truly plays a role in the learning process.\nE\u0302(h,DUt , w)\n\u2264E(h, P0(x, y)) +DiscD(P0(x, y), w(Pt(x, y)), h) \u2264E\u0302(h,DL) + var(H, nl, k, \u03b41) + var(H, nwu t, k, \u03b42) +DiscD(P0(x, y), P w t (x, y), h) (21)\nNow considering that labeled and unlabeled data are not only from inconsistent data distributions but also inconsistent data spaces, we need an extra feature mapping function to complete the features and an extra weighting function to filter out samples from new classes. Both the mapping function and the weighting function aim to project unlabeled data to the same space as labeled data.\nAccording to eqs. (8), (9) and (21), for any pseudo-label predictor h \u2208 H, 0 \u2264 \u03b41 \u2264 1 and 0 \u2264 \u03b42 \u2264 1, with the probability of at least (1\u2212 \u03b41)(1\u2212 \u03b42):\nE\u0302(h,w,mapXt\u2192X0 , DUt)\n\u2264E\u0302(h,DL) + var(H, nl, k0, \u03b41) + var(H, nwu t, k0, \u03b42) + \u03b8 w(t)DiscL(P w t (x \u2217),Y0) +\u03b8w(t)DiscF (\u03a0Y0 [P w t (x)],\u03a0Y0 [P w t (x \u2217)],mapXt\u2192X0 , h) +\u03b8w(t)DiscD(P0(x, y),\u03a0X0 [\u03a0Y0 [P w t (x \u2217, y)]], h) (22)\nwhere E\u0302(h,DL) is the empirical error of h on DL and E\u0302(h,w,mapXt\u2192X0 , DUt) is the empirical error of h on DUt with ground truth labels.\nProof of Theorem 2\nWe denote the mixture of two distributions D1 and D2 with proportion \u03b1 as: Mix\u03b1(D1,D2) = \u03b1D1 + (1\u2212 \u03b1)D2 (23)\nIn SSL with inconsistent distributions, the target predictor is trained with both labeled dataset DL and weighted unlabeled dataset with noisy pseudo-labels D\u0303Ut . DL and D\u0303Ut can be considered as a mixed dataset with nl + nut samples from the mixed distribution Mix nl\nnl+nut\n(P0(x, y), Pt(x, y))\nwhose noisy rate is nutnl+nut E\u0302(h,DUt).\nSo, for any target predictor f \u2208 F , pseudo-label predictor h \u2208 H, 0 \u2264 \u03b43 \u2264 1, with the probability of at least 1\u2212 \u03b43:\nE(f,Mix nl nl+nut\n(P0(x, y), Pt(x, y))|h,DL, DU )\n\u2264 nl nl + nut E\u0302(f,DL) + nut nl + nut E\u0302(f, D\u0303U ) + var(F , nl + nut t, k, \u03b43) + nut nl + nut E\u0302(h,DU )\n(24)\nwhere E(f,Mix nl nl+nut (P0(x, y), Pt(x, y))|h,DL, DU ) is the generalization error of f on the distribution Mix nl\nnl+nut\n(P0(x, y), Pt(x, y) corresponding to pseudo-label predictor h.\nWhen labeled data and unlabeled data are from different distributions, for any f \u2208 F :\nE(f, P0(x, y)|h,DL, DUt) \u2264E(f,Mix nl\nnl+nut\n(P0(x, y), Pt(x, y))|h,DL, DUt)\n+|px,y\u223cP0(x,y)(h(x) \u0338= y)\u2212 px,y\u223cMix nl nl+nut (P0(x,y),Pt(x,y)(h(x) \u0338= y)|\n=E(f,Mix nl nl+nut (P0(x, y), Pt(x, y))|h,DL, DUt)\n+DiscD(f, P0(x, y),Mix nl nl+nut (P0(x, y), Pt(x, y)) (25)\nAccording to eqs. (21), (24) and (25), for any target predictor f \u2208 F , pseudo-label predictor h \u2208 H, 0 \u2264 \u03b41 \u2264 1, 0 \u2264 \u03b42 \u2264 1 and 0 \u2264 \u03b43 \u2264 1, with the probability of at least (1\u2212 \u03b41)(1\u2212 \u03b42)(1\u2212 \u03b43):\nE(f, P0(x, y)|h,DL, DUt) \u2264E(f,Mix nl\nnl+nut\n(P0(x, y), Pt(x, y))|h,DL, DUt)\n+DiscD(f, P0(x, y),Mix nl nl+nut (P0(x, y), Pt(x, y))) \u2264 nl nl + nut E\u0302(f,DL) + nut nl + nut E\u0302(f, D\u0303Ut) + var(F , nl + nut , k, \u03b43) + nut\nnl + nut E\u0302(h,DUt) +DiscD(f, P0(x, y),Mix nl nl+nut (P0(x, y), Pt(x, y)))\n\u2264 nl nl + nut E\u0302(f,DL) + nut nl + nut E\u0302(f, D\u0303Ut) + var(F , nl + nut , k, \u03b41)\n+DiscD(f, P0(x, y),Mix nl nl+nut (P0(x, y), Pt(x, y))) + nut\nnl + nut (E\u0302(h,DL) + var(H, nl, k, \u03b42) + var(H, nut , k, \u03b43) +DiscD(h, P0(x, y), Pt(x, y)))\n(26)\nwhere E\u0302(f, D\u0303Ut) is the weighted empirical inconsistency rate between the noisy pseudo-labels and the prediction results of f on the unlabeled dataset D\u0303Ut .\nTaking into account inconsistent label spaces and weighting function w:\nE(f, P0(x, y)|h,DL, DUt , w)\n\u2264 nl nl + nwut E\u0302(f,DL) + nwut nl + nwut E\u0302(f, D\u0303wUt) + var(F , nl + n w ut , k, \u03b41)\n+DiscD(f, P0(x, y),Mix nl nl+n\nw ut\n(P0(x, y), P w t (x, y)))\n+ nwut\nnl + nwut (E\u0302(h,DL) + var(H, nl, k, \u03b42) + var(H, nwut , k, \u03b43)\n+DiscD(h, P0(x, y), P w t (x, y))) (27)\nTaking into account inconsistent feature spaces and mapping function mapXt\u2192X0 , the final error bound can be obtained.\nAccording to eqs. (8), (9), (22), (26) and (27), for any target predictor f \u2208 F , pseudo-label predictor h \u2208 H, 0 \u2264 \u03b41 \u2264 1, 0 \u2264 \u03b42 \u2264 1 and 0 \u2264 \u03b43 \u2264 1, with the probability of at least (1 \u2212 \u03b41)(1 \u2212\n\u03b42)(1\u2212 \u03b43):\nE(f, P0(x, y)|h,w,mapXt\u2192X0 , DL, DtU )\n\u2264 nl nl + nwut E\u0302(f,DL) + nwut nl + nwut E\u0302(f, D\u0303wUt)) + var(F , nl + n w ut , k0, \u03b41) + nwut\nnl + nwut (\u03b8w(t)DiscL(P w t (x \u2217),Y0) + \u03b8w(t)DiscF (\u03a0Y0 [Pwt (x)],\u03a0Y0 [Pwt (x\u2217)],mapXt\u2192X0 , f)\n+\u03b8w(t)DiscD(\u03a0X0 [\u03a0Y0 [P w t (x \u2217, y)]], P0(x, y), f)) + nwut\nnl + nwut (E\u0302(h,DL) + var(H, nl, k, \u03b42) + var(H, nwut , k0, \u03b43)\n+\u03b8w(t)DiscL(P w t (x \u2217),Y0) + \u03b8w(t)DiscF (\u03a0Y0 [Pt(x)],\u03a0Y0 [Pwt (x\u2217)],mapXt\u2192X0 , h) +\u03b8w(t)DiscD(\u03a0X0 [\u03a0Y0 [P w t (x \u2217, y)]], P0(x, y), h)) (28)\nwhere E\u0302(f, D\u0303wUt) is the weighted disagreement rate between the noisy pseudo-labels and the prediction results of f on the weighted unlabeled dataset D\u0303wUt .\nA.8 BENCHMARK RESULTS\nDataset Model AUC Acc(0) WA EVM VS RCC\niris\nFT Transformer 0.982 0.982 0.982 - - - PseudoLabel 0.982 0.982 0.982 0.000 0.000 -\nPiModel 0.982 0.982 0.978 0.003 0.003 -0.507 MeanTeacher 0.982 0.982 0.982 0.000 0.000 -\nVAT 0.981 0.982 0.973 0.004 0.007 -0.054 ICT 0.982 0.982 0.982 0.000 0.000 -\nUDA 0.984 0.987 0.982 0.001 0.002 -0.828 FixMatch 0.984 0.982 0.982 0.002 0.003 0.414\nContinued on next page\nTable 9 \u2013 continued from previous page Dataset) Method AUC Acc(0) WA EVM VS RCC\nFlexMatch 0.976 0.987 0.956 0.017 0.020 0.145 FreeMatch 0.970 0.982 0.947 0.014 0.018 0.418 SoftMatch 0.980 0.987 0.973 0.004 0.006 -0.567\nUASD 0.983 0.982 0.982 0.002 0.003 -0.393 CAFA 0.986 0.987 0.982 0.004 0.005 -0.213 MTCF 0.973 0.987 0.956 0.011 0.011 -0.323 Fix-A-Step 0.984 0.987 0.978 0.004 0.005 -0.524\nwine\nFT Transformer 0.947 0.947 0.947 - - - PseudoLabel 0.964 0.967 0.947 0.013 0.013 -0.033\nPiModel 0.966 0.967 0.953 0.009 0.009 -0.660 MeanTeacher 0.920 0.920 0.920 0.000 0.000 -\nVAT 0.974 0.977 0.967 0.005 0.005 -0.782 ICT 0.960 0.960 0.960 0.000 0.000 -\nUDA 0.972 0.973 0.960 0.009 0.010 -0.567 FixMatch 0.959 0.980 0.947 0.008 0.007 -0.938 FlexMatch 0.957 0.967 0.930 0.013 0.011 -0.797 FreeMatch 0.903 0.907 0.853 0.031 0.039 0.204 SoftMatch 0.932 0.950 0.917 0.022 0.025 -0.135\nUASD 0.970 0.970 0.970 0.000 0.000 - CAFA 0.939 0.943 0.917 0.011 0.013 -0.716 MTCF 0.958 0.973 0.937 0.020 0.021 -0.457 Fix-A-Step 0.968 0.990 0.953 0.015 0.014 -0.638\nletter\nFT Transformer 0.776 0.776 0.776 - - - PseudoLabel 0.790 0.792 0.789 0.001 0.001 -0.917\nPiModel 0.784 0.791 0.777 0.003 0.003 -0.935 MeanTeacher 0.795 0.795 0.795 0.000 0.000 -\nVAT 0.779 0.791 0.769 0.004 0.002 -0.986 ICT 0.780 0.780 0.780 0.000 0.000 -\nUDA 0.784 0.784 0.784 0.000 0.000 - FixMatch 0.772 0.823 0.738 0.018 0.011 -0.959 FlexMatch 0.735 0.754 0.711 0.014 0.014 -0.782 FreeMatch 0.625 0.773 0.538 0.055 0.046 -0.888 SoftMatch 0.728 0.776 0.695 0.016 0.017 -0.898\nUASD 0.794 0.797 0.790 0.004 0.004 -0.276 CAFA 0.779 0.780 0.778 0.001 0.001 0.155 MTCF 0.530 0.757 0.426 0.076 0.077 -0.834 Fix-A-Step 0.781 0.805 0.763 0.009 0.007 -0.972\ncovertype\nFT Transformer 0.638 0.638 0.638 - - - PseudoLabel 0.652 0.652 0.649 0.001 0.001 -0.839\nPiModel 0.656 0.661 0.651 0.005 0.006 -0.139 MeanTeacher 0.626 0.626 0.626 0.000 0.000 -\nVAT 0.640 0.634 0.634 0.003 0.002 0.943 ICT 0.645 0.645 0.645 0.000 0.000 -\nUDA 0.622 0.624 0.620 0.002 0.002 -0.251 FixMatch 0.620 0.620 0.610 0.007 0.008 0.686 FlexMatch 0.619 0.627 0.603 0.006 0.004 -0.945 FreeMatch 0.613 0.626 0.599 0.008 0.009 -0.823 SoftMatch 0.628 0.645 0.615 0.006 0.004 -0.953\nUASD 0.626 0.627 0.619 0.005 0.006 -0.361 CAFA 0.617 0.617 0.613 0.003 0.003 -0.513 MTCF 0.617 0.606 0.606 0.007 0.007 0.530 Fix-A-Step 0.657 0.660 0.648 0.005 0.007 -0.271\nTable 10 \u2013 continued from previous page Task(Labeled/Unlabeled) Method AUC Acc(0) WA EVM VS RCC\nMTCF 0.946 0.947 0.939 0.007 0.007 -0.300 Fix-A-Step 0.940 0.948 0.931 0.007 0.008 -0.474\nCaltech/Pascal\nSupervised 0.945 0.945 0.945 - - - PseudoLabel 0.951 0.949 0.949 0.001 0.001 0.781\nPiModel 0.950 0.945 0.945 0.001 0.002 0.655 MeanTeacher 0.951 0.953 0.948 0.002 0.003 -0.825\nVAT 0.944 0.956 0.936 0.006 0.006 -0.678 ICT 0.949 0.945 0.945 0.002 0.003 -0.222\nUDA 0.946 0.943 0.939 0.006 0.007 0.030 FixMatch 0.933 0.957 0.852 0.022 0.036 -0.729 FlexMatch 0.952 0.965 0.937 0.007 0.007 -0.920 FreeMatch 0.876 0.956 0.777 0.036 0.024 -0.980 SoftMatch 0.933 0.960 0.892 0.017 0.013 -0.920\nUASD 0.962 0.963 0.961 0.000 0.001 -0.655 CAFA 0.961 0.960 0.960 0.001 0.001 0.169 MTCF 0.946 0.943 0.943 0.004 0.005 0.146 Fix-A-Step 0.942 0.928 0.928 0.010 0.011 0.590\nImageNet/Caltech\nSupervised 0.909 0.909 0.909 - - - PseudoLabel 0.907 0.908 0.907 0.001 0.001 -0.621\nPiModel 0.909 0.907 0.907 0.001 0.001 0.655 MeanTeacher 0.903 0.904 0.900 0.003 0.003 0.169\nVAT 0.888 0.881 0.881 0.002 0.002 0.928 ICT 0.907 0.909 0.903 0.003 0.004 -0.359\nUDA 0.896 0.904 0.891 0.006 0.007 -0.512 FixMatch 0.902 0.905 0.887 0.005 0.007 -0.726 FlexMatch 0.906 0.921 0.893 0.008 0.010 -0.861 FreeMatch 0.864 0.916 0.832 0.031 0.028 -0.786 SoftMatch 0.904 0.908 0.891 0.007 0.007 -0.805\nUASD 0.897 0.897 0.897 0.000 0.000 - CAFA 0.893 0.892 0.889 0.002 0.002 0.820 MTCF 0.880 0.904 0.855 0.016 0.015 -0.841 Fix-A-Step 0.869 0.876 0.856 0.007 0.011 -0.347\nImageNet/Pascal\nSupervised 0.909 0.909 0.909 - - - PseudoLabel 0.906 0.908 0.901 0.004 0.004 -0.375\nPiModel 0.907 0.907 0.901 0.005 0.006 -0.085 MeanTeacher 0.904 0.909 0.900 0.003 0.003 -0.813\nVAT 0.884 0.889 0.875 0.006 0.007 -0.781 ICT 0.906 0.909 0.903 0.004 0.004 0.000\nUDA 0.894 0.904 0.883 0.013 0.017 0.173 FixMatch 0.901 0.905 0.889 0.013 0.017 -0.528 FlexMatch 0.911 0.921 0.897 0.006 0.007 -0.915 FreeMatch 0.855 0.876 0.824 0.022 0.025 -0.665 SoftMatch 0.888 0.908 0.839 0.014 0.018 -0.846\nUASD 0.897 0.896 0.896 0.000 0.001 0.655 CAFA 0.890 0.897 0.887 0.003 0.003 -0.768 MTCF 0.875 0.909 0.840 0.014 0.012 -0.928 Fix-A-Step 0.877 0.885 0.867 0.009 0.010 -0.524\nPascal/Caltech\nSupervised 0.732 0.732 0.732 - - - PseudoLabel 0.726 0.724 0.724 0.001 0.002 0.804\nPiModel 0.723 0.724 0.723 0.000 0.001 -0.655 MeanTeacher 0.727 0.728 0.721 0.007 0.008 -0.398\nVAT 0.721 0.731 0.712 0.007 0.009 -0.216 ICT 0.727 0.727 0.721 0.006 0.006 -0.241\nUDA 0.731 0.728 0.725 0.003 0.003 0.916 FixMatch 0.698 0.713 0.611 0.027 0.043 -0.628 FlexMatch 0.719 0.719 0.701 0.013 0.018 -0.529\nContinued on next page\nDataset Model AUC Acc(0) WA EVM VS RCC\niris\nFT Transformer 0.937 0.937 0.937 - - - PseudoLabel 0.922 0.920 0.920 0.001 0.002 -0.293\nPiModel 0.913 0.910 0.910 0.003 0.004 -0.293 MeanTeacher 0.917 0.917 0.917 0.000 0.000 -\nVAT 0.918 0.923 0.907 0.007 0.010 -0.345 ICT 0.923 0.923 0.923 0.000 0.000 -\nUDA 0.926 0.927 0.92 0.005 0.006 -0.744 FixMatch 0.933 0.940 0.927 0.007 0.008 -0.319 FlexMatch 0.909 0.893 0.857 0.024 0.028 0.825 FreeMatch 0.844 0.913 0.790 0.025 0.016 -0.976 SoftMatch 0.934 0.923 0.923 0.003 0.004 0.925\nUASD 0.913 0.913 0.900 0.009 0.012 -0.026 CAFA 0.931 0.933 0.927 0.003 0.003 -0.071 MTCF 0.900 0.860 0.860 0.014 0.017 0.845 Fix-A-Step 0.898 0.877 0.877 0.014 0.017 0.339\nwine\nFT Transformer 0.938 0.938 0.938 - - - PseudoLabel 0.888 0.911 0.868 0.012 0.010 -0.937\nPiModel 0.932 0.943 0.916 0.005 0.006 -0.934 MeanTeacher 0.914 0.914 0.914 0.000 0.000 -\nVAT 0.928 0.943 0.905 0.015 0.016 -0.789 ICT 0.916 0.916 0.916 0.000 0.000 -\nUDA 0.924 0.935 0.881 0.03 0.035 0.326 FixMatch 0.912 0.959 0.881 0.022 0.025 -0.564 FlexMatch 0.910 0.932 0.851 0.048 0.056 -0.153 FreeMatch 0.888 0.922 0.835 0.031 0.036 -0.068 SoftMatch 0.887 0.957 0.816 0.050 0.069 0.000\nUASD 0.962 0.962 0.962 0.000 0.000 - CAFA 0.928 0.951 0.873 0.035 0.046 -0.228 MTCF 0.905 0.951 0.881 0.015 0.012 -0.913\nContinued on next page\nDataset Model AUC Acc(0) WA EVM VS RCC\niris\nFT Transformer 0.949 0.949 0.949 - - - PseudoLabel 0.944 0.947 0.942 0.001 0.002 -0.878\nPiModel 0.941 0.938 0.938 0.003 0.003 0.414 MeanTeacher 0.951 0.951 0.951 0.000 0.000 -\nVAT 0.941 0.947 0.933 0.006 0.007 -0.767 ICT 0.942 0.942 0.942 0.000 0.000 -\nUDA 0.943 0.942 0.938 0.004 0.005 0.213 FixMatch 0.943 0.938 0.938 0.002 0.002 0.924 FlexMatch 0.950 0.956 0.938 0.008 0.011 0.109 FreeMatch 0.929 0.929 0.916 0.015 0.018 0.195 SoftMatch 0.941 0.920 0.920 0.006 0.008 0.884\nUASD 0.945 0.951 0.933 0.011 0.013 -0.183 CAFA 0.948 0.956 0.947 0.002 0.004 -0.655 MTCF 0.861 0.902 0.796 0.027 0.028 -0.919 Fix-A-Step 0.939 0.920 0.920 0.005 0.005 0.883\nwine\nFT Transformer 0.964 0.964 0.964 - - - PseudoLabel 0.947 0.953 0.942 0.007 0.008 -0.427\nPiModel 0.954 0.959 0.949 0.003 0.003 -0.883 MeanTeacher 0.956 0.956 0.956 0.000 0.000 -\nContinued on next page\nDataset Model AUC Acc(0) WA EVM VS RCC\niris\nFT Transformer 1.000 1.000 1.000 - - PseudoLabel 0.994 1.000 0.990 0.003 0.004 -0.355\nPiModel 0.995 1.000 0.985 0.003 0.004 -0.878 MeanTeacher 1.000 1.000 1.000 0.000 0.000 -\nVAT 0.999 1.000 0.996 0.001 0.002 -0.655 ICT 1.000 1.000 1.000 0.000 0.000 - UDA 0.996 1.000 0.990 0.002 0.002 -0.924 FixMatch 0.995 1.000 0.985 0.003 0.002 -0.930 FlexMatch 0.996 1.000 0.990 0.003 0.005 -0.497\nContinued on next page\nTable 19 \u2013 continued from previous page Dataset) Method AUC Acc(0) WA EVM VS RCC\nFreeMatch 0.956 0.991 0.871 0.028 0.023 -0.905 SoftMatch 0.996 1.000 0.989 0.006 0.007 -0.393\nUASD 0.987 0.987 0.987 0.000 0.000 - CAFA 1.000 1.000 1.000 0.000 0.000 - MTCF 0.995 1.000 0.985 0.007 0.007 -0.676 Fix-A-Step 0.998 1.000 0.985 0.003 0.006 -0.655\nwine\nFT Transformer 0.964 0.964 0.964 - - - PseudoLabel 0.885 0.898 0.862 0.015 0.016 -0.815\nPiModel 0.862 0.880 0.840 0.015 0.016 -0.611 MeanTeacher 0.920 0.920 0.920 0.000 0.000 -\nVAT 0.882 0.920 0.837 0.031 0.029 -0.891 ICT 0.920 0.920 0.920 0.000 0.000 -\nUDA 0.881 0.873 0.858 0.024 0.028 0.138 FixMatch 0.879 0.913 0.833 0.016 0.009 -0.978 FlexMatch 0.871 0.927 0.833 0.031 0.036 -0.648 FreeMatch 0.831 0.877 0.790 0.026 0.038 -0.601 SoftMatch 0.943 0.976 0.920 0.013 0.011 -0.911\nUASD 0.923 0.923 0.923 0.000 0.000 - CAFA 0.880 0.815 0.815 0.042 0.043 0.779 MTCF 0.908 0.971 0.825 0.041 0.045 -0.853 Fix-A-Step 0.947 0.960 0.927 0.007 0.004 -0.976\nletter\nFT Transformer 0.628 0.628 0.628 - - - PseudoLabel 0.628 0.634 0.620 0.003 0.002 -0.970\nPiModel 0.649 0.653 0.639 0.005 0.007 -0.673 MeanTeacher 0.635 0.635 0.635 0.000 0.000 -\nVAT 0.640 0.656 0.622 0.007 0.004 -0.984 ICT 0.607 0.607 0.607 0.000 0.000 -\nUDA 0.606 0.605 0.605 0.001 0.001 0.657 FixMatch 0.602 0.663 0.556 0.021 0.012 -0.983 FlexMatch 0.644 0.662 0.621 0.008 0.006 -0.975 FreeMatch 0.528 0.634 0.447 0.042 0.041 -0.937 SoftMatch 0.638 0.657 0.613 0.009 0.008 -0.946\nUASD 0.638 0.640 0.628 0.005 0.007 0.138 CAFA 0.620 0.622 0.615 0.002 0.002 -0.918 MTCF 0.547 0.668 0.417 0.050 0.027 -0.984 Fix-A-Step 0.648 0.668 0.614 0.013 0.008 -0.966\ncovertype\nFT Transformer 0.546 0.546 0.546 - - - PseudoLabel 0.567 0.571 0.563 0.003 0.003 -0.828\nPiModel 0.575 0.570 0.564 0.010 0.011 0.645 MeanTeacher 0.561 0.561 0.561 0.000 0.000 -\nVAT 0.565 0.567 0.559 0.003 0.004 -0.766 ICT 0.550 0.550 0.550 0.000 0.000 -\nUDA 0.583 0.583 0.578 0.005 0.006 0.395 FixMatch 0.554 0.530 0.530 0.011 0.009 0.947 FlexMatch 0.555 0.539 0.530 0.020 0.024 0.526 FreeMatch 0.555 0.545 0.545 0.005 0.006 0.605 SoftMatch 0.537 0.524 0.524 0.011 0.009 0.893\nUASD 0.553 0.549 0.549 0.007 0.007 -0.049 CAFA 0.560 0.563 0.556 0.006 0.007 -0.381 MTCF 0.584 0.572 0.572 0.004 0.002 0.984 Fix-A-Step 0.552 0.535 0.535 0.009 0.006 0.954\nTable 22 \u2013 continued from previous page Dataset) Method AUC Acc(0) WA EVM VS RCC\nVAT 0.945 0.949 0.920 0.020 0.021 -0.465 ICT 0.956 0.956 0.956 0.000 0.000 -\nUDA 0.952 0.978 0.938 0.009 0.011 -0.838 FixMatch 0.924 0.942 0.911 0.008 0.01 -0.871 FlexMatch 0.904 0.947 0.867 0.027 0.032 -0.342 FreeMatch 0.884 0.902 0.855 0.016 0.024 -0.791 SoftMatch 0.941 0.937 0.931 0.008 0.009 0.626\nUASD 0.913 0.913 0.913 0.000 0.000 - CAFA 0.904 0.942 0.858 0.017 0.011 -0.979 MTCF 0.922 0.985 0.858 0.031 0.044 -0.889 Fix-A-Step 0.933 0.949 0.902 0.013 0.015 -0.883\nletter\nFT Transformer 0.707 0.707 0.707 - - - PseudoLabel 0.726 0.728 0.724 0.001 0.001 -0.937\nPiModel 0.732 0.738 0.724 0.003 0.003 -0.963 MeanTeacher 0.729 0.729 0.729 0.000 0.000 -\nVAT 0.742 0.743 0.739 0.002 0.003 -0.652 ICT 0.729 0.729 0.729 0.000 0.000 -\nUDA 0.728 0.728 0.728 0.000 0.000 0.188 FixMatch 0.708 0.741 0.673 0.014 0.008 -0.978 FlexMatch 0.736 0.754 0.723 0.007 0.008 -0.931 FreeMatch 0.673 0.734 0.615 0.024 0.017 -0.973 SoftMatch 0.732 0.741 0.724 0.004 0.005 -0.901\nUASD 0.725 0.729 0.720 0.005 0.006 -0.488 CAFA 0.735 0.736 0.732 0.002 0.002 -0.752 MTCF 0.631 0.717 0.557 0.034 0.032 -0.942 Fix-A-Step 0.738 0.756 0.718 0.008 0.002 -0.992\ncovertype\nFT Transformer 0.626 0.626 0.626 - - - PseudoLabel 0.64 0.639 0.638 0.001 0.001 0.725\nPiModel 0.634 0.624 0.624 0.003 0.003 0.888 MeanTeacher 0.638 0.638 0.638 0.000 0.000 -\nVAT 0.628 0.629 0.626 0.004 0.006 -0.398 ICT 0.63 0.63 0.63 0.000 0.000 -\nUDA 0.621 0.618 0.618 0.002 0.002 0.925 FixMatch 0.645 0.627 0.627 0.01 0.01 0.916 FlexMatch 0.625 0.612 0.611 0.005 0.008 0.807 FreeMatch 0.618 0.615 0.605 0.006 0.007 -0.436 SoftMatch 0.64 0.637 0.63 0.008 0.009 0.342\nUASD 0.635 0.636 0.631 0.003 0.003 0.080 CAFA 0.642 0.639 0.639 0.001 0.001 0.970 MTCF 0.628 0.619 0.619 0.003 0.001 0.993 Fix-A-Step 0.649 0.632 0.632 0.007 0.006 0.861"
        }
    ],
    "year": 2023
}