{
    "abstractText": "Conventional Time Series Classification (TSC) methods are often black boxes that obscure inherent interpretation of their decision-making processes. In this work, we leverage Multiple Instance Learning (MIL) to overcome this issue, and propose a new framework called MILLET: Multiple Instance Learning for Locally Explainable Time series classification. We apply MILLET to existing deep learning TSC models and show how they become inherently interpretable without compromising (and in some cases, even improving) predictive performance. We evaluate MILLET on 85 UCR TSC datasets and also present a novel synthetic dataset that is specially designed to facilitate interpretability evaluation. On these datasets, we show MILLET produces sparse explanations quickly that are of higher quality than other well-known interpretability methods. To the best of our knowledge, our work with MILLET is the first to develop general MIL methods for TSC and apply them to an extensive variety of domains. Figure 1: Conventional TSC techniques (left) usually only provide class-level predictive probabilities. In addition, our proposed method (MILLET, right) also highlights class-conditional discriminatory motifs that influence the predicted class. In the heatmap, green regions indicate support for the predicted class, red regions refute the predicted class, and darker regions are more influential.",
    "authors": [],
    "id": "SP:d5dd28ee14f4ad25fc57cdb1137abf6c59febfdc",
    "references": [
        {
            "authors": [
                "Jo\u00e3o Bento",
                "Pedro Saleiro",
                "Andr\u00e9 F Cruz",
                "M\u00e1rio AT Figueiredo",
                "Pedro Bizarro"
            ],
            "title": "TimeSHAP: Explaining recurrent models through sequence perturbations",
            "venue": "In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining,",
            "year": 2021
        },
        {
            "authors": [
                "Jonathan Crabb\u00e9",
                "Mihaela Van Der Schaar"
            ],
            "title": "Explaining time series predictions with dynamic masks",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Hoang Anh Dau",
                "Anthony Bagnall",
                "Kaveh Kamgar",
                "Chin-Chia Michael Yeh",
                "Yan Zhu",
                "Shaghayegh Gharghabi",
                "Chotirat Ann Ratanamahatana",
                "Eamonn Keogh"
            ],
            "title": "The UCR time series archive",
            "venue": "IEEE/CAA Journal of Automatica Sinica,",
            "year": 2019
        },
        {
            "authors": [
                "Angus Dempster",
                "Fran\u00e7ois Petitjean",
                "Geoffrey I. Webb"
            ],
            "title": "ROCKET: Exceptionally fast and accurate time series classification using random convolutional kernels",
            "venue": "Data Mining and Knowledge Discovery,",
            "year": 2020
        },
        {
            "authors": [
                "Angus Dempster",
                "Daniel F. Schmidt",
                "Geoffrey I. Webb"
            ],
            "title": "Hydra: Competing convolutional kernels for fast and accurate time series classification",
            "venue": "Data Mining and Knowledge Discovery,",
            "year": 2023
        },
        {
            "authors": [
                "Janez Dem\u0161ar"
            ],
            "title": "Statistical comparisons of classifiers over multiple data sets",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2006
        },
        {
            "authors": [
                "Don Dennis",
                "Chirag Pabbaraju",
                "Harsha Vardhan Simhadri",
                "Prateek Jain"
            ],
            "title": "Multiple instance learning for efficient sequential data classification on resource-constrained devices",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Thomas G. Dietterich",
                "Richard H. Lathrop",
                "Tom\u00e1s Lozano-P\u00e9rez"
            ],
            "title": "Solving the multiple instance problem with axis-parallel rectangles",
            "venue": "Artificial Intelligence,",
            "year": 1997
        },
        {
            "authors": [
                "Joseph Early",
                "Christine Evers",
                "Sarvapali Ramchurn"
            ],
            "title": "Model agnostic interpretability for multiple instance learning",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Joseph Early",
                "Tom Bewley",
                "Christine Evers",
                "Sarvapali Ramchurn"
            ],
            "title": "Non-markovian reward modelling from trajectory labels via interpretable multiple instance learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Navid Mohammadi Foumani",
                "Lynn Miller",
                "Chang Wei Tan",
                "Geoffrey I. Webb",
                "Germain Forestier",
                "Mahsa Salehi"
            ],
            "title": "Deep learning for time series classification and extrinsic regression: A current survey",
            "venue": "arXiv preprint arXiv:2302.02515,",
            "year": 2023
        },
        {
            "authors": [
                "Pedro J Freire",
                "Sasipim Srivallapanondh",
                "Antonio Napoli",
                "Jaroslaw E Prilepsky",
                "Sergei K Turitsyn"
            ],
            "title": "Computational complexity evaluation of neural network applications in signal processing",
            "venue": "arXiv preprint arXiv:2206.12191,",
            "year": 2022
        },
        {
            "authors": [
                "Mononito Goswami",
                "Cristian Challu",
                "Laurent Callot",
                "Lenon Minorics",
                "Andrey Kan"
            ],
            "title": "Unsupervised model selection for time-series anomaly detection",
            "venue": "In International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Riccardo Guidotti",
                "Matteo D\u2019Onofrio"
            ],
            "title": "Matrix profile-based interpretable time series classifier",
            "venue": "Frontiers in Artificial Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Tsung-Yu Hsieh",
                "Suhang Wang",
                "Yiwei Sun",
                "Vasant Honavar"
            ],
            "title": "Explainable multivariate time series classification: a deep neural network which learns to attend to important variables as well as time intervals",
            "venue": "In Proceedings of the 14th ACM International Conference on Web Search and Data Mining,",
            "year": 2021
        },
        {
            "authors": [
                "Maximilian Ilse",
                "Jakub Tomczak",
                "Max Welling"
            ],
            "title": "Attention-based deep multiple instance learning",
            "venue": "In International Conference on Machine Learning,",
            "year": 2018
        },
        {
            "authors": [
                "Aya Abdelsalam Ismail",
                "Mohamed Gunady",
                "Hector Corrada Bravo",
                "Soheil Feizi"
            ],
            "title": "Benchmarking deep learning interpretability in time series predictions",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Hassan Ismail Fawaz",
                "Germain Forestier",
                "Jonathan Weber",
                "Lhassane Idoumghar",
                "Pierre-Alain Muller"
            ],
            "title": "Deep learning for time series classification: a review",
            "venue": "Data Mining and Knowledge Discovery,",
            "year": 2019
        },
        {
            "authors": [
                "Hassan Ismail Fawaz",
                "Benjamin Lucas",
                "Germain Forestier",
                "Charlotte Pelletier",
                "Daniel F Schmidt",
                "Jonathan Weber",
                "Geoffrey I Webb",
                "Lhassane Idoumghar",
                "Pierre-Alain Muller",
                "Fran\u00e7ois Petitjean"
            ],
            "title": "InceptionTime: Finding AlexNet for time series classification",
            "venue": "Data Mining and Knowledge Discovery,",
            "year": 1936
        },
        {
            "authors": [
                "Vijay Manikandan Janakiraman"
            ],
            "title": "Explaining aviation safety incidents using deep temporal multiple instance learning",
            "venue": "In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining,",
            "year": 2018
        },
        {
            "authors": [
                "Syed Ashar Javed",
                "Dinkar Juyal",
                "Harshith Padigela",
                "Amaro Taylor-Weiner",
                "Limin Yu",
                "Aaditya Prakash"
            ],
            "title": "Additive MIL: Intrinsically interpretable multiple instance learning for pathology",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Kin Kwan Leung",
                "Clayton Rooke",
                "Jonathan Smith",
                "Saba Zuberi",
                "Maksims Volkovs"
            ],
            "title": "Temporal dependencies in feature importance for time series prediction",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Scott M. Lundberg",
                "Su-In Lee"
            ],
            "title": "A unified approach to interpreting model predictions",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Matthew Middlehurst",
                "James Large",
                "Michael Flynn",
                "Jason Lines",
                "Aaron Bostrom",
                "Anthony Bagnall"
            ],
            "title": "HIVECOTE 2.0: a new meta ensemble for time series classification",
            "venue": "Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Matthew Middlehurst",
                "Patrick Sch\u00e4fer",
                "Anthony Bagnall"
            ],
            "title": "Bake off redux: a review and experimental evaluation of recent time series classification algorithms",
            "venue": "arXiv preprint arXiv:2304.13029,",
            "year": 2023
        },
        {
            "authors": [
                "Christoph Molnar"
            ],
            "title": "Interpretable machine learning. 2 edition, 2022",
            "venue": "URL https://christophm. github.io/interpretable-ml-book",
            "year": 2022
        },
        {
            "authors": [
                "Amin Nayebi",
                "Sindhu Tipirneni",
                "Chandan K Reddy",
                "Brandon Foreman",
                "Vignesh Subbian"
            ],
            "title": "WindowSHAP: An efficient framework for explaining time-series classifiers based on Shapley values",
            "venue": "Journal of Biomedical Informatics,",
            "year": 2023
        },
        {
            "authors": [
                "Rafael Poyiadzi",
                "Raul Santos-Rodriguez",
                "Niall Twomey"
            ],
            "title": "Label propagation for learning with label proportions",
            "venue": "IEEE 28th International Workshop on Machine Learning for Signal Processing (MLSP),",
            "year": 2018
        },
        {
            "authors": [
                "Rafael Poyiadzis",
                "Raul Santos-Rodriguez",
                "Niall Twomey"
            ],
            "title": "Active learning with label proportions",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2019
        },
        {
            "authors": [
                "Owen Queen",
                "Thomas Hartvigsen",
                "Teddy Koker",
                "Huan He",
                "Theodoros Tsiligkaridis",
                "Marinka Zitnik"
            ],
            "title": "Encoding time-series explanations through self-supervised model behavior consistency",
            "venue": "arXiv preprint arXiv:2306.02109,",
            "year": 2023
        },
        {
            "authors": [
                "Marco Tulio Ribeiro",
                "Sameer Singh",
                "Carlos Guestrin"
            ],
            "title": "Why should I trust you?\u201d Explaining the predictions of any classifier",
            "venue": "In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining,",
            "year": 2016
        },
        {
            "authors": [
                "Bhoomi Shah",
                "Hetal Bhavsar"
            ],
            "title": "Time complexity in deep learning models",
            "venue": "Procedia Computer Science,",
            "year": 2022
        },
        {
            "authors": [
                "Divya Shanmugam",
                "Davis Blalock",
                "John Guttag"
            ],
            "title": "Multiple instance learning for ECG risk stratification",
            "venue": "In Machine Learning for Healthcare Conference,",
            "year": 2019
        },
        {
            "authors": [
                "Ilija \u0160imi\u0107",
                "Vedran Sabol",
                "Eduardo Veas"
            ],
            "title": "XAI Methods for Neural Time Series Classification: A Brief Review",
            "venue": "arXiv preprint arXiv:2108.08009,",
            "year": 2021
        },
        {
            "authors": [
                "Torty Sivill",
                "Peter Flach"
            ],
            "title": "Limesegment: Meaningful, realistic time series explanations",
            "venue": "In International Conference on Artificial Intelligence and Statistics,",
            "year": 2022
        },
        {
            "authors": [
                "Andreas Theissler",
                "Francesco Spinnato",
                "Udo Schlegel",
                "Riccardo Guidotti"
            ],
            "title": "Explainable AI for time series classification: a review, taxonomy and research directions",
            "venue": "IEEE Access,",
            "year": 2022
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N. Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Kaili Wang",
                "Jose Oramas",
                "Tinne Tuytelaars"
            ],
            "title": "In defense of LSTMs for addressing multiple instance learning problems",
            "venue": "In Proceedings of the Asian Conference on Computer Vision,",
            "year": 2020
        },
        {
            "authors": [
                "Xinggang Wang",
                "Yongluan Yan",
                "Peng Tang",
                "Xiang Bai",
                "Wenyu Liu"
            ],
            "title": "Revisiting multiple instance neural networks",
            "venue": "Pattern Recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Zhiguang Wang",
                "Weizhong Yan",
                "Tim Oates"
            ],
            "title": "Time series classification from scratch with deep neural networks: A strong baseline",
            "venue": "In 2017 International Joint Conference on Neural Networks (IJCNN),",
            "year": 2017
        },
        {
            "authors": [
                "Chin-Chia Michael Yeh",
                "Nickolas Kavantzas",
                "Eamonn Keogh"
            ],
            "title": "Matrix profile IV: using weakly labeled time series to predict outcomes",
            "venue": "Proceedings of the VLDB Endowment,",
            "year": 2017
        },
        {
            "authors": [
                "Matthew D. Zeiler",
                "Rob Fergus"
            ],
            "title": "Visualizing and understanding convolutional networks",
            "venue": "In Computer Vision\u2013ECCV 2014: 13th European Conference,",
            "year": 2014
        },
        {
            "authors": [
                "Bolei Zhou",
                "Aditya Khosla",
                "Agata Lapedriza",
                "Aude Oliva",
                "Antonio Torralba"
            ],
            "title": "Learning deep features for discriminative localization",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Yuansheng Zhu",
                "Weishi Shi",
                "Deep Shankar Pandey",
                "Yang Liu",
                "Xiaofan Que",
                "Daniel E. Krutz",
                "Qi Yu"
            ],
            "title": "Uncertainty-aware multiple instance learning from large-scale long time series data",
            "venue": "IEEE International Conference on Big Data (Big Data),",
            "year": 2021
        },
        {
            "authors": [
                "LIMESegements (Sivill",
                "Flach"
            ],
            "title": "2022), an extension of LIME for TSC, has the same issue as TimeSHAP and WindowSHAP mentioned above: it groups time points into subsequences so does not produce interpretations at the same granularity as MILLET. Alternative approaches for TSC interpretability have similar granularity issues or are often very expensive to compute (relative to the cost of using MILLET or CAM)",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Time Series Classification (TSC) is the process of assigning labels to sequences of data, and occurs in a wide range of settings \u2013 examples from the popular UCR collection of datasets include predicting heart failure from electrocardiogram data, and identifying household electric appliance usage from electricity data (Dau et al., 2019). Each of these domains have their own set of classconditional discriminatory motifs (the signatures that determine the class of a time series). Deep Learning (DL) methods have emerged as a popular family of approaches for solving TSC problems. However, we identify two drawbacks with these conventional supervised learning approaches: 1) representations are learnt for each time point in a time series, but these representations are then lost through an aggregation process that weights all time points equally, and 2) these methods are black boxes that provide no inherent explanations for their decision making, i.e. they cannot localise the class-conditional discriminatory motifs. These drawbacks not only limit predictive performance, but also introduce barriers to their adoption in practice as the models are not transparent.\nTo mitigate these shortcomings, we take an alternative view of DL for TSC, approaching it as a Multiple Instance Learning (MIL) problem. MIL is a weakly supervised learning paradigm in which a collection (bag) of elements (MIL instances) all share the same label. In the context of TSC, a bag is a time series of data over a contiguous interval. In the MIL setting, the learning objective is to assign class labels to unlabelled bags of time series data whilst also discovering the salient motifs within the time series that explain the reasons for the predicted class. As we explore in this work, MIL is well-suited to overcome the drawbacks identified above, leading to inherent interpretability\nwithout compromising predictive performance (even improving it in some cases). We propose a new general framework applying MIL to TSC called MILLET: Multiple Instance Learning for Locally Explainable Time series classification. Demonstrative MILLETmodel outputs are depicted in Fig. 1.\nMIL is well-suited to this problem setting since it was developed for weakly supervised contexts, can be learnt in an end-to-end framework, and boasts many successes across several domains. Furthermore, MIL has the same label specificity as TSC: labels are given at the bag level, but not at the MIL instance level. To explore the intersection of these two areas, we propose plug-and-play concepts that are adapted from MIL and applied to existing TSC approaches (in this work, DL models1). Furthermore, to aid in our evaluation of the interpretability of these new methods, we introduce a new synthetic TSC dataset, WebTraffic, where the location of the class-conditional discriminatory motifs within time series are known. The time series shown in Fig. 1 is sampled from this dataset.\nOur key contributions are as follows:\n1. We propose MILLET, a new framework for TSC that utilises MIL to provide inherent interpretability without compromising predictive performance (even improving it in some cases).\n2. We design plug-and-play MIL methods for TSC within MILLET. 3. We propose a new method of MIL aggregation, Conjunctive pooling, that outper-\nforms existing pooling methods in our TSC experiments. 4. We propose and evaluate 12 novel MILLETmodels on 85 univariate datasets from the UCR\nTSC Archive (Dau et al., 2019), as well as a novel synthetic dataset that facilitates better evaluation of TSC interpretability."
        },
        {
            "heading": "2 BACKGROUND AND RELATED WORK",
            "text": "Time Series Classification While a range of TSC methods exist, in this work we apply MIL to DL TSC approaches. Methods in this family are effective and widely used (Ismail Fawaz et al., 2019; Foumani et al., 2023); popular methods include Fully Convolutional Networks (FCN), Residual Networks (ResNet), and InceptionTime (Wang et al., 2017; Ismail Fawaz et al., 2020). Indeed, a recent TSC survey, Bake Off Redux (Middlehurst et al., 2023), found InceptionTime to be competitive with SOTA approaches such as the ensemble method HIVE-COTE 2 (HC2; Middlehurst et al., 2021) and the hybrid dictionary-convolutional method Hydra-MultiRocket (Hydra-MR; Dempster et al., 2023). Although the application of Matrix Profile for TSC also yields inherent interpretability (Yeh et al., 2017; Guidotti & D\u2019Onofrio, 2021), we choose to focus on DL approaches due to their popularity, strong performance, and scope for improvement (Middlehurst et al., 2023).\nMultiple Instance Learning In its standard assumption, MIL is a binary classification problem: a bag is positive if and only if at least one of its instances is positive (Dietterich et al., 1997). As we are designing MILLET to be a general and widely applicable TSC approach, we do not constrain it to any specific MIL assumption except that there are temporal relationships, i.e. the order of instances within bags matters (Early et al., 2022; Wang et al., 2020). As we explore in Sec. 3.4, this allows us to use positional encodings in our MILLET methods. Although the application of MIL to TSC has been explored prior to this study, earlier work focused on domain-specific problems such as intensive care in medicine and human activity recognition (Dennis et al., 2018; Janakiraman, 2018; Poyiadzi et al., 2018; Poyiadzis et al., 2019; Shanmugam et al., 2019). Furthermore, existing work considers MIL as its own unique approach separate from existing TSC methods. The work most closely related to ours is Zhu et al. (2021), which proposes an uncertainty-aware MIL TSC framework specifically designed for long time series (marine vessel tracking), but without the generality and plug-and-play nature of MILLET. Therefore, to the best of our knowledge, our work with MILLET is the first to apply MIL to TSC in a more general sense and to do so across an extensive variety of domains.\nInterpretability TSC interpretability methods can be grouped into several categories (Theissler et al., 2022) \u2013 in this work we focus on class-wise time point attribution (saliency maps), i.e. identifying the discriminatory time points in a time series that support and refute different classes. This is a form of local interpretation, where model decision-making is explained for individual time series (Molnar, 2022). It also aligns with MIL interpretability as proposed by Early et al. (2021): which\n1While we focus on DL TSC in this work, we envision that our MILLET framework can be applied to other TSC approaches in the future, such as the ROCKET family of methods (Dempster et al., 2020; 2023).\nare the key MIL instances in a bag, and what outcomes do they support/refute? MILLET facilitates interpretability by inherently enhancing existing TSC approaches such that they provide interpretations alongside their predictions with a single forward pass of the model. This is in contrast to perturbation methods such as LIME (Ribeiro et al., 2016), SHAP (Lundberg & Lee, 2017), Occlusion Sensitivity (Zeiler & Fergus, 2014), and MILLI (Early et al., 2021), which are much more expensive to run (often requiring 100+ forward passes per interpretation). An interpretability approach that can be run with a single forward pass is Class Activation Mapping (CAM) (Zhou et al., 2016; Wang et al., 2017). It uses the model\u2019s weights to identify discriminatory time points, and serves as a benchmark in this work. For more details on existing TSC interpretability methods and their evaluation metrics, see App. A.1, Theissler et al. (2022), and S\u030cimic\u0301 et al. (2021)."
        },
        {
            "heading": "3 METHODOLOGY",
            "text": "To apply MIL to TSC, we propose the broad framework MILLET: Multiple Instance Learning for Locally Explainable Time series classification. We advocate for the use of MIL in TSC as it is a natural fit that provides inherent interpretability (explanations for free) without requiring any additional labelling beyond that already provided by existing TSC datasets (we further discuss our motivation for using MIL in App. A).\n3.1 THE MILLET FRAMEWORK\nA TSC model within our MILLET framework has to satisfy three requirements:\nRequirement 1: Time Series as MIL Bags Input data consists of time series, Xi, where a time series is formed of t > 1 time points: Xi = {x1i ,x2i , . . . ,xti} and i is the sample index.2 Each time step is a c-dimensional vector, where c is the number of channels in the time series \u2013 in this work we focus on univariate time series (c = 1) and assume all time series in a dataset have the same length. We consider each time series as a MIL bag, meaning each time point is a MIL instance.3 A time series bag can be denoted as Xi \u2208 Rt\u00d7c, and each bag has an associated bag-level label Yi which is the original time series label. There is also the concept of MIL instance labels {y1i , y2i , . . . , yti}, but these are not provided for most MIL datasets (like the absence of time point labels in TSC). Framing TSC as a MIL problem allows us to obtain interpretability by imposing the next requirement.\nRequirement 2: Time Point Predictions To facilitate interpretability in our framework, we specify that models must provide time point predictions along with their time series predictions. Furthermore, the time point predictions should be inherent to the model \u2013 this makes it possible to identify which time points support and refute different classes without having to use post-hoc methods.\nRequirement 3: Temporal Ordering TSC is a sequential learning problem so we impose a further requirement that the framework must respect the ordering of time points. This is in contrast to classical MIL methods that assume MIL instances are iid.\n3.2 MILLET FOR DL TSC: RETHINKING POOLING\nTo demonstrate the use of MILLET, we apply it to DL TSC methods. Existing DL TSC architectures (e.g. FCN, ResNet, and InceptionTime) mainly consist of two modules: a feature extractor \u03c8FE (we refer to these as backbones) and a classifier \u03c8CLF . For an input univariate time series Xi, \u03c8FE produces a set of d-dimensional feature embeddings Zi \u2208 Rt\u00d7d = [z1i , z2i , . . . , zti ]. These embeddings are consolidated via aggregation with Global Average Pooling (GAP) to give a single feature vector of length d. This is then passed to \u03c8CLF to produce predictions for the time series:\nFeature Extraction: Zi = \u03c8FE ( Xi ) ; GAP + Classification: Y\u0302i = \u03c8CLF\n( 1\nt t\u2211 j=1 z ji\n) . (1)\n2Following convention from MIL, we use uppercase variables to denote MIL bag / time series data and lowercase variables to denote MIL instance / time point data.\n3There is an overlap in TSC and MIL terminology: both use the term \u2018instance\u2019 but in different ways. In MIL it denotes an element in a bag, and in TSC it refers to an entire time series (e.g. \u201cinstance-based explanations\u201d from Theissler et al., 2022). To avoid confusion, we use \u2018time series\u2019 to refer to entire time series (a TSC instance) and \u2018time point\u2019 to refer to a value for a particular step in a time series (a MIL instance).\nThe specification of \u03c8FE naturally satisfies Req. 1 from our MILLET framework as discriminatory information is extracted on a time point level. Req. 3 is satisfied as long as the DL architecture makes use of layers that respect the ordering of the time series such as convolutional or recurrent layers. In the MIL domain, the GAP + Classification process (Eqn. 1) is known as mean Embedding pooling, as used in methods such as MI-Net (Wang et al., 2018). However, this aggregation step does not inherently produce time point class predictions, and consequently does not fit Req. 2.\nTo upgrade existing DL TSC methods into the MILLET framework and satisfy Req. 2, we explore four MIL pooling methods for replacing GAP. Attention, Instance, and Additive are inspired by existing MIL approaches, while Conjunctive is proposed in this work. Replacing GAP in this way is plug-and-play, i.e. any TSC method using GAP or similar pooling can easily be upgraded to one of these methods and meet the requirements for MILLET.\nAttention pooling (Ilse et al., 2018) does weighted averaging via an attention head \u03c8ATTN :\naji \u2208 [0, 1] = \u03c8ATTN ( z ji ) ; Y\u0302i = \u03c8CLF\n( 1\nt t\u2211 j=1 ajiz j i\n) . (2)\nInstance pooling (Wang et al., 2018) makes a prediction for each time point:\ny\u0302 ji \u2208 R c = \u03c8CLF ( z ji ) ; Y\u0302i = 1\nt t\u2211 j=1 ( y\u0302 ji ) . (3)\nAdditive pooling (Javed et al., 2022) is a combination of Attention and Instance:\naji \u2208 [0, 1] = \u03c8ATTN ( z ji ) ; y\u0302 ji = \u03c8CLF ( ajiz j i ) ; Y\u0302i = 1\nt t\u2211 j=1 ( y\u0302 ji ) . (4)\nConjunctive pooling is our proposed novel pooling approach, where attention and classification are independently applied to the time point embeddings, after which the attention values are used to scale the time point predictions. This is expected to benefit performance as the attention and classifier heads are trained in parallel rather than sequentially, i.e. the classifier cannot rely on the attention head to alter the time point embeddings prior to classification, making it more robust. We use the term Conjunctive to emphasise that, from an interpretability perspective, a discriminatory time point must be considered important by both the attention head and the classification head. Formally, Conjunctive is described as:\naji \u2208 [0, 1] = \u03c8ATTN ( z ji ) ; y\u0302 ji = \u03c8CLF ( z ji ) ; Y\u0302 = 1\nt t\u2211 j=1 ( aji y\u0302 j i ) . (5)\nFig. 2 shows a schematic representation comparing these pooling approaches with Embedding (GAP). Note there are other variations of these MIL pooling methods, such as replacing mean with max in Embedding and Instance, but these alternative approaches are not explored in this work.\n3.3 MILLET DL INTERPRETABILITY\nAs a result of Requirement 2 in Sec. 3.1, we expect the models to be inherently interpretable. For DL methods, this is achieved through the MIL pooling methods given in Sec. 3.2 \u2013 different MIL pooling approaches provide alternative forms of inherent interpretability. Instance performs classification before pooling (see Eqn. 3), so it produces a set of time point predictions y\u0302i \u2208 Rt\u00d7c = [y\u03021i , y\u03022i , . . . , y\u0302ti ]. Additive and Conjunctive also make time point predictions, but include attention. To combine these two outputs, we weight the time point predictions by the attention scores: y\u0302\u2217i \u2208 Rt\u00d7c = [a1i y\u03021i , a2i y\u03022i , . . . , atiy\u0302ti ]. Note that y\u0302\u2217i is used to signify the attention weighting of the original time point predictions y\u0302i.\nOn the other hand, Attention is inherently interpretable through its attention weights ai \u2208 [0, 1]t = [a1i , a 2 i , . . . , a t i], which can be interpreted as a measure of importance for each time point. Note, unlike Instance, Additive, and Conjunctive, the interpretability output for Attention is not class specific (but only a general measure of importance across all classes).\n3.4 MILLET DL MODEL DESIGN\nWe design three MILLET DL models by adapting existing backbone models that use GAP: FCN, ResNet, and InceptionTime. While extensions of these methods and other DL approaches exist (see Foumani et al., 2023), we do not explore these as none have been shown to outperform InceptionTime (Middlehurst et al., 2023). Nevertheless, the MILLET framework can be applied to any generic DL TSC approach that uses GAP or follows the high-level structure in Eqn. 1.\nReplacing GAP with one of the four pooling methods in Sec. 3.2 yields a total of 12 new models. In each case, the backbone models produce feature embeddings of length d = 128 ( Zi \u2208 Rt\u00d7128 ) . The models are trained end-to-end in the same manner as the original backbone methods \u2013 we discuss additional options for training in Sec. 6. We introduce three further enhancements:\n1. Positional Encoding: As time point classification and attention are applied to each time point independently, the position of a time point within the times series can be utilised (with GAP, positional encoding would be lost through averaging) \u2013 this allows for further expressivity of the ordering of time points and enforces Req. 3 of our MILLET framework. We inject fixed positional encodings (Vaswani et al., 2017) after feature extraction.\n2. Replicate padding: Zero padding is used in the convolutional layers of the backbone architectures. However, in our interpretability experiments, we found this biased the models towards the start and end of the time series \u2013 padding with zeros was creating a false signal in the time series. As such, we replaced zero padding with replicate padding (padding with the boundary value) which alleviated the start/end bias. However, we note that particular problems may benefit from other padding strategies.\n3. Dropout: To mitigate overfitting in the new pooling methods, we apply dropout after injecting the positional encodings (p = 0.1). No dropout was used in the original backbones.\nThe original InceptionTime approach is an ensemble of five identical network architectures trained from different initialisations, where the overall output is the mean output of the five networks. To facilitate a fair comparison, we use the same approach for FCN, ResNet, and MILLET. See Sec. 6 for implementation details, and App. B for model, training, and hyperparameter details."
        },
        {
            "heading": "4 INITIAL CASE STUDY",
            "text": ""
        },
        {
            "heading": "4.1 SYNTHETIC DATASET",
            "text": "In order to explore our MILLET concept, and to evaluate the inherent interpretability of our models, we propose a new synthetic dataset called WebTraffic. By demonstrating daily and weekly seasonality, it is designed to mimic trends observed in streaming and e-commerce platforms. We inject different signatures into a collection of synthetic time series to create ten different classes: a zeroth normal class and nine signature classes. The signatures are partially inspired by the synthetic anomaly types proposed in Goswami et al. (2023). The discriminatory time points are known as we are able to control the location of the injected signatures. Therefore, we are able to evaluate if models can identify both the signature and the location of the discriminatory time points \u2013 both can be achieved inherently by our MILLET models. Each time series is of length t = 1008, and the training and test set contain 500 time series \u2013 see Fig. 3 for examples and App. C.1 for more details.\n4.2 WebTraffic RESULTS\nWe compare the four proposed MIL pooling approaches for MILLET with GAP on our WebTraffic dataset. Each pooling method is applied to the FCN, ResNet, and InceptionTime backbones. We conduct five training repeats of each model, starting from different network initialisations, and then ensemble them to a single model. We find that MILLET improves interpretability without being detrimental to predictive performance. In actuality, MILLET improves accuracy averaged across all backbones from 0.850 to 0.874, with a maximum accuracy of 0.940 for Conjunctive InceptionTime.4 In Fig. 4 we give example interpretations for this best-performing model. The model is able to identify the correct discriminatory regions for the different classes, but focuses on certain parts of the different signatures. For example, for the Spikes class, the model identifies regions surrounding the individual spike time points, and for the Cutoff class, the model mainly identifies the start and end of the discriminatory region. This demonstrates how our interpretability outputs are not only able to convey where the discriminatory regions are located, but also provide insight into the model\u2019s decision-making process, providing transparency.\nTo quantitatively evaluate interpretability on our WebTraffic dataset, we use the same process as Early et al. (2021). This approach uses ranking metrics, i.e. looking at the predicted importance order rather than actual interpretation values. The two metrics used are Area Over The Perturbation Curve to Random (AOPCR) and Normalised Discounted Cumulative Gain at n (NDCG@n). The former is used to evaluate without time point labels, and the latter is used to evaluate with time point labels.5 In this evaluation, we compare to baselines CAM (applied to the original GAP models) and SHAP (applied to all models, see App. B.3). CAM is a lightweight post-hoc interpretability method (but not intrinsically part of the model output unlike our MILLET interpretations). SHAP is much more expensive to run than CAM or MILLET as it has to make repeated forward passes of the model. In this case, we use SHAP with 500 samples, meaning it is 500 times more expensive than MILLET. In actuality, we find MILLET is over 800 times faster than SHAP (see App. E.4).\n4For complete results, see App. D.2. 5For more details on both metrics, see App. D.1. Note that NDCG@n can be used for this dataset as we know the locations of discriminatory time points (where the signatures were injected). However, for the UCR datasets used in Sec. 5, the discriminatory time points are unknown, therefore only AOPCR can be used.\nAs shown in Table 1, MILLET provides better interpretability performance than CAM or SHAP. SHAP performs particularly poorly, especially considering it is so much more expensive to run. Due to the exponential number of possible coalitions, SHAP struggles with the large number of time points. In some cases, it even has a negative AOPCR score, meaning its explanations are worse than random. For each backbone, MILLET has the best AOPCR and NDCG@n performance. The exception to this is NDCG@n for InceptionTime, where CAM is better (despite MILLET having a better AOPCR score). This is likely due to the sparsity of MILLET explanations \u2013 as shown for the Cutoff and Peak examples in Fig. 4, MILLET produces explanations that may not achieve full coverage of the discriminatory regions. While sparsity is beneficial for AOPCR (fewer time points need to be removed to decay the prediction), it can reduce NDCG@n as some discriminatory time points may not be identified (for example those in the middle of the Cutoff region)."
        },
        {
            "heading": "5 UCR RESULTS",
            "text": "We evaluate MILLET on the UCR TSC Archive (Dau et al., 2019) \u2013 widely acknowledged as a definitive TSC benchmark spanning diverse domains across 85 univariate datasets (see App. C.2). Below are results for predictive performance, followed by results for interpretability."
        },
        {
            "heading": "5.1 PREDICTIVE PERFORMANCE",
            "text": "For the three backbones, we compare the performance of the four MIL pooling methods with that of GAP. This is used to evaluate the change in predictive performance when using MILLET in a wide variety of different domains, and determine which of the pooling methods proposed in Sec. 3.2 is best. Averaged across all backbones, we find Conjunctive gives the best performance, with an accuracy improvement from 0.841 \u00b1 0.009 to 0.846 \u00b1 0.009 when compared to GAP. Conjunctive InceptionTime has the highest average accuracy of 0.856 \u00b1 0.015 (see App. D.3 for all results). Given this result, we then compare the performance of the three MILLET Conjunctive approaches with current SOTA methods, which is intended to provide better context for the performance of our newly proposed models. We select the top performing method from seven families of TSC approaches as outlined by Middlehurst et al. (2023).\nFigure 5: Critical difference diagram comparing Conjunctive MILLET methods with SOTA.\nAs HC2 is a very computationally expensive meta-ensemble of multiple different classifiers, we do not consider it to be an equal comparison to our methods. In Fig. 5 we give a critical difference (CD) diagram (Dems\u030car, 2006) for balanced accuracy. We find that 1) using Conjunctive improves performance in all cases, and 2) Conjunctive InceptionTime is comparable to (even slightly better than) the SOTA of HC2 and Hydra-MR. We provide further results in Table 2. While Conjunctive\nInceptionTime is the best approach on balanced accuracy (outperforming the HC2 and Hydra-MR SOTA methods), it is not quite as strong on the other metrics. However, it remains competitive, and for each backbone using MILLET improves performance across all metrics."
        },
        {
            "heading": "5.2 INTERPRETABILITY PERFORMANCE",
            "text": "In order to understand the interpretability of our MILLET methods on a wide variety of datasets, we evaluate their interpretability on the same set of 85 UCR datasets used in Sec. 5.1.\nAs the UCR datasets do not have time point labels, we can only evaluate model interpretability using AOPCR. Averaged across all backbones, we find that MILLET has a best AOPCR of 6.00, compared to 5.71 achieved by GAP. Of the individual pooling methods within MILLET, we find that Conjunctive has the best interpretability performance. Attention performs poorly \u2013 this is expected as it does not create class-specific interpretations, but only general measures of importance; also identified in general MIL interpretability by Early et al. (2021). In Fig. 6 we observe a trade-off between interpretability and prediction, something we believe is insightful for model selection in practice. As backbone complexity increases, predictive performance increases while interpretability decreases.6 The\nPareto front shows MILLET dominates GAP for FCN and InceptionTime, but not ResNet. MILLET gives better interpretability than GAP for individual ResNet models, but struggles with the ensemble ResNet models. For complete results and a further discussion, see App. D.3.\n6InceptionTime is the most complex backbone, followed by ResNet, and then FCN is the simplest.\nFig. 7 shows interpretations for CAM, SHAP, and MILLET on LargeKitchenAppliances; a UCR dataset for identifying household electric appliances (Washing Machine, Tumble Dryer, or Dishwasher) from electricity usage. From the MILLET interpretability outputs, we identify that the model has learnt different motifs for each class: long periods of usage just above zero indicate Washing Machine, spikes above five indicate Tumble Dryer, and prolonged usage at just below five indicates Dishwasher. The Washing Machine example contains short spikes above five but MILLET identifies these as refuting the prediction, suggesting the model does not relate these spikes with the Washing Machine class. SHAP provides very noisy interpretations and does not show strong performance on the perturbation curves. Similar to our findings for WebTraffic (Sec. 4.2), MILLET provides sparser explanations than CAM, i.e. focusing on smaller regions and returning fewer discriminatory time points, which is helpful when explaining longer time series. We provide further analysis in the Appendix: head-to-heads (App. D.3), false negatives (App. E.1), dataset variance (App. E.2), model variance (App. E.3), run time (App. E.4), and an ablation study (App. E.5)."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "The MILLET framework presented in this work is the first comprehensive analysis of MIL for TSC. Its positive value is demonstrated across the 85 UCR datasets and the WebTraffic dataset proposed in this work. MILLET provides inherent mechanisms to localise, interpret, and explain influences on model behaviour, and improves predictive accuracy in most cases. Through the transparent decisionmaking gained from MILLET, practitioners can improve their understanding of model dynamics without the need for expensive (and often ineffective) post-hoc explainability methods. In addition, MILLET explanations are sparse \u2013 they distill the salient signatures of classes to a small number of relevant sub-sequences, which is especially important for long time series. We believe this work lays firm foundations for increased development of MIL methods in TSC, and facilitates future work:\nExtension to more datasets: This could include the full set of 142 datasets used in Bake Off Redux (Middlehurst et al., 2023), multivariate datasets, and variable length time series. Variable lengths would not require any methodological changes (contrary to several other methods), but multivariate settings would require interpretability to consider the input channel (Hsieh et al., 2021).\nApplication to other models: We have demonstrated the use of MILLET for DL TSC models. Future work could extend its use to other types of TSC models, e.g. the ROCKET (convolutional) family of methods (Dempster et al., 2020), which includes Hydra-MR. Our proposed pooling method, Conjunctive, is also applicable in general MIL problems beyond TSC.\nPre-training/fine-tuning: While we trained MILLET models in an end-to-end manner, an alternative approach is to take a pre-trained GAP model, replace the GAP layers with one of the proposed MIL pooling methods, and then fine-tune the network (facilitating faster training)."
        },
        {
            "heading": "REPRODUCIBILITY STATEMENT",
            "text": "The code for this project was implemented in Python 3.8, with PyTorch as the main library for machine learning. A standalone code release will be undertaken following publication, which will include our synthetic dataset and the ability to use our plug-and-play MILLET models.\nModel training was performed using an NVIDIA Tesla V100 GPU with 16GB of VRAM and CUDA v12.0 to enable GPU support. For reproducibility, all experiments with a stochastic nature (e.g. model training, synthetic data generation, and sample selection) used pseudo-random fixed seeds. A list of all required libraries will be given alongside the code release."
        },
        {
            "heading": "A WHY MIL?",
            "text": "To answer the question of \u201cwhy MIL\u201d, we first consider the requirements for interpreting TSC models. Our underlying assumption is that, for a classifier to predict a certain class for a time series, there must be one or more underlying motifs (a single time point or a collection of time points) in the time series that the model has identified as being indicative of the predicted class. In other words, only certain time points within a time series are considered discriminatory by the model (those that form the motifs), and the other time points are ignored (background time points or noise). This could also be extended to class refutation, where the model has learnt that certain motifs indicate that a time series does not belong to a particular class. To this end, the objective of TSC interpretability is then to uncover these supporting and refuting motifs, and present them as an explanation as to why a particular class prediction has been made.\nWe next consider how we could train a model to find these motifs if we already knew where and what they were, i.e. in a conventional supervised learning sense where the motifs are labelled. In this case, we would be able to train a model to predict a motif label from an input motif \u2013 in the simplest case this could be predicting whether a motif is discriminatory or non-discriminatory. This model could then be applied to an unlabelled time series and used to identify its motifs. Effectively, this hypothetical motif model is making time point level predictions.\nUnfortunately, it is very difficult to train models in the above manner. The vast majority of time series datasets are only labelled at the time series level \u2013 no labels are provided at the time point level, therefore the motifs and their locations are unknown. While there are several successful deep learning models that are able to learn to make time series level predictions from time series level labels (e.g FCN, ResNet, InceptionTime), they are black boxes \u2013 they provide no inherent interpretation in the form of supporting/refuting motifs via time point level predictions. Without time point level labels, conventional supervised learning is unable to develop models that inherently make time point level predictions. While post-hoc interpretability methods could be used to uncover the motifs, they can be expensive (in the case of LIME and SHAP), or as we show in this work, inferior to inherent interpretability. As such, we can draw a set of requirements for a new and improved interpretable TSC approach:\n1. Inherent interpretability: The model should provide time point predictions (i.e. motif identification) as part of its time series prediction process. This means interpretations are gained effectively for free as a natural byproduct of time series prediction. It also means expensive or ineffective post-hoc interpretability approaches are not required.\n2. Learn from time series labels: As stated above, time series classification datasets rarely provide time point-level labels. Therefore, the model must be able to learn something insightful at the time point level given only time series labels.\n3. Provide a unified framework: As there are a diverse range of existing TSC methods of different families, an effective TSC interpretability approach should be as widely applicable as possible to facilitate continued research in these different areas. This would also mean existing bodies of research can be applied in conjunction with the framework.\nGiven these requirements, we advocate for MIL as an appropriate method for an inherently interpretable TSC framework. MIL is designed for settings with bags of instances, where only the bags are labelled, not the instances. In a TSC setting, this equates to having time series labels without time point labels, which is exactly what we outlined above (and describes the vast majority of TSC datasets). Furthermore, certain types of existing MIL approaches, for example Instance and Additive, work by first making a prediction for every time point, and then aggregating over these predictions to make a time series prediction. This is inherently interpretable and facilitates motif identification. Finally, the over-arching concept of MIL for TSC, i.e. learning from time series labels but making both time point and time series predictions, is not tied to any one family of machine learning approach.\nWe also consider answers to potential questions about alternative solutions:\n1. Q: Why not label the motifs/time points to allow for supervised learning? A: As discussed above, it is rare for a time series dataset to have time point labels. It could be possible to label the motifs, for example by having medical practitioners identify\ndiscriminatory irregular patterns in ECG data. However, this labelling process is very timeconsuming, and in some cases would require domain expertise, which is challenging and expensive to acquire. Furthermore, it would then require anyone interested in applying such supervised techniques to their own data to fully label everything at the time point level, increasing the cost and time of developing new datasets.\n2. Q: Why not label some of the motifs/time points and use a semi-supervised approach? A: While it might be possible to train a semi-supervised model that only requires some of the time points to be labelled, the model is no longer end-to-end. As the model only learns to predict at the time point level, it does not provide time series level predictions itself. Rather, some additional process is required to take the time point predictions and transform them into a time series prediction. Furthermore, a semi-supervised model has no method for leveraging both the time series labels and the partial time point labels.\n3. Q: What does MIL achieve beyond methods such as attention? A: While attention has been utilised previously in TSC, it does not provide class-specific interpretations, only general measures of importance across all classes. So while attention might identify motifs, it does not state which class these motifs belong to, nor whether they are supporting or refuting. Furthermore, attention is far less explicit than time point predictions \u2013 there is no guarantee that attention actually reflects the underlying motifs of decision-making, whereas in MIL the time point predictions directly determine the time series prediction."
        },
        {
            "heading": "A.1 RELATED WORK (CONTINUED)",
            "text": "In this section, we further discuss existing TSC interpretability approaches and contrast them with our approach. MILLET is a time point-based explanation approach (following the taxonomy of Theissler et al., 2022), meaning it produces interpretations for each individual time point in a time series. This is in contrast to other types of methods such as subsequence-based techniques, which provide interpretations at a lower granularity. As such, we choose to compare against interpretability methods with the same level of granularity, i.e. those that also make time point-based interpretations (such as CAM and SHAP).\nWhile alternative time series-specific SHAP approaches such as TimeSHAP (Bento et al., 2021) or WindowSHAP (Nayebi et al., 2023) could have been used, these both reduce the exponential sampling problem of SHAP by grouping time points together. As such, they do not have the same granularity as MILLET and do not meet our requirements. Two similar methods to SHAP are LIME (Ribeiro et al., 2016) and MILLI (Early et al., 2021), but these require careful tuning of hyperparameters (which SHAP does not), so are infeasible to run over the large number of datasets used in this work. LIMESegements (Sivill & Flach, 2022), an extension of LIME for TSC, has the same issue as TimeSHAP and WindowSHAP mentioned above: it groups time points into subsequences so does not produce interpretations at the same granularity as MILLET. Alternative approaches for TSC interpretability have similar granularity issues or are often very expensive to compute (relative to the cost of using MILLET or CAM). Such methods include Dynamask (Crabbe\u0301 & Van Der Schaar, 2021), WinIT (Leung et al., 2022), and TimeX (Queen et al., 2023). If MILLET were to be extended to multivariate TSC problems, it would be compatible with Temporal Saliency Rescaling (TSR; Ismail et al., 2020), which has been shown to improve time point-based interpretability methods in multivariate settings.\nWhile our choice of interpretability evaluation metrics are based on those used by Early et al. (2021), different metrics are used in existing TSC interpretability works, but these come with their own set of disadvantages. Metrics such as area under the precision curve (AUP) and area under the recall curve (AUR) are useful in that they separate whether the identified time points are indeed discriminatory (precision) from whether all discriminatory time points are identified (recall/coverage). While this would be beneficial in evaluating the effect of MILLET sparsity, it does not account for the ordering of time points in the interpretation, i.e. we want to reward the interpretation method for placing discriminatory time points earlier in the ordering (and punish it for placing non-discriminatory time points earlier on); this is something that NDCG@n achieves. The mean rank metric (Leung et al., 2022) also suffers from this issue \u2014 it is effectively an unweighted version of NDCG@n. Furthermore, AUP and AUR metrics require time point labels and sometimes need access to the underlying data generation process for resampling; AOPCR does not need either."
        },
        {
            "heading": "B MODEL DETAILS",
            "text": "B.1 MILLET MODEL ARCHITECTURES\nIn the following section, we provide details on our MILLET models. For conciseness, we omit details on the backbone architectures. See Wang et al. (2017) for details on FCN and ResNet, and Ismail Fawaz et al. (2020) for details on InceptionTime. Each of the three feature extractor backbones used in this work produce fixed-size time point embeddings of length 128: Zi \u2208 Rt\u00d7128 = [z1i , z2i , . . . , zti ], where t is the input time series length. This is the initial Feature Extraction phase and uses unchanged versions of the original backbones.\nA breakdown of the general model structure is given in Eqn. A.1. Note how Positional Encoding is only applied after Feature Extraction. Furthermore, Positional Encoding and Dropout are only applied in MILLET models, i.e. when using Instance, Attention, Additive, or Conjunctive pooling.\nFeature Extraction \u2192 Positional Encoding \u2192 Dropout \u2192 MIL Pooling (A.1)\nBelow we detail the Positional Encoding processes, and then give architectures for each of the MIL Pooling approaches. These are kept unchanged across backbones."
        },
        {
            "heading": "B.1.1 POSITIONAL ENCODING",
            "text": "Our approach for Positional Encoding uses fixed positional encodings (Vaswani et al., 2017):\nPE(pos,2i) = sin(pos/10000 2i/dmodel),\nPE(pos,2i+1) = cos(pos/10000 2i/dmodel), (A.2)\nwhere pos = [1, . . . , t] is the position in the time series, dmodel = 128 is the size of the time point embeddings, and i = [1, . . . , dmodel/2]. As such, the Positional Encoding output is the same shape as the input time point embeddings (Rt\u00d7128). The positional encodings are then simply added to the time point embeddings.\nIn cases where time points are removed from the bag, e.g. when calculating AOPCR (see App. D.1), we ensure the positional encodings remain the same for the time points that are still in the bag. For example, if the first 20 time points are removed from a time series, the 21st time point will still have positional encoding PE(21), not PE(1)."
        },
        {
            "heading": "B.1.2 MIL POOLING ARCHITECTURES",
            "text": "In Tables A.1 to A.5 we provide architectures for the different MIL pooling methods used in this work (see Fig. 2 for illustrations). Each row describes a layer in the pooling architecture. In each case, the input is a bag of time point embeddings (potentially with positional encodings and dropout already applied, which does not change the input shape; see App. B.1.1). The input is batched with a batch size of b, and each time series is assumed to have the same length t. Therefore, the input is four dimensional: batch size \u00d7 number of channels \u00d7 time series length \u00d7 embedding size. However, in this work, as we are using univariate time series, the number of channels is always one. The problem has c classes, and the pooling methods produce logit outputs \u2013 softmax is later applied as necessary."
        },
        {
            "heading": "B.2 TRAINING AND HYPERPARAMETERS",
            "text": "In this work, all models were trained in the same manner. We used the Adam optimiser with a fixed learning rate of 0.001 for 1500 epochs, and trained to minimise cross entropy loss. Training was performed in an end-to-end manner, i.e. all parts of the networks (including the backbone feature extraction layers) were trained together, and no pre-training or fine-tuning was used. Dropout (if used) was set to 0.1, and batch size was set to min(16, \u230anum training time series/10\u230b) to account for datasets with small training set sizes. For example, if a dataset contains only 100 training time series, the batch size is set to 10.\nNo tuning of hyperparameters was used \u2013 values were set based on the those used for training the original backbone models. As the existing DL TSC methods used fixed hyperparameters, our decision not to tune the hyperparameters facilitates a fairer comparison. It also has the benefit of providing a robust set of default values for use in derivative works. However, better performance could be achieved by tuning hyperparameters for individual datasets. We would expect hyperparameter tuning for MILLET to have a greater impact than doing so for the GAP versions of the models as\nMILLET provides more scope for tuning (e.g. dropout, attention-head size, and whether to include positional encodings). As greater flexibility is achieved by having the ability to add/remove/tune the additional elements of MILLET, this would lead to more specialised models (and larger performance improvements) for each dataset. For example, in our ablation study (App. E.5), we found that positional encoding was beneficial for some datasets but not others.\nNo validation datasets were used during training or evaluation. Instead, the final model weights were selected based on the epoch that provides the lowest training loss. As such, training was terminated early if a loss of zero was reached (which was a very rare occurrence, but did happen). Models started with random weight initialisations, but pseudo-random fixed seeds were used to enable reproducibility. For repeat training, the seeds were different for each repeat (i.e. starting from different random initialisations), but these were consistent across models and datasets."
        },
        {
            "heading": "B.3 SHAP DETAILS",
            "text": "In our SHAP implementation we used random sampling of coalitions. Guided sampling (selecting coalitions to maximise the SHAP kernel) would have proved too expensive: the first coalitions sampled would be all the single time point coalitions and all the t \u2212 1 length coalitions (for a time series of length t), which results in 2t coalitions and thus 2t calls to the model. In the case of WebTraffic, this would be 2016 samples, rather than the 500 we used with random sampling (which still took far longer to run than MILLET, see App. E.4). Furthermore, Early et al. (2021) showed random sampling to be equal to or better than guided sampling in some cases."
        },
        {
            "heading": "C DATASET DETAILS",
            "text": "C.1 SYNTHETIC DATASET DETAILS (WebTraffic)\nIn our synthetic time series dataset, WebTraffic, each time series is a week long with a sample rate of 10 minutes (60 \u2217 24 \u2217 7/10 = 1008 time points). The training and test set are independently generated using fixed seeds to facilitate reproducibility, and are both balanced with 50 time series per class (500 total time series for each dataset).\nTo explain the synthetic time series generation process, we first introduce a new function:\nWarpedSin(a, b, p, s, x) = a\n2 sin\n( x\u2032 \u2212 sin(x \u2032)\ns\n) + b,where x\u2032 = 2\u03c0(x\u2212 p). (A.3)\nParameters a, b, p, and s control amplitude, bias (intercept), phase, and skew respectively. WarpedSin is used to generate daily seasonality in the following way:\nSampleDay(aD, b, p, s, \u03c3, j) = N (RateDay(aD, b, p, s, j), \u03c3), (A.4) RateDay(aD, b, p, s, j) =WarpedSin(aD, b, p+ 0.55, s, j/144) (A.5)\nwhere j \u2208 [1, . . . , 1008] is the time index and \u03c3 is a parameter that controls the amount of noise created when sampling (via a normal distribution). The daily rates provide daily seasonality (i.e. peaks in the evening and troughs in the morning). However, to take this further we also add weekly seasonality (i.e. more traffic at the weekends than early in the week). To do so, we further utilise WarpedSin:\nRateWeek(aW , j) =WarpedSin(aW , 1, 0.6, 2, j/1008). (A.6)\nTo add this weekly seasonality, we multiply the daily sampled values by the weekly rate. Therefore, to produce a base time series, we arrive at a formula with six parameters:\nSampleWeek(aD, aW , b, p, s, \u03c3) = SampleDay(aD, b, p, s, \u03c3, j) \u2217RateWeek(aW , j) for j \u2208 [1, . . . , 1008]. (A.7)\nTo generate a collection of n time series, we sample the following parameter distributions n times (i.e. once for every time series we want to generate):\n\u2022 Amplitude daily: aD \u223c UR(2, 4) \u2022 Amplitude weekly aW \u223c UR(0.8, 1.2) \u2022 Bias b \u223c UR(2.5, 5) \u2022 Phase p \u223c UR(\u22120.05, 0.05) \u2022 Skew s \u223c UR(1, 3) \u2022 Noise \u03c3 \u223c UR(2, 4)\nWe use a \u223c UR(b, c) to denote uniform random sampling between b and c, where a \u2208 R. Below, we also use uniform random integer sampling a\u2032 \u223c UZ(b, c), where a\u2032 \u2208 Z. The above generation process results in a collection of n time series, but currently they are all class zero (Class 0: None) as they have had no signatures injected. We describe how we inject each of the nine signature types below. Aside from the Spikes signature (Class 1), all signatures are injected in random windows of length l \u223c UZ(36, 288) starting in position p \u223c UZ(0, t \u2212 l). The minimum window size of 36 corresponds to 0.25 days, and the maximum size of 288 corresponds to 2 days. In all cases, values are clipped to be non-negative, i.e. all time points following signature injection are \u2265 0. These methods are inspired by, but not identical to, the work of Goswami et al. (2023). We provide an overview of the entire synthetic dataset generation process in Fig. A.1. Exact details on the injected signatures are given below, along with focused examples in Fig. A.2.\nClass 1: Spikes Spikes are injected at random time points throughout the time series, with probability p = 0.01 for each time point. The magnitude of a spike is drawn from N (3.0, 2.0), and then added to or subtracted from the original time point value with equal probability.\nClass 2: Flip The randomly selected window is flipped in the time dimension.\nClass 3: Skew A skew is applied to the time points in the randomly selected window. A skew amount is first sampled from UR(0.25, 0.45), which is then add to or subtracted from 0.5 with equal probability. This gives a new skew value w \u2208 [0.05, 0.25] \u222a [0.75, 0.95]. The random window is then interpolated such that the value at the midpoint is now located at time point \u230aw \u2217 l\u230b within the window, i.e. stretching the time series on one side and compressing it on the other.\nClass 4: Noise Noise is added to the random window. The amount of noise is first sampled from \u03c3Noise \u223c UR(0.5, 1.0). Then, for each time point in the selected window, noise is added according to N (0, \u03c3Noise).\nClass 5: Cutoff A cutoff value is sampled from c \u223c UR(0.0, 0.2) The values in the random window are then set to N (c, 0.1).\nClass 6: Average This signature is the opposite of noise injection, i.e. applying smoothing to the values in the random window. This is achieved through applying a moving average with a window size sampled from UZ(5, 10).\nClass 7: Wander A linear trend is applied to values in the random window. The trend linearly transitions from 0 to UR(2.0, 3.0), and is then added to or subtracted from the values in the window with equal probability.\nClass 8: Peak A smooth peak is created from the probability density function (PDF) of N (0, 1) from -5 to 5, and then the values are multiplied by a scalar sampled from UR(1.5, 2.5). Values in the random window are then multiplied by the values of the peak, creating a smooth transition from the existing time series.\nClass 9: Trough The same method to generate the Peak signatures is used to generate a trough, but the PDF values are instead multiplied by UR(\u22122.5,\u22121.5) (same scalar sample range but negative)."
        },
        {
            "heading": "C.2 UCR DATASET DETAILS",
            "text": "For the UCR datasets, we used the original train/test splits as provided from the archive source.7 z-normalisation was applied to datasets that were not already normalised. The exhaustive list of univariate UCR datasets used in this work is: Adiac, ArrowHead, Beef, BeetleFly, BirdChicken, Car, CBF, ChlorineConcentration, CinCECGTorso, Coffee, Computers, CricketX, CricketY, CricketZ, DiatomSizeReduction, DistalPhalanxOutlineAgeGroup, DistalPhalanxOutlineCorrect, DistalPhalanxTW, Earthquakes, ECG200, ECG5000, ECGFiveDays, ElectricDevices, FaceAll, FaceFour, FacesUCR, FiftyWords, Fish, FordA, FordB, GunPoint, Ham, HandOutlines, Haptics, Herring, InlineSkate, InsectWingbeatSound, ItalyPowerDemand, LargeKitchenAppliances, Lightning2, Lightning7, Mallat, Meat, MedicalImages, MiddlePhalanxOutlineAgeGroup, MiddlePhalanxOutlineCorrect, MiddlePhalanxTW, MoteStrain, NonInvasiveFetalECGThorax1, NonInvasiveFetalECGThorax2, OliveOil, OSULeaf, PhalangesOutlinesCorrect, Phoneme, Plane, ProximalPhalanxOutlineAgeGroup, ProximalPhalanxOutlineCorrect, ProximalPhalanxTW, RefrigerationDevices, ScreenType, ShapeletSim, ShapesAll, SmallKitchenAppliances, SonyAIBORobotSurface1, SonyAIBORobotSurface2, StarLightCurves, Strawberry, SwedishLeaf, Symbols, SyntheticControl, ToeSegmentation1, ToeSegmentation2, Trace, TwoLeadECG, TwoPatterns, UWaveGestureLibraryAll, UWaveGestureLibraryX, UWaveGestureLibraryY, UWaveGestureLibraryZ, Wafer, Wine, WordSynonyms, Worms, WormsTwoClass, Yoga."
        },
        {
            "heading": "D ADDITIONAL RESULTS",
            "text": "D.1 INTERPRETABILITY METRICS\nBelow we provide more details on the metrics used to evaluate interpretability, which are based on the process proposed by Early et al. (2021).\nAOPCR: Evaluation without time point labels When time point labels are not present, the model can be evaluated via perturbation analysis. The underlying intuition is that, given a correct ordering of time point importance in a time series, iteratively removing the most important (discriminatory) time points should cause the model prediction to rapidly decrease. Conversely, a random or incorrect ordering will lead to a much slower decrease in prediction. Formally, when evaluating the interpretations generated by a classifier Fc for a time series Xi = {x1i ,x2i , . . . ,xti} with respect to class c, we first re-order the time series according to the importance scores (with the most important time points first): Oi,c = {o1i ,o2i , . . . ,oti}. The perturbation metric is then calculated by:\nAOPC(Xi,Oi,c) = 1\nt\u2212 1 t\u22121\u2211 j=1 Fc(Xi)\u2212 Fc(MoRF (Xi,Oi,c, j)), (A.8)\nwhere MoRF (Xi,Oi,c, j) =MoRF (Xi,Oi,c, j \u2212 1) \\ {o ji}, and MoRF (Xi,Oi,c, 0) = Xi.\n7https://www.cs.ucr.edu/\u02dceamonn/time_series_data_2018/\nMoRF is used to signify the ordering is Most Relevant First. In Eqn. A.8, the perturbation curve is calculated by removing individual time points and continues until all but one time point (the least important as assessed by the model) is left. This is expensive to compute, as a call to the model must be made for each perturbation. To improve the efficiency of this calculation, we group time points together into blocks equal to 5% of the total time series length, and only perturb the time series until 50% of the time points have been removed. As such, we only need to make 10 calls to the model per time series evaluation.\nTo facilitate better comparison between models, we normalise by comparing to a random ordering. To compensate for the stochastic nature of using random orderings, we average over three different random orderings, where R(r)i is the r th repeat random ordering:\nAOPCR(Xi,Oi,c) = 1\n3 3\u2211 r=1 ( AOPC ( Xi,Oi,c)\u2212AOPC(Xi,R(r)i )) . (A.9)\nNDCG@n: Evaluation with time point labels If the time point labels are known, a perfect ordering of time point importance would have every discriminatory time point occurring at the start. If there are n discriminatory time points, we would expect to see these in the first n places in the ordered interpretability output. The fewer true discriminatory time points there are in the first n places, the worse the interpretability output. Furthermore, we want to reward the model for placing discriminatory time points earlier in the ordering (and punish it for placing non-discriminatory time points earlier on). This can be achieved by placing a higher weight on the start of the ordering. Formally,\nNDCG@n(Oi,c) = 1\nIDCG n\u2211 j=1 rel(Oi,c, j) log2(j + 1) , (A.10)\nwhere IDCG = n\u2211\nj=1\n1\nlog2(j + 1) ,\nand rel(Oi,c, j) = { 1 if oji is a discriminatory time point, 0 otherwise.\nD.2 WebTraffic ADDITIONAL RESULTS\nWe first provide a complete set of results for predictive performance on WebTraffic, comparing the GAP with MILLET. Tables A.6, A.7, A.8 give results on accuracy, AUROC, and loss respectively.\nTable A.6: WebTraffic Accuracy.\nTable A.7: WebTraffic AUROC.\nFCN ResNet ITime Mean\nGAP 0.939 0.633 0.268 0.614 Attention 0.863 0.701 0.279 0.614 Instance 0.871 0.709 0.257 0.612 Additive 0.882 0.678 0.252 0.604 Conjunctive 0.866 0.638 0.277 0.594\nTable A.9 shows the complete interpretability results. Best performance in each case comes from one of MILLET Instance, Additive, or Conjunctive. The exception is NDCG@n for CAM on InceptionTime, which, as discussed in Section 4.2, is likely due to the sparsity of MILLET explanations. We also note that SHAP performs very poorly across all pooling methods, and that Attention is also worse than the other methods (as it does not make class-specific explanations)."
        },
        {
            "heading": "D.3 UCR ADDITIONAL RESULTS",
            "text": "In this section we give extended results on the UCR datasets. First, Table A.10 compares predictive performance of the MILLET methods \u2013 we find that Conjunctive gives the best average performance. In Table A.11 we give MILLET interpretability results on the UCR datasets for both individual and ensemble models. Interestingly, we find that MILLET performs well on all cases except the ensemble ResNet models. In this case, its performance drops significantly compared to the individual ResNet performance \u2013 something that is not observed for the other backbones. We observe something similar in our ablation study, see App. E.5. We then compare MILLET performance with that of six SOTA methods in Table A.12.8\nWe perform a further direct comparison against the best two SOTA results, HC2 and Hydra-MR, allowing us to evaluate how many and on which datasets MILLET performs better. As shown in Fig. A.3, we find that our best-performing MILLET approach (Conjunctive InceptionTime) wins or draws on 48/85 (56.5%), 49/85 (57.7%), and 52/85 (61.2%) UCR datasets against InceptionTime, HC2, and Hydra-MR respectively."
        },
        {
            "heading": "E ADDITIONAL EXPERIMENTS",
            "text": "E.1 INVESTIGATION OF FALSE NEGATIVES\nOur experiments showed Conjunctive pooling gives better predictive and interpretability performance than other pooling methods. To further investigate why this is the case, we analysed its performance on our proposed WebTraffic dataset. We found its accuracy was between 0.86 and 1.0 for nine of the ten classes, showing it had consistently strong performance in identifying most signatures. However, for class 8 (Peak), its accuracy dropped to 0.76 \u2014 a rather large drop relative to the other classes. Class 0 (None) was the prediction that was made for the majority of the incorrect predictions for class 8, i.e. the model failed to identify the peak and did not find any other class signatures, so predicted the None class. We used the interpretability facilitated by MILLET to investigate what was happening with these incorrect predictions. We found that, in some cases, the model was able to identify the correct region (positive predictions for class 8 at the location of the peak) despite its final prediction being incorrect. Examples are shown in Fig. A.4 \u2014 observe how the middle example shows support for class 8 (Peak) in the correct region despite the model getting the overall prediction incorrect. This identification of incorrect predictions and ability to analyse the model\u2019s decision-making in more detail further highlights how MILLET can be useful in production."
        },
        {
            "heading": "E.2 PERFORMANCE BY DATASET PROPERTIES",
            "text": "As discussed in Sec. 5.1, Conjunctive InceptionTime had the best performance on balanced accuracy when evaluated over 85 UCR datasets (outperforming SOTA methods such as HC2 and Hydra-MR). To investigate whether this improvement is due to better performance on imbalanced datasets, we assessed balanced accuracy with respect to test dataset imbalance. To measure the imbalance of a dataset, we use normalised Shannon entropy:\nDataset Balance = \u2212 1 log c c\u2211 i=1 ci n log (ci n ) , (A.11)\nwhere c is the number of classes, ci is the number of time series for class i, and n is the total number of time series in the dataset (\u2211c i=1 ci = n ) . This gives a score of dataset balance between\n0 and 1, where 1 is perfectly balanced (equal number of time series per class) and values close to 0 indicate high levels of imbalance. We used a threshold of 0.9 to identify imbalanced datasets, and provide comparisons of Conjunctive InceptionTime with GAP InceptionTime, HC2, and Hydra-MR on these datasets in Fig. A.5.\nFrom these results, we identify that MILLET has better performance than the SOTA methods when there is high (test) dataset imbalance. It wins on 8/13, 9/13, and 9/13 datasets against GAP InceptionTime, HC2, and Hydra-MR respectively, and improves balanced accuracy by up to 7.4%. This demonstrates MILLET is more robust to dataset imbalance than the other methods, potentially due to the fact that is has to make timestep predictions. Note this is without any specific focus on optimising for class imbalance, e.g. weighted cross entropy loss could be used during training to further improve performance on imbalanced datasets.\nUsing the UCR results, we now compare performance across time series length and the number of training time series. Figure Fig. A.6 shows the average balanced accuracy rank on different partitions of the UCR datasets for HC2, Hydra-MR, InceptionTime, and Conjunctive InceptionTime. The results are relatively consistent across different time series lengths. However, for the number of training time series, we see that Conjunctive InceptionTime is worse than GAP InceptionTime for smaller datasets, but excels on the larger datasets."
        },
        {
            "heading": "E.3 MODEL VARIANCE STUDY",
            "text": "As noted by Middlehurst et al. (2023), while InceptionTime performs well overall, it often performs terribly on certain datasets, i.e. it has high variance in its predictive performance. In Fig. A.7, we show that MILLET does aid in reducing variance while improving overall performance. Notably, Conjunctive InceptionTime has lower variance than HC2 and Hydra-MR."
        },
        {
            "heading": "E.4 RUN TIME ANALYSIS",
            "text": "Below we analyse how MILLET increases the complexity of the original backbone models. We first compare the number of model parameters (App. E.4.1) for different backbone and pooling combinations applied to the UCR Fish dataset, which is chosen as it is relatively central in the distribution of dataset statistics (175 training time series, 463 time points per time series, and 7 classes). We then compare the training and inference times on this same dataset (App. E.4.2). Finally, we calculate the time complexity of the differnet pooling approaches and assess how they scale with respect to the number of timesteps and the number of classes (App. E.4.3)."
        },
        {
            "heading": "E.4.1 NUMBER OF PARAMETERS",
            "text": "In Table A.13, we detail the number of model parameters for the different backbones and aggregation approaches on Fish. Instance has the same number of parameters as GAP as the only change in the aggregation process is to swap the order in which pooling and classification are applied. Similarly, Attention, Additive, and Conjunctive all have the same number of parameters as each other, as they all include the same attention head (just applied in different ways). Including this attention head only leads to an increase of approximately 0.4% in the number of parameters. Note these exact values will change for datasets with different numbers of classes, but the number of additional parameters for the attention head will remain the same."
        },
        {
            "heading": "E.4.2 TRAINING/INFERENCE TIME",
            "text": "In Table A.14, we compare model training and inference times for Fish, and also include how long SHAP takes to run for these models. Due to the additional complexity of these methods (e.g. making time point predictions, applying attention, and using positional embeddings), the training times increase by up to 6%. Similarly, inference time increases by up to 7.5%. For SHAP, generating a single explanation takes 6+ seconds compared to the MILLET explanations which are generated as part of the inference step. Using Conjunctive as an example, SHAP is over 800 times slower than MILLET."
        },
        {
            "heading": "E.4.3 TIME COMPLEXITY ANALYSIS",
            "text": "To provide more thorough theoretical analysis, we give results for the number of real multiplications in the pooling methods (Freire et al., 2022). We focus solely on the pooling methods, omitting the computation of the backbone (as it is independent of the choice of pooling method). We also omit the computation of activation functions and other non-linear operations. Results are given for single inputs (no batching) and ignore possible underlying optimisation/parallelisation of functions. We first define the number of real multiplications for common functions of the pooling methods:\n\u2022 Feed-forward layer: t \u2217 d \u2217 o, where t is the number of timesteps, d is the input size (128 in this work), and o is the output size (based on Shah & Bhavsar, 2022).\n\u2022 Attention head consisting of two layers: t \u2217 d \u2217 a+ t \u2217 a, where a is the size of the hidden layer in the attention head (8 in this work). This is consistent for the architectures that use attention (Attention, Additive, and Conjunctive); see Tables A.2, A.4 and A.5.\n\u2022 Weight or average a list of tensors: n \u2217 l, where n is the number of tensors and l is the length of each tensor.\nGiven these definitions, we calculate the overall number of multiplications for each pooling method. We provide results in Table A.15 and visualisation in Fig. A.8. We make several observations:\n\u2022 Attention has the best scaling with respect to t and c. However, it remains a poor choice overall due to its poor interpretability performance compared to other methods.\n\u2022 GAP + CAM and Instance are the next fastest pooling methods. Instance is more efficient than GAP + CAM when t \u2217 c < d \u2217 (t+ c). In this work, as d = 128, Instance is marginally quicker than GAP + CAM.\n\u2022 Additive and Conjunctive are the two slowest methods due to the additional overhead of applying attention and also making instance predictions, but the margin between them and GAP + CAM / Instance is not as large as one might expect. This is due to the small hidden layer size in the attention head (a = 8). Conjunctive is more efficient than Additive when c < d, which is true in all cases in this work as d = 128.\n\u2022 In general, Conjunctive is the best choice when computational cost is less important than predictive performance. If faster computation is required, or if the number of timesteps or number of classes is very large, Instance becomes a better choice."
        },
        {
            "heading": "E.5 ABLATION STUDY",
            "text": "Our MILLET models make several improvements over the backbones, adding MIL pooling, positional encodings, replicate padding, and dropout (Sec. 3.4). To understand where the gains in performance over the backbone models come from, we conduct an ablation study. To do so, we run additional model training runs, starting with the original backbone models and incrementally adding MILLET components in the order: MIL pooling, positional encoding, replicate padding, dropout, and ensembling. The final stage represents the full MILLET implementation. To reduce overheads in model training time and compute resources, we focus on Conjunctive InceptionTime and conduct the study on the three UCR datasets where MILLET shows the biggest increase in balanced accuracy over the backbone: BeetleFly, Lightning7, and FaceAll.\nIn Table A.16 we provide results of the ablation study for balanced accuracy (predictive performance). We note that, on average, each component of MILLET improves performance, and the complete implementation (Step 6) has the best performance. For these datasets, the use of MIL pooling always improves performance over GAP. Additional components then change the performance differently for the different datasets. For example, positional encoding is very important for BeetleFly, but replicate padding is most important for FaceAll. Interestingly, replicate padding gives the biggest average performance increase across these datasets. However, these results are confounded by the order of implementation, i.e. replicate padding is only applied after MIL pooling and positional encoding have been applied. As such, further studies are required to untangle the contribution of each component, but that is beyond the scope of this work.\nIn Table A.17 we provide results of the ablation study for AOPCR (interpretability performance). Interesting, the addition of MIL pooling and positional encoding is detrimental to interpretability in these examples, despite improving predictive performance. However, interpretability improves once replicate padding and dropout are included. Further work is required to understand if these interpretability increases would also occur if replicate padding and dropout were applied to the GAP backbones, or if they improve performance when applied in conjunction with MIL pooling. Finally, we observe that interpretability decreases when ensembling the models. This is somewhat intuitive, as the interpretations are now explaining the decision-making of five models working in conjunction \u2013 it would be interesting to explore the difference in interpretations when analysing each model in the ensemble separately rather than together."
        }
    ],
    "year": 2023
}