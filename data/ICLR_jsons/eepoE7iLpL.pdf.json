{
    "abstractText": "Learning neural subset selection tasks, such as compound selection in AI-aided drug discovery, have become increasingly pivotal across diverse applications. The existing methodologies in the field primarily concentrate on constructing models that capture the relationship between utility function values and subsets within their respective supersets. However, these approaches tend to overlook the valuable information contained within the superset when utilizing neural networks to model set functions. In this work, we address this oversight by adopting a probabilistic perspective. Our theoretical findings demonstrate that when the target value is conditioned on both the input set and subset, it is essential to incorporate an invariant sufficient statistic of the superset into the subset of interest for effective learning. This ensures that the output value remains invariant to permutations of the subset and its corresponding superset, enabling identification of the specific superset from which the subset originated. Motivated by these insights, we propose a simple yet effective information aggregation module designed to merge the representations of subsets and supersets from a permutation invariance perspective. Comprehensive empirical evaluations across diverse tasks and datasets validate the enhanced efficacy of our approach over conventional methods, underscoring the practicality and potency of our proposed strategies in real-world contexts.",
    "authors": [
        {
            "affiliations": [],
            "name": "Binghui Xie"
        },
        {
            "affiliations": [],
            "name": "Yatao Bian"
        },
        {
            "affiliations": [],
            "name": "Kaiwen zhou"
        },
        {
            "affiliations": [],
            "name": "Yongqiang Chen"
        },
        {
            "affiliations": [],
            "name": "Peilin Zhao"
        },
        {
            "affiliations": [],
            "name": "Bo Han"
        },
        {
            "affiliations": [],
            "name": "Wei Meng"
        },
        {
            "affiliations": [],
            "name": "James Cheng"
        }
    ],
    "id": "SP:3ca7acd7c47d20e42503f597c46dfabb24332199",
    "references": [
        {
            "authors": [
                "Maria-Florina Balcan",
                "Nicholas J.A. Harvey"
            ],
            "title": "Submodular functions: Learnability, structure, and optimization",
            "venue": "SIAM J. Comput.,",
            "year": 2018
        },
        {
            "authors": [
                "Benjamin Bloem-Reddy",
                "Yee Whye Teh"
            ],
            "title": "Probabilistic symmetries and invariant neural networks",
            "venue": "J. Mach. Learn. Res.,",
            "year": 2020
        },
        {
            "authors": [
                "Andreis Bruno",
                "Jeffrey Willette",
                "Juho Lee",
                "Sung Ju Hwang"
            ],
            "title": "Mini-batch consistent slot set encoder for scalable set encoding",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "C\u0103t\u0103lina Cangea",
                "Petar Veli\u010dkovi\u0107",
                "Nikola Jovanovi\u0107",
                "Thomas Kipf",
                "Pietro Li\u00f2"
            ],
            "title": "Towards sparse hierarchical graph classifiers",
            "venue": "arXiv preprint arXiv:1811.01287,",
            "year": 2018
        },
        {
            "authors": [
                "Cen Chen",
                "Xiaofeng Zou",
                "Zeng Zeng",
                "Zhongyao Cheng",
                "Le Zhang",
                "Steven CH Hoi"
            ],
            "title": "Exploring structural knowledge for automated visual inspection of moving trains",
            "venue": "IEEE transactions on cybernetics,",
            "year": 2020
        },
        {
            "authors": [
                "Micha\u00ebl Defferrard",
                "Xavier Bresson",
                "Pierre Vandergheynst"
            ],
            "title": "Convolutional neural networks on graphs with fast localized spectral filtering",
            "venue": "Advances in neural information processing systems,",
            "year": 2016
        },
        {
            "authors": [
                "Jia Deng",
                "Nan Ding",
                "Yangqing Jia",
                "Andrea Frome",
                "Kevin Murphy",
                "Samy Bengio",
                "Yuan Li",
                "Hartmut Neven",
                "Hartwig Adam"
            ],
            "title": "Large-scale object classification using label relation graphs",
            "venue": "In European conference on computer vision,",
            "year": 2014
        },
        {
            "authors": [
                "Harrison Edwards",
                "Amos Storkey"
            ],
            "title": "Towards a neural statistician",
            "venue": "arXiv preprint arXiv:1606.02185,",
            "year": 2016
        },
        {
            "authors": [
                "Octavian-Eugen Ganea",
                "Xinyuan Huang",
                "Charlotte Bunne",
                "Yatao Bian",
                "Regina Barzilay",
                "Tommi S. Jaakkola",
                "Andreas Krause"
            ],
            "title": "Independent SE(3)-equivariant models for end-to-end rigid protein docking",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Hongyang Gao",
                "Shuiwang Ji"
            ],
            "title": "Graph u-nets",
            "venue": "In international conference on machine learning,",
            "year": 2019
        },
        {
            "authors": [
                "Jennifer Gillenwater",
                "Alex Kulesza",
                "Emily B. Fox",
                "Benjamin Taskar"
            ],
            "title": "Expectation-maximization for learning determinantal point processes",
            "venue": "In Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems",
            "year": 2014
        },
        {
            "authors": [
                "Jennifer A Gillenwater",
                "Alex Kulesza",
                "Emily Fox",
                "Ben Taskar"
            ],
            "title": "Expectation-maximization for learning determinantal point processes",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2014
        },
        {
            "authors": [
                "Aleix Gimeno",
                "Mar\u0131\u0301a Jos\u00e9 Ojeda-Montes",
                "Sarah Tom\u00e1s-Hern\u00e1ndez",
                "Adri\u00e0 Cereto-Massagu\u00e9",
                "Ra\u00fal Beltr\u00e1n-Deb\u00f3n",
                "Miquel Mulero",
                "Gerard Pujadas",
                "Santiago Garcia-Vallv\u00e9"
            ],
            "title": "The light and dark sides of virtual screening: what is there to know",
            "venue": "International journal of molecular sciences,",
            "year": 2019
        },
        {
            "authors": [
                "Joseph Gomes",
                "Bharath Ramsundar",
                "Evan N Feinberg",
                "Vijay S Pande"
            ],
            "title": "Atomic convolutional networks for predicting protein-ligand binding affinity",
            "venue": "arXiv preprint arXiv:1703.10603,",
            "year": 2017
        },
        {
            "authors": [
                "Paul R Halmos",
                "Leonard J Savage"
            ],
            "title": "Application of the radon-nikodym theorem to the theory of sufficient statistics",
            "venue": "The Annals of Mathematical Statistics,",
            "year": 1949
        },
        {
            "authors": [
                "Jiaqi Han",
                "Yu Rong",
                "Tingyang Xu",
                "Fuchun Sun",
                "Wenbing Huang"
            ],
            "title": "Equivariant graph hierarchybased neural networks",
            "venue": "arXiv preprint arXiv:2202.10643,",
            "year": 2022
        },
        {
            "authors": [
                "Max Horn",
                "Michael Moor",
                "Christian Bock",
                "Bastian Rieck",
                "Karsten Borgwardt"
            ],
            "title": "Set functions for time series",
            "venue": "Proceedings of the 37th International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Jingjia Huang",
                "Zhangheng Li",
                "Nannan Li",
                "Shan Liu",
                "Ge Li"
            ],
            "title": "Attpool: Towards hierarchical feature representation in graph convolutional networks via attention mechanism",
            "venue": "IEEE/CVF International Conference on Computer Vision (ICCV),",
            "year": 2019
        },
        {
            "authors": [
                "Yuanfeng Ji",
                "Lu Zhang",
                "Jiaxiang Wu",
                "Bingzhe Wu",
                "Long-Kai Huang",
                "Tingyang Xu",
                "Yu Rong",
                "Lanqing Li",
                "Jie Ren",
                "Ding Xue",
                "Houtim Lai",
                "Shaoyong Xu",
                "Jing Feng",
                "Wei Liu",
                "Ping Luo",
                "Shuigeng Zhou",
                "Junzhou Huang",
                "Peilin Zhao",
                "Yatao Bian"
            ],
            "title": "Drugood: Out-of-distribution (OOD) dataset curator and benchmark for ai-aided drug discovery - A focus on affinity prediction problems with noise",
            "venue": "annotations. CoRR,",
            "year": 2022
        },
        {
            "authors": [
                "Wengong Jin",
                "Regina Barzilay",
                "Tommi Jaakkola"
            ],
            "title": "Hierarchical generation of molecular graphs using structural motifs",
            "venue": "In International conference on machine learning,",
            "year": 2020
        },
        {
            "authors": [
                "Minyoung Kim"
            ],
            "title": "Differentiable expectation-maximization for set representation learning",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "arXiv preprint arXiv:1412.6980,",
            "year": 2014
        },
        {
            "authors": [
                "Juho Lee",
                "Yoonho Lee",
                "Jungtaek Kim",
                "Adam Kosiorek",
                "Seungjin Choi",
                "Yee Whye Teh"
            ],
            "title": "Set transformer: A framework for attention-based permutation-invariant neural networks",
            "venue": "In International conference on machine learning,",
            "year": 2019
        },
        {
            "authors": [
                "Juho Lee",
                "Yoonho Lee",
                "Jungtaek Kim",
                "Adam R. Kosiorek",
                "Seungjin Choi",
                "Yee Whye Teh"
            ],
            "title": "Set transformer: A framework for attention-based permutation-invariant neural networks",
            "venue": "In Proceedings of the 36th International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Shuangli Li",
                "Jingbo Zhou",
                "Tong Xu",
                "Liang Huang",
                "Fan Wang",
                "Haoyi Xiong",
                "Weili Huang",
                "Dejing Dou",
                "Hui Xiong"
            ],
            "title": "Structure-aware interactive graph neural networks for the prediction of protein-ligand binding affinity",
            "venue": "In KDD \u201921: The 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Virtual Event,",
            "year": 2021
        },
        {
            "authors": [
                "Yang Li",
                "Haidong Yi",
                "Christopher Bender",
                "Siyuan Shan",
                "Junier B Oliva"
            ],
            "title": "Exchangeable neural ode for set modeling",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Tiqing Liu",
                "Yuhmei Lin",
                "Xin Wen",
                "Robert N Jorissen",
                "Michael K Gilson"
            ],
            "title": "Bindingdb: a webaccessible database of experimentally determined protein\u2013ligand binding affinities",
            "venue": "Nucleic acids research,",
            "year": 2007
        },
        {
            "authors": [
                "Zhihai Liu",
                "Yan Li",
                "Li Han",
                "Jie Li",
                "Jie Liu",
                "Zhixiong Zhao",
                "Wei Nie",
                "Yuchen Liu",
                "Renxiao Wang"
            ],
            "title": "Pdb-wide collection of binding data: current status of the pdbbind database",
            "year": 2015
        },
        {
            "authors": [
                "Ziwei Liu",
                "Ping Luo",
                "Xiaogang Wang",
                "Xiaoou Tang"
            ],
            "title": "Deep learning face attributes in the wild",
            "venue": "In Proceedings of the IEEE international conference on computer vision,",
            "year": 2015
        },
        {
            "authors": [
                "Haggai Maron",
                "Or Litany",
                "Gal Chechik",
                "Ethan Fetaya"
            ],
            "title": "On learning sets of symmetric elements",
            "venue": "In Proceedings of the 37th International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Ryan L Murphy",
                "Balasubramaniam Srinivasan",
                "Vinayak Rao",
                "Bruno Ribeiro"
            ],
            "title": "Janossy pooling: Learning deep permutation-invariant functions for variable-size inputs",
            "year": 1811
        },
        {
            "authors": [
                "Venkatesh N Murthy",
                "Vivek Singh",
                "Terrence Chen",
                "R Manmatha",
                "Dorin Comaniciu"
            ],
            "title": "Deep decision network for multi-class image classification",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Zijing Ou",
                "Tingyang Xu",
                "Qinliang Su",
                "Yingzhen Li",
                "Peilin Zhao",
                "Yatao Bian"
            ],
            "title": "Learning neural set functions under the optimal subset oracle",
            "venue": "NeurIPS,",
            "year": 2022
        },
        {
            "authors": [
                "Hakime \u00d6zt\u00fcrk",
                "Arzucan \u00d6zg\u00fcr",
                "Elif Ozkirimli"
            ],
            "title": "Deepdta: deep drug\u2013target binding affinity",
            "venue": "prediction. Bioinformatics,",
            "year": 2018
        },
        {
            "authors": [
                "Charles Ruizhongtai Qi",
                "Hao Su",
                "Kaichun Mo",
                "Leonidas J. Guibas"
            ],
            "title": "Pointnet: Deep learning on point sets for 3d classification and segmentation",
            "venue": "IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Siamak Ravanbakhsh",
                "Jeff Schneider",
                "Barnabas Poczos"
            ],
            "title": "Equivariance through parameter-sharing",
            "venue": "In International conference on machine learning,",
            "year": 2017
        },
        {
            "authors": [
                "Lei Ren",
                "Xuejun Cheng",
                "Xiaokang Wang",
                "Jin Cui",
                "Lin Zhang"
            ],
            "title": "Multi-scale dense gate recurrent unit networks for bearing remaining useful life prediction",
            "venue": "Future generation computer systems,",
            "year": 2019
        },
        {
            "authors": [
                "Lei Ren",
                "Yuxin Liu",
                "Xiaokang Wang",
                "Jinhu L\u00fc",
                "M Jamal Deen"
            ],
            "title": "Cloud\u2013edge-based lightweight temporal convolutional networks for remaining useful life prediction in iiot",
            "venue": "IEEE Internet of Things Journal,",
            "year": 2020
        },
        {
            "authors": [
                "S Hamid Rezatofighi",
                "Vijay Kumar BG",
                "Anton Milan",
                "Ehsan Abbasnejad",
                "Anthony Dick",
                "Ian Reid"
            ],
            "title": "Deepsetnet: Predicting sets with deep neural networks",
            "venue": "IEEE International Conference on Computer Vision (ICCV),",
            "year": 2017
        },
        {
            "authors": [
                "Shao-Hua Sun"
            ],
            "title": "Multi-digit mnist for few-shot learning, 2019",
            "year": 2019
        },
        {
            "authors": [
                "Sebastian Tschiatschek",
                "Aytunc Sahin",
                "Andreas Krause"
            ],
            "title": "Differentiable submodular maximization",
            "venue": "arXiv preprint arXiv:1803.01785,",
            "year": 2018
        },
        {
            "authors": [
                "Renhao Wang",
                "Marjan Albooyeh",
                "Siamak Ravanbakhsh"
            ],
            "title": "Equivariant networks for hierarchical structures",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Jeffrey Willette",
                "Andreis Bruno",
                "Juho Lee",
                "Sung Ju Hwang"
            ],
            "title": "Universal mini-batch consistency for set encoding functions",
            "venue": "arXiv preprint arXiv:2208.12401,",
            "year": 2022
        },
        {
            "authors": [
                "Han Xiao",
                "Kashif Rasul",
                "Roland Vollgraf"
            ],
            "title": "Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms",
            "venue": "arXiv preprint arXiv:1708.07747,",
            "year": 2017
        },
        {
            "authors": [
                "Tianjun Xiao",
                "Jiaxing Zhang",
                "Kuiyuan Yang",
                "Yuxin Peng",
                "Zheng Zhang"
            ],
            "title": "Error-driven incremental learning in deep convolutional neural network for large-scale image classification",
            "venue": "In Proceedings of the 22nd ACM international conference on Multimedia,",
            "year": 2014
        },
        {
            "authors": [
                "Zhitao Ying",
                "Jiaxuan You",
                "Christopher Morris",
                "Xiang Ren",
                "Will Hamilton",
                "Jure Leskovec"
            ],
            "title": "Hierarchical graph representation learning with differentiable pooling",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2018
        },
        {
            "authors": [
                "Zhitao Ying",
                "Jiaxuan You",
                "Christopher Morris",
                "Xiang Ren",
                "Will Hamilton",
                "Jure Leskovec"
            ],
            "title": "Hierarchical graph representation learning with differentiable pooling",
            "venue": "Advances in neural information processing systems,",
            "year": 2018
        },
        {
            "authors": [
                "David W Zhang",
                "Gertjan J Burghouts",
                "Cees GM Snoek"
            ],
            "title": "Set prediction without imposing structure as conditional density estimation",
            "venue": "arXiv preprint arXiv:2010.04109,",
            "year": 2020
        },
        {
            "authors": [
                "Y\u22a5\u22a5M(S",
                "V )(S"
            ],
            "title": "This lemma serves as a refined version of Lemma 20 presented in (Bloem-Reddy",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "The prediction of set-valued outputs plays a crucial role in various real-world applications. For instance, anomaly detection involves identifying outliers from a majority of data (Zhang et al., 2020), and compound selection in drug discovery aims to extract the most effective compounds from a given compound database (Gimeno et al., 2019). In these applications, there exists an implicit learning of a set function (Rezatofighi et al., 2017; Zaheer et al., 2017) that quantifies the utility of a given set input, where the highest utility value corresponds to the most desirable set output.\nMore formally, let\u2019s consider the compound selection task: given a compound database V , the goal is to select a subset of compounds S\u2217 \u2286 V that exhibit the highest utility. This utility can be modeled by a parameterized utility function F\u03b8(S;V ), and the optimization criteria can be expressed as:\nS\u2217 = argmax S\u22082V F\u03b8(S;V ). (1)\nOne straightforward method is to explicitly model the utility by learning U = F\u03b8(S;V ) using supervised data in the form of {(Si, Vi), Ui}Ni=1, where Ui represents the true utility value of subset Si given Vi. However, this training approach becomes prohibitively expensive due to the need for constructing a large amount of supervision signals (Balcan & Harvey, 2018).\nTo address this limitation, another way is to solve Eq.1 with an implicit learning approach from a probabilistic perspective. Specifically, it is required to utilize data in the form of {(Vi, S\u2217i )}Ni=1, where S\u2217i represents the optimal subset corresponding to Vi. The goal is to estimate \u03b8 such that\nEq. 1 holds for all possible (Vi, Si). During practical training, with limited data (S\u2217i , Vi) N i=1 sampled from the underlying data distribution P (S, V ), the empirical log likelihood \u2211N\ni=1[log p\u03b8(S \u2217|V )] is\nmaximized among all data pairs {S, V }, where p\u03b8(S|V ) \u221d F\u03b8(S;V ) for all S \u2208 2V . To achieve this objective, Ou et al. (2022) proposed to use a variational distribution q(Y |S, V ) to approximate the distribution of P (S|V ) within the variational inference framework, where Y \u2208 [0, 1]|V | represents a set of |V | independent Bernoulli distributions, representing the odds or probabilities of selecting element i in an output subset S. (More details can be found in Appendix D.1.) Thus, the main challenge lies in characterizing the structure of neural networks capable of modeling hierarchical permutation invariant conditional distributions. These distributions should remain unchanged under any permutation of elements in S and V while capturing the interaction between them.\nHowever, the lack of guiding principles for designing a framework to learn the permutation invariant conditional distribution P (Y |S, V ) or F (S, V ) has been a challenge in the literature. A commonly used approach in the literature involves employing an encoder to generate feature vectors for each element in V . These vectors are then fed into DeepSets (Zaheer et al., 2017), using the corresponding supervised subset S, to learn the permutation invariant set function F (S). However, this procedure might overlook the interplay between S and V , thereby reducing the expressive power of models. See Figure 1 for an illustrative depiction of this concept.\nTo address these challenges, our research focuses on the aggregation of background information from the superset V into the subset S from a symmetric perspective. Initially, we describe the symmetry group of (S, V ) during neural subset selection, as outlined in Section 3.2. Specifically, the subset S is required to fulfill permutation symmetry, while the superset V needs to satisfy a corresponding symmetry group within the nested sets scheme. We denote this hierarchical symmetry of (S, V ) as G. Subsequently, we theoretically investigate the connection between functional symmetry and probabilistic symmetry within F (S, V ) and P (Y |S, V ), indicating that the conditional distribution can be utilized to construct a neural network that processes the invariant sufficient representation of (S, V ) with respect to G. These representations, defined in Section 3.3, are proven to satisfy Sufficiency and Adequacy, which means such representations retain the information of the prediction Y while disregarding the order of the elements in S or V . Building upon the above theoretical results, we propose an interpretable and powerful model called INSET (Invariant Representation of Subsets) for neural subset selection in Section 3.4. INSET incorporates an information aggregation step between the invariant sufficient representations of S and V , as illustrated in Figure 1. This ensures that the model\u2019s output can approximate the relationship between Y and (S, V ) while being unaffected by the transformations of G. Furthermore, in contrast to previous works that often disregard the information embedded within the set V , our exceptional model (INSET) excels in identifying the superset V from which the subset S originates.\nIn summary, we makes the following contributions. Firstly, we approach neural set selection from a symmetric perspective and establish the connection between functional symmetry and probabilistic symmetry in P (Y |S, V ), which enables us to characterize the model structure. Secondly, we introduce INSET, an effective and interpretable approach model for neural subset selection. Lastly, we empirically validate the effectiveness of INSET through comprehensive experiments on diverse datasets, encompassing tasks such as product recommendation and set anomaly detection."
        },
        {
            "heading": "2 RELATED WORK",
            "text": "Encoding Interactions for Set Representations. Designing network architectures for set-structured input has emerged as a highly popular research topic. Several prominent works, including those (Ravanbakhsh et al., 2017; Edwards & Storkey, 2016; Zaheer et al., 2017; Qi et al., 2017a; Horn et al., 2020; Bloem-Reddy & Teh, 2020) have focused on constructing permutation equivariant models using standard feed-forward neural networks. These models demonstrate the ability to universally approximate continuous permutation-invariant functions through the utilization of set-pooling layers. However, existing approaches solely address the representation learning at the set level and overlook interactions within sub-levels, such as those between elements and subsets.\nMotivated by this limitation, subsequent studies have proposed methods to incorporate richer interactions when modeling invariant set functions for different tasks. For instance, (Lee et al., 2019b) introduced the use of self-attention mechanisms to process elements within input sets, naturally capturing pairwise interactions. Murphy et al. (2018) proposed Janossy pooling as a means to encode higher-order interactions within the pooling operation. Further improvements have been proposed by (Kim, 2021; Li et al., 2020), among others. Additionally, Bruno et al. (2021); Willette et al. (2022) developed techniques to ensure Mini-Batch Consistency in set encoding, enabling the provable equivalence between mini-batch encodings and full set encodings by leveraging interactions. These studies emphasize the significance of incorporating interactions between different components.\nInformation-Sharing in Neural Networks. In addition to set learning tasks, the interaction between different components holds significance across various data types and neural networks. Recent years have witnessed the development of several deep neural network-based methods that explore hierarchical structures. For Convolutional Neural Networks (CNNs), various hierarchical modules have been proposed by Deng et al. (2014); Murthy et al. (2016); Xiao et al. (2014); Chen et al. (2020); Ren et al. (2020; 2019) to address different image-related tasks. In the context of graph-based tasks, (Defferrard et al., 2016; Cangea et al., 2018; Gao & Ji, 2019; Ying et al., 2018b; Huang et al., 2019; Ying et al., 2018a; Jin et al., 2020; Han et al., 2022), and others have put forth different methods to learn hierarchical representations. The focus of these works lies in capturing local information effectively and integrating it with global information.\nHowever, the above works ignore the symmetry and expressive power in designing models. Motivated by this, Maron et al. (2020); Wang et al. (2020) proposed how to design linear equivariant and invariant layers for learning hierarchical symmetries to handle per-element symmetries. Moreover, there are some works proposed for different tasks considering symmetry and hierarchical structure, e.g., (Han et al., 2022; Ganea et al., 2022). Our method differs from previous work by focusing on generating a subset S \u2208 V as the final output, rather than output the entire set V . Besides, INSET embraces a probabilistic perspective, aligning with the nature of the Optimal Subset (OS) oracle."
        },
        {
            "heading": "3 METHOD",
            "text": ""
        },
        {
            "heading": "3.1 BACKGROUND",
            "text": "Let\u2019s consider the ground set composed of n elements, denoted as xi, i.e., V = {x1, x2, ..., xn}. In order to facilitate the proposition of Property 3.1, we describe V as a collection of several disjoint subsets, specifically V = {S1, . . . , Sm}, where Si \u2208 Rni\u00d7d. Here, ni represents the size of subset Si, and each element xi \u2208 X is represented by a d-dimensional tensor. It is worth noting that, without loss of generality, we can treat Si as individual elements, i.e., ni = 1. As an example of neural subset selection, the task involves encoding subsets Si into representative vectors to predict the corresponding function value Y \u2208 Y , as discussed in the introduction section. Existing methods such as (Zaheer et al., 2017) and (Ou et al., 2022) directly select Si from the encoding embeddings of all elements in V , and then input Si into feed-forward networks. However, these methods approximate the function F (Si, V ) using only the explicit subsets Si, which can be suboptimal since the function also relies on information from the ground set V . Furthermore, this approach leads to a conditional distribution P (Y |S) instead of the desired P (Y |S, V ). Throughout this study, we assume that all random variables take values in standard Borel spaces, and all introduced maps are measurable.\nIn this section, we introduce a principled approach for encoding subset representations that leverages background information from the entire input set V to achieve better performance. Additionally,\nour theoretical results naturally align with the task of neural subset selection in OS Oracle, as they focus on investigating the probabilistic relationship between Y and (S, Y ), which also establishes a connection between the conditional distribution and the functional representation of both S and V . By linking the functional representation to the conditional distribution, our results also provide insights into constructing a neural network that effectively approximates the desired function F (S, V )."
        },
        {
            "heading": "3.2 THE SYMMETRIC INFORMATION FROM SUPERSETS",
            "text": "When considering the invariant representation of S alone, we can directly utilize DeepSets with a maxpooling operation. However, incorporating background information from V into the representations poses the challenge of determining the appropriate inductive bias for the modeling process. One straightforward approach is to assume the existence of two permutation groups that act independently on V and S. However, this assumption is impractical since S is a part of V . If we transform S, the corresponding adjustments should also be made to V . From the perspective of the interaction between subsets and supersets, a natural consideration is to view the supersets as a nested set of sets, i.e., V = S1 \u222a S2 \u222a \u00b7 \u00b7 \u00b7 \u222a Sm, where Si \u2229 Sj = \u2205 if i \u0338= j. In this perspective, the symmetric properties will become more evident.\nWe assume the presence of an outer permutation \u03c0m \u2208 Sm that maps indices of the subsets to new indices, resulting in a reordering of the subsets within V . Furthermore, within each subset Si, there exists a permutation group denoted by hi \u2208 Hi, which captures the possible rearrangements of elements within that specific subset. Each element of hi represents a distinct permutation on the elements of Si. The symmetry of nested sets of sets, referred to as R, can be defined as the wreath product of the symmetric group Sm (representing outer permutations on the m subsets) and the direct product of the permutation groups associated with each subset (G1 \u00d7 G2 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 Gm). Formally, R = Sm \u2240 (G1 \u00d7 G2 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 Gm). Therefore, for any transformation r \u2208 R acting on V , there must exist a corresponding h \u2208 H acting on S. Keeping this in mind, we define the conditional distribution P (Y |S, V ) to adhere to the following property: Property 3.1. Let S \u2208 S, V \u2208 V and Y \u2208 Y, where H and R act on S and V, respectively. Then, the conditional distribution P (Y |S, V ) of Y give (S, V ) is said to be invariant under a group H and R if and only if:\nP (Y | S, V ) = P (Y | g \u00b7 (S, V )) = P (Y | h \u00b7 S, r \u00b7 V ) for any h \u2208 H and r \u2208 R .\nIn this context, we denote the composite group G = H\u00d7R, which acts on the product space S \u00d7 V . We have now clarified the specific inductive bias that should be considered when characterizing neural networks. In the subsequent subsection, we will delve into the exploration of constructing neural networks that fulfill this property."
        },
        {
            "heading": "3.3 INVARIANT SUFFICIENT REPRESENTATION",
            "text": "Functional and probabilistic notions of symmetries represent two different approaches to achieving the same goal: a principled framework for constructing models from symmetry considerations. To characterize the precise structure of the neural network satisfying Property 3.1, we need to use a technical tool, that transfers a conditional probability distribution P (Y |S, V ) into a representation of Y as a function of statics of (S, V ) and random noise, i.e., f(\u03be,M(S, V )). Here, M are maps, which are based on the idea that a statistic may contain all the information that is needed for an inferential procedure. There are mainly two terms as Sufficiency and Adequacy. The ideas go hand-in-hand with notions of symmetry: while invariance describes information that is irrelevant, sufficiency and adequacy describe the information that is relevant.\nThere are various methods to describe sufficiency and adequacy, which are equivalent under some constraints. For convenience and completeness, we follow the concept from (Halmos & Savage, 1949; Bloem-Reddy & Teh, 2020). We begin by defining the sufficient statistic as follows, where BX represents the Borel \u03c3-algebra of X :\nDefinition 3.2. Assume M : S \u00d7 V \u2192 M a measurable map and there is a Markov kernel k : BX \u00d7 S \u00d7 V \u2192 R+ such that for all X \u2208 X and m \u2208 M, P ( \u2022 | M(S, V ) = m) = k( \u2022 ,m). Then M is a sufficient statistic for P (S, V ).\nThis definition characterizes the information pertaining to the distribution of (S, V ). More specifically, it signifies that there exists a single Markov kernel that yields the same conditional distribution of (S, V ) conditioned on M(S, V ) = m, regardless of the distribution P (S, V ). It is important to note that if S \u2288 V , the corresponding value of M(S, V ) would be zero, which is an invalid case. When examining the distribution of Y conditioned on S and V , an additional definition is required: Definition 3.3. Let M : S \u00d7 V \u2192 M be a measurable map and assume M is sufficient for P (S, V ). If for all s \u2208 S, v \u2208 V and y \u2208 Y,\nP (Y \u2208 \u2022 | S = s, V = v) = P (Y \u2208 \u2022 | M(S, V ) = m) . (2) Then, M serves as an adequate statistic of (S, V ) for Y , and also acts as the sufficient statistic.\nActually, Equation (2) is equivalent to conditional independence of Y and (S, V ), given M(S, V ), i.e., Y\u22a5\u22a5M(X)X, This is also called M d-separates (S, V ) and Y. In other words, if our goal is to approximate the invariant conditional distribution P (Y |S, V ), we can first seek an invariant representation of (S, V ) under G, which also acts as an adequate statistic for (S, V ) with respect to Y . Consequently, modeling the relationship between (S, V ) and Y directly is equivalent to learning the relationship between M(S, V ) and Y , which naturally satisfies Property 3.1.\nWith the given definitions, it becomes evident that we can discover an invariant representation of (S, V ) with respect to the symmetric groups G. This representation is referred to as the Invariant Sufficient Representation, signifying that an invariant effective representation should eliminate the information influenced by the actions of G, while preserving the remaining information regarding its distribution. This concept is also referred to as Maximal Invariant in some previous literature, such as (Kallenberg et al., 2017; Bloem-Reddy & Teh, 2020). Definition 3.4. (Invariant Sufficient Representation) For a group G of actions on any (s, v) \u2208 S\u00d7V , we say M : S \u00d7 V \u2192 M is a invariant sufficient representation for space S \u00d7 V , if it satisfies: If M(s1, v1) = M(s2, v2), then (s2, v2) = g \u00b7 (s1, v1) for some g \u2208 G; otherwise, there is no such g that satisfies (s2, v2) = g \u00b7 (s1, v1).\nClearly, the invariant sufficient representation M serves as the sufficient statistic for (S, V ). Furthermore, if the conditional distribution P (Y |S, V ) is invariant to transformations induced by the group G, we can establish that M(S, V ) is an adequate statistic for (S, V ), as stated in Corollary 3.6. In other words, M(S, V ) can be considered to encompass all the relevant information for predicting the label given (S, V ) while eliminating the redundant information about G. Hence, we can construct models that learn the relationship between M(S, V ) and Y , ultimately resulting in an invariant function Y = f(S, V ) under the group G. From a probabilistic standpoint, this implies that P (Y |S, V ) = P (Y |M(S, V ))."
        },
        {
            "heading": "3.4 CHARACTERIZING THE MODEL STRUCTURE",
            "text": "Hence, by computing the invariant sufficient representations of (S, V ), we can construct a G-invariant layer. This idea can give rise to the following theorem: Theorem 3.5. Consider a measurable group G acting on S \u00d7 V . Suppose we select an invariant sufficient representation denoted as M : S \u00d7V \u2192 M. In this case, P (Y |S, V ) satisfies Property 3.1 if and only if there exists a measurable function denoted as f : [0, 1] \u00d7 S \u00d7 V \u2192 Y such that the following equation holds:\n(S, V, Y ) =a.s. ( S, V, f(\u03be,M(S, V )) ) where \u03be \u223c Unif[0, 1] and \u03be\u22a5\u22a5(S, V ); . (3)\nIn this context, the variable \u03be represents generic noise, which can be disregarded when focusing solely on the model structure rather than the complete training framework (Bloem-Reddy & Teh, 2020; Ou et al., 2022). Consequently, the theorem highlights the necessity of characterizing the neural networks in the form of f(M(S, V )). Moreover, Theorem 3.5 implies that the invariant sufficient representation M(S, V ) also serves as an adequate statistic. This can be illustrated as follows: P (Y \u2208 \u2022 |S = s, V = v) = P (Y \u2208 \u2022 |S = s, V = v,M(S, V ) = m) = P (Y \u2208 \u2022 |M(S, V ) = m). To provide additional precision and clarity, we present the following corollary, which demonstrates that M(S, V ) is an adequate statistic of (S, V ) for Y. Corollary 3.6. Let G be a compact group acting measurably on standard Borel spaces S \u00d7 V , and let M be another Borel space. Then Any invariant sufficient representation M : S \u00d7 V \u2192 M under G is an adequate statistic of (S, V ) for Y."
        },
        {
            "heading": "3.5 IMPLEMENTATION",
            "text": "In theory, invariant sufficient representations can be computed by selecting a representative element for each orbit under the group G. However, this approach is impractical due to the high dimensions of the input space and the potentially enormous number of orbits. Instead, in practice, a neural network can be employed to approximate this process and generate the desired representations (Zaheer et al., 2017; Bloem-Reddy & Teh, 2020), particularly in tasks involving sets or set-like structures.\nHowever, the approach to approximating such a representation for (S, V ) under G remains unclear. To simplify the problem, we can divide the task of finding the invariant sufficient representation of S and V under H and R, respectively, as defined in Section 3.2. This concept is guaranteed by the following proposition: Proposition 3.7. Assuming that Ms : S \u2192 S1 and Mv : V \u2192 V1 serve as invariant sufficient representations for S and V with respect to H and R, respectively, then there exist maps f : S1 \u00d7 V1 \u2192 M that establish the invariant sufficient representation of M .\nProposition 3.7 specifically states that we can construct the invariant sufficient representations for S and V individually, as they are comparatively easier to construct compared to M(S, V ). In the work of (Bloem-Reddy & Teh, 2020), it is demonstrated that for S under H , the empirical measure Ms(S) = \u2211 si\u2208S \u03b4(si) can be chosen as a suitable invariant sufficient representation. Here, \u03b4(si) represents an atom of unit mass located at si, such as one-hot embeddings. Additionally, leveraging the proposition established by Zaheer et al. (2017), we can employ \u03c1 \u2211 s\u2208S \u03d5(s) to approximate the empirical measure. This approximation offers a practical and effective approach to constructing the invariant sufficient representation. Proposition 3.8. If f is a valid permutation invariant function on S, it can be approximated arbitrarily close in the form of f(S) = \u03c1 (\u2211 s\u2208S \u03d5(s) ) , for suitable transformations \u03d5 and \u03c1.\nDuring the implementation, an encoder \u03d5 is utilized to generate embeddings for each element. For example, when dealing with sets of images, ResNet can be employed as the encoder. On the other hand, \u03c1 can represent various feedforward networks, such as fully connected layers combined with nonlinear activation functions. Similarly, for the symmetric group R acting on V , Maron et al. (2020) has demonstrated that the universal approximators of the invariant sufficient representations are\u2211\nS\u2208V \u2211 s\u2208S \u03d5(s), which is equivalent to \u2211\nxj\u2208V \u03d5(xj). Hence, for neural subset selection tasks, when considering a specific subset S \u2208 V , The neural network construction is outlined as follows:\n\u03b8(S, V ) = \u03c3 ( \u03b81( ni\u2211 i=1 \u03d5(xi)) + \u03b82 ( n\u2211 i=1 \u03d5(xj) )) , (4)\nHere, the feed-forward modules \u03b81 and \u03b82 are accompanied by a non-linear activation layer denoted by \u03c3. Intuitively, the inherent simplicity of the structure enables us to utilize the DeepSet module to process all elements in V and integrate them with the invariant sufficient representations of S. In practice, there are different ways to integrate the representation of V into the representation of S, such as concatenation (Qi et al., 2017a) or addition (Maron et al., 2020). Although this idea is straightforward, in the following section, we will demonstrate how this modification significantly enhances the performance of baseline methods. Notably, this idea has been empirically utilized in previous works, such as (Qi et al., 2017a;b). However, we propose it from a probabilistic invariant perspective. A corresponding equivariant framework was also introduced in Wang et al. (2020), which complements our results in the development of deep equivariant neural networks."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": "The proposed methods are assessed across multiple tasks, including product recommendation, set anomaly detection, and compound selection. To ensure robustness, all experiments are repeated five times using different random seeds, and the means and standard deviations of the results are reported. For additional experimental details and settings, we provide comprehensive information in Appendix E.\nEvaluations. The main goal of the following tasks is to predict the corresponding S\u22c6 given V. Therefore, we evaluate the methods using the mean Jaccard coefficient (MJC) metric. Specifically,\nfor each data sample (S\u22c6, V ) if the model\u2019s prediction is S\u2032, then the Jaccard coefficient is given as: JC(S\u22c6, S\u2032) = |S\n\u22c6\u2229S\u2032| |S\u22c6\u222aS\u2032| . Therefore, the MJC is computed by averaging JC metric over all samples in\nthe test set.\nBaselines. To show our method can achieve better performance on real applications, we compare it with the following methods:\n\u2022 Random. The results are calculated based on random estimates, which provide a measure of how challenging the tasks are.\n\u2022 PGM (Tschiatschek et al., 2018). PGM is a probabilistic greedy model (PGM) solves optimization Problem 1 with a differentiable extension of greedy maximization algorithm. In our paper, we leverage the results of PGD conducted on various datasets as reported in the study by (Ou et al., 2022).\n\u2022 DeepSet (Zaheer et al., 2017). Here, we use DeepSet as a baseline by predicting the probability of which instance should be in S\u22c6, i.e., learn an invariant permutation mapping 2V 7\u2192 [0, 1]|V |. It serves as the backbone in EquiVSet to learn set functions, and can also be employed as a baseline.\n\u2022 Set Transformer (Lee et al., 2019a). Set Transformer, compared with DeepSet, goes beyond by incorporating the self-attention mechanism to account for pairwise interactions among elements. This will make models to capture dependencies and relationships between different elements.\n\u2022 EquiVSet (Ou et al., 2022). EquiVSet uses an energy-based model (EBM) to construct the set mass function P (S|V ) from a probabilistic perspective, i.e, they mainly focus on learning a distribution P (S|V ) monotonically growing with the utility function F (S, V ). This requires to learn a conditional distribution P (Y |S, V ) as approximation distribution. Actually, their framework is to approximate symmetric F (S) instead of symmetric F (S, V )."
        },
        {
            "heading": "4.1 PRODUCT RECOMMENDATION",
            "text": "The task requires models to recommend the most interested subset for a customer given 30 products in a category. We use the dataset (Gillenwater et al., 2014a) from the Amazon baby registry for this experiment, which includes many product subsets chosen by various customers. Amazon classifies each item on a baby registry as being under one of several categories, such as \u201cHealth\u201d and \u201cFeeding\u201d. Moreover, each product is encoded into a 768-dimensional vector by the pre-trained BERT model based on its textual description. Table 1 reports the performance of all the models across different categories. Out of the twelve cases evaluated, INSET performs best in ten of them, except for Furniture and Safety tasks. The discrepancy in performance can be attributed to the fact that our method is built upon the EquiVSet framework, with the main modification being the model structure for modeling F (S, V ). Consequently, when EquiVSet performs poorly, it also affects the performance of INSET. Nonetheless, it is worth noting that INSET consistently outperforms EquiVSet and achieves significantly better results than other baselines in the majority of cases. The margin of improvement is substantial, demonstrating the effectiveness and superiority of INSET."
        },
        {
            "heading": "4.2 SET ANOMALY DETECTION",
            "text": "We conduct set anomaly detection tasks on three real-world datasets: the double MNIST (Sun, 2019), the CelebA (Liu et al., 2015b) and the F-MNIST (Xiao et al., 2017). Each dataset is divided into the training, validation, and test sets with sizes of 10,000, 1,000, and 1,000, respectively. For each dataset, we randomly sample n \u2208 {2, 3, 4} images as the OS oracle S\u2217. The setting is followed by (Zaheer et al., 2017; Ou et al., 2022). Let\u2019s take CelebA as an example. In this case, the objective is to identify anomalous faces within each set solely through visual observation, without any access to attribute values. The CelebA dataset comprises 202,599 face images, each annotated with 40 boolean attributes. When constructing sets, for every ground set V , we randomly choose n images from the dataset to form the OS Oracle S\u2217, ensuring that none of the selected images contain any of the two attributes. Additionally, it is ensured that no individual person\u2019s face appears in both the training and test sets. Regarding Table 2, it is evident that our model demonstrates a substantial performance advantage over all the baselines. Specifically, in the case of Double MNIST, our model shows a remarkable improvement of 23% compared to EquiVSet, which itself exhibits the best performance among all the baselines considered. This significant margin of improvement highlights the superior capabilities of our model in tackling the given task."
        },
        {
            "heading": "4.3 COMPOUND SELECTION IN AI-AIDED DRUG DISCOVERY",
            "text": "The screening of compounds with diverse biological activities and satisfactory ADME (absorption, distribution, metabolism, and excretion) properties is a crucial stage in drug discovery tasks (Li et al., 2021; Ji et al., 2022; Gimeno et al., 2019). Consequently, virtual screening is often a sequential filtering procedure with numerous necessary filters, such as selecting diverse subsets from the highly active compounds first and then removing compounds that are harmful for ADME. After several filtering stages, we reach the optimal compound subset. However, it is hard for neural networks to learn the full screening process due to a lack of intermediate supervision signals, which can be very expensive or impossible to obtain due to the pharmacy\u2019s protection policy. Therefore, the models are supposed to learn this complicated selection process in an end-to-end manner, i.e., models will predict S\u2217 only given the optimal subset supervision signals without knowing the intermediate process. However, this is out of the scope of this paper, since the task is much more complex and requires extra knowledge, and thus we leave it as future work.\nTo simulate the process, we only apply one filter: high bioactivity to acquire the optimal subset of compound selection following (Ou et al., 2022). We conduct experiments using the following datasets: PDBBind (Liu et al., 2015a) and BindingDB (Liu et al., 2007). Table 2 shows that our method performs better than the baselines and significantly outperform the random guess, especially on the BindingDB dataset. Different from the previous tasks, the performance of these methods is closer to each other. That is\nbecause the structure of complexes (the elements in a set) can provide much information for this task. Thus, the model could predict the activity value of complexes well without considering the interactions between the optimal subset and the complementary. However, our method can still achieve more satisfactory results than the other methods."
        },
        {
            "heading": "4.4 COMPUTATION COST",
            "text": "The main difference between INSET and EquiVSet is the additional information-sharing module to incorporate the representations of V . A possible concern is that the better performance of INSET might come from the extra parameters instead of our framework proposed. To address this concern, we conducted experiments on CelebA datasets. We add an additional convolution layer in the encoders to improve the capacity of EquiVSet. According to the location and size, we propose two variants of EquiVSet, details can be found in the appendix. We report the performance of models with different model sizes in Table 3. It is evident that INSET surpasses all the variants of EquiVSet, clearly demonstrating superior performance. Notably, the improvement achieved through the parameters is considerably less significant when compared to the substantial improvement resulting from the information aggregation process. This highlights the crucial role of information aggregation in driving the overall performance enhancement of INSET."
        },
        {
            "heading": "4.5 PERFORMANCE VERSUS TRAINING EPOCHS",
            "text": "In addition to the notable improvement in the final MJC achieved by INSET, we have also observed that incorporating more information from the superset leads to enhanced training speed and better overall performance. To illustrate this, we present two figures depicting the validation performance against the number of training epochs for the Toys and Diaper datasets. It is evident that INSET achieves favorable performance in fewer training epochs. For instance, on the Toy dataset, INSET reaches the best performance of EquiVSet, at approximately epoch 18. Furthermore, around epoch 25, INSET approaches its optimal performance, while EquiVSet and Set Transformer attain their best performance around epoch 40. This highlights the efficiency and effectiveness of INSET in achieving competitive results within a shorter training time."
        },
        {
            "heading": "5 CONCLUSION",
            "text": "In this study, we have identified a significant limitation in subset encoding methods, such as neural subset selection, where the output is either the subset itself or a function value associated with the subset. By incorporating the concept of permutation invariance, we reformulate this problem as the modeling of a conditional distribution P (Y |S, V ) that adheres to Property 3.1. Our theoretical analysis further reveals that to accomplish this objective, it is essential to construct a neural network based on the invariant sufficient representation of both S and V . In response, we introduce INSET, a highly accurate and theoretical-driven approach for neural subset selection, which also consistently outperforms previous methods according to empirical evaluations.\nLimitations and Future Work. INSET is a simple yet effective method in terms of implementation, indicating that there is still potential for further improvement by integrating additional information, such as pairwise interactions between elements. Furthermore, our theoretical analysis is not limited to set-based tasks; it can be applied to more general scenarios with expanded definitions and theoretical contributions. We acknowledge that these potential enhancements and extensions are left as future work, offering opportunities for further exploration and development."
        },
        {
            "heading": "6 ACKNOWLEDGEMENTS",
            "text": "We thanks the reviewers for their valuable comments. Additionally, we extend our thanks to Zijing Ou for his assistance with the code and datasets. This work was supported by CUHK direct grant 4055146. BH was supported by the NSFC General Program No. 62376235, Guangdong Basic and Applied Basic Research Foundation No. 2022A1515011652, Tencent AI Lab Rhino-Bird Gift Fund."
        },
        {
            "heading": "A APPENDIX",
            "text": ""
        },
        {
            "heading": "B PROOF OF THEOREM 3.5",
            "text": ""
        },
        {
            "heading": "B.1 THE CONNECTION BETWEEN FUNCTIONAL AND PROBABILISTIC SYMMETRIES",
            "text": "To prove Theorem 3.5, the main objective is to establish a relationship between the conditional distribution P (Y |S, V ) and a functional representation of samples generated from P (Y |S, V ) in terms of (S, V ) and independent noise \u03be. This functional representation can be expressed as Y =a.s. f(\u03be, S, V ), where f captures the underlying relationship between the variables.\nIn order to achieve this goal, a valuable technique called the transducer (Kallenberg, 2002) or Noise Outsourcing (Bloem-Reddy & Teh, 2020) comes into play. This technique allows us to effectively connect the conditional distribution with the functional representation by leveraging the concept of independent noise variables. By applying the transducer or Noise Outsourcing approach, we can establish a clear mapping between the observed variables (S, V ), the independent noise \u03be, and the resulting output Y . More formally, we give the corresponding lemma as:\nLemma B.1 (Conditional independence and randomization). Let S, V, Y, Z be random elements in some measurable spaces S,V,Y,Z , respectively, where Y is Borel. Then Y\u22a5\u22a5Z(S, V ) if and only if Y =a.s. f(\u03be, Z) for some measurable function f : [0, 1] \u00d7 S \u00d7 V \u2192 Y and some uniform random variable \u03be\u22a5\u22a5(S, V, Z).\nIn the main text, we put forth the idea of utilizing an invariant sufficient representation M(S, V ) as an alternative to (S, V ) to ensure compliance with symmetry groups. This representation captures the essential information while discarding unnecessary details, making it well-suited for addressing the challenges posed by symmetry.\nBy employing the invariant sufficient representation M(S, V ), we can redefine and refine the aforementioned lemma in a more concise and expressive manner. This approach allows us to establish a direct connection between the conditional distribution P (Y |S, V ) and the functional representation f(\u03be,M(S, V )), where \u03be represents independent noise variables. The use of M(S, V ) as a replacement for (S, V ) enables us to effectively model and analyze the relationship between the input variables and the desired output.\nLemma B.2. Let S, V and Y be random variables with joint distribution P (S, V, Y ). Assume there exists a mapping M : S \u00d7 V \u2192 M, then M(S, V ) d-separates (S, V ) and Y if and only if there is a measurable function f : [0, 1]\u00d7 S \u00d7 V \u2192 Y such that\n(S, V, Y ) =a.s. (S, V, f(\u03be,M(S, V ))) where \u03be \u223c Unif[0, 1] and \u03be\u22a5\u22a5X . In particular, Y = f(\u03be,M(S, V )) has distribution P (Y |S, V ).\nThis lemma implies that if the invariant sufficient representation is capable of d-separating (S, V ) and Y , it will yield the equation presented in Eq.3 of Theorem3.5. Subsequently, we must establish why the conditional distribution P (Y |S, V ) remains invariant under the group G if and only if an invariant sufficient representation M exists. This overarching concept can be succinctly summarized in the following lemma:\nLemma B.3. Let S,V and Y be Borel spaces, G a compact group acting measurably on (S,V), and M : S \u00d7 V \u2192 Y a invariant sufficient representation on (S,V) under G. If (S, V ) is a random element of (S,V), then its distribution P (S, V ) is G-invariant if and only if\nP ((S, V ) \u2208 \u2022 | M(S, V ) = m) = q( \u2022 ,m) , (5) for some Markov kernel q : BS\u00d7V \u00d7M \u2192 R+. If P (S, V ) is G-invariant and Y is any other random variable, then P (Y |S, V ) is G-invariant if and only if Y\u22a5\u22a5M(S,V )(S, V ).\nThis lemma serves as a refined version of Lemma 20 presented in (Bloem-Reddy & Teh, 2020). Proving this lemma involves a comprehensive set of definitions and notations, which are beyond the scope of this paper. We encourage interested readers to refer to (Bloem-Reddy & Teh, 2020) for detailed proof.\nBy leveraging the insights and techniques established above, we are able to establish Theorem 3.5. The theorem provides a formal characterization of the relationship between the invariance of the conditional distribution P (Y |S, V ) under the group G and the existence of an invariant sufficient representation M(S, V ).\nTheorem 3.5. Consider a measurable group G acting on S \u00d7 V . Suppose we select an invariant sufficient representation denoted as M : S \u00d7V \u2192 M. In this case, P (Y |S, V ) satisfies Property 3.1 if and only if there exists a measurable function denoted as f : [0, 1] \u00d7 S \u00d7 V \u2192 Y such that the following equation holds:\n(S, V, Y ) =a.s. ( S, V, f(\u03be,M(S, V )) ) where \u03be \u223c Unif[0, 1] and \u03be\u22a5\u22a5(S, V ); . (3)\nProof. Lemma B.3 plays a crucial role in establishing the conditional independence relationship between Y and (S, V ) based on the invariant sufficient representation M(S, V ). This lemma demonstrates that when M(S, V ) is employed, the variables Y and (S, V ) become conditionally independent, meaning that knowledge of M(S, V ) is sufficient to explain the relationship between Y and (S, V ).\nBy leveraging the insights provided by Lemma B.3, we can derive the conclusion of Theorem 3.5 with the support of lemma B.2. Lemma B.2 further strengthens the link between the conditional independence relationship and the existence of an invariant sufficient representation. It establishes the notion that the invariance of the conditional distribution P (Y |S, V ) under the group G is directly related to the presence of an invariant sufficient representation M(S, V ).\nGiven the established conditional independence relationship Y\u22a5\u22a5M(S,V )(S, V ) as demonstrated in Lemma B.3, we can now proceed to prove the following corollary by examining the definition of adequacy:\nCorollary 3.6. Let G be a compact group acting measurably on standard Borel spaces S \u00d7 V , and let M be another Borel space. Then Any invariant sufficient representation M : S \u00d7 V \u2192 M under G is an adequate statistic of (S, V ) for Y.\nThis corollary follows directly from the nature of the conditional independence relationship and the definition of adequacy. The fact that Y and (S, V ) are conditionally independent given M(S, V ) indicates that the representation M(S, V ) contains all the necessary information to explain the relationship between Y and (S, V ). In other words, the representation M(S, V ) adequately captures the relevant features and factors that influence the conditional distribution of Y given (S, V )."
        },
        {
            "heading": "B.2 SOME REMARKS",
            "text": "Throughout the course of our proof, it is possible that some points may cause confusion. To address this, we present a set of remarks and additional propositions that aim to provide further clarity and insights. Specifically, we address the question of why Eq 3 can lead to an invariant conditional distribution, considering its nature as a joint distribution scheme.\nIn the probabilistic literature, it is often more convenient to establish the invariance of joint distributions as a starting point. In our case, we focus on the joint distribution P (S, V, Y ), which can be considered invariant under certain conditions. This joint distribution is invariant if and only if:\n(S, V, Y ) d = (g \u00b7 (S, V ), Y ) for all g \u2208 G .\nThese conditions ensure that the joint distribution P (S, V, Y ) possesses the desired invariance properties required for the subsequent analysis. By establishing an invariant joint distribution, we pave the way for investigating the properties of the conditional distribution P (Y |S, V ). By leveraging the invariance of the joint distribution, we can derive the invariance of the conditional distribution P (Y |S, V ). This arises from the fact that the joint distribution scheme inherently captures the relationship between Y and (S, V ), allowing us to analyze their conditional distribution in an invariant manner (Kallenberg et al., 2017; Bloem-Reddy & Teh, 2020):\nProposition B.4. Assume a group G acting on (S,V), and then P (Y |S, V ) is G-invariant if and only if (S, V, Y ) d= (g \u00b7 (S, V ), Y ) for all g \u2208 G.\nThis proposition establishes a direct correspondence between the invariance of the conditional distribution P (Y |S, V ) and the symmetry of the joint distribution (S, V, Y ) under the group action. Specifically, it states that the conditional distribution remains invariant if and only if the joint distribution exhibits the same structural patterns and properties when transformed by any element of the group G. To address another potential source of confusion, we delve into the distinction between f(\u03be, S, V ) and f(S, V ), where the former represents a stochastic function and the latter a deterministic functional model. It is worth noting that deterministic functional models can be viewed as a special case of stochastic functions. In the context of an invariant stochastic function Y = f(\u03be, S, V ), we can establish the following relationship:\nE[Y | S, V ] = \u222b [0,1] f(\u03be, S, V ) d\u03be = h(S, V ) , (6)\nHere, E[Y | S, V ] denotes the conditional expectation of Y given S and V . By integrating the stochastic function f(\u03be, S, V ) with respect to \u03be over the range [0, 1], we arrive at the invariant deterministic function h(S, V ). Equation 6 establishes a crucial connection between the invariant stochastic function f(\u03be, S, V ) and the corresponding invariant deterministic function h(S, V ). This relationship highlights the interplay between stochasticity and determinism in modeling invariant behavior."
        },
        {
            "heading": "C PROOF OF PROPOSITION 3.7",
            "text": "Definition 3.4. (Invariant Sufficient Representation) For a group G of actions on any (s, v) \u2208 S\u00d7V , we say M : S \u00d7 V \u2192 M is a invariant sufficient representation for space S \u00d7 V , if it satisfies: If M(s1, v1) = M(s2, v2), then (s2, v2) = g \u00b7 (s1, v1) for some g \u2208 G; otherwise, there is no such g that satisfies (s2, v2) = g \u00b7 (s1, v1).\nIt is important to note that the definition of the invariant sufficient representation is formulated with respect to the variables (S, V ). To facilitate comprehension, let us revisit the definition of the invariant sufficient representation. Furthermore, to enhance clarity and ease of understanding, we will also provide the corresponding invariant sufficient representation for the marginal distribution X , which can represent either S or V . Definition C.1. For a group G of actions on any x \u2208 X , we say M : X \u2192 M is a invariant sufficient representation for some space X , if it satisfies: If M(x1) = M(x2), then x2 = g \u00b7 x1 for some g \u2208 G; otherwise, there is no such g that satisfies x2 = g \u00b7 x1. Proposition 3.7. Assuming that Ms : S \u2192 S1 and Mv : V \u2192 V1 serve as invariant sufficient representations for S and V with respect to H and R, respectively, then there exist maps f : S1 \u00d7 V1 \u2192 M that establish the invariant sufficient representation of M .\nProof. For any (s, v) Consider the function f(Ms(s),Mv(v)), where Ms and Mv represent the mappings from S and V to their corresponding invariant sufficient representations. We aim to show that f(Ms(S),Mv(V )) serves as the invariant sufficient representation of (S, V ) under the group G. First, let\u2019s examine the three possible cases. If s1 \u0338= s2 while v1 = v2, it is evident that if f(Ms(s1),Mv(v2)) = f(Ms(s2),Mv(v2)), there must exist g = (h, e) \u2208 G such that (s2, v2) = g \u00b7 (s1, v1), where e denotes the identity transformation. Similarly, if s1 = s2 but v1 \u0338= v2, the same argument holds.\nNow, let\u2019s consider the case where s1 \u0338= s2 and v1 \u0338= v2. Since f is an injective function, if f(Ms(s1),Mv(v2)) = f(Ms(s2),Mv(v2)), it implies that Ms(s1) = Ms(s2) and Mv(v1) = Mv(v2). By the definition of the invariant sufficient representation, we can conclude that if f(Ms(s1),Mv(v1)) = f(Ms(s2),Mv(v2)), then (s2, v2) = g \u00b7 (s1, v1) for some g \u2208 G. Conversely, if no such g exists to satisfy (s2, v2) = g \u00b7 (s1, v1), it implies that f(Ms(s1),Mv(v1)) \u0338= f(Ms(s2),Mv(v2)). Therefore, we can conclude that f(Ms(S),Mv(V )) serves as the invariant sufficient representation of (S, V ) under the group G. This function captures the essential information required to explain the relationship between (S, V ) and Y , ensuring that the conditional distribution P (Y |S, V ) remains invariant under the group action.\nTable 4: The statistics of Amazon product dataset, which is from (Ou et al., 2022)\nCategories |D| |V |\n\u2211\n|S\u2217| E[|S\u2217|] minS\u2217 |S\u2217| maxS\u2217 |S\u2217| Toys 2,421 30 9,924 4.09 3 14\nFurniture 280 30 892 3.18 3 6 Gear 4,277 30 16,288 3.80 3 10 Carseats 483 30 1,576 3.26 3 6 Bath 3,195 30 12,147 3.80 3 11\nHealth 2,995 30 11,053 3.69 3 9 Diaper 6,108 30 25,333 4.14 3 15\nBedding 4,524 30 17,509 3.87 3 12 Safety 267 30 846 3.16 3 5 Feeding 8,202 30 37,901 4.62 3 23 Apparel 4,675 30 21,176 4.52 3 21 Media 1,485 30 6,723 4.52 3 19"
        },
        {
            "heading": "D DETAILS OF NEURAL SUBSET SELECTION IN OS ORACLE",
            "text": ""
        },
        {
            "heading": "D.1 THE OBJECTIVE OF NEURAL SUBSET SELECTION IN OPTIMAL SUBSET ORACLE",
            "text": "Our formulation of the optimization objective is based on the framework established in (Ou et al., 2022). Specifically, the optimization objective is to address Equation 1 by adopting an implicit learning strategy grounded in probabilistic reasoning. This approach can be succinctly formulated as follows:\nargmax \u03b8\nEP(V,S)[log p\u03b8(S\u2217|V )]\ns. t. p\u03b8(S|V ) \u221d F\u03b8(S;V ),\u2200S \u2208 2V ,\nThe important step in addressing this problem involves constructing an appropriate set mass function p\u03b8(S|V ) that is monotonically increasing in relation to the utility function F\u03b8(S;V ). To achieve this, we can employ the Energy-Based Model (EBM):\np\u03b8(S|V ) = exp(F\u03b8(S;V ))\nZ , Z := \u2211 S\u2032\u2286V exp(F\u03b8(S \u2032;V )),\nIn practice, we approximate the EBM by solving a variational approximation\n\u03d5\u2217 = argmin\u03d5D(q\u03d5(Y |S, V ))||p\u03b8(S|V )), The expression of q(Y |S, V ) is defined as follows:\nq(Y |S, V ) = \u220f i\u2208S Yi \u220f i \u0338\u2208S (1\u2212 Yi), Y \u2208 [0, 1]|V |.\nNext, we would like to explain why q(Y |S, V ) can approximate P (S|V ). We have defined that Y \u2208 [0, 1]|V | and S = {0, 1}|V |. In this case, Y can be viewed as a stochastic version of S since Y can also be generated as Y = {0, 1}|V | while still satisfying the constraint Y \u2208 [0, 1]|V |. To facilitate comprehension, let us consider an illustrative scenario. Suppose we have a ground set V = {x1, x2, x3}, and the optimal subset S\u2217 is {x1, x2}, which can be represented as [1, 1, 0]. Specifically, we define P (S\u2217|V ) = 1, indicating that S\u2217 is the correct subset, while for any S \u0338= S\u2217, we have P (S|V ) = 0. Now, let\u2019s examine the case when Y = [1, 1, 0]. In this situation, we can calculate that q(Y |S\u2217, V ) = 1. This implies that q(Y |S\u2217, V ) accurately represents the probability of observing S\u2217 given V , and it correctly assigns a high probability to the optimal subset.\nMoreover, to generate Y, we construct an EquiNet (Ou et al., 2022), denoted as Y = EquiNet(V ;\u03d5) : 2V \u2192 [0, 1]|V |. This network takes the ground set V as input and outputs probabilities indicating the likelihood of each element x \u2208 V being part of the optimal subset S\u2217. In the inference stage, EquiNet is employed to predict the optimal subset for a given ground set V , using a TopN rounding approach."
        },
        {
            "heading": "E EXPERIMENTAL DETAILS",
            "text": ""
        },
        {
            "heading": "E.1 DETAILED DESCRIPTION OF DATASETS",
            "text": "Amazon Baby Registry Dataset. The Amazon baby registry data (Gillenwater et al., 2014b) is collected from Amazon with several datasets in different categories, such as toys, furniture, etc. For each category, we are provided with |V | sets of products selected by different customers. We then construct a sample (S\u2217, V ) as follows. We first removed any subset whose optimal subset size |S\u2217| is greater than or equal to 30. Then we divided the remaining subsets into the training, validation, and test folds with a 1 : 1 : 1 ratio. Finally, we randomly sampled additional 30 \u2212 |S\u2217| products from the same category to construct (S\u22c6, V ). In this way, we constructed a data point (S\u2217, V ). For completeness, We provide the statistics of the categories in Table. 4 from (Ou et al., 2022).\nDouble MNIST. This dataset includes 1000 photos, ranging from 00 to 99. To construct (S\u2217, V ), we first sampled |S\u2217| \u2208 {2, . . . , 5} images with the same digit as S\u2217. Then we selected 20 \u2212 |S\u2217| images with different digits to construct the set V \\S\u2217.\nCelebA. The CelebA dataset contains 202, 599 images and 40 attributes. We randomly chose two attributes to construct each set V with the size of 8. Then, for each set, we selected |S\u22c6| \u2208 {2, 3} images without the two attributes as the S\u22c6. To facilitate comprehension, we have included an illustrative example of the dataset in Fig. 3, sourced from the work of (Ou et al., 2022).\nPDBBind. This dataset provides a comprehensive collection of experimentally measured binding affinity data for biomolecular complexes. We used the \u201crefined\u201d part of the whole PDBBind to construct our dataset of subsets, which contain 179 complexes. To construct a data point (V, S\u22c6), we randomly sampled 30 complexes as the ground set V , and then S\u22c6\u2217 was generated by the five most active complexes in V . We constructed 1000, 100, and 100 data points for the training, validation, and test split, respectively.\nBindingDB. BindingDB is a public, web-accessible database of measured binding affinities consisting of 52, 273 drug-targets with small, drug-like molecules. Same as PDBBind, We randomly sampled 300 drug-targets from the BindingDB database to construct the ground set V and select 15 most active drug-target pairs as S\u22c6. Finally, we also generated the training, validation, and test set with the size of 1000, 100, and 100, respectively.\nE.2 THE ARCHITECTURE OF INSET\nThe structure of INSET is similar to EquiVSet, while INSET has an additional information-sharing component, i.e., EquiVSet uses only one DeepSets layer in the set function modelue, while INSET uses two. For completeness, we also provide the detailed architectures of INSET in this subsection. Firstly, the structure of DeepSets in INSET is as follows:\nSpecifically, InitLayer(S, d) encodes the set objects into vector representations. FC(d, h, f) is a fully-connected layer with activation function f . In particular, we set h as 256 and hd as 500, same as (Ou et al., 2022).\nIn all experiments, the structure of InitLayer will change based on the type of datasets.\nSynthetic datasets. The synthetic datasets consist of the Tow-Moons and Gaussian-Mixture datasets. Each instance of the set is a two-dimensional vector, which represents the corresponding Cartesian coordinates. In this dataset, the InitLayer is a one-layer feed-forward neural network FC(2, 256,\u2212). Amazon Baby Registry. In this datasets, each product is encoded into a 768-dimensional vector by the pre-trained BERT model based on its textual description. Therefore, each element of the set is a 768-dimensional feature vector, and FC(768, 256,\u2212) will be the InitLayer to process each embedding of the product.\nDouble MNIST. The double MNIST dataset consists of different digit images with a shape of (64, 64) and we transformed it as (4096, ). Then, the InitLayer is also a fully connected layer as FC(4096, 256,\u2212). CelebA. The CelebA dataset includes face images in the shape of (3, 64, 64). We used 3-depth convolutional neural networks as the InitLayer. Specifically,\nModuleList([Conv(32, 3, 2,ReLU),Conv(64, 4, 2,ReLU),\nConv(128, 5, 2,ReLU),MaxPooling,FC(128, 256,\u2212)]), where Conv(d, k, s, f) is a convolutional layer with d output channels, k kernel size, s stride size, and activation function f .\nPDBBind. The PDBBind database consists of experimentally measured binding affinities for biomolecular complexes (Liu et al., 2015a). The atomic convolutional network (ACNN) (Gomes et al., 2017) provides meaningful feature vectors for complexes by constructing nearest neighbor graphs based on the 3D coordinates of atoms and predicting binding free energies. In this work, we used ACNN as the pre-train model and used the output of the second to the lastlayer of the ACNN model to obtain the representations of complexes. Specifically, the InitLayer is defined as\nModuleList([ACNN[: \u22121],FC(1922, 2048,ReLU),FC(2048, 256,\u2212)]), where ACNN[: \u22121] denotes the ACNN module without the last prediction layer, whose output dimensionality is 1922.\nBindingDB. We employ the DeepDTA model (O\u0308ztu\u0308rk et al., 2018) as the based-encoder to transform drug-target pairs as vector representations. The detailed architecture of InitLayer used in our code is defined as follows:"
        },
        {
            "heading": "E.3 TRAINING DETAILS",
            "text": "We applied the early stopping strategy to train both the baselines and our models as in EquiVSet. Specifically, if the best validation performance is not improved in the continuous 6 epochs, we will\nstop the training process. The maximum of epochs is set as 100 for each dataset. We saved the models with the best validation performance and evaluated them on the test set. We repeated all experiments 5 times with different random seeds, and the average performance metrics and their standard deviations are reported as the final performances.\nThe proposed models are trained using the Adam optimizer (Kingma & Ba, 2014) with a fixed learning rate of 1e\u2212 4 and a weight decay rate of 1e\u2212 5. To accommodate the varying model sizes across different datasets, we select the batch size from the set {4, 8, 16, 32, 64, 128}. Notably, we choose the largest batch size that allows the model to be trained on a single GeForce RTX 2080 Ti GPU, ensuring efficient training."
        },
        {
            "heading": "F ADDITIONAL EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "F.1 SYNTHETIC EXPERIMENTS",
            "text": "We substantiate the effectiveness of our models by conducting experiments on learning set functions using two synthetic datasets: the two-moons dataset with additional noise of variance \u03c32 = 0.1, and a mixture of Gaussians represented by 12N (\u00b50,\u03a3) + 1 2N (\u00b51,\u03a3).\nFor the Gaussian mixture dataset, we specify the following data generation procedure: i) We first select an index, denoted as b, using a Bernoulli distribution with a probability of 12 . ii) Next, we sample 10 points from the Gaussian distribution N (\u00b5b,\u03a3) to construct the set S\u2217. iii) Subsequently, we sample 90 points for V \\S\u2217 from the Gaussian distribution N (\u00b51\u2212b,\u03a3). We repeat this process to obtain a total of 1,000 samples, which are then divided into training, validation, and test sets.\nBoth the two-moon dataset and the Gaussian mixture dataset serve as valuable benchmarks for evaluating the performance of our models. By conducting experiments on these datasets and collecting the necessary data points, we are able to demonstrate the efficacy of our approach in learning complex set functions. The results are reported in Table 7."
        },
        {
            "heading": "F.2 COMPUTATION COST",
            "text": "One of the key distinctions between INSET and EquiVSet lies in the inclusion of an informationsharing module, specifically a DeepSets Layer, in our architecture. However, a legitimate concern\nthat arises is whether the improved performance of INSET can be solely attributed to the additional parameters introduced by this module, rather than the underlying framework itself. To address this concern and gain deeper insights, we conducted experiments using the CelebA dataset.\nIn order to enhance the capacity of EquiVSet and enable a fair comparison, we introduced an additional convolution layer within the InitLayer. By doing so, we ensured that both EquiVSet and INSET had comparable model sizes. The experimental results, including the performance of models with different model sizes, are reported in Table 9.\nMoreover, we further investigated and analyzed the specific architecture of the initial layer for EquiVSet in two different versions, denoted as EquiVSet(v1) and EquiVSet(v2). For EquiVSet(v1), the initial layer is structured as follows:\nModuleList([Conv(32, 3, 2,ReLU),Conv(64, 4, 2,ReLU),Conv(64, 4, 2,ReLU),\nConv(128, 5, 2,ReLU),MaxPooling,FC(128, 256,\u2212)]),\nThe initial layer for EquiVSet(v2) is:"
        },
        {
            "heading": "F.3 COMPOUND SELECTION",
            "text": "In the main text, we focused on the application of a single filter. However, to provide a more practical perspective, we extended our analysis by simulating the OS (Objective Selector) oracle of compound selection using two filters: the high bioactivity filter and the diversity filter. By incorporating these additional filters, we aimed to evaluate the performance of our approach in a more realistic scenario.\nThe results of this extended analysis are presented in Table 8. These findings shed light on the effectiveness and applicability of our approach when considering multiple filters for compound selection. By incorporating both high bioactivity and diversity filters, we demonstrate the potential of our method to enhance the selection process and improve the overall quality and diversity of the selected compounds."
        },
        {
            "heading": "F.4 ABLATION STUDIES",
            "text": "To further verify the robustness of INSET, we have now conducted ablation studies focusing on the Monte-Carlo (MC) sample numbers for each input pair {(Vi, S\u2217i )}. In the context of neural subset selection tasks, our primary aim is to train the model \u03b8 to predict the optimal subset S\u2217 from a given ground set V . During training, we sample m subsets from V to optimize our model parameters \u03b8, thereby maximizing the conditional probability distribution p\u03b8(S\u2217|V ) among of all pairs of (S, V ) for for a given V. In our main experiments, we adhere to EquiVSet\u2019s protocol by setting the sample number m to 5 across all the tasks. The empirical results depicted in Figure 4 demonstrate that INSET consistently achieves satisfactory results, even with decreasing values of m."
        },
        {
            "heading": "F.5 EXPERIMENTS ON SET ANOMALY DETECTION.",
            "text": "In this experiment, we further perform set anomaly detection on CIFAR-10. Following the setup of (Ou et al., 2022), we randomly sample n \u2208 {2, 3} images as the OS oracle S\u2217, and then select 8\u2212|S\u2217| images with different labels to construct the set V \\S\u2217. We finally obtain the training, validation, and test set with the size of 10, 000, 1, 000, 1, 000, respectively. We report all the set anomaly detection results in Table 10. It is obviously that INSET outperform the baselines significantly across different datasets on set anomaly detection tasks."
        }
    ],
    "title": "ENHANCING NEURAL SUBSET SELECTION: INTEGRAT-",
    "year": 2024
}