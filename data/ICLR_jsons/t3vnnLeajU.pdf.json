{
    "abstractText": "Vision-language models such as CLIP have shown great impact on diverse downstream tasks for zero-shot or label-free predictions. However, when it comes to low-level vision such as image restoration their performance deteriorates dramatically due to corrupted inputs. In this paper, we present a degradation-aware visionlanguage model (DA-CLIP) to better transfer pretrained vision-language models to low-level vision tasks as a multi-task framework for image restoration. More specifically, DA-CLIP trains an additional controller that adapts the fixed CLIP image encoder to predict high-quality feature embeddings. By integrating the embedding into an image restoration network via cross-attention, we are able to pilot the model to learn a high-fidelity image reconstruction. The controller itself will also output a degradation feature that matches the real corruptions of the input, yielding a natural classifier for different degradation types. In addition, we construct a mixed degradation dataset with synthetic captions for DA-CLIP training. Our approach advances state-of-the-art performance on both degradation-specific and unified image restoration tasks, showing a promising direction of prompting image restoration with large-scale pretrained vision-language models. Our code is available at https://github.com/Algolzw/daclip-uir.",
    "authors": [
        {
            "affiliations": [],
            "name": "Ziwei Luo"
        },
        {
            "affiliations": [],
            "name": "Fredrik K. Gustafsson"
        },
        {
            "affiliations": [],
            "name": "Zheng Zhao"
        },
        {
            "affiliations": [],
            "name": "Jens Sj\u00f6lund"
        },
        {
            "affiliations": [],
            "name": "Thomas B. Sch\u00f6n"
        }
    ],
    "id": "SP:053898b5c114199cc4df7b46f661d79e7d92b892",
    "references": [
        {
            "authors": [
                "Eirikur Agustsson",
                "Radu Timofte"
            ],
            "title": "Ntire 2017 challenge on single image super-resolution: dataset and study",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops,",
            "year": 2017
        },
        {
            "authors": [
                "Tim Brooks",
                "Aleksander Holynski",
                "Alexei A Efros"
            ],
            "title": "Instructpix2pix: Learning to follow image editing instructions",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Tom Brown",
                "Benjamin Mann",
                "Nick Ryder",
                "Melanie Subbiah",
                "Jared D Kaplan",
                "Prafulla Dhariwal",
                "Arvind Neelakantan",
                "Pranav Shyam",
                "Girish Sastry",
                "Amanda Askell"
            ],
            "title": "Language models are few-shot learners",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Dongdong Chen",
                "Mingming He",
                "Qingnan Fan",
                "Jing Liao",
                "Liheng Zhang",
                "Dongdong Hou",
                "Lu Yuan",
                "Gang Hua"
            ],
            "title": "Gated context aggregation network for image dehazing and deraining",
            "venue": "IEEE Winter Conference on Applications of Computer Vision (WACV),",
            "year": 2019
        },
        {
            "authors": [
                "Liangyu Chen",
                "Xiaojie Chu",
                "Xiangyu Zhang",
                "Jian Sun"
            ],
            "title": "Simple baselines for image restoration",
            "venue": "In European Conference on Computer Vision,",
            "year": 2022
        },
        {
            "authors": [
                "Alexey Dosovitskiy",
                "Lucas Beyer",
                "Alexander Kolesnikov",
                "Dirk Weissenborn",
                "Xiaohua Zhai",
                "Thomas Unterthiner",
                "Mostafa Dehghani",
                "Matthias Minderer",
                "Georg Heigold",
                "Sylvain Gelly"
            ],
            "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
            "year": 2010
        },
        {
            "authors": [
                "Xiuye Gu",
                "Tsung-Yi Lin",
                "Weicheng Kuo",
                "Yin Cui"
            ],
            "title": "Open-vocabulary object detection via vision and language knowledge distillation",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Martin Heusel",
                "Hubert Ramsauer",
                "Thomas Unterthiner",
                "Bernhard Nessler",
                "Sepp Hochreiter"
            ],
            "title": "GANs trained by a two time-scale update rule converge to a local nash equilibrium",
            "venue": "In Proceedings of Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2017
        },
        {
            "authors": [
                "Jonathan Ho",
                "Ajay Jain",
                "Pieter Abbeel"
            ],
            "title": "Denoising diffusion probabilistic models",
            "venue": "In Proceedings of Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2020
        },
        {
            "authors": [
                "Chao Jia",
                "Yinfei Yang",
                "Ye Xia",
                "Yi-Ting Chen",
                "Zarana Parekh",
                "Hieu Pham",
                "Quoc Le",
                "Yun-Hsuan Sung",
                "Zhen Li",
                "Tom Duerig"
            ],
            "title": "Scaling up visual and vision-language representation learning with noisy text supervision",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Yifan Jiang",
                "Xinyu Gong",
                "Ding Liu",
                "Yu Cheng",
                "Chen Fang",
                "Xiaohui Shen",
                "Jianchao Yang",
                "Pan Zhou",
                "Zhangyang Wang"
            ],
            "title": "Enlightengan: Deep light enhancement without paired supervision",
            "venue": "IEEE Transactions on Image Processing,",
            "year": 2021
        },
        {
            "authors": [
                "Orest Kupyn",
                "Volodymyr Budzan",
                "Mykola Mykhailych",
                "Dmytro Mishkin",
                "Ji\u0159\u0131\u0301 Matas"
            ],
            "title": "Deblurgan: Blind motion deblurring using conditional adversarial networks",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2018
        },
        {
            "authors": [
                "Orest Kupyn",
                "Tetiana Martyniuk",
                "Junru Wu",
                "Zhangyang Wang"
            ],
            "title": "Deblurgan-v2: deblurring (orders-of-magnitude) faster and better",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV),",
            "year": 2019
        },
        {
            "authors": [
                "Boyun Li",
                "Xiao Liu",
                "Peng Hu",
                "Zhongqin Wu",
                "Jiancheng Lv",
                "Xi Peng"
            ],
            "title": "All-in-one image restoration for unknown corruption",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Junnan Li",
                "Dongxu Li",
                "Caiming Xiong",
                "Steven Hoi"
            ],
            "title": "Blip: Bootstrapping language-image pretraining for unified vision-language understanding and generation",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Siyuan Li",
                "Iago Breno Araujo",
                "Wenqi Ren",
                "Zhangyang Wang",
                "Eric K Tokuda",
                "Roberto Hirata Junior",
                "Roberto Cesar-Junior",
                "Jiawan Zhang",
                "Xiaojie Guo",
                "Xiaochun Cao"
            ],
            "title": "Single image deraining: A comprehensive benchmark analysis",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2019
        },
        {
            "authors": [
                "Xinqi Lin",
                "Jingwen He",
                "Ziyan Chen",
                "Zhaoyang Lyu",
                "Ben Fei",
                "Bo Dai",
                "Wanli Ouyang",
                "Yu Qiao",
                "Chao Dong"
            ],
            "title": "Diffbir: Towards blind image restoration with generative diffusion prior",
            "venue": "arXiv preprint arXiv:2308.15070,",
            "year": 2023
        },
        {
            "authors": [
                "Xiaohong Liu",
                "Yongrui Ma",
                "Zhihao Shi",
                "Jun Chen"
            ],
            "title": "Griddehazenet: Attention-based multiscale network for image dehazing",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2019
        },
        {
            "authors": [
                "Yun-Fu Liu",
                "Da-Wei Jaw",
                "Shih-Chia Huang",
                "Jenq-Neng Hwang"
            ],
            "title": "Desnownet: Context-aware deep network for snow removal",
            "venue": "IEEE Transactions on Image Processing,",
            "year": 2018
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter"
            ],
            "title": "Decoupled weight decay regularization",
            "venue": "arXiv preprint arXiv:1711.05101,",
            "year": 2017
        },
        {
            "authors": [
                "Andreas Lugmayr",
                "Martin Danelljan",
                "Andres Romero",
                "Fisher Yu",
                "Radu Timofte",
                "Luc Van Gool"
            ],
            "title": "Repaint: Inpainting using denoising diffusion probabilistic models",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Ziwei Luo",
                "Fredrik K Gustafsson",
                "Zheng Zhao",
                "Jens Sj\u00f6lund",
                "Thomas B Sch\u00f6n"
            ],
            "title": "Image restoration with mean-reverting stochastic differential equations",
            "venue": "In Proceedings of the 40th International Conference on Machine Learning,",
            "year": 2023
        },
        {
            "authors": [
                "Ziwei Luo",
                "Fredrik K Gustafsson",
                "Zheng Zhao",
                "Jens Sj\u00f6lund",
                "Thomas B Sch\u00f6n"
            ],
            "title": "Refusion: Enabling large-size realistic image restoration with latent-space diffusion models",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "David Martin",
                "Charless Fowlkes",
                "Doron Tal",
                "Jitendra Malik"
            ],
            "title": "A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics",
            "venue": "In Proceedings of the 18th IEEE International Conference on Computer Vision (ICCV),",
            "year": 2001
        },
        {
            "authors": [
                "Seungjun Nah",
                "Tae Hyun Kim",
                "Kyoung Mu Lee"
            ],
            "title": "Deep multi-scale convolutional neural network for dynamic scene deblurring",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2017
        },
        {
            "authors": [
                "Minheng Ni",
                "Xiaoming Li",
                "Wangmeng Zuo"
            ],
            "title": "NUWA-LIP: Language-guided image inpainting with defect-free VQGAN",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Vaishnav Potlapalli",
                "Syed Waqas Zamir",
                "Salman Khan",
                "Fahad Shahbaz Khan"
            ],
            "title": "Promptir: Prompting for all-in-one blind image restoration",
            "venue": "Advances in Neural Information Processing Systems (NeurIPS),",
            "year": 2023
        },
        {
            "authors": [
                "Rui Qian",
                "Robby T Tan",
                "Wenhan Yang",
                "Jiajun Su",
                "Jiaying Liu"
            ],
            "title": "Attentive generative adversarial network for raindrop removal from a single image",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Xu Qin",
                "Zhilin Wang",
                "Yuanchao Bai",
                "Xiaodong Xie",
                "Huizhu Jia"
            ],
            "title": "FFA-Net: Feature fusion attention network for single image dehazing",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Liangqiong Qu",
                "Jiandong Tian",
                "Shengfeng He",
                "Yandong Tang",
                "Rynson WH Lau"
            ],
            "title": "Deshadownet: A multi-context embedding deep network for shadow removal",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Dongwei Ren",
                "Wangmeng Zuo",
                "Qinghua Hu",
                "Pengfei Zhu",
                "Deyu Meng"
            ],
            "title": "Progressive image deraining networks: a better and simpler baseline",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2019
        },
        {
            "authors": [
                "Robin Rombach",
                "Andreas Blattmann",
                "Dominik Lorenz",
                "Patrick Esser",
                "Bj\u00f6rn Ommer"
            ],
            "title": "Highresolution image synthesis with latent diffusion models",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Christoph Schuhmann",
                "Romain Beaumont",
                "Richard Vencu",
                "Cade Gordon",
                "Ross Wightman",
                "Mehdi Cherti",
                "Theo Coombes",
                "Aarush Katta",
                "Clayton Mullis",
                "Mitchell Wortsman"
            ],
            "title": "Laion-5b: An open large-scale dataset for training next generation image-text models",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "H Sheikh"
            ],
            "title": "Live image quality assessment database release 2. http://live",
            "venue": "ece. utexas. edu/research/quality,",
            "year": 2005
        },
        {
            "authors": [
                "Yuda Song",
                "Zhuqing He",
                "Hui Qian",
                "Xin Du"
            ],
            "title": "Vision transformers for single image dehazing",
            "venue": "IEEE Transactions on Image Processing,",
            "year": 1927
        },
        {
            "authors": [
                "Yonglong Tian",
                "Dilip Krishnan",
                "Phillip Isola"
            ],
            "title": "Contrastive multiview coding",
            "venue": "In European Conference on Computer Vision,",
            "year": 2020
        },
        {
            "authors": [
                "Radu Timofte",
                "Eirikur Agustsson",
                "Luc Van Gool",
                "Ming-Hsuan Yang",
                "Lei Zhang"
            ],
            "title": "NTIRE 2017 challenge on single image super-resolution: methods and results",
            "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops,",
            "year": 2017
        },
        {
            "authors": [
                "Zhengzhong Tu",
                "Hossein Talebi",
                "Han Zhang",
                "Feng Yang",
                "Peyman Milanfar",
                "Alan Bovik",
                "Yinxiao Li"
            ],
            "title": "Maxim: Multi-axis mlp for image processing",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Jianyi Wang",
                "Zongsheng Yue",
                "Shangchen Zhou",
                "Kelvin CK Chan",
                "Chen Change Loy"
            ],
            "title": "Exploiting diffusion prior for real-world image super-resolution",
            "venue": "arXiv preprint arXiv:2305.07015,",
            "year": 2023
        },
        {
            "authors": [
                "Xintao Wang",
                "Liangbin Xie",
                "Chao Dong",
                "Ying Shan"
            ],
            "title": "Real-esrgan: Training real-world blind super-resolution with pure synthetic data",
            "venue": "In Proceedings of the IEEE/CVF international conference on computer vision,",
            "year": 2021
        },
        {
            "authors": [
                "Zhou Wang",
                "Alan C Bovik",
                "Hamid R Sheikh",
                "Eero P Simoncelli"
            ],
            "title": "Image quality assessment: from error visibility to structural similarity",
            "venue": "IEEE Transactions on Image Processing,",
            "year": 2004
        },
        {
            "authors": [
                "Chen Wei",
                "Wenjing Wang",
                "Wenhan Yang",
                "Jiaying Liu"
            ],
            "title": "Deep retinex decomposition for low-light enhancement",
            "venue": "arXiv preprint arXiv:1808.04560,",
            "year": 2018
        },
        {
            "authors": [
                "Wenhui Wu",
                "Jian Weng",
                "Pingping Zhang",
                "Xu Wang",
                "Wenhan Yang",
                "Jianmin Jiang"
            ],
            "title": "Uretinex-net: Retinex-based deep unfolding network for low-light image enhancement",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Liangbin Xie",
                "Xintao Wang",
                "Chao Dong",
                "Zhongang Qi",
                "Ying Shan"
            ],
            "title": "Finding discriminative filters for specific degradations in blind super-resolution",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Wenhan Yang",
                "Robby T Tan",
                "Jiashi Feng",
                "Jiaying Liu",
                "Zongming Guo",
                "Shuicheng Yan"
            ],
            "title": "Deep joint rain detection and removal from a single image",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Wenhan Yang",
                "Robby T Tan",
                "Jiashi Feng",
                "Zongming Guo",
                "Shuicheng Yan",
                "Jiaying Liu"
            ],
            "title": "Joint rain detection and removal from a single image with contextualized deep networks",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 2019
        },
        {
            "authors": [
                "Syed Waqas Zamir",
                "Aditya Arora",
                "Salman Khan",
                "Munawar Hayat",
                "Fahad Shahbaz Khan",
                "MingHsuan Yang",
                "Ling Shao"
            ],
            "title": "Learning enriched features for real image restoration and enhancement",
            "venue": "In Proceedings of the 16th European Conference on Computer Vision,",
            "year": 2020
        },
        {
            "authors": [
                "Syed Waqas Zamir",
                "Aditya Arora",
                "Salman Khan",
                "Munawar Hayat",
                "Fahad Shahbaz Khan",
                "MingHsuan Yang",
                "Ling Shao"
            ],
            "title": "Multi-stage progressive image restoration",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2021
        },
        {
            "authors": [
                "Syed Waqas Zamir",
                "Aditya Arora",
                "Salman Khan",
                "Munawar Hayat",
                "Fahad Shahbaz Khan",
                "MingHsuan Yang"
            ],
            "title": "Restormer: efficient transformer for high-resolution image restoration",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2022
        },
        {
            "authors": [
                "Jingyi Zhang",
                "Jiaxing Huang",
                "Sheng Jin",
                "Shijian Lu"
            ],
            "title": "Vision-language models for vision tasks: A survey",
            "venue": "arXiv preprint arXiv:2304.00685,",
            "year": 2023
        },
        {
            "authors": [
                "Kai Zhang",
                "Wangmeng Zuo",
                "Yunjin Chen",
                "Deyu Meng",
                "Lei Zhang"
            ],
            "title": "Beyond a Gaussian denoiser: Residual learning of deep cnn for image denoising",
            "venue": "IEEE Transactions on Image Processing,",
            "year": 2017
        },
        {
            "authors": [
                "Kai Zhang",
                "Jingyun Liang",
                "Luc Van Gool",
                "Radu Timofte"
            ],
            "title": "Designing a practical degradation model for deep blind image super-resolution",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Lvmin Zhang",
                "Maneesh Agrawala"
            ],
            "title": "Adding conditional control to text-to-image diffusion models",
            "venue": "arXiv preprint arXiv:2302.05543,",
            "year": 2023
        },
        {
            "authors": [
                "Renrui Zhang",
                "Wei Zhang",
                "Rongyao Fang",
                "Peng Gao",
                "Kunchang Li",
                "Jifeng Dai",
                "Yu Qiao",
                "Hongsheng Li"
            ],
            "title": "Tip-adapter: Training-free adaption of clip for few-shot classification",
            "venue": "In European Conference on Computer Vision,",
            "year": 2022
        },
        {
            "authors": [
                "Richard Zhang",
                "Phillip Isola",
                "Alexei A Efros",
                "Eli Shechtman",
                "Oliver Wang"
            ],
            "title": "The unreasonable effectiveness of deep features as a perceptual metric",
            "venue": "In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2018
        },
        {
            "authors": [
                "Kaiyang Zhou",
                "Jingkang Yang",
                "Chen Change Loy",
                "Ziwei Liu"
            ],
            "title": "Learning to prompt for visionlanguage models",
            "venue": "International Journal of Computer Vision,",
            "year": 2022
        },
        {
            "authors": [
                "Yang"
            ],
            "title": "mixed-degradation dataset only contains heavy rain images (from the Rain100H dataset), Rain100L is an out-of-distribution dataset with a relatively low-level degradation. Table 5 summarises the results. It is observed that our method performs surprisingly well on this unseen dataset and surpasses all other unified approaches by a significant margin",
            "year": 2017
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Large-scale pretrained vision-language models (VLMs) such as CLIP (Radford et al., 2021) have recently garnered significant attention, in part because of their wide-reaching usefulness on many fundamental computer vision tasks (Gu et al., 2021; Zhang et al., 2022; 2023). However, existing VLMs have so far had limited impact on low-level vision tasks such as image restoration (IR), presumably because they do not capture the fine-grained difference between image degradation types such as \u201cblurry\u201d and \u201cnoisy\u201d (Ni et al., 2023). Consequently, the existing VLMs often misalign image features to degradation texts. This is not surprising, considering that VLMs are generally trained on diverse, web-scale datasets, in contrast to most image restoration models which are trained on comparatively small datasets that are curated for a specific task without corresponding image-text pairs (Li et al., 2019; Zhang et al., 2017; Zamir et al., 2022).\nImage restoration methods often simply learn to generate images pixel-by-pixel without leveraging task knowledge, which usually requires repeated training of the same model for specific degradation types. A recent line of work has, however, focused on unified image restoration, where a single model is trained on a mixed degradation dataset and implicitly classify the type of degradation in the restoration process (Li et al., 2022a; Potlapalli et al., 2023). While the results are impressive, they are still limited to a small number of degradation types and the specific datasets that go with them. In particular, they do not make use of the vast amount of information embedded in VLMs.\nIn this paper, we combine the large-scale pretrained vision-language model CLIP with image restoration networks and present a multi-task framework that can be applied to both degradation-specific and unified image restoration problems. Specifically, aiming at addressing feature mismatching between corrupted inputs and clean captions, we propose an Image Controller that adapts the VLM\u2019s image encoder to output high-quality (HQ) content embeddings aligned with clean captions. Meanwhile, the controller itself also predicts a degradation embedding to match the real degradation types. This novel framework, which we call degradation-aware CLIP (DA-CLIP), incorporates the humanlevel knowledge from VLMs into general networks that improve image restoration performance and enable unified image restoration. In addition, to learn high-quality features and degradation types from low-quality (LQ) inputs, we construct a large mixed-degradation dataset for ten different image restoration tasks based on BLIP (Li et al., 2022b). As shown in Figure 1, our DA-CLIP accurately classifies the ten different degradation types and can readily be integrated into existing restoration models, helping produce visually appealing results across the different degradations.\nOur main contributions are summarised as follows: (1) We present DA-CLIP to leverage large-scale pretrained vision-language models as a universal framework for image restoration. The key component is an image controller that predicts the degradation and adapts the fixed CLIP image encoder to output high-quality content embeddings from corrupted inputs. (2) We use cross-attention to integrate the content embedding into restoration networks to improve their performance. Moreover, we introduce a prompt learning module to better utilize the degradation context for unified image restoration. (3) We construct a mixed degradation dataset containing ten different degradation types with high-quality synthetic captions. This dataset can be used to train either DA-CLIP or a unified image restoration model. (4) We demonstrate the effectiveness of DA-CLIP by applying it to image restoration models for both degradation-specific and unified image restoration. Our approach achieves highly competitive performance across all ten degradation types."
        },
        {
            "heading": "2 BACKGROUND AND RELATED WORK",
            "text": "Image restoration Image restoration aims to recover a high-quality image from its corrupted counterpart, which is a fundamental and long-standing problem in computer vision and contains various tasks such as image denoising (Zhang et al., 2017), deraining (Ren et al., 2019), dehazing (Song et al., 2023), deblurring (Kupyn et al., 2018), etc. Most existing works focus on the degradationspecific task which trains multiple models for different tasks separately by directly learning the target with common pixel-based losses (e.g., \u21131 or \u21132). Recently, however, increasing attention has been paid to unified image restoration where multiple image restoration tasks are handled with a single model (Li et al., 2022a; Potlapalli et al., 2023). These works achieve impressive results by using an additional encoder (Li et al., 2022a; Zhou et al., 2022) or a visual prompt module (Potlapalli et al., 2023) to implicitly cluster inputs according to the degradation type. However, they are still limited to a few image restoration tasks and do not consider auxiliary information about the degradation type.\n(a) Degradation-aware CLIP (DA-CLIP) (b) Image restortion with DA-CLIP\nBlind image restoration (BIR) To deal with unknown degradation levels, BIR comes into view and has shown promising results for photos captured in the real-world (Zhang et al., 2021; Wang et al., 2021; Xie et al., 2021). In particular, BSRGAN (Zhang et al., 2021) and Real-ESRGAN (Wang et al., 2021) utilize GANs with practical degradation settings. Inspired by recent advanced diffusion models, StableSR (Wang et al., 2023) and DiffBIR (Lin et al., 2023) propose to exploit diffusion priors to generate realistic outputs. Although pretrained diffusion weights are well utilized in these methods, they do not make use of the semantic information embedded in vision-language models.\nVision-language models (VLMs) Recent works have demonstrated the great potential of applying pretrained VLMs to improve downstream tasks with generic visual and text representations (Radford et al., 2021; Jia et al., 2021; Li et al., 2022b). A classic VLM usually consists of a text encoder and an image encoder and tries to learn aligned multimodal features from noisy image-text pairs with contrastive learning (Radford et al., 2021). BLIP (Li et al., 2022b) further proposes to remove noisy web data by bootstrapping synthetic captions. Although VLMs provide a strong capability of zeroshot and label-free classification for downstream tasks, they have so far had limited effect on image restoration due to the need for specialized and accurate terminology. A noteworthy approach for finetuning vision-language models is so-called prompt learning (Zhou et al., 2022), where the prompt\u2019s context words are represented by learnable vectors that are then optimized for the downstream task.\nText-to-image generation Text-to-image models such as stable diffusion (Rombach et al., 2022) have gained extraordinary attention from both researchers and the general public. ControlNet (Zhang & Agrawala, 2023) builds upon this work and proposes to add controls to the diffusion network to make it adapt to task-specific conditions. InstructPix2Pix (Brooks et al., 2023) further combines GPT-3 (Brown et al., 2020) with stable diffusion to perform an instruction-based image-to-image translation. Although it can generate highly realistic images, it can not be directly applied to image restoration tasks since the latter requires highly accurate reconstruction abilities."
        },
        {
            "heading": "3 DEGRADATION-AWARE CLIP",
            "text": "At the core of our approach is the idea of controlling a pre-trained CLIP model to output the highquality image feature from a corrupted image while simultaneously predicting the degradation type.\nAs summarised in Figure 2, the image content embedding eIc matches the clean caption embedding eTc . Moreover, the image degradation embedding e I d predicted by the controller specifies the corruption type of the input, i.e. the corresponding degradation embedding eTd from text encoder. These features can then be integrated into other image restoration models to improve their performance."
        },
        {
            "heading": "3.1 IMAGE CONTROLLER",
            "text": "The image controller is a copy of the CLIP image encoder but wrapped with a few zero-initialised connections to add controls to the encoder. It manipulates the outputs of all encoder blocks to control the prediction of the image encoder. In this paper, we use ViT (Dosovitskiy et al., 2020) as the default backbone for both the encoder and the controller. Figure 3(a) illustrates the controlling procedure, where the output of the controller consists of two parts: an image degradation embedding eId and hidden controls hc. Note that the latter contains all outputs from the transformer blocks, which are subsequently added to the corresponding encoder blocks to control their predictions. The connections between the transformer blocks are simple dense neural networks with all parameters initialized to zero, which gradually influence the image encoder during training (Zhang & Agrawala, 2023). Since our training dataset is tiny compared to the web-scale datasets used in VLMs, this controlling strategy alleviates overfitting while preserving the capability of the original image encoder.\nOptimising the image controller We freeze all weights of the pretrained CLIP model and only fine-tune the image controller. To make the degradation-embedding spaces discriminative and wellseparated, we use a contrastive objective (Tian et al., 2020) to learn the embedding matching process. Let N denote the number of paired embeddings (from text encoder and image encoder/controller) in a training batch. The contrastive loss is defined as:\nLcon(x,y) = \u2212 1\nN N\u2211 i=1 log ( exp (x\u22bai yi/\u03c4)\u2211N j=1 exp(x \u22ba i yj/\u03c4) ) , (1)\nwhere x and y are normalised vectors, and \u03c4 is a learnable temperature parameter that controls the contrastive strength. Minimising Equation 1 amounts to optimising the cosine similarity between correctly paired embeddings while enlarging the difference with other embeddings, similar to the cross-entropy loss (Radford et al., 2021). To optimise both content and degradation embeddings, we use the following joint objective:\nLc(\u03c9) = Lcon(eIc , eTc ;\u03c9) + Lcon(eId, eTd ;\u03c9), (2) where \u03c9 represents the learnable parameters of the controller. Note that all image-based embeddings (i.e., eIc and e I d) are obtained from the LQ image and all text-based embeddings (i.e., e T c and e T d ) are from the clean caption and real degradation. Learning to align these embeddings enables DA-CLIP to predict real degradation types and HQ content features for corrupted image inputs."
        },
        {
            "heading": "3.2 IMAGE RESTORATION WITH DA-CLIP",
            "text": "We use IR-SDE (Luo et al., 2023a) as the base framework for image restoration. It adapts a U-Net architecture similar to DDPM (Ho et al., 2020) but removes all self-attention layers. To inject clean content embeddings into the diffusion process, we introduce a cross-attention (Rombach et al., 2022) mechanism to learn semantic guidance from pre-trained VLMs. Considering the varying input sizes in image restoration tasks and the increasing cost of applying attention to high-resolution features, we only use cross-attention in the bottom blocks of the U-Net for sample efficiency.\nOn the other hand, the predicted degradation embeddings are useful for unified image restoration, where the aim is to process low-quality images of multiple degradation types with a single model (Li et al., 2022a). As illustrated in Figure 1, our DA-CLIP accurately classifies the degradation across different datasets and various degradation types, which is crucial for unified image restoration (Li et al., 2022a). Moreover, to make use of these degradation embeddings, we combine them with a prompt learning (Zhou et al., 2022) module to further improve the results, as shown in Figure 3(b). Given state xt and low-quality image \u00b5, our final diffusion network is conditioned on the time t and additional embeddings eIc and e I d, as \u03f5\u03b8(xt, \u00b5, t, e I c , e I d), which can be trained with either noisematching loss (Ho et al., 2020) or maximum likelihood loss (Luo et al., 2023a;b).\nGenerally, we can use cross-attention to integrate content embedding into networks to improve their performance on an image restoration task. In contrast, the prompt module combined with degradation embedding specifically aims to improve the classification of the degradation type in the context of unified image restoration."
        },
        {
            "heading": "4 DATASET CONSTRUCTION",
            "text": "A crucial point of this paper is to leverage powerful vision-language models to learn multiple image degradations and extract degradation-free features for image restoration. For this purpose, we collect a large dataset with ten different image degradation types: blurry, hazy, JPEG-compression, lowlight, noisy, raindrop, rainy, shadowed, snowy, and inpainting. Table 1 summarises the tasks and the number of training and testing images for each degradation type, and more details are provided in Appendix A. Example low-quality images for the ten degradations are also shown in Figure 1.\nGenerating image-text-degradation pairs In order to train DA-CLIP on the mixed-degradation dataset, we use the bootstrapped vision-language framework BLIP (Li et al., 2022b) to generate synthetic captions for all HQ images. Since the inputs are clean, the generated captions are assumed to be accurate and of high-quality. As illustrated in Figure 4, we are then able to construct imagetext-degradation pairs by directly combining these clean captions, LQ images, and the corresponding degradation types. This dataset allows us to train either vision-language models (based on imagetext-degradation pairs) or a unified image restoration framework (based on LQ-HQ image pairs)."
        },
        {
            "heading": "5 EXPERIMENTS",
            "text": "We experimentally evaluate our method on two types of tasks: degradation-specific image restoration and unified image restoration. In the degradation-specific setting (Section 5.1), restoration models are separately trained for each of the considered degradation types. In unified image restoration (Section 5.2), a single model is instead jointly trained on all degradation types. Implementation details and additional results are provided in Appendix B and Appendix C.\nModel Evaluation Our DA-CLIP is mainly evaluated in terms of how it affects the performance of the downstream image restoration model, which we evaluate both on multiple degradation-specific tasks and on unified image restoration. IR-SDE (Luo et al., 2023a) is used as our primary baseline model. We use the Learned Perceptual Image Patch Similarity (LPIPS) (Zhang et al., 2018) and Fre\u0301chet inception distance (FID) (Heusel et al., 2017) as our main metrics for perceptual evaluation, but also report PSNR and SSIM (Wang et al., 2004) for reference. Additionally, we evaluate DACLIP in terms of how well it can classify the ten different degradation types on the mixed dataset."
        },
        {
            "heading": "5.1 DEGRADATION-SPECIFIC IMAGE RESTORATION",
            "text": "We integrate our DA-CLIP into the baseline diffusion model IR-SDE and evaluate them on four degradation-specific tasks: image deraining on the Rain100H dataset (Yang et al., 2017), low-light image enhancement on LOL (Wei et al., 2018), image deblurring on the GoPro dataset (Nah et al., 2017), and image dehazing on RESIDE-6k (Qin et al., 2020). All training and testing datasets are taken from the mixed degradation dataset described in Section 4.\nComparison approaches For all tasks, we compare our method to the prevailing approaches in their respective fields such as 1) JORDER (Yang et al., 2019), PReNet (Ren et al., 2019), and MPRNet (Zamir et al., 2021) for deraining; 2) EnlightenGAN (Jiang et al., 2021), MIRNet (Zamir et al., 2020), and URetinex-Net (Wu et al., 2022) for low-light enhancement; 3) DeepDeblur (Nah et al., 2017), DeblurGAN (Kupyn et al., 2018), and DeblurGAN-v2 (Kupyn et al., 2019) for GoPro deblurring; 4) GCANet (Chen et al., 2019), GridDehazeNet (Liu et al., 2019), and DehazeFormer (Song et al., 2023) for image dehazing. We also compare with MAXIM (Tu et al., 2022), an advanced network architecture that achieves state-of-the-art performance on multiple degradation-specific tasks.\nResults The quantitative results on different datasets are summarised in Table 2. Our method achieves the best perceptual results across all tasks, and even sets a new state-of-the-art performance for all metrics on the image deraining task. Compared to the baseline method IR-SDE, our approach consistently improves its results for all datasets and metrics, demonstrating that the HQ content embedding from DA-CLIP leads to better performance for downstream image restoration models. A visual comparison of our method with other approaches is also illustrated in Figure 5. Our method produces mostly clear and visually appealing results that are close to the HQ images."
        },
        {
            "heading": "5.2 UNIFIED IMAGE RESTORATION",
            "text": "We evaluate our method for unified image restoration on the mixed degradation dataset which contains ten different degradation types (see Section 4 for details).\nComparison approaches We compare our method with three approaches: Restormer (Zamir et al., 2022), AirNet (Li et al., 2022a), and PromptIR (Potlapalli et al., 2023). Restormer is an advanced transformer-based network. AirNet trains an extra encoder to differentiate degradation types using contrastive learning, whereas PromptIR employs a visual prompt module to guide the restoration. The latter two methods are designed specifically for unified image restoration.\nResults We illustrate the comprehensive comparisons in Figure 6 and also provide the average results across all ten degradation types in Table 3. The results show that our method achieves the best perceptual results (especially in terms of FID) across the ten degradations, while still having a good distortion performance. More importantly, by simply integrating DA-CLIP into the network, we significantly outperform the IR-SDE baseline in terms of all four metrics. A visual comparison is shown in Figure 8. On JPEG and noise removal, our method produces realistic-looking but somewhat noisy images, giving lower distortion metrics for those particular tasks. In Appendix B.3, we evaluate methods on out-of-distribution light rain images, in which DA-CLIP surpasses all other unified approaches, demonstrating that our method can generalize to unseen degradation levels.\nMoreover, we further integrate1 DA-CLIP into an MSE-based network NAFNet (Chen et al., 2022) as a variant of our method. The results are illustrated in Table 3 and Figure 7. We observe that adding our degradation context significantly improves the results, and the final performance of NAFNet with DA-CLIP even surpasses PromptIR across all metrics. This demonstrates that DA-CLIP can be integrated with both diffusion-based and direct restoration models, further improving their performance for unified image restoration. More results can be found in Appendix B.4.\nFinally, we evaluate DA-CLIP in terms of degradation type classification as shown in Figure 1 and Table 4 (in the Appendix). The original CLIP model achieves less than 2% accuracy in recognizing noisy and raindrop images, and even 0% accuracy for inpainting, which would confuse a downstream unified image restoration model. By training the controller on the mixed degradation dataset, DA-CLIP perfectly predicts all degradations except blurry, for which it achieves 91.6% accuracy.\n1We add the prompt module to all blocks and use cross-attention in the bottom layers."
        },
        {
            "heading": "5.3 DISCUSSION AND ANALYSIS",
            "text": "We have two different embeddings from the DA-CLIP: an HQ content embedding and a degradation embedding, which are both predicted from the LQ image. The former can be used for improving general image restoration performance, whereas the latter is predicted to classify degradation types for unified image restoration models. As can be observed in Figure 9a, separately applying either the degradation embedding or the HQ content embedding improves the unified restoration performance, and the performance is further improved by applying both embeddings as in our DA-CLIP. Moreover, directly embedding the ground truth HQ image and the real degradation type leads to an upper-bound performance of our method, as illustrated in Figure 9b.\nWe also compare using the HQ content image embedding from our DA-CLIP with the original image content embedding obtained from the OpenAI CLIP (Radford et al., 2021). For the unified image restoration task, we observe in Figure 9c that using the original CLIP content embedding fails to substantially improve the performance. For the degradation-specific setting as shown in Figure 9d, we also observe that our DA-CLIP clearly outperforms the baseline of using the original CLIP.\nAs an alternative to using the HQ content and degradation embeddings of DA-CLIP (from the LQ image), the degradation type and HQ caption could be encoded directly using the CLIP text encoder. This alternative approach thus requires access to the ground truth degradation label and caption text for each LQ image. Surprisingly, we observe in Figure 9e that the two degradation embeddings give very similar performance. And Figure 9f further shows that caption embeddings are also helpful for image restoration. Moreover, while using the ground truth text embeddings improves the baseline performance, it is slightly inferior compared to using our DA-CLIP image embeddings (Figure 9g), meaning that DA-CLIP has learned to accurately predict the degradation type and HQ content embeddings. Finally, an analysis of the degradation prompt module is provided in Figure 9h, demonstrating that prompt learning also facilitates unified image restoration.\nWhile we have demonstrated the effectiveness of our proposed DA-CLIP in various settings, it is worth acknowledging one potential limitation: increased model complexity and computational cost. As can be observed in Table 8 in the Appendix, DA-CLIP significantly increases the memory requirements compared to the baseline models (both NAFNet and IR-SDE). The test-time computational cost (FLOPs and runtime) is however virtually unaffected."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "This paper presents DA-CLIP to leverage large-scale pretrained vision-language models as a universal framework for image restoration. At the core of our approach is a controller that accurately predicts the degradation embeddings from LQ images and also controls the CLIP image encoder to output clean content embeddings. To train DA-CLIP, we construct a mixed degradation dataset containing synthetic captions from HQ images. DA-CLIP is then integrated into downstream image restoration models using a prompt learning module and a cross-attention mechanism. Experimental evaluation on both degradation-specific and unified image restoration tasks demonstrates that DACLIP consistently improves the restoration performance, across a variety of degradation types. On the other hand, we notice that the current dataset makes it hard to restore multiple degradations in the same scene. In future work, we are interested in creating practical models that are more robust to real-world captured photos and are able to fully restore images with mixed degradation types."
        },
        {
            "heading": "ACKNOWLEDGEMENTS",
            "text": "This research was supported by the Wallenberg AI, Autonomous Systems and Software Program (WASP) funded by the Knut and Alice Wallenberg Foundation; by the project Deep Probabilistic Regression \u2013 New Models and Learning Algorithms (contract number: 2021-04301) funded by the Swedish Research Council; and by the Kjell & Ma\u0308rta Beijer Foundation. The computations were enabled by the Berzelius resource provided by the Knut and Alice Wallenberg Foundation at the National Supercomputer Centre."
        },
        {
            "heading": "A MORE DETAILS ABOUT DATASETS",
            "text": "In this section, we give more details about the mixed degradation dataset in Section 4. We collect images for 10 different image restoration tasks, including blurry, hazy, JPEG-compressing, lowlight, noisy, raindrop, rainy, shadowed, snowy, and inpainting, as shown in Figure 1. The details of these datasets are listed below:\n\u2022 Blurry: collected from the GoPro (Nah et al., 2017) dataset containing 2103 and 1111 training and testing images, respectively.\n\u2022 Hazy: collected from the RESIDE-6k (Qin et al., 2020) dataset which has mixed indoor and outdoor images with 6000 images for training and 1000 images for testing.\n\u2022 JPEG-compressing: the training dataset has 3440 images collected from DIV2K (Agustsson & Timofte, 2017) and Flickr2K (Timofte et al., 2017). The testing dataset contains 29 images from LIVE1 (Sheikh, 2005). Moreover, all LQ images are synthetic data with a JPEG quality factor of 10.\n\u2022 Low-light: collected from the LOL (Wei et al., 2018) dataset containing 485 images for training and 15 images for testing.\n\u2022 Noisy: the training dataset is the same as that in JPEG-compressing but all LQ images are generated by adding Gaussian noise with noise level 50. The testing images are from CBSD68 (Martin et al., 2001) and also added that Gaussian noise.\n\u2022 Raindrop: collected from the RainDrop (Qian et al., 2018) dataset containing 861 images for training and 58 images for testing.\n\u2022 Rainy: collected from the Rain100H (Yang et al., 2017) dataset containing 1800 images for training and 100 images for testing.\n\u2022 Shadowed: collection from the SRD (Qu et al., 2017) dataset containing 2680 images for training and 408 images for testing.\n\u2022 Snowy: collected from the Snow100K-L (Liu et al., 2018) dataset. Since the original dataset is too large (100K images), we only use a subset which contains 1872 images for training and 601 images for testing.\n\u2022 Inpainting: we use CelebaHQ as the training dataset and divide 100 images with 100 thin masks from RePaint (Lugmayr et al., 2022) for testing.\nWe also provide several visual examples for each task for a better understanding of the 10 degradations and datasets, as shown in Figure 10.\nB IMPLEMENTATION DETAILS AND MORE ANALYSIS\nB.1 IMPLEMENTATION DETAILS\nThe base CLIP model uses ViT-B-32 as the backbone for its image encoder, with weights pretrained on the LAION-2B dataset (Schuhmann et al., 2022). Built upon it, we fine-tune the DA-CLIP on the mixed degradation dataset with a batch size of 3 136 (784 \u00d7 4) and learning rate 3 \u00d7 10\u22125. In preprocessing, all inputs are normalized in the range [0, 1] and resized to 224 \u00d7 224 with bicubic interpolation. We train the DA-CLIP model on four NVIDIA A100 GPUs for 50 epochs, in approximately 3 hours. For the restoration model, we use a batch size of 16 and randomly crop images to 256 \u00d7 256 for data augmentation. The initial learning rate is 2 \u00d7 10\u22124. We use the AdamW (Loshchilov & Hutter, 2017) optimizer (\u03b21 = 0.9, \u03b21 = 0.99) with cosine decay for a total of 700K iterations. All training is done using one A100 GPU for about 5 days."
        },
        {
            "heading": "B.2 ADDITIONAL ANALYSIS OF DA-CLIP",
            "text": "We provide the degradation classification results of three strategies: 1) retraining CLIP on our dataset; 2) fine-tuning CLIP from pretrained weights; and 3) adding a controller but without zeroinitializing its dense layers. The prompt is set to \u201ca [degradation type] photo\u201d for all models. The results are reported in Table 4. Obviously, CLIP with finetuning significantly improves the results of\ndirect retraining, demonstrating the effectiveness of large-scale vision-language model pretraining. Note that although fine-tuning CLIP achieves slightly better performance on blurry classification than DA-CLIP, it is not able to predict HQ content embeddings from the LQ image input, which leads to limited effects on downstream restoration models. Moreover, initializing dense layers to zero can further improve the accuracy of all datasets without adding additional costs."
        },
        {
            "heading": "B.3 EXPERIMENTS ON OUT-OF-DISTRIBUTION (OOD) DEGRADATION",
            "text": "To evaluate our model\u2019s generalization capability, we apply our model on an additional light rain dataset Rain100L Yang et al. (2017). Since our mixed-degradation dataset only contains heavy rain images (from the Rain100H dataset), Rain100L is an out-of-distribution dataset with a relatively low-level degradation. Table 5 summarises the results. It is observed that our method performs surprisingly well on this unseen dataset and surpasses all other unified approaches by a significant margin. In addition, we also provide some real-world visual examples in Figure 17, showing that our model seems to generalize well compared to the AirNet and PrompIR baselines.\nB.4 IMPROVING MSE-BASED METHOD WITH DA-CLIP\nIt is worth noting that our DA-CLIP can not only benefit the diffusion-based image restoration approach but also facilitate standard MSE-based model learning. Table 6 provides two ablation experiments on NAFNet (Chen et al., 2022) and Restormer (Zamir et al., 2022). Note that PromptIR is built upon Restormer but with additional degradation prompt modules. As can be seen, the degradation context is useful for both methods, and adding additional content embedding can further improve their performance."
        },
        {
            "heading": "B.5 TRAINING CURVES ON SINGLE DEGRADATION TASKS",
            "text": "To further illustrate the effectiveness of our method, we compare the training curves between the baseline model IR-SDE (Luo et al., 2023a) and that with our DA-CLIP embeddings on four different degradation-specific image restoration tasks: image deraining, low-light enhancement, image deblurring, and image dehazing. The results are shown in Figure 11, from which we can see the training with our DA-CLIP obviously performs better than the baseline model."
        },
        {
            "heading": "B.6 ANALYSIS ABOUT THE MODEL COMPLEXITY",
            "text": "We have shown the effectiveness of applying our DA-CLIP to various image restoration models and tasks. However, it is worth acknowledging one potential limitation: the model complexity is also increased along with the DA-CLIP inference and with additional prompt modules and crossattention layers. The comparison of model complexities is shown in Table 8. DA-CLIP significantly increases the memory requirements compared to the baseline models (both NAFNet and IR-SDE). The test-time computational cost (FLOPs and runtime) is however virtually unaffected."
        },
        {
            "heading": "B.7 ANALYSIS OF PATCH SIZES",
            "text": "Intuitively, large patch sizes always contain more semantic information that might be important for guiding image restoration. We explore this property by training our model with two different patch sizes on the unified image restoration task. As shown in Figure 12, increasing the patch size from 128\u00d7 128 to 256\u00d7 256 improves the training process, which is consistent with our conjecture."
        },
        {
            "heading": "METHOD MAXIM PROMPTIR NAFNET NAFNET + DA-CLIP IR-SDE IR-SDE + DA-CLIP",
            "text": ""
        },
        {
            "heading": "C ADDITION EXPERIMENTAL RESULTS",
            "text": "In this section, we provide more details and additional experimental results."
        },
        {
            "heading": "C.1 DETAILED QUANTITATIVE RESULTS FOR UNIFIED IMAGE RESTORATION",
            "text": "We provide some examples to show the degradation prediction on specific images with different corruptions in Figure 13. In Section 5.2 we have reported the average results and radar figures for different metrics. Here we also provide a more detailed comparison between our method with other approaches for unified image restoration. The results on four metrics are shown in Table 9, Table 10, Table 11, and Table 12. Additionally, we also provide a detailed comparison between NAFNet (Chen et al., 2022) and its variant (with DA-CLIP) in all tables, which illustrates the potential of applying our DACLIP to other general image restoration frameworks."
        },
        {
            "heading": "C.2 ADDITIONAL VISUAL RESULTS",
            "text": "The additional visual results for unified image restoration are shown in Figure 14, Figure 16. Figure 15 shows the comparison with Real-ESRGAN and StableSR on a compressed image and a blurry image. Figure 17 also illustrates some examples of testing our method on real-world images. We basically compare with PromptIR (Potlapalli et al., 2023) and IR-SDE (Luo et al., 2023a) but also add the results generated from AirNet (Li et al., 2022a)."
        },
        {
            "heading": "D LIMITATION ON MIXED DEGRADATIONS",
            "text": "Since the mixed degradation dataset contains a single degradation label for each image, our current model has not been trained to restore multiple degradations in the same scene. For example, the\nraindrop image in Figure 8 contains a shadow area, but our model only removes the raindrop degradation. Also, due to this limitation, this paper can\u2019t well-explore and justify the effectiveness of linguistic components based on mixed degradations.\nIn future work, we are very interested in 1) creating practical models that are able to process realworld degradations and more complex degradations; 2) exploring other pretrained VLM backbones for more robust visual representations/embeddings; 3) exploring more interesting works for the linguistic side (from text encoder) such as instruction-based image restoration."
        }
    ],
    "title": "MULTI-TASK IMAGE RESTORATION",
    "year": 2024
}