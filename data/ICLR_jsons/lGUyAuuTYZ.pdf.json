{
    "abstractText": "Binary Neural networks (BNN) have emerged as an attractive computing paradigm for a wide range of low-power vision tasks. However, state-of-theart (SOTA) BNNs do not yield any sparsity, and induce a significant number of non-binary operations. On the other hand, activation sparsity can be provided by spiking neural networks (SNN), that too have gained significant traction in recent times. Thanks to this sparsity, SNNs when implemented on neuromorphic hardware, have the potential to be significantly more power-efficient compared to traditional artifical neural networks (ANN). However, SNNs incur multiple time steps to achieve close to SOTA accuracy. Ironically, this increases latency and energy\u2014costs that SNNs were proposed to reduce\u2014and presents itself as a major hurdle in realizing SNNs\u2019 theoretical gains in practice. This raises an intriguing question: Can we obtain SNN-like sparsity and BNN-like accuracy and enjoy the energy-efficiency benefits of both? To answer this question, in this paper, we present a training framework for sparse binary activation neural networks (BANN) using a novel variant of the Hoyer regularizer. We estimate the threshold of each BANN layer as the Hoyer extremum of a clipped version of its activation map, where the clipping value is trained using gradient descent with our Hoyer regularizer. This approach shifts the activation values away from the threshold, thereby mitigating the effect of noise that can otherwise degrade the BANN accuracy. Our approach outperforms existing BNNs, SNNs, and adder neural networks (that also avoid energy-expensive multiplication operations similar to BNNs and SNNs) in terms of the accuracy-FLOPs trade-off for complex image recognition tasks. Downstream experiments on object detection further demonstrate the efficacy of our approach. Lastly, we demonstrate the portability of our approach to SNNs with multiple time steps. Codes are publicly available here.",
    "authors": [
        {
            "affiliations": [],
            "name": "Gourav Datta"
        },
        {
            "affiliations": [],
            "name": "Zeyu Liu"
        },
        {
            "affiliations": [],
            "name": "Peter A. Beerel"
        }
    ],
    "id": "SP:c344f7a74884ad89e774981793236fa686e1b78d",
    "references": [
        {
            "authors": [
                "Yoshua Bengio",
                "Nicholas L\u00e9onard",
                "Aaron Courville"
            ],
            "title": "Estimating or propagating gradients through stochastic neurons for conditional computation",
            "venue": "arXiv preprint arXiv:1308.3432,",
            "year": 2013
        },
        {
            "authors": [
                "Joseph Bethge",
                "Christian Bartz",
                "Haojin Yang",
                "Ying Chen",
                "Christoph Meinel"
            ],
            "title": "Meliusnet: An improved network architecture for binary neural networks",
            "venue": "IEEE Winter Conference on Applications of Computer Vision (WACV),",
            "year": 2021
        },
        {
            "authors": [
                "Tong Bu",
                "Jianhao Ding",
                "Zhaofei Yu",
                "Tiejun Huang"
            ],
            "title": "Optimized potential initialization for lowlatency spiking neural networks",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 1987
        },
        {
            "authors": [
                "Tong Bu",
                "Wei Fang",
                "Jianhao Ding",
                "PENGLIN DAI",
                "Zhaofei Yu",
                "Tiejun Huang"
            ],
            "title": "Optimal ANNSNN conversion for high-accuracy and ultra-low-latency spiking neural networks",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Adrian Bulat",
                "Georgios Tzimiropoulos"
            ],
            "title": "Xnor-net++: Improved binary neural networks, 2019",
            "year": 2019
        },
        {
            "authors": [
                "Hanting Chen",
                "Yunhe Wang",
                "Chunjing Xu",
                "Boxin Shi",
                "Chao Xu",
                "Qi Tian",
                "Chang Xu"
            ],
            "title": "Addernet: Do we really need multiplications in deep learning",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2020
        },
        {
            "authors": [
                "Kai Chen",
                "Jiaqi Wang",
                "Jiangmiao Pang",
                "Yuhang Cao",
                "Yu Xiong",
                "Xiaoxiao Li",
                "Shuyang Sun",
                "Wansen Feng",
                "Ziwei Liu",
                "Jiarui Xu",
                "Zheng Zhang",
                "Dazhi Cheng",
                "Chenchen Zhu",
                "Tianheng Cheng",
                "Qijie Zhao",
                "Buyu Li",
                "Xin Lu",
                "Rui Zhu",
                "Yue Wu",
                "Jifeng Dai",
                "Jingdong Wang",
                "Jianping Shi",
                "Wanli Ouyang",
                "Chen Change Loy",
                "Dahua Lin"
            ],
            "title": "MMDetection: Open MMLab detection toolbox and benchmark",
            "venue": "arXiv preprint arXiv:1906.07155,",
            "year": 2019
        },
        {
            "authors": [
                "Tianlong Chen",
                "Zhenyu Zhang",
                "Xu Ouyang",
                "Zechun Liu",
                "Zhiqiang Shen",
                "Zhangyang Wang"
            ],
            "title": "bnn - bn = ?\u201d: Training binary neural networks without batch normalization",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops,",
            "year": 2021
        },
        {
            "authors": [
                "Sayeed Shafayet Chowdhury"
            ],
            "title": "One timestep is all you need: Training spiking neural networks with ultra low latency",
            "venue": "arXiv preprint arXiv:2110.05929,",
            "year": 2021
        },
        {
            "authors": [
                "G. Datta",
                "S. Kundu",
                "Z. Yin",
                "R.T. Lakkireddy",
                "J. Mathai",
                "A.P. Jacob",
                "P.A. Beerel",
                "A.R. Jaiswal"
            ],
            "title": "A processing-in-pixel-in-memory paradigm for resource-constrained tinyml applications",
            "venue": "Scientific Reports,",
            "year": 2022
        },
        {
            "authors": [
                "Gourav Datta",
                "Peter A. Beerel"
            ],
            "title": "Can deep neural networks be converted to ultra low-latency spiking neural networks",
            "venue": "Design, Automation & Test in Europe Conference & Exhibition (DATE),",
            "year": 2022
        },
        {
            "authors": [
                "Gourav Datta",
                "Souvik Kundu",
                "Akhilesh R. Jaiswal",
                "Peter A. Beerel"
            ],
            "title": "ACE-SNN: AlgorithmHardware co-design of energy-efficient & low-latency deep spiking neural networks for 3D image recognition",
            "venue": "Frontiers in Neuroscience,",
            "year": 2022
        },
        {
            "authors": [
                "Gourav Datta",
                "Souvik Kundu",
                "Zihan Yin",
                "Joe Mathai",
                "Zeyu Liu",
                "Zixu Wang",
                "Mulin Tian",
                "Shunlin Lu",
                "Ravi T. Lakkireddy",
                "Andrew Schmidt",
                "Wael Abd-Almageed",
                "Ajey P. Jacob",
                "Akhilesh R. Jaiswal",
                "Peter A. Beerel"
            ],
            "title": "P2M-DeTrack: Processing-in-Pixel-in-Memory for energy-efficient and real-time multi-object detection and tracking",
            "venue": "arXiv preprint arXiv:2205.14285,",
            "year": 2022
        },
        {
            "authors": [
                "Gourav Datta",
                "Zihan Yin",
                "Ajey Jacob",
                "Akhilesh R. Jaiswal",
                "Peter A. Beerel"
            ],
            "title": "Toward efficient hyperspectral image processing inside camera pixels",
            "venue": "arXiv preprint arXiv:2203.05696,",
            "year": 2022
        },
        {
            "authors": [
                "Gourav Datta",
                "Haoqin Deng",
                "Robert Aviles",
                "Zeyu Liu",
                "Peter A. Beerel"
            ],
            "title": "Bridging the gap between spiking neural networks & lstms for latency & energy efficiency",
            "venue": "IEEE/ACM International Symposium on Low Power Electronics and Design (ISLPED),",
            "year": 2023
        },
        {
            "authors": [
                "Gourav Datta",
                "Zeyu Liu",
                "James Diffenderfer",
                "Bhavya Kailkhura",
                "Peter A. Beerel"
            ],
            "title": "When bioinspired computing meets deep learning: Low-latency, accurate, & energy-efficient spiking neural networks from artificial neural networks",
            "venue": "arXiv preprint arXiv:2312.06900,",
            "year": 2023
        },
        {
            "authors": [
                "Gourav Datta",
                "Zeyu Liu",
                "Md Abdullah-Al Kaiser",
                "Souvik Kundu",
                "Joe Mathai",
                "Zihan Yin",
                "Ajey P. Jacob",
                "Akhilesh R. Jaiswal",
                "Peter A. Beerel"
            ],
            "title": "In-sensor & neuromorphic computing are all you need for energy efficient computer vision",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2023
        },
        {
            "authors": [
                "Gourav Datta"
            ],
            "title": "Training energy-efficient deep spiking neural networks with single-spike hybrid input encoding",
            "venue": "In 2021 International Joint Conference on Neural Networks (IJCNN),",
            "year": 2021
        },
        {
            "authors": [
                "J. Deng",
                "W. Dong",
                "R. Socher",
                "L.-J. Li",
                "K. Li",
                "L. Fei-Fei"
            ],
            "title": "ImageNet: A Large-Scale Hierarchical Image Database",
            "venue": "In CVPR09,",
            "year": 2009
        },
        {
            "authors": [
                "Shikuang Deng",
                "Yuhang Li",
                "Shanghang Zhang",
                "Shi Gu"
            ],
            "title": "Temporal efficient training of spiking neural network via gradient re-weighting",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Shikuang Deng",
                "Hao Lin",
                "Yuhang Li",
                "Shi Gu"
            ],
            "title": "Surrogate module learning: Reduce the gradient error accumulation in training spiking neural networks",
            "venue": "Proceedings of the 40th International Conference on Machine Learning,",
            "year": 2023
        },
        {
            "authors": [
                "Shikuang Deng"
            ],
            "title": "Optimal conversion of conventional artificial neural networks to spiking neural networks",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "James Diffenderfer",
                "Bhavya Kailkhura"
            ],
            "title": "Multi-prize lottery ticket hypothesis: Finding accurate binary neural networks by pruning a randomly weighted network",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Ruizhou Ding",
                "Ting-Wu Chin",
                "Zeye Liu",
                "Diana Marculescu"
            ],
            "title": "Regularizing activation distribution for training binarized deep networks",
            "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2019
        },
        {
            "authors": [
                "Chaoteng Duan",
                "Jianhao Ding",
                "Shiyan Chen",
                "Zhaofei Yu",
                "Tiejun Huang"
            ],
            "title": "Temporal effective batch normalization in spiking neural networks",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "M. Everingham",
                "L. Van Gool",
                "C.K.I. Williams",
                "J. Winn",
                "A. Zisserman"
            ],
            "title": "The pascal visual object classes (voc) challenge",
            "venue": "International Journal of Computer Vision,",
            "year": 2010
        },
        {
            "authors": [
                "Sieger Falkena",
                "Hadi Jamali-Rad",
                "Jan van Gemert"
            ],
            "title": "Lab: Learnable activation binarizer for binary neural networks",
            "venue": "In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV),",
            "year": 2023
        },
        {
            "authors": [
                "Wei Fang",
                "Zhaofei Yu",
                "Yanqi Chen",
                "Timothee Masquelier",
                "Tiejun Huang",
                "Yonghong Tian"
            ],
            "title": "Incorporating learnable membrane time constant to enhance learning of spiking neural networks",
            "year": 2007
        },
        {
            "authors": [
                "Wei Fang",
                "Zhaofei Yu",
                "Yanqi Chen",
                "Tiejun Huang",
                "Timoth\u00e9e Masquelier",
                "Yonghong Tian"
            ],
            "title": "Deep residual learning in spiking neural networks",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Martin Lefebvre",
                "Jean-Didier Legat",
                "David Bol"
            ],
            "title": "A 0.086-mm2 12.7-pj/sop 64k-synapse 256-neuron online-learning digital spiking neuromorphic processor in 28-nm cmos",
            "venue": "IEEE Transactions on Biomedical Circuits and Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Xue Geng",
                "Jiawei Hu",
                "Jie Lin",
                "Zhe Wang",
                "Shaohua Li",
                "Min Wu",
                "Mohamed M. Sabry Aly"
            ],
            "title": "MABA-net: Masked additive binary activation network, 2023",
            "venue": "URL https://openreview. net/forum?id=LlWfawcSpf",
            "year": 2023
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification",
            "venue": "arXiv preprint arXiv:1502.01852,",
            "year": 2015
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Patrik O Hoyer"
            ],
            "title": "Non-negative matrix factorization with sparseness constraints",
            "venue": "Journal of machine learning research,",
            "year": 2004
        },
        {
            "authors": [
                "Jie Hu",
                "Ziheng Wu",
                "Vince Tan",
                "Zhilin Lu",
                "Mengze Zeng",
                "Enhua Wu"
            ],
            "title": "Elastic-link for binarized neural networks",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2022
        },
        {
            "authors": [
                "Hyungjun Kim",
                "Jihoon Park",
                "Changhun Lee",
                "Jae-Joon Kim"
            ],
            "title": "Improving accuracy of binary neural networks using unbalanced activation distribution",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2021
        },
        {
            "authors": [
                "Seijoon Kim",
                "Seongsik Park",
                "Byunggook Na",
                "Sungroh Yoon"
            ],
            "title": "Spiking-yolo: Spiking neural network for energy-efficient object detection, 2019",
            "year": 2019
        },
        {
            "authors": [
                "Youngeun Kim",
                "Priyadarshini Panda"
            ],
            "title": "Optimizing deeper spiking neural networks for dynamic vision sensing",
            "venue": "Neural Networks,",
            "year": 2021
        },
        {
            "authors": [
                "Youngeun Kim",
                "Joshua Chough",
                "Priyadarshini Panda"
            ],
            "title": "Beyond classification: Directly training spiking neural networks for semantic segmentation",
            "venue": "Neuromorphic Computing and Engineering,",
            "year": 2022
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "arXiv preprint arXiv:1412.6980,",
            "year": 2014
        },
        {
            "authors": [
                "Alex Krizhevsky"
            ],
            "title": "Learning multiple layers of features from tiny images, 2009. URL https: //www.cs.toronto.edu/ \u0303kriz/learning-features-2009-TR.pdf",
            "year": 2009
        },
        {
            "authors": [
                "Changhun Lee",
                "Hyungjun Kim",
                "Eunhyeok Park",
                "Jae-Joon Kim"
            ],
            "title": "Insta-bnn: Binary neural network with instance-aware",
            "year": 2022
        },
        {
            "authors": [
                "Jeong-Jun Lee",
                "Peng Li"
            ],
            "title": "Reconfigurable dataflow optimization for spatiotemporal spiking neural computation on systolic array accelerators",
            "venue": "IEEE 38th International Conference on Computer Design (ICCD),",
            "year": 2020
        },
        {
            "authors": [
                "Jun Haeng Lee"
            ],
            "title": "Training deep spiking neural networks using backpropagation",
            "venue": "Frontiers in Neuroscience,",
            "year": 2016
        },
        {
            "authors": [
                "Yuhang Li",
                "Yufei Guo",
                "Shanghang Zhang",
                "Shikuang Deng",
                "Yongqing Hai",
                "Shi Gu"
            ],
            "title": "Differentiable spike: Rethinking gradient-descent for training spiking neural networks",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Yuhang Li",
                "Yufei Guo",
                "Shanghang Zhang",
                "Shikuang Deng",
                "Yongqing Hai",
                "Shi Gu"
            ],
            "title": "Differentiable spike: Rethinking gradient-descent for training spiking neural networks",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Yuhang Li",
                "Youngeun Kim",
                "Hyoungseob Park",
                "Tamar Geller",
                "Priyadarshini Panda"
            ],
            "title": "Neuromorphic data augmentation for training spiking neural networks",
            "year": 2002
        },
        {
            "authors": [
                "Mingbao Lin",
                "Rongrong Ji",
                "Zihan Xu",
                "Baochang Zhang",
                "Fei Chao",
                "Chia-Wen Lin",
                "Ling Shao"
            ],
            "title": "Siman: Sign-to-magnitude network binarization",
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
            "year": 2023
        },
        {
            "authors": [
                "Tsung-Yi Lin",
                "Priya Goyal",
                "Ross Girshick",
                "Kaiming He",
                "Piotr Doll\u00e1r"
            ],
            "title": "Focal loss for dense object detection",
            "venue": "arXiv preprint arXiv:1708.02002,",
            "year": 2017
        },
        {
            "authors": [
                "Xiaofan Lin",
                "Cong Zhao",
                "Wei Pan"
            ],
            "title": "Towards accurate binary convolutional neural network",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Zechun Liu",
                "Baoyuan Wu",
                "Wenhan Luo",
                "Xin Yang",
                "Wei Liu",
                "Kwang-Ting Cheng"
            ],
            "title": "Bi-real net: Enhancing the performance of 1-bit cnns with improved representational capability and advanced training algorithm",
            "venue": "In Proceedings of the European Conference on Computer Vision (ECCV), September 2018a",
            "year": 2018
        },
        {
            "authors": [
                "Zechun Liu",
                "Baoyuan Wu",
                "Wenhan Luo",
                "Xin Yang",
                "Wei Liu",
                "Kwang-Ting Cheng"
            ],
            "title": "Bi-real net: Enhancing the performance of 1-bit cnns with improved representational capability and advanced training algorithm",
            "venue": "In Proceedings of the European Conference on Computer Vision (ECCV),",
            "year": 2018
        },
        {
            "authors": [
                "Zechun Liu",
                "Zhiqiang Shen",
                "Marios Savvides",
                "Kwang-Ting Cheng"
            ],
            "title": "Reactnet: Towards precise binary neural network with generalized activation functions",
            "venue": "In European Conference on Computer Vision (ECCV),",
            "year": 2020
        },
        {
            "authors": [
                "Zechun Liu",
                "Zhiqiang Shen",
                "Marios Savvides",
                "Kwang-Ting Cheng"
            ],
            "title": "Reactnet: Towards precise binary neural network with generalized activation functions",
            "venue": "In European conference on computer vision,",
            "year": 2020
        },
        {
            "authors": [
                "Qingyan Meng",
                "Mingqing Xiao",
                "Shen Yan",
                "Yisen Wang",
                "Zhouchen Lin",
                "Zhi-Quan Luo"
            ],
            "title": "Training high-performance low-latency spiking neural networks by differentiation on spike representation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2022
        },
        {
            "authors": [
                "E.O. Neftci",
                "H. Mostafa",
                "F. Zenke"
            ],
            "title": "Surrogate gradient learning in spiking neural networks: Bringing the power of gradient-based optimization to spiking neural networks",
            "venue": "IEEE Signal Processing Magazine,",
            "year": 2019
        },
        {
            "authors": [
                "Fabrizio Ottati",
                "Chang Gao",
                "Qinyu Chen",
                "Giovanni Brignone",
                "Mario R. Casu",
                "Jason K. Eshraghian",
                "Luciano Lavagno"
            ],
            "title": "To spike or not to spike: A digital hardware perspective on deep learning acceleration",
            "venue": "arXiv preprint arXiv:2306.15749,",
            "year": 2023
        },
        {
            "authors": [
                "Priyadarshini Panda",
                "Kaushik Roy"
            ],
            "title": "Unsupervised regenerative learning of hierarchical features in spiking deep networks for object recognition",
            "venue": "arXiv preprint arXiv:1602.01510,",
            "year": 2016
        },
        {
            "authors": [
                "Mohammad Rastegari",
                "Vicente Ordonez",
                "Joseph Redmon",
                "Ali Farhadi"
            ],
            "title": "Xnor-net: Imagenet classification using binary convolutional neural networks",
            "venue": "In European conference on computer vision,",
            "year": 2016
        },
        {
            "authors": [
                "Nitin Rathi"
            ],
            "title": "DIET-SNN: Direct input encoding with leakage and threshold optimization in deep spiking neural networks",
            "venue": "arXiv preprint arXiv:2008.03658,",
            "year": 2020
        },
        {
            "authors": [
                "Nitin Rathi"
            ],
            "title": "Enabling deep spiking neural networks with hybrid conversion and spike timing dependent backpropagation",
            "venue": "arXiv preprint arXiv:2005.01807,",
            "year": 2020
        },
        {
            "authors": [
                "Shaoqing Ren",
                "Kaiming He",
                "Ross Girshick",
                "Jian Sun"
            ],
            "title": "Faster r-cnn: Towards real-time object detection with region proposal networks",
            "venue": "Advances in neural information processing systems,",
            "year": 2015
        },
        {
            "authors": [
                "Charbel Sakr",
                "Jungwook Choi",
                "Zhuo Wang",
                "Kailash Gopalakrishnan",
                "Naresh Shanbhag"
            ],
            "title": "True gradient-based training of deep binary activated neural networks via continuous binarization",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2018
        },
        {
            "authors": [
                "Abhronil Sengupta"
            ],
            "title": "Going deeper in spiking neural networks: VGG and residual architectures",
            "venue": "Frontiers in Neuroscience,",
            "year": 2019
        },
        {
            "authors": [
                "Xulong Shi",
                "Zhi Qi",
                "Jiaxuan Cai",
                "Keqi Fu",
                "Yaru Zhao",
                "Zan Li",
                "Xuanyu Liu",
                "Hao Liu"
            ],
            "title": "Repbnn: towards a precise binary neural network with enhanced feature map via repeating",
            "venue": "arXiv preprint arXiv:2207.09049,",
            "year": 2022
        },
        {
            "authors": [
                "Karen Simonyan",
                "Andrew Zisserman"
            ],
            "title": "Very deep convolutional networks for large-scale image recognition",
            "venue": "arXiv preprint arXiv:1409.1556,",
            "year": 2014
        },
        {
            "authors": [
                "Dewei Wang",
                "Pavan Kumar Chundi",
                "Sung Justin Kim",
                "Minhao Yang",
                "Joao Pedro Cerqueira",
                "Joonsung Kang",
                "Seungchul Jung",
                "Sangjoon Kim",
                "Mingoo Seok"
            ],
            "title": "Always-on, sub-300-nw, eventdriven spiking neural network based on spike-driven clock-generation and clock- and powergating for an ultra-low-power intelligent device",
            "venue": "IEEE Asian Solid-State Circuits Conference (A-SSCC),",
            "year": 2020
        },
        {
            "authors": [
                "Peisong Wang",
                "Xiangyu He",
                "Gang Li",
                "Tianli Zhao",
                "Jian Cheng"
            ],
            "title": "Sparsity-inducing binarized neural networks",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Yikai Wang",
                "Wenbing Huang",
                "Yinpeng Dong",
                "Fuchun Sun",
                "Anbang Yao"
            ],
            "title": "Compacting binary neural networks by sparse kernel selection",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2023
        },
        {
            "authors": [
                "Ziwei Wang",
                "Ziyi Wu",
                "Jiwen Lu",
                "Jie Zhou"
            ],
            "title": "Bidet: An efficient binarized object detector",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2020
        },
        {
            "authors": [
                "Mingqing Xiao",
                "Qingyan Meng",
                "Zongpeng Zhang",
                "Di He",
                "Zhouchen Lin"
            ],
            "title": "Online training through time for spiking neural networks",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Yixing Xu",
                "Kai Han",
                "Chang Xu",
                "Yehui Tang",
                "Chunjing XU",
                "Yunhe Wang"
            ],
            "title": "Learning frequency domain approximation for binary neural networks",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Zihan Xu",
                "Mingbao Lin",
                "Jianzhuang Liu",
                "Jie Chen",
                "Ling Shao",
                "Yue Gao",
                "Yonghong Tian",
                "Rongrong Ji"
            ],
            "title": "Recu: Reviving the dead weights in binary neural networks",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV),",
            "year": 2021
        },
        {
            "authors": [
                "Huanrui Yang",
                "Wei Wen",
                "Hai Li"
            ],
            "title": "Deephoyer: Learning sparser neural network with differentiable scale-invariant sparsity measures",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Ruokai Yin",
                "Abhishek Moitra",
                "Abhiroop Bhattacharjee",
                "Youngeun Kim",
                "Priyadarshini Panda"
            ],
            "title": "Sata: Sparsity-aware training accelerator for spiking neural networks",
            "venue": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Wenrui Zhang",
                "Peng Li"
            ],
            "title": "Temporal spike sequence learning via backpropagation for deep spiking neural networks",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Yichi Zhang",
                "Zhiru Zhang",
                "Lukasz Lew"
            ],
            "title": "Pokebnn: A binary pursuit of lightweight accuracy",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2022
        },
        {
            "authors": [
                "Hanle Zheng",
                "Yujie Wu",
                "Lei Deng",
                "Yifan Hu",
                "Guoqi Li"
            ],
            "title": "Going deeper with directly-trained larger spiking neural networks",
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Pengju Ren Zhijun Tu",
                "Xinghao Chen",
                "Yunhe Wang"
            ],
            "title": "Adabin: Improving binary neural networks with adaptive binary sets",
            "venue": "In European Conference on Computer Vision (ECCV),",
            "year": 2022
        },
        {
            "authors": [
                "Pengju Ren Zhijun Tu",
                "Xinghao Chen",
                "Yunhe Wang"
            ],
            "title": "Adabin: Improving binary neural networks with adaptive binary sets",
            "venue": "In European Conference on Computer Vision (ECCV),",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Due to its low memory footprint and use of cheaper pop-count operations instead of energyexpensive multiply-and-accumulates (MAC), BNNs have emerged as a promising low-power alternative to compute- and memory-expensive deep neural networks (DNN) (Rastegari et al., 2016; Liu et al., 2020a; 2018a). Recent works have proposed novel network architectures (Bethge et al., 2021; Zhang et al., 2022; Shi et al., 2022; Hu et al., 2022) and training algorithms (Zhijun Tu & Wang, 2022a; Xu et al., 2021b; Rastegari et al., 2016; Bulat & Tzimiropoulos, 2019; Geng et al., 2023; Chen et al., 2021; Xu et al., 2021a; Kim et al., 2021; Wang et al., 2023; Lee et al., 2022) to approximate the full-precision representation of the weights and activations with 1-bit bi-polar values without significant drop in accuracy.\nHowever, most of these efforts induce a significant number of non-binary operations which degrade the computational efficiency. For example, ReactNet-based BNNs (Liu et al., 2020a; Zhijun Tu & Wang, 2022b) incur custom non-linear functions, including RPReLU that are significantly more complex compared to threshold or ReLU operations, duplicated basic blocks that significantly increase the total number of floating point operations (FLOPs) and parameter count. Moreover, all\nthese BNNs employ the sign quantization function to generate the bi-polar weights and activation values. However, bi-polar activations do not yield any sparsity and thus can not benefit any computational efficiency from hardware that can leverage sparsity. Moreover, as shown in (Wang et al., 2020b; Lin et al., 2023; Falkena et al., 2023), it is also unclear whether sign is the optimal binarization function.\nIn contrast, SNNs can provide significant sparsity, even up to 80% for ultra-low number of time steps (Chowdhury et al., 2021). However, this comes at the cost of significant accuracy drop (Datta et al., 2021) without any specialized neuron model (Datta et al., 2023b). In an attempt to push the frontier of sparsity-accuracy trade-off, we propose a class of uni-polar binary activation neural networks (BANNs) that can enjoy sparsity similar to SNNs while getting rid of the temporal dimension, and achieve BNN-like accuracies. Our BANNs are more compute-efficient than existing bi-polar BNNs. This efficiency stems from the fact that we can skip the memory access of the weight and the eventual accumulate/bit-count operation when the corresponding activation value is 0.\nOur Contributions. Our training framework for BANNs is based on a novel application of the Hoyer regularizer and a novel Hoyer thresholding layer. More specifically, our threshold is traininginput-dependent and is set to be equal to the Hoyer extremum of a clipped version of the activation tensor, where the clipping value is trained using gradient descent with our Hoyer regularizer. In this way, compared to traditional uni-polar BNNs and SNNs with ultra-low number of time steps, our threshold increases the rate of weight updates and our Hoyer regularizer shifts the activation distribution away from this threshold, improving convergence. We consistently surpass the accuracies obtained by SOTA uni-polar BNNs (Wang et al., 2020b; Sakr et al., 2018) on diverse image recognition datasets with different convolutional architectures. Compared to SNNs, and adder neural network (AddNN) models that are also compute-efficient, our BANN models yield higher test accuracy with a \u223c5.5\u00d7 reduction in floating point operations (FLOPs), thanks to the extreme sparsity enabled by our training framework. Incorporating the training framework of BANNs can also improve the test accuracy in advanced bi-polar BNNs. Downstream tasks on object detection also demonstrate that our approach surpasses the test mAP of existing BNNs and SNNs. Our approach can also be incorporated to SNNs with multiple time steps, thereby leading to small but significant accuracy increase at the cost of significant increase in memory and compute cost. Thus, our proposed approach acts as a continuum between one time-step sparse BNNs and low time-step SNNs, and can help bridge both the BNN and SNN communities for low-power vision tasks."
        },
        {
            "heading": "2 PRELIMINARIES",
            "text": ""
        },
        {
            "heading": "2.1 HOYER REGULARIZER",
            "text": "Based on the interplay between \u21131 and \u21132 norms, a new measure of sparsity was first introduced in (Hoyer, 2004), based on which, (Yang et al., 2020) proposed a new regularizer, termed the Hoyer regularizer for the trainable weights that was incorporated into the loss term to train DNNs. We adopt the same form of Hoyer regularizer for the activation to train our BANN models as H(ul) =(\n\u2225ul\u22251 \u2225ul\u22252\n)2 (Kurtz et al., 2020). Here, \u2225ul\u2225i represents the \u2113i norm of the activation tensor ul, and the\nsuperscript t for the time step is omitted for simplicity. Compared to the \u21131 and \u21132 regularizers, the Hoyer regularizer has scale-invariance (similar to the \u21130 regularizer). It is also differentiable almost everywhere (see equation 1) where |ul| represents the element-wise absolute of the tensor ul.\n\u2202H(ul) \u2202ul = 2\u2225ul\u22251 \u2225ul\u222522\n( sign(ul) \u00b7 \u2225ul\u22252 \u2212\n\u2225ul\u22251 \u2225ul\u22252 ul\n) (1)\nLetting the gradient \u2202H(ul)\u2202ul =0 and making all the ul positive, the value of the Hoyer extremum becomes E(ul)= \u2225ul\u222522 \u2225ul\u22251\n. This extremum is the minimum, because the second derivative is greater than zero for any value of the output element. Training with the Hoyer regularizer can effectively help push the activation values that are larger than the extremum (ul>E(ul)) even larger and those that are smaller than the extremum (ul<E(ul)) even smaller."
        },
        {
            "heading": "2.2 BINARY NEURAL NETWORKS",
            "text": "Given a input ol\u22121 \u2208 Rc\u00d7h\u00d7w and weight w \u2208 Rn\u00d7c\u00d7k\u00d7k of a layer l, we can get the output ul \u2208 Rc\u00d7h\n\u2032\u00d7w\u2032 by convolution operation as ul = ol\u22121 \u2217 wl. To accelerate the inference process, bi-polar BNNs (Liu et al., 2020a; Zhijun Tu & Wang, 2022b) partition the input and weight into two clusters, -1 and +1 with sign function as Eq. 2.\nsign(x) = { 1, if x \u2265 0; \u22121, otherwise (2)\nTo yield sparsity similar to ReLU activation, uni-polar BNNs (Wang et al., 2020b; Sakr et al., 2018) parition the weights to -1 and +1, while the activations to 0 and 1 as illustrated below in Eq. 3.\nzl = ul vthl\nol = { 1, if zl \u2265 1; 0, otherwise\n(3)\nwhere zl denotes the normalized activation output, and vthl denotes a trainable threshold parameter. While our BANNs primarily focus on sparse uni-polar activations with full-precision weights (activations are significantly more difficult to binarize compared to weights (Lin et al., 2017b; Ding et al., 2019)), we also demonstrate the efficacy of BANNs with binary weights (-1 and +1) for a fair comparison with BNNs."
        },
        {
            "heading": "3 PROPOSED TRAINING FRAMEWORK",
            "text": "Our approach is inspired by the fact that Hoyer regularizers can shift the pre-activation distributions away from the Hoyer extremum in a DNN (Yang et al., 2020). Our principal insight is that setting our activation threshold to this extremum shifts the distribution away from the threshold value, reducing noise and improving convergence. To obtain our BANNs, we present a novel Hoyer threshold layer that sets the threshold based upon a Hoyer regularized training process, as described below."
        },
        {
            "heading": "3.1 HOYER THRESHOLD LAYER",
            "text": "As illustrated above in Eq. 3, a binary neuron with a unit step activation function is difficult to optimize with straight through estimator (STE) based approaches (Bengio et al., 2013), which either approximates the binary neuron functionality with a continuous differentiable model or surrogate gradient approaches used in SNNs (Panda & Roy, 2016; Lee et al., 2016). This is because the number of ones in the activations becomes too low to adjust the weights sufficiently using\ngradient descent. If a pre-synaptic neuron does not output 1, the synaptic weight connected to it cannot be updated because its gradient from neuron i is calculated as guj\u00d7oi, where guj is the gradient of the activation uj and oi is the output of neuron i.\nTherefore, it is crucial to reduce the value of the threshold to generate enough 1s for better network convergence. However, a sufficiently low value of threshold can generate a one for every neuron, but that would yield random outputs in the final classifier layer. Hence, it is challenging to yield the optimal threshold for each layer. Previous works (Wang et al., 2020b) show that this problem can be partially mitigating by training the threshold term vthl using gradient descent. However, that still leads to a large accuracy gap with the SOTA bi-polar BNNs, especially for ImageNet-level tasks.\nIn contrast, we propose to dynamically down-scale the threshold (see Fig. 1(a)) based on the activation using our proposed form of the Hoyer regularizer. In particular, we clip the activation\ncorresponding to each convolutional layer to the trainable threshold vthl obtained from the gradient descent with our Hoyer loss, as detailed later in Eq. 12. Unlike existing approaches (Rathi et al., 2020a; Chowdhury et al., 2021) that require vthl to be initialized from a pre-trained full-precision model, our approach can be used to train BANNs from scratch with a Kaiming uniform initialization (He et al., 2015) for both the weights and thresholds. In particular, the normalized down-scaled threshold value, with which we compare the normalized activation zl, for each layer is computed as the Hoyer extremum of the clipped activation tensor as shown in Fig. 1(a) and below.\nzclipl =  1, if zl>1 zl, if 0\u2264zl\u22641 0, if zl < 0 ol=hs(zl)= { 1, if zl\u2265E(zclipl ) 0, otherwise\n(4)\nNote that our normalized threshold E(zclipl ) is less than the normalized threshold whose value is 1 for any output (proof in supplementary materials). Hence, our actual threshold value E(zclipl )\u00d7vthl is indeed less than the trainable threshold vthl used in earlier works (Datta & Beerel, 2022; Rathi et al., 2020a). We also observe that the Hoyer extremum in each layer changes only slightly during the later stages of training, which indicates that it is most likely an inherent attribute of the dataset and model architecture. Hence, to estimate the threshold during inference, we calculate the exponential average of the Hoyer extremums during training, similar to batch normalization (BN) layers, and use the same during inference."
        },
        {
            "heading": "3.2 HOYER REGULARIZED TRAINING",
            "text": "The loss function (Ltotal) of our proposed approach is shown below in Eq. 5.\nLtotal = LCE + \u03bbHLH = LCE + \u03bbH L\u22121\u2211 l=1 H(zclipl ) (5)\nwhere LCE denotes the cross-entropy loss calculated on the softmax output of the last layer L, and LH represents the Hoyer regularizer calculated on the input of our Hoyer threshold layer after dividing the threshold term vthl and clipping. The weight update for the layer L\u22121 is computed as\n\u2206WL\u22121= \u2202LCE \u2202wL\u22121 +\u03bbH \u2202LH \u2202wL\u22121 = \u2202LCE \u2202oL\u22121 \u2202oL\u22121 \u2202uL\u22121 \u2202uL\u22121 \u2202wL\u22121 +\u03bbH \u2202LH \u2202uL\u22121 \u2202uL\u22121 \u2202wL\u22121\n= ( \u2202LCE \u2202oL\u22121 \u2202oL\u22121 \u2202uL\u22121 +\u03bbH \u2202H(zclipL\u22121) \u2202uL\u22121 ) oL\u22122\n(6)\n\u2202LCE \u2202oL\u22121 = \u2202LCE \u2202uL \u2202uL \u2202oL\u22121 = (s\u2212 y)wL (7)\nwhere s denotes the output softmax tensor, i.e., si = e uiL\u2211N\nk=1 u k L\nwhere uiL and u k L denote the i th and\nkth elements of the activation tensor of the last layer L, and N denotes the number of classes. Note that y denotes the one-hot encoded tensor of the true label, and \u2202H(uL)\u2202uL is computed using Eq. 1. The last layer does not have any threshold and yields full-precision outputs.\nThe weight and threshold update computations for the hidden layers are in Appendix A.1.\nNote that all the derivatives to update the trainable parameters can be computed by Pytorch autograd, except the derivative \u2202ol\u2202zl , whose gradient is zero almost everywhere and undefined at zl=0. We extend the existing idea of surrogate gradient descent (SGD) (Neftci et al., 2019) in SNNs to compute this derivative for our BANNs with Hoyer threshold layers, as illustrated in Fig. 1(b) and mathematically defined as follows.\n\u2202ol \u2202zl = { scale\u00d71 if 0 < zl < 2 0 otherwise\n(8)\nwhere scale denotes a hyperparameter that controls the dampening of the gradient. Note that ol jumps from 0 to 1 at zl=1. We assume the surrogate gradient is non-zero spanning a width of 1 in both directions around zl=1."
        },
        {
            "heading": "4 EXPERIMENTAL RESULTS",
            "text": "Datasets & Models: Similar to existing BNN and SNN works (Rathi et al., 2020b;a; Liu et al., 2020a), we perform object recognition experiments on CIFAR10 (Krizhevsky, 2009) and ImageNet (Deng et al., 2009) dataset using VGG16 (Simonyan & Zisserman, 2014) and several variants of ResNet (He et al., 2016) architectures. For object detection, we use the MMDetection framework (Chen et al., 2019) with PASCAL VOC2007 and VOC2014 (Everingham et al., 2010) as training dataset, and benchmark our BANN models and the baselines on the VOC2007 test dataset. We use the Faster R-CNN (Ren et al., 2015) and RetinaNet (Lin et al., 2017a) framework, and substitute the original backbone with our BANN models pretrained on ImageNet. Object Recognition Results: For training the recognition models, we use the Adam (Kingma & Ba, 2014) optimizer for VGG16, and use SGD optimizer for ResNet models. As shown in Table 1 with FP weights, we obtain the SOTA accuracy of 93.44% on CIFAR10 with VGG16; the accuracy of our ResNet-based BANNs on ImageNet also surpasses the existing works. Note that our ResNet models are based on the spike-element-wise (SEW) architecture (Fang et al., 2020) as detailed in Appendix A.2. On ImageNet, we obtain a 68.00% top-1 accuracy with VGG16 which is only \u223c2% lower compared to the iso-architecture full-precision counterpart. All our BANN models yield a sparsity of \u223c75% or higher on both CIFAR10 and ImageNet, which is significantly higher compared to existing SNNs/BNNs as shown in Fig. 2. Accuracy Comparison with SNNs: We compare our results with various SOTA ultra low-latency SNNs for image recognition tasks in Table 2. Our BANNs yield comparable or better test accuracy compared to all the existing works on both CIFAR10 and ImageNet, with significantly lower inference latency (due to the existence of multiple time steps in SOTA SNNs). The only exception for the latency reduction is the one-time-step SNN proposed in (Chowdhury et al., 2021), however, it increases the training time significantly as illustrated later in Fig. 3. Energy Efficiency Comparison with SNNs and full-precision DNNs: We compare the energyefficiency of our BANNs with full-precision DNNs and existing multi-time-step SNNs in Fig. 2. The compute-efficiency of BANNs stems from two factors: 1) sparsity, that reduces the number of synaptic operations in convolutional and linear layers compared to full-precision DNNs according to SNNflopsl = Sl \u00d7 DNN flops l (Chowdhury et al., 2021), where Sl denotes the average number of spikes per neuron per inference over all timesteps in layer l. Note that the sparsity induces a small\noverhead of checking whether the 1-bit activation is zero, which consumes 0.05pJ in 28nm Kintex-7 FPGA platform according to our post place-and-route simulations. 2) Use of only AC (1.8pJ) operations that consume 7.4\u00d7 lower compared to each MAC (13.32pJ) operation in our FPGA setup for floating point (FP) representation. Note that the binary activations can replace the FP multiplications with logical operations, i.e., conditional assignment to 0 with a bank of AND gates. These replacements may be realized using existing hardware depending on the compiler and the details of their data paths. Building a custom accelerator that can efficiently implement these reduced operations is also possible (Wang et al., 2020a; Frenkel et al., 2019; Lee & Li, 2020). Additionally, our BANNs also enjoy superior memory-efficiency compared to existing multi-time-step SNNs since the latter requires the membrane potentials and weights to be fetched from and read to the on-/offchip memory for each time step. Our BANNs can avoid these repetitive read/write operations as it does involve any state and lead to a T\u00d7 reduction in the number of memory accesses compared to a T-time-step SNN model. Compared to traditional bi-polar BNNs, our BANNs can also reduce the number of memory accesses with the support of zero gating logic leveraging the high activation sparsity. This can be achieved by skipping the reading of the weights when the activation associated to it is zero. However, the exact savings in memory energy will depend on the weight reuse scheme and the underlying hardware. We have provided the detailed energy model of traditional BNNs, multi-time-step SNNs, BANNs, and full-precision DNNs, considering the sparsity overhead and memory accesses without any weight reuse scheme, in Appendix A.9. We compare the layer-wise spiking activities Sl for time steps ranging from 5 to 1 in Fig. 2(a-b) that are computed based on the\nreplicated implementation of an existing low-latency SNN (Rathi et al., 2020a), against our work (where the spiking activity denotes the number of 1s) that represents a time step of 1. Note, the spike rates decrease significantly with time step reduction from 5 to 1, leading to considerably lower FLOPs in our one-time-step SNNs. These lower FLOPs, coupled with the 7.4\u00d7 reduction for AC operations and the sparsity overhead, lead to a 22.7\u00d7 and 31.9\u00d7 reduction in compute energy on CIFAR10 and ImageNet respectively with VGG16. Considering the memory accesses as provided in our energy models in A.9, the total energy reduces by 70.28\u00d7 and 81.13\u00d7. Comparison with different training strategies: Based on existing BNN and SNN literature, we hypothesize that two training strategies that can be effectively used to train BANNs, other than our proposed approach. Pre-trained DNN, followed by BANN fine-tuning. Similar to the hybrid training proposed in (Rathi et al., 2020b), we pre-train a full-precision DNN model, and copy its weights to the BANN model. Initialized with these weights, we train our BANN with normal cross-entropy loss. Iteratively convert ReLU neurons to binary neurons. First, we train a DNN model which uses the ReLU activation with threshold, then we iteratively reduce the number of the ReLU neurons whose output activations are multi-bit. Specifically, we first force the activation values in the top N percentile to output 1, and those in bottom N percentile percent to output 0, and gradually increase N until the accuracy drops beyond a certain threshold or all neuron outputs are either 1 or 0. Proposed training from scratch. With our proposed Hoyer threshold layer and Hoyer regularized training, we train a BANN model from scratch. Our results with these training strategies are shown in Table 3, which indicates that it is difficult for training strategies that involve pre-training and finetuning to approach the accuracy of full-precision models with BANNs. One possible reason for this might be the difference in the distribution of the pre-activation values between the DNN and BANN models (Datta & Beerel, 2022). Our Hoyer threshold layer and our Hoyer regularizer, can help train a BANN model with SOTA accuracy from scratch. Effect on Quantization: In order to further improve the energy efficiency of our BANNs, we perform quantization-aware training of the weights in our models to 1\u22126 bits for both CIFAR10 and ImageNet. This transforms the FP ACs to 1\u22126 bit ACs, thereby leading to a 4.8\u221213.8 reduction in compute energy as obtained from our FPGA simulations. The reduced weight precision also reduces the memory access cost by \u223c5\u221232\u00d7 with efficient bit-packing (bit, 2021) for 1\u22126 bits according to our FPGA simulations. Note that we only quantize the convolutional layers, as quantizing the linear layers lead to a noticeable drop in accuracy. As shown in Table 4, when quantized to 6 bits, our VGG-based BANN incur a negligible accuracy drop of only 0.02% on CIFAR10, while reducing the total energy by 3.3\u00d7 (energy computed from the models developed in A.9). Even with 1-bit quantization, our model can yield an accuracy of 92.80% without any special modification, while still yielding a sparsity of \u223c78%. Comparison with AddNNs & BNNs: We compare the accuracy and energy of our BANN\nmodels with recently proposed AddNN models (Chen et al., 2020) that also removes multiplications for increased energy-efficiency in Table 5. With the VGG16 architecture, on CIFAR10, we obtain 0.6% lower accuracy, while on ImageNet, we obtain 1.0% higher accuracy. Moreover, unlike our BANNs and SNNs, AddNNs do not yield any sparsity, and consume \u223c5.1\u00d7 more compute energy (with the sparsity overhead incorporated) compared to our BANNs on average across both CIFAR10 and ImageNet (see Table 5). We also compare our BANN models with SOTA BNNs in Table 7 that replaces the costly MAC operations with cheaper pop-count counterparts, thanks to the binary weights and activations.\nOur BANNs with 2-bit quantized weights consume 2.6\u00d7 lower energy compared to the bi-polar BNNs (see (Diffenderfer & Kailkhura, 2021) in Table 5). This is due to the improved trade-off between the high sparsity activity (\u223c78% as shown in Table 1)\nprovided by our BANN models, and less energy due to XOR operations and 1-bit memory access compared to weight-quantized ACs and 2-bit memory access. On the other hand, our BANNs with 1-bit weights i.e., our sparse uni-polar BNNs consume similar energy compared to existing unipolar BNNs (see (Sakr et al., 2018; Wang et al., 2020b) in Table 5) while yielding 2.6% higher accuracy (92.80% vs 90.2%) on CIFAR10 and 4.9% higher accuracy (64.62% vs 59.7%) on ImageNet at iso-architecture. Lastly, it is worth mentioning that though our sparse BNNs can surpass test accuracies obtained by a few existing BNNs, it fails to yield accuracies similar to the SOTA bi-polar BNNs (Zhang et al., 2022; Bethge et al., 2021; Zhijun Tu & Wang, 2022a; Xu et al., 2021b). This is due to their higher expressivity (further explained in A.11) and network modifications (e.g. ReactNet with PReLU compared to our simple VGG and ResNets) that increases the FLOPs by more than 2\u00d7. However, applying our proposed training technique on these BNNs results in\n0.2\u22121.3% increase in test accuracy as shown in Table 6. This is done using the same way as using the Hoyer extremum of the normalized and clipped activation as the threshold, where the clipping is done at -1 and +1 (instead of 0 and +1 for our BANNs). This demonstrate the efficacy of our proposed method. Extension to SNNs: We extend our proposed approach to multi-time-step SNN models by adopting the standard LIF model (Sengupta et al., 2019) for the neurons but computing the threshold based on the Hoyer extremum defined in Eq. 3 and unrolling the gradients derived in Section 3.2 using traditional backpropagation through time (Rathi et al., 2020a). As shown in Table 7, as time step increases from 1 to 6, the accuracy of our model also increases from 93.44% to 94.14%, which validates the effectiveness of our method. However, this accuracy increases comes at the cost of a significant increase in spiking activity (see Table 7 where the spiking activity is computed across the total number of time steps similar to (Sengupta et al., 2019)), increasing the compute energy. The memory cost also increases due to the repetitive potential and weight accesses across time steps."
        },
        {
            "heading": "5 RELATIONSHIP BETWEEN SNN, BNN, AND BANN",
            "text": "Our BANNs are identical to uni-polar sparse BNNs (Sakr et al., 2018; Wang et al., 2020b) when the weight precision is quantized down to 1-bit. Our BANNs are also identical to one-time-step SNNs, and can be readily extended to traditional multi-time-step SNNs with the incorporation of the membrane potential, using the traditional LIF model for the neurons where the membrane potential integrates the weight modulated input spikes and leaks over time. We use the soft reset mechanism that reduces the membrane potential by the threshold value when an output spike generated (Datta et al., 2023a). It has been shown that soft reset minimizes the information loss by allowing the spiking neuron to carry forward the surplus potential above the firing threshold to the subsequent time step (). We use our proposed combination of Hoyer regularized training and Hoyer spike layer to train the per layer threshold, while we train the weights and leak term using SGL. The dynamics of our LIF model for our multi-time-step SNNs are shown below.\nUmpi (t)=\u03bbUi(t\u22121)+ \u2211 jWijSj(t), Si(t)=1 if U mp i (t)>V th else 0, Ui(t)=U mp i (t)\u2212Si(t)V th Here Ui(t) denotes the membrane potential of the ith neuron at timer step t, Si(t) denotes the binary output of the ith neuron at time step t, V th denotes the threshold, \u03bb denotes the leak, and Wij denotes the weight connecting the pre-synaptic neuron j and the neuron i.\nThe sparse activation in BANNs bears resemblance to SNNs while the difference is that there is no temporal dimension. On the other hand, the 1-bit sparse activation in BANNs bears resemblance to BNNs, in particular uni-polar BNNs, while the difference is that the weight precision is multi-bit. Thus, BANNs can bridge the gap between BNNs and SNNs by bringing the activation sparsity of SNNs to BNNs, and removing the temporal dimension of SNNs for equivalence with BNNs."
        },
        {
            "heading": "6 CONCLUSIONS",
            "text": "In this work, we propose BANNs trained using a novel variant of Hoyer Regularizer and a Hoyer threshold layer that jointly optimizes the distribution of the BANN pre-activations and the placement of the BANN threshold in order to improve the accuracy-FLOPs trade-off compared to existing networks. Our Hoyer regularizer introduces a loss term that penalizes the normalized and clipped activation values around their Hoyer extremum (which depends on the SGD trained threshold). Our proposed Hoyer threshold layer takes advantage of this distribution shift by setting the IF threshold to the Hoyer extremum. Our BANN models surpass test accuracies obtained by existing SNN, unipolar BNN, and AddNN models with similar or better energy-efficiency. Our training framework can also improve the accuracy of a number of advanced bi-polar BNNs. Our SNNs can further improve the inference energy and latency efficiency with recently proposed in-sensor computing systems (Datta et al., 2022d;a;c) where the bandwidth between the sensor that implements the first few CNN layers and the back-end processing unit that implements the remaining layers can be significantly reduced, thanks to the binary activations for only one time step (Datta et al., 2023c)."
        },
        {
            "heading": "7 ACKNOWLEDGEMENT",
            "text": "This work is supported by a gift funding from Intel Labs."
        },
        {
            "heading": "A APPENDIX",
            "text": ""
        },
        {
            "heading": "A.1 THRESHOLD & WEIGHT UPDATES DUE TO HOYER LOSS",
            "text": "For a hidden layer l, the weight update is computed as\n\u2206Wl= \u2202LCE \u2202wl +\u03bbH \u2202LH \u2202wl = \u2202LCE \u2202ol \u2202ol \u2202zl \u2202zl \u2202ul \u2202ul \u2202wl\n+\u03bbH \u2202LH \u2202ul \u2202ul \u2202wl = \u2202LCE \u2202ol \u2202ol \u2202zl ol\u22121 vthl +\u03bbH \u2202LH \u2202ul ol\u22121\n(9)\nwhere \u2202LH\u2202ul can be computed as\n\u2202LH \u2202ul = \u2202LH \u2202ul+1 \u2202ul+1 \u2202ol \u2202ol \u2202zl \u2202zl \u2202ul\n+ \u2202H(zclipl )\n\u2202ul\n= \u2202LH \u2202ul+1 wl+1 \u2202ol \u2202zl 1 vthl +\n\u2202H(zclipl )\n\u2202zclipl\n\u2202zclipl \u2202zl 1 vthl\n(10)\nThis is because LH is the sum of the Hoyer regularizer of the first l\u22121 layers, the lth layer, and the layers after the lth layer. While computing \u2202LH\u2202ul , the gradient of the 1\nst part is 0, and the gradient of the 2nd (3rd) part is captured in the second (first) term in Eq. 8. Note that \u2202LH\u2202ul+1 is the gradient backpropagated from the (l + 1)th layer, that is iteratively computed from the last layer L (see Eqs. 7 and 11). Also, note that for any hidden layer l, there are two gradients that contribute to the Hoyer loss with respect to the activation ul; one is from the subsequent layer (l+1) and the other is directly from its Hoyer regularizer. Similarly, \u2202LCE\u2202ol is computed iteratively, starting from the penultimate layer (L\u22121) defined in Eq. 7, as follows.\n\u2202LCE \u2202ol = \u2202LCE \u2202ol+1 \u2202ol+1 \u2202zl+1 \u2202zl+1 \u2202ul+1 \u2202ul+1 \u2202ol = \u2202LCE \u2202ol+1 \u2202ol+1 \u2202zl+1 wl+1 vthl\n(11)\nFinally, the threshold update for the hidden layer l is computed as\n\u2206vthl = \u2202LCE \u2202vthl +\u03bbH \u2202LH \u2202vthl = \u2202LCE \u2202ol \u2202ol \u2202zl \u2202zl \u2202vthl +\u03bbH \u2202LH \u2202vthl\n= \u2202LCE \u2202ol \u2202ol \u2202zl \u2212ul (vthl ) 2 +\u03bbH \u2202LH \u2202ul+1 \u2202ul+1 \u2202vthl\n(12)\n\u2202ul+1 \u2202vthl = \u2202ul+1 \u2202ol \u00b7 \u2202ol \u2202vthl = wl+1 \u00b7 \u2202ol \u2202zl \u00b7 \u2212ul (vthl ) 2 (13)\nNote that we use this vthl , which is updated in each iteration, to estimate the threshold in our BANN model using Eq. 4."
        },
        {
            "heading": "A.2 NETWORK ARCHITECTURE",
            "text": "We adopt a series of network architectural tricks for our BANNs (Datta & Beerel, 2022; Chowdhury et al., 2021; Rathi et al., 2020a). As shown in Fig. 3(a), for the VGG variant, we use the max pooling layer immediately after the convolutional layer that is common in many BNN architectures (Rastegari et al., 2016), and introduce the BN layer after max pooling. Similar to recently developed multi-time-step SNN models (Zheng et al., 2021; Li et al., 2021a; Deng et al., 2022; Meng et al., 2022), we observe that BN helps increase the test accuracy with one time step. In contrast, for the ResNet variants, inspired by (Liu et al., 2018b), we observe models with shortcuts that bypass every block can also further improve the performance of the SNN. We also observe that the sequence of BN layer, Hoyer spike layer, and convolution layer outperforms the original bottleneck in ResNet. More details are shown in Fig. 3(b)."
        },
        {
            "heading": "A.3 ABLATION STUDIES",
            "text": "We conduct ablation studies to analyze the contribution of each technique in our proposed approach. For fairness, we train all the ablated models on CIFAR10 dataset for 400 epochs, and use Adam as the optimizer, with 0.0001 as the initial learning rate. Our results are shown in Table 8, where the model without Hoyer threshold layer indicates that we set the threshold as vthl similar to existing works (Datta & Beerel, 2022; Rathi et al., 2020a) rather than our proposed Hoyer extremum.\nWith VGG16, our Hoyer regularizer approach leads to a 2.03% increase in accuracy. Together, with our Hoyer threshold layer, the accuracy improves by 2.68% to 93.13% while also yielding a 2.09% reduction in sparsity.\nWe observe a similar trend for our network modifications and Hoyer threshold layer with ResNet18. However, Hoyer regularizer in this case also increases the sparsity from 72.38% to 77.22% to 79.50%, while also negligibly reducing the accuracy. In summary, the combination of our Hoyer regularizer and Hoyer threshold layer yield the SOTA BNN/SNN performance."
        },
        {
            "heading": "A.4 OBJECT DETECTION RESULTS",
            "text": "Object Detection Results: For object detection on VOC2007, we compare the performance obtained by our BANN models with full-precision DNNs and BNNs in Table 9. For two-stage architectures, such as Faster R-CNN, the mAP of our BANN models surpass the existing BNNs by >0.6%. For one-stage architectures, such as RetinaNet (chosen because of its SOTA performance), our BANN models with a ResNet50 backbone yields a mAP of 70.5% (highest among existing BNN, SNN, AddNNs). Note that our BANN-based VGG and ResNet-based backbones lead to a significant drop in mAP with the YOLO framework that is more compatible with the DarkNet backbone (even existing DarkNet-based SNNs lead to very low mAP with YOLO as shown in Table 9)."
        },
        {
            "heading": "A.5 PROOF OF THRESHOLD DOWNSCALING WITH HOYER EXTREMUM",
            "text": "In order to prove that our Hoyer regularized threshold is always less than or equal to the trainable threshold vth, we first prove that the Hoyer extremum of zclipl is less than or equal to 1. Let us use cl to represent z clip l , so \u2200j, 0 \u2264 c j l \u2264 1\nExt(cl) = \u2225cl\u222522 \u2225cl\u22251 =\n\u2211 j(c j l )\n2\u2211 j c j l\n\u2264 \u2211 j(c j l \u00b7max(c j l ))\u2211\nj c j l\n\u2264 max(cjl )) \u2264 1\n(14)\nSo the Hoyer extremum of zclipl is always less than or equal one, and thus, our Hoyer regularized threshold, which is the product of vthl and Ext(z clip l ) is always less than or equal to the trainable threshold vthl ."
        },
        {
            "heading": "A.6 DATASETS & HYPERPARAMETERS",
            "text": "For training VGG16 models, we using Adam optimizer with initial learning rate of 0.0001, weight decay of 0.0001, dropout of 0.1 and batch size of 128 in CIFAR10 for 600 epochs, and Adam optimizer with weight decay of 5e\u22126 and with batch size 64 in ImageNet for 180 epochs. For training ResNet models, we using SGD optimizer with initial learning rate of 0.1, weight decay of 0.0001 and batch size of 128 in CIFAR10 for 400 epochs, and Adam optimizer with weight decay of 5e\u22126 and with batch size 64 in ImageNet for 120 epochs. We divide the learning rate by 5 at 60%, 80%, and 90% of the total number of epochs. Note that \u03bbh = 1e\u22128 for all our experiments. When calculating the Hoyer extremum we implement two versions, one that calculates the Hoyer extremum for the whole batch, while another that calculates it channel-wise. Our experiments show that using the channel-wise version can bring 0.1\u22120.3% increase in accuracy. All the experimental results reported in this paper use this channel-wise version.\nFor Faster R-CNN, we use SGD optimizer with initial learning rate of 0.01 for 50 epochs, and divide the learning rate by 10 after 25 and 40 epochs each. For Retinanet, we use SGD optimizer with initial learning rate of 0.001 with the same learning rate scheduler as Faster R-CNN."
        },
        {
            "heading": "A.7 EXTENSION TO SNNS FOR DYNAMIC VISION SENSOR (DVS) TASKS",
            "text": "The inherent temporal dynamics in SNNs may be better leveraged in DVS or event-based tasks (Deng et al., 2022; Li et al., 2022; Kim & Panda, 2021; Kim et al., 2022) compared to standard static vision tasks. Hence, we also evaluate our framework on two DVS datasets, namely DVS-CIFAR10 and IBM-Gesture. As illustrated in Table 10, we surpass the test accuracy of existing works (Li et al., 2022; Kim & Panda, 2021) by 0.93% on average at iso-time-step and architecture. Note that the architecture VGGSNN employed in our work and (Deng et al., 2022) is based on VGG11 with two fully connected layers removed as (Deng et al., 2022) found that additional fully connected layers were unnecessary for neuromorphic datasets. To compensate for the loss of momentum in SGD and improve the accuracy, we adopt the temporal efficient training (TET) method in our framework (Deng et al., 2022). In fact, our accuracy gain is more significant at low time steps, thereby implying the portability of our approach to DVS tasks. Note that similar to static datasets, a large number of time steps increase the temporal overhead in SNNs, resulting in a large memory footprint and spiking activity."
        },
        {
            "heading": "A.8 ENERGY EVALUATION FRAMEWORK",
            "text": ""
        },
        {
            "heading": "A.8.1 COMPUTE ENERGY",
            "text": "The total compute energy (CE) of a BANN (BANNCE) can be estimated as\nBANNCE=DNN op 1 Emac+ L\u2211 l=2 (SlDNN op l Eac + EspDNN op l )+ L\u2211 l=1 DNN coml Ecom (15)\nbecause BANN receives full-precision input in the first layer (l=1) without any sparsity (Chowdhury et al., 2021; Datta et al., 2022b; Rathi et al., 2020a). Note that DNN coml denotes the total number of comparison (thresholding) operations in the layer l with each operation consuming 1.64pJ energy in our FPGA platform for FP reperesentation. Also, note that DNNopl denote the total number of floating point (MAC or AC) operations in layer l, where each FP MAC operation consumes 13.32pJ energy, while each FP AC operation consumes 1.8pJ energy. Lastly, Sl denote the spiking activity in the layer l, and Esp = 0.05pJ denotes the energy overhead due to sparsity, that is incurred in checking whether the binary activation is zero.\nThe CE of the full-precision DNN (DNNCE) and bi-polar BNN (BNNCE) is estimated as\nDNNCE = L\u2211 l=1 DNNopl Emac, BNNCE = DNN op 1 Emac + L\u2211 l=2 DNNopl Exor (16)\nwhere we ignore the energy consumed by the ReLU operation (significantly lower compared to thresholding operation) since that includes only checking the sign bit. Note that Exor denotes the XOR energy incurred in the binary convolution/linear operations in bi-polar BNNs.\nThe compute energy of an SNN with a total of T time steps is denoted as\nSNNCE= T\u2211 t=1 ( DNNop1 Emac+ L\u2211 l=2 (StlDNN op l Eac + EspDNN op l )+ L\u2211 l=1 DNN coml Ecom ) (17) where Stl denotes the activation sparsity of layer l at time step t, and we replicate the BANN energy for all T time steps. Note that we ignore the potential reset energy since the number of reset operations is negligible compared to the FLOPs, and the energy consumed per reset operation is similar to that of an AC operation.\nIntuitively, BANNs with low-bit (\u22646-bits) weights, consume lower compute energy compared to BNNs since the energy savings due to the activation sparsity even with its overhead outweighs the energy savings due to XOR operations compared to low-precision AC operations. BANNs also consume lower compute energy compared to SNNs and DNNs due to the higher activation sparsity, leading to less number of FP operations."
        },
        {
            "heading": "A.8.2 MEMORY ENERGY",
            "text": "Since memory accesses often dominate the total energy consumption in DNNs, we incorporate the same in our energy model using (Ottati et al., 2023).\nThe memory energy of a BANN is estimated as\nBANNME= L\u2211 l=1 CilC o l k 2 l E wt rd+C i 1C o 1k 2 l H1W1E inp rd +\nL\u2211 l=2 CilC o l k 2 l HlWl(SlE sp rd+Esp)+ L\u2211 l=1 HlWlC o l E sp wr\n(18)\nwhere Cil , C o l , kl, Hl, and Wl denote the number of input channels, number of output channels, kernel size, output feature map height, and output feature map width respectively. Note that Esprd and Espwr denote the energy incurred in reading a spike input and writing a spike output respectively from/to the on-chip FPGA memory. On the other hand, Einprd denotes the read energy of each 8-bit input pixel value, and Ewtrd denotes the read energy of the weights from the memory.\nThe memory energy of a traditional bi-polar BNN is estimated as\nBNNME= L\u2211 l=1 CilC o l k 2 l E wt rd+C i 1C o 1k 2 l H1W1E inp rd + L\u2211 l=1 CilC o l k 2 l HlWlE sp rd+ L\u2211 l=1 HlWlC o l E of wr\n(19) where Eofwr denotes the energy incurred in writing the FP output feature map to the on-chip memory. While Ewtrd is lower for BNNs compared to multi-bit-weighted BANNs, BNNs incur higher output\nfeature map write energy due to multi-bit outputs, and higher input feature map read energy due to absence of spike sparsity.\nIn summary, BANN can hep reduce both the compute and memory energy compared to BNNs, SNNs, and non-spiking DNNs, provided the hardware supports the activation sparsity.\nThe memory energy of an SNN with T time steps is estimated as\nSNNME= L\u2211 l=1 CilC o l k 2 l E wt rd+C i 1C o 1k 2 l H1W1E inp rd +\nT\u2211 t=1\n( L\u2211\nl=2\nCilC o l k 2 l HlWl(S t lE sp rd+Esp)+ L\u2211 l=1 HlWlC o l (E sp wr + E mem rd + E mem wr ) ) (20) where Ememrd and E mem wr denote the energies incurred in read and write operations of the membrane potential respectively, that happens for every time step. Lastly, the memory energy of a FP DNN is computed as\nDNNME= L\u2211 l=1 CilC o l k 2 l E wt rd+C i 1C o 1k 2 l H1W1E inp rd + L\u2211 l=1 CilC o l k 2 l HlWlE if rd+ L\u2211 l=1 HlWlC o l E of wr\n(21) where Eifrd denotes the read energy of the full-precision input feature map, that is higher than the spike read energy incurred in the BNN/BANN/SNN. Note that all the memory access energy numbers, Ewtrd , E sp rd , E sp wr, E inp rd , E if rd, E of wr, E mem rd , E mem wr , are obtained from our FPGA simulation setup with a 42MB of on-chip (BRAM and URAM) capacity. Note that each of these numbers depend on the bit-precision of the value read/written. For example, assuming that 32 spikes are encoded to an 32b memory word, the energy associated with a spike read memory operation is almost same as Ewtrd 32 , for 32-bit FP weights. All these numbers, along with those for the compute operations, are tabulated in Table 11, where the weights, membrane potentials, input and output feature maps have FP representation.\nNote that for SNNs, the memory footprint is primarily dominated by the read and write accesses of the post-synaptic potential at each time step (Yin et al., 2022). This is because these memory accesses are not influenced by the SNN sparsity since each post-synaptic potential is the sum of k2cin weight-modulated spikes. For a typical convolutional layer, k = 3, cin = 128, and so it is almost impossible that all the k2cin spike values are zero for the membrane potential to be kept unchanged at a particular time step. Note that the number of input (output) feature map read (write) accesses can be reduced with the spike sparsity, and typically do not dominate the memory footprint of SNNs. Since BANNs do not have the temporal membrane potential, they can significantly reduce the memory cost compared to SNNs. Compared to DNNs and BNNs, the memory cost, that is dominated by the feature map accesses (with weight re-use), can be reduced in BANNs by the zero gating logic that leverages spike sparsity."
        },
        {
            "heading": "A.9 TRAINING & INFERENCE TIME REQUIREMENTS",
            "text": "Because SOTA SNNs require iteration over multiple time steps and storage of the membrane potentials for each neuron, their training and inference time can be substantially higher than the DNN/BNN counterparts. Though reducing their latency\nto 1 time step can bridge this gap significantly, as shown in Figure 4, it significantly increases the training complexity due to the iterative time-step reduction.\nOn average, our BANNs represent a 2.38\u00d7 and 2.33\u00d7 reduction in training and inference time per epoch respectively, compared to the multitime-step training approaches (Datta & Beerel, 2022; Rathi et al., 2020a) with iso-batch and hardware conditions.\nCompared to the existing one-time-step SNNs (Chowdhury et al., 2021), we yield a 19\u00d7 and 1.25\u00d7 reduction in training and inference time. This inference time reduction is possibly due to more efficient PyTorch tensor operations used in our code which may be better optimized using the underlying CUDA compiler. Such significant savings in training time, which trans-\nlates to power savings in big data centers, can potentially reduce AI\u2019s environmental impact."
        },
        {
            "heading": "A.10 COMPARISON OF LATENCY FOOTPRINT",
            "text": "Using a hardware validated latency model on ResNet-18, our 6-bit weighted BANNs incur 1.34\u00d7 higher latency compared to uni-polar BNNs, as activation sparsity favors energy not latency & popcount incurs lower latency than accumulates. On the contrary, our binary weighted BANNs incur almost similar (0.96\u00d7) latency as a uni-polar BNN."
        },
        {
            "heading": "A.11 STABILITY OF HOYER REGULARIZED TRAINING",
            "text": "The effect of the Hoyer spike layer is a down-scaling of the thresholds obtained from SGD. However, because these thresholds are fixed during inference, the mapping to neuromorphic hardware is no more complex than a traditional SNN. Moreover, since our training method incorporates the learnable training threshold obtained via SGD, we report its evolvement during the training stage in Fig. 5 (a) and (b) for VGG16 and ResNet20 respectively on CIFAR10. As we can see, our threshold value stabilizes well during the later parts of the training for all the layers, demonstrating the robustness of our method."
        }
    ],
    "title": "EFFICIENT COMPUTER VISION?",
    "year": 2024
}