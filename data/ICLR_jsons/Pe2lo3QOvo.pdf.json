{
    "abstractText": "Reinforcement Learning algorithms that learn from human feedback (RLHF) need to be efficient in terms of statistical complexity, computational complexity, and query complexity. In this work, we consider the RLHF setting where the feedback is given in the format of preferences over pairs of trajectories. In the linear MDP model, using randomization in algorithm design, we present an algorithm that is sample efficient (i.e., has near-optimal worst-case regret bounds) and has polynomial running time (i.e., computational complexity is polynomial with respect to relevant parameters). Our algorithm further minimizes the query complexity through a novel randomized active learning procedure. In particular, our algorithm demonstrates a near-optimal tradeoff between the regret bound and the query complexity. To extend the results to more general nonlinear function approximation, we design a model-based randomized algorithm inspired by the idea of Thompson sampling. Our algorithm minimizes Bayesian regret bound and query complexity, again achieving a near-optimal tradeoff between these two quantities. Computation-wise, similar to the prior Thompson sampling algorithms under the regular RL setting, the main computation primitives of our algorithm are Bayesian supervised learning oracles which have been heavily investigated on the empirical side when applying Thompson sampling algorithms to RL benchmark problems.",
    "authors": [
        {
            "affiliations": [],
            "name": "Runzhe Wu"
        },
        {
            "affiliations": [],
            "name": "Wen Sun"
        }
    ],
    "id": "SP:591b30f179cedbe89598e518be6d4e09e38d70dd",
    "references": [
        {
            "authors": [
                "Yasin Abbasi-Yadkori",
                "D\u00e1vid P\u00e1l",
                "Csaba Szepesv\u00e1ri"
            ],
            "title": "Improved algorithms for linear stochastic bandits",
            "venue": "Advances in neural information processing systems,",
            "year": 2011
        },
        {
            "authors": [
                "Marc Abeille",
                "Alessandro Lazaric"
            ],
            "title": "Linear thompson sampling revisited",
            "venue": "In Artificial Intelligence and Statistics,",
            "year": 2017
        },
        {
            "authors": [
                "Alekh Agarwal"
            ],
            "title": "Selective sampling algorithms for cost-sensitive multiclass prediction",
            "venue": "In International Conference on Machine Learning,",
            "year": 2013
        },
        {
            "authors": [
                "Alekh Agarwal",
                "Tong Zhang"
            ],
            "title": "Model-based rl with optimistic posterior sampling: Structural conditions and sample complexity",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Alekh Agarwal",
                "Sham Kakade",
                "Akshay Krishnamurthy",
                "Wen Sun"
            ],
            "title": "Flambe: Structural complexity and representation learning of low rank mdps",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Priyank Agrawal",
                "Jinglin Chen",
                "Nan Jiang"
            ],
            "title": "Improved worst-case regret bounds for randomized least-squares value iteration",
            "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Shipra Agrawal",
                "Randy Jia"
            ],
            "title": "Optimistic posterior sampling for reinforcement learning: worstcase regret bounds",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2017
        },
        {
            "authors": [
                "Alex Ayoub",
                "Zeyu Jia",
                "Csaba Szepesvari",
                "Mengdi Wang",
                "Lin Yang"
            ],
            "title": "Model-based reinforcement learning with value-targeted regression",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Mohammad Gheshlaghi Azar",
                "Ian Osband",
                "R\u00e9mi Munos"
            ],
            "title": "Minimax regret bounds for reinforcement learning",
            "venue": "In International Conference on Machine Learning,",
            "year": 2017
        },
        {
            "authors": [
                "Viktor Bengs",
                "Aadirupa Saha",
                "Eyke H\u00fcllermeier"
            ],
            "title": "Stochastic contextual dueling bandits under linear stochastic transitivity models",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Ralph Allan Bradley",
                "Milton E Terry"
            ],
            "title": "Rank analysis of incomplete block designs: I. the method of paired comparisons",
            "year": 1952
        },
        {
            "authors": [
                "Nicolo Cesa-Bianchi",
                "G\u00e1bor Lugosi",
                "Gilles Stoltz"
            ],
            "title": "Minimizing regret with label efficient prediction",
            "venue": "IEEE Transactions on Information Theory,",
            "year": 2005
        },
        {
            "authors": [
                "Niladri Chatterji",
                "Aldo Pacchiano",
                "Peter Bartlett",
                "Michael Jordan"
            ],
            "title": "On the theory of reinforcement learning with once-per-episode feedback",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Xiaoyu Chen",
                "Han Zhong",
                "Zhuoran Yang",
                "Zhaoran Wang",
                "Liwei Wang"
            ],
            "title": "Human-in-the-loop: Provably efficient preference-based reinforcement learning with general function approximation",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Paul F Christiano",
                "Jan Leike",
                "Tom Brown",
                "Miljan Martic",
                "Shane Legg",
                "Dario Amodei"
            ],
            "title": "Deep reinforcement learning from human preferences",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Ofer Dekel",
                "Claudio Gentile",
                "Karthik Sridharan"
            ],
            "title": "Selective sampling and active learning from single and multiple experts",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2012
        },
        {
            "authors": [
                "Simon Du",
                "Sham Kakade",
                "Jason Lee",
                "Shachar Lovett",
                "Gaurav Mahajan",
                "Wen Sun",
                "Ruosong Wang"
            ],
            "title": "Bilinear classes: A structural framework for provable generalization in rl",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Miroslav Dud\u0131\u0301k",
                "Katja Hofmann",
                "Robert E Schapire",
                "Aleksandrs Slivkins",
                "Masrour Zoghi"
            ],
            "title": "Contextual dueling bandits",
            "venue": "In Conference on Learning Theory,",
            "year": 2015
        },
        {
            "authors": [
                "Yonathan Efroni",
                "Nadav Merlis",
                "Shie Mannor"
            ],
            "title": "Reinforcement learning with trajectory feedback",
            "venue": "In Proceedings of the AAAI conference on artificial intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Dylan Foster",
                "Alexander Rakhlin",
                "David Simchi-Levi",
                "Yunzong Xu"
            ],
            "title": "Instance-dependent complexity of contextual bandits and reinforcement learning: A disagreement-based perspective",
            "venue": "In Conference on Learning Theory,",
            "year": 2021
        },
        {
            "authors": [
                "Sara A Geer"
            ],
            "title": "Empirical Processes in M-estimation, volume 6",
            "venue": "Cambridge university press,",
            "year": 2000
        },
        {
            "authors": [
                "Aditya Gopalan",
                "Shie Mannor"
            ],
            "title": "Thompson sampling for learning parameterized markov decision processes",
            "venue": "In Conference on Learning Theory,",
            "year": 2015
        },
        {
            "authors": [
                "Steve Hanneke",
                "Liu Yang"
            ],
            "title": "Minimax analysis of active learning",
            "venue": "J. Mach. Learn. Res.,",
            "year": 2015
        },
        {
            "authors": [
                "Steve Hanneke",
                "Liu Yang"
            ],
            "title": "Toward a general theory of online selective sampling: Trading off mistakes and queries",
            "venue": "In International Conference on Artificial Intelligence and Statistics,",
            "year": 2021
        },
        {
            "authors": [
                "Haque Ishfaq",
                "Qiwen Cui",
                "Viet Nguyen",
                "Alex Ayoub",
                "Zhuoran Yang",
                "Zhaoran Wang",
                "Doina Precup",
                "Lin Yang"
            ],
            "title": "Randomized exploration in reinforcement learning with general value function approximation",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Ashesh Jain",
                "Brian Wojcik",
                "Thorsten Joachims",
                "Ashutosh Saxena"
            ],
            "title": "Learning trajectory preferences for manipulators via iterative improvement",
            "venue": "Advances in neural information processing systems,",
            "year": 2013
        },
        {
            "authors": [
                "Ashesh Jain",
                "Shikhar Sharma",
                "Thorsten Joachims",
                "Ashutosh Saxena"
            ],
            "title": "Learning preferences for manipulation tasks from online coactive feedback",
            "venue": "The International Journal of Robotics Research,",
            "year": 2015
        },
        {
            "authors": [
                "Chi Jin",
                "Zhuoran Yang",
                "Zhaoran Wang",
                "Michael I Jordan"
            ],
            "title": "Provably efficient reinforcement learning with linear function approximation",
            "venue": "In Conference on Learning Theory,",
            "year": 2020
        },
        {
            "authors": [
                "Chi Jin",
                "Qinghua Liu",
                "Sobhan"
            ],
            "title": "Miryoosefi. Bellman eluder dimension: New rich classes of rl problems, and sample-efficient algorithms",
            "venue": "Advances in neural information processing systems,",
            "year": 2021
        },
        {
            "authors": [
                "Michael Laskey",
                "Sam Staszak",
                "Wesley Yu-Shu Hsieh",
                "Jeffrey Mahler",
                "Florian T Pokorny",
                "Anca D Dragan",
                "Ken Goldberg. Shiv"
            ],
            "title": "Reducing supervisor burden in dagger using support vectors for efficient learning from demonstrations in high dimensional state spaces",
            "venue": "IEEE International Conference on Robotics and Automation (ICRA),",
            "year": 2016
        },
        {
            "authors": [
                "Hunter Lightman",
                "Vineet Kosaraju",
                "Yuri Burda",
                "Harrison Edwards",
                "Bowen Baker",
                "Teddy Lee",
                "Jan Leike",
                "John Schulman",
                "Ilya Sutskever",
                "Karl Cobbe"
            ],
            "title": "Let\u2019s verify step by step",
            "venue": "In The Twelfth International Conference on Learning Representations,",
            "year": 2023
        },
        {
            "authors": [
                "Qinghua Liu",
                "Alan Chung",
                "Csaba Szepesv\u00e1ri",
                "Chi Jin"
            ],
            "title": "When is partially observable reinforcement learning not scary",
            "venue": "In Conference on Learning Theory,",
            "year": 2022
        },
        {
            "authors": [
                "Qinghua Liu",
                "Csaba Szepesv\u00e1ri",
                "Chi Jin"
            ],
            "title": "Sample-efficient reinforcement learning of partially observable markov games",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Qinghua Liu",
                "Praneeth Netrapalli",
                "Csaba Szepesvari",
                "Chi Jin"
            ],
            "title": "Optimistic mle: A generic modelbased algorithm for partially observable sequential decision making",
            "venue": "In Proceedings of the 55th Annual ACM Symposium on Theory of Computing,",
            "year": 2023
        },
        {
            "authors": [
                "Ellen Novoseller",
                "Yibing Wei",
                "Yanan Sui",
                "Yisong Yue",
                "Joel Burdick"
            ],
            "title": "Dueling posterior sampling for preference-based reinforcement learning",
            "venue": "In Conference on Uncertainty in Artificial Intelligence,",
            "year": 2020
        },
        {
            "authors": [
                "Ian Osband",
                "Benjamin Van Roy"
            ],
            "title": "Model-based reinforcement learning and the eluder dimension",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2014
        },
        {
            "authors": [
                "Ian Osband",
                "Benjamin Van Roy"
            ],
            "title": "Near-optimal reinforcement learning in factored mdps",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2014
        },
        {
            "authors": [
                "Ian Osband",
                "Daniel Russo",
                "Benjamin Van Roy"
            ],
            "title": "more) efficient reinforcement learning via posterior sampling",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2013
        },
        {
            "authors": [
                "Ian Osband",
                "Charles Blundell",
                "Alexander Pritzel",
                "Benjamin Van Roy"
            ],
            "title": "Deep exploration via bootstrapped dqn",
            "venue": "Advances in neural information processing systems,",
            "year": 2016
        },
        {
            "authors": [
                "Ian Osband",
                "Benjamin Van Roy",
                "Zheng Wen"
            ],
            "title": "Generalization and exploration via randomized value functions",
            "venue": "In International Conference on Machine Learning,",
            "year": 2016
        },
        {
            "authors": [
                "Ian Osband",
                "Zheng Wen",
                "Seyed Mohammad Asghari",
                "Vikranth Dwaracherla",
                "Morteza Ibrahimi",
                "Xiuyuan Lu",
                "Benjamin Van Roy"
            ],
            "title": "Approximate thompson sampling via epistemic neural networks",
            "venue": "In Uncertainty in Artificial Intelligence,",
            "year": 2023
        },
        {
            "authors": [
                "Long Ouyang",
                "Jeffrey Wu",
                "Xu Jiang",
                "Diogo Almeida",
                "Carroll Wainwright",
                "Pamela Mishkin",
                "Chong Zhang",
                "Sandhini Agarwal",
                "Katarina Slama",
                "Alex Ray"
            ],
            "title": "Training language models to follow instructions with human feedback",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "David Pollard"
            ],
            "title": "Empirical processes: theory and applications",
            "year": 1990
        },
        {
            "authors": [
                "Nikita Puchkin",
                "Nikita Zhivotovskiy"
            ],
            "title": "Exponential savings in agnostic active learning through abstention",
            "venue": "In Conference on Learning Theory,",
            "year": 2021
        },
        {
            "authors": [
                "St\u00e9phane Ross",
                "Narek Melik-Barkhudarov",
                "Kumar Shaurya Shankar",
                "Andreas Wendel",
                "Debadeepta Dey",
                "J Andrew Bagnell",
                "Martial Hebert"
            ],
            "title": "Learning monocular reactive uav control in cluttered natural environments",
            "venue": "IEEE international conference on robotics and automation,",
            "year": 2013
        },
        {
            "authors": [
                "Daniel Russo",
                "Benjamin Van Roy"
            ],
            "title": "Eluder dimension and the sample complexity of optimistic exploration",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2013
        },
        {
            "authors": [
                "Daniel Russo",
                "Benjamin Van Roy"
            ],
            "title": "Learning to optimize via posterior sampling",
            "venue": "Mathematics of Operations Research,",
            "year": 2014
        },
        {
            "authors": [
                "Aadirupa Saha",
                "Aldo Pacchiano",
                "Jonathan Lee"
            ],
            "title": "Dueling rl: Reinforcement learning with trajectory preferences",
            "venue": "In International Conference on Artificial Intelligence and Statistics,",
            "year": 2023
        },
        {
            "authors": [
                "John Schulman",
                "Filip Wolski",
                "Prafulla Dhariwal",
                "Alec Radford",
                "Oleg Klimov"
            ],
            "title": "Proximal policy optimization algorithms",
            "venue": "arXiv preprint arXiv:1707.06347,",
            "year": 2017
        },
        {
            "authors": [
                "Ayush Sekhari",
                "Karthik Sridharan",
                "Wen Sun",
                "Runzhe Wu"
            ],
            "title": "Contextual bandits and imitation learning with preference-based active queries",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2023
        },
        {
            "authors": [
                "Ayush Sekhari",
                "Karthik Sridharan",
                "Wen Sun",
                "Runzhe Wu"
            ],
            "title": "Selective sampling and imitation learning via online regression",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2023
        },
        {
            "authors": [
                "Nisan Stiennon",
                "Long Ouyang",
                "Jeffrey Wu",
                "Daniel Ziegler",
                "Ryan Lowe",
                "Chelsea Voss",
                "Alec Radford",
                "Dario Amodei",
                "Paul F Christiano"
            ],
            "title": "Learning to summarize with human feedback",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Masatoshi Uehara",
                "Wen Sun"
            ],
            "title": "Pessimistic model-based offline reinforcement learning under partial coverage",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Sara Van de Geer"
            ],
            "title": "Empirical Processes in M-estimation, volume 6",
            "venue": "Cambridge university press,",
            "year": 2000
        },
        {
            "authors": [
                "Ruosong Wang",
                "Russ R Salakhutdinov",
                "Lin Yang"
            ],
            "title": "Reinforcement learning with general value function approximation: Provably efficient approach via bounded eluder dimension",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Yuanhao Wang",
                "Qinghua Liu",
                "Chi Jin"
            ],
            "title": "Is rlhf more difficult than standard rl",
            "venue": "arXiv preprint arXiv:2306.14111,",
            "year": 2023
        },
        {
            "authors": [
                "Zheng Wen",
                "Benjamin Van Roy"
            ],
            "title": "Efficient exploration and value function generalization in deterministic systems",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2013
        },
        {
            "authors": [
                "Runzhe Wu",
                "Masatoshi Uehara",
                "Wen Sun"
            ],
            "title": "Distributional offline policy evaluation with predictive error guarantees",
            "venue": "In International Conference on Machine Learning,",
            "year": 2023
        },
        {
            "authors": [
                "Yue Wu",
                "Tao Jin",
                "Hao Lou",
                "Farzad Farnoud",
                "Quanquan Gu"
            ],
            "title": "Borda regret minimization for generalized linear dueling bandits",
            "venue": "arXiv preprint arXiv:2303.08816,",
            "year": 2023
        },
        {
            "authors": [
                "Tengyang Xie",
                "Dylan J Foster",
                "Yu Bai",
                "Nan Jiang",
                "Sham M Kakade"
            ],
            "title": "The role of coverage in online reinforcement learning",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Yichong Xu",
                "Ruosong Wang",
                "Lin Yang",
                "Aarti Singh",
                "Artur Dubrawski"
            ],
            "title": "Preference-based reinforcement learning with finite-time guarantees",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Yisong Yue",
                "Thorsten Joachims"
            ],
            "title": "Beat the mean bandit",
            "venue": "In Proceedings of the 28th international conference on machine learning",
            "year": 2011
        },
        {
            "authors": [
                "Yisong Yue",
                "Josef Broder",
                "Robert Kleinberg",
                "Thorsten Joachims"
            ],
            "title": "The k-armed dueling bandits problem",
            "venue": "Journal of Computer and System Sciences,",
            "year": 2012
        },
        {
            "authors": [
                "Andrea Zanette",
                "David Brandfonbrener",
                "Emma Brunskill",
                "Matteo Pirotta",
                "Alessandro Lazaric"
            ],
            "title": "Frequentist regret bounds for randomized least-squares value iteration",
            "venue": "In International Conference on Artificial Intelligence and Statistics,",
            "year": 2020
        },
        {
            "authors": [
                "Wenhao Zhan",
                "Masatoshi Uehara",
                "Nathan Kallus",
                "Jason D Lee",
                "Wen Sun"
            ],
            "title": "Provable offline reinforcement learning with human feedback",
            "venue": "arXiv preprint arXiv:2305.14816,",
            "year": 2023
        },
        {
            "authors": [
                "Wenhao Zhan",
                "Masatoshi Uehara",
                "Wen Sun",
                "Jason D Lee"
            ],
            "title": "How to query human feedback efficiently in rl",
            "venue": "arXiv preprint arXiv:2305.18505,",
            "year": 2023
        },
        {
            "authors": [
                "Han Zhong",
                "Wei Xiong",
                "Sirui Zheng",
                "Liwei Wang",
                "Zhaoran Wang",
                "Zhuoran Yang",
                "Tong Zhang"
            ],
            "title": "Gec: A unified framework for interactive decision making in mdp, pomdp, and beyond",
            "year": 2022
        },
        {
            "authors": [
                "Banghua Zhu",
                "Michael Jordan",
                "Jiantao Jiao"
            ],
            "title": "Principled reinforcement learning with human feedback from pairwise or k-wise comparisons",
            "venue": "In International Conference on Machine Learning,",
            "year": 2023
        },
        {
            "authors": [
                "Yinglun Zhu",
                "Robert Nowak"
            ],
            "title": "Efficient active learning with abstention",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "e.g",
                "UCBVI (Azar"
            ],
            "title": "2017)) and necessitates leveraging feedback signal at a trajectory level",
            "year": 2017
        },
        {
            "authors": [
                "terji"
            ],
            "title": "2021)), which are restrictive and do not generally hold even in tabular MDPs",
            "year": 2021
        },
        {
            "authors": [
                "Xie"
            ],
            "title": "Sequential Extrapolation Coefficient (SEC",
            "venue": "(Xie et al.,",
            "year": 2022
        },
        {
            "authors": [
                "\u221a cd"
            ],
            "title": "The proof of the first inequality is provided in Abeille & Lazaric (2017, Appendix A). We provide a proof here for completeness",
            "venue": "Let \u03b7 \u223c N (0,",
            "year": 2017
        },
        {
            "authors": [],
            "title": "ELUDER DIMENSION The following lemmas are adapted from Russo ",
            "venue": "Van Roy (2013) and Liu et al. (2022a). Lemma D.11. Following the notation of Definition",
            "year": 2022
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Reinforcement learning from human feedback (RLHF) has been widely used across various domains, including robotics (Jain et al., 2013; 2015) and natural language processing (Stiennon et al., 2020; Ouyang et al., 2022). Unlike standard RL, RLHF requires the agent to learn from feedback in the format of preferences between pairs of trajectories instead of per-step reward since assigning a dense reward function for each state is challenging in many tasks. For instance, in natural language generation, rating each generated token individually is challenging. Hence, it is more realistic to ask humans to compare two pieces of text and indicate their preference. Recent works have shown that, by integrating preference-based feedback into the training process, we can align models with human intention and enable high-quality human-machine interaction.\nDespite the existing empirical applications of RLHF, its theoretical foundation remains far from satisfactory. Empirically, researchers first learn reward models from preference-based feedback and then optimize the reward models via policy gradient-based algorithms such as PPO (Schulman et al., 2017). Questions such as whether or not the learned reward model is accurate, whether PPO is sufficient for deep exploration, and how to strategically collect more feedback on the fly are often ignored. Theoretically, prior works study the regret bound for RL with preference-based feedback (Saha et al., 2023; Chen et al., 2022). Despite achieving sublinear worst-case regret, these algorithms are computationally intractable even for simplified models such as tabular Markov Decision Processes (MDPs). This means that we cannot easily leverage the algorithmic ideas in prior work to guide or improve how we perform RLHF in practice.\nIn addition to maximizing reward, another important metric in RLHF is the query complexity since human feedback is expensive to collect. To illustrate, we note that InstructGPT\u2019s training data comprises a mere 30K instances of human feedback (Ouyang et al., 2022), which is significantly fewer\nthan the internet-scale dataset for training the GPT-3 base model. This underscores the challenge of scaling up the size of human feedback datasets. Ross et al. (2013); Laskey et al. (2016) also pointed out that extensively querying for feedback puts too much burden on human experts. Empirically, Lightman et al. (2023) observes that active learning reduces query complexity and improves the learned reward model. In theory, query complexity is mostly studied in the settings of active learning, online learning, and bandits (Cesa-Bianchi et al., 2005; Dekel et al., 2012; Agarwal, 2013; Hanneke & Yang, 2021; Zhu & Nowak, 2022; Sekhari et al., 2023a;b), but overlooked in RL.\nIn this work, we aim to design new RL algorithms that can learn from preference-based feedback and can be efficient in statistical complexity (i.e., regret), computational complexity, and query complexity. In particular, we strike a near-optimal balance between regret minimization and query complexity minimization. To achieve this goal, our key idea is to use randomization in algorithm design. We summarize our new algorithmic ideas and key contributions as follows.\n1. For MDPs with linear structure (i.e., linear MDP (Jin et al., 2020)), we propose the first RL algorithm that achieves sublinear worst-case regret and computational efficiency simultaneously with preference-based feedback. Even when reduced to tabular MDPs, it is still the first to achieve a no-regret guarantee and computational efficiency. Moreover, it has an active learning procedure and attains a near-optimal tradeoff between the regret and the query complexity. Our algorithm adds random Gaussian noises to the learned state-action-wise reward model and the least-squares value iteration (LSVI) procedure. Using random noise instead of the UCB-style technique (Azar et al., 2017) preserves the Markovian property in the reward model and allows one to use dynamic programming to achieve computation efficiency.\n2. For function approximation beyond linear, we present a model-based Thompson-sampling (TS) algorithm that forms posterior distributions over the transitions and reward models. Assuming the transition and the reward model class both have small \u21131-norm eluder dimension \u2013 a structural condition introduced in Liu et al. (2022a) that is more general than the common \u21132-norm eluder dimension (Russo & Van Roy, 2013), we show that our algorithm again achieves a near-optimal tradeoff between the Bayesian regret and the Bayesian query complexity. Computation-wise, similar to previous TS algorithms for regular RL (e.g., Osband et al. (2013)), the primary computation primitives are Bayesian supervised learning oracles for transition and reward learning.\n3. Our query conditions for both algorithms are based on variance-style uncertainty quantification of the preference induced by the randomness of the reward model. We query for preference feedback only when the uncertainty of the preference on a pair of trajectories is large. Approximately computing the uncertainty can be easily done using i.i.d. random reward models drawn from the reward model distribution, which makes the active query procedure computationally tractable.\nOverall, while our main contribution is on the theoretical side, our theoretical investigation provides several new practical insights. For instance, for regret minimization, our algorithms propose to draw a pair of trajectories with one from the latest policy and the other from an older policy instead of drawing two trajectories from the same policy (e.g., Christiano et al. (2017)), avoiding the situation of drawing two similar trajectories when the policy becomes more and more deterministic. Our theory shows that drawing two trajectories from a combination of new and older policies balances exploration and exploitation better. Another practical insight is the variance-style uncertainty measure for designing the query condition. Compared to more standard active learning procedure that relies on constructing version space and confidence intervals (Dekel et al., 2012; Puchkin & Zhivotovskiy, 2021; Zhu & Nowak, 2022; Sekhari et al., 2023a;b), our new approach comes with strong theoretical guarantees and is more computationally tractable. It is also amenable to existing implementations of Thompson sampling RL algorithms (e.g., using bootstrapping to approximate the posterior sampling (Osband et al., 2016a; 2023))."
        },
        {
            "heading": "2 COMPARISON TO PRIOR WORK",
            "text": "RL with preference-based feedback. Many recent works have obtained statistically efficient algorithms but are computationally inefficient even for tabular MDPs due to intractable policy search and version space construction (Chen et al., 2022; Zhan et al., 2023a;b; Saha et al., 2023). For example, Zhan et al. (2023b); Saha et al. (2023) use the idea from optimal design and rely on the computation oracle: argmax\u03c0,\u03c0\u2032\u2208\u03a0 \u2225Es,a\u223c\u03c0\u03d5(s, a) \u2212 Es,a\u223c\u03c0\u2032\u03d5(s, a)\u2225A with some positive definite matrix A.\nHere \u2225x\u22252A := x\u22a4Ax, and \u03d5 is some state-action feature.1 It is unclear how to implement this oracle since standard planning approaches based on dynamic programming cannot be applied here. In addition, these methods also actively maintain a policy space by eliminating potentially sub-optimal policies. The policy class can be exponentially large even in tabular settings, so how to maintain it computationally efficiently is unclear. We provide a more detailed discussion on the challenges in achieving computational efficiency in RLHF in Appendix A.\nWhile the work mentioned above is intractable even for tabular MDPs, there are some other works that could be computationally efficient but have weaker statistical results. For instance, very recently, Wang et al. (2023) proposed a reduction framework that can be computationally efficient (depending on the base algorithm used in the reduction). However, their algorithms have PAC bounds while we focus on regret minimization. Moreover, we achieve a near-optimal balance between regret and query complexity. Novoseller et al. (2020) proposed a posterior sampling algorithm for tabular MDP but their analysis is asymptotic (i.e., they do not address exploration, exploitation, and query complexity tradeoff). Xu et al. (2020) proposed efficient algorithms that do reward-free exploration. However, it is limited to tabular MDPs and PAC bounds.\nIn contrast to the above works, our algorithms aim to achieve efficiency in statistical, computational, and query complexities simultaneously. Our algorithms leverage randomization to balance exploration, exploitation, and feedback query. Randomization allows us to avoid non-standard computational oracles and only use standard Dynamic Programming (DP) based oracles (e.g., value iteration), which makes our algorithm computationally more tractable. Prior works that simultaneously achieve efficiency in all three aspects are often restricted in the bandit and imitation learning settings where the exploration problem is much easier (Sekhari et al., 2023a).\nRL via randomization. There are two lines of work that study RL via randomization. The first injects random noise into the learning object to encourage exploration. A typical example is the randomized least-squares value iteration (RLSVI) (Osband et al., 2016b), which adds Gaussian noise into the least-squares estimation and achieves near-optimal worst-case regret (Zanette et al., 2020; Agrawal et al., 2021) for linear MDPs. The other line of work is Bayesian RL and uses Thompson sampling (TS) (Osband et al., 2013; Osband & Van Roy, 2014b;a; Gopalan & Mannor, 2015; Agrawal & Jia, 2017; Efroni et al., 2021; Zhong et al., 2022; Agarwal & Zhang, 2022). They achieve provable Bayesian regret upper bound by maintaining posterior distributions over models.\nActive learning. Numerous studies have studied active learning across various settings (CesaBianchi et al., 2005; Dekel et al., 2012; Agarwal, 2013; Hanneke & Yang, 2015; 2021; Zhu & Nowak, 2022; Sekhari et al., 2023b;a). However, most of them focus on the bandits and online learning settings, and their active learning procedures are usually computationally intractable due to computing version spaces or upper and lower confidence bounds. In contrast, we design a variancestyle uncertainty quantification for our query condition, which can be easily estimated by random samples of reward model. This makes our active learning procedure more computationally tractable."
        },
        {
            "heading": "3 PRELIMINARY",
            "text": "Notations. For two real numbers a and b, we denote [a, b] := {x : a \u2264 x \u2264 b}. For an integer N , we denote [N ] := {1, 2, . . . , N}. For a set S, we denote \u2206(S) as the set of distributions over S. Let dTV(\u00b7, \u00b7) denote the total variation distance. We consider a finite-horizon Markov decision process (MDP), which is a tuple M(S,A, r\u22c6, P \u22c6, H) where S is the state space, A is the action space, P \u22c6 : S \u00d7 A \u2192 \u2206(S) is the transition kernel, r\u22c6 : S \u00d7 A \u2192 [0, 1] is the reward function, and H is the length of the episode. The interaction proceeds for T rounds. At each round t \u2208 [T ], we need to select two policies \u03c00t and \u03c01t and execute them separately, which generates two trajectories \u03c40t and \u03c4 1 t where \u03c4 i t = (s i t,1, a i t,1, . . . , s i t,H , a i t,H) for i \u2208 {0, 1}. For the ease of notation, we assume a fixed initial state s1. Then, we need to decide whether to make a query for the preference between \u03c40t and \u03c4 1 t . If making a query, we obtain a preference feedback ot \u2208 {0, 1} that is sampled from the Bernoulli distribution:\nPr(ot = 1 | \u03c41t , \u03c40t , r\u22c6) = Pr(\u03c41t is preferred to \u03c40t | r\u22c6) = \u03a6 ( r\u22c6(\u03c41t )\u2212 r\u22c6(\u03c40t ) ) 1These works typically assume trajectory-wise feature \u03d5(\u03c4) for a trajectory \u03c4 . However, even when speci-\nfied to state-action-wise features, these algorithms are still computationally intractable, even in tabular MDPs.\nwhere r\u22c6(\u03c4 it ) = \u2211H h=1 r \u22c6(sit,h, a i t,h) for i \u2208 {0, 1} is the trajectory reward, and \u03a6 : R \u2192 [0, 1] is a monotonically increasing link function. We note that, by symmetry, we have \u03a6(r\u22c6(\u03c40t )\u2212 r\u22c6(\u03c41t )) + \u03a6(r\u22c6(\u03c41t )\u2212 r\u22c6(\u03c40t )) = 1. If not making a query, we receive no feedback. This feedback model is weaker than the standard RL where the per-step reward signal is revealed. We impose the following assumption on the link function \u03a6, which has appeared in many existing works of RLHF (Saha et al., 2023; Zhu et al., 2023; Zhan et al., 2023a). Assumption 3.1. We assume \u03a6 is differentiable and there exists constants \u03ba, \u03ba > 0 such that \u03ba\u22121 \u2264 \u03a6\u2032(x) \u2264 \u03ba\u22121 for any x \u2208 [\u2212H,H].\nThe constants \u03ba and \u03ba characterize the non-linearity of \u03a6 and determine the difficulty of estimating the reward from preference feedback. It is noteworthy that, in the theoretical results of our algorithms, the bounds depend polynomially on \u03ba but logarithmically on \u03ba. Some typical examples of the link functions are provided below. Example 3.2 (Link functions). It is common to have \u03a6(x) = 1/(1 + exp(\u2212x)), which recovers the Bradley-Terry-Luce (BTL) model (Bradley & Terry, 1952), and we have \u03ba = 2+exp(\u2212H)+exp(H) and \u03ba = 4. Additionally, if the trajectory-wise reward is scaled within the interval of [0, 1], then the difference in reward will be within the range of [\u22121, 1]. In this case, another common choice of the link function is \u03a6(x) = (x+ 1)/2, which results in \u03ba = \u03ba = 2.\nThe goal is to minimize the worst-case regret and the query complexity simultaneously:\nRegretT := T\u2211 t=1 ( 2V \u22c6(s1)\u2212 V \u03c0 0 t (s1)\u2212 V \u03c0 1 t (s1) ) , QueriesT := T\u2211 t=1 Zt.\nHere V \u03c0(s1) := E\u03c0[ \u2211H h=1 r \u22c6(sh, ah)] denotes the state-value function of policy \u03c0, and we define V \u22c6(s1) := V \u03c0\u22c6(s1) where \u03c0\u22c6 is the optimal policy that maximizes the state-value function. The variable Zt \u2208 {0, 1} indicates whether a query is made at round t. Note that the regret looks at the sum of the performance gaps between two pairs of policies: (\u03c0\u22c6, \u03c00t ) and (\u03c0\n\u22c6, \u03c01t ). This is standard in dueling bandits (Yue & Joachims, 2011; Yue et al., 2012; Dud\u0131\u0301k et al., 2015; Bengs et al., 2022; Wu et al., 2023b) and RL with preference-based feedback (Saha et al., 2023; Chen et al., 2022).\nBayesian RL. We also consider Bayesian RL in this work when learning with general function approximation. In the Bayesian setting, P \u22c6 and r\u22c6 are sampled from some known prior distributions \u03c1P and \u03c1r. The goal is to minimize the Bayesian regret and the Bayesian query complexity:\nBayesRegretT := E\n[ T\u2211\nt=1\n( 2V \u22c6(s1)\u2212 V \u03c0 0 t (s1)\u2212 V \u03c0 1 t (s1) )] , BayesQueriesT := E [ T\u2211\nt=1\nZt\n] .\nHere the expectation is taken with respect to the prior distribution over P \u22c6 and r\u22c6. We will use Bayesian supervised learning oracles to compute posteriors over the transition and reward model."
        },
        {
            "heading": "4 A MODEL-FREE RANDOMIZED ALGORITHM FOR LINEAR MDPS",
            "text": "In this section, we present a model-free algorithm for linear MDPs which is defined as follows. Assumption 4.1 (Linear MDP (Jin et al., 2020)). We assume a known feature map \u03d5 : S \u00d7 A \u2192 Rd, an unknown (signed) measure \u00b5 : S \u2192 Rd, and an unknown vector \u03b8\u22c6r such that for any (s, a) \u2208 S \u00d7 A, we have P \u22c6(s\u2032 | s, a) = \u03d5\u22a4(s, a) \u00b7 \u00b5(s\u2032) and r\u22c6(s, a) = \u03d5\u22a4(s, a) \u00b7 \u03b8\u22c6r . We assume \u2225\u03d5(s, a)\u22252 \u2264 1 for all (s, a) \u2208 S\u00d7A, \u222b S \u2225\u00b5(s)\u22252 ds \u2264 \u221a d, and \u2225\u03b8\u22c6r \u22252 \u2264 B for some B > 0. For a\ntrajectory \u03c4 = (s1, a1, . . . , sH , aH), we define \u03d5(\u03c4) = \u2211H h=1 \u03d5(sh, ah) and assume \u2225\u03d5(\u03c4)\u22252 \u2264 1.\nLinear MDPs can capture tabular MDPs by setting d = |S||A| and \u03d5(s, a) to be the one-hot encoding of (s, a). In this case, we have \u2225\u03d5(\u03c4)\u22252 \u2264 H . However, we can scale it down to get \u2225\u03d5(\u03c4)\u22252 \u2264 1 at the expense of scaling B up by H . We define \u0398B = {\u03b8 \u2208 Rd : \u2225\u03b8\u22252 \u2264 B}, which contains \u03b8\u22c6r ."
        },
        {
            "heading": "4.1 ALGORITHM",
            "text": "The algorithm, called PR-LSVI, is presented in Algorithm 1. At the beginning of episode k, it first computes the maximum likelihood estimate \u03b8\u0302r,t (Line 3). Computation-wise, while the likelihood\nobjective is not guaranteed to be concave due to the generality of \u03a6, efficient algorithms exist in certain common scenarios. For example, if \u03a6(x) = 1/(1 + exp(\u2212x)), it recovers the BTL model (Example 3.2). In this case, the MLE objective is concave in \u03b8 and thus can be solved in polynomial running time. Moreover, we emphasize that the reward is learned under trajectory-wise features, which is different from the standard RL setting where it is learned under state-action features.\nGiven the MLE \u03b8\u0302r,t, it next samples \u03b8r,t from a Gaussian distribution centered at \u03b8\u0302r,t (Line 4). Note that the covariance matrix \u03a3\u22121t\u22121 uses trajectory-wise features (Line 16) which allows the randomized Gaussian vector to capture trajectory-wise uncertainty of the learned reward. The noise aims to encourage exploration. Then, it computes the least-squares estimate of the state-action value function \u03b8\u0302P,t,h for each h \u2208 [H] and samples \u03b8P,t,h from a Gaussian distribution centered at \u03b8\u0302P,t,h (Lines 7-8). Similar to the reward model, the noise is added to the state-value function to encourage exploration. We then define the value function Qt,h and V t,h as\nQt,h(s, a) := \u03d5(s, a) \u22a4\u03b8r,t + \u03c9t,h(s, a), V t,h(s) := max\na Qt,h(s, a) (1)\nand the function \u03c9 : S \u00d7A \u2192 R is defined as\n\u03c9t,h(s, a) =  \u03d5(s, a)\u22a4\u03b8P,t,h if \u2225\u03d5(s, a)\u2225\u03a3\u22121t\u22121,h \u2264 \u03b1L \u03c1(s, a) ( \u03d5(s, a)\u22a4\u03b8P,t,h ) + (1\u2212 \u03c1(s, a))(H \u2212 h) if \u03b1L < \u2225\u03d5(s, a)\u2225\u03a3\u22121t\u22121,h \u2264 \u03b1U\nH \u2212 h if \u2225\u03d5(s, a)\u2225\u03a3\u22121t\u22121,h > \u03b1U\nwhere \u03c1(s, a) = (\u03b1U \u2212 \u2225\u03d5(s, a)\u2225\u03a3\u22121t\u22121,h)/(\u03b1U \u2212 \u03b1L) interpolates between the two regimes to ensure continuity. This truncation trick is from Zanette et al. (2020) and is crucial. It controls the abnormally high value estimates. Specifically, when \u2225\u03d5(s, a)\u2225\u03a3\u22121t\u22121,h is large, the uncertainty in the direction of \u03d5(s, a) is large, which makes the estimate \u03d5(s, a)\u22a4\u03b8P,t,h abnormally large. In this case, we have to truncate it to H \u2212h. Moreover, we note that the usual \u201cvalue clipping\u201d trick (i.e., simply constraining the value function within the range of [0, H \u2212 h + 1] by clipping) cannot easily work here since it introduces bias to the random walk analysis, also pointed out by Zanette et al. (2020).\nThen, the algorithm computes the greedy policy \u03c00t with respect to Qt,h. The comparator policy \u03c0 1 t is simply set to the greedy policy from the previous episode, \u03c00t\u22121 . In other words, we are comparing the two most recent greedy policies. This is different from previous work, which compares the current greedy policy with a fixed comparator (Wang et al., 2023). Analytically, for our algorithm, the cumulative regret incurred by \u03c01t for all t \u2208 [T ] is equivalent to that incurred by \u03c00t for all t \u2208 [T ]. Hence, it suffices to compute the regret for one of them and multiply it by two to get the total regret.\nGiven the trajectories \u03c40t and \u03c4 1 t generated by \u03c0 0 t and \u03c0 1 t , we compute the expected absolute reward difference between the trajectories under the same noisy distribution of the reward parameter:\nE \u03b80,\u03b81\u223cN (\u03b8\u0302r,t,\u03c32r\u03a3 \u22121 t\u22121) [\u2223\u2223(\u03d5(\u03c40t )\u2212 \u03d5(\u03c41t ))\u22a4(\u03b80 \u2212 \u03b81)\u2223\u2223]. (2) This represents the uncertainty of the preference between the two trajectories, and we make a query only when it is larger than a threshold \u03f5 (Line 13). Intuitively, we only make a query on two trajectories when we are uncertain about the preference (e.g., the expected disagreement between two randomly sampled reward models is large). Computationally, we can estimate this expectation by drawing polynomially many reward models from the distribution N (\u03b8\u0302r,t, \u03c32r\u03a3\u22121t\u22121) and computing the empirical average. The deviation of the empirical average to the true mean can be easily bounded by standard concentration inequalities. We simply use expectation here for analytical simplicity. If the query condition is triggered, we make a query for feedback on \u03c40t , \u03c4 1 t , and update the trajectory-wise feature covariance matrix accordingly."
        },
        {
            "heading": "4.2 ANALYSIS",
            "text": "The theoretical results of Algorithm 1 are stated in Theorem 4.2. The detailed assignment of hyperparameters can be found in Table 1, and the proof is provided in Appendix B. Theorem 4.2. Define \u03b3 = \u221a \u03ba+B2, which characterizes the difficulty of estimating the reward model. Set \u03c3r = \u0398\u0303(\u03b3 \u221a d), \u03c3P = \u0398\u0303(H3/2d2\u03b3), \u03b1U = (d5/2H3/2\u03b3)\u22121, \u03b1L = \u03b1U/2, and \u03bb = 1.\nAlgorithm 1 Preference-based and Randomized Least-Squares Value Iteration (PR-LSVI) Require: STD \u03c3r, \u03c3P, threshold \u03f5, value cutoff parameters \u03b1L, \u03b1U, and regularization parameter \u03bb.\n1: Let \u03c000 be an arbitrary policy, \u03a30 \u2190 \u03bbI , \u03a30,h \u2190 \u03bbI (\u2200h \u2208 [H]). 2: for t = 1, . . . , T do 3: \u03b8\u0302r,t \u2190 argmax\u03b8\u2208\u0398B \u2211t\u22121 s=1 Zs ln(os\u03a6((\u03d5(\u03c4 1 s )\u2212\u03d5(\u03c40s ))\u22a4\u03b8)+(1\u2212os)\u03a6((\u03d5(\u03c40s )\u2212\u03d5(\u03c41s ))\u22a4\u03b8)) 4: \u03b8r,t \u223c N (\u03b8\u0302r,t, \u03c32r\u03a3\u22121t\u22121) 5: \u03b8\u0302P,t,H \u2190 0, \u03b8P,t,H \u2190 0 6: for h = H \u2212 1, . . . , 1 do 7: \u03b8\u0302P,t,h \u2190 \u03a3\u22121t\u22121,h( \u2211t\u22121 i=1 \u03d5(s 0 i,h, a 0 i,h)V t,h+1(s 0 i,h+1)) 8: \u03b8P,t,h \u223c N (\u03b8\u0302P,t,h, \u03c32P\u03a3 \u22121 t\u22121,h)\n9: Define Qt,h and V t,h as in (1). 10: end for 11: Set \u03c00t \u2190 {\u03c00t,h : \u03c00t,h(s) = argmaxa Qt,h(s, a), \u2200s \u2208 S, h \u2208 [H]} and \u03c01t \u2190 \u03c00t\u22121. 12: Sample \u03c40t \u223c \u03c00t and \u03c41t \u223c \u03c01t . 13: Zt \u2190 1{E\u03b80,\u03b81\u223cN (\u03b8\u0302r,t,\u03c32r\u03a3\u22121t\u22121)[|(\u03d5(\u03c4 0 t )\u2212 \u03d5(\u03c41t ))\u22a4(\u03b80 \u2212 \u03b81)|] > \u03f5} 14: if Zt = 1 then 15: Query preference feedback ot on {\u03c40t , \u03c41t } 16: \u03a3t \u2190 \u03a3t\u22121 + (\u03d5(\u03c40t )\u2212 \u03d5(\u03c41t ))(\u03d5(\u03c40t )\u2212 \u03d5(\u03c41t ))\u22a4 17: else 18: \u03a3t \u2190 \u03a3t\u22121 19: end if 20: \u03a3t,h \u2190 \u03a3t\u22121,h + \u03d5(s0t,h, a0t,h)\u03d5\u22a4(s0t,h, a0t,h) (\u2200h \u2208 [H]). 21: end for\nThen, PR-LSVI (Algorithm 1) guarantees the following with probability at least 1\u2212 \u03b4: RegretT = O\u0303 ( \u03f5Td1/2 + \u221a T \u00b7 d3H5/2\u03b3 + d17/2H11/2\u03b33 ) , QueriesT = O\u0303 ( d4\u03b34/\u03f52 ) .\nTo further study the balance between the regret and the query complexity, we let \u03f5 = T\u2212\u03b2 for some \u03b2 \u2264 1/2. Then, the upper bounds in Theorem 4.2 can be rewritten as\nRegretT = O\u0303(T 1\u2212\u03b2), QueriesT = O\u0303(T 2\u03b2)\nwhere we only focus on the dependence on T and omit any other factors for simplicity. We see that there is a tradeoff in T between the regret and the query complexity \u2014 the smaller regret we want, the more queries we need to make. For example, when \u03b2 = 0, the regret is O\u0303(T ), and the query complexity is O\u0303(1), meaning that we will incur linear regret if we don\u2019t make any query. If we increase \u03b2 to 1/2, the regret decreases to O\u0303( \u221a T ) while the query complexity increases to O\u0303(T ), meaning that the regret bound is optimal in T but we make queries every episode.\nWe emphasize that this tradeoff in T is optimal, as evidenced by a lower bound result established by Sekhari et al. (2023a). Their lower bound was originally proposed for contextual dueling bandits, which is a special case of our setting. Their results are stated below. Theorem 4.3. (Sekhari et al., 2023a, Theorem 5) The following two claims hold: (1) For any algorithm, there exists an instance that leads to RegretT = \u2126( \u221a T ); (2) For any algorithm achieving an expected regret upper bound in the form of E[RegretT ] = O(T 1\u2212\u03b2) for some \u03b2 > 0, there exists an instance that results in E[QueriesT ] = \u2126(T 2\u03b2).\nHowever, the dependence on other parameters (e.g., d and H) can be loose, and further improvement may be possible. We leave further investigation of these factors as future work.\nAlthough injecting random noise is inspired by RLSVI (Zanette et al., 2020), we highlight five key differences between ours and theirs: (1) Since the feedback is trajectory-wise, we need to design random noise that preserves the state-action-wise format (so that it can be used in DP) but captures the trajectory-wise uncertainty. We do this by maintaining \u03a3t, which uses trajectory-wise feature differences; (2) Since the preference feedback is generated from some probabilistic model, we learn the\nreward model via MLE and use MLE generalization bound (Geer, 2000) to capture the uncertainty in learning. This allows us to use a more general link function \u03a6; (3) We design a new regret decomposition technique to accommodate preference-based feedback. Particularly, we decompose regret to characterizes the reward difference between \u03c00t and \u03c0 1 t : RegretT \u2272 \u2211T t=1(V t\u2212 V\u0303t)\u2212 (V \u03c0 0 t \u2212V \u03c01t ) where V t is an estimate of V \u03c0 0 t , and V\u0303t := E\u03c4\u223c\u03c01t [ \u2211H h=1 \u03d5(sh, ah) \u22a4\u03b8r,t] is an estimate of V \u03c0 1 t under the real transition and the learned reward model. This is different from standard RL (Zanette et al., 2020), and is necessary since we cannot guarantee the learned reward model will be accurate in a state-action-wise manner under the preference-based feedback. (4) Our algorithms have a new randomized active learning procedure for reducing the number of queries, and our analysis achieves a near-optimal tradeoff between regret and query complexity; (5) In every round t, we propose to draw a pair of trajectories where one is from the current greedy policy \u03c00t and the other is from the greedy policy of the previous round, \u03c00t\u22121. This ensures \u03c0 1 t is conditionally independent of the Gaussian noises at round t, which is the key to optimism (with a constant probability).\nRunning time. To assess the time complexity of Algorithm 1, assuming finite number of actions2, all steps can be computed in polynomial running time (i.e., polynomial in d,H,A) except the MLE of the reward model (Line 3), which depends on the link function \u03a6. For the popular BTL model where \u03a6(x) = 1/(1 + exp(\u2212x)), the MLE objective is concave with respect to \u03b8 and \u03b8 belongs to a convex set \u0398B . In this case, we can use any convex programming algorithms for the MLE procedure (e.g., projected gradient ascent)."
        },
        {
            "heading": "5 A MODEL-BASED THOMPSON SAMPLING ALGORITHM",
            "text": "In this section, we aim to extend to nonlinear function approximation. We do so in a model-based framework with Thompson sampling (TS). The motivation is that TS is often considered a computationally more tractable alternative to UCB-style algorithms."
        },
        {
            "heading": "5.1 ALGORITHM",
            "text": "The algorithm, called PbTS, is presented in Algorithm 2. At the beginning of episode k, it computes the reward model posterior \u03c1r,t and the transition model posterior \u03c1P,t (Line 3). Then, it samples Pt and rt from the posteriors and computes the optimal policy \u03c00t assuming the true reward function is rt and the true model is Pt (Line 5). Here we denote V \u03c0r,P as the state-value function of \u03c0 under reward function r and model P . Note that this oracle is a standard planning oracle. The comparator policy \u03c01t is simply set to be the policy from the previous episode, \u03c0 0 t\u22121, as we did in Algorithm 1. The two policies then generate respective trajectories \u03c40t and \u03c4 1 t . To decide whether we should make a query, we compute the uncertainty quantity under the posterior distribution of the reward: Er,r\u2032\u223c\u03c1r,t [|r(\u03c40t )\u2212 r(\u03c41t )\u2212 (r\u2032(\u03c40t )\u2212 r\u2032(\u03c41t ))|], which is analogous to (2) in Algorithm 1. We make a query only when it is larger than a threshold \u03f5. Similar to Algorithm 1, we can approximate this expectation by sampling polynomial many pairs of r and r\u2032 and then compute the empirical average."
        },
        {
            "heading": "5.2 ANALYSIS",
            "text": "The theoretical results of Algorithm 2 should rely on the complexity of the reward and the transition model. In our analysis, we employ two complexity measures \u2014 eluder dimension and bracketing number. We start by introducing a generic notion of \u2113p-eluder dimension (Russo & Van Roy, 2013).\nDefinition 5.1 (\u2113p-norm \u03f5-dependence). Let p > 0. Let X and Y be two sets and d(\u00b7, \u00b7) be a distance function on Y . Let F \u2286 X \u2192 Y be a function class. We say an element x \u2208 X is \u2113p-norm \u03f5-dependent on {x1, x2, . . . , xn} \u2286 X with respect to F and d if any pair of functions f, f \u2032 \u2208 F satisfying \u2211n i=1 d p(f(xi), f \u2032(xi)) \u2264 \u03f5p also satisfies d(f(x), f \u2032(x)) \u2264 \u03f5. Otherwise, we say x is \u2113p-norm \u03f5-independent of {x1, x2, . . . , xn}. Definition 5.2 (\u2113p-norm eluder dimension). The \u2113p-norm \u03f5-eluder dimension of function class F \u2286 X \u2192 Y , denoted by dimp(F , \u03f5, d), is the length of the longest sequence of elements in X satisfying that there exists \u03f5\u2032 \u2265 \u03f5 such that every element in the sequence is \u2113p-norm \u03f5\u2032-independent of its predecessors.\n2This is to ensure that argmaxa Q(s, a) can be computed efficiently.\nAlgorithm 2 Preference-based Thompson Sampling (PbTS) Require: priors \u03c1P and \u03c1r, threshold \u03f5.\n1: Let \u03c000 be an arbitrary policy. 2: for t = 1, . . . , T do 3: Compute posteriors:\n\u03c1P,t(P ) \u221d \u03c1P(P ) t\u22121\u220f i=1 H\u220f h=1 P (s0i,h+1 | s0i,h, a0i,h),\n\u03c1r,t(r) \u221d \u03c1r(r) t\u22121\u220f i=1 ( oi\u03a6 ( r(\u03c41i )\u2212 r(\u03c40i ) ) + (1\u2212 oi)\u03a6 ( r(\u03c40i )\u2212 r(\u03c41i ) ))Zi .\n4: Sample Pt \u223c \u03c1P,t and rt \u223c \u03c1r,t. 5: Compute \u03c00t \u2190 argmax\u03c0 V \u03c0rt,Pt(s1) and \u03c0 1 t \u2190 \u03c00t\u22121. 6: Sample \u03c40t \u223c \u03c00t and \u03c41t \u223c \u03c01t . 7: Zt \u2190 1{Er,r\u2032\u223c\u03c1r,t [|r(\u03c40t )\u2212 r(\u03c41t )\u2212 (r\u2032(\u03c40t )\u2212 r\u2032(\u03c41t ))|] > \u03f5} 8: if Zt = 1 then 9: Query preference feedback ot on {\u03c40t , \u03c41t }\n10: end if 11: end for\nThe eluder dimension is non-decreasing in p, i.e., dimp(F , \u03f5, d) \u2264 dimq(F , \u03f5, d) for any p \u2264 q. In the analysis, we will focus on \u21131- and \u21132-norm eluder dimension, which have been used in nonlinear bandits and RL extensively (Wen & Van Roy, 2013; Osband & Van Roy, 2014a; Jain et al., 2015; Wang et al., 2020; Ayoub et al., 2020; Foster et al., 2021; Ishfaq et al., 2021; Chen et al., 2022; Liu et al., 2022a; Sekhari et al., 2023a;b). Examples where eluder dimension is small include linear functions, generalized linear models, and functions in Reproducing Kernel Hilbert Space (RKHS).\nThe other complexity measure we use is the bracketing number (Van de Geer, 2000). Definition 5.3 (Bracketing number). Consider a function class F \u2286 X \u2192 R. Given two functions l, u : X \u2192 R, the bracket [l, u] is defined as the set of functions f \u2208 F with l(x) \u2264 f(x) \u2264 u(x) for all x \u2208 X . It is called an \u03c9-bracket if \u2225l\u2212u\u2225 \u2264 \u03c9. The bracketing number of F w.r.t. the metric \u2225 \u00b7 \u2225, denoted by N[](\u03c9,F , \u2225 \u00b7 \u2225), is the minimum number of \u03c9-brackets needed to cover F .\nThe logarithm of the bracketing number is small in many common scenarios, which has been extensively examined by previous studies (e.g., Van de Geer (2000)) for deriving MLE generalization bound (Agarwal et al., 2020; Uehara & Sun, 2021; Liu et al., 2022b; 2023). For example, when F is finite, the bracketing number is bounded by its size. When F is a d-dimensional linear function class, the logarithm of the bracketing number is upper bounded by d up to logarithmic factors.\nIt is worth noting that while we will employ both measures to the model classP , we can not similarly apply them to the reward class R. Instead, we have to rely on the complexity of the following function class, which comprises functions mapping pairs of trajectories to reward differences:\nR\u0303 := { r\u0303 : r\u0303(\u03c40, \u03c41) = H\u2211 h=1 r(s0h, a 0 h)\u2212 r(s1h, a1h), \u2200\u03c4 i = {sih, aih}h, i \u2208 {0, 1}, r \u2208 R } . (3)\nWe have to use R\u0303 instead of R because we only receive preference feedback, and thus we cannot guarantee that the learned reward model is accurate state-action-wise. Now we are ready to state our main results. The proofs are provided in Appendix C. Theorem 5.4. PbTS (Algorithm 2) guarantees that\nBayesRegretT = O\u0303 ( T\u03f5+H2 \u00b7 dim1 ( P, 1/T ) \u00b7 \u221a T \u00b7 \u03b9P + \u03ba \u00b7 dim1 ( R\u0303, 1/T ) \u00b7 \u221a T \u00b7 \u03b9R ) ,\nBayesQueriesT = O\u0303\n( min { \u03ba \u221a T \u00b7 \u03b9R \u03f5 \u00b7 dim1 ( R\u0303, \u03f5/2 ) , \u03ba2 \u00b7 \u03b9R \u03f52 \u00b7 dim2 ( R\u0303, \u03f5/2 )}) where we denote \u03b9P := log(N[]((HT |S|)\u22121,P, \u2225 \u00b7 \u2225\u221e)) and \u03b9R := log(N[](\u03ba(2T )\u22121, R\u0303, \u2225 \u00b7 \u2225\u221e)).\nSimilar to the analysis of Algorithm 1, we study the balance between the Bayesian regret and the query complexity by setting \u03f5 = T\u2212\u03b2 for some \u03b2 \u2264 1/2. Then, we can simplify the bounds into BayesRegretT = O\u0303(T 1\u2212\u03b2) and\nBayesQueriesT = O\u0303\n( min { T \u03b2+ 1 2 \u00b7 dim1 ( R\u0303, \u03f5/2 ) \ufe38 \ufe37\ufe37 \ufe38\n(i)\n, T 2\u03b2 \u00b7 dim2 ( R\u0303, \u03f5/2 ) \ufe38 \ufe37\ufe37 \ufe38\n(ii)\n})\nwhere we have hidden factors except T and the eluder dimension for brevity. We see that there is again a tradeoff in T between the Bayesian regret and the query complexity, similar to the one in Theorem 4.2. Term (ii) demonstrates that the tradeoff in T is again optimal, evidenced by the lower bound (Theorem 4.3). Moreover, term (i) further improves the dependence on the eluder dimension (recalling that \u21131-norm version is smaller than the \u21132-norm version). However, the T -dependence is worse. It is desired to derive a query complexity upper bound that scales as O\u0303(T 2\u03b2 \u00b7dim1(R\u0303, \u03f5/2)), attaining the favorable dependence on both T and the eluder dimension. We leave it as future work.\nWe emphasize that the Bayesian regret analysis in Theorem 5.4 is not a simple extension of previous TS works. We highlight four main differences: (1) The feedback is preference-based, which necessitates a new Bayesian regret decomposition:\nBayesRegretT = T\u2211 t=0 E [ V \u03c00t rt,Pt \u2212 V \u03c0 0 t rt,P\u22c6 ] \ufe38 \ufe37\ufe37 \ufe38\nTmodel\n+ T\u2211 t=0 E [( V \u03c00t rt,P\u22c6 \u2212 V \u03c0 1 t rt,P\u22c6 ) \u2212 ( V \u03c00t r\u22c6,P\u22c6 \u2212 V \u03c01t r\u22c6,P\u22c6 )] \ufe38 \ufe37\ufe37 \ufe38\nTreward\n.\nHere Tmodel and Treward are the respective regret incurred due to model and reward misspecification. We highlight that Treward characterizes the misspecification in terms of the reward difference between \u03c00t and \u03c0 1 t , which is different from the standard Bayesian RL. (2) Unlike prior works (Russo & Van Roy, 2014), we do not rely on upper confidence bounds (UCB) or optimism. Instead, we construct version spaces by classic MLE generalization bound. Taking the reward learning as an example, given the preference data {\u03c40i , \u03c41i , oi} t\u22121 i=1 , we construct the version space at round t as\nVt = { r \u2208 R :\nt\u22121\u2211 i=1 d2TV ( Pr(\u00b7 | \u03c41i , \u03c40i , r\u0302t), Pr(\u00b7 | \u03c41i , \u03c40i , r) ) \u2264 \u03b2 } where r\u0302t := argmaxr log \u2211t\u22121 i=1 Pr(oi | \u03c41i , \u03c40i , r) is the MLE from the preference data and \u03b2 is tuned appropriately to ensure r\u22c6 \u2208 Vt with high probability. We then show the posterior probability of rt and r\u22c6 not belonging to Vt is small. (3) Our analysis uses the tighter \u21131-norm eluder dimension, which is strictly better than the \u21132-norm eluder dimension used in prior work. (4) We also equipped it with a randomized active learning procedure for query complexity minimization.\nComputation. The computational bottleneck of Algorithm 2 lies in the computation of the posterior distribution (Line 3). Prior TS works have used Bootstrapping to approximate posterior sampling (Osband et al., 2016a; 2023) and achieved competitive performance in common RL benchmarks.\nNon-Markovian reward. Algorithm 2 can also be applied to non-Markovian reward (i.e., reward model is trajectory-wise) without any change. Here we consider Markovian reward for the consistency with Algorithm 1 and for the purpose of using a standard planning oracle for computing an optimal policy from a reward and transition model. While non-Markovian reward is more general, it is unclear how to solve the planning problem efficiently even in tabular MDPs. This computational intractability makes non-Markovian rewards not easily applicable in practice.\nExtension to SEC. In Appendix C.4, we extend the eluder dimension in Theorem 5.4 to the Sequential Extrapolation Coefficient (SEC) (Xie et al., 2022), which is more general."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "We use randomization to design algorithms for RL with preference-based feedback. Randomization allows us to minimize regret and query complexity while at the same time maintaining computation efficiency. For linear models, our algorithms achieve a near-optimal balance between the worst-case reward regret and query complexity with computational efficiency. For models beyond linear, using eluder dimension, we present a TS-inspired algorithm that balances Bayesian regret and Bayesian query complexity nearly optimally."
        },
        {
            "heading": "ACKNOWLEDGEMENTS",
            "text": "Both RW and WS acknowledge support from NSF IIS-2154711, NSF CAREER 2339395, and Cornell Infosys Collaboration."
        },
        {
            "heading": "A COMPUTATIONAL CHALLENGES IN RL WITH PREFERENCE-BASED FEEDBACK",
            "text": "For RL with preference-based feedback, there is currently no algorithm that achieves sublinear worst-case regret and computational efficiency simultaneously, even for tabular MDPs. In this section, we discuss the challenges that hindered previous works from being computationally efficient. Specifically, the reasons are twofold:\n(1) Trajectory-wise information. The feedback is trajectory-wise, meaning that we only receive information about cumulative rewards instead of per-step rewards. In this case, there is no longer a ground truth per-step reward signal. To see this, consider an MDP with two steps and a trajectory with a cumulative reward of 1. Then, we cannot decide the respective per-step reward for the two steps \u2014 it could be that the first step has reward 1 and the second has reward 0, or both have reward 0.5. The preference-based feedback is strictly harder than trajectory-wise reward feedback, and thus the same issue persists. This invalidates all algorithms relying on per-step reward information (e.g., UCBVI (Azar et al., 2017)) and necessitates leveraging feedback signal at a trajectory level. However, trajectory-level approaches typically entail maintaining a version space via trajectory constraints (Saha et al., 2023; Chen et al., 2022; Zhan et al., 2023a), and the computational complexity of searching within this version space is at least exponential in the length of the episode. Some works circumvent this computational obstacle by additional assumptions (e.g., explorability (Chatterji et al., 2021)), which are restrictive and do not generally hold even in tabular MDPs.\n(2) Preference-based information. The feedback relies on a pair of policies. Standard algorithms based on a single policy become computationally intractable when adapting to this setting since optimizing over a pair of policies simultaneously is qualitatively different. For example, Zhan et al. (2023b); Saha et al. (2023) use the idea from optimal design and need the computation oracle: argmax\u03c0,\u03c0\u2032\u2208\u03a0 \u2225Es,a\u223c\u03c0\u03d5(s, a)\u2212 Es,a\u223c\u03c0\u2032\u03d5(s, a)\u2225A for some positive definite matrix A. Here \u2225x\u22252A := x\u22a4Ax, and \u03d5 is some state-action wise feature.3 It is unclear how to implement this oracle since standard planning approaches relying on dynamic programming cannot be applied here. Additionally, these methods also require actively maintaining a policy space \u03a0 by eliminating potentially sub-optimal policies. The policy class can be exponentially large even in tabular settings, so it is unclear how to maintain a valid policy space in a computationally tractable manner.\nThese challenges motivate us to devise a novel algorithm using a technique distinct from previous approaches. Our solution centers around the concept of randomization, which allows us to balance exploration and exploitation and thus enable standard efficient computation oracles (e.g., DP-style planning oracle like value iteration)."
        },
        {
            "heading": "B PROOF OF THEOREM 4.2",
            "text": ""
        },
        {
            "heading": "B.1 NOTATIONS",
            "text": "We define some symbols and their values in Table 1. We have categorized them into four classes for the ease of reference.\nThe concept of covering number is defined below, which will be used to bound the statistical error of our algorithm.\nDefinition B.1 (Covering number). Consider a function class F \u2286 X \u2192 R. The \u03c9-cover of a function f\u0302 \u2208 F is defined as the set of functions f \u2208 F for which \u2225f \u2212 f\u0302\u2225 \u2264 \u03c9. The covering number of F w.r.t. the metric \u2225 \u00b7 \u2225 denoted by N(\u03c9,F , \u2225 \u00b7 \u2225) is the minimum number of \u03c9-covers needed to cover F .\n3These prior work typically assume trajectory wise feature \u03d5(\u03c4) for a state-action wise trajectory \u03c4 . However, even when specializing to state-action-wise features, these algorithms are still not computationally tractable even in tabular MDPs."
        },
        {
            "heading": "B.2 SUPPORTING LEMMAS",
            "text": "Lemma B.2 (Covering number of Euclidean balls). (Pollard, 1990) Let \u0398B := {\u03b8 \u2208 Rd : \u2225\u03b8\u22252 \u2264 B}. Then we have N(\u03c9,\u0398B , \u2225 \u00b7 \u22252) \u2264 (3B/\u03c9)d. Lemma B.3. There exists \u03c7 = O(log(\u03f5P,\u03be + \u03f5P,\u03b7 + \u03f5\u03bb)) such that \u03f5\u2032P,\u03b7 \u2264 \u03f5P,\u03b7 (recalling that \u03c7 is in the definition of \u03f5P,\u03b7).\nProof of Lemma B.3. By definition, we have\n\u03f5\u2032P,\u03b7 = 6\u221a \u03bb + 16dVmax\n\u221a \u03b9\u03f5 \u2212 log ( (\u03b1U \u2212 \u03b1L)\u03b4\u03bb ) =\n6\u221a \u03bb + 16dVmax\n\u221a \u03b9\u03f5 \u2212 log(\u03b4\u03bb)\u2212 log(\u03b1U \u2212 \u03b1L)\n= 6\u221a \u03bb + 16dVmax\n\u221a \u03b9\u03f5 \u2212 log(\u03b4\u03bb) + log 2(\u03f5P,\u03be + \u03f5P,\u03b7 + \u03f5\u03bb)\n\u2264 \u03c7 (\n6\u221a \u03bb + 16dVmax\n\u221a \u03b9\u03f5 \u2212 log(\u03b4\u03bb) ) = \u03f5P,\u03b7.\nHere the third equality is by the definition of \u03b1L and \u03b1U. The inequality holds by setting \u03c7 = O(log(\u03f5P,\u03be + \u03f5P,\u03b7 + \u03f5\u03bb)) to be large enough.\nLemma B.4. It holds that \u03b7P,t,h + \u03bbt,h = \u03b8\u0302P,t,h \u2212 \u03b8\u22c6P,t,h for any t \u2208 [T ] and h \u2208 [H].\nProof of Lemma B.4. By definition, we have \u03b7P,t,h + \u03bbt,h\n= \u03a3\u22121t\u22121,h ( t\u22121\u2211 i=1 \u03d5(si,h, ai,h) ( V t,h+1(si,h+1)\u2212 E sh+1 [ V t,h+1(si,h+1) \u2223\u2223 si,h, ai,h]))\n+ \u03bb\u03a3\u22121t\u22121,h \u222b s\u2032 \u00b5\u22c6h(s \u2032)V t,h+1(s \u2032) ds\u2032\n= \u03b8\u0302P,t,h \u2212 \u03a3\u22121t\u22121,h ( t\u22121\u2211 i=1 \u03d5(si,h, ai,h)\u03d5(si,h, ai,h) + \u03bbI )\u22a4 \u222b s\u2032 \u00b5\u22c6h(s \u2032) [ V t,h+1(s \u2032) ] ds\u2032\n= \u03b8\u0302P,t,h \u2212 \u222b s\u2032 \u00b5\u22c6h(s \u2032) [ V t,h+1(s \u2032) ] ds\u2032\n= \u03b8\u0302P,t,h \u2212 \u03b8\u22c6P,t,h.\nThe following lemma is adapted from Zanette et al. (2020, Lemma 1) to our setting. Lemma B.5 (One-step decomposition). For any t \u2208 [T ], h \u2208 [H], s \u2208 S, a \u2208 A, and policy \u03c0, we have \u03d5(s, a)\u22a4 ( \u03b8r,t + \u03b8P,t,h ) \u2212Q\u03c0h(s, a)\n= \u03d5(s, a)\u22a4(\u03ber,t + \u03beP,t,h + \u03b7r,t + \u03b7P,t,h \u2212 \u03bbt,h) + E s\u2032\n[ V t,h+1(s \u2032)\u2212 V \u03c0h+1(s\u2032) \u2223\u2223 s, a] .\nProof. We have \u03d5(s, a)\u22a4 ( \u03b8r,t + \u03b8P,t,h ) \u2212Q\u03c0h(s, a)\n= \u03d5(s, a)\u22a4 ( \u03b8r,t + \u03b8P,t,h ) \u2212 \u03d5(s, a)\u22a4\u03b8\u22c6r \u2212 E\ns\u2032 [V \u03c0h+1(s \u2032) | s, a]\n= \u03d5(s, a)\u22a4(\u03b8r,t \u2212 \u03b8\u0302r,t) + \u03d5(s, a)\u22a4(\u03b8P,t,h \u2212 \u03b8\u0302P,t,h) + \u03d5(s, a)\u22a4(\u03b8\u0302r,t \u2212 \u03b8\u22c6r )\n+ \u03d5(s, a)\u22a4\u03b8\u0302P,t,h \u2212 E s\u2032 [V \u03c0h+1(s \u2032) | s, a].\nFor the last but one term, we note that\n\u03d5(s, a)\u22a4\u03b8\u0302P,t,h\n= \u03d5(s, a)\u22a4\u03a3\u22121t\u22121,h ( t\u22121\u2211 i=1 \u03d5(si,h, ai,h)V t,h+1(si,h+1) )\n= \u03d5(s, a)\u22a4\u03a3\u22121t\u22121,h ( t\u22121\u2211 i=1 \u03d5(si,h, ai,h) E sh+1 [ V t,h+1(si,h+1) \u2223\u2223 si,h, ai,h])+ \u03d5(s, a)\u22a4\u03b7P,t,h = \u03d5(s, a)\u22a4\u03a3\u22121t\u22121,h ( t\u22121\u2211 i=1 \u03d5(si,h, ai,h)\u03d5(si,h, ai,h) \u22a4 \u222b s\u2032 \u00b5\u22c6h(s \u2032)V t,h+1(s \u2032) ds\u2032 ) + \u03d5(s, a)\u22a4\u03b7P,t,h\n= \u03d5(s, a)\u22a4 \u222b s\u2032 \u00b5\u22c6h(s \u2032)V t,h+1(s \u2032) ds\u2032 \u2212 \u03bb\u03d5(s, a)\u22a4\u03a3\u22121t\u22121,h \u222b s\u2032 \u00b5\u22c6h(s \u2032)V t,h+1(s \u2032) ds\u2032 + \u03d5(s, a)\u22a4\u03b7P,t,h\n= E s\u2032\n[ V t,h+1(s \u2032) \u2223\u2223 s, a]\u2212 \u03d5(s, a)\u22a4\u03bbt,h + \u03d5(s, a)\u22a4\u03b7P,t,h.\nPlugging this back, we get \u03d5(s, a)\u22a4 ( \u03b8r,t + \u03b8P,t,h ) \u2212Q\u03c0h(s, a)\n= \u03d5(s, a)\u22a4(\u03b8r,t \u2212 \u03b8\u0302r,t) + \u03d5(s, a)\u22a4(\u03b8P,t,h \u2212 \u03b8\u0302P,t,h) + \u03d5(s, a)\u22a4(\u03b8\u0302r,t \u2212 \u03b8\u22c6r ) + E\ns\u2032\n[ V t,h+1(s \u2032)\u2212 V \u03c0h+1(s\u2032) \u2223\u2223 s, a]\u2212 \u03d5(s, a)\u22a4\u03bbt,h + \u03d5(s, a)\u22a4\u03b7P,t,h\n= \u03d5(s, a)\u22a4(\u03ber,t + \u03beP,t,h + \u03b7r,t + \u03b7P,t,h \u2212 \u03bbt,h) + E s\u2032\n[ V t,h+1(s \u2032)\u2212 V \u03c0h+1(s\u2032) \u2223\u2223 s, a] .\nThis completes the proof.\nLemma B.6. For any t \u2208 [T ], if Zt = 0, then we have\n\u2225\u03d5(\u03c40t )\u2212 \u03d5(\u03c41t )\u2225\u03a3\u22121t\u22121 \u2264 \u03f5 \u221a \u03c0/(2\u03c3r).\nProof. By the definition of Zt, we have\n\u03f5 \u2265 E \u03b80,\u03b81\u223cN (\u03b8\u0302r,t,\u03c32r\u03a3 \u22121 t\u22121) |(\u03d5(\u03c40t )\u2212 \u03d5(\u03c41t ))\u22a4(\u03b80 \u2212 \u03b81)|\n= E u0,u1\u223cN (0,Id)\n|(\u03d5(\u03c40t )\u2212 \u03d5(\u03c41t ))\u22a4\u03c3r\u03a3 \u22121/2 t\u22121 (u0 \u2212 u1)|\n= E u\u223cN (0,Id)\n|(\u03d5(\u03c40t )\u2212 \u03d5(\u03c41t ))\u22a4 \u221a 2\u03c3r\u03a3 \u22121/2 t\u22121 u|\n= \u03c3r \u221a 2 E u\u223cN (0,Id)\n\u221a u\u22a4\u03a3\n\u22121/2 t\u22121 (\u03d5(\u03c4 0 t )\u2212 \u03d5(\u03c41t ))(\u03d5(\u03c40t )\u2212 \u03d5(\u03c41t ))\u22a4\u03a3 \u22121/2 t\u22121 u\n= \u03c3r \u221a 2 E u\u223cN (0,Id) \u2225u\u2225 \u03a3 \u22121/2 t\u22121 (\u03d5(\u03c4 0 t )\u2212\u03d5(\u03c41t ))(\u03d5(\u03c40t )\u2212\u03d5(\u03c41t ))\u22a4\u03a3 \u22121/2 t\u22121 .\nWe then apply Lemma D.9 and obtain\n\u03f5 \u2265 \u03c3r \u221a 2 \u00b7 \u221a\u221a\u221a\u221a2 tr(\u03a3\u22121/2t\u22121 (\u03d5(\u03c40t )\u2212 \u03d5(\u03c41t ))(\u03d5(\u03c40t )\u2212 \u03d5(\u03c41t ))\u22a4\u03a3\u22121/2t\u22121 ) \u03c0\n= 2\u03c3r \u00b7\n\u221a (\u03d5(\u03c40t )\u2212 \u03d5(\u03c41t ))\u22a4\u03a3 \u22121/2 t\u22121 \u03a3 \u22121/2 t\u22121 (\u03d5(\u03c4 0 t )\u2212 \u03d5(\u03c41t ))\n\u03c0\n= 2\u03c3r\u221a \u03c0 \u00b7 \u2225\u03d5(\u03c40t )\u2212 \u03d5(\u03c41t )\u2225\u03a3\u22121t\u22121 .\nHence, we complete the proof."
        },
        {
            "heading": "B.3 STATISTICAL UPPER BOUNDS",
            "text": "Lemma B.7 (Gaussian noise). Each of the following holds with probability at least 1\u2212 \u03b4:\n1. \u2225\u03ber,t\u2225\u03a3t\u22121 \u2264 \u03f5r,\u03be for all t \u2208 [T ],\n2. \u2225\u03beP,t,h\u2225\u03a3t\u22121,h \u2264 \u03f5P,\u03be for all t \u2208 [T ] and h \u2208 [H].\nProof of Lemma B.7. It simply follows from Lemma D.3 and the union bound.\nLemma B.8. Fix t \u2208 [T ]. It holds with probability at least 1\u2212 \u03b4 that\nt\u22121\u2211 s=1 \u2223\u2223\u2223(\u03d5(\u03c40s )\u2212 \u03d5(\u03c41s ))\u22a4 (\u03b8\u22c6r \u2212 \u03b8\u0302r,t)\u2223\u2223\u22232 \u22648\nt\u22121\u2211 s=1 E \u03c40s ,\u03c4 1 s \u2223\u2223\u2223(\u03d5(\u03c40s )\u2212 \u03d5(\u03c41s ))\u22a4 (\u03b8\u22c6r \u2212 \u03b8\u0302r,t)\u2223\u2223\u22232 + 168B2d log(3TB/\u03b4). Here the expectation is taken over the randomness of sampling \u03c40s and \u03c4 1 s from \u03c0 0 s and \u03c0 1 s , respectively.\nProof of Lemma B.8. Let \u0398\u0303B denote an \u03c9-covering of \u0398B with respect to \u2225 \u00b7 \u22252 for some \u03c9 > 0. By Lemma B.2, we have |\u0398\u0303B | \u2264 (3B/\u03c9)d. Moreover, for any s \u2208 [t \u2212 1], by Cauchy-Schwarz inequality, it holds that\u2223\u2223\u2223(\u03d5(\u03c40s )\u2212 \u03d5(\u03c41s ))\u22a4 (\u03b8\u22c6r \u2212 \u03b8\u0303)\u2223\u2223\u22232 \u2264 \u2225\u2225\u03d5(\u03c40s )\u2212 \u03d5(\u03c41s )\u2225\u222522 \u00b7 \u2225\u2225\u03b8\u22c6r \u2212 \u03b8\u0303\u2225\u222522 \u2264 16B2. Then, by Lemma D.4 and union bound over \u0398\u0303B , it holds that\nt\u22121\u2211 s=1 \u2223\u2223\u2223(\u03d5(\u03c40s )\u2212 \u03d5(\u03c41s ))\u22a4 (\u03b8\u22c6r \u2212 \u03b8\u0303)\u2223\u2223\u22232 \u22642\nt\u22121\u2211 s=1 E \u03c40s ,\u03c4 1 s \u2223\u2223\u2223(\u03d5(\u03c40s )\u2212 \u03d5(\u03c41s ))\u22a4 (\u03b8\u22c6r \u2212 \u03b8\u0303)\u2223\u2223\u22232 + 64B2d log(3B/(\u03c9\u03b4)) (4) for all \u03b8\u0303 \u2208 \u0398\u0303B with probability at least 1 \u2212 \u03b4. Now we consider an arbitrary \u03b8 \u2208 \u0398B and let \u03b8\u0303 \u2208 \u0398\u0303B be the closest to \u03b8. Then, by triangle inequality and Cauchy-Schwarz inequality, we have the following two ineuqalities:\u2223\u2223\u2223(\u03d5(\u03c40s )\u2212 \u03d5(\u03c41s ))\u22a4 (\u03b8\u22c6r \u2212 \u03b8)\u2223\u2223\u22232\n\u2264 2 \u2223\u2223\u2223(\u03d5(\u03c40s )\u2212 \u03d5(\u03c41s ))\u22a4 (\u03b8\u22c6r \u2212 \u03b8\u0303)\u2223\u2223\u22232 + 2\u2225\u03d5(\u03c40s )\u2212 \u03d5(\u03c41s )\u222522\u2225\u03b8 \u2212 \u03b8\u0303\u222522\n\u2264 2 \u2223\u2223\u2223(\u03d5(\u03c40s )\u2212 \u03d5(\u03c41s ))\u22a4 (\u03b8\u22c6r \u2212 \u03b8\u0303)\u2223\u2223\u22232 + 8\u03c92,\nand \u2223\u2223\u2223(\u03d5(\u03c40s )\u2212 \u03d5(\u03c41s ))\u22a4 (\u03b8\u22c6r \u2212 \u03b8\u0303)\u2223\u2223\u22232 \u2264 2\n\u2223\u2223\u2223(\u03d5(\u03c40s )\u2212 \u03d5(\u03c41s ))\u22a4 (\u03b8\u22c6r \u2212 \u03b8)\u2223\u2223\u22232 + 2\u2225\u03d5(\u03c40s )\u2212 \u03d5(\u03c41s )\u222522\u2225\u03b8 \u2212 \u03b8\u0303\u222522 \u2264 2 \u2223\u2223\u2223(\u03d5(\u03c40s )\u2212 \u03d5(\u03c41s ))\u22a4 (\u03b8\u22c6r \u2212 \u03b8)\u2223\u2223\u22232 + 8\u03c92\nApplying these inequalities and (4), we get\nt\u22121\u2211 s=1 \u2223\u2223\u2223(\u03d5(\u03c40s )\u2212 \u03d5(\u03c41s ))\u22a4 (\u03b8\u22c6r \u2212 \u03b8)\u2223\u2223\u22232 \u22642\nt\u22121\u2211 s=1 \u2223\u2223\u2223(\u03d5(\u03c40s )\u2212 \u03d5(\u03c41s ))\u22a4 (\u03b8\u22c6r \u2212 \u03b8\u0303)\u2223\u2223\u22232 + 8\u03c92t \u22644\nt\u22121\u2211 s=1 E \u03c40s ,\u03c4 1 s \u2223\u2223\u2223(\u03d5(\u03c40s )\u2212 \u03d5(\u03c41s ))\u22a4 (\u03b8\u22c6r \u2212 \u03b8\u0303)\u2223\u2223\u22232 + 128B2d log(3B/(\u03c9\u03b4)) + 8\u03c92t \u22648\nt\u22121\u2211 s=1 E \u03c40s ,\u03c4 1 s \u2223\u2223\u2223(\u03d5(\u03c40s )\u2212 \u03d5(\u03c41s ))\u22a4 (\u03b8\u22c6r \u2212 \u03b8)\u2223\u2223\u22232 + 128B2d log(3B/(\u03c9\u03b4)) + 40\u03c92t \u22648\nt\u22121\u2211 s=1 E \u03c40s ,\u03c4 1 s \u2223\u2223\u2223(\u03d5(\u03c40s )\u2212 \u03d5(\u03c41s ))\u22a4 (\u03b8\u22c6r \u2212 \u03b8)\u2223\u2223\u22232 + 168B2d log(3TB/\u03b4) where the last step holds by setting \u03c9 = 1/t.\nLemma B.9 (Reward estimation error). It holds with probability at least 1\u2212\u03b4 that \u2225\u03b7r,t\u2225\u03a3t\u22121 \u2264 \u03f5r,\u03b7 for all t \u2208 [T ].\nProof of Lemma B.9. Since \u03b8\u0302r,t is the MLE, by Lemma D.10, we have\nt\u22121\u2211 s=1 E \u03c40s ,\u03c4 1 s d2TV ( Pr ( \u00b7 \u2223\u2223 \u03c40s , \u03c41s , \u03b8\u22c6r ) ,Pr(\u00b7 \u2223\u2223\u2223 \u03c40s , \u03c41s , \u03b8\u0302r,t)) \u2264 10 log (N[]((2T )\u22121,R, \u2225 \u00b7 \u2225\u221e)/\u03b4)\n(5)\nwith probability at least 1 \u2212 \u03b4, where R denotes the set of probability distributions of preference feedback and Pr(\u00b7) denotes the preference feedback generating probability. Now we upper bound the covering number ofR by the covering number of \u0398B . We notice the following:\nsup o,\u03c40,\u03c41 \u2223\u2223\u2223Pr (o \u2223\u2223 \u03c40, \u03c41, \u03b8\u22c6r )\u2212 Pr(o \u2223\u2223\u2223 \u03c40, \u03c41, \u03b8\u0302r,t)\u2223\u2223\u2223 = sup\n\u03c40,\u03c41 \u2223\u2223\u2223\u03a6((\u03d5(\u03c40)\u2212 \u03d5(\u03c41))\u22a4\u03b8\u22c6r)\u2212 \u03a6((\u03d5(\u03c40)\u2212 \u03d5(\u03c41))\u22a4\u03b8\u0302r,t)\u2223\u2223\u2223 \u2264\u03ba\u22121 sup\n\u03c40,\u03c41 \u2223\u2223\u2223(\u03d5(\u03c40)\u2212 \u03d5(\u03c41))\u22a4(\u03b8\u22c6r \u2212 \u03b8\u0302r,t)\u2223\u2223\u2223 \u2264\u03ba\u22121 sup\n\u03c40,\u03c41 \u2225\u03d5(\u03c40)\u2212 \u03d5(\u03c41)\u22252\u2225\u03b8\u22c6r \u2212 \u03b8\u0302r,t\u22252\n\u22642\u03ba\u22121\u2225\u03b8\u22c6r \u2212 \u03b8\u0302r,t\u22252\nwhere the equality holds since the feedback o is binary, the first inequality is Lemma D.1, and the last inequality is by the condition that \u2225\u03d5(\u03c4)\u22252 \u2264 1 for any trajectory \u03c4 . This means that an \u03c9-covering of the space of parameter \u03b8 implies a 2\u03c9\u03ba\u22121-covering of the space of the probability distribution of the preference feedbacks. Hence, (5) can be further upper bounded by\nt\u22121\u2211 s=1 E \u03c40s ,\u03c4 1 s d2TV ( Pr ( \u00b7 \u2223\u2223 \u03c40s , \u03c41s , \u03b8\u22c6r ) ,Pr(\u00b7 \u2223\u2223\u2223 \u03c40s , \u03c41s , \u03b8\u0302r,t))\n\u2264 10 log ( N[] ( (2T )\u22121,R, \u2225 \u00b7 \u2225\u221e ) /\u03b4 )\n\u2264 10 log ( N ( \u03ba/(4T ),\u0398B , \u2225 \u00b7 \u22252 ) /\u03b4 )\n\u2264 10d log ( 12BT/(\u03ba\u03b4) ) (6)\nwhere the last inequality is Lemma B.2. For the left side, we note that\nt\u22121\u2211 s=1 E \u03c40s ,\u03c4 1 s d2TV ( Pr ( \u00b7 \u2223\u2223 \u03c40s , \u03c41s , \u03b8\u22c6r ) ,Pr(\u00b7 \u2223\u2223\u2223 \u03c40s , \u03c41s , \u03b8\u0302r,t))\n= t\u22121\u2211 s=1 E \u03c40s ,\u03c4 1 s \u2223\u2223\u2223\u03a6((\u03d5(\u03c40s )\u2212 \u03d5(\u03c41s ))\u22a4\u03b8\u22c6r)\u2212 \u03a6((\u03d5(\u03c40s )\u2212 \u03d5(\u03c41s ))\u22a4\u03b8\u0302r,t)\u2223\u2223\u22232 \u2265\u03ba\u22121\nt\u22121\u2211 s=1 E \u03c40s ,\u03c4 1 s \u2223\u2223\u2223(\u03d5(\u03c40s )\u2212 \u03d5(\u03c41s ))\u22a4 (\u03b8\u22c6r \u2212 \u03b8\u0302r,t)\u2223\u2223\u22232 \u22651 8 \u03ba\u22121 t\u22121\u2211 s=1\n\u2223\u2223\u2223(\u03d5(\u03c40s )\u2212 \u03d5(\u03c41s ))\u22a4 (\u03b8\u22c6r \u2212 \u03b8\u0302r,t)\u2223\u2223\u22232 \u2212 21\u03ba\u22121B2d log(3TB/\u03b4) (7) with probability at least 1\u2212 \u03b4, where the first inequality is by Lemma D.1 and the last inequality is by Lemma B.8. Furthermore, we have\nt\u22121\u2211 s=1 \u2223\u2223\u2223(\u03d5(\u03c40s )\u2212 \u03d5(\u03c41s ))\u22a4 (\u03b8\u22c6r \u2212 \u03b8\u0302r,t)\u2223\u2223\u22232 = ( \u03b8\u22c6r \u2212 \u03b8\u0302r,t\n)\u22a4 t\u22121\u2211 s=1 ( \u03d5(\u03c40s )\u2212 \u03d5(\u03c41s ) )( \u03d5(\u03c40s )\u2212 \u03d5(\u03c41s ) )\u22a4 ( \u03b8\u22c6r \u2212 \u03b8\u0302r,t ) = ( \u03b8\u22c6r \u2212 \u03b8\u0302r,t )\u22a4 \u03a3t\u22121 ( \u03b8\u22c6r \u2212 \u03b8\u0302r,t ) \u2212 ( \u03b8\u22c6r \u2212 \u03b8\u0302r,t )\u22a4 \u03bbI ( \u03b8\u22c6r \u2212 \u03b8\u0302r,t ) \u2265 \u2225\u03b7r,t\u22252\u03a3t\u22121 \u2212 4\u03bbB 2 (8)\nPutting (6), (7), and (8) together, we get \u2225\u03b7r,t\u22252\u03a3t\u22121 \u2264 80\u03bad log ( 12BT/(\u03ba\u03b4) ) + 168B2d log(3TB/\u03b4) + 4\u03bbB2\nwith probability at least 1 \u2212 2\u03b4. Adjusting \u03b4 to \u03b4/2 and taking the union bound over t \u2208 [T ] yields the desired result.\nLemma B.10 (One-step transition estimation error). Assume the events defined in Lemma B.7 hold. Then, the following claim holds for all t \u2208 [T ] and h \u2208 [H] with probability at least 1 \u2212 \u03b4: if maxs |V t,h+1(s)| \u2264 Vmax, it holds that \u2225\u03b7P,t,h\u2225\u03a3t\u22121,h \u2264 \u03f5P,\u03b7.\nProof of Lemma B.10. By definition, we have\n\u2225\u03b7P,t,h\u2225\u03a3t\u22121,h = \u2225\u2225\u2225\u2225\u2225 t\u22121\u2211 i=1 \u03d5(si,h, ai,h) ( V t,h+1(si,h+1)\u2212 E si,h+1 [ V t,h+1(si,h+1) \u2223\u2223 si,h, ai,h]) \u2225\u2225\u2225\u2225\u2225 \u03a3\u22121t\u22121,h .\nTowards an upper bound of the above, we will conduct some covering arguments. To that end, we first derive the upper bounds of norms of \u03b8r,t and \u03b8P,t,h.\nBounding the \u21132-norms of \u03b8r,t and \u03b8P,t,h. For \u03b8r,t, applying triangle inequality, we have \u2225\u03b8r,t\u22252 \u2264 \u2225\u03ber,t\u22252 + \u2225\u03b8\u0302r,t\u22252 \u2264 \u2225\u03ber,t\u22252 +B. For \u2225\u03ber,t\u22252, we have\n\u2225\u03ber,t\u22252 = \u221a \u03be\u22a4r,t\u03a3 \u22121/2 t\u22121 \u03a3 1/2 t\u22121\u03ber,t \u2264 \u221a \u2225\u03ber,t\u2225\u03a3t\u22121\u2225\u03ber,t\u2225\u03a3\u22121t\u22121 \u2264 \u221a \u03f5r,\u03be \u00b7 \u2225\u03ber,t\u22252/ \u221a \u03bb\nwhere in the last step we applied Lemma B.7 and the fact that \u2225\u03ber,t\u2225\u03a3\u22121t\u22121 \u2264 \u2225\u03ber,t\u22252\u2225\u03a3 \u22121/2 t\u22121 \u22252 \u2264\n\u2225\u03ber,t\u22252/ \u221a \u03bb. This implies that \u2225\u03ber,t\u22252 \u2264 \u03f5r,\u03be/ \u221a \u03bb, and thus, \u2225\u03b8r,t\u22252 \u2264 \u03f5r,\u03be/ \u221a \u03bb+B.\nFor \u03b8P,t,h, we have \u2225\u03b8P,t,h\u22252 \u2264 \u2225\u03beP,t,h\u22252 + \u2225\u03b8\u0302P,t,h\u22252. For \u2225\u03beP,t,h\u22252, following a similar argument as above, we have\n\u2225\u03beP,t,h\u22252 \u2264 \u221a \u2225\u03beP,t,h\u2225\u03a3t\u22121,h\u2225\u03beP,t,h\u2225\u03a3\u22121t\u22121,h \u2264 \u221a \u03f5P,\u03be \u00b7 \u2225\u03beP,t,h\u22252/ \u221a \u03bb,\nwhich implies that \u2225\u03beP,t,h\u22252 \u2264 \u03f5P,\u03be/ \u221a \u03bb. For \u2225\u03b8\u0302P,t,h\u22252, we have\n\u2225\u03b8\u0302P,t,h\u22252 = \u2225\u2225\u2225\u2225\u2225\u03a3\u22121/2t\u22121,h\u03a3\u22121/2t\u22121,h ( t\u22121\u2211 i=1 \u03d5(si,h, ai,h)V t,h+1(si,h+1) )\u2225\u2225\u2225\u2225\u2225 2\n\u2264\u2225\u03a3\u22121/2t\u22121,h\u22252 \u00b7 \u2225\u2225\u2225\u2225\u2225 t\u22121\u2211 i=1 \u03d5(si,h, ai,h)V t,h+1(si,h+1) \u2225\u2225\u2225\u2225\u2225 \u03a3\u22121t\u22121,h\n\u2264 \u221a \u03bb\u22121 \u00b7 \u221a t \u00b7 \u221a\u221a\u221a\u221at\u22121\u2211 i=1\n\u2225\u2225\u03d5(si,h, ai,h)V t,h+1(si,h+1)\u2225\u22252\u03a3\u22121t\u22121,h \u2264 \u221a \u03bb\u22121 \u00b7 \u221a t \u00b7 Vmax \u00b7 \u221a\u221a\u221a\u221at\u22121\u2211 i=1 \u2225\u03d5(si,h, ai,h)\u22252\u03a3\u22121t\u22121,h \u2264Vmax \u221a dt\u221a\n\u03bb\nwhere the second inequality is by the Jensen\u2019s inequality, the third inequality is by the condition that maxs |V t,h(s)| \u2264 Vmax, and the last inequality is by Lemma D.7. Putting these together, we get \u2225\u03b8P,t,h\u22252 \u2264 \u03f5P,\u03be/ \u221a \u03bb+ Vmax \u221a dt/ \u221a \u03bb.\nCovering construction. With these bounds in hand, we can now proceed with a covering construction. First define\nQ\u03b8r,\u03b8P,\u03a3(s, a)\n:= \u03d5(s, a)\u22a4\u03b8r +  \u03d5(s, a)\u22a4\u03b8P if \u2225\u03d5(s, a)\u2225\u03a3 \u2264 \u03b1L \u03c1(s, a) ( \u03d5(s, a)\u22a4\u03b8P ) + (1\u2212 \u03c1(s, a))(H \u2212 h) if \u03b1L < \u2225\u03d5(s, a)\u2225\u03a3 \u2264 \u03b1U\nH \u2212 h if \u2225\u03d5(s, a)\u2225\u03a3 > \u03b1U\n,\nV\u03b8r,\u03b8P,\u03a3(s) := max a Q\u03b8r,\u03b8P,\u03a3(s, a). (9)\nwhere \u03c1(s, a) := \u03b1U\u2212\u2225\u03d5(s,a)\u2225\u03a3\u03b1U\u2212\u03b1L . This definition aims to mimic the behavior of Algorithm 1. Then, we define the space of parameters as follows\nU := { (\u03b8r, \u03b8P,\u03a3) : \u2225\u03b8r\u22252 \u2264 B + \u03f5r,\u03be/ \u221a \u03bb, \u2225\u03b8P\u22252 \u2264 (Vmax \u221a dt+ \u03f5P,\u03be)/ \u221a \u03bb,\n\u2225\u03a3\u2225F \u2264 \u221a d/\u03bb, \u03a3 = \u03a3\u22a4, \u03a3 \u2ab0 0, \u2225V\u03b8r,\u03b8P,\u03a3\u2225\u221e \u2264 Vmax } .\nClearly, \u03b8r,t and \u03b8P,t,h satisfy the constraints of \u03b8r and \u03b8P. Moreover, \u2225\u03a3\u22121t\u22121\u2225F \u2264 \u221a d\u2225\u03a3\u22121t\u22121\u22252 \u2264\u221a\nd/\u03bb, and thus \u03a3\u22121t\u22121 satisfies the constraint of \u03a3. Hence, we have (\u03b8r,t, \u03b8P,t,h,\u03a3t\u22121) \u2208 U .\nBy Lemma B.2, the size of an \u03c9-covering of U by treating it as a subset of R2d+d2 can be bounded by\nN(\u03c9,U , \u2225 \u00b7 \u22252) \u2264\n3 ( (B + \u03f5r,\u03be/ \u221a \u03bb) + (Vmax \u221a dt+ \u03f5P,\u03be)/ \u221a \u03bb+ \u221a d/\u03bb ) \u03c9 2d+d 2\nWe denote the covering set by U\u0303 . We will write N := N(\u03c9,U , \u2225 \u00b7\u22252) for simplicity when there is no ambiguity. We note that it is possible that not all points in U\u0303 belong to U . However, we can project all points to U so it becomes a 2\u03c9-covering of U . We will omit this subtlety.\nCovering argument. For any (\u03b8r, \u03b8P,\u03a3) \u2208 U , we define\nxi,\u03b8r,\u03b8P,\u03a3 := V\u03b8r,\u03b8P,\u03a3(si,h+1)\u2212 E s\u2032 [V\u03b8r,\u03b8P,\u03a3(s \u2032) | si,h, ai,h] .\nBy construction, we have |xi,\u03b8r,\u03b8P,\u03a3| \u2264 2Vmax, so it is 4Vmax-subgaussian conditioning on (si,h, ai,h). Hence, for all (\u03b8r, \u03b8P,\u03a3) \u2208 U\u0303 , we have\u2225\u2225\u2225\u2225\u2225 t\u22121\u2211 i=1 \u03d5(si,h, ai,h)xi,\u03b8r,\u03b8P,\u03a3 \u2225\u2225\u2225\u2225\u2225 \u03a3\u22121t\u22121,h \u2264 \u221a 32V 2max ( d log ( 1 + t \u03bb ) + log(N/\u03b4) ) (10)\nwith probability at least 1\u2212\u03b4 by Lemma D.5 and the union bound over U\u0303 . Now consider an arbitrary tuple (\u03b8r, \u03b8P,\u03a3) \u2208 U and the nearest point (\u03b8\u0303r, \u03b8\u0303P, \u03a3\u0303) \u2208 U\u0303 . Then, we have\u2225\u2225\u2225\u2225\u2225 t\u22121\u2211 i=1 \u03d5(si,h, ai,h)xi,\u03b8r,\u03b8P,\u03a3 \u2225\u2225\u2225\u2225\u2225 \u03a3\u22121t\u22121,h\n\u2264 \u2225\u2225\u2225\u2225\u2225 t\u22121\u2211 i=1 \u03d5(si,h, ai,h)xi,\u03b8\u0303r,\u03b8\u0303P,\u03a3\u0303 \u2225\u2225\u2225\u2225\u2225 \u03a3\u22121t\u22121,h + \u2225\u2225\u2225\u2225\u2225 t\u22121\u2211 i=1 \u03d5(si,h, ai,h)(xi,\u03b8r,\u03b8P,\u03a3 \u2212 xi,\u03b8\u0303r,\u03b8\u0303P,\u03a3\u0303) \u2225\u2225\u2225\u2225\u2225 \u03a3\u22121t\u22121,h\n\u2264 \u2225\u2225\u2225\u2225\u2225 t\u22121\u2211 i=1 \u03d5(si,h, ai,h)xi,\u03b8\u0303r,\u03b8\u0303P,\u03a3\u0303 \u2225\u2225\u2225\u2225\u2225 \u03a3\u22121t\u22121,h + t\u22121\u2211 i=1 \u2225\u03d5(si,h, ai,h)\u2225\u03a3\u22121t\u22121,h \u00b7maxi |xi,\u03b8r,\u03b8P,\u03a3 \u2212 xi,\u03b8\u0303r,\u03b8\u0303P,\u03a3\u0303|\n\u2264 \u2225\u2225\u2225\u2225\u2225 t\u22121\u2211 i=1 \u03d5(si,h, ai,h)xi,\u03b8\u0303r,\u03b8\u0303P,\u03a3\u0303 \u2225\u2225\u2225\u2225\u2225 \u03a3\u22121t\u22121,h + \u221a \u03bb\u22121 \u00b7 t \u00b7max i |xi,\u03b8r,\u03b8P,\u03a3 \u2212 xi,\u03b8\u0303r,\u03b8\u0303P,\u03a3\u0303|\nwhere the last step is by the fact that \u2225\u03d5(si,h, ai,h)\u2225\u03a3\u22121t\u22121,h \u2264 \u221a \u03bb\u22121. Observing the derived upper bound above, the first term is already bounded by (10) since (\u03b8\u0303r, \u03b8\u0303P, \u03a3\u0303) \u2208 U\u0303 . To bound the second term, we note that, by the definiton of xi,\u03b8r,\u03b8P,\u03a3 and xi,\u03b8\u0303r,\u03b8\u0303P,\u03a3\u0303,\n|xi,\u03b8r,\u03b8P,\u03a3 \u2212 xi,\u03b8\u0303r,\u03b8\u0303P,\u03a3\u0303| \u22642maxs \u2223\u2223\u2223V\u03b8r,\u03b8P,\u03a3(s)\u2212 V\u03b8\u0303r,\u03b8\u0303P,\u03a3\u0303(s)\u2223\u2223\u2223 =2max\ns \u2223\u2223\u2223max a Q\u03b8r,\u03b8P,\u03a3(s, a)\u2212max a Q\u03b8\u0303r,\u03b8\u0303P,\u03a3\u0303(s, a) \u2223\u2223\u2223\n\u22642max s,a \u2223\u2223\u2223Q\u03b8r,\u03b8P,\u03a3(s, a)\u2212Q\u03b8\u0303r,\u03b8\u0303P,\u03a3\u0303(s, a)\u2223\u2223\u2223 We assume \u03c9 \u2264 (\u03b1U \u2212 \u03b1L)2 (and this will be satisfied later when we specify the value of \u03c9). Recall that we have three cases in the definition of Q\u03b8r,\u03b8P,\u03a3(s, a) (see (9)), and we will refer to them as Case L, Case M, and Case U. There are in total 6 possible combinations of cases for the pair Q\u03b8r,\u03b8P,\u03a3(s, a), Q\u03b8\u0303r,\u03b8\u0303P,\u03a3\u0303(s, a), and we will discuss them one by one below. For the ease of notation, we denote Q(s, a) and Q\u0303(s, a) as shorthand for Q\u03b8r,\u03b8P,\u03a3(s, a) and Q\u03b8\u0303r,\u03b8\u0303P,\u03a3\u0303(s, a).\n(1) Case L + Case U. This is impossible, because, in this case, we have |\u2225\u03d5\u2225\u03a3 \u2212 \u2225\u03d5\u2225\u03a3\u0303| > \u03b1U \u2212 \u03b1L, but we also have |\u2225\u03d5\u2225\u03a3 \u2212 \u2225\u03d5\u2225\u03a3\u0303| = | \u221a \u03d5\u22a4\u03a3\u03d5 \u2212 \u221a \u03d5\u22a4\u03a3\u0303\u03d5| \u2264 \u221a |\u03d5\u22a4(\u03a3\u2212 \u03a3\u0303)\u03d5| \u2264\u221a\n\u2225\u03d5\u22252\u2225\u03a3\u2212 \u03a3\u0303\u2225F\u2225\u03d5\u22252 \u2264 \u221a \u03c9 < \u03b1U \u2212 \u03b1L.\n(2) Both are Case L. Then we immediately have |Q(s, a)\u2212 Q\u0303(s, a)| = \u2223\u2223\u2223\u03d5(s, a)\u22a4(\u03b8r + \u03b8P \u2212 \u03b8\u0303r \u2212 \u03b8\u0303P)\u2223\u2223\u2223 \u2264 \u2225\u03d5(s, a)\u22252(\u2225\u03b8r \u2212 \u03b8\u0303r\u22252 + \u2225\u03b8P \u2212 \u03b8\u0303P\u22252) \u2264 2\u03c9.\n(3) Both are Case U. Then we immediately have |Q(s, a)\u2212 Q\u0303(s, a)| = \u2223\u2223\u2223\u03d5(s, a)\u22a4(\u03b8r \u2212 \u03b8\u0303r)\u2223\u2223\u2223 \u2264 |\u03d5(s, a)\u22252 \u00b7 \u2225\u03b8r \u2212 \u03b8\u0303r\u22252 \u2264 \u03c9.\n(4) Case L + Case M. Without loss of generality, we assume Q(s, a) is in Case M and Q\u0303(s, a) is in Case L. Then we have |Q(s, a)\u2212 Q\u0303(s, a)| \u2264\u03c1(s, a) \u2223\u2223\u2223\u03d5(s, a)\u22a4(\u03b8r + \u03b8P \u2212 \u03b8\u0303r \u2212 \u03b8\u0303P)\u2223\u2223\u2223\n+ (1\u2212 \u03c1(s, a)) \u2223\u2223\u2223\u03d5(s, a)\u22a4(\u03b8\u0303r + \u03b8\u0303P)\u2212 (H \u2212 h)\u2223\u2223\u2223\n\u2264\u03c1(s, a) \u00b7 2\u03c9 + (1\u2212 \u03c1(s, a)) \u00b7 2Vmax\nRecall that \u03c1(s, a) := \u03b1U\u2212\u2225\u03d5(s,a)\u2225\u03a3\u03b1U\u2212\u03b1L , so 1\u2212 \u03c1(s, a) := \u2225\u03d5(s,a)\u2225\u03a3\u2212\u03b1L \u03b1U\u2212\u03b1L . Futhermore,\n1\u2212 \u03c1(s, a) = \u2225\u03d5(s, a)\u2225\u03a3 \u2212 \u03b1L \u03b1U \u2212 \u03b1L = \u2225\u03d5(s, a)\u2225\u03a3 \u2212 \u2225\u03d5(s, a)\u2225\u03a3\u0303 + \u2225\u03d5(s, a)\u2225\u03a3\u0303 \u2212 \u03b1L \u03b1U \u2212 \u03b1L\n\u2264 \u2225\u03d5(s, a)\u2225\u03a3 \u2212 \u2225\u03d5(s, a)\u2225\u03a3\u0303\n\u03b1U \u2212 \u03b1L =\n\u221a \u03d5\u22a4\u03a3\u03d5\u2212 \u221a \u03d5\u22a4\u03a3\u0303\u03d5\n\u03b1U \u2212 \u03b1L\n\u2264 \u221a |\u03d5\u22a4(\u03a3\u2212 \u03a3\u0303)\u03d5| \u03b1U \u2212 \u03b1L \u2264 \u221a \u2225\u03d5\u22252\u2225\u03a3\u2212 \u03a3\u0303\u22252\u2225\u03d5\u22252 \u03b1U \u2212 \u03b1L \u2264 \u221a \u03c9\n\u03b1U \u2212 \u03b1L where the first inequality is by the condition that Q\u0303 is in Case L. Inserting this back, we get |Q(s, a)\u2212 Q\u0303(s, a)| \u2264 2\u03c9 + 2Vmax \u221a \u03c9\n\u03b1U\u2212\u03b1L .\n(5) Case M + Case U. Without loss of generality, we assume Q(s, a) is in Case M and Q\u0303(s, a) is in Case U. Then we have |Q(s, a)\u2212 Q\u0303(s, a)| \u2264\u03c1(s, a) \u2223\u2223\u03d5(s, a)\u22a4(\u03b8r + \u03b8P \u2212 (H \u2212 h))\u2223\u2223+ (1\u2212 \u03c1(s, a)) |(H \u2212 h)\u2212 (H \u2212 h)| =\u03c1(s, a)\n\u2223\u2223\u03d5(s, a)\u22a4(\u03b8r + \u03b8P \u2212 (H \u2212 h))\u2223\u2223 \u2264 \u03c1(s, a) \u00b7 2Vmax \u2264 2Vmax\u221a\u03c9 \u03b1U \u2212 \u03b1L\nwhere the last inequality is from\n\u03c1(s, a) = \u03b1U \u2212 \u2225\u03d5(s, a)\u2225\u03a3\n\u03b1U \u2212 \u03b1L = \u03b1U \u2212 \u2225\u03d5(s, a)\u2225\u03a3 \u2212 \u2225\u03d5(s, a)\u2225\u03a3\u0303 + \u2225\u03d5(s, a)\u2225\u03a3\u0303 \u03b1U \u2212 \u03b1L\n\u2264 \u2225\u03d5(s, a)\u2225\u03a3\u0303 \u2212 \u2225\u03d5(s, a)\u2225\u03a3\n\u03b1U \u2212 \u03b1L =\n\u221a \u03d5\u22a4\u03a3\u0303\u03d5\u2212 \u221a \u03d5\u22a4\u03a3\u03d5\n\u03b1U \u2212 \u03b1L\n\u2264 \u221a |\u03d5\u22a4(\u03a3\u2212 \u03a3\u0303)\u03d5| \u03b1U \u2212 \u03b1L \u2264 \u221a \u2225\u03d5\u22252\u2225\u03a3\u2212 \u03a3\u0303\u22252\u2225\u03d5\u22252 \u03b1U \u2212 \u03b1L \u2264 \u221a \u03c9 \u03b1U \u2212 \u03b1L .\nwhere the first inequality is by the condition that Q\u0303 is in Case U.\n(6) Both are Case M. Denote \u03c1(s, a) = \u03b1U\u2212\u2225\u03d5(s,a)\u2225\u03a3\u03b1U\u2212\u03b1L and \u03c1\u0303(s, a) = \u03b1U\u2212\u2225\u03d5(s,a)\u2225\u03a3\u0303\n\u03b1U\u2212\u03b1L . Then, we have\n|\u03c1(s, a)\u2212 \u03c1\u0303(s, a)| = \u2223\u2223\u2223\u2225\u03d5(s, a)\u2225\u03a3\u0303 \u2212 \u2225\u03d5(s, a)\u2225\u03a3\u2223\u2223\u2223 \u03b1U \u2212 \u03b1L \u2264 \u221a \u2225\u03d5\u22252\u2225\u03a3\u0303\u2212 \u03a3\u2225F\u2225\u03d5\u22252 \u03b1U \u2212 \u03b1L \u2264 \u221a \u03c9 \u03b1U \u2212 \u03b1L ,\nwhich also implies |(1\u2212 \u03c1(s, a))\u2212 (1\u2212 \u03c1\u0303(s, a))| \u2264 \u221a \u03c9\n\u03b1U\u2212\u03b1L . Hence, we have |Q(s, a)\u2212 Q\u0303(s, a)| \u2264 \u2223\u2223\u2223\u03c1(s, a)\u03d5(s, a)\u22a4(\u03b8r + \u03b8P)\u2212 \u03c1\u0303(s, a)\u03d5(s, a)\u22a4(\u03b8\u0303r + \u03b8\u0303P)\u2223\u2223\u2223 + |(1\u2212 \u03c1(s, a))(H \u2212 h)\u2212 (1\u2212 \u03c1\u0303(s, a))(H \u2212 h)|\n\u2264 \u2223\u2223\u2223\u03c1(s, a)\u03d5(s, a)\u22a4(\u03b8r + \u03b8P)\u2212 \u03c1(s, a)\u03d5(s, a)\u22a4(\u03b8\u0303r + \u03b8\u0303P)\u2223\u2223\u2223 + \u2223\u2223\u2223\u03c1(s, a)\u03d5(s, a)\u22a4(\u03b8\u0303r + \u03b8\u0303P)\u2212 \u03c1\u0303(s, a)\u03d5(s, a)\u22a4(\u03b8\u0303r + \u03b8\u0303P)\u2223\u2223\u2223\n+ |(1\u2212 \u03c1(s, a))(H \u2212 h)\u2212 (1\u2212 \u03c1\u0303(s, a))(H \u2212 h)|\n\u22642\u03c9 + Vmax \u221a \u03c9\n\u03b1U \u2212 \u03b1L +\nH \u221a \u03c9\n\u03b1U \u2212 \u03b1L\nPutting Everything Together. Taking the maximum of the six cases, we conclude that\n|Q(s, a)\u2212 Q\u0303(s, a)| \u2264 2\u03c9 + 2Vmax \u221a \u03c9\n\u03b1U \u2212 \u03b1L We set \u03c9 = (\n\u03b1U\u2212\u03b1L 2tVmax\n)2 and obtain\n|Q(s, a)\u2212 Q\u0303(s, a)| \u2264 2 ( \u03b1U \u2212 \u03b1L 2tVmax )2 + 1 t \u2264 3 t\nwhere we leverage the condition that \u03b1L, \u03b1U \u2264 1. Thus, we have\n|xi,\u03b8r,\u03b8P,\u03a3 \u2212 xi,\u03b8\u0303r,\u03b8\u0303P,\u03a3\u0303| \u2264 2maxs,a \u2223\u2223\u2223Q(s, a)\u2212 Q\u0303(s, a)\u2223\u2223\u2223 \u2264 6 t .\nFinal Bound. The covering argument is now complete, and we can use it to derive an upper bound for \u2225\u03b7P,t,h\u2225\u03a3t\u22121,h . To that end, we note that, for \u03b7P,t,h, there must exists (\u03b8r, \u03b8P,\u03a3) \u2208 U and its closest element (\u03b8\u0303r, \u03b8\u0303P, \u03a3\u0303) \u2208 U\u0303 such that\n\u2225\u03b7P,t,h\u2225\u03a3t\u22121,h\n= \u2225\u2225\u2225\u2225\u2225 t\u22121\u2211 i=1 \u03d5(si,h, ai,h) ( V t,h+1(si,h+1)\u2212 E si,h+1 [V t,h+1(si,h+1) | si,h, ai,h] )\u2225\u2225\u2225\u2225\u2225 \u03a3\u22121t\u22121,h\n= \u2225\u2225\u2225\u2225\u2225 t\u22121\u2211 i=1 \u03d5(si,h, ai,h)xi,\u03b8r,\u03b8P,\u03a3 \u2225\u2225\u2225\u2225\u2225 \u03a3\u22121t\u22121,h\n= \u2225\u2225\u2225\u2225\u2225 t\u22121\u2211 i=1 \u03d5(si,h, ai,h)xi,\u03b8\u0303r,\u03b8\u0303P,\u03a3 \u2225\u2225\u2225\u2225\u2225 \u03a3\u22121t\u22121,h + \u2225\u2225\u2225\u2225\u2225 t\u22121\u2211 i=1 \u03d5(si,h, ai,h) ( xi,\u03b8r,\u03b8P,\u03a3 \u2212 xi,\u03b8\u0303r,\u03b8\u0303P,\u03a3 )\u2225\u2225\u2225\u2225\u2225 \u03a3\u22121t\u22121,h\nwhere the second term is bounded by\u2225\u2225\u2225\u2225\u2225 t\u22121\u2211 i=1 \u03d5(si,h, ai,h) ( xi,\u03b8r,\u03b8P,\u03a3 \u2212 xi,\u03b8\u0303r,\u03b8\u0303P,\u03a3 )\u2225\u2225\u2225\u2225\u2225 \u03a3\u22121t\u22121,h\n\u2264 t\u22121\u2211 i=1 \u2225\u2225\u2225\u03d5(si,h, ai,h)(xi,\u03b8r,\u03b8P,\u03a3 \u2212 xi,\u03b8\u0303r,\u03b8\u0303P,\u03a3)\u2225\u2225\u2225\u03a3\u22121t\u22121,h \u2264 \u221a \u03bb\u22121 \u00b7 t \u00b7max\ni |xi,\u03b8r,\u03b8P,\u03a3 \u2212 xi,\u03b8\u0303r,\u03b8\u0303P,\u03a3|\n\u22646/ \u221a \u03bb,\nand, applying (10), the first term is bounded by\u2225\u2225\u2225\u2225\u2225 t\u22121\u2211 i=1 \u03d5(si,h, ai,h)xi,\u03b8\u0303r,\u03b8\u0303P,\u03a3 \u2225\u2225\u2225\u2225\u2225 \u03a3\u22121\nt\u22121,h\n\u2264 \u221a 32V 2max ( d log ( 1 + t\n\u03bb\n) + log(N/\u03b4) )\n\u2264 \u221a\u221a\u221a\u221a\u221a32V 2max d log(1 + t\n\u03bb\n) + (2d+ d2) log 3 ( (B + \u03f5r,\u03be/ \u221a \u03bb) + (Vmax \u221a dt+ \u03f5P,\u03be)/ \u221a \u03bb+ \u221a d/\u03bb ) \u03c9\u03b4 \n=Vmax \u221a\u221a\u221a\u221a\u221a32d log(1 + t \u03bb ) + 32(2d+ d2) log 12V 2maxt2 ( (B + \u03f5r,\u03be/ \u221a \u03bb) + (Vmax \u221a dt+ \u03f5P,\u03be)/ \u221a \u03bb+ \u221a d/\u03bb ) (\u03b1U \u2212 \u03b1L)2\u03b4 \nwhere the second inequality is by inserting the upper bound of the covering number N , and the equality is by inserting the value of \u03c9. The last line looks complicated and we can further simplify it and get\u2225\u2225\u2225\u2225\u2225 t\u22121\u2211 i=1 \u03d5(si,h, ai,h)xi,\u03b8\u0303r,\u03b8\u0303P,\u03a3 \u2225\u2225\u2225\u2225\u2225 \u03a3\u22121t\u22121,h\n\u226416dVmax \u221a\u221a\u221a\u221a\u221alog 12Vmaxt(t+ \u03bb) ( (B + \u03f5r,\u03be/ \u221a \u03bb) + (Vmax \u221a dt+ \u03f5P,\u03be)/ \u221a \u03bb+ \u221a d/\u03bb ) (\u03b1U \u2212 \u03b1L)\u03b4\u03bb . Plugging these upper bounds back, we obtain\n\u2225\u03b7P,t,h\u2225\u03a3t\u22121,h\n\u2264 6\u221a \u03bb + 16dVmax\n\u221a\u221a\u221a\u221a\u221alog 12Vmaxt(t+ \u03bb) ( (B + \u03f5r,\u03be/ \u221a \u03bb) + (Vmax \u221a dt+ \u03f5P,\u03be)/ \u221a \u03bb+ \u221a d/\u03bb ) (\u03b1U \u2212 \u03b1L)\u03b4\u03bb . Applying union bound over all t \u2208 [T ] and h \u2208 [H], the upper bound exactly becomes \u03f5\u2032P,\u03b7 . Further invoking Lemma B.3 finishes the proof.\nLemma B.11 (Boundness of value functions and transition estimation Error). Assume the events define in Lemmas B.7 and B.9 hold. Then, the following holds with probability at least 1\u2212 \u03b4:\n1. maxs,a |Qt,h(s, a)| \u2264 Vmax for all t \u2208 [T ] and h \u2208 [H].\n2. \u2225\u03b7P,t,h\u2225\u03a3t\u22121,h \u2264 \u03f5P,\u03b7 for all t \u2208 [T ] and h \u2208 [H].\n3. \u2225\u03bbt,h\u2225\u03a3t\u22121,h \u2264 \u03f5\u03bb for all t \u2208 [T ] and h \u2208 [H].\nThe first statement implies maxs |V t,h(s)| \u2264 Vmax for all t \u2208 [T ] and h \u2208 [H].\nProof. We prove the three statements together by induction. For the first statement, we define Vmax,h := (H\u2212h+1)(1+(\u03f5r,\u03be+ \u03f5r,\u03b7)/ \u221a \u03bb), and we will actually show that maxs,a |Qt,h(s, a) \u2212 Q\u22c6h(s, a)| \u2264 Vmax,h in the induction, which immediately leads to maxs,a |Qt,h(s, a)| \u2264 Vmax,h + (H \u2212 h+ 1) \u2264 Vmax.\nBase. For h = H , we can directly apply Lemma B.10 without the transition argument, which leads to the desired upper bound of \u2225\u03b7P,t,H\u2225\u03a3t\u22121,h . Moreover, the upper bound of \u2225\u03bbt,h\u2225\u03a3t\u22121,h also trivially holds. For an upper bound on the value function, we immediately have\u2223\u2223Qt,H(s, a)\u2212Q\u22c6H(s, a)\u2223\u2223 = \u2223\u2223\u03d5(s, a)\u22a4(\u03b8r,t \u2212 \u03b8\u22c6r )\u2223\u2223 \u2264 \u2225\u03d5(s, a)\u2225\u03a3\u22121t\u22121\u2225\u03b8r,t \u2212 \u03b8\u22c6r \u2225\u03a3t\u22121 by Cauchy-Schwarz inequality. We note that \u2225\u03d5(s, a)\u2225\u03a3\u22121t\u22121 \u2264 \u221a \u03bb\u22121 and \u2225\u03b8r,t\u2212 \u03b8\u22c6r \u2225\u03a3t\u22121 \u2264 \u2225\u03ber,t+ \u03b7r,t\u2225\u03a3t\u22121 \u2264 \u2225\u03ber,t\u2225\u03a3t\u22121 + \u2225\u03b7r,t\u2225\u03a3t\u22121 \u2264 \u03f5r,\u03be + \u03f5r,\u03b7 . Hence, we have\u2223\u2223Qt,H(s, a)\u2212Q\u22c6H(s, a)\u2223\u2223 \u2264 (\u03f5r,\u03be + \u03f5r,\u03b7)/\u221a\u03bb \u2264 Vmax,H . Inductive Steps. Now consider h < H . By induction hypothesis, we have maxt,s |V t,h+1(s)| \u2264 Vmax. Thus, we can apply Lemma B.10 and get \u2225\u03b7P,t,h\u2225\u03a3t\u22121,h \u2264 \u03f5P,\u03b7 . For the upper bound of \u2225\u03bbt,h\u2225\u03a3t\u22121,h , by definition, we have\n\u2225\u03bbt,h\u2225\u03a3t\u22121,h = \u2225\u2225\u2225\u2225\u03bb \u222b\ns\u2032 \u00b5\u22c6h(s \u2032)V t,h+1(s \u2032) ds\u2032 \u2225\u2225\u2225\u2225 \u03a3\u22121t\u22121,h\n\u2264 \u221a \u03bb \u2225\u2225\u2225\u2225\u222b s\u2032 \u00b5\u22c6h(s \u2032)V t,h+1(s \u2032) ds\u2032 \u2225\u2225\u2225\u2225 2 \u2264Vmax \u221a \u03bbd = \u03f5\u03bb\nwhere the second inequality is by \u2225\u03a3\u22121/2t\u22121,h\u22252 \u2264 \u221a \u03bb, and the last inequality is by Assumption 4.1.\nFor the upper bound on the value function, consider the following three cases.\nCase 1: \u2225\u03d5(s, a)\u2225\u03a3\u22121t\u22121,h \u2264 \u03b1L. We apply Lemma B.5 and get |\u03d5(s, a)\u22a4 ( \u03b8r,t + \u03b8P,t,h ) \u2212Q\u22c6h(s, a)|\n= \u2223\u2223\u2223\u2223\u03d5(s, a)\u22a4(\u03ber,t + \u03beP,t,h + \u03b7r,t + \u03b7P,t,h \u2212 \u03bbt,h) + E s\u2032 [ V t,h+1(s \u2032)\u2212 V \u22c6h+1(s\u2032) \u2223\u2223 s, a]\u2223\u2223\u2223\u2223\n\u2264 \u2223\u2223\u03d5(s, a)\u22a4(\u03ber,t + \u03b7r,t)\u2223\u2223+ \u2223\u2223\u03d5(s, a)\u22a4(\u03beP,t,h + \u03b7P,t,h \u2212 \u03bbt,h)\u2223\u2223+ \u2223\u2223\u2223\u2223E\ns\u2032\n[ V t,h+1(s \u2032)\u2212 V \u22c6h+1(s\u2032) \u2223\u2223 s, a]\u2223\u2223\u2223\u2223\n\u2264(\u03f5r,\u03be + \u03f5r,\u03b7)/ \u221a \u03bb+ \u2225\u03d5(s, a)\u2225\u03a3\u22121t\u22121,h(\u03f5P,\u03be + \u03f5P,\u03b7 + \u03f5\u03bb) +\n( (H \u2212 h)(1 + ( \u03f5r,\u03be + \u03f5r,\u03b7)/ \u221a \u03bb ))\n\u2264(\u03f5r,\u03be + \u03f5r,\u03b7)/ \u221a \u03bb+ 1 + ( (H \u2212 h)(1 + ( \u03f5r,\u03be + \u03f5r,\u03b7)/ \u221a \u03bb ))\n=(H \u2212 h+ 1)(1 + ( \u03f5r,\u03be + \u03f5r,\u03b7)/ \u221a \u03bb )\n=Vmax,h\nwhere the second inequality is by Cauchy-Schwarz inequality, Lemmas B.7 and B.9, and the induction hypothesis. The third inequality is by the condition that \u2225\u03d5(s, a)\u2225\u03a3\u22121t\u22121,h \u2264 \u03b1L and the definition of \u03b1L.\nCase 2: \u2225\u03d5(s, a)\u2225\u03a3\u22121t\u22121,h > \u03b1U. We have |Qt,h(s, a)\u2212Q\u22c6h(s, a) \u2223\u2223\u2223 \u2264|\u03d5(s, a)\u22a4\u03b8r,t +H \u2212 h\u2212Q\u22c6h(s, a)| =|\u03d5(s, a)\u22a4(\u03ber,t + \u03b7r,t) +H \u2212 h+ \u03d5(s, a)\u22a4\u03b8\u22c6r \u2212Q\u22c6h(s, a)|\n\u2264(\u03f5r,\u03be + \u03f5r,\u03b7)/ \u221a \u03bb+ (H \u2212 h+ 1)\n\u2264Vmax,h\nwhere we used the fact that 0 \u2264 \u03d5(s, a)\u22a4\u03b8\u22c6r \u2264 1 and 0 \u2264 Q\u22c6h(s, a) \u2264 H \u2212 h + 1, which leads to \u2212(H \u2212 h+ 1) \u2264 \u03d5(s, a)\u22a4\u03b8\u22c6r \u2212Q\u22c6h(s, a) \u2264 1.\nCase 3: \u03b1L < \u2225\u03d5(s, a)\u2225\u03a3\u22121t\u22121,h \u2264 \u03b1U. Denoting \u03c1 := \u03c1(s, a) for simplicity, we have\u2223\u2223\u2223Qt,h(s, a)\u2212Q\u22c6h(s, a)\u2223\u2223\u2223 \u2264\u03c1 \u2223\u2223\u2223\u03d5(s, a)\u22a4 (\u03b8r,t + \u03b8P,t,h)\u2212Q\u22c6h(s, a)\u2223\u2223\u2223+ (1\u2212 \u03c1)\u2223\u2223\u2223\u03d5(s, a)\u22a4\u03b8r,t +H \u2212 h\u2212Q\u22c6h(s, a)\u2223\u2223\u2223\n\u2264\u03c1(H \u2212 h+ 1) ( 1 + (\u03f5r,\u03be + \u03f5r,\u03b7)/ \u221a \u03bb ) + (1\u2212 \u03c1) \u2223\u2223\u2223\u03d5(s, a)\u22a4(\u03ber,t + \u03b7r,t) +H \u2212 h+ \u03d5(s, a)\u22a4\u03b8\u22c6r \u2212Q\u22c6h(s, a)\u2223\u2223\u2223 \u2264\u03c1(H \u2212 h+ 1) ( 1 + (\u03f5r,\u03be + \u03f5r,\u03b7)/ \u221a \u03bb ) + (1\u2212 \u03c1) ( (\u03f5r,\u03be + \u03f5r,\u03b7)/ \u221a \u03bb+ (H \u2212 h+ 1)\n) \u2264Vmax,h where we have used the similar arguments from Case 1 and 2.\nTaking the maximum over the three cases, we conclude that\n|Qt,h(s, a)\u2212Q\u22c6h(s, a)| \u2264 Vmax,h\nwhich implies maxs,a |Qt,h(s, a)| \u2264 Vmax.\nLemma B.12 (High probability bounds \u2013 summary). All of the following events hold simultaneously with probability at least 1\u2212 \u03b4:\n1. \u2225\u03ber,t\u2225\u03a3t\u22121 \u2264 \u03f5r,\u03be for all t \u2208 [T ]\n2. \u2225\u03beP,t,h\u2225\u03a3t\u22121,h \u2264 \u03f5P,\u03be for all t \u2208 [T ] and h \u2208 [H].\n3. \u2225\u03b7r,t\u2225\u03a3t\u22121 \u2264 \u03f5r,\u03b7 for all t \u2208 [T ].\n4. \u2225\u03b7P,t,h\u2225\u03a3t\u22121,h \u2264 \u03f5P,\u03b7 for all t \u2208 [T ] and h \u2208 [H].\n5. \u2225\u03bbt,h\u2225\u03a3t\u22121,h \u2264 \u03f5\u03bb for all t \u2208 [T ] and h \u2208 [H].\n6. |Qt,h(s, a)| \u2264 Vmax and |V t,h(s)| \u2264 Vmax for all (s, a) \u2208 S \u00d7A, t \u2208 [T ], and h \u2208 [H].\nProof of Lemma B.12. The first and second statements are by Lemma B.7. The third is by Lemma B.9. The last three are by Lemma B.11. Then we take a union bound over all of them."
        },
        {
            "heading": "B.4 BOUNDING REGRET",
            "text": "We define\nV\u0303t = E \u03c4\u223c\u03c01t\n[ H\u2211\nh=1\n\u03d5(sh, ah) \u22a4\u03b8r,t ] .\nLemma B.13. Assume all events listed in Lemma B.12 hold. Fix t \u2208 [T ]. Then, we have (V \u22c6 \u2212 V t) + (V\u0303 \u2212 V \u03c0 1 t ) \u2264 0 with probability at least F2(\u22121) where F denotes the CDF of a standard normal distribution (i.e., N (0, 1)).\nProof. Define the indicators L\u22c6t,h(s) :=1 { \u2225\u03d5(s, \u03c0\u22c6(s))\u2225\u03a3\u22121t\u22121,h \u2264 \u03b1L } ,\nM\u22c6t,h(s) :=1 { \u03b1L < \u2225\u03d5(s, \u03c0\u22c6(s))\u2225\u03a3\u22121t\u22121,h \u2264 \u03b1U } ,\nU\u22c6t,h(s) :=1 { \u2225\u03d5(s, \u03c0\u22c6(s))\u2225\u03a3\u22121t\u22121,h > \u03b1U } .\nWe note that they are independent of the random noises at episode t and only depends on the information up to episode t\u2212 1. Then, we can decompose V \u22c6 \u2212 V t:\nV \u22c6 \u2212 V t = E s1\n[ V \u22c61 (s1)\u2212 V t,1(s1) ] = E\ns1\n[ Q\u22c61(s1, \u03c0 \u22c6(s1))\u2212Qt,1(s1, \u03c00t,1(s1)) ]\n\u2264 E s1\n[ Q\u22c61(s1, \u03c0 \u22c6(s1))\u2212Qt,1(s1, \u03c0\u22c6(s1)) ]\n= E s1\n[ L\u22c6t,1(s1) ( Q\u22c61(s1, \u03c0 \u22c6(s1))\u2212Qt,1(s1, \u03c0\u22c6(s1)) )]\n+ E s1\n[ M\u22c6t,1(s1) ( Q\u22c61(s1, \u03c0 \u22c6(s1))\u2212Qt,1(s1, \u03c0\u22c6(s1)) )]\n+ E s1\n[ U\u22c6t,1(s1) ( Q\u22c61(s1, \u03c0 \u22c6(s1))\u2212Qt,1(s1, \u03c0\u22c6(s1)) )]\n=:TL + TM,+TU.\nNow we establish upper bounds for each term. For TU, we have\nE s1\n[ U\u22c6t,1(s1) ( Q\u22c61(s1, \u03c0 \u22c6(s1))\u2212Qt,1(s1, \u03c0\u22c6(s1)) )]\n= E s1\n[ U\u22c6t,1(s1) ( Q\u22c61(s1, \u03c0 \u22c6(s1))\u2212 \u03d5(s1, \u03c0\u22c6(s1))\u22a4\u03b8r,t \u2212 (H \u2212 1) )]\n= E s1\n[ U\u22c6t,1(s1) ( Q\u22c61(s1, \u03c0 \u22c6(s1))\u2212 \u03d5(s1, \u03c0\u22c6(s1))\u22a4\u03b8\u22c6r \u2212 \u03d5(s1, \u03c0\u22c6(s1))\u22a4(\u03b7r,t + \u03ber,t)\u2212 (H \u2212 1) )]\n\u2264 E s1\n[( \u2212U\u22c6t,1(s1)\u03d5(s1, \u03c0\u22c6(s1)) )\u22a4 (\u03b7r,t + \u03ber,t) ] where the inequality holds since\nQ\u22c61(s1, \u03c0 \u22c6(s1))\u2212 \u03d5(s1, \u03c0\u22c6(s1))\u22a4\u03b8\u22c6r \u2212 (H \u2212 1)\n=\u03d5(s1, \u03c0 \u22c6(s1)) \u22a4\u03b8\u22c6r + E s2 [V \u22c62 (s2) | s1, \u03c0\u22c6(s1)]\u2212 \u03d5(s1, \u03c0\u22c6(s1))\u22a4\u03b8\u22c6r \u2212 (H \u2212 1) \u2264 0.\nFor TL, we have\nE s1\n[ L\u22c6t,1(s1) ( Q\u22c61(s1, \u03c0 \u22c6(s1))\u2212Qt,1(s1, \u03c0\u22c6(s1)) )]\n= E s1\n[ L\u22c6t,1(s1) ( Q\u22c61(s1, \u03c0 \u22c6(s1))\u2212 \u03d5(s1, \u03c0\u22c6(s1))\u22a4(\u03b8r,t + \u03b8P,t,h) )]\n= E s1\n[ L\u22c6t,1(s1) ( \u2212\u03d5(s1, \u03c0\u22c6(s1))\u22a4(\u03ber,t + \u03beP,t,1 + \u03b7r,t + \u03b7P,t,1 \u2212 \u03bbt,1) + E\ns2\n[ V \u22c62 (s2)\u2212 V t,2(s\u2032) \u2223\u2223 s1, \u03c0\u22c6(s1)])] where the last equality is by Lemma B.5.\nFor TM, we have\nE s1\n[ M\u22c6t,1(s1) ( Q\u22c61(s1, \u03c0 \u22c6(s1))\u2212Qt,1(s1, \u03c0\u22c6(s1)) )]\n= E s1\n[ M\u22c6t,1(s1) ( \u03c1(s1, \u03c0 \u22c6(s1)) ( Q\u22c61(s1, \u03c0 \u22c6(s1))\u2212 \u03d5(s1, \u03c0\u22c6(s1))\u22a4(\u03b8r,t + \u03b8P,t,1) )\n+ ( 1\u2212 \u03c1(s1, \u03c0\u22c6(s1)) )( Q\u22c61(s1, \u03c0 \u22c6(s1))\u2212 \u03d5(s1, \u03c0\u22c6(s1))\u22a4\u03b8r,t \u2212 (H \u2212 h) ))]\n\u2264 E s1\n[ M\u22c6t,1(s1) ( \u03c1(s1, \u03c0 \u22c6(s1)) ( \u2212 \u03d5(s1, \u03c0\u22c6(s1))\u22a4(\u03ber,t + \u03beP,t,1 + \u03b7r,t + \u03b7P,t,1 \u2212 \u03bbt,1)\n+ E s2\n[ V \u22c62 (s2)\u2212 V t,2(s\u2032) \u2223\u2223 s1, \u03c0\u22c6(s1)])+ (1\u2212 \u03c1(s1, \u03c0\u22c6(s1))) (\u2212\u03d5(s1, \u03c0\u22c6(s1)))\u22a4 (\u03b7r,t + \u03ber,t))] where the inequality is by similar arguments as in the case of TU and TL.\nSo putting TL, TM, TU together, we have\nV \u22c6 \u2212 V t\n\u2264 E s1\n[( U\u22c6t,1(s1) +M \u22c6 t,1(s1)(1\u2212 \u03c1(s1, \u03c0\u22c6(s1))) ) \ufe38 \ufe37\ufe37 \ufe38\n=:UM\u22c6t,1(s1)\n(\u2212\u03d5(s1, \u03c0\u22c6(s1)))\u22a4(\u03b7r,t + \u03ber,t)\n+ ( L\u22c6t,1(s1) +M \u22c6 t,1(s1)\u03c1(s1, \u03c0 \u22c6(s1)) )\n\ufe38 \ufe37\ufe37 \ufe38 =:LM\u22c6t,1(s1)\n( \u2212 \u03d5(s1, \u03c0\u22c6(s1))\u22a4(\u03ber,t + \u03beP,t,1 + \u03b7r,t + \u03b7P,t,1 \u2212 \u03bbt,1)\n+ E s2\n[ V \u22c62 (s2)\u2212 V t,2(s\u2032) \u2223\u2223 s1, \u03c0\u22c6(s1)] )] Keeping expanding the last term, we arrive at\nV \u22c6 \u2212 V t \u2264 E \u03c4\u223c\u03c0\u22c6\n[ H\u2211\nh=1\n( UM\u22c6t,h(sh) h\u22121\u220f i=1 LM\u22c6t,i(si) )( \u2212\u03d5(sh, ah)\u22a4(\u03ber,t + \u03b7r,t) ) +\nH\u2211 h=1\n( h\u220f\ni=1\nLM\u22c6t,i(si) )( \u2212\u03d5(sh, ah)\u22a4(\u03ber,t + \u03beP,t,h + \u03b7r,t + \u03b7P,t,h \u2212 \u03bbt,h) ) ]\n= E \u03c4\u223c\u03c0\u22c6\n[ H\u2211\nh=1\n( UM\u22c6t,h(sh) h\u22121\u220f i=1 LM\u22c6t,i(si) + h\u220f i=1 LM\u22c6t,i(si) )( \u2212\u03d5(sh, ah)\u22a4(\u03ber,t + \u03b7r,t) ) +\nH\u2211 h=1\n( h\u220f\ni=1\nLM\u22c6t,i(si) )( \u2212\u03d5(sh, ah)\u22a4(\u03beP,t,h + \u03b7P,t,h \u2212 \u03bbt,h) ) ]\n=:(\u2212\u03d5\u22c6r )\u22a4(\u03ber,t + \u03b7r,t) + H\u2211\nh=1\n(\u2212\u03d5\u22c6P,h)\u22a4(\u03beP,t,h + \u03b7P,t,h \u2212 \u03bbt,h)\nwhere in the last step we defined \u03d5\u22c6r and \u03d5 \u22c6 P,h as\n\u03d5\u22c6r := E \u03c4\u223c\u03c0\u22c6\n[ H\u2211\nh=1\n( UM\u22c6t,h(sh) h\u22121\u220f i=1 LM\u22c6t,i(si) + h\u220f i=1 LM\u22c6t,i(si) ) \u03d5(sh, ah) ] ,\n\u03d5\u22c6P,h := E \u03c4\u223c\u03c0\u22c6\n[( h\u220f\ni=1\nLM\u22c6t,i(si) ) \u03d5(sh, ah) ] .\nNow we also decompose V\u0303t \u2212 V \u03c0 1 t and get\nV\u0303t \u2212 V \u03c0 1 t = E\n\u03c4\u223c\u03c01t\n[ H\u2211\nh=1\n\u03d5(sh, ah) ]\u22a4 ( \u03b8r,t \u2212 \u03b8\u22c6r\n) = E\n\u03c4\u223c\u03c01t\n[ H\u2211\nh=1\n\u03d5(sh, ah) ]\u22a4 (\u03ber,t + \u03b7r,t)\n=:(\u03d5\u0303t) \u22a4 (\u03ber,t + \u03b7r,t) .\nCombining the decomposition together, we obtain\n(V \u22c6 \u2212 V t) + (V\u0303 \u2212 V \u03c0 1 t )\n\u2264 ( (\u2212\u03d5\u22c6r )\u22a4(\u03ber,t + \u03b7r,t) + H\u2211 h=1 (\u2212\u03d5\u22c6P,h)\u22a4(\u03beP,t,h + \u03b7P,t,h \u2212 \u03bbt,h) ) + \u03d5\u0303\u22a4t (\u03ber,t + \u03b7r,t)\n= ( \u03d5\u0303t \u2212 \u03d5\u22c6 )\u22a4 (\u03ber,t + \u03b7r,t) + H\u2211 h=1 (\u2212\u03d5\u22c6P,h)\u22a4 (\u03beP,t,h + \u03b7P,t,h \u2212 \u03bbt,h)\n\u2264\u2225\u03d5\u0303t \u2212 \u03d5\u22c6\u2225\u03a3\u22121t\u22121\u2225\u03b7r,t\u2225\u03a3t\u22121 + (\u03d5\u0303t \u2212 \u03d5 \u22c6)\u22a4\u03ber,t\n+ H\u2211 h=1 \u2225\u03d5\u22c6P,h\u2225\u03a3\u22121t\u22121,h ( \u2225\u03b7P,t,h\u2225\u03a3t\u22121,h + \u2225\u03bbt,h\u2225\u03a3t\u22121,h ) \u2212 H\u2211 h=1 \u03d5\u22c6P,h \u22a4\u03beP,t,h\n\u2264\u2225\u03d5\u0303t \u2212 \u03d5\u22c6\u2225\u03a3\u22121t\u22121\u03f5r,\u03b7 \u2212 (\u03d5\u0303t \u2212 \u03d5 \u22c6)\u22a4\u03ber,t + \u221a\u221a\u221a\u221a H\u2211 h=1 \u2225\u03d5\u22c6P,h\u22252\u03a3\u22121t\u22121,h \u00b7 \u221a H(\u03f5P,\u03b7 + \u03f5\u03bb)2 \u2212 H\u2211 h=1 \u03d5\u22c6P,h \u22a4\u03beP,t,h\n= \u2225\u03d5\u0303t \u2212 \u03d5\u22c6\u2225\u03a3\u22121t\u22121\u03f5r,\u03b7 \u2212 (\u03d5\u0303t \u2212 \u03d5 \u22c6)\u22a4\u03ber,t\ufe38 \ufe37\ufe37 \ufe38\n(a)\n+ \u221a\u221a\u221a\u221a H\u2211 h=1 min { 1, \u2225\u03d5\u22c6P,h\u22252\u03a3\u22121t\u22121,h } \u00b7 \u221a H(\u03f5P,\u03b7 + \u03f5\u03bb)2 \u2212 H\u2211 h=1\n\u03d5\u22c6P,h \u22a4\u03beP,t,h\ufe38 \ufe37\ufe37 \ufe38\n(b)\nwhere the second inequality is by Cauchy-Schwarz inequality, the third inequality is Lemma B.12 and Cauchy-Schwarz inequality again, and the last step is by the fact that \u2225\u03d5\u22c6P,h\u22252\u03a3\u22121t\u22121,h \u2264 1/\u03bb \u2264 1.\nFor (a), we note that (\u03d5\u0303t\u2212\u03d5\u22c6)\u22a4\u03ber,t \u223c N (0, \u03c32r \u2225\u03d5\u0303t\u2212\u03d5\u22c6\u22252\u03a3\u22121t\u22121 ). So by setting \u03c3r \u2265 \u03f5r,\u03b7 , we have (a) \u2264 0 with probability at least F(\u22121). For (b), similarly, we note that \u2211H\nh=1 \u03d5 \u22c6 P,h \u22a4\u03beP,t,h \u223c N (0, \u03c32P \u2211H\nh=1 \u2225\u03d5\u22c6P,h\u22252\u03a3\u22121t\u22121,h ). So by setting \u03c3P \u2265 \u221a H(\u03f5P,\u03b7 + \u03f5\u03bb)2, we have (b) \u2264 0 with probability at least F(\u22121).\nConclusion. Finally, we note that (a) and (b) are independent, so the probability that both (a) and (b) hold is at least F2(\u22121).\nLemma B.14. It holds that T\u2211\nt=1 H\u2211 h=1 L\u2201t,h(st,h) \u2264 Lmax.\nProof. Denote at,h = \u03c0t(st,h). Then, we have\nT\u2211 t=1 H\u2211 h=1 L\u2201t,h(st,h) = T\u2211 t=1 H\u2211 h=1 1 { \u2225\u03d5(st,h, at,h)\u22252\u03a3\u22121t\u22121,h > \u03b1 2 L }\n\u2264 T\u2211\nt=1 H\u2211 h=1 min \u2225\u03d5(st,h, at,h)\u2225 2 \u03a3\u22121t\u22121,h \u03b12L , 1  \u2264 1 \u03b12L H\u2211 h=1 T\u2211 t=1 min { \u2225\u03d5(st,h, at,h)\u22252\u03a3\u22121t\u22121,h , 1\n} \u2264 H \u03b12L \u00b7 2d log ( \u03bb+ T \u03bb ) = Lmax\nwhere the second inequality uses the fact that \u03b1L < 1, and the last inequality is by Lemma D.6.\nLemma B.15. Assume all events listed in Lemma B.12 hold. Then, the following holds\u2223\u2223\u2223\u2223\u2223\u2211 t ( (V t \u2212 V \u03c0 0 t )\u2212 (V\u0303t \u2212 V \u03c0 1 t ) )\u2223\u2223\u2223\u2223\u2223 =O\u0303 ( VmaxLmax + Vmax \u221a HT\n+ (\u03f5r,\u03be + \u03f5r,\u03b7) (\u221a dT + Lmax + \u221a HT + \u03f5T \u221a \u03c0\n2\u03c3r ) + (\u03f5P,\u03be + \u03f5P,\u03b7 + \u03f5\u03bb) ( H \u221a dT + Lmax + \u221a HT ) . )\nProof. By definition, we have\u2223\u2223\u2223\u2223\u2223 T\u2211\nt=1\n( (V t \u2212 V \u03c0 0 t )\u2212 (V\u0303t \u2212 V \u03c0 1 t ) )\u2223\u2223\u2223\u2223\u2223\n= \u2223\u2223\u2223\u2223\u2223 T\u2211\nt=1\nE s1,a1\u223c\u03c00t\n[ Qt,1(s1, a1)\u2212Q \u03c00t 1 (s1, a1) ] \u2212 T\u2211 t=1 H\u2211 h=1 E sh,ah\u223c\u03c01t \u03d5(sh, ah) ( \u03b8r,t \u2212 \u03b8\u22c6r )\u2223\u2223\u2223\u2223\u2223 By triangle inequality, we have\n\u2264 \u2223\u2223\u2223\u2223\u2223 T\u2211\nt=1\nE s1,a1\u223c\u03c00t\n[ Lt,1(st,1) ( Qt,1(s1, a1)\u2212Q \u03c00t 1 (s1, a1) )] \u2212 T\u2211 t=1 H\u2211 h=1 E sh,ah\u223c\u03c01t \u03d5(sh, ah) (\u03ber,t + \u03b7r,t) \u2223\u2223\u2223\u2223\u2223 + 2Vmax\nT\u2211 t=1 E s1,a1\u223c\u03c00t [ L\u2201t,1(st,1) ] For the first term, conditioning on Lt,1(st,1), we have\n= \u2223\u2223\u2223\u2223\u2223 T\u2211\nt=1\nE s1,a1\u223c\u03c00t\n[ Lt,1(st,1) ( \u03d5(s1, a1) \u22a4(\u03b8r,t + \u03b8P,t,1)\u2212Q \u03c00t 1 (s1, a1) )] \u2212\nT\u2211 t=1 H\u2211 h=1 E sh,ah\u223c\u03c01t \u03d5(sh, ah) (\u03ber,t + \u03b7r,t) \u2223\u2223\u2223\u2223\u2223+ 2Vmax T\u2211 t=1 E s1,a1\u223c\u03c00t [ L\u2201t,1(st,1) ]\nApplying triangle inequality again, we get\n\u2264 \u2223\u2223\u2223\u2223\u2223 T\u2211\nt=1\nE s1,a1\u223c\u03c00t\n[( \u03d5(s1, a1) \u22a4(\u03b8r,t + \u03b8P,t,1)\u2212Q \u03c00t 1 (s1, a1) )] \u2212 T\u2211 t=1 H\u2211 h=1 E sh,ah\u223c\u03c01t \u03d5(sh, ah) (\u03ber,t + \u03b7r,t) \u2223\u2223\u2223\u2223\u2223 + 2Vmax\nT\u2211 t=1 E s1,a1\u223c\u03c00t [ L\u2201t,1(st,1) ] + \u2223\u2223\u2223\u2223\u2223 T\u2211\nt=1\nE s1,a1\u223c\u03c00t\n[ L\u2201t,1(st,1) ( \u03d5(s1, a1) \u22a4(\u03b8r,t + \u03b8P,t,1)\u2212Q \u03c00t 1 (s1, a1) )]\u2223\u2223\u2223\u2223\u2223 Applying Lemma B.5 and the triangle inequality, we get\n\u2264 \u2223\u2223\u2223\u2223\u2223 T\u2211\nt=1\nE s2\u223c\u03c00t\n[ V t,2(s2)\u2212 V \u03c00t 2 (s2) ] \u2212 T\u2211 t=1 H\u2211 h=2 E sh,ah\u223c\u03c01t \u03d5(sh, ah) (\u03ber,t + \u03b7r,t)\n+ T\u2211 t=1\n( E\ns1,a1\u223c\u03c00t \u03d5\u22a4(s1, a1)(\u03ber,t + \u03beP,t,1 + \u03b7r,t + \u03b7P,t,1 \u2212 \u03bbt,1)\u2212 E s1,a1\u223c\u03c01t \u03d5\u22a4(s1, a1)(\u03ber,t + \u03b7r,t) )\u2223\u2223\u2223\u2223\u2223 + 2Vmax\nT\u2211 t=1 E s1,a1\u223c\u03c00t [ L\u2201t,1(st,1) ] + \u2223\u2223\u2223\u2223\u2223 T\u2211\nt=1\nE s1,a1\u223c\u03c00t\n[ L\u2201t,1(st,1) ( \u03d5(s1, a1) \u22a4(\u03b8r,t + \u03b8P,t,1)\u2212Q \u03c00t 1 (s1, a1) )]\u2223\u2223\u2223\u2223\u2223 Keep expanding the first term, we get\u2223\u2223\u2223\u2223\u2223 T\u2211 t=1 ( (V t \u2212 V \u03c0 0 t )\u2212 (V\u0303t \u2212 V \u03c0 1 t ) )\u2223\u2223\u2223\u2223\u2223\n\u2264 2Vmax H\u2211\nh=1 T\u2211 t=1 E sh,ah\u223c\u03c00t [ L\u2201t,h(st,h) ] +\nH\u2211 h=1 \u2223\u2223\u2223\u2223\u2223 T\u2211 t=1 E sh,ah\u223c\u03c00t [ L\u2201t,h(st,h) ( \u03d5(sh, ah) \u22a4(\u03b8r,t + \u03b8P,t,h)\u2212Q \u03c00t h (sh, ah) )]\u2223\u2223\u2223\u2223\u2223 + \u2223\u2223\u2223\u2223\u2223 T\u2211\nt=1 H\u2211 h=1\n( E\nsh,ah\u223c\u03c00t E s\u0303h,a\u0303h\u223c\u03c01t\n( \u03d5(sh, ah)\u2212 \u03d5(s\u0303h, a\u0303h) )\u22a4 (\u03ber,t + \u03b7r,t) )\u2223\u2223\u2223\u2223\u2223 + \u2223\u2223\u2223\u2223\u2223 T\u2211\nt=1 H\u2211 h=1 E sh,ah\u223c\u03c00t \u03d5\u22a4(sh, ah) ( \u03beP,t,h + \u03b7P,t,h \u2212 \u03bbt,h )\u2223\u2223\u2223\u2223\u2223 =: T1 + T2 + T3 + T4.\nWe bound each term separately.\nBounding T1. By Hoeffding\u2019s inequality and Lemma B.14, we have\nT1 \u2264 2Vmax H\u2211\nh=1 T\u2211 t=1 [ L\u2201t,h(st,h) ] + 2Vmax \u221a HT 2 log(1/\u03b4)\n\u2264 2Vmax ( Lmax + \u221a HT\n2 log(1/\u03b4)\n) .\nwith probability at least 1\u2212 \u03b4.\nBounding T2. By definition and Lemma B.4, we have\nT2 = H\u2211 h=1 \u2223\u2223\u2223\u2223\u2223 T\u2211 t=1 E sh,ah\u223c\u03c00t [ L\u2201t,h(st,h) ( \u03d5(sh, ah) \u22a4(\u03ber,t + \u03b7r,t + \u03b8 \u22c6 r + \u03beP,t,h + \u03b7P,t,h + \u03bbt,h + \u03b8 \u22c6 P,t,h)\n\u2212Q\u03c0 0 t h (sh, ah) )]\u2223\u2223\u2223\u2223\u2223\n\u2264 H\u2211\nh=1\n\u2223\u2223\u2223\u2223\u2223 T\u2211\nt=1\nE sh,ah\u223c\u03c00t\n[ L\u2201t,h(st,h) ( \u03d5(sh, ah) \u22a4(\u03b8\u22c6r + \u03b8 \u22c6 P,t,h)\u2212Q \u03c00t h (sh, ah)\n+ \u03d5\u22a4(sh, ah)(\u03ber,t + \u03b7r,t) + \u03d5 \u22a4(sh, ah)(\u03beP,t,h + \u03b7P,t,h + \u03bbt,h) )]\u2223\u2223\u2223\u2223\u2223 \u2264\nH\u2211 h=1 \u2223\u2223\u2223\u2223\u2223 T\u2211 t=1 E sh,ah\u223c\u03c00t [ L\u2201t,h(st,h) ( \u03d5(sh, ah) \u22a4(\u03b8\u22c6r + \u03b8 \u22c6 P,t,h)\u2212Q \u03c00t h (sh, ah)\n+ \u2225\u03d5(sh, ah)\u2225\u03a3\u22121t\u22121(\u2225\u03ber,t\u2225\u03a3t\u22121 + \u2225\u03b7r,t\u2225\u03a3t\u22121) + \u2225\u03d5(sh, ah)\u2225\u03a3\u22121t\u22121,h(\u2225\u03beP,t,h\u2225\u03a3t\u22121,h + \u2225\u03b7P,t,h\u2225\u03a3t\u22121,h + \u2225\u03bbt,h\u2225\u03a3t\u22121,h) )]\u2223\u2223\u2223\u2223\u2223.\nWe note that |\u03d5(sh, ah)\u22a4(\u03b8\u22c6r + \u03b8\u22c6P,t,h)| \u2264 Vmax and |Q \u03c00t h (sh, ah)| \u2264 Vmax. Moreover, we have \u2225\u03d5(sh, ah)\u2225\u03a3\u22121t\u22121 \u2264 1/ \u221a \u03bb and \u2225\u03d5(sh, ah)\u2225\u03a3\u22121t\u22121,h \u2264 1/ \u221a \u03bb. Apply triangle inequality and inserting these upper bounds back, we obtain T2 \u2264 H\u2211\nh=1\n\u2223\u2223\u2223\u2223\u2223 T\u2211\nt=1\nE sh,ah\u223c\u03c00t\n[ L\u2201t,h(st,h) ( \u03d5(sh, ah) \u22a4(\u03b8\u22c6r + \u03b8 \u22c6 P,t,h)\u2212Q \u03c00t h (sh, ah) )]\u2223\u2223\u2223\u2223\u2223 +\nH\u2211 h=1 T\u2211 t=1 E sh,ah\u223c\u03c00t [ L\u2201t,h(st,h)\u2225\u03d5(sh, ah)\u2225\u03a3\u22121t\u22121(\u2225\u03ber,t\u2225\u03a3t\u22121 + \u2225\u03b7r,t\u2225\u03a3t\u22121) ]\n+ H\u2211 h=1 T\u2211 t=1 E sh,ah\u223c\u03c00t [ L\u2201t,h(st,h)\u2225\u03d5(sh, ah)\u2225\u03a3\u22121t\u22121,h(\u2225\u03beP,t,h\u2225\u03a3t\u22121,h + \u2225\u03b7P,t,h\u2225\u03a3t\u22121,h + \u2225\u03bbt,h\u2225\u03a3t\u22121,h) ]\n\u2264 2Vmax H\u2211\nh=1 T\u2211 t=1 E sh,ah\u223c\u03c00t [ L\u2201t,h(st,h) ]\n+ \u221a \u03bb\u22121 H\u2211 h=1 T\u2211 t=1 E sh,ah\u223c\u03c00t [ L\u2201t,h(st,h)(\u03f5r,\u03be + \u03f5r,\u03b7 + \u03f5P,\u03be + \u03f5P,\u03b7 + \u03f5\u03bb) ]\n= H\u2211 h=1 T\u2211 t=1 E sh,ah\u223c\u03c00t [ L\u2201t,h(st,h) ] \ufe38 \ufe37\ufe37 \ufe38\n(\u2217)\n( 2Vmax +\n\u03f5r,\u03be + \u03f5r,\u03b7 + \u03f5P,\u03be + \u03f5P,\u03b7 + \u03f5\u03bb\u221a \u03bb\n) .\nWe notice that (\u2217) can be bounded by Hoeffding\u2019s inequality and Lemma B.14, as we did similarly for T1:\n(\u2217) \u2264 Lmax + \u221a HT\n2 log(1/\u03b4)\nHence, we have\nT2 \u2264 ( Lmax + \u221a HT\n2 log(1/\u03b4)\n)( 2Vmax +\n\u03f5r,\u03be + \u03f5r,\u03b7 + \u03f5P,\u03be + \u03f5P,\u03b7 + \u03f5\u03bb\u221a \u03bb ) with probability at least 1\u2212 \u03b4.\nBounding T3. By definition, we have\nT3 = \u2223\u2223\u2223\u2223\u2223 T\u2211\nt=1\nE \u03c4\u223c\u03c00t E \u03c4\u0303\u223c\u03c01t\n( \u03d5(\u03c4)\u2212 \u03d5(\u03c4\u0303) ) (\u03ber,t + \u03b7r,t) \u2223\u2223\u2223\u2223\u2223 By Cauchy-Schwarz inequality, we have\n\u2264 T\u2211\nt=1\nE \u03c4\u223c\u03c00t E \u03c4\u0303\u223c\u03c01t \u2225\u2225\u2225\u03d5(\u03c4)\u2212 \u03d5(\u03c4\u0303)\u2225\u2225\u2225 \u03a3\u22121t\u22121 ( \u2225\u03ber,t\u2225\u03a3t\u22121 + \u2225\u03b7r,t\u2225\u03a3t\u22121 ) \u2264 ( \u03f5r,\u03be + \u03f5r,\u03b7\n) T\u2211 t=1 E \u03c4\u223c\u03c00t E \u03c4\u0303\u223c\u03c01t \u2225\u2225\u2225\u03d5(\u03c4)\u2212 \u03d5(\u03c4\u0303)\u2225\u2225\u2225 \u03a3\u22121t\u22121\nSince \u2225\u03d5(\u03c4)\u2212 \u03d5(\u03c4\u0303)\u2225\u03a3\u22121t\u22121 \u2264 2/ \u221a \u03bb, by Hoeffding\u2019s inequality, we have\n\u2264 ( \u03f5r,\u03be + \u03f5r,\u03b7 )( T\u2211 t=1 \u2225\u2225\u2225\u03d5(\u03c4)\u2212 \u03d5(\u03c4\u0303)\u2225\u2225\u2225 \u03a3\u22121t\u22121 + \u221a 2T log(1/\u03b4) \u03bb )\n= ( \u03f5r,\u03be + \u03f5r,\u03b7 )( T\u2211 t=1 Zt \u2225\u2225\u2225\u03d5(\u03c4)\u2212 \u03d5(\u03c4\u0303)\u2225\u2225\u2225 \u03a3\u22121t\u22121\ufe38 \ufe37\ufe37 \ufe38\n(i)\n+ T\u2211 t=1 (1\u2212 Zt) \u2225\u2225\u2225\u03d5(\u03c4)\u2212 \u03d5(\u03c4\u0303)\u2225\u2225\u2225\n\u03a3\u22121t\u22121\ufe38 \ufe37\ufe37 \ufe38 (ii)\n+\n\u221a 2T log(1/\u03b4)\n\u03bb\n) .\nTo bound (i), we have\n(i) = T\u2211 t=1 Zt min { 2/ \u221a \u03bb, \u2225\u2225\u2225\u03d5(\u03c4)\u2212 \u03d5(\u03c4\u0303)\u2225\u2225\u2225 \u03a3\u22121t\u22121 }\n\u2264 \u221a\u221a\u221a\u221aT T\u2211 t=1 Zt min { 4/\u03bb, \u2225\u2225\u2225\u03d5(\u03c4)\u2212 \u03d5(\u03c4\u0303)\u2225\u2225\u22252 \u03a3\u22121t\u22121 }\n\u22642 \u221a\u221a\u221a\u221aT T\u2211 t=1 Zt min { 1, \u2225\u2225\u2225\u03d5(\u03c4)\u2212 \u03d5(\u03c4\u0303)\u2225\u2225\u22252 \u03a3\u22121t\u22121 }\n\u22642 \u221a T \u00b7 2d log ( \u03bb+ 4T\n\u03bb ) where the last inequality is Lemma D.6. To bound (ii), we have\n(ii) = T\u2211 t=1 (1\u2212 Zt) \u2225\u2225\u2225\u03d5(\u03c4)\u2212 \u03d5(\u03c4\u0303)\u2225\u2225\u2225 \u03a3\u22121t\u22121\n\u2264 T\u2211\nt=1\n(1\u2212 Zt) \u00b7 \u03f5 \u221a \u03c0/(2\u03c3r)\n\u2264\u03f5T \u221a \u03c0/(2\u03c3r)\nwhere the first inequality is by Lemma B.6. Putting the two upper bounds together, we have\nT3 \u2264 ( \u03f5r,\u03be + \u03f5r,\u03b7 )( 2 \u221a T \u00b7 2d log ( \u03bb+ 4T\n\u03bb\n) + \u03f5T \u221a \u03c0/(2\u03c3r) + \u221a 2T log(1/\u03b4)\n\u03bb\n) .\nBounding T4. By Cauchy-Schwarz inequality, we have\nT4 \u2264 \u2223\u2223\u2223\u2223\u2223 T\u2211\nt=1 H\u2211 h=1 E sh,ah\u223c\u03c00t \u2225\u03d5(sh, ah)\u2225\u03a3\u22121t\u22121,h ( \u2225\u03beP,t,h\u2225\u03a3t\u22121,h + \u2225\u03b7P,t,h\u2225\u03a3t\u22121,h + \u2225\u03bbt,h\u2225\u03a3t\u22121,h )\u2223\u2223\u2223\u2223\u2223 \u2264 ( \u03f5P,\u03be + \u03f5P,\u03b7 + \u03f5\u03bb\n) H\u2211 h=1 T\u2211 t=1 E sh,ah\u223c\u03c00t \u2225\u03d5(sh, ah)\u2225\u03a3\u22121t\u22121,h\nSince \u2225\u03d5(sh, ah)\u2225\u03a3\u22121t\u22121,h \u2264 1/ \u221a \u03bb, by Hoeffding\u2019s inequality, we have\n\u2264 ( \u03f5P,\u03be + \u03f5P,\u03b7 + \u03f5\u03bb )( H\u2211 h=1 T\u2211 t=1 \u2225\u03d5(sh, ah)\u2225\u03a3\u22121t\u22121,h + \u221a TH log(1/\u03b4) 2\u03bb )\n= ( \u03f5P,\u03be + \u03f5P,\u03b7 + \u03f5\u03bb )( H\u2211 h=1 T\u2211 t=1 min { 1/ \u221a \u03bb, \u2225\u03d5(sh, ah)\u2225\u03a3\u22121t\u22121,h } + \u221a TH log(1/\u03b4) 2\u03bb ) \u2264 ( \u03f5P,\u03be + \u03f5P,\u03b7 + \u03f5\u03bb\n) H\u2211 h=1 \u221a\u221a\u221a\u221aT T\u2211 t=1 min { 1/\u03bb, \u2225\u03d5(sh, ah)\u22252\u03a3\u22121t\u22121,h } + \u221a TH log(1/\u03b4) 2\u03bb  Since \u03bb \u2265 1, we have\n\u2264 ( \u03f5P,\u03be + \u03f5P,\u03b7 + \u03f5\u03bb ) H\u2211 h=1 \u221a\u221a\u221a\u221aT T\u2211 t=1 min { 1, \u2225\u03d5(sh, ah)\u22252\u03a3\u22121t\u22121,h } + \u221a TH log(1/\u03b4) 2\u03bb  \u2264 ( \u03f5P,\u03be + \u03f5P,\u03b7 + \u03f5\u03bb )( H \u221a T \u00b7 2d log ( \u03bb+ T\n\u03bb\n) + \u221a TH log(1/\u03b4)\n2\u03bb ) where the last inequality is Lemma D.6.\nConclusion. Combining the above bounds, we have\u2223\u2223\u2223\u2223\u2223 T\u2211\nt=1\n( (V t \u2212 V \u03c0 0 t )\u2212 (V\u0303t \u2212 V \u03c0 1 t ) )\u2223\u2223\u2223\u2223\u2223\n\u2264 2Vmax ( Lmax + \u221a HT\n2 log(1/\u03b4)\n)\n+ ( Lmax + \u221a HT\n2 log(1/\u03b4)\n)( 2Vmax +\n\u03f5r,\u03be + \u03f5r,\u03b7 + \u03f5P,\u03be + \u03f5P,\u03b7 + \u03f5\u03bb\u221a \u03bb\n)\n+ ( \u03f5r,\u03be + \u03f5r,\u03b7 )( 2 \u221a T \u00b7 2d log ( \u03bb+ 4T\n\u03bb\n) + \u03f5T \u221a \u03c0\n2\u03c3r +\n\u221a 2T log(1/\u03b4)\n\u03bb\n)\n+ ( \u03f5P,\u03be + \u03f5P,\u03b7 + \u03f5\u03bb )( H \u221a T \u00b7 2d log ( \u03bb+ T\n\u03bb\n) + \u221a TH log(1/\u03b4)\n2\u03bb\n)\n\u2264 4Vmax ( Lmax + \u221a HT\n2 log(1/\u03b4)\n)\n+ ( \u03f5r,\u03be + \u03f5r,\u03b7 )( 2 \u221a T \u00b7 2d log ( \u03bb+ 4T\n\u03bb\n) + \u03f5T \u221a \u03c0\n2\u03c3r +\nLmax + 2 \u221a\nTH log(1/\u03b4)/2\u221a \u03bb\n)\n+ ( \u03f5P,\u03be + \u03f5P,\u03b7 + \u03f5\u03bb )( H \u221a T \u00b7 2d log ( \u03bb+ T\n\u03bb\n) + Lmax + 2 \u221a TH log(1/\u03b4)/2\u221a \u03bb )\n= O\u0303 ( VmaxLmax + Vmax \u221a HT\n+ (\u03f5r,\u03be + \u03f5r,\u03b7) (\u221a dT + Lmax + \u221a HT + \u03f5T \u221a \u03c0\n2\u03c3r ) + (\u03f5P,\u03be + \u03f5P,\u03b7 + \u03f5\u03bb) ( H \u221a dT + Lmax + \u221a HT )) .\nLemma B.16 (Regret decomposition). Assume all events listed in Lemma B.12 hold. Then, we have T\u2211\nt=1\n(V \u22c6 \u2212 V t) + (V\u0303t \u2212 V \u03c0 1 t )\n\u2264 1 F2(\u22121) T\u2211 t=1 ( (V t \u2212 V \u03c0 0 t )\u2212 (V\u0303t \u2212 V \u03c0 1 t ) + (V \u03c0 0 t \u2212 V \u2212t )\u2212 (V \u03c0 1 t \u2212 V\u0303 \u2212t ) ) +\n2Vmax F2(\u22121)\n(\u221a T log(1/\u03b4)\n2 + 4\u03b4 ) with probability at least 1\u2212 \u03b4.\nProof. We note that, at any round t \u2208 [T ], conditioning on all information collected up to round t \u2212 1, the randomness of V t and V\u0303t only comes from the randomness of Gaussian noise variables \u03ber,t, \u03beP,t,1, . . . , \u03beP,t,H . In other words, the values of V t and V\u0303t are determined once given these Gaussian noise variables. In light of this, we write out the dependence on the Gaussian noise variables explicitly: we treat V t and V\u0303t as functions of \u03ber,t, \u03beP,t,1, \u00b7 \u00b7 \u00b7 , \u03beP,t,H and define V t[\u03ber,t, \u03beP,t,1, \u00b7 \u00b7 \u00b7 , \u03beP,t,H ] and V\u0303t[\u03be\u2212r,t, \u03be\u2212P,t,1, \u00b7 \u00b7 \u00b7 , \u03be \u2212 P,t,H ] as the values of V t and V\u0303t obtained at round t with the Gaussian noise variables \u03ber,t, \u03beP,1, \u00b7 \u00b7 \u00b7 , \u03beP,H . Then, we define a notion of \u201cworstcase\u201d Gaussian noise variables as follows:\n\u03be\u2212r,t, \u03be \u2212 P,t,1, \u00b7 \u00b7 \u00b7 , \u03be \u2212 P,t,H := argmin\n\u03be\u2212r,t,\u03be \u2212 P,t,1,\u00b7\u00b7\u00b7 ,\u03be \u2212 P,t,H\nV t[\u03be \u2212 r,t, \u03be \u2212 P,t,1, \u00b7 \u00b7 \u00b7 , \u03be \u2212 P,t,H ]\u2212 V\u0303t[\u03be \u2212 r,t, \u03be \u2212 P,t,1, \u00b7 \u00b7 \u00b7 , \u03be \u2212 P,t,H ]\ns.t. \u2225\u03be\u2212r,t\u2225\u03a3t\u22121 \u2264 \u03f5r,\u03be and \u2200h \u2208 [H] : \u2225\u03be\u2212P,t,h\u2225\u03a3t\u22121,h \u2264 \u03f5P,\u03be.\nAnd we denote V \u2212 t and V\u0303 \u2212 t as the value functions specified by \u03be \u2212 r,t, \u03be \u2212 P,t,1, \u00b7 \u00b7 \u00b7 , \u03be \u2212 P,t,H :\nV \u2212 t := V t[\u03be \u2212 r,t, \u03be \u2212 P,t,1, \u00b7 \u00b7 \u00b7 , \u03be \u2212 P,t,H ], V\u0303 \u2212 t := V\u0303t[\u03be \u2212 r,t, \u03be \u2212 P,t,1, \u00b7 \u00b7 \u00b7 , \u03be \u2212 P,t,H ].\nIn other words, V \u2212 t and V\u0303 \u2212 t are counterparts of V t and V\u0303t that attain the smallest difference, V t\u2212V\u0303t, while the noise variables still satisfy the high probability bounds. By Lemma B.12, we immediately have\nPr(Elow) := Pr ( V \u2212 t \u2212 V\u0303 \u2212t \u2264 V t \u2212 V\u0303t ) \u2265 1\u2212 \u03b4/T. (11)\nWe note that here we have \u03b4/T instead of \u03b4 because we are considering a fixed t and the results of Lemma B.12 are derived by the union bound over all t \u2208 [T ] \u2014 thus, the probability of the event Elow is at least 1\u2212 \u03b4/T for a single t.\nThen, we denote Eopt as the event that V t \u2212 V\u0303t \u2265 V \u22c6 \u2212 V \u03c0 1 t . Thus, by Lemma B.13, we have Pr(Eopt) \u2265 F2(\u22121). Moreover, we denote palg as the randomness of the algorithm. Then, we define the joint distribution popt of V t and V\u0303t by restricting palg to the event Eopt. Specifically, it is defined as\npopt(V t, V\u0303t) := { palg(V t, V\u0303t)/Pr(Eopt) if Eopt 0 otherwise\nLet z := V t \u2212 V\u0303t. Then, we have\n(V \u22c6 \u2212 V t) + (V\u0303t \u2212 V \u03c0 1 t ) \u2264V \u22c6 \u2212 V \u2212t + V\u0303 \u2212t \u2212 V \u03c0 1 t\n= E z\u223cpopt\n[ V \u22c6 \u2212 V \u2212t \u2212 z + z + V\u0303 \u2212t \u2212 V \u03c0 1 t ] \u2264 E\nz\u223cpopt\n[ z \u2212 V \u2212t + V\u0303 \u2212t ] =\n\u222b z 1{Eopt} \u00b7 palg(z) Pr(Eopt) \u00b7 (z \u2212 V \u2212t + V\u0303 \u2212t ) dz\n= E z\u223cpalg\n[ 1{Eopt} ( z \u2212 V \u2212t + V\u0303 \u2212t )] /Pr(Eopt)\nwhere the second inequality is by the definition of popt, which rules out the case that Eopt does not hold. Considering the event Elow, we have\n(V \u22c6 \u2212 V t) + (V\u0303t \u2212 V \u03c0 1 t ) \u2264 (\nE z\u223cpalg\n[ 1{Eopt}1{Elow} ( z \u2212 V \u2212t + V\u0303 \u2212t )] + E\nz\u223cpalg\n[ 1{Eopt}1{E\u2201low} ( z \u2212 V \u2212t + V\u0303 \u2212t )]) /Pr(Eopt)\n\u2264 (\nE z\u223cpalg\n[ 1{Eopt}1{Elow} ( z \u2212 V \u2212t + V\u0303 \u2212t )] + E\nz\u223cpalg\n[ 1{Eopt}1{E\u2201low}4Vmax ]) /Pr(Eopt)\n\u2264 (\nE z\u223cpalg\n[ 1{Eopt}1{Elow} ( z \u2212 V \u2212t + V\u0303 \u2212t )] + 4Vmax\u03b4/T ) /Pr(Eopt)\nwhere the last inequality is by (11). For the first term, we have\nE z\u223cpalg\n[ 1{Eopt}1{Elow} ( z \u2212 V \u2212t + V\u0303 \u2212t )] \u2264 E\nz\u223cpalg\n[ 1{Elow} ( z \u2212 V \u2212t + V\u0303 \u2212t )] = E\nz\u223cpalg\n[( z \u2212 V \u2212t + V\u0303 \u2212t )] + E\nz\u223cpalg\n[ 1{E\u2201low} ( z \u2212 V \u2212t + V\u0303 \u2212t )] \u2264 E\nz\u223cpalg\n[( z \u2212 V \u2212t + V\u0303 \u2212t )] + 4Vmax\u03b4/T\n= E [ V t \u2212 V\u0303t \u2212 V \u2212 t + V\u0303 \u2212 t ] + 4Vmax\u03b4/T\n= E [ (V t \u2212 V \u03c0 0 t )\u2212 (V\u0303t \u2212 V \u03c0 1 t ) ] + E [ (V \u03c0 0 t \u2212 V \u2212t )\u2212 (V \u03c0 1 t \u2212 V\u0303 \u2212t ) ] + 4Vmax\u03b4/T.\nwhere the first inequality holds since z \u2212 V \u2212t + V\u0303 \u2212t \u2265 0 conditioning on Elow, and the second inequality is by (11) again.\nRecall that Pr(Eopt) \u2265 F2(\u22121). Inserting all of these back, we obtain T\u2211 t=1 (V \u22c6 \u2212 V t) + (V\u0303t \u2212 V \u03c0 1 t )\n\u2264 F\u22122(\u22121) ( 8Vmax\u03b4 +\nT\u2211 t=1 ( E [ (V t \u2212 V \u03c0 0 t )\u2212 (V\u0303t \u2212 V \u03c0 1 t ) ] + E [ (V \u03c0 0 t \u2212 V \u2212t )\u2212 (V \u03c0 1 t \u2212 V\u0303 \u2212t )\n]))\n\u2264 F\u22122(\u22121) ( 8Vmax\u03b4 +\nT\u2211 t=1 ( (V t \u2212 V \u03c0 0 t )\u2212 (V\u0303t \u2212 V \u03c0 1 t ) + (V \u03c0 0 t \u2212 V \u2212t )\u2212 (V \u03c0 1 t \u2212 V\u0303 \u2212t ) ) + 2Vmax \u221a T log(1/\u03b4)\n2\n) .\nwhere the last inequality is the Hoeffding\u2019s inequality.\nGiven all these lemmas, we are ready to establish an upper bound of regret. We first note that, since \u03c01t = \u03c0 0 t\u22121 for all t, the regret incurred by \u03c0 1 t for all t is equivalent to that incurred by \u03c0 0 t for all t. Hence, it suffices to compute the regret incurred by \u03c00t for t \u2208 [T ] and multiply it by two to get the total regret.\nWe start with the following regret decomposition:\nRegretT \u2264 T\u2211\nt=1\n( V \u22c6 \u2212 V \u03c0 0 t ) = T\u2211 t=1 ( V \u22c6 \u2212 V t + V\u0303t \u2212 V \u03c0 1 t ) \ufe38 \ufe37\ufe37 \ufe38\n(\u2217)\n+ T\u2211 t=1 ( V t \u2212 V \u03c0 0 t + V \u03c0 1 t \u2212 V\u0303t ) .\nBy Lemma B.16, we can further decompose (\u2217) and obtain\nRegretT \u2264 1\nF2(\u22121) T\u2211 t=1 ( V \u03c0 0 t \u2212 V \u2212t + V\u0303 \u2212t \u2212 V \u03c01t t ) \ufe38 \ufe37\ufe37 \ufe38\n(i)\n+ ( 1 +\n1\nF2(\u22121) ) T\u2211 t=1 ( V t \u2212 V \u03c0 0 t + V \u03c0 1 t \u2212 V\u0303t ) \ufe38 \ufe37\ufe37 \ufe38\n(ii)\n+ 2Vmax F2(\u22121)\n(\u221a T log(1/\u03b4)\n2 + 4\u03b4\n) .\nWe note that both (i) and (ii) can be bounded by Lemma B.15: (i), (ii) \u2264 O\u0303 ( VmaxLmax + Vmax \u221a HT\n+ (\u03f5r,\u03be + \u03f5r,\u03b7) (\u221a dT + Lmax + \u221a HT + \u03f5T \u221a \u03c0\n2\u03c3r ) + (\u03f5P,\u03be + \u03f5P,\u03b7 + \u03f5\u03bb) ( H \u221a dT + Lmax + \u221a HT\n)) Inserting this back, we obtain\nRegretT = O\u0303 ( VmaxLmax + Vmax \u221a HT\n+ (\u03f5r,\u03be + \u03f5r,\u03b7) (\u221a dT + Lmax + \u221a HT + \u03f5T \u221a \u03c0\n2\u03c3r ) + (\u03f5P,\u03be + \u03f5P,\u03b7 + \u03f5\u03bb) ( H \u221a dT + Lmax + \u221a HT\n) + Vmax \u221a T\n) To compute this quantity, we note the following asymptotic rate:\n\u2022 \u03f5r,\u03be + \u03f5r,\u03b7 = O\u0303 ( d \u221a \u03ba+B2 ) \u2022 \u03f5P,\u03be + \u03f5P,\u03b7 + \u03f5\u03bb = O\u0303 ( d5/2H3/2 \u221a \u03ba+B2\n) \u2022 Lmax = O\u0303 ( d6H4(\u03ba+B2)\n) Hence, we have\nRegretT = O\u0303\n( \u03f5T \u221a d+ \u221a T \u00b7 d3H5/2 \u221a \u03ba+B2 + d17/2H11/2(\u03ba+B2)3/2 ) ."
        },
        {
            "heading": "B.5 BOUNDING NUMBER OF QUERIES",
            "text": "Recall that the number of queries are computed via\nQueriesT = T\u2211 t=1 Zt = T\u2211 t=1 Zt1\n{ E\n\u03b80,\u03b81\u223cN (\u03b8\u0302r,t,\u03c32r\u03a3 \u22121 t\u22121)\n[\u2223\u2223\u2223(\u03d5(\u03c40t )\u2212 \u03d5(\u03c41t ))\u22a4(\u03b80 \u2212 \u03b81)\u2223\u2223\u2223] > \u03f5 } .\nNotice that Cauchy-Schwarz inequality implies\nE \u03b80,\u03b81 \u2223\u2223\u2223(\u03d5(\u03c40t )\u2212 \u03d5(\u03c41t ))\u22a4(\u03b80 \u2212 \u03b81)\u2223\u2223\u2223 \u2264 \u2225\u03d5(\u03c40t )\u2212 \u03d5(\u03c41t )\u2225\u03a3\u22121t\u22121 E\u03b80,\u03b81 \u2225\u03b80 \u2212 \u03b81\u2225\u03a3t\u22121 \u2264 1\u221a\n\u03bb \u2225\u03d5(\u03c40t )\u2212 \u03d5(\u03c41t )\u22252 E \u03b80,\u03b81 \u2225\u03b80 \u2212 \u03b81\u2225\u03a3t\u22121 \u2264 2\u221a \u03bb E \u03b80,\u03b81 \u2225\u03b80 \u2212 \u03b81\u2225\u03a3t\u22121 \u2264 2 \u221a 2\u03c32r d\u221a \u03bb\nwhere the second inequality is by \u2225\u03a3\u22121t\u22121\u22252 = 1/\u03bb, and the last step is by the fact that \u03b80\u2212\u03b81 follows N (0, 2\u03c32r\u03a3\u22121t\u22121) and Lemma D.3. We denote \u03b6 := 2\u03c3r \u221a 2d/\u03bb for the ease of notation. Then, we\nhave\nQueriesT = T\u2211 t=1 Zt1 { min { \u03b6, E \u03b80,\u03b81 [\u2223\u2223\u2223(\u03d5(\u03c40t )\u2212 \u03d5(\u03c41t ))\u22a4(\u03b80 \u2212 \u03b81)\u2223\u2223\u2223]} > \u03f5}\n\u2264 T\u2211\nt=1\nZt1\n{ min { 1, E\n\u03b80,\u03b81 [\u2223\u2223\u2223(\u03d5(\u03c40t )\u2212 \u03d5(\u03c41t ))\u22a4(\u03b80 \u2212 \u03b81)\u2223\u2223\u2223]} > \u03f5/\u03b6} Applying Cauchy-Schwarz inequality, we have\nQueriesT \u2264 T\u2211\nt=1\nZt1\n{ min { 1, E\n\u03b80,\u03b81\n[( (\u03d5(\u03c40t )\u2212 \u03d5(\u03c41t ))\u22a4(\u03b80 \u2212 \u03b81) )2]} > \u03f52/\u03b62 } .\nLet u0, u1 denotes two independent standard Gaussian variables with zero mean and identity covariance matrix. Then, \u03b80 \u2212 \u03b81 has the same joint distribution as \u03c3r\u03a3\u22121/2t\u22121 (u0 \u2212 u1). Hence, we can rewrite the expectation in the indicator as\nE \u03b80,\u03b81\n[( (\u03d5(\u03c40t )\u2212 \u03d5(\u03c41t ))\u22a4(\u03b80 \u2212 \u03b81) )2] = E\nu0,u1\n[(( \u03d5(\u03c40t )\u2212 \u03d5(\u03c41t ) )\u22a4 \u03c3r\u03a3 \u22121/2 t\u22121 (u0 \u2212 u1) )2] Furthermore, we have\nE u0,u1\n[(( \u03d5(\u03c40t )\u2212 \u03d5(\u03c41t ) )\u22a4 \u03c3r\u03a3 \u22121/2 t\u22121 (u0 \u2212 u1) )2] = E\nu0,u1\n[( \u03d5(\u03c40t )\u2212 \u03d5(\u03c41t ) )\u22a4 \u03c3r\u03a3 \u22121/2 t\u22121 (u0 \u2212 u1) (u0 \u2212 u1) \u22a4 \u03c3r\u03a3 \u22121/2 t\u22121 ( \u03d5(\u03c40t )\u2212 \u03d5(\u03c41t ) )] = ( \u03d5(\u03c40t )\u2212 \u03d5(\u03c41t ) )\u22a4 \u03c3r\u03a3 \u22121/2 t\u22121 E\nu0,u1\n[ (u0 \u2212 u1) (u0 \u2212 u1)\u22a4 ] \u03c3r\u03a3 \u22121/2 t\u22121 ( \u03d5(\u03c40t )\u2212 \u03d5(\u03c41t ) ) .\nFor the expectation in the middle, we have\nE u0,u1\n[ (u0 \u2212 u1) (u0 \u2212 u1)\u22a4 ] = E[u0u\u22a40 ] + E[u1u\u22a41 ] = 2I\nwhere we have used the fact that E[u0u\u22a41 ] = 0 by independence. Therefore, we have\nE \u03b80,\u03b81\n[(( \u03d5(\u03c40t )\u2212 \u03d5(\u03c41t ) )\u22a4 (\u03b80 \u2212 \u03b81) )2] = ( \u03d5(\u03c40t )\u2212 \u03d5(\u03c41t ) )\u22a4 \u03c3r\u03a3 \u22121/2 t\u22121 (2I)\u03c3r\u03a3 \u22121/2 t\u22121 ( \u03d5(\u03c40t )\u2212 \u03d5(\u03c41t )\n) =2\u03c32r\n\u2225\u2225\u03d5(\u03c40t )\u2212 \u03d5(\u03c41t )\u2225\u22252\u03a3\u22121t\u22121 . Inserting this back, we obtain\nQueriesT \u2264 T\u2211\nt=1\nZt1 { min { 1, 2\u03c32r \u2225\u2225\u03d5(\u03c40t )\u2212 \u03d5(\u03c41t )\u2225\u22252\u03a3\u22121t\u22121} > \u03f52/\u03b62} \u2264\nT\u2211 t=1 Zt1 { min { 1, \u2225\u2225\u03d5(\u03c40t )\u2212 \u03d5(\u03c41t )\u2225\u22252\u03a3\u22121t\u22121} > \u03f522\u03b62\u03c32r }\n\u22642\u03b6 2\u03c32r \u03f52 T\u2211 t=1 Zt min { 1, \u2225\u2225\u03d5(\u03c40t )\u2212 \u03d5(\u03c41t )\u2225\u22252\u03a3\u22121t\u22121}\n\u22642\u03b6 2\u03c32r \u03f52\n\u00b7 2d log ( \u03bb+ 4T\n\u03bb ) where the last step is Lemma D.6. Plugging the value of \u03b6 and all other variables, we obtain\nQueriesT \u2264 32\u03c34r d 2\n\u03bb\u03f52 log\n( \u03bb+ 4T\n\u03bb\n) = O\u0303 ( d4(\u03ba+B2)2\n\u03f52\n) ."
        },
        {
            "heading": "C PROOF OF THEOREM 5.4",
            "text": "We first present some supporting results in Appendix C.1. Then, we prove the upper bound of Bayesian regret in Appendix C.2 and the number of queries in Appendix C.3."
        },
        {
            "heading": "C.1 SUPPORTING LEMMAS",
            "text": "In Appendix C.1.1, we establish some supporting results for a probability estimation problem in the frequentist setting. In Appendix C.1.2, we adapt these results to the Bayesian setting. The Bayesian results will be heavily used later in the proof of Theorem 5.4 in Appendices C.2 and C.3."
        },
        {
            "heading": "C.1.1 SUPPORTING RESULTS FROM FREQUENTIST SETTING",
            "text": "Consider a conditional probability estimation problem. Let X and Y be the instance space and the target space, respectively. Let F : (X \u00d7 Y) \u2192 R be a function class. We are given a dataset D := {(xi, yi)}ni=1 where xi \u223c Di and yi \u223c f\u22c6(x, \u00b7). We assume f\u22c6 \u2208 F . Regarding the data generation process, we assume the data distribution Di is history-dependent, i.e., xi can depend on the previous samples: x1, y1, . . . , xi\u22121, yi\u22121 for any i \u2208 [n]. Our goal is to estimate the true conditional probability f\u22c6 using the dataset D.\nAt a high level, this problem is designed to capture both the reward learning and the model learning problems in the RL setting. Specifically, in the reward learning problem, we will instantiate X = S \u00d7 A and Y = {0, 1}, where we recall that S and A are the state space and the action space, respectively, and the preference feedback is binary. In the model learning problem, we can instantiate X = S \u00d7A and Y = S. We abstract these two problems into this conditional probability estimation problem to make the analysis more concise. However, one caveat is that all we derived in this section are frequentist results, while we are considering Bayesian RL. Thus, the results are not directly applicable. In Appendix C.1.2, we will adapt these frequentist results to the Bayesian setting so that they can be applied.\nNow we establish some important results for this problem. First, we have the following lemma, which is a consequence of Lemma D.10.\nLemma C.1 (MLE generalization bound). Fix \u03b4 \u2208 (0, 1). Follow the setting stated above. Let f\u0302 be the maximum likelihood estimator:\nf\u0302 = argmax f\u2208F n\u2211 i=1 log f(xi, yi)."
        },
        {
            "heading": "Define the version space:",
            "text": "VF = { f \u2208 F :\nn\u2211 i=1 d2TV ( f\u0302(xi, \u00b7), f(xi, \u00b7) ) \u2264 \u03b2F (n) } where \u03b2F (n) := 98 log(2N[]((n|Y|)\u22121,F , \u2225 \u00b7 \u2225\u221e)/\u03b4). Then, the following holds\n(i) f\u22c6 \u2208 VF with probability at least 1\u2212 \u03b4,\n(ii) for any f, f \u2032 \u2208 VF , it holds that n\u2211\ni=1\nd2TV ( f(xi, \u00b7), f \u2032(xi, \u00b7) ) \u2264 4\u03b2F (n).\nProof of Lemma C.1. We first construct an auxiliary version space V\u0303 as follows\nV\u0303F = { f \u2208 F :\nn\u2211 i=1 E x\u223cDi d2TV ( f\u0302(x, \u00b7), f(x, \u00b7) ) \u2264 10 log ( N[] ( (n|Y|)\u22121,F , \u2225 \u00b7 \u2225\u221e ) /\u03b4 )}\nBy Lemma D.10, f\u22c6 \u2208 V\u0303F with probability at least 1\u2212 \u03b4. To prove (i), we will show that whenever f\u22c6 \u2208 V\u0303F , we have f\u22c6 \u2208 VF as well with high probability. Let F[] denote an (n|Y|)\u22121-bracket of F\nwith respect to \u2225 \u00b7 \u2225\u221e. Then for all f[] \u2208 F[], the following holds with probability at least 1 \u2212 \u03b4 by Lemma D.4 and the union bound on F[],\nn\u2211 i=1 d2TV ( f[](xi, \u00b7), f\u22c6(xi, \u00b7) ) \u2264 2\nn\u2211 i=1 E x\u223cDi d2TV ( f[](x, \u00b7), f\u22c6(x, \u00b7) ) + 4 log ( N[] ( (n|Y|)\u22121,F , \u2225 \u00b7 \u2225\u221e ) /\u03b4 ) . (12)\nNow for any f \u2208 F , there must exist f[] \u2208 F[] such that \u2225f \u2212 f[]\u2225\u221e \u2264 (n|Y|)\u22121, which impies the following\nn\u2211 i=1 d2TV ( f(xi, \u00b7), f[](xi, \u00b7) ) \u2264 n\u2211 i=1 |Y|2\u2225f \u2212 f[]\u22252\u221e \u2264 n\u2211 i=1 n\u22122 = 1/n, (13)\nn\u2211 i=1 E x\u223cDi d2TV ( f(x, \u00b7), f[](x, \u00b7) ) \u2264 n\u2211 i=1 |Y|2\u2225f \u2212 f[]\u22252\u221e \u2264 n\u2211 i=1 n\u22122 = 1/n (14)\nHence, for all f \u2208 F , we have\nn\u2211 i=1 d2TV ( f(xi, \u00b7), f\u22c6(xi, \u00b7) ) \u22642\nn\u2211 i=1 d2TV ( f[](xi, \u00b7), f\u22c6(xi, \u00b7) ) + 2 n\u2211 i=1 d2TV ( f(xi, \u00b7), f[](xi, \u00b7) ) \u2264 2\nn\u2211 i=1 d2TV ( f[](xi, \u00b7), f\u22c6(xi, \u00b7) ) + 2/n\n\u2264 4 n\u2211\ni=1\nE x\u223cDi\nd2TV ( f[](x, \u00b7), f\u22c6(x, \u00b7) ) + 8 log ( N[] ( (n|Y|)\u22121,F , \u2225 \u00b7 \u2225\u221e ) /\u03b4 ) + 2/n\n\u2264 8 n\u2211\ni=1\nE x\u223cDi\nd2TV (f(x, \u00b7), f\u22c6(x, \u00b7)) + 8 log ( N[] ( (n|Y|)\u22121,F , \u2225 \u00b7 \u2225\u221e ) /\u03b4 ) + 10/n.\nwith probability at least 1\u2212 \u03b4. Here the second inequality uses (13), the third uses (12), and the last uses (14). Therefore, for any possible value of the estimator f\u0302 , conditioning on f\u22c6 \u2208 V\u0303F , we have\nn\u2211 i=1 d2TV ( f\u0302(xi, \u00b7), f\u22c6(xi, \u00b7) ) \u22648\nn\u2211 i=1 E x\u223cDi d2TV ( f\u0302(x, \u00b7), f\u22c6(x, \u00b7) ) + 8 log ( N[] ( (n|Y|)\u22121,F , \u2225 \u00b7 \u2225\u221e ) /\u03b4 ) + 10/n\n\u226488 log ( N[] ( (n|Y|)\u22121,F , \u2225 \u00b7 \u2225\u221e ) /\u03b4 ) + 10/n\n\u226498 log ( N[] ( (n|Y|)\u22121,F , \u2225 \u00b7 \u2225\u221e ) /\u03b4 )\nwith probability at least 1 \u2212 \u03b4. Here the second inequality is by the definition of V\u0303F . The last inequality holds since it is reasonable to assume that the bracketing number is at least some constant so that log(N[]((n|Y|)\u22121,F , \u2225 \u00b7 \u2225\u221e)/\u03b4) > 1 \u2265 1/n.\nThe above means whenever f\u22c6 \u2208 V\u0303F , we have f\u22c6 \u2208 VF as well with probability at least 1 \u2212 \u03b4. Since f\u22c6 \u2208 V\u0303F with probability at least 1 \u2212 \u03b4, we have f\u22c6 \u2208 VF with probability at least 1 \u2212 2\u03b4. Adjusting \u03b4 completes the proof of (i).\nFor (ii), we have n\u2211\ni=1\nd2TV\n( f(x, \u00b7), f \u2032(x, \u00b7) ) \u2264 n\u2211 i=1 ( dTV ( f(x, \u00b7), f\u0302(x, \u00b7) ) + dTV ( f\u0302(x, \u00b7), f \u2032(x, \u00b7) ))2 \u2264\nn\u2211 i=1 2d2TV ( f(x, \u00b7), f\u0302(x, \u00b7) ) + n\u2211 i=1 2d2TV ( f\u0302(x, \u00b7), f \u2032(x, \u00b7) ) \u22644\u03b2F (n)\nwhere the second inequality is by the fact that (a + b)2 \u2264 2a2 + 2b2 for any a and b, and the third inequality is by the definition of VF .\nNext, we further assume that the function class F is parameterized by a function class G via a link function \u03a6. The reason for this assumption is that we want to capture the structure of the reward learning problem, where the feedback generating distribution is parameterized by the reward function via a link function \u03a6. Assumption C.2 (Binary label and function parameterization.). Assume Y = {0, 1} is binary, and there is a function class G \u2286 X \u2192 [0, G] that parameterizes F via a link function \u03a6. Specifically, we assume\nF = { f(x, 0) = \u03a6 ( g(x) ) , f(x, 1) = 1\u2212 \u03a6(g(x)) : g \u2208 G } ,\nwhere we further assume \u03a6 satisfies Assumption 3.1. For any f \u2208 F , let gf denote the function g that parameterizes f . We define g\u0302 := gf\u0302 and g \u22c6 := gf\u22c6 .\nAs a preliminary note, the function class G will actually correspond to the function class R\u0303 (3) later in the proof.\nWe should emphasize again that, although the function class G aims to capture the structure of the reward learning problem, the function class F will be used to capture both the reward learning and the model learning problems. When it is applied to the model learning problem, we can simply ignore the function class G and any results that are related to G. On the other hand, when it is applied to the reward learning problem, such a function class G will be helpful to derive some results in the reward function itself.\nBefore diving into the analysis of the function class G, we first introduce the following lemma which shows that the bracketing numbers of F and G are bounded by each other. Lemma C.3. Under Assumption C.2, for any \u03c9 > 0, we have\nN[] ( \u03c9,F , \u2225 \u00b7 \u2225\u221e ) \u2264 N[] ( \u03ba\u03c9,G, \u2225 \u00b7 \u2225\u221e ) and N[] ( \u03ba\u03c9,G, \u2225 \u00b7 \u2225\u221e ) \u2264 N[] ( \u03c9,F , \u2225 \u00b7 \u2225\u221e ) .\nProof. For any f, f \u2032 \u2208 F , we assume they are parameterized by g := gf and g\u2032 := gf \u2032 , respectively. Then, we have\nsup x,y |f(x, y)\u2212 f \u2032(x, y))| = sup x \u2223\u2223\u03a6(g(x))\u2212 \u03a6(g\u2032(x))\u2223\u2223 \u2264 \u03ba\u22121 sup x |g(x)\u2212 g\u2032(x)|\nwhere the inequality is Lemma D.1. Hence, if we have a \u03ba\u03c9-bracket of G in the infinite norm, then we have an \u03c9-bracket of F in the infinite norm. This proves the first claim. For the second claim, by Lemma D.1, we have\nsup x |g(x)\u2212 g\u2032(x)| \u2264 \u03ba sup x \u2223\u2223\u03a6(g(x))\u2212 \u03a6(g\u2032(x))\u2223\u2223 = \u03ba sup x,y |f(x, y)\u2212 f \u2032(x, y)| .\nThis proves the second claim.\nThen, Lemma C.1 leads to similar results with a version space constructed on G. Corollary C.4. Under Assumption C.2, define\nVG = { g \u2208 G :\nn\u2211 i=1 ( g\u0302(xi)\u2212 g(xi) )2 \u2264 \u03b2G(n) } where \u03b2G(n) := 98\u03ba2 log(2N[](\u03ba(n|Y|)\u22121,G, \u2225 \u00b7 \u2225\u221e)/\u03b4) and we denote g\u0302 := gf\u0302 as the function that parameterizes the maximum likelihood estimator f\u0302 . Then, the following holds\n(1) g\u22c6 \u2208 VG with probability at least 1\u2212 \u03b4\n(2) for any g, g\u2032 \u2208 VG , we have n\u2211\ni=1\n( g(xi)\u2212 g\u2032(xi) )2 \u2264 4\u03b2G(n).\nProof of Corollary C.4. To prove (i), we claim that for any f \u2208 VF (defined in Lemma C.1), we have gf \u2208 VG as well. To see this, we note that\nn\u2211 i=1 E x\u223cDi ( g\u0302(x)\u2212 gf (x) )2 \u2264 \u03ba2 n\u2211 i=1 E x\u223cDi \u2223\u2223\u03a6(g\u0302(x))\u2212 \u03a6(gf (x))\u2223\u22232 =\u03ba2\nn\u2211 i=1 E x\u223cDi \u2223\u2223\u2223f\u0302(x, 0)\u2212 f(x, 0)\u2223\u2223\u22232 = \u03ba2 n\u2211 i=1 E x\u223cDi d2TV ( f\u0302(x, \u00b7), f(x, \u00b7) ) where the inequality is Lemma D.1, and the last equality holds since we assume Y = {0, 1} is binary. Hence, for any f \u2208 VF , it holds that\nn\u2211 i=1 E x\u223cDi ( g\u0302(x)\u2212 gf (x) )2 \u2264\u03ba2 n\u2211 i=1 E x\u223cDi d2TV ( f\u0302(x, \u00b7), f(x, \u00b7) ) \u2264\u03ba2\u03b2F (n) =\u03ba2 \u00b7 98 log(2N[]((n|Y|)\u22121,F , \u2225 \u00b7 \u2225\u221e)/\u03b4)\nBy Lemma C.3, we have log(2N[]((n|Y|)\u22121,F , \u2225 \u00b7 \u2225\u221e)/\u03b4) \u2264 log(2N[](\u03ba(n|Y|)\u22121,G, \u2225 \u00b7 \u2225\u221e)/\u03b4).\nHence, we have n\u2211\ni=1\nE x\u223cDi\n( g\u0302(x)\u2212 gf (x) )2 \u2264 \u03b2G(n)\nwhich implies gf \u2208 VG . Therefore, whenever f\u22c6 \u2208 VF , we have gf\u22c6 \u2208 VG as well. Since f\u22c6 \u2208 VF with probability at least 1\u2212 \u03b4, we have gf\u22c6 \u2208 VG with probability at least 1\u2212 \u03b4. This completes the proof of (i).\nNow we prove (2). For any g, g\u2032 \u2208 VG , we have n\u2211\ni=1\n( g(xi)\u2212 g\u2032(xi) )2 \u2264 2 n\u2211 i=1 ( g\u0302(xi)\u2212 g(xi) )2 + 2 n\u2211 i=1 ( g\u0302(xi)\u2212 g\u2032(xi) )2 \u2264 4\u03b2G(n)\nwhere the first inequality is by the fact that (a+ b)2 \u2264 2a2 + 2b2 for any a and b."
        },
        {
            "heading": "C.1.2 ADAPTING RESULTS INTO BAYESIAN SETTING",
            "text": "Now we change the setting defined in Appendix C.1.1 into a Bayesian online setting. The reason for this adaptation is to make these results applicable to the Bayesian RL setting. We formally define the Bayesian online conditional probability estimation problem below.\nLet X and Y be the instance space and the target space, respectively. Let F : (X \u00d7 Y) \u2192 R be a function class. The interaction proceeds for T rounds. At each round t \u2208 [T ], we observe an instance xt \u2208 X and need to outputs a function ft \u2208 F as our prediction. Then, the label yt \u2208 Y is revealed. We assume that xi \u223c Di for some distribution Di and yi \u223c f\u22c6(x, \u00b7). We assume the true conditional distribution f\u22c6 is sampled from some known prior distribution \u03c1 \u2208 \u2206(F) at the beginning. Regarding the data generation process, we assume the data distribution Di is historydependent, i.e., xi can depend on the previous samples: x1, y1, . . . , xi\u22121, yi\u22121 for any i \u2208 [n]. Denote Ht as the history up to round t, i.e., Ht = {x1, f1, y1, x2, f2, y2, . . . , xt, ft, yt}. Define the maximum likelihood estimator of f\u22c6 on the dataset {(xs, ys)}t\u22121s=1 as f\u0302t, i.e.,\nf\u0302t = argmax f\u2208F t\u22121\u2211 s=1 log f(xs, ys).\nSimilar to the previous section, we will also consider the presence of a function class G (Assumption C.2) in the analysis presented in this section. It\u2019s worth noting that although Assumption C.2 is initially introduced in the frequentist setting, when we mention that Assumption C.2 holds in this section, we are actually considering the Bayesian setting.\nLemma C.5. It holds that\nT\u2211 t=1 E Ht\u22121 [ E f,f \u2032 [ dTV ( f(xt), f \u2032(xt) ) \u2223\u2223\u2223Ht\u22121]] \u2264 6\u221aT\u03b2\u2032F (T ) \u00b7 dim1(F , 1/T) \u00b7 log(T ) (15)\nand\nT\u2211 t=1 E Ht\u22121 [ E f,f \u2032 [ d2TV ( f(xt), f \u2032(xt) ) \u2223\u2223\u2223Ht\u22121]] \u2264 4\u03b2\u2032F (T ) + 2 (16)\nwhere \u03b2\u2032F (t) := 98 log(2TN[]((t|Y|)\u22121,F , \u2225 \u00b7 \u2225\u221e)), and f, f \u2032 are sampled from the posterior of f\u22c6 conditioning on Ht\u22121 in the inner conditional expectation. Moreover, if F is parameterized by G (i.e., if Assumption C.2 holds), we have\nT\u2211 t=1 E Ht\u22121 [ E g,g\u2032 [\u2223\u2223g(xt)\u2212 g\u2032(xt)\u2223\u2223 \u2223\u2223\u2223Ht\u22121]] \u2264 6\u221aT\u03b2\u2032G(T ) \u00b7 dim1(G, 1/T) \u00b7 log(GT ) (17) and\nT\u2211 t=1 E Ht\u22121 [ E g,g\u2032 [( g(xt)\u2212 g\u2032(xt) )2 \u2223\u2223\u2223Ht\u22121]] \u2264 4\u03b2\u2032G(T ) + 2 (18) where \u03b2\u2032G(t) := 98\u03ba\n2 log(2GTN[](\u03ba(t|Y|)\u22121,G, \u2225 \u00b7 \u2225\u221e)), and g, g\u2032 are sampled from the posterior of g\u22c6 conditioning onHt\u22121 in the inner conditional expectation.\nProof of Lemma C.5. We will only prove (15) and (16) since the proof of (17) and (18) is almost identical.\nProof of (15). We define the version space at round t for t \u2208 [T ] as follows\nVFt =\n{ f \u2208 F :\nt\u22121\u2211 s=1 d2TV ( f\u0302t(xs, \u00b7), f(xs, \u00b7) ) \u2264 \u03b2F (t\u2212 1)\n}\nwhere f\u0302t is the maximum likelihood estimator on the dataset {(xs, ys)}t\u22121s=1. This construction aims to mimic the version space VF defined in Lemma C.1, which enable us to apply Lemma C.1. We first split the left side of (15) into two cases based on whether f and f \u2032 belong to VFt :\nT\u2211 t=1 E Ht\u22121 [ E f,f \u2032 [ dTV ( f(xt), f \u2032(xt) ) \u2223\u2223\u2223Ht\u22121]]\n= T\u2211 t=1 E Ht\u22121\n[ E\nf,f \u2032\n[ 1 { f \u2208 VFt and f \u2032 \u2208 VFt } dTV ( f(xt), f \u2032(xt) ) \u2223\u2223\u2223Ht\u22121]\n+ E f,f \u2032\n[ 1 { f \u0338\u2208 VFt or f \u2032 \u0338\u2208 VFt } dTV ( f(xt), f \u2032(xt) ) \u2223\u2223\u2223Ht\u22121]]\n=:T1 + T2.\nTo bound T2, since the total variation distance is upper bounded by 1, we have\nT2 \u2264 T\u2211\nt=1\nE Ht\u22121\n[ E\nf,f \u2032\n[ 1 { f \u0338\u2208 VFt or f \u2032 \u0338\u2208 VFt } \u2223\u2223\u2223Ht\u22121]]\n= T\u2211 t=1 E Ht\u22121 [ E f,f \u2032 [ 1 { f \u0338\u2208 VFt or f \u2032 \u0338\u2208 VFt } \u2223\u2223\u2223Ht\u22121]]\n\u2264 T\u2211\nt=1\nE Ht\u22121 [ 2 E f\u22c6 [ 1 { f\u22c6 \u0338\u2208 VFt } \u2223\u2223\u2223Ht\u22121]]\n=2 T\u2211 t=1 E Ht\u22121,f\u22c6 [ 1 { f\u22c6 \u0338\u2208 VFt }] =2\nT\u2211 t=1 E f\u22c6 [ E Ht\u22121 [ 1 { f\u22c6 \u0338\u2208 VFt } \u2223\u2223\u2223 f\u22c6]] \u22642T\u03b4\nwhere the second inequality holds by the union bound and the condition that f and f \u2032 have the same posterior distributions as f\u22c6 conditioning onHt\u22121. The last two equalities holds by the law of probability. The last inequality is Lemma C.1.\nNow let\u2019s bound T1. We first notice that we can replace the expectation with the supremum:\nT1 \u2264 T\u2211\nt=1\nE Ht\u22121\n[ sup\nf,f \u2032\u2208VFt dTV\n( f(xt), f \u2032(xt) )] .\nApplying Lemma D.12, we have T1 \u2264 4 \u221a T\u03b2F (t\u2212 1) \u00b7 dim1 ( F , 1/T ) \u00b7 log T\nCombining the upper bounds of T1 and T2 and setting \u03b4 = 1/T , we complete the proof of (15).\nProof of (16). We split the left side of (16) into two cases based on whether f and f \u2032 belong to VFt : T\u2211\nt=1\nE Ht\u22121\n[ E\nf,f \u2032\n[ d2TV ( f(xt), f \u2032(xt) ) \u2223\u2223\u2223Ht\u22121]]\n\u2264 T\u2211\nt=1\nE Ht\u22121\n[ E\nf,f \u2032\n[ 1 { f \u2208 VFt and f \u2032 \u2208 VFt } d2TV ( f(xt), f \u2032(xt) ) \u2223\u2223\u2223Ht\u22121]]\n+ T\u2211 t=1 E Ht\u22121 [ E f,f \u2032 [ 1 { f \u0338\u2208 VFt or f \u2032 \u0338\u2208 VFt } d2TV ( f(xt), f \u2032(xt) ) \u2223\u2223\u2223Ht\u22121]]\n=: T3 + T4.\nFollowing a similar argument as in the proof of (15) above, we have T4 \u2264 2T\u03b4. For T3, by the definition o VFt , we directly have T3 \u2264 4\u03b2F (T ). Setting \u03b4 = 1/T , we complete the proof of (16). Proof of (17) and (18). An identical argument can be applied to prove (17) and (18) leveraging Corollary C.4.\nLemma C.6. Under Assumption C.2, it holds that T\u2211\nt=1\nE Ht\u22121\n[ 1 { E g,g\u2032 [\u2223\u2223g(xt)\u2212 g\u2032(xt)\u2223\u2223 \u2223\u2223\u2223Ht\u22121] > \u03f5}]\n\u2264 min 9 \u221a T\u03b2\u2032G(T ) \u03f5 \u00b7 dim1 ( G, \u03f5/2 ) , 21\u03b2\u2032G(T ) \u03f52 \u00b7 dim2(G, \u03f5/2)  . where \u03b2\u2032G(t) := 98\u03ba\n2 log(2GTN[](\u03ba(t|Y|)\u22121,G, \u2225\u00b7\u2225\u221e)). In the inner expectation, g, g\u2032 are sampled from the posterior of g\u22c6 conditioning onHt\u22121.\nProof of Lemma C.6. We define the version space at round t for t \u2208 [T ] as follows\nVGt =\n{ g \u2208 G :\nt\u22121\u2211 s=1 ( g\u0302t(xs)\u2212 g(xs) )2 \u2264 \u03b2G(t\u2212 1) } where recall that g\u0302t is defined as the function that parameterizes the maximum likelihood estimator on the dataset {(xs, ys)}t\u22121s=1 (i.e., g\u0302t = gf\u0302t ). This construction aims to mimic the version space V G defined in Corollary C.4, which enable us to apply Corollary C.4.\nWe first split the left side into two cases based on whether both g and g\u2032 belong to VGt : T\u2211\nt=1\nE Ht\u22121\n[ 1 { E g,g\u2032 [\u2223\u2223g(xt)\u2212 g\u2032(xt)\u2223\u2223 \u2223\u2223\u2223Ht\u22121] > \u03f5}]\n= T\u2211 t=1 E Ht\u22121\n[ 1 { E g,g\u2032 [ 1 { g \u2208 VGt and g\u2032 \u2208 VGt }\u2223\u2223g(xt)\u2212 g\u2032(xt)\u2223\u2223 \u2223\u2223\u2223Ht\u22121] + E\ng,g\u2032\n[ 1 { g \u0338\u2208 VGt or g\u2032 \u0338\u2208 VGt }\u2223\u2223g(xt)\u2212 g\u2032(xt)\u2223\u2223 \u2223\u2223\u2223Ht\u22121] > \u03f5}]\n\u2264 T\u2211\nt=1\nE Ht\u22121\n[ 1 { E g,g\u2032 [ 1 { g \u2208 VGt and g\u2032 \u2208 VGt }\u2223\u2223g(xt)\u2212 g\u2032(xt)\u2223\u2223 \u2223\u2223\u2223Ht\u22121] > \u03f5/2}]\n+ T\u2211 t=1 E Ht\u22121\n[ 1 { E g,g\u2032 [ 1 { g \u0338\u2208 VGt or g\u2032 \u0338\u2208 VGt }\u2223\u2223g(xt)\u2212 g\u2032(xt)\u2223\u2223 \u2223\u2223\u2223Ht\u22121] > \u03f5/2}] =:T1 + T2.\nHere the inequality is due to the fact that 1{a+ b > c} \u2264 1{a > c/2}+1{b > c/2} for any a, b, c. To bound T2, recalling that g(\u00b7) \u2208 [0, G], then we have\nT2 \u2264 T\u2211\nt=1\nE Ht\u22121\n[ 1 { E g,g\u2032 [ 1 { g \u0338\u2208 VGt or g\u2032 \u0338\u2208 VGt } \u00b7G \u2223\u2223\u2223Ht\u22121] > \u03f5/2}]\n\u22642G \u03f5 T\u2211 t=1 E Ht\u22121 [ E g,g\u2032 [ 1 { g \u0338\u2208 VGt or g\u2032 \u0338\u2208 VGt } \u2223\u2223\u2223Ht\u22121]]\n\u22642G \u03f5 T\u2211 t=1 E Ht\u22121 [ 2 E g\u22c6 [ 1 { g\u22c6 \u0338\u2208 VGt } \u2223\u2223\u2223Ht\u22121]]\n= 4G\n\u03f5 T\u2211 t=1 E g\u22c6,Ht\u22121 [ 1 { g\u22c6 \u0338\u2208 VGt }] = 4G\n\u03f5 T\u2211 t=1 E g\u22c6 [ E Ht\u22121 [ 1 { g\u22c6 \u0338\u2208 VGt } \u2223\u2223\u2223 g\u22c6]] \u22644GT\u03b4\n\u03f5 .\nwhere the second inequality is by the fact that 1{a > b} \u2264 a/b for any a \u2265 0 and b > 0, the third inequality holds since g and g\u2032 are identically distributed as g\u22c6 conditioning on Ht\u22121, the two equalities holds by the law of probability, and the last inequality is Corollary C.4.\nNow let us bound T1. We first notice that we can replace the expectation with the supremum:\nT1 \u2264 T\u2211\nt=1\nE Ht\u22121\n[ 1 { sup\ng,g\u2032\u2208VGt \u2223\u2223g(xt)\u2212 g\u2032(xt)\u2223\u2223 > \u03f5/2}] Recall that, for any g \u2208 VGt , we have\nt\u22121\u2211 s=1 ( g(xs)\u2212 g\u0302(xs) )2 \u2264 \u03b2G(t\u2212 1).\nHence, applying Lemma D.11, we get\nT1 \u2264min {( 2 \u221a T\u03b2G(t\u2212 1) \u03f5/2 + 1 ) \u00b7 dim1 ( G, \u03f5/2 ) , ( 4\u03b2G(t\u2212 1) \u03f52/4 + 1 ) dim2(G, \u03f5/2) }\n\u2264min\n{ 5 \u221a T\u03b2G(t\u2212 1)\n\u03f5 \u00b7 dim1\n( G, \u03f5/2 ) , 17\u03b2G(t\u2212 1)\n\u03f52 \u00b7 dim2(G, \u03f5/2) } Combining the upper bounds of T1 and T2, we obtain\nT\u2211 t=1 E [ 1 { E g,g\u2032 [\u2223\u2223g(xt)\u2212 g\u2032(xt)\u2223\u2223 \u2223\u2223\u2223Ht\u22121] > \u03f5}]\n\u2264 4GT\u03b4 \u03f5 +min\n{ 5 \u221a\nT\u03b2G(t\u2212 1) \u03f5\n\u00b7 dim1 ( G, \u03f5/2 ) , 17\u03b2G(t\u2212 1)\n\u03f52 \u00b7 dim2(G, \u03f5/2)\n} .\nSetting \u03b4 = 1/(TG), the parameter \u03b2G(t\u22121) becomes \u03b2\u2032G(t\u22121), which is upper bounded by \u03b2\u2032G(T ). Then, we finish the proof."
        },
        {
            "heading": "C.2 BOUNDING BAYESIAN REGRET",
            "text": "Now we are ready to prove Theorem 5.4. We will prove the upper bound on the Bayesian regret in this section and prove the upper bound on the number of queries in the next section.\nFor the ease of notation, we will omit the dependence of the state-value function on the initial state s1 and simply write V := V1(s1) for any state-value function V throughout the proof.\nWe start by simplifying the Bayesian regret. Since \u03c01t = \u03c0 0 t\u22121, we have\nBayesRegretT = E r\u22c6,P\u22c6\n[ T\u2211\nt=1\n( 2V \u22c6 \u2212 V \u03c0 0 t \u2212 V \u03c0 1 t\n)]\n= E r\u22c6,P\u22c6\n[ T\u2211\nt=1\n( 2V \u22c6 \u2212 V \u03c0 0 t \u2212 V \u03c0 0 t\u22121\n)]\n= E r\u22c6,P\u22c6\n[ T\u2211\nt=1\n( V \u22c6 \u2212 V \u03c0 0 t )] + E\nr\u22c6,P\u22c6 [ T\u22121\u2211 t=0 ( V \u22c6 \u2212 V \u03c0 0 t )]\n\u22642 E r\u22c6,P\u22c6\n[ T\u2211\nt=0\n( V \u22c6 \u2212 V \u03c0 0 t )] .\nHence, we only need to consider the regret incurred by \u03c00t . We defined V \u03c0 r,P as the state-value function of policy \u03c0 with reward function r and model P . Given that, we can express the Bayesian regret as\nBayesRegretT \u2264 2 E r\u22c6,P\u22c6\n[ T\u2211\nt=0\n( V \u03c0 \u22c6\nr\u22c6,P\u22c6 \u2212 V \u03c00t r\u22c6,P\u22c6 )] We reformulate the expectation by first taking the expectation of the historical data up to round t \u2212 1 and then taking the conditional expectation of P \u22c6 and r\u22c6. Concretely, we denote Ht\u22121 = {\u03c401 , \u03c411 , (o1), . . . , \u03c40t\u22121, \u03c41t\u22121, (ot\u22121)} as the history up to round t\u2212 1, and then we have\nBayesRegretT \u22642 T\u2211\nt=0\nE Ht\u22121 E r\u22c6,P\u22c6\n[ V \u03c0 \u22c6\nr\u22c6,P\u22c6 \u2212 V \u03c00t r\u22c6,P\u22c6 \u2223\u2223\u2223Ht\u22121] =2\nT\u2211 t=0 E Ht\u22121 E r\u22c6,P\u22c6 [ V \u03c00t rt,Pt \u2212 V \u03c0 0 t r\u22c6,P\u22c6 \u2223\u2223\u2223Ht\u22121] .\nFor the equality above, we note that (rt, Pt) and (r\u22c6, P \u22c6) are identically distributed givenHt\u22121 and \u03c00t and \u03c0 \u22c6 are the respective optimal policies of (rt, Pt) and (r\u22c6, P \u22c6). Hence, V \u03c00t rt,Pt and V \u03c0 \u22c6 r\u22c6,P\u22c6 are identically distributed given Ht\u22121, which is the reason that we can do the replacement in the conditional expectation. Next, we proceed by decomposing the regret:\nBayesRegretT =2 T\u2211 t=0 E Ht\u22121 E r\u22c6,P\u22c6 [ V \u03c00t rt,Pt \u2212 V \u03c0 0 t rt,P\u22c6 + V \u03c00t rt,P\u22c6 \u2212 V \u03c0 1 t r\u22c6,P\u22c6 + V \u03c01t r\u22c6,P\u22c6 \u2212 V \u03c00t r\u22c6,P\u22c6 \u2223\u2223\u2223Ht\u22121] =2\nT\u2211 t=0 E Ht\u22121 E r\u22c6,P\u22c6 [ V \u03c00t rt,Pt \u2212 V \u03c0 0 t rt,P\u22c6 + V \u03c00t rt,P\u22c6 \u2212 V \u03c0 1 t rt,P\u22c6 + V \u03c01t r\u22c6,P\u22c6 \u2212 V \u03c00t r\u22c6,P\u22c6 \u2223\u2223\u2223Ht\u22121] =2\nT\u2211 t=0 E [ V \u03c00t rt,Pt \u2212 V \u03c0 0 t rt,P\u22c6 + V \u03c00t rt,P\u22c6 \u2212 V \u03c0 1 t rt,P\u22c6 + V \u03c01t r\u22c6,P\u22c6 \u2212 V \u03c00t r\u22c6,P\u22c6 ] =2 ( T\u2211\nt=0\nE [ V\n\u03c00t rt,Pt \u2212 V \u03c0 0 t rt,P\u22c6 ] \ufe38 \ufe37\ufe37 \ufe38\n=:Tmodel\n+ T\u2211 t=0 E [ V \u03c00t rt,P\u22c6 \u2212 V \u03c0 1 t rt,P\u22c6 + V \u03c01t r\u22c6,P\u22c6 \u2212 V \u03c00t r\u22c6,P\u22c6 ] \ufe38 \ufe37\ufe37 \ufe38\n=:Treward\n)\n(19)\nHere we added and substracted V \u03c0 0 t rt,P\u22c6 and V \u03c0 1 t r\u22c6,P\u22c6 in the first equality. For the second equality, we note that \u03c01t = \u03c0 0 t\u22121 is measurable with respect to Ht\u22121 (i.e., E[\u03c01t |Ht\u22121] = \u03c01t ).4 Hence, V \u03c01t r\u22c6,P\u22c6 and V \u03c0 1 t\nrt,P\u22c6 are identically distributed conditioning onHt\u22121.\nNext, we will show that the two terms, Tmodel and Treward, arise from the estimation errors in the model and reward, respectively.\nBounding Tmodel. By simulation lemma (Lemma D.2) and the tower rule, we have\nTmodel \u2264H E  T\u2211 t=0 H\u2211 h=1 E (sh,ah)\u223cd \u03c00t h dTV ( Pt(sh, ah), P \u22c6(sh, ah) )\n=H E\n[ T\u2211\nt=0 H\u2211 h=1 dTV ( Pt(sh, ah), P \u22c6(sh, ah) )]\n=H E\n[ T\u2211\nt=0 H\u2211 h=1 E Pt,P\u22c6 [ dTV ( Pt(sh, ah), P \u22c6(sh, ah) ) \u2223\u2223\u2223Ht\u22121]]\n=H H\u2211 h=1 E\n[ T\u2211\nt=0\nE Pt,P\u22c6\n[ dTV ( Pt(sh, ah), P \u22c6(sh, ah) ) \u2223\u2223\u2223Ht\u22121]]\n\u2264O ( H2 \u00b7 dim1 ( P, 1/T ) \u00b7 log(T ) \u00b7 \u221a T log(TN[]((HT |S|)\u22121,P, \u2225 \u00b7 \u2225\u221e)) ) where the last step is Lemma C.5.\nBounding Treward. By the definition of state-value function, we have\nTreward = T\u2211 t=0 E [ rt(\u03c4 0 t )\u2212 rt(\u03c41t ) + r\u22c6(\u03c41t )\u2212 r\u22c6(\u03c40t ) ]\n= T\u2211 t=0 E [ (1\u2212 Zt) ( rt(\u03c4 0 t )\u2212 rt(\u03c41t ) + r\u22c6(\u03c41t )\u2212 r\u22c6(\u03c40t ) )]\n+ T\u2211 t=0 E [ Zt ( rt(\u03c4 0 t )\u2212 rt(\u03c41t ) + r\u22c6(\u03c41t )\u2212 r\u22c6(\u03c40t ) )] .\n4Strictly speaking, \u03c01t is measurable with respect to the \u03c3-algebra generated by Ht.\nFor the first term, by the query condition, it holds that\nT\u2211 t=0 E [ (1\u2212 Zt) ( rt(\u03c4 0 t )\u2212 rt(\u03c41t ) + r\u22c6(\u03c41t )\u2212 r\u22c6(\u03c40t ) )]\n= T\u2211 t=0 E [ 1 { E [ |r(\u03c40t )\u2212 r(\u03c41t )\u2212 (r\u2032(\u03c40t )\u2212 r\u2032(\u03c41t ))| \u2223\u2223Ht\u22121, \u03c40t , \u03c41t ] \u2264 \u03f5}( rt(\u03c4 0 t )\u2212 rt(\u03c41t ) + r\u22c6(\u03c41t )\u2212 r\u22c6(\u03c40t ) )]\n= T\u2211 t=0 E Ht\u22121,\u03c40t ,\u03c41t E [ 1 { E [ |r(\u03c40t )\u2212 r(\u03c41t )\u2212 (r\u2032(\u03c40t )\u2212 r\u2032(\u03c41t ))| \u2223\u2223Ht\u22121, \u03c40t , \u03c41t ] \u2264 \u03f5}( rt(\u03c4 0 t )\u2212 rt(\u03c41t ) + r\u22c6(\u03c41t )\u2212 r\u22c6(\u03c40t ) ) \u2223\u2223\u2223\u2223\u2223Ht\u22121, \u03c40t , \u03c41t ]\n= T\u2211 t=0 E Ht\u22121,\u03c40t ,\u03c41t [ 1 { E [ |r(\u03c40t )\u2212 r(\u03c41t )\u2212 (r\u2032(\u03c40t )\u2212 r\u2032(\u03c41t ))| \u2223\u2223Ht\u22121, \u03c40t , \u03c41t ] \u2264 \u03f5} E [( rt(\u03c4 0 t )\u2212 rt(\u03c41t ) + r\u22c6(\u03c41t )\u2212 r\u22c6(\u03c40t ) ) \u2223\u2223\u2223Ht\u22121, \u03c40t , \u03c41t ] ]\n\u2264 T\u03f5.\nHere the first equality is by definition. The second equality is by the law of probability. The third equality holds since the indicator is measurable with respect toHt\u22121, \u03c40t , and \u03c41t . The last inequality is by the indicator and the fact that rt and r\u22c6 have the same posterior conditioning onHt\u22121. Plugging this upper bound back, we have\nTreward \u2264T\u03f5+ T\u2211\nt=0\nE [ Zt ( rt(\u03c4 0 t )\u2212 rt(\u03c41t ) + r\u22c6(\u03c41t )\u2212 r\u22c6(\u03c40t ) )]\n=T\u03f5+ T\u2211 t=0 E Ht\u22121 [ Zt E rt,r\u22c6 [\u2223\u2223\u2223rt(\u03c40t )\u2212 rt(\u03c41t ) + r\u22c6(\u03c41t )\u2212 r\u22c6(\u03c40t )\u2223\u2223\u2223 \u2223\u2223\u2223\u2223Ht\u22121]] . In the inner expectation of the second term above, we notice that both rt and r\u22c6 are sampled from the posterior of r\u22c6 conditioning on Ht\u22121. Thus, we can invoke the second statement in Lemma C.5 where the function g corresponds to rt and the function g\u2032 corresponds to r\u22c6. This gives\nTreward \u2264 O ( T\u03f5+ \u221a T\u03ba2 log ( HTN[](\u03ba(2T )\u22121, R\u0303, \u2225 \u00b7 \u2225\u221e) ) \u00b7 dim1 ( R\u0303, 1/T ) \u00b7 log(HT ) ) .\nConclusion. Given the upper bounds on Tmodel and Treward, ignoring logarithmic factors on T and H , we conclude that\nBayesRegretT = O\u0303 ( H2 \u00b7 dim1 ( P, 1/T ) \u00b7 \u221a T log(N[]((HT |S|)\u22121,P, \u2225 \u00b7 \u2225\u221e))\n+ T\u03f5+ \u03ba \u00b7 dim1 ( R\u0303, 1/T ) \u00b7 \u221a T log ( N[](\u03ba(2T )\u22121, R\u0303, \u2225 \u00b7 \u2225\u221e) )) ."
        },
        {
            "heading": "C.3 BOUNDING NUMBER OF QUERIES",
            "text": "It holds that\nBayesQueriesT =E\n[ T\u2211\nt=0\nZt\n]\n= T\u2211 t=0 E Ht\u22121 [ Zt1 { E r,r\u2032\u223c\u03c1r,t [( (r(\u03c40t )\u2212 r(\u03c41t ) ) \u2212 ( r\u2032(\u03c40t )\u2212 r\u2032(\u03c41t )) )] > \u03f5 }] .\nWe notice that, in the inner expectaion, r and r\u2032 are sampled from the posterior of r\u22c6 conditioning on Ht\u22121 (recalling the definition of \u03c1r,t). Thus, we can invoke Lemma C.6 where the function g corresponds to r and the function g\u2032 corresponds to r\u2032. This gives\nBayesQueriesT \u2264min { 9 \u221a T\u03b2R \u03f5 \u00b7 dim1 ( R\u0303, \u03f5/2 ) , 21\u03b2R \u03f52 \u00b7 dim2(R\u0303, \u03f5/2) } .\nwhere the inequality is Lemma C.6 and \u03b2R := 98\u03ba2 log(2THN[](\u03ba(2T )\u22121, R\u0303, \u2225 \u00b7 \u2225\u221e))."
        },
        {
            "heading": "C.4 GENERALIZING THEOREM 5.4 THROUGH SEC",
            "text": "In this section, we show that the dependence on the eluder dimension in Theorem 5.4 can be generalized to the Sequential Extrapolation Coefficient (SEC) (Xie et al., 2022). It have been shown by Xie et al. (2022) that the SEC can be upper bounded by the eluder dimenion, the Bellman-eluder dimension (Jin et al., 2021), and the bilinear rank (Du et al., 2021). Thus, SEC is a more general measure of complexity.\nWe start by introducing the Bayesian Sequential Extrapolation Coefficient (Bayesian SEC) in the preference-based feedback, which is a variant of the original SEC to accommodate the Bayesian and preference-based learning setting. We define the Bayesian SEC for a model class P as\nBayesSECP(P) := E r\u22c6,P\u22c6  sup P1,...,PT\u2208P\n(\u03c001 ,\u03c0 1 1),...,(\u03c0 0 T ,\u03c0 1 T )\nT\u2211 t=1\n(\u2211H h=1 Es,a\u223cd\u03c0th [dTV (Pt(\u00b7 | s, a), P \u22c6(\u00b7 | s, a))] )2\n1 \u2228 \u2211t\u22121\ni=1 \u2211H h=1 Es,a\u223cd\u03c0i\nh\n[d2TV (Pt(\u00b7 | s, a), P \u22c6(\u00b7 | s, a))]  and the Bayesian SEC for a reward function classR as\nBayesSECR(R) := E r\u22c6,P\u22c6  sup r1,...,rT\u2208R\n(\u03c001 ,\u03c0 1 1),...,(\u03c0 0 T ,\u03c0 1 T )\nT\u2211 t=1\nE\u03c40t \u223c\u03c00t ,\u03c41t \u223c\u03c01t [ ( rt(\u03c4 0 t )\u2212 rt(\u03c41t ) + r\u22c6(\u03c41t )\u2212 r\u22c6(\u03c40t ) )2 ] 1 \u2228 \u2211t\u22121 i=1 E\u03c40i \u223c\u03c00i ,\u03c41i \u223c\u03c01i [( rt(\u03c40i )\u2212 rt(\u03c41i ) + r\u22c6(\u03c41i )\u2212 r\u22c6(\u03c40i ) )2] \nWe note that the Bayesian SEC can be easily reduced to the frequentist SEC by specifying the prior on r\u22c6 and P \u22c6 to be the Dirac delta function. We note that our definition of the reward function SEC involves squaring within the expectation in the numerator rather than externally, potentially making it larger.\nNow, we are ready to state the generalization of Theorem 5.4. Theorem C.7. PbTS (Algorithm 2) guarantees that\nBayesRegretT = O\u0303\n( T\u03f5+H \u221a BayesSECP(P) \u00b7 TH \u00b7 \u03b9P + \u221a BayesSECR(R) \u00b7 T \u00b7 \u03ba2 \u00b7 \u03b9R ) ,\nBayesQueriesT = O\u0303\n( \u03ba2 \u00b7 BayesSECR(R) \u00b7 \u03b9R\n\u03f52\n) .\nwhere we denote \u03b9P := log(N[]((HT |S|)\u22121,P, \u2225 \u00b7 \u2225\u221e)) and \u03b9R := log(N[](\u03ba(2T )\u22121, R\u0303, \u2225 \u00b7 \u2225\u221e)).\nThe proofs are provided in the following sections (Appendices C.4.1 and C.4.2)."
        },
        {
            "heading": "C.4.1 BOUNDING BAYESIAN REGRET VIA SEC",
            "text": "We first prove the upper bound on the Bayesian regret. Following (19), it suffices to separately bound Tmodel and Treward via SEC. We start with Tmodel.\nBounding Tmodel. By simulation lemma (Lemma D.2), we have\nTmodel \u2264 H E  T\u2211 t=0 H\u2211 h=1 E (sh,ah)\u223cd \u03c00t h dTV ( Pt(sh, ah), P \u22c6(sh, ah) )\nwhere d\u03c0h(s, a) denotes the probability of \u03c0 reaching (s, a) at time step h. By multiplying and dividing by the same term, we obtain the following.\nTmodel \u2264H E  T\u2211 t=0\n\u2211H h=1 E(sh,ah)\u223cd\u03c0 0 t\nh\ndTV ( Pt(sh, ah), P \u22c6(sh, ah) )\n\u221a 1 \u2228 \u2211t\u22121 i=0 \u2211H h=1 E(sh,ah)\u223cd\u03c0 0 i\nh\nd2TV ( Pt(sh, ah), P \u22c6(sh, ah) )\n\u00d7 \u221a\u221a\u221a\u221a\u221a1 \u2228 t\u22121\u2211 i=0 H\u2211 h=1 E (sh,ah)\u223cd \u03c00 i\nh\nd2TV ( Pt(sh, ah), P \u22c6(sh, ah) )\n\u2264H \u221a\u221a\u221a\u221a\u221a\u221a\u221a\u221aE  T\u2211 t=0 (\u2211H h=1 E(sh,ah)\u223cd\u03c0 0 t h dTV ( Pt(sh, ah), P \u22c6(sh, ah) ))2 1 \u2228 \u2211t\u22121 i=0 \u2211H h=1 E(sh,ah)\u223cd\u03c0 0 i\nh\nd2TV ( Pt(sh, ah), P \u22c6(sh, ah) ) \n\ufe38 \ufe37\ufe37 \ufe38 (i)\n\u00d7 \u221a\u221a\u221a\u221a\u221a T\u2211 t=0 E 1 \u2228 t\u22121\u2211 i=0 H\u2211 h=1 E (sh,ah)\u223cd \u03c00 i\nh\nd2TV ( Pt(sh, ah), P \u22c6(sh, ah) ) \ufe38 \ufe37\ufe37 \ufe38\n(ii)\nHere we note that a \u2228 b := max(a, b) \u2264 a + b for a, b \u2265 0. The last step above is the CauchySchwarz inequality. We observe that term (i) is exactly bounded by the Bayesian SEC of models. For term (ii), we can invoke (16) in Lemma C.5 since Pt has an identical posterior distribution as P \u22c6 given the historical data up to round t \u2212 1. This gives (ii) \u2264 O\u0303( \u221a TH \u00b7 \u03b9P). Plugging these back, we obtain\nTmodel \u2264 O\u0303 ( H \u221a BayesSECP(P) \u00b7 TH \u00b7 \u03b9P )\nBounding Treward. The reward part can be bounded similarly, except that we need to additionally consider the query conditions. To begin with, by the definition of state-value function, we have\nTreward = T\u2211 t=0 E [ rt(\u03c4 0 t )\u2212 rt(\u03c41t ) + r\u22c6(\u03c41t )\u2212 r\u22c6(\u03c40t ) ]\n= T\u2211 t=0 E [ (1\u2212 Zt) ( rt(\u03c4 0 t )\u2212 rt(\u03c41t ) + r\u22c6(\u03c41t )\u2212 r\u22c6(\u03c40t ) )] \ufe38 \ufe37\ufe37 \ufe38\n(i)\n+ T\u2211 t=0 E [ Zt ( rt(\u03c4 0 t )\u2212 rt(\u03c41t ) + r\u22c6(\u03c41t )\u2212 r\u22c6(\u03c40t ) )] \ufe38 \ufe37\ufe37 \ufe38\n(ii)\n.\nFollowing the same argument as in the proof of Appendix C.2, we can bound (i) as (i) \u2264 T\u03f5. Now we proceed to bound (ii), which is the regret incurred when making queries. By multiplying and\ndividing by the same term, we obtain the following.\n(ii) = T\u2211\nt=0\nE \u03c00t ,\u03c0 1 t ,rt,Zt\n E [ Zt ( rt(\u03c4 0 t )\u2212 rt(\u03c41t ) + r\u22c6(\u03c41t )\u2212 r\u22c6(\u03c40t )\n) \u2223\u2223\u03c00t , \u03c01t , rt, Zt]\u221a 1 \u2228 \u2211t\u22121 i=0 E [ Zt (rt(\u03c40i )\u2212 rt(\u03c41i ) + r\u22c6(\u03c41i )\u2212 r\u22c6(\u03c40i )) 2 \u2223\u2223\u2223\u03c00t , \u03c01t , rt, Zt]\n\u00d7 \u221a\u221a\u221a\u221a1 \u2228 t\u22121\u2211 i=0 E \u03c40t ,\u03c4 1 t [ Zt ( rt(\u03c40i )\u2212 rt(\u03c41i ) + r\u22c6(\u03c41i )\u2212 r\u22c6(\u03c40i ) )2 \u2223\u2223\u2223\u2223\u2223\u03c00t , \u03c01t , rt, Zt ] \n\u2264 \u221a\u221a\u221a\u221a\u221a\u221a T\u2211 t=0 E \u03c00t ,\u03c0 1 t ,rt,Zt  ( E [Zt (rt(\u03c40t )\u2212 rt(\u03c41t ) + r\u22c6(\u03c41t )\u2212 r\u22c6(\u03c40t )) |\u03c00t , \u03c01t , rt, Zt] )2 1 \u2228 \u2211t\u22121 i=0 E [ Zt (rt(\u03c40i )\u2212 rt(\u03c41i ) + r\u22c6(\u03c41i )\u2212 r\u22c6(\u03c40i )) 2 \u2223\u2223\u2223\u03c00t , \u03c01t , rt, Zt]  \ufe38 \ufe37\ufe37 \ufe38\n(iii)\n\u00d7 \u221a\u221a\u221a\u221a T\u2211 t=0 ( 1 \u2228 t\u22121\u2211 i=0 E [ Zt ( rt(\u03c40i )\u2212 rt(\u03c41i ) + r\u22c6(\u03c41i )\u2212 r\u22c6(\u03c40i ) )2]) \ufe38 \ufe37\ufe37 \ufe38\n(iv)\nwhere the last inequality is Cauchy-Schwarz inequality. We observe that term (iii) is exactly bounded by the Bayesian SEC of reward functions via Jensen\u2019s inequality. For term (iv), we invoke (18) in Lemma C.5 and get (iv) \u2264 O\u0303( \u221a \u03ba2T \u00b7 \u03b9R). Now plugging these upper bounds back, we get\nTreward \u2264 T\u03f5+ O\u0303 (\u221a BayesSECR(R) \u00b7 \u03ba2T \u00b7 \u03b9R ) .\nConclusion. Given the upper bounds on Tmodel and Treward, we conclude that\nBayesRegretT \u2264O\u0303 ( H \u221a BayesSECP(P) \u00b7 TH \u00b7 \u03b9P + T\u03f5+ \u221a BayesSECR(R) \u00b7 \u03ba2T \u00b7 \u03b9R ) ."
        },
        {
            "heading": "C.4.2 BOUNDING NUMBER OF QUERIES VIA SEC",
            "text": "By the definition of Zt, we have\nBayesQueriesT =E\n[ T\u2211\nt=0\nZt ] = E [ T\u2211\nt=0\nZ2t\n]\n=E\n[ T\u2211\nt=0\nZt1 { E\nr,r\u2032\u223c\u03c1r,t\n[\u2223\u2223(r(\u03c40t )\u2212 r(\u03c41t ))\u2212 (r\u2032(\u03c40t )\u2212 r\u2032(\u03c41t ))\u2223\u2223] > \u03f5} ]\n=E\n[ T\u2211\nt=0\nZt1 { E\nr,r\u2032\u223c\u03c1r,t\n[\u2223\u2223(r(\u03c40t )\u2212 r(\u03c41t ))\u2212 (r\u2032(\u03c40t )\u2212 r\u2032(\u03c41t ))\u2223\u22232] > \u03f52} ]\n\u2264 1 \u03f52 E\n[ T\u2211\nt=0\nZt E r,r\u2032\u223c\u03c1r,t\n[\u2223\u2223(r(\u03c40t )\u2212 r(\u03c41t ))\u2212 (r\u2032(\u03c40t )\u2212 r\u2032(\u03c41t ))\u2223\u22232] ]\n= 1\n\u03f52 E\n[ T\u2211\nt=0\nZt E r,r\u2032\n[\u2223\u2223(rt(\u03c40t )\u2212 rt(\u03c41t ))\u2212 (r\u22c6(\u03c40t )\u2212 r\u22c6(\u03c41t ))\u2223\u22232 \u2223\u2223\u2223Ht\u22121] ] .\nHere the last step is by the definition of \u03c1r,t. Now we swap the order of expectation and get\nBayesQueriesT\n\u2264 1 \u03f52 E\n[ T\u2211\nt=0\nE \u03c40t ,\u03c4 1 t\n[ Zt ( (rt(\u03c4 0 t )\u2212 rt(\u03c41t ) ) \u2212 ( r\u22c6(\u03c40t )\u2212 r\u22c6(\u03c41t )) )2 \u2223\u2223\u2223\u03c00t , \u03c01t , r\u22c6, rt, Zt] ]\n= 1\n\u03f52 E  T\u2211 t=0\nE\u03c40t ,\u03c41t [ Zt ( (rt(\u03c4 0 t )\u2212 rt(\u03c41t ) ) \u2212 ( r\u22c6(\u03c40t )\u2212 r\u22c6(\u03c41t )) )2 \u2223\u2223\u2223\u03c00t , \u03c01t , r\u22c6, rt, Zt] 1 \u2228 \u2211t\u22121 i=0 E\u03c40i ,\u03c41i [ Zt ( (rt(\u03c40i )\u2212 rt(\u03c41i ) ) \u2212 ( r\u22c6(\u03c40i )\u2212 r\u22c6(\u03c41i ))\n)2 \u2223\u2223\u2223\u03c00i , \u03c01i , r\u22c6, rt, Zt] \u00d7 ( 1 \u2228\nt\u22121\u2211 i=0 E \u03c40i ,\u03c4 1 i [ Zt ( (rt(\u03c4 0 i )\u2212 rt(\u03c41i ) ) \u2212 ( r\u22c6(\u03c40i )\u2212 r\u22c6(\u03c41i )) )2 \u2223\u2223\u2223\u03c00i , \u03c01i , r\u22c6, rt, Zt] )]\n\u2264 1 \u03f52 E  T\u2211 t=0\nE\u03c40t ,\u03c41t [ Zt ( (rt(\u03c4 0 t )\u2212 rt(\u03c41t ) ) \u2212 ( r\u22c6(\u03c40t )\u2212 r\u22c6(\u03c41t )) )2 \u2223\u2223\u2223\u03c00t , \u03c01t , r\u22c6, rt, Zt] 1 \u2228 \u2211t\u22121 i=0 E\u03c40i ,\u03c41i [ Zt ( (rt(\u03c40i )\u2212 rt(\u03c41i ) ) \u2212 ( r\u22c6(\u03c40i )\u2212 r\u22c6(\u03c41i )) )2 \u2223\u2223\u2223\u03c00i , \u03c01i , r\u22c6, rt, Zt] \n\u00d7 O\u0303 ( \u03ba2 \u00b7 \u03b9R ) where in the last step we applied Jensen\u2019s inequality to the first multiplicative term and applied (18) in Lemma C.5 to the second term. We observe that the first term can be bounded by SEC, and thus we obtain\nBayesQueriesT \u2264 O\u0303 ( \u03ba2 \u00b7 BayesSECR(R) \u00b7 \u03b9R\n\u03f52\n) ."
        },
        {
            "heading": "D TECHNICAL LEMMAS",
            "text": "Lemma D.1. Under Assumption 3.1, we have \u03ba(\u03a6(a)\u2212 \u03a6(b)) \u2264 a\u2212 b \u2264 \u03ba(\u03a6(a)\u2212 \u03a6(b)) for any a, b \u2208 [0, H]. Proof of Lemma D.1. We note that \u03a6(a)\u2212 \u03a6(b) = \u222b a b \u03a6\u2032(x) dx and\n\u03ba\u22121(a\u2212 b) \u2264 (a\u2212 b) inf x\u2208[a,b] \u03a6\u2032(x) \u2264 \u222b a b \u03a6\u2032(x) dx \u2264 (a\u2212 b) sup x\u2208[a,b] \u03a6\u2032(x) \u2264 \u03ba\u22121(a\u2212 b).\nLemma D.2 (Simulation lemma). For any models P, P\u0302 and any policy \u03c0, we have\u2223\u2223\u2223V \u03c0P,1(s1)\u2212 V \u03c0P\u0302 ,1(s1)\u2223\u2223\u2223 \u2264 H\u2211 h=1 E (sh,ah)\u223cd\u03c0P,h \u2223\u2223\u2223\u2223\u2223 Es\u2032\u223cP (sh,ah)V \u03c0P\u0302 ,h+1(s\u2032)\u2212 Es\u2032\u223cP\u0302 (sh,ah)V \u03c0P\u0302 ,h+1(s\u2032) \u2223\u2223\u2223\u2223\u2223\n\u2264H H\u2211\nh=1\nE (sh,ah)\u223cd\u03c0P,h dTV\n( P (\u00b7 | sh, ah), P\u0302 (\u00b7 | sh, ah) ) where d\u03c0P,h(s, a) is the probability of \u03c0 reaching (s, a) at time step h given model P . Lemma D.3 (Gaussian concentration). Let \u03f5 \u223c N (0, c\u03a3\u22121) for c \u2208 R+ and \u03a3 a positive definite matrix. Then, for any \u03b4 > 0, we have\n1. Pr ( \u2225\u03f5\u2225\u03a3 > \u221a 2cd log(2d/\u03b4) ) \u2264 \u03b4.\n2. E [\u2225\u03f5\u2225\u03a3] \u2264 \u221a cd.\nProof of Lemma D.3. The proof of the first inequality is provided in Abeille & Lazaric (2017, Appendix A). We provide a proof here for completeness. Let \u03b7 \u223c N (0, Id) where Id denotes the d-dimensional identity matrix. Fix an arbitrary \u03b1. We have\nPr ( \u2225\u03b7\u22252 > \u03b1 \u221a d ) \u2264 Pr ( \u2203i : |\u03b7i| > \u03b1 ) \u2264 dPr ( |\u03b71| > \u03b1 ) \u2264 d \u00b7 2e\u2212\u03b1 2/2\nwhere the second inequality is the union bound, and the last inequality is the standard concentration inequality for one-dimensional Gaussian random variable. We choose \u03b1 = \u221a 2 log(2d/\u03b4) and get\nPr ( \u2225\u03b7\u22252 > \u221a 2d log(2d/\u03b4) ) \u2264 \u03b4.\nLet B denote the square root of \u03a3, i.e., BB = \u03a3. Then, we have\n\u2225\u03f5\u2225\u03a3 = \u221a \u03f5\u22a4\u03a3\u03f5 = \u221a(\u221a cB\u22121\u03b7 )\u22a4 BB (\u221a cB\u22121\u03b7 ) = \u221a c\u2225\u03b7\u22252 (20)\nHence, it holds that Pr ( \u2225\u03f5\u2225\u03a3 > \u221a 2cd log(2d/\u03b4) ) = Pr ( \u2225\u03b7\u22252 > \u221a 2d log(2d/\u03b4) ) \u2264 \u03b4.\nThis proves the first inequality. To prove the second inequality, we note that, by (20),\nE [\u2225\u03f5\u2225\u03a3] = \u221a cE [\u2225\u03b7\u22252] \u2264 \u221a c \u221a E [\u03b7\u22a4\u03b7] = \u221a cd.\nThe following inequalities are well-known, and we use the version in Zhu & Nowak (2022). Lemma D.4. (Zhu & Nowak, 2022) Let {Xt}t\u2264T be a sequence of positive valued random variables adapted to a filtration Ft, and let Et[\u00b7] := E[\u00b7 | Ft\u22121]. If Xt \u2264 B almost surely, then with probability at least 1\u2212 \u03b4, the following holds:\nT\u2211 t=1 Xt \u2264 3 2 T\u2211 t=1 Et[Xt] + 4B log(1/\u03b4),\nT\u2211 t=1 Et[Xt] \u2264 2 T\u2211 t=1 Xt + 8B log(1/\u03b4).\nLemma D.5 (Self-normalized process). (Abbasi-Yadkori et al., 2011) Let {xi}\u221ei=1 be a real valued stochastic process sequence over the filtration {Fi}\u221ei=1. Let xi be conditionally B-subgaussian given Fi\u22121. Let {\u03d5i}\u221ei=1 with \u03d5i \u2208 Fi\u22121 be a stochastic process in Rd with each \u2225\u03d5i\u2225 \u2a7d L\u03d5. Define \u03a3i = \u03bbI + \u2211i\u22121 j=1 \u03d5i\u03d5\n\u22a4 i . Then for any \u03b4 > 0 and all i \u2a7e 0, with probability at least 1\u2212 \u03b4\u2225\u2225\u2225\u2225\u2225 k\u22121\u2211 i=1 \u03d5ixi \u2225\u2225\u2225\u2225\u2225 2\n\u03a3\u22121k\n\u2a7d 2B2 log\n( det (\u03a3i) 1/2 det(\u03bbI)\u22121/2\n\u03b4\n) \u2a7d 2B2 ( d log ( \u03bb+ kL2\u03d5\n\u03bb\n) + log(1/\u03b4) )\nLemma D.6 (Elliptical potential lemma). (Abbasi-Yadkori et al., 2011) Following the setting of Lemma D.5, we have\nk\u2211 i=1 min { 1, \u2225\u03d5i\u22252\u03a3\u22121i } \u2a7d 2d log ( \u03bb+ kL2\u03d5 \u03bb ) Lemma D.7 (Sum of features). (Jin et al., 2020) Following the setting of Lemma D.5, we have\nk\u22121\u2211 i=1 \u2225\u03d5i\u22252\u03a3\u22121k \u2a7d d\nThe following lemma converts the TV distance between Gaussian distributions into the distance between their means. Lemma D.8. Assume p1 = N (\u00b51, \u03c32I) and p2 = N (\u00b52, \u03c32I) are two Gaussian distributions over Rd. Assume \u2225\u00b51 \u2212 \u00b52\u22252 \u2264 m. Then, we have\n\u2225\u00b51 \u2212 \u00b52\u22252 \u00b7min\n{\u221a 1\n2e\u03c0\u03c32 ,\n1\n2m } \ufe38 \ufe37\ufe37 \ufe38\n=: C1\n\u2264 dTV(p1, p2) \u2264 \u2225\u00b51 \u2212 \u00b52\u22252 \u00b7 \u221a 1\n2\u03c0\u03c32\ufe38 \ufe37\ufe37 \ufe38 =: C2\nProof. Without loss of generality, we can rotate and translate the Rd space and assume that p1 = N (0, \u03c32I) and p2 = N (\u03b1 \u00b7 e1, \u03c32I) where \u03b1 = \u2225\u00b51 \u2212 \u00b52\u22252 and e1 = [1, 0, . . . , 0] \u2208 Rd. It is clear that the total variation distance is preserved. Then, by definition,\ndTV(p1, p2) = 1\n2 \u222b x\u2208Rd |p1(x)\u2212 p2(x)| dx = \u222b x\u00b7e1\u2264\u03b1/2 p1(x)\u2212 p2(x) dx\nwhere the last step holds since p1(x) \u2212 p2(x) \u2265 0 if and only if x \u00b7 e1 \u2264 \u03b1/2. Note that the above formulation is independent of any dimension except for first dimension, so we can marginize other dimensions and reduce into a one-dimensional problem:\ndTV(p1, p2) = \u222b x\u2264\u03b1/2 N (x | 0, \u03c32)\u2212N (x |\u03b1, \u03c32) dx\n= ( F (\u03b1/(2\u03c3))\u2212 F (\u2212\u03b1/(2\u03c3)) ) =\n\u222b \u03b1/(2\u03c3) \u2212\u03b1/(2\u03c3) N (x | 0, 1) dx\nwhere we denote F as the CDF of standard Gaussian N (0, 1) in the middle step. Below we show that the above integral is upper bounded by C2\u03b1 and lower bounded by C1\u03b1.\nUpper bound. For an upper bound, we note that for any v > 0, we have\u222b v \u2212v N (x | 0, 1) dx \u2264 2v \u00b7max x N (x | 0, 1) \u2264 v \u221a 2 \u03c0 .\nInserting v = \u03b1/(2\u03c3), we obtain the desired upper bound.\nLower bound. For a lower bound, we note that for any 0 < v \u2264 m/(2\u03c3), if v < 1, we have\u222b v \u2212v N (x | 0, 1) dx \u2265 2v max x\u2208[\u2212v,v] N (x | 0, 1) \u2265 2vN (1 | 0, 1) = 2v \u00b7 e \u22121/2 \u221a 2\u03c0 = v \u00b7 \u221a 2 e\u03c0 . When v \u2265 1, we have\u222b v \u2212v N (x | 0, 1) dx \u2265 \u222b \u03c3 \u2212\u03c3 N (x | 0, 1) dx \u2248 0.6827 \u2265 v \u00b7 1 2v \u2265 v \u00b7 \u03c3 m . Combining the two cases together, we have\u222b v \u2212v N (x | 0, 1) dx \u2265 v \u00b7min {\u221a 2 e\u03c0 , \u03c3 m } .\nInserting v = \u03b1/(2\u03c3), we obtain the desired lower bound.\nLemma D.9. Let x \u223c N (0, Id) and A be a positive semi-definite matrix. Then, we have\nE x \u2225x\u2225A \u2265\n\u221a 2 tr(A)\n\u03c0 .\nProof. Let P denote the orthogonal matrix diagonalizing A, i.e., A = P\u039bP\u22a4 where \u039b = diag(\u03bb1, . . . , \u03bbd). Then, we have Ex \u2225x\u2225A = Ex \u221a x\u22a4Ax = Ey \u221a y\u22a4\u039by where y = P\u22a4x \u223c N (0, Id). Now we consider the following optimization problem:\nmin M\u2208M f(M) := E y\n\u221a y\u22a4My where M := {M \u2ab0 0 : M is diagonal and tr(M) = tr(\u039b)}.\nThen, we have \u039b \u2208M and Ex \u2225x\u2225A = f(\u039b). Thus, the solution to the above optimization problem is a lower bound of Ex \u2225x\u2225A. Since f(M) is concave in M , it must attain the minimum on the bounary of M. By symmetry of entries in the diagonal, we know that the minimizer must have tr(\u039b) at one entry and 0 at all other entries. Hence, the minimum value is\nE y\n\u221a tr(\u039b)y21 = \u221a tr(\u039b)E\ny |y1| =\n\u221a 2 tr(\u039b)\n\u03c0 .\nWe finish the proof by noticing that tr(\u039b) = tr(A).\nThe following lemma was originally established by Agarwal et al. (2020) and later adapted by Wu et al. (2023a) to accommodate infinitely large function classes.\nLemma D.10 (Maximum likelihood estimation). (Wu et al., 2023a, Lemma C.3) Consider a sequential conditional probability estimation problem. Let X and Y be the instance space and the target space, respectively. Let F : (X \u00d7 Y) \u2192 R denote the function class. We are given a dataset D := {(xi, yi)}ni=1 where xi \u223c Di and yi \u223c f\u22c6(x, \u00b7). We assume f\u22c6 \u2208 F . For the data generating process, we assume the data distribution Di is history-dependent, i.e., xi can depend on the previous samples: x1, y1, . . . , xi\u22121, yi\u22121 for any i \u2208 [n]. We fix \u03b4 \u2208 (0, 1). Let f\u0302 denote the maximum likelihood estimator,\nf\u0302 = argmax f\u2208F n\u2211 i=1 log f(xi, yi).\nThen, we have n\u2211\ni=1\nE x\u223cDi d2TV\n( f\u0302(x, \u00b7), f\u22c6(x, \u00b7) ) \u2264 10 log ( N[] ( (n|Y|)\u22121,F , \u2225 \u00b7 \u2225\u221e ) /\u03b4 )\n(21)\nwith probability at least 1\u2212 \u03b4. Here |Y| denotes the total measure of space Y , i.e., |Y| := \u222b Y 1 dy."
        },
        {
            "heading": "D.1 ELUDER DIMENSION",
            "text": "The following lemmas are adapted from Russo & Van Roy (2013) and Liu et al. (2022a).\nLemma D.11. Following the notation of Definition 5.2, define\nFt = { f \u2208 F :\nt\u22121\u2211 s=1 d2 ( f(xs), f\u0302t(xs) ) \u2264 \u03b2\n}\nwhere f\u0302t \u2208 F is an arbitrary function. Then, we have\nT\u2211 t=1 1\n{ sup\nf,f \u2032\u2208Ft d (f(xt), f\n\u2032(xt)) \u2265 \u03c9 } \u2264 ( 2 \u221a T\u03b2\n\u03c9 + 1\n) dim1(F , \u03c9), (22)\nT\u2211 t=1 1\n{ sup\nf,f \u2032\u2208Ft d (f(xt), f\n\u2032(xt)) \u2265 \u03c9 } \u2264 ( 4\u03b2\n\u03c92 + 1\n) dim2(F , \u03c9) (23)\nfor any constant \u03c9 > 0.\nProof of Lemma D.11. We first prove (22) and then (23).\nProof of (22). We begin by showing that if\nsup f,f \u2032\u2208Ft\nd (f(xt), f \u2032(xt)) \u2265 \u03c9\nfor some t \u2208 [T ], then xt is \u03c9-dependent on at most 2\u03b2/\u03c9 disjoint subsequence of its predecessors. To see this, we note that, if xt is \u03c9-dependent on a subsequence (xi1 , xi2 , . . . , xin) of its predecessors, we must have\nn\u2211 s=1 d ( f(xis)\u2212 f \u2032(xis) ) > \u03c9.\nHence, if xt is \u03c9-dependent on l disjoint subsequences, we have\nt\u22121\u2211 s=1 d ( f(xs), f \u2032(xs) ) > l\u03c9. (24)\nFor the left-hand side, we also have\nt\u22121\u2211 s=1 d ( f(xs), f \u2032(xs) ) \u2264 t\u22121\u2211 s=1 d ( f(xs), f\u0302t(xs) ) + t\u22121\u2211 s=1 d ( f\u0302t(xs), f \u2032(xs) )\n\u2264 \u221a T \u221a\u221a\u221a\u221at\u22121\u2211 s=1 d2 ( f(xs), f\u0302t(xs) ) + \u221a T \u221a\u221a\u221a\u221at\u22121\u2211 s=1 d2 ( f\u0302t(xs), f \u2032(xs)\n) \u22642 \u221a T\u03b2 (25)\nwhere the first inequality is the triangle inequality, and the second inequality holds by the definition of Ft. Combining (24) and (25), we get that l \u2264 2 \u221a T\u03b2/\u03c9.\nNext, we show that for any sequence (x\u20321, . . . , x \u2032 \u03c4 ), there is at least one element that is \u03c9-dependent on at least \u03c4/d\u2212 1 disjoint subsequence of its predecessors, where d := dim1(F , \u03c9). To show this, let m be the integer satisfying md+ 1 \u2264 \u03c4 \u2264 md+ d. We will construct m disjoint subsequences, B1, . . . , Bm. At the beginning, let Bi = (x\u2032i) for i \u2208 [m]. If x\u2032m+1 is \u03c9-dependent on each subsequence B1, . . . , Bm, then we are done. Otherwise, we select a subsequence Bi which x\u2032m+1 is \u03c9-independent of and append x\u2032m+1 to Bi. We repeat this process for all elements with indices j > m + 1 until either x\u2032j is \u03c9-dependent on each subsequence or j = \u03c4 . For the latter, we have\u2211m\ni=1 |Bi| \u2265 md, and since each element of a subsequence Bi is \u03c9-independent of its predecesors, we must have |Bi| = d for all i. Then, x\u03c4 must be \u03c9-dependent on each subsequence by the definition of eluder dimension.\nFinally, let\u2019s set the sequence (x\u20321, . . . x \u2032 \u03c4 ) to be the subsequence of (x1, . . . , xT ) consisting of elements xt for which d (f(xt), f \u2032(xt)) > \u03c9. As we have established, we have\n1. each x\u2032i is \u21131-norm \u03c9-dependent on at most 2 \u221a T\u03b2/\u03c9 disjoint subsequences, and\n2. some x\u2032i is \u21131-norm \u03c9-dependent on at least \u03c4/d\u2212 1 disjoint subsequences.\nTherefore, we must have \u03c4/d\u2212 1 \u2264 2 \u221a T\u03b2/\u03c9, implying that \u03c4 \u2264 (2 \u221a T\u03b2/\u03c9 + 1)d.\nProof of (23). The proof is quite similar to the proof of (22). For \u21132-norm, following the same argument, we can show the following:\n1. each x\u2032i is \u21132-norm \u03c9-dependent on at most 4\u03b2/\u03c9 2 disjoint subsequences, and\n2. some x\u2032i is \u21132-norm \u03c9-dependent on at least \u03c4/d\u2212 1 disjoint subsequences.\nTherefore, we must have \u03c4/d\u2212 1 \u2264 4\u03b2/\u03c92, implying that \u03c4 \u2264 (4\u03b2/\u03c92 + 1)d.\nLemma D.12. Following the setting of Lemma D.11, assume d(\u00b7, \u00b7) is upper bounded by D \u2264 \u03b2. Then, we have\nT\u2211 t=1 sup f,f \u2032\u2208Ft d ( f(xt), f \u2032(xt) ) \u2264 4 \u221a T\u03b2 \u00b7 dim1(F , 1/T ) \u00b7 log(DT )\nProof of Lemma D.12. For notational simplicity, we denote\nwt := sup f,f \u2032\u2208Ft\nd ( f(xt), f \u2032(xt) )\nNow we fix a constant \u2206 > 0, and then we have\nT\u2211 t=1 wt = T\u2211 t=1 \u222b D 0 1{wt \u2265 \u03b4} d\u03b4\n= \u222b D 0 T\u2211 t=1 1{wt \u2265 \u03b4} d\u03b4\n\u2264\u2206T + \u222b D \u2206 T\u2211 t=1 1{wt \u2265 \u03b4} d\u03b4\n\u2264\u2206T + \u222b D \u2206 ( 2 \u221a T\u03b2 \u03b4 + 1 ) dim1(F , \u03b4) d\u03b4\n\u2264\u2206T + \u222b D \u2206 3 \u221a T\u03b2 \u03b4 \u00b7 dim1(F , \u03b4) d\u03b4\n\u2264\u2206T + 3 \u221a T\u03b2 \u00b7 dim1(F ,\u2206) \u00b7 log(D/\u2206)\nwhere the second inequality is Lemma D.11, and the last inequality uses the fact that dim1(F , \u03b4) is non-increasing in \u03b4. We can conclude the proof by setting \u2206 = 1/T ."
        }
    ],
    "year": 2024
}