{
    "abstractText": "Learning multi-scale representations is central to semantic segmentation. We visualize the effective receptive field (ERF) of canonical multi-scale representations and point out two risks in learning them: scale inadequacy and field inactivation. To address these issues, a novel multi-scale learner, varying window attention (VWA), is presented. VWA leverages the local window attention (LWA) and disentangles LWA into the query window and context window, allowing the scale of context to vary for the query to learn representations at specific scales. However, varying the context to large-scale windows (enlarging ratio R) can significantly increase the memory footprint and computation cost (R times larger than LWA). We propose a simple but professional re-scaling strategy to zero the extra induced cost without compromising any performance. In consequence, VWA shows great superiority to previous multi-scale learners. Furthermore, building upon VWA and employing various MLPs, we introduce a multi-scale decoder (MSD), VWFormer, to improve learning multi-scale representations in semantic segmentation. VWFormer achieves efficiency competitive with the most computefriendly MSDs, like FPN and MLP decoder, but performs much better than any MSDs. For instance, at little extra overhead, \u223c 10G FLOPs, VWFormer improves Mask2Former by 1.0% \u2212 1.3% mIoU. Using only half of the computation, VWFormer outperforms the popular UperNet by 1.0%\u2212 2.1% mIoU.",
    "authors": [],
    "id": "SP:0b66e9fec41200eed58013f467839c23da8ee8ad",
    "references": [
        {
            "authors": [
                "Holger Caesar",
                "Jasper Uijlings",
                "Vittorio Ferrari"
            ],
            "title": "Coco-stuff: Thing and stuff classes in context",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Liang-Chieh Chen",
                "George Papandreou",
                "Florian Schroff",
                "Hartwig Adam"
            ],
            "title": "Rethinking atrous convolution for semantic image segmentation",
            "venue": "arXiv preprint arXiv:1706.05587,",
            "year": 2017
        },
        {
            "authors": [
                "Liang-Chieh Chen",
                "Yukun Zhu",
                "George Papandreou",
                "Florian Schroff",
                "Hartwig Adam"
            ],
            "title": "Encoderdecoder with atrous separable convolution for semantic image segmentation",
            "venue": "In Proceedings of the European conference on computer vision (ECCV),",
            "year": 2018
        },
        {
            "authors": [
                "Zhe Chen",
                "Yuchen Duan",
                "Wenhai Wang",
                "Junjun He",
                "Tong Lu",
                "Jifeng Dai",
                "Yu Qiao"
            ],
            "title": "Vision transformer adapter for dense predictions",
            "venue": "arXiv preprint arXiv:2205.08534,",
            "year": 2022
        },
        {
            "authors": [
                "Bowen Cheng",
                "Alex Schwing",
                "Alexander Kirillov"
            ],
            "title": "Per-pixel classification is not all you need for semantic segmentation",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Bowen Cheng",
                "Ishan Misra",
                "Alexander G Schwing",
                "Alexander Kirillov",
                "Rohit Girdhar"
            ],
            "title": "Maskedattention mask transformer for universal image segmentation",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Marius Cordts",
                "Mohamed Omran",
                "Sebastian Ramos",
                "Timo Rehfeld",
                "Markus Enzweiler",
                "Rodrigo Benenson",
                "Uwe Franke",
                "Stefan Roth",
                "Bernt Schiele"
            ],
            "title": "The cityscapes dataset for semantic urban scene understanding",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Xiaohan Ding",
                "Xiangyu Zhang",
                "Jungong Han",
                "Guiguang Ding"
            ],
            "title": "Scaling up your kernels to 31x31: Revisiting large kernel design in cnns",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Jiaqi Gu",
                "Hyoukjun Kwon",
                "Dilin Wang",
                "Wei Ye",
                "Meng Li",
                "Yu-Hsin Chen",
                "Liangzhen Lai",
                "Vikas Chandra",
                "David Z Pan"
            ],
            "title": "Multi-scale high-resolution vision transformer for semantic segmentation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Junjun He",
                "Zhongying Deng",
                "Yu Qiao"
            ],
            "title": "Dynamic multi-scale filters for semantic segmentation",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision,",
            "year": 2019
        },
        {
            "authors": [
                "Junjun He",
                "Zhongying Deng",
                "Lei Zhou",
                "Yali Wang",
                "Yu Qiao"
            ],
            "title": "Adaptive pyramid context network for semantic segmentation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Shihua Huang",
                "Zhichao Lu",
                "Ran Cheng",
                "Cheng He"
            ],
            "title": "Fapn: Feature-aligned pyramid network for dense image prediction",
            "venue": "In Proceedings of the IEEE/CVF international conference on computer vision,",
            "year": 2021
        },
        {
            "authors": [
                "Alexander Kirillov",
                "Ross Girshick",
                "Kaiming He",
                "Piotr Doll\u00e1r"
            ],
            "title": "Panoptic feature pyramid networks",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Tsung-Yi Lin",
                "Piotr Doll\u00e1r",
                "Ross Girshick",
                "Kaiming He",
                "Bharath Hariharan",
                "Serge Belongie"
            ],
            "title": "Feature pyramid networks for object detection",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Ze Liu",
                "Yutong Lin",
                "Yue Cao",
                "Han Hu",
                "Yixuan Wei",
                "Zheng Zhang",
                "Stephen Lin",
                "Baining Guo"
            ],
            "title": "Swin transformer: Hierarchical vision transformer using shifted windows",
            "venue": "In Proceedings of the IEEE/CVF international conference on computer vision,",
            "year": 2021
        },
        {
            "authors": [
                "Zhuang Liu",
                "Hanzi Mao",
                "Chao-Yuan Wu",
                "Christoph Feichtenhofer",
                "Trevor Darrell",
                "Saining Xie. A convnet for the"
            ],
            "title": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp",
            "venue": "11976\u201311986, 2022.",
            "year": 2020
        },
        {
            "authors": [
                "Wenjie Luo",
                "Yujia Li",
                "Raquel Urtasun",
                "Richard Zemel"
            ],
            "title": "Understanding the effective receptive field in deep convolutional neural networks",
            "venue": "Advances in neural information processing systems,",
            "year": 2016
        },
        {
            "authors": [
                "Adam Paszke",
                "Sam Gross",
                "Francisco Massa",
                "Adam Lerer",
                "James Bradbury",
                "Gregory Chanan",
                "Trevor Killeen",
                "Zeming Lin",
                "Natalia Gimelshein",
                "Luca Antiga"
            ],
            "title": "Pytorch: An imperative style, highperformance deep learning library",
            "venue": "Advances in neural information processing systems,",
            "year": 2019
        },
        {
            "authors": [
                "Sucheng Ren",
                "Daquan Zhou",
                "Shengfeng He",
                "Jiashi Feng",
                "Xinchao Wang"
            ],
            "title": "Shunted self-attention via multi-scale token aggregation",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Christian Szegedy",
                "Vincent Vanhoucke",
                "Sergey Ioffe",
                "Jon Shlens",
                "Zbigniew Wojna"
            ],
            "title": "Rethinking the inception architecture for computer vision",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Tete Xiao",
                "Yingcheng Liu",
                "Bolei Zhou",
                "Yuning Jiang",
                "Jian Sun"
            ],
            "title": "Unified perceptual parsing for scene understanding",
            "venue": "In Proceedings of the European conference on computer vision (ECCV),",
            "year": 2018
        },
        {
            "authors": [
                "Enze Xie",
                "Wenhai Wang",
                "Zhiding Yu",
                "Anima Anandkumar",
                "Jose M Alvarez",
                "Ping Luo"
            ],
            "title": "Segformer: Simple and efficient design for semantic segmentation with transformers",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Jianwei Yang",
                "Chunyuan Li",
                "Pengchuan Zhang",
                "Xiyang Dai",
                "Bin Xiao",
                "Lu Yuan",
                "Jianfeng Gao"
            ],
            "title": "Focal self-attention for local-global interactions in vision transformers",
            "venue": "arXiv preprint arXiv:2107.00641,",
            "year": 2021
        },
        {
            "authors": [
                "Maoke Yang",
                "Kun Yu",
                "Chi Zhang",
                "Zhiwei Li",
                "Kuiyuan Yang"
            ],
            "title": "Denseaspp for semantic segmentation in street scenes",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Qihang Yu",
                "Yingda Xia",
                "Yutong Bai",
                "Yongyi Lu",
                "Alan L Yuille",
                "Wei Shen"
            ],
            "title": "Glance-and-gaze vision transformer",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Yuhui Yuan",
                "Lang Huang",
                "Jianyuan Guo",
                "Chao Zhang",
                "Xilin Chen",
                "Jingdong Wang"
            ],
            "title": "Ocnet: Object context network for scene parsing",
            "venue": "arXiv preprint arXiv:1809.00916,",
            "year": 2018
        },
        {
            "authors": [
                "Bowen Zhang",
                "Liyang Liu",
                "Minh Hieu Phan",
                "Zhi Tian",
                "Chunhua Shen",
                "Yifan Liu"
            ],
            "title": "Segvitv2: Exploring efficient and continual semantic segmentation with plain vision transformers",
            "venue": "arXiv preprint arXiv:2306.06289,",
            "year": 2023
        },
        {
            "authors": [
                "Hengshuang Zhao",
                "Jianping Shi",
                "Xiaojuan Qi",
                "Xiaogang Wang",
                "Jiaya Jia"
            ],
            "title": "Pyramid scene parsing network",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Bolei Zhou",
                "Hang Zhao",
                "Xavier Puig",
                "Sanja Fidler",
                "Adela Barriuso",
                "Antonio Torralba"
            ],
            "title": "Scene parsing through ade20k dataset",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2017
        },
        {
            "authors": [
                "Xizhou Zhu",
                "Weijie Su",
                "Lewei Lu",
                "Bin Li",
                "Xiaogang Wang",
                "Jifeng Dai"
            ],
            "title": "Deformable detr: Deformable transformers for end-to-end object detection",
            "venue": "arXiv preprint arXiv:2010.04159,",
            "year": 2020
        },
        {
            "authors": [
                "Zhen Zhu",
                "Mengde Xu",
                "Song Bai",
                "Tengteng Huang",
                "Xiang Bai"
            ],
            "title": "Asymmetric non-local neural networks for semantic segmentation",
            "venue": "In Proceedings of the IEEE/CVF international conference on computer vision,",
            "year": 2019
        },
        {
            "authors": [
                "Chen"
            ],
            "title": "For fairness, we employ the same backbones for all the methods, including ResNet50 and ResNet101",
            "venue": "DMNet He et al. (2019a), ANN Zhu et al. (2019), and ISANet Yuan et al",
            "year": 2019
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "In semantic segmentation, there are two typical paradigms for learning multi-scale representations. The first involves applying filters with receptive-field-variable kernels, classic techniques like atrous convolution (Chen et al., 2018) or adaptive pooling (Zhao et al., 2017). By adjusting hyper-parameters, such as dilation rates and pooling output sizes, the network can vary the receptive field to learn representations at multiple scales.\nThe second leverages hierarchical backbones He et al. (2016); Liu et al. (2021; 2022) to learn multiscale representations. Typical hierarchical backbones are usually divided into four different levels, each learning representations on feature maps with different sizes. For semantic segmentation, the multi-scale decoder (MSD) (Xiao et al., 2018; Kirillov et al., 2019; Xie et al., 2021) fuses feature maps from every level (i.e. scale) and output an aggregation of multi-scale features.\nEssentially, the second is analogous to the first in that it can also be understood from the perspective of varying receptive fields of filters. It is a common belief that deeper networks have larger receptive fields (Szegedy et al., 2016; Ding et al., 2022). Therefore, when MSDs work for semantic segmentation, they actually fuse representations learned by filters with multiple receptive fields, which denotes different stages of hierarchical backbones.\nTo delve into the receptive field of these approaches, we visualize their effective receptive fields (ERF) (Luo et al., 2016), as shown in Fig. 1a-e. For the first paradigm, advanced methods like ASPP (applying atrous convolution) Chen et al. (2018) and PSP Zhao et al. (2017) (applying adaptive pooling) are selected. For the second paradigm, ERF visualization is performed on multi-level feature maps of ResNet (He et al., 2016), Swin Transformer Liu et al. (2021), and SegFormer (MiT) Xie et al. (2021), all of which are successful hierarchical backbones on semantic segmentation.\nBased on the visual results, we conduct a qualitative analysis of the scale, shape, and quantity of ERFs of these typical methods (please refer to Appendix A for detailed analysis). As analyzed, learning multi-scale representations faces two issues. On one hand, there is a risk of scale inadequacy, such as missing global information (Swin Transformer, ResNet, ASPP), missing local information (PSP), or having only local and global information while missing other scales (SegFormer). On the other hand, there are inactivated areas within the receptive field range, as observed in ASPP, Swin Transformer, and the low-level layers of SegFormer. We refer to this as field inactivation.\nTo address these issues, we explore a new way to learn multi-scale representations. The focus of our research is on answering whether the local window attention (LWA) mechanism can be extended to function as a relational filter whose receptive field is variable to meet the scale specification for learning multi-scale representations in semantic segmentation while preserving the efficiency advantages of LWA. Our resulting approach is varying window attention (VWA), which learns multi-scale representations with no room for scale inadequacy and field inactivation (See Fig. 1f).\nSpecifically, VWA disentangles LWA into the query window and context window. The query remains positioned on the local window, while the context is enlarged to cover more surrounding areas, thereby varying the receptive field of the query. Since this enlargement results in a substantial overhead impairing the high efficiency of LWA (R2 times than LWA), we will analyze how the extra cost arises and particularly devise pre-scaling principle, densely overlapping patch embedding (DOPE), and copy-shift padding mode (CSP) to eliminate it without compromising performance.\nMore prominently, tailored to semantic segmentation, we propose a multi-scale decoder (MSD), VWFormer, which takes VWA as its core and incorporates MLPs with diverse functionalities including multi-layer aggregation and low-level enhancement. To prove the superiority of VWFormer, we evaluate it on various high-performance segmentation backbones such as ResNet, Swin Transformer, SegFormer, and compare it with SOTA MSDs like FPN (Lin et al., 2017), UperNet (Xiao et al., 2018), MLP-decoder (Xie et al., 2021), and deformable-attention FPN (Zhu et al., 2020) on datasets including ADE20K (Zhou et al., 2017), Cityscapes (Cordts et al., 2016), and COCOStuff110k (Caesar et al., 2018). Experiments show that replacing these previous MSDs with VWFormer consistently leads to performance and efficiency gains. The highest improvements can reach an increase of 2.4% mIoU and a FLOPs reduction of 48%, which are credited to VWA rectifying multiscale representations of multi-level feature maps. Besides, we substitute VWA in VWFormer with ASPP and PSP for comparison to aforesaid filters with variable receptive fields. Results demonstrate that with the same backbone, VWA outperforms ASPP and PSP by 1.2% and 2.2% mIoU, respectively, and consumes less computational budget, approximately 140G and 53G FLOPs less.\nIn summary, this work has a three-fold contribution:\n\u2014 We make full use of the ERF technique to visualize the scale of representations learned by existing multi-scale learning paradigms, including receptive-field-variable kernels and different levels of hierarchical backbones, revealing the issues of scale inadequacy and field inactivation.\n\u2014 We propose VWA, a relational representation learner, allowing for varying context window sizes toward multiple receptive fields like variable kernels. It is as efficient as LWA due to our pre-scaling principle along with DOPE. We also propose a CSP padding mode specifically for perfecting VWA.\n\u2014 A novel MSD, VWFormer, designed for semantic segmentation, is presented as the product of VWA. VWFormer shows its effectiveness in improving multi-scale representations of hierarchical backbones, by surpassing existing MSDs in both performance and efficiency on classic datasets."
        },
        {
            "heading": "2 RELATED WORKS",
            "text": ""
        },
        {
            "heading": "2.1 MULTI-SCALE LEARNER",
            "text": "The multi-scale learner is referred to as the paradigm utilizing receptive-field-variable filters to learn multi-scale representations. Sec. 1 has introduced ASPP and PSP. There are also more multi-scale learners proposed previously for semantic segmentation. These works can be categorized into three groups. The first involves using atrous convs, e.g. ASPP, and improving its feature fusion way and efficiency of atrous convolution (Yang et al., 2018; Chen et al., 2018). The second involves extending adaptive pooling, incorporating PSP into other types of representation learners (He et al., 2019a) (He et al., 2019b). However, there are issues of scale inadequacy and field inactivation associated with these methods\u2019 core mechanisms, i.e. atrous convs and adaptive pooling, as analyzed in Sec. 1.\nThe third uses a similar idea to ours, computing the attention matrices between the query and contexts with different scales, to learn multi-scale representations in a relational way for semantic segmentation or even image recognition. In the case of Yuan et al. (2018) and Yu et al. (2021), their core mechanisms are almost identical. As for Zhu et al. (2019), Yang et al. (2021), and Ren et al. (2022), the differences among the three are also trivial. We briefly introduce Yuan et al. (2018) and Zhu et al. (2019), visualizing their ERFs and analyzing their issues (See Fig. 7 and Appendix B for more information). In a word, all of the existing multi-scale learners in a relational way (also known as multi-scale attention) do not address the issues we find, i.e. scale inadequacy and field inactivation."
        },
        {
            "heading": "2.2 MULTI-SCALE DECODER",
            "text": "The multi-scale decoder (MSD) fuses multi-scale representations (multi-level feature maps) learned by hierarchical backbones. One of the most representative MSDs is the Feature Pyramid Network (FPN) (Lin et al., 2017), originally designed for object detection. It has also been applied to image segmentation by using its lowest-level output, even in SOTA semantic segmentation methods such as MaskFormer (Cheng et al., 2021). Lin et al. (2017) has also given rise to methods like (Kirillov et al., 2019) and (Huang et al., 2021). In Mask2Former (Cheng et al., 2022), FPN is combined with deformable attention Zhu et al. (2020) to allow relational interaction between different level feature maps, achieving higher results. Apart from FPN and its derivatives, other widely used methods include the UperNet (Xiao et al., 2018) and the lightweight MLP-decoder proposed by SegFormer.\nIn summary, all of these methods focus on how to fuse multi-scale representations from hierarchical backbones or enable them to interact with each other. However, our analysis points out that there are scale inadequacy and field inactivation issues with referring to multi-level feature maps of hierarchical backbones as multi-scale representations. VWFormer further learns multi-scale representations with distinct scale variations and regular ERFs, surpassing existing MSDs in terms of performance while consuming the same computational budget as lightweight ones like FPN and MLP-decoder."
        },
        {
            "heading": "3 VARING WINDOW ATTENTION",
            "text": ""
        },
        {
            "heading": "3.1 PRELIMINARY: LOCAL WINDOW ATTENTION",
            "text": "Local window attention (LWA) is an efficient variant of Multi-Head Self-Attention (MHSA), as shown in Fig. 2a. Assuming the input is a 2D feature map denoted as x2d \u2208 RC\u00d7H\u00d7W , the first step is reshaping it to local windows, which can be formulated by:\nx\u03022d = Unfold (kernel = P , stride = P) (x2d) , (1) where Unfold() is a Pytorch (Paszke et al., 2019) function (See Pytorch official website for more information). Then the MHSA operates only within the local window instead of the whole feature.\nTo show the efficiency of local window attention, we list its computation cost to compare with that of MHSA on the global feature (Global Attention):\n\u2126 (GA) = 4(HW )C 2 + 2(HW ) 2 C , \u2126 (LWA) = 4(HW )C 2 + 2(HW )P2C . (2)\nNote that the first term is on linear mappings, i.e., query, key, value, and out, and the second term is on the attention computation, i.e., calculation of attention matrices and the weighted-summation of value. In the high-dimensional feature space, P 2 is smaller than C and much smaller than HW . Therefore, the cost of attention computation in LWA is much smaller than the cost of linear mappings which is much smaller than the cost of attention computation in GA.\nBesides, the memory footprints of GA and LWA are listed below, showing the hardware-friendliness of LWA. The intermediate outputs of the attention mechanism involve query, key, value, and out, all of which are outputs of linear mappings, and attention matrices output from attention computation.\nMem. (GA) \u221d (HW )C + (HW )2, Mem. (LWA) \u221d (HW )C + (HW )P2. (3) Obviously, the consequence of the computational comparison remains valid. In GA the second term is much larger than the first, but in LWA the second term is smaller than the first."
        },
        {
            "heading": "3.2 VARYING THE CONTEXT WINDOW",
            "text": "In LWA, x\u03022d output by Eq. 1 will attend to itself. In VWA, the query is still x\u03022d, but for the context, by denoting it as c2d, the generation can be formulated as:\nc2d = Unfold (kernel = RP , stride = P ,padding = zero) (x2d) , (4)\nFrom the view of window sliding, the query generation is a P \u00d7 P window with a stride of P \u00d7 P sliding on x2d, and the context generation is a larger RP \u00d7RP window with still a stride of P \u00d7P sliding on x2d. R is the varying ratio, a constant value in one VWA. As shown in Fig. 2, when R is 1, VWA becomes LWA, and the query and context are entangled together in the local window. But when R > 1, with the enlargement of context, the query can see wider than the field of the local window. Thus, VWA is a variant of LWA and LWA is a special case of VWA, where R = 1 in VWA.\nFrom the illustration of Fig. 2b, the computation cost of VWA can be computed by: \u2126 (VWA) = 2 ( R2 + 1 ) (HW )C 2 + 2(HW )(RP) 2 C . (5)\nSubtracting Eq. 5 from Eq. 2, we can quantify the extra computation cost caused by enlarging the context patch:\n\u2126 (EX.) = 2 ( R2 \u2212 1 ) (HW )C 2 + 2 ( R2 \u2212 1 ) (HW )P2C . (6)\nFor the memory footprint of VWA, it can be computed by: Mem. (VWA) \u221d ( R2 ) (HW )C + (HW )(RP) 2 . (7)\nSubtracting Eq. 7 from Eq. 3, the extra memory footprint is clear: Mem. (EX.) \u221d ( R2 \u2212 1 ) (HW )C + ( R2 \u2212 1 ) (HW )P2. (8)\nClearly, the larger the window, the more challenging the problem becomes. First, the efficiency advantage of attention computation (the second term) in LWA does not hold. Second, linear mappings, the first term, yield much more computation budget, which is more challenging because to our knowledge existing works on making attention mechanisms efficient rarely take effort to reduce both the computation cost and memory footprint of linear mappings and their mapping outputs. Next, we will introduce how to address the dilemma caused by varying the context window."
        },
        {
            "heading": "3.3 ELIMINATING EXTRA COSTS",
            "text": "With the analysis of Eq. 6 and Eq. 8, the most straightforward way to eliminate the extra cost and memory footprint is re-scaling the large context \u2208 RC\u00d7R\u00d7P\u00d7R\u00d7P back to the same size as that of the local query \u2208 RC\u00d7P\u00d7P , which means R is set to 1 and thereby both of Eq. 6 and Eq. 8 is 0. Above all, it is necessary to clarify the difference between using this idea to deal with the extra computation cost and the extra memory footprint. As shown in Fig. 2b, the intermediate produced\nby varying (enlarging) the window, which is the output of Eq. 4, already takes the memory that is R2(HW )C. Therefore, re-scaling the large context after generating it does not work, the right step should be re-scaling the feature x2d before running Eq. 4. We name this pre-scaling principle.\nSolving the problem is begun by the pre-scaling principle. A new feature scaling paradigm, densely overlapping patch embedding (DOPE), is proposed. This method is different from patch embedding (PE) widely applied in ViT and HVT as it does not change the spatial dimension but only changes the dimensionality. Specifically, for x2d, originally, after applying Eq. 4, the output\u2019s shape is:\nH/P \u00d7W/P \u00d7RP \u00d7RP \u00d7 C. (9)\nwhich means memory footprint of R2HWC. Instead, DOPE first reduces the dimensionality of x2d from C to C/R2, and then applies Eq. 4, resulting in the context with a shape of:\nH/P \u00d7W/P \u00d7RP \u00d7RP \u00d7 C/R2. (10)\nwhich means memory footprint of HWC, the same as that of x2d, eliminating the extra memory.\nSince PE is often implemented using conv layers, how DOPE re-scales features is expressed as:\nDOPE = Conv2d(in = C , out = C/R2, kernel = R, stride = 1). (11)\nSo, the term \u201ddensely overlapping\u201d of DOPE is used to describe the densely arranged pattern of convolutional kernels, especially when R is large, to filter every position. The computation cost introduced by DOPE can be computed by:\n\u2126 (DOPE) = R\u00d7R\u00d7 C \u00d7 C/R2 \u00d7HW = HWC2. (12)\nThis is equivalent to the computation source required for one linear mapping.\nHowever, the context window \u2208 RRP\u00d7RP\u00d7C/R2 cannot be attended to by the query window \u2208 RP\u00d7P\u00d7C . We choose PE to downsample the context and increase its dimensionality to a new context window \u2208 RP\u00d7P\u00d7C . The PE function can be formulated as:\nPE = Conv2d(in = C/R2, out = C , kernel = R, stride = R). (13)\nThe computation cost for one context window applying PE is:\n\u2126 (PE for one context) = R\u00d7R\u00d7 C/R2 \u00d7 C \u00d7RP/R\u00d7RP/R = P 2C. (14)\nFor all context windows from DOPE, with a total of H/P \u00d7W/P , the computation cost becomes:\n\u2126 (PE) = H/P \u00d7W/P \u00d7 \u2126 (PE for one context) = HWC. (15)\nThis is still the same as linear mapping.\nAfter applying the re-scaling strategy described, as shown in Fig. 2c, it is clear that the memory footprint of VWA is the same as Eq. 3, not affected by the context window enlargement. The attention computation cost is also the same as LWA (Eq. 2). For DOPE, VWA uses it once, thus adding one linear mapping computation to Eq. 2. For PE, VWA uses it twice, generating key and value from the DOPE\u2019s output, replacing the key and value mapping. So the computation cost of VWA merely increases 25% (one linear mapping, HWC) than that of LWA, which becomes:\n\u2126 (VWA) = (4 + 1)(HW )C 2 + 2(HW )P2C . (16)"
        },
        {
            "heading": "3.4 ATTENTION COLLAPSE AND COPY-SHIFT PADDING",
            "text": "The padding mode in Eq. 4 is zero padding. However, visualizing attention maps of VWA, we find that the attention weights of the context window at the corner and edge tend to have the same value, which makes attention collapse. The reason is too many same zeros lead to the smoothing of the probability distribution during Softmax activation. As shown in Fig. 3, to address this problem, we propose copy-shift padding (CSP), which is equivalent to making the coverage of the large window move towards the feature. Specifically, for the left and right edges, x2d after CSP is:\nx2d = Concat(d = 4)(x2d[..., (R+ 1)P/2 : RP ],x2d,x2d[...,\u2212RP : \u2212(R+ 1)P/2]). (17) where Concat() is a function concatenating a tuple of features along the dimension d. Based on x2d obtained by Eq. 17, CSP padding the top and bottom sides can be formulated by: x2d = Concat(d = 3)(x2d[..., (R+ 1)P/2 : RP, :],x2d,x2d[...,\u2212RP : \u2212(R+ 1)P/2, :]). (18)"
        },
        {
            "heading": "4 VWFORMER",
            "text": "Multi-Layer Aggregation As illustrated in Fig. 4, VWFormer first concatenates feature maps from the last three stages instead of all four levels for efficiency, by upsampling the last two (F16 and F32) both to the same size as the 2nd-stage one (F8), and then transform the concatenation with one linear layer (MLP0) to reduce the channel number, with F as the outcome.\nMulti-Scale Representations To learn multi-scale representations, three VWA mechanisms with varying ratios R = 2, 4, 8 are paralleled to act on the multi-layer aggregation\u2019s output F . The local window size P of every VWA is set to H8 \u00d7 W 8 , subject to the spatial size of F . Additionally, the short path, exactly a linear mapping layer, consummates the very local scale. The MLPs of VWFormer consist of two layers. The first layer (MLP1) is a linear reduction of multi-scale representations.\nLow-Level Enhancement The second layer (MLP2) of MLPs empowers the output (F1) of the first layer with low-level enhancement (LLE). LLE first uses a linear layer (MLPlow) with small output channel numbers 48 to reduce the lowest-level (F4) dimensionality. Then F1 is upsampled to the same size as MLPlow\u2019s output (Flow) and fused with it through MLP2, outputting F2."
        },
        {
            "heading": "5 EXPERIMENTS",
            "text": ""
        },
        {
            "heading": "5.1 DATASET AND IMPLEMENTATION",
            "text": "Experiments are conducted on three public datasets including Cityscapes, ADE20K, and COCOStuff-164K (See D.2 for more information). The experiment protocols are the same as the compared method\u2019s official repository. For ablation studies, we choose the Swin-Base backbone as the testbed and use the same protocols as Swin-UperNet (See D.3 for more information)."
        },
        {
            "heading": "5.2 MAIN RESULTS",
            "text": ""
        },
        {
            "heading": "5.2.1 COMPARISON WITH SEGFORMER (MLP-DECODER)",
            "text": "SegFormer uses MixFormer (MiT) as the backbone and designs a lightweight MLP-decoder as MSD to decode multi-scale representations of MixFormer. To demonstrate the effectiveness of VWFormer in improving multi-scale representations by VWA, we replace the MLP-decoder in SegFormer with VWFormer. Table 1 shows the number of parameters, FLOPs, memory footprints, and mIoU. Across all variants of backbone MiT (B0\u2192B5), VWFormer trumps MLP-decoder on every metric."
        },
        {
            "heading": "5.2.2 COMPARISON WITH UPERNET",
            "text": "In recent research, UperNet is often used as MSD to evaluate the proposed Vision Transformer in semantic segmentation. Before multi-scale fusion, UperNet learns multi-scale representations by utilizing PSPNet (with scale inadequacy issue) merely on the highest-level feature maps. In contrast, VWFormer can rectify ERFs of every fused multi-level feature map in advance. Table 2 shows VWFormer consistently uses much fewer budgets to achieve higher performance."
        },
        {
            "heading": "5.2.3 COMPARISON WITH MASKFORMER AND MASK2FORMER",
            "text": "MaskFormer and Mask2Former introduce the mask classification mechanism for image segmentation but also rely on MSDs. MaskFormer uses the simple and lightweight FPN as MSD, while Mask2Former empowers multi-level feature maps with feature interaction by integrating\nDeformable Attention (Zhu et al., 2020) into FPN (DAF). Table 3 demonstrates that VWFormer is as efficient as FPN and achieves mIoU gains from 0.8% to 1.7%. The results also show the combo of VWFormer and Deformable Attention improves DAF by 1.0%-1.3%. This demonstrates the feature interaction can still boost the performance of VWFormer, highlighting its generability."
        },
        {
            "heading": "5.3 ABLATION STUDIES",
            "text": ""
        },
        {
            "heading": "5.3.1 SCALE CONTRIBUTION",
            "text": "Table 4 shows the performance drops a lot if removing any-scale VWA of VWFormer. These results indicate every scale is crucial, also suggesting that scale inadequacy is fatal to multi-scale learning. Also, we add a VWA branch with R = 1 context windows which is exactly LWA, and then substitute R = 2 VWA with it. The results show LWA is unnecessary in VWFormer because the short path (1\u00d7 1 convolution) in VWFormer can provide a very local receptive field, as visualized in Fig. 1f."
        },
        {
            "heading": "5.3.2 PRE-SCALING VS. POST-SCALING",
            "text": "Table 5 compares: applying VWA without rescaling, with a naive rescaling as depicted in Fig. 2b, and our proposed professional strategy. LWA originally consumes unaffordable FLOPs and memory footprints. Applying the naive scaling strategy saves some FLOPs and memory footprints, but introduces positional embedding (PE) leading to an increase in the number of parameters. Our proposed strategy not only eliminates the computation and memory introduced by varying the context window but also only adds a small number of parameters. It does not sacrifice performance for efficiency."
        },
        {
            "heading": "5.3.3 ZERO PADDING VS. VW PADDING",
            "text": "The left table of Table 6 shows using zero padding to obtain the context window results in a 1.0% lower mIoU than applying CSP to obtain the context window. Such a performance loss is even more serious than removing one scale of VWA, which demonstrates the harm of attention collapse and the necessity of our proposed CSP in applying the varying window scheme."
        },
        {
            "heading": "5.3.4 CHANNEL NUMBER OF LOW-LEVEL ENHANCEMENT",
            "text": "The channel number of LLE should be small because it is performed on the lowest-level feature map on which the operation can easily introduce the most computation costs. The right table of Table 6 analyzes the channel setting of LLE. From Fig. 1, it can be seen that the lowest-level feature map is of unique receptivity, very local or global, adding new scales to VWFormer\u2019s multi-scale learning."
        },
        {
            "heading": "6 SPECIFIC EFR VISUALIZATION",
            "text": "The EFR visualization of Fig. 1 is averaged on many ADE20k val images. To further substantiate the proposed issue, Fig. 5 analyzes the specific ADE20K val image with EFRs of segformer and VWFormer contrastively. This new visualization can help to understand the receptive issue of SegFormer and show the strengths of VWFormer\u2019s multi-scale learning.\nFig. 5a showcases a waterfall along with rocks. Our VWFormer\u2019s result labels most of the rocks, but SegFormer\u2019s result struggles to distinguish between \u201crock\u201d and \u201c mountain\u201d. From their ERFs, it can be contrastively revealed that VWFormer helps the query to understand the complex scene, even delineating the waterfall and rocks, more distinctly than SegFormer within the whole image.\nFig. 5b showcases a meeting room with a table surrounded by swivel chairs. Our VWFormer\u2019s result labels all of the swivel chairs, but SegFormer\u2019s result mistakes two swivel chairs as general chairs. From their ERFs, it can be contrastively revealed when VWFormer infers the location, it incorporates the context of swivel chairs, within the Red box on the opposite side of the table. But SegFormer neglects to learn about that contextual information due to its scale issues.\nFig. 5c showcases a white tall building. Our VWFormer\u2019s result labels it correctly, but SegFormer\u2019s result mistakes part of the building as the class \u201chouse\u201d. From their ERFs, it can be contrastively revealed that VWFormer has a clearer receptivity than SegFormer within the Red box which indicates this object is a church-style building."
        },
        {
            "heading": "7 INTRODUCTION TO APPENDIX",
            "text": "We sincerely recommend readers to see Appendix, including a detailed analysis for Fig. 1, EFRs of multi-scale attention, and extra substantial experimental results showing VWFormer\u2019s superioity."
        },
        {
            "heading": "A QUALITATIVE ANALYSIS OF TYPICAL METHODS\u2019 ERFS",
            "text": "Below is a detailed analysis of the issues with methods visualized in Fig. 1. For good readability, Fig. 1 is copied and pasted here as Fig. 6\nASPP employs atrous convs with a set of reasonable fixed atrous rates to learn representations at multi scales. However, as shown in Fig. 6a, the largest receptive field does not capture the desired scale of representations. This is because the parameter settings are manual and do not adapt to the image size. The lack of adaptability becomes more severe when training and testing samples have different sizes, a common occurrence with applying strategies like test-time augmentation (TTA). Furthermore, when the receptive field is large, contributions from the atrous parts are zero, leading to inactivated subareas within larger receptive fields.\nPSP applies pooling filters with different scales by adjusting the hyper-parameter, output size of adaptive pooling, to learn multi-scale representations. However, as shown in Fig. 6b, the receptive field sizes are exactly the same for output sizes 1 and 2 and for output sizes 3 and 6. This is because the super small output needs to be interpolated to the original feature size. In this process, if a position does not require interpolation to obtain its value, its receptive field remains unchanged. However, if interpolation is needed, the receptive field can be influenced by other positions.\nResNet stages\u2019 receptive field sizes change from small to large as the network deepens. This is because the stacking of multiple 3x3 convolutions can simulate larger convolutional kernels. However, as shown in Fig. 6c, compared to ASPP and PSP, the largest receptive field of the four scales in ResNet only covers half of the original image and does not capture a global representation because the 3x3 conv is of too much locality. Additionally, it is challenging to distinguish between the receptive field sizes of the third and fourth stages.\nSwin Transformer\u2019s basic layers consist of local window attention mechanisms and shift-window attention mechanisms. The feature maps in its four stages exhibit an increase in receptive field size from small to large. Swin Transformer also faces challenges in learning global representations effectively. Moreover, due to the shift operation of the local window, its receptive field shape is irregular as shown in Fig. 6d, leading to inactivated subareas within the receptive field range.\nSegFormer\u2019s basic layers are sophisticated, incorporating local window attention, global pooling attention, and 3x3 convolutions. It is hence difficult to imagine the receptive field shape and size for its four-level feature maps. Fig. 6e indicates that SegFormer learns global representations in the low-level layers (i.e., the first and second levels) but still suffers from inactivated subareas within the receptive field range. In the higher layers (i.e., the third and fourth levels), they learn more localized representations but their field ranges are very similar. Therefore SegFormer also meets the scale inadequacy because it can only learn global and local representations."
        },
        {
            "heading": "B ERFS OF EXISTING MULTI-SCALE ATTENTION (RELATIONAL MULTI-SCALE LEARNER)",
            "text": "Fig. 7a shows two-scales of ERFs of ISANet (Yuan et al., 2018), merely learning local and global representations while ignoring other scales. So the issue of scale inadequacy for ISANet is very clear. The local representation is learned using the local window attention mechanism, while the global representation is obtained by interlacing pixels from all windows to create new windows that contain pixels from each original local window. Then, the window attention mechanism is applied to the new window. The ERF of the global hierarchy shows that the global receptive field is not continuous due to interlacing, suggesting that ISANet also meets field inactivation.\nANN Zhu et al. (2019) uses adaptive pooling to capture multi-scale features, in a PSP-like fashion. Then they are together attended to by the original feature which serves as the query. The scale of the receptive field is singly global because every context filtered by adaptive pooling is derived from the whole feature map. So the issue of scale inadequacy is also very clear for ANN. Fig. 7b shows the activation does not spread the global range uniformly and the bottom area is insufficiently activated. Therefore, both scale inadequacy and field inactivation are issues of ANN and its relevant methods.\nThe bottom three rows of Table 7 indeed compare ours to ISANet and ANN. VWFormer outperforms both of them by large margins consistently across different backbones and benchmarks."
        },
        {
            "heading": "C MORE EXPERIMENTAL ANALYSES",
            "text": "C.1 COMPARISON OF VWFORMER WITH MULTI-SCALE LEARNERS\nTo verify the superiority of VWFormer over representative multi-scale learners for semantic segmentation, Table 7 compares VWFormer with PSPNet Zhao et al. (2017), DeepLabV3 Chen et al. (2017), DeepLabV3+ Chen et al. (2018), DenseASPP Yang et al. (2018), APCNet He et al. (2019b), DMNet He et al. (2019a), ANN Zhu et al. (2019), and ISANet Yuan et al. (2018). For fairness, we employ the same backbones for all the methods, including ResNet50 and ResNet101 He et al. (2016). All methods are trained for 80000 iterations, and evaluated on Cityscapes as well as ADE20K. The input size is 768/769\u00d7 768/769 for Cityscapes, and 512\u00d7 512 for ADE20K. From Table 7, we can find that VWFormer brings the best results to both ResNet50 and ResNet101 on both datasets. Specifically, on Cityscapes, VWFormer achieves 81.2% mIoU with ResNet50,\nand 82.7% mIoU with ResNet101, which are the best results among all methods. DeepLabV3+ achieves the closest performance to ours but has more computation costs and parameters by 49.6G and 12.2M, respectively. On ADE20K, VWFormer outperforms other methods by large margins consistently. APCNet performs most closely to ours, but VWFormer uses the least FLOPs and parameters. In short word, VWFormer is more powerful than any other multi-scale learners.\nC.2 VWA VS. ATROUS CONV VS. ADAPTIVE POOLING\nTable 8 compares the performance and efficiency of VWA with two classic multi-scale learners analyzed earlier: atrous convolution (ASPP) and adaptive pooling (PSP), both of which are receptivefield-variable kernels. We use Swin-Base as the backbone and adopt the structure of VWFormer as the Multi-Scale Decoder (MSD). But VWA which plays the role of learning multi-scale representations is replaced with ASPP and PSP to show the superiority of VWA as a multi-scale learner.\nLike Swin, ResNet\u2019s multi-level feature maps are also subject to the problem of scale inadequacy when considered as multi-scale representations. Table 8 further reports the performance and efficiency comparisons of multi-scale learners in improving multi-scale representations of ResNet101.\nIt is worth noting that the result of UperNet with Swin-B in Table 2 can be compared with the 1stgroup results in Table 8. From this, we can conclude that using ASPP or PSP to improve multi-scale representations of hierarchical backbones does not lead to clear performance improvements. This empirically confirms the issues aforementioned regarding their effectiveness as multi-scale learners.\nC.3 VWFORMER WITH SOTA METHODS\nTable 9 analyzes our method briefly with state-of-the-art semantic segmentation methods created on other tracks. Left of Table 9 shows the comparison with HRViT Gu et al. (2022), which is a hierarchical Vision Transformer (HVT) with complex multi-scale learning. It was paired originally with MLP-decoder from SegFormer as MSD. Moreover, MLP-decoder is replaced with our VWFormer. The performance gains are considerable, supporting VWFormer\u2019s capability of improving multi-scale representations.\nCenter compares VWFormer with SegViT-V2 Zhang et al. (2023). SegViT-V2 is a decoder specifically for ViT (or categorized as plain Vision Transformer). Here VWFormer cooperates with plain Vision Transformer at the first time. The improvement shows that VWFormer is not only effective in HVT but also powerful in plain backbone architecture.\nRight shows the comparison with ViT-Adapter Chen et al. (2022), which is a pre-training technique for improving ViT on dense prediction tasks. Like many works on Vision Transformer employing UperNet as MSD for semantic segmentation, ViT-Adapter was also originally paired with UperNet. Moreover, UperNet is replaced with VWFormer, achieving considerable performance gains.\nC.4 EXAMINING INFERENCE TIME\nTable 10 shows supplementary results of inference time for Table 1, Table 2, and Table 3. From Top of Table 10, VWFormer\u2019s inference time is faster than SegFormer. From Bottom Left, VWFormer\u2019s inference time is much faster than UperNet. From Bottom Right, VWFormer\u2019s inference time is slightly faster than FPN. Additionally, by comparing the results in the last row of Bottom Left and the first row of Bottom Right, it can be observed that VWFormer is faster than MaskFormer.\nC.5 BREAKDOWN OF PERFORMANCE GAINS\nTable 11 shows a breakdown of performance gains within Cityscapes which has 19-class segments. The upper results are obtained by MiT-B5 paired with SegFormer head (mIoU 82.26%) and the lower results are obtained by MiT-B5 paired with our VWFormer (mIoU 82.87%).\nThe bold number is the class that the counterpart performs better than ours. Except for the \u201dtruck\u201d class where SegFormer outperforms ours largely, which seems like a biased result, on the \u2019wall\u2019, \u2019sky\u2019, and \u2019train\u2019 SegFormer only slightly outperforms ours (by avg. 0.2%). And on the other 15 classes, Ours shows consistent superiority to SegFormer (by avg. 1.4%)."
        },
        {
            "heading": "D SOME DETAILS",
            "text": "D.1 DETAILS OF VWFORMER CAPACITY SETTING\nSec. 4 and Fig. 4 indicate the flow of channel numbers is 512(output of multi-layer aggregation)\u2192 2048(concatenation of learnt multi-scale representations)\u2192 512(output of multiscale aggregation)\u2192 512 + 48 = 560(concatenation of LLE)\u2192 256(final output of VWFormer). For some lightweight backbones, such channel settings incur too much computational burden. We further introduce an efficient setting for VWFormer to cooperate with lightweight backbones such as SegFormer-B0 and SegFormer-B1. The new flow of channels is 64(output of multi-layer aggregation)\u2192 256(concatenation of learned multi-scale representations)\u2192 64(output of multiscale aggregation)\u2192 64 + 32 = 96(concatenation of LLE)\u2192 64(final output of VWFormer)\nD.2 DETAILS OF DATASET\nCityscapes is an urban scene parsing dataset that contains 5, 000 fine-annotated images captured from 50 cities with 19 semantic classes. There are 2, 975 images divided into a training set, 500 images divided into a validation set, and 1, 525 images divided into a testing set.\nADE20K is one of the most challenging datasets in semantic segmentation. It consists of a training set of 20, 210 images with 150 categories, a testing set of 3, 352 images, and a validation set of 2, 000 images.\nCOCOStuff-164K is also a very challenging benchmark that consists of 164k images with 172 semantic classes. The training set contains 118k images, the test-dev dataset contains 20K images and the validation set contains 5k images.\nD.3 DETAILS OF IMPLEMENTATION\nExperiments comparing with SegFormer and Swin-UperNet in Sec. 5 are implemented based on the MMSegmentation codebase. In addition, ablation studies are done with MMSegmentation. Experiments comparing with MaskFormer and Mask2Former are implemented based on the Detectron2 codebase. The computing server on which all experiments are run has 16 Tesla V100 GPU cards. For other methods\u2019 results, we report the number shown in their papers.\nFor the evaluation of mIoU, FLOPs, and Parameters, the two codebases MMSegmentation and Detectron2 use the same algorithm to obtain the numbers of these metrics. However, for memory footprint, these two codebases use different methods to count it. As a result, note that the memory reported in Table 3 is not comparable to the memory of any other table, because all other table\u2019s results are based on MMSegmentation, but only Table 3 is based on Detectron2."
        },
        {
            "heading": "E QUALITATIVE RESULTS",
            "text": "As shown in Fig. 8 and Fig. 9, we present more qualitative results on ADE20K of SegFormer and VWFormer with MiT-B5 as the backbone. The yellow dotted box focuses on the apparent visualization difference between them and the Ground Truth (GT). Compared to SegFormer\u2019s results, VWFormer improves the inner consistency of objects. Taking the bedroom (the first row shown in Fig 8 as an example, part of the shelf that is near the bed is misidentified as the shelf by SegFormer, and the boundary between the bed and shelf is extremely unclear. In contrast, VWFormer segments the two objects very finely, which provides a coherent boundary. Moreover, we observe that with VWFormer similar objects are hardly confused. For example, in the living room shown in the last row of Fig. 9, SegFormer mistakes the sofa for an armchair. And in the first row of Fig. 9, SegFormer mistakes the blind for windowpanes. However, VWFormer accurately distinguishes between the sofa and the armchair, as well as between the blinds and the windowpanes."
        },
        {
            "heading": "F CONCLUSION",
            "text": "This paper analyzes learning multi-scale representations by examining the scale of receptive fields. It utilizes the ERF technique for visualization and conducts a qualitative analysis of EFRs for various multi-scale representations that are effective in semantic segmentation. The study reveals the widespread presence of scale inadequacy and field inadequacy as shortcomings.\nTo address these issues, the paper introduces VWA as a receptive-field-variable multi-scale learner capable of avoiding these two problems. Specifically, VWFormer is proposed as the practical outcome of VWA. In comparison to other multi-scale decoders, VWFormer improves the original multiscale representations of the backbone network. VWFormer also exhibits channel scalability, making it adaptable to lightweight backbones.\nThe experimental section primarily compares VWFormer to other advanced multi-scale decoders (MSD), demonstrating its dual superiority in both performance and efficiency for semantic segmentation. Furthermore, the experiments systematically compare VWA to classical receptive-fieldvariable multi-scale learners, showing that VWA outperforms its counterparts consistently. Overall, results and analyses are rigorously conducted, affirming that VWFormer stands out as the premier MSD, while VWA is the top-performing multi-scale learner. This work aspires to drive progress in multi-scale learning for semantic segmentation tasks across various domains."
        }
    ],
    "year": 2023
}