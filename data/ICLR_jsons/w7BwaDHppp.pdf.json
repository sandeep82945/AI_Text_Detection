{
    "abstractText": "Estimating neural radiance fields (NeRFs) is able to generate novel views of a scene from known imagery. Recent approaches have afforded dramatic progress on small bounded regions of the scene. For an unbounded scene where cameras point in any direction and contents exist at any distance, certain mapping functions are used to represent it within a bounded space, yet they either work in object-centric scenes or focus on objects close to the camera. The goal of this paper is to understand how to design a proper mapping function that considers per-scene optimization, which remains unexplored. We first present a geometric understanding of existing mapping functions that express the relation between the bounded and unbounded scenes. Here, we exploit a stereographic projection method to explain failures of the mapping functions, where input ray samples are too sparse to account for scene geometry in unbounded regions. To overcome the failures, we propose a novel mapping function based on a p-norm distance, allowing to adaptively sample the rays by adjusting the p-value according to scene geometry, even in unbounded regions. To take the advantage of our mapping function, we also introduce a new ray parameterization to properly allocate ray samples in the geometry of unbounded regions. Through the incorporation of both the novel mapping function and the ray parameterization within existing NeRF frameworks, our method achieves state-ofthe-art novel view synthesis results on a variety of challenging datasets.",
    "authors": [
        {
            "affiliations": [],
            "name": "Junoh Lee"
        },
        {
            "affiliations": [],
            "name": "Hyunjun Jung"
        },
        {
            "affiliations": [],
            "name": "Jin-Hwi Park"
        },
        {
            "affiliations": [],
            "name": "Inhwan Bae"
        },
        {
            "affiliations": [],
            "name": "Hae-Gon Jeon"
        }
    ],
    "id": "SP:da72d0c33c42daffcd7fc1ca5a156bcb6a646ec7",
    "references": [
        {
            "authors": [
                "Relja Arandjelovi\u0107",
                "Andrew Zisserman"
            ],
            "title": "Nerf in detail: Learning to sample for view synthesis",
            "venue": "arXiv preprint arXiv:2106.05264,",
            "year": 2021
        },
        {
            "authors": [
                "Benjamin Attal",
                "Selena Ling",
                "Aaron Gokaslan",
                "Christian Richardt",
                "James Tompkin"
            ],
            "title": "MatryODShka: Real-time 6DoF video view synthesis using multi-sphere images",
            "venue": "In Proceedings of the European Conference on Computer Vision (ECCV),",
            "year": 2020
        },
        {
            "authors": [
                "Jonathan T. Barron",
                "Ben Mildenhall",
                "Matthew Tancik"
            ],
            "title": "Mip-nerf: A multiscale representation for anti-aliasing neural radiance fields",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV),",
            "year": 2021
        },
        {
            "authors": [
                "Jonathan T Barron",
                "Ben Mildenhall",
                "Dor Verbin",
                "Pratul P Srinivasan",
                "Peter Hedman"
            ],
            "title": "Mip-nerf 360: Unbounded anti-aliased neural radiance fields",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2022
        },
        {
            "authors": [
                "Jonathan T. Barron",
                "Ben Mildenhall",
                "Dor Verbin",
                "Pratul P. Srinivasan",
                "Peter Hedman"
            ],
            "title": "Zip-nerf: Anti-aliased grid-based neural radiance fields",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV),",
            "year": 2023
        },
        {
            "authors": [
                "Che-Han Chang",
                "Min-Chun Hu",
                "Wen-Huang Cheng",
                "Yung-Yu Chuang"
            ],
            "title": "Rectangling stereographic projection for wide-angle image visualization",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV),",
            "year": 2013
        },
        {
            "authors": [
                "Anpei Chen",
                "Zexiang Xu",
                "Andreas Geiger",
                "Jingyi Yu",
                "Hao Su"
            ],
            "title": "Tensorf: Tensorial radiance fields",
            "venue": "In Proceedings of the European Conference on Computer Vision (ECCV),",
            "year": 2022
        },
        {
            "authors": [
                "Rongsen Chen",
                "Fang-Lue Zhang",
                "Simon Finnie",
                "Andrew Chalmers",
                "Taehyun Rhee"
            ],
            "title": "Casual 6-dof: free-viewpoint panorama using a handheld 360 camera",
            "venue": "IEEE Transactions on Visualization and Computer Graphics (TVCG),",
            "year": 2022
        },
        {
            "authors": [
                "Sun Cheng",
                "Sun Min",
                "Chen Hwann-Tzong"
            ],
            "title": "Improved direct voxel grid optimization for radiance fields reconstruction",
            "venue": "arXiv preprint arXiv:2211.12285,",
            "year": 2022
        },
        {
            "authors": [
                "Changwoon Choi",
                "Sang Min Kim",
                "Young Min Kim"
            ],
            "title": "Balanced spherical grid for egocentric view synthesis",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2023
        },
        {
            "authors": [
                "Kangle Deng",
                "Andrew Liu",
                "Jun-Yan Zhu",
                "Deva Ramanan"
            ],
            "title": "Depth-supervised nerf: Fewer views and faster training for free",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "year": 2022
        },
        {
            "authors": [
                "Stephan J. Garbin",
                "Marek Kowalski",
                "Matthew Johnson",
                "Jamie Shotton",
                "Julien Valentin"
            ],
            "title": "Fastnerf: High-fidelity neural rendering at 200fps",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV),",
            "year": 2021
        },
        {
            "authors": [
                "Peter Hedman",
                "Pratul P Srinivasan",
                "Ben Mildenhall",
                "Jonathan T Barron",
                "Paul Debevec"
            ],
            "title": "Baking neural radiance fields for real-time view synthesis",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV),",
            "year": 2021
        },
        {
            "authors": [
                "Rudolph Emil Kalman"
            ],
            "title": "A new approach to linear filtering and prediction problems",
            "venue": "Journal of Basic Engineering,",
            "year": 1960
        },
        {
            "authors": [
                "Mijeong Kim",
                "Seonguk Seo",
                "Bohyung Han"
            ],
            "title": "Infonerf: Ray entropy minimization for few-shot neural volume rendering",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "year": 2022
        },
        {
            "authors": [
                "Ben Mildenhall",
                "Pratul P. Srinivasan",
                "Matthew Tancik",
                "Jonathan T. Barron",
                "Ravi Ramamoorthi",
                "Ren Ng"
            ],
            "title": "Nerf: Representing scenes as neural radiance fields for view synthesis",
            "venue": "In Proceedings of the European Conference on Computer Vision (ECCV),",
            "year": 2020
        },
        {
            "authors": [
                "Thomas M\u00fcller",
                "Alex Evans",
                "Christoph Schied",
                "Alexander Keller"
            ],
            "title": "Instant neural graphics primitives with a multiresolution hash encoding",
            "venue": "ACM Transactions on Graphics (TOG),",
            "year": 2022
        },
        {
            "authors": [
                "Thomas Neff",
                "Pascal Stadlbauer",
                "Mathias Parger",
                "Andreas Kurz",
                "Joerg H. Mueller",
                "Chakravarty R. Alla Chaitanya",
                "Anton S. Kaplanyan",
                "Markus Steinberger"
            ],
            "title": "Donerf: Towards real-time rendering of compact neural radiance fields using depth oracle networks",
            "venue": "Computer Graphics Forum,",
            "year": 2021
        },
        {
            "authors": [
                "Richard A Newcombe",
                "Shahram Izadi",
                "Otmar Hilliges",
                "David Molyneaux",
                "David Kim",
                "Andrew J Davison",
                "Pushmeet Kohi",
                "Jamie Shotton",
                "Steve Hodges",
                "Andrew Fitzgibbon"
            ],
            "title": "Kinectfusion: Real-time dense surface mapping and tracking",
            "venue": "In Proceedings of the IEEE international symposium on mixed and augmented reality (ISMAR),",
            "year": 2011
        },
        {
            "authors": [
                "Martin Piala",
                "Ronald Clark"
            ],
            "title": "Terminerf: Ray termination prediction for efficient neural rendering",
            "venue": "In Proceedings of the International Conference on 3D Vision (3DV),",
            "year": 2021
        },
        {
            "authors": [
                "Thomas Porter",
                "Tom Duff"
            ],
            "title": "Compositing digital images",
            "venue": "ACM Transactions on Graphics (ToG),",
            "year": 1984
        },
        {
            "authors": [
                "Albert Pumarola",
                "Enric Corona",
                "Gerard Pons-Moll",
                "Francesc Moreno-Noguer"
            ],
            "title": "D-NeRF: Neural Radiance Fields for Dynamic Scenes",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2020
        },
        {
            "authors": [
                "Daniel Rebain",
                "Wei Jiang",
                "Soroosh Yazdani",
                "Ke Li",
                "Kwang Moo Yi",
                "Andrea Tagliasacchi"
            ],
            "title": "Derf: Decomposed radiance fields",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2021
        },
        {
            "authors": [
                "Christian Reiser",
                "Songyou Peng",
                "Yiyi Liao",
                "Andreas Geiger"
            ],
            "title": "Kilonerf: Speeding up neural radiance fields with thousands of tiny mlps",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV),",
            "year": 2021
        },
        {
            "authors": [
                "Sara Fridovich-Keil",
                "Alex Yu",
                "Matthew Tancik",
                "Qinhong Chen",
                "Benjamin Recht",
                "Angjoo Kanazawa"
            ],
            "title": "Plenoxels: Radiance fields without neural networks",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2022
        },
        {
            "authors": [
                "Johannes L. Schonberger",
                "Jan-Michael Frahm"
            ],
            "title": "Structure-from-motion revisited",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "year": 2016
        },
        {
            "authors": [
                "Vincent Sitzmann",
                "Julien N.P. Martel",
                "Alexander W. Bergman",
                "David B. Lindell",
                "Gordon Wetzstein"
            ],
            "title": "Implicit neural representations with periodic activation functions",
            "venue": "In Proceedings of the Neural Information Processing Systems (NeurIPS),",
            "year": 2020
        },
        {
            "authors": [
                "Cheng Sun",
                "Min Sun",
                "Hwann-Tzong Chen"
            ],
            "title": "Direct voxel grid optimization: Super-fast convergence for radiance fields reconstruction",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "year": 2022
        },
        {
            "authors": [
                "D Swart",
                "B Torrence"
            ],
            "title": "Mathematics meets photography (parts 1 & 2)",
            "venue": "Math Horizons, Sept.-Nov,",
            "year": 2011
        },
        {
            "authors": [
                "Towaki Takikawa",
                "Alex Evans",
                "Jonathan Tremblay",
                "Thomas M\u00fcller",
                "Morgan McGuire",
                "Alec Jacobson",
                "Sanja Fidler"
            ],
            "title": "Variable bitrate neural fields",
            "venue": "ACM Transactions on Graphics (TOG),",
            "year": 2022
        },
        {
            "authors": [
                "Matthew Tancik",
                "Vincent Casser",
                "Xinchen Yan",
                "Sabeek Pradhan",
                "Ben Mildenhall",
                "Pratul P Srinivasan",
                "Jonathan T Barron",
                "Henrik Kretzschmar"
            ],
            "title": "Block-nerf: Scalable large scene neural view synthesis",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2022
        },
        {
            "authors": [
                "Haithem Turki",
                "Deva Ramanan",
                "Mahadev Satyanarayanan"
            ],
            "title": "Mega-nerf: Scalable construction of large-scale nerfs for virtual fly-throughs",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "year": 2022
        },
        {
            "authors": [
                "Dor Verbin",
                "Peter Hedman",
                "Ben Mildenhall",
                "Todd Zickler",
                "Jonathan T. Barron",
                "Pratul P. Srinivasan"
            ],
            "title": "Ref-NeRF: Structured view-dependent appearance for neural radiance fields",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2022
        },
        {
            "authors": [
                "Peng Wang",
                "Yuan Liu",
                "Zhaoxi Chen",
                "Lingjie Liu",
                "Ziwei Liu",
                "Taku Komura",
                "Christian Theobalt",
                "Wenping Wang"
            ],
            "title": "F2-nerf: Fast neural radiance field training with free camera trajectories",
            "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2023
        },
        {
            "authors": [
                "Zhou Wang",
                "Alan C Bovik",
                "Hamid R Sheikh",
                "Eero P Simoncelli"
            ],
            "title": "Image quality assessment: from error visibility to structural similarity",
            "venue": "IEEE Transactions on Image Processing (TIP),",
            "year": 2004
        },
        {
            "authors": [
                "Silvan Weder",
                "Johannes Schonberger",
                "Marc Pollefeys",
                "Martin R Oswald"
            ],
            "title": "Routedfusion: Learning real-time depth map fusion",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2020
        },
        {
            "authors": [
                "Yi Wei",
                "Shaohui Liu",
                "Yongming Rao",
                "Wang Zhao",
                "Jiwen Lu",
                "Jie Zhou"
            ],
            "title": "Nerfingmvs: Guided optimization of neural radiance fields for indoor multi-view stereo",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV),",
            "year": 2021
        },
        {
            "authors": [
                "Yuanbo Xiangli",
                "Linning Xu",
                "Xingang Pan",
                "Nanxuan Zhao",
                "Anyi Rao",
                "Christian Theobalt",
                "Bo Dai",
                "Dahua Lin"
            ],
            "title": "Bungeenerf: Progressive neural radiance field for extreme multi-scale scene rendering",
            "venue": "In Proceedings of the European Conference on Computer Vision (ECCV),",
            "year": 2022
        },
        {
            "authors": [
                "Wenyan Yang",
                "Yanlin Qian",
                "Joni-Kristian K\u00e4m\u00e4r\u00e4inen",
                "Francesco Cricri",
                "Lixin Fan"
            ],
            "title": "Object detection in equirectangular panorama",
            "venue": "In Proceedings of the IEEE International Conference on Pattern Recognition (ICPR),",
            "year": 2018
        },
        {
            "authors": [
                "Alex Yu",
                "Ruilong Li",
                "Matthew Tancik",
                "Hao Li",
                "Ren Ng",
                "Angjoo Kanazawa"
            ],
            "title": "Plenoctrees for real-time rendering of neural radiance fields",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV),",
            "year": 2021
        },
        {
            "authors": [
                "Alex Yu",
                "Vickie Ye",
                "Matthew Tancik",
                "Angjoo Kanazawa"
            ],
            "title": "pixelnerf: Neural radiance fields from one or few images",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2021
        },
        {
            "authors": [
                "Lihi Zelnik-Manor",
                "Gabriele Peters",
                "Pietro Perona"
            ],
            "title": "Squaring the circle in panoramas",
            "venue": "In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV),",
            "year": 2005
        },
        {
            "authors": [
                "Kai Zhang",
                "Gernot Riegler",
                "Noah Snavely",
                "Vladlen Koltun"
            ],
            "title": "Nerf++: Analyzing and improving neural radiance fields",
            "venue": "arXiv preprint arXiv:2010.07492,",
            "year": 2020
        },
        {
            "authors": [
                "Richard Zhang",
                "Phillip Isola",
                "Alexei A Efros",
                "Eli Shechtman",
                "Oliver Wang"
            ],
            "title": "The unreasonable effectiveness of deep features as a perceptual metric",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),",
            "year": 2018
        },
        {
            "authors": [
                "Xiaoshuai Zhang",
                "Sai Bi",
                "Kalyan Sunkavalli",
                "Hao Su",
                "Zexiang Xu"
            ],
            "title": "Nerfusion: Fusing radiance fields for large-scale scene reconstruction",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "year": 2022
        },
        {
            "authors": [
                "Barron"
            ],
            "title": "misaligned with training poses. Even with the misaligned camera poses, we can successfully synthesize novel views. F IMPLEMENTATION DETAILS In this section, we explain the details of our implementation to verify the flexibility of the proposed mapping strategy",
            "year": 2022
        },
        {
            "authors": [
                "Cheng"
            ],
            "title": "DVGO To implement the DVGO baseline, we adopt a voxel model from Sun et al",
            "year": 2022
        },
        {
            "authors": [
                "Zhang"
            ],
            "title": "output values of the density MLP and the view direction projected onto the spherical harmonics basis with a degree of 4. The models were trained for 25k iterations using a batch size of 216. We utilize the Adam optimizer and decay our learning rate logarithmically from 10\u22122 to 10\u22123 over training. NeRF The NeRF model is based on the original implementation Mildenhall et al",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Starting from the advance of neural radiance field (NeRF) framework Mildenhall et al. (2020), a series of novel view synthesis methods Barron et al. (2021); Sitzmann et al. (2020) have recently shown impressive performance in various scenarios such as surrounding views Chen et al. (2022b), moving foreground objects Pumarola et al. (2020), and reflective media Verbin et al. (2022). Furthermore, Instant-NGP (iNGP) Mu\u0308ller et al. (2022) and TensoRF Chen et al. (2022a) represent a paradigm shift that defines 3D query points using voxel grids for fast training and rendering. All the methods utilize a bounded volume that covers all objects in a scene at first, and then render images from arbitrary viewpoints in common. Unfortunately, the bounded volume cannot fully encompass whole rendering spaces. For unbounded scenes, the models fail to learn accurate 3D geometry and yield erroneous color representations for distant objects. As an intuitive way, expanding the size of the bounded volume can be considerable, but increases the number of samples in a ray and learnable parameters, which causes a huge computational burden like memory issues.\nTo address this issue on unbounded scenes, state-of-the-art methods Zhang et al. (2020); Barron et al. (2022; 2023) use deterministic functions to map ray samples in the unbounded region into the bounded region. To be specific, in NeRF++ Zhang et al. (2020), an unbounded space is partitioned into an inner sphere and an outer volume. The inner sphere is an identity mapping which contains a foreground of a scene and all cameras, and the the outer volume has a background scene. This is achieved using an inverted-sphere mapping, which utilizes an inverse distance from an origin of a scene to render the background pixels. mip-NeRF 360 and Zip-NeRF Barron et al. (2022; 2023) parameterize unbounded scenes using an identity mapping for a foreground and contract mapping to sample the volumetric density for a background. Here, with the parameterization of the contract mapping, sampling ranges in a ray can be expanded to an infinite depth of the scene, representing\n\u2217Corresponding author\nBounded region\nQ\nO\nQ\nO\nHyper-axis\nO\nQ\nQ\nO\n(d) Our p-norm-based mapping\n(c) Paraboloid\n(b) Cylinder\n(a) Steregographic proj. Sliced-axis\nFigure 1: Examples of changes of embedding space w.r.t. manifold shape. We plot how to be mapped an unbounded space into a bounded space. The red arrow indicates the mapping between the unbounded space and the surface of the manifold based on the center of projection Q. The yellow line refers a mapping space between the tree and the tower, and the horizontal and vertical axis represent a sliced axis of the 3-dimensional real-world space and the 4th dimensions (i.e., hyper-axis), respectively. The hyper-axis acts as a homogeneous coordinate, and is introduced to describe a projection fuction. (a) Stereographic projection uses a sphere for the projection. (b) Inverted sphere mapping in Zhang et al. (2020), and (c) contract mapping in Barron et al. (2022) utilize a cylinderical and a paraboloid manifold, respectively, instead of the sphere. (d) Our mapping strategy adaptively allocates the space with the different shapes of manifolds based on the scene geometry.\npixels located in the unbounded regions. While these mapping functions effectively model unbounded regions from novel viewpoints, they encounter a difficulty in rendering distant objects, particularly when camera poses are positioned significantly away from a center of themselves in world coordinate, called a scene origin in Zhang et al. (2020). Obviously, ray samplings also fail if the ray origin is far away from the scene origin, which has a significant impact on the usability of neural rendering such as 360\u25e6 real-world Barron et al. (2022) and free trajectory Wang et al. (2023).\nIn this paper, we first introduce a unified framework for analyzing the unbounded scene representations using a concept of stereographic projection1, a well-known method to make a map of the Earth in Fig. 1-(a). The stereographic projection can be adopted to describe a boundless region within a specific manifold (e.g., panorama image) Chang et al. (2013); Yang et al. (2018); Zelnik-Manor et al. (2005). With the property of the stereographic projection, we can project an infinite space into a bounded region, and analyze the previous approaches Zhang et al. (2020); Barron et al. (2022) to explore the relationship between unbounded real-world spaces and bounded spaces based on closed-form mapping functions. Through this analysis, we investigate the vulnerability of the existing mapping functions in terms of the manifold\u2019s shape. Fig. 1-(b) and (c) illustrate that the inverted sphere and the contract mapping project a real-world space into cylindrical space Zhang et al. (2020) and paraboloidal space Barron et al. (2022), respectively. They show that the two mapping functions pay more attention to the near object, which indicates that they do not have enough capacity to represent the far region. Since their mapping functions are invariant even if a scene varies, its ratio between the pre-defined near and far regions does not change.\nOur key insight is that the manifold shape should be adjusted according to scene geometry to ensure the representation capacity of both near and far regions. To do this, we design an adaptive mapping function based on the p-norm distance Deza et al. (2014), a method to define finite-dimensional vector spaces using the p-norm. The parameter p enables a mapping function to deform the surface of the embedding space depending on the scene configurations. A large p makes the surface convex and allocates more capacity to nearby contents on the finite volume. In contrast, a small p induces the concave of the surface, which leads to a larger capacity to distant contents. For example, in Fig. 1-(d), the adaptive mapping enables us to assign the much less representation capacity to the free space between the tree and tower. Since it is important to determine the p value according to the scene geometry, we show how to automatically set a proper p value using a RANSAC framework. We randomly choose certain 3D points from a point cloud in a scene and then iteratively project them onto the embedding space. We measure distances among them and find a final p value with the maximum distance, which exploits the full capacity of the embedding space.\nGiven our novel mapping function, it requires an appropriate ray parametrization, making ray intervals of sampled points to be evenly-spaced in the deformed embedding space. If the intervals are either too small or too large in the space, there will be over- or under-sampling problems. For this, we need to devise a strategy for choosing adaptive intervals according to scene geometry because the conventional ray parameterizations Zhang et al. (2020); Barron et al. (2022) use fixed intervals regardless of the shape of the manifold. We thus propose a novel angular ray parameterization to\n1Stereographic projection is a projective mapping, which can be realized as the projection of points from one projective space to another, where all projections come from a fixed point (called the center of projection) and pass through the points of the space being mapped.\nadaptively select proper intervals, which are determined based on an angle between two sampled points in an unbounded space from a center of the manifold.\nTo demonstrate the effectiveness of the proposed mapping function and ray parameterization, we conduct extensive experiments on unbounded scenes, including 360\u25e6 object-centric and free trajectory. Experimental results show that our method can be successfully integrated with existing multilayer perceptron (MLP)-based and voxel-based models and contribute to significant performance improvements for novel view synthesis, where the conventional mapping functions often fail."
        },
        {
            "heading": "2 RELATED WORK",
            "text": ""
        },
        {
            "heading": "2.1 NEURAL RADIANCE FIELD",
            "text": "NeRF frameworks Mildenhall et al. (2020) implicitly encode radiance and density fields of target scenes with MLP layers without any explicit geometric proxy. In terms of the sparsity of input images, works in Yu et al. (2021b); Kim et al. (2022); Deng et al. (2022) reconstruct scenes with fewer images than that of conventional NeRF techniques. Meanwhile, methods to accelerate training Mu\u0308ller et al. (2022); Takikawa et al. (2022); Chen et al. (2022a); Sun et al. (2022); Yu et al. (2021a); Sara Fridovich-Keil and Alex Yu et al. (2022) and inference Reiser et al. (2021); Yu et al. (2021a); Piala & Clark (2021); Hedman et al. (2021); Garbin et al. (2021) are developed. We note that these works have shown impressive performances if contents exist inside a pre-defined bounded volume.\nTo alleviate the constraint on the bounded volume, some relevant works have scaled up the sampling boundary of the volumetric representations by addressing technical limitations such as the radiance ambiguity Wei et al. (2021), network capacity Rebain et al. (2021), and matching ambiguity of 3D content Arandjelovic\u0301 & Zisserman (2021). For indoor scenes, NeRFusion Zhang et al. (2022) constructs each local feature volume from each input image and fuses them through a truncated signed distance function Newcombe et al. (2011); Weder et al. (2020). For outdoor scenes, both Block-NeRF Tancik et al. (2022) and Mega-NeRF Turki et al. (2022) represent block-size scenes with divide-and-conquer strategies in which one large-scale place is divided into a set of small-scale scenes. They are then trained with volumetric renderers. For larger-scale scenes, BungeeNeRF Xiangli et al. (2022) uses a multi-scale rendering with very different levels of details in a wide span of viewpoints. Although these scalable methods extend the rendering ranges by using additional networks, the issue on the unbounded scenes still remains."
        },
        {
            "heading": "2.2 NERF FOR UNBOUNDED SCENES",
            "text": "To learn radiance fields for unbounded scenes, previous methods adopt multi-sphere images Sara Fridovich-Keil and Alex Yu et al. (2022); Attal et al. (2020), balanced spherical grid Choi et al. (2023), space subdivision Wang et al. (2023) and coordinate transformation Barron et al. (2022); Zhang et al. (2020). Plenoxels Sara Fridovich-Keil and Alex Yu et al. (2022) employs multi-sphere images Attal et al. (2020) to render background scenes, handling unbounded scenes with voxel grids. Nevertheless, this approach manifests limitations, particularly in the emergence of blur and ghost artifacts when depicting objects positioned between the pre-defined layered spheres. The work in Choi et al. (2023) uses sequential spherical panorama images to reconstruct a large-scale scene. And then, two non-overlapped spherical grids allow the scene to be represented with a similar length in angular and radial directions, useful for distant backgrounds. F2-NeRF Wang et al. (2023) models unbounded scenes with subdivided spaces and warping functions. However, this subdivision is intrinsically tied to camera poses and is sensitive to scene dependency. For example, F2-NeRF would use the same structure of subdivided space if camera poses are the same with other scenes. DONeRF Neff et al. (2021) applies the inverse square function for an efficient representation of distant points, but only works on forward-facing scenarios. mip-NeRF 360 Barron et al. (2022) borrows the idea of the Extended Kalman filter Kalman (1960) to reparameterize the ray distance so that distant points should be denser and less spaced. NeRF++ Zhang et al. (2020) takes an inverse function on the ray distance as the mapping function and uses it as an additional input coordinate. Specifically, they divide the background and foreground parts of scenes based on the distance from the origin and make MLPs to represent each.\nSince the mapping functions in the previous methods are strictly defined, they struggle to learn radiance fields in diverse scenarios due to their inability to handle scene dependency. In contrast, we present a universal mapping function, which is flexible and general to learn both bounded and unbounded regions in neural radiance spaces."
        },
        {
            "heading": "3 PRELIMINARY",
            "text": "Radiance field Mildenhall et al. (2020); Chen et al. (2022a); Sun et al. (2022); Mu\u0308ller et al. (2022) represents a 3D scene through MLPs or voxel grids with sampled points from a ray function r(t) = o+ td, where o is a ray origin, d is a vector of a viewing direction and t is a distance along the ray. Input 3D points are sampled along the ray function and fed to MLPs or voxel grids to infer an output color c and a density \u03c3. The output color and density along the ray are cumulated by a classical volume rendering equation Porter & Duff (1984) below:\nC(r) = N\u2211 i=1 Ti ( 1\u2212 exp(\u2212\u03c3i\u03b4i) ) ci, s.t. Ti = exp ( \u2212 i\u22121\u2211 j=1 \u03c3j\u03b4j ) (1)\nwhere C(r) is a synthesized pixel color of the ray, N denotes the number of samples along the ray and \u03b4i means a distance between the i-th sample and its next sample on the ray. The radiance field is optimized by minimizing a loss function between a ground-truth and the synthesized pixel color.\nTo render the pixel color in unbounded regions, NeRF++ Zhang et al. (2020) adopts an inverted sphere parameterization as follows:\ninverted sphere(x) =\n{ (x1, x2, x3), \u2225x\u2225 \u2264 1(\nx1 \u2225x\u2225 , x2 \u2225x\u2225 , x3 \u2225x\u2225 , 1 \u2225x\u2225\n) , \u2225x\u2225 > 1, (2)\nwhere x = (x1, x2, x3) refers to 3D coordinate in the unbounded space.\nThis parameterization first defines a foreground of a scene with a unit sphere centered at the scene origin, otherwise background regions. A key idea of this parameterization is to map the background points onto to 4-dimensional bounded space made from a concatenation of the inverse distance 1\n\u2225x\u2225 from the scene origin and its normalized coordinate ( x1 \u2225x\u2225 , x2 \u2225x\u2225 , x3 \u2225x\u2225 ) . The foreground and the background contents are separately learned on different MLPs.\nThe contract parameterization used in mip-NeRF 360 Barron et al. (2022) and Zip-NeRF Barron et al. (2023) is the other way to map unbounded points into a bounded region, which is formulated as:\ncontract(x) = { x, \u2225x\u2225 \u2264 1( 2\u2212 1\u2225x\u2225 )( x \u2225x\u2225 ) , \u2225x\u2225 > 1. (3)\nThe contract parameterization maps an infinity point on the boundary of the bounded volume.\nAlthough the inverted sphere and contract parameterizations can represent objects located at an unlimited distance from the scene origin, they fail to consider regions where sampled points are allocated sparsely. To efficiently assign the samples, dense sampling around the objects is necessary. This problem can be managed by constructing a scene geometry-aware mapping function with a relation between sampled points in the unbounded region and the transformed points."
        },
        {
            "heading": "4 METHODOLOGY",
            "text": ""
        },
        {
            "heading": "4.1 UNDERSTANDING UNBOUNDED SCENES WITH STEREOGRAPHIC PROJECTION",
            "text": "We understand the limitations of the existing mapping functions, the inverted sphere mapping Zhang et al. (2020) and the contract mapping Barron et al. (2022; 2023), for unbounded scenes via stereographic projection. First, we decompose the mapping functions into an inverse stereographic projection and an orthogonal projection. The inverse stereographic projection is used to construct a one-to-one relationship between an unbounded region and a certain manifold, while the orthogonal projection maps points into the surface of a manifold in a bounded region. As shown in Fig. 2-(a), the na\u0131\u0308ve stereographic projection is defined with a unit sphere S in RL+1 where L is the number of dimensions and the center of projection Q, which is a point on the north pole of the sphere. The projection is conducted from a point xm in the S \\{Q} to a point x on the projective line (red line), which intersects a plane \u03a0 in RL with exactly one point x. Since the original stereographic method (Fig. 2-(a)) maps points on the surface of the sphere, we cannot perform one-to-one orthogonal projection over the bounded region. There may exist a case that at most two points xm are projected on the same point xb in the bounded region (Refer to Fig. 8 in Appendix). To solve this problem, we utilize a certain manifold (e.g., a lower unit hemisphere) and change the location of the center of projection Q (Fig. 2-(b) and (c)), which guarantees one-to-one projection between the\nQ Hyper-axis\nHyper-axis Hyper-axis\n(a) Stereographic projection (c) 2D plot of (b)\nOO\nQ\nHyper-axis\n(d) Project to original dim. (e) Deformation of (d)\nHyper-axis\nf1 f2 f3\n(b) Modified inverse stereographic projection\nQ S\nxm\nx\nQ\nO\nQ\nxm x\nxm\nx\nxm xm1 xm2\nxm3\nx xxb1 xb2 xb3 Sliced-axis\nFigure 2: How to modify the stereographic projection for a deformable mapping function. (a) Conventional stereographic projection. A point xm on the sphere S is projected onto a point x in a plane \u03a0 (red-colored line). (b) The inverse stereographic projection. The point x in \u03a0 is projected at the point xm. (c) The center of projection Q is moved to the center of the manifold. We can only use a half side of the sphere. (d) The orthogonal projection (blue-colored line) is used to locate a point xb in \u03a0 at the bounded space (purple-colored region) to reduce the dimensionality. (e) If we adaptively deform a shape of the manifold, the point x can be mapped onto the different bounded spaces.\nmanifold and the bounded space (Fig. 2-(d)). If we can deform the manifold shape, we construct diverse mapping functions in Fig. 2-(e), whose solution will be described in Section 4.3.\nWith the property of the stereographic projection, we derive the previous mapping functions in Eqs. (2) and (3) as a unified form which is able to show their manifold shapes. They utilize a coordinate transformation, which constructs a relationship between a points in real-world space R3 and the points in bounded space; Eq. (2) defines the mapping function that utilizes R3 and R4 for the foreground and the background, respectively, and Eq. (3) uses R3 space as the bounded space. To investigate the manifold\u2019s shape defined in R4, we design a closed-form mapping function that can represent the manifold\u2019s shape (e.g., cylinder and paraboloid) on R3. Since the orthogonal projection acts as a transformation between the bounded space and the manifold, we set the inverse of the orthogonal projection to f : xb \u2192 xm where xb \u2208 R4 is a mapped point in the bounded region and xm \u2208 R4 is a point on the manifold."
        },
        {
            "heading": "4.2 ANALYSIS OF PREVIOUS APPROACHES",
            "text": "We can decompose the previous mapping function using the fact that the stereographic projection transforms a point along a line (Fig. 3-(a)) as follows:\nx\u2212Q = m (xm\u2212Q) s.t. x = mxb, (4) where m is a parameter of the line and x \u2208 R4 is a point on \u03a0. Thus, the xm can be represented as\nxm = xb + ( 1\u2212 1\nm\n) Q. (5)\nInverted Sphere Parametrizaton. We first investigate the inverted sphere parameterization in Eq. (2) using Eq. (5). According to Zhang et al. (2020), foreground points are mapped by an identity mapping (i.e., m = 1) and the function f can be defined as xm = xb, where \u2225xb\u2225 \u2264 1. Otherwise, points in the background are mapped based on the norm of \u2225x\u2225, i.e., m = \u2225x\u2225. Since the mapping function from x and xb is not bijective in this case, we represent the mapping with xm. In particular, when \u2225xb\u2225 = 1, xm is described by a set of { xb+ ( 1\u2212 1\u2225x\u2225 ) Q | \u2225x\u2225 > 1 } . Therefore, the manifold can be defined as the union of two sets based on the norm of xb as:\nXm = { xb \u2223\u2223 \u2225xb\u2225 \u2264 1} \u222a {xb + (1\u2212 1\u2225x\u2225)Q \u2223\u2223\u2223 \u2225x\u2225 > 1 and \u2225xb\u2225 = 1}, (6) Eq. (6) means that the parameterization maps points in \u03a0 to a cylinderical manifold. We can see that the points in the background are mapped to the side of the cylinder, indicating that the orthogonal projection is a non-invertible function in this manifold. Here, the background points are still located in R4, and the parameterization is hard to apply to conventional NeRF frameworks using the R3 coordinate system.\nContract Parametrizaton. We can employ Eq. (5) on the contract function Eq. (3). A function for foreground points, the same as the inverted sphere parameterization, is xm = xb, where \u2225xb\u2225 \u2264 1. For background points where \u2225x\u2225 > 1, we know that 1m = ( 2 \u2212 1\u2225x\u2225 ) 1\n\u2225x\u2225 . Substisuting m and x with xb in Eq. (5) yields xm = xb + ( 1 \u2212 2 \u2225xb\u2225 + \u2225xb\u22252 ) Q. Therefore, the f of the contract parameterization is Xm = { xb \u2223\u2223 \u2225xb\u2225 \u2264 1} \u222a {xb + ( \u2225xb\u2225 \u2212 1 )2Q \u2223\u2223 1 < \u2225xb\u2225 < 2}, (7)\nThe Eq. (7) can also be represented as the form of the function f below:\nf(xb) = { xb, \u2225xb\u2225 \u2264 1 xb + ( \u2225xb\u2225 \u2212 1 )2 Q, 1 < \u2225xb\u2225 < 2\n(8)\nThe parameterization in Eq. (8) maps points on \u03a0 to a paraboloidal manifold. Eq. (6) indicates that the inverted sphere mapping allocates spatial representation capacity based on the 1\u2225x\u2225 , i.e., the inverse distance from the scene origin. On the other hand, the contract parameterization Eq. (8) utilizes the quadratic form to handle an unbounded scene. They assume that points in the real world can be adequately projected onto a surface of a certain manifold using their fixed mapping function. However, to allocate the capacity in the whole embedding space, the distribution of the points should follow their assumption as well (e.g., the inverse distance and the inverse of the square distance). If the assumption breaks down, the spatial capacity is not enough to represent all objects in a scene well. In addition, allocating excessive space to objects wastes the capacity. Therefore, we need a function that achieves an optimal trade-off between the quality and the efficiency. In this work, our key idea is to deform and find the manifold that fits the distribution of contents in the real world, which will be described in the next section.\n4.3 p-NORM PARAMETERIZATION\nBased on the analysis in Section 4.2, we design an adaptable mapping function in consideration of distributions of contents using the modified stereographic projection and p-norm distance metric. Here, we assume that the distributions follow an inverse of p-norm distance from Q as shown in Fig. 3-(b). The manifold where unbounded points are mapped can be represented as:\nXm = { xm \u2223\u2223 \u2225xm\u2212Q\u2225p = 1 and (xm\u2212Q) \u00b7Q < 0}, (9) The mapping function xm is computed based on the p-norm distance from the center of projection Q\n2, defined as \u2225x\u2212Q\u2225p. In particular, by setting m = 1\u2225x\u2212Q\u2225p, we can formulate the mapping function from the unbounded region to the bounded region as xb = x\u2225x\u2212Q\u2225p. Fig. 3-(b) shows an example of the p-norm function indicating that we can map points into different locations according to the p value. It shows that increasing the p value makes xb to be moved further from the scene origin, which assigns a bigger capacity to nearer contents.\nWe determine scene-dependent p values using a RANSAC framework in Fig. 3-(b). We reuse a point cloud that had already been made for camera pose estimation at an initial step in conventional NeRF frameworks, using COLMAP Schonberger & Frahm (2016). Our hypothesis is that, in order to make full use of the embedding space, points should be evenly distributed in the whole space. To do this, we first randomly sample two 3D points in the point cloud and project them on the embedding space. We then compute an Euclidean distance between them. This process is repeated for the given number of iterations, and we select a p value with the maximum distance."
        },
        {
            "heading": "4.4 ANGULAR RAY PARAMETERIZATION",
            "text": "Conventionally, ray parameterizations for unbounded scenes employ various distance metrics on t of the ray function r(t) to sample points on a ray Neff et al. (2021); Zhang et al. (2020); Barron et al. (2022). For example, NeRF++ uses t and an inverse distance from the scene origin for a foreground\n2In this paper, we regard Q = (0, 0, 0, 1) for simplicity\nand a background region, respectively. In addition, DoNeRF adopts a logmetric distance, and mipNeRF 360 uses a disparity distance for the ray parameterization. However, our ray parameterization should consider a distortion of the embedding space to benefit from our mapping function due to its non-linearity. Without this consideration, the sampled points are likely to be biased. Fig. 4\u2013(b) and (c) shows a toy example of a case where p = 2 and the conventional ray parameterization Barron et al. (2022) is used. Since the parameterization is based on a uniform sampling with a normalized distance in the normal embedding space, it occurs over- and under-sampling if the ray origin is far from the scene origin.\nWe thus introduce a new ray parameterization that regards the distance between points in the distorted embedding space. Our idea is to use an angle based on the center of the projection Q to preserve the relative distance between the points across varying manifold shapes. The angular ray parameterization is formulated as follows (see Fig. 4-(a)):\n\u03b8\n\u03b8max , \u03b8 = \u2220(x\u2212Q, o\u2212Q ), \u03b8max = \u2220(d\u2212Q, o\u2212Q ). (10)\nwhere \u03b8 is angle parameter of the ray and \u03b8max is a maximum value of \u03b8. Intuitively, we can know \u03b8max = \u2220(d \u2212 Q, o \u2212 Q ) (considering x is extremely far from the ray origin o). Therefore, the angular parameterization is a normalized interval which is \u03b8\u03b8max \u2208 [ 0, 1).\nAs shown in Fig. 4-(d), compared to the normalized distance approach, the uniform sampling on our angular parameterization has a more evenly-spaced interval in the embedding space. This indicates that our parameterization enable to avoid over- and under-sampling problems, even with the same number of samples."
        },
        {
            "heading": "5 EXPERIMENT",
            "text": ""
        },
        {
            "heading": "5.1 COMPARISON OF OURS AND STATE-OF-THE-ART METHODS",
            "text": "To demonstrate the generality and applicability of our method, we incorporate it into a variety of types of NeRF frameworks, including DVGO Sun et al. (2022); Cheng et al. (2022), TensoRF Chen et al. (2022a), iNGP Mu\u0308ller et al. (2022); Barron et al. (2023), and NeRF Mildenhall et al. (2020); Zhang et al. (2020). We just follow their own training configurations, such as the number of voxels, parameters, samples, and iterations. For the iNGP model, we use Zip-NeRF Barron et al. (2023) codebase and modify it similar to the original NeRF model Mildenhall et al. (2020); we utilize a na\u0131\u0308ve sampling strategy and single evaluation on a coarse module. To balance the single evaluation, we double the number of samples on a ray. For the NeRF model, we double the number of samples as well and increase the number of hidden units to match the sample numbers and parameters as NeRF++. Please refer to the supplementary material for more details.\nFor our experiments, we use three datasets: two 360\u25e6 object-centric datasets\u2013Tanks and Temples Zhang et al. (2020) and mip-NeRF 360 Barron et al. (2022)\u2013and a free trajectory dataset, named Free Dataset Wang et al. (2023). To simulate the movement of cameras away from the scene origin, we adjust the camera\u2019s position based on the boundary of xb for the object-centric datasets, illustrated in Fig. 5. We consider two scenarios for the camera positions: the near and far boundary. Here, we use three quantitative metrics: PSNR, SSIM Wang et al. (2004), and LPIPS Zhang et al. (2018).\nSince the NeRF frameworks used in this experiment are not designed to work in unbounded scenes, we embed the contract mapping and normalized ray parameterization Barron et al. (2022) into them for fair comparisons. As comparison methods, we additionally choose inverted sphere parameteriza-\nMean distnace from to\nProjected manifold (a) 1 (b) 2\nFigure 5: An illustration of the experimental setup. We intentionally change the locations of the cameras to take unbounded scenes: (a) The cameras are located near a boundary of the embedding space. (b) All cameras are outside of the boundary.\nC o\nn tr\nac t\nO u rs\n(b) DVGO (c) TensoRF (d) iNGP (e) NeRF(a) Ground Truth\nFigure 6: A comparison of ours and the contract mapping is the case of \u00d72 in the Tanks and Temples dataset. (a) The ground truth images. (b, c, d, e) From left to right, we embed the two mapping functions into the DVGO, TensoRF, iNGP, and NeRF models to test their generality and applicability. Our model shows impressive results, while the contract mapping fails to capture some fine detailed objects such as the pole and barricade.\ntion Zhang et al. (2020) and F2-NeRF Zhang et al. (2020); Wang et al. (2023). Note that the inverted sphere parameterization only works in MLP-based models, and F2-NeRF does not provide any source code for the experiment on the Tanks and Temples dataset.\nWe report the quantitative evaluation in Table 1. Our method shows better performance overall, except in some cases of \u00d71 where \u00d7 denotes the multiplier of the camera position from the scene origin. This can be attributed to the camera orientation: since the contract mapping does the identity mapping at the nearby regions, it works well if many objects are located in the regions. However, when the camera is positioned further from the scene origin (case \u00d72), all the comparison methods with the contract mapping face the challenge of representing unbounded scenes. In general, our mapping function is beneficial for the voxel based-mehods like DVGO and TensoRF. Since they use explicit correspondences between coordinates and features, the explicit representation can be evenly allocated in the embedding space through our mapping function. In practice, we have a limited capacity to express neural radiance fields, and our method enables to make maximum use of the capacity. DVGO and TensoRF exhibit a smaller performance degradation compared to iNGP and NeRF. This is because their extensive point sampling along rays mitigates the potential undersampling issue. Notably, in the \u00d72 case, the iNGP with the contract mapping struggles with scene optimization, which implies that it is not applicable for the hash-grid approach. In total, the comparison methods fail to efficiently allocate the representation capacity on the scenes.\nIn contrast, our method provides a distinctive advantage in the \u00d72 scenarios. As shown in Fig. 6, our method shows the promising performance for the distant objects. Thanks to the efficient capacity allocation of our mapping function, we can see clear rendering images that capture both the near\nand far poles well. A similar trend is observed for the Free dataset. Fig. 7 shows better results from ours than that of the comparison methods. Through the experiments using these public datasets, we confirm the importance of adaptive mapping and its ray parameterization."
        },
        {
            "heading": "5.2 ABLATION STUDY",
            "text": "We demonstrate the effectiveness of our mapping function and ray parameterization. For this, we replace each component with the normalized ray parameterization and contract mapping in Barron et al. (2022). In addition, we automatically adjust the p values to validate the advantage of the geometric-aware mapping strategy. We change the camera position to implement the various distribution of objects in the scene. In this study, a bicycle scene in the mip-NeRF 360 dataset is used.\nAs shown in Table 2, our mapping function and ray parameterization yield reasonable outcomes. When the p value is large, the points will map more broadly to the scene origin in the bounded space. This means that more capacity is allocated to nearby objects. In this case, the nearby object means that they are close to the scene origin. The closer the camera is to the scene origin, the larger p value represents the near object better because it is more aligned with the camera. In particular, the further away the camera is from the scene origin, the better our method performs. Plus, when the proper p value is determined via the RANSAC, the performance gap widens."
        },
        {
            "heading": "6 COUCLUSION",
            "text": "In this work, we numerically analyze the weakness of the existing mapping functions and present a novel p-norm-based mapping function and an angular ray parameterization in consideration of scene geometry to handle a rending issue on unbounded scenes in NeRFs. In the experiment, we demonstrate the effectiveness of our method on various NeRF frameworks and achieve the stateof-the-art results by successfully synthesizing novel view images in challenging unbounded scene scenarios. For future work, our work has room for improvement with function designs that better optimize for scene geometry, object distribution and the position of scene origin Limitation Although our method shows the state-of-the-art results, there are still rooms for improvement. First, if the camera position is extremely far from the scene origin, the model struggles to learn the scene. In addition, our ray parameterization samples more points in distant regions than in nearby regions, which is likely to miss near objects when the camera is far from the scene origin. For optimal p values, we have plan to design an iterative approach. We expect that better results are obtained if an optimal p value is used by recovering SfM errors, whose initial implementation is described in Appendix D. A region-aware p value estimation is also an interesting future direction in consideration of a variety of distributions of objects and structures in a scene."
        },
        {
            "heading": "ACKNOWLEDGMENTS",
            "text": "This work is in part supported by the Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government (MSIT) (No.2019-0-01842, Artificial Intelligence Graduate School Program (GIST) and No.2021-0-02068, Artificial Intelligence Innovation Hub), the Ministry of Trade, Industry and Energy (MOTIE) and Korea Institute for Advancement of Technology (KIAT) through the International Cooperative R&D program in part (P0019797), \u2019Project for Science and Technology Opens the Future of the Region\u2019 program through the INNOPOLIS FOUNDATION funded by Ministry of Science and ICT (Project Number: 2022- DD-UP-0312), and GIST-MIT Research Collaboration grant funded by the GIST in 2024."
        },
        {
            "heading": "B STEREOGRAPHIC PROJECTION",
            "text": "Stereographic projection is the perspective projection of a sphere that is projected onto a plane perpendicular to the diameter based on a particular point on the sphere (the pole or the center of projection). The projective mapping is a smooth and bijective function from the entire sphere to the entire plane, except the projection center. It is conformal, transforming spherical circles to plane circles or lines, maintaining angles at curve intersections, and roughly preserving shapes in the local area. The stereographic projection is used in a variety of disciplines, including cartography, geology, and photography. This is why it has the advantage of being utilized to graphically display the boundless space, which is essential for the recognition and interpretation of patterns. For example, the stereographic projection has been used to map spherical panorama Swart & Torrence (2011), which is the widest possible photograph, and is used to capture a wide-angle view by mapping a hemisphere to a plane Chang et al. (2013).\nWe provide a formulation of stereographic projection in three-dimensional space R3, which establishes a one-to-one correspondence between the sphere S2 and its equatorial plane. The correspondence between the points of the sphere and the plane is obtained in the following way: From a point on the sphere, i.e., the center of projection, the other points of the sphere are projected by lines onto a plane. The unit sphere S2 in R3 is the set of points (x, y, z) such that x2 + y2 + z2 = 1. Let Q = (0, 0, 1) be the center of projection, and G be the rest of the sphere. Here, we assume a plane z = 0 runs through the center of the sphere, and then the sphere\u2019s equator is the intersection of the sphere with the plane. For any point H on G, there is a unique line through Q and H that intersects the plane z = 0 at exactly one point, H \u2032. We define the stereographic projection of H to be the point H \u2032 in the plane. Given cartesian coordinates for H = (x, y, z) and H \u2032 = (X,Y ), we can define the stereographic projection, and its inverse, by\n(X,Y ) = ( x 1\u2212 z , y 1\u2212 z ) , (11)\n(x, y, z) = ( 2X 1 +X2 + Y 2 , 2Y 1 +X2 + Y 2 , 1\u2212 2 1 +X2 + Y 2 ) . (12)\nQ\nO\nHyper-axis many-to-one\n(a) Orthogonal projection with stereographic projection (b) Orthogonal projection with modified stereographic projection\nxm\nxb x1 x2\nQ\nO\nHyper-axis\nxm\nxb x\none-to-one x1 xbx1 xb x2 xb\nFigure 8: Visualization of stereographic projection and its modification. (a) Combining orthogonal projection with stereographic projection is not a bijective function. (b) Mapping from x to xb is oneto-one projection.\nStereographic projection can construct one-to-one correspondence between the sphere and the plane, but it cannot be combined with orthogonal projection. As shown in Fig. 8, the original stereographic projection maps points in unbounded space to points on the sphere. If we perform the orthogonal projection from points on the sphere to bounded space, two points x1 and x2 (points from upper and lower hemisphere) can be projected on the same point xb as shown in Fig. 8. Therefore, the combination of the original stereographic projection and orthogonal projection is not a bijective function. To resolve this issue, we introduce the modified stereographic projection which moves the center of projection from the pole of the sphere to the center of the sphere."
        },
        {
            "heading": "C ADDITIONAL DESCRIPTIONS FOR MAPPING FUNCTIONS",
            "text": "In this section, we provide additional visualization of the mapping functions described in Section 4.2 and Section 4.3. Fig. 9 shows the decomposed mapping functions. The projection on the modified stereographic projection, which maps unbounded points on a half-sphere, is shown in Fig. 9-(a). The other mappings can be constructed by deforming the half-sphere. The inverted sphere (Eq. (2)) can be interpreted as a modified stereographic projection between unbounded points and cylinder, which maps foreground points on the bottom of the cylinder and background points on the side of cylinders in Fig. 9-(b). The contract parameterization maps the foreground points with an identity mapping, while the background points are first mapped on the paraboloid and projected to bounded space in Fig. 9-(c). Fig. 9-(d) shows our parameterzation with p = 1 and p = 2 cases. We can see that the unbounded point x is mapped on x\u2032b and xb by 1-norm and 2-norm function, respectively.\nThe effect of different p is shown in Fig. 10. For the uniformly sampled input points in the near and distant region Fig. 10-(a), we visualize the result of projected points by inverted sphere Fig. 10-(b), contract Fig. 10-(c), our 1-norm Fig. 10-(d) and 2-norm mapping Fig. 10-(e). The inverted sphere and contract mapping show similar results. On the other hand, the 1-norm and 2-norm display different distributions with respect to the input distribution. The mapping functions marked with red dots are the ones that utilize the most embedding space among the mapping functions. We can see that the 1-norm function has an advantage in the distant region, and the 2-norm function is better in the near region.\np=3 p=2.1 p=2.0\np=1.9 p=1.95\np=1.0 p=1.5\np=2.0\nD ITERATIVE APPROACH TO ESTIMATE p VALUE\nSince SfM can be erroneous in regions where have either homogeneous and repeated textures, we design an iterative p value estimation from the trained NeRF models. With the trained NeRF model, we render all training views, and measure absolute errors between them and their ground-truth. We next select pixels whose error is below a pre-defined threshold. Based on the selected pixels, we make a point cloud using its depth values which can be acquired from the trained NeRF model. Using the point clouds, we estimate better p value with our RANSAC method. With the estimated p value, we train NeRF model again. Thie process is repeatedly done until the p value converges. With this method, we can correct the erroneous initial p value in Figs. 11 and 12.\nTo validate the effectiveness of the iterative manner, we perform an experiment. In this experiment, we use three types of initial p values such as small, near-optimal, and large numbers. We render the train views from the trained NeRF, select pixels that had an absolute error of less than 0.04, and then find the 3D points corresponding to those pixels. Using these points, we again predict the p value. As shown in Fig. 11 (a), it is possible to re-estimate the p value with iterative refinement. It shows that even with different initial p values, the optimal p value is found. Fig. 12 shows an example result corresponding to the estimated p values. As the iteration goes on and the p value gets closer to its optimal value, the overall image quality increases in terms of PSNR and SSIM. The difference in rendering quality is amplified when the cameras are misaligned with the train positions. In Fig. 13, we can see that correcting the p value can balance the model capacity by reducing blurry regions. In addition, the optimal p value shows better rendering quality than the contract mapping which shows undesired artifacts and mispredicted luminance. This shows the potential to overcome the current limitation of SfM manner for the initial geometry acquisition.\nE MORE ANALYSIS FOR p VALUE\nWe further carry out an experiment how the RANSAC algorithm is affected when the COLMAP point cloud is inaccurate. We consider two cases where COLMAP fails: (1) noisy point cloud due to repeated patterns of a scene (2) sparse point cloud from mismatching correspondences. In addition, we demonstrate the effect of rendering quality according to p value changes. We also visualize\n\u00d74 and \u00d78 scenarios of Table 2, when p is extremely small and large, and test camera poses are misaligned with training poses.\np value estimation on noisy point cloud We predict the p value when the point cloud is noisy. To simulate this, we add noise to all the points. The larger the noise level, the larger variations of point clouds. As shown in Fig. 14 (a), it can be seen that the prediction of p value is different from the optimal value for the large noise than that of small noise, which shows that large noise affects the scene geometry and causes p to be incorrectly predicted. Nevertheless, it shows that it is possible to predict p using RANSAC even if COLMAP\u2019s prediction is in error as seen in the rendering results in Fig. 14 (a).\np value estimation on sparse point cloud We perform an experiment for sparse point cloud setup which can be easily happened due to mis-matched correspondences like textureless regions. To simulate this scenario, we remove out 3D points from the SfM result by a certain ratio. As shown in Fig. 14 (b), the RANSAC algorithm was relatively insensitive to the sparsity of the points. This shows that it is possible to predict the overall structure even if some points are missing. When only 20% of the points are left, the p value is a little different from the optimal value, but the reasonable rendering is feasible.\nSpatially-variant rendering quality with respect to p value We test the changes of rendering quality depending on the p value. In Fig. 16, we can see that the small p value learns well in the background region, and the large p value more focuses on the foreground object. This shows that our method can adjust the quality of the desired parts according to user\u2019s selection.\nVisualization of \u00d74 and \u00d78 scenarios We visualize \u00d74 and \u00d78 scenarios in Table 2. For contract mapping, we combine angular ray parameterization because the disparity ray sampling does not work in these scenarios. As shown in Fig. 17, our p-norm based method shows more apparent results while the contract mapping struggles to handle the scenarios. This result shows that a smaller p value is beneficial the farther the camera is from the scene origin. This indicates that the optimal p value is different depending on where the camera is located and choosing the optimal p value can preserve the rendering quality.\nResults of extreme cases We add Fig. 18 for two extreme p values: when p = 0.5 and p = 10. For p less than 1, the manifold becomes non-convex, but it is learnable. Interestingly, despite the poor quality of the foreground, it is able to render the distant objects well. On the contrary, when p is very large, the foreground object is more visible than that of small p. However, floater artifacts are observed depending on the viewpoint.\nResults of different camera allignment We add Fig. 19 which shows the result when the camera is misaligned with training poses. Even with the misaligned camera poses, we can successfully synthesize novel views.\nF IMPLEMENTATION DETAILS\nIn this section, we explain the details of our implementation to verify the flexibility of the proposed mapping strategy. We additionally implement the contract parameterization Barron et al. (2022) for comparison. Following the original implementation, the scene origin is a mean of camera poses and r is parameterized by a normlized distance Barron et al. (2022). We use four baseline models on PyTorch framework: DVGO, TensoRF, iNGP, and NeRF. For a fair comparison, we strictly follow the training scheme of each method using the official codes for DVGO3, TensoRF4, and NeRF5. For the iNGP baseline, since no public codes are available, we customize a Pytorch re-implementation code6.\n3https://github.com/sunset1995/DirectVoxGO 4https://github.com/apchenstu/TensoRF 5https://github.com/Kai-46/nerfplusplus 6https://github.com/SuLvXiangXin/zipnerf-pytorch\nDVGO To implement the DVGO baseline, we adopt a voxel model from Sun et al. (2022); Cheng et al. (2022). The number of voxels is initially 503, and we gradually scale up the voxel grid at the [2, 000, 4, 000, 6, 000, 8, 000, 10, 000, 12, 000, 14, 000, 16, 000] training steps up to 3203 voxels. The feature dimension of the voxel grid is set to 12. For the viewing direction d, we embed it within a positional embedding. There are two hidden layers with 128 channels in the shallow MLP layer. We skip the low-density query points in the unknown space using the threshold 10\u22124. The point sampling is performed using the marching step strategy; input points on a ray are moved in small steps, which is set to half of the voxel sizes. Since the size of the marching step in real-world space and embedding space may be different, it samples more points on the ray and prunes the oversampled points. We optimize scene representations using the Adam optimizer with a batch size of 213 rays for 40k iterations. All voxel grids\u2019 base learning rates are 5\u00d7 10\u22124, while the MLP\u2019s learning rate is 10\u22123. Applying the exponential learning rate decay, the learning rates are scaled down by 0.1 after 20k iterations.\nTensoRF We use a TensoRF model with Vector-Matrix (VM) decomposition Chen et al. (2022a) Similar to DVGO, we begin with a low-resolution grid of 1283 and upsample the vectors and matrices at steps [2, 000, 3, 000, 4, 000, 5, 500, 7, 000] where the final grid resolution is 3003. We employ a tiny MLP with two FC layers (with 128 hidden units) and a ReLU activation to make the feature decoding function. With initial learning rates of 2\u00d7 10\u22122 for tensor factors and 10\u22123 for the MLP decoder, we employ the Adam optimizer and learning rate of 5\u00d7 10\u22124. Our model is optimized for 30k steps and 4096-pixel rays per batch.\niNGP Following Mu\u0308ller et al. (2022); Barron et al. (2023), we adopt a coarse-to-fine strategy to sample points by using 128 coarse samples and 64 fine samples. In our iNGP hierarchy of grids and hashes, we use 10 grid scales that are spaced by a power of 2 from 16 to 2048, and we use 4 channels per level with 219 hashmap size. We employ an MLP with two layers of 64 hidden units each; the hidden layers contain activation functions for rectified linear units (ReLUs) and a linear output layer.\nWe employ another MLP for the density of the output color whose input is the concatenation of the 64 output values of the density MLP and the view direction projected onto the spherical harmonics basis with a degree of 4. The models were trained for 25k iterations using a batch size of 216. We utilize the Adam optimizer and decay our learning rate logarithmically from 10\u22122 to 10\u22123 over training.\nNeRF The NeRF model is based on the original implementation Mildenhall et al. (2020); Zhang et al. (2020). We employ 8 MLP layers with 384 hidden units for both coarse and fine modules. The input points are embedded with a 12 degree of positional encoding, and the view direction is projected on the spherical harmonics basis with a degree of 4. We employ a batch size of 4, 096 rays in our studies, sampling each one at 128 coordinates in the coarse module and 256 extra locations in the fine module. We employ the Adam optimizer with a learning rate of 5\u00d7 10\u22124 and train the model for 250K iterations until convergence."
        },
        {
            "heading": "G QUANLITATIVE AND QUALITATIVE RESULTS",
            "text": "In this section, we provide quantitative evaluations per scene on the mip-NeRF 360, Tanks and Temples and Free dataset in Tables 3 to 11. Since our method aims to produce visually-pleasing results when the camera poses are far from the scene origin, we display the qualitative results on \u00d72 cases of each dataset in Fig. 20.\nx2"
        }
    ],
    "title": "UNBOUNDED NEURAL RADIANCE FIELDS",
    "year": 2024
}