{
    "abstractText": "We study infinite-horizon average-reward Markov decision processes (AMDPs) in the context of general function approximation. Specifically, we propose a novel algorithmic framework named Local-fitted Optimization with OPtimism (LOOP), which incorporates both model-based and value-based incarnations. In particular, LOOP features a novel construction of confidence sets and a low-switching policy updating scheme, which are tailored to the average-reward and function approximation setting. Moreover, for AMDPs, we propose a novel complexity measure \u2014 average-reward generalized eluder coefficient (AGEC) \u2014 which captures the challenge of exploration in AMDPs with general function approximation. Such a complexity measure encompasses almost all previously known tractable AMDP models, such as linear AMDPs and linear mixture AMDPs, and also includes newly identified cases such as kernel AMDPs and AMDPs with Bellman eluder dimensions. Using AGEC, we prove that LOOP achieves a sublinear \u00d5(poly(d, sp(V \u2217)) \u221a T\u03b2) regret, where d and \u03b2 correspond to AGEC and logcovering number of the hypothesis class respectively, sp(V \u2217) is the span of the optimal state bias function, T denotes the number of steps, and \u00d5(\u00b7) omits logarithmic factors. When specialized to concrete AMDP models, our regret bounds are comparable to those established by the existing algorithms designed specifically for these special cases. To the best of our knowledge, this paper presents the first comprehensive theoretical framework capable of handling nearly all AMDPs.",
    "authors": [],
    "id": "SP:10038ef3e2525d1ff0de883d190f17d8a0a74b25",
    "references": [
        {
            "authors": [
                "Yasin Abbasi-Yadkori",
                "D\u00e1vid P\u00e1l",
                "Csaba Szepesv\u00e1ri"
            ],
            "title": "Improved algorithms for linear stochastic bandits",
            "venue": "Advances in neural information processing systems,",
            "year": 2011
        },
        {
            "authors": [
                "Yasin Abbasi-Yadkori",
                "Peter Bartlett",
                "Kush Bhatia",
                "Nevena Lazic",
                "Csaba Szepesvari",
                "Gell\u00e9rt Weisz"
            ],
            "title": "Politex: Regret bounds for policy iteration using expert prediction",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Alekh Agarwal",
                "Daniel Hsu",
                "Satyen Kale",
                "John Langford",
                "Lihong Li",
                "Robert Schapire"
            ],
            "title": "Taming the monster: A fast and simple algorithm for contextual bandits",
            "venue": "In International Conference on Machine Learning,",
            "year": 2014
        },
        {
            "authors": [
                "Alekh Agarwal",
                "Yujia Jin",
                "Tong Zhang"
            ],
            "title": "Vo q l: Towards optimal regret in model-free rl with nonlinear function approximation",
            "venue": "arXiv preprint arXiv:2212.06069,",
            "year": 2022
        },
        {
            "authors": [
                "Peter Auer",
                "Thomas Jaksch",
                "Ronald Ortner"
            ],
            "title": "Near-optimal regret bounds for reinforcement learning",
            "venue": "Advances in neural information processing systems,",
            "year": 2008
        },
        {
            "authors": [
                "Alex Ayoub",
                "Zeyu Jia",
                "Csaba Szepesvari",
                "Mengdi Wang",
                "Lin Yang"
            ],
            "title": "Model-based reinforcement learning with value-targeted regression",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Yu Bai",
                "Tengyang Xie",
                "Nan Jiang",
                "Yu-Xiang Wang"
            ],
            "title": "Provably efficient q-learning with low switching cost",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Peter L Bartlett",
                "Ambuj Tewari"
            ],
            "title": "Regal: A regularization based algorithm for reinforcement learning in weakly communicating mdps",
            "venue": "arXiv preprint arXiv:1205.2661,",
            "year": 2012
        },
        {
            "authors": [
                "Qi Cai",
                "Zhuoran Yang",
                "Chi Jin",
                "Zhaoran Wang"
            ],
            "title": "Provably efficient exploration in policy optimization",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Xiaoyu Chen",
                "Jiachen Hu",
                "Chi Jin",
                "Lihong Li",
                "Liwei Wang"
            ],
            "title": "Understanding domain randomization for sim-to-real transfer",
            "venue": "International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Zixiang Chen",
                "Chris Junchi Li",
                "Angela Yuan",
                "Quanquan Gu",
                "Michael I Jordan"
            ],
            "title": "A general framework for sample-efficient function approximation in reinforcement learning",
            "venue": "arXiv preprint arXiv:2209.15634,",
            "year": 2022
        },
        {
            "authors": [
                "Varsha Dani",
                "Thomas P Hayes",
                "Sham M Kakade"
            ],
            "title": "Stochastic linear optimization under bandit feedback",
            "year": 2008
        },
        {
            "authors": [
                "Christoph Dann",
                "Mehryar Mohri",
                "Tong Zhang",
                "Julian Zimmert"
            ],
            "title": "A provably efficient model-free posterior sampling method for episodic reinforcement learning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Omar Darwiche Domingues",
                "Pierre M\u00e9nard",
                "Matteo Pirotta",
                "Emilie Kaufmann",
                "Michal Valko"
            ],
            "title": "A kernel-based approach to non-stationary reinforcement learning in metric spaces",
            "venue": "In International Conference on Artificial Intelligence and Statistics,",
            "year": 2021
        },
        {
            "authors": [
                "Simon Du",
                "Sham Kakade",
                "Jason Lee",
                "Shachar Lovett",
                "Gaurav Mahajan",
                "Wen Sun",
                "Ruosong Wang"
            ],
            "title": "Bilinear classes: A structural framework for provable generalization in rl",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Dylan J Foster",
                "Sham M Kakade",
                "Jian Qian",
                "Alexander Rakhlin"
            ],
            "title": "The statistical complexity of interactive decision making",
            "venue": "arXiv preprint arXiv:2112.13487,",
            "year": 2021
        },
        {
            "authors": [
                "Dylan J Foster",
                "Noah Golowich",
                "Yanjun Han"
            ],
            "title": "Tight guarantees for interactive decision making with the decision-estimation coefficient",
            "venue": "arXiv preprint arXiv:2301.08215,",
            "year": 2023
        },
        {
            "authors": [
                "Ronan Fruit",
                "Matteo Pirotta",
                "Alessandro Lazaric",
                "Ronald Ortner"
            ],
            "title": "Efficient bias-span-constrained exploration-exploitation in reinforcement learning",
            "venue": "In International Conference on Machine Learning,",
            "year": 2018
        },
        {
            "authors": [
                "Botao Hao",
                "Nevena Lazic",
                "Yasin Abbasi-Yadkori",
                "Pooria Joulani",
                "Csaba Szepesv\u00e1ri"
            ],
            "title": "Adaptive approximate policy iteration",
            "venue": "In International Conference on Artificial Intelligence and Statistics,",
            "year": 2021
        },
        {
            "authors": [
                "Jiafan He",
                "Heyang Zhao",
                "Dongruo Zhou",
                "Quanquan Gu"
            ],
            "title": "Nearly minimax optimal reinforcement learning for linear markov decision processes",
            "venue": "arXiv preprint arXiv:2212.06132,",
            "year": 2022
        },
        {
            "authors": [
                "On\u00e9simo Hern\u00e1ndez-Lerma"
            ],
            "title": "Adaptive Markov control processes, volume 79",
            "venue": "Springer Science & Business Media,",
            "year": 2012
        },
        {
            "authors": [
                "Jiachen Hu",
                "Han Zhong",
                "Chi Jin",
                "Liwei Wang"
            ],
            "title": "Provable sim-to-real transfer in continuous domain with partial observations",
            "venue": "arXiv preprint arXiv:2210.15598,",
            "year": 2022
        },
        {
            "authors": [
                "Jiayi Huang",
                "Han Zhong",
                "Liwei Wang",
                "Lin F Yang"
            ],
            "title": "Tackling heavy-tailed rewards in reinforcement learning with function approximation: Minimax optimal and instance-dependent regret bounds",
            "venue": "arXiv preprint arXiv:2306.06836,",
            "year": 2023
        },
        {
            "authors": [
                "Thomas Jaksch",
                "Ronald Ortner",
                "Peter Auer"
            ],
            "title": "Near-optimal regret bounds for reinforcement learning",
            "venue": "Journal of Machine Learning Research,",
            "year": 2010
        },
        {
            "authors": [
                "Nan Jiang",
                "Akshay Krishnamurthy",
                "Alekh Agarwal",
                "John Langford",
                "Robert E Schapire"
            ],
            "title": "Contextual decision processes with low bellman rank are pac-learnable",
            "venue": "In International Conference on Machine Learning,",
            "year": 2017
        },
        {
            "authors": [
                "Chi Jin",
                "Zhuoran Yang",
                "Zhaoran Wang",
                "Michael I Jordan"
            ],
            "title": "Provably efficient reinforcement learning with linear function approximation",
            "venue": "In Conference on Learning Theory,",
            "year": 2020
        },
        {
            "authors": [
                "Chi Jin",
                "Qinghua Liu",
                "Sobhan"
            ],
            "title": "Miryoosefi. Bellman eluder dimension: New rich classes of rl problems, and sample-efficient algorithms",
            "venue": "Advances in neural information processing systems,",
            "year": 2021
        },
        {
            "authors": [
                "Chi Jin",
                "Qinghua Liu",
                "Tiancheng Yu"
            ],
            "title": "The power of exploiter: Provable multi-agent rl in large state spaces",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Dingwen Kong",
                "Ruslan Salakhutdinov",
                "Ruosong Wang",
                "Lin F Yang"
            ],
            "title": "Online sub-sampling for reinforcement learning with general function approximation",
            "venue": "arXiv preprint arXiv:2106.07203,",
            "year": 2021
        },
        {
            "authors": [
                "Akshay Krishnamurthy",
                "Alekh Agarwal",
                "John Langford"
            ],
            "title": "Pac reinforcement learning with rich observations",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2016
        },
        {
            "authors": [
                "Nevena Lazic",
                "Dong Yin",
                "Yasin Abbasi-Yadkori",
                "Csaba Szepesvari"
            ],
            "title": "Improved regret bound and experience replay in regularized policy iteration",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Xiang Li",
                "Qiang Sun"
            ],
            "title": "Variance-aware robust reinforcement learning with linear function approximation with heavy-tailed rewards",
            "venue": "arXiv preprint arXiv:2303.05606,",
            "year": 2023
        },
        {
            "authors": [
                "Qinghua Liu",
                "Alan Chung",
                "Csaba Szepesv\u00e1ri",
                "Chi Jin"
            ],
            "title": "When is partially observable reinforcement learning not scary",
            "venue": "In Conference on Learning Theory,",
            "year": 2022
        },
        {
            "authors": [
                "Qinghua Liu",
                "Praneeth Netrapalli",
                "Csaba Szepesvari",
                "Chi Jin"
            ],
            "title": "Optimistic mle: A generic modelbased algorithm for partially observable sequential decision making",
            "venue": "In Proceedings of the 55th Annual ACM Symposium on Theory of Computing,",
            "year": 2023
        },
        {
            "authors": [
                "Zhihan Liu",
                "Miao Lu",
                "Wei Xiong",
                "Han Zhong",
                "Hao Hu",
                "Shenao Zhang",
                "Sirui Zheng",
                "Zhuoran Yang",
                "Zhaoran Wang"
            ],
            "title": "One objective to rule them all: A maximization objective fusing estimation and planning for exploration",
            "venue": "arXiv preprint arXiv:2305.18258,",
            "year": 2023
        },
        {
            "authors": [
                "Ronald Ortner"
            ],
            "title": "Regret bounds for reinforcement learning via markov chain concentration",
            "venue": "Journal of Artificial Intelligence Research,",
            "year": 2020
        },
        {
            "authors": [
                "Yi Ouyang",
                "Mukul Gagrani",
                "Ashutosh Nayyar",
                "Rahul Jain"
            ],
            "title": "Learning unknown markov decision processes: A thompson sampling approach",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Scott Proper",
                "Prasad Tadepalli"
            ],
            "title": "Scaling model-based average-reward reinforcement learning for product delivery",
            "venue": "In European Conference on Machine Learning,",
            "year": 2006
        },
        {
            "authors": [
                "Martin L Puterman"
            ],
            "title": "Markov decision processes: discrete stochastic dynamic programming",
            "year": 2014
        },
        {
            "authors": [
                "Daniel Russo",
                "Benjamin Van Roy"
            ],
            "title": "Eluder dimension and the sample complexity of optimistic exploration",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2013
        },
        {
            "authors": [
                "Niranjan Srinivas",
                "Andreas Krause",
                "Sham M Kakade",
                "Matthias Seeger"
            ],
            "title": "Gaussian process optimization in the bandit setting: No regret and experimental design",
            "venue": "arXiv preprint arXiv:0912.3995,",
            "year": 2009
        },
        {
            "authors": [
                "Wen Sun",
                "Nan Jiang",
                "Akshay Krishnamurthy",
                "Alekh Agarwal",
                "John Langford"
            ],
            "title": "Model-based rl in contextual decision processes: Pac bounds and exponential improvements over model-free approaches",
            "venue": "In Conference on learning theory,",
            "year": 2019
        },
        {
            "authors": [
                "Richard S Sutton",
                "Andrew G Barto"
            ],
            "title": "Reinforcement learning: An introduction",
            "venue": "MIT press,",
            "year": 2018
        },
        {
            "authors": [
                "Csaba Szepesv\u00e1ri"
            ],
            "title": "Algorithms for reinforcement learning",
            "venue": "Synthesis lectures on artificial intelligence and machine learning,",
            "year": 2010
        },
        {
            "authors": [
                "Jinghan Wang",
                "Mengdi Wang",
                "Lin F Yang"
            ],
            "title": "Near sample-optimal reduction-based policy learning for average reward mdp",
            "venue": "arXiv preprint arXiv:2212.00603,",
            "year": 2022
        },
        {
            "authors": [
                "Ruosong Wang",
                "Russ R Salakhutdinov",
                "Lin Yang"
            ],
            "title": "Reinforcement learning with general value function approximation: Provably efficient approach via bounded eluder dimension",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Yining Wang",
                "Ruosong Wang",
                "Simon S Du",
                "Akshay Krishnamurthy"
            ],
            "title": "Optimism in reinforcement learning with generalized linear function approximation",
            "year": 1912
        },
        {
            "authors": [
                "Chen-Yu Wei",
                "Mehdi Jafarnia Jahromi",
                "Haipeng Luo",
                "Hiteshi Sharma",
                "Rahul Jain"
            ],
            "title": "Modelfree reinforcement learning in infinite-horizon average-reward markov decision processes",
            "venue": "In International conference on machine learning,",
            "year": 2020
        },
        {
            "authors": [
                "Chen-Yu Wei",
                "Mehdi Jafarnia Jahromi",
                "Haipeng Luo",
                "Rahul Jain"
            ],
            "title": "Learning infinite-horizon average-reward mdps with linear function approximation",
            "venue": "In International Conference on Artificial Intelligence and Statistics,",
            "year": 2021
        },
        {
            "authors": [
                "Yue Wu",
                "Dongruo Zhou",
                "Quanquan Gu"
            ],
            "title": "Nearly minimax optimal regret for learning infinitehorizon average-reward mdps with linear function approximation",
            "venue": "In International Conference on Artificial Intelligence and Statistics,",
            "year": 2022
        },
        {
            "authors": [
                "Nuoya Xiong",
                "Zhuoran Yang",
                "Zhaoran Wang"
            ],
            "title": "A general framework for sequential decisionmaking under adaptivity constraints",
            "venue": "arXiv preprint arXiv:2306.14468,",
            "year": 2023
        },
        {
            "authors": [
                "Lin Yang",
                "Mengdi Wang"
            ],
            "title": "Sample-optimal parametric q-learning using linearly additive features",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Andrea Zanette",
                "Alessandro Lazaric",
                "Mykel Kochenderfer",
                "Emma Brunskill"
            ],
            "title": "Learning near optimal policies with low inherent bellman error",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Zihan Zhang",
                "Xiangyang Ji"
            ],
            "title": "Regret minimization for reinforcement learning by evaluating the optimal bias function",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2019
        },
        {
            "authors": [
                "Zihan Zhang",
                "Qiaomin Xie"
            ],
            "title": "Sharper model-free reinforcement learning for average-reward markov decision processes",
            "venue": "In The Thirty Sixth Annual Conference on Learning Theory,",
            "year": 2023
        },
        {
            "authors": [
                "Zihan Zhang",
                "Yuan Zhou",
                "Xiangyang Ji"
            ],
            "title": "Almost optimal model-free reinforcement learningvia reference-advantage decomposition",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Heyang Zhao",
                "Jiafan He",
                "Dongruo Zhou",
                "Tong Zhang",
                "Quanquan Gu"
            ],
            "title": "Variance-dependent regret bounds for linear bandits and reinforcement learning: Adaptivity and computational efficiency",
            "venue": "arXiv preprint arXiv:2302.10371,",
            "year": 2023
        },
        {
            "authors": [
                "Han Zhong",
                "Tong Zhang"
            ],
            "title": "A theoretical analysis of optimistic proximal policy optimization in linear markov decision processes",
            "venue": "arXiv preprint arXiv:2305.08841,",
            "year": 2023
        },
        {
            "authors": [
                "Han Zhong",
                "Wei Xiong",
                "Sirui Zheng",
                "Liwei Wang",
                "Zhaoran Wang",
                "Zhuoran Yang",
                "Tong Zhang"
            ],
            "title": "Gec: A unified framework for interactive decision making in mdp, pomdp, and beyond",
            "year": 1962
        },
        {
            "authors": [
                "Dongruo Zhou",
                "Quanquan Gu"
            ],
            "title": "Computationally efficient horizon-free reinforcement learning for linear mixture mdps",
            "venue": "arXiv preprint arXiv:2205.11507,",
            "year": 2022
        },
        {
            "authors": [
                "Dongruo Zhou",
                "Quanquan Gu",
                "Csaba Szepesvari"
            ],
            "title": "Nearly minimax optimal reinforcement learning for linear mixture markov decision processes",
            "venue": "In Conference on Learning Theory,",
            "year": 2021
        },
        {
            "authors": [
                "Dongruo Zhou",
                "Jiafan He",
                "Quanquan Gu"
            ],
            "title": "Provably efficient reinforcement learning for discounted mdps with feature mapping",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Wei"
            ],
            "title": "2021) proposed an optimistic Q-learning algorithm FOPO for the linear function approximation, and achieve a near-optimal \u00d5",
            "year": 2021
        },
        {
            "authors": [],
            "title": "2022) delved into the linear function approximation under the framework of linear mixture model, which is mutually uncoverable concerning linear MDPs (Wei et al., 2021), and proposed UCRL2-VTR based on the value-targeted regression (Ayoub",
            "year": 2021
        },
        {
            "authors": [
                "Chen"
            ],
            "title": "2022a) expanded the scope of research by addressing the general function approximation problem in average-reward RL and proposed the SIM-TO-REAL algorithm, which can be regarded as an extension to UCRL2-VTR. In comparison to the works mentioned, our algorithm, LOOP, not only addresses all the problems examined in those studies but also extends its applicability to newly identified models. See Table 1 for a summary",
            "year": 2022
        },
        {
            "authors": [
                "Jin"
            ],
            "title": "Function Approximation in Finite-horizon MDPs. In the pursuit of developing sample-efficient algorithms capable of handling large state spaces, extensive research efforts have converged on the linear function approximation problems within the finite-horizon setting",
            "year": 2020
        },
        {
            "authors": [
                "Huang"
            ],
            "title": "2020) studied RL with general function approximation and adopted the eluder dimension (Russo",
            "venue": "Van Roy,",
            "year": 2013
        },
        {
            "authors": [
                "Liu"
            ],
            "title": "discrepancy loss function to handle the broader admissible Bellman characterization (ABC) class",
            "year": 2021
        },
        {
            "authors": [
                "Zhang"
            ],
            "title": "Addressing low-switching cost problems in bandit and reinforcement learning has seen notable progress. Abbasi-Yadkori et al. (2011) first proposed an algorithm for linear bandits with O(log T ) switching cost. Subsequent research extended this to tabular MDPs, including works of Bai et al",
            "venue": "A significant stride was made by Kong et al",
            "year": 2020
        },
        {
            "authors": [
                "Van Roy",
                "2013). Recently",
                "Xiong"
            ],
            "title": "general function approximation with complexity measured by eluder dimension (Russo",
            "venue": "Chen et al. (2022a); Hu et al",
            "year": 2022
        },
        {
            "authors": [
                "\u221a T ) (Wei"
            ],
            "title": "2021). We remark that such an adaptive lazy updating design and corresponding analysis are pivotal in achieving the optimal \u00d5",
            "year": 2021
        },
        {
            "authors": [],
            "title": "\u00d5(T ) regret in (OLSVI.FH; Wei et al., 2021). Moreover, our approach is an extension to the existing lazy update approaches for average-reward setting (Wei et al., 2021; Wu et al., 2022) that leverages the postulated linear structure and is not applicable to problems with general function approximation",
            "year": 2022
        },
        {
            "authors": [
                "T min{\u03baG"
            ],
            "title": "2T\u03b52. The main difference between the MLE-based variant (see Definition 9) and the original AGEC (see Definition 3) is that the transferability coefficient is defined over the l1-norm of out-sample training error rather than the l2-norm, and similar condition is considered in Liu et al",
            "venue": "\u03baG",
            "year": 2023
        },
        {
            "authors": [
                "Wei et al",
                "Wu"
            ],
            "title": "2022) and model-based general function approximation (Chen et al., 2022a). Here, we introduce a variety of function classes with low AGEC. Beyond the examples considered in existing work, these newly proposed function classes are mostly natural extensions from their counterpart the finite-horizon episode setting (Jin et al., 2020",
            "year": 2021
        },
        {
            "authors": [],
            "title": "Next, we provide the AMDPs with linear Bellman completion, modified from Zanette et al. (2020), which is a more general setting than linear AMDPs. Definition 11 (Linear Bellman completion). There exists a known feature mapping\u03c6 : S\u00d7A 7\u2192 R such that for all",
            "venue": "S \u00d7A and J \u2208 J(H),",
            "year": 2020
        },
        {
            "authors": [
                "Wang"
            ],
            "title": "2019) for the episodic setting, we define AMDPs with generalized linear Bellman completion as follows. Definition 12 (Generalized linear Bellman completion). There exists a known feature mapping",
            "venue": "HLin",
            "year": 2019
        },
        {
            "authors": [
                "Jin"
            ],
            "title": "Here, the concept of \u03b5-effective dimension is inspired by the measurement of maximum information gain (Srinivas et al., 2009) and is later introduced as a complexity measure of Hilbert space in Du et al",
            "venue": "Zhong et al",
            "year": 2022
        },
        {
            "authors": [],
            "title": "\u00d7A} as the collection of feature mappings. The proposition above demonstrates that the kernel FA with a low \u03b5-effective dimension over the Hilbert space has low AGEC. As a special case of kernel FA, if we choose K = R, then we can prove that the RHS in the proposition above is upper bounded by \u00d5(d) (Jin et al., 2022)",
            "venue": "X = {\u03c6(s,",
            "year": 2022
        },
        {
            "authors": [],
            "title": "regret, which is nearly minimax optimal, in both linear AMDP and linear mixture AMDP. We remark that existing algorithms were tailored for specific problems individually (Wei et al., 2021; Wu et al., 2022), LOOP offers a unified approach that covers them with comparable performance with an additional cost of generality for trade-off",
            "year": 2022
        },
        {
            "authors": [
                "Jin"
            ],
            "title": "2021) for detailed proof. Lemma 13 (Pigeon-hole principle). Given function class \u03a6 defined on X with |\u03c6(x)| \u2264 C for all \u03c6 \u2208 \u03a6 and x \u2208 X , and a family of probability measure over X",
            "year": 2024
        },
        {
            "authors": [
                "Wei"
            ],
            "title": "2021) for detailed proof. In Theorem 3, the proved regret contains the logarithmic term of the 1/T -covering number of the function",
            "venue": "S \u00d7A. Proof. See Lemma",
            "year": 2021
        },
        {
            "authors": [
                "Wei"
            ],
            "title": "2021) for detailed proof. H.1 PROOF OF TECHNICAL LEMMAS In this subsection, we present the proofs of technical auxiliary lemmas with modifications",
            "venue": "PROOF OF LEMMA",
            "year": 2021
        },
        {
            "authors": [
                "Zhong"
            ],
            "title": "2022) with adjustment, and we preserve it for comprehension",
            "venue": "Proof of Lemma 14. Denote dDE = dimDE(\u03a6,\u0393,",
            "year": 2022
        },
        {
            "authors": [
                "Liu"
            ],
            "title": "2022) with slight modifications. 2 Lemma 23 (Akin to Lemma 11). Let N (T ) be the switching cost with time horizon T , given fixed covering coefficient \u03c1 > 0 and pre-determined optimistic parameter \u03b2 = c",
            "venue": "\u03b4. Proof Sketch of Lemma 22. See Proposition",
            "year": 2022
        },
        {
            "authors": [
                "Auer"
            ],
            "title": "EXTENDED VALUE ITERATION (EVI) FOR MODEL-BASED HYPOTHESES In model-based problems, the discrepancy function sometimes relies on the optimal state bias function Vf and optimal average-reward Jf (see linear mixture model in Section D)",
            "year": 2008
        }
    ],
    "sections": [
        {
            "text": "\u221a T\u03b2) regret, where d and \u03b2 correspond to AGEC and log-\ncovering number of the hypothesis class respectively, sp(V \u2217) is the span of the optimal state bias function, T denotes the number of steps, and O\u0303(\u00b7) omits logarithmic factors. When specialized to concrete AMDP models, our regret bounds are comparable to those established by the existing algorithms designed specifically for these special cases. To the best of our knowledge, this paper presents the first comprehensive theoretical framework capable of handling nearly all AMDPs."
        },
        {
            "heading": "1 INTRODUCTION",
            "text": "Reinforcement learning (RL) (Sutton & Barto, 2018) is a powerful tool for addressing intricate sequential decision-making problems. In this context, Markov decision processes (MDPs) (Puterman, 2014; Sutton & Barto, 2018) frequently serve as a fundamental model for modeling such decisionmaking scenarios. Motivated by different feedback structures in real applications, MDPs consist of three subclasses \u2014 finite-horizon MDPs, infinite-horizon discounted MDPs, and infinite-horizon average-reward MDPs. Each of these MDP variants is of paramount significance and operates in a parallel fashion, with none being amenable to complete reduction into another. Of these three MDP subclasses, finite-horizon MDPs have received significant research efforts in understanding their exploration challenge, especially in the presence of large state spaces which necessitates function approximation tools. Existing works on finite-horizon MDPs have proposed numerous structural conditions on the MDP model that empower sample-efficient learning. These structural conditions include but are not limited to linear function approximation (Jin et al., 2020), Bellman rank (Jiang et al., 2017), eluder dimension (Wang et al., 2020), Bellman eluder dimension (Jin et al., 2021), bilinear class (Du et al., 2021), decision estimation coefficient (Foster et al., 2021), and generalized eluder coefficient (Zhong et al., 2022). Moreover, these works have designed various model-based and value-based algorithms to address finite-horizon MDPs governed by these structural conditions.\nIn contrast to the rich literature devoted to finite-horizon MDPs, the study of sample-efficient exploration in infinite-horizon MDPs has hitherto been relatively limited. Importantly, it remains elusive how to design in a principled fashion a sample-efficient RL algorithm in the online setting with general function approximation. To this end, we focus on infinite-horizon average-reward MDPs (AMDPs), which offer a suitable framework for addressing real-world decision-making scenarios\nthat prioritize long-term returns, such as product delivery (Proper & Tadepalli, 2006). Our work endeavors to provide a unified theoretical foundation for understanding infinite-horizon averagereward MDPs from the perspective of general function approximation, akin to the comprehensive investigations conducted in the domain of finite-horizon MDPs. To pursue this overarching objective, we have delineated two subsidiary goals that form the crux of our research endeavor.\n- Development of a Novel Structural Condition/Complexity Measure. Existing works are restricted to tabular AMDPs (Bartlett & Tewari, 2012; Jaksch et al., 2010) and AMDPs with linear function approximation (Wu et al., 2022; Wei et al., 2021), with Chen et al. (2022a) as the only exception (to our best knowledge). While Chen et al. (2022b) does extend the eluder dimension for finite-horizon MDPs (Ayoub et al., 2020) to the infinite-horizon average-reward context, their complexity measure seems to be only slightly more general than the linear mixture AMDPs (Wu et al., 2022) and falls short in capturing other fundamental models such as linear AMDPs (Wei et al., 2021). Hence, our first subgoal is proposing a new structural condition. This condition is envisioned to be sufficiently versatile to encompass all known tractable AMDPs, while also potentially introducing innovative and tractable models into the framework.\n- Algorithmic Framework for Addressing Identified Structural Condition. The second subgoal is anchored in the development of sample-efficient algorithms for AMDPs characterized by the structural condition proposed in our work. Our aspiration is to devise an algorithmic framework that can be flexibly implemented in both model-based and value-based paradigms, depending on the nature of the problem at hand. This adaptability guarantees that our algorithms possess the ability to effectively address a wide range of AMDPs.\nOur work attains these two pivotal subgoals through the introduction of (i) a novel complexity measure \u2014 Average-reward Generalized Eluder Coefficient (AGEC), and (ii) a corresponding algorithmic framework dubbed as Local-fitted Optimization with OPtimism (LOOP). Our primary contributions and novelties are summarized below:\n- AGEC Complexity Measure. Our complexity measure AGEC extends the generalized eluder coefficient (GEC) in Zhong et al. (2022) to the infinite-horizon average-reward setting. However, it incorporates significant modifications. AGEC not only establishes a connection between the Bellman error and the training error, akin to GEC but also imposes certain constraints on transferability (see Definition 3 for details). This modification proves instrumental in attaining sample efficiency in the realm of AMDPs (see Section 5 for detailed discussion). We demonstrate that AGEC not only encompasses all previously recognized tractable AMDPs, including tabular AMDPs (Bartlett & Tewari, 2012; Jaksch et al., 2010), linear AMDPs (Wei et al., 2021), linear mixture MDPs (Wu et al., 2022), AMDPs with low eluder dimension (Chen et al., 2022a), but also captures some new identified models like linear Q\u2217/V \u2217 AMDPs (see Definition 13), kernel AMDPs (see Proposition 7), and AMDPs with low Bellman eluder dimension (see Definition 8).\n- LOOP Algorithmic Framework. Our algorithm LOOP is based on the optimism principle and features a novel construction of confidence sets along with a low-switching updating scheme. Remarkably, LOOP offers the flexibility to be implemented either in the model-based or valuebased paradigm, depending on the problem type.\n- Unified Theoretical Results. From the theoretical side, we prove that LOOP enjoys the regret of O\u0303(poly(d, sp(V \u2217)) \u221a T\u03b2), where d and \u03b2 correspond to the AGEC and the log-covering number\nof the hypothesis class respectively, sp(V \u2217) denotes the span of the optimal state bias function, T is the number of steps, and O\u0303 hides logarithmic factors. This result shows that LOOP is capable of solving all AMDPs with low AGEC.\nIn summary, we provide a unified theoretical understanding of infinite-horizon AMDPs with general function approximation. Further elaboration on our contributions and technical novelties are provided in Appendix A. Due to space limits, we only provide a comparison between our results and mostly related works on AMDPs in Table 1. More related works are deferred to Appendix A."
        },
        {
            "heading": "2 PRELIMINARIES",
            "text": "Notations. For any integer n \u2208 N+, we take the convention to use [n] = {1, . . . , n}. Consider two non-negative sequences {an}n\u22650 and {bn}n\u22650, if lim sup an/bn < \u221e, then we write it as\nan = O(bn). Else if lim sup an/bn = 0, then we write it as an = o(bn). And we use O\u0303 to omit the logarithmic terms. Denote \u2206(X ) be the probability simplex over the set X . Denote by supx |v(x)| the supremum norm of a given function. x\u2227y stands for min{x, y} and x\u2228y stands for max{x, y}. Given any continuum S, let |S| be the cardinality. Given two distributions P,Q \u2208 \u2206(X ), the TV distance of the two distributions is defined as TV(P,Q) = 12Ex\u223cP [|dQ(x)/dP (x)\u2212 1|]. An infinite-horizon average-reward Markov Dependent Process (AMDPs) is characterized byM = (S,A, r,P), where S is a Borel state space with a possibly infinite number of elements, A is a finite set of actions, r : S \u00d7 A 7\u2192 [\u22121, 1] is an unknown reward function1 and P(\u00b7|s, a) is the unknown transition kernel. The learning protocol for infinite-horizon average-reward RL is as follows: the agent interacts withM over a fixed number of T steps, starting from a pre-determined initial state s1 \u2208 S . At each step t \u2208 [T ], the agent observe a state st \u2208 S and takes an action at \u2208 A, receives a reward r(st, at) and transits to the next state st+1 drawn from P(\u00b7|st, at). Denote \u2206(A) be the probability simplex over the action space A. Specifically, the stationary policy \u03c0 is a mapping \u03c0 : S 7\u2192 \u2206(A) with \u03c0(a|s) specifying the probability of taking action a at state s. Given a stationary policy \u03c0, the long-term average reward starting is defined as\nJ\u03c0(s) := lim inf T 7\u2192\u221e\n1 T E [ T\u2211 t=1 r(st, at)|s1 = s ] , \u2200s \u2208 [S],\nwhere the expectation is taken with respect to the policy \u03c0 and transition P. In the infinite-horizon average-reward RL, existing works mostly rely on additional assumptions to achieve sample efficiency. The necessity arises from the absence of a natural counterpart to the celebrated Bellman optimality equation in the average-reward RL that is self-evident and crucial within episodic and discounted settings (Puterman, 2014). To this end, we consider a broad subclass where a modified version of the Bellman optimality equation holds (Hern\u00e1ndez-Lerma, 2012). Assumption 1 (Bellman optimality equation). There exists bounded measurable function Q\u2217 : S \u00d7 A 7\u2192 R, V \u2217 : S 7\u2192 R and unique constant J\u2217 \u2208 [\u22121, 1] such that for all (s, a) \u2208 S \u00d7A, it holds\nJ\u2217 +Q\u2217(s, a) = r(s, a) + Es\u2032\u223cP(\u00b7|s,a)[V \u2217(s\u2032)], V \u2217(s) = max a\u2208A Q\u2217(s, a). (2.1)\nThe Bellman optimality equation, adapted for average-reward RL, posits that for any initial states s1 \u2208 S , the optimal reward is independent such that J\u2217(s1) = J\u2217 under a deterministic optimal policy following \u03c0\u2217(\u00b7) = argmaxa\u2208AQ\u2217(\u00b7, a). The justification is presented in Wei et al. (2021). Note that functions V \u2217(s) and Q\u2217(s, a) reveal the relative advantage of starting from state s and state-action pair (s, a) under the optimal policy, and are respectively called the optimal state and\n1Throughout this paper, we consider the deterministic reward for notational simplicity and all results are readily generalized to the stochastic setting. Also, we assume reward lies in [\u22121, 1] without loss of generality.\nstate-action bias function (Wei et al., 2021). Denote sp(V ) = sups,s\u2032\u2208S |V (s) \u2212 V (s\u2032)| as the span of any bounded measurable function. Note that for any solution pair (V \u2217, Q\u2217) satisfying the Bellman optimality equation in (2.1), the shifted pair (V \u2217 \u2212 c,Q\u2217 \u2212 c) for any constant c is still a solution. Thus, without loss of generality, we can focus on the unique centralized solution such that \u2225V \u2217\u2225\u221e \u2264 12 sp(V\n\u2217). Following the tradition in the average-reward RL (Wei et al., 2020; 2021; Wang et al., 2022; Zhang & Xie, 2023), the span sp(V \u2217) is assumed to be known.\nAs aforementioned in the paper, distinct assumptions have been employed in average-reward RL research to ensure the explorability of the problem, which includes ergodic AMDPs (Wei et al., 2020; Hao et al., 2021; Zhang & Xie, 2023), communicating AMDPs (Chen et al., 2022a; Wang et al., 2022; Wu et al., 2022) and the Bellman optimality equation (Wei et al., 2021). Among these widely adopted assumptions, we remark that the Bellman optimality equation is the least stringent one. Note that the ergodic MDP suggests the existence of bias functions for each \u03c0 \u2208 \u03a0, while the latter two only require the existence of bias functions for the optimal policy. As for weak communicating assumption, a weaker form of communicating MDP (Wang et al., 2022), it directly implies the existence of the Bellman optimality equation and thus is stronger (Hern\u00e1ndez-Lerma, 2012). Given the Bellman optimality assumption in (2.1), we introduce the average-reward Bellman operator below:\n(TJF )(s, a) := r(s, a) + Es\u2032\u223cP(\u00b7|s,a) [ max a\u2032\u2208A F (s\u2032, a\u2032) ] \u2212 J, \u2200(s, a) \u2208 S \u00d7A, (2.2)\nfor any bounded function F : S \u00d7 A 7\u2192 R and constant J \u2208 [\u22121, 1]. Then, the Bellman optimality equation in (2.1) can be written as TJ\u2217Q\u2217 = Q\u2217. Moreover, we define the Bellman error:\nE(F, J)(s, a) := F (s, a)\u2212 (TJF )(s, a), \u2200(s, a) \u2208 S \u00d7A. (2.3)\nLearning Objective Under the framework of online learning for AMDPs, the agent aims to learn the optimal policy by interacting with the environment over potentially infinite steps. The (empirical) regret measures the cumulative difference between the optimal average-reward and the reward achieved after interacting for T steps, formally defined as Reg(T ) = \u2211T t=1 (J \u2217 \u2212 r(st, at))."
        },
        {
            "heading": "3 GENERAL FUNCTION APPROXIMATION",
            "text": "To capture both model-free and model-based problems with function approximation, we consider a general hypotheses classHwhich contains a class of functions. We consider two kinds of hypothesis classes, targeting at value-based problems and model-based problems respectively. Definition 1 (Value-based hypothesis). We sayH is a value-based hypotheses class if all hypothesis f \u2208 H is defined over state-action bias function Q and average-reward J such that f = (Qf , Jf ) \u2208 H. Let Vf (\u00b7) = maxa\u2208AQf (\u00b7, a) and \u03c0f (\u00b7) = argmaxa\u2208AQf (\u00b7, a) be the greedy bias function and policy induced from hypothesis f \u2208 H. Denote f\u2217 be the optimal hypothesis under true modelM. Definition 2 (Model-based hypothesis). We sayH is a model-based hypotheses class if all hypothesis f \u2208 H is defined over the transition kernel P and reward function r such that f = (Pf , rf ) \u2208 H. Let Qf , Vf , Jf , and \u03c0f respectively be the optimal bias functions, average-reward and policy induced from hypothesis f \u2208 H, which satisfies the Bellman optimality equation such that\nQf (s, a) + Jf = ( rf + PfVf ) (s, a), Pf \u2032Vf (s, a) := Es\u2032\u223cPf\u2032 (\u00b7|s,a)[Vf (s \u2032)],\nfor all s \u2208 S and a \u2208 A. Denote f\u2217 as the hypothesis concerning the true modelM.\nThe definition of hypotheses class H over the value-based (see Definition 1) and the model-based (see Definition 2) problems in AMDP is different from the episodic setting (Du et al., 2021; Zhong et al., 2022). The most significant difference is that the Bellman equation has a different form. As a result, in the value-based scenario, instead of using a single state-action value function Qf in episodic setting, the paired hypothesis (Qf , Jf ) is introduced to fully capture the average-reward structure. Besides, we retain the definition of hypothesis over model-based problems, augmenting it with an additional average-reward term Jf induced from (Pf , rf ). Since we do not impose any specific structural form to the hypothesis class, we stay in the realm of general function approximation.\nAs function approximation is challenging without further assumptions (Krishnamurthy et al., 2016), we introduce the realizability assumption, which is widely adopted in literatures (Jin et al., 2021).\nAssumption 2 (Realizablity). We assume that f\u2217 \u2208 H.\nMoreover, we establish the fundamental distribution families over the state-action pair upon which the metric is built. Considering the learning goal defined over the empirical regret, throughout the paper we focus on the point-wise distribution familyD\u2206 = {\u03b4s,a(\u00b7)|(s, a) \u2208 S\u00d7A}, which includes collections of Dirac probability measure over S \u00d7A. Discussions are deferred to Appendix B."
        },
        {
            "heading": "3.1 AVERAGE-REWARD GENERALIZED ELUDER COEFFICIENTS",
            "text": "In this subsection, we are going to introduce a novel metric \u2014 average-reward generalized eluder coefficients (AGEC), to capture the complexity of hypothesis class H for AMDP. Extended from the generalized Eluder coefficients (GEC; Zhong et al., 2022) for finite-horizon MDPs, AGEC is a variant to fit the infinite-horizon learning with average reward, and imposes an additional structural constraint \u2014 transferability. Definition 3 (AGEC). Given hypothesis classH, discrepancy function set {lf}f\u2208H and constant \u03f5 > 0, the average-reward generalized eluder coefficients AGEC(H, {lf},\u03f5) is defined as the smallest coefficients \u03baG and dG, such that following two conditions hold with absolute constants C1, C2 > 0:\n(i) (Bellman dominance) There exists constant dG > 0 such that T\u2211 t=1\nE(ft)(st, at)\ufe38 \ufe37\ufe37 \ufe38 Bellman error\n\u2264 [ dG\u00b7 T\u2211 t=1 t\u22121\u2211 i=1\n\u2225E\u03b6i [lfi(ft, ft, \u03b6i)]\u222522\ufe38 \ufe37\ufe37 \ufe38 In-sample training error\n]1/2 +C1 \u00b7 sp(V \u2217)min{dG, T}+ T\u03f5\ufe38 \ufe37\ufe37 \ufe38\nBurn-in cost\n.\n(ii) (Transferability) There exists constant \u03baG > 0 such that for hypotheses f1, . . . , fT \u2208 H, if\u2211t\u22121 i=1 \u2225E\u03b6i [lfi(ft, ft, \u03b6i)]\u222522 \u2264 \u03b2 holds for all t \u2208 [T ], then we have\nT\u2211 t=1\n\u2225E\u03b6t [lft(ft, ft, \u03b6t)]\u2225 2 2\ufe38 \ufe37\ufe37 \ufe38 Out-sample training error \u2264 \u03baG \u00b7 \u03b2 log T + C2 \u00b7 sp(V \u2217)2 min{\u03baG, T}+ 2T\u03f52\ufe38 \ufe37\ufe37 \ufe38 Burn-in cost .\nIn the definition above, \u03b6i is a subset of trajectory with varying meaning concerning the specific choice of discrepancy function, and the expectation is taken over it; C1, C2 are absolute constants related to span sp(V \u2217). To simplify the notation, we denote E(ft)(s, a) := E(Qft , Jft)(s, a) for all (s, a) \u2208 S \u00d7 A. Besides, the Burn-in cost is taken at the worst case and it varies across different settings but usually non-dominating. The intuition behind the metric is that, on average, if hypotheses have small in-sample training error on the well-explored dataset, then the prediction error on a different trajectory is expected to maintain a consistently low level (Zhong et al., 2022). In specific, the dominance coefficient dG encapsulates the challenge inherent in assessing the performance of prediction, specifically the Bellman error, given the consistently controlled in-sample training error within the designated function class H. Moreover, due to the unique challenge of infinite-horizon average-reward setting, we introduce the transferability coefficient \u03baG to quantify the transferability from the in-sample training error to the out-of-sample ones. Despite this additional structural condition, we can verify that nearly all tractable AMDPs admit a low AGEC value (see Section 3.2). Moreover, in Section 5, we will demonstrate the importance of such additional structural conditions for achieving sample efficiency in AMDPs from the theoretical perspective.\nMoreover, to facilitate further theoretical analysis, we make further assumptions on the discrepancy function and hypothesis class as Chen et al. (2022b); Zhong et al. (2022). Assumption 3 (Boundedness). Given any f \u2208 H, it holds that \u2225lf\u2225\u221e \u2264 C \u00b7 sp(V \u2217) with C > 0.\nThe boundedness assumption is reasonable and uniformly satisfied, as in most cases, it takes the Bellman discrepancy, defined as: for all \u03b6t = {st, at, rt, st+1} \u2208 S \u00d7A\u00d7 R\u00d7 S , we have\nlf \u2032(f, g, \u03b6t) = Qg(st, at)\u2212 r(st, at)\u2212 Vf (st+1) + Jg, (3.1) or other natural derivatives, so that the discrepancy is generally upper bounded by O ( sp(V \u2217) ) .\nAssumption 4 (Generalized completeness). Let G be an auxiliary function class and there exists a functional operator P : H 7\u2192 G, we say that H satisfies generalized completeness in G concerning discrepancy function lf \u2032 if for any (f, g) \u2208 H \u00d7 (H \u222a G), it holds that\nlf \u2032(f, g, \u03b6)\u2212 lf \u2032(f,P(f), \u03b6) = E\u03b6 [ lf \u2032(f, g, \u03b6) ] , (3.2)\nwhere the expectation is taken over trajectory \u03b6. Besides, the operator satisfies that P(f\u2217) = f\u2217.\nThe completeness assumption is an extension of Bellman completeness under the value-based hypothesis (Jin et al., 2021), incorporating the notion of the decomposition loss function (DLF) property proposed in Chen et al. (2022b). Our assumption diverges from the one posited in Zhong et al. (2022), where an auxiliary function class P(H) \u2286 G is introduced to enrich choices, accompanied with modifications tailored to accommodate the nuances of the average-reward setting. Example 1 (Bellman completeness \u2286 Generalized completeness). Let the discrepancy function be the Bellman discrepancy in (3.1) with \u03b6t = {st, at, rt, st+1}, and takes the (hypothesis-scheme) Bellman operator, defined as T (f) = {TJf (Qf ), Jf} for all f \u2208 H, modified from (2.2). Then,\nlf \u2032(f, g, \u03b6t)\u2212 lf \u2032 (f, T (f), \u03b6t) = Qg(st, at)\u2212 TJfQf (st, at) = Qg(st, at)\u2212 r(st, at) + E\u03b6t [ Vf (st+1) ] + Jf = E\u03b6t [ lf \u2032(f, g, \u03b6t) ] ,\nwhere the expectation is taken over the transition state st+1 from P(\u00b7|st, at).\nThe preceding example illustrates that the Bellman discrepancy, a frequently employed discrepancy function across problems, satisfies both assumptions. More examples and choices of the discrepancy function for MLE-based algorithms are respectively provided in Appendix C and D."
        },
        {
            "heading": "3.2 RELATION WITH TRACTABLE COMPLEXITY METRIC",
            "text": "To bridge the gap between concrete function approximation instances and the relatively abstract measure AGEC, this section introduces two intermediate metrics: the Eluder dimension (Russo & Van Roy, 2013) and the Average-reward Bellman Eluder (ABE) dimension. In particular, Chen et al. (2022a) employs the Eluder dimension to gauge the complexity of model-based hypothesis classes for infinite-horizon learning. To provide an intuitive complexity of value-based hypothesis classes, we additionally propose the ABE dimension, which is a generalization of the standard BE dimension (Jin et al., 2021). These two metrics provide valuable insights into the nature of AGEC.\nEluder Dimension We start with point-wise \u03f5-independence notation (Russo & Van Roy, 2013). Definition 4 (Point-wise \u03f5-independence). Let H be a function class defined on X and consider sequence {z, x1, . . . , xn} \u2208 X . We say z is \u03f5-independent of {x1, . . . , xn} with respect to H if there exists f, f \u2032 \u2208 H such that \u221a\u2211n i=1(f(xi)\u2212 f \u2032(xi))2 \u2264 \u03f5, but |f(z)\u2212 f \u2032(z)| \u2265 \u03f5.\nBased on \u03f5-independence, the Eluder dimension can be efficiently defined as below. Definition 5 (Eluder dimension). Let H be a function class defined on X . The Eluder dimension dimE(H, \u03f5) is the length of the longest sequence {x1, . . . , xn} \u2282 X such that there exists \u03f5\u2032 \u2265 \u03f5 where xi is \u03f5\u2032-independent of {x1, . . . , xi\u22121} for all i \u2208 [n].\nThe following lemma shows that a model-based hypothesis class with a low Eluder dimension has low AGEC. Motivated by Ayoub et al. (2020); Chen et al. (2022a), we consider the Eluder dimension over function class derived from the model-based hypotheses classH, defined as\nXH := { Xf,f \u2032(s, a) = ( rf + Pf \u2032Vf ) (s, a) : f, f \u2032 \u2208 H } .\nNote that Chen et al. (2022a) considered function class XH,V := {PfV (s, a) : f \u2208 P(H), V \u2208 V}, where P(H) denotes the hypotheses class over the transition kernel and V denotes the hypotheses class over the optimal bias function. We remark that definitions over function class based on (Pf , V ) with (f, V ) \u2208 H \u00d7 V and (Pf , rf ) with f \u2208 H is almost equivalent and in this paper we focus on XH under the latter framework, aligning with the model-based hypothesis (see Definition 2). Lemma 1 (Low Eluder dim \u2286 Low AGEC). Consider the discrepancy function, defined as\nlf \u2032(f, g, \u03b6t) = ( rg + PgVf \u2032 ) (st, at)\u2212 r(st, at) + Vf \u2032(st+1), (3.3)\nand the expectation is taken over the transition state st+1 from P(\u00b7|st, at). Let dE = dimE(XH, \u03f5) be the \u03f5-Eluder dimension defined over XH, then we have dG \u2264 2dE \u00b7 log T and \u03baG \u2264 dE.\nAverage-Reward Bellman Eluder (ABE) Dimension Before delving into details of the averagereward BE (ABE) dimension, we start with two useful notations, distributional \u03f5-independence and distributional Eluder (DE) dimension proposed by Jin et al. (2021), which is a generalization of point-wise \u03f5-independence and Eluder dimension defined above (see Definitions 4 and 5). Definition 6 (Distributional \u03f5-independence). LetH be a function class defined on X and sequence {\u03c5, \u00b51, . . . , \u00b5n} be the probability measures over X . We say \u03c5 is \u03f5-independent of {\u00b51, . . . , \u00b5n} with respect toH if there exists f \u2208 H such that \u221a\u2211n i=1(E\u00b5i [f ])2 \u2264 \u03f5, but |E\u03c5[f ]| \u2265 \u03f5. Definition 7 (Distributional Eluder dimension). Let H be a function class defined on X and \u0393 be a family of probability measures over X . The distributional Eluder dimension dimDE(H,\u0393, \u03f5) is the length of the longest sequence {\u03c11, . . . , \u03c1n} \u2282 \u0393 such that there exists \u03f5\u2032 \u2265 \u03f5 where \u03c1i is \u03f5\u2032-independent of the remaining distribution sequence {\u03c11, . . . , \u03c1i\u22121} for all i \u2208 [n].\nNow we are ready to introduce the average-reward Bellman Eluder (ABE) dimension. It is defined as the distributional Eluder (DE) dimension of average-reward Bellman error in (2.3) overH. Definition 8 (ABE dimension). Denote EH = {E(f)(s, a) : f \u2208 H} be the collection of averagereward Bellman errors defined over S \u00d7 A. For any constant \u03f5 > 0, the \u03f5-ABE dimension of given hypotheses classH is defined as dimABE(H, \u03f5) := dimDE ( EH,D\u2206, \u03f5 ) .\nThe lemma below posits that the value-based hypothesis problem with a low ABE dimension shall have a low AGEC in terms of the Bellman discrepancy. Lemma 2 (Low ABE dim \u2286 Low AGEC). Consider the Bellman discrepancy function as defined in (3.1) , and the expectation is taken over the transition state st+1 from P(\u00b7|st, at). Let dABE = dimABE(H, \u03f5), then we have dG \u2264 2dABE \u00b7 log T and \u03baABE \u2264 dABE.\nThe Eluder dimension and ABE dimension can capture numerous concrete problems, respectively under model-based and value-based scenarios. Specifically, the Eluder dimension incorporates rich model-based problems like linear mixture AMDPs (Wu et al., 2022), and the ABE dimension can characterize tabular AMDPs (Jaksch et al., 2010), linear AMDPs (Wei et al., 2021), AMDPs with Bellman Completeness, generalized linear AMDPs, and kernel AMDPs, where the latter three problems are newly proposed for AMDPs. Details about the concrete examples are deferred to Appendix D. Combining these facts and Lemmas 1 and 2, we can conclude that AGEC serves as a unified complexity measure, as it encompasses all of these tractable AMDP models illustrated above."
        },
        {
            "heading": "4 LOCAL-FITTED OPTIMIZATION WITH OPTIMISM",
            "text": "To solve AMDPs with low AGEC value (see Definition 3), we propose the algorithm Local-fitted Optimization with OPtimism (LOOP), whose pseudocode is given in Algorithm 1. At a high level, LOOP is a modified version of the classical fitted Q-iteration (Szepesv\u00e1ri, 2010) with optimistic planning and lazy policy updates. That is, the policies are only updated when a certain criterion is met (Line 2). When this is the case, LOOP performs three main steps:\n- Optimistic planning (Line 4.1): Compute the most optimistic ft \u2208 H within Bt that maximizes the corresponding average-reward Jt by solving a constrained optimization problem.\n- Construct confidence set (Line 4.2): Construct the confidence setBt for optimization usingDt\u22121, where all ft \u2208 H satisfying LDt\u22121(ft, ft) \u2212 infg\u2208G LDt\u22121(ft, g) \u2264 \u03b2 is included. Here, \u03b2 > 0 defines the radius, corresponding to the log covering number of the hypothesis classH.\n- Execute Policy and Update \u03a5t (Line 8-10): Choose the greedy policy \u03c0t = \u03c0ft as the exploration policy. Execute policy, collect data, and update trigger \u03a5t = LDt(ft, ft)\u2212 infg\u2208G LDt(ft, g).\nNote that both the confidence set Bt and the update condition \u03a5t are constructed upon the (cumulative) squared discrepancy, which is crucial to the algorithmic design. It takes the form\nLDt(f, f)\u2212 inf g\u2208G LDt(f, g), where LDt(f, g) = \u2211 (f \u2032,\u03b6)\u2208Dt \u2225lf \u2032(f, g, \u03b6)\u222522, (4.1)\nwhere f \u2032 and \u03b6 are drawn fromDt, and lf \u2032(f, g, \u03b6) represents the discrepancy function, which varies across different RL problems. The cumulative squared discrepancy serves as an empirical estimation of the in-sample training error (see Definition 3). Besides, we highlight two key designs of LOOP:\nAlgorithm 1 Local-fitted Optimization with Optimism - LOOP(H,G, T, \u03b4) Parameter: Initial s1, span sp(V \u2217), optimistic parameter \u03b2 = c log ( TN 2H\u222aG(1/T )/\u03b4 ) \u00b7 sp(V \u2217) Initialize: Draw a1 \u223c Unif(A) and set \u03c40 \u2190 0, \u03a50 \u2190 0, B0 \u2190 \u2205, D0 \u2190 \u2205. 1: for t = 1, . . . , T do 2: if t = 1 or \u03a5t\u22121 \u2265 4\u03b2 then 3: Set \u03c4t = t. 4: Solve optimization problem ft = argmaxft\u2208BtJft , where\nBt = { f \u2208 H : LDt\u22121(f, f)\u2212 inf\ng\u2208G LDt\u22121(f, g) \u2264 \u03b2\n} , (4.2)\n5: Compute Qt = Qft , Vt = Vft and Jt = Jft . 6: else 7: Retain (ft, Jt, Vt, Qt, \u03c4t) = (ft\u22121, Jt\u22121, Vt\u22121, Qt\u22121, \u03c4t\u22121). 8: Take at = argmaxa\u2208A Qt(st, a). 9: Observe rt = r(st, at) and transition state st+1.\n10: Update Dt = Dt\u22121 \u222a {(st, at, rt, st+1; ft)} and \u03a5t = LDt(ft, ft)\u2212 infg\u2208G LDt(ft, g).\n- Consistent control over discrepancy: The construction of confidence set Bt ensures that the cumulative squared discrepancy is controlled at level \u03b2 in each step. To see this, suppose \u03c4t = t, i.e., policy switches at the t-th step, then (4.2) ensures that LDt\u22121(ft, ft)\u2212 infg\u2208G LDt\u22121(ft, g) \u2264 \u03b2. Otherwise, it we do not switch policy at step t, then we must have \u03a5t\u22121 = LDt\u22121(ft, ft) \u2212 infg\u2208G LDt\u22121(ft, g) \u2264 4\u03b2, as hypothesis ft = ft\u22121 remains unchanged.\n- Lazy policy update: The regret decomposition in (5.1) elucidates that each policy switch incurs an additional cost of |Vt+1(st+1) \u2212 Vt(st+1)| in regret at each step (see (5.3)). This underscores the necessity of implementing lazy updates to achieve sublinear regret. Within the LOOP framework, policy updates occur adaptively, triggered only when a substantial increase in cumulative discrepancy surpassing 3\u03b2 has occurred since the last update. Intuitively, a policy switch occurs when there is a notable infusion of new information from the recently collected data. Importantly, such gap is pivotal as it provides the theoretical foundation for the implementation of lazy updates, leveraging the problem\u2019s transferability structure (see (ii), Definition 3).LOOP employs a threshold of 4\u03b2, considering inherent uncertainty and estimation errors between the minimizer g and P(ft), ensuring that the out-of-sample error will exceed \u03b2 under the updating rule.\nSimilar to previous works in general function approximation (Jin et al., 2021; Du et al., 2021), our algorithm lacks a computationally efficient solution for constrained optimization problems. Instead, our focus is on the sample efficiency, as guaranteed by the theorem below. Theorem 3 (Regret). Under Assumptions 1-4, there exists constant c such that for any \u03b4 \u2208 (0, 1) and horizon T , with probability at least 1\u2212 5\u03b4, the regret of LOOP satisfies that\nReg(T ) \u2264 O ( sp(V \u2217) \u00b7 d \u221a T\u03b2 ) ,\nwhere \u03b2 = c log ( TN 2H\u222aG(1/T )/\u03b4 ) \u00b7sp(V \u2217) and d = max{ \u221a dG, \u03baG}. Here, (dG, \u03baG) = AGEC(H,\n{lf}, 1/ \u221a T ) are AGEC defined in Definition 3, G is the auxiliary function class defined in Definition 4, and NH\u222aG(\u00b7) denotes the covering number as defined in Definition 16.\nTheorem 3 asserts that both value-based and model-based problems with low AGEC are tractable. Our algorithm LOOP achieves a O\u0303( \u221a T ) regret and the multiplicative factor depends on span sp(V \u2217), problem complexity max{ \u221a dG, \u03baG} and the log covering number. The proof sketch is provided in Section 5 and the detailed proof is deferred to Appendix E."
        },
        {
            "heading": "5 PROOF OVERVIEW OF REGRET ANALYSIS",
            "text": "In this section, we present the proof sketch of Theorem 3. In Section 4, we elucidated the construction of the confidence set and the circumstances in which updates are performed. Here, we delve into the theoretical analysis to substantiate the necessity of such designs.\nOptimism and Regret Decomposition In LOOP, we apply an optimization based method to ensure the optimism Jt \u2265 J\u2217 at each step t \u2208 [T ]. Based on the optimistic algorithm, we propose a new regret decomposition method motivated by the standard performance difference lemma (PDL) in episodic setting (Jiang et al., 2017), following the form as below:\nReg(T ) \u2264 T\u2211 t=1\nE(ft)(st, at)\ufe38 \ufe37\ufe37 \ufe38 Bellman error +\nT\u2211 t=1\nEst+1\u223cP(\u00b7|st,at)[Vt(st+1)]\u2212 Vt(st)\ufe38 \ufe37\ufe37 \ufe38 Realization error . (5.1)\nBound over Bellman Error The control over the Bellman error is achieved through the design of a confidence set and update condition that combinely controls the empirical squared discrepancy. Note that the construction of the confidence set filters ft with a limited sum of empirical squared discrepancy. Note that LDt\u22121(ft, ft)\u2212 infg\u2208G LDt\u22121(ft, g) can be regarded as an empirical overestimation of the squared discrepancy, controlled at O(\u03b2) regardless of updating. Then,\nIn-sample training error = t\u22121\u2211 i=1 \u2225E\u03b6i [lfi(ft, ft, \u03b6i)]\u222522 \u2272 \u03b2 \u2200t \u2208 [T ], (5.2)\nwith high probability, and \u03b2 is pre-determined optimistic parameter depends on horizon T and the log \u03c1-covering number. Recall that the dominance coefficient dG regulates that Bellman error \u2272[ dG \u2211T t=1 (\u2211t\u22121 i=1 \u2225E\u03b6i [lfi(ft, ft, \u03b6i)]\u222522 )]1/2 , thus we have Bellman error \u2264 O( \u221a dG\u03b2T ).\nBound over Realization error The realization error is small if the switching cost is low as the concentration arguments indicated that with high probability it holds\nRealization error \u2264 sp(V \u2217) \u00b7 N (T )\ufe38 \ufe37\ufe37 \ufe38 Switching cost\n+O ( sp(V \u2217) \u00b7 \u221a T log(1/\u03b4) )\ufe38 \ufe37\ufe37 \ufe38 Azuma-Hoeffding term , (5.3)\nwhere N (t) denote Switching cost defined as N (T ) = #{t \u2208 [T ] : \u03c4t \u0338= \u03c4t\u22121}. Motivated by the recent work of Xiong et al. (2023), the main idea of low-switching control is summarized below. The key step is that the minimizer g is a good approximator of P(ft) such that\n0 \u2264 LDt(ft,P(ft))\u2212 inf g\u2208G LDt(ft, g) \u2264 \u03b2, \u2200t \u2208 [T ] (5.4)\nwith high probability based on the minimization and definition of optimistic parameter \u03b2. In the following analysis, we assume that (5.4) holds true. Consider an update occurs at step t + 1, it implies that LDt(ft, ft) \u2212 infg\u2208G LDt(ft, g) > 4\u03b2 and the latest update at step \u03c4t ensures that LD\u03c4t\u22121(f\u03c4t , f\u03c4t)\u2212 infg\u2208G LD\u03c4t\u22121(f\u03c4t , g) \u2264 \u03b2. Combine (5.4) with arguments above, we have (i). LDt(ft, ft)\u2212 LDt ( ft,P(ft) ) > 3\u03b2, (ii). LD\u03c4t\u22121(f\u03c4t , f\u03c4t)\u2212 LD\u03c4t\u22121 ( f\u03c4t ,P(f\u03c4t) ) \u2264 \u03b2. (5.5) Using the concentration argument and (i), (ii) in (5.5), the out-sample training error between updates is lower bounded by \u2211t i=\u03c4t \u2225E\u03b6i [lfi(fi, fi, \u03b6i)]\u222522 > \u03b2. Let b1, . . . , bN (T )+1 be the sequence of updated steps, take summation over the whole process and we have\nN (T )\u03b2 \u2264 N (T )\u2211 u=1 bu+1\u22121\u2211 t=bu \u2225E\u03b6t [lft(ft, ft, \u03b6t)]\u222522 = T\u2211 t=1 \u2225E\u03b6t [lft(ft, ft, \u03b6t)]\u222522\u2264O(\u03baG \u00b7\u03b2 log T ), (5.6) where the first inequality follows arguments above and the second is based on the definition of transferability (see Definition 3) given \u2211t\u22121 i=1 \u2225E\u03b6i [lfi(ft, ft, \u03b6i)]\u2225 2 2 \u2264 O ( \u03b2 )\nfor all t \u2208 [T ]. Thus, we have N (T ) \u2264 O(\u03baG log T ) and the Realization error is bounded by O ( \u03baG \u00b7 sp(V \u2217) log T ) . Please refer to Lemma 11 in Appendix E.4 for a formal statement and detailed techniques."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "This work studies the infinite-horizon average-reward MDPs under general function approximation. To address the unique challenges of AMDPs, we introduce a new complexity metric \u2014 averagereward generalized eluder coefficient (AGEC) and a unified algorithm named Local-fitted Optimization with OPtimism (LOOP). We posit that our work paves the way for future work, including developing more general frameworks for AMDPs and new algorithms with sharper regret bounds."
        },
        {
            "heading": "A BACKGROUNDS AND TECHNICAL NOVELTIES",
            "text": "Infinite-horizon Average-reward MDPs. Pioneering works by Auer et al. (2008) and Bartlett & Tewari (2012) laid the foundation for model-based algorithms operating within the online framework with sub-linear regret. In recent years, the pursuit of improved regret guarantees has led to the emergence of a multitude of new algorithms. In tabular case, these advancements include numerous model-based approaches (Ouyang et al., 2017; Fruit et al., 2018; Zhang & Ji, 2019; Ortner, 2020) and model-free algorithms (Abbasi-Yadkori et al., 2019; Wei et al., 2020; Hao et al., 2021; Lazic et al., 2021; Zhang & Xie, 2023). In the context of function approximation, POLITEX (Abbasi-Yadkori et al., 2019), a variant of the regularized policy iteration, is the first model-free algorithm with linear value-function approximation, and achieves O\u0303(T 34 ) regret for the ergodic MDP. The work by Hao et al. (2021) followed the same setting and improved the results to O\u0303(T 23 ) with an adaptive approximate policy iteration (AAPI) algorithm. Wei et al. (2021) proposed an optimistic Q-learning algorithm FOPO for the linear function approximation, and achieve a near-optimal O\u0303( \u221a T ) regret. On another line of research, Wu et al. (2022) delved into the linear function approximation under the framework of linear mixture model, which is mutually uncoverable concerning linear MDPs (Wei et al., 2021), and proposed UCRL2-VTR based on the value-targeted regression (Ayoub et al., 2020). Recent work of Chen et al. (2022a) expanded the scope of research by addressing the general function approximation problem in average-reward RL and proposed the SIM-TO-REAL algorithm, which can be regarded as an extension to UCRL2-VTR. In comparison to the works mentioned, our algorithm, LOOP, not only addresses all the problems examined in those studies but also extends its applicability to newly identified models. See Table 1 for a summary.\nFunction Approximation in Finite-horizon MDPs. In the pursuit of developing sample-efficient algorithms capable of handling large state spaces, extensive research efforts have converged on the linear function approximation problems within the finite-horizon setting. See Yang & Wang (2019); Wang et al. (2019); Jin et al. (2020); Ayoub et al. (2020); Cai et al. (2020); Zhou et al. (2021a;b); Zhou & Gu (2022); Agarwal et al. (2022); He et al. (2022); Zhong & Zhang (2023); Zhao et al. (2023); Huang et al. (2023); Li & Sun (2023) and references therein. Furthermore, Wang et al. (2020) studied RL with general function approximation and adopted the eluder dimension (Russo & Van Roy, 2013) as a complexity measure. Before this, Jiang et al. (2017) considered a substantial subset of problems with low Bellman ranks. Building upon these foundations, Jin et al. (2021) combined both the Eluder dimension and Bellman error, thereby broadening the scope of solvable problems under the concept of the Bellman Eluder (BE) dimension. In a parallel line of research, Sun et al. (2019) proposed the witness ranking focusing on the low-rank structures, and Du et al. (2021) extended it to encompass more scenarios with the bilinear class. Besides, Foster et al. (2021; 2023) provided a unified framework, decision estimation coefficient, for interactive decision making. The work of Chen et al. (2022b) extended the value-based GOLF (Jin et al., 2021) with the introduction of the discrepancy loss function to handle the broader admissible Bellman characterization (ABC) class. More recently, Zhong et al. (2022); Liu et al. (2023b) proposed a unified framework measured by generalized eluder coefficient (GEC), an extension to Dann et al. (2021) that captures almost all known tractable problems. All these works are restricted to the finite-horizon regime, and their complexity measure and algorithms are not applicable in the infinite-horizon average-reward setting.\nLow-Switching Cost Algorithms. Addressing low-switching cost problems in bandit and reinforcement learning has seen notable progress. Abbasi-Yadkori et al. (2011) first proposed an algorithm for linear bandits with O(log T ) switching cost. Subsequent research extended this to tabular MDPs, including works of Bai et al. (2019); Zhang et al. (2020). A significant stride was made by Kong et al. (2021), who introduced importance scores to handle low-switching cost scenarios in general function approximation with complexity measured by eluder dimension (Russo & Van Roy, 2013). Recently, Xiong et al. (2023) introduced the eluder condition (EC) class, offering a comprehensive framework to address all tractable low-switching cost problems above. In the context of average-reward RL, Wei et al. (2021); Wu et al. (2022); Chen et al. (2022a); Hu et al. (2022) developed low-switching algorithms to control the regret under linear structure or model-based class, leaving a unifying framework for both value-based and model-based problems an open problem.\nFurther Elaboration on Our Contributions and Technical Novelties. Compared to episodic MDPs or discounted MDPs, AMDPs present unique challenges that prevent a straightforward exten-\nsion of existing algorithms and analyses from these well-studied domains. One notable distinction is a different regret notion in average-reward RL due to a different form of the Bellman optimality equation. Furthermore, such a difference is coupled with the challenge of exploration in the context of general function approximation. To effectively bound this regret, we introduce a new regret decomposition approach within the context of general function approximation (refer to (5.1) and (5.3)). This regret decomposition suggests that the total regret can be controlled by the cumulative Bellman error and the switching cost. Inspired by this, we propose an optimistic algorithm with lazy updates in the general function approximation setting, which uses the residue of the loss function as the indicator for deciding when to conduct policy updates. Such a lazy policy update scheme adaptively divides the total of T steps into O(log T ) epochs, which is significantly different from (OLSVI.FH; Wei et al., 2021) that reduces the infinite-horizon setting to the finite-horizon setting by splitting the whole learning procedure into several H-length epoch, where H typically chosen as \u0398( \u221a T ) (Wei et al., 2021). We remark that such an adaptive lazy updating design and corresponding analysis are pivotal in achieving the optimal O\u0303( \u221a T ) rate, as opposed to the O\u0303(T 3/4) regret in (OLSVI.FH; Wei et al., 2021). Moreover, our approach is an extension to the existing lazy update approaches for average-reward setting (Wei et al., 2021; Wu et al., 2022) that leverages the postulated linear structure and is not applicable to problems with general function approximation. Furthermore, to accommodate the average-reward term, we introduce a new complexity measure AGEC, which characterizes the exploration challenge in general function approximation. Compared with Zhong et al. (2022), our additional transferability restriction is tailored for the infinite-horizon setting and plays a crucial role in analyzing the low-switching error. Despite this additional transferability restriction, AGEC can still serve as a unifying complexity measure in the infinite-horizon average-reward setting, like the role of GEC in the finite-horizon setting. Specifically, AGEC captures a rich class of tractable AMDP models, including all previously recognized AMDPs, including all known tractable AMDPs, and some newly identified AMDPs. See Table 1 for a summary."
        },
        {
            "heading": "B DISCUSSIONS",
            "text": "Discussion about distribution families Beyond the singleton distribution familyD\u2206 taken in this paper, there exists a notable distribution family DH = {DH,t}t\u2208[T ], proposed in Jin et al. (2021), where DH,t characterizes probability measure over S \u00d7 A obtained by executing different policies induced by f1, . . . , ft\u22121 \u2208 H, measures the detailed distribution under sequential policies. However, in this paper, we exclude the consideration of DH for two principal reasons. First, evaluations of average-reward RL focus on the difference between observed rewards r(st, at) and optimal average reward J\u2217 \u2014 as opposed to the expected value V \u03c0h (i.e expected sum of reward) under specific policy and optimal value at step h \u2208 [H] in episodic setting \u2014 rendering the introduction of DH unnecessary. Second, in infinite settings, the measure of such distribution becomes highly intricate and impractical given different policy induced by f1, . . . , fT over a potentially infinite T -steps. As a comparison, in the episode setting a fixed policy induced by ft is executed over a finite H-step.\nDiscussion about V-type variant Previous research has demonstrated the existence of extensive classes of MDPs characterized by a low V-type BE dimension but cannot be captured by a infinite Q-type BE dimension in episodic setting (Jiang et al., 2017; Jin et al., 2021), and similar scenarios can be straightforwardly extended to AMDP. However, in this paper, we abstain from introducing the V-type variant, because to the best of our knowledge, solving a V-type problem relies on the offpolicy strategy or auxiliary simulators, which is unfortunately infeasible under the infinite-horizion online learning procedure. We maintain this for potential solutions in future research endeavors."
        },
        {
            "heading": "C ALTERNATIVE CHOICES OF DISCREPANCY FUNCTION",
            "text": "Note that there is another line of research that addresses model-based problems using Maximum Likelihood Estimator (MLE)-based approaches (Liu et al., 2023a; Zhong et al., 2022), as opposed to the value-targeted regression. We remark that these MLE-based approaches can be also incorporated within our framework through the use of the discrepancy function:\nlf \u2032(f, g, \u03b6t) = 1\n2 |Pf (st+1|st, at)/Pf\u2217(st+1|st, at)\u2212 1|, (C.2)\nAlgorithm 2 MLE-based Local-fitted Optimization with Optimism - MLE-LOOP(H, T, \u03b4) Parameter: Initial s1, span sp(V \u2217), optimistic parameter \u03b2 = c log ( TNH(1/T )/\u03b4 ) Initialize: Draw a1 \u223c Unif(A) and set \u03c40 \u2190 0, \u03a50 \u2190 0, B0 \u2190 \u2205, D0 \u2190 \u2205.\n1: for t = 1, . . . , T do 2: if t = 1 or \u03a5t\u22121 \u2265 3 \u221a \u03b2t then 3: Update \u03c4t = t. 4: Solve optimization problem ft = argmaxft\u2208BtJft , where\nBt = { f \u2208 H : LDt\u22121(f, f)\u2212 inf\ng\u2208G LDt\u22121(f, g) \u2264 \u03b2\n} , (C.1)\n5: Update Qt = Qft , Vt = Vft , Jt = Jft and gt = inf g\u2208G LDt\u22121(ft, g). 6: else 7: Retain (ft, gt, Jt, Vt, Qt, \u03c4t) = (ft\u22121, gt\u22121, Jt\u22121, Vt\u22121, Qt\u22121, \u03c4t\u22121). 8: Take at = argmaxa\u2208A Qt(st, a). 9: Collect reward rt = r(st, at) and transition state st+1.\n10: Update Dt = Dt\u22121 \u222a {(st, at, rt, st+1)}, \u03a5t = \u2211 (s,a)\u2208Dt TV ( Pft(\u00b7|s, a),Pgt(\u00b7|s, a) ) .\nwhere the trajectory is \u03b6t = (st, at, st+1) with expectation taken over the next transition state st+1 from P(\u00b7|st, at) such that E\u03b6t [lf \u2032(f, g, \u03b6t)] = TV ( Pf (\u00b7|st, at),Pf\u2217(\u00b7|st, at) ) . To accommodate the discrepancy function in (C.2), we introduce a natural variant of AGEC defined below. Definition 9 (MLE-AGEC). Given hypothesis class H, discrepancy function {lf}f\u2208H in (C.2) and constant \u03f5 > 0, the average-reward generalized eluder coefficients MLE-AGEC(H, {lf},\u03f5) is defined as the smallest coefficients \u03baG and dG, such that following two conditions hold with absolute constants C1, C2 > 0:\n(i) (Bellman dominance) There exists constant dG > 0 such that T\u2211 t=1 E(ft)(st, at) \u2264 [ dG \u00b7 T\u2211 t=1 t\u22121\u2211 i=1 \u2225E\u03b6i [lfi(ft, ft, \u03b6i)]\u222522 ]1/2 +C1 \u00b7 sp(V \u2217)min{dG, T}+ T\u03f5.\n(ii) (MLE-Transferability) There exists constant \u03baG > 0 such that for hypotheses f1, . . . , fT \u2208 H, if it holds that \u2211t\u22121 i=1 \u2225E\u03b6i [lfi(ft, ft, \u03b6i)]\u2225 2 2 \u2264 \u03b2 for all t \u2208 [T ], then we have\nT\u2211 t=1 \u2225E\u03b6t [lft(ft, ft, \u03b6t)]\u22251 \u2264 poly(log T ) \u221a \u03baG \u00b7 \u03b2T + C2 \u00b7 sp(V \u2217)2 min{\u03baG, T}+ 2T\u03f52.\nThe main difference between the MLE-based variant (see Definition 9) and the original AGEC (see Definition 3) is that the transferability coefficient is defined over the l1-norm of out-sample training error rather than the l2-norm, and similar condition is considered in Liu et al. (2023a); Xiong et al. (2023). Now we are ready to introduce the algorithm for the alternative discrepancy function in (C.2) and please see Algorithm 2 for complete pseudocode. The main modification lies in the construction of the confidence set and the update condition \u03a5t. Here, the confidence set follows\nBt = {ft \u2208 H : LDt\u22121(ft, ft)\u2212 inf g\u2208G LDt\u22121(ft, g) \u2264 \u03b2}, LD(f, g) = \u2212 \u2211 (s,a,s\u2032)\u2208D logPg(s\u2032|s, a).\nIn comparison, the update condition follows that \u03a5t = \u2211 (s,a)\u2208Dt TV ( Pft(\u00b7|s, a),Pgt(\u00b7|s, a) ) . Unlike the standard LOOP algorithm, the the confidence set and update condition in the MLE-based varint no longer shares the same construction. Here, we explicitly redesign the algorithm for MLEbased approaches, and the theoretical guarantee is provided below. Theorem 4 (Cumulative regret). Under Assumptions 1-2 and the discepancy function in (C.2) with self-completeness such that G = H, there exists constant c such that for any \u03b4 \u2208 (0, 1) and time horizon T , with probability at least 1\u2212 4\u03b4, the regret of MLE-LOOP satisfies that\nReg(T ) \u2264 O ( sp(V \u2217) \u00b7 d \u221a T\u03b2 ) ,\nwhere \u03b2 = c log ( TBH(1/T )/\u03b4 ) , d = max{ \u221a dG, \u03baG}. (dG, \u03baG) = MLE-AGEC(H, {lf}, 1/ \u221a T ) denotes MLE-AGEC defined in Definition 9 and BH(\u00b7) denotes the bracketing number.\nThe proof of Theorem 4 is similar to that of Theorem 3, and can be found in Appendix I.1."
        },
        {
            "heading": "D CONCRETE EXAMPLES",
            "text": "In this section, we present concrete examples of problems for AMDP. We remark that the understanding of function approximation problems under the average-reward setting is quite limited, and to our best knowledge, existing works have primarily focused on linear approximation (Wei et al., 2021; Wu et al., 2022) and model-based general function approximation (Chen et al., 2022a). Here, we introduce a variety of function classes with low AGEC. Beyond the examples considered in existing work, these newly proposed function classes are mostly natural extensions from their counterpart the finite-horizon episode setting (Jin et al., 2020; Zanette et al., 2020; Du et al., 2021; Domingues et al., 2021), which can be extended to the average-reward problems with moderate justifications."
        },
        {
            "heading": "D.1 LINEAR FUNCTION APPROXIMATION AND VARIANTS",
            "text": "Linear function approximation Consider the linear FA, which encompasses a wide range of concrete problems with state-action bias function linear in a d-dimensional feature mapping. Specifically, the linear function class HLin is formally defined as HLin = {Q(\u00b7, \u00b7) = \u27e8\u03c9,\u03d5(\u00b7, \u00b7)\u27e9, J \u2208 J(H)\n\u2223\u2223 \u2225\u03c9\u22252 \u2264 12 sp(V \u2217)\u221ad}, where \u2225\u03d5(s, a)\u22252 \u2264 \u221a2 for all (s, a) \u2208 S \u00d7 A and first coordinate fixed to 1. We emphasize that such scaling is without loss of generality justified in Lemma 19. To begin with, we introduce two problems: linear AMDP and AMDP with linear Bellman completion. Definition 10 (Linear AMDP, Wei et al. (2021)). There exists a known feature mapping\u03d5 : S\u00d7A 7\u2192 Rd, an unknown d-dimensional signed measures \u00b5 = ( \u00b51, . . . , \u00b5d ) over S, and an unknown reward parameter \u03b8 \u2208 Rd, such that the transition kernel and the reward function can be written as P(\u00b7|s, a) = \u27e8\u03d5(s, a),\u00b5(\u00b7)\u27e9, r(s, a) = \u27e8\u03d5(s, a),\u03b8\u27e9. (D.1)\nfor all (s, a) \u2208 S \u00d7A. Without loss of generality, we assume that the feature mapping satisfies that \u2225\u03d5(s, a)\u22252 \u2264 \u221a 2 for all (s, a) \u2208 S \u00d7 A with \u03d5(1) = 1 fixed, \u2225\u03b8\u22252 \u2264 \u221a d and \u2225\u00b5(S)\u22252 \u2264 \u221a d,\nwhere denote \u00b5(S) = ( \u00b51(S), . . . , \u00b5d(S) ) and \u00b5i(S) = \u222b S d\u00b5i(s) be the total measure of S.\nWe remark that the scaling on the feature mapping can help in overcoming the gap between the episodic setting and the average-reward one without incurring any additional computational or statistical costs. To illustrate the necessity, note that for linear AMDP we have\nQ\u2217(s, a) = r(s, a) + Es\u2032\u223cP(s,a)[V \u2217(s\u2032)]\u2212 J\u2217 (see (2.1)) = \u03d5(s, a)\u22a4 ( \u03b8 \u2212 J\u2217e(1) + \u222b S V \u2217(s\u2032)d\u00b5(s\u2032) ) := \u27e8\u03d5(s, a),\u03c9\u27e9, (D.2)\nwhere denote e(1) = (1, 0, . . . , 0) \u2208 Rd. Next, we provide the AMDPs with linear Bellman completion, modified from Zanette et al. (2020), which is a more general setting than linear AMDPs. Definition 11 (Linear Bellman completion). There exists a known feature mapping\u03d5 : S\u00d7A 7\u2192 Rd such that for all (s, a) \u2208 S \u00d7A and J \u2208 J(H), we have\n\u27e8TJ(\u03c9),\u03d5(s, a)\u27e9 := r(s, a) + Es\u2032\u223cP(\u00b7|s,a) [ max a\u2032\u2208A { \u03c9\u22a4\u03d5(s\u2032, a\u2032) }] \u2212 J, (D.3)\nwhich indicates that the function remains linear in feature mapping under the Bellman operator.\nGeneralized linear function approximation To introduce the nonlinearity beyond linear FA, we expand the linear function classHLin by incorporating a link function. In the context of generalized linear FA, the function class is defined as HGlin = {Q(\u00b7, \u00b7) = \u03c3 ( \u03c9\u22a4\u03d5(\u00b7, \u00b7) ) , J \u2208 J(H) \u2223\u2223 \u2225\u03c9\u22252 \u2264\u221a d}, where \u2225\u03d5(s, a)\u22252 \u2264 1 holds for all (s, a) \u2208 S \u00d7 A, and \u03c3 : R 7\u2192 R is an \u03b1-bi-Lipschitz link function such that |\u03c3(x)| \u2264 12 sp(V \u2217) for all x \u2208 R. Formally, \u03c3 is \u03b1-bi-Lipschitz continuous if\n1 \u03b1 |x\u2212 x\u2032| \u2264 |\u03c3(x)\u2212 \u03c3(x\u2032)| \u2264 \u03b1|x\u2212 x\u2032|, \u2200x, x\u2032 \u2208 R. (D.4)\nWe remark that the generalized linear function classHGlin degenerates to the standard linear function class HLin if we choose \u03c3(x) = x. Modified from Wang et al. (2019) for the episodic setting, we define AMDPs with generalized linear Bellman completion as follows.\nDefinition 12 (Generalized linear Bellman completion). There exists a known feature mapping \u03d5 : S \u00d7A 7\u2192 Rd such that for all (s, a) \u2208 S \u00d7A and J \u2208 J(H), we have\n\u03c3 ( TJ(\u03c9)\u22a4\u03d5(\u00b7, \u00b7) ) := r(s, a) + Es\u2032\u223cP(\u00b7|s,a) [ max a\u2032\u2208A { \u03c3 ( \u03c9\u22a4\u03d5(s\u2032, a\u2032) )}] \u2212 J, (D.5)\nwhere \u03c3 is an \u03b1-bi-Lipschitz function with |\u03c3(x)| \u2264 12 sp(V \u2217) for all x \u2208 R, and \u03b1 \u2265 1.\nThe proposition below states that (generalized) linear function classes have low AGEC.\nProposition 5 (Linear FA\u2282 Low AGEC). Consider linear function classHLin and generalized linear function classHGlin with a d-dimensional feature mapping \u03d5 : S \u00d7A 7\u2192 Rd, if the problem follows one of Definitions 10-12, then it have low AGEC under Bellman discrepancy in (3.1) such that\ndG \u2264 O ( d log ( sp(V \u2217) \u221a d\u03f5\u22121 ) log T ) , \u03baG \u2264 O ( d log ( sp(V \u2217) \u221a d\u03f5\u22121 )) .\nLinearQ\u2217/V \u2217 AMDP Moreover, we consider the linearQ\u2217/V \u2217 AMDPs, which is modified from the one in Du et al. (2021) under the episodic setting. Note that the linear Q\u2217/V \u2217 AMDPs are not strictly a value-based problem (see Definition 1), as there exists an additional assumption made beyond (Q\u2217, J\u2217) to impose the linear structure of state bias function V \u2217.\nDefinition 13 (Linear Q\u2217/V \u2217 AMDP). There exists known feature mappings \u03d5 : S \u00d7 A 7\u2192 Rd1 , \u03c8 : S 7\u2192 Rd2 , and unknown vectors \u03c9\u2217 \u2208 Rd1 , \u03b8\u2217 \u2208 Rd2 such that optimal value functions follow\nQ\u2217(s, a) = \u27e8\u03d5(s, a),\u03c9\u2217\u27e9, V \u2217(s\u2032) = \u27e8\u03c8(s\u2032),\u03b8\u2217\u27e9,\nfor all (s, a, s\u2032) \u2208 S \u00d7 A \u00d7 S. Without loss of generality, we assume the feature mappings hold \u2225\u03d5(s, a)\u22252 \u2264 \u221a 2, \u2225\u03c8(s, a)\u22252 \u2264 \u221a 2 for all (s, a) \u2208 S \u00d7 A with first coordinate \u03d5(1) = \u03c8(1) = 1 fixed, and the parameters follow \u2225\u03c9\u2217\u22252 \u2264 12 sp(V \u2217) \u221a d1 and \u2225\u03b8\u2217\u22252 \u2264 12 sp(V \u2217) \u221a d2.\nNote that for linear Q\u2217/V \u2217 AMDPs, if we disregard the information related to the feature mapping \u03c8 : S 7\u2192 Rd2 , the problem is a standard linear FA problem (with further assumption required). To fully leverage the structural information provided in the linear Q\u2217/V \u2217 structure, AGEC can also offer an appropriate complexity measure for this variant of the value-based problem, as stated below.\nProposition 6 (Linear Q\u2217/V \u2217 \u2282 Low AGEC). Linear Q\u2217/V \u2217 AMDPs with coupled (d1, d2)dimensional feature mappings \u03d5 : S \u00d7A 7\u2192 Rd1 and \u03c8 : S 7\u2192 Rd2 have low AGEC such that\ndG \u2264 O ( d log ( sp(V \u2217) \u221a d\u03f5\u22121 ) log T ) , \u03baG \u2264 O ( d log ( sp(V \u2217) \u221a d\u03f5\u22121 )) ,\nwhere denote d = d1 + d2 be the sum of dimensions of both feature mappings.\nThe proposition above asserts that in linearQ\u2217/V \u2217 AMDPs, acquiring additional structural information through the state bias function comes at an added computational cost of O\u0303(d2) in comparison to standard linear FA. Notably, linear FA, generalized linear FA, and linear Q\u2217/V \u2217 AMDPs are representative value-based problems. The proof of this proposition relies on the ABE dimension as an intermediate complexity measure, and the further result directly follows Lemma 2."
        },
        {
            "heading": "D.2 KERNEL FUNCTION APPROXIMATION",
            "text": "In this subsection, we first introduce the notion of effective dimension. With this notion, we prove a useful proposition that any linear kernel function class with a low effective dimension also has low AGEC. Consider the kernel FA, a natural extension to linear FA (see Appendix D.1 for detailed), from the d-dimensional Euclidean space Rd to a separable kernel Hilbert space K. Note that the linear kernel function class is defined as HKer = { Q(\u00b7, \u00b7) = \u27e8\u03d5(\u00b7, \u00b7),\u03c9\u27e9K, J \u2208 J(H)\n\u2223\u2223 \u2225\u03c9\u2225K \u2264 sp(V \u2217)R } given a separable Hilbert space K, where the feature mapping \u03d5 : S \u00d7 A 7\u2192 K satisfies that \u2225\u03d5(s, a)\u2225K \u2264 1 for all (s, a) \u2208 S\u00d7A. To better measure the complexity of problems in Hilbert space K with potentially infinite dimension, we introduce the \u03f5-effective dimension.\nDefinition 14 (\u03f5-effective dimension). Consider a set Z with the possibly infinite elements in a given separable Hilbert space K, the \u03f5-effective dimension, denoted by dimeff(Z, \u03f5), is defined as the length n of the longest sequence satisfying the condition below:\nsup z1,...,zn\u2208Z\n{ 1\nn log det\n( I+ 1\n\u03f52 n\u2211 t=1 ziz \u22a4 i ) \u2264 e\u22121 } .\nHere, the concept of \u03f5-effective dimension is inspired by the measurement of maximum information gain (Srinivas et al., 2009) and is later introduced as a complexity measure of Hilbert space in Du et al. (2021); Zhong et al. (2022). Similar to Jin et al. (2021), we strengthen the assumption over completeness and require the H to be self-complete concerning the operator such that G = H to simplify the analysis of the complexity ofHKer. We\u2019re now ready to show kernel FA has low AGEC.\nProposition 7 (Kernel FA \u2282 Low AGEC). Under the self-completeness, kernel FA with function classHKer concerning a known feature mapping \u03d5 : S \u00d7A 7\u2192 K have low AGEC such that\ndG \u2264 dimeff ( X , \u03f5/2sp(V \u2217)R ) log T, \u03baG \u2264 dimeff ( X , \u03f5/2sp(V \u2217)R ) ,\nwhere denote X = {\u03d5(s, a) : (s, a) \u2208 S \u00d7A} as the collection of feature mappings.\nThe proposition above demonstrates that the kernel FA with a low \u03f5-effective dimension over the Hilbert space has low AGEC. As a special case of kernel FA, if we choose K = Rd, then we can prove that the RHS in the proposition above is upper bounded by O\u0303(d) (Jin et al., 2022)."
        },
        {
            "heading": "D.3 LINEAR MIXTURE AMDP",
            "text": "In this subsection, we focus on the average-reward linear mixture problem considered in Wu et al. (2022). In this context, the hypotheses function class is defined as HLM = {P(s\u2032|s, a) = \u03b8\u22a4\u03d5(s, a, s\u2032), r(s, a) = \u03b8\u22a4\u03c8(s, a)| \u2225\u03b8\u22252 \u2264 1}with known feature mappings\u03d5 : S\u00d7A\u00d7S 7\u2192 Rd, \u03c8 : S \u00d7A 7\u2192 Rd, and an unknown parameter \u03b8 \u2208 Rd. Detailedly, the problem is defined as below.\nDefinition 15 (Linear mixture AMDPs, Wu et al. (2022)). There exists a known feature mapping \u03d5 : S \u00d7A\u00d7 S 7\u2192 Rd, \u03c8 : S \u00d7A 7\u2192 Rd, and unknown vectors \u03b8 \u2208 Rd, it holds that\nP(s\u2032|s, a) = \u27e8\u03b8,\u03d5(s, a, s\u2032)\u27e9, r(s, a) = \u27e8\u03b8,\u03c8(s, a)\u27e9,\nfor all (s, a, s\u2032) \u2208 S \u00d7 A \u00d7 S. Without loss of generality, we assume that the feature mappings satisfy that \u2225\u03d5(s, a)\u22252 \u2264 \u221a d and \u2225\u03c8(s, a)\u22252 \u2264 \u221a d for all (s, a) \u2208 S \u00d7A.\nNow we show that the linear mixture problem is tractable under the framework of AGEC.\nProposition 8 (Linear mixture \u2282 Low AGEC). Consider linear mixture problem with hypotheses classHLin and d-dimensional feature (\u03d5, \u03c8). If the discrepancy function is chosen as\nlf \u2032(f, g, \u03b6t) = \u03b8 \u22a4 g { \u03c8(st, at) + \u222b S \u03d5(st, at, s \u2032)Vf \u2032(s \u2032)ds\u2032 } \u2212 r(st, at)\u2212 Vf \u2032(st+1), (D.6)\nand takesH = G with operator following PJf (f) = f\u2217 for all f \u2208 H, it has low AGEC such that\ndG \u2264 O ( d log ( sp(V \u2217)T/ \u221a d\u03f5 )) , \u03baG \u2264 O ( d log ( sp(V \u2217)T/ \u221a d\u03f5 )) .\nThe proposition posits that AGEC can capture the linear mixture AMDP, based on a modified version of the Bellman discrepancy function in (2.2). In contrast to the linear FA discussed in Appendix D.1 above, the presence of the average-reward term in this model-based problem does not impose any additional computational or statistical burden, and there is no need for structural assumptions on feature mappings, such as a fixed first coordinate, when considering the discrepancy in (D.6)."
        },
        {
            "heading": "E PROOF OF MAIN RESULTS FOR LOOP",
            "text": ""
        },
        {
            "heading": "E.1 PROOF OF THEOREM 3",
            "text": "Proof of Theorem 3. Note that the regret can be decomposed as\nReg(T ) = T\u2211 t=1 ( J\u2217 \u2212 r(st, at) ) \u2264 T\u2211 t=1 ( Jt \u2212 r(st, at) ) (optimism)\n(a) = T\u2211 t=1 E(ft)(st, at)\u2212 T\u2211 t=1 Est+1\u223cP(\u00b7|st,at) [ Qt(st, at)\u2212max a\u2208A Qt(st+1, a) ] (b) =\nT\u2211 i=1\nE(ft)(st, at)\ufe38 \ufe37\ufe37 \ufe38 Bellman error +\nT\u2211 t=1 [ Est+1\u223cP(\u00b7|st,at)[Vt(st+1)]\u2212 Vt(st) ] \ufe38 \ufe37\ufe37 \ufe38\nRealization error\n, (E.1)\nwhere step (a) and step (b) follow the definition of the Bellman optimality operator and the greedy policy. Below, we will present the bound of Bellman error and Realization error respectively.\nStep 1: Bound over Bellman error Recall that the of confidence set ensures that LDt\u22121(ft, ft) \u2212 infg\u2208G LDt\u22121(ft, g) \u2264 O(\u03b2) across all steps. Using the concentration arguments, we can infer\nt\u22121\u2211 i=1 \u2225E\u03b6i [lfi(ft, ft, \u03b6i)]\u222522 \u2264 O(\u03b2), (E.2)\nwith high probability and the formal statements are deferred to Lemma 10 in Appendix E.3. In the following arguments, we assume the above event holds. Take \u03f5 = 1/ \u221a T , recall the definition of dominance coefficient dG in AGEC(H,J , l, \u03f5) and it directly indicates that Bellman error = T\u2211 t=1 E(ft)(st, at) \u2264 [ dG T\u2211 t=1 t\u22121\u2211 i=1 \u2225E\u03b6i [lfi(ft, ft, \u03b6)]\u2225 2 2 ]1/2 +O ( sp(V \u2217) \u221a dGT ) ,\nand thus the Bellman error can be upper bounded by O ( sp(V \u2217) \u221a dG\u03b2T ) .\nStep 2: Bound over Realization error To bound the Realization error, we shall use the concentration argument and the upper bound of the switching cost. Note that\nRealization error (c) = T\u2211 t=1 [Vt(st+1)\u2212 Vt(st)] +O ( sp(V \u2217) \u221a T log(1/\u03b4) ) ,\n= T\u2211 t=1 [ V\u03c4t(st+1)\u2212 V\u03c4t+1(st+1) ] +O ( sp(V \u2217) \u221a T log(1/\u03b4) ) , (Shift)\n= T\u2211 t=1 [ V\u03c4t(st+1)\u2212 V\u03c4t+1(st+1) ] 1(\u03c4t \u0338= \u03c4t+1) +O ( sp(V \u2217) \u221a T log(1/\u03b4) ) \u2264 sp(V \u2217) \u00b7 N (T ) +O ( sp(V \u2217) \u221a T log(1/\u03b4) ) (d) \u2264 O ( sp(V \u2217) \u00b7 \u03baG \u221a T log(1/\u03b4) ) ,\n(E.3)\nwhere step (c) directly follows the Azuma-Hoeffding inequality and step (d) is based the fact that \u2225V\u03c4t \u2212 V\u03c4t+1\u2225\u221e \u2264 sp(V \u2217) and the bounded switching cost such that N (T ) \u2264 O(\u03baG log T ), where \u03baG is the transferability coefficient in AGEC with \u03f5 = 1/ \u221a T . Please refer to Lemma 11 in Appendix E.4 for the detailed statement and proof of the bounded switching cost.\nStep 3: Combine the bounded erroes Plugging (E.2) and (E.3) back into n (E.1), we have Reg(T ) \u2264 Bellman error + Realization error\n\u2264 O ( sp(V \u2217) \u221a dG\u03b2T ) +O ( sp(V \u2217)\u03baG \u221a T log(1/\u03b4) ) = O ( sp(V \u2217) \u00b7 d \u221a T\u03b2 ) ,\nwhere denote d = max{ \u221a dG, \u03baG} is a function of (dG, \u03baG) = AGEC(H, {lf}f\u2208H, 1/ \u221a T ). In the arguments above, the optimistic parameter is chosen as \u03b2 = c log ( TN 2H\u222aG(1/T )/\u03b4 ) \u00b7 sp(V \u2217), which takes the upper bound of the optimistic parameters, aligning with the choice in Lemma 9, Lemma 10, and Lemma 11. Then finish the proof of cumulative regret for LOOP in Algorithm 1. 2"
        },
        {
            "heading": "E.2 PROOF OF LEMMA 9",
            "text": "Lemma 9 (Optimism). Under Assumptions 1-4, LOOP is an optimistic algorithm such that it ensures Jt \u2265 J\u2217 for all t \u2208 [T ] with probability greater than 1\u2212 \u03b4.\nProof of Lemma 9. Denote V\u03c1(G) be the \u03c1-cover of G and NG(\u03c1) be the size of \u03c1-cover V\u03c1(G). Consider fixed (i, g) \u2208 [T ]\u00d7 G and define the auxiliary function\nXi,fi(g) := \u2225lfi(f\u2217, g, \u03b6i)\u2225 2 2 \u2212 \u2225lfi(f \u2217, f\u2217, \u03b6i)\u222522 , (E.4) where f\u2217 is the optimal hypothesis in value-based problems and the true hypothesis in model-based ones. Let Ft be the filtration induced by {s1, a1, . . . , st, at} and note that f1, . . . , ft is fixed under the filtration, then we have E[Xi,fi(g)|Fi] = E\u03b6i [\u2225lfi(f\u2217, g, \u03b6i)\u222522 \u2212 \u2225lfi(f\u2217,P(f\u2217), \u03b6i)\u222522|Fi]\n= E\u03b6i [[ lfi(f \u2217, g, \u03b6i)\u2212 lfi(f\u2217,P(f\u2217), \u03b6i) ] \u00b7 [ lfi(f \u2217, g, \u03b6i) + lfi(f \u2217,P(f\u2217), \u03b6i) ]\u2223\u2223\u2223Fi] = E\u03b6i [ E\u03b6i [ lfi(f \u2217, g, \u03b6i) ] \u00b7 [ lfi(f \u2217, g, \u03b6i) + lfi(f \u2217,P(f\u2217), \u03b6i)\n]\u2223\u2223\u2223Fi] = \u2225E\u03b6i [ lfi(f \u2217, g, \u03b6i) ] \u222522,\nwhere the equation follows the definition of generalized completeness (see Assumption 4):{ E\u03b6i [lf \u2032(f, g, \u03b6)] = lf \u2032(f, g, \u03b6)\u2212 lf \u2032(f,P(f), \u03b6), E\u03b6i [lf \u2032(f, g, \u03b6)] = E\u03b6i [ lf \u2032(f, g, \u03b6) + lf \u2032(f,P(f), \u03b6) ] .\nSimilarly, we can obtain that the second moment of the auxiliary function is bounded by E[Xi,fi(g)2|Fi] \u2264 O ( sp(v\u2217)2 \u2225E\u03b6i [lfi(f\u2217, g, \u03b6i)]\u2225 2 2 ) ,\nBy Freedman\u2019s inequality (see Lemma 18), with probability greater than 1\u2212 \u03b4 it holds that\u2223\u2223\u2223 t\u2211 i=1 Xi,fi(g)\u2212 t\u2211 i=1 \u2225E\u03b6i [lfi(f\u2217, g, \u03b6i)]\u2225 2 2 \u2223\u2223\u2223 \u2264 O  \u221a\u221a\u221a\u221alog(1/\u03b4) \u00b7 sp(V \u2217)2 t\u2211\ni=1\n\u2225E\u03b6i [lfi(f\u2217, g, \u03b6i)]\u2225 2 2 + log(1/\u03b4)  . By taking union bound over [T ]\u00d7V\u03c1(G), for any (t, \u03d5) \u2208 [T ]\u00d7V\u03c1(G) we have\u2212 \u2211t i=1Xi,fi(\u03d5) \u2264 O(\u03b6), where \u03b6 = sp(V \u2217) log(TNG(\u03c1)/\u03b4) and we use the fact that \u2225E\u03b6i [lfi(f\u2217, g, \u03b6i)]\u2225 2 2 is nonnegative. Recall the definition of \u03c1-cover, it ensures that for any g \u2208 G, there exists \u03d5 \u2208 V\u03f5(G) such that \u2225g(s, a)\u2212 \u03d5(s, a)\u22251 \u2264 \u03c1 for all (s, a) \u2208 S \u00d7A. Therefore, for any g \u2208 G we have\n\u2212 t\u2211 i=1 Xi,fi(g) \u2264 O ( \u03b6 + t\u03c1 ) . (E.5)\nCombine the (E.5) above and the designed confidence set, then for all t \u2208 [T ] it holds that\nLDt\u22121(f\u2217, f\u2217)\u2212 inf g\u2208G LDt\u22121(f\u2217, g) = \u2212 t\u22121\u2211 i=1 Xi,fi(g\u0303) \u2264 O ( \u03b6 + t\u03c1 ) < \u03b2, (E.6)\nwhere g\u0303 is the local minimizer to LDt\u22121(f\u2217, g), and we take the covering coefficient as \u03c1 = 1/T and optimistic parameter as \u03b2 = c log ( TN 2H\u222aG(1/T )/\u03b4 ) sp(V \u2217). Based on (E.6), with probability greater than 1\u2212 \u03b4, f\u2217 is a candidate of the confidence set such that Jt \u2265 J\u2217 for all t \u2208 [T ]. 2"
        },
        {
            "heading": "E.3 PROOF OF LEMMA 10",
            "text": "Lemma 10. For fixed \u03c1 > 0 and the optimistic parameter \u03b2 = c ( sp(V \u2217) log ( TN 2H\u222aG(\u03c1)/\u03b4 ) +T\u03c1 ) where c > 0 is constant large enough, then it holds that\nt\u22121\u2211 i=1 E\u03b6i \u2225lfi(ft, ft, \u03b6i)\u2225 2 \u2264 O(\u03b2), (E.7)\nfor all t \u2208 [T ] with probability greater than 1\u2212 \u03b4.\nProof of Lemma 10. Denote V\u03c1(H) be the \u03c1-cover of H and NH(\u03c1) be the size of \u03c1-cover V\u03c1(H). Consider fixed (i, f) \u2208 [T ]\u00d7H and define the auxiliary function\nXi,fi(f) := \u2225\u2225lfi(f, f, \u03b6i)\u2225\u222522 \u2212 \u2225\u2225lfi(f,P(f), \u03b6i)\u2225\u222522 ,\nLet Ft be the filtration induced by {s1, a1, . . . , st, at} and note that f1, . . . , ft is fixed under the filtration, then we have\nE[Xi,fi(f)|Fi] = E\u03b6i [\u2225lfi(f, f, \u03b6i)\u2225 2 2 \u2212 \u2225lfi(f,P(f), \u03b6i)\u2225 2 2 |Fi] = E\u03b6i [[ lfi(f, f, \u03b6i)\u2212 lfi(f,P(f), \u03b6i) ] \u00b7 [ lfi(f, f, \u03b6i) + lfi(f,P(f), \u03b6i) ]\u2223\u2223\u2223Fi] = E\u03b6i [ lfi(f, f, \u03b6i) ] \u00b7 E\u03b6i [ lfi(f, f, \u03b6i) + lfi(f,P(f), \u03b6i)\n\u2223\u2223Fi] = \u2225\u2225E\u03b6i[lfi(f, f, \u03b6i)]\u2225\u222522 ,\nwhere the equation generalized completeness (see Lemma 9). Similarly, we can obtain that the second moment of the auxiliary function is bounded by\nE[Xi,fi(f)2|Fi] \u2264 O ( sp(v\u2217)2 \u2225\u2225E\u03b6i[lfi(f, f, \u03b6i)]\u2225\u222522 ), By Freedman\u2019s inequality in Lemma 18, with probability greater than 1\u2212 \u03b4 we have\u2223\u2223\u2223 t\u2211\ni=1 Xi,fi(f)\u2212 t\u2211 i=1 \u2225\u2225E\u03b6i[lfi(f, f, \u03b6i)]\u2225\u222522 \u2223\u2223\u2223 \u2264 O  \u221a\u221a\u221a\u221alog(1/\u03b4) \u00b7 sp(V \u2217)2 t\u2211\ni=1\n\u2225\u2225E\u03b6i[lfi(f, f, \u03b6i)]\u2225\u222522 + log(1/\u03b4)  .\nDefine \u03b6 = sp(V \u2217) log(TNH(\u03c1)/\u03b4), by taking a union bound over \u03c1-covering of hypothesis set H, we can obtain that with probability greater than 1\u2212 \u03b4, for all (t, \u03d5) \u2208 [T ]\u00d7 V\u03c1(H) we have\u2223\u2223\u2223 t\u2211\ni=1 Xi,fi(\u03d5)\u2212 t\u2211 i=1 \u2225E\u03b6i [ lfi(\u03d5, \u03d5, \u03b6i) ] \u222522 \u2223\u2223\u2223\n\u2264 O  \u221a\u221a\u221a\u221a\u03b6 \u00b7 sp(V \u2217)2 t\u2211\ni=1\n\u2225E\u03b6i [ lfi(\u03d5, \u03d5, \u03b6i) ] \u222522 + \u03b6  . (E.8) The following analysis assumes that the event above is true. Recall that the LOOP ensures that\nt\u22121\u2211 i=1 Xi,fi(ft) = t\u22121\u2211 i=1 \u2225lfi(ft, ft, \u03b6i)\u222522 \u2212 t\u22121\u2211 i=1 \u2225lfi ( ft,P(ft), \u03b6i ) \u222522,\n\u2264 t\u22121\u2211 i=1 \u2225\u2225lfi(ft, ft, \u03b6i)\u2225\u222522 \u2212 infg\u2208G t\u22121\u2211 i=1 \u2225\u2225lfi(ft, g, \u03b6i)\u2225\u222522 , = LDt\u22121(ft, ft)\u2212 inf\ng\u2208G LDt\u22121(ft, g) \u2264 O(\u03b2), (E.9)\nwhere the last inequality is based on the confidence set and the update condition combined. Note that if the update is executed at time t, the confidence set ensures that\nLDt\u22121(ft, ft)\u2212 inf g\u2208G LDt\u22121(ft, g) \u2264 \u03b2,\nwithin the update step t. Otherwise, if the update condition is not triggered, we have f\u03c4t = ft and\n\u03a5t\u22121 = LDt\u22121(ft, ft)\u2212 inf g\u2208G LDt\u22121(ft, g) \u2264 4\u03b2.\nRecall that based on the definition of \u03c1-cover for any f \u2208 H, there exists \u03d5 \u2208 V\u03c1(H) such that \u2225g(s, a)\u2212 \u03d5(s, a)\u22251 \u2264 \u03c1 for all (s, a) \u2208 S \u00d7A, we have the in-sample training error is bounded by\nt\u22121\u2211 i=1 \u2225\u2225E\u03b6i[lfi(ft, ft, \u03b6i)]\u2225\u222522 \u2264 t\u22121\u2211 i=1 \u2225\u2225E\u03b6i[lfi(\u03d5t, \u03d5t, \u03b6i)]\u2225\u222522 +O(t\u03c1), (\u03c1-approximation) =\nt\u22121\u2211 i=1 Xi,fi(\u03d5t) +O(t\u03c1+ \u03b6) ( (E.8))\n= t\u22121\u2211 i=1 Xi,fi(ft) +O(t\u03c1+ \u03b6) \u2264 O(T\u03c1+ \u03b6 + \u03b2) = O(\u03b2), (E.10)\nwhere the last inequality follows (E.9), and takes \u03b2 = c ( (sp(V \u2217) log ( TN 2H\u222aG(\u03c1)/\u03b4 ) + T\u03c1 ) . 2"
        },
        {
            "heading": "E.4 PROOF OF LEMMA 11",
            "text": "Lemma 11. Let N (T ) be the switching cost with time horizon T , defined as\nN (T ) = #{t \u2208 [T ] : \u03c4t \u0338= \u03c4t\u22121}. Given fixed \u03c1 > 0 and the optimistic parameter \u03b2 = c ( sp(V \u2217) log ( TN 2H\u222aG(\u03c1)/\u03b4 ) + T\u03c1 ) , where c > 0 is large enough constant, then with probability greater than 1\u2212 2\u03b4 we have\nN (T ) \u2264 O ( \u03baG log T + \u03b2 \u22121T log T\u03f52 ) ,\nwhere \u03baG is the transferability coefficient with respect to AGEC(H, {lf \u2032}, \u03f5).\nProof of Lemma 11. Denote V\u03c1(H) be the \u03c1-cover ofH and NH(\u03c1) be the size of \u03c1-cover V\u03c1(H). Step 1: Bound the difference of discrepancy between the minimizer and P(f). Consider fixed tuple (i, f, g) \u2208 [T ]\u00d7H\u00d7 G and define auxiliary function as\nXi,fi(f, g) := \u2225\u2225lfi(f, g, \u03b6i)\u2225\u222522 \u2212 \u2225\u2225lfi(f,P(f), \u03b6i)\u2225\u222522\nLet Ft be the filtration induced by {s1, a1, . . . , st, at} and note that f1, . . . , ft is fixed under the filtration, then we have\nE[Xi,fi(f, g)|Fi] = E\u03b6i [ \u2225\u2225lfi(f, g, \u03b6i)\u2225\u222522 \u2212 \u2225\u2225lfi(f,P(f), \u03b6i)\u2225\u222522 |Fi]\n= E\u03b6i [[ lfi(f, g, \u03b6i)\u2212 lfi(f,PJf (f), \u03b6i) ] \u00b7 [ lfi(f, g, \u03b6i) + lfi(f,PJf (f), \u03b6i) ]\u2223\u2223\u2223Fi] = E\u03b6i [ lfi(f, g, \u03b6i) ] \u00b7 E\u03b6i [ lfi(f, g, \u03b6i) + lfi(f,PJf (f), \u03b6i)\n\u2223\u2223Fi] = \u2225\u2225E\u03b6i[lfi(f, g, \u03b6i)]\u2225\u222522 ,\nwhere the equation generalized completeness (see Lemma 9). Similarly, we can obtain that the second moment of the auxiliary function is bounded by\nE[Xi,fi(f, g)2|Fi] \u2264 O ( sp(V \u2217)2 \u2225\u2225E\u03b6i[lfi(f, g, \u03b6i)]\u2225\u222522 ), By Freedman\u2019s inequality in Lemma 18, with probability greater than 1\u2212 \u03b4\u2223\u2223\u2223 t\u2211\ni=1 Xi,fi(f, g)\u2212 t\u2211 i=1 \u2225\u2225E\u03b6i[lfi(f, g, \u03b6i)]\u2225\u222522 \u2223\u2223\u2223 \u2264 O  \u221a\u221a\u221a\u221alog(1/\u03b4) \u00b7 sp(V \u2217)2 t\u2211\ni=1\n\u2225\u2225E\u03b6i[lfi(f, g, \u03b6i)]\u2225\u222522 + log(1/\u03b4) \nDefine \u03b6 = sp(V \u2217) log(TN 2H\u222aG(\u03c1)/\u03b4), by taking a union bound over \u03c1-covering of hypothesis set H\u00d7 G, with probability greater than 1\u2212 \u03b4, for all (t, \u03d5, \u03c6) \u2208 [T ]\u00d7 V\u03c1(H)\u00d7 V\u03c1(G) it holds\u2223\u2223\u2223 t\u2211\ni=1 Xi,fi(\u03d5, \u03c8)\u2212 t\u2211 i=1 \u2225\u2225E\u03b6i[lfi(\u03d5, \u03c8, \u03b6i)]\u2225\u222522 \u2223\u2223\u2223 \u2264 O  \u221a\u221a\u221a\u221a\u03b6 \u00b7 sp(V \u2217)2 t\u2211\ni=1\n\u2225\u2225E\u03b6i[lfi(\u03d5, \u03c8, \u03b6i)]\u2225\u222522 + \u03b6  , (E.11)\nwhere \u03b6 = sp(V \u2217) log(TN 2H\u222aG(\u03c1)/\u03b4). Note that \u2225\u2225E\u03b6i[lfi(\u03d5, \u03c8, \u03b6i)]\u2225\u222522 is non-negative, then it\nholds that \u2212 \u2211t i=1Xi,fi(\u03d5, \u03c6) \u2264 O(\u03b6) for all t \u2208 [T ]. Based on (E.11) and the \u03c1-approximation, we have\n\u2212 t\u2211 i=1 Xi,fi(f, g) \u2264 O ( \u03b6 + t\u03c1 ) , \u2200t \u2208 [T ],\nfor any (f, g) \u2208 H \u00d7 G. Recall that \u03b2 = c log(TN 2H\u222aG(\u03c1)/\u03b4)sp(V \u2217), for all t \u2208 [T ] we have\nLDt(ft,P(ft))\u2212 inf g\u2208G LDt(ft, g) = t\u2211 i=1 \u2225lfi(ft,P(ft), \u03b6i)\u2225 2 2 \u2212 infg\u2208G t\u2211 i=1 \u2225lfi(ft, g, \u03b6i)\u2225 2 2\n= \u2212 t\u2211 i=1 Xi,fi(ft, g\u0303) \u2264 O ( \u03b6 + t\u03c1 ) \u2264 \u03b2. (E.12)\nCombine (E.12), and the fact that g is defined as the local minimizer among auxiliary class G and P(ft) \u2208 G, then for all t \u2208 [T ] we have the difference of discrepancy bounded by\n0 \u2264 LDt(ft,PJt(ft))\u2212 inf g\u2208G LDt(ft, g) \u2264 \u03b2. (E.13)\nStep 2: Bound the out-sample training error between updates. Consider an update is executed at step t+1, it directly implies thatLDt(ft, ft)\u2212infg\u2208G LDt(ft, g) > 4\u03b2, while the latest update at step \u03c4t ensures that LD\u03c4t\u22121(f\u03c4t , f\u03c4t) \u2212 infg\u2208G LD\u03c4t\u22121(f\u03c4t , g) \u2264 \u03b2, where \u03c4t is the pointer of the lastest update. Combined the results above with (E.13), we have\nLDt(ft, ft)\u2212 LDt ( ft,P(ft) ) > 3\u03b2, LD\u03c4t\u22121(f\u03c4t , f\u03c4t)\u2212 LD\u03c4t\u22121 ( f\u03c4t ,P(f\u03c4t) ) \u2264 \u03b2. (E.14)\nIt indicates that the sum of squared empirical discrepancy between two adjacent updates follows t\u2211\ni=\u03c4t\n\u2225lft(ft, ft, \u03b6t)\u222522 = LD\u03c4t:t(ft, ft)\u2212 LD\u03c4t:t(ft,PJt(ft)) > 2\u03b2, (E.15)\nwhere denoteD\u03c4t:t = Dt/D\u03c4t . Based on the similar concentration arguments as Lemma 10, we have the out-sample training error between updates is bounded by \u2211t i=\u03c4t \u2225E\u03b6i [lfi(fi, fi, \u03b6i)]\u222522 > \u03b2.\nStep 3: Bound the switching cost under the transferability constraint. Denote b1, . . . , bN (T ), bN (T )+1 be the sequence of updated steps such that \u03c4t \u2208 {bt} for all t \u2208 [T ], and we fix the recorder b1 = 1 and bN (T )+1 = T + 1. Note that based on (E.15), the sum of out-sample training error shall have a lower bound such that\nT\u2211 t=1 \u2225E\u03b6t [lft(ft, ft, \u03b6t)]\u222522 = N (T )\u2211 u=1 bu+1\u22121\u2211 t=bu \u2225E\u03b6t [lft(ft, ft, \u03b6t)]\u222522 \u2265 N (T )\u03b2. (E.16)\nBesides, note that the in-sample training error \u2211t\u22121 i=1 \u2225E\u03b6t [lfi(ft, ft, \u03b6t)] \u222522 \u2264 O ( \u03b2 )\nfor all t \u2208 [T ] and based on the definition of transferability coefficient \u03baG (see Definition 3), we have\nT\u2211 t=1 \u2225E\u03b6t [lft(ft, ft, \u03b6t)]\u222522 \u2264 O ( \u03baG max{\u03b2, sp(V \u2217)2} log T + T log T\u03f52 ) (E.17)\nCombine (E.16) and (E.17), it holdsN (T ) \u2264 O ( \u03baG log T + \u03b2 \u22121T log T\u03f52 ) and finish the proof. 2"
        },
        {
            "heading": "F PROOF OF RESULTS ABOUT COMPLEXITY MEASURES",
            "text": "In this section, we provide the proof of results about the complexity metrics mentioned in Section 3."
        },
        {
            "heading": "F.1 PROOF OF LEMMA 1",
            "text": "Proof of Lemma 1. Recall that the eluder dimension is defined over the function class following XH := { Xf,f \u2032(s, a) = ( rf + Pf \u2032Vf ) (s, a) : f, f \u2032 \u2208 H } .\nStart with the transferability coefficient, the given condition can be written as t\u22121\u2211 i=1 \u2225E\u03b6i [lfi(ft, ft, \u03b6i)]\u2225 2 2 = t\u22121\u2211 i=1 [( rft + PftVft \u2212 rf\u2217 + Pf\u2217Vft ) (si, ai) ]2 =\nt\u22121\u2211 i=1 [( Xft,ft \u2212Xft,f\u2217 ) (si, ai) ]2 \u2264 O(\u03b2), (F.1) for all t \u2208 [T ]. Denote XH \u2212 XH = {f \u2212 f \u2032 : f, f \u2032 \u2208 XH}, the generalized pigeon-hole principle (see Lemma 13) suggests that if we take \u0393 = D\u2206 and \u03d5t = Xft,ft \u2212Xft,f\u2217 \u2208 XH \u2212XH, it holds\nt\u2211 i=1 \u2225E\u03b6i [lfi(fi, fi, \u03b6i)]\u2225 2 = t\u2211 i=1 [( Xfi,fi \u2212Xfi,f\u2217 ) (si, ai) ]2 \u2264 O ( dimDE(XH \u2212XH,D\u2206, \u03f5)\u03b2 log t+ sp(V \u2217)2 min{dimDE(XH \u2212XH,D\u2206, \u03f5), t}+ t\u03f52\n) = O ( dimE(XH, \u03f5)\u03b2 log t+ sp(V \u2217)2 min{dimE(XH, \u03f5), t}+ t\u03f52 ) , (F.2)\nwhere the last equation is based on the fact that dimDE(XH\u2212XH,D\u2206, \u03f5) = dimE(XH, \u03f5) according to Definitions 5 and 7. Let dE = dimE(XH, \u03f5), then we have \u03baG \u2264 dE. To show function class with a low Eluder dimension also shares a low dominance coefficient, use Lemma 14 and it follows T\u2211 t=1 E(ft)(st, at)2 = T\u2211 t=1 \u2225E\u03b6t [ lft(ft, ft, \u03b6t) ] \u222522 = T\u2211 t=1 [( Xft,ft \u2212Xft,f\u2217 ) (st, at)\n]2 \u2264 [ dDE (1 + log T )\nT\u2211 t=1 t\u22121\u2211 i=1 [( Xft,ft \u2212Xft,f\u2217 ) (si, ai)\n]2]1/2 + sp(V \u2217)min{dDE, T}+ T\u03f5\n\u2264 [ dE (1 + log T )\nT\u2211 t=1 t\u22121\u2211 i=1 \u2225\u2225E\u03b6i [lfi(ft, ft, \u03b6i)]\u2225\u222522 ]1/2 + sp(V \u2217)min{dE, T}+ T\u03f5,\nby taking \u0393 = D\u2206, \u03d5t = Xft,ft \u2212 Xft,f\u2217 , \u03a6 = XH \u2212 XH, C \u2272 sp(V \u2217) and using the fact that 1 + log T \u2264 2 log T and the equivalence of dimDE(XH \u2212XH,D\u2206, \u03f5) and dimE(XH, \u03f5). 2"
        },
        {
            "heading": "F.2 PROOF OF LEMMA 2",
            "text": "Proof of Lemma 2. Consider the Bellman discrepancy function, defined as\nlf \u2032(f, g, \u03b6t) = Qg(st, at)\u2212 r(st, at)\u2212 Vf (st+1) + Jg, and the expectation is taken over the transition state si+1 from P(\u00b7|si, ai) such that\nt\u22121\u2211 i=1 \u2225E\u03b6i [lfi(ft, ft, \u03b6i)]\u2225 2 2 = t\u22121\u2211 i=1 E(fi)(si, ai)2, \u2200t \u2208 [T ].\nFirst, we\u2019re going to demonstrate the transferability. Note that the generalized pigeon-hole principle (see Lemma 13) directly indicates that, given \u2211t\u22121 i=1 \u2225E(ft)(si, ai)\u222522 \u2264 O(\u03b2) holds true for all t \u2208 [T ], if we take \u03d5t = E(ft), \u03a6 = (I \u2212 T )H and \u0393 = D\u2206, then we have t\u2211 i=1 \u2225E(fi)(si, ai)\u222522 \u2264 O ( dABE \u00b7 \u03b2 log t+ sp(V \u2217)2 min{dABE, t}+ t\u03f52 ) , \u2200t \u2208 [T ], (F.3)\nand thus we upper bound the transferability coefficient by \u03baG \u2264 dABE, where denote dABE = dimABE(H, \u03f5). To demonstrate that the function class H with a low ABE dimension also shares a low dominance coefficient, we can use Lemma 14, which directly provides the result: T\u2211 t=1 E(ft)(st, at) \u2264 [ 2dABE log T T\u2211 t=1 t\u22121\u2211 i=1 \u2225E(ft)(si, ai)\u222522 ]1/2 +O ( sp(V \u2217)min{dABE, T} ) + T\u03f5,\nwhere we takes \u03d5t = E(ft), \u03a6 = (I \u2212 T )H and \u0393 = D\u2206 for Lemma 14. Here, we also use the fact that 1+ log T \u2264 2 log T and \u2225E(ft)(s, a)\u22251 \u2272 sp(V \u2217) for all (s, a) \u2208 S \u00d7A. The inequality above demonstrates that the dominance coefficient is upper bounded by dG \u2264 2dABE \u00b7 log T . 2"
        },
        {
            "heading": "G PROOF OF RESULTS FOR CONCRETE EXAMPLES",
            "text": "In this section, we provide detailed proofs of results for concrete examples in Appendix D."
        },
        {
            "heading": "G.1 PROOF OF PROPOSITION 5",
            "text": "Proof of Proposition 5. To demonstrate that the linear FA has low AGEC, we will prove that it can be characterized by a low ABE dimension, denoted as dimABE(H, \u03f5), and the result can be directly drawn using Lemma 2. Recall definition of the ABE dimension, we assume that there exists constant \u03f5\u2032 \u2265 \u03f5, a sequence of distributions {\u03b4si,ai}i\u2208[m] \u2286 D\u2206 and hypotheses {fi}i\u2208[m] \u2286 H such that\u221a\u221a\u221a\u221at\u22121\u2211\ni=1\n[ E(ft)(si, ai) ]2 \u2264 \u03f5\u2032, \u2223\u2223E(ft)(st, at)\u2223\u2223 > \u03f5\u2032, \u2200t \u2208 [m], (G.1) based on Definitions 7-8. Next, we present a detailed discussion about the concrete problems, including linear AMDPs, AMDPs with linear Bellmen completion, and AMDPs with generalized linear completion defined in Definition 10-12. We assume that the feature mapping is all d-dimensional, and we first illustrate that these problems share a similar Bellman error structure.\n(i). Linear AMDPs. As defined in Definition 10, it implies that for any ft \u2208 H E(ft)(si, ai) = \u03d5(si, ai)\u22a4\u03c9t \u2212 \u03d5(si, ai)\u22a4\u03b8 \u2212 \u03d5(si, ai)\u22a4 \u222b S Vt(s)d\u00b5(s) + Jt\n= \u2329 \u03d5(si, ai), \u03c9t \u2212 \u03b8 + \u222b S Vt(s)d\u00b5(s) + Jte(1) \u232a , (G.2)\nwhere denotes e(1) = (1, 0, . . . , 0) \u2208 Rd.\n(ii). AMDPs with linear Bellmen completion. As a natural extension to the linear AMDPs, linear Bellmen completeness (see Definition 11) suggests that Bellman errors shall have the form:\nE(ft)(si, ai) = \u03d5(si, ai)\u22a4\u03c9t \u2212 \u03d5(si, ai)\u22a4TJt(\u03c9t) = \u2329 \u03d5(si, ai), \u03c9t \u2212 TJt(\u03c9t) \u232a . (G.3)\n(iii). AMDPs with generalized linear completion. Moreover, AMDPs with generalized linear completion (see Definition 12) further extends the standard linear FA by introducing the link function, and the \u03b1-bi-Lipschitz continuity ensures the explorability of the problem. Detailedly, we have\nE(ft)(si, ai) = \u03c3 ( \u03d5(si, ai) \u22a4\u03c9t ) \u2212 \u03c3 ( \u03d5(si, ai) \u22a4TJt(\u03c9t) )\n\u2208 [ 1 \u03b1 \u2329 \u03d5(si, ai),\u03c9t \u2212 TJt(\u03c9t) \u232a , \u03b1 \u2329 \u03d5(si, ai),\u03c9t \u2212 TJt(\u03c9t) \u232a] , (G.4)\nwhere the bound is based on the definition of \u03b1-bi-Lipschitz continuity in (D.4). Based on the Lemma 15 and arguments above, we; \u2019re ready to provide a unified proof for both the linear FA and generalized linear FA. If we substitute the arguments (G.2), (G.3) and (G.4) back into (G.1), we have\u221a\u221a\u221a\u221at\u22121\u2211\ni=1\n[\u27e8\u03d5(si, ai),\u03c9t \u2212 TJt(\u03c9t)\u27e9] 2 \u2264 \u03b1\u03f5\u2032, |\u27e8\u03d5(st, at),\u03c9t \u2212 TJt(\u03c9t)\u27e9| >\n\u03f5\u2032 \u03b1 , (G.5)\nfor all t \u2208 [T ]. Detailedly, we take \u03b1 = 1 for standard linear FA (e.g. linear AMPDs, AMDPs with linear Bellman completion), and denote \u03b1 as the Lipschitz continuity coefficient for generalized linear FA (e.g. AMDPs with generalized linear Bellman completion). Based on the d-upper bound lemma (see Lemma 15 for formal statement), if we take \u03d5t = \u03d5(st, at), \u03c8t = \u03c9t \u2212TJt(\u03c9t), B\u03d5 =\u221a 2, B\u03c8 = sp(V \u2217) \u221a d, \u03b5 = \u03f5, c1 = \u03b1, c2 = \u03b1\n\u22121, then the length of the sequence should be upper bounded by m \u2264 O ( d log(sp(V \u2217) \u221a d/\u03f5) ) with a d-dimensional feature mapping. Thus, we have\ndimABE(H, \u03f5) \u2264 O ( d log ( sp(V \u2217) \u221a d/\u03f5 )) ,\nas the ABE dimension is defined as the longest sequence satisfying the condition in (G.5). Based on Lemma 2, dG \u2264 O ( d log ( sp(V \u2217) \u221a d\u03f5\u22121 ) log T ) and \u03baG \u2264 O ( d log ( sp(V \u2217) \u221a d\u03f5\u22121 )) . 2"
        },
        {
            "heading": "G.2 PROOF OF PROPOSITION 6",
            "text": "Proof of Proposition 6. Consider the linear Q\u2217/V \u2217 AMDPs, recall that the definition of linear Q\u2217/V \u2217 indicates that the Bellman error can be written as below:\nE(ft)(si, ai) = \u03d5(si, ai)\u22a4\u03c9t \u2212 Esi+1\u223cP(\u00b7|si,ai)[\u03c8(si+1)] \u22a4\u03b8t + Jt \u2212 r(si, ai)\n= \u03d5(si, ai) \u22a4\u03c9t \u2212 Esi+1\u223cP(\u00b7|si,ai)[\u03c8(si+1)] \u22a4\u03b8t \u2212 ( Q\u2217(si, ai)\u2212 Esi+1\u223cP(\u00b7|si,ai)[V \u2217(si+1)]\u2212 J\u2217 ) + Jt (see (2.1))\n=\n\u2329[ \u03d5(si, ai)\nEsi+1\u223cP(\u00b7|si,ai)[\u03c8(si+1)]\n] , [ \u03c9t \u2212 \u03c9\u2217 \u03b8\u2217 \u2212 \u03b8t ] + (Jt \u2212 J\u2217)e(1) \u232a , (G.6)\nwhere \u03c9\u2217 \u2208 Rd and \u03b8\u2217 \u2208 Rd denote the true optimal parameter, and e(1) = (1, 0, . . . , 0) \u2208 Rd. Following a similar argument in the proof of Proposition 5, we can show that the linear Q\u2217/V \u2217 AMDPs have a low ABE dimension with an equivalent (d1 + d2)-dimensional compound feature mapping in (G.6). Based on Lemma 2, then we have it has low AGEC such that\ndG \u2264 O ( d log ( sp(V \u2217) \u221a d\u03f5\u22121 ) log T ) , \u03baG \u2264 O ( d log ( sp(V \u2217) \u221a d\u03f5\u22121 )) where denote d = d1 + d2 be the sum of dimensions. 2"
        },
        {
            "heading": "G.3 PROOF OF PROPOSITION 7",
            "text": "Proof of Proposition 7. Similar to linear FA, we will prove that the kernel FA can be characterized by a low ABE dimension, denoted as dimABE(H, \u03f5), and the result can be directly drawn from Lemma 2. Based on the definition of the ABE dimension, we assume that there exists constant \u03f5\u2032 \u2265 \u03f5, a sequence of distributions {\u03b4si,ai}i\u2208[m] \u2286 D\u2206 and hypotheses {fi}i\u2208[m] \u2286 H such that we have\u221a\u221a\u221a\u221at\u22121\u2211\ni=1\n[ E(ft)(si, ai) ]2 \u2264 \u03f5\u2032, \u2223\u2223E(ft)(st, at)\u2223\u2223 > \u03f5\u2032, \u2200t \u2208 [m]. (G.7) Suppose the kernel function class has a finite \u03f5-effective dimension concerning the feature mapping \u03d5. The existence of Bellman error E(ft) is equivalent to the one of Wt \u2208 (W \u2212W): E(ft)(\u00b7, \u00b7) = (Qft \u2212 TJtQft)(\u00b7, \u00b7) = \u27e8\u03d5(\u00b7, \u00b7),\u03c9t \u2212 \u03c9\u2032t\u27e9K := \u27e8\u03d5(\u00b7, \u00b7),Wt\u27e9K, (G.8) where the second equation is based on the self-completeness assumption with kernel FA such that G = H. Denote Xt = \u03d5(st, at), we can rewrite the condition in (G.7) as\u221a\u221a\u221a\u221at\u22121\u2211\ni=1\n(X\u22a4i Wt) 2 \u2264 \u03f5\u2032, |X\u22a4t Wt| > \u03f5\u2032, \u2200t \u2208 [m]. (G.9)\nDefine \u03a3t = \u2211t\u22121 i=1XiX \u22a4 i +{\u03f5\u20322/4R2 \u00b7sp(V \u2217)2}\u00b7I, then \u2225Wt\u2225\u03a3t \u2264 \u221a 2\u03f5\u2032 and \u03f5\u2032 \u2264 \u221a 2\u03f5\u2032 \u00b7\u2225Xt\u2225\u03a3\u22121t using the Cauchy-Swartz inequlity based on (G.9). Thus, we have \u2225Xt\u22252\u03a3\u22121t \u2265 0.5 and we have m\u2211 t=1 log ( 1 + \u2225Xt\u22252\u03a3\u22121t ) = log ( det[\u03a3m+1]/det[\u03a31]\n) = log det [ I+ 4R2 \u00b7 sp(V \u2217)2\n\u03f5\u20322\nm\u2211 t=1 XtX \u22a4 t ] , (G.10)\nbased on the matrix determinant lemma. Therefore, the (G.9) directly implies that\n1 e \u2264 log 3 2 \u2264 1 m log det\n[ I+ 4R2sp(V \u2217)2\n\u03f5\u20322\nm\u2211 t=1 XtX \u22a4 t ] \u21d2 m \u2264 dimeff ( X , \u03f5/2sp(V \u2217)R ) .\nRecall that the \u03f5-effective dimension is defined as the minimum positive integer satisfying the condition. As the ABE dimension is defined as the length of the longest sequence satisfying (G.9), then we can bound ABE dimension by dimABE(H, \u03f5) \u2264 dimeff ( X , \u03f5/2sp(V \u2217)R ) . Based on the Lemma\n2, we have dG \u2264 dimeff ( X , \u03f5/2sp(V \u2217)R ) log T and \u03baG \u2264 dimeff ( X , \u03f5/2sp(V \u2217)R ) . 2"
        },
        {
            "heading": "G.4 PROOF OF PROPOSITION 8",
            "text": "Proof of Proposition 8. Note that expected discrepancy function follows: for any t \u2208 [T ] \u2225E\u03b6i [lfi(ft, ft, \u03b6i)]\u22252 = \u03b8\u22a4t ( \u03c8(si, ai) + \u222b S \u03d5(si, ai, s \u2032)Vfi(s \u2032)ds\u2032 ) \u2212 r(si, ai)\u2212 E\u03b6i [Vfi(si+1)]\n= (\u03b8t \u2212 \u03b8\u2217)\u22a4 ( \u03c8(si, ai) + \u222b S \u03d5(si, ai, s \u2032)Vfi(s \u2032)ds\u2032 ) . (realizability)\nDenote \u03c9t = \u03b8t \u2212 \u03b8\u2217 and Xt = \u03c8(si, ai) + \u222b S \u03d5(si, ai, s\n\u2032)Vfi(s \u2032)ds\u2032, we define that \u03a3t = \u03f5I +\u2211t\u22121\ni=1XtX \u22a4 t given any constant \u03f5 > 0. Note that\n\u2225\u03b8t \u2212 \u03b8\u2217\u2225\u03a3t = [ \u03f5\u2225\u03b8t \u2212 \u03b8\u2217\u222522 + t\u22121\u2211 i=1 \u2225E\u03b6i [lfi(ft, ft, \u03b6i)]\u222522 ]1/2\n\u2264 2 \u221a \u03f5+ [ t\u22121\u2211 i=1 \u2225E\u03b6i [lfi(ft, ft, \u03b6i)]\u222522 ]1/2 (G.11)\nwhere we use \u2225\u03b8t\u2225 \u2264 1. Based on the elliptical potential lemma (see Lemma 17), we have T\u2211 t=1 \u2225\u2225Xt\u2225\u2225\u03a3\u22121t \u2227 1 \u2264 T\u2211 t=1 2d \u00b7 log ( 1 + d\u22121 T\u2211 t=1 \u2225Xt \u2225\u2225 2 )\n\u2264 2d \u00b7 log ( 1 + (1 + sp(V \u2217)/2) \u00b7 T (\u221a d\u03f5 )\u22121) := d(\u03f5), (G.12)\nwhere the last inequality results from scaling conditions \u2225\u03d5\u2225\u221e \u2264 \u221a d, \u2225\u03c8\u2225\u221e \u2264 \u221a d and \u2225Vf\u2225\u221e \u2264\n1 2 sp(V \u2217). Combine (G.11) and the fact that 1 ( \u2225Xt \u2225\u2225 \u03a3\u22121t \u2265 1 ) \u2264 \u2225Xt \u2225\u2225 \u03a3\u22121t \u2227 1, then it holds\nT\u2211 t=1 1 ( \u2225Xt \u2225\u2225 \u03a3\u22121t \u2265 1 ) \u2264 T\u2211 t=1 \u2225\u2225Xt\u2225\u2225\u03a3\u22121t \u2227 1 \u2264 d(\u03f5). (G.13) Dominance Note the sum of Bellman errors follows that\nT\u2211 t=1 E(ft)(st, at) = T\u2211 t=1 (( rft + PftVft ) (st, at)\u2212 ( rf\u2217 + Pf\u2217Vft ) (st, at) ) =\nT\u2211 t=1 (\u03b8t \u2212 \u03b8\u2217)\u22a4 ( \u03c8(st, at) + \u222b S \u03d5(st, at, s \u2032)Vft(s \u2032)ds\u2032 ) =\nT\u2211 t=1 \u03c9\u22a4t Xt \u00b7 ( 1 ( \u2225Xt \u2225\u2225 \u03a3\u22121t \u2264 1 ) + 1 ( \u2225Xt \u2225\u2225 \u03a3\u22121t > 1 ))\n\u2264 T\u2211 t=1 \u03c9\u22a4t Xt \u00b7 1 ( \u2225Xt \u2225\u2225 \u03a3\u22121t \u2264 1 ) + (sp(V \u2217) + 2) \u00b7min{d(\u03f5), T}\n\u2264 T\u2211 t=1 \u2225\u03c9t\u2225\u03a3t \u00b7 ( \u2225Xt \u2225\u2225 \u03a3\u22121t \u2227 1 ) + (sp(V \u2217) + 2) \u00b7min{d(\u03f5), T}, (G.14)\nwhere the first inequality uses (G.13) and the last inequality follows the Cauchy-Swartz inequality. Based on the inequality (G.11) and (G.12), we have\nT\u2211 t=1 \u2225\u03c9t\u2225\u03a3t \u00b7 ( \u2225Xt \u2225\u2225 \u03a3\u22121t \u2227 1 ) \u2264 T\u2211 t=1 2\u221a\u03f5+ [t\u22121\u2211 i=1 \u2225lfi(ft, ft, \u03b6i)\u222522 ]1/2 \u00b7 (\u2225Xi\u2225\u2225\u03a3\u22121i \u2227 1)\n\u2264 [ T\u2211 t=1 4\u03f5 ]1/2 [ T\u2211 t=1 \u2225Xi \u2225\u2225 \u03a3\u22121i \u2227 1 ]1/2 + [ d(\u03f5) T\u2211 t=1 t\u22121\u2211 i=1 \u2225lfi(ft, ft, \u03b6i)\u222522 ]1/2 \u2264 2 \u221a T\u03f5 \u00b7min{d(\u03f5), T}+ [ d(\u03f5)\nT\u2211 t=1 t\u22121\u2211 i=1 \u2225lfi(ft, ft, \u03b6i)\u222522\n]1/2 . (G.15)\nPlugging the result back into the inequality above, we conclude that\nT\u2211 t=1 E(ft)(st, at) \u2264\n[ d(\u03f5)\nT\u2211 t=1 t\u22121\u2211 i=1 \u2225E\u03b6i [lfi(ft, ft, \u03b6i)]\u222522 ]1/2 + 2 \u221a T\u03f5 \u00b7min{d(\u03f5), T}+ (sp(V \u2217) + 2)min{d(\u03f5), T}\n\u2264 [ d(\u03f5)\nT\u2211 t=1 t\u22121\u2211 i=1 \u2225E\u03b6i [lfi(ft, ft, \u03b6i)]\u222522\n]1/2 + (sp(V \u2217) + 3)min{d(\u03f5), T}+ T\u03f5,\n(G.16)\nwhere the last inequality follows AM-GM inequality. Then, dG \u2264 O(d log(sp(V \u2217)T/ \u221a d\u03f5)). Transferability Given \u2211t\u22121 i=1 \u2225E[lfi(ft, ft, \u03b6i)]\u222522 \u2264 \u03b2 for all t \u2208 [T ] and following the similar arguments in the proof of dominance, we have T\u2211 t=1 \u2225E[lft(ft, ft, \u03b6t)]\u222522 = T\u2211 t=1 [ (\u03b8i \u2212 \u03b8\u2217)\u22a4 ( \u03c8(st, at) + \u222b S \u03d5(st, at, s \u2032)Vfi(s \u2032)ds\u2032 )]2\n\u2264 T\u2211 t=1 (\u03c9\u22a4t Xt) 2 \u00b7 1 ( \u2225Xt \u2225\u22252 \u03a3\u22121t \u2264 1 ) + (sp(V \u2217) + 2)2 min{d(\u03f5), T}\n\u2264 T\u2211 i=1 (\u03b2 + 4\u03f5) \u00b7 ( \u2225Xt \u2225\u22252 \u03a3\u22121t \u2227 1 ) + (sp(V \u2217) + 2)2 min{d(\u03f5), T}\n\u2264 d(\u03f5) \u00b7 \u03b2 + (sp(V \u2217)2 + 4 \u00b7 sp(V \u2217) + 6)min{d(\u03f5), T}+ 2T\u03f52, (G.17) where we use a variant of (G.12) and (G.13), following that\nT\u2211 t=1 1 ( \u2225Xt \u2225\u22252 \u03a3\u22121t \u2265 1 ) \u2264 T\u2211 t=1 \u2225\u2225Xt\u2225\u22252\u03a3\u22121t \u2227 1 \u2264 T\u2211 t=1 \u2225\u2225Xt\u2225\u2225\u03a3\u22121t \u2227 1 \u2264 d(\u03f5), and the last inequality follows a similar proof as (G.15) and (G.16) based on Cauchy-Swartz and AM-GM inequality. Recall the definition of d(\u03f5) in (G.12), then \u03baG \u2264 O(d log(sp(V \u2217)T/ \u221a d\u03f5)).2"
        },
        {
            "heading": "G.5 DISCUSSION ABOUT PERFORMANCE ON CONCRETE EXAMPLES",
            "text": "In this subsection, we demonstrate the detailed performance of LOOP under specific scenarios. Note that the algorithm achieves an O\u0303( \u221a T ) regret, which is nearly minimax optimal, in both linear AMDP and linear mixture AMDP. We remark that existing algorithms were tailored for specific problems individually (Wei et al., 2021; Wu et al., 2022), LOOP offers a unified approach that covers them with comparable performance with an additional cost of generality for trade-off.\nLinear AMDP Recall that AMDP falls into the linear function class, defined asHLin = { (Q, J) :\nQ(\u00b7, \u00b7) = \u03c9\u22a4\u03d5(\u00b7, \u00b7) \u2223\u2223 \u2225\u03c9\u22252 \u2264 12 sp(V \u2217)\u221ad, J \u2208 J(H)}. Consider the \u03c1-covering number, note that |Q(s, a)\u2212Q\u2032(s, a)| \u2264 |(\u03c9 \u2212 \u03c9\u2032)\u22a4\u03d5(s, a)| \u2264 \u221a 2\u2225\u03c9 \u2212 \u03c9\u2032\u22251.\nBased on the Lemma 20, combine the fact that |J(H)| \u2264 2 and its \u03c1-covering number N\u03c1(J(H)) is at most 2\u03c1\u22121, we can get the log covering number of the hypotheses classHLin is upper bounded by\nlogNH(\u03c1) \u2264 d log ( sp(V \u2217)2 3 2 d 3 2 \u03c1\u22122 ) , (G.18)\nby taking \u03b1 = w, P = d, B = sp(V \u2217) \u221a d/2. Recall that the AGEC for linear function approximation is bounded in Proposition 5, and it holds that\ndG \u2264 O ( d log ( sp(V \u2217) \u221a d\u03c1\u22121 ) log T ) , \u03baG \u2264 O ( d log ( sp(V \u2217) \u221a d\u03c1\u22121 )) . (G.19)\nCombine (G.18), (G.19) and the regret guarantee shown in Theorem 3, we get Reg(T ) \u2264 O ( sp(V \u2217)max{dG, \u03baG} \u221a T log ( TN 2H\u222aG(1/T )/\u03b4 ) sp(V \u2217) ) \u2264 O\u0303 ( sp(V \u2217) 3 2 d 3 2 \u221a T ) . For linear AMDPs, our method achieves a regret bound of O\u0303(sp(V \u2217) 32 d 32 \u221a T ) for both linear and generalized linear AMDPs. In comparison, the FOPO algorithm (Wei et al., 2021) achieves the best-known regret bound of O\u0303(sp(V \u2217)d 32 \u221a T ) for linear AMDPs. However, our method incurs an additional sp(V \u2217) 1 2 in the regret bound, which is inevitable and is a trade-off for generalization.\nLinear mixture Recall that the Proposition 8 posits that AGEC of the linear mixture probelm satisfies that max{ \u221a dG, \u03baG} \u2264 O(d \u221a log T ). Note that the hypotheses class is defined as\nHLM = {(P, r) : P(\u00b7|s, a) = \u03b8\u22a4\u03d5(s, a, \u00b7), r(s, a) = \u03b8\u22a4\u03c8(s, a)| \u2225\u03b8\u22252 \u2264 1}. Consider the covering number note that both transition function and reward can be written as\n(i). |(P\u2212 P\u2032)(s\u2032|s, a)| = |(\u03b8 \u2212 \u03b8\u2032)\u22a4\u03d5(s, a, s\u2032)| \u2264 \u221a d \u00b7 \u2225\u03b8 \u2212 \u03b8\u2032\u22251, (ii). |(r \u2212 r\u2032)(s, a)| = |(\u03b8 \u2212 \u03b8\u2032)\u22a4\u03c8(s, a)| \u2264 \u221a d \u00b7 \u2225\u03b8 \u2212 \u03b8\u2032\u22251.\nBased on the Lemma 20, the log covering number of the hypotheses classHLM is upper bounded by logNH(\u03c1) \u2264 2d log ( d 3 2 \u03c1\u22121 ) ,\nby taking \u03b1 = \u03b8, P = d, B = 1. Combine results above and Theorem 3, we get Reg(T ) \u2264 O ( sp(V \u2217)max{dG, \u03baG} \u221a T log ( TN 2H\u222aG(1/T )/\u03b4 ) sp(V \u2217) ) \u2264 O\u0303 ( sp(V \u2217) 3 2 d 3 2 \u221a T ) .\nwhich shares the same rate of cumulative regret as linear AMDPs. At our best knowledge, the UCRL2-VTR (Wu et al., 2022) achieves the best O\u0303(Dd \u221a T ) regret for linear mixture AMDP, whereD denotes the diameter under the communicating AMDP assumptionand sp(V \u2217) \u2264 D (Wang et al., 2022). In comparison, our method achieves O\u0303(sp(V \u2217) 32 d 32 \u221a T ). The two algorithms are incomparable under different assumptions and both achieve a near minimax optimal regret at O\u0303( \u221a T )."
        },
        {
            "heading": "H TECHNICAL LEMMAS",
            "text": "In this section, we provide useful technical lemmas used in later theoretical analysis. Most are directly borrowed from existing works and proof of modified lemmas is provided in Section H.1. Lemma 12. Given function class \u03a6 defined on X , and a family of probability measures \u0393 over X . Suppose sequence {\u03d5k}Kk=1 \u2282 \u03a6 and {\u00b5k}Kk=1 \u2282 \u0393 satisfy that for all k \u2208 [K], \u2211k\u22121 t=1 (E\u00b5t [\u03d5k])2 \u2264 \u03b2. Then, for all k \u2208 [K], we have k\u2211 t=1 1 (\u2223\u2223E\u00b5t [\u03d5t]\u2223\u2223 > \u03f5) \u2264 ( \u03b2\u03f52 + 1)dimDE(\u03a6,\u03a0, \u03f5).\nProof. See Lemma 43 of Jin et al. (2021) for detailed proof. Lemma 13 (Pigeon-hole principle). Given function class \u03a6 defined on X with |\u03d5(x)| \u2264 C for all \u03d5 \u2208 \u03a6 and x \u2208 X , and a family of probability measure over X . Suppose sequence {\u03d5k}Kk=1 \u2282 \u03a6 and {\u00b5k}Kk=1 \u2282 \u0393 satisfy that for all k \u2208 [K], it holds \u2211k\u22121 t=1 (E\u00b5t [\u03d5k])2 \u2264 \u03b2. Let dDE = dimDE(\u03a6,\u0393, \u03f5) be the DE dimension, then for all k \u2208 [K] and \u03f5 > 0, we have k\u2211 t=1\n\u2223\u2223\u2223E\u00b5t [\u03d5t]\u2223\u2223\u2223 \u2264 O(\u221adDE\u03b2k +min{k, d}C + k\u03f5), and\nk\u2211 t=1 [ E\u00b5t [\u03d5t] ]2 \u2264 O ( dDE\u03b2 log k +min{k, d}C2 + k\u03f52 ) .\nProof. See Section H.1.1. Lemma 14. Given function class \u03a6 defined on X with |\u03d5(x)| \u2264 C for all \u03d5 \u2208 \u03a6 and x \u2208 X , and a family of probability measure over X . Let dDE = dimDE(\u03a6,\u0393, \u03f5) be the DE dimension, then for all k \u2208 [K] and \u03f5 > 0, we have\nk\u2211 t=1 \u2223\u2223\u2223E\u00b5k [\u03d5k]\u2223\u2223\u2223 \u2264 [ dDE (1 + logK) K\u2211 k=1 k\u22121\u2211 t=1 (E\u00b5t [\u03d5k])2 ]1/2 +min{dDE, k}C + k\u03f5.\nProof. See Section H.1.2. Lemma 15 (d-upper bound). Let \u03a6 and \u03a8 be sets of d-dimensional vectors and \u2225\u03d5\u22252 \u2264 B\u03d5, \u2225\u03c8\u22252 \u2264 B\u03c8 for any \u03d5 \u2208 \u03a6 and \u03c8 \u2208 \u03a8. If there exists set (\u03d51, . . . ,\u03d5m) and (\u03c81, . . . ,\u03c8m)such that for all t \u2208 [m], \u221a\u2211t\u22121 k=1\u27e8\u03d5t,\u03c8k\u27e92 \u2264 c1\u03b5 and |\u27e8\u03d5t,\u03c8t\u27e9| > c2\u03b5, where c1 \u2265 c2 > 0 is a constant\nand \u03b5 > 0, then the number of elements in set is bounded by m \u2264 O ( d log(B\u03d5B\u03c8/\u03b5) ) .\nProof. See Section H.1.3. Lemma 16. For any sequence of positive reals x1, . . . , xm, it holds that\u2211m\ni=1 xi\u221a\u2211m i=1 ix 2 i\n\u2264 \u221a 1 + log n.\nProof. See Lemma 6 in Dann et al. (2021) for detailed proof. Lemma 17. Let {xi}i\u2208[t] be a sequence of vectors defined over Hilbert space X . Let \u039b0 be a positive definite matrix and \u039bt = \u039b0 + \u2211t\u22121 i=1 xtx \u22a4 t . It holds that\nt\u2211 i=1 \u2225xt\u22252\u039b\u22121t \u2227 1 \u2264 2 log ( det(\u039bt+1) det(\u039b0) ) .\nProof. See Elliptical Potential Lemma (EPL) in Dani et al. (2008) for a detailed proof. Lemma 18 (Freedman\u2019s inequality). Let X1, . . . , XT be a real-valued martingale difference sequence adapted to filtration {Ft}Tt=1. Assume for all t \u2208 [T ] Xt \u2264 R, then for any \u03b7 \u2208 (0, 1/R), with probability greater than 1\u2212 \u03b4\nT\u2211 t=1 Xt \u2264 O ( \u03b7 T\u2211 t=1 E [ X2t |Ft ] + log(1/\u03b4) \u03b7 ) ,\nProof. See Lemma 7 in Agarwal et al. (2014) for detailed proof.\nLemma 19 (Scaling lemma). Let \u03d5 : S\u00d7A 7\u2192 Rd be a d-dimensional feature mapping, there exists an invertible linear transformation A \u2208 Rd\u00d7d such that for any bounded function f : S \u00d7 A 7\u2192 R and z \u2208 Rd defined by\nf(s, a) = \u03d5(s, a)\u22a4z,\nwe have \u2225A\u03d5(s, a)\u2225 \u2264 1 and \u2225A\u22121z\u2225 \u2264 sups,a|f | \u221a d for all (s, a) \u2208 S \u00d7A.\nProof. See Lemma 8 in Wei et al. (2021) for detailed proof.\nIn Theorem 3, the proved regret contains the logarithmic term of the 1/T -covering number of the function classesNH(1/T ), which can be regarded as a surrogate cardinality of the function classH. Here, we provide a formal definition of \u03c1-covering and the upper bound of \u03c1-covering number. Definition 16 (\u03c1-covering). The \u03c1-covering number of a function class F is the minimum integer t satisfying that there exists subset F \u2032 \u2286 F with |F \u2032| = t such that for any f \u2208 F we can find a correspondence f \u2032 \u2208 F \u2032 that it holds \u2225f \u2212 f \u2032\u2225\u221e \u2264 \u03c1. Lemma 20 (\u03c1-covering number). Let F be a function defined over X that can be parametrized by \u03b1 = (\u03b11, . . . , \u03b1P ) \u2208 RP with |\u03b1i| \u2264 B for all i \u2208 [P ]. Suppose that for any f, f \u2032 \u2208 F it holds that supx\u2208X |f(x)\u2212 f \u2032(x)| \u2264 L\u2225\u03b1\u2212\u03b1\u2032\u22251 and let NF (\u03c1) be the \u03c1-covering number of F , then\nlogNF (\u03c1) \u2264 P log (2BLP\n\u03c1\n) .\nProof. See Lemma 12 in Wei et al. (2021) for detailed proof."
        },
        {
            "heading": "H.1 PROOF OF TECHNICAL LEMMAS",
            "text": "In this subsection, we present the proofs of technical auxiliary lemmas with modifications."
        },
        {
            "heading": "H.1.1 PROOF OF LEMMA 13",
            "text": "Proof of Lemma 13. The first statement is directly from Lemma 41 in Jin et al. (2021), and the second statement follows a similar procedure as below. Note that Lemma 12 suggests that\nk\u2211 t=1 1 ([ E\u00b5t [\u03d5t] ]2 > \u03f52 ) \u2264 ( \u03b2 \u03f52 + 1 ) dimDE(\u03a6,\u0393, \u03f5),\nand note that the sum of squared expectation can be decomposed as k\u2211 t=1 [ E\u00b5t [\u03d5t] ]2 = k\u2211 t=1 [ E\u00b5t [\u03d5t] ]2 1 ([ E\u00b5t [\u03d5t] ]2 > \u03f52 ) + k\u2211 t=1 [ E\u00b5t [\u03d5t] ]2 1 ([ E\u00b5t [\u03d5t] ]2 \u2264 \u03f52)\n\u2264 k\u2211 t=1 [ E\u00b5t [\u03d5t] ]2 1 ([ E\u00b5t [\u03d5t] ]2 > \u03f52 ) + k\u03f52. (H.1)\nAssume sequence [ E\u00b51 [\u03d51] ]2 , . . . , [ E\u00b5k [\u03d5k] ]2 are sorted in the decreasing order and consider t \u2208\n[k] such that [E\u00b5t [\u03d5t] ]2 > \u03f52, there exists a constant \u03b1 \u2208 (\u03f52, [E\u00b5t [\u03d5t] ]2 ) satisfying\nt \u2264 k\u2211 i=1 1 ([ E\u00b5i [\u03d5i] ]2 > \u03b1 ) \u2264 (\u03b2 \u03b1 + 1 ) dimDE(\u03a6,\u0393, \u221a \u03b1) \u2264 (\u03b2 \u03b1 + 1 ) dimDE(\u03a6,\u0393, \u03f5),\nwhere the last inequality is based on the fact that the DE dimension is monotonically decreasing in terms of \u03f5 as proposed in Jin et al. (2021). Denote dDE = dimDE(\u03a6,\u0393, \u03f5) and the inequality above implies that \u03b1 \u2264 dDE\u03b2/t\u2212 d. Thus, we have [ E\u00b5t [\u03d5t] ]2 \u2264 dDE\u03b2/t\u2212 d. Beside, based on the definition we also have [ E\u00b5t [\u03d5t]\n]2 \u2264 C2 and thus [E\u00b5t [\u03d5t]]2 \u2264 min{dDE\u03b2/t\u2212 d,C2}, then k\u2211 t=1 [ E\u00b5t [\u03d5t] ]2 1 ([ E\u00b5t [\u03d5t] ]2 > \u03f52 ) \u2264 min{dDE, k}C2 + k\u2211 t=d+1 ( dDE\u03b2 t\u2212 dDE )\n\u2264 min{dDE, k}C2 + dDE \u00b7 \u03b2 \u222b k 0 1 t dt \u2264 min{dDE, k}C2 + dDE \u00b7 \u03b2 log k. (H.2)\nCombine (H.1) and (H.2), then finishes the proof. 2"
        },
        {
            "heading": "H.1.2 PROOF OF LEMMA 14",
            "text": "We remark that the proof provided in this subsection follows the almost same procedure as Lemma 3.16 in Zhong et al. (2022) with adjustment, and we preserve it for comprehension.\nProof of Lemma 14. Denote dDE = dimDE(\u03a6,\u0393, \u03f5), \u03f5\u0302t,k = |E\u00b5t [\u03d5k]| and \u03f5t,k = \u03f5\u0302t,k1(\u03f5\u0302t,k > \u03f5) for t, k \u2208 [K], \u00b5t \u2208 \u0393 and \u03d5k \u2208 \u03a6. The proof follows the procedure. Consider K empty buckets B0, . . . , BK\u22121 as initialization, and we examine \u03f5k,k one by one for all k \u2208 [K] as below:\nCase 1 If \u03f5k,k = 0, i.e., \u03f5\u0302k,k \u2264 \u03f5, then discard it. Case 2 If \u03f5k,k > 0, i.e., \u03f5\u0302k,k > \u03f5, at bucket j we add k into Bj if \u2211 t\u2264k\u22121,t\u2208Bj (\u03f5t,k)\n2 \u2264 (\u03f5k,k)2, otherwise we continue with the next bucket Bj+1.\nDenote by bk the index of bucket that at step k the non-zero \u03f5k,k falls in, i.e. k \u2208 Bbk . Based on the rule above, it holds that\nK\u2211 k=1 k\u22121\u2211 t=1 (\u03f5t,k) 2 \u2265 K\u2211 k=1 \u2211 0\u2264j\u2264bk\u22121,bk\u22651 \u2211 t\u2264k\u22121,t\u2208Bj (\u03f5t,k) 2 \u2265 K\u2211 k=1 bk \u00b7 (\u03f5k,k)2,\nwhere the first inequality arises from {t \u2208 Bj : t \u2264 k \u2212 1, 0 \u2264 j \u2264 bk \u2212 1, bk \u2265 1} \u2286 [k \u2212 1] due to the discarding of the bkth bucket, and the second equality directly follows the allocation rule such that \u2211 t\u2264k\u22121,t\u2208Bj (\u03f5t,k)\n2 \u2265 (\u03f5k,k)2 for any j \u2264 bk \u2212 1. Recall that based on the definition of distributional eluder (DE) dimension, it is suggested the size |Bj | is no larger than dDE. Then,\nK\u2211 k=1 bk(\u03f5k,k) 2 = K\u22121\u2211 j=1 j \u2211 t\u2208Bj (\u03f5t,t) 2 (re-summation) \u2265 K\u22121\u2211 j=1 j |Bj | \u2211 t\u2208Bj \u03f5t,t 2 \u2265 K\u22121\u2211 j=1 j dDE \u2211 t\u2208Bj \u03f5t,t\n2 (|Bj | \u2264 dDE) \u2265 (dDE (1 + logK))\u22121\nK\u22121\u2211 j=1 \u2211 t\u2208Bj \u03f5t,t 2 = (dDE (1 + logK))\u22121  \u2211 t\u2208[K]\\B0 \u03f5t,t 2 , (H.3) where the second inequality follows Lemma 16. Combine the (H.1.2) and (H.3) above, we have\nK\u2211 k=1 \u03f5\u0302k,k \u2264 K\u2211 k=1 \u03f5k,k +K\u03f5 \u2264 \u2211\nt\u2208[K]\\B0\n\u03f5t,t +min{dDE,K}\u2225\u03d5\u2225\u221e +K\u03f5\n\u2264 [ dDE (1 + logK)\nK\u2211 k=1 k\u22121\u2211 t=1 (\u03f5t,k) 2\n]1/2 +min{dDE,K}C +K\u03f5\n\u2264 [ dDE (1 + logK)\nK\u2211 k=1 k\u22121\u2211 t=1 (\u03f5\u0302t,k) 2\n]1/2 +min{dDE,K}C +K\u03f5.\nSubstitute the definition \u03f5\u0302t,k = |E\u00b5t [\u03d5k]| back into the inequality, then finishes the proof. 2"
        },
        {
            "heading": "H.1.3 PROOF OF LEMMA 15",
            "text": "Proof of Lemma 15. For notation simplicity, denote \u039bt = \u2211t\u22121 k=1\u03c8t\u03c8 \u22a4 t + \u03b52\nB2\u03d5 \u00b7 I, then for all t \u2208 [m] we have \u2225\u03d5t\u2225\u039bt \u2264 \u221a\u2211t\u22121 k=1(\u03d5 \u22a4 t \u03c8k) 2 + \u03b5 2\nB2\u03d5 \u2225\u03d5t\u222522 =\n\u221a c21 + 1 \u03b5 based on the given condition. Us-\ning the Cauchy-Swartz inequality and results above, then it holds \u2225\u03c8t\u2225\u039b\u22121t \u2265 |\u27e8\u03d5t,\u03c8t\u27e9|/\u2225\u03d5t\u2225\u039bt =\nc2/ \u221a c21 + 1. On one hand, the matrix determinant lemma ensures that\ndet(\u039bm) = det(\u039b0) \u00b7 m\u22121\u220f t=1 ( 1 + \u2225\u03c8t\u22252\u039b\u22121t ) \u2265 ( 1 + c22/1 + c 2 1 )m\u22121( \u03b52/B2\u03d5 )d . (H.4)\nOn the other hand, according to the definition of \u039bt, we have\ndet(\u039bm) \u2264 (trace(\u039bm)/d)d \u2264 ( t\u22121\u2211 k=1 \u2225\u03c8k\u222522/d+ \u03b52/B2\u03d5 )d \u2264 ( B2\u03c8(m\u2212 1)/d+ \u03b52/B2\u03d5 )d .\n(H.5) Combine (H.4) and (H.5), if we take logarithms at both sides, then we have\nm \u2264 1 + d log\n( B2\u03d5B\n2 \u03c8(m\u2212 1) d\u03b52 + 1\n)/ log ( 1 +\nc22 1 + c21\n) .\nAfter simple calculations, we can obtain that m is upper bounded by O ( d log(B\u03d5B\u03c8/\u03b5) ) . 2"
        },
        {
            "heading": "I SUPPLEMENTARY DISCUSSIONS",
            "text": ""
        },
        {
            "heading": "I.1 PROOF SKETCH OF MLE-BASED RESULTS",
            "text": "In this section, we provide the proof sketch of Theorem 4. First, we introduce several useful lemmas, which is the variant of ones in Appendix E for MLE-based problems, and most have been fully researched in Liu et al. (2022; 2023a); Xiong et al. (2023). As there\u2019s no significant technical gap between the episodic and average-reward for model-based problems, we only provide a sketch.\nLemma 21 (Akin to Lemma 9). Under Assumptions 1-2, MLE-LOOP is an optimistic algorithm such that it ensures Jt \u2265 J\u2217 for all t \u2208 [T ] with probability greater than 1\u2212 \u03b4.\nProof Sketch of Lemma 21. See Proposition 13 in Liu et al. (2022) with slight modifications. 2\nLemma 22 (Akin to Lemma 10). For fixed \u03c1 > 0 and pre-determined optimistic parameter \u03b2 = c(log ( TBH(\u03c1)/\u03b4) + T\u03c1 ) where constant c > 0, it holds that\nt\u22121\u2211 i=1 \u2225E\u03b6i [lfi(ft, ft, \u03b6i)]\u222522 = t\u22121\u2211 i=1 TV ( Pft(\u00b7|si, ai),Pf\u2217(\u00b7|si, ai) )2 \u2264 O(\u03b2), (I.1) for all t \u2208 [T ] with probability greater than 1\u2212 \u03b4.\nProof Sketch of Lemma 22. See Proposition 14 in Liu et al. (2022) with slight modifications. 2\nLemma 23 (Akin to Lemma 11). Let N (T ) be the switching cost with time horizon T , given fixed covering coefficient \u03c1 > 0 and pre-determined optimistic parameter \u03b2 = c ( log ( TBH(\u03c1)/\u03b4 ) +T\u03c1 ) where c is a large enough constant, with probability greater than 1\u2212 2\u03b4 we have\nN (T ) \u2264 O ( \u03baG \u00b7 poly(log T ) + \u03b2\u22121T log T\u03f52 ) ,\nwhere \u03baG is the transferability coefficient with respect to MLE-AGEC(H, {lf \u2032}, \u03f5).\nProof Sketch of Lemma 23. The proof is almost the same as Lemma 11.\nStep 1: Bound the difference of discrepancy between the minimizer and f\u2217. As proposed in Proposition 14, Liu et al. (2022), with a high probability it holds that\n0 \u2264 t\u2211 i=1 TV ( Pf\u2217(\u00b7|si, ai),Pgi(\u00b7|si, ai) ) \u2264 \u221a \u03b2t, \u2200t \u2208 [T ]. (I.2)\nAlgorithm 3 Extended Value Iteration (EVI) Input: hypothesis f = (Pf , rf ), desired accuracy level \u03f5. Initialize: V (0)(s) = 0 for all s \u2208 S, J (0) = 0 and counter i = 0.\n1: repeat 2: for s \u2208 S and a \u2208 A do 3: Set Q(i)(s, a)\u2190 rf (s, a) + Es\u2032\u223cPf (s,a)[V (i)(s\u2032)]\u2212 J (i) 4: Update V (i+1)(s)\u2190 maxa\u2208AQ(i)(s, a) 5: Update counter i\u2190 i+ 1 6: until maxs\u2208S{V (i+1)(s)\u2212 V (i)(s)} \u2212mins\u2208S{V (i+1)(s)\u2212 V (i)(s)} \u2264 \u03f5\nStep 2: Bound the expected discrepancy between updates. Note that for all t+ 1 \u2208 [T ], the update happens only if\nt\u2211 i=1 TV ( Pft(\u00b7|si, ai),Pgi(\u00b7|si, ai) ) > 3 \u221a \u03b2t. (I.3)\nCombine the (I.2) and (I.3) above, and apply the triangle inequality, we have\nt\u2211 i=1 TV ( Pft(\u00b7|si, ai),Pf\u2217(\u00b7|si, ai) ) \u2265\nt\u2211 i=1 TV ( Pft(\u00b7|si, ai),Pgt(\u00b7|si, ai) ) \u2212 TV ( Pf\u2217(\u00b7|si, ai),Pgt(\u00b7|si, ai) ) \u2265 2 \u221a \u03b2t.\nand the construction of confidence set ensures that \u2211\u03c4t i=1 TV ( Pft(\u00b7|si, ai),Pf\u2217(\u00b7|si, ai) ) \u2264 \u221a \u03b2t with high probability (Liu et al., 2022, Proposition 14). Recall the definition of MLE-transferability coefficient, then the switching cost can be bounded following the same argument in Lemma 11. 2\nProof Sketch of Theorem 4. Recall that\nReg(T ) \u2264 T\u2211 i=1\nE(ft)(st, at)\ufe38 \ufe37\ufe37 \ufe38 Bellman error +\nT\u2211 t=1 ( Est+1\u223cP(\u00b7|st,at)[Vt(st+1)]\u2212 Vt(st) ) \ufe38 \ufe37\ufe37 \ufe38\nRealization error\n, (I.4)\nwhere the inequality follows the optimism in Lemma 21. Combine Lemma 22, Lemma 23 and the definition of MLE-AGEC (see Definition 9), we can easily finish the proof of regret. 2"
        },
        {
            "heading": "I.2 EXTENDED VALUE ITERATION (EVI) FOR MODEL-BASED HYPOTHESES",
            "text": "In model-based problems, the discrepancy function sometimes relies on the optimal state bias function Vf and optimal average-reward Jf (see linear mixture model in Section D). In this section, we provide an algorithm, extended value iteration (EVI) proposed in Auer et al. (2008), to output the optimal function and average-reward under given a model-based hypothesis f = (Pf , rf ). See Algorithm 3 for complete pseudocode. The convergence of EVI is guaranteed by the theorem below. Theorem 24. UnderAssumption 1, there exists a unique centralized solution pair (Q\u2217, J\u2217) to the Bellman optimality equation for any AMDPMf characterized by hypothesis f \u2208 H. Then, if the extended value iteration (EVI) is stopped under the condition that\nmax s\u2208S {V (i+1)(s)\u2212 V (i)(s)} \u2212min s\u2208S {V (i+1)(s)\u2212 V (i)(s)} \u2264 \u03f5,\nthen the achieved greedy policy \u03c0(i) is \u03f5-optimal such that J\u03c0 (i) Mf \u2265 J \u2217 Mf + \u03f5.\nProof Sketch: See Theorem 12 in Auer et al. (2008)."
        }
    ],
    "year": 2023
}