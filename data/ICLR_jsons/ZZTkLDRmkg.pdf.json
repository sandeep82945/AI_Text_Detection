{
    "abstractText": "Elliptic partial differential equations (PDEs) are a major class of time-independent PDEs that play a key role in many scientific and engineering domains such as fluid dynamics, plasma physics, and solid mechanics. Recently, neural operators have emerged as a promising technique to solve elliptic PDEs more efficiently by directly mapping the input to solutions. However, existing networks typically cannot handle complex geometries and inhomogeneous boundary values present in the real world. Here we introduce Boundary-Embedded Neural Operators (BENO), a novel neural operator architecture that embeds the complex geometries and inhomogeneous boundary values into the solving of elliptic PDEs. Inspired by classical Green\u2019s function, BENO consists of two branches of Graph Neural Networks (GNNs) for interior source term and boundary values, respectively. Furthermore, a Transformer encoder maps the global boundary geometry into a latent vector which influences each message passing layer of the GNNs. We test our model extensively in elliptic PDEs with various boundary conditions. We show that all existing baseline methods fail to learn the solution operator. In contrast, our model, endowed with boundary-embedded architecture, outperforms state-of-the-art neural operators and strong baselines by an average of 60.96%. Our source code can be found https://github.com/AI4Science-WestlakeU/beno.git.",
    "authors": [
        {
            "affiliations": [],
            "name": "Haixin Wang"
        },
        {
            "affiliations": [],
            "name": "Jiaxin Li"
        },
        {
            "affiliations": [],
            "name": "Anubhav Dwivedi"
        },
        {
            "affiliations": [],
            "name": "Kentaro Hara"
        },
        {
            "affiliations": [],
            "name": "Tailin Wu"
        }
    ],
    "id": "SP:e39a0406839866c18908336fb18a52e579c99ac6",
    "references": [
        {
            "authors": [
                "Filipe De Avila Belbute-Peres",
                "Thomas Economon",
                "Zico Kolter"
            ],
            "title": "Combining differentiable pde solvers and graph neural networks for fluid flow prediction",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Marshall Bern",
                "David Eppstein"
            ],
            "title": "Mesh generation and optimal triangulation",
            "venue": "In Computing in Euclidean geometry,",
            "year": 1995
        },
        {
            "authors": [
                "Johannes Brandstetter",
                "Daniel Worrall",
                "Max Welling"
            ],
            "title": "Message passing neural pde solvers",
            "venue": "arXiv preprint arXiv:2202.03376,",
            "year": 2022
        },
        {
            "authors": [
                "Francis F Chen"
            ],
            "title": "Introduction to Plasma Physics and Controlled Fusion (3rd Ed.)",
            "year": 2016
        },
        {
            "authors": [
                "Ivan Dimov",
                "Istv\u00e1n Farag\u00f3",
                "Lubin Vulkov"
            ],
            "title": "Finite difference methods, theory and applications",
            "year": 2015
        },
        {
            "authors": [
                "Stefan Elfwing",
                "Eiji Uchibe",
                "Kenji Doya"
            ],
            "title": "Sigmoid-weighted linear units for neural network function approximation in reinforcement learning",
            "venue": "Neural networks,",
            "year": 2018
        },
        {
            "authors": [
                "Moshe Eliasof",
                "Eldad Haber",
                "Eran Treister"
            ],
            "title": "Pde-gcn: Novel architectures for graph neural networks motivated by partial differential equations",
            "venue": "Advances in neural information processing systems,",
            "year": 2021
        },
        {
            "authors": [
                "Matthias Fey",
                "Jan Eric Lenssen"
            ],
            "title": "Fast Graph Representation Learning with PyTorch Geometric",
            "year": 2019
        },
        {
            "authors": [
                "Han Gao",
                "Matthew J Zahr",
                "Jian-Xun Wang"
            ],
            "title": "Physics-informed graph neural galerkin networks: A unified framework for solving pde-governed forward and inverse problems",
            "venue": "Computer Methods in Applied Mechanics and Engineering,",
            "year": 2022
        },
        {
            "authors": [
                "Justin Gilmer",
                "Samuel S Schoenholz",
                "Patrick F Riley",
                "Oriol Vinyals",
                "George E Dahl"
            ],
            "title": "Neural message passing for quantum chemistry",
            "venue": "In International conference on machine learning,",
            "year": 2017
        },
        {
            "authors": [
                "John Guibas",
                "Morteza Mardani",
                "Zongyi Li",
                "Andrew Tao",
                "Anima Anandkumar",
                "Bryan Catanzaro"
            ],
            "title": "Adaptive fourier neural operators: Efficient token mixers for transformers",
            "venue": "arXiv preprint arXiv:2111.13587,",
            "year": 2021
        },
        {
            "authors": [
                "Gaurav Gupta",
                "Xiongye Xiao",
                "Paul Bogdan"
            ],
            "title": "Multiwavelet-based operator learning for differential equations",
            "venue": "Advances in neural information processing systems,",
            "year": 2021
        },
        {
            "authors": [
                "Gaurav Gupta",
                "Xiongye Xiao",
                "Radu Balan",
                "Paul Bogdan"
            ],
            "title": "Non-linear operator approximations for initial value problems",
            "venue": "In International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Jacob Helwig",
                "Xuan Zhang",
                "Cong Fu",
                "Jerry Kurtin",
                "Stephan Wojtowytsch",
                "Shuiwang Ji"
            ],
            "title": "Group equivariant fourier neural operators for partial differential equations",
            "venue": "arXiv preprint arXiv:2306.05697,",
            "year": 2023
        },
        {
            "authors": [
                "C. Hirsch"
            ],
            "title": "Numerical computation of internal and external flows: The fundamentals of computational fluid dynamics",
            "year": 2007
        },
        {
            "authors": [
                "K Ho-Le"
            ],
            "title": "Finite element mesh generation methods: a review and classification",
            "venue": "Computer-aided design,",
            "year": 1988
        },
        {
            "authors": [
                "T.J.R. Hughes"
            ],
            "title": "The finite element method: linear static and dynamic finite element analysis",
            "venue": "Courier Corporation,",
            "year": 2012
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "arXiv preprint arXiv:1412.6980,",
            "year": 2014
        },
        {
            "authors": [
                "Thomas N Kipf",
                "Max Welling"
            ],
            "title": "Semi-supervised classification with graph convolutional networks",
            "venue": "arXiv preprint arXiv:1609.02907,",
            "year": 2016
        },
        {
            "authors": [
                "Der-Tsai Lee",
                "Bruce J Schachter"
            ],
            "title": "Two algorithms for constructing a delaunay triangulation",
            "venue": "International Journal of Computer & Information Sciences,",
            "year": 1980
        },
        {
            "authors": [
                "Jae Yong Lee",
                "Seungchan Ko",
                "Youngjoon Hong"
            ],
            "title": "Finite element operator network for solving parametric pdes",
            "venue": "arXiv preprint arXiv:2308.04690,",
            "year": 2023
        },
        {
            "authors": [
                "R.J. LeVeque"
            ],
            "title": "Finite difference methods for ordinary and partial differential equations: steady-state and time-dependent problems",
            "year": 2007
        },
        {
            "authors": [
                "Zongyi Li",
                "Nikola Kovachki",
                "Kamyar Azizzadenesheli",
                "Burigede Liu",
                "Kaushik Bhattacharya",
                "Andrew Stuart",
                "Anima Anandkumar"
            ],
            "title": "Fourier neural operator for parametric partial differential equations",
            "venue": "arXiv preprint arXiv:2010.08895,",
            "year": 2020
        },
        {
            "authors": [
                "Zongyi Li",
                "Nikola Kovachki",
                "Kamyar Azizzadenesheli",
                "Burigede Liu",
                "Kaushik Bhattacharya",
                "Andrew Stuart",
                "Anima Anandkumar"
            ],
            "title": "Neural operator: Graph kernel network for partial differential equations",
            "venue": "arXiv preprint arXiv:2003.03485,",
            "year": 2020
        },
        {
            "authors": [
                "Zongyi Li",
                "Nikola Kovachki",
                "Kamyar Azizzadenesheli",
                "Burigede Liu",
                "Andrew Stuart",
                "Kaushik Bhattacharya",
                "Anima Anandkumar"
            ],
            "title": "Multipole graph neural operator for parametric partial differential equations",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Zongyi Li",
                "Daniel Zhengyu Huang",
                "Burigede Liu",
                "Anima Anandkumar"
            ],
            "title": "Fourier neural operator with learned deformations for pdes on general geometries",
            "venue": "arXiv preprint arXiv:2207.05209,",
            "year": 2022
        },
        {
            "authors": [
                "Marten Lienen",
                "Stephan G\u00fcnnemann"
            ],
            "title": "Learning the dynamics of physical systems from sparse observations with finite element networks",
            "venue": "arXiv preprint arXiv:2203.08852,",
            "year": 2022
        },
        {
            "authors": [
                "Ilya Loshchilov",
                "Frank Hutter"
            ],
            "title": "Sgdr: Stochastic gradient descent with warm restarts",
            "venue": "arXiv preprint arXiv:1608.03983,",
            "year": 2016
        },
        {
            "authors": [
                "Winfried L\u00f6tzsch",
                "Simon Ohler",
                "Johannes S Otterbach"
            ],
            "title": "Learning the solution operator of boundary value problems using graph neural networks",
            "venue": "arXiv preprint arXiv:2206.14092,",
            "year": 2022
        },
        {
            "authors": [
                "Lu Lu",
                "Pengzhan Jin",
                "George Em Karniadakis"
            ],
            "title": "Deeponet: Learning nonlinear operators for identifying differential equations based on the universal approximation theorem of operators",
            "year": 1910
        },
        {
            "authors": [
                "Xiao Luo",
                "Haixin Wang",
                "Zijie Huang",
                "Huiyu Jiang",
                "Abhijeet Sadashiv Gangan",
                "Song Jiang",
                "Yizhou Sun"
            ],
            "title": "Care: Modeling interacting dynamics under temporal environmental variation",
            "venue": "In Thirty-seventh Conference on Neural Information Processing Systems,",
            "year": 2023
        },
        {
            "authors": [
                "Fionn Murtagh"
            ],
            "title": "Multilayer perceptrons for classification and regression",
            "year": 1991
        },
        {
            "authors": [
                "SGOPAL Patro",
                "Kishore Kumar Sahu"
            ],
            "title": "Normalization: A preprocessing stage",
            "venue": "arXiv preprint arXiv:1503.06462,",
            "year": 2015
        },
        {
            "authors": [
                "Tobias Pfaff",
                "Meire Fortunato",
                "Alvaro Sanchez-Gonzalez",
                "Peter W Battaglia"
            ],
            "title": "Learning meshbased simulation with graph networks",
            "venue": "arXiv preprint arXiv:2010.03409,",
            "year": 2020
        },
        {
            "authors": [
                "Alfio Quarteroni",
                "Alberto Valli"
            ],
            "title": "Numerical approximation of partial differential equations, volume 23",
            "venue": "Springer Science & Business Media,",
            "year": 2008
        },
        {
            "authors": [
                "B\u00e9atrice Rivi\u00e8re"
            ],
            "title": "Discontinuous Galerkin Methods for Solving Elliptic and Parabolic Equations",
            "venue": "Society for Industrial and Applied Mathematics,",
            "year": 2008
        },
        {
            "authors": [
                "Nadim Saad",
                "Gaurav Gupta",
                "Shima Alizadeh",
                "Danielle C Maddix"
            ],
            "title": "Guiding continuous operator learning through physics-based boundary constraints",
            "venue": "arXiv preprint arXiv:2212.07477,",
            "year": 2022
        },
        {
            "authors": [
                "Y. Saad"
            ],
            "title": "Iterative methods for sparse linear systems",
            "year": 2003
        },
        {
            "authors": [
                "Alvaro Sanchez-Gonzalez",
                "Nicolas Heess",
                "Jost Tobias Springenberg",
                "Josh Merel",
                "Martin Riedmiller",
                "Raia Hadsell",
                "Peter Battaglia"
            ],
            "title": "Graph networks as learnable physics engines for inference and control",
            "venue": "In International Conference on Machine Learning,",
            "year": 2018
        },
        {
            "authors": [
                "Alvaro Sanchez-Gonzalez",
                "Jonathan Godwin",
                "Tobias Pfaff",
                "Rex Ying",
                "Jure Leskovec",
                "Peter Battaglia"
            ],
            "title": "Learning to simulate complex physics with graph networks",
            "venue": "In International conference on machine learning,",
            "year": 2020
        },
        {
            "authors": [
                "Ivar Stakgold",
                "Michael J Holst"
            ],
            "title": "Green\u2019s functions and boundary value problems",
            "year": 2011
        },
        {
            "authors": [
                "Alasdair Tran",
                "Alexander Mathews",
                "Lexing Xie",
                "Cheng Soon Ong"
            ],
            "title": "Factorized fourier neural operators",
            "venue": "arXiv preprint arXiv:2111.13802,",
            "year": 2021
        },
        {
            "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N Gomez",
                "\u0141ukasz Kaiser",
                "Illia Polosukhin"
            ],
            "title": "Attention is all you need",
            "venue": "Advances in neural information processing systems,",
            "year": 2017
        },
        {
            "authors": [
                "Xiongye Xiao",
                "Defu Cao",
                "Ruochen Yang",
                "Gaurav Gupta",
                "Gengshuo Liu",
                "Chenzhong Yin",
                "Radu Balan",
                "Paul Bogdan"
            ],
            "title": "Coupled multiwavelet neural operator learning for coupled partial differential equations",
            "venue": "arXiv preprint arXiv:2303.02304,",
            "year": 2023
        },
        {
            "authors": [
                "Ling Yang",
                "Shenda Hong"
            ],
            "title": "Omni-granular ego-semantic propagation for self-supervised graph representation learning",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Ling Yang",
                "Ye Tian",
                "Minkai Xu",
                "Zhongyi Liu",
                "Shenda Hong",
                "Wei Qu",
                "Wentao Zhang",
                "Bin Cui",
                "Muhan Zhang",
                "Jure Leskovec"
            ],
            "title": "Vqgraph: Rethinking graph representation space for bridging gnns and mlps",
            "venue": "In International Conference on Learning Representations,",
            "year": 2024
        },
        {
            "authors": [
                "Qingqing Zhao",
                "David B Lindell",
                "Gordon Wetzstein"
            ],
            "title": "Learning to solve pde-constrained inverse problems with graph networks",
            "venue": "arXiv preprint arXiv:2206.00711,",
            "year": 2022
        },
        {
            "authors": [
                "u(x"
            ],
            "title": "Each Poisson run generates two files: one for the interior cells with discrete values of x, y, f , and u and the other for the boundary interfaces with discrete values of x, y, and g. The simulations are performed on the Sherlock cluster at Stanford University",
            "venue": "D MORE IMPLEMENTATION DETAILS",
            "year": 2003
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Partial differential equations (PDEs), which include elliptic, parabolic, and hyperbolic types, play a fundamental role in diverse fields across science and engineering. For all types of PDEs, but especially for elliptic PDEs, the treatment of boundary conditions plays an important role in the solutions. In particular, the Laplace and Poisson equations constitute prime examples of linear elliptic PDEs, which are used in a wide range of disciplines, including solid mechanics (Rivie\u0300re, 2008), plasma physics (Chen, 2016), and fluid dynamics (Hirsch, 2007).\nRecently, neural operators have emerged as a promising tool for solving elliptic PDEs by directly mapping input to solutions (Li et al., 2020b;c;a; Lo\u0308tzsch et al., 2022). Lowering the computation efforts makes neural operators more attractive compared with classical approaches like finite element methods (FEM) (Quarteroni & Valli, 2008) and finite difference methods (FDM) (Dimov et al., 2015). However, existing neural operators have not essentially considered the influence of boundary conditions on solving elliptic PDEs. A distinctive feature of elliptic PDEs is their sensitivity to boundary conditions, which can heavily influence the behavior of solutions.\nIn fact, boundary conditions pose two major challenges for neural operators in terms of inhomogeneous boundary values and complex boundary geometry. First, inhomogeneous boundary conditions can cause severe fluctuations in the solution, and have a distinctive influence on the solution compared to the interior source terms. For example, as shown in Fig. 1, the inhomogeneous boundary\n\u2217Equal contribution. Work done as an intern at Westlake University. \u2020Corresponding author.\nvalues cause high-frequency fluctuations in the solution especially near the boundary, which make it extremely hard to learn. Second, since elliptic PDEs are boundary value problems whose solution describes the steady-state of the system, any variation in the boundary geometry and values would influence the interior solution globally (Hirsch, 2007). The above challenges need to be properly addressed to develop a neural operator suitable for more general and realistic settings.\nIn this paper, we propose Boundary-Embedded Neural Operators (BENO), a novel neural operator architecture to address the above two key challenges. Inspired by classical Green\u2019s function, BENO consists of two Graph Neural Networks (GNNs) that model the boundary influence and the interior source terms, respectively, addressing the first challenge. Moreover, to model the global influence of the boundary to the solution, we employ a Transformer (Vaswani et al., 2017) to encode the full boundary information to a latent vector and feed it to each message passing layer of the GNNs. This captures how the global geometry and values of the boundary influence the pairwise interaction between interior points, addressing the second challenge. As a whole, BENO provides a simple architecture for solving elliptic PDEs with complex boundary conditions, incorporating physics intuition into its boundary-embedded architecture. In Table 1, we provide a comparison between BENO and prior deep learning methods for elliptic PDE solving.\nTable 1: Comparison of data-driven methods to time-independent elliptic PDE solving.\nMethods 1. PDE-agnostic prediction on new initial condition 2. Train/Test space grid independence 3. Evaluation at unobserved spatial locations 4. Free-form spatial domain for boundary shape 5. Inhomogeneous boundary condition value\nGKN (Li et al., 2020b) \u2713 \u2713 \u2713 % % FNO (Li et al., 2020a) \u2713 % \u2713 % % GNN-PDE (Lo\u0308tzsch et al., 2022) \u2713 \u2713 % \u2713 % MP-PDE (Brandstetter et al., 2022) \u2713 % % % %\nBENO (ours) \u2713 \u2713 \u2713 \u2713 \u2713\nTo fully evaluate our model on inhomogeneous boundary value problems, we construct a novel dataset encompassing various boundary shapes, different boundary values, different types of boundary conditions, and varying resolutions. The experimental results demonstrate that our approach not only outperforms the existing state-of-the-art methods by about an average of 60.96% in solving elliptic PDEs problems but also exhibits excellent generalization capabilities in other scenarios. In contrast, all existing baselines fail to learn solution operators for the above challenging elliptic PDEs."
        },
        {
            "heading": "2 PROBLEM SETUP",
            "text": "In this work, we consider the solution of elliptic PDEs in a compact domain subject to inhomogeneous boundary conditions along the domain boundary. Let u \u2208 Cd(R) be a d-dimnesion-differentiable function of N interior grid nodes over an open domain \u2126. Specifically, we consider the Poisson equation with Dirichlet (and Neumann in Appendix K) boundary conditions in a d-dimensional domain, and we consider d = 2 in the following experiments:\n\u22072u ([x1, x2, . . . , xd]) = f ([x1, x2, . . . , xd]) , \u2200 ([x1, x2, . . . , xd]) \u2208 \u2126, u ([x1, x2, . . . , xd]) = g ([x1, x2, . . . , xd]) , \u2200 ([x1, x2, . . . , xd]) \u2208 \u2202\u2126,\n(1)\nwhere f and g are sufficiently smooth function defined on the domain \u2126 = {(x1,i, x2,i, . . . , xd,i)}Ni=1, and boundary \u2202\u2126, respectively. Eq. 1 is utilized in a range of applications in science and engineering to describe the equilibrium state, given by f in the presence of time-independent boundary constraints specified by g. A distinctive feature of elliptic PDEs is their sensitivity to boundary values g and shape \u2202\u2126, which can heavily influence the behavior of their solutions. Appropriate boundary conditions must often be carefully prescribed to ensure well-posedness of elliptic boundary value problems."
        },
        {
            "heading": "3 METHOD",
            "text": "In this section, we detail our method BENO. We first motivate our method using Green\u2019s function, a classical approach to solving elliptic boundary value problems in Section 3.1. We then introduce our graph construction method in Section 3.2. Inspired by the Green\u2019s function, we introduce BENO\u2019s architecture in Section 3.3."
        },
        {
            "heading": "3.1 MOTIVATION",
            "text": "How to facilitate boundary-interior interaction? To design the boundary-embedded message passing neural network, we draw inspiration from the traditional Green\u2019s function (Stakgold & Holst, 2011) method which is based on a numerical solution. Take the Poisson equation with Dirichlet boundary conditions for example. Suppose the Green\u2019s function is G : \u2126 \u00d7 \u2126 \u2192 R, which is the solution of the corresponding equation as follows:{\n\u22072G = \u03b4(x\u2212 x0)\u03b4(y \u2212 y0) G|\u2202\u2126 = 0\n(2)\nBased on the aforementioned equations and the detailed representation of the Green\u2019s function formula in the Appendix A, we can derive the solution in the following form:\nu(x, y) = \u222b\u222b \u2126 G(x, y, x0, y0)f(x0, y0)d\u03c30 \u2212 \u222b \u2202\u2126 g(x0, y0) \u2202G(x, y, x0, y0) \u2202n0 dl0 (3)\nMotivated by the two terms presented in Eq. 3, our objective is to approach boundary embedding by extending the Green\u2019s function. Following the mainstream work of utilizing GNNs as surrogate models (Pfaff et al., 2020; Eliasof et al., 2021; Lo\u0308tzsch et al., 2022), we exploit the graph network simulator (Sanchez-Gonzalez et al., 2020) as the backbone to mimic the Green\u2019s function, and add the boundary embedding to the node update in the message passing. Besides, in order to decouple the learning of the boundary and interior, we adopt a dual-branch network structure, where one branch sets the boundary value g to 0 to only learn the structural information of interior nodes, and the other branch sets the source term f of interior nodes to 0 to only learn the structural information of the boundary. The Poisson equation solving can then be disentangled into two parts:\n{ \u22072u(x, y) = f(x, y) u(x, y) = g(x, y) \u21d2 { \u22072u(x, y) = f(x, y) u(x, y) = 0\ufe38 \ufe37\ufe37 \ufe38\nBranch 1\n+ { \u22072u(x, y) = 0 u(x, y) = g(x, y)\ufe38 \ufe37\ufe37 \ufe38\nBranch 2\n(4)\nTherefore, our BENO will use a dual-branch design to build two different types of edges on the same graph separately. Branch 1 considers the effects of interior nodes and Branch 2 focuses solely on how to propagate the relationship between boundary values and interior nodes in the graph. Finally, we aggregate them together to obtain a more accurate solution under complex boundary conditions.\nHow to embed boundary? Since boundary conditions are crucially important for solving PDEs, how to better embed the boundary information into the neural network is key to our design. During a pilot study, we found that directly concatenating the interior node information with boundary information fails to solve for elliptic PDEs, and tends to cause severe over-fitting. Therefore, we propose to embed the boundary to represent its global information for further fusion. In recent years, Transformer (Vaswani et al., 2017) has been widely adopted due to its global receptive field. By leveraging its attention mechanism, the Transformer can effectively capture long-range dependencies and interactions within the boundary nodes. This is particularly advantageous when dealing with complex boundary conditions (i.e., irregular shape and inhomogeneous boundary values), as it allows for the modeling of complex relationships between boundary points and the interior solution."
        },
        {
            "heading": "3.2 GRAPH CONSTRUCTION",
            "text": "Before designing our method, it is an important step to construct graph G = {(V, E)} with the finite discrete interior nodes as node set V on the PDE\u2019s solution domain \u2126. In traditional solution methods such as FEM, the solution domain is initially constructed by triangulating the mesh graph (Bern & Eppstein, 1995; Ho-Le, 1988), followed by the subsequent solving process. Therefore, the first step is to implement Delaunay triangulation (Lee & Schachter, 1980) to construct mesh graph with edge set Emesh, in which each cell consists of three edges. Then we proceed to construct the edge set Ekn by selecting the K-nearest nodes for each individual node. K is the quantity of neighboring nodes that we deem as closely connected based on the Euclidean distance Dij between node i and j. The final edge set is E = Emesh \u222a Ekn. Examples of graph construction are shown in Fig. 2."
        },
        {
            "heading": "3.3 OVERALL ARCHITECTURE",
            "text": "In this section, we will introduce the detailed architecture of our proposed BENO, as shown in Figure 3. Our overall neural operator is divided into two branches, with each branch receiving different graph information and boundary data. However, the operator architecture remains the same with the encoder, boundary-embedded message passing neural network and decoder. Therefore, we will only focus on the common operator architecture."
        },
        {
            "heading": "3.3.1 ENCODER & DECODER",
            "text": "Encoder. The encoder computes node and edge embeddings. For each node i, the node encoder \u03f5v maps the node coordinates pi = (xi, yi), forcing term fi, and distances to boundary dxi, dyi to node embedding vector vi = \u03f5v([xi, yi, fi, dxi, dyi]) \u2208 RD in a high-dimensional space. The same mapping is implemented on edge attributes with edge encoder \u03f5e for edge embedding vector eij . For both node and edge encoders \u03f5, we exploit a two-layer Multi-Layer Perceptron (MLP) (Murtagh, 1991) with Sigmoid Linear Unit (SiLU) activation (Elfwing et al., 2018).\nDecoder. We use a two-layer MLP to map the features to solutions. Considering our dual-branch architecture, we will add the outputs from each decoder to obtain the final predicted solution u\u0302."
        },
        {
            "heading": "3.3.2 BOUNDARY-EMBEDDED MESSAGE PASSING NEURAL NETWORK (BE-MPNN)",
            "text": "To address the inherent differences in physical properties between boundary and interior nodes, we opt not to directly merge these distinct sources of information into a single network representation. Instead, we first employ the Transformer to specifically embed the boundary nodes. Then, the obtained boundary information is incorporated into the graph message passing processor. We will provide detailed explanations for these two components separately.\nEmbedding Boundary with Transformer. With the boundary node coordinates pB = (xB, yB), the boundary value g, and the distance to the geometric center of solution domain dc as input features, we first utilize the position embedding to include relative position relationship for initial representation HB0 , followed by a Transformer encoder with L layers to embed the boundary information H\nB. The resulting boundary features, denoted as B, are obtained by applying global average pooling (Lin et al., 2013) to the encoder outputs HB.\nEach self-attention layer applies multi-head self-attention and feed-forward neural networks to the input. The output of the i-th self-attention layer is denoted as HBi . The self-attention mechanism calculates the attention weights Ai as follows:\nAi = Softmax ( QiH B i (KiH B i ) T\n\u221a dk\n) (5)\nwhere Qi, Ki, and Vi are linear projections of HBi\u22121 with learnable weight matrices, and dk is the dimension of the key vectors. The attention output is computed as:\nHBi+1 = LayerNorm ( AiVi ( HBi ) +HBi ) (6)\nwhere LayerNorm denotes layer normalization, which helps to mitigate the problem of internal covariate shift. After passing through the L self-attention layers, the output HB is subject to global average pooling to obtain the boundary features: B = AvgPool(HB). Boundary-Embedded Message Passing Processor. The processor computes T steps of message passing, with an intermediate graph representation G1, \u00b7 \u00b7 \u00b7 , GT and boundary representation B1, \u00b7 \u00b7 \u00b7 , BT . The specific passing message mtij in step t in our processor is formed by:\nmtij = MLPs ( vti , v t j , e t ij , pi \u2212 pj ) (7)\nwhere mt+1ij represents the message sent from node j to i. pi \u2212 pj is the relative position which can enhance the equivariance by justifying the symmetry of the PDEs.\nThen we update the node feature vti and edge feature e t ij as follows:\nvt+1i = MLPs vti ,Bt, \u2211 j\u2208N (i) mtij  , (8) et+1ij = MLPs ( etij ,m t ij ) (9)\nHere, boundary information is embedded into the message passing. N (i) represents the gathering of all the neighbors of node i.\nLearning objective. Given the ground truth solution u and the predicted solution u\u0302, we minimize the mean squared error (MSE) of the predicted solution on \u2126."
        },
        {
            "heading": "4 EXPERIMENTS",
            "text": "We aim to answer the following questions: (1) Compared with existing baselines, can BENO learn the solution operator for elliptic PDEs with complex geometry and inhomogeneous boundary values? (2) Can BENO generalize to out-of-distribution boundary geometries and boundary values, and different grid resolutions? (3) Are all components of BENO essential for its performance? We first introduce experiment setup in Sec. 4.1, then answer the above three questions in the following three sections."
        },
        {
            "heading": "4.1 EXPERIMENT SETUP",
            "text": "Datasets. For elliptic PDEs simulations, we construct five different datasets with inhomogeneous boundary values, including 4/3/2/1-corner squares and squares without corners. Each dataset consists of 1000 samples with randomly initialized boundary shapes and values, with 900 samples used for\ntraining and validation, and 100 samples for testing. Each sample covers a grid of 32\u00d732 nodes and 128 boundary nodes. To further assess model performance, higher-resolution versions of each data sample, such as 64\u00d764, are also provided. Details on data generation are provided in Appendix C. Baselines. We adopt two of the most mainstream series of neural PDE solvers as baselines, one is graph-based, including GKN (Li et al., 2020b), GNN-PDE (Lo\u0308tzsch et al., 2022), and MPPDE (Brandstetter et al., 2022); the other is operator-based, including FNO (Li et al., 2020a). For fair comparison and adaption to irregular boundary shapes in our datasets, all of the baselines are re-implemented with the same input as ours, including all the interior and boundary node features. Please refer to Appendix E for re-implementation details.\nImplementation Details. All experiments are based on PyTorch (Paszke et al., 2019) and PyTorchGeometric (Fey & Lenssen, 2019) on 2\u00d7 NVIDIA A100 GPUs (80G). Following (Brandstetter et al., 2022), we also apply graph message passing neural network as our backbone for all the datasets. We use Adam (Kingma & Ba, 2014) optimizer with a weight decay of 5\u00d7 10\u22124 and a learning rate of 5\u00d7 10\u22125 obtained from grid search for all experiments. The relative L2 error measures the difference between the predicted and the ground truth values, normalized by the magnitude of the ground truth. MAE measures the average absolute difference between the predicted values and the ground truth values. Please refer to Appendix D for more implementation details."
        },
        {
            "heading": "4.2 MAIN EXPERIMENTAL RESULTS",
            "text": "We first test whether our BENO has a strong capability to solve elliptic PDEs with varying shapes. Table 2 and 3 summarize the results for the shape generalization task (more in Appendix H).\nFrom the results, we see that recent neural PDE solving methods (i.e., MP-PDE) overall fail to solve elliptic PDEs with inhomogeneous boundary values, not to mention generalizing to datasets with different boundary shapes. This precisely indicates that existing neural solvers are insufficient for solving this type of boundary value problems.\nIn contrast, from Table 2, we see that our proposed BENO trained only on 4-Corners dataset consistently achieves a significant improvement and strong generalization capability over the previous methods by a large margin. More precisely, the improvements of BENO over the best baseline are 55.17%, 52.18%, 52.43%, 47.38%, and 52.94% in terms of relative L2 norm when testing on 4/3/2/1/No-Corner dataset respectively. We attribute the remarkable performance to two factors: (i) BENO comprehensively leverages boundary information, and fuses them with the interior graph message for solving. (ii) BENO integrates dual-branch architecture to fully learn boundary and interior in a decoupled way and thus improves generalized solving performance.\nSimilarly, from Table 3, we see that among mixed corner training results, BENO always achieves the best performance among various compared baselines when varying the test sets, which validates the consistent superiority of our BENO with respect to different boundary shapes.\nAdditionally, we plot the visualization of the best baseline and our proposed BENO trained on 4-Corners dataset in Figure 4. It can be clearly observed that the predicted solution of BENO is closed to the ground truth, while MP-PDE fails to learn any features of the solution. We observe similar behaviors for all other baselines."
        },
        {
            "heading": "4.3 GENERALIZATION STUDY",
            "text": ""
        },
        {
            "heading": "4.3.1 RESULTS ON DIFFERENT BOUNDARY VALUES",
            "text": "To investigate the generalization ability on boundary value, we again train the models on 4-Corners dataset with inhomogeneous boundary value but utilize the test set with zero boundary value, which makes the boundary inhomogeneities totally different. Table 4 compares the best baseline and summarizes the results. From the results, we see that BENO has a significant advantage, successfully reducing the L2 norm to around 0.1. In addition, our method outperforms the best baseline by approximately 60.96% in terms of performance improvement. This not only demonstrates BENO\u2019s\nstrong generalization ability regarding boundary values but also provides solid experimental evidence for the successful application of our elliptic PDE solver."
        },
        {
            "heading": "4.3.2 DIFFERENT GRID RESOLUTIONS",
            "text": "Data-driven PDE solvers often face limitations in terms of the scale of the training data, making the ability to generalize to higher resolutions a crucial metric. Table 5 provides a summary of our performance in resolution generalization experiments. The model was trained on the 4-Corners homogeneous boundary value dataset with 32 \u00d7 32 resolution and tested with 64 \u00d7 64 samples not seen in training. The results demonstrate a significant advantage of our method over MP-PDE, with an improvement of approximately 22.46%. We attribute this advantage in generalization to two main factors. Firstly, it stems from the inherent capability of GNNs to process input graphs of various sizes. Secondly, it is due to our incorporation of relative positions as part of the network\u2019s edge features. Consequently, our approach can be deployed on different resolutions using the same setup."
        },
        {
            "heading": "4.4 ABLATION STUDY",
            "text": "To investigate the effectiveness of inner components in BENO, we study four variants of BENO. Table 6 shows the effectiveness of our BENO on ablation experiments, which is implemented based on 4-Corners dataset training. Firstly, BENO w. M replaces the BE-MPNN with a vanilla message passing neural network (Gilmer et al., 2017) and merely keeps the interior node feature. Secondly, BENO w/o. D removes the dual-branch structure of BENO and merely utilizes a single EncoderBE-MPNN-Decoder procedure. Thirdly, BENO w. E adds the Transformer output for edge message passing. Finally, BENO w. G replaces the Transformer architecture with a vanilla graph convolution network (Kipf & Welling, 2016).\nFrom the results we can draw conclusions as follows. Firstly, BENO w. M performs significantly worse than ours, which indicates the importance of fusing interior and boundary in BENO. Secondly, comparing the results of BENO w/o. D with ours we can conclude that decoupled learning of the interior and boundary proves to be effective. Thirdly, comparing the results of BENO w. E and ours, we can find that boundary information only helps in node-level message passing. In other words, it is not particularly suitable to directly inject the global information of the boundary into the edges. Finally, comparing results of BENO w. G with ours validates the design of Transformer for boundary embedding is crucial."
        },
        {
            "heading": "5 RELATED WORK",
            "text": ""
        },
        {
            "heading": "5.1 CLASSIC ELLIPTIC PDE SOLVERS",
            "text": "The classical numerical solution of elliptic PDEs approximates the domain \u2126 and its boundary \u2202\u2126 in Eq. 1 using a finite number of non-overlapping partitions. The solution to Eq. 1 is then approximated over these partitions. A variety of strategies are available for computing this discrete solution. Popular approaches include finite volume method (FVM) (Hirsch, 2007), finite element method (FEM) (Hughes, 2012), and finite difference method (FDM) (LeVeque, 2007). In the present work we utilize the FVM to generate the dataset which can easily accommodate complex boundary shapes. This approach partitions the domains into cells, and the boundary is specified using cell interfaces. After numerically approximating the operator \u22072 over these cells, the numerical solution is obtained on the centers of the cells constituting our domain. Further details are provided in Appendix B."
        },
        {
            "heading": "5.2 GNN FOR PDE SOLVER",
            "text": "GNNs are initially applied in physics-based simulations on solids and fluids represented by particles (Sanchez-Gonzalez et al., 2018). Recently, an important advancement MeshGraphNets (Pfaff et al., 2020) emerge to learn mesh-based simulations. Subsequently, several variations have been proposed, including techniques for accelerating finer-level simulations by utilizing GNNs (BelbutePeres et al., 2020; Yang & Hong, 2022), combining GNNs with Physics-Informed Neural Networks (PINNs) (Gao et al., 2022), solving inverse problems with GNNs and autodecoder-style priors (Zhao et al., 2022), and handling temporal distribution shift (Luo et al., 2023). However, the research focus on addressing boundary issues is limited. T-FEN (Lienen & Gu\u0308nnemann, 2022), FEONet (Lee et al., 2023), VQGraph Yang et al. (2024) and GNN-PDE (Lo\u0308tzsch et al., 2022) are pioneering efforts in this regard, encompassing complex domains and various boundary shapes. Nevertheless, the boundary values are still set to zero, which does not account for the presence of inhomogeneous boundary values. This discrepancy is precisely the problem that we aim to address."
        },
        {
            "heading": "5.3 NEURAL OPERATOR AS PDE SOLVER",
            "text": "Neural operators map from initial/boundary conditions to solutions through supervised learning in a mesh-invariant manner. Prominent examples of neural operators include the Fourier neural operator (FNO) (Li et al., 2020a), graph neural operator (Li et al., 2020b), and DeepONet(Lu et al., 2019). Neural operators exhibit invariance to discretization, making them highly suitable for solving PDEs. Moreover, neural operators enable the learning of operator mappings between infinite-dimensional function spaces. Subsequently, further variations have been proposed, including techniques for solving arbitrary geometries PDEs with both the computation efficiency and the flexibility (Li et al., 2022), enabling deeper stacks of Fourier layers by independently applying transformations (Tran et al., 2021), utilizing Fourier layers as a replacement for spatial self-attention (Guibas et al., 2021), facilitating boundary condition satisfaction in neural operators by implementing structural modifications to the operator kernel (Saad et al., 2022) and incorporating symmetries in the physical domain using group theory (Helwig et al., 2023). (Gupta et al., 2021; 2022; Xiao et al., 2023) continuously improve the design of the operator by introducing novel methods for numerical computation."
        },
        {
            "heading": "6 CONCLUSION",
            "text": "In this work, we have proposed Boundary-Embedded Neural Operators (BENO), a neural operator architecture to address the challenges posed by inhomogeneous boundary conditions with complex boundary geometry in solving elliptic PDEs. Our approach BENO incorporates physics intuition through a boundary-embedded architecture, consisting of GNNs and a Transformer, to model the influence of boundary conditions on the solution. By constructing a diverse dataset with various boundary shapes, values, and resolutions, we have demonstrated the effectiveness of our approach in outperforming existing state-of-the-art methods by an average of 60.96% in solving elliptic PDE problems. Furthermore, our method BENO exhibits strong generalization capabilities across different scenarios. The development of BENO opens up new possibilities for efficiently and accurately solving elliptic PDEs with complex boundary conditions, making them more useful to various scientific and engineering fields."
        },
        {
            "heading": "ACKNOWLEDGEMENT",
            "text": "We gratefully acknowledge the support of Westlake University Research Center for Industries of the Future, and Westlake University Center for High-performance Computing."
        },
        {
            "heading": "A DERIVATION OF THE GREEN\u2019S FUNCTION METHOD.",
            "text": "We first review the definition of the Green\u2019s function, which is G : \u2126\u00d7\u2126 \u2192 R, which is the solution of the corresponding equation as follows:{\n\u22072G = \u03b4(x\u2212 x0)\u03b4(y \u2212 y0) G|\u2202\u2126 = 0\n(10)\nAccording to Green\u2019s identities,\u222b\u222b \u2126 (u\u22072G)d\u03c3 = \u222b \u2202\u2126 u \u2202G \u2202n dl \u2212 \u222b\u222b \u2126 (\u2207u \u00b7 \u2207G)d\u03c3 (11) Since u and G are arbitrary, we can change the position to obtain that,\u222b\u222b \u2126 (G\u22072u)d\u03c3 = \u222b \u2202\u2126 G \u2202u \u2202n dl \u2212 \u222b\u222b \u2126 (\u2207u \u00b7 \u2207G)d\u03c3 (12) Subtract Eq. 12 from Eq. 11, we have,\u222b\u222b \u2126 (u\u22072G\u2212G\u22072u)d\u03c3 = \u222b \u2202\u2126 ( u \u2202G \u2202n \u2212G\u2202u \u2202n ) dl (13) Substitute Eq. 13 into Eq. 10, we can have that,\u222b \u2202\u2126 ( u \u2202G \u2202n \u2212G\u2202u \u2202n ) dl = \u222b\u222b \u2126 (u \u00b7 \u22072G\u2212G \u00b7 \u22072u)d\u03c3\n= \u222b\u222b \u2126 (\u2212u\u03b4(x\u2212 x0)\u03b4(y \u2212 y0)\u2212G\u22072u)d\u03c3\n= \u2212u(x0, y0)\u2212 \u222b\u222b\n\u2126\nG\u22072ud\u03c3\n= \u2212u(x0, y0) + \u222b\u222b\n\u2126\nGf(x, y)d\u03c3\n(14)\nNamely, we have that,\nu(x, y) = \u222b\u222b \u2126 G(x, y, x0, y0)f(x0, y0)d\u03c30\n+ \u222b \u2202\u2126 [ G(x, y, x0, y0) \u2202u(x0, y0) \u2202n0 \u2212 u(x0, y0) \u2202G(x, y, x0, y0) \u2202n0 ] dl0\n(15)\nWhen considering the Dirichlet boundary conditions, we can simplify the solution in the following form:\nu(x, y) = \u222b\u222b \u2126 G(x, y, x0, y0)f(x0, y0)d\u03c30 \u2212 \u222b \u2202\u2126 g(x0, y0) \u2202G(x, y, x0, y0) \u2202n0 dl0 (16)"
        },
        {
            "heading": "B NUMERICAL SOLUTION OF THE ELLIPTIC PDE",
            "text": "The strong solution to equation 1 can be expressed in terms of the Green\u2019s function (see Section 3.1 and Appendix A for discussion). However, obtaining a closed form expression using the Green\u2019s function is typically not possible, except for some limited canonical domain shapes. In the present paper, we obtain the solution to equation 1 in arbitrary two dimensional domains \u2126 using the finite volume method (Hirsch, 2007). This numerical approach relies on discretizing the domain \u2126 using cells. The surfaces of these cells at the boundary, which are called cell interfaces, are used to specify the given boundary condition. The solution of equation 1 is then numerically approximated over N (e.g., for 32\u00d732 cells, N = 1024) computational cells by solving,\nPu\u0302 = f , (17)\nwhere P \u2208 RN\u00d7N is an N \u00d7 N matrix which denotes a second-order discretization of the \u22072 operator incorporating the boundary conditions, u\u0302 \u2208 RN\u00d71 is a vector of values at the cell centers, and f \u2208 RN\u00d71 is a vector with values f(\u00b7, \u00b7) at cell centers used to discretize the domain \u2126. The matrix P resulting from this approach is positive definite and diagonally dominant, making it convenient to solve Equation 17 with a matrix-free iterative approach such as the Gauss-Seidel method (Saad, 2003)."
        },
        {
            "heading": "C DETAILS OF DATASETS",
            "text": "In this paper, we have established a comprehensive dataset for solving elliptic PDEs to facilitate various research endeavors. The elliptic PDEs solver is performed as follows. (1) A square domain is set with Nc number of cells in both x and y directions (note N = N2c ). The number of corners is set, however, the size of the corners is chosen randomly. (2) The source term f(x, y) is assigned assuming a variety of basis functions, including sinusoidal, exponential, logarithmic, and polynomial distributions. (3) The values of the boundary conditions g(x, y) are set using continuous periodic functions with a uniformly distributed wavelength \u2208 [1, 5]. (4) The Gauss-Seidel method (Saad, 2003) is used to iteratively obtain the solution u(x, y). Each Poisson run generates two files: one for the interior cells with discrete values of x, y, f , and u and the other for the boundary interfaces with discrete values of x, y, and g. The simulations are performed on the Sherlock cluster at Stanford University."
        },
        {
            "heading": "D MORE IMPLEMENTATION DETAILS",
            "text": "Our normalization process is performed using the z-score method (Patro & Sahu, 2015), where the mean and standard deviation are calculated from the training set. This ensures that all features are normalized based on the mean and variance of the training data. We also apply the CosineAnnealingWarmRestarts scheduler (Loshchilov & Hutter, 2016) during the training. Each experiment is trained for 1000 epochs, and validation is performed after each epoch. For the final evaluation, we select the model parameters from the epoch with the lowest validation loss. Consistency is maintained across all experiments by utilizing the same random seed.\nAll our experiments are evaluated on relative L2 error, abbreviated as L2, and mean absolute error (MAE), which are two commonly used metrics for evaluating the performance of models or algorithms. The relative L2 error, also known as the normalized L2 error, measures the difference between the predicted values and the ground truth values, normalized by the magnitude of the ground truth values. It is typically calculated as the L2 norm of the difference between the predicted and ground truth values, divided by the L2 norm of the ground truth values. On the other hand, MAE measures the average absolute difference between the predicted values and the ground truth values. It is calculated by taking the mean of the absolute differences between each predicted value and its corresponding ground truth value."
        },
        {
            "heading": "E DETAILS OF BASELINES",
            "text": "Our proposed BENO is compared with a range of competing baselines as follows:\n\u2022 GKN (Li et al., 2020b) develops an approximation method for mapping in infinite-dimensional spaces by combining non-linear activation functions with a set of integral operators. The integration of kernels is achieved through message passing on graph networks. For fair comparison, we re-implement it by adding the boundary nodes to the graph. To better distinguish between nodes belonging to the interior and those belonging to the boundary, we have also added an additional column of one-hot encoding to the nodes for differentiation.\n\u2022 FNO (Li et al., 2020a) introduces a novel approach that directly learns the mapping from functional parametric dependencies to the solution. The method implements a series of layers computing global convolution operators with the fast Fourier transform (FFT) followed by mixing weights in the frequency domain and inverse Fourier transform, enabling an architecture that is both expressive and computationally efficient. For fair comparison, we re-implement it by fixing the value of out-domain nodes with a large number, and then implement the global operation.\n\u2022 GNN-PDE (Lo\u0308tzsch et al., 2022) represents the pioneering effort in training neural networks on simulated data generated by a finite element solver, encompassing various boundary shapes. It evaluates the generalization capability of the trained operator across previously unobserved scenarios by designing a versatile solution operator using spectral graph convolutions. For fair comparison, we re-implement it by adding the boundary nodes to the graph. To better distinguish between nodes belonging to the interior and those belonging to the boundary, we have also added an additional column of one-hot encoding to the nodes for differentiation.\n\u2022 MP-PDE (Brandstetter et al., 2022) presents a groundbreaking solver that utilizes neural message passing for all its components. This approach replaces traditionally heuristic-designed elements in the computation graph with neural function approximators that are optimized through backpropagation. For fair comparison, we re-implement it by adding the boundary nodes to the graph. To better distinguish between nodes belonging to the interior and those belonging to the boundary, we have also added an additional column of one-hot encoding to the nodes for differentiation."
        },
        {
            "heading": "F DIFFERENCES WITH OTHER NEURAL OPERATORS",
            "text": "In this section, we compare our method, BENO, with existing approaches in terms of several key aspects according to Table 1.\n\u2022 PDE-agnostic prediction on new initial conditions: GKN, FNO, GNN-PDE, MP-PDE, and BENO are all capable of predicting new initial conditions.\n\u2022 Train/Test space grid independence: GKN, GNN-PDE, and BENO exhibit independence between the training and testing spaces, while FNO and MP-PDE lack this independence.\n\u2022 Evaluation at unobserved spatial locations: GKN, FNO, and BENO are capable of evaluating the PDE at locations that are not observed during training, while GNN-PDE and MP-PDE do not possess this capability.\n\u2022 Free-form spatial domain for boundary shape: Only GNN-PDE and BENO are capable of dealing with arbitrary boundary shapes, while GKN and MP-PDE are limited in this aspect.\n\u2022 Inhomogeneous boundary condition value: Only our method, BENO, has the ability to handle inhomogeneous boundary conditions, while GKN, FNO, GNN-PDE, and MP-PDE are unable to handle them.\nIn summary, compared to the existing methods, our method, BENO, possesses several distinct advantages. It can predict new initial conditions regardless of the specific PDE, maintains grid independence between training and testing spaces, allows evaluation at unobserved spatial locations, handles free-form spatial domains for boundary shapes, and accommodates inhomogeneous boundary condition values. These capabilities make BENO a versatile and powerful approach for solving time-independent elliptic PDEs."
        },
        {
            "heading": "G ALGORITHM",
            "text": "The whole learning algorithm of BENO is summarized in Algorithm 1."
        },
        {
            "heading": "H MORE EXPERIMENTAL RESULTS",
            "text": "H.1 SENSITIVITY ANALYSIS\nIn this section, we discuss the process of determining the optimal values for the number of MLP layers (M ) and the number of Transformer layers (L) using grid search, a systematic approach for hyper-parameter tuning.\nGrid search involves defining a parameter grid consisting of different combinations of M and L values. We specified M in the range of [2, 3, 4] and L in the range of [1, 2, 3] to explore a diverse set of configurations. We build multiple models, each with a different combination of M and L values, and train them on 4-Corners training dataset. The models are then evaluated using appropriate\nAlgorithm 1 Learning Algorithm of the proposed BENO Input: The forcing term f , the inhomogeneous boundary condition g on \u2202\u2126 . Output: The solution prediction u\u0302 of the elliptic PDEs.\n1: Construct the graph G = {(V, E)} following Section 3.2; 2: Initialize the parameters in our model;\n# Training procedure 3: while not convergence do 4: for each training input do 5: Set the boundary value of one branch to zero following Eq. 16; 6: Set the source term of interior in the other branch to zero; 7: Feed the node/edge attributes to encoder following Section 3.3.1.; 8: Feed the boundary to the Transformer for boundary features B; 9: Add B to the message passing processor following Eq. 8;\n10: Feed output features into a decoder to get the predictions u\u0302; 11: Calculate the loss using MSE; 12: Update the parameters in BENO using back propagation; 13: end for 14: end while\nevaluation metrics on a separate validation set. The evaluation results allowed us to compare the performance of models across different parameter combinations.\nAfter evaluating the models, we select the combination of M and L that yield the best performance according to our chosen evaluation metric. This combination became our final choice for M and L, representing the optimal configuration for our model. To ensure the reliability of our chosen parameters, we validate them on an independent validation set. This step confirmed that the model\u2019s performance remained consistent and reliable.\nThe grid search process provided a systematic and effective approach to determine the optimal values for M = 3 and L = 1, allowing us to fine-tune our model and achieve improved performance.\nH.2 MORE EXPERIMENTAL RESULTS\nWe have successfully validated our method\u2019s performance on the 4-Corners and mixed corners datasets during training and testing on other shape datasets, yielding favorable results. In this section, we will further supplement the evaluation by training on the No Corner dataset and testing on other shape datasets. Since the No Corner dataset does not include any corner scenarios, the remaining datasets present completely unseen scenarios for it, thereby providing a stronger test of the model\u2019s generalization performance.\nTable 7 summarizes the results of training on 900 No-Corner samples and tested on all datasets. We can infer similar conclusions to those in the experimental section above. Our BENO performs well in learning on No-Corner cases, yielding more accurate solutions. Additionally, our method demonstrates stronger generalization ability, as it can obtain good solutions even in cases where corners of any shape have not been encountered.\nH.3 CONVERGENCE ANALYSIS\nWe draw the training curve of the train L2 norm and test L2 norm of three models trained on the 4-Corners dataset with inhomogeneous boundary value in Figure 5. It is obviously that although two baselines also contains the boundary information, they fail to learn the elliptic PDEs with nondecreasing convergence curves. However, our proposed BENO is capable of successfully learning complex boundary conditions with the use of the CosineAnnealingWarmRestarts scheduler, converges to a satisfactory result."
        },
        {
            "heading": "I LIMITATIONS & BROADER IMPACTS",
            "text": "Limitations. Although this paper primarily focuses on Dirichlet boundary conditions, it is essential to acknowledge that there are other types of boundary treatments, including Neumann and Robin boundary conditions. While the framework presented in this study may not directly address these alternative boundary conditions, it still retains its usefulness. Future research should explore the extension of the developed framework to incorporate these different boundary treatments, allowing for a more comprehensive and versatile solution for a broader range of practical problems.\nBroader Impact. The development of a fast, efficient, and accurate neural network for solving PDEs holds significant potential for numerous physics and engineering disciplines. The impact of such advancements cannot be understated. By providing a more streamlined and computationally efficient approach, this research can revolutionize fields such as computational fluid dynamics, solid mechanics, electromagnetics, and many others. The ability to solve PDEs more efficiently opens up new possibilities for modeling and simulating complex physical systems, leading to improved designs, optimizations, and decision-making processes. The resulting advancements can have far-reaching implications, including the development of more efficient and sustainable technologies, enhanced understanding of natural phenomena, and improved safety and reliability in engineering applications. It is crucial to continue exploring and refining these neural network-based approaches to maximize their potential impact across a wide range of scientific and engineering disciplines."
        },
        {
            "heading": "J MORE VISUALIZATION ANALYSIS",
            "text": "In this section, we visualize the experimental results on a broader range of experiments. Figure 6 presents the comparison of solution prediction on 64 \u00d7 64 grid resolution. Figure 7 presents the comparison of solution prediction on data with zero boundary value. Figure 8 presents the qualitative results of training on the 4-Corners dataset and testing on data with various other shapes."
        },
        {
            "heading": "K EXPERIMENTS ON NEUMANN BOUNDARY",
            "text": "In this section, we consider to solve the Poisson equation with Neumann boundary conditions using our proposed BENO. In the context of Neumann boundary conditions, the equation takes the form:\n\u22072u(x, y) = f(x, y), \u2200(x, y) \u2208 \u2126, \u2202u(x, y)\n\u2202n = g(x, y), \u2200(x, y) \u2208 \u2202\u2126,\n(18)\nwhere f represents the source term, n typically represents the unit normal vector perpendicular to the boundary surface, and g specifies the prescribed rate of change normal to the boundary \u2202\u2126. The challenge in solving Poisson\u2019s equation with Neumann boundary conditions lies in the proper treatment of the boundary derivative term, which requires sophisticated numerical schemes to approximate accurately.\nSpecifically, the model is trained exclusively on a dataset consisting of 900 4-corners samples. The robustness and generalizability of our approach were then evaluated on 5 different test datasets, which represent various boundary configurations encountered in practical applications. Each dataset is constructed to challenge the model with different boundary complexities.\nThe results are shown in Table 8 and Table 9. Our proposed BENO still demonstrates superior performance across all test datasets in comparison to the baselines, including GNN-PDE, and MPPDE models. Particularly, BENO achieves the lowest MAE and relative L2 norm scores in the majority of the scenarios. In Table 8, when tested on the 4-Corners dataset, BENO exhibites an\nL2 norm of 0.3568 and an MAE of 0.8311, outperforming all other methods and showcasing the effectiveness of our approach under strict 4-corners conditions.\nWhen trained on mixed boundary conditions in Table 9, BENO still maintains the highest accuracy, yielding an relative L2 norm of 0.4237 and an MAE of 1.0114 on the 4-Corners test set, confirming its robustness to varied training conditions. Notably, the improvement is significant in the more challenging No-Corner test set, where BENO\u2019s L2 is 0.3344, a remarkable enhancement over the baseline methods. The bolded figures in the tables highlight the instances where BENO outperforms all other models, underscoring the impact of our boundary-embedded techniques.\nThe consistency of BENO\u2019s performance under different boundary conditions underscores its potential for applications in computational physics where such scenarios are prevalent. Besides, the experimental outcomes affirm the efficacy of BENO in handling complex boundary problems in the context of PDEs. It is also worth noting that the BENO model not only improves the prediction accuracy but also exhibits a significant reduction in error across different test cases, which is critical for high-stakes applications such as numerical simulation in engineering and physical sciences."
        },
        {
            "heading": "L EXPERIMENTS ON DARCY FLOW",
            "text": "In this section, we consider the solution of the Darcy flow using our proposed BENO approach. The 2-d Darcy flow is a second-order linear elliptic equation of the form\n\u2207 \u00b7 (\u03ba(x, y)\u2207u(x, y)) = f(x, y), \u2200(x, y) \u2208 \u2126, u(x, y) = g(x, y), \u2200(x, y) \u2208 \u2202\u2126, (19)\nwhere the coefficients \u03ba is generated by taking a linear combination of smooth basis function in the solution domain. The coefficients of the linear combination of these basis functions is taken from\nuniform distribution of random numbers. Dirichlet boundary condition is imposed along the boundary \u2202\u2126 using the function g which is sufficiently smooth. The objective of BENO is to map from the coefficient \u03ba to solution u of the PDE in Equation 19.\nThe model was exclusively trained on a dataset comprised of 900 samples, each featuring 4-corner configurations. To assess the robustness and adaptability of our method, we conduct evaluations on five distinct test datasets. These datasets are deliberately chosen to represent a variety of boundary conditions commonly encountered in real-world applications, with each one designed to present the model with different levels of boundary complexity. The outcomes of these evaluations are detailed in Table 10. Our proposed BENO model consistently outperforms the best baseline across all test datasets. Notably, BENO achieves the lowest Mean MAE and relative L2 norm in the majority of these scenarios. This performance underscores the effectiveness of our approach, particularly under the stringent conditions of 4-corner boundaries.\nM VISUALIZATION OF TWO BRANCHES\nIn this section, the visualized outputs of two distinct branches offer a deeper insight into our model\u2019s functionality. Branch1, with the boundary input set to zero, is posited to approximate the impact emanating from the interior, while Branch2, nullifying the interior inputs, is conjectured to capture the boundary\u2019s influence on the interior. The observations from Figure 9 lend credence to our hypothesis, indicating a discernible delineation of roles between the two branches.\nExtending this analysis, we further postulate that the interplay between Branch1 and Branch2 is critical for accurately modeling the PDE solution landscape. The synergy of these branches, as evidenced in our results, showcases a composite model that effectively balances the intricate boundary-interior dynamics. This balance is crucial in situations where boundary conditions significantly dictate the behavior of the system, further emphasizing the robustness and adaptability of our model. The innovative dual-branch strategy presents a promising avenue for enhancing the interpretability and precision of PDE solutions in complex domains."
        },
        {
            "heading": "N HYPER-PARAMETER LIST",
            "text": ""
        }
    ],
    "title": "BENO: BOUNDARY-EMBEDDED NEURAL OPERATORS FOR ELLIPTIC PDES",
    "year": 2024
}