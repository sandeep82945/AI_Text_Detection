{
    "abstractText": "Data augmentation, a cornerstone technique in deep learning, is crucial in enhancing model performance, especially with scarce labeled data. While traditional methods, such as hand-crafted augmentations, are effective but limited in scope, modern, adaptable techniques often come at the cost of computational complexity and are hard to fit into existing processes. In this work, we unveil an efficient approach that universally enhances existing data augmentation techniques by enabling their adaptation and refinement, thereby providing a significant and comprehensive improvement across all existing methods. We present SAFLEX (SelfAdaptive Augmentation via Feature Label EXtrapolation), an approach that utilizes an efficient bilevel optimization to learn the sample weights and soft labels of augmented samples. This is applicable to augmentations from any source, seamlessly integrating with existing upstream augmentation pipelines. Remarkably, SAFLEX effectively reduces the noise and label errors of the upstream augmentation pipeline with a marginal computational cost. As a versatile module, SAFLEX excels across diverse datasets, including natural, medical images, and tabular data, showcasing its prowess in few-shot learning and out-of-distribution generalization. SAFLEX seamlessly integrates with common augmentation strategies like RandAug and CutMix, as well as augmentations from large pre-trained generative models like stable diffusion. It is also compatible with contrastive learning frameworks, such as fine-tuning CLIP. Our findings highlight the potential to adapt existing augmentation pipelines for new data types and tasks, signaling a move towards more adaptable and resilient training frameworks.",
    "authors": [],
    "id": "SP:bc822094cb8e90f994b47b997e6c8fbf329fa860",
    "references": [
        {
            "authors": [
                "Sercan \u00d6 Arik",
                "Tomas Pfister"
            ],
            "title": "Tabnet: Attentive interpretable tabular learning",
            "venue": "In Proceedings of the AAAI conference on artificial intelligence,",
            "year": 2021
        },
        {
            "authors": [
                "Shekoofeh Azizi",
                "Simon Kornblith",
                "Chitwan Saharia",
                "Mohammad Norouzi",
                "David J Fleet"
            ],
            "title": "Synthetic data from diffusion models improves imagenet classification",
            "venue": "arXiv preprint arXiv:2304.08466,",
            "year": 2023
        },
        {
            "authors": [
                "Jonathan F Bard"
            ],
            "title": "Practical bilevel optimization: algorithms and applications, volume 30",
            "venue": "Springer Science & Business Media,",
            "year": 2013
        },
        {
            "authors": [
                "Binod Bhattarai",
                "Seungryul Baek",
                "Rumeysa Bodur",
                "Tae-Kyun Kim"
            ],
            "title": "Sampling strategies for gan synthetic data",
            "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),",
            "year": 2020
        },
        {
            "authors": [
                "Tim Brooks",
                "Aleksander Holynski",
                "Alexei A Efros"
            ],
            "title": "Instructpix2pix: Learning to follow image editing instructions",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Razvan Caramalau",
                "Binod Bhattarai",
                "Tae-Kyun Kim"
            ],
            "title": "Sequential graph convolutional network for active learning",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2021
        },
        {
            "authors": [
                "Tianqi Chen",
                "Carlos Guestrin"
            ],
            "title": "Xgboost: A scalable tree boosting system",
            "venue": "In Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining,",
            "year": 2016
        },
        {
            "authors": [
                "Ekin D Cubuk",
                "Barret Zoph",
                "Dandelion Mane",
                "Vijay Vasudevan",
                "Quoc V Le"
            ],
            "title": "Autoaugment: Learning augmentation strategies from data",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2019
        },
        {
            "authors": [
                "Ekin D Cubuk",
                "Barret Zoph",
                "Jonathon Shlens",
                "Quoc V Le"
            ],
            "title": "Randaugment: Practical automated data augmentation with a reduced search space",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops,",
            "year": 2020
        },
        {
            "authors": [
                "Jia Deng",
                "Wei Dong",
                "Richard Socher",
                "Li-Jia Li",
                "Kai Li",
                "Li Fei-Fei"
            ],
            "title": "Imagenet: A large-scale hierarchical image database",
            "venue": "IEEE conference on computer vision and pattern recognition,",
            "year": 2009
        },
        {
            "authors": [
                "Lisa Dunlap",
                "Clara Mohri",
                "Devin Guillory",
                "Han Zhang",
                "Trevor Darrell",
                "Joseph E Gonzalez",
                "Aditi Raghunathan",
                "Anna Rohrbach"
            ],
            "title": "Using language to extend to unseen domains",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Lisa Dunlap",
                "Alyssa Umino",
                "Han Zhang",
                "Jiezhi Yang",
                "Joseph E Gonzalez",
                "Trevor Darrell"
            ],
            "title": "Diversify your vision datasets with automatic diffusion-based augmentation",
            "venue": "arXiv preprint arXiv:2305.16289,",
            "year": 2023
        },
        {
            "authors": [
                "Jiahui Gao",
                "Renjie Pi",
                "LIN Yong",
                "Hang Xu",
                "Jiacheng Ye",
                "Zhiyong Wu",
                "Weizhong Zhang",
                "Xiaodan Liang",
                "Zhenguo Li",
                "Lingpeng Kong"
            ],
            "title": "Self-guided noise-free data generation for efficient zeroshot learning",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Ian J Goodfellow",
                "Jonathon Shlens",
                "Christian Szegedy"
            ],
            "title": "Explaining and harnessing adversarial examples",
            "venue": "In International Conference on Learning Representations,",
            "year": 2015
        },
        {
            "authors": [
                "Sachin Goyal",
                "Ananya Kumar",
                "Sankalp Garg",
                "Zico Kolter",
                "Aditi Raghunathan"
            ],
            "title": "Finetune like you pretrain: Improved finetuning of zero-shot vision models",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Beliz Gunel",
                "Jingfei Du",
                "Alexis Conneau",
                "Veselin Stoyanov"
            ],
            "title": "Supervised contrastive learning for pre-trained language model fine-tuning",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        },
        {
            "authors": [
                "Bo Han",
                "Quanming Yao",
                "Xingrui Yu",
                "Gang Niu",
                "Miao Xu",
                "Weihua Hu",
                "Ivor Tsang",
                "Masashi Sugiyama"
            ],
            "title": "Co-teaching: Robust training of deep neural networks with extremely noisy labels",
            "venue": "Advances in neural information processing systems,",
            "year": 2018
        },
        {
            "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
            ],
            "title": "Deep residual learning for image recognition",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2016
        },
        {
            "authors": [
                "Ruifei He",
                "Shuyang Sun",
                "Xin Yu",
                "Chuhui Xue",
                "Wenqing Zhang",
                "Philip Torr",
                "Song Bai",
                "XIAOJUAN QI"
            ],
            "title": "Is synthetic data from generative models ready for image recognition",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Chih-Hui Ho",
                "Nuno Nvasconcelos"
            ],
            "title": "Contrastive learning with adversarial examples",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2020
        },
        {
            "authors": [
                "Daniel Ho",
                "Eric Liang",
                "Xi Chen",
                "Ion Stoica",
                "Pieter Abbeel"
            ],
            "title": "Population based augmentation: Efficient learning of augmentation policy schedules",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Sheng-Wei Huang",
                "Che-Tsung Lin",
                "Shu-Ping Chen",
                "Yen-Yi Wu",
                "Po-Hao Hsu",
                "Shang-Hong Lai"
            ],
            "title": "Auggan: Cross domain adaptation with gan-based data augmentation",
            "venue": "In Proceedings of the European Conference on Computer Vision (ECCV),",
            "year": 2018
        },
        {
            "authors": [
                "Saachi Jain",
                "Hannah Lawrence",
                "Ankur Moitra",
                "Aleksander Madry"
            ],
            "title": "Distilling model failures as directions in latent space",
            "venue": "In The Eleventh International Conference on Learning Representations,",
            "year": 2022
        },
        {
            "authors": [
                "Prannay Khosla",
                "Piotr Teterwak",
                "Chen Wang",
                "Aaron Sarna",
                "Yonglong Tian",
                "Phillip Isola",
                "Aaron Maschinot",
                "Ce Liu",
                "Dilip Krishnan"
            ],
            "title": "Supervised contrastive learning",
            "venue": "Advances in neural information processing systems,",
            "year": 2020
        },
        {
            "authors": [
                "Diederik P Kingma",
                "Jimmy Ba"
            ],
            "title": "Adam: A method for stochastic optimization",
            "venue": "arXiv preprint arXiv:1412.6980,",
            "year": 2014
        },
        {
            "authors": [
                "Pang Wei Koh",
                "Shiori Sagawa",
                "Henrik Marklund",
                "Sang Michael Xie",
                "Marvin Zhang",
                "Akshay Balsubramani",
                "Weihua Hu",
                "Michihiro Yasunaga",
                "Richard Lanas Phillips",
                "Irena Gao"
            ],
            "title": "Wilds: A benchmark of in-the-wild distribution shifts",
            "venue": "In International Conference on Machine Learning,",
            "year": 2021
        },
        {
            "authors": [
                "Nikola Konstantinov",
                "Christoph Lampert"
            ],
            "title": "Robust learning from untrusted sources",
            "venue": "In International conference on machine learning,",
            "year": 2019
        },
        {
            "authors": [
                "Jan Kremer",
                "Fei Sha",
                "Christian Igel"
            ],
            "title": "Robust active label correction",
            "venue": "In International conference on artificial intelligence and statistics,",
            "year": 2018
        },
        {
            "authors": [
                "Alex Krizhevsky",
                "Ilya Sutskever",
                "Geoffrey E Hinton"
            ],
            "title": "Imagenet classification with deep convolutional neural networks",
            "venue": "Communications of the ACM,",
            "year": 2017
        },
        {
            "authors": [
                "Ananya Kumar",
                "Aditi Raghunathan",
                "Robbie Matthew Jones",
                "Tengyu Ma",
                "Percy Liang"
            ],
            "title": "Finetuning can distort pretrained features and underperform out-of-distribution",
            "venue": "In International Conference on Learning Representations,",
            "year": 2021
        },
        {
            "authors": [
                "Hunter Lang",
                "Aravindan Vijayaraghavan",
                "David Sontag"
            ],
            "title": "Training subset selection for weak supervision",
            "venue": "In Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Chi-Heng Lin",
                "Chiraag Kaushik",
                "Eva L Dyer",
                "Vidya Muthukumar"
            ],
            "title": "The good, the bad and the ugly sides of data augmentation: An implicit spectral regularization perspective",
            "venue": "arXiv preprint arXiv:2210.05021,",
            "year": 2022
        },
        {
            "authors": [
                "Bo Liu",
                "Mao Ye",
                "Stephen Wright",
                "Peter Stone",
                "Qiang Liu"
            ],
            "title": "Bome! bilevel optimization made easy: A simple first-order approach",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Yang Liu",
                "Shen Yan",
                "Laura Leal-Taix\u00e9",
                "James Hays",
                "Deva Ramanan"
            ],
            "title": "Soft augmentation for image classification",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Jonathan Lorraine",
                "Paul Vicol",
                "David Duvenaud"
            ],
            "title": "Optimizing millions of hyperparameters by implicit differentiation",
            "venue": "In International conference on artificial intelligence and statistics,",
            "year": 2020
        },
        {
            "authors": [
                "Xingjun Ma",
                "Yisen Wang",
                "Michael E Houle",
                "Shuo Zhou",
                "Sarah Erfani",
                "Shutao Xia",
                "Sudanthi Wijewickrema",
                "James Bailey"
            ],
            "title": "Dimensionality-driven learning with noisy labels",
            "venue": "In International Conference on Machine Learning,",
            "year": 2018
        },
        {
            "authors": [
                "Saypraseuth Mounsaveng",
                "Issam Laradji",
                "Ismail Ben Ayed",
                "David Vazquez",
                "Marco Pedersoli"
            ],
            "title": "Learning data augmentation with online bilevel optimization for image classification",
            "venue": "In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision,",
            "year": 2021
        },
        {
            "authors": [
                "Saypraseuth Mounsaveng",
                "Issam Laradji",
                "David V\u00e1zquez",
                "Marco Perdersoli",
                "Ismail Ben Ayed"
            ],
            "title": "Automatic data augmentation learning using bilevel optimization for histopathological images",
            "venue": "arXiv preprint arXiv:2307.11808,",
            "year": 2023
        },
        {
            "authors": [
                "Augustus Odena",
                "Christopher Olah",
                "Jonathon Shlens"
            ],
            "title": "Conditional image synthesis with auxiliary classifier gans",
            "venue": "In International conference on machine learning,",
            "year": 2017
        },
        {
            "authors": [
                "Soma Onishi",
                "Shoya Meguro"
            ],
            "title": "Rethinking data augmentation for tabular data in deep learning",
            "venue": "arXiv preprint arXiv:2305.10308,",
            "year": 2023
        },
        {
            "authors": [
                "Alec Radford",
                "Jong Wook Kim",
                "Chris Hallacy",
                "Aditya Ramesh",
                "Gabriel Goh",
                "Sandhini Agarwal",
                "Girish Sastry",
                "Amanda Askell",
                "Pamela Mishkin",
                "Jack Clark"
            ],
            "title": "Learning transferable visual models from natural language supervision",
            "venue": "In International conference on machine learning,",
            "year": 2021
        },
        {
            "authors": [
                "Robin Rombach",
                "Andreas Blattmann",
                "Dominik Lorenz",
                "Patrick Esser",
                "Bj\u00f6rn Ommer"
            ],
            "title": "Highresolution image synthesis with latent diffusion models",
            "venue": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Swami Sankaranarayanan",
                "Yogesh Balaji",
                "Carlos D Castillo",
                "Rama Chellappa"
            ],
            "title": "Generate to adapt: Aligning domains using generative adversarial networks",
            "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
            "year": 2018
        },
        {
            "authors": [
                "Jordan Shipard",
                "Arnold Wiliem",
                "Kien Nguyen Thanh",
                "Wei Xiang",
                "Clinton Fookes"
            ],
            "title": "Diversity is definitely needed: Improving model-agnostic zero-shot classification via stable diffusion",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2023
        },
        {
            "authors": [
                "Connor Shorten",
                "Taghi M Khoshgoftaar"
            ],
            "title": "A survey on image data augmentation for deep learning",
            "venue": "Journal of big data,",
            "year": 2019
        },
        {
            "authors": [
                "Patrice Y Simard",
                "David Steinkraus",
                "John C Platt"
            ],
            "title": "Best practices for convolutional neural networks applied to visual document analysis",
            "venue": "In Icdar,",
            "year": 2003
        },
        {
            "authors": [
                "Gowthami Somepalli",
                "Avi Schwarzschild",
                "Micah Goldblum",
                "C Bayan Bruss",
                "Tom Goldstein"
            ],
            "title": "Saint: Improved neural networks for tabular data via row attention and contrastive pre-training",
            "venue": "NeurIPS",
            "year": 2022
        },
        {
            "authors": [
                "Nitish Srivastava",
                "Geoffrey Hinton",
                "Alex Krizhevsky",
                "Ilya Sutskever",
                "Ruslan Salakhutdinov"
            ],
            "title": "Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning",
            "year": 1929
        },
        {
            "authors": [
                "Sunil Thulasidasan",
                "Tanmoy Bhattacharya",
                "Jeff Bilmes",
                "Gopinath Chennupati",
                "Jamal MohdYusof"
            ],
            "title": "Combating label noise in deep learning using abstention",
            "venue": "In International Conference on Machine Learning,",
            "year": 2019
        },
        {
            "authors": [
                "Brandon Trabucco",
                "Kyle Doherty",
                "Max Gurinas",
                "Ruslan Salakhutdinov"
            ],
            "title": "Effective data augmentation with diffusion models",
            "venue": "In ICLR 2023 Workshop on Mathematical and Empirical Understanding of Foundation Models,",
            "year": 2023
        },
        {
            "authors": [
                "Mitchell Wortsman",
                "Gabriel Ilharco",
                "Jong Wook Kim",
                "Mike Li",
                "Simon Kornblith",
                "Rebecca Roelofs",
                "Raphael Gontijo Lopes",
                "Hannaneh Hajishirzi",
                "Ali Farhadi",
                "Hongseok Namkoong"
            ],
            "title": "Robust fine-tuning of zero-shot models",
            "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
            "year": 2022
        },
        {
            "authors": [
                "Sen Wu",
                "Hongyang Zhang",
                "Gregory Valiant",
                "Christopher R\u00e9"
            ],
            "title": "On the generalization effects of linear transformations in data augmentation",
            "venue": "In International Conference on Machine Learning,",
            "year": 2020
        },
        {
            "authors": [
                "Jiancheng Yang",
                "Rui Shi",
                "Bingbing Ni"
            ],
            "title": "Medmnist classification decathlon: A lightweight automl benchmark for medical image analysis",
            "venue": "IEEE 18th International Symposium on Biomedical Imaging (ISBI),",
            "year": 2021
        },
        {
            "authors": [
                "Jiancheng Yang",
                "Rui Shi",
                "Donglai Wei",
                "Zequan Liu",
                "Lin Zhao",
                "Bilian Ke",
                "Hanspeter Pfister",
                "Bingbing Ni"
            ],
            "title": "Medmnist v2-a large-scale lightweight benchmark for 2d and 3d biomedical image classification",
            "venue": "Scientific Data,",
            "year": 2023
        },
        {
            "authors": [
                "Kaiwen Yang",
                "Yanchao Sun",
                "Jiahao Su",
                "Fengxiang He",
                "Xinmei Tian",
                "Furong Huang",
                "Tianyi Zhou",
                "Dacheng Tao"
            ],
            "title": "Adversarial auto-augment with label preservation: A representation learning principle guided approach",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2022
        },
        {
            "authors": [
                "Kaiwen Yang",
                "Tianyi Zhou",
                "Xinmei Tian",
                "Dacheng Tao"
            ],
            "title": "Identity-disentangled adversarial augmentation for self-supervised learning",
            "venue": "In International Conference on Machine Learning,",
            "year": 2022
        },
        {
            "authors": [
                "Sangdoo Yun",
                "Dongyoon Han",
                "Seong Joon Oh",
                "Sanghyuk Chun",
                "Junsuk Choe",
                "Youngjoon Yoo"
            ],
            "title": "Cutmix: Regularization strategy to train strong classifiers with localizable features",
            "venue": "In Proceedings of the IEEE/CVF international conference on computer vision,",
            "year": 2019
        },
        {
            "authors": [
                "Hongyi Zhang",
                "Moustapha Cisse",
                "Yann N Dauphin",
                "David Lopez-Paz"
            ],
            "title": "mixup: Beyond empirical risk minimization",
            "venue": "In International Conference on Learning Representations,",
            "year": 2018
        },
        {
            "authors": [
                "Yifan Zhang",
                "Bryan Hooi",
                "Dapeng Hu",
                "Jian Liang",
                "Jiashi Feng"
            ],
            "title": "Unleashing the power of contrastive self-supervised visual models via contrast-regularized fine-tuning",
            "venue": "Advances in Neural Information Processing Systems,",
            "year": 2021
        },
        {
            "authors": [
                "Bo Zhao",
                "Konda Reddy Mopuri",
                "Hakan Bilen"
            ],
            "title": "Dataset condensation with gradient matching",
            "venue": "In International Conference on Learning Representations,",
            "year": 2020
        }
    ],
    "sections": [
        {
            "heading": "1 INTRODUCTION",
            "text": "Data augmentation is a cornerstone in improving machine learning models, especially when labeled data is scarce. It enhances model performance by introducing varied training samples. Though traditional methods like rotation and cropping are widely used, they operate under a one-size-fits-all assumption that often falls short in the complexity of real-world data. The key is not just to augment data, but to do it in a way that does not mislead the learning process.\nRecent work emphasizes the benefits of learned data augmentation, where techniques such as AutoAugment (Cubuk et al., 2019) and RandAugment (Cubuk et al., 2020) adapt to specific datasets and tasks. While promising, this area is still nascent and lacks a comprehensive framework to address diverse tasks and data nuances. Furthermore, selecting meaningful transformations remains a challenge, often relying on heuristics or domain expertise, which is especially problematic in specialized fields. Inappropriate transformations can harm model performance, underscoring the need for systematic selection. Amid the rise of image generation methods, such as diffusion models and other generative AI, an abundance of synthetic data is available but requires discerning use. A recent study, LP-A3 (Yang et al., 2022a), aims to generate \u201chard positive examples\u201d for augmentation but risks introducing false positives that could mislead learning. Another recent work, Soft-Augmentation (Liu et al., 2023), introduces soft learning targets and loss reweighting to train on augmented samples but is primarily limited to improving image crop augmentation. The overarching need is for smarter, more adaptable data augmentation algorithms.\nThis paper proposes SAFLEX (Self-Adaptive Augmentation via Feature Label Extrapolation), which automatically learns the sample weights and soft labels of augmented samples provided by any given upstream augmentation pipeline. Existing learnable augmentation methods that directly learn in the feature space (e.g., image space) often restrict augmentation scope due to differentiability needs\nand suffer from complicated training in high-dimensional spaces. Contrary to this, we advocate for learning only low-dimensional sample weights and soft labels for each augmented instance sourced from a pre-existing upstream augmentation pipeline like synthetic data generation. While upstream augmentation methods can sometimes alter labels or introduce noise, especially when creating samples outside the data distribution, our approach offers a mechanism to correct them. By calibrating sample weights and labels after augmentation, we considerably alleviate issues stemming from upstream augmentation methods. Without the complexity of learning augmentation transformations from scratch, this strategy ensures that augmentation is both diverse and consistent with the inherent data distribution, thereby fostering better generalization across various tasks. See Fig. 1 for a demonstration of our proposed SAFLEX.\nWe frame learning sample weights and soft labels as a bilevel optimization problem. This captures the interdependent nature of the model and its augmented data: the model\u2019s performance depends on the quality of the augmented data, which in turn is guided by the model itself (Bard, 2013). This new perspective advances our understanding of data augmentation, offering a theoretical framework that underpins its practical applications. Despite the bilevel nature of the problem, direct solutions are computationally infeasible for large-scale real-world applications. To mitigate this, we propose a streamlined, greedy, online, single-level approximation algorithm, which optimizes a gradientmatching objective to accelerate the learning process.\nWe conducted extensive empirical evaluations to highlight SAFLEX\u2019s superior performance and adaptability. On eight medical images (Yang et al., 2023), SAFLEX elevates popular augmentation techniques like RandAugment (Cubuk et al., 2020) and Mixup (Zhang et al., 2018), boosting performance by up to 3.6%. On seven tabular datasets, SAFLEX shows compatibility with categorical data and effectively enhances CutMix (Yun et al., 2019). Furthermore, SAFLEX improves image augmentations from diffusion models, yielding an average improvement of 1.9% in fine-grained classification and out-of-distribution generalization against three diffusion-augmentation methods, harnessing on their pre-trained expertise. We also validate SAFLEX\u2019s integration with contrastive learning through a CLIP fine-tuning experiment. These findings underline its versatility across varied data types and learning tasks.\nOur contributions are threefold: (1) We unveil a novel parametrization for learnable augmentation complemented by an adept bilevel algorithm primed for online optimization. (2) Our SAFLEX method is distinguished by its universal compatibility, allowing it to be effortlessly incorporated into a plethora of supervised learning processes and to collaborate seamlessly with an extensive array of upstream augmentation procedures. (3) The potency of our approach is corroborated by empirical tests on a diverse spectrum of datasets and tasks, all underscoring SAFLEX\u2019s efficiency and versatility, boosting performance by1.2% on average over all experiments."
        },
        {
            "heading": "2 PROPOSED METHOD: SAFLEX",
            "text": "Our goal is to refine augmented samples from any upstream pipeline to enhance classifier generalization. The proposed methodology is founded on two pivotal questions: (1) Which aspects of the augmented samples should be refined? (2) What approach should be taken to learn these refined samples? We start from these questions and defer the derivation of the algorithm to Section 3.\nLimitations of Augmentation Methods. Data augmentation is pivotal in enhancing model generalization. However, its limitations, particularly the unintentional introduction of noise, can sometimes outweigh its benefits. For instance, consider the widespread use of random cropping on natural images. Although largely effective, there are times when this approach inadvertently omits task-relevant information, leading to unintended outcomes like false positives. This inherent noise creates a trade-off: under-augmentation may yield insufficient challenging examples, whereas overaugmentation can flood the dataset with misleading samples. As shown in Fig. 2a, reducing the noise in augmentation is the key to resolving the dilemma.\nNoise in augmentation primarily arises from two fundamental challenges: (1) the deviation of augmented samples from the original data distribution and (2) the potential mislabeling of augmented samples. We shall envision augmentation as a method to harness prior knowledge in capturing the underlying data distribution. This distribution is encapsulated in the joint distribution, PXY (x, y), where x \u2208 X are features and y \u2208 {1, . . . ,K} represents labels, with K indicating the number of classes. Breaking down this joint distribution: PXY (x, y) = PX(x) \u00b7 PY |X(y|x), we observe that the primary source of noise is associated with the feature distribution PX(x), while the secondary source is tied to the conditional distribution PY |X(y|x). Addressing these challenges, our methodology is designed to integrate seamlessly with any upstream augmentation process, amending both types of errors post-augmentation, and considering the initial augmentation process as a separate, unchanged entity.\nFeature and Label Extrapolation. A key concern in data augmentation pertains to addressing these two types of errors. Some prior works on learning augmentation (e.g., (Yang et al., 2022a)) attempted to reduce noise by fine-tuning augmented features, using them as initializations. Specifically, the aim was to derive a modified feature x\u2032 that eliminates both error types. Yet, due to the high-dimensionality of feature space X , manipulating x is computationally burdensome. A more efficient strategy is to handle the errors individually and abstain from modifying x. When encountering erroneous estimation of the feature distribution PX(x), even if augmented samples lie in low-density areas, we can compensate by modulating the sample weights w \u2208 [0, 1] in the empirical risk minimization loss. Specifically, rarer augmented features are assigned decreased sample weights. For inaccuracies in estimating the conditional distribution PY |X(y|x), it\u2019s advantageous to modify the augmented label y directly. We also propose transitioning from a hard class label to a soft one, denoted as y, representing a probability mass across K classes, residing in the K-dimensional simplex y \u2208 \u2206K . The proposed refinement of augmented samples is depicted in Eq. (1). Remarkably, optimizing these sample weights and soft labels effectively mitigates errors resulting from varied augmentation methods across numerous classification challenges.\n(x, y) Upstream Augment\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2192 (xaug, yaug) SAFLEX\u2212\u2212\u2212\u2212\u2212\u2192\n( waug\nsample weight \u2208 [0, 1] , xaug, yaug ) soft label \u2208 \u2206K (1)\nTo elucidate, consider a hypothetical example in Fig. 2b. Envision a training sample from the green class (represented by a pronounced green dot). Upon applying a noise-prone augmentation, such as Gaussian perturbation in a 2D setting, the augmented sample could either (1) fall into a region with few validation samples regardless of their class, or (2) be overwhelmingly encompassed by validation samples from a different class. In the former case, it is judicious to reduce the sample weights since they might not be pivotal in discerning the conditional distribution. In the latter instance, the label of the augmented sample should be fine-tuned. This can entail a shift to a soft label to rectify or mitigate potential label inconsistencies, informed by patterns in the validation set.\nBilevel Formulation. The remaining question in our design is how to learn the sample weights and soft labels for augmented samples. The overarching goal of augmentation is enhancing model generalization. While the test set remains inaccessible, a prevalent approach is to fine-tune performance using a validation set. This methodology aligns with standard practices in hyperparameter optimization and is evidenced in learnable augmentation methods such as AutoAugment (Cubuk et al., 2019) and RandAugment (Cubuk et al., 2020). Given a neural network f (\u00b7) : X \u2192 \u2206K (where we assume Softmax is already applied and the outputs are L1 normalized) with parameter \u03b8, let us denote the training set, validation set, and the set of augmented samples as Dtrain, Dval, and Daug, respectively. The ambition is to refine Daug such that a model trained on the amalgamation of Dtrain \u222a Daug optimizes performance on Dval.\nmin Daug, \u03b8 L(Dval, \u03b8) s.t. \u03b8 \u2208 argmin \u03b8\u2032 L(Dtrain \u222a Daug, \u03b8\u2032) (2)\nThis scenario can be cast as a bilevel optimization problem as in Eq. (2), where Daug, the set of augmented samples with parametrized by sample weights and soft labels, and the model parameters \u03b8 are learnable. The conventional model training constitutes the inner level, while the quest to identify optimal augmented samples Daug, which minimize the validation loss post-inner level training, establishes the outer problem. Such a paradigm inherently transforms learnable augmentation into bilevel optimization. Intriguingly, much of the existing literature on learnable augmentation eschews this representation. The primary reservations stem from concerns related to efficiency and differentiability. Notably, works such as (Mounsaveng et al., 2021; 2023) are among the sparse few to apply bilevel optimization for augmentation learning, yet their focus remains constricted to affine transformations. In contrast, our approach sidesteps the modification and modeling of feature augmentation, obviating the challenge of differentiability. The low-dimensional nature of sample weights and soft labels potentially simplifies the learning process. In subsequent sections, we demonstrate that, under benign approximations, we can adeptly navigate the bilevel problem, determining the apt sample weights and soft labels within a singular step for each training iteration."
        },
        {
            "heading": "3 ALGORITHM",
            "text": "We now develop an algorithm for the bilevel problem described in Eq. (2).\nThe Greedy Approach. Bilevel optimization is notoriously challenging, often necessitating nested loops, which introduces significant computational overhead. Upon inspecting Eq. (2), it becomes evident that an essential characteristic of the problem \u2014 the training dynamics of the model \u2014 has been understated. In standard practice, augmented samples are typically generated during model training for each minibatch across all iterations. Therefore, the actual problem deviates from Eq. (2) in two ways: (1) different batches of augmentation may influence the learned parameters differently, and the model is not trained on a cumulative set of augmented samples, and conversely, (2) the learned parameters are affected differently by the refined augmented samples across batches, implying that augmentation should be optimized with respect to the corresponding model parameters.\nTo incorporate model optimization dynamics, we should reformulate the problem on a finer scale: Given the model parameter \u03b8t\u22121 at an intermediate training step, how can we determine the batch of refined augmented samples, Dbatchaug ? Through a greedy approach, we posit that the granular objective is to minimize the validation loss after a single update, denoted as L(Dval, \u03b8t), where \u03b8t is the model parameter updated from \u03b8t\u22121.\nmin Dbatchaug , \u03b8t\nL(Dval, \u03b8t) s.t. \u03b8t = \u03b8t\u22121 \u2212 \u03b1 \u00b7 \u2207\u03b8L(Dbatchtrain \u222a Dbatchaug , \u03b8t\u22121) (3)\nThis micro-perspective of Eq. (2) is represented in Eq. (3), where the batch of augmented samples Dbatchaug = {(w aug 1 , x aug 1 ,y aug 1 ), . . . , (w aug B , x aug B ,y aug B )} is parametrized by the set of sample weights (waug1 , . . . , w aug B ) and soft labels (y aug 1 , . . . ,y aug B ).\nAs a direct consequence, if the inner loop uses a first-order optimizer like SGD (as assumed), this significantly eases the optimization task. The emergent problem is no longer bilevel. With the analytical solution of the \u201cinner problem\u201d at our disposal, we can integrate the formula for \u03b8t into the outer objective, L(Dval, \u03b8t), converting it into a single-level problem. Efficient Solution. We next derive an algorithm for efficiently addressing Eq. (3). Crucially, due to the linearity of the loss function L(\u00b7, \u03b8t\u22121) with respect to datasets and the inherent linearity of gradient computation, the gradient vector for the combined training and augmentation batch linearly\nrelates to the sample weights and soft labels, assuming the sample-wise loss function, such as the cross-entropy loss, behaves linearly with respect to sample weights and soft labels. Validating this, the cross-entropy loss is indeed linear concerning these variables, a typical characteristic based on their definitions.\nBy approximating the validation loss L(Dval, \u03b8t) up to the first order around parameter \u03b8t, we can recast Eq. (3) as a linear programming problem. The objective seeks to maximize the inner product of the gradient vectors on the combined train and augmented batch and the validation batch, effectively yielding a gradient-matching loss (Zhao et al., 2020). Here, both the objective and normalization constraints linearly correspond to our learnable variables: sample weights and soft labels. The derivation is provided in Appendix A, and we summarize the solution in the subsequent notations and theorem. Notation 1. Let the Jacobian matrix of logits with respect to the model parameter be \u2207\u03b8f (xaug) |\u03b8=\u03b8t\u22121\u2208 RK\u00d7m and the gradient vector on the validation set be \u2207\u03b8L(Dval, \u03b8t\u22121) \u2208 Rm, where m is the parameter count and K is the class count. The Jacobian-vector product is denoted as \u03a0 = \u2207\u03b8f (xaug) |\u03b8=\u03b8t\u22121 \u2207\u03b8L(Dval, \u03b8t\u22121) \u2208 RK , which is efficiently computable. Theorem 1 (Solution of Eq. (3)). The approximated soft label solution is y = OneHot (argmaxk[\u03a0]k), where OneHot(\u00b7) denotes one-hot encoding, and the sample weight solution is w = 1 if \u2211K k=1[\u03a0]k \u2265 0; otherwise, w = 0. Theorem 1 illustrates that an effective approximation of Eq. (3) is computationally efficient. The gradient inner product, \u03a0, a Jacobian-vector product, is readily computed alongside standard back-propagation on the combined training and augmentation batch. While determining the validation set gradient vector mandates an additional back-propagation step, we can approximate the gradient vector for the complete validation set, \u2207\u03b8L(Dval, \u03b8t\u22121), using a minibatch gradient, \u2207\u03b8L(Dbatchval , \u03b8t\u22121). Despite necessitating a solution for Eq. (3) at every iteration, our efficient SAFLEX algorithm incurs minimal computational overhead.\nA notable takeaway from Theorem 1 is that while we aim to learn continuous sample weights (in [0, 1]) and soft labels (in \u2206K), the derived solutions consistently yield discrete values: either 0 or 1 and one-hot vectors. This consistency does not signify a coarse approximation, especially considering we resolve Eq. (3) with a O(\u03b1) tolerance, where \u03b1 is typically small. Nonetheless, this characteristic could potentially impact model generalization in under-parameterized scenarios.\nGeneralization Aspects. Let\u2019s interpret and examine the solution provided by Theorem 1 from a generalization standpoint, which is our primary objective. The loss function\u2019s linearity helps understand Theorem 1.. Given that Eq. (3) is a linear program with straightforward normalization constraints, we effectively form a linear combination of KB gradient vectors (each pertaining to a logit of the augmented sample), with B representing the augmented batch size, to approximate the m-dimensional validation gradient vector. The total constraints sum up to B + 1. If these KB gradient vectors are linearly independent, we can always align the combined gradient vector with the validation gradient vector when the degree of freedom, B+KB\u2212(B+1), is greater than or equal to the gradient vector dimension, m. This is represented by the condition KB > m. Such a scenario, exceedingly under-parametrized, is rare in deep learning. If the combined gradient vector consistently aligns with the validation gradient vector, training with SAFLEX will approximate training on the combined training and validation sets, potentially limiting the generalization improvements.\nTo enhance generalization, it is essential to circumvent the challenges of the under-parametrized paradigm, even if we are not closely approaching it. Here, we suggest two modifications to the solution given by Theorem 1:\n1. Encouraging Retention of the Original Label. We can introduce a minor constant penalty term to the gradient inner product to incentivize retaining the augmented sample\u2019s original label. Thus, we substitute \u03a0 with \u03a0+ \u03b2eyaug, where eyaug is a one-hot vector with a value of 1 at the yaug-th position. If no other entry in \u03a0 exceeds [\u03a0]yaug by a margin of at least \u03b2, the learned label remains unaltered. This approach proves especially valuable when the validation set is of limited size.\n2. Substituting argmax with Gumbel-SoftMax. Our current solution invariably yields hard labels. This can sometimes manifest as an excessive degree of confidence, particularly when \u03a0 contains multiple significant entries. To alleviate this, we can employ the Gumbel-SoftMax function to introduce a \u201dsoftening\u201d effect to the learned labels, adding a measure of stochasticity. Hence, we have y = softmax (( \u03a0 + \u03b2eyaug + g ) /\u03c4 ) , where g consists of i.i.d. random variables sourced from\nGumbel(0, 1). Typically, unless specified otherwise, we opt for a relatively low fixed temperature value, \u03c4 = 0.01.\nThe pseudo-code of SAFLEX for cross-entropy loss is shown as Algorithm 1.\nAlgorithm 1: SAFLEX (Cross-Entropy Loss, Single batch). Input: Neural network f (\u00b7) : X \u2192 \u2206K (softmax applied on outputs) with parameters \u03b8, upstream\naugmented batch {(xaug1 , y aug 1 ), . . . , (x aug B , y aug B )}, validation batch\nDbatchval = {(xval1 , yval1 ), . . . , (xvalB\u2032 , yvalB\u2032 )}, penalty coefficient \u03b2, temperature \u03c4 . 1 Compute the gradient vector for the validation batch \u2207\u03b8L(Dbatchval , \u03b8). 2 for i = 1, . . . , B do // The actual implementation is vectorized. 3 Determine the gradient inner product \u03a0i = \u2207\u03b8f (xaugi )\u2207\u03b8L(D batch val , \u03b8) via Jacobian-vector product.\n4 Apply Gumbel-SoftMax to get yi = softmax (( \u03a0i + \u03b2eyaugi + g ) /\u03c4 ) , where eyaugi \u2208 R\nK is one-hot at yaugi , and g consists of i.i.d. random variables taken from Gumbel(0, 1).\n5 Set wi = 1 if \u03a0i \u00b7 yi \u2265 0, otherwise set wi = 0. 6 Renormalize the sample weights w1, . . . , wB to sum to 1. 7 return Sample weights waug1 , . . . , w aug B , and soft labels y aug 1 , . . . ,y aug B .\nSAFLEX for Contrastive Learning. We conclude this section by discussing to encompass the generalization of the proposed method for certain contrastive learning losses, as illustrated in Eq. (12) and Eq. (13). Notably, the latter is utilized for CLIP training. In the realm of contrastive learning, labels are not conventionally defined. Yet, one can perceive the contrastive training objectives in Eq. (12) and Eq. (13) as proxy classification tasks. Here, we posit that the batch of size B can be construed as containing B classes: one positive example coupled with B\u22121 negative examples. This interpretation paves the way to introduce the notion of (soft) labels over this surrogate classification task with its B distinct classes.\nUnder this paradigm, the loss function remains linear concerning the soft labels and sample weights, making the methodology in Theorem 1 applicable. The sole requisite modification pertains to the gradient inner product\u2019s definition. Rather than employing gradients from the cross-entropy logits, \u2207\u03b8f (xaug), we substitute them with gradients corresponding to the contrastive learning logits."
        },
        {
            "heading": "4 RELATED WORKS",
            "text": "Traditional data augmentation techniques such as random flipping and cropping (Krizhevsky et al., 2017; Simard et al., 2003; Shorten & Khoshgoftaar, 2019) are hand-crafted and static, unlike our adaptive SAFLEX method that tunes sample weights based on validation performance. Autonomous approaches like AutoAugment (Cubuk et al., 2019; Lim et al., 2019; Ho et al., 2019; Mounsaveng et al., 2021; 2023) learn transformations but are restricted in scope, primarily focusing on affine transformations. Generative methods employing GANs or diffusion models (Odena et al., 2017; Sankaranarayanan et al., 2018; Huang et al., 2018; He et al., 2022; Shipard et al., 2023; Dunlap et al., 2023; Trabucco et al., 2023) can inadvertently alter class-relevant features, which our method avoids by adaptively adjusting sample weights. Research on adversarial perturbations (Goodfellow et al., 2015; Yang et al., 2022a;b; Ho & Nvasconcelos, 2020) and noise-robust learning (Han et al., 2018; Lang et al., 2022; Thulasidasan et al., 2019; Konstantinov & Lampert, 2019; Gao et al., 2022; Ma et al., 2018; Kremer et al., 2018) address similar problems but often suffer from complexity and stability issues, which we mitigate by our principled approach to weight adjustment. Recently, Soft-Augmentation (Liu et al., 2023) also proposes to use soft labels and sample weights to train on augmented samples. However, it implements a specific formula to generate them based on the strength parameter of upstream augmentations. This limits the applicability of Soft-Augmentation mostly to crop augmentation on images. In addition, Bhattarai et al. (2020) proposed a progressive sampling strategy for GAN synthetic data, while Caramalau et al. (2021) introduced a sequential graph convolutional network for active learning. Our work extends these findings by developing a novel sampling and purifying method for augmented data that is specifically designed to improve the performance of downstream tasks.\nFor a more detailed discussion of related works, please refer to Appendix B."
        },
        {
            "heading": "5 EXPERIMENTS",
            "text": "We validate the effectiveness of SAFLEX under four very different learning scenarios: (1) adapting augmentations to medical images, (2) refining augmentations for tabular data, (3) purifying diffusion-model-generated augments, and (4) applying to contrastive fine-tuning. Experimental setups and implementation details are provided in Appendix C.\nAdapting Augmentations to Medical Images. Unlike natural images, medical images often carry quantitative information (e.g., encoded as color) and objects without a canonical orientation. While we usually lack the domain knowledge to design effective heuristic augmentation transformations for these images, applying augmentation pipelines designed for natural images, such as RandAugment (Cubuk et al., 2020), can sometimes degrade performance in the medical context (Yang et al., 2022a). Consequently, we investigate whether SAFLEX can adapt these augmentation pipelines for medical images.\nWe assess multi-class classification across eight medical image datasets from MedMNIST (Yang et al., 2023), with each dataset comprising 10K to 236K 28\u00d728 images and 4 to 11 classes. In line with (Yang et al., 2021), we train a ResNet-18 model (He et al., 2016) using the Adam optimizer (Kingma & Ba, 2014) for 100 epochs. For upstream augmentation, we utilize the widelyadopted RandAugment (Cubuk et al., 2020) and Mixup (Zhang et al., 2018) methods. Test accuracies are presented in Table 1, highlighting that SAFLEX significantly enhances the performance of both RandAugment and Mixup. It\u2019s noteworthy that SAFLEX, when combined with basic upstream augmentations as shown in Table 1, achieves better performance than Soft-Augmentation (Liu et al., 2023), and comparable or superior performance than the adversarial-perturbation-based augmentation, LP-A3 (Yang et al., 2022a). The latter not only takes significantly longer to train but also demands careful hyperparameter tuning. For a comprehensive view, Soft-Augmentation\u2019s performance and LP-A3\u2019s performance on the MedMNIST datasets can be found in Appendix C.\nIn terms of efficiency, SAFLEX, designed as an augmentation plug-in, requires only a single-step update per iteration. It only extends the average wall-clock time of a training epoch by roughly 42% in this experiment; see Appendix C for details.\nRefining Augmentations for Tabular Data. Tabular data typically encompasses heterogeneous features that include a blend of continuous, categorical, and ordinal values. The presence of discrete features constrains the space of potential transformations. Furthermore, the domain knowledge to design invariant, label-preserving transformations is often absent. One of the few traditional augmentation techniques directly applicable to tabular data is CutMix (Yun et al., 2019), which substitutes a portion of continuous or discrete features with values from other randomly chosen samples (see Appendix C for implementation details). However, studies suggest that CutMix, with a relatively small augmentation probability like 0.1, struggles to bolster tabular classification performance (Onishi & Meguro, 2023). Conversely, a higher augmentation probability can introduce excessive noise, potentially downgrading the performance. This leads us to explore whether SAFLEX can mitigate the noise from CutMix and enhance classification performance.\nOur experiments span seven tabular datasets varying in size (from 452 to 494K) and feature types (from exclusively continuous features to predominantly discrete ones); detailed dataset information and statistics are available in Appendix C. Except for the Volkert dataset, which involves 10-way classification, all other datasets focus on binary classification. Notably, some datasets, like Credit, exhibit a significantly skewed class distribution (e.g., only 0.17% positive). We consider backbone models such as the sample Multilayer Perceptron (MLP) with two hidden layers and 256 neurons\neach and tranformer-based models like SAINT (Somepalli et al., 2022) (without contrastive pretraining). These models undergo training with dropout (Srivastava et al., 2014) and, in certain cases, batch normalization, for 200 epochs.\nTable 2 shows that SAFLEX almost consistently enhances the performance of CutMix across all datasets, regardless of whether the MLP or SAINT model is used. This improvement is especially noticeable with the MLP backbone, which is typically more intricate to train, and on datasets abundant in discrete features, such as Click and Shrutime, where CutMix tends to inject more noise. Notably, the Volkert dataset demonstrates a considerable performance impovement, potentially attributed to the fact that it has 10 classes where soft labels might be more useful.\nPurifying Diffusion-Model-Generated Augments. Recent research (Dunlap et al., 2023; Trabucco et al., 2023) has advocated the application of diffusion models for image editing via text prompts. Compared to traditional augmentation techniques, images produced by pretrained diffusion models maintain task-specific details while offering enhanced domain diversity, as dictated by the prompts. Diffusion-model-generated augmentations have been found particularly efficacious in fine-grained classification and out-of-distribution (OOD) generalization tasks (Dunlap et al., 2023). However, these diffusion models occasionally generate subtle image alterations, potentially corrupting classessential information, thus underscoring the necessity for noise reduction (Dunlap et al., 2023). In this context, we probe the capability of SAFLEX to enhance the purity of diffusion-model-generated augmentations, aiming for improved classification outcomes.\nIn our experimentation, we adhere to the setups in (Dunlap et al., 2023). We assess SAFLEX using diffusion-model-generated images derived from two distinct approaches: (1) The Img2Img approach involves an image encoder that first converts a given image into a latent representation. Subsequently, employing a diffusion model (specifically, Stable Diffusion v1.5 (Rombach et al., 2022) for this experiment), this latent representation undergoes a series of prompt-conditioned transformations. Ultimately, the altered representation is decoded, yielding an augmented image reflecting the modifications stipulated in the prompt. Notably, the diffusion model may or may not undergo finetuning (w/ and w/o finetune) on the dataset in question. (2) The InstructPix2Pix approach (Brooks et al., 2023) accepts an image and an edit instruction sampled (e.g., \u201cposition the animals within the forest\u201d) and outputs a correspondingly modified image. InstructPix2Pix is a conditional diffusion model pretrained on a dataset containing paired images and their associated edit instructions.\nOur evaluation encompasses two tasks: (1) Fine-grained classification on a CUB dataset subset (Wah et al., 2011) (featuring 25 images per category). (2) OOD generalization on an iWildCam subset from the Wilds dataset (Koh et al., 2021) (consisting of over 6,000 images and simplified to 7- way classification). We use a ResNet-50 model (He et al., 2016) pretrained on ImageNet (Deng et al., 2009). For comparison, we also consider data generated solely from text (Text2Img) and the RandAugment method as baselines.\nResults, as depicted in Table 3, affirm that SAFLEX consistently elevates the performance of all three diffusion-model-generated augmentation techniques, across both fine-grained classification and OOD generalization tasks. Notably, the performance boost is more prominent within the OOD generalization task, where feature and label distortions are particularly detrimental. We confirm that SAFLEX is useful to refine diffusion-model-generated augmentations, leading to enhanced classification accuracy.\nApplying to Contrastive Fine-Tuning. We next shift our focus from the empirical risk minimization (ERM) framework utilizing cross-entropy loss, as demonstrated in the prior scenarios. To test\nthe adaptability and compatibility of SAFLEX with contrastive loss, we turn to a contrastive finetuning paradigm termed \u201cFinetune Like You Pretrain\u201d (FLYP)(Goyal et al., 2023). This methodology offers a straightforward yet potent means to fine-tune pretrained image-text models, including notable ones like CLIP (Radford et al., 2021). Remarkably, by simply fine-tuning classifiers through the initial pretraining contrastive loss (refer to Eq. (13)), FLYP achieves uniformly better classification performance. This entails constructing prompts from class labels and subsequently minimizing the contrastive loss between these prompts and the image embeddings within the fine-tuning set.\nOur experimentation adopts the framework presented in (Goyal et al., 2023). Specifically, we finetune a CLIP model equipped with a ViT-B/16 encoder on the full iWildCam dataset from Wilds (Koh et al., 2021). Post fine-tuning, we adopt a strategy from (Goyal et al., 2023) that linearly interpolates weights between the pretrained and the fine-tuned checkpoints to optimize in-distribution (ID) performance. As our upstream augmentation technique, we employ RandAugment, following hyperparameter setups as described in (Koh et al., 2021). For handling the CLIP contrastive loss, we apply our tailored algorithm, detailed in Section 3. For an in-depth understanding, please refer to Appendix A and Appendix C.\nAs evidenced in Table 4, incorporating RandAugment alongside FLYP yields favorable outcomes. Moreover, the introduction of SAFLEX amplifies performance gains for both ID and OOD tasks, irrespective of whether ensembling is applied. This observation is particularly noteworthy, as it demonstrates that SAFLEX is compatible with contrastive loss, which is a key component of many training paradigms, including self-supervised learning."
        },
        {
            "heading": "6 CONCLUSIONS",
            "text": "Our study presents SAFLEX, a novel solution to current challenges in data augmentation. At its core, SAFLEX offers a paradigm shift from traditional, one-size-fits-all augmentation strategies to a more adaptive, data-driven approach. It allows for the learning of low-dimensional sample weights and soft labels for each augmented instance, thereby circumventing the complexities and limitations inherent in direct augmentation feature learning. Our method demonstrates universal compatibility, underscoring its vast potential for diverse data types in learning scenarios. Extensive empirical evaluations confirm SAFLEX\u2019s prowess, proving its adaptability from medical imaging contexts to the nuances of tabular and natural image datasets. While SAFLEX demonstrates promising results, there are certain factors to consider for optimal performance. A substantial and high-quality validation set is beneficial. A suboptimal set could limit its effectiveness. Additionally, the type of upstream augmentation methods selected plays a role, as it impacts the overall performance of SAFLEX. Our approach also entails some computational overhead due to frequent gradient evaluations. These considerations will be the focus of future studies to further refine the methodology. In essence, SAFLEX stands as a testament to the advancements in learnable data augmentation, ushering in a more adaptive and customized era of data-centric AI."
        }
    ],
    "year": 2023
}